{
  "date": "2024-04-05",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-04-05 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文主要聚焦于 AI 模型的优化、强化学习应用、LLMs 在科学推理和医疗诊断中的潜力，以及计算机视觉和知识图谱的创新。其中，AI 在科学研究的创意模拟（如 Anirban Mukherjee 的作品）和因果效应分析（如 David S. Watson 的研究）特别令人印象深刻，这些论文突显了 LLMs 的鲁棒性、解释性和实际应用价值。\n\n下面，我将挑选并讨论最具影响力和话题度的论文，先从核心 AI 和强化学习主题入手，再快速概述其他相关内容。限于篇幅，我会简要掠过较基础或应用性不强的论文。\n\n### 1. AI Knowledge and Reasoning: Emulating Expert Creativity in Scientific Research  \n（AI 知识与推理：模拟专家创意在科学研究中的应用）  \n这篇论文由 Anirban Mukherjee 和 Hannah Hanwen Chang 撰写，探讨了现代 AI 是否能模拟专家在科学领域的创意。核心贡献是通过一种新方法，让 AI 在未见过的研究文章上进行预测和评估，展示了 AI 在心理学研究中的推理能力。发现显示，AI 能处理专业知识和证据对齐，提升学术效率，可能加速知识型工作的自动化。\n\n### 2. Bounding Causal Effects with Leaky Instruments  \n（使用泄漏工具变量界定因果效应）  \n作者包括 David S. Watson 和 Ricardo Silva 等知名学者。论文提出了一种处理因果估计的新方法，针对传统工具变量的“排除准则”假设失效问题。关键创新是开发凸优化目标来界定平均治疗效应，并提供 R 包实现。发现表明，该方法在模拟数据上优于现有技术，提高了因果分析的鲁棒性，对社会科学和医学决策有重要启发。\n\n### 3. Hypothesis Generation with Large Language Models  \n（使用大型语言模型进行假设生成）  \n论文探索 LLMs 在数据驱动假设生成中的潜力。贡献包括一个强化学习算法，从少量样本中迭代生成假设，并证明其在分类任务中超越传统方法。发现显示，LLMs 不仅能生成高质量假设，还能揭示新洞见，如在真实数据集上的准确率提升 12.8%，这为科学发现提供了一个高效工具。\n\n### 4. Longitudinal Targeted Minimum Loss-based Estimation with Temporal-Difference Heterogeneous Transformer  \n（基于时差异异构 Transformer 的纵向目标最小损失估计）  \n作者 Mark van der Laan 等。论文引入 Deep LTMLE 方法，使用 Transformer 架构和时差异学习估计动态治疗策略的逆事实均值。核心发现是，该方法在复杂时序场景下优于现有算法，并在心血管研究中实际应用，展示了 AI 在流行病学中的统计推断潜力。\n\n### 5. Best Response Shaping  \n（最佳响应塑造）  \n论文提出 BRS 框架，用于多代理强化学习中的部分竞争环境。创新点是通过可微条件机制和问题回答方法，优化代理策略以对抗强大对手。发现显示，BRS 在 Coin Game 等环境中提升了社会福利，扩展了多代理 RL 的适用性。\n\n### 6. Exploring Autonomous Agents through the Lens of Large Language Models: A Review  \n（从大型语言模型视角探索自主代理）  \n这篇综述讨论 LLMs 在自主代理中的作用，如工具利用和评估平台。贡献在于总结了 LLMs 的挑战（如幻觉和多模态问题）和潜力，强调其在数字生活中的应用。发现显示，LLMs 正推动代理在医疗和客服领域的革命，但需解决评估难题。\n\n其他论文中，LLMs 相关主题继续活跃，如第10 篇 Fine-Tuning, Quantization, and LLMs（微调、量化与 LLMs：处理意外结果）探讨了 LLMs 安全性的权衡，发现微调可能增加攻击风险，但守卫机制能提升鲁棒性；第15 篇 Scope Ambiguities in Large Language Models（LLMs 中的范围歧义）揭示 LLMs 在处理歧义句时的表现接近人类，准确率高达 90%。这些工作强化了 LLMs 的解释性和泛化能力。\n\n在强化学习领域，第18 篇 Growing Q-Networks（增长 Q 网络：适应性控制分辨率的连续控制）引入自适应动作空间，显著提升了机器人任务的性能。医疗应用方面，第24 篇 Influence based explainability（基于影响的脑肿瘤分割解释性）使用 TracIn 算法提升 MRI 图像分割的可解释性，发现能更好地捕捉神经网络决策。\n\n快速掠过较少话题度的论文：如第14 篇 A Repository for Formal Contexts（形式上下文仓库），主要提出 FCA 数据集存储框架，贡献在于可持续性数据共享，但影响有限；第28 篇 Sentiment analysis and random forest（情感分析与随机森林用于 LLM 来源分类），使用随机森林检测文本来源，准确性较高，但方法较传统。\n\n总之，今天的论文突显 AI 模型在推理、医疗和优化方面的进展，LLMs 作为热点，展现了从创意生成到实际应用的潜力。读者可关注上述关键论文，探索 AI 的新边界！如果有特定兴趣，建议查看原摘要深入。明天的快报再见！",
  "papers": [
    {
      "arxiv_id": "2404.04452v2",
      "title": "Vision transformers in domain adaptation and domain generalization: a study of robustness",
      "title_zh": "视觉Transformer在领域适配和领域泛化中的鲁棒性研究",
      "authors": [
        "Shadi Alijani",
        "Jamil Fayyad",
        "Homayoun Najjaran"
      ],
      "abstract": "Deep learning models are often evaluated in scenarios where the data\ndistribution is different from those used in the training and validation\nphases. The discrepancy presents a challenge for accurately predicting the\nperformance of models once deployed on the target distribution. Domain\nadaptation and generalization are widely recognized as effective strategies for\naddressing such shifts, thereby ensuring reliable performance. The recent\npromising results in applying vision transformers in computer vision tasks,\ncoupled with advancements in self-attention mechanisms, have demonstrated their\nsignificant potential for robustness and generalization in handling\ndistribution shifts. Motivated by the increased interest from the research\ncommunity, our paper investigates the deployment of vision transformers in\ndomain adaptation and domain generalization scenarios. For domain adaptation\nmethods, we categorize research into feature-level, instance-level, model-level\nadaptations, and hybrid approaches, along with other categorizations with\nrespect to diverse strategies for enhancing domain adaptation. Similarly, for\ndomain generalization, we categorize research into multi-domain learning,\nmeta-learning, regularization techniques, and data augmentation strategies. We\nfurther classify diverse strategies in research, underscoring the various\napproaches researchers have taken to address distribution shifts by integrating\nvision transformers. The inclusion of comprehensive tables summarizing these\ncategories is a distinct feature of our work, offering valuable insights for\nresearchers. These findings highlight the versatility of vision transformers in\nmanaging distribution shifts, crucial for real-world applications, especially\nin critical safety and decision-making scenarios.",
      "tldr_zh": "这篇论文研究了视觉变压器（Vision Transformers）在领域适应（Domain Adaptation）和领域泛化（Domain Generalization）中的鲁棒性，旨在解决深度学习模型在数据分布偏移时的性能挑战。作者对领域适应方法进行了分类，包括feature-level、instance-level、model-level adaptations和混合方法，并对领域泛化策略分类为multi-domain learning、meta-learning、regularization techniques和data augmentation策略。论文通过提供全面的表格总结这些方法，强调了视觉变压器在管理分布偏移方面的多功能性和潜力，尤其适用于真实世界的安全决策场景。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "I.2.10; I.4.9; I.4.7; I.5.1"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.04452v2",
      "published_date": "2024-04-05 23:38:57 UTC",
      "updated_date": "2024-10-15 19:49:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:58:21.706103"
    },
    {
      "arxiv_id": "2404.04446v2",
      "title": "Bounding Causal Effects with Leaky Instruments",
      "title_zh": "翻译失败",
      "authors": [
        "David S. Watson",
        "Jordan Penn",
        "Lee M. Gunderson",
        "Gecia Bravo-Hermsdorff",
        "Afsaneh Mastouri",
        "Ricardo Silva"
      ],
      "abstract": "Instrumental variables (IVs) are a popular and powerful tool for estimating\ncausal effects in the presence of unobserved confounding. However, classical\napproaches rely on strong assumptions such as the $\\textit{exclusion\ncriterion}$, which states that instrumental effects must be entirely mediated\nby treatments. This assumption often fails in practice. When IV methods are\nimproperly applied to data that do not meet the exclusion criterion, estimated\ncausal effects may be badly biased. In this work, we propose a novel solution\nthat provides $\\textit{partial}$ identification in linear systems given a set\nof $\\textit{leaky instruments}$, which are allowed to violate the exclusion\ncriterion to some limited degree. We derive a convex optimization objective\nthat provides provably sharp bounds on the average treatment effect under some\ncommon forms of information leakage, and implement inference procedures to\nquantify the uncertainty of resulting estimates. We demonstrate our method in a\nset of experiments with simulated data, where it performs favorably against the\nstate of the art. An accompanying $\\texttt{R}$ package, $\\texttt{leakyIV}$, is\navailable from $\\texttt{CRAN}$.",
      "tldr_zh": "本文研究了工具变量（IVs）在存在未观测混杂时的因果效应估计问题，但传统方法依赖于排除标准（exclusion criterion），该假设在实践中常被违反，导致估计偏差。为解决这一问题，作者提出了一种新方法，用于线性系统中处理“leaky instruments”，这些工具变量允许有限度违反排除标准，通过凸优化目标（convex optimization objective）提供平均治疗效应（average treatment effect）的精确边界，并实现了推理程序（inference procedures）来量化不确定性。在模拟数据实验中，该方法比现有技术表现更优，并提供了配套 R 包（leakyIV）以便应用。",
      "categories": [
        "stat.ME",
        "cs.AI"
      ],
      "primary_category": "stat.ME",
      "comment": "Camera ready version (UAI 2024)",
      "pdf_url": "http://arxiv.org/pdf/2404.04446v2",
      "published_date": "2024-04-05 23:17:25 UTC",
      "updated_date": "2024-05-08 09:59:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:58:33.085116"
    },
    {
      "arxiv_id": "2404.04442v1",
      "title": "Exploring Autonomous Agents through the Lens of Large Language Models: A Review",
      "title_zh": "翻译失败",
      "authors": [
        "Saikat Barua"
      ],
      "abstract": "Large Language Models (LLMs) are transforming artificial intelligence,\nenabling autonomous agents to perform diverse tasks across various domains.\nThese agents, proficient in human-like text comprehension and generation, have\nthe potential to revolutionize sectors from customer service to healthcare.\nHowever, they face challenges such as multimodality, human value alignment,\nhallucinations, and evaluation. Techniques like prompting, reasoning, tool\nutilization, and in-context learning are being explored to enhance their\ncapabilities. Evaluation platforms like AgentBench, WebArena, and ToolLLM\nprovide robust methods for assessing these agents in complex scenarios. These\nadvancements are leading to the development of more resilient and capable\nautonomous agents, anticipated to become integral in our digital lives,\nassisting in tasks from email responses to disease diagnosis. The future of AI,\nwith LLMs at the forefront, is promising.",
      "tldr_zh": "这篇综述探讨了大型语言模型 (LLMs) 如何驱动自主代理在多领域任务中的发展，例如文本理解和生成，从而革新行业如客服和医疗。论文指出了代理面临的挑战，包括多模态、人类价值对齐、幻觉和评估问题，并介绍了提升技术的策略，如 prompting、reasoning、tool utilization 和 in-context learning。评估平台如 AgentBench、WebArena 和 ToolLLM 提供了 robust 方法来评估这些代理，推动更 resilient 和高效的自主系统在数字生活中的应用。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "47 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2404.04442v1",
      "published_date": "2024-04-05 22:59:02 UTC",
      "updated_date": "2024-04-05 22:59:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:58:44.936132"
    },
    {
      "arxiv_id": "2404.04436v1",
      "title": "AI Knowledge and Reasoning: Emulating Expert Creativity in Scientific Research",
      "title_zh": "翻译失败",
      "authors": [
        "Anirban Mukherjee",
        "Hannah Hanwen Chang"
      ],
      "abstract": "We investigate whether modern AI can emulate expert creativity in complex\nscientific endeavors. We introduce novel methodology that utilizes original\nresearch articles published after the AI's training cutoff, ensuring no prior\nexposure, mitigating concerns of rote memorization and prior training. The AI\nare tasked with redacting findings, predicting outcomes from redacted research,\nand assessing prediction accuracy against reported results. Analysis on 589\npublished studies in four leading psychology journals over a 28-month period,\nshowcase the AI's proficiency in understanding specialized research, deductive\nreasoning, and evaluating evidentiary alignment--cognitive hallmarks of human\nsubject matter expertise and creativity. These findings suggest the potential\nof general-purpose AI to transform academia, with roles requiring\nknowledge-based creativity become increasingly susceptible to technological\nsubstitution.",
      "tldr_zh": "本研究探讨现代 AI 是否能模仿专家在复杂科学研究中的创造力，引入一种新方法：利用 AI 训练截止日期后发布的原创文章，确保避免记忆化问题。AI 被要求从研究中删除发现（redacting findings）、预测结果，并评估预测与实际结果的匹配度。分析了 589 篇心理学期刊文章（来自四本领先期刊，覆盖 28 个月），结果显示 AI 在理解专业研究、deductive reasoning 和 evidentiary alignment 等方面表现出色，体现了人类专家的认知特征。这些发现表明，通用 AI 有潜力改造学术界，使知识型创造力角色更容易被技术取代。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.04436v1",
      "published_date": "2024-04-05 22:30:47 UTC",
      "updated_date": "2024-04-05 22:30:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:58:57.193023"
    },
    {
      "arxiv_id": "2404.06519v1",
      "title": "Best Response Shaping",
      "title_zh": "最佳响应塑造",
      "authors": [
        "Milad Aghajohari",
        "Tim Cooijmans",
        "Juan Agustin Duque",
        "Shunichi Akatsuka",
        "Aaron Courville"
      ],
      "abstract": "We investigate the challenge of multi-agent deep reinforcement learning in\npartially competitive environments, where traditional methods struggle to\nfoster reciprocity-based cooperation. LOLA and POLA agents learn\nreciprocity-based cooperative policies by differentiation through a few\nlook-ahead optimization steps of their opponent. However, there is a key\nlimitation in these techniques. Because they consider a few optimization steps,\na learning opponent that takes many steps to optimize its return may exploit\nthem. In response, we introduce a novel approach, Best Response Shaping (BRS),\nwhich differentiates through an opponent approximating the best response,\ntermed the \"detective.\" To condition the detective on the agent's policy for\ncomplex games we propose a state-aware differentiable conditioning mechanism,\nfacilitated by a question answering (QA) method that extracts a representation\nof the agent based on its behaviour on specific environment states. To\nempirically validate our method, we showcase its enhanced performance against a\nMonte Carlo Tree Search (MCTS) opponent, which serves as an approximation to\nthe best response in the Coin Game. This work expands the applicability of\nmulti-agent RL in partially competitive environments and provides a new pathway\ntowards achieving improved social welfare in general sum games.",
      "tldr_zh": "本研究探讨了多智能体深度强化学习（multi-agent deep reinforcement learning）在部分竞争环境中的挑战，传统方法如LOLA和POLA难以促进基于互惠的合作，因为它们仅考虑对手的少数优化步骤而易被利用。作者提出了一种新方法Best Response Shaping (BRS)，通过一个近似最佳响应的“detective”代理进行微分优化，并引入状态感知的可微条件机制，利用问答(QA)方法提取代理在特定环境状态下的行为表示，以适应复杂游戏。实验结果显示，BRS在Coin Game中对抗Monte Carlo Tree Search (MCTS)对手时表现出色，显著提升了性能。该工作扩展了多智能体RL在部分竞争环境中的适用性，并为一般和游戏中提升社会福利提供了新途径。",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.GT",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.06519v1",
      "published_date": "2024-04-05 22:03:35 UTC",
      "updated_date": "2024-04-05 22:03:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:59:09.036760"
    },
    {
      "arxiv_id": "2404.04420v1",
      "title": "The NES Video-Music Database: A Dataset of Symbolic Video Game Music Paired with Gameplay Videos",
      "title_zh": "翻译失败",
      "authors": [
        "Igor Cardoso",
        "Rubens O. Moraes",
        "Lucas N. Ferreira"
      ],
      "abstract": "Neural models are one of the most popular approaches for music generation,\nyet there aren't standard large datasets tailored for learning music directly\nfrom game data. To address this research gap, we introduce a novel dataset\nnamed NES-VMDB, containing 98,940 gameplay videos from 389 NES games, each\npaired with its original soundtrack in symbolic format (MIDI). NES-VMDB is\nbuilt upon the Nintendo Entertainment System Music Database (NES-MDB),\nencompassing 5,278 music pieces from 397 NES games. Our approach involves\ncollecting long-play videos for 389 games of the original dataset, slicing them\ninto 15-second-long clips, and extracting the audio from each clip.\nSubsequently, we apply an audio fingerprinting algorithm (similar to Shazam) to\nautomatically identify the corresponding piece in the NES-MDB dataset.\nAdditionally, we introduce a baseline method based on the Controllable Music\nTransformer to generate NES music conditioned on gameplay clips. We evaluated\nthis approach with objective metrics, and the results showed that the\nconditional CMT improves musical structural quality when compared to its\nunconditional counterpart. Moreover, we used a neural classifier to predict the\ngame genre of the generated pieces. Results showed that the CMT generator can\nlearn correlations between gameplay videos and game genres, but further\nresearch has to be conducted to achieve human-level performance.",
      "tldr_zh": "本文引入NES-VMDB数据集，包含98,940个NES游戏的游戏画面视频，每段视频配对其原声轨的MIDI格式音乐，旨在填补神经音乐生成模型从游戏数据学习的大规模数据集空白。数据集基于NES-MDB构建，通过收集长视频、切分成15秒片段、提取音频并使用音频指纹算法（如Shazam）自动匹配音乐。研究者提出基于Controllable Music Transformer的基线方法，实验结果显示条件生成模型提高了音乐结构质量，并能学习游戏画面与游戏类型（如游戏流派）的相关性，但尚未达到人类水平。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted for publication at the 19th International Conference on the\n  Foundations of Digital Games",
      "pdf_url": "http://arxiv.org/pdf/2404.04420v1",
      "published_date": "2024-04-05 21:41:20 UTC",
      "updated_date": "2024-04-05 21:41:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:59:21.426818"
    },
    {
      "arxiv_id": "2404.05758v1",
      "title": "Implicit Assimilation of Sparse In Situ Data for Dense & Global Storm Surge Forecasting",
      "title_zh": "翻译失败",
      "authors": [
        "Patrick Ebel",
        "Brandon Victor",
        "Peter Naylor",
        "Gabriele Meoni",
        "Federico Serva",
        "Rochelle Schneider"
      ],
      "abstract": "Hurricanes and coastal floods are among the most disastrous natural hazards.\nBoth are intimately related to storm surges, as their causes and effects,\nrespectively. However, the short-term forecasting of storm surges has proven\nchallenging, especially when targeting previously unseen locations or sites\nwithout tidal gauges. Furthermore, recent work improved short and medium-term\nweather forecasting but the handling of raw unassimilated data remains\nnon-trivial. In this paper, we tackle both challenges and demonstrate that\nneural networks can implicitly assimilate sparse in situ tide gauge data with\ncoarse ocean state reanalysis in order to forecast storm surges. We curate a\nglobal dataset to learn and validate the dense prediction of storm surges,\nbuilding on preceding efforts. Other than prior work limited to known gauges,\nour approach extends to ungauged sites, paving the way for global storm surge\nforecasting.",
      "tldr_zh": "这篇论文解决了风暴潮的短期预测挑战，特别是针对未知位置或无潮位计（ungauged sites）的区域。研究者使用神经网络隐式同化（Implicit Assimilation）稀疏的现场（In Situ）数据和粗略的海洋状态再分析（Ocean State Reanalysis），以实现密集和全球风暴潮预测。论文构建了一个全球数据集，并证明该方法能扩展到无测量站点的区域，为全球风暴潮预报提供新途径。",
      "categories": [
        "physics.data-an",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "physics.ao-ph",
        "stat.AP"
      ],
      "primary_category": "physics.data-an",
      "comment": "Accepted at CVPR EarthVision 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.05758v1",
      "published_date": "2024-04-05 21:28:56 UTC",
      "updated_date": "2024-04-05 21:28:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:59:31.788721"
    },
    {
      "arxiv_id": "2404.04403v1",
      "title": "Low-Rank Robust Subspace Tensor Clustering for Metro Passenger Flow Modeling",
      "title_zh": "低秩鲁棒子空间张量聚类用于地铁客流建",
      "authors": [
        "Jiuyun Hu",
        "Ziyue Li",
        "Chen Zhang",
        "Fugee Tsung",
        "Hao Yan"
      ],
      "abstract": "Tensor clustering has become an important topic, specifically in\nspatio-temporal modeling, due to its ability to cluster spatial modes (e.g.,\nstations or road segments) and temporal modes (e.g., time of the day or day of\nthe week). Our motivating example is from subway passenger flow modeling, where\nsimilarities between stations are commonly found. However, the challenges lie\nin the innate high-dimensionality of tensors and also the potential existence\nof anomalies. This is because the three tasks, i.e., dimension reduction,\nclustering, and anomaly decomposition, are inter-correlated to each other, and\ntreating them in a separate manner will render a suboptimal performance. Thus,\nin this work, we design a tensor-based subspace clustering and anomaly\ndecomposition technique for simultaneously outlier-robust dimension reduction\nand clustering for high-dimensional tensors. To achieve this, a novel low-rank\nrobust subspace clustering decomposition model is proposed by combining Tucker\ndecomposition, sparse anomaly decomposition, and subspace clustering. An\neffective algorithm based on Block Coordinate Descent is proposed to update the\nparameters. Prudent experiments prove the effectiveness of the proposed\nframework via the simulation study, with a gain of +25% clustering accuracy\nthan benchmark methods in a hard case. The interrelations of the three tasks\nare also analyzed via ablation studies, validating the interrelation\nassumption. Moreover, a case study in the station clustering based on real\npassenger flow data is conducted, with quite valuable insights discovered.",
      "tldr_zh": "本研究针对地铁客流建模中的高维张量聚类问题，提出了一种低秩鲁棒子空间聚类分解模型，以同时处理降维、聚类和异常分解的相互关联挑战。该模型结合Tucker decomposition、sparse anomaly decomposition和subspace clustering，确保在高维数据中实现鲁棒的维度降低和聚类。实验结果显示，该框架在模拟研究中比基准方法提高25%的聚类准确率，并通过实际地铁站聚类案例分析了三任务的相互关系，发现了宝贵的时空模式见解。",
      "categories": [
        "stat.ME",
        "cs.AI"
      ],
      "primary_category": "stat.ME",
      "comment": "Conditionally Accepted in INFORMS Journal of Data Science",
      "pdf_url": "http://arxiv.org/pdf/2404.04403v1",
      "published_date": "2024-04-05 21:00:43 UTC",
      "updated_date": "2024-04-05 21:00:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:59:44.178876"
    },
    {
      "arxiv_id": "2404.04399v1",
      "title": "Longitudinal Targeted Minimum Loss-based Estimation with Temporal-Difference Heterogeneous Transformer",
      "title_zh": "翻译失败",
      "authors": [
        "Toru Shirakawa",
        "Yi Li",
        "Yulun Wu",
        "Sky Qiu",
        "Yuxuan Li",
        "Mingduo Zhao",
        "Hiroyasu Iso",
        "Mark van der Laan"
      ],
      "abstract": "We propose Deep Longitudinal Targeted Minimum Loss-based Estimation (Deep\nLTMLE), a novel approach to estimate the counterfactual mean of outcome under\ndynamic treatment policies in longitudinal problem settings. Our approach\nutilizes a transformer architecture with heterogeneous type embedding trained\nusing temporal-difference learning. After obtaining an initial estimate using\nthe transformer, following the targeted minimum loss-based likelihood\nestimation (TMLE) framework, we statistically corrected for the bias commonly\nassociated with machine learning algorithms. Furthermore, our method also\nfacilitates statistical inference by enabling the provision of 95% confidence\nintervals grounded in asymptotic statistical theory. Simulation results\ndemonstrate our method's superior performance over existing approaches,\nparticularly in complex, long time-horizon scenarios. It remains effective in\nsmall-sample, short-duration contexts, matching the performance of\nasymptotically efficient estimators. To demonstrate our method in practice, we\napplied our method to estimate counterfactual mean outcomes for standard versus\nintensive blood pressure management strategies in a real-world cardiovascular\nepidemiology cohort study.",
      "tldr_zh": "本研究提出了一种名为 Deep LTMLE 的新方法，用于估计纵向问题设置中动态治疗策略下的反事实结果均值。该方法采用 transformer 架构结合 heterogeneous type embedding 和 temporal-difference learning 进行训练，并在初始估计后通过 Targeted Minimum Loss-based Estimation (TMLE) 框架修正偏差，从而提供基于渐近统计理论的95%置信区间。模拟实验显示，Deep LTMLE 在复杂长时序场景中显著优于现有方法，并在小样本短时序环境中与渐近高效估计器性能相当。最后，该方法应用于真实心血管流行病学队列研究，评估了标准与强化血压管理策略的反事实结果均值。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "stat.AP",
        "stat.ME"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.04399v1",
      "published_date": "2024-04-05 20:56:15 UTC",
      "updated_date": "2024-04-05 20:56:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:59:55.611345"
    },
    {
      "arxiv_id": "2404.04392v3",
      "title": "Fine-Tuning, Quantization, and LLMs: Navigating Unintended Outcomes",
      "title_zh": "微调、量化以及大型语言模型：应对意外结果",
      "authors": [
        "Divyanshu Kumar",
        "Anurakt Kumar",
        "Sahil Agarwal",
        "Prashanth Harshangi"
      ],
      "abstract": "Large Language Models (LLMs) have gained widespread adoption across various\ndomains, including chatbots and auto-task completion agents. However, these\nmodels are susceptible to safety vulnerabilities such as jailbreaking, prompt\ninjection, and privacy leakage attacks. These vulnerabilities can lead to the\ngeneration of malicious content, unauthorized actions, or the disclosure of\nconfidential information. While foundational LLMs undergo alignment training\nand incorporate safety measures, they are often subject to fine-tuning, or\ndoing quantization resource-constrained environments. This study investigates\nthe impact of these modifications on LLM safety, a critical consideration for\nbuilding reliable and secure AI systems. We evaluate foundational models\nincluding Mistral, Llama series, Qwen, and MosaicML, along with their\nfine-tuned variants. Our comprehensive analysis reveals that fine-tuning\ngenerally increases the success rates of jailbreak attacks, while quantization\nhas variable effects on attack success rates. Importantly, we find that\nproperly implemented guardrails significantly enhance resistance to jailbreak\nattempts. These findings contribute to our understanding of LLM vulnerabilities\nand provide insights for developing more robust safety strategies in the\ndeployment of language models.",
      "tldr_zh": "这篇论文探讨了 Large Language Models (LLMs) 在 fine-tuning 和 quantization 过程中可能引发的安全漏洞，包括 jailbreak attacks、prompt injection 和 privacy leakage attacks，这些可能导致恶意内容生成或信息泄露。研究通过评估 Mistral、Llama 系列、Qwen 和 MosaicML 等基础模型及其 fine-tuned 变体，发现 fine-tuning 通常会增加 jailbreak attacks 的成功率，而 quantization 对攻击成功率的影响则因具体情况而异。重要的是，适当实施的 guardrails 可以显著提升模型对攻击的抵抗力。这些发现为构建更可靠的 LLM 安全策略提供了关键见解。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.04392v3",
      "published_date": "2024-04-05 20:31:45 UTC",
      "updated_date": "2024-09-09 06:25:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:00:08.909112"
    },
    {
      "arxiv_id": "2404.04376v1",
      "title": "ClickDiffusion: Harnessing LLMs for Interactive Precise Image Editing",
      "title_zh": "翻译失败",
      "authors": [
        "Alec Helbling",
        "Seongmin Lee",
        "Polo Chau"
      ],
      "abstract": "Recently, researchers have proposed powerful systems for generating and\nmanipulating images using natural language instructions. However, it is\ndifficult to precisely specify many common classes of image transformations\nwith text alone. For example, a user may wish to change the location and breed\nof a particular dog in an image with several similar dogs. This task is quite\ndifficult with natural language alone, and would require a user to write a\nlaboriously complex prompt that both disambiguates the target dog and describes\nthe destination. We propose ClickDiffusion, a system for precise image\nmanipulation and generation that combines natural language instructions with\nvisual feedback provided by the user through a direct manipulation interface.\nWe demonstrate that by serializing both an image and a multi-modal instruction\ninto a textual representation it is possible to leverage LLMs to perform\nprecise transformations of the layout and appearance of an image. Code\navailable at https://github.com/poloclub/ClickDiffusion.",
      "tldr_zh": "本研究指出，现有的图像生成和操作系统难以用纯文本指令精确指定某些变换，例如改变图像中特定对象的属性和位置。作者提出 ClickDiffusion 系统，该系统结合自然语言指令与用户视觉反馈，通过将图像和多模态指令序列化为文本表示，利用 LLMs 执行精确的图像布局和外观变换。这种方法提升了交互式图像编辑的准确性和便利性，并提供了开源代码（https://github.com/poloclub/ClickDiffusion）。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "arXiv admin note: substantial text overlap with arXiv:2402.07925",
      "pdf_url": "http://arxiv.org/pdf/2404.04376v1",
      "published_date": "2024-04-05 19:38:18 UTC",
      "updated_date": "2024-04-05 19:38:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:00:21.685112"
    },
    {
      "arxiv_id": "2404.08674v1",
      "title": "Effects of Different Prompts on the Quality of GPT-4 Responses to Dementia Care Questions",
      "title_zh": "翻译失败",
      "authors": [
        "Zhuochun Li",
        "Bo Xie",
        "Robin Hilsabeck",
        "Alyssa Aguirre",
        "Ning Zou",
        "Zhimeng Luo",
        "Daqing He"
      ],
      "abstract": "Evidence suggests that different prompts lead large language models (LLMs) to\ngenerate responses with varying quality. Yet, little is known about prompts'\neffects on response quality in healthcare domains. In this exploratory study,\nwe address this gap, focusing on a specific healthcare domain: dementia\ncaregiving. We first developed an innovative prompt template with three\ncomponents: (1) system prompts (SPs) featuring 4 different roles; (2) an\ninitialization prompt; and (3) task prompts (TPs) specifying different levels\nof details, totaling 12 prompt combinations. Next, we selected 3 social media\nposts containing complicated, real-world questions about dementia caregivers'\nchallenges in 3 areas: memory loss and confusion, aggression, and driving. We\nthen entered these posts into GPT-4, with our 12 prompts, to generate 12\nresponses per post, totaling 36 responses. We compared the word count of the 36\nresponses to explore potential differences in response length. Two experienced\ndementia care clinicians on our team assessed the response quality using a\nrating scale with 5 quality indicators: factual, interpretation, application,\nsynthesis, and comprehensiveness (scoring range: 0-5; higher scores indicate\nhigher quality).",
      "tldr_zh": "该研究探讨了不同提示（prompts）对GPT-4在痴呆护理问题响应质量的影响，填补了这一医疗领域的研究空白。研究者开发了一个创新的提示模板，包括系统提示（SPs）（4个角色）、初始化提示和任务提示（TPs）（不同细节水平），总共12种组合，并使用它针对3个真实社交媒体帖子生成36个响应。评估结果显示，这些提示导致响应长度和质量（如factual, interpretation, application, synthesis及comprehensiveness指标）存在显著差异，由两名经验丰富的痴呆护理临床医生进行评分。总体而言，此工作为优化LLMs在医疗领域的应用提供了初步见解。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.08674v1",
      "published_date": "2024-04-05 19:24:57 UTC",
      "updated_date": "2024-04-05 19:24:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:00:35.048121"
    },
    {
      "arxiv_id": "2404.04351v2",
      "title": "Assisting humans in complex comparisons: automated information comparison at scale",
      "title_zh": "辅助人类进行复杂比较：大规模自动化信息比较",
      "authors": [
        "Truman Yuen",
        "Graham A. Watt",
        "Yuri Lawryshyn"
      ],
      "abstract": "Generative Large Language Models enable efficient analytics across knowledge\ndomains, rivalling human experts in information comparisons. However, the\napplications of LLMs for information comparisons face scalability challenges\ndue to the difficulties in maintaining information across large contexts and\novercoming model token limitations. To address these challenges, we developed\nthe novel Abstractive Summarization & Criteria-driven Comparison Endpoint\n(ASC$^2$End) system to automate information comparison at scale. Our system\nemploys Semantic Text Similarity comparisons for generating evidence-supported\nanalyses. We utilize proven data-handling strategies such as abstractive\nsummarization and retrieval augmented generation to overcome token limitations\nand retain relevant information during model inference. Prompts were designed\nusing zero-shot strategies to contextualize information for improved model\nreasoning. We evaluated abstractive summarization using ROUGE scoring and\nassessed the generated comparison quality using survey responses. Models\nevaluated on the ASC$^2$End system show desirable results providing insights on\nthe expected performance of the system. ASC$^2$End is a novel system and tool\nthat enables accurate, automated information comparison at scale across\nknowledge domains, overcoming limitations in context length and retrieval.",
      "tldr_zh": "该研究探讨了生成式大型语言模型（Generative Large Language Models, LLMs）在信息比较中的可扩展性挑战，如上下文维护和 token 限制问题，并提出了一种新型系统 Abstractive Summarization & Criteria-driven Comparison Endpoint (ASC²End) 来实现大规模自动化信息比较。系统采用 Semantic Text Similarity 比较、abstractive summarization 和 retrieval augmented generation 等策略来克服这些限制，并通过 zero-shot 策略设计提示以提升模型推理能力。实验评估使用 ROUGE 评分衡量摘要质量，并通过调查响应评估比较输出，结果显示系统在知识领域提供准确、可靠的分析，支持人类进行复杂比较。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "I.2.7; I.2.8"
      ],
      "primary_category": "cs.CL",
      "comment": "11 pages, 7 figures, 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2404.04351v2",
      "published_date": "2024-04-05 18:44:54 UTC",
      "updated_date": "2024-09-19 00:18:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:00:45.950752"
    },
    {
      "arxiv_id": "2404.04344v1",
      "title": "A Repository for Formal Contexts",
      "title_zh": "翻译失败",
      "authors": [
        "Tom Hanika",
        "Robert Jäschke"
      ],
      "abstract": "Data is always at the center of the theoretical development and investigation\nof the applicability of formal concept analysis. It is therefore not surprising\nthat a large number of data sets are repeatedly used in scholarly articles and\nsoftware tools, acting as de facto standard data sets. However, the\ndistribution of the data sets poses a problem for the sustainable development\nof the research field. There is a lack of a central location that provides and\ndescribes FCA data sets and links them to already known analysis results. This\narticle analyses the current state of the dissemination of FCA data sets,\npresents the requirements for a central FCA repository, and highlights the\nchallenges for this.",
      "tldr_zh": "这篇论文讨论了 Formal Concept Analysis (FCA) 中数据集的传播问题，强调数据是理论发展和应用的核心，但现有数据集缺乏统一管理，导致研究可持续性不足。文章分析了当前 FCA 数据集的分布状态，并提出了建立一个中心 FCA 仓库的要求，包括提供、描述数据集并链接已知分析结果。最终，它突出了实现这一仓库的潜在挑战，如数据标准化和维护难题。",
      "categories": [
        "cs.AI",
        "cs.DL"
      ],
      "primary_category": "cs.AI",
      "comment": "16 pages",
      "pdf_url": "http://arxiv.org/pdf/2404.04344v1",
      "published_date": "2024-04-05 18:27:04 UTC",
      "updated_date": "2024-04-05 18:27:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:00:58.029854"
    },
    {
      "arxiv_id": "2404.04332v1",
      "title": "Scope Ambiguities in Large Language Models",
      "title_zh": "大型语言模型中的作用域歧义",
      "authors": [
        "Gaurav Kamath",
        "Sebastian Schuster",
        "Sowmya Vajjala",
        "Siva Reddy"
      ],
      "abstract": "Sentences containing multiple semantic operators with overlapping scope often\ncreate ambiguities in interpretation, known as scope ambiguities. These\nambiguities offer rich insights into the interaction between semantic structure\nand world knowledge in language processing. Despite this, there has been little\nresearch into how modern large language models treat them. In this paper, we\ninvestigate how different versions of certain autoregressive language models --\nGPT-2, GPT-3/3.5, Llama 2 and GPT-4 -- treat scope ambiguous sentences, and\ncompare this with human judgments. We introduce novel datasets that contain a\njoint total of almost 1,000 unique scope-ambiguous sentences, containing\ninteractions between a range of semantic operators, and annotated for human\njudgments. Using these datasets, we find evidence that several models (i) are\nsensitive to the meaning ambiguity in these sentences, in a way that patterns\nwell with human judgments, and (ii) can successfully identify human-preferred\nreadings at a high level of accuracy (over 90% in some cases).",
      "tldr_zh": "本研究探讨了大型语言模型（Large Language Models）在处理范围歧义（Scope Ambiguities）方面的表现，这些歧义源于句子中多个语义操作符的互动。研究者引入了包含近1,000个独特歧义句子的全新数据集，涵盖各种语义操作符，并与人类判断进行比较，测试了如GPT-2、GPT-3/3.5、Llama 2和GPT-4等autoregressive语言模型的表现。结果显示，这些模型对歧义句子的含义敏感，且其处理模式与人类判断高度一致，在识别人类首选解读时准确率可达90%以上，为理解模型的语义结构和世界知识互动提供了重要洞见。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "To be published in Transactions of the Association for Computational\n  Linguistics",
      "pdf_url": "http://arxiv.org/pdf/2404.04332v1",
      "published_date": "2024-04-05 18:01:02 UTC",
      "updated_date": "2024-04-05 18:01:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:01:10.390958"
    },
    {
      "arxiv_id": "2404.04326v3",
      "title": "Hypothesis Generation with Large Language Models",
      "title_zh": "基于大语言模型的假设生成",
      "authors": [
        "Yangqiaoyu Zhou",
        "Haokun Liu",
        "Tejes Srivastava",
        "Hongyuan Mei",
        "Chenhao Tan"
      ],
      "abstract": "Effective generation of novel hypotheses is instrumental to scientific\nprogress. So far, researchers have been the main powerhouse behind hypothesis\ngeneration by painstaking data analysis and thinking (also known as the Eureka\nmoment). In this paper, we examine the potential of large language models\n(LLMs) to generate hypotheses. We focus on hypothesis generation based on data\n(i.e., labeled examples). To enable LLMs to handle arbitrarily long contexts,\nwe generate initial hypotheses from a small number of examples and then update\nthem iteratively to improve the quality of hypotheses. Inspired by multi-armed\nbandits, we design a reward function to inform the exploitation-exploration\ntradeoff in the update process. Our algorithm is able to generate hypotheses\nthat enable much better predictive performance than few-shot prompting in\nclassification tasks, improving accuracy by 31.7% on a synthetic dataset and by\n13.9%, 3.3% and, 24.9% on three real-world datasets. We also outperform\nsupervised learning by 12.8% and 11.2% on two challenging real-world datasets.\nFurthermore, we find that the generated hypotheses not only corroborate\nhuman-verified theories but also uncover new insights for the tasks.",
      "tldr_zh": "本文探讨了大型语言模型(LLMs)在基于数据生成假设方面的潜力，通过从少量标记例子生成初始假设，并使用受多臂赌博机启发的奖励函数进行迭代更新，以优化利用-探索权衡。实验结果显示，该方法在分类任务中显著提升预测性能，比few-shot prompting提高了31.7%（合成数据集）和13.9%、3.3%、24.9%（三个真实数据集）的准确率，并超越监督学习12.8%和11.2%在两个挑战性数据集上。生成的假设不仅验证了人类已知的理论，还揭示了新的任务洞见，为科学假设生成提供了高效工具。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "28 pages, 6 figures, code link:\n  https://github.com/ChicagoHAI/hypothesis_generation. Accepted by the 1st\n  Workshop on NLP for Science (NLP4Science) at EMNLP 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.04326v3",
      "published_date": "2024-04-05 18:00:07 UTC",
      "updated_date": "2024-12-18 19:00:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:01:22.142175"
    },
    {
      "arxiv_id": "2404.04254v3",
      "title": "Watermark-based Attribution of AI-Generated Content",
      "title_zh": "基于",
      "authors": [
        "Zhengyuan Jiang",
        "Moyang Guo",
        "Yuepeng Hu",
        "Neil Zhenqiang Gong"
      ],
      "abstract": "Several companies have deployed watermark-based detection to identify\nAI-generated content. However, attribution--the ability to trace back to the\nuser of a generative AI (GenAI) service who created a given piece of\nAI-generated content--remains largely unexplored despite its growing\nimportance. In this work, we aim to bridge this gap by conducting the first\nsystematic study on watermark-based, user-level attribution of AI-generated\ncontent. Our key idea is to assign a unique watermark to each user of the GenAI\nservice and embed this watermark into the AI-generated content created by that\nuser. Attribution is then performed by identifying the user whose watermark\nbest matches the one extracted from the given content. This approach, however,\nfaces a key challenge: How should watermarks be selected for users to maximize\nattribution performance? To address the challenge, we first theoretically\nderive lower bounds on detection and attribution performance through rigorous\nprobabilistic analysis for any given set of user watermarks. Then, we select\nwatermarks for users to maximize these lower bounds, thereby optimizing\ndetection and attribution performance. Our theoretical and empirical results\nshow that watermark-based attribution inherits both the accuracy and\n(non-)robustness properties of the underlying watermark. Specifically,\nattribution remains highly accurate when the watermarked AI-generated content\nis either not post-processed or subjected to common post-processing such as\nJPEG compression, as well as black-box adversarial post-processing with limited\nquery budgets.",
      "tldr_zh": "这篇论文探讨了基于watermark的技术，用于实现AI生成内容的用户级attribution，即追踪使用GenAI服务的用户。研究的关键想法是为每个用户分配唯一watermark，并将其嵌入AI生成内容中，然后通过匹配提取的水印进行归因；为解决watermark选择挑战，他们通过概率分析理论推导性能下界，并优化watermark选择以最大化检测和归因效果。结果显示，这种方法继承了watermark的准确性和非鲁棒性，在无后处理或常见后处理（如JPEG压缩）下保持高准确性，即使面对黑箱对抗性后处理（有限查询预算）时也表现良好。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.04254v3",
      "published_date": "2024-04-05 17:58:52 UTC",
      "updated_date": "2024-11-20 19:17:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:01:35.106050"
    },
    {
      "arxiv_id": "2404.04253v1",
      "title": "Growing Q-Networks: Solving Continuous Control Tasks with Adaptive Control Resolution",
      "title_zh": "增长 Q-网络：通过自适应控制分辨率解决连续控制任务",
      "authors": [
        "Tim Seyde",
        "Peter Werner",
        "Wilko Schwarting",
        "Markus Wulfmeier",
        "Daniela Rus"
      ],
      "abstract": "Recent reinforcement learning approaches have shown surprisingly strong\ncapabilities of bang-bang policies for solving continuous control benchmarks.\nThe underlying coarse action space discretizations often yield favourable\nexploration characteristics while final performance does not visibly suffer in\nthe absence of action penalization in line with optimal control theory. In\nrobotics applications, smooth control signals are commonly preferred to reduce\nsystem wear and energy efficiency, but action costs can be detrimental to\nexploration during early training. In this work, we aim to bridge this\nperformance gap by growing discrete action spaces from coarse to fine control\nresolution, taking advantage of recent results in decoupled Q-learning to scale\nour approach to high-dimensional action spaces up to dim(A) = 38. Our work\nindicates that an adaptive control resolution in combination with value\ndecomposition yields simple critic-only algorithms that yield surprisingly\nstrong performance on continuous control tasks.",
      "tldr_zh": "该论文提出Growing Q-Networks方法，通过自适应控制分辨率来解决连续控制任务中的性能差距。具体而言，它从粗到细地增长离散动作空间，利用decoupled Q-learning扩展到高维动作空间（如dim(A) = 38），以平衡早期探索和最终平滑控制需求。实验结果显示，这种结合价值分解的简单critic-only算法在连续控制基准上表现出色，桥接了强化学习在机器人应用中的实际挑战。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.04253v1",
      "published_date": "2024-04-05 17:58:37 UTC",
      "updated_date": "2024-04-05 17:58:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:01:45.582080"
    },
    {
      "arxiv_id": "2404.04251v3",
      "title": "Who Evaluates the Evaluations? Objectively Scoring Text-to-Image Prompt Coherence Metrics with T2IScoreScore (TS2)",
      "title_zh": "翻译失败",
      "authors": [
        "Michael Saxon",
        "Fatima Jahara",
        "Mahsa Khoshnoodi",
        "Yujie Lu",
        "Aditya Sharma",
        "William Yang Wang"
      ],
      "abstract": "With advances in the quality of text-to-image (T2I) models has come interest\nin benchmarking their prompt faithfulness -- the semantic coherence of\ngenerated images to the prompts they were conditioned on. A variety of T2I\nfaithfulness metrics have been proposed, leveraging advances in cross-modal\nembeddings and vision-language models (VLMs). However, these metrics are not\nrigorously compared and benchmarked, instead presented with correlation to\nhuman Likert scores over a set of easy-to-discriminate images against seemingly\nweak baselines.\n  We introduce T2IScoreScore, a curated set of semantic error graphs containing\na prompt and a set of increasingly erroneous images. These allow us to\nrigorously judge whether a given prompt faithfulness metric can correctly order\nimages with respect to their objective error count and significantly\ndiscriminate between different error nodes, using meta-metric scores derived\nfrom established statistical tests. Surprisingly, we find that the\nstate-of-the-art VLM-based metrics (e.g., TIFA, DSG, LLMScore, VIEScore) we\ntested fail to significantly outperform simple (and supposedly worse)\nfeature-based metrics like CLIPScore, particularly on a hard subset of\nnaturally-occurring T2I model errors. TS2 will enable the development of better\nT2I prompt faithfulness metrics through more rigorous comparison of their\nconformity to expected orderings and separations under objective criteria.",
      "tldr_zh": "该研究质疑了现有文本到图像 (T2I) 提示忠实度指标的评估方法，并引入了 T2IScoreScore (TS2)，一个精选的语义错误图集，用于客观比较这些指标。TS2 包含一个提示和一系列渐进错误的图像，通过统计测试评估指标是否能正确排序图像并显著区分错误级别。结果显示，先进的视觉语言模型 (VLM) 指标如 TIFA、DSG、LLMScore 和 VIEScore 在某些情况下并未显著优于简单的特征-based 指标如 CLIPScore，尤其在自然发生的错误子集中。TS2 的引入将推动更可靠的 T2I 忠实度指标开发，通过严格的客观标准提升评估的准确性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "NeurIPS 2024 Spotlight",
      "pdf_url": "http://arxiv.org/pdf/2404.04251v3",
      "published_date": "2024-04-05 17:57:16 UTC",
      "updated_date": "2024-10-31 01:39:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:02:00.027420"
    },
    {
      "arxiv_id": "2404.04318v1",
      "title": "Robust Depth Enhancement via Polarization Prompt Fusion Tuning",
      "title_zh": "翻译失败",
      "authors": [
        "Kei Ikemura",
        "Yiming Huang",
        "Felix Heide",
        "Zhaoxiang Zhang",
        "Qifeng Chen",
        "Chenyang Lei"
      ],
      "abstract": "Existing depth sensors are imperfect and may provide inaccurate depth values\nin challenging scenarios, such as in the presence of transparent or reflective\nobjects. In this work, we present a general framework that leverages\npolarization imaging to improve inaccurate depth measurements from various\ndepth sensors. Previous polarization-based depth enhancement methods focus on\nutilizing pure physics-based formulas for a single sensor. In contrast, our\nmethod first adopts a learning-based strategy where a neural network is trained\nto estimate a dense and complete depth map from polarization data and a sensor\ndepth map from different sensors. To further improve the performance, we\npropose a Polarization Prompt Fusion Tuning (PPFT) strategy to effectively\nutilize RGB-based models pre-trained on large-scale datasets, as the size of\nthe polarization dataset is limited to train a strong model from scratch. We\nconducted extensive experiments on a public dataset, and the results\ndemonstrate that the proposed method performs favorably compared to existing\ndepth enhancement baselines. Code and demos are available at\nhttps://lastbasket.github.io/PPFT/.",
      "tldr_zh": "本文提出一个通用框架，利用偏振成像（polarization imaging）提升各种深度传感器的准确性，针对透明或反射物体等挑战场景中的不准确深度测量问题。框架采用基于学习的策略，训练神经网络（neural network）从偏振数据和传感器深度图估计密集完整的深度图；同时引入Polarization Prompt Fusion Tuning (PPFT)策略，通过融合预训练的RGB-based模型来优化性能，以应对偏振数据集规模有限的挑战。在公共数据集上的广泛实验表明，该方法比现有深度增强基线表现出色，代码和演示已开源。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2024. Project page: https://lastbasket.github.io/PPFT/. The\n  first two authors contribute equally",
      "pdf_url": "http://arxiv.org/pdf/2404.04318v1",
      "published_date": "2024-04-05 17:55:33 UTC",
      "updated_date": "2024-04-05 17:55:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:02:10.631012"
    },
    {
      "arxiv_id": "2404.04243v3",
      "title": "Identity Decoupling for Multi-Subject Personalization of Text-to-Image Models",
      "title_zh": "翻译失败",
      "authors": [
        "Sangwon Jang",
        "Jaehyeong Jo",
        "Kimin Lee",
        "Sung Ju Hwang"
      ],
      "abstract": "Text-to-image diffusion models have shown remarkable success in generating\npersonalized subjects based on a few reference images. However, current methods\noften fail when generating multiple subjects simultaneously, resulting in mixed\nidentities with combined attributes from different subjects. In this work, we\npresent MuDI, a novel framework that enables multi-subject personalization by\neffectively decoupling identities from multiple subjects. Our main idea is to\nutilize segmented subjects generated by a foundation model for segmentation\n(Segment Anything) for both training and inference, as a form of data\naugmentation for training and initialization for the generation process.\nMoreover, we further introduce a new metric to better evaluate the performance\nof our method on multi-subject personalization. Experimental results show that\nour MuDI can produce high-quality personalized images without identity mixing,\neven for highly similar subjects as shown in Figure 1. Specifically, in human\nevaluation, MuDI obtains twice the success rate for personalizing multiple\nsubjects without identity mixing over existing baselines and is preferred over\n70% against the strongest baseline.",
      "tldr_zh": "该研究针对文本到图像扩散模型（Text-to-Image Models）在多主题个性化生成中存在的身份混合问题，提出了一种名为 MuDI 的新框架，通过有效解耦多个主题的身份来实现高质量生成。MuDI 的核心方法是利用 Segment Anything 模型生成的分割主题作为训练数据增强和生成过程的初始化，从而避免属性混合。研究还引入了一个新的评估指标来量化多主题个性化的性能。实验结果表明，MuDI 即使在主题高度相似的情况下也能产生高质量图像，在人类评估中，其成功率是现有基线的两倍，并在 70% 的情况下优于最强基线。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "NeurIPS 2024. Project page: https://mudi-t2i.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2404.04243v3",
      "published_date": "2024-04-05 17:45:22 UTC",
      "updated_date": "2024-10-28 08:22:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:02:23.090689"
    },
    {
      "arxiv_id": "2404.04242v1",
      "title": "Physical Property Understanding from Language-Embedded Feature Fields",
      "title_zh": "翻译失败",
      "authors": [
        "Albert J. Zhai",
        "Yuan Shen",
        "Emily Y. Chen",
        "Gloria X. Wang",
        "Xinlei Wang",
        "Sheng Wang",
        "Kaiyu Guan",
        "Shenlong Wang"
      ],
      "abstract": "Can computers perceive the physical properties of objects solely through\nvision? Research in cognitive science and vision science has shown that humans\nexcel at identifying materials and estimating their physical properties based\npurely on visual appearance. In this paper, we present a novel approach for\ndense prediction of the physical properties of objects using a collection of\nimages. Inspired by how humans reason about physics through vision, we leverage\nlarge language models to propose candidate materials for each object. We then\nconstruct a language-embedded point cloud and estimate the physical properties\nof each 3D point using a zero-shot kernel regression approach. Our method is\naccurate, annotation-free, and applicable to any object in the open world.\nExperiments demonstrate the effectiveness of the proposed approach in various\nphysical property reasoning tasks, such as estimating the mass of common\nobjects, as well as other properties like friction and hardness.",
      "tldr_zh": "这篇论文提出了一种新方法，用于仅通过视觉图像对物体的物理属性进行密集预测，灵感来源于人类视觉推理。方法利用large language models为每个物体提出候选材料，构建language-embedded point cloud，并采用zero-shot kernel regression来估计每个3D点的物理属性。该方法无需标注、准确且适用于开放世界中的任何物体，实验证明其在估计常见物体的质量、摩擦和硬度等任务中表现出色。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2024. Project page (with code):\n  https://ajzhai.github.io/NeRF2Physics/",
      "pdf_url": "http://arxiv.org/pdf/2404.04242v1",
      "published_date": "2024-04-05 17:45:07 UTC",
      "updated_date": "2024-04-05 17:45:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:02:35.495056"
    },
    {
      "arxiv_id": "2404.04234v3",
      "title": "player2vec: A Language Modeling Approach to Understand Player Behavior in Games",
      "title_zh": "翻译失败",
      "authors": [
        "Tianze Wang",
        "Maryam Honari-Jahromi",
        "Styliani Katsarou",
        "Olga Mikheeva",
        "Theodoros Panagiotakopoulos",
        "Sahar Asadi",
        "Oleg Smirnov"
      ],
      "abstract": "Methods for learning latent user representations from historical behavior\nlogs have gained traction for recommendation tasks in e-commerce, content\nstreaming, and other settings. However, this area still remains relatively\nunderexplored in video and mobile gaming contexts. In this work, we present a\nnovel method for overcoming this limitation by extending a long-range\nTransformer model from the natural language processing domain to player\nbehavior data. We discuss specifics of behavior tracking in games and propose\npreprocessing and tokenization approaches by viewing in-game events in an\nanalogous way to words in sentences, thus enabling learning player\nrepresentations in a self-supervised manner in the absence of ground-truth\nannotations. We experimentally demonstrate the efficacy of the proposed\napproach in fitting the distribution of behavior events by evaluating intrinsic\nlanguage modeling metrics. Furthermore, we qualitatively analyze the emerging\nstructure of the learned embedding space and show its value for generating\ninsights into behavior patterns to inform downstream applications.",
      "tldr_zh": "这篇论文提出了player2vec方法，使用长程Transformer模型将游戏玩家的行为数据视为类似自然语言的序列，从而学习潜在玩家表示。作者讨论了游戏行为跟踪的细节，并提出预处理和标记化策略，使游戏事件类似于句子中的单词，实现自监督学习，而无需地面真实标签。实验结果显示，该方法在行为事件分布拟合方面表现出色，并通过嵌入空间分析提供了宝贵的玩家行为模式洞见，支持下游应用如推荐系统。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.04234v3",
      "published_date": "2024-04-05 17:29:47 UTC",
      "updated_date": "2024-06-07 22:01:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:02:47.167708"
    },
    {
      "arxiv_id": "2405.12222v1",
      "title": "Influence based explainability of brain tumors segmentation in multimodal Magnetic Resonance Imaging",
      "title_zh": "翻译失败",
      "authors": [
        "Tommaso Torda",
        "Andrea Ciardiello",
        "Simona Gargiulo",
        "Greta Grillo",
        "Simone Scardapane",
        "Cecilia Voena",
        "Stefano Giagu"
      ],
      "abstract": "In recent years Artificial Intelligence has emerged as a fundamental tool in\nmedical applications. Despite this rapid development, deep neural networks\nremain black boxes that are difficult to explain, and this represents a major\nlimitation for their use in clinical practice. We focus on the segmentation of\nmedical images task, where most explainability methods proposed so far provide\na visual explanation in terms of an input saliency map. The aim of this work is\nto extend, implement and test instead an influence-based explainability\nalgorithm, TracIn, proposed originally for classification tasks, in a\nchallenging clinical problem, i.e., multiclass segmentation of tumor brains in\nmultimodal Magnetic Resonance Imaging. We verify the faithfulness of the\nproposed algorithm linking the similarities of the latent representation of the\nnetwork to the TracIn output. We further test the capacity of the algorithm to\nprovide local and global explanations, and we suggest that it can be adopted as\na tool to select the most relevant features used in the decision process. The\nmethod is generalizable for all semantic segmentation tasks where classes are\nmutually exclusive, which is the standard framework in these cases.",
      "tldr_zh": "该论文探讨了人工智能在医疗图像分割中的解释性挑战，特别是针对脑肿瘤在多模态 Magnetic Resonance Imaging 中的多类分割任务。作者扩展并实现了 TracIn 算法——一种基于影响的解释方法——原本用于分类，现应用于此临床问题，并通过网络潜在表示的相似性验证其忠实度。实验结果表明，TracIn 能提供局部和全局解释，帮助选择决策过程中的关键特征，且适用于所有语义分割任务，其中类是互斥的。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "15 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.12222v1",
      "published_date": "2024-04-05 17:07:21 UTC",
      "updated_date": "2024-04-05 17:07:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:03:00.074235"
    },
    {
      "arxiv_id": "2404.04220v1",
      "title": "Multi-modal perception for soft robotic interactions using generative models",
      "title_zh": "基于生成模型的多模态感知用于软体机器人交互",
      "authors": [
        "Enrico Donato",
        "Egidio Falotico",
        "Thomas George Thuruthel"
      ],
      "abstract": "Perception is essential for the active interaction of physical agents with\nthe external environment. The integration of multiple sensory modalities, such\nas touch and vision, enhances this perceptual process, creating a more\ncomprehensive and robust understanding of the world. Such fusion is\nparticularly useful for highly deformable bodies such as soft robots.\nDeveloping a compact, yet comprehensive state representation from multi-sensory\ninputs can pave the way for the development of complex control strategies. This\npaper introduces a perception model that harmonizes data from diverse\nmodalities to build a holistic state representation and assimilate essential\ninformation. The model relies on the causality between sensory input and\nrobotic actions, employing a generative model to efficiently compress fused\ninformation and predict the next observation. We present, for the first time, a\nstudy on how touch can be predicted from vision and proprioception on soft\nrobots, the importance of the cross-modal generation and why this is essential\nfor soft robotic interactions in unstructured environments.",
      "tldr_zh": "这篇论文探讨了使用 generative models 实现软机器人的多模式感知，以提升其与外部环境的互动能力。研究引入了一个感知模型，通过融合触觉、视觉和本体感觉等感官数据，构建整体状态表示，并依赖感官输入与机器人动作的因果关系来压缩信息并预测下一个观察。首次研究了从视觉和本体感觉预测触觉的可能性，强调跨模式生成对软机器人在非结构化环境中的交互至关重要，这为开发复杂控制策略奠定了基础。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted for presentation at IEEE RoboSoft 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.04220v1",
      "published_date": "2024-04-05 17:06:03 UTC",
      "updated_date": "2024-04-05 17:06:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:03:12.659158"
    },
    {
      "arxiv_id": "2404.04219v1",
      "title": "Continual Policy Distillation of Reinforcement Learning-based Controllers for Soft Robotic In-Hand Manipulation",
      "title_zh": "翻译失败",
      "authors": [
        "Lanpei Li",
        "Enrico Donato",
        "Vincenzo Lomonaco",
        "Egidio Falotico"
      ],
      "abstract": "Dexterous manipulation, often facilitated by multi-fingered robotic hands,\nholds solid impact for real-world applications. Soft robotic hands, due to\ntheir compliant nature, offer flexibility and adaptability during object\ngrasping and manipulation. Yet, benefits come with challenges, particularly in\nthe control development for finger coordination. Reinforcement Learning (RL)\ncan be employed to train object-specific in-hand manipulation policies, but\nlimiting adaptability and generalizability. We introduce a Continual Policy\nDistillation (CPD) framework to acquire a versatile controller for in-hand\nmanipulation, to rotate different objects in shape and size within a\nfour-fingered soft gripper. The framework leverages Policy Distillation (PD) to\ntransfer knowledge from expert policies to a continually evolving student\npolicy network. Exemplar-based rehearsal methods are then integrated to\nmitigate catastrophic forgetting and enhance generalization. The performance of\nthe CPD framework over various replay strategies demonstrates its effectiveness\nin consolidating knowledge from multiple experts and achieving versatile and\nadaptive behaviours for in-hand manipulation tasks.",
      "tldr_zh": "本研究针对软机器人手在物体抓取和操控中的手指协调控制挑战，提出了一种基于 Reinforcement Learning (RL) 的 Continual Policy Distillation (CPD) 框架，以提升操控策略的适应性和泛化能力。该框架通过 Policy Distillation (PD) 将知识从特定物体专家策略转移到一个不断演化的学生策略网络，并整合 Exemplar-based rehearsal 方法来缓解 catastrophic forgetting，确保对不同形状和大小物体的旋转操控。实验结果显示，CPD 在多种 replay 策略下表现出色，能够巩固多专家知识，实现高效的多功能和适应性 in-hand manipulation 行为。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted for presentation at IEEE RoboSoft 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.04219v1",
      "published_date": "2024-04-05 17:05:45 UTC",
      "updated_date": "2024-04-05 17:05:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:03:24.618799"
    },
    {
      "arxiv_id": "2404.04205v1",
      "title": "Enhancing IoT Intelligence: A Transformer-based Reinforcement Learning Methodology",
      "title_zh": "翻译失败",
      "authors": [
        "Gaith Rjoub",
        "Saidul Islam",
        "Jamal Bentahar",
        "Mohammed Amin Almaiah",
        "Rana Alrawashdeh"
      ],
      "abstract": "The proliferation of the Internet of Things (IoT) has led to an explosion of\ndata generated by interconnected devices, presenting both opportunities and\nchallenges for intelligent decision-making in complex environments. Traditional\nReinforcement Learning (RL) approaches often struggle to fully harness this\ndata due to their limited ability to process and interpret the intricate\npatterns and dependencies inherent in IoT applications. This paper introduces a\nnovel framework that integrates transformer architectures with Proximal Policy\nOptimization (PPO) to address these challenges. By leveraging the\nself-attention mechanism of transformers, our approach enhances RL agents'\ncapacity for understanding and acting within dynamic IoT environments, leading\nto improved decision-making processes. We demonstrate the effectiveness of our\nmethod across various IoT scenarios, from smart home automation to industrial\ncontrol systems, showing marked improvements in decision-making efficiency and\nadaptability. Our contributions include a detailed exploration of the\ntransformer's role in processing heterogeneous IoT data, a comprehensive\nevaluation of the framework's performance in diverse environments, and a\nbenchmark against traditional RL methods. The results indicate significant\nadvancements in enabling RL agents to navigate the complexities of IoT\necosystems, highlighting the potential of our approach to revolutionize\nintelligent automation and decision-making in the IoT landscape.",
      "tldr_zh": "本论文针对物联网（IoT）环境中数据爆炸带来的挑战，提出了一种将Transformer架构与Proximal Policy Optimization (PPO)整合的新型强化学习（Reinforcement Learning）框架，以提升智能决策能力。该框架利用Transformer's self-attention机制，帮助RL代理更好地处理IoT数据的复杂模式和依赖关系，从而在动态环境中实现更高效的决策和适应。在多种IoT场景（如智能家居和工业控制系统）中的实验表明，该方法显著优于传统RL方法，决策效率和适应性得到改善，主要贡献包括对Transformer在异构IoT数据处理中的作用探索、框架性能评估以及基准测试。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.04205v1",
      "published_date": "2024-04-05 16:30:45 UTC",
      "updated_date": "2024-04-05 16:30:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:03:35.782117"
    },
    {
      "arxiv_id": "2404.08673v1",
      "title": "Sentiment analysis and random forest to classify LLM versus human source applied to Scientific Texts",
      "title_zh": "翻译失败",
      "authors": [
        "Javier J. Sanchez-Medina"
      ],
      "abstract": "After the launch of ChatGPT v.4 there has been a global vivid discussion on\nthe ability of this artificial intelligence powered platform and some other\nsimilar ones for the automatic production of all kinds of texts, including\nscientific and technical texts. This has triggered a reflection in many\ninstitutions on whether education and academic procedures should be adapted to\nthe fact that in future many texts we read will not be written by humans\n(students, scholars, etc.), at least, not entirely. In this work it is proposed\na new methodology to classify texts coming from an automatic text production\nengine or a human, based on Sentiment Analysis as a source for feature\nengineering independent variables and then train with them a Random Forest\nclassification algorithm. Using four different sentiment lexicons, a number of\nnew features where produced, and then fed to a machine learning random forest\nmethodology, to train such a model. Results seem very convincing that this may\nbe a promising research line to detect fraud, in such environments where human\nare supposed to be the source of texts.",
      "tldr_zh": "这篇论文探讨了使用情感分析(Sentiment Analysis)和随机森林(Random Forest)算法来区分大型语言模型(LLM)如ChatGPT生成的科学文本与人类撰写的文本。研究提出了一种新方法，通过四个不同情感词汇表(sentiment lexicons)生成特征变量，然后将这些特征输入随机森林分类算法进行训练，以检测文本来源。实验结果显示，该方法效果显著，为学术环境中识别AI生成文本的欺诈行为提供了有前景的解决方案，可能有助于教育和学术程序的适应。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.LG",
        "68"
      ],
      "primary_category": "cs.CL",
      "comment": "12 Pages, 3 tables, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2404.08673v1",
      "published_date": "2024-04-05 16:14:36 UTC",
      "updated_date": "2024-04-05 16:14:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:03:47.993722"
    },
    {
      "arxiv_id": "2404.15324v1",
      "title": "Advanced simulation-based predictive modelling for solar irradiance sensor farms",
      "title_zh": "先进的基于模拟的预测建模技术用于太阳能辐照度传感器农场",
      "authors": [
        "José L. Risco-Martín",
        "Ignacio-Iker Prado-Rujas",
        "Javier Campoy",
        "María S. Pérez",
        "Katzalin Olcoz"
      ],
      "abstract": "As solar power continues to grow and replace traditional energy sources, the\nneed for reliable forecasting models becomes increasingly important to ensure\nthe stability and efficiency of the grid. However, the management of these\nmodels still needs to be improved, and new tools and technologies are required\nto handle the deployment and control of solar facilities. This work introduces\na novel framework named Cloud-based Analysis and Integration for Data\nEfficiency (CAIDE), designed for real-time monitoring, management, and\nforecasting of solar irradiance sensor farms. CAIDE is designed to manage\nmultiple sensor farms simultaneously while improving predictive models in\nreal-time using well-grounded Modeling and Simulation (M&S) methodologies. The\nframework leverages Model Based Systems Engineering (MBSE) and an Internet of\nThings (IoT) infrastructure to support the deployment and analysis of solar\nplants in dynamic environments. The system can adapt and re-train the model\nwhen given incorrect results, ensuring that forecasts remain accurate and\nup-to-date. Furthermore, CAIDE can be executed in sequential, parallel, and\ndistributed architectures, assuring scalability. The effectiveness of CAIDE is\ndemonstrated in a complex scenario composed of several solar irradiance sensor\nfarms connected to a centralized management system. Our results show that CAIDE\nis scalable and effective in managing and forecasting solar power production\nwhile improving the accuracy of predictive models in real time. The framework\nhas important implications for the deployment of solar plants and the future of\nrenewable energy sources.",
      "tldr_zh": "这篇论文提出了一种名为 CAIDE 的新型框架，用于实时监控、管理和预测太阳能辐照度传感器农场，以提升电网的稳定性和效率。CAIDE 整合 Modeling and Simulation (M&S) 方法、Model Based Systems Engineering (MBSE) 和 Internet of Things (IoT) 基础设施，支持模型的实时改进、适应性训练以及顺序、并行和分布式架构，确保系统的可扩展性。实验在多个传感器农场的复杂场景中验证了 CAIDE 的有效性，提高了预测模型的准确性，并为太阳能设备的部署和可再生能源发展提供了重要启示。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.15324v1",
      "published_date": "2024-04-05 15:44:51 UTC",
      "updated_date": "2024-04-05 15:44:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:04:00.295924"
    },
    {
      "arxiv_id": "2406.11857v1",
      "title": "AI Royalties -- an IP Framework to Compensate Artists & IP Holders for AI-Generated Content",
      "title_zh": "翻译失败",
      "authors": [
        "Pablo Ducru",
        "Jonathan Raiman",
        "Ronaldo Lemos",
        "Clay Garner",
        "George He",
        "Hanna Balcha",
        "Gabriel Souto",
        "Sergio Branco",
        "Celina Bottino"
      ],
      "abstract": "This article investigates how AI-generated content can disrupt central\nrevenue streams of the creative industries, in particular the collection of\ndividends from intellectual property (IP) rights. It reviews the IP and\ncopyright questions related to the input and output of generative AI systems. A\nsystematic method is proposed to assess whether AI-generated outputs,\nespecially images, infringe previous copyrights, using a similarity metric\n(CLIP) between images against historical copyright rulings. An examination\n(economic and technical feasibility) of previously proposed compensation\nframeworks reveals their financial implications for creatives and IP holders.\nLastly, we propose a novel IP framework for compensation of artists and IP\nholders based on their published \"licensed AIs\" as a new medium and asset from\nwhich to collect AI royalties.",
      "tldr_zh": "这篇论文探讨了AI生成内容如何干扰创意产业的收入来源，特别是知识产权(IP)权利的红利，并审视了生成AI系统输入和输出的IP和版权问题。作者提出一种系统方法，使用CLIP相似度指标评估AI生成输出（如图像）是否侵犯现有版权，并参考历史版权裁决。论文还考察了先前补偿框架的经济和技术可行性及其对创作者和IP持有者的财务影响。最后，作者提出一个新颖的IP框架，通过“licensed AIs”作为新媒体和资产，允许艺术家和IP持有者收取AI版税，从而为AI时代提供公平补偿机制。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "7 pages, 2 figures, submitted to AAAI",
      "pdf_url": "http://arxiv.org/pdf/2406.11857v1",
      "published_date": "2024-04-05 15:35:08 UTC",
      "updated_date": "2024-04-05 15:35:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:04:11.730960"
    },
    {
      "arxiv_id": "2404.04316v2",
      "title": "Parameter Efficient Quasi-Orthogonal Fine-Tuning via Givens Rotation",
      "title_zh": "翻译失败",
      "authors": [
        "Xinyu Ma",
        "Xu Chu",
        "Zhibang Yang",
        "Yang Lin",
        "Xin Gao",
        "Junfeng Zhao"
      ],
      "abstract": "With the increasingly powerful performances and enormous scales of pretrained\nmodels, promoting parameter efficiency in fine-tuning has become a crucial need\nfor effective and efficient adaptation to various downstream tasks. One\nrepresentative line of fine-tuning methods is Orthogonal Fine-tuning (OFT),\nwhich rigorously preserves the angular distances within the parameter space to\npreserve the pretrained knowledge. Despite the empirical effectiveness, OFT\nstill suffers low parameter efficiency at $\\mathcal{O}(d^2)$ and limited\ncapability of downstream adaptation. Inspired by Givens rotation, in this\npaper, we proposed quasi-Givens Orthogonal Fine-Tuning (qGOFT) to address the\nproblems. We first use $\\mathcal{O}(d)$ Givens rotations to accomplish\narbitrary orthogonal transformation in $SO(d)$ with provable equivalence,\nreducing parameter complexity from $\\mathcal{O}(d^2)$ to $\\mathcal{O}(d)$. Then\nwe introduce flexible norm and relative angular adjustments under soft\northogonality regularization to enhance the adaptation capability of downstream\nsemantic deviations. Extensive experiments on various tasks and pretrained\nmodels validate the effectiveness of our methods.",
      "tldr_zh": "该论文针对预训练模型的微调问题，提出了一种参数高效的准正交微调方法quasi-Givens Orthogonal Fine-Tuning (qGOFT)，利用Givens Rotation将参数复杂度从$\\mathcal{O}(d^2)$降低到$\\mathcal{O}(d)$，并通过$\\mathcal{O}(d)$的Givens旋转实现任意正交变换在$SO(d)$中的等价性。方法还引入了灵活的范数和相对角度调整以及软正交正则化，以提升模型对下游任务语义偏差的适应能力。实验在多种任务和预训练模型上验证了qGOFT的有效性，展示了其在保持预训练知识的同时提高参数效率的优势。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Appeared at ICML 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.04316v2",
      "published_date": "2024-04-05 15:28:44 UTC",
      "updated_date": "2024-06-07 03:54:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:04:25.048876"
    },
    {
      "arxiv_id": "2404.04167v5",
      "title": "Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model",
      "title_zh": "翻译失败",
      "authors": [
        "Xinrun Du",
        "Zhouliang Yu",
        "Songyang Gao",
        "Ding Pan",
        "Yuyang Cheng",
        "Ziyang Ma",
        "Ruibin Yuan",
        "Xingwei Qu",
        "Jiaheng Liu",
        "Tianyu Zheng",
        "Xinchen Luo",
        "Guorui Zhou",
        "Wenhu Chen",
        "Ge Zhang"
      ],
      "abstract": "In this study, we introduce CT-LLM, a 2B large language model (LLM) that\nillustrates a pivotal shift towards prioritizing the Chinese language in\ndeveloping LLMs. Uniquely initiated from scratch, CT-LLM diverges from the\nconventional methodology by primarily incorporating Chinese textual data,\nutilizing an extensive corpus of 1,200 billion tokens, including 800 billion\nChinese tokens, 300 billion English tokens, and 100 billion code tokens. This\nstrategic composition facilitates the model's exceptional proficiency in\nunderstanding and processing Chinese, a capability further enhanced through\nalignment techniques. Demonstrating remarkable performance on the CHC-Bench,\nCT-LLM excels in Chinese language tasks, and showcases its adeptness in English\nthrough SFT. This research challenges the prevailing paradigm of training LLMs\npredominantly on English corpora and then adapting them to other languages,\nbroadening the horizons for LLM training methodologies. By open-sourcing the\nfull process of training a Chinese LLM, including a detailed data processing\nprocedure with the obtained Massive Appropriate Pretraining Chinese Corpus\n(MAP-CC), a well-chosen multidisciplinary Chinese Hard Case Benchmark\n(CHC-Bench), and the 2B-size Chinese Tiny LLM (CT-LLM), we aim to foster\nfurther exploration and innovation in both academia and industry, paving the\nway for more inclusive and versatile language models.",
      "tldr_zh": "这篇论文介绍了CT-LLM，一款2B参数的中文中心大型语言模型（LLM），旨在优先处理中文语言任务。模型从零开始训练，使用1200亿tokens的语料库（包括800亿中文tokens、300亿英文tokens和100亿代码tokens），并通过对齐技术增强其理解能力。该方法挑战了传统LLM训练模式，即先基于英文语料再适应其他语言，在CHC-Bench中文基准测试中表现出色，并在英文任务上也表现出良好适应性。通过开源训练过程、数据处理语料库（MAP-CC）和基准测试，论文推动了更具包容性的LLM开发创新。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.04167v5",
      "published_date": "2024-04-05 15:20:02 UTC",
      "updated_date": "2024-09-13 09:47:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:04:36.306599"
    },
    {
      "arxiv_id": "2404.04159v1",
      "title": "Noisy Label Processing for Classification: A Survey",
      "title_zh": "噪声标签处理在分类任务中的应用",
      "authors": [
        "Mengting Li",
        "Chuang Zhu"
      ],
      "abstract": "In recent years, deep neural networks (DNNs) have gained remarkable\nachievement in computer vision tasks, and the success of DNNs often depends\ngreatly on the richness of data. However, the acquisition process of data and\nhigh-quality ground truth requires a lot of manpower and money. In the long,\ntedious process of data annotation, annotators are prone to make mistakes,\nresulting in incorrect labels of images, i.e., noisy labels. The emergence of\nnoisy labels is inevitable. Moreover, since research shows that DNNs can easily\nfit noisy labels, the existence of noisy labels will cause significant damage\nto the model training process. Therefore, it is crucial to combat noisy labels\nfor computer vision tasks, especially for classification tasks. In this survey,\nwe first comprehensively review the evolution of different deep learning\napproaches for noisy label combating in the image classification task. In\naddition, we also review different noise patterns that have been proposed to\ndesign robust algorithms. Furthermore, we explore the inner pattern of\nreal-world label noise and propose an algorithm to generate a synthetic label\nnoise pattern guided by real-world data. We test the algorithm on the\nwell-known real-world dataset CIFAR-10N to form a new real-world data-guided\nsynthetic benchmark and evaluate some typical noise-robust methods on the\nbenchmark.",
      "tldr_zh": "这篇调查论文探讨了深度神经网络（DNNs）在图像分类任务中面临的 noisy labels 问题，这些标签错误会干扰模型训练。论文全面回顾了现有深度学习方法的发展，用于对抗 noisy labels，并分析了各种噪声模式的设计。作者进一步探索了真实世界标签噪声的内在模式，提出了一种基于真实数据指导的算法来生成合成噪声模式，并在 CIFAR-10N 数据集上创建新基准，并评估了典型噪声鲁棒方法的效果。整体上，这为改进分类任务的鲁棒性提供了宝贵见解。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.04159v1",
      "published_date": "2024-04-05 15:11:09 UTC",
      "updated_date": "2024-04-05 15:11:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:04:47.427990"
    },
    {
      "arxiv_id": "2404.04139v1",
      "title": "Precision Guided Approach to Mitigate Data Poisoning Attacks in Federated Learning",
      "title_zh": "翻译失败",
      "authors": [
        "K Naveen Kumar",
        "C Krishna Mohan",
        "Aravind Machiry"
      ],
      "abstract": "Federated Learning (FL) is a collaborative learning paradigm enabling\nparticipants to collectively train a shared machine learning model while\npreserving the privacy of their sensitive data. Nevertheless, the inherent\ndecentralized and data-opaque characteristics of FL render its susceptibility\nto data poisoning attacks. These attacks introduce malformed or malicious\ninputs during local model training, subsequently influencing the global model\nand resulting in erroneous predictions. Current FL defense strategies against\ndata poisoning attacks either involve a trade-off between accuracy and\nrobustness or necessitate the presence of a uniformly distributed root dataset\nat the server. To overcome these limitations, we present FedZZ, which harnesses\na zone-based deviating update (ZBDU) mechanism to effectively counter data\npoisoning attacks in FL. Further, we introduce a precision-guided methodology\nthat actively characterizes these client clusters (zones), which in turn aids\nin recognizing and discarding malicious updates at the server. Our evaluation\nof FedZZ across two widely recognized datasets: CIFAR10 and EMNIST, demonstrate\nits efficacy in mitigating data poisoning attacks, surpassing the performance\nof prevailing state-of-the-art methodologies in both single and multi-client\nattack scenarios and varying attack volumes. Notably, FedZZ also functions as a\nrobust client selection strategy, even in highly non-IID and attack-free\nscenarios. Moreover, in the face of escalating poisoning rates, the model\naccuracy attained by FedZZ displays superior resilience compared to existing\ntechniques. For instance, when confronted with a 50% presence of malicious\nclients, FedZZ sustains an accuracy of 67.43%, while the accuracy of the\nsecond-best solution, FL-Defender, diminishes to 43.36%.",
      "tldr_zh": "这篇论文提出了一种名为 FedZZ 的方法，用于缓解 Federated Learning (FL) 中的数据中毒攻击，通过基于区域的偏差更新 (ZBDU) 机制和精确引导的方法来识别并丢弃恶意更新，从而在不依赖根数据集的情况下平衡准确性和鲁棒性。FedZZ 在 CIFAR10 和 EMNIST 数据集上的实验中，显著优于现有技术，在单客户端和多客户端攻击场景下表现突出，即使在非 IID 环境中也能作为有效的客户端选择策略。特别地，当恶意客户端占比达 50% 时，FedZZ 维持 67.43% 的准确率，而第二佳方法 FL-Defender 仅为 43.36%。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "14 pages, 11 figures, 5 tables, Accepted in ACM CODASPY 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.04139v1",
      "published_date": "2024-04-05 14:37:49 UTC",
      "updated_date": "2024-04-05 14:37:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:05:01.827726"
    },
    {
      "arxiv_id": "2404.04108v2",
      "title": "Large language models as oracles for instantiating ontologies with domain-specific knowledge",
      "title_zh": "翻译失败",
      "authors": [
        "Giovanni Ciatto",
        "Andrea Agiollo",
        "Matteo Magnini",
        "Andrea Omicini"
      ],
      "abstract": "Background. Endowing intelligent systems with semantic data commonly requires\ndesigning and instantiating ontologies with domain-specific knowledge.\nEspecially in the early phases, those activities are typically performed\nmanually by human experts possibly leveraging on their own experience. The\nresulting process is therefore time-consuming, error-prone, and often biased by\nthe personal background of the ontology designer. Objective. To mitigate that\nissue, we propose a novel domain-independent approach to automatically\ninstantiate ontologies with domain-specific knowledge, by leveraging on large\nlanguage models (LLMs) as oracles. Method. Starting from (i) an initial schema\ncomposed by inter-related classes and properties and (ii) a set of query\ntemplates, our method queries the LLM multiple times, and generates instances\nfor both classes and properties from its replies. Thus, the ontology is\nautomatically filled with domain-specific knowledge, compliant to the initial\nschema. As a result, the ontology is quickly and automatically enriched with\nmanifold instances, which experts may consider to keep, adjust, discard, or\ncomplement according to their own needs and expertise. Contribution. We\nformalise our method in general way and instantiate it over various LLMs, as\nwell as on a concrete case study. We report experiments rooted in the\nnutritional domain where an ontology of food meals and their ingredients is\nautomatically instantiated from scratch, starting from a categorisation of\nmeals and their relationships. There, we analyse the quality of the generated\nontologies and compare ontologies attained by exploiting different LLMs.\nExperimentally, our approach achieves a quality metric that is up to five times\nhigher than the state-of-the-art, while reducing erroneous entities and\nrelations by up to ten times. Finally, we provide a SWOT analysis of the\nproposed method.",
      "tldr_zh": "该论文提出了一种域独立方法，使用大型语言模型(LLMs)作为预言机，自动实例化本体(ontologies)以融入领域特定知识，从而解决传统手动过程耗时、易出错的问题。方法基于初始schema（包括类和属性）和查询模板，通过多次查询LLMs生成实例，快速丰富本体，并允许专家后续调整。实验在营养领域从零开始实例化食物餐点本体，结果显示该方法的质量指标比现有技术高出五倍，同时减少了十倍的错误实体和关系，并提供了SWOT analysis以评估其优势和局限。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IR",
        "cs.LG",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.04108v2",
      "published_date": "2024-04-05 14:04:07 UTC",
      "updated_date": "2024-12-12 10:56:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:05:14.728873"
    },
    {
      "arxiv_id": "2404.04106v1",
      "title": "Intervention-Assisted Policy Gradient Methods for Online Stochastic Queuing Network Optimization: Technical Report",
      "title_zh": "干预辅助策略梯度方法用于在线随机排队网络优化：技术报告",
      "authors": [
        "Jerrod Wigmore",
        "Brooke Shrader",
        "Eytan Modiano"
      ],
      "abstract": "Deep Reinforcement Learning (DRL) offers a powerful approach to training\nneural network control policies for stochastic queuing networks (SQN). However,\ntraditional DRL methods rely on offline simulations or static datasets,\nlimiting their real-world application in SQN control. This work proposes Online\nDeep Reinforcement Learning-based Controls (ODRLC) as an alternative, where an\nintelligent agent interacts directly with a real environment and learns an\noptimal control policy from these online interactions. SQNs present a challenge\nfor ODRLC due to the unbounded nature of the queues within the network\nresulting in an unbounded state-space. An unbounded state-space is particularly\nchallenging for neural network policies as neural networks are notoriously poor\nat extrapolating to unseen states. To address this challenge, we propose an\nintervention-assisted framework that leverages strategic interventions from\nknown stable policies to ensure the queue sizes remain bounded. This framework\ncombines the learning power of neural networks with the guaranteed stability of\nclassical control policies for SQNs. We introduce a method to design these\nintervention-assisted policies to ensure strong stability of the network.\nFurthermore, we extend foundational DRL theorems for intervention-assisted\npolicies and develop two practical algorithms specifically for ODRLC of SQNs.\nFinally, we demonstrate through experiments that our proposed algorithms\noutperform both classical control approaches and prior ODRLC algorithms.",
      "tldr_zh": "本研究提出了一种干预辅助的策略梯度方法（Intervention-Assisted Policy Gradient Methods），用于在线深度强化学习控制（ODRLC）优化随机排队网络（SQN），以解决传统DRL方法依赖离线模拟的局限性。框架通过引入已知稳定策略进行战略干预，确保队列大小保持在界限内，从而克服SQN的无界状态空间对神经网络的挑战。研究扩展了DRL理论，设计了两个实用算法来实现网络的强稳定性。实验结果显示，该方法在SQN优化中优于经典控制方法和现有ODRLC算法。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "F.2.2; I.2.6"
      ],
      "primary_category": "cs.AI",
      "comment": "25 pages, 6 Figures",
      "pdf_url": "http://arxiv.org/pdf/2404.04106v1",
      "published_date": "2024-04-05 14:02:04 UTC",
      "updated_date": "2024-04-05 14:02:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:05:25.222311"
    },
    {
      "arxiv_id": "2404.04102v2",
      "title": "ROPO: Robust Preference Optimization for Large Language Models",
      "title_zh": "ROPO: 针对大型语言模型的稳健偏好优化",
      "authors": [
        "Xize Liang",
        "Chao Chen",
        "Shuang Qiu",
        "Jie Wang",
        "Yue Wu",
        "Zhihang Fu",
        "Zhihao Shi",
        "Feng Wu",
        "Jieping Ye"
      ],
      "abstract": "Preference alignment is pivotal for empowering large language models (LLMs)\nto generate helpful and harmless responses. However, the performance of\npreference alignment is highly sensitive to the prevalent noise in the\npreference data. Recent efforts for this problem either marginally alleviate\nthe impact of noise without the ability to actually reduce its presence, or\nrely on costly teacher LLMs prone to reward misgeneralization. To address these\nchallenges, we propose the RObust Preference Optimization (ROPO) framework, an\niterative alignment approach that integrates noise-tolerance and filtering of\nnoisy samples without the aid of external models. Specifically, ROPO\niteratively solves a constrained optimization problem, where we dynamically\nassign a quality-aware weight for each sample and constrain the sum of the\nweights to the number of samples we intend to retain. For noise-tolerant\ntraining and effective noise identification, we derive a robust loss by\nsuppressing the gradients of samples with high uncertainty. We demonstrate both\nempirically and theoretically that the derived loss is critical for\ndistinguishing noisy samples from clean ones. Furthermore, inspired by our\nderived loss, we propose a robustness-guided rejection sampling technique to\ncompensate for the potential important information in discarded queries.\nExperiments on three widely-used datasets with Mistral-7B and Llama-2-7B\ndemonstrate that ROPO significantly outperforms existing preference alignment\nmethods, with its superiority growing as the noise rate increases.",
      "tldr_zh": "该研究提出ROPO框架，一种鲁棒的偏好优化方法，用于提升大型语言模型（LLMs）的偏好对齐性能，以生成更有帮助且无害的响应，同时缓解偏好数据中噪声的影响。ROPO通过迭代解决约束优化问题，为每个样本动态分配质量感知权重，并采用一个鲁棒损失函数来抑制高不确定性样本的梯度，从而实现噪声容忍训练和有效识别噪声样本；此外，还引入基于鲁棒性的拒绝采样技术来保留丢弃查询中的重要信息。实验在Mistral-7B和Llama-2-7B模型上使用三个常用数据集表明，ROPO显著优于现有方法，尤其在噪声率增加时，其性能优势更为突出。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.04102v2",
      "published_date": "2024-04-05 13:58:51 UTC",
      "updated_date": "2024-05-28 17:11:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:05:37.561676"
    },
    {
      "arxiv_id": "2405.10957v1",
      "title": "Statistical Mechanics and Artificial Neural Networks: Principles, Models, and Applications",
      "title_zh": "统计力学和人工神经网络：原理、模型和应用",
      "authors": [
        "Lucas Böttcher",
        "Gregory Wheeler"
      ],
      "abstract": "The field of neuroscience and the development of artificial neural networks\n(ANNs) have mutually influenced each other, drawing from and contributing to\nmany concepts initially developed in statistical mechanics. Notably, Hopfield\nnetworks and Boltzmann machines are versions of the Ising model, a model\nextensively studied in statistical mechanics for over a century. In the first\npart of this chapter, we provide an overview of the principles, models, and\napplications of ANNs, highlighting their connections to statistical mechanics\nand statistical learning theory.\n  Artificial neural networks can be seen as high-dimensional mathematical\nfunctions, and understanding the geometric properties of their loss landscapes\n(i.e., the high-dimensional space on which one wishes to find extrema or\nsaddles) can provide valuable insights into their optimization behavior,\ngeneralization abilities, and overall performance. Visualizing these functions\ncan help us design better optimization methods and improve their generalization\nabilities. Thus, the second part of this chapter focuses on quantifying\ngeometric properties and visualizing loss functions associated with deep ANNs.",
      "tldr_zh": "这篇论文探讨了统计力学与人工神经网络(ANNs)的紧密联系，包括其原则、模型和应用，强调神经科学和ANNs如何相互借鉴统计力学的概念，如Hopfield networks和Boltzmann machines作为Ising模型的变体。论文第一部分概述了ANNs的核心原理及其与统计力学和统计学习理论的关联。第二部分则聚焦于ANNs损失景观的几何属性，通过量化与可视化这些高维函数，来提供优化行为、泛化能力和整体性能的洞见，从而帮助设计更有效的优化方法。",
      "categories": [
        "cond-mat.dis-nn",
        "cond-mat.stat-mech",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cond-mat.dis-nn",
      "comment": "45 pages, 12 figures. arXiv admin note: text overlap with\n  arXiv:2208.13219",
      "pdf_url": "http://arxiv.org/pdf/2405.10957v1",
      "published_date": "2024-04-05 13:54:58 UTC",
      "updated_date": "2024-04-05 13:54:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:05:49.128693"
    },
    {
      "arxiv_id": "2404.04095v1",
      "title": "Dynamic Prompt Optimizing for Text-to-Image Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Wenyi Mo",
        "Tianyu Zhang",
        "Yalong Bai",
        "Bing Su",
        "Ji-Rong Wen",
        "Qing Yang"
      ],
      "abstract": "Text-to-image generative models, specifically those based on diffusion models\nlike Imagen and Stable Diffusion, have made substantial advancements. Recently,\nthere has been a surge of interest in the delicate refinement of text prompts.\nUsers assign weights or alter the injection time steps of certain words in the\ntext prompts to improve the quality of generated images. However, the success\nof fine-control prompts depends on the accuracy of the text prompts and the\ncareful selection of weights and time steps, which requires significant manual\nintervention. To address this, we introduce the \\textbf{P}rompt\n\\textbf{A}uto-\\textbf{E}diting (PAE) method. Besides refining the original\nprompts for image generation, we further employ an online reinforcement\nlearning strategy to explore the weights and injection time steps of each word,\nleading to the dynamic fine-control prompts. The reward function during\ntraining encourages the model to consider aesthetic score, semantic\nconsistency, and user preferences. Experimental results demonstrate that our\nproposed method effectively improves the original prompts, generating visually\nmore appealing images while maintaining semantic alignment. Code is available\nat https://github.com/Mowenyii/PAE.",
      "tldr_zh": "本研究针对基于扩散模型（如Imagen和Stable Diffusion）的文本到图像生成模型，解决了文本提示的精细优化问题，该过程通常需要手动调整权重和注入时间步骤。论文提出Prompt Auto-Editing (PAE)方法，通过在线强化学习策略自动精炼原始提示，并动态探索每个词的权重和注入时间步骤。奖励函数结合美学分数、语义一致性和用户偏好来指导优化。实验结果表明，PAE显著提升了图像生成质量，使输出更具吸引力，同时保持了语义对齐。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to CVPR 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.04095v1",
      "published_date": "2024-04-05 13:44:39 UTC",
      "updated_date": "2024-04-05 13:44:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:06:01.088281"
    },
    {
      "arxiv_id": "2404.04067v4",
      "title": "Does Biomedical Training Lead to Better Medical Performance?",
      "title_zh": "翻译失败",
      "authors": [
        "Amin Dada",
        "Marie Bauer",
        "Amanda Butler Contreras",
        "Osman Alperen Koraş",
        "Constantin Marc Seibold",
        "Kaleb E Smith",
        "Jens Kleesiek"
      ],
      "abstract": "Large Language Models (LLMs) are expected to significantly contribute to\npatient care, diagnostics, and administrative processes. Emerging biomedical\nLLMs aim to address healthcare-specific challenges, including privacy demands\nand computational constraints. Assessing the models' suitability for this\nsensitive application area is of the utmost importance. However, biomedical\ntraining has not been systematically evaluated on medical tasks. This study\ninvestigates the effect of biomedical training in the context of six practical\nmedical tasks evaluating $25$ models. In contrast to previous evaluations, our\nresults reveal a performance decline in nine out of twelve biomedical models\nafter fine-tuning, particularly on tasks involving hallucinations, ICD10\ncoding, and instruction adherence. General-domain models like\nMeta-Llama-3.1-70B-Instruct outperformed their biomedical counterparts,\nindicating a trade-off between domain-specific fine-tuning and general medical\ntask performance. We open-source all evaluation scripts and datasets at\nhttps://github.com/TIO-IKIM/CLUE to support further research in this critical\narea.",
      "tldr_zh": "本研究评估了生物医学训练是否能提升大型语言模型（LLMs）在医疗任务中的表现，涉及25个模型和六个实际医疗任务，如患者护理和诊断。结果显示，12个生物医学模型中有9个在微调后性能下降，特别是 hallucination、ICD10编码和指令遵守任务上。通用领域模型如Meta-Llama-3.1-70B-Instruct表现优于生物医学版本，揭示了领域特定微调与整体医疗任务性能之间的权衡。该论文开源了评估脚本和数据集（https://github.com/TIO-IKIM/CLUE），以支持进一步研究。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.04067v4",
      "published_date": "2024-04-05 12:51:37 UTC",
      "updated_date": "2024-09-17 08:19:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:06:13.045029"
    },
    {
      "arxiv_id": "2404.04057v3",
      "title": "Score identity Distillation: Exponentially Fast Distillation of Pretrained Diffusion Models for One-Step Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Mingyuan Zhou",
        "Huangjie Zheng",
        "Zhendong Wang",
        "Mingzhang Yin",
        "Hai Huang"
      ],
      "abstract": "We introduce Score identity Distillation (SiD), an innovative data-free\nmethod that distills the generative capabilities of pretrained diffusion models\ninto a single-step generator. SiD not only facilitates an exponentially fast\nreduction in Fr\\'echet inception distance (FID) during distillation but also\napproaches or even exceeds the FID performance of the original teacher\ndiffusion models. By reformulating forward diffusion processes as semi-implicit\ndistributions, we leverage three score-related identities to create an\ninnovative loss mechanism. This mechanism achieves rapid FID reduction by\ntraining the generator using its own synthesized images, eliminating the need\nfor real data or reverse-diffusion-based generation, all accomplished within\nsignificantly shortened generation time. Upon evaluation across four benchmark\ndatasets, the SiD algorithm demonstrates high iteration efficiency during\ndistillation and surpasses competing distillation approaches, whether they are\none-step or few-step, data-free, or dependent on training data, in terms of\ngeneration quality. This achievement not only redefines the benchmarks for\nefficiency and effectiveness in diffusion distillation but also in the broader\nfield of diffusion-based generation. The PyTorch implementation is available at\nhttps://github.com/mingyuanzhou/SiD",
      "tldr_zh": "我们引入了 Score identity Distillation (SiD)，一种创新的无数据方法，用于将预训练的 diffusion models 快速蒸馏到一个单步生成器中，实现指数级的 Fréchet Inception Distance (FID) 减少，并达到或超过原教师模型的性能。通过将前向扩散过程重构为半隐式分布并利用三个分数相关身份，SiD 创造了一个创新损失机制，仅使用生成器自身的合成图像进行训练，从而显著缩短生成时间。在四个基准数据集上的评估显示，SiD 展现出高迭代效率，并超越了其他一步或多步蒸馏方法，在生成质量方面重新定义了 diffusion-based 生成领域的基准。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "ICML 2024, PyTorch implementation:\n  https://github.com/mingyuanzhou/SiD",
      "pdf_url": "http://arxiv.org/pdf/2404.04057v3",
      "published_date": "2024-04-05 12:30:19 UTC",
      "updated_date": "2024-05-24 17:20:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:06:27.257997"
    },
    {
      "arxiv_id": "2404.04313v1",
      "title": "JobFormer: Skill-Aware Job Recommendation with Semantic-Enhanced Transformer",
      "title_zh": "翻译失败",
      "authors": [
        "Zhihao Guan",
        "Jia-Qi Yang",
        "Yang Yang",
        "Hengshu Zhu",
        "Wenjie Li",
        "Hui Xiong"
      ],
      "abstract": "Job recommendation aims to provide potential talents with suitable job\ndescriptions (JDs) consistent with their career trajectory, which plays an\nessential role in proactive talent recruitment. In real-world management\nscenarios, the available JD-user records always consist of JDs, user profiles,\nand click data, in which the user profiles are typically summarized as the\nuser's skill distribution for privacy reasons. Although existing sophisticated\nrecommendation methods can be directly employed, effective recommendation still\nhas challenges considering the information deficit of JD itself and the natural\nheterogeneous gap between JD and user profile. To address these challenges, we\nproposed a novel skill-aware recommendation model based on the designed\nsemantic-enhanced transformer to parse JDs and complete personalized job\nrecommendation. Specifically, we first model the relative items of each JD and\nthen adopt an encoder with the local-global attention mechanism to better mine\nthe intra-job and inter-job dependencies from JD tuples. Moreover, we adopt a\ntwo-stage learning strategy for skill-aware recommendation, in which we utilize\nthe skill distribution to guide JD representation learning in the recall stage,\nand then combine the user profiles for final prediction in the ranking stage.\nConsequently, we can embed rich contextual semantic representations for\nlearning JDs, while skill-aware recommendation provides effective JD-user joint\nrepresentation for click-through rate (CTR) prediction. To validate the\nsuperior performance of our method for job recommendation, we present a\nthorough empirical analysis of large-scale real-world and public datasets to\ndemonstrate its effectiveness and interpretability.",
      "tldr_zh": "该论文提出JobFormer，一种基于Semantic-Enhanced Transformer的技能感知职位推荐模型，旨在解决职位描述(JDs)信息不足以及JDs与用户资料（技能分布）之间异构差距的问题。具体而言，该模型使用局部-全局注意力机制挖掘JD的内部和外部依赖，并采用两阶段学习策略：在召回阶段指导JD表示学习，在排名阶段结合用户资料进行点击率(CTR)预测。实验结果显示，JobFormer在大型真实世界和公共数据集上表现出色，证明了其有效性和可解释性。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.04313v1",
      "published_date": "2024-04-05 12:25:00 UTC",
      "updated_date": "2024-04-05 12:25:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:06:38.477439"
    },
    {
      "arxiv_id": "2404.04312v1",
      "title": "Half-Space Feature Learning in Neural Networks",
      "title_zh": "神经网络中的半空间特征学习",
      "authors": [
        "Mahesh Lorik Yadav",
        "Harish Guruprasad Ramaswamy",
        "Chandrashekar Lakshminarayanan"
      ],
      "abstract": "There currently exist two extreme viewpoints for neural network feature\nlearning -- (i) Neural networks simply implement a kernel method (a la NTK) and\nhence no features are learned (ii) Neural networks can represent (and hence\nlearn) intricate hierarchical features suitable for the data. We argue in this\npaper neither interpretation is likely to be correct based on a novel\nviewpoint. Neural networks can be viewed as a mixture of experts, where each\nexpert corresponds to a (number of layers length) path through a sequence of\nhidden units. We use this alternate interpretation to motivate a model, called\nthe Deep Linearly Gated Network (DLGN), which sits midway between deep linear\nnetworks and ReLU networks. Unlike deep linear networks, the DLGN is capable of\nlearning non-linear features (which are then linearly combined), and unlike\nReLU networks these features are ultimately simple -- each feature is\neffectively an indicator function for a region compactly described as an\nintersection of (number of layers) half-spaces in the input space. This\nviewpoint allows for a comprehensive global visualization of features, unlike\nthe local visualizations for neurons based on saliency/activation/gradient\nmaps. Feature learning in DLGNs is shown to happen and the mechanism with which\nthis happens is through learning half-spaces in the input space that contain\nsmooth regions of the target function. Due to the structure of DLGNs, the\nneurons in later layers are fundamentally the same as those in earlier layers\n-- they all represent a half-space -- however, the dynamics of gradient descent\nimpart a distinct clustering to the later layer neurons. We hypothesize that\nReLU networks also have similar feature learning behaviour.",
      "tldr_zh": "本文质疑神经网络特征学习的两种极端观点，即仅实现核方法（如NTK）而不学习特征，或能学习复杂层次特征，提出神经网络可视为专家混合模型。作者引入Deep Linearly Gated Network (DLGN)，一种介于深线性网络和ReLU networks之间的框架，DLGN能学习非线性特征，这些特征本质上是输入空间中半空间交集的指示函数，便于全局可视化。实验结果显示，DLGN通过学习包含目标函数平滑区域的半空间实现特征学习，后层神经元表现出聚类行为，并推测ReLU networks可能具有类似机制。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.04312v1",
      "published_date": "2024-04-05 12:03:19 UTC",
      "updated_date": "2024-04-05 12:03:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:06:50.945949"
    },
    {
      "arxiv_id": "2404.04311v1",
      "title": "A Real-time Anomaly Detection Using Convolutional Autoencoder with Dynamic Threshold",
      "title_zh": "翻译失败",
      "authors": [
        "Sarit Maitra",
        "Sukanya Kundu",
        "Aishwarya Shankar"
      ],
      "abstract": "The majority of modern consumer-level energy is generated by real-time smart\nmetering systems. These frequently contain anomalies, which prevent reliable\nestimates of the series' evolution. This work introduces a hybrid modeling\napproach combining statistics and a Convolutional Autoencoder with a dynamic\nthreshold. The threshold is determined based on Mahalanobis distance and moving\naverages. It has been tested using real-life energy consumption data collected\nfrom smart metering systems. The solution includes a real-time, meter-level\nanomaly detection system that connects to an advanced monitoring system. This\nmakes a substantial contribution by detecting unusual data movements and\ndelivering an early warning. Early detection and subsequent troubleshooting can\nfinancially benefit organizations and consumers and prevent disasters from\noccurring.",
      "tldr_zh": "这篇论文提出了一种实时异常检测方法，使用Convolutional Autoencoder结合统计技术，并引入基于Mahalanobis distance和移动平均的动态阈值。方法针对智能电表系统的能源消耗数据进行测试，能够实时识别异常数据模式，并连接到高级监控系统提供早期警告。总体上，该方案显著提升了异常检测的准确性，有助于组织和消费者减少经济损失并预防潜在灾害。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.04311v1",
      "published_date": "2024-04-05 11:03:36 UTC",
      "updated_date": "2024-04-05 11:03:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:07:02.200114"
    },
    {
      "arxiv_id": "2404.14416v1",
      "title": "Conditional diffusion models for downscaling & bias correction of Earth system model precipitation",
      "title_zh": "翻译失败",
      "authors": [
        "Michael Aich",
        "Philipp Hess",
        "Baoxiang Pan",
        "Sebastian Bathiany",
        "Yu Huang",
        "Niklas Boers"
      ],
      "abstract": "Climate change exacerbates extreme weather events like heavy rainfall and\nflooding. As these events cause severe losses of property and lives, accurate\nhigh-resolution simulation of precipitation is imperative. However, existing\nEarth System Models (ESMs) struggle with resolving small-scale dynamics and\nsuffer from biases, especially for extreme events. Traditional statistical bias\ncorrection and downscaling methods fall short in improving spatial structure,\nwhile recent deep learning methods lack controllability over the output and\nsuffer from unstable training. Here, we propose a novel machine learning\nframework for simultaneous bias correction and downscaling. We train a\ngenerative diffusion model in a supervised way purely on observational data. We\nmap observational and ESM data to a shared embedding space, where both are\nunbiased towards each other and train a conditional diffusion model to reverse\nthe mapping. Our method can be used to correct any ESM field, as the training\nis independent of the ESM. Our approach ensures statistical fidelity, preserves\nlarge-scale spatial patterns and outperforms existing methods especially\nregarding extreme events and small-scale spatial features that are crucial for\nimpact assessments.",
      "tldr_zh": "本研究针对气候变化导致的极端天气事件（如暴雨和洪水），提出了一种新型机器学习框架，用于同时进行 Earth System Models (ESMs) 降水的 downscaling 和 bias correction。框架基于条件扩散模型（conditional diffusion models），通过在观测数据上进行监督训练，将观测数据和 ESM 数据映射到共享的嵌入空间，并逆转该映射以生成更准确的输出。该方法独立于特定 ESM，确保统计保真度并保留大尺度空间模式，尤其在极端事件和小尺度特征上，优于传统统计方法和深度学习方法。实验结果显示，该框架在影响评估中表现出色，能有效提升高分辨率降水的模拟精度。",
      "categories": [
        "physics.geo-ph",
        "cs.AI",
        "cs.LG",
        "physics.ao-ph"
      ],
      "primary_category": "physics.geo-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.14416v1",
      "published_date": "2024-04-05 11:01:50 UTC",
      "updated_date": "2024-04-05 11:01:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:07:15.251891"
    },
    {
      "arxiv_id": "2404.04310v1",
      "title": "Suppressing Modulation Instability with Reinforcement Learning",
      "title_zh": "通过强化学习抑制调制不稳定性",
      "authors": [
        "Nikolay Kalmykov",
        "Rishat Zagidullin",
        "Oleg Rogov",
        "Sergey Rykovanov",
        "Dmitry V. Dylov"
      ],
      "abstract": "Modulation instability is a phenomenon of spontaneous pattern formation in\nnonlinear media, oftentimes leading to an unpredictable behaviour and a\ndegradation of a signal of interest. We propose an approach based on\nreinforcement learning to suppress the unstable modes by optimizing the\nparameters for the time modulation of the potential in the nonlinear system. We\ntest our approach in 1D and 2D cases and propose a new class of\nphysically-meaningful reward functions to guarantee tamed instability.",
      "tldr_zh": "本文研究了调制不稳定性(Modulation Instability)，这是一种在非线性介质中导致自发图案形成、信号退化和不可预测行为的现象。作者提出了一种基于强化学习(Reinforcement Learning)的方法，通过优化非线性系统中势能的时间调制参数来抑制不稳定模式，并在1D和2D情况下进行了测试。同时，论文引入了新的物理意义明确的奖励函数，以确保不稳定性得到有效控制。",
      "categories": [
        "nlin.PS",
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY",
        "physics.app-ph"
      ],
      "primary_category": "nlin.PS",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.04310v1",
      "published_date": "2024-04-05 10:29:18 UTC",
      "updated_date": "2024-04-05 10:29:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:07:26.223775"
    },
    {
      "arxiv_id": "2404.04001v1",
      "title": "Approximate UMAP allows for high-rate online visualization of high-dimensional data streams",
      "title_zh": "翻译失败",
      "authors": [
        "Peter Wassenaar",
        "Pierre Guetschel",
        "Michael Tangermann"
      ],
      "abstract": "In the BCI field, introspection and interpretation of brain signals are\ndesired for providing feedback or to guide rapid paradigm prototyping but are\nchallenging due to the high noise level and dimensionality of the signals. Deep\nneural networks are often introspected by transforming their learned feature\nrepresentations into 2- or 3-dimensional subspace visualizations using\nprojection algorithms like Uniform Manifold Approximation and Projection\n(UMAP). Unfortunately, these methods are computationally expensive, making the\nprojection of data streams in real-time a non-trivial task. In this study, we\nintroduce a novel variant of UMAP, called approximate UMAP (aUMAP). It aims at\ngenerating rapid projections for real-time introspection. To study its\nsuitability for real-time projecting, we benchmark the methods against standard\nUMAP and its neural network counterpart parametric UMAP. Our results show that\napproximate UMAP delivers projections that replicate the projection space of\nstandard UMAP while decreasing projection speed by an order of magnitude and\nmaintaining the same training time.",
      "tldr_zh": "本研究针对脑机接口(BCI)领域的高维脑信号实时可视化问题，提出了一种新型投影算法approximate UMAP (aUMAP)，旨在加速Uniform Manifold Approximation and Projection (UMAP)以支持高维数据流的在线处理。aUMAP通过优化投影过程，显著降低计算开销，同时保持与标准UMAP相似的投影空间。实验结果显示，aUMAP将投影速度提高了10倍，同时训练时间不变，为BCI中的实时内省和反馈提供高效解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.HC",
        "eess.SP",
        "I.5.3; I.5.3; J.4"
      ],
      "primary_category": "cs.LG",
      "comment": "6 pages, 3 figures, submitted to the Graz BCI conference 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.04001v1",
      "published_date": "2024-04-05 10:25:26 UTC",
      "updated_date": "2024-04-05 10:25:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:07:38.055249"
    },
    {
      "arxiv_id": "2404.03997v1",
      "title": "Demonstration Guided Multi-Objective Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Junlin Lu",
        "Patrick Mannion",
        "Karl Mason"
      ],
      "abstract": "Multi-objective reinforcement learning (MORL) is increasingly relevant due to\nits resemblance to real-world scenarios requiring trade-offs between multiple\nobjectives. Catering to diverse user preferences, traditional reinforcement\nlearning faces amplified challenges in MORL. To address the difficulty of\ntraining policies from scratch in MORL, we introduce demonstration-guided\nmulti-objective reinforcement learning (DG-MORL). This novel approach utilizes\nprior demonstrations, aligns them with user preferences via corner weight\nsupport, and incorporates a self-evolving mechanism to refine suboptimal\ndemonstrations. Our empirical studies demonstrate DG-MORL's superiority over\nexisting MORL algorithms, establishing its robustness and efficacy,\nparticularly under challenging conditions. We also provide an upper bound of\nthe algorithm's sample complexity.",
      "tldr_zh": "这篇论文针对多目标强化学习 (MORL) 的挑战，提出了一种 demonstration-guided multi-objective reinforcement learning (DG-MORL) 方法，以处理真实场景中多个目标的权衡和用户偏好多样性问题。DG-MORL 利用先验演示，通过 corner weight support 与用户偏好对齐，并引入 self-evolving mechanism 来优化次优演示，从而提升训练效率。实验结果显示，DG-MORL 比现有 MORL 算法更具鲁棒性和有效性，尤其在复杂条件下表现突出，并提供了算法的样本复杂度上界。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.03997v1",
      "published_date": "2024-04-05 10:19:04 UTC",
      "updated_date": "2024-04-05 10:19:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:07:51.064753"
    },
    {
      "arxiv_id": "2404.03996v1",
      "title": "Fast Genetic Algorithm for feature selection -- A qualitative approximation approach",
      "title_zh": "翻译失败",
      "authors": [
        "Mohammed Ghaith Altarabichi",
        "Sławomir Nowaczyk",
        "Sepideh Pashami",
        "Peyman Sheikholharam Mashhadi"
      ],
      "abstract": "Evolutionary Algorithms (EAs) are often challenging to apply in real-world\nsettings since evolutionary computations involve a large number of evaluations\nof a typically expensive fitness function. For example, an evaluation could\ninvolve training a new machine learning model. An approximation (also known as\nmeta-model or a surrogate) of the true function can be used in such\napplications to alleviate the computation cost. In this paper, we propose a\ntwo-stage surrogate-assisted evolutionary approach to address the computational\nissues arising from using Genetic Algorithm (GA) for feature selection in a\nwrapper setting for large datasets. We define 'Approximation Usefulness' to\ncapture the necessary conditions to ensure correctness of the EA computations\nwhen an approximation is used. Based on this definition, we propose a procedure\nto construct a lightweight qualitative meta-model by the active selection of\ndata instances. We then use a meta-model to carry out the feature selection\ntask. We apply this procedure to the GA-based algorithm CHC (Cross generational\nelitist selection, Heterogeneous recombination and Cataclysmic mutation) to\ncreate a Qualitative approXimations variant, CHCQX. We show that CHCQX\nconverges faster to feature subset solutions of significantly higher accuracy\n(as compared to CHC), particularly for large datasets with over 100K instances.\nWe also demonstrate the applicability of the thinking behind our approach more\nbroadly to Swarm Intelligence (SI), another branch of the Evolutionary\nComputation (EC) paradigm with results of PSOQX, a qualitative approximation\nadaptation of the Particle Swarm Optimization (PSO) method. A GitHub repository\nwith the complete implementation is available.",
      "tldr_zh": "该论文提出了一种两阶段 surrogate-assisted 进化方法，用于加速 Genetic Algorithm (GA) 在特征选择中的应用，旨在解决 Evolutionary Algorithms (EAs) 计算成本高的问题。作者定义了 'Approximation Usefulness' 概念，并通过 active selection of data instances 构建轻量级 qualitative meta-model，以在 wrapper 设置下处理大型数据集。实验结果显示，该方法应用于 CHC 算法的变体 CHCQX，能比原 CHC 更快收敛并显著提高特征子集准确率，尤其在超过 100K 实例的数据集上；此外，该思路扩展到 Swarm Intelligence (SI) 的 Particle Swarm Optimization (PSO)，创建了 PSOQX 变体，并提供了 GitHub 仓库实现。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.03996v1",
      "published_date": "2024-04-05 10:15:24 UTC",
      "updated_date": "2024-04-05 10:15:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:08:03.047965"
    },
    {
      "arxiv_id": "2404.03995v1",
      "title": "Balancing Progress and Responsibility: A Synthesis of Sustainability Trade-Offs of AI-Based Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Apoorva Nalini Pradeep Kumar",
        "Justus Bogner",
        "Markus Funke",
        "Patricia Lago"
      ],
      "abstract": "Recent advances in artificial intelligence (AI) capabilities have increased\nthe eagerness of companies to integrate AI into software systems. While AI can\nbe used to have a positive impact on several dimensions of sustainability, this\nis often overshadowed by its potential negative influence. While many studies\nhave explored sustainability factors in isolation, there is insufficient\nholistic coverage of potential sustainability benefits or costs that\npractitioners need to consider during decision-making for AI adoption. We\ntherefore aim to synthesize trade-offs related to sustainability in the context\nof integrating AI into software systems. We want to make the sustainability\nbenefits and costs of integrating AI more transparent and accessible for\npractitioners.\n  The study was conducted in collaboration with a Dutch financial organization.\nWe first performed a rapid review that led to the inclusion of 151 research\npapers. Afterward, we conducted six semi-structured interviews to enrich the\ndata with industry perspectives. The combined results showcase the potential\nsustainability benefits and costs of integrating AI. The labels synthesized\nfrom the review regarding potential sustainability benefits were clustered into\n16 themes, with \"energy management\" being the most frequently mentioned one. 11\nthemes were identified in the interviews, with the top mentioned theme being\n\"employee wellbeing\". Regarding sustainability costs, the review discovered\nseven themes, with \"deployment issues\" being the most popular one, followed by\n\"ethics & society\". \"Environmental issues\" was the top theme from the\ninterviews. Our results provide valuable insights to organizations and\npractitioners for understanding the potential sustainability implications of\nadopting AI.",
      "tldr_zh": "该研究综合分析了将AI整合到软件系统中的可持续性权衡，旨在平衡AI的积极影响（如能源管理）和潜在负面成本（如部署问题和伦理社会问题），以帮助从业者进行决策。研究方法包括对151篇文献的快速综述和与一家荷兰金融组织的6次半结构化访谈，从中提炼出16个可持续性益处主题（以\"energy management\"为最常见）和7个成本主题（以\"deployment issues\"为首）。结果显示，访谈强调了\"employee wellbeing\"和\"environmental issues\"的重要性，为组织理解AI采用的可持续性影响提供了宝贵洞见。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted for publication at the 8th International Workshop on Green\n  and Sustainable Software (GREENS'24), collocated with ICSA'24",
      "pdf_url": "http://arxiv.org/pdf/2404.03995v1",
      "published_date": "2024-04-05 10:11:08 UTC",
      "updated_date": "2024-04-05 10:11:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:08:15.114698"
    },
    {
      "arxiv_id": "2404.03992v1",
      "title": "Rolling the dice for better deep learning performance: A study of randomness techniques in deep neural networks",
      "title_zh": "翻译失败",
      "authors": [
        "Mohammed Ghaith Altarabichi",
        "Sławomir Nowaczyk",
        "Sepideh Pashami",
        "Peyman Sheikholharam Mashhadi",
        "Julia Handl"
      ],
      "abstract": "This paper investigates how various randomization techniques impact Deep\nNeural Networks (DNNs). Randomization, like weight noise and dropout, aids in\nreducing overfitting and enhancing generalization, but their interactions are\npoorly understood. The study categorizes randomness techniques into four types\nand proposes new methods: adding noise to the loss function and random masking\nof gradient updates. Using Particle Swarm Optimizer (PSO) for hyperparameter\noptimization, it explores optimal configurations across MNIST, FASHION-MNIST,\nCIFAR10, and CIFAR100 datasets. Over 30,000 configurations are evaluated,\nrevealing data augmentation and weight initialization randomness as main\nperformance contributors. Correlation analysis shows different optimizers\nprefer distinct randomization types. The complete implementation and dataset\nare available on GitHub.",
      "tldr_zh": "这篇论文研究了随机化技术对 Deep Neural Networks (DNNs) 的影响，旨在减少过拟合并提升泛化性能，同时探讨了这些技术的交互作用。论文将随机化技术分类为四类，并提出新方法，包括向 loss function 添加噪声和随机 masking of gradient updates，并使用 Particle Swarm Optimizer (PSO) 优化超参数，在 MNIST、FASHION-MNIST、CIFAR10 和 CIFAR100 数据集上评估超过 30,000 个配置。实验发现，data augmentation 和 weight initialization randomness 是主要性能贡献者，而相关分析显示不同优化器偏好不同的随机化类型。该研究提供了完整的开源实现，以促进进一步应用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.03992v1",
      "published_date": "2024-04-05 10:02:32 UTC",
      "updated_date": "2024-04-05 10:02:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:08:28.912741"
    },
    {
      "arxiv_id": "2404.03978v2",
      "title": "Random Walk in Random Permutation Set Theory",
      "title_zh": "翻译失败",
      "authors": [
        "Jiefeng Zhou",
        "Zhen Li",
        "Yong Deng"
      ],
      "abstract": "Random walk is an explainable approach for modeling natural processes at the\nmolecular level. The Random Permutation Set Theory (RPST) serves as a framework\nfor uncertainty reasoning, extending the applicability of Dempster-Shafer\nTheory. Recent explorations indicate a promising link between RPST and random\nwalk. In this study, we conduct an analysis and construct a random walk model\nbased on the properties of RPST, with Monte Carlo simulations of such random\nwalk. Our findings reveal that the random walk generated through RPST exhibits\ncharacteristics similar to those of a Gaussian random walk and can be\ntransformed into a Wiener process through a specific limiting scaling\nprocedure. This investigation establishes a novel connection between RPST and\nrandom walk theory, thereby not only expanding the applicability of RPST, but\nalso demonstrating the potential for combining the strengths of both approaches\nto improve problem-solving abilities.",
      "tldr_zh": "本研究探讨了Random Permutation Set Theory (RPST) 与随机游走之间的联系，RPST 作为一种扩展Dempster-Shafer Theory 的不确定性推理框架，用于建模分子级自然过程。研究者分析了 RPST 的属性，构建了一个基于其特性的随机游走模型，并通过Monte Carlo simulations 进行模拟。结果显示，该模型生成的随机游走类似于Gaussian random walk，并可通过特定限制缩放过程转化为Wiener process，从而建立了RPST 与随机游走理论的新连接，扩展了RPST 的适用性和问题解决潜力。",
      "categories": [
        "cs.AI",
        "cs.IT",
        "math.IT"
      ],
      "primary_category": "cs.AI",
      "comment": "27 pages, 8 figures; references added",
      "pdf_url": "http://arxiv.org/pdf/2404.03978v2",
      "published_date": "2024-04-05 09:19:55 UTC",
      "updated_date": "2024-04-22 15:18:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:08:39.398735"
    },
    {
      "arxiv_id": "2404.04308v1",
      "title": "Visual Knowledge in the Big Model Era: Retrospect and Prospect",
      "title_zh": "视觉知识在大模型时代：回顾与展望",
      "authors": [
        "Wenguan Wang",
        "Yi Yang",
        "Yunhe Pan"
      ],
      "abstract": "Visual knowledge is a new form of knowledge representation that can\nencapsulate visual concepts and their relations in a succinct, comprehensive,\nand interpretable manner, with a deep root in cognitive psychology. As the\nknowledge about the visual world has been identified as an indispensable\ncomponent of human cognition and intelligence, visual knowledge is poised to\nhave a pivotal role in establishing machine intelligence. With the recent\nadvance of Artificial Intelligence (AI) techniques, large AI models (or\nfoundation models) have emerged as a potent tool capable of extracting\nversatile patterns from broad data as implicit knowledge, and abstracting them\ninto an outrageous amount of numeric parameters. To pave the way for creating\nvisual knowledge empowered AI machines in this coming wave, we present a timely\nreview that investigates the origins and development of visual knowledge in the\npre-big model era, and accentuates the opportunities and unique role of visual\nknowledge in the big model era.",
      "tldr_zh": "该论文回顾了视觉知识（visual knowledge）作为一种简洁、全面且可解释的知识表示形式，其源于认知心理学，并在人类认知和机器智能中发挥关键作用。作者分析了在大数据模型时代（big model era）之前视觉知识的起源和发展，强调大型AI模型（large AI models或foundation models）能够从海量数据中提取隐性知识并抽象为参数。展望未来，该研究突出了视觉知识在增强AI机器智能方面的独特机遇和潜力。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.04308v1",
      "published_date": "2024-04-05 07:31:24 UTC",
      "updated_date": "2024-04-05 07:31:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:08:51.632006"
    },
    {
      "arxiv_id": "2404.04306v1",
      "title": "AuditGPT: Auditing Smart Contracts with ChatGPT",
      "title_zh": "AuditGPT：使用 ChatGPT 审计智能合约",
      "authors": [
        "Shihao Xia",
        "Shuai Shao",
        "Mengting He",
        "Tingting Yu",
        "Linhai Song",
        "Yiying Zhang"
      ],
      "abstract": "To govern smart contracts running on Ethereum, multiple Ethereum Request for\nComment (ERC) standards have been developed, each containing a set of rules to\nguide the behaviors of smart contracts. Violating the ERC rules could cause\nserious security issues and financial loss, signifying the importance of\nverifying smart contracts follow ERCs. Today's practices of such verification\nare to either manually audit each single contract or use expert-developed,\nlimited-scope program-analysis tools, both of which are far from being\neffective in identifying ERC rule violations. This paper presents a tool named\nAuditGPT that leverages large language models (LLMs) to automatically and\ncomprehensively verify ERC rules against smart contracts. To build AuditGPT, we\nfirst conduct an empirical study on 222 ERC rules specified in four popular\nERCs to understand their content, their security impacts, their specification\nin natural language, and their implementation in Solidity. Guided by the study,\nwe construct AuditGPT by separating the large, complex auditing process into\nsmall, manageable tasks and design prompts specialized for each ERC rule type\nto enhance LLMs' auditing performance. In the evaluation, AuditGPT successfully\npinpoints 418 ERC rule violations and only reports 18 false positives,\nshowcasing its effectiveness and accuracy. Moreover, AuditGPT beats an auditing\nservice provided by security experts in effectiveness, accuracy, and cost,\ndemonstrating its advancement over state-of-the-art smart-contract auditing\npractices.",
      "tldr_zh": "这篇论文引入了 AuditGPT，一种利用大型语言模型 (LLMs) 自动审计智能合约是否遵守 Ethereum Request for Comment (ERC) 标准的工具，以解决手动审计或程序分析工具的低效问题。通过对 222 个 ERC 规则进行实证研究，AuditGPT 将审计过程分解为小任务，并为每种规则类型设计专用提示来提升准确性。实验结果显示，该工具成功识别 418 个规则违反，仅有 18 个假阳性，并在有效性、准确性和成本上优于现有专家审计服务。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.04306v1",
      "published_date": "2024-04-05 07:19:13 UTC",
      "updated_date": "2024-04-05 07:19:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:09:04.253685"
    },
    {
      "arxiv_id": "2404.03913v1",
      "title": "Concept Weaver: Enabling Multi-Concept Fusion in Text-to-Image Models",
      "title_zh": "翻译失败",
      "authors": [
        "Gihyun Kwon",
        "Simon Jenni",
        "Dingzeyu Li",
        "Joon-Young Lee",
        "Jong Chul Ye",
        "Fabian Caba Heilbron"
      ],
      "abstract": "While there has been significant progress in customizing text-to-image\ngeneration models, generating images that combine multiple personalized\nconcepts remains challenging. In this work, we introduce Concept Weaver, a\nmethod for composing customized text-to-image diffusion models at inference\ntime. Specifically, the method breaks the process into two steps: creating a\ntemplate image aligned with the semantics of input prompts, and then\npersonalizing the template using a concept fusion strategy. The fusion strategy\nincorporates the appearance of the target concepts into the template image\nwhile retaining its structural details. The results indicate that our method\ncan generate multiple custom concepts with higher identity fidelity compared to\nalternative approaches. Furthermore, the method is shown to seamlessly handle\nmore than two concepts and closely follow the semantic meaning of the input\nprompt without blending appearances across different subjects.",
      "tldr_zh": "本文介绍了 Concept Weaver，一种在推理时组合自定义文本到图像扩散模型的方法，用于解决生成融合多个个性化概念图像的挑战。该方法分为两步：首先创建与输入提示语义对齐的模板图像，然后通过概念融合策略将目标概念的外观融入模板，同时保留其结构细节。实验结果表明，Concept Weaver 比替代方法提供更高的身份 fidelity，能无缝处理超过两个概念，并准确遵循输入提示的语义而不混合不同主体的外观。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.03913v1",
      "published_date": "2024-04-05 06:41:27 UTC",
      "updated_date": "2024-04-05 06:41:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:09:16.007493"
    },
    {
      "arxiv_id": "2404.03912v1",
      "title": "Forget NLI, Use a Dictionary: Zero-Shot Topic Classification for Low-Resource Languages with Application to Luxembourgish",
      "title_zh": "翻译失败",
      "authors": [
        "Fred Philippy",
        "Shohreh Haddadan",
        "Siwen Guo"
      ],
      "abstract": "In NLP, zero-shot classification (ZSC) is the task of assigning labels to\ntextual data without any labeled examples for the target classes. A common\nmethod for ZSC is to fine-tune a language model on a Natural Language Inference\n(NLI) dataset and then use it to infer the entailment between the input\ndocument and the target labels. However, this approach faces certain\nchallenges, particularly for languages with limited resources. In this paper,\nwe propose an alternative solution that leverages dictionaries as a source of\ndata for ZSC. We focus on Luxembourgish, a low-resource language spoken in\nLuxembourg, and construct two new topic relevance classification datasets based\non a dictionary that provides various synonyms, word translations and example\nsentences. We evaluate the usability of our dataset and compare it with the\nNLI-based approach on two topic classification tasks in a zero-shot manner. Our\nresults show that by using the dictionary-based dataset, the trained models\noutperform the ones following the NLI-based approach for ZSC. While we focus on\na single low-resource language in this study, we believe that the efficacy of\nour approach can also transfer to other languages where such a dictionary is\navailable.",
      "tldr_zh": "本论文提出了一种新的零-shot topic classification (ZSC) 方法，使用字典作为数据来源，以替代传统的 Natural Language Inference (NLI) 训练方式，特别针对低资源语言的挑战。研究者针对 Luxembourgish 语言构建了两个新数据集，利用字典提供的同义词、翻译和示例句子来生成相关数据。实验结果显示，在零-shot 任务中，使用字典方法的模型在主题分类性能上超过了 NLI-based 接近，并认为此方法可扩展到其他有可用字典的低资源语言。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "3rd Annual Meeting of the ELRA/ISCA Special Interest Group on\n  Under-resourced Languages (SIGUL 2024)",
      "pdf_url": "http://arxiv.org/pdf/2404.03912v1",
      "published_date": "2024-04-05 06:35:31 UTC",
      "updated_date": "2024-04-05 06:35:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:09:27.604950"
    },
    {
      "arxiv_id": "2404.03908v1",
      "title": "Multi-Task Learning for Lung sound & Lung disease classification",
      "title_zh": "翻译失败",
      "authors": [
        "Suma K V",
        "Deepali Koppad",
        "Preethi Kumar",
        "Neha A Kantikar",
        "Surabhi Ramesh"
      ],
      "abstract": "In recent years, advancements in deep learning techniques have considerably\nenhanced the efficiency and accuracy of medical diagnostics. In this work, a\nnovel approach using multi-task learning (MTL) for the simultaneous\nclassification of lung sounds and lung diseases is proposed. Our proposed model\nleverages MTL with four different deep learning models such as 2D CNN,\nResNet50, MobileNet and Densenet to extract relevant features from the lung\nsound recordings. The ICBHI 2017 Respiratory Sound Database was employed in the\ncurrent study. The MTL for MobileNet model performed better than the other\nmodels considered, with an accuracy of74\\% for lung sound analysis and 91\\% for\nlung diseases classification. Results of the experimentation demonstrate the\nefficacy of our approach in classifying both lung sounds and lung diseases\nconcurrently.\n  In this study,using the demographic data of the patients from the database,\nrisk level computation for Chronic Obstructive Pulmonary Disease is also\ncarried out. For this computation, three machine learning algorithms namely\nLogistic Regression, SVM and Random Forest classifierswere employed. Among\nthese ML algorithms, the Random Forest classifier had the highest accuracy of\n92\\%.This work helps in considerably reducing the physician's burden of not\njust diagnosing the pathology but also effectively communicating to the patient\nabout the possible causes or outcomes.",
      "tldr_zh": "本研究提出了一种多任务学习 (Multi-Task Learning, MTL) 方法，用于同时分类肺音 (Lung sound) 和肺部疾病 (Lung disease)。该方法利用 2D CNN、ResNet50、MobileNet 和 DenseNet 等深度学习模型，从 ICBHI 2017 数据库的肺音记录中提取特征，其中 MobileNet 在 MTL 框架下表现最佳，实现肺音分类准确率 74% 和肺部疾病分类准确率 91%。此外，研究还使用患者人口统计数据计算慢性阻塞性肺疾病风险水平，采用 Logistic Regression、SVM 和 Random Forest 算法，其中 Random Forest 分类器准确率达 92%。这项工作证明了 MTL 方法的有效性，有助于减轻医生的诊断负担并改善患者沟通。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.03908v1",
      "published_date": "2024-04-05 06:15:58 UTC",
      "updated_date": "2024-04-05 06:15:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:09:41.021857"
    },
    {
      "arxiv_id": "2404.03900v1",
      "title": "Nonparametric Modern Hopfield Models",
      "title_zh": "非参数现代 Hopfield 模型",
      "authors": [
        "Jerry Yao-Chieh Hu",
        "Bo-Yu Chen",
        "Dennis Wu",
        "Feng Ruan",
        "Han Liu"
      ],
      "abstract": "We present a nonparametric construction for deep learning compatible modern\nHopfield models and utilize this framework to debut an efficient variant. Our\nkey contribution stems from interpreting the memory storage and retrieval\nprocesses in modern Hopfield models as a nonparametric regression problem\nsubject to a set of query-memory pairs. Crucially, our framework not only\nrecovers the known results from the original dense modern Hopfield model but\nalso fills the void in the literature regarding efficient modern Hopfield\nmodels, by introducing \\textit{sparse-structured} modern Hopfield models with\nsub-quadratic complexity. We establish that this sparse model inherits the\nappealing theoretical properties of its dense analogue -- connection with\ntransformer attention, fixed point convergence and exponential memory capacity\n-- even without knowing details of the Hopfield energy function. Additionally,\nwe showcase the versatility of our framework by constructing a family of modern\nHopfield models as extensions, including linear, random masked, top-$K$ and\npositive random feature modern Hopfield models. Empirically, we validate the\nefficacy of our framework in both synthetic and realistic settings.",
      "tldr_zh": "本文提出了一种非参数化的现代 Hopfield 模型框架，将记忆存储和检索过程视为基于查询-记忆对的非参数回归问题，从而构建了一个高效的稀疏结构变体。该框架不仅恢复了原密集模型的特性，还继承了关键理论属性，包括与 Transformer attention 的连接、固定点收敛和指数记忆容量。论文进一步扩展了多种模型变体，如线性、随机掩码、Top-K 和正随机特征现代 Hopfield 模型，并通过合成和真实场景实验验证了其有效性。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "cs.NE"
      ],
      "primary_category": "stat.ML",
      "comment": "59 pages; Code available at\n  https://github.com/MAGICS-LAB/NonparametricHopfield",
      "pdf_url": "http://arxiv.org/pdf/2404.03900v1",
      "published_date": "2024-04-05 05:46:20 UTC",
      "updated_date": "2024-04-05 05:46:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:09:52.451870"
    },
    {
      "arxiv_id": "2404.08672v3",
      "title": "Taxonomy and Analysis of Sensitive User Queries in Generative AI Search",
      "title_zh": "生成式 AI 搜索中敏感用户查询的分类和分析",
      "authors": [
        "Hwiyeol Jo",
        "Taiwoo Park",
        "Hyunwoo Lee",
        "Nayoung Choi",
        "Changbong Kim",
        "Ohjoon Kwon",
        "Donghyeon Jeon",
        "Eui-Hyeon Lee",
        "Kyoungho Shin",
        "Sun Suk Lim",
        "Kyungmi Kim",
        "Jihye Lee",
        "Sun Kim"
      ],
      "abstract": "Although there has been a growing interest among industries in integrating\ngenerative LLMs into their services, limited experience and scarcity of\nresources act as a barrier in launching and servicing large-scale LLM-based\nservices. In this paper, we share our experiences in developing and operating\ngenerative AI models within a national-scale search engine, with a specific\nfocus on the sensitiveness of user queries. We propose a taxonomy for sensitive\nsearch queries, outline our approaches, and present a comprehensive analysis\nreport on sensitive queries from actual users. We believe that our experiences\nin launching generative AI search systems can contribute to reducing the\nbarrier in building generative LLM-based services.",
      "tldr_zh": "该论文分享了在国家级搜索引擎中开发和操作生成式 AI 模型的经验，特别关注用户查询的敏感性问题。作者提出了一种敏感搜索查询的taxonomy，并描述了相应的处理方法，同时基于实际用户数据进行了全面分析。研究结果表明，这种方法有助于降低构建生成式 LLM 服务的障碍，为行业提供宝贵经验。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "NAACL2025(Findings), corrected typo in co-corresponding authors",
      "pdf_url": "http://arxiv.org/pdf/2404.08672v3",
      "published_date": "2024-04-05 05:14:46 UTC",
      "updated_date": "2025-04-16 18:59:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:10:03.503440"
    },
    {
      "arxiv_id": "2404.03894v1",
      "title": "Holon: a cybernetic interface for bio-semiotics",
      "title_zh": "翻译失败",
      "authors": [
        "Jon McCormack",
        "Elliott Wilson"
      ],
      "abstract": "This paper presents an interactive artwork, \"Holon\", a collection of 130\nautonomous, cybernetic organisms that listen and make sound in collaboration\nwith the natural environment. The work was developed for installation on water\nat a heritage-listed dock in Melbourne, Australia. Conceptual issues informing\nthe work are presented, along with a detailed technical overview of the\nimplementation. Individual holons are of three types, inspired by biological\nmodels of animal communication: composer/generators, collector/critics and\ndisruptors. Collectively, Holon integrates and occupies elements of the\nacoustic spectrum in collaboration with human and non-human agents.",
      "tldr_zh": "这篇论文介绍了互动艺术作品 Holon，这是一个由 130 个自主 cybernetic 生物体组成的系统，这些生物体通过聆听和制作声音与自然环境协作。Holon 的设计受动物通信生物模型启发，分为三种类型：composer/generators（作曲/生成器）、collector/critics（收集/评论者）和 disruptors（破坏者）。作品安装在澳大利亚墨尔本一个遗产保护码头的水上，并提供了概念问题和技术实现的详细概述。整体上，Holon 促进了人类和非人类代理在声学频谱上的整合与互动，展示了 bio-semiotics 在艺术中的应用潜力。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.MA",
        "eess.AS",
        "I.2.11; J.5"
      ],
      "primary_category": "cs.SD",
      "comment": "Paper accepted at ISEA 24, The 29th International Symposium on\n  Electronic Art, Brisbane, Australia, 21-29 June 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.03894v1",
      "published_date": "2024-04-05 05:03:39 UTC",
      "updated_date": "2024-04-05 05:03:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:10:18.086717"
    },
    {
      "arxiv_id": "2404.03893v1",
      "title": "KGExplainer: Towards Exploring Connected Subgraph Explanations for Knowledge Graph Completion",
      "title_zh": "KGExplainer: 针对知识图谱补全的连接子图解释探索",
      "authors": [
        "Tengfei Ma",
        "Xiang song",
        "Wen Tao",
        "Mufei Li",
        "Jiani Zhang",
        "Xiaoqin Pan",
        "Jianxin Lin",
        "Bosheng Song",
        "xiangxiang Zeng"
      ],
      "abstract": "Knowledge graph completion (KGC) aims to alleviate the inherent\nincompleteness of knowledge graphs (KGs), which is a critical task for various\napplications, such as recommendations on the web. Although knowledge graph\nembedding (KGE) models have demonstrated superior predictive performance on KGC\ntasks, these models infer missing links in a black-box manner that lacks\ntransparency and accountability, preventing researchers from developing\naccountable models. Existing KGE-based explanation methods focus on exploring\nkey paths or isolated edges as explanations, which is information-less to\nreason target prediction. Additionally, the missing ground truth leads to these\nexplanation methods being ineffective in quantitatively evaluating explored\nexplanations. To overcome these limitations, we propose KGExplainer, a\nmodel-agnostic method that identifies connected subgraph explanations and\ndistills an evaluator to assess them quantitatively. KGExplainer employs a\nperturbation-based greedy search algorithm to find key connected subgraphs as\nexplanations within the local structure of target predictions. To evaluate the\nquality of the explored explanations, KGExplainer distills an evaluator from\nthe target KGE model. By forwarding the explanations to the evaluator, our\nmethod can examine the fidelity of them. Extensive experiments on benchmark\ndatasets demonstrate that KGExplainer yields promising improvement and achieves\nan optimal ratio of 83.3% in human evaluation.",
      "tldr_zh": "知识图谱补全（KGC）任务中，现有知识图谱嵌入（KGE）模型虽预测性能出色，但缺乏透明性和可解释性，导致解释方法仅限于关键路径或孤立边，无法有效评估。论文提出KGExplainer，一种模型无关的方法，通过基于扰动的贪婪搜索算法在目标预测的局部结构中识别连接子图解释，并蒸馏评估器从KGE模型中量化评估这些解释的保真度。实验在基准数据集上显示，该方法显著提升解释质量，在人类评估中达到83.3%的最佳比率。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "13 pages, 7 figures, 11 tables. Under Review",
      "pdf_url": "http://arxiv.org/pdf/2404.03893v1",
      "published_date": "2024-04-05 05:02:12 UTC",
      "updated_date": "2024-04-05 05:02:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:10:31.905021"
    },
    {
      "arxiv_id": "2404.03892v3",
      "title": "Enhancing Breast Cancer Diagnosis in Mammography: Evaluation and Integration of Convolutional Neural Networks and Explainable AI",
      "title_zh": "翻译失败",
      "authors": [
        "Maryam Ahmed",
        "Tooba Bibi",
        "Rizwan Ahmed Khan",
        "Sidra Nasir"
      ],
      "abstract": "The Deep learning (DL) models for diagnosing breast cancer from mammographic\nimages often operate as \"black boxes\", making it difficult for healthcare\nprofessionals to trust and understand their decision-making processes. The\nstudy presents an integrated framework combining Convolutional Neural Networks\n(CNNs) and Explainable Artificial Intelligence (XAI) for the enhanced diagnosis\nof breast cancer using the CBIS-DDSM dataset. The methodology encompasses an\nelaborate data preprocessing pipeline and advanced data augmentation techniques\nto counteract dataset limitations and transfer learning using pre-trained\nnetworks such as VGG-16, Inception-V3 and ResNet was employed. A focal point of\nour study is the evaluation of XAI's effectiveness in interpreting model\npredictions, highlighted by utilizing the Hausdorff measure to assess the\nalignment between AI-generated explanations and expert annotations\nquantitatively. This approach is critical for XAI in promoting trustworthiness\nand ethical fairness in AI-assisted diagnostics. The findings from our research\nillustrate the effective collaboration between CNNs and XAI in advancing\ndiagnostic methods for breast cancer, thereby facilitating a more seamless\nintegration of advanced AI technologies within clinical settings. By enhancing\nthe interpretability of AI driven decisions, this work lays the groundwork for\nimproved collaboration between AI systems and medical practitioners, ultimately\nenriching patient care. Furthermore, the implications of our research extended\nwell beyond the current methodologies. It encourages further research into how\nto combine multimodal data and improve AI explanations to meet the needs of\nclinical practice.",
      "tldr_zh": "本研究提出了一种整合 Convolutional Neural Networks (CNNs) 和 Explainable AI (XAI) 的框架，以提升乳腺癌在乳房X光检查中的诊断准确性和可解释性。方法包括数据预处理管道、数据增强技术以及转移学习，使用预训练模型如 VGG-16、Inception-V3 和 ResNet 来处理 CBIS-DDSM 数据集，并通过 Hausdorff measure 量化评估 AI 生成解释与专家标注的匹配度。结果显示，该框架显著提高了诊断性能，促进了 AI 在临床环境中的可信度和公平性，最终为 AI 与医务人员协作提供基础，并鼓励未来对多模态数据和 AI 解释的进一步探索。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.03892v3",
      "published_date": "2024-04-05 05:00:21 UTC",
      "updated_date": "2024-04-27 08:24:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:10:43.108064"
    },
    {
      "arxiv_id": "2404.03891v1",
      "title": "Can only LLMs do Reasoning?: Potential of Small Language Models in Task Planning",
      "title_zh": "翻译失败",
      "authors": [
        "Gawon Choi",
        "Hyemin Ahn"
      ],
      "abstract": "In robotics, the use of Large Language Models (LLMs) is becoming prevalent,\nespecially for understanding human commands. In particular, LLMs are utilized\nas domain-agnostic task planners for high-level human commands. LLMs are\ncapable of Chain-of-Thought (CoT) reasoning, and this allows LLMs to be task\nplanners. However, we need to consider that modern robots still struggle to\nperform complex actions, and the domains where robots can be deployed are\nlimited in practice. This leads us to pose a question: If small LMs can be\ntrained to reason in chains within a single domain, would even small LMs be\ngood task planners for the robots? To train smaller LMs to reason in chains, we\nbuild `COmmand-STeps datasets' (COST) consisting of high-level commands along\nwith corresponding actionable low-level steps, via LLMs. We release not only\nour datasets but also the prompt templates used to generate them, to allow\nanyone to build datasets for their domain. We compare GPT3.5 and GPT4 with the\nfinetuned GPT2 for task domains, in tabletop and kitchen environments, and the\nresult shows that GPT2-medium is comparable to GPT3.5 for task planning in a\nspecific domain. Our dataset, code, and more output samples can be found in\nhttps://github.com/Gawon-Choi/small-LMs-Task-Planning",
      "tldr_zh": "本研究探讨了小型语言模型 (Small LMs) 在机器人任务规划中的潜力，质疑是否只有大型语言模型 (LLMs) 才能进行 Chain-of-Thought (CoT) 推理。通过构建 COmmand-STeps datasets (COST)，利用 LLMs 生成高层命令和对应的低层步骤，并对 GPT2 模型进行微调，以使其在单一领域实现链式推理。实验比较了 GPT3.5、GPT4 和微调后 GPT2-medium，在桌面和厨房环境的任务规划中，结果显示 GPT2-medium 的性能可与 GPT3.5 媲美。该工作发布了数据集、提示模板和代码，促进了领域特定任务规划的进一步发展。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "8 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2404.03891v1",
      "published_date": "2024-04-05 04:58:34 UTC",
      "updated_date": "2024-04-05 04:58:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:10:58.607886"
    },
    {
      "arxiv_id": "2404.03888v2",
      "title": "A proximal policy optimization based intelligent home solar management",
      "title_zh": "翻译失败",
      "authors": [
        "Kode Creer",
        "Imitiaz Parvez"
      ],
      "abstract": "In the smart grid, the prosumers can sell unused electricity back to the\npower grid, assuming the prosumers own renewable energy sources and storage\nunits. The maximizing of their profits under a dynamic electricity market is a\nproblem that requires intelligent planning. To address this, we propose a\nframework based on Proximal Policy Optimization (PPO) using recurrent rewards.\nBy using the information about the rewards modeled effectively with PPO to\nmaximize our objective, we were able to get over 30\\% improvement over the\nother naive algorithms in accumulating total profits. This shows promise in\ngetting reinforcement learning algorithms to perform tasks required to plan\ntheir actions in complex domains like financial markets. We also introduce a\nnovel method for embedding longs based on soliton waves that outperformed\nnormal embedding in our use case with random floating point data augmentation.",
      "tldr_zh": "这篇论文提出了一种基于 Proximal Policy Optimization (PPO) 的框架，用于智能家庭太阳能管理，帮助 prosumers 在动态电力市场中最大化利润，通过利用 recurrent rewards 进行有效决策。实验结果显示，该框架与朴素算法相比，总利润积累提高了超过30%，证明了其在复杂领域如金融市场的应用潜力。论文还引入了一种基于 soliton waves 的新嵌入方法，在随机浮点数据增强的场景中表现出色，进一步提升了系统的性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "This manuscript has been accepted for IEEE EIT Conference",
      "pdf_url": "http://arxiv.org/pdf/2404.03888v2",
      "published_date": "2024-04-05 04:34:43 UTC",
      "updated_date": "2024-05-09 03:51:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:11:07.869835"
    },
    {
      "arxiv_id": "2404.03887v4",
      "title": "SAAS: Solving Ability Amplification Strategy for Enhanced Mathematical Reasoning in Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Hyeonwoo Kim",
        "Gyoungjin Gim",
        "Yungi Kim",
        "Jihoo Kim",
        "Byungju Kim",
        "Wonseok Lee",
        "Chanjun Park"
      ],
      "abstract": "This study presents a novel learning approach designed to enhance both\nmathematical reasoning and problem-solving abilities of Large Language Models\n(LLMs). We focus on integrating the Chain-of-Thought (CoT) and the\nProgram-of-Thought (PoT) learning, hypothesizing that prioritizing the learning\nof mathematical reasoning ability is helpful for the amplification of\nproblem-solving ability. Thus, the initial learning with CoT is essential for\nsolving challenging mathematical problems. To this end, we propose a sequential\nlearning approach, named SAAS (Solving Ability Amplification Strategy), which\nstrategically transitions from CoT learning to PoT learning. Our empirical\nstudy, involving an extensive performance comparison using several benchmarks,\ndemonstrates that our SAAS achieves state-of-the-art (SOTA) performance. The\nresults underscore the effectiveness of our sequential learning approach,\nmarking a significant advancement in the field of mathematical reasoning in\nLLMs.",
      "tldr_zh": "本研究提出了一种名为 SAAS (Solving Ability Amplification Strategy) 的顺序学习方法，以提升大型语言模型 (LLMs) 的数学推理和问题解决能力。SAAS 假设优先通过 Chain-of-Thought (CoT) 学习来增强数学推理基础，然后过渡到 Program-of-Thought (PoT) 学习，从而放大整体问题解决能力。实验结果显示，在多个基准测试中，SAAS 达到了 state-of-the-art (SOTA) 性能，标志着 LLMs 数学推理领域的显著进步。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to EMNLP 2024 Industry Track",
      "pdf_url": "http://arxiv.org/pdf/2404.03887v4",
      "published_date": "2024-04-05 04:25:47 UTC",
      "updated_date": "2024-10-02 11:56:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:11:19.234149"
    },
    {
      "arxiv_id": "2404.03869v2",
      "title": "Heterogeneous Multi-Agent Reinforcement Learning for Zero-Shot Scalable Collaboration",
      "title_zh": "翻译失败",
      "authors": [
        "Xudong Guo",
        "Daming Shi",
        "Junjie Yu",
        "Wenhui Fan"
      ],
      "abstract": "The emergence of multi-agent reinforcement learning (MARL) is significantly\ntransforming various fields like autonomous vehicle networks. However,\nreal-world multi-agent systems typically contain multiple roles, and the scale\nof these systems dynamically fluctuates. Consequently, in order to achieve\nzero-shot scalable collaboration, it is essential that strategies for different\nroles can be updated flexibly according to the scales, which is still a\nchallenge for current MARL frameworks. To address this, we propose a novel MARL\nframework named Scalable and Heterogeneous Proximal Policy Optimization\n(SHPPO), integrating heterogeneity into parameter-shared PPO-based MARL\nnetworks. We first leverage a latent network to learn strategy patterns for\neach agent adaptively. Second, we introduce a heterogeneous layer to be\ninserted into decision-making networks, whose parameters are specifically\ngenerated by the learned latent variables. Our approach is scalable as all the\nparameters are shared except for the heterogeneous layer, and gains both\ninter-individual and temporal heterogeneity, allowing SHPPO to adapt\neffectively to varying scales. SHPPO exhibits superior performance in classic\nMARL environments like Starcraft Multi-Agent Challenge (SMAC) and Google\nResearch Football (GRF), showcasing enhanced zero-shot scalability, and\noffering insights into the learned latent variables' impact on team performance\nby visualization.",
      "tldr_zh": "本研究针对多智能体强化学习(MARL)中多角色系统和动态规模变化的挑战，提出了一种新型框架Scalable and Heterogeneous Proximal Policy Optimization (SHPPO)，以实现零样本可扩展协作。SHPPO将异质性融入基于PPO的MARL网络中，通过潜在网络(latent network)适应性地学习每个智能体的策略模式，并引入异质层(heterogeneous layer)来生成特定参数，从而实现参数共享下的个体间和时间异质性。实验结果显示，SHPPO在Starcraft Multi-Agent Challenge (SMAC)和Google Research Football (GRF)环境中表现出色，提升了零样本可扩展性，并通过可视化分析揭示了潜在变量对团队性能的影响。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA",
        "cs.RO",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.03869v2",
      "published_date": "2024-04-05 03:02:57 UTC",
      "updated_date": "2024-10-02 14:52:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:11:31.510736"
    },
    {
      "arxiv_id": "2404.03868v2",
      "title": "Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction",
      "title_zh": "提取、定义、规范化：基于LLM的知识图谱构建框架",
      "authors": [
        "Bowen Zhang",
        "Harold Soh"
      ],
      "abstract": "In this work, we are interested in automated methods for knowledge graph\ncreation (KGC) from input text. Progress on large language models (LLMs) has\nprompted a series of recent works applying them to KGC, e.g., via zero/few-shot\nprompting. Despite successes on small domain-specific datasets, these models\nface difficulties scaling up to text common in many real-world applications. A\nprincipal issue is that, in prior methods, the KG schema has to be included in\nthe LLM prompt to generate valid triplets; larger and more complex schemas\neasily exceed the LLMs' context window length. Furthermore, there are scenarios\nwhere a fixed pre-defined schema is not available and we would like the method\nto construct a high-quality KG with a succinct self-generated schema. To\naddress these problems, we propose a three-phase framework named\nExtract-Define-Canonicalize (EDC): open information extraction followed by\nschema definition and post-hoc canonicalization. EDC is flexible in that it can\nbe applied to settings where a pre-defined target schema is available and when\nit is not; in the latter case, it constructs a schema automatically and applies\nself-canonicalization. To further improve performance, we introduce a trained\ncomponent that retrieves schema elements relevant to the input text; this\nimproves the LLMs' extraction performance in a retrieval-augmented\ngeneration-like manner. We demonstrate on three KGC benchmarks that EDC is able\nto extract high-quality triplets without any parameter tuning and with\nsignificantly larger schemas compared to prior works. Code for EDC is available\nat https://github.com/clear-nus/edc.",
      "tldr_zh": "本研究提出了一种基于大型语言模型(LLM)的框架Extract-Define-Canonicalize (EDC)，用于从输入文本中自动构建知识图谱(KGC)，以解决现有方法在处理大型schema时超出上下文窗口以及缺乏预定义schema的问题。框架分为三个阶段：首先进行开放信息提取(Extract)，然后定义schema(Define)，最后进行后处理规范(Canonicalize)，使其适用于有或无预定义schema的场景。EDC 还引入了一个训练组件来检索与输入文本相关的schema元素，从而提升提取性能，类似于检索增强生成技术。实验在三个KGC基准上证明，该框架无需参数调整即可生成高质量triplets，并支持比先前工作更大的schema。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "18 pages, 3 figures, Proceedings of the 2024 Conference on Empirical\n  Methods in Natural Language Processing",
      "pdf_url": "http://arxiv.org/pdf/2404.03868v2",
      "published_date": "2024-04-05 02:53:51 UTC",
      "updated_date": "2024-10-02 05:51:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:11:42.526585"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 67,
  "processed_papers_count": 67,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-17T22:12:04.293433"
}
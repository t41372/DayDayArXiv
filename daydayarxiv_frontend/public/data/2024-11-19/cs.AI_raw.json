[
  {
    "arxiv_id": "2411.12925v1",
    "title": "Loss-to-Loss Prediction: Scaling Laws for All Datasets",
    "authors": [
      "David Brandfonbrener",
      "Nikhil Anand",
      "Nikhil Vyas",
      "Eran Malach",
      "Sham Kakade"
    ],
    "abstract": "While scaling laws provide a reliable methodology for predicting train loss\nacross compute scales for a single data distribution, less is known about how\nthese predictions should change as we change the distribution. In this paper,\nwe derive a strategy for predicting one loss from another and apply it to\npredict across different pre-training datasets and from pre-training data to\ndownstream task data. Our predictions extrapolate well even at 20x the largest\nFLOP budget used to fit the curves. More precisely, we find that there are\nsimple shifted power law relationships between (1) the train losses of two\nmodels trained on two separate datasets when the models are paired by training\ncompute (train-to-train), (2) the train loss and the test loss on any\ndownstream distribution for a single model (train-to-test), and (3) the test\nlosses of two models trained on two separate train datasets (test-to-test). The\nresults hold up for pre-training datasets that differ substantially (some are\nentirely code and others have no code at all) and across a variety of\ndownstream tasks. Finally, we find that in some settings these shifted power\nlaw relationships can yield more accurate predictions than extrapolating\nsingle-dataset scaling laws.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12925v1",
    "published_date": "2024-11-19 23:23:16 UTC",
    "updated_date": "2024-11-19 23:23:16 UTC"
  },
  {
    "arxiv_id": "2411.12924v2",
    "title": "Human-In-the-Loop Software Development Agents",
    "authors": [
      "Wannita Takerngsaksiri",
      "Jirat Pasuksmit",
      "Patanamon Thongtanunam",
      "Chakkrit Tantithamthavorn",
      "Ruixiong Zhang",
      "Fan Jiang",
      "Jing Li",
      "Evan Cook",
      "Kun Chen",
      "Ming Wu"
    ],
    "abstract": "Recently, Large Language Models (LLMs)-based multi-agent paradigms for\nsoftware engineering are introduced to automatically resolve software\ndevelopment tasks (e.g., from a given issue to source code). However, existing\nwork is evaluated based on historical benchmark datasets, rarely considers\nhuman feedback at each stage of the automated software development process, and\nhas not been deployed in practice. In this paper, we introduce a\nHuman-in-the-loop LLM-based Agents framework (HULA) for software development\nthat allows software engineers to refine and guide LLMs when generating coding\nplans and source code for a given task. We design, implement, and deploy the\nHULA framework into Atlassian JIRA for internal uses. Through a multi-stage\nevaluation of the HULA framework, Atlassian software engineers perceive that\nHULA can minimize the overall development time and effort, especially in\ninitiating a coding plan and writing code for straightforward tasks. On the\nother hand, challenges around code quality remain a concern in some cases. We\ndraw lessons learned and discuss opportunities for future work, which will pave\nthe way for the advancement of LLM-based agents in software development.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "10 pages, 9 figures, ICSE SEIP 2025",
    "pdf_url": "http://arxiv.org/pdf/2411.12924v2",
    "published_date": "2024-11-19 23:22:33 UTC",
    "updated_date": "2025-01-10 03:55:57 UTC"
  },
  {
    "arxiv_id": "2411.12921v2",
    "title": "A Comparative Study of Text Retrieval Models on DaReCzech",
    "authors": [
      "Jakub Stetina",
      "Martin Fajcik",
      "Michal Stefanik",
      "Michal Hradis"
    ],
    "abstract": "This article presents a comprehensive evaluation of 7 off-the-shelf document\nretrieval models: Splade, Plaid, Plaid-X, SimCSE, Contriever, OpenAI ADA and\nGemma2 chosen to determine their performance on the Czech retrieval dataset\nDaReCzech. The primary objective of our experiments is to estimate the quality\nof modern retrieval approaches in the Czech language. Our analyses include\nretrieval quality, speed, and memory footprint. Secondly, we analyze whether it\nis better to use the model directly in Czech text, or to use machine\ntranslation into English, followed by retrieval in English. Our experiments\nidentify the most effective option for Czech information retrieval. The\nfindings revealed notable performance differences among the models, with\nGemma22 achieving the highest precision and recall, while Contriever performing\npoorly. Conclusively, SPLADE and PLAID models offered a balance of efficiency\nand performance.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12921v2",
    "published_date": "2024-11-19 23:19:46 UTC",
    "updated_date": "2024-12-20 23:02:35 UTC"
  },
  {
    "arxiv_id": "2411.12919v3",
    "title": "Robust multi-coil MRI reconstruction via self-supervised denoising",
    "authors": [
      "Asad Aali",
      "Marius Arvinte",
      "Sidharth Kumar",
      "Yamin I. Arefeen",
      "Jonathan I. Tamir"
    ],
    "abstract": "We study the effect of incorporating self-supervised denoising as a\npre-processing step for training deep learning (DL) based reconstruction\nmethods on data corrupted by Gaussian noise. K-space data employed for training\nare typically multi-coil and inherently noisy. Although DL-based reconstruction\nmethods trained on fully sampled data can enable high reconstruction quality,\nobtaining large, noise-free datasets is impractical. We leverage Generalized\nStein's Unbiased Risk Estimate (GSURE) for denoising. We evaluate two DL-based\nreconstruction methods: Diffusion Probabilistic Models (DPMs) and Model-Based\nDeep Learning (MoDL). We evaluate the impact of denoising on the performance of\nthese DL-based methods in solving accelerated multi-coil magnetic resonance\nimaging (MRI) reconstruction. The experiments were carried out on T2-weighted\nbrain and fat-suppressed proton-density knee scans. We observed that\nself-supervised denoising enhances the quality and efficiency of MRI\nreconstructions across various scenarios. Specifically, employing denoised\nimages rather than noisy counterparts when training DL networks results in\nlower normalized root mean squared error (NRMSE), higher structural similarity\nindex measure (SSIM) and peak signal-to-noise ratio (PSNR) across different SNR\nlevels, including 32dB, 22dB, and 12dB for T2-weighted brain data, and 24dB,\n14dB, and 4dB for fat-suppressed knee data. Overall, we showed that denoising\nis an essential pre-processing technique capable of improving the efficacy of\nDL-based MRI reconstruction methods under diverse conditions. By refining the\nquality of input data, denoising enables training more effective DL networks,\npotentially bypassing the need for noise-free reference MRI scans.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12919v3",
    "published_date": "2024-11-19 23:17:09 UTC",
    "updated_date": "2025-04-23 02:07:52 UTC"
  },
  {
    "arxiv_id": "2411.12913v1",
    "title": "MLDGG: Meta-Learning for Domain Generalization on Graphs",
    "authors": [
      "Qin Tian",
      "Chen Zhao",
      "Minglai Shao",
      "Wenjun Wang",
      "Yujie Lin",
      "Dong Li"
    ],
    "abstract": "Domain generalization on graphs aims to develop models with robust\ngeneralization capabilities, ensuring effective performance on the testing set\ndespite disparities between testing and training distributions. However,\nexisting methods often rely on static encoders directly applied to the target\ndomain, constraining its flexible adaptability. In contrast to conventional\nmethodologies, which concentrate on developing specific generalized models, our\nframework, MLDGG, endeavors to achieve adaptable generalization across diverse\ndomains by integrating cross-multi-domain meta-learning with structure learning\nand semantic identification. Initially, it introduces a generalized structure\nlearner to mitigate the adverse effects of task-unrelated edges, enhancing the\ncomprehensiveness of representations learned by Graph Neural Networks (GNNs)\nwhile capturing shared structural information across domains. Subsequently, a\nrepresentation learner is designed to disentangle domain-invariant semantic and\ndomain-specific variation information in node embedding by leveraging causal\nreasoning for semantic identification, further enhancing generalization. In the\ncontext of meta-learning, meta-parameters for both learners are optimized to\nfacilitate knowledge transfer and enable effective adaptation to graphs through\nfine-tuning within the target domains, where target graphs are inaccessible\nduring training. Our empirical results demonstrate that MLDGG surpasses\nbaseline methods, showcasing its effectiveness in three different distribution\nshift settings.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted in KDD 2025 (research track)",
    "pdf_url": "http://arxiv.org/pdf/2411.12913v1",
    "published_date": "2024-11-19 22:57:38 UTC",
    "updated_date": "2024-11-19 22:57:38 UTC"
  },
  {
    "arxiv_id": "2411.15200v1",
    "title": "Deep Learning-Based Classification of Hyperkinetic Movement Disorders in Children",
    "authors": [
      "Nandika Ramamurthy",
      "Dr Daniel Lumsden",
      "Dr Rachel Sparks"
    ],
    "abstract": "Hyperkinetic movement disorders (HMDs) in children, including dystonia\n(abnormal twisting) and chorea (irregular, random movements), pose significant\ndiagnostic challenges due to overlapping clinical features. The prevalence of\ndystonia ranges from 2 to 50 per million, and chorea from 5 to 10 per 100,000.\nThese conditions are often diagnosed with delays averaging 4.75 to 7.83 years.\nTraditional diagnostic methods depend on clinical history and expert physical\nexaminations, but specialized tests are ineffective due to the complex\npathophysiology of these disorders. This study develops a neural network model\nto differentiate between dystonia and chorea from video recordings of\npaediatric patients performing motor tasks. The model integrates a Graph\nConvolutional Network (GCN) to capture spatial relationships and Long\nShort-Term Memory (LSTM) networks to account for temporal dynamics. Attention\nmechanisms were incorporated to improve model interpretability. The model was\ntrained and validated on a dataset of 50 videos (31 chorea-predominant, 19\ndystonia-predominant) collected under regulatory approval from Guy's and St\nThomas' NHS Foundation Trust. The model achieved 85% accuracy, 81% sensitivity,\nand 88% specificity at 15 frames per second. Attention maps highlighted the\nmodel's ability to correctly identify involuntary movement patterns, with\nmisclassifications often due to occluded body parts or subtle movement\nvariations. This work demonstrates the potential of deep learning to improve\nthe accuracy and efficiency of HMD diagnosis and could contribute to more\nreliable, interpretable clinical tools.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "59 pages, 20 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.15200v1",
    "published_date": "2024-11-19 22:02:04 UTC",
    "updated_date": "2024-11-19 22:02:04 UTC"
  },
  {
    "arxiv_id": "2411.12880v1",
    "title": "Advancing Large Language Models for Spatiotemporal and Semantic Association Mining of Similar Environmental Events",
    "authors": [
      "Yuanyuan Tian",
      "Wenwen Li",
      "Lei Hu",
      "Xiao Chen",
      "Michael Brook",
      "Michael Brubaker",
      "Fan Zhang",
      "Anna K. Liljedahl"
    ],
    "abstract": "Retrieval and recommendation are two essential tasks in modern search tools.\nThis paper introduces a novel retrieval-reranking framework leveraging Large\nLanguage Models (LLMs) to enhance the spatiotemporal and semantic associated\nmining and recommendation of relevant unusual climate and environmental events\ndescribed in news articles and web posts. This framework uses advanced natural\nlanguage processing techniques to address the limitations of traditional manual\ncuration methods in terms of high labor cost and lack of scalability.\nSpecifically, we explore an optimized solution to employ cutting-edge embedding\nmodels for semantically analyzing spatiotemporal events (news) and propose a\nGeo-Time Re-ranking (GT-R) strategy that integrates multi-faceted criteria\nincluding spatial proximity, temporal association, semantic similarity, and\ncategory-instructed similarity to rank and identify similar spatiotemporal\nevents. We apply the proposed framework to a dataset of four thousand Local\nEnvironmental Observer (LEO) Network events, achieving top performance in\nrecommending similar events among multiple cutting-edge dense retrieval models.\nThe search and recommendation pipeline can be applied to a wide range of\nsimilar data search tasks dealing with geospatial and temporal data. We hope\nthat by linking relevant events, we can better aid the general public to gain\nan enhanced understanding of climate change and its impact on different\ncommunities.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12880v1",
    "published_date": "2024-11-19 21:57:22 UTC",
    "updated_date": "2024-11-19 21:57:22 UTC"
  },
  {
    "arxiv_id": "2412.04478v1",
    "title": "LibEvolutionEval: A Benchmark and Study for Version-Specific Code Generation",
    "authors": [
      "Sachit Kuhar",
      "Wasi Uddin Ahmad",
      "Zijian Wang",
      "Nihal Jain",
      "Haifeng Qian",
      "Baishakhi Ray",
      "Murali Krishna Ramanathan",
      "Xiaofei Ma",
      "Anoop Deoras"
    ],
    "abstract": "Recent advancements in code completion models have primarily focused on local\nfile contexts. However, these studies do not fully capture the complexity of\nreal-world software development, which often requires the use of\nrapidly-evolving public libraries. To fill the gap, we introduce\nLibEvolutionEval, a detailed study requiring an understanding of library\nevolution to perform in-line code completion accurately. LibEvolutionEval\nprovides a version-specific code-completion task comprised of eight libraries\n(torch, torchvision, scipy, pil, tqdm, pyyaml, matplotlib, and pandas) as they\nevolve over the year along with a detailed analysis of the evolution of two\npopular and well-maintained public libraries: PyTorch and Matplotlib. We\nevaluate popular public models and find that public library evolution\nsignificantly influences model performance. We explored mitigation methods by\nstudying how retrieved version-specific library documentation and prompting can\nimprove the model's capability in handling these fast-evolving packages, paving\na promising future path in better handling fast-evolving libraries.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04478v1",
    "published_date": "2024-11-19 21:52:23 UTC",
    "updated_date": "2024-11-19 21:52:23 UTC"
  },
  {
    "arxiv_id": "2411.12877v4",
    "title": "The Illusion of Empathy: How AI Chatbots Shape Conversation Perception",
    "authors": [
      "Tingting Liu",
      "Salvatore Giorgi",
      "Ankit Aich",
      "Allison Lahnala",
      "Brenda Curtis",
      "Lyle Ungar",
      "João Sedoc"
    ],
    "abstract": "As AI chatbots increasingly incorporate empathy, understanding user-centered\nperceptions of chatbot empathy and its impact on conversation quality remains\nessential yet under-explored. This study examines how chatbot identity and\nperceived empathy influence users' overall conversation experience. Analyzing\n155 conversations from two datasets, we found that while GPT-based chatbots\nwere rated significantly higher in conversational quality, they were\nconsistently perceived as less empathetic than human conversational partners.\nEmpathy ratings from GPT-4o annotations aligned with user ratings, reinforcing\nthe perception of lower empathy in chatbots compared to humans. Our findings\nunderscore the critical role of perceived empathy in shaping conversation\nquality, revealing that achieving high-quality human-AI interactions requires\nmore than simply embedding empathetic language; it necessitates addressing the\nnuanced ways users interpret and experience empathy in conversations with\nchatbots.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12877v4",
    "published_date": "2024-11-19 21:47:08 UTC",
    "updated_date": "2025-03-06 20:06:51 UTC"
  },
  {
    "arxiv_id": "2411.12876v1",
    "title": "Puppet-CNN: Input-Adaptive Convolutional Neural Networks with Model Compression using Ordinary Differential Equation",
    "authors": [
      "Yucheng Xing",
      "Xin Wang"
    ],
    "abstract": "Convolutional Neural Network (CNN) has been applied to more and more\nscenarios due to its excellent performance in many machine learning tasks,\nespecially with deep and complex structures. However, as the network goes\ndeeper, more parameters need to be stored and optimized. Besides, almost all\ncommon CNN models adopt \"train-and-use\" strategy where the structure is\npre-defined and the kernel parameters are fixed after the training with the\nsame structure and set of parameters used for all data without considering the\ncontent complexity. In this paper, we propose a new CNN framework, named as\n$\\textit{Puppet-CNN}$, which contains two modules: a $\\textit{puppet module}$\nand a $\\textit{puppeteer module}$. The puppet module is a CNN model used to\nactually process the input data just like other works, but its depth and\nkernels are generated by the puppeteer module (realized with Ordinary\nDifferential Equation (ODE)) based on the input complexity each time. By\nrecurrently generating kernel parameters in the puppet module, we can take\nadvantage of the dependence among kernels of different convolutional layers to\nsignificantly reduce the size of CNN model by only storing and training the\nparameters of the much smaller puppeteer ODE module. Through experiments on\nseveral datasets, our method has proven to be superior than the traditional\nCNNs on both performance and efficiency. The model size can be reduced more\nthan 10 times.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12876v1",
    "published_date": "2024-11-19 21:44:21 UTC",
    "updated_date": "2024-11-19 21:44:21 UTC"
  },
  {
    "arxiv_id": "2411.12872v2",
    "title": "From Text to Pose to Image: Improving Diffusion Model Control and Quality",
    "authors": [
      "Clément Bonnet",
      "Ariel N. Lee",
      "Franck Wertel",
      "Antoine Tamano",
      "Tanguy Cizain",
      "Pablo Ducru"
    ],
    "abstract": "In the last two years, text-to-image diffusion models have become extremely\npopular. As their quality and usage increase, a major concern has been the need\nfor better output control. In addition to prompt engineering, one effective\nmethod to improve the controllability of diffusion models has been to condition\nthem on additional modalities such as image style, depth map, or keypoints.\nThis forms the basis of ControlNets or Adapters. When attempting to apply these\nmethods to control human poses in outputs of text-to-image diffusion models,\ntwo main challenges have arisen. The first challenge is generating poses\nfollowing a wide range of semantic text descriptions, for which previous\nmethods involved searching for a pose within a dataset of (caption, pose)\npairs. The second challenge is conditioning image generation on a specified\npose while keeping both high aesthetic and high pose fidelity. In this article,\nwe fix these two main issues by introducing a text-to-pose (T2P) generative\nmodel alongside a new sampling algorithm, and a new pose adapter that\nincorporates more pose keypoints for higher pose fidelity. Together, these two\nnew state-of-the-art models enable, for the first time, a generative\ntext-to-pose-to-image framework for higher pose control in diffusion models. We\nrelease all models and the code used for the experiments at\nhttps://github.com/clement-bonnet/text-to-pose.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Published at the NeurIPS 2024 Workshop on Compositional Learning:\n  Perspectives, Methods, and Paths Forward",
    "pdf_url": "http://arxiv.org/pdf/2411.12872v2",
    "published_date": "2024-11-19 21:34:50 UTC",
    "updated_date": "2024-11-22 10:26:27 UTC"
  },
  {
    "arxiv_id": "2411.15199v1",
    "title": "Adaptively Controllable Diffusion Model for Efficient Conditional Image Generation",
    "authors": [
      "Yucheng Xing",
      "Xiaodong Liu",
      "Xin Wang"
    ],
    "abstract": "With the development of artificial intelligence, more and more attention has\nbeen put onto generative models, which represent the creativity, a very\nimportant aspect of intelligence. In recent years, diffusion models have been\nstudied and proven to be more reasonable and effective than previous methods.\nHowever, common diffusion frameworks suffer from controllability problems.\nAlthough extra conditions have been considered by some work to guide the\ndiffusion process for a specific target generation, it only controls the\ngeneration result but not its process. In this work, we propose a new adaptive\nframework, $\\textit{Adaptively Controllable Diffusion (AC-Diff) Model}$, to\nautomatically and fully control the generation process, including not only the\ntype of generation result but also the length and parameters of the generation\nprocess. Both inputs and conditions will be first fed into a\n$\\textit{Conditional Time-Step (CTS) Module}$ to determine the number of steps\nneeded for a generation. Then according to the length of the process, the\ndiffusion rate parameters will be estimated through our $\\textit{Adaptive\nHybrid Noise Schedule (AHNS) Module}$. We further train the network with the\ncorresponding adaptive sampling mechanism to learn how to adjust itself\naccording to the conditions for the overall performance improvement. To enable\nits practical applications, AC-Diff is expected to largely reduce the average\nnumber of generation steps and execution time while maintaining the same\nperformance as done in the literature diffusion models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15199v1",
    "published_date": "2024-11-19 21:26:30 UTC",
    "updated_date": "2024-11-19 21:26:30 UTC"
  },
  {
    "arxiv_id": "2411.12859v1",
    "title": "The Game-Theoretic Symbiosis of Trust and AI in Networked Systems",
    "authors": [
      "Yunfei Ge",
      "Quanyan Zhu"
    ],
    "abstract": "This chapter explores the symbiotic relationship between Artificial\nIntelligence (AI) and trust in networked systems, focusing on how these two\nelements reinforce each other in strategic cybersecurity contexts. AI's\ncapabilities in data processing, learning, and real-time response offer\nunprecedented support for managing trust in dynamic, complex networks. However,\nthe successful integration of AI also hinges on the trustworthiness of AI\nsystems themselves. Using a game-theoretic framework, this chapter presents\napproaches to trust evaluation, the strategic role of AI in cybersecurity, and\ngovernance frameworks that ensure responsible AI deployment. We investigate how\ntrust, when dynamically managed through AI, can form a resilient security\necosystem. By examining trust as both an AI output and an AI requirement, this\nchapter sets the foundation for a positive feedback loop where AI enhances\nnetwork security and the trust placed in AI systems fosters their adoption.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12859v1",
    "published_date": "2024-11-19 21:04:53 UTC",
    "updated_date": "2024-11-19 21:04:53 UTC"
  },
  {
    "arxiv_id": "2411.13608v2",
    "title": "Integrating Dynamic Correlation Shifts and Weighted Benchmarking in Extreme Value Analysis",
    "authors": [
      "Dimitrios P. Panagoulias",
      "Elissaios Sarmas",
      "Vangelis Marinakis",
      "Maria Virvou",
      "George A. Tsihrintzis"
    ],
    "abstract": "This paper presents an innovative approach to Extreme Value Analysis (EVA) by\nintroducing the Extreme Value Dynamic Benchmarking Method (EVDBM). EVDBM\nintegrates extreme value theory to detect extreme events and is coupled with\nthe novel Dynamic Identification of Significant Correlation (DISC)-Thresholding\nalgorithm, which enhances the analysis of key variables under extreme\nconditions. By integrating return values predicted through EVA into the\nbenchmarking scores, we are able to transform these scores to reflect\nanticipated conditions more accurately. This provides a more precise picture of\nhow each case is projected to unfold under extreme conditions. As a result, the\nadjusted scores offer a forward-looking perspective, highlighting potential\nvulnerabilities and resilience factors for each case in a way that static\nhistorical data alone cannot capture. By incorporating both historical and\nprobabilistic elements, the EVDBM algorithm provides a comprehensive\nbenchmarking framework that is adaptable to a range of scenarios and contexts.\nThe methodology is applied to real PV data, revealing critical low - production\nscenarios and significant correlations between variables, which aid in risk\nmanagement, infrastructure design, and long-term planning, while also allowing\nfor the comparison of different production plants. The flexibility of EVDBM\nsuggests its potential for broader applications in other sectors where\ndecision-making sensitivity is crucial, offering valuable insights to improve\noutcomes.",
    "categories": [
      "stat.AP",
      "cs.AI"
    ],
    "primary_category": "stat.AP",
    "comment": "33 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.13608v2",
    "published_date": "2024-11-19 21:00:39 UTC",
    "updated_date": "2024-11-25 12:21:17 UTC"
  },
  {
    "arxiv_id": "2411.12847v1",
    "title": "mDAE : modified Denoising AutoEncoder for missing data imputation",
    "authors": [
      "Mariette Dupuy",
      "Marie Chavent",
      "Remi Dubois"
    ],
    "abstract": "This paper introduces a methodology based on Denoising AutoEncoder (DAE) for\nmissing data imputation. The proposed methodology, called mDAE hereafter,\nresults from a modification of the loss function and a straightforward\nprocedure for choosing the hyper-parameters. An ablation study shows on several\nUCI Machine Learning Repository datasets, the benefit of using this modified\nloss function and an overcomplete structure, in terms of Root Mean Squared\nError (RMSE) of reconstruction. This numerical study is completed by comparing\nthe mDAE methodology with eight other methods (four standard and four more\nrecent). A criterion called Mean Distance to Best (MDB) is proposed to measure\nhow a method performs globally well on all datasets. This criterion is defined\nas the mean (over the datasets) of the distances between the RMSE of the\nconsidered method and the RMSE of the best method. According to this criterion,\nthe mDAE methodology was consistently ranked among the top methods (along with\nSoftImput and missForest), while the four more recent methods were\nsystematically ranked last. The Python code of the numerical study will be\navailable on GitHub so that results can be reproduced or generalized with other\ndatasets and methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12847v1",
    "published_date": "2024-11-19 20:31:53 UTC",
    "updated_date": "2024-11-19 20:31:53 UTC"
  },
  {
    "arxiv_id": "2411.12843v1",
    "title": "Reward Modeling with Ordinal Feedback: Wisdom of the Crowd",
    "authors": [
      "Shang Liu",
      "Yu Pan",
      "Guanting Chen",
      "Xiaocheng Li"
    ],
    "abstract": "Learning a reward model (RM) from human preferences has been an important\ncomponent in aligning large language models (LLMs). The canonical setup of\nlearning RMs from pairwise preference data is rooted in the classic\nBradley-Terry (BT) model that accepts binary feedback, i.e., the label being\neither Response 1 is better than Response 2, or the opposite. Such a setup\ninevitably discards potentially useful samples (such as \"tied\" between the two\nresponses) and loses more fine-grained information (such as \"slightly better\").\nIn this paper, we propose a framework for learning RMs under ordinal feedback\nwhich generalizes the case of binary preference feedback to any arbitrary\ngranularity. Specifically, we first identify a marginal unbiasedness condition,\nwhich generalizes the assumption of the BT model in the existing binary\nfeedback setting. The condition validates itself via the sociological concept\nof the wisdom of the crowd. Under the condition, we develop a natural\nprobability model for pairwise preference data under ordinal feedback and\nanalyze its properties. We prove the statistical benefits of ordinal feedback\nin terms of reducing the Rademacher complexity compared to the case of binary\nfeedback. The proposed learning objective and the theory also extend to hinge\nloss and direct policy optimization (DPO). In particular, the theoretical\nanalysis may be of independent interest when applying to a seemingly unrelated\nproblem of knowledge distillation to interpret the bias-variance trade-off\ntherein. The framework also sheds light on writing guidance for human\nannotators. Our numerical experiments validate that fine-grained feedback leads\nto better reward learning for both in-distribution and out-of-distribution\nsettings. Further experiments show that incorporating a certain proportion of\nsamples with tied preference boosts RM learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12843v1",
    "published_date": "2024-11-19 20:17:04 UTC",
    "updated_date": "2024-11-19 20:17:04 UTC"
  },
  {
    "arxiv_id": "2411.14483v2",
    "title": "Ranking Unraveled: Recipes for LLM Rankings in Head-to-Head AI Combat",
    "authors": [
      "Roland Daynauth",
      "Christopher Clarke",
      "Krisztian Flautner",
      "Lingjia Tang",
      "Jason Mars"
    ],
    "abstract": "Deciding which large language model (LLM) to use is a complex challenge.\nPairwise ranking has emerged as a new method for evaluating human preferences\nfor LLMs. This approach entails humans evaluating pairs of model outputs based\non a predefined criterion. By collecting these comparisons, a ranking can be\nconstructed using methods such as Elo. However, applying these algorithms as\nconstructed in the context of LLM evaluation introduces several challenges. In\nthis paper, we explore the effectiveness of ranking systems for head-to-head\ncomparisons of LLMs. We formally define a set of fundamental principles for\neffective ranking and conduct a series of extensive evaluations on the\nrobustness of several ranking algorithms in the context of LLMs. Our analysis\nuncovers key insights into the factors that affect ranking accuracy and\nefficiency, offering guidelines for selecting the most appropriate methods\nbased on specific evaluation contexts and resource constraints.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.14483v2",
    "published_date": "2024-11-19 20:16:26 UTC",
    "updated_date": "2025-02-17 16:21:10 UTC"
  },
  {
    "arxiv_id": "2411.12833v1",
    "title": "Efficient Medicinal Image Transmission and Resolution Enhancement via GAN",
    "authors": [
      "Rishabh Kumar Sharma",
      "Mukund Sharma",
      "Pushkar Sharma",
      "Jeetashree Aparjeeta"
    ],
    "abstract": "While X-ray imaging is indispensable in medical diagnostics, it inherently\ncarries with it those noises and limitations on resolution that mask the\ndetails necessary for diagnosis. B/W X-ray images require a careful balance\nbetween noise suppression and high-detail preservation to ensure clarity in\nsoft-tissue structures and bone edges. While traditional methods, such as CNNs\nand early super-resolution models like ESRGAN, have enhanced image resolution,\nthey often perform poorly regarding high-frequency detail preservation and\nnoise control for B/W imaging. We are going to present one efficient approach\nthat improves the quality of an image with the optimization of network\ntransmission in the following paper. The pre-processing of X-ray images into\nlow-resolution files by Real-ESRGAN, a version of ESRGAN elucidated and\nimproved, helps reduce the server load and transmission bandwidth.\nLower-resolution images are upscaled at the receiving end using Real-ESRGAN,\nfine-tuned for real-world image degradation. The model integrates\nResidual-in-Residual Dense Blocks with perceptual and adversarial loss\nfunctions for high-quality upscaled images with low noise. We further fine-tune\nReal-ESRGAN by adapting it to the specific B/W noise and contrast\ncharacteristics. This suppresses noise artifacts without compromising detail.\nThe comparative evaluation conducted shows that our approach achieves superior\nnoise reduction and detail clarity compared to state-of-the-art CNN-based and\nESRGAN models, apart from reducing network bandwidth requirements. These\nbenefits are confirmed both by quantitative metrics, including Peak\nSignal-to-Noise Ratio and Structural Similarity Index, and by qualitative\nassessments, which indicate the potential of Real-ESRGAN for diagnostic-quality\nX-ray imaging and for efficient medical data transmission.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12833v1",
    "published_date": "2024-11-19 19:39:42 UTC",
    "updated_date": "2024-11-19 19:39:42 UTC"
  },
  {
    "arxiv_id": "2411.12828v1",
    "title": "Probing the Capacity of Language Model Agents to Operationalize Disparate Experiential Context Despite Distraction",
    "authors": [
      "Sonny George",
      "Chris Sypherd",
      "Dylan Cashman"
    ],
    "abstract": "Large language model (LLM) agents show promise in an increasing number of\ndomains. In many proposed applications, it is expected that the agent reasons\nover accumulated experience presented in an input prompt. We propose the OEDD\n(Operationalize Experience Despite Distraction) corpus, a\nhuman-annotator-validated body of scenarios with pre-scripted agent histories\nwhere the agent must make a decision based on disparate experiential\ninformation in the presence of a distractor. We evaluate three state-of-the-art\nLLMs (GPT-3.5 Turbo, GPT-4o, and Gemini 1.5 Pro) using a minimal\nchain-of-thought prompting strategy and observe that when (1) the input context\ncontains over 1,615 tokens of historical interactions, (2) a crucially\ndecision-informing premise is the rightful conclusion over two disparate\nenvironment premises, and (3) a trivial, but distracting red herring fact\nfollows, all LLMs perform worse than random choice at selecting the better of\ntwo actions. Our code and test corpus are publicly available at:\nhttps://github.com/sonnygeorge/OEDD .",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12828v1",
    "published_date": "2024-11-19 19:33:16 UTC",
    "updated_date": "2024-11-19 19:33:16 UTC"
  },
  {
    "arxiv_id": "2411.12820v1",
    "title": "Declare and Justify: Explicit assumptions in AI evaluations are necessary for effective regulation",
    "authors": [
      "Peter Barnett",
      "Lisa Thiergart"
    ],
    "abstract": "As AI systems advance, AI evaluations are becoming an important pillar of\nregulations for ensuring safety. We argue that such regulation should require\ndevelopers to explicitly identify and justify key underlying assumptions about\nevaluations as part of their case for safety. We identify core assumptions in\nAI evaluations (both for evaluating existing models and forecasting future\nmodels), such as comprehensive threat modeling, proxy task validity, and\nadequate capability elicitation. Many of these assumptions cannot currently be\nwell justified. If regulation is to be based on evaluations, it should require\nthat AI development be halted if evaluations demonstrate unacceptable danger or\nif these assumptions are inadequately justified. Our presented approach aims to\nenhance transparency in AI development, offering a practical path towards more\neffective governance of advanced AI systems.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12820v1",
    "published_date": "2024-11-19 19:13:56 UTC",
    "updated_date": "2024-11-19 19:13:56 UTC"
  },
  {
    "arxiv_id": "2412.04477v3",
    "title": "Intelligent Tutors for Adult Learners: An Analysis of Needs and Challenges",
    "authors": [
      "Adit Gupta",
      "Momin Siddiqui",
      "Glen Smith",
      "Jenn Reddig",
      "Christopher MacLellan"
    ],
    "abstract": "This work examines the sociotechnical factors that influence the adoption and\nusage of intelligent tutoring systems in self-directed learning contexts,\nfocusing specifically on adult learners. The study is divided into two parts.\nFirst, we present Apprentice Tutors, a novel intelligent tutoring system\ndesigned to address the unique needs of adult learners. The platform includes\nadaptive problem selection, real-time feedback, and visual dashboards to\nsupport learning in college algebra topics. Second, we investigate the specific\nneeds and experiences of adult users through a deployment study and a series of\nfocus groups. Using thematic analysis, we identify key challenges and\nopportunities to improve tutor design and adoption. Based on these findings, we\noffer actionable design recommendations to help developers create intelligent\ntutoring systems that better align with the motivations and learning\npreferences of adult learners. This work contributes to a wider understanding\nof how to improve educational technologies to support lifelong learning and\nprofessional development.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04477v3",
    "published_date": "2024-11-19 19:05:04 UTC",
    "updated_date": "2025-02-19 03:08:14 UTC"
  },
  {
    "arxiv_id": "2411.12808v2",
    "title": "Conversational Medical AI: Ready for Practice",
    "authors": [
      "Antoine Lizée",
      "Pierre-Auguste Beaucoté",
      "James Whitbeck",
      "Marion Doumeingts",
      "Anaël Beaugnon",
      "Isabelle Feldhaus"
    ],
    "abstract": "The shortage of doctors is creating a critical squeeze in access to medical\nexpertise. While conversational Artificial Intelligence (AI) holds promise in\naddressing this problem, its safe deployment in patient-facing roles remains\nlargely unexplored in real-world medical settings. We present the first\nlarge-scale evaluation of a physician-supervised LLM-based conversational agent\nin a real-world medical setting.\n  Our agent, Mo, was integrated into an existing medical advice chat service.\nOver a three-week period, we conducted a randomized controlled experiment with\n926 cases to evaluate patient experience and satisfaction. Among these, Mo\nhandled 298 complete patient interactions, for which we report\nphysician-assessed measures of safety and medical accuracy.\n  Patients reported higher clarity of information (3.73 vs 3.62 out of 4, p <\n0.05) and overall satisfaction (4.58 vs 4.42 out of 5, p < 0.05) with\nAI-assisted conversations compared to standard care, while showing equivalent\nlevels of trust and perceived empathy. The high opt-in rate (81% among\nrespondents) exceeded previous benchmarks for AI acceptance in healthcare.\nPhysician oversight ensured safety, with 95% of conversations rated as \"good\"\nor \"excellent\" by general practitioners experienced in operating a medical\nadvice chat service.\n  Our findings demonstrate that carefully implemented AI medical assistants can\nenhance patient experience while maintaining safety standards through physician\nsupervision. This work provides empirical evidence for the feasibility of AI\ndeployment in healthcare communication and insights into the requirements for\nsuccessful integration into existing healthcare services.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to AAAI25 (Oral, workshop) 14 pages, 7 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2411.12808v2",
    "published_date": "2024-11-19 19:00:31 UTC",
    "updated_date": "2025-04-10 09:32:48 UTC"
  },
  {
    "arxiv_id": "2411.12736v1",
    "title": "ACING: Actor-Critic for Instruction Learning in Black-Box Large Language Models",
    "authors": [
      "Salma Kharrat",
      "Fares Fourati",
      "Marco Canini"
    ],
    "abstract": "The effectiveness of Large Language Models (LLMs) in solving tasks vastly\ndepends on the quality of the instructions, which often require fine-tuning\nthrough extensive human effort. This highlights the need for automated\ninstruction optimization; however, this optimization is particularly\nchallenging when dealing with black-box LLMs, where model parameters and\ngradients remain inaccessible. We propose ACING, a task-specific prompt\noptimization approach framed as a stateless continuous-action Reinforcement\nLearning (RL) problem, known as the continuum bandit setting. ACING leverages\nan actor-critic-based method to optimize prompts, learning from\nnon-differentiable reward signals. We validate ACING by optimizing prompts for\nChatGPT on 30 instruction-based tasks. ACING consistently outperforms baseline\nmethods, achieving a median score improvement of 10 percentage points.\nFurthermore, ACING not only recovers but also surpasses human-crafted expert\ninstructions, achieving up to a 39 percentage point improvement against human\nbenchmarks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.SY",
      "math.OC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12736v1",
    "published_date": "2024-11-19 18:58:03 UTC",
    "updated_date": "2024-11-19 18:58:03 UTC"
  },
  {
    "arxiv_id": "2411.12732v1",
    "title": "Benchmarking Positional Encodings for GNNs and Graph Transformers",
    "authors": [
      "Florian Grötschla",
      "Jiaqing Xie",
      "Roger Wattenhofer"
    ],
    "abstract": "Recent advances in Graph Neural Networks (GNNs) and Graph Transformers (GTs)\nhave been driven by innovations in architectures and Positional Encodings\n(PEs), which are critical for augmenting node features and capturing graph\ntopology. PEs are essential for GTs, where topological information would\notherwise be lost without message-passing. However, PEs are often tested\nalongside novel architectures, making it difficult to isolate their effect on\nestablished models. To address this, we present a comprehensive benchmark of\nPEs in a unified framework that includes both message-passing GNNs and GTs. We\nalso establish theoretical connections between MPNNs and GTs and introduce a\nsparsified GRIT attention mechanism to examine the influence of global\nconnectivity. Our findings demonstrate that previously untested combinations of\nGNN architectures and PEs can outperform existing methods and offer a more\ncomprehensive picture of the state-of-the-art. To support future research and\nexperimentation in our framework, we make the code publicly available.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12732v1",
    "published_date": "2024-11-19 18:57:01 UTC",
    "updated_date": "2024-11-19 18:57:01 UTC"
  },
  {
    "arxiv_id": "2411.12724v2",
    "title": "Heuristic-Free Multi-Teacher Learning",
    "authors": [
      "Huy Thong Nguyen",
      "En-Hung Chu",
      "Lenord Melvix",
      "Jazon Jiao",
      "Chunglin Wen",
      "Benjamin Louie"
    ],
    "abstract": "We introduce Teacher2Task, a novel framework for multi-teacher learning that\neliminates the need for manual aggregation heuristics. Existing multi-teacher\nmethods typically rely on such heuristics to combine predictions from multiple\nteachers, often resulting in sub-optimal aggregated labels and the propagation\nof aggregation errors. Teacher2Task addresses these limitations by introducing\nteacher-specific input tokens and reformulating the training process. Instead\nof relying on aggregated labels, the framework transforms the training data,\nconsisting of ground truth labels and annotations from N teachers, into N+1\ndistinct tasks: N auxiliary tasks that predict the labeling styles of the N\nindividual teachers, and one primary task that focuses on the ground truth\nlabels. This approach, drawing upon principles from multiple learning\nparadigms, demonstrates strong empirical results across a range of\narchitectures, modalities, and tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12724v2",
    "published_date": "2024-11-19 18:45:16 UTC",
    "updated_date": "2025-01-24 13:54:43 UTC"
  },
  {
    "arxiv_id": "2411.12713v1",
    "title": "CATCH: Complementary Adaptive Token-level Contrastive Decoding to Mitigate Hallucinations in LVLMs",
    "authors": [
      "Zhehan Kan",
      "Ce Zhang",
      "Zihan Liao",
      "Yapeng Tian",
      "Wenming Yang",
      "Junyuan Xiao",
      "Xu Li",
      "Dongmei Jiang",
      "Yaowei Wang",
      "Qingmin Liao"
    ],
    "abstract": "Large Vision-Language Model (LVLM) systems have demonstrated impressive\nvision-language reasoning capabilities but suffer from pervasive and severe\nhallucination issues, posing significant risks in critical domains such as\nhealthcare and autonomous systems. Despite previous efforts to mitigate\nhallucinations, a persistent issue remains: visual defect from vision-language\nmisalignment, creating a bottleneck in visual processing capacity. To address\nthis challenge, we develop Complementary Adaptive Token-level Contrastive\nDecoding to Mitigate Hallucinations in LVLMs (CATCH), based on the Information\nBottleneck theory. CATCH introduces Complementary Visual Decoupling (CVD) for\nvisual information separation, Non-Visual Screening (NVS) for hallucination\ndetection, and Adaptive Token-level Contrastive Decoding (ATCD) for\nhallucination mitigation. CATCH addresses issues related to visual defects that\ncause diminished fine-grained feature perception and cumulative hallucinations\nin open-ended scenarios. It is applicable to various visual question-answering\ntasks without requiring any specific data or prior knowledge, and generalizes\nrobustly to new tasks without additional training, opening new possibilities\nfor advancing LVLM in various challenging applications.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12713v1",
    "published_date": "2024-11-19 18:27:31 UTC",
    "updated_date": "2024-11-19 18:27:31 UTC"
  },
  {
    "arxiv_id": "2411.12712v1",
    "title": "Enhancing Multi-Class Disease Classification: Neoplasms, Cardiovascular, Nervous System, and Digestive Disorders Using Advanced LLMs",
    "authors": [
      "Ahmed Akib Jawad Karim",
      "Muhammad Zawad Mahmud",
      "Samiha Islam",
      "Aznur Azam"
    ],
    "abstract": "In this research, we explored the improvement in terms of multi-class disease\nclassification via pre-trained language models over Medical-Abstracts-TC-Corpus\nthat spans five medical conditions. We excluded non-cancer conditions and\nexamined four specific diseases. We assessed four LLMs, BioBERT, XLNet, and\nBERT, as well as a novel base model (Last-BERT). BioBERT, which was pre-trained\non medical data, demonstrated superior performance in medical text\nclassification (97% accuracy). Surprisingly, XLNet followed closely (96%\naccuracy), demonstrating its generalizability across domains even though it was\nnot pre-trained on medical data. LastBERT, a custom model based on the lighter\nversion of BERT, also proved competitive with 87.10% accuracy (just under\nBERT's 89.33%). Our findings confirm the importance of specialized models such\nas BioBERT and also support impressions around more general solutions like\nXLNet and well-tuned transformer architectures with fewer parameters (in this\ncase, LastBERT) in medical domain tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "7 Pages, 4 tables and 11 figures. Under review in a IEEE conference",
    "pdf_url": "http://arxiv.org/pdf/2411.12712v1",
    "published_date": "2024-11-19 18:27:25 UTC",
    "updated_date": "2024-11-19 18:27:25 UTC"
  },
  {
    "arxiv_id": "2411.12701v3",
    "title": "When Backdoors Speak: Understanding LLM Backdoor Attacks Through Model-Generated Explanations",
    "authors": [
      "Huaizhi Ge",
      "Yiming Li",
      "Qifan Wang",
      "Yongfeng Zhang",
      "Ruixiang Tang"
    ],
    "abstract": "Large Language Models (LLMs) are known to be vulnerable to backdoor attacks,\nwhere triggers embedded in poisoned samples can maliciously alter LLMs'\nbehaviors. In this paper, we move beyond attacking LLMs and instead examine\nbackdoor attacks through the novel lens of natural language explanations.\nSpecifically, we leverage LLMs' generative capabilities to produce\nhuman-readable explanations for their decisions, enabling direct comparisons\nbetween explanations for clean and poisoned samples. Our results show that\nbackdoored models produce coherent explanations for clean inputs but diverse\nand logically flawed explanations for poisoned data, a pattern consistent\nacross classification and generation tasks for different backdoor attacks.\nFurther analysis reveals key insights into the explanation generation process.\nAt the token level, explanation tokens associated with poisoned samples only\nappear in the final few transformer layers. At the sentence level, attention\ndynamics indicate that poisoned inputs shift attention away from the original\ninput context during explanation generation. These findings enhance our\nunderstanding of backdoor mechanisms in LLMs and present a promising framework\nfor detecting vulnerabilities through explainability.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12701v3",
    "published_date": "2024-11-19 18:11:36 UTC",
    "updated_date": "2025-02-16 03:19:01 UTC"
  },
  {
    "arxiv_id": "2411.12697v2",
    "title": "Attribute Inference Attacks for Federated Regression Tasks",
    "authors": [
      "Francesco Diana",
      "Othmane Marfoq",
      "Chuan Xu",
      "Giovanni Neglia",
      "Frédéric Giroire",
      "Eoin Thomas"
    ],
    "abstract": "Federated Learning (FL) enables multiple clients, such as mobile phones and\nIoT devices, to collaboratively train a global machine learning model while\nkeeping their data localized. However, recent studies have revealed that the\ntraining phase of FL is vulnerable to reconstruction attacks, such as attribute\ninference attacks (AIA), where adversaries exploit exchanged messages and\nauxiliary public information to uncover sensitive attributes of targeted\nclients. While these attacks have been extensively studied in the context of\nclassification tasks, their impact on regression tasks remains largely\nunexplored. In this paper, we address this gap by proposing novel model-based\nAIAs specifically designed for regression tasks in FL environments. Our\napproach considers scenarios where adversaries can either eavesdrop on\nexchanged messages or directly interfere with the training process. We\nbenchmark our proposed attacks against state-of-the-art methods using\nreal-world datasets. The results demonstrate a significant increase in\nreconstruction accuracy, particularly in heterogeneous client datasets, a\ncommon scenario in FL. The efficacy of our model-based AIAs makes them better\ncandidates for empirically quantifying privacy leakage for federated regression\ntasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12697v2",
    "published_date": "2024-11-19 18:06:06 UTC",
    "updated_date": "2025-04-16 12:29:45 UTC"
  },
  {
    "arxiv_id": "2411.15197v1",
    "title": "K-means Derived Unsupervised Feature Selection using Improved ADMM",
    "authors": [
      "Ziheng Sun",
      "Chris Ding",
      "Jicong Fan"
    ],
    "abstract": "Feature selection is important for high-dimensional data analysis and is\nnon-trivial in unsupervised learning problems such as dimensionality reduction\nand clustering. The goal of unsupervised feature selection is finding a subset\nof features such that the data points from different clusters are well\nseparated. This paper presents a novel method called K-means Derived\nUnsupervised Feature Selection (K-means UFS). Unlike most existing spectral\nanalysis based unsupervised feature selection methods, we select features using\nthe objective of K-means. We develop an alternating direction method of\nmultipliers (ADMM) to solve the NP-hard optimization problem of our K-means UFS\nmodel. Extensive experiments on real datasets show that our K-means UFS is more\neffective than the baselines in selecting features for clustering.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15197v1",
    "published_date": "2024-11-19 18:05:02 UTC",
    "updated_date": "2024-11-19 18:05:02 UTC"
  },
  {
    "arxiv_id": "2411.12593v3",
    "title": "AdaCM$^2$: On Understanding Extremely Long-Term Video with Adaptive Cross-Modality Memory Reduction",
    "authors": [
      "Yuanbin Man",
      "Ying Huang",
      "Chengming Zhang",
      "Bingzhe Li",
      "Wei Niu",
      "Miao Yin"
    ],
    "abstract": "The advancements in large language models (LLMs) have propelled the\nimprovement of video understanding tasks by incorporating LLMs with visual\nmodels. However, most existing LLM-based models (e.g., VideoLLaMA, VideoChat)\nare constrained to processing short-duration videos. Recent attempts to\nunderstand long-term videos by extracting and compressing visual features into\na fixed memory size. Nevertheless, those methods leverage only visual modality\nto merge video tokens and overlook the correlation between visual and textual\nqueries, leading to difficulties in effectively handling complex\nquestion-answering tasks. To address the challenges of long videos and complex\nprompts, we propose AdaCM$^2$, which, for the first time, introduces an\nadaptive cross-modality memory reduction approach to video-text alignment in an\nauto-regressive manner on video streams. Our extensive experiments on various\nvideo understanding tasks, such as video captioning, video question answering,\nand video classification, demonstrate that AdaCM$^2$ achieves state-of-the-art\nperformance across multiple datasets while significantly reducing memory usage.\nNotably, it achieves a 4.5% improvement across multiple tasks in the LVU\ndataset with a GPU memory consumption reduction of up to 65%.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2025 Highlight",
    "pdf_url": "http://arxiv.org/pdf/2411.12593v3",
    "published_date": "2024-11-19 18:04:13 UTC",
    "updated_date": "2025-04-04 17:58:08 UTC"
  },
  {
    "arxiv_id": "2411.12685v1",
    "title": "Enhanced Sign Language Translation between American Sign Language (ASL) and Indian Sign Language (ISL) Using LLMs",
    "authors": [
      "Malay Kumar",
      "S. Sarvajit Visagan",
      "Tanish Sarang Mahajan",
      "Anisha Natarajan"
    ],
    "abstract": "We have come up with a research that hopes to provide a bridge between the\nusers of American Sign Language and the users of spoken language and Indian\nSign Language (ISL). The research enabled us to create a novel framework that\nwe have developed for Learner Systems. Leveraging art of Large models to create\nkey features including: - Real-time translation between these two sign\nlanguages in an efficient manner. Making LLM's capability available for\nseamless translations to ISL. Here is the full study showing its implementation\nin this paper. The core of the system is a sophisticated pipeline that begins\nwith reclassification and recognition of ASL gestures based on a strong Random\nForest Classifier. By recognizing the ASL, it is translated into text which can\nbe more easily processed. Highly evolved natural language NLP (Natural Language\nProcessing) techniques come in handy as they play a role in our LLM integration\nwhere you then use LLMs to be able to convert the ASL text to ISL which\nprovides you with the intent of sentence or phrase. The final step is to\nsynthesize the translated text back into ISL gestures, creating an end-to-end\ntranslation experience using RIFE-Net. This framework is tasked with key\nchallenges such as automatically dealing with gesture variability and\novercoming the linguistic differences between ASL and ISL. By automating the\ntranslation process, we hope to vastly improve accessibility for sign language\nusers. No longer will the communication gap between ASL and ISL create\nbarriers; this totally cool innovation aims to bring our communities closer\ntogether. And we believe, with full confidence in our framework, that we're\nable to apply the same principles across a wide variety of sign language\ndialects.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12685v1",
    "published_date": "2024-11-19 17:45:12 UTC",
    "updated_date": "2024-11-19 17:45:12 UTC"
  },
  {
    "arxiv_id": "2411.17715v1",
    "title": "Hybrid Quantum Deep Learning Model for Emotion Detection using raw EEG Signal Analysis",
    "authors": [
      "Ali Asgar Chandanwala",
      "Srutakirti Bhowmik",
      "Parna Chaudhury",
      "Sheena Christabel Pravin"
    ],
    "abstract": "Applications in behavioural research, human-computer interaction, and mental\nhealth depend on the ability to recognize emotions. In order to improve the\naccuracy of emotion recognition using electroencephalography (EEG) data, this\nwork presents a hybrid quantum deep learning technique. Conventional EEG-based\nemotion recognition techniques are limited by noise and high-dimensional data\ncomplexity, which make feature extraction difficult. To tackle these issues,\nour method combines traditional deep learning classification with\nquantum-enhanced feature extraction. To identify important brain wave patterns,\nBandpass filtering and Welch method are used as preprocessing techniques on EEG\ndata. Intricate inter-band interactions that are essential for determining\nemotional states are captured by mapping frequency band power attributes\n(delta, theta, alpha, and beta) to quantum representations. Entanglement and\nrotation gates are used in a hybrid quantum circuit to maximize the model's\nsensitivity to EEG patterns associated with different emotions. Promising\nresults from evaluation on a test dataset indicate the model's potential for\naccurate emotion recognition. The model will be extended for real-time\napplications and multi-class categorization in future study, which could\nimprove EEG-based mental health screening instruments. This method offers a\npromising tool for applications in adaptive human-computer systems and mental\nhealth monitoring by showcasing the possibilities of fusing traditional deep\nlearning with quantum processing for reliable, scalable emotion recognition.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17715v1",
    "published_date": "2024-11-19 17:44:04 UTC",
    "updated_date": "2024-11-19 17:44:04 UTC"
  },
  {
    "arxiv_id": "2411.12681v1",
    "title": "AI Guided Early Screening of Cervical Cancer",
    "authors": [
      "Dharanidharan S I",
      "Suhitha Renuka S V",
      "Ajishi Singh",
      "Sheena Christabel Pravin"
    ],
    "abstract": "In order to support the creation of reliable machine learning models for\nanomaly detection, this project focuses on preprocessing, enhancing, and\norganizing a medical imaging dataset. There are two classifications in the\ndataset: normal and abnormal, along with extra noise fluctuations. In order to\nimprove the photographs' quality, undesirable artifacts, including visible\nmedical equipment at the edges, were eliminated using central cropping.\nAdjusting the brightness and contrast was one of the additional preprocessing\nprocesses. Normalization was then performed to normalize the data. To make\nclassification jobs easier, the dataset was methodically handled by combining\nseveral image subsets into two primary categories: normal and pathological. To\nprovide a strong training set that adapts well to real-world situations,\nsophisticated picture preprocessing techniques were used, such as contrast\nenhancement and real-time augmentation (including rotations, zooms, and\nbrightness modifications). To guarantee efficient model evaluation, the data\nwas subsequently divided into training and testing subsets. In order to create\nprecise and effective machine learning models for medical anomaly detection,\nhigh-quality input data is ensured via this thorough approach. Because of the\nproject pipeline's flexible and scalable design, it can be easily integrated\nwith bigger clinical decision-support systems.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12681v1",
    "published_date": "2024-11-19 17:39:03 UTC",
    "updated_date": "2024-11-19 17:39:03 UTC"
  },
  {
    "arxiv_id": "2411.12678v1",
    "title": "Deep Learning-Driven Heat Map Analysis for Evaluating thickness of Wounded Skin Layers",
    "authors": [
      "Devakumar GR",
      "JB Kaarthikeyan",
      "Dominic Immanuel T",
      "Sheena Christabel Pravin"
    ],
    "abstract": "Understanding the appropriate skin layer thickness in wounded sites is an\nimportant tool to move forward on wound healing practices and treatment\nprotocols. Methods to measure depth often are invasive and less specific. This\npaper introduces a novel method that is non-invasive with deep learning\ntechniques using classifying of skin layers that helps in measurement of wound\ndepth through heatmap analysis. A set of approximately 200 labeled images of\nskin allows five classes to be distinguished: scars, wounds, and healthy skin,\namong others. Each image has annotated key layers, namely the stratum cornetum,\nthe epidermis, and the dermis, in the software Roboflow. In the preliminary\nstage, the Heatmap generator VGG16 was used to enhance the visibility of tissue\nlayers, based upon which their annotated images were used to train ResNet18\nwith early stopping techniques. It ended up at a very high accuracy rate of\n97.67%. To do this, the comparison of the models ResNet18, VGG16, DenseNet121,\nand EfficientNet has been done where both EfficientNet and ResNet18 have\nattained accuracy rates of almost 95.35%. For further hyperparameter tuning,\nEfficientNet and ResNet18 were trained at six different learning rates to\ndetermine the best model configuration. It has been noted that the accuracy has\nhuge variations with different learning rates. In the case of EfficientNet, the\nmaximum achievable accuracy was 95.35% at the rate of 0.0001. The same was true\nfor ResNet18, which also attained its peak value of 95.35% at the same rate.\nThese facts indicate that the model can be applied and utilized in actual-time,\nnon-invasive wound assessment, which holds a great promise to improve clinical\ndiagnosis and treatment planning.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12678v1",
    "published_date": "2024-11-19 17:31:36 UTC",
    "updated_date": "2024-11-19 17:31:36 UTC"
  },
  {
    "arxiv_id": "2411.12671v1",
    "title": "Neurosymbolic Graph Enrichment for Grounded World Models",
    "authors": [
      "Stefano De Giorgis",
      "Aldo Gangemi",
      "Alessandro Russo"
    ],
    "abstract": "The development of artificial intelligence systems capable of understanding\nand reasoning about complex real-world scenarios is a significant challenge. In\nthis work we present a novel approach to enhance and exploit LLM reactive\ncapability to address complex problems and interpret deeply contextual\nreal-world meaning. We introduce a method and a tool for creating a multimodal,\nknowledge-augmented formal representation of meaning that combines the\nstrengths of large language models with structured semantic representations.\nOur method begins with an image input, utilizing state-of-the-art large\nlanguage models to generate a natural language description. This description is\nthen transformed into an Abstract Meaning Representation (AMR) graph, which is\nformalized and enriched with logical design patterns, and layered semantics\nderived from linguistic and factual knowledge bases. The resulting graph is\nthen fed back into the LLM to be extended with implicit knowledge activated by\ncomplex heuristic learning, including semantic implicatures, moral values,\nembodied cognition, and metaphorical representations. By bridging the gap\nbetween unstructured language models and formal semantic structures, our method\nopens new avenues for tackling intricate problems in natural language\nunderstanding and reasoning.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.ET"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12671v1",
    "published_date": "2024-11-19 17:23:55 UTC",
    "updated_date": "2024-11-19 17:23:55 UTC"
  },
  {
    "arxiv_id": "2411.12663v1",
    "title": "PoM: Efficient Image and Video Generation with the Polynomial Mixer",
    "authors": [
      "David Picard",
      "Nicolas Dufour"
    ],
    "abstract": "Diffusion models based on Multi-Head Attention (MHA) have become ubiquitous\nto generate high quality images and videos. However, encoding an image or a\nvideo as a sequence of patches results in costly attention patterns, as the\nrequirements both in terms of memory and compute grow quadratically. To\nalleviate this problem, we propose a drop-in replacement for MHA called the\nPolynomial Mixer (PoM) that has the benefit of encoding the entire sequence\ninto an explicit state. PoM has a linear complexity with respect to the number\nof tokens. This explicit state also allows us to generate frames in a\nsequential fashion, minimizing memory and compute requirement, while still\nbeing able to train in parallel. We show the Polynomial Mixer is a universal\nsequence-to-sequence approximator, just like regular MHA. We adapt several\nDiffusion Transformers (DiT) for generating images and videos with PoM\nreplacing MHA, and we obtain high quality samples while using less\ncomputational resources. The code is available at\nhttps://github.com/davidpicard/HoMM.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12663v1",
    "published_date": "2024-11-19 17:16:31 UTC",
    "updated_date": "2024-11-19 17:16:31 UTC"
  },
  {
    "arxiv_id": "2411.12650v1",
    "title": "Optimizing Airline Reservation Systems with Edge-Enabled Microservices: A Framework for Real-Time Data Processing and Enhanced User Responsiveness",
    "authors": [
      "Biman Barua",
      "M. Shamim Kaiser"
    ],
    "abstract": "The growing complexity of the operations of airline reservations requires a\nsmart solution for the adoption of novel approaches to the development of\nquick, efficient, and adaptive reservation systems. This paper outlines in\ndetail a conceptual framework for the implementation of edge computing\nmicroservices in order to address the shortcomings of traditional centralized\narchitectures. Specifically, as edge computing allows for certain activities\nsuch as seat inventory checks, booking processes and even confirmation to be\ndone nearer to the user, thus lessening the overall response time and improving\nthe performance of the system. In addition, the framework value should include\nachieving the high performance of the system such as low latency, high\nthroughput and higher user experience. The major design components include\ndeployed distributed computing microservices orchestrated by Kubernetes,\nreal-time message processing system with Kafka and its elastic scaling. Other\noperational components include Prometheus and Grafana, which are used to\nmonitor and manage resources, ensuring that all operational processes are\noptimized. Although this research focuses on a design and theoretical scheming\nof the framework, its use is foreseen to be more advantageous in facilitating a\ntransform in the provision of services in the airline industry by improving\ncustomers' satisfaction, providing infrastructure which is cheap to install and\nefficiently supporting technology changes such as artificial intelligence and\ninternet of things embedded systems. This research addresses the increasing\ndemand for new technologies with modern well-distributed and real-time-centric\nsystems and also provides a basis for future case implementation and testing.\nAs such, the proposed architecture offers a market-ready, extensible solution\nto the problems posed by existing airline reservation systems .",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CE",
      "cs.CL",
      "cs.DC"
    ],
    "primary_category": "cs.SE",
    "comment": "22 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.12650v1",
    "published_date": "2024-11-19 16:58:15 UTC",
    "updated_date": "2024-11-19 16:58:15 UTC"
  },
  {
    "arxiv_id": "2411.12644v2",
    "title": "CodeXEmbed: A Generalist Embedding Model Family for Multiligual and Multi-task Code Retrieval",
    "authors": [
      "Ye Liu",
      "Rui Meng",
      "Shafiq Joty",
      "Silvio Savarese",
      "Caiming Xiong",
      "Yingbo Zhou",
      "Semih Yavuz"
    ],
    "abstract": "Despite the success of text retrieval in many NLP tasks, code retrieval\nremains a largely underexplored area. Most text retrieval systems are tailored\nfor natural language queries, often neglecting the specific challenges of\nretrieving code. This gap leaves existing models unable to effectively capture\nthe diversity of programming languages and tasks across different domains,\nhighlighting the need for more focused research in code retrieval. To address\nthis, we introduce CodeXEmbed, a family of large-scale code embedding models\nranging from 400M to 7B parameters. Our novel training pipeline unifies\nmultiple programming languages and transforms various code-related tasks into a\ncommon retrieval framework, enhancing model generalizability and retrieval\nperformance. Our 7B model sets a new state-of-the-art (SOTA) in code retrieval,\noutperforming the previous leading model, Voyage-Code, by over 20% on CoIR\nbenchmark. In addition to excelling in code retrieval, our models demonstrate\ncompetitive performance on the widely adopted BeIR text retrieval benchmark,\noffering versatility across domains. Experimental results demonstrate that\nimproving retrieval performance significantly enhances end-to-end\nRetrieval-Augmented Generation (RAG) performance for code-related tasks.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12644v2",
    "published_date": "2024-11-19 16:54:45 UTC",
    "updated_date": "2024-11-24 18:52:38 UTC"
  },
  {
    "arxiv_id": "2411.12643v2",
    "title": "DLBacktrace: A Model Agnostic Explainability for any Deep Learning Models",
    "authors": [
      "Vinay Kumar Sankarapu",
      "Chintan Chitroda",
      "Yashwardhan Rathore",
      "Neeraj Kumar Singh",
      "Pratinav Seth"
    ],
    "abstract": "The rapid growth of AI has led to more complex deep learning models, often\noperating as opaque \"black boxes\" with limited transparency in their\ndecision-making. This lack of interpretability poses challenges, especially in\nhigh-stakes applications where understanding model output is crucial. This work\nhighlights the importance of interpretability in fostering trust,\naccountability, and responsible deployment. To address these challenges, we\nintroduce DLBacktrace, a novel, model-agnostic technique designed to provide\nclear insights into deep learning model decisions across a wide range of\ndomains and architectures, including MLPs, CNNs, and Transformer-based LLM\nmodels. We present a comprehensive overview of DLBacktrace and benchmark its\nperformance against established interpretability methods such as SHAP, LIME,\nand GradCAM. Our results demonstrate that DLBacktrace effectively enhances\nunderstanding of model behavior across diverse tasks. DLBacktrace is compatible\nwith models developed in both PyTorch and TensorFlow, supporting architectures\nsuch as BERT, ResNet, U-Net, and custom DNNs for tabular data. The library is\nopen-sourced and available at https://github.com/AryaXAI/DLBacktrace .",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12643v2",
    "published_date": "2024-11-19 16:54:30 UTC",
    "updated_date": "2025-02-04 06:55:49 UTC"
  },
  {
    "arxiv_id": "2411.12633v2",
    "title": "Instant Policy: In-Context Imitation Learning via Graph Diffusion",
    "authors": [
      "Vitalis Vosylius",
      "Edward Johns"
    ],
    "abstract": "Following the impressive capabilities of in-context learning with large\ntransformers, In-Context Imitation Learning (ICIL) is a promising opportunity\nfor robotics. We introduce Instant Policy, which learns new tasks instantly\n(without further training) from just one or two demonstrations, achieving ICIL\nthrough two key components. First, we introduce inductive biases through a\ngraph representation and model ICIL as a graph generation problem with a\nlearned diffusion process, enabling structured reasoning over demonstrations,\nobservations, and actions. Second, we show that such a model can be trained\nusing pseudo-demonstrations - arbitrary trajectories generated in simulation -\nas a virtually infinite pool of training data. Simulated and real experiments\nshow that Instant Policy enables rapid learning of various everyday robot\ntasks. We also show how it can serve as a foundation for cross-embodiment and\nzero-shot transfer to language-defined tasks. Code and videos are available at\nhttps://www.robot-learning.uk/instant-policy.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Code and videos are available on our project webpage at\n  https://www.robot-learning.uk/instant-policy",
    "pdf_url": "http://arxiv.org/pdf/2411.12633v2",
    "published_date": "2024-11-19 16:45:52 UTC",
    "updated_date": "2025-04-25 15:22:45 UTC"
  },
  {
    "arxiv_id": "2411.12629v1",
    "title": "Estimating Dark Matter Halo Masses in Simulated Galaxy Clusters with Graph Neural Networks",
    "authors": [
      "Nikhil Garuda",
      "John F. Wu",
      "Dylan Nelson",
      "Annalisa Pillepich"
    ],
    "abstract": "Galaxies grow and evolve in dark matter halos. Because dark matter is not\nvisible, galaxies' halo masses ($\\rm{M}_{\\rm{halo}}$) must be inferred\nindirectly. We present a graph neural network (GNN) model for predicting\n$\\rm{M}_{\\rm{halo}}$ from stellar mass ($\\rm{M}_{*}$) in simulated galaxy\nclusters using data from the IllustrisTNG simulation suite. Unlike traditional\nmachine learning models like random forests, our GNN captures the\ninformation-rich substructure of galaxy clusters by using spatial and kinematic\nrelationships between galaxy neighbour. A GNN model trained on the TNG-Cluster\ndataset and independently tested on the TNG300 simulation achieves superior\npredictive performance compared to other baseline models we tested. Future work\nwill extend this approach to different simulations and real observational\ndatasets to further validate the GNN model's ability to generalise.",
    "categories": [
      "astro-ph.GA",
      "astro-ph.CO",
      "astro-ph.IM",
      "cs.AI"
    ],
    "primary_category": "astro-ph.GA",
    "comment": "9 pages, 4 figures, accepted at the NeurIPS ML4PS 2024 workshop",
    "pdf_url": "http://arxiv.org/pdf/2411.12629v1",
    "published_date": "2024-11-19 16:40:17 UTC",
    "updated_date": "2024-11-19 16:40:17 UTC"
  },
  {
    "arxiv_id": "2411.15195v1",
    "title": "Graph Neural Network-Based Entity Extraction and Relationship Reasoning in Complex Knowledge Graphs",
    "authors": [
      "Junliang Du",
      "Guiran Liu",
      "Jia Gao",
      "Xiaoxuan Liao",
      "Jiacheng Hu",
      "Linxiao Wu"
    ],
    "abstract": "This study proposed a knowledge graph entity extraction and relationship\nreasoning algorithm based on a graph neural network, using a graph\nconvolutional network and graph attention network to model the complex\nstructure in the knowledge graph. By building an end-to-end joint model, this\npaper achieves efficient recognition and reasoning of entities and\nrelationships. In the experiment, this paper compared the model with a variety\nof deep learning algorithms and verified its superiority through indicators\nsuch as AUC, recall rate, precision rate, and F1 value. The experimental\nresults show that the model proposed in this paper performs well in all\nindicators, especially in complex knowledge graphs, it has stronger\ngeneralization ability and stability. This provides strong support for further\nresearch on knowledge graphs and also demonstrates the application potential of\ngraph neural networks in entity extraction and relationship reasoning.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15195v1",
    "published_date": "2024-11-19 16:23:49 UTC",
    "updated_date": "2024-11-19 16:23:49 UTC"
  },
  {
    "arxiv_id": "2411.12603v2",
    "title": "STREAM: A Universal State-Space Model for Sparse Geometric Data",
    "authors": [
      "Mark Schöne",
      "Yash Bhisikar",
      "Karan Bania",
      "Khaleelulla Khan Nazeer",
      "Christian Mayr",
      "Anand Subramoney",
      "David Kappel"
    ],
    "abstract": "Handling sparse and unstructured geometric data, such as point clouds or\nevent-based vision, is a pressing challenge in the field of machine vision.\nRecently, sequence models such as Transformers and state-space models entered\nthe domain of geometric data. These methods require specialized preprocessing\nto create a sequential view of a set of points. Furthermore, prior works\ninvolving sequence models iterate geometric data with either uniform or learned\nstep sizes, implicitly relying on the model to infer the underlying geometric\nstructure. In this work, we propose to encode geometric structure explicitly\ninto the parameterization of a state-space model. State-space models are based\non linear dynamics governed by a one-dimensional variable such as time or a\nspatial coordinate. We exploit this dynamic variable to inject relative\ndifferences of coordinates into the step size of the state-space model. The\nresulting geometric operation computes interactions between all pairs of N\npoints in O(N) steps. Our model deploys the Mamba selective state-space model\nwith a modified CUDA kernel to efficiently map sparse geometric data to modern\nhardware. The resulting sequence model, which we call STREAM, achieves\ncompetitive results on a range of benchmarks from point-cloud classification to\nevent-based vision and audio classification. STREAM demonstrates a powerful\ninductive bias for sparse geometric data by improving the PointMamba baseline\nwhen trained from scratch on the ModelNet40 and ScanObjectNN point cloud\nanalysis datasets. It further achieves, for the first time, 100% test accuracy\non all 11 classes of the DVS128 Gestures dataset.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12603v2",
    "published_date": "2024-11-19 16:06:32 UTC",
    "updated_date": "2024-11-22 11:21:50 UTC"
  },
  {
    "arxiv_id": "2411.12600v3",
    "title": "Provable unlearning in topic modeling and downstream tasks",
    "authors": [
      "Stanley Wei",
      "Sadhika Malladi",
      "Sanjeev Arora",
      "Amartya Sanyal"
    ],
    "abstract": "Machine unlearning algorithms are increasingly important as legal concerns\narise around the provenance of training data, but verifying the success of\nunlearning is often difficult. Provable guarantees for unlearning are often\nlimited to supervised learning settings. In this paper, we provide the first\ntheoretical guarantees for unlearning in the pre-training and fine-tuning\nparadigm by studying topic models, simple bag-of-words language models that can\nbe adapted to solve downstream tasks like retrieval and classification. First,\nwe design a provably effective unlearning algorithm for topic models that\nincurs a computational overhead independent of the size of the original\ndataset. Our analysis additionally quantifies the deletion capacity of the\nmodel -- i.e., the number of examples that can be unlearned without incurring a\nsignificant cost in model performance. Finally, we formally extend our analyses\nto account for adaptation to a given downstream task. In particular, we design\nan efficient algorithm to perform unlearning after fine-tuning the topic model\nvia a linear head. Notably, we show that it is easier to unlearn pre-training\ndata from models that have been fine-tuned to a particular task, and one can\nunlearn this data without modifying the base model.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12600v3",
    "published_date": "2024-11-19 16:04:31 UTC",
    "updated_date": "2025-04-20 15:34:51 UTC"
  },
  {
    "arxiv_id": "2411.12587v1",
    "title": "Whisper Finetuning on Nepali Language",
    "authors": [
      "Sanjay Rijal",
      "Shital Adhikari",
      "Manish Dahal",
      "Manish Awale",
      "Vaghawan Ojha"
    ],
    "abstract": "Despite the growing advancements in Automatic Speech Recognition (ASR)\nmodels, the development of robust models for underrepresented languages, such\nas Nepali, remains a challenge. This research focuses on making an exhaustive\nand generalized dataset followed by fine-tuning OpenAI's Whisper models of\ndifferent sizes to improve transcription (speech-to-text) accuracy for the\nNepali language. We leverage publicly available ASR datasets and self-recorded\ncustom datasets with a diverse range of accents, dialects, and speaking styles\nfurther enriched through augmentation. Our experimental results demonstrate\nthat fine-tuning Whisper models on our curated custom dataset substantially\nreduces the Word Error Rate (WER) across all model sizes attributed to larger\ndata variations in terms of speaker's age, gender, and sentiment, acoustic\nenvironment, dialect, denser audio segments (15-30 seconds) that are more\ncompatible with Whisper's input, and manual curation of audios and\ntranscriptions. Notably, our approach outperforms Whisper's baseline models\ntrained on Fleur's dataset, achieving WER reductions of up to 36.2% on the\nsmall and 23.8% on medium models. Furthermore, we show that data augmentation\nplays a significant role in enhancing model robustness. Our approach underlines\nthe importance of dataset quality, variation, and augmentation in the\nadaptation of state-of-the-art models to underrepresented languages for\ndeveloping accurate ASR systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12587v1",
    "published_date": "2024-11-19 15:55:56 UTC",
    "updated_date": "2024-11-19 15:55:56 UTC"
  },
  {
    "arxiv_id": "2412.04476v3",
    "title": "The Moral Mind(s) of Large Language Models",
    "authors": [
      "Avner Seror"
    ],
    "abstract": "As large language models (LLMs) increasingly participate in tasks with\nethical and societal stakes, a critical question arises: do they exhibit an\nemergent \"moral mind\" - a consistent structure of moral preferences guiding\ntheir decisions - and to what extent is this structure shared across models? To\ninvestigate this, we applied tools from revealed preference theory to nearly 40\nleading LLMs, presenting each with many structured moral dilemmas spanning five\nfoundational dimensions of ethical reasoning. Using a probabilistic rationality\ntest, we found that at least one model from each major provider exhibited\nbehavior consistent with approximately stable moral preferences, acting as if\nguided by an underlying utility function. We then estimated these utility\nfunctions and found that most models cluster around neutral moral stances. To\nfurther characterize heterogeneity, we employed a non-parametric permutation\napproach, constructing a probabilistic similarity network based on revealed\npreference patterns. The results reveal a shared core in LLMs' moral reasoning,\nbut also meaningful variation: some models show flexible reasoning across\nperspectives, while others adhere to more rigid ethical profiles. These\nfindings provide a new empirical lens for evaluating moral consistency in LLMs\nand offer a framework for benchmarking ethical alignment across AI systems.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04476v3",
    "published_date": "2024-11-19 15:40:16 UTC",
    "updated_date": "2025-04-25 15:47:33 UTC"
  },
  {
    "arxiv_id": "2411.12571v1",
    "title": "Large Language Models for Combinatorial Optimization of Design Structure Matrix",
    "authors": [
      "Shuo Jiang",
      "Min Xie",
      "Jianxi Luo"
    ],
    "abstract": "Combinatorial optimization (CO) is essential for improving efficiency and\nperformance in engineering applications. As complexity increases with larger\nproblem sizes and more intricate dependencies, identifying the optimal solution\nbecome challenging. When it comes to real-world engineering problems,\nalgorithms based on pure mathematical reasoning are limited and incapable to\ncapture the contextual nuances necessary for optimization. This study explores\nthe potential of Large Language Models (LLMs) in solving engineering CO\nproblems by leveraging their reasoning power and contextual knowledge. We\npropose a novel LLM-based framework that integrates network topology and domain\nknowledge to optimize the sequencing of Design Structure Matrix (DSM)-a common\nCO problem. Our experiments on various DSM cases demonstrate that the proposed\nmethod achieves faster convergence and higher solution quality than benchmark\nmethods. Moreover, results show that incorporating contextual domain knowledge\nsignificantly improves performance despite the choice of LLMs. These findings\nhighlight the potential of LLMs in tackling complex real-world CO problems by\ncombining semantic and mathematical reasoning. This approach paves the way for\na new paradigm in in real-world combinatorial optimization.",
    "categories": [
      "cs.CE",
      "cs.AI",
      "cs.CL",
      "I.2.7; I.2.1"
    ],
    "primary_category": "cs.CE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12571v1",
    "published_date": "2024-11-19 15:39:51 UTC",
    "updated_date": "2024-11-19 15:39:51 UTC"
  },
  {
    "arxiv_id": "2411.12560v2",
    "title": "Topological Symmetry Enhanced Graph Convolution for Skeleton-Based Action Recognition",
    "authors": [
      "Zeyu Liang",
      "Hailun Xia",
      "Naichuan Zheng",
      "Huan Xu"
    ],
    "abstract": "Skeleton-based action recognition has achieved remarkable performance with\nthe development of graph convolutional networks (GCNs). However, most of these\nmethods tend to construct complex topology learning mechanisms while neglecting\nthe inherent symmetry of the human body. Additionally, the use of temporal\nconvolutions with certain fixed receptive fields limits their capacity to\neffectively capture dependencies in time sequences. To address the issues, we\n(1) propose a novel Topological Symmetry Enhanced Graph Convolution (TSE-GC) to\nenable distinct topology learning across different channel partitions while\nincorporating topological symmetry awareness and (2) construct a Multi-Branch\nDeformable Temporal Convolution (MBDTC) for skeleton-based action recognition.\nThe proposed TSE-GC emphasizes the inherent symmetry of the human body while\nenabling efficient learning of dynamic topologies. Meanwhile, the design of\nMBDTC introduces the concept of deformable modeling, leading to more flexible\nreceptive fields and stronger modeling capacity of temporal dependencies.\nCombining TSE-GC with MBDTC, our final model, TSE-GCN, achieves competitive\nperformance with fewer parameters compared with state-of-the-art methods on\nthree large datasets, NTU RGB+D, NTU RGB+D 120, and NW-UCLA. On the\ncross-subject and cross-set evaluations of NTU RGB+D 120, the accuracies of our\nmodel reach 90.0\\% and 91.1\\%, with 1.1M parameters and 1.38 GFLOPS for one\nstream.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12560v2",
    "published_date": "2024-11-19 15:23:59 UTC",
    "updated_date": "2024-11-20 02:51:08 UTC"
  },
  {
    "arxiv_id": "2411.12558v1",
    "title": "Recall and Refine: A Simple but Effective Source-free Open-set Domain Adaptation Framework",
    "authors": [
      "Ismail Nejjar",
      "Hao Dong",
      "Olga Fink"
    ],
    "abstract": "Open-set Domain Adaptation (OSDA) aims to adapt a model from a labeled source\ndomain to an unlabeled target domain, where novel classes - also referred to as\ntarget-private unknown classes - are present. Source-free Open-set Domain\nAdaptation (SF-OSDA) methods address OSDA without accessing labeled source\ndata, making them particularly relevant under privacy constraints. However,\nSF-OSDA presents significant challenges due to distribution shifts and the\nintroduction of novel classes. Existing SF-OSDA methods typically rely on\nthresholding the prediction entropy of a sample to identify it as either a\nknown or unknown class but fail to explicitly learn discriminative features for\nthe target-private unknown classes. We propose Recall and Refine (RRDA), a\nnovel SF-OSDA framework designed to address these limitations by explicitly\nlearning features for target-private unknown classes. RRDA employs a two-step\nprocess. First, we enhance the model's capacity to recognize unknown classes by\ntraining a target classifier with an additional decision boundary, guided by\nsynthetic samples generated from target domain features. This enables the\nclassifier to effectively separate known and unknown classes. In the second\nstep, we adapt the entire model to the target domain, addressing both domain\nshifts and improving generalization to unknown classes. Any off-the-shelf\nsource-free domain adaptation method (e.g., SHOT, AaD) can be seamlessly\nintegrated into our framework at this stage. Extensive experiments on three\nbenchmark datasets demonstrate that RRDA significantly outperforms existing\nSF-OSDA and OSDA methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12558v1",
    "published_date": "2024-11-19 15:18:50 UTC",
    "updated_date": "2024-11-19 15:18:50 UTC"
  },
  {
    "arxiv_id": "2411.12790v1",
    "title": "Visual-Oriented Fine-Grained Knowledge Editing for MultiModal Large Language Models",
    "authors": [
      "Zhen Zeng",
      "Leijiang Gu",
      "Xun Yang",
      "Zhangling Duan",
      "Zenglin Shi",
      "Meng Wang"
    ],
    "abstract": "Knowledge editing aims to efficiently and cost-effectively correct\ninaccuracies and update outdated information. Recently, there has been growing\ninterest in extending knowledge editing from Large Language Models (LLMs) to\nMultimodal Large Language Models (MLLMs), which integrate both textual and\nvisual information, introducing additional editing complexities. Existing\nmultimodal knowledge editing works primarily focus on text-oriented,\ncoarse-grained scenarios, failing to address the unique challenges posed by\nmultimodal contexts. In this paper, we propose a visual-oriented, fine-grained\nmultimodal knowledge editing task that targets precise editing in images with\nmultiple interacting entities. We introduce the Fine-Grained Visual Knowledge\nEditing (FGVEdit) benchmark to evaluate this task. Moreover, we propose a\nMultimodal Scope Classifier-based Knowledge Editor (MSCKE) framework. MSCKE\nleverages a multimodal scope classifier that integrates both visual and textual\ninformation to accurately identify and update knowledge related to specific\nentities within images. This approach ensures precise editing while preserving\nirrelevant information, overcoming the limitations of traditional text-only\nediting methods. Extensive experiments on the FGVEdit benchmark demonstrate\nthat MSCKE outperforms existing methods, showcasing its effectiveness in\nsolving the complex challenges of multimodal knowledge editing.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12790v1",
    "published_date": "2024-11-19 14:49:36 UTC",
    "updated_date": "2024-11-19 14:49:36 UTC"
  },
  {
    "arxiv_id": "2411.12539v1",
    "title": "Predicting Customer Satisfaction by Replicating the Survey Response Distribution",
    "authors": [
      "Etienne Manderscheid",
      "Matthias Lee"
    ],
    "abstract": "For many call centers, customer satisfaction (CSAT) is a key performance\nindicator (KPI). However, only a fraction of customers take the CSAT survey\nafter the call, leading to a biased and inaccurate average CSAT value, and\nmissed opportunities for coaching, follow-up, and rectification. Therefore,\ncall centers can benefit from a model predicting customer satisfaction on calls\nwhere the customer did not complete the survey. Given that CSAT is a closely\nmonitored KPI, it is critical to minimize any bias in the average predicted\nCSAT (pCSAT). In this paper, we introduce a method such that predicted CSAT\n(pCSAT) scores accurately replicate the distribution of survey CSAT responses\nfor every call center with sufficient data in a live production environment.\nThe method can be applied to many multiclass classification problems to improve\nthe class balance and minimize its changes upon model updates.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12539v1",
    "published_date": "2024-11-19 14:39:29 UTC",
    "updated_date": "2024-11-19 14:39:29 UTC"
  },
  {
    "arxiv_id": "2411.12525v1",
    "title": "Rethinking Top Probability from Multi-view for Distracted Driver Behaviour Localization",
    "authors": [
      "Quang Vinh Nguyen",
      "Vo Hoang Thanh Son",
      "Chau Truong Vinh Hoang",
      "Duc Duy Nguyen",
      "Nhat Huy Nguyen Minh",
      "Soo-Hyung Kim"
    ],
    "abstract": "Naturalistic driving action localization task aims to recognize and\ncomprehend human behaviors and actions from video data captured during\nreal-world driving scenarios. Previous studies have shown great action\nlocalization performance by applying a recognition model followed by\nprobability-based post-processing. Nevertheless, the probabilities provided by\nthe recognition model frequently contain confused information causing challenge\nfor post-processing. In this work, we adopt an action recognition model based\non self-supervise learning to detect distracted activities and give potential\naction probabilities. Subsequently, a constraint ensemble strategy takes\nadvantages of multi-camera views to provide robust predictions. Finally, we\nintroduce a conditional post-processing operation to locate distracted\nbehaviours and action temporal boundaries precisely. Experimenting on test set\nA2, our method obtains the sixth position on the public leaderboard of track 3\nof the 2024 AI City Challenge.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Computer Vision and Pattern Recognition Workshop 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.12525v1",
    "published_date": "2024-11-19 14:18:02 UTC",
    "updated_date": "2024-11-19 14:18:02 UTC"
  },
  {
    "arxiv_id": "2411.15194v1",
    "title": "Guiding Word Equation Solving using Graph Neural Networks (Extended Technical Report)",
    "authors": [
      "Parosh Aziz Abdulla",
      "Mohamed Faouzi Atig",
      "Julie Cailler",
      "Chencheng Liang",
      "Philipp Rümmer"
    ],
    "abstract": "This paper proposes a Graph Neural Network-guided algorithm for solving word\nequations, based on the well-known Nielsen transformation for splitting\nequations. The algorithm iteratively rewrites the first terms of each side of\nan equation, giving rise to a tree-like search space. The choice of path at\neach split point of the tree significantly impacts solving time, motivating the\nuse of Graph Neural Networks (GNNs) for efficient split decision-making. Split\ndecisions are encoded as multi-classification tasks, and five graph\nrepresentations of word equations are introduced to encode their structural\ninformation for GNNs. The algorithm is implemented as a solver named DragonLi.\nExperiments are conducted on artificial and real-world benchmarks. The\nalgorithm performs particularly well on satisfiable problems. For single word\n\\mbox{equations}, DragonLi can solve significantly more problems than\nwell-established string solvers. For the conjunction of multiple word\nequations, DragonLi is competitive with state-of-the-art string solvers.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.LO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15194v1",
    "published_date": "2024-11-19 14:15:34 UTC",
    "updated_date": "2024-11-19 14:15:34 UTC"
  },
  {
    "arxiv_id": "2411.12517v2",
    "title": "The Hermeneutic Turn of AI: Are Machines Capable of Interpreting?",
    "authors": [
      "Remy Demichelis"
    ],
    "abstract": "This article aims to demonstrate how the approach to computing is being\ndisrupted by deep learning (artificial neural networks), not only in terms of\ntechniques but also in our interactions with machines. It also addresses the\nphilosophical tradition of hermeneutics (Don Ihde, Wilhelm Dilthey) to\nhighlight a parallel with this movement and to demystify the idea of human-like\nAI.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "4 pages;\n  https://theconversation.com/lia-est-elle-capable-dinterpreter-ce-quon-lui-demande-230890",
    "pdf_url": "http://arxiv.org/pdf/2411.12517v2",
    "published_date": "2024-11-19 13:59:16 UTC",
    "updated_date": "2024-11-28 09:24:06 UTC"
  },
  {
    "arxiv_id": "2411.12502v3",
    "title": "Transformer Neural Processes - Kernel Regression",
    "authors": [
      "Daniel Jenson",
      "Jhonathan Navott",
      "Mengyan Zhang",
      "Makkunda Sharma",
      "Elizaveta Semenova",
      "Seth Flaxman"
    ],
    "abstract": "Neural Processes (NPs) are a rapidly evolving class of models designed to\ndirectly model the posterior predictive distribution of stochastic processes.\nOriginally developed as a scalable alternative to Gaussian Processes (GPs),\nwhich are limited by $O(n^3)$ runtime complexity, the most accurate modern NPs\ncan often rival GPs but still suffer from an $O(n^2)$ bottleneck due to their\nattention mechanism. We introduce the Transformer Neural Process - Kernel\nRegression (TNP-KR), a scalable NP featuring: (1) a Kernel Regression Block\n(KRBlock), a simple, extensible, and parameter efficient transformer block with\ncomplexity $O(n_c^2 + n_c n_t)$, where $n_c$ and $n_t$ are the number of\ncontext and test points, respectively; (2) a kernel-based attention bias; and\n(3) two novel attention mechanisms: scan attention (SA), a memory-efficient\nscan-based attention that when paired with a kernel-based bias can make TNP-KR\ntranslation invariant, and deep kernel attention (DKA), a Performer-style\nattention that implicitly incoporates a distance bias and further reduces\ncomplexity to $O(n_c)$. These enhancements enable both TNP-KR variants to\nperform inference with 100K context points on over 1M test points in under a\nminute on a single 24GB GPU. On benchmarks spanning meta regression, Bayesian\noptimization, image completion, and epidemiology, TNP-KR with DKA outperforms\nits Performer counterpart on nearly every benchmark, while TNP-KR with SA\nachieves state-of-the-art results.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12502v3",
    "published_date": "2024-11-19 13:40:49 UTC",
    "updated_date": "2025-02-11 11:03:24 UTC"
  },
  {
    "arxiv_id": "2411.12498v2",
    "title": "Enhancing Reasoning Capabilities of LLMs via Principled Synthetic Logic Corpus",
    "authors": [
      "Terufumi Morishita",
      "Gaku Morio",
      "Atsuki Yamaguchi",
      "Yasuhiro Sogawa"
    ],
    "abstract": "Large language models (LLMs) are capable of solving a wide range of tasks,\nyet they have struggled with reasoning. To address this, we propose\n$\\textbf{Additional Logic Training (ALT)}$, which aims to enhance LLMs'\nreasoning capabilities by program-generated logical reasoning samples. We first\nestablish principles for designing high-quality samples by integrating symbolic\nlogic theory and previous empirical insights. Then, based on these principles,\nwe construct a synthetic corpus named $\\textbf{Formal Logic Deduction Diverse}$\n($\\textbf{FLD}$$_{\\times 2}$), comprising numerous samples of multi-step\ndeduction with unknown facts, diverse reasoning rules, diverse linguistic\nexpressions, and challenging distractors. Finally, we empirically show that ALT\non FLD$_{\\times2}$ substantially enhances the reasoning capabilities of\nstate-of-the-art LLMs, including LLaMA-3.1-70B. Improvements include gains of\nup to 30 points on logical reasoning benchmarks, up to 10 points on math and\ncoding benchmarks, and 5 points on the benchmark suite BBH.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.12498v2",
    "published_date": "2024-11-19 13:31:53 UTC",
    "updated_date": "2024-12-23 10:36:38 UTC"
  },
  {
    "arxiv_id": "2411.12483v1",
    "title": "Analysing Explanation-Related Interactions in Collaborative Perception-Cognition-Communication-Action",
    "authors": [
      "Marc Roig Vilamala",
      "Jack Furby",
      "Julian de Gortari Briseno",
      "Mani Srivastava",
      "Alun Preece",
      "Carolina Fuentes Toro"
    ],
    "abstract": "Effective communication is essential in collaborative tasks, so AI-equipped\nrobots working alongside humans need to be able to explain their behaviour in\norder to cooperate effectively and earn trust. We analyse and classify\ncommunications among human participants collaborating to complete a simulated\nemergency response task. The analysis identifies messages that relate to\nvarious kinds of interactive explanations identified in the explainable AI\nliterature. This allows us to understand what type of explanations humans\nexpect from their teammates in such settings, and thus where AI-equipped robots\nmost need explanation capabilities. We find that most explanation-related\nmessages seek clarification in the decisions or actions taken. We also confirm\nthat messages have an impact on the performance of our simulated task.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.HC",
    "comment": "4 pages, 3 figures, published as a Late Breaking Report in RO-MAN\n  2024",
    "pdf_url": "http://arxiv.org/pdf/2411.12483v1",
    "published_date": "2024-11-19 13:07:04 UTC",
    "updated_date": "2024-11-19 13:07:04 UTC"
  },
  {
    "arxiv_id": "2411.14480v1",
    "title": "Associative Knowledge Graphs for Efficient Sequence Storage and Retrieval",
    "authors": [
      "Przemysław Stokłosa",
      "Janusz A. Starzyk",
      "Paweł Raif",
      "Adrian Horzyk",
      "Marcin Kowalik"
    ],
    "abstract": "This paper presents a novel approach for constructing associative knowledge\ngraphs that are highly effective for storing and recognizing sequences. The\ngraph is created by representing overlapping sequences of objects, as tightly\nconnected clusters within the larger graph. Individual objects (represented as\nnodes) can be a part of multiple sequences or appear repeatedly within a single\nsequence. To retrieve sequences, we leverage context, providing a subset of\nobjects that triggers an association with the complete sequence. The system's\nmemory capacity is determined by the size of the graph and the density of its\nconnections. We have theoretically derived the relationships between the\ncritical density of the graph and the memory capacity for storing sequences.\nThe critical density is the point beyond which error-free sequence\nreconstruction becomes impossible. Furthermore, we have developed an efficient\nalgorithm for ordering elements within a sequence. Through extensive\nexperiments with various types of sequences, we have confirmed the validity of\nthese relationships. This approach has potential applications in diverse\nfields, such as anomaly detection in financial transactions or predicting user\nbehavior based on past actions.",
    "categories": [
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.14480v1",
    "published_date": "2024-11-19 13:00:31 UTC",
    "updated_date": "2024-11-19 13:00:31 UTC"
  },
  {
    "arxiv_id": "2411.12476v1",
    "title": "Comparing Prior and Learned Time Representations in Transformer Models of Timeseries",
    "authors": [
      "Natalia Koliou",
      "Tatiana Boura",
      "Stasinos Konstantopoulos",
      "George Meramveliotakis",
      "George Kosmadakis"
    ],
    "abstract": "What sets timeseries analysis apart from other machine learning exercises is\nthat time representation becomes a primary aspect of the experiment setup, as\nit must adequately represent the temporal relations that are relevant for the\napplication at hand. In the work described here we study wo different\nvariations of the Transformer architecture: one where we use the fixed time\nrepresentation proposed in the literature and one where the time representation\nis learned from the data. Our experiments use data from predicting the energy\noutput of solar panels, a task that exhibits known periodicities (daily and\nseasonal) that is straight-forward to encode in the fixed time representation.\nOur results indicate that even in an experiment where the phenomenon is\nwell-understood, it is difficult to encode prior knowledge due to side-effects\nthat are difficult to mitigate. We conclude that research work is needed to\nwork the human into the learning loop in ways that improve the robustness and\ntrust-worthiness of the network.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Presented at the AI in Natural Sciences and Technology (AINST) track\n  of the 13th Conference on Artificial Intelligence (SETN 2024), 11-13\n  September 2024, Piraeus, Greece",
    "pdf_url": "http://arxiv.org/pdf/2411.12476v1",
    "published_date": "2024-11-19 12:56:43 UTC",
    "updated_date": "2024-11-19 12:56:43 UTC"
  },
  {
    "arxiv_id": "2411.12469v4",
    "title": "AI Flow at the Network Edge",
    "authors": [
      "Jiawei Shao",
      "Xuelong Li"
    ],
    "abstract": "Recent advancements in large language models (LLMs) and their multimodal\nvariants have led to remarkable progress across various domains, demonstrating\nimpressive capabilities and unprecedented potential. In the era of ubiquitous\nconnectivity, leveraging communication networks to distribute intelligence is a\ntransformative concept, envisioning AI-powered services accessible at the\nnetwork edge. However, pushing large models from the cloud to\nresource-constrained environments faces critical challenges. Model inference on\nlow-end devices leads to excessive latency and performance bottlenecks, while\nraw data transmission over limited bandwidth networks causes high communication\noverhead. This article presents AI Flow, a framework that streamlines the\ninference process by jointly leveraging the heterogeneous resources available\nacross devices, edge nodes, and cloud servers, making intelligence flow across\nnetworks. To facilitate cooperation among multiple computational nodes, the\nproposed framework explores a paradigm shift in the design of communication\nnetwork systems from transmitting information flow to intelligence flow, where\nthe goal of communications is task-oriented and folded into the inference\nprocess. Experimental results demonstrate the effectiveness of the proposed\nframework through an image captioning use case, showcasing the ability to\nreduce response latency while maintaining high-quality captions. This article\nserves as a position paper for identifying the motivation, challenges, and\nprinciples of AI Flow.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG",
      "cs.NI"
    ],
    "primary_category": "eess.SP",
    "comment": "This paper has been accepted to IEEE Network Magazine",
    "pdf_url": "http://arxiv.org/pdf/2411.12469v4",
    "published_date": "2024-11-19 12:51:17 UTC",
    "updated_date": "2025-02-13 06:18:46 UTC"
  },
  {
    "arxiv_id": "2411.12460v2",
    "title": "Exploring Iterative Controllable Summarization with Large Language Models",
    "authors": [
      "Sangwon Ryu",
      "Heejin Do",
      "Daehee Kim",
      "Hwanjo Yu",
      "Dongwoo Kim",
      "Yunsu Kim",
      "Gary Geunbae Lee",
      "Jungseul Ok"
    ],
    "abstract": "Large language models (LLMs) have demonstrated remarkable performance in\nabstractive summarization tasks. However, their ability to precisely control\nsummary attributes (e.g., length or topic) remains underexplored, limiting\ntheir adaptability to specific user preferences. In this paper, we\nsystematically explore the controllability of LLMs. To this end, we revisit\nsummary attribute measurements and introduce iterative evaluation metrics,\nfailure rate and average iteration count to precisely evaluate controllability\nof LLMs, rather than merely assessing errors. Our findings show that LLMs\nstruggle more with numerical attributes than with linguistic attributes. To\naddress this challenge, we propose a guide-to-explain framework (GTE) for\ncontrollable summarization. Our GTE framework enables the model to identify\nmisaligned attributes in the initial draft and guides it in self-explaining\nerrors in the previous output. By allowing the model to reflect on its\nmisalignment, GTE generates well-adjusted summaries that satisfy the desired\nattributes with robust effectiveness, requiring surprisingly fewer iterations\nthan other iterative approaches.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12460v2",
    "published_date": "2024-11-19 12:36:02 UTC",
    "updated_date": "2025-03-03 10:35:27 UTC"
  },
  {
    "arxiv_id": "2411.15193v1",
    "title": "Gradient-Weighted Feature Back-Projection: A Fast Alternative to Feature Distillation in 3D Gaussian Splatting",
    "authors": [
      "Joji Joseph",
      "Bharadwaj Amrutur",
      "Shalabh Bhatnagar"
    ],
    "abstract": "We introduce a training-free method for feature field rendering in Gaussian\nsplatting. Our approach back-projects 2D features into pre-trained 3D\nGaussians, using a weighted sum based on each Gaussian's influence in the final\nrendering. While most training-based feature field rendering methods excel at\n2D segmentation but perform poorly at 3D segmentation without post-processing,\nour method achieves high-quality results in both 2D and 3D segmentation.\nExperimental results demonstrate that our approach is fast, scalable, and\noffers performance comparable to training-based methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15193v1",
    "published_date": "2024-11-19 12:17:15 UTC",
    "updated_date": "2024-11-19 12:17:15 UTC"
  },
  {
    "arxiv_id": "2411.17835v1",
    "title": "Arabic-Nougat: Fine-Tuning Vision Transformers for Arabic OCR and Markdown Extraction",
    "authors": [
      "Mohamed Rashad"
    ],
    "abstract": "We present Arabic-Nougat, a suite of OCR models for converting Arabic book\npages into structured Markdown text. Based on Meta's Nougat architecture,\nArabic-Nougat includes three specialized models: arabic-small-nougat,\narabic-base-nougat, and arabic-large-nougat. These models are fine-tuned on a\nsynthetic dataset, arabic-img2md, comprising 13.7k pairs of Arabic book pages\nand their Markdown representations. Key contributions include the\nAranizer-PBE-86k tokenizer, designed for efficient tokenization, and the use of\ntorch.bfloat16 precision with Flash Attention 2 for optimized training and\ninference. Our models achieve state-of-the-art performance, with\narabic-large-nougat delivering the highest Markdown Structure Accuracy and the\nlowest Character Error Rate. Additionally, we release a large-scale dataset\ncontaining 1.1 billion Arabic tokens extracted from over 8,500 books using our\nbest-performing model, providing a valuable resource for Arabic OCR research.\nAll models, datasets, and code are open-sourced and available at\nhttps://github.com/MohamedAliRashad/arabic-nougat.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "7 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2411.17835v1",
    "published_date": "2024-11-19 12:09:12 UTC",
    "updated_date": "2024-11-19 12:09:12 UTC"
  },
  {
    "arxiv_id": "2411.12433v1",
    "title": "Preference-Conditioned Gradient Variations for Multi-Objective Quality-Diversity",
    "authors": [
      "Hannah Janmohamed",
      "Maxence Faldor",
      "Thomas Pierrot",
      "Antoine Cully"
    ],
    "abstract": "In a variety of domains, from robotics to finance, Quality-Diversity\nalgorithms have been used to generate collections of both diverse and\nhigh-performing solutions. Multi-Objective Quality-Diversity algorithms have\nemerged as a promising approach for applying these methods to complex,\nmulti-objective problems. However, existing methods are limited by their search\ncapabilities. For example, Multi-Objective Map-Elites depends on random genetic\nvariations which struggle in high-dimensional search spaces. Despite efforts to\nenhance search efficiency with gradient-based mutation operators, existing\napproaches consider updating solutions to improve on each objective separately\nrather than achieving desired trade-offs. In this work, we address this\nlimitation by introducing Multi-Objective Map-Elites with\nPreference-Conditioned Policy-Gradient and Crowding Mechanisms: a new\nMulti-Objective Quality-Diversity algorithm that uses preference-conditioned\npolicy-gradient mutations to efficiently discover promising regions of the\nobjective space and crowding mechanisms to promote a uniform distribution of\nsolutions on the Pareto front. We evaluate our approach on six robotics\nlocomotion tasks and show that our method outperforms or matches all\nstate-of-the-art Multi-Objective Quality-Diversity methods in all six,\nincluding two newly proposed tri-objective tasks. Importantly, our method also\nachieves a smoother set of trade-offs, as measured by newly-proposed\nsparsity-based metrics. This performance comes at a lower computational storage\ncost compared to previous methods.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12433v1",
    "published_date": "2024-11-19 11:50:03 UTC",
    "updated_date": "2024-11-19 11:50:03 UTC"
  },
  {
    "arxiv_id": "2411.12787v2",
    "title": "Visual Cue Enhancement and Dual Low-Rank Adaptation for Efficient Visual Instruction Fine-Tuning",
    "authors": [
      "Pengkun Jiao",
      "Bin Zhu",
      "Jingjing Chen",
      "Chong-Wah Ngo",
      "Yu-Gang Jiang"
    ],
    "abstract": "Parameter-efficient fine-tuning multimodal large language models (MLLMs)\npresents significant challenges, including reliance on high-level visual\nfeatures that limit fine-grained detail comprehension, and data conflicts that\narise from task complexity. To address these issues, we propose an efficient\nfine-tuning framework with two novel approaches: Vision Cue Enhancement (VCE)\nand Dual Low-Rank Adaptation (Dual-LoRA). VCE enhances the vision projector by\nintegrating multi-level visual cues, improving the model's ability to capture\nfine-grained visual features. Dual-LoRA introduces a dual low-rank structure\nfor instruction tuning, decoupling learning into skill and task spaces to\nenable precise control and efficient adaptation across diverse tasks. Our\nmethod simplifies implementation, enhances visual comprehension, and improves\nadaptability. Experiments on both downstream tasks and general benchmarks\ndemonstrate the effectiveness of our proposed approach.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12787v2",
    "published_date": "2024-11-19 11:03:09 UTC",
    "updated_date": "2024-12-02 07:41:38 UTC"
  },
  {
    "arxiv_id": "2411.14479v1",
    "title": "GRL-Prompt: Towards Knowledge Graph based Prompt Optimization via Reinforcement Learning",
    "authors": [
      "Yuze Liu",
      "Tingjie Liu",
      "Tiehua Zhang",
      "Youhua Xia",
      "Jinze Wang",
      "Zhishu Shen",
      "Jiong Jin",
      "Fei Richard Yu"
    ],
    "abstract": "Large language models (LLMs) have demonstrated impressive success in a wide\nrange of natural language processing (NLP) tasks due to their extensive general\nknowledge of the world. Recent works discovered that the performance of LLMs is\nheavily dependent on the input prompt. However, prompt engineering is usually\ndone manually in a trial-and-error fashion, which can be labor-intensive and\nchallenging in order to find the optimal prompts. To address these problems and\nunleash the utmost potential of LLMs, we propose a novel LLMs-agnostic\nframework for prompt optimization, namely GRL-Prompt, which aims to\nautomatically construct optimal prompts via reinforcement learning (RL) in an\nend-to-end manner. To provide structured action/state representation for\noptimizing prompts, we construct a knowledge graph (KG) that better encodes the\ncorrelation between the user query and candidate in-context examples.\nFurthermore, a policy network is formulated to generate the optimal action by\nselecting a set of in-context examples in a rewardable order to construct the\nprompt. Additionally, the embedding-based reward shaping is utilized to\nstabilize the RL training process. The experimental results show that\nGRL-Prompt outperforms recent state-of-the-art methods, achieving an average\nincrease of 0.10 in ROUGE-1, 0.07 in ROUGE-2, 0.07 in ROUGE-L, and 0.05 in\nBLEU.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.14479v1",
    "published_date": "2024-11-19 10:52:25 UTC",
    "updated_date": "2024-11-19 10:52:25 UTC"
  },
  {
    "arxiv_id": "2412.00029v2",
    "title": "Planning vs Reasoning: Ablations to Test Capabilities of LoRA layers",
    "authors": [
      "Neel Redkar"
    ],
    "abstract": "Low-Rank Adaptation (LoRA) layers have emerged as a promising approach for\nefficient model fine-tuning, but their capabilities and limitations have not\nbeen fully explored. This paper: 1) Investigates the fundamental question of\nwhether LoRA layers are effective at increasing reasoning + planning abilities\n2) We introduce HashChain Reasoning, a novel evaluation dataset that\ndeterministically tests reasoning capabilities.\n  Through systematic ablation studies on GPT-2, we demonstrate that reasoning\ncapabilities appear to exist primarily in low-rank spaces and can be\neffectively enhanced using LoRA layers. The effective rank analysis of trained\nLoRA matrices reveals a 2-3x lower rank requirement for reasoning tasks\ncompared to planning tasks, giving context on where LoRA layers would be\neffective. This also provides evidence for reasoning fundamentally preferring\nlow-parameter spaces for generalization.",
    "categories": [
      "cs.AI",
      "I.2.7, I.2.6"
    ],
    "primary_category": "cs.AI",
    "comment": "7 pages, 5 figures, preprint",
    "pdf_url": "http://arxiv.org/pdf/2412.00029v2",
    "published_date": "2024-11-19 10:51:49 UTC",
    "updated_date": "2025-02-05 10:01:29 UTC"
  },
  {
    "arxiv_id": "2411.12405v2",
    "title": "Evaluating the Prompt Steerability of Large Language Models",
    "authors": [
      "Erik Miehling",
      "Michael Desmond",
      "Karthikeyan Natesan Ramamurthy",
      "Elizabeth M. Daly",
      "Pierre Dognin",
      "Jesus Rios",
      "Djallel Bouneffouf",
      "Miao Liu"
    ],
    "abstract": "Building pluralistic AI requires designing models that are able to be shaped\nto represent a wide range of value systems and cultures. Achieving this\nrequires first being able to evaluate the degree to which a given model is\ncapable of reflecting various personas. To this end, we propose a benchmark for\nevaluating the steerability of model personas as a function of prompting. Our\ndesign is based on a formal definition of prompt steerability, which analyzes\nthe degree to which a model's joint behavioral distribution can be shifted from\nits baseline. By defining steerability indices and inspecting how these indices\nchange as a function of steering effort, we can estimate the steerability of a\nmodel across various persona dimensions and directions. Our benchmark reveals\nthat the steerability of many current models is limited -- due to both a skew\nin their baseline behavior and an asymmetry in their steerability across many\npersona dimensions. We release an implementation of our benchmark at\nhttps://github.com/IBM/prompt-steering.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "Short version appeared at the Pluralistic Alignment workshop at\n  NeurIPS 2024; extended version appeared at NAACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2411.12405v2",
    "published_date": "2024-11-19 10:41:54 UTC",
    "updated_date": "2025-02-15 15:34:26 UTC"
  },
  {
    "arxiv_id": "2411.12395v1",
    "title": "Do LLMs Understand Ambiguity in Text? A Case Study in Open-world Question Answering",
    "authors": [
      "Aryan Keluskar",
      "Amrita Bhattacharjee",
      "Huan Liu"
    ],
    "abstract": "Ambiguity in natural language poses significant challenges to Large Language\nModels (LLMs) used for open-domain question answering. LLMs often struggle with\nthe inherent uncertainties of human communication, leading to\nmisinterpretations, miscommunications, hallucinations, and biased responses.\nThis significantly weakens their ability to be used for tasks like\nfact-checking, question answering, feature extraction, and sentiment analysis.\nUsing open-domain question answering as a test case, we compare off-the-shelf\nand few-shot LLM performance, focusing on measuring the impact of explicit\ndisambiguation strategies. We demonstrate how simple, training-free,\ntoken-level disambiguation methods may be effectively used to improve LLM\nperformance for ambiguous question answering tasks. We empirically show our\nfindings and discuss best practices and broader impacts regarding ambiguity in\nLLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at the REU Symposium at IEEE BigData 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.12395v1",
    "published_date": "2024-11-19 10:27:26 UTC",
    "updated_date": "2024-11-19 10:27:26 UTC"
  },
  {
    "arxiv_id": "2411.12357v1",
    "title": "A Layered Architecture for Developing and Enhancing Capabilities in Large Language Model-based Software Systems",
    "authors": [
      "Dawen Zhang",
      "Xiwei Xu",
      "Chen Wang",
      "Zhenchang Xing",
      "Robert Mao"
    ],
    "abstract": "Significant efforts has been made to expand the use of Large Language Models\n(LLMs) beyond basic language tasks. While the generalizability and versatility\nof LLMs have enabled widespread adoption, evolving demands in application\ndevelopment often exceed their native capabilities. Meeting these demands may\ninvolve a diverse set of methods, such as enhancing creativity through either\ninference temperature adjustments or creativity-provoking prompts. Selecting\nthe right approach is critical, as different methods lead to trade-offs in\nengineering complexity, scalability, and operational costs. This paper\nintroduces a layered architecture that organizes LLM software system\ndevelopment into distinct layers, each characterized by specific attributes. By\naligning capabilities with these layers, the framework encourages the\nsystematic implementation of capabilities in effective and efficient ways that\nultimately supports desired functionalities and qualities. Through practical\ncase studies, we illustrate the utility of the framework. This work offers\ndevelopers actionable insights for selecting suitable technologies in LLM-based\nsoftware system development, promoting robustness and scalability.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.MA"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12357v1",
    "published_date": "2024-11-19 09:18:20 UTC",
    "updated_date": "2024-11-19 09:18:20 UTC"
  },
  {
    "arxiv_id": "2411.15191v2",
    "title": "Finding One's Bearings in the Hyperparameter Landscape of a Wide-Kernel Convolutional Fault Detector",
    "authors": [
      "Dan Hudson",
      "Jurgen van den Hoogen",
      "Martin Atzmueller"
    ],
    "abstract": "State-of-the-art algorithms are reported to be almost perfect at\ndistinguishing the vibrations arising from healthy and damaged machine\nbearings, according to benchmark datasets at least. However, what about their\napplication to new data? In this paper, we confirm that neural networks for\nbearing fault detection can be crippled by incorrect hyperparameterisation, and\nalso that the correct hyperparameter settings can change when transitioning to\nnew data. The paper combines multiple methods to explain the behaviour of the\nhyperparameters of a wide-kernel convolutional neural network and how to set\nthem. Since guidance already exists for generic hyperparameters like minibatch\nsize, we focus on how to set architecture-specific hyperparameters such as the\nwidth of the convolutional kernels, a topic which might otherwise be obscure.\nWe reflect different data properties by fusing information from seven different\nbenchmark datasets, and our results show that the kernel size in the first\nlayer in particular is sensitive to changes in the data. Looking deeper, we use\nmanipulated copies of one dataset in an attempt to spot why the kernel size\nsometimes needs to change. The relevance of sampling rate is studied by using\ndifferent levels of resampling, and spectral content is studied by increasingly\nfiltering out high frequencies. We find that, contrary to speculation in\nearlier work, high-frequency noise is not the main reason why a wide kernel is\npreferable to a narrow kernel. Finally, we conclude by stating clear guidance\non how to set the hyperparameters of our neural network architecture to work\neffectively on new data.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "24 pages, 10 figures, 8 tables",
    "pdf_url": "http://arxiv.org/pdf/2411.15191v2",
    "published_date": "2024-11-19 09:17:13 UTC",
    "updated_date": "2025-05-16 11:50:36 UTC"
  },
  {
    "arxiv_id": "2411.13602v2",
    "title": "Translating Electrocardiograms to Cardiac Magnetic Resonance Imaging Useful for Cardiac Assessment and Disease Screening: A Multi-Center Study AI for ECG to CMR Translation Study",
    "authors": [
      "Zhengyao Ding",
      "Ziyu Li",
      "Yujian Hu",
      "Youyao Xu",
      "Chengchen Zhao",
      "Yiheng Mao",
      "Haitao Li",
      "Zhikang Li",
      "Qian Li",
      "Jing Wang",
      "Yue Chen",
      "Mengjia Chen",
      "Longbo Wang",
      "Xuesen Chu",
      "Weichao Pan",
      "Ziyi Liu",
      "Fei Wu",
      "Hongkun Zhang",
      "Ting Chen",
      "Zhengxing Huang"
    ],
    "abstract": "Cardiovascular diseases (CVDs) are the leading cause of global mortality,\nnecessitating accessible and accurate diagnostic tools. While cardiac magnetic\nresonance imaging (CMR) provides gold-standard insights into cardiac structure\nand function, its clinical utility is limited by high cost and complexity. In\ncontrast, electrocardiography (ECG) is inexpensive and widely available but\nlacks the granularity of CMR. We propose CardioNets, a deep learning framework\nthat translates 12-lead ECG signals into CMR-level functional parameters and\nsynthetic images, enabling scalable cardiac assessment. CardioNets integrates\ncross-modal contrastive learning and generative pretraining, aligning ECG with\nCMR-derived cardiac phenotypes and synthesizing high-resolution CMR images via\na masked autoregressive model. Trained on 159,819 samples from five cohorts,\nincluding the UK Biobank (n=42,483) and MIMIC-IV-ECG (n=164,550), and\nexternally validated on independent clinical datasets (n=3,767), CardioNets\nachieved strong performance across disease screening and phenotype estimation\ntasks. In the UK Biobank, it improved cardiac phenotype regression R2 by 24.8%\nand cardiomyopathy AUC by up to 39.3% over baseline models. In MIMIC, it\nincreased AUC for pulmonary hypertension detection by 5.6%. Generated CMR\nimages showed 36.6% higher SSIM and 8.7% higher PSNR than prior approaches. In\na reader study, ECG-only CardioNets achieved 13.9% higher accuracy than human\nphysicians using both ECG and real CMR. These results suggest that CardioNets\noffers a promising, low-cost alternative to CMR for large-scale CVD screening,\nparticularly in resource-limited settings. Future efforts will focus on\nclinical deployment and regulatory validation of ECG-based synthetic imaging.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "27 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.13602v2",
    "published_date": "2024-11-19 09:09:14 UTC",
    "updated_date": "2025-05-15 05:56:38 UTC"
  },
  {
    "arxiv_id": "2411.12350v1",
    "title": "DiM: $f$-Divergence Minimization Guided Sharpness-Aware Optimization for Semi-supervised Medical Image Segmentation",
    "authors": [
      "Bingli Wang",
      "Houcheng Su",
      "Nan Yin",
      "Mengzhu Wang",
      "Li Shen"
    ],
    "abstract": "As a technique to alleviate the pressure of data annotation, semi-supervised\nlearning (SSL) has attracted widespread attention. In the specific domain of\nmedical image segmentation, semi-supervised methods (SSMIS) have become a\nresearch hotspot due to their ability to reduce the need for large amounts of\nprecisely annotated data. SSMIS focuses on enhancing the model's generalization\nperformance by leveraging a small number of labeled samples and a large number\nof unlabeled samples. The latest sharpness-aware optimization (SAM) technique,\nwhich optimizes the model by reducing the sharpness of the loss function, has\nshown significant success in SSMIS. However, SAM and its variants may not fully\naccount for the distribution differences between different datasets. To address\nthis issue, we propose a sharpness-aware optimization method based on\n$f$-divergence minimization (DiM) for semi-supervised medical image\nsegmentation. This method enhances the model's stability by fine-tuning the\nsensitivity of model parameters and improves the model's adaptability to\ndifferent datasets through the introduction of $f$-divergence. By reducing\n$f$-divergence, the DiM method not only improves the performance balance\nbetween the source and target datasets but also prevents performance\ndegradation due to overfitting on the source dataset.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "68T07, 92C55, 62H35",
      "I.2.6; I.4.10; J.3"
    ],
    "primary_category": "cs.CV",
    "comment": "8page",
    "pdf_url": "http://arxiv.org/pdf/2411.12350v1",
    "published_date": "2024-11-19 09:07:26 UTC",
    "updated_date": "2024-11-19 09:07:26 UTC"
  },
  {
    "arxiv_id": "2411.12319v2",
    "title": "CLIP Unreasonable Potential in Single-Shot Face Recognition",
    "authors": [
      "Nhan T. Luu"
    ],
    "abstract": "Face recognition is a core task in computer vision designed to identify and\nauthenticate individuals by analyzing facial patterns and features. This field\nintersects with artificial intelligence image processing and machine learning\nwith applications in security authentication and personalization. Traditional\napproaches in facial recognition focus on capturing facial features like the\neyes, nose and mouth and matching these against a database to verify\nidentities. However challenges such as high false positive rates have persisted\noften due to the similarity among individuals facial features. Recently\nContrastive Language Image Pretraining (CLIP) a model developed by OpenAI has\nshown promising advancements by linking natural language processing with vision\ntasks allowing it to generalize across modalities. Using CLIP's vision language\ncorrespondence and single-shot finetuning the model can achieve lower false\npositive rates upon deployment without the need of mass facial features\nextraction. This integration demonstrating CLIP's potential to address\npersistent issues in face recognition model performance without complicating\nour training paradigm.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12319v2",
    "published_date": "2024-11-19 08:23:52 UTC",
    "updated_date": "2024-11-20 03:31:17 UTC"
  },
  {
    "arxiv_id": "2411.15189v3",
    "title": "Order is All You Need for Categorical Data Clustering",
    "authors": [
      "Yiqun Zhang",
      "Mingjie Zhao",
      "Hong Jia",
      "Yang Lu",
      "Mengke Li",
      "Yiu-ming Cheung"
    ],
    "abstract": "Categorical data composed of qualitative valued attributes are ubiquitous in\nmachine learning tasks. Due to the lack of well-defined metric space,\ncategorical data distributions are difficult to be intuitively understood.\nClustering is a popular data analysis technique suitable for data distribution\nunderstanding. However, the success of clustering often relies on reasonable\ndistance metrics, which happens to be what categorical data naturally lack.\nThis paper therefore introduces a new finding that the order relation among\nattribute values is the decisive factor in clustering accuracy, and is also the\nkey to understanding categorical data clusters, because the essence of\nclustering is to order the clusters in terms of their admission to samples. To\nobtain the orders, we propose a new learning paradigm that allows joint\nlearning of clusters and the orders. It alternatively partitions the data into\nclusters based on the distance metric built upon the orders and estimates the\nmost likely orders according to the clusters. The algorithm achieves superior\nclustering accuracy with a convergence guarantee, and the learned orders\nfacilitate the understanding of the non-intuitive cluster distribution of\ncategorical data. Extensive experiments with ablation studies, statistical\nevidence, and case studies have validated the new insight into the importance\nof value order and the method proposition. The source code is temporarily\nopened in https://anonymous.4open.science/r/OCL-demo.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15189v3",
    "published_date": "2024-11-19 08:23:25 UTC",
    "updated_date": "2025-04-18 12:15:25 UTC"
  },
  {
    "arxiv_id": "2411.12308v3",
    "title": "SNN-Based Online Learning of Concepts and Action Laws in an Open World",
    "authors": [
      "Christel Grimaud",
      "Dominique Longin",
      "Andreas Herzig"
    ],
    "abstract": "We present the architecture of a fully autonomous, bio-inspired cognitive\nagent built around a spiking neural network (SNN) implementing the agent's\nsemantic memory. This agent explores its universe and learns concepts of\nobjects/situations and of its own actions in a one-shot manner. While\nobject/situation concepts are unary, action concepts are triples made up of an\ninitial situation, a motor activity, and an outcome. They embody the agent's\nknowledge of its universe's action laws. Both kinds of concepts have different\ndegrees of generality. To make decisions the agent queries its semantic memory\nfor the expected outcomes of envisaged actions and chooses the action to take\non the basis of these predictions. Our experiments show that the agent handles\nnew situations by appealing to previously learned general concepts and rapidly\nmodifies its concepts to adapt to environment changes.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.NE",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12308v3",
    "published_date": "2024-11-19 07:49:22 UTC",
    "updated_date": "2025-04-23 13:28:28 UTC"
  },
  {
    "arxiv_id": "2411.12307v1",
    "title": "Balancing Accuracy and Efficiency in Multi-Turn Intent Classification for LLM-Powered Dialog Systems in Production",
    "authors": [
      "Junhua Liu",
      "Yong Keat Tan",
      "Bin Fu",
      "Kwan Hui Lim"
    ],
    "abstract": "Accurate multi-turn intent classification is essential for advancing\nconversational AI systems. However, challenges such as the scarcity of\ncomprehensive datasets and the complexity of contextual dependencies across\ndialogue turns hinder progress. This paper presents two novel approaches\nleveraging Large Language Models (LLMs) to enhance scalability and reduce\nlatency in production dialogue systems. First, we introduce Symbol Tuning,\nwhich simplifies intent labels to reduce task complexity and improve\nperformance in multi-turn dialogues. Second, we propose C-LARA\n(Consistency-aware, Linguistics Adaptive Retrieval Augmentation), a framework\nthat employs LLMs for data augmentation and pseudo-labeling to generate\nsynthetic multi-turn dialogues. These enriched datasets are used to fine-tune a\nsmall, efficient model suitable for deployment. Experiments conducted on\nmultilingual dialogue datasets demonstrate significant improvements in\nclassification accuracy and resource efficiency. Our methods enhance multi-turn\nintent classification accuracy by 5.09%, reduce annotation costs by 40%, and\nenable scalable deployment in low-resource multilingual industrial systems,\nhighlighting their practicality and impact.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12307v1",
    "published_date": "2024-11-19 07:48:35 UTC",
    "updated_date": "2024-11-19 07:48:35 UTC"
  },
  {
    "arxiv_id": "2411.13599v2",
    "title": "Can ChatGPT Overcome Behavioral Biases in the Financial Sector? Classify-and-Rethink: Multi-Step Zero-Shot Reasoning in the Gold Investment",
    "authors": [
      "Shuoling Liu",
      "Gaoguo Jia",
      "Yuhang Jiang",
      "Liyuan Chen",
      "Qiang Yang"
    ],
    "abstract": "Large Language Models (LLMs) have achieved remarkable success recently,\ndisplaying exceptional capabilities in creating understandable and organized\ntext. These LLMs have been utilized in diverse fields, such as clinical\nresearch, where domain-specific models like Med-Palm have achieved human-level\nperformance. Recently, researchers have employed advanced prompt engineering to\nenhance the general reasoning ability of LLMs. Despite the remarkable success\nof zero-shot Chain-of-Thoughts (CoT) in solving general reasoning tasks, the\npotential of these methods still remains paid limited attention in the\nfinancial reasoning task.To address this issue, we explore multiple prompt\nstrategies and incorporated semantic news information to improve LLMs'\nperformance on financial reasoning tasks.To the best of our knowledge, we are\nthe first to explore this important issue by applying ChatGPT to the gold\ninvestment.In this work, our aim is to investigate the financial reasoning\ncapabilities of LLMs and their capacity to generate logical and persuasive\ninvestment opinions. We will use ChatGPT, one of the most powerful LLMs\nrecently, and prompt engineering to achieve this goal. Our research will focus\non understanding the ability of LLMs in sophisticated analysis and reasoning\nwithin the context of investment decision-making. Our study finds that ChatGPT\nwith CoT prompt can provide more explainable predictions and overcome\nbehavioral biases, which is crucial in finance-related tasks and can achieve\nhigher investment returns.",
    "categories": [
      "q-fin.ST",
      "cs.AI"
    ],
    "primary_category": "q-fin.ST",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.13599v2",
    "published_date": "2024-11-19 07:45:58 UTC",
    "updated_date": "2025-01-16 03:17:25 UTC"
  },
  {
    "arxiv_id": "2411.12778v1",
    "title": "Lucia: A Temporal Computing Platform for Contextual Intelligence",
    "authors": [
      "Weizhe Lin",
      "Junxiao Shen"
    ],
    "abstract": "The rapid evolution of artificial intelligence, especially through\nmulti-modal large language models, has redefined user interactions, enabling\nresponses that are contextually rich and human-like. As AI becomes an integral\npart of daily life, a new frontier has emerged: developing systems that not\nonly understand spatial and sensory data but also interpret temporal contexts\nto build long-term, personalized memories. This report introduces Lucia, an\nopen-source Temporal Computing Platform designed to enhance human cognition by\ncapturing and utilizing continuous contextual memory. Lucia introduces a\nlightweight, wearable device that excels in both comfort and real-time data\naccessibility, distinguishing itself from existing devices that typically\nprioritize either wearability or perceptual capabilities alone. By recording\nand interpreting daily activities over time, Lucia enables users to access a\nrobust temporal memory, enhancing cognitive processes such as decision-making\nand memory recall.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12778v1",
    "published_date": "2024-11-19 07:38:31 UTC",
    "updated_date": "2024-11-19 07:38:31 UTC"
  },
  {
    "arxiv_id": "2411.12290v1",
    "title": "SSEditor: Controllable Mask-to-Scene Generation with Diffusion Model",
    "authors": [
      "Haowen Zheng",
      "Yanyan Liang"
    ],
    "abstract": "Recent advancements in 3D diffusion-based semantic scene generation have\ngained attention. However, existing methods rely on unconditional generation\nand require multiple resampling steps when editing scenes, which significantly\nlimits their controllability and flexibility. To this end, we propose SSEditor,\na controllable Semantic Scene Editor that can generate specified target\ncategories without multiple-step resampling. SSEditor employs a two-stage\ndiffusion-based framework: (1) a 3D scene autoencoder is trained to obtain\nlatent triplane features, and (2) a mask-conditional diffusion model is trained\nfor customizable 3D semantic scene generation. In the second stage, we\nintroduce a geometric-semantic fusion module that enhance the model's ability\nto learn geometric and semantic information. This ensures that objects are\ngenerated with correct positions, sizes, and categories. Extensive experiments\non SemanticKITTI and CarlaSC demonstrate that SSEditor outperforms previous\napproaches in terms of controllability and flexibility in target generation, as\nwell as the quality of semantic scene generation and reconstruction. More\nimportantly, experiments on the unseen Occ-3D Waymo dataset show that SSEditor\nis capable of generating novel urban scenes, enabling the rapid construction of\n3D scenes.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12290v1",
    "published_date": "2024-11-19 07:19:05 UTC",
    "updated_date": "2024-11-19 07:19:05 UTC"
  },
  {
    "arxiv_id": "2411.12276v1",
    "title": "libcll: an Extendable Python Toolkit for Complementary-Label Learning",
    "authors": [
      "Nai-Xuan Ye",
      "Tan-Ha Mai",
      "Hsiu-Hsuan Wang",
      "Wei-I Lin",
      "Hsuan-Tien Lin"
    ],
    "abstract": "Complementary-label learning (CLL) is a weakly supervised learning paradigm\nfor multiclass classification, where only complementary labels -- indicating\nclasses an instance does not belong to -- are provided to the learning\nalgorithm. Despite CLL's increasing popularity, previous studies highlight two\nmain challenges: (1) inconsistent results arising from varied assumptions on\ncomplementary label generation, and (2) high barriers to entry due to the lack\nof a standardized evaluation platform across datasets and algorithms. To\naddress these challenges, we introduce \\texttt{libcll}, an extensible Python\ntoolkit for CLL research. \\texttt{libcll} provides a universal interface that\nsupports a wide range of generation assumptions, both synthetic and real-world\ndatasets, and key CLL algorithms. The toolkit is designed to mitigate\ninconsistencies and streamline the research process, with easy installation,\ncomprehensive usage guides, and quickstart tutorials that facilitate efficient\nadoption and implementation of CLL techniques. Extensive ablation studies\nconducted with \\texttt{libcll} demonstrate its utility in generating valuable\ninsights to advance future CLL research.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.12276v1",
    "published_date": "2024-11-19 06:56:24 UTC",
    "updated_date": "2024-11-19 06:56:24 UTC"
  },
  {
    "arxiv_id": "2411.12275v1",
    "title": "Building Trust: Foundations of Security, Safety and Transparency in AI",
    "authors": [
      "Huzaifa Sidhpurwala",
      "Garth Mollett",
      "Emily Fox",
      "Mark Bestavros",
      "Huamin Chen"
    ],
    "abstract": "This paper explores the rapidly evolving ecosystem of publicly available AI\nmodels, and their potential implications on the security and safety landscape.\nAs AI models become increasingly prevalent, understanding their potential risks\nand vulnerabilities is crucial. We review the current security and safety\nscenarios while highlighting challenges such as tracking issues, remediation,\nand the apparent absence of AI model lifecycle and ownership processes.\nComprehensive strategies to enhance security and safety for both model\ndevelopers and end-users are proposed. This paper aims to provide some of the\nfoundational pieces for more standardized security, safety, and transparency in\nthe development and operation of AI models and the larger open ecosystems and\ncommunities forming around them.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12275v1",
    "published_date": "2024-11-19 06:55:57 UTC",
    "updated_date": "2024-11-19 06:55:57 UTC"
  },
  {
    "arxiv_id": "2411.12256v2",
    "title": "Restructuring Tractable Probabilistic Circuits",
    "authors": [
      "Honghua Zhang",
      "Benjie Wang",
      "Marcelo Arenas",
      "Guy Van den Broeck"
    ],
    "abstract": "Probabilistic circuits (PCs) are a unifying representation for probabilistic\nmodels that support tractable inference. Numerous applications of PCs like\ncontrollable text generation depend on the ability to efficiently multiply two\ncircuits. Existing multiplication algorithms require that the circuits respect\nthe same structure, i.e. variable scopes decomposes according to the same\nvtree. In this work, we propose and study the task of restructuring\nstructured(-decomposable) PCs, that is, transforming a structured PC such that\nit conforms to a target vtree. We propose a generic approach for this problem\nand show that it leads to novel polynomial-time algorithms for multiplying\ncircuits respecting different vtrees, as well as a practical depth-reduction\nalgorithm that preserves structured decomposibility. Our work opens up new\navenues for tractable PC inference, suggesting the possibility of training with\nless restrictive PC structures while enabling efficient inference by changing\ntheir structures at inference time.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12256v2",
    "published_date": "2024-11-19 06:10:22 UTC",
    "updated_date": "2025-04-30 06:42:02 UTC"
  },
  {
    "arxiv_id": "2411.12255v1",
    "title": "Error-Feedback Model for Output Correction in Bilateral Control-Based Imitation Learning",
    "authors": [
      "Hiroshi Sato",
      "Masashi Konosu",
      "Sho Sakaino",
      "Toshiaki Tsuji"
    ],
    "abstract": "In recent years, imitation learning using neural networks has enabled robots\nto perform flexible tasks. However, since neural networks operate in a\nfeedforward structure, they do not possess a mechanism to compensate for output\nerrors. To address this limitation, we developed a feedback mechanism to\ncorrect these errors. By employing a hierarchical structure for neural networks\ncomprising lower and upper layers, the lower layer was controlled to follow the\nupper layer. Additionally, using a multi-layer perceptron in the lower layer,\nwhich lacks an internal state, enhanced the error feedback. In the\ncharacter-writing task, this model demonstrated improved accuracy in writing\npreviously untrained characters. In the character-writing task, this model\ndemonstrated improved accuracy in writing previously untrained characters.\nThrough autonomous control with error feedback, we confirmed that the lower\nlayer could effectively track the output of the upper layer. This study\nrepresents a promising step toward integrating neural networks with control\ntheories.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12255v1",
    "published_date": "2024-11-19 06:09:09 UTC",
    "updated_date": "2024-11-19 06:09:09 UTC"
  },
  {
    "arxiv_id": "2411.12246v1",
    "title": "Efficient Training in Multi-Agent Reinforcement Learning: A Communication-Free Framework for the Box-Pushing Problem",
    "authors": [
      "David Ge",
      "Hao Ji"
    ],
    "abstract": "Self-organizing systems consist of autonomous agents that can perform complex\ntasks and adapt to dynamic environments without a central controller. Prior\nresearch often relies on reinforcement learning to enable agents to gain the\nskills needed for task completion, such as in the box-pushing environment.\nHowever, when agents push from opposing directions during exploration, they\ntend to exert equal and opposite forces on the box, resulting in minimal\ndisplacement and inefficient training. This paper proposes a model called\nShared Pool of Information (SPI), which enables information to be accessible to\nall agents and facilitates coordination, reducing force conflicts among agents\nand enhancing exploration efficiency. Through computer simulations, we\ndemonstrate that SPI not only expedites the training process but also requires\nfewer steps per episode, significantly improving the agents' collaborative\neffectiveness.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "17 pages, 16 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.12246v1",
    "published_date": "2024-11-19 05:51:10 UTC",
    "updated_date": "2024-11-19 05:51:10 UTC"
  },
  {
    "arxiv_id": "2411.12240v2",
    "title": "Evaluating Tokenizer Performance of Large Language Models Across Official Indian Languages",
    "authors": [
      "S. Tamang",
      "D. J. Bora"
    ],
    "abstract": "Large Language Models (LLMs) based on transformer architectures have\nrevolutionized a variety of domains, with tokenization playing a pivotal role\nin their pre-processing and fine-tuning stages. In multilingual models,\nparticularly those tailored for Indic languages, effective tokenization is\ncrucial for optimizing performance. This paper presents a comprehensive\nevaluation of tokenizers used by 12 LLMs across all 22 official languages of\nIndia, with a focus on comparing the efficiency of their tokenization\nprocesses. We employed the Normalized Sequence Length (NSL) as a key metric in\nour analysis. Our findings reveal that the SUTRA tokenizer outperforms all\nother models, including several Indic-specific models, excelling in 14\nlanguages. Notable insights include the SUTRA tokenizer's superior handling of\nIndic languages, GPT-4o's advancement over its predecessor GPT-4 in processing\nIndian languages, and the limited performance of Project Indus in certain\nlanguages. This study underscores the critical importance of developing\ntargeted tokenization strategies for multilingual and Indic-centric models,\nlaying the groundwork for future improvements in tokenizer design to enhance\nlinguistic coverage and model efficiency.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12240v2",
    "published_date": "2024-11-19 05:37:17 UTC",
    "updated_date": "2024-11-26 18:14:50 UTC"
  },
  {
    "arxiv_id": "2411.14476v1",
    "title": "StreetviewLLM: Extracting Geographic Information Using a Chain-of-Thought Multimodal Large Language Model",
    "authors": [
      "Zongrong Li",
      "Junhao Xu",
      "Siqin Wang",
      "Yifan Wu",
      "Haiyang Li"
    ],
    "abstract": "Geospatial predictions are crucial for diverse fields such as disaster\nmanagement, urban planning, and public health. Traditional machine learning\nmethods often face limitations when handling unstructured or multi-modal data\nlike street view imagery. To address these challenges, we propose\nStreetViewLLM, a novel framework that integrates a large language model with\nthe chain-of-thought reasoning and multimodal data sources. By combining street\nview imagery with geographic coordinates and textual data, StreetViewLLM\nimproves the precision and granularity of geospatial predictions. Using\nretrieval-augmented generation techniques, our approach enhances geographic\ninformation extraction, enabling a detailed analysis of urban environments. The\nmodel has been applied to seven global cities, including Hong Kong, Tokyo,\nSingapore, Los Angeles, New York, London, and Paris, demonstrating superior\nperformance in predicting urban indicators, including population density,\naccessibility to healthcare, normalized difference vegetation index, building\nheight, and impervious surface. The results show that StreetViewLLM\nconsistently outperforms baseline models, offering improved predictive accuracy\nand deeper insights into the built environment. This research opens new\nopportunities for integrating the large language model into urban analytics,\ndecision-making in urban planning, infrastructure management, and environmental\nmonitoring.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.14476v1",
    "published_date": "2024-11-19 05:15:19 UTC",
    "updated_date": "2024-11-19 05:15:19 UTC"
  },
  {
    "arxiv_id": "2411.12775v1",
    "title": "Revisiting Fake News Detection: Towards Temporality-aware Evaluation by Leveraging Engagement Earliness",
    "authors": [
      "Junghoon Kim",
      "Junmo Lee",
      "Yeonjun In",
      "Kanghoon Yoon",
      "Chanyoung Park"
    ],
    "abstract": "Social graph-based fake news detection aims to identify news articles\ncontaining false information by utilizing social contexts, e.g., user\ninformation, tweets and comments. However, conventional methods are evaluated\nunder less realistic scenarios, where the model has access to future knowledge\non article-related and context-related data during training. In this work, we\nnewly formalize a more realistic evaluation scheme that mimics real-world\nscenarios, where the data is temporality-aware and the detection model can only\nbe trained on data collected up to a certain point in time. We show that the\ndiscriminative capabilities of conventional methods decrease sharply under this\nnew setting, and further propose DAWN, a method more applicable to such\nscenarios. Our empirical findings indicate that later engagements (e.g.,\nconsuming or reposting news) contribute more to noisy edges that link real\nnews-fake news pairs in the social graph. Motivated by this, we utilize feature\nrepresentations of engagement earliness to guide an edge weight estimator to\nsuppress the weights of such noisy edges, thereby enhancing the detection\nperformance of DAWN. Through extensive experiments, we demonstrate that DAWN\noutperforms existing fake news detection methods under real-world environments.\nThe source code is available at https://github.com/LeeJunmo/DAWN.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SI",
    "comment": "WSDM 2025",
    "pdf_url": "http://arxiv.org/pdf/2411.12775v1",
    "published_date": "2024-11-19 05:08:00 UTC",
    "updated_date": "2024-11-19 05:08:00 UTC"
  },
  {
    "arxiv_id": "2411.12222v1",
    "title": "Contrast Similarity-Aware Dual-Pathway Mamba for Multivariate Time Series Node Classification",
    "authors": [
      "Mingsen Du",
      "Meng Chen",
      "Yongjian Li",
      "Xiuxin Zhang",
      "Jiahui Gao",
      "Cun Ji",
      "Shoushui Wei"
    ],
    "abstract": "Multivariate time series (MTS) data is generated through multiple sensors\nacross various domains such as engineering application, health monitoring, and\nthe internet of things, characterized by its temporal changes and high\ndimensional characteristics. Over the past few years, many studies have\nexplored the long-range dependencies and similarities in MTS. However,\nlong-range dependencies are difficult to model due to their temporal changes\nand high dimensionality makes it difficult to obtain similarities effectively\nand efficiently. Thus, to address these issues, we propose contrast\nsimilarity-aware dual-pathway Mamba for MTS node classification (CS-DPMamba).\nFirstly, to obtain the dynamic similarity of each sample, we initially use\ntemporal contrast learning module to acquire MTS representations. And then we\nconstruct a similarity matrix between MTS representations using Fast Dynamic\nTime Warping (FastDTW). Secondly, we apply the DPMamba to consider the\nbidirectional nature of MTS, allowing us to better capture long-range and\nshort-range dependencies within the data. Finally, we utilize the\nKolmogorov-Arnold Network enhanced Graph Isomorphism Network to complete the\ninformation interaction in the matrix and MTS node classification task. By\ncomprehensively considering the long-range dependencies and dynamic similarity\nfeatures, we achieved precise MTS node classification. We conducted experiments\non multiple University of East Anglia (UEA) MTS datasets, which encompass\ndiverse application scenarios. Our results demonstrate the superiority of our\nmethod through both supervised and semi-supervised experiments on the MTS\nclassification task.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Submitted to Knowledge-Based Systems on Nov 17, 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.12222v1",
    "published_date": "2024-11-19 04:32:41 UTC",
    "updated_date": "2024-11-19 04:32:41 UTC"
  },
  {
    "arxiv_id": "2411.12220v2",
    "title": "DeTrigger: A Gradient-Centric Approach to Backdoor Attack Mitigation in Federated Learning",
    "authors": [
      "Kichang Lee",
      "Yujin Shin",
      "Jonghyuk Yun",
      "Songkuk Kim",
      "Jun Han",
      "JeongGil Ko"
    ],
    "abstract": "Federated Learning (FL) enables collaborative model training across\ndistributed devices while preserving local data privacy, making it ideal for\nmobile and embedded systems. However, the decentralized nature of FL also opens\nvulnerabilities to model poisoning attacks, particularly backdoor attacks,\nwhere adversaries implant trigger patterns to manipulate model predictions. In\nthis paper, we propose DeTrigger, a scalable and efficient backdoor-robust\nfederated learning framework that leverages insights from adversarial attack\nmethodologies. By employing gradient analysis with temperature scaling,\nDeTrigger detects and isolates backdoor triggers, allowing for precise model\nweight pruning of backdoor activations without sacrificing benign model\nknowledge. Extensive evaluations across four widely used datasets demonstrate\nthat DeTrigger achieves up to 251x faster detection than traditional methods\nand mitigates backdoor attacks by up to 98.9%, with minimal impact on global\nmodel accuracy. Our findings establish DeTrigger as a robust and scalable\nsolution to protect federated learning environments against sophisticated\nbackdoor threats.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "68T07",
      "I.2.11"
    ],
    "primary_category": "cs.LG",
    "comment": "21 pages",
    "pdf_url": "http://arxiv.org/pdf/2411.12220v2",
    "published_date": "2024-11-19 04:12:14 UTC",
    "updated_date": "2025-02-03 16:20:25 UTC"
  },
  {
    "arxiv_id": "2411.12198v2",
    "title": "CCIS-Diff: A Generative Model with Stable Diffusion Prior for Controlled Colonoscopy Image Synthesis",
    "authors": [
      "Yifan Xie",
      "Jingge Wang",
      "Tao Feng",
      "Fei Ma",
      "Yang Li"
    ],
    "abstract": "Colonoscopy is crucial for identifying adenomatous polyps and preventing\ncolorectal cancer. However, developing robust models for polyp detection is\nchallenging by the limited size and accessibility of existing colonoscopy\ndatasets. While previous efforts have attempted to synthesize colonoscopy\nimages, current methods suffer from instability and insufficient data\ndiversity. Moreover, these approaches lack precise control over the generation\nprocess, resulting in images that fail to meet clinical quality standards. To\naddress these challenges, we propose CCIS-DIFF, a Controlled generative model\nfor high-quality Colonoscopy Image Synthesis based on a Diffusion architecture.\nOur method offers precise control over both the spatial attributes (polyp\nlocation and shape) and clinical characteristics of polyps that align with\nclinical descriptions. Specifically, we introduce a blur mask weighting\nstrategy to seamlessly blend synthesized polyps with the colonic mucosa, and a\ntext-aware attention mechanism to guide the generated images to reflect\nclinical characteristics. Notably, to achieve this, we construct a new\nmulti-modal colonoscopy dataset that integrates images, mask annotations, and\ncorresponding clinical text descriptions. Experimental results demonstrate that\nour method generates high-quality, diverse colonoscopy images with fine control\nover both spatial constraints and clinical consistency, offering valuable\nsupport for downstream segmentation and diagnostic tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "5 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.12198v2",
    "published_date": "2024-11-19 03:30:06 UTC",
    "updated_date": "2025-01-05 14:10:02 UTC"
  },
  {
    "arxiv_id": "2411.12196v2",
    "title": "A More Advanced Group Polarization Measurement Approach Based on LLM-Based Agents and Graphs",
    "authors": [
      "Zixin Liu",
      "Ji Zhang",
      "Yiran Ding"
    ],
    "abstract": "Group polarization is an important research direction in social media content\nanalysis, attracting many researchers to explore this field. Therefore, how to\neffectively measure group polarization has become a critical topic. Measuring\ngroup polarization on social media presents several challenges that have not\nyet been addressed by existing solutions. First, social media group\npolarization measurement involves processing vast amounts of text, which poses\na significant challenge for information extraction. Second, social media texts\noften contain hard-to-understand content, including sarcasm, memes, and\ninternet slang. Additionally, group polarization research focuses on holistic\nanalysis, while texts is typically fragmented. To address these challenges, we\ndesigned a solution based on a multi-agent system and used a graph-structured\nCommunity Sentiment Network (CSN) to represent polarization states.\nFurthermore, we developed a metric called Community Opposition Index (COI)\nbased on the CSN to quantify polarization. Finally, we tested our multi-agent\nsystem through a zero-shot stance detection task and achieved outstanding\nresults. In summary, the proposed approach has significant value in terms of\nusability, accuracy, and interpretability.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12196v2",
    "published_date": "2024-11-19 03:29:17 UTC",
    "updated_date": "2024-12-16 12:13:49 UTC"
  },
  {
    "arxiv_id": "2411.15185v1",
    "title": "Hybrid Gaussian Process Regression with Temporal Feature Extraction for Partially Interpretable Remaining Useful Life Interval Prediction in Aeroengine Prognostics",
    "authors": [
      "Tian Niu",
      "Zijun Xu",
      "Heng Luo",
      "Ziqing Zhou"
    ],
    "abstract": "The estimation of Remaining Useful Life (RUL) plays a pivotal role in\nintelligent manufacturing systems and Industry 4.0 technologies. While recent\nadvancements have improved RUL prediction, many models still face\ninterpretability and compelling uncertainty modeling challenges. This paper\nintroduces a modified Gaussian Process Regression (GPR) model for RUL interval\nprediction, tailored for the complexities of manufacturing process development.\nThe modified GPR predicts confidence intervals by learning from historical data\nand addresses uncertainty modeling in a more structured way. The approach\neffectively captures intricate time-series patterns and dynamic behaviors\ninherent in modern manufacturing systems by coupling GPR with deep adaptive\nlearning-enhanced AI process models. Moreover, the model evaluates feature\nsignificance to ensure more transparent decision-making, which is crucial for\noptimizing manufacturing processes. This comprehensive approach supports more\naccurate RUL predictions and provides transparent, interpretable insights into\nuncertainty, contributing to robust process development and management.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15185v1",
    "published_date": "2024-11-19 03:00:02 UTC",
    "updated_date": "2024-11-19 03:00:02 UTC"
  },
  {
    "arxiv_id": "2411.12184v1",
    "title": "Testability of Instrumental Variables in Additive Nonlinear, Non-Constant Effects Models",
    "authors": [
      "Xichen Guo",
      "Zheng Li",
      "Biwei Huang",
      "Yan Zeng",
      "Zhi Geng",
      "Feng Xie"
    ],
    "abstract": "We address the issue of the testability of instrumental variables derived\nfrom observational data. Most existing testable implications are centered on\nscenarios where the treatment is a discrete variable, e.g., instrumental\ninequality (Pearl, 1995), or where the effect is assumed to be constant, e.g.,\ninstrumental variables condition based on the principle of independent\nmechanisms (Burauel, 2023). However, treatments can often be continuous\nvariables, such as drug dosages or nutritional content levels, and non-constant\neffects may occur in many real-world scenarios. In this paper, we consider an\nadditive nonlinear, non-constant effects model with unmeasured confounders, in\nwhich treatments can be either discrete or continuous, and propose an\nAuxiliary-based Independence Test (AIT) condition to test whether a variable is\na valid instrument. We first show that if the candidate instrument is valid,\nthen the AIT condition holds. Moreover, we illustrate the implications of the\nAIT condition and demonstrate that, in certain conditions, AIT conditions are\nnecessary and sufficient to detect all invalid IVs. We also extend the AIT\ncondition to include covariates and introduce a practical testing algorithm.\nExperimental results on both synthetic and three different real-world datasets\nshow the effectiveness of our proposed condition.",
    "categories": [
      "stat.ME",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ME",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12184v1",
    "published_date": "2024-11-19 02:56:45 UTC",
    "updated_date": "2024-11-19 02:56:45 UTC"
  },
  {
    "arxiv_id": "2411.12182v1",
    "title": "Diffusion-Inspired Cold Start with Sufficient Prior in Computerized Adaptive Testing",
    "authors": [
      "Haiping Ma",
      "Aoqing Xia",
      "Changqian Wang",
      "Hai Wang",
      "Xingyi Zhang"
    ],
    "abstract": "Computerized Adaptive Testing (CAT) aims to select the most appropriate\nquestions based on the examinee's ability and is widely used in online\neducation. However, existing CAT systems often lack initial understanding of\nthe examinee's ability, requiring random probing questions. This can lead to\npoorly matched questions, extending the test duration and negatively impacting\nthe examinee's mindset, a phenomenon referred to as the Cold Start with\nInsufficient Prior (CSIP) task. This issue occurs because CAT systems do not\neffectively utilize the abundant prior information about the examinee available\nfrom other courses on online platforms. These response records, due to the\ncommonality of cognitive states across different knowledge domains, can provide\nvaluable prior information for the target domain. However, no prior work has\nexplored solutions for the CSIP task. In response to this gap, we propose\nDiffusion Cognitive States TransfeR Framework (DCSR), a novel domain transfer\nframework based on Diffusion Models (DMs) to address the CSIP task.\nSpecifically, we construct a cognitive state transition bridge between domains,\nguided by the common cognitive states of examinees, encouraging the model to\nreconstruct the initial ability state in the target domain. To enrich the\nexpressive power of the generated data, we analyze the causal relationships in\nthe generation process from a causal perspective. Redundant and extraneous\ncognitive states can lead to limited transfer and negative transfer effects.\nOur DCSR can seamlessly apply the generated initial ability states in the\ntarget domain to existing question selection algorithms, thus improving the\ncold start performance of the CAT system. Extensive experiments conducted on\nfive real-world datasets demonstrate that DCSR significantly outperforms\nexisting baseline methods in addressing the CSIP task.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by KDD2025",
    "pdf_url": "http://arxiv.org/pdf/2411.12182v1",
    "published_date": "2024-11-19 02:48:58 UTC",
    "updated_date": "2024-11-19 02:48:58 UTC"
  },
  {
    "arxiv_id": "2411.12181v1",
    "title": "Enhancing Low Dose Computed Tomography Images Using Consistency Training Techniques",
    "authors": [
      "Mahmut S. Gokmen",
      "Jie Zhang",
      "Ge Wang",
      "Jin Chen",
      "Cody Bumgardner"
    ],
    "abstract": "Diffusion models have significant impact on wide range of generative tasks,\nespecially on image inpainting and restoration. Although the improvements on\naiming for decreasing number of function evaluations (NFE), the iterative\nresults are still computationally expensive. Consistency models are as a new\nfamily of generative models, enable single-step sampling of high quality data\nwithout the need for adversarial training. In this paper, we introduce the beta\nnoise distribution, which provides flexibility in adjusting noise levels. This\nis combined with a sinusoidal curriculum that enhances the learning of the\ntrajectory between the noise distribution and the posterior distribution of\ninterest, allowing High Noise Improved Consistency Training (HN-iCT) to be\ntrained in a supervised fashion. Additionally, High Noise Improved Consistency\nTraining with Image Condition (HN-iCT-CN) architecture is introduced, enables\nto take Low Dose images as a condition for extracting significant features by\nWeighted Attention Gates (WAG).Our results indicate that unconditional image\ngeneration using HN-iCT significantly outperforms basic CT and iCT training\ntechniques with NFE=1 on the CIFAR10 and CelebA datasets. Moreover, our\nimage-conditioned model demonstrates exceptional performance in enhancing\nlow-dose (LD) CT scans.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12181v1",
    "published_date": "2024-11-19 02:48:36 UTC",
    "updated_date": "2024-11-19 02:48:36 UTC"
  },
  {
    "arxiv_id": "2411.12174v2",
    "title": "Just KIDDIN: Knowledge Infusion and Distillation for Detection of INdecent Memes",
    "authors": [
      "Rahul Garg",
      "Trilok Padhi",
      "Hemang Jain",
      "Ugur Kursuncu",
      "Ponnurangam Kumaraguru"
    ],
    "abstract": "Toxicity identification in online multimodal environments remains a\nchallenging task due to the complexity of contextual connections across\nmodalities (e.g., textual and visual). In this paper, we propose a novel\nframework that integrates Knowledge Distillation (KD) from Large Visual\nLanguage Models (LVLMs) and knowledge infusion to enhance the performance of\ntoxicity detection in hateful memes. Our approach extracts sub-knowledge graphs\nfrom ConceptNet, a large-scale commonsense Knowledge Graph (KG) to be infused\nwithin a compact VLM framework. The relational context between toxic phrases in\ncaptions and memes, as well as visual concepts in memes enhance the model's\nreasoning capabilities. Experimental results from our study on two hate speech\nbenchmark datasets demonstrate superior performance over the state-of-the-art\nbaselines across AU-ROC, F1, and Recall with improvements of 1.1%, 7%, and 35%,\nrespectively. Given the contextual complexity of the toxicity detection task,\nour approach showcases the significance of learning from both explicit (i.e.\nKG) as well as implicit (i.e. LVLMs) contextual cues incorporated through a\nhybrid neurosymbolic approach. This is crucial for real-world applications\nwhere accurate and scalable recognition of toxic content is critical for\ncreating safer online environments.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12174v2",
    "published_date": "2024-11-19 02:39:28 UTC",
    "updated_date": "2025-02-24 06:19:39 UTC"
  },
  {
    "arxiv_id": "2411.12173v1",
    "title": "SkillTree: Explainable Skill-Based Deep Reinforcement Learning for Long-Horizon Control Tasks",
    "authors": [
      "Yongyan Wen",
      "Siyuan Li",
      "Rongchang Zuo",
      "Lei Yuan",
      "Hangyu Mao",
      "Peng Liu"
    ],
    "abstract": "Deep reinforcement learning (DRL) has achieved remarkable success in various\nresearch domains. However, its reliance on neural networks results in a lack of\ntransparency, which limits its practical applications. To achieve\nexplainability, decision trees have emerged as a popular and promising\nalternative to neural networks. Nonetheless, due to their limited\nexpressiveness, traditional decision trees struggle with high-dimensional\nlong-horizon continuous control tasks. In this paper, we proposes SkillTree, a\nnovel framework that reduces complex continuous action spaces into discrete\nskill spaces. Our hierarchical approach integrates a differentiable decision\ntree within the high-level policy to generate skill embeddings, which\nsubsequently guide the low-level policy in executing skills. By making skill\ndecisions explainable, we achieve skill-level explainability, enhancing the\nunderstanding of the decision-making process in complex tasks. Experimental\nresults demonstrate that our method achieves performance comparable to\nskill-based neural networks in complex robotic arm control domains.\nFurthermore, SkillTree offers explanations at the skill level, thereby\nincreasing the transparency of the decision-making process.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12173v1",
    "published_date": "2024-11-19 02:35:14 UTC",
    "updated_date": "2024-11-19 02:35:14 UTC"
  },
  {
    "arxiv_id": "2411.15183v1",
    "title": "Balancing property optimization and constraint satisfaction for constrained multi-property molecular optimization",
    "authors": [
      "Xin Xia",
      "Yajie Zhang",
      "Xiangxiang Zeng",
      "Xingyi Zhang",
      "Chunhou Zheng",
      "Yansen Su"
    ],
    "abstract": "Molecular optimization, which aims to discover improved molecules from a vast\nchemical search space, is a critical step in chemical development. Various\nartificial intelligence technologies have demonstrated high effectiveness and\nefficiency on molecular optimization tasks. However, few of these technologies\nfocus on balancing property optimization with constraint satisfaction, making\nit difficult to obtain high-quality molecules that not only possess desirable\nproperties but also meet various constraints. To address this issue, we propose\na constrained multi-property molecular optimization framework (CMOMO), which is\na flexible and efficient method to simultaneously optimize multiple molecular\nproperties while satisfying several drug-like constraints. CMOMO improves\nmultiple properties of molecules with constraints based on dynamic cooperative\noptimization, which dynamically handles the constraints across various\nscenarios. Besides, CMOMO evaluates multiple properties within discrete\nchemical spaces cooperatively with the evolution of molecules within an\nimplicit molecular space to guide the evolutionary search. Experimental results\nshow the superior performance of the proposed CMOMO over five state-of-the-art\nmolecular optimization methods on two benchmark tasks of simultaneously\noptimizing multiple non-biological activity properties while satisfying two\nstructural constraints. Furthermore, the practical applicability of CMOMO is\nverified on two practical tasks, where it identified a collection of candidate\nligands of $\\beta$2-adrenoceptor GPCR and candidate inhibitors of glycogen\nsynthase kinase-3$\\beta$ with high properties and under drug-like constraints.",
    "categories": [
      "physics.chem-ph",
      "cs.AI",
      "q-bio.BM"
    ],
    "primary_category": "physics.chem-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15183v1",
    "published_date": "2024-11-19 02:01:13 UTC",
    "updated_date": "2024-11-19 02:01:13 UTC"
  },
  {
    "arxiv_id": "2411.12164v1",
    "title": "UrbanDiT: A Foundation Model for Open-World Urban Spatio-Temporal Learning",
    "authors": [
      "Yuan Yuan",
      "Chonghua Han",
      "Jingtao Ding",
      "Depeng Jin",
      "Yong Li"
    ],
    "abstract": "The urban environment is characterized by complex spatio-temporal dynamics\narising from diverse human activities and interactions. Effectively modeling\nthese dynamics is essential for understanding and optimizing urban systems In\nthis work, we introduce UrbanDiT, a foundation model for open-world urban\nspatio-temporal learning that successfully scale up diffusion transformers in\nthis field. UrbanDiT pioneers a unified model that integrates diverse\nspatio-temporal data sources and types while learning universal spatio-temporal\npatterns across different cities and scenarios. This allows the model to unify\nboth multi-data and multi-task learning, and effectively support a wide range\nof spatio-temporal applications. Its key innovation lies in the elaborated\nprompt learning framework, which adaptively generates both data-driven and\ntask-specific prompts, guiding the model to deliver superior performance across\nvarious urban applications. UrbanDiT offers three primary advantages: 1) It\nunifies diverse data types, such as grid-based and graph-based data, into a\nsequential format, allowing to capture spatio-temporal dynamics across diverse\nscenarios of different cities; 2) With masking strategies and task-specific\nprompts, it supports a wide range of tasks, including bi-directional\nspatio-temporal prediction, temporal interpolation, spatial extrapolation, and\nspatio-temporal imputation; and 3) It generalizes effectively to open-world\nscenarios, with its powerful zero-shot capabilities outperforming nearly all\nbaselines with training data. These features allow UrbanDiT to achieves\nstate-of-the-art performance in different domains such as transportation\ntraffic, crowd flows, taxi demand, bike usage, and cellular traffic, across\nmultiple cities and tasks. UrbanDiT sets up a new benchmark for foundation\nmodels in the urban spatio-temporal domain.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12164v1",
    "published_date": "2024-11-19 02:01:07 UTC",
    "updated_date": "2024-11-19 02:01:07 UTC"
  },
  {
    "arxiv_id": "2411.12156v1",
    "title": "HNCSE: Advancing Sentence Embeddings via Hybrid Contrastive Learning with Hard Negatives",
    "authors": [
      "Wenxiao Liu",
      "Zihong Yang",
      "Chaozhuo Li",
      "Zijin Hong",
      "Jianfeng Ma",
      "Zhiquan Liu",
      "Litian Zhang",
      "Feiran Huang"
    ],
    "abstract": "Unsupervised sentence representation learning remains a critical challenge in\nmodern natural language processing (NLP) research. Recently, contrastive\nlearning techniques have achieved significant success in addressing this issue\nby effectively capturing textual semantics. Many such approaches prioritize the\noptimization using negative samples. In fields such as computer vision, hard\nnegative samples (samples that are close to the decision boundary and thus more\ndifficult to distinguish) have been shown to enhance representation learning.\nHowever, adapting hard negatives to contrastive sentence learning is complex\ndue to the intricate syntactic and semantic details of text. To address this\nproblem, we propose HNCSE, a novel contrastive learning framework that extends\nthe leading SimCSE approach. The hallmark of HNCSE is its innovative use of\nhard negative samples to enhance the learning of both positive and negative\nsamples, thereby achieving a deeper semantic understanding. Empirical tests on\nsemantic textual similarity and transfer task datasets validate the superiority\nof HNCSE.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12156v1",
    "published_date": "2024-11-19 01:26:20 UTC",
    "updated_date": "2024-11-19 01:26:20 UTC"
  },
  {
    "arxiv_id": "2411.12155v3",
    "title": "Coarse-to-fine Q-Network with Action Sequence for Data-Efficient Robot Learning",
    "authors": [
      "Younggyo Seo",
      "Pieter Abbeel"
    ],
    "abstract": "In reinforcement learning (RL), we train a value function to understand the\nlong-term consequence of executing a single action. However, the value of\ntaking each action can be ambiguous in robotics as robot movements are\ntypically the aggregate result of executing multiple small actions. Moreover,\nrobotic training data often consists of noisy trajectories, in which each\naction is noisy but executing a series of actions results in a meaningful robot\nmovement. This further makes it difficult for the value function to understand\nthe effect of individual actions. To address this, we introduce Coarse-to-fine\nQ-Network with Action Sequence (CQN-AS), a novel value-based RL algorithm that\nlearns a critic network that outputs Q-values over a sequence of actions, i.e.,\nexplicitly training the value function to learn the consequence of executing\naction sequences. We study our algorithm on 53 robotic tasks with sparse and\ndense rewards, as well as with and without demonstrations, from BiGym,\nHumanoidBench, and RLBench. We find that CQN-AS outperforms various baselines,\nin particular on humanoid control tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "15 Pages. Website: https://younggyo.me/cqn-as/",
    "pdf_url": "http://arxiv.org/pdf/2411.12155v3",
    "published_date": "2024-11-19 01:23:52 UTC",
    "updated_date": "2025-02-01 04:09:07 UTC"
  },
  {
    "arxiv_id": "2411.15182v1",
    "title": "Forecasting Application Counts in Talent Acquisition Platforms: Harnessing Multimodal Signals using LMs",
    "authors": [
      "Md Ahsanul Kabir",
      "Kareem Abdelfatah",
      "Shushan He",
      "Mohammed Korayem",
      "Mohammad Al Hasan"
    ],
    "abstract": "As recruitment and talent acquisition have become more and more competitive,\nrecruitment firms have become more sophisticated in using machine learning (ML)\nmethodologies for optimizing their day to day activities. But, most of\npublished ML based methodologies in this area have been limited to the tasks\nlike candidate matching, job to skill matching, job classification and\nnormalization. In this work, we discuss a novel task in the recruitment domain,\nnamely, application count forecasting, motivation of which comes from designing\nof effective outreach activities to attract qualified applicants. We show that\nexisting auto-regressive based time series forecasting methods perform poorly\nfor this task. Henceforth, we propose a multimodal LM-based model which fuses\njob-posting metadata of various modalities through a simple encoder.\nExperiments from large real-life datasets from CareerBuilder LLC show the\neffectiveness of the proposed method over existing state-of-the-art methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15182v1",
    "published_date": "2024-11-19 01:18:32 UTC",
    "updated_date": "2024-11-19 01:18:32 UTC"
  },
  {
    "arxiv_id": "2412.03581v1",
    "title": "A Survey on E-Commerce Learning to Rank",
    "authors": [
      "Md. Ahsanul Kabir",
      "Mohammad Al Hasan",
      "Aritra Mandal",
      "Daniel Tunkelang",
      "Zhe Wu"
    ],
    "abstract": "In e-commerce, ranking the search results based on users' preference is the\nmost important task. Commercial e-commerce platforms, such as, Amazon, Alibaba,\neBay, Walmart, etc. perform extensive and relentless research to perfect their\nsearch result ranking algorithms because the quality of ranking drives a user's\ndecision to purchase or not to purchase an item, directly affecting the\nprofitability of the e-commerce platform. In such a commercial platforms, for\noptimizing search result ranking numerous features are considered, which emerge\nfrom relevance, personalization, seller's reputation and paid promotion. To\nmaintain their competitive advantage in the market, the platforms do no publish\ntheir core ranking algorithms, so it is difficult to know which of the\nalgorithms or which of the features is the most effective for finding the most\noptimal search result ranking in e-commerce. No extensive surveys of ranking to\nrank in the e-commerce domain is also not yet published. In this work, we\nsurvey the existing e-commerce learning to rank algorithms. Besides, we also\ncompare these algorithms based on query relevance criterion on a large\nreal-life e-commerce dataset and provide a quantitative analysis. To the best\nof our knowledge this is the first such survey which include an experimental\ncomparison among various learning to rank algorithms.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.03581v1",
    "published_date": "2024-11-19 01:12:51 UTC",
    "updated_date": "2024-11-19 01:12:51 UTC"
  },
  {
    "arxiv_id": "2411.12150v2",
    "title": "HEIGHT: Heterogeneous Interaction Graph Transformer for Robot Navigation in Crowded and Constrained Environments",
    "authors": [
      "Shuijing Liu",
      "Haochen Xia",
      "Fatemeh Cheraghi Pouria",
      "Kaiwen Hong",
      "Neeloy Chakraborty",
      "Zichao Hu",
      "Joydeep Biswas",
      "Katherine Driggs-Campbell"
    ],
    "abstract": "We study the problem of robot navigation in dense and interactive crowds with\nenvironmental constraints such as corridors and furniture. Previous methods\nfail to consider all types of interactions among agents and obstacles, leading\nto unsafe and inefficient robot paths. In this article, we leverage a\ngraph-based representation of crowded and constrained scenarios and propose a\nstructured framework to learn robot navigation policies with deep reinforcement\nlearning. We first split the representations of different components in the\nenvironment and propose a heterogeneous spatio-temporal (st) graph to model\ndistinct interactions among humans, robots, and obstacles. Based on the\nheterogeneous st-graph, we propose HEIGHT, a novel navigation policy network\narchitecture with different components to capture heterogeneous interactions\namong entities through space and time. HEIGHT utilizes attention mechanisms to\nprioritize important interactions and a recurrent network to track changes in\nthe dynamic scene over time, encouraging the robot to avoid collisions\nadaptively. Through extensive simulation and real-world experiments, we\ndemonstrate that HEIGHT outperforms state-of-the-art baselines in terms of\nsuccess and efficiency in challenging navigation scenarios. Furthermore, we\ndemonstrate that our pipeline achieves better zero-shot generalization\ncapability than previous works when the densities of humans and obstacles\nchange. More videos are available at\nhttps://sites.google.com/view/crowdnav-height/home.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12150v2",
    "published_date": "2024-11-19 00:56:35 UTC",
    "updated_date": "2025-05-01 20:03:05 UTC"
  },
  {
    "arxiv_id": "2411.12142v2",
    "title": "A Computational Method for Measuring \"Open Codes\" in Qualitative Analysis",
    "authors": [
      "John Chen",
      "Alexandros Lotsos",
      "Lexie Zhao",
      "Caiyi Wang",
      "Jessica Hullman",
      "Bruce Sherin",
      "Uri Wilensky",
      "Michael Horn"
    ],
    "abstract": "Qualitative analysis is critical to understanding human datasets in many\nsocial science disciplines. Open coding is an inductive qualitative process\nthat identifies and interprets \"open codes\" from datasets. Yet, meeting\nmethodological expectations (such as \"as exhaustive as possible\") can be\nchallenging. While many machine learning (ML)/generative AI (GAI) studies have\nattempted to support open coding, few have systematically measured or evaluated\nGAI outcomes, increasing potential bias risks. Building on Grounded Theory and\nThematic Analysis theories, we present a computational method to measure and\nidentify potential biases from \"open codes\" systematically. Instead of\noperationalizing human expert results as the \"ground truth,\" our method is\nbuilt upon a team-based approach between human and machine coders. We\nexperiment with two HCI datasets to establish this method's reliability by 1)\ncomparing it with human analysis, and 2) analyzing its output stability. We\npresent evidence-based suggestions and example workflows for ML/GAI to support\nopen coding.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12142v2",
    "published_date": "2024-11-19 00:44:56 UTC",
    "updated_date": "2024-11-26 04:31:07 UTC"
  },
  {
    "arxiv_id": "2411.12136v1",
    "title": "Visualizing Loss Functions as Topological Landscape Profiles",
    "authors": [
      "Caleb Geniesse",
      "Jiaqing Chen",
      "Tiankai Xie",
      "Ge Shi",
      "Yaoqing Yang",
      "Dmitriy Morozov",
      "Talita Perciano",
      "Michael W. Mahoney",
      "Ross Maciejewski",
      "Gunther H. Weber"
    ],
    "abstract": "In machine learning, a loss function measures the difference between model\npredictions and ground-truth (or target) values. For neural network models,\nvisualizing how this loss changes as model parameters are varied can provide\ninsights into the local structure of the so-called loss landscape (e.g.,\nsmoothness) as well as global properties of the underlying model (e.g.,\ngeneralization performance). While various methods for visualizing the loss\nlandscape have been proposed, many approaches limit sampling to just one or two\ndirections, ignoring potentially relevant information in this extremely\nhigh-dimensional space. This paper introduces a new representation based on\ntopological data analysis that enables the visualization of higher-dimensional\nloss landscapes. After describing this new topological landscape profile\nrepresentation, we show how the shape of loss landscapes can reveal new details\nabout model performance and learning dynamics, highlighting several use cases,\nincluding image segmentation (e.g., UNet) and scientific machine learning\n(e.g., physics-informed neural networks). Through these examples, we provide\nnew insights into how loss landscapes vary across distinct hyperparameter\nspaces: we find that the topology of the loss landscape is simpler for\nbetter-performing models; and we observe greater variation in the shape of loss\nlandscapes near transitions from low to high model performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12136v1",
    "published_date": "2024-11-19 00:28:14 UTC",
    "updated_date": "2024-11-19 00:28:14 UTC"
  }
]
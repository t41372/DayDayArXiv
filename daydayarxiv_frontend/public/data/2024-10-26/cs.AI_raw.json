[
  {
    "arxiv_id": "2410.20287v1",
    "title": "AI-Driven Cyber Threat Intelligence Automation",
    "authors": [
      "Shrit Shah",
      "Fatemeh Khoda Parast"
    ],
    "abstract": "This study introduces an innovative approach to automating Cyber Threat\nIntelligence (CTI) processes in industrial environments by leveraging\nMicrosoft's AI-powered security technologies. Historically, CTI has heavily\nrelied on manual methods for collecting, analyzing, and interpreting data from\nvarious sources such as threat feeds. This study introduces an innovative\napproach to automating CTI processes in industrial environments by leveraging\nMicrosoft's AI-powered security technologies. Historically, CTI has heavily\nrelied on manual methods for collecting, analyzing, and interpreting data from\nvarious sources such as threat feeds, security logs, and dark web forums -- a\nprocess prone to inefficiencies, especially when rapid information\ndissemination is critical. By employing the capabilities of GPT-4o and advanced\none-shot fine-tuning techniques for large language models, our research\ndelivers a novel CTI automation solution. The outcome of the proposed\narchitecture is a reduction in manual effort while maintaining precision in\ngenerating final CTI reports. This research highlights the transformative\npotential of AI-driven technologies to enhance both the speed and accuracy of\nCTI and reduce expert demands, offering a vital advantage in today's dynamic\nthreat landscape.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CR",
    "comment": "11 pages",
    "pdf_url": "http://arxiv.org/pdf/2410.20287v1",
    "published_date": "2024-10-26 22:56:53 UTC",
    "updated_date": "2024-10-26 22:56:53 UTC"
  },
  {
    "arxiv_id": "2410.20285v6",
    "title": "SWE-Search: Enhancing Software Agents with Monte Carlo Tree Search and Iterative Refinement",
    "authors": [
      "Antonis Antoniades",
      "Albert Örwall",
      "Kexun Zhang",
      "Yuxi Xie",
      "Anirudh Goyal",
      "William Wang"
    ],
    "abstract": "Software engineers operating in complex and dynamic environments must\ncontinuously adapt to evolving requirements, learn iteratively from experience,\nand reconsider their approaches based on new insights. However, current large\nlanguage model (LLM)-based software agents often follow linear, sequential\nprocesses that prevent backtracking and exploration of alternative solutions,\nlimiting their ability to rethink their strategies when initial approaches\nprove ineffective. To address these challenges, we propose SWE-Search, a\nmulti-agent framework that integrates Monte Carlo Tree Search (MCTS) with a\nself-improvement mechanism to enhance software agents' performance on\nrepository-level software tasks. SWE-Search extends traditional MCTS by\nincorporating a hybrid value function that leverages LLMs for both numerical\nvalue estimation and qualitative evaluation. This enables self-feedback loops\nwhere agents iteratively refine their strategies based on both quantitative\nnumerical evaluations and qualitative natural language assessments of pursued\ntrajectories. The framework includes a SWE-Agent for adaptive exploration, a\nValue Agent for iterative feedback, and a Discriminator Agent that facilitates\nmulti-agent debate for collaborative decision-making. Applied to the SWE-bench\nbenchmark, our approach demonstrates a 23% relative improvement in performance\nacross five models compared to standard open-source agents without MCTS. Our\nanalysis reveals how performance scales with increased inference-time compute\nthrough deeper search, providing a pathway to improve software agents without\nrequiring larger models or additional training data. This highlights the\npotential of self-evaluation driven search techniques in complex software\nengineering environments.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Main body: 10 pages, 5 figures. Appendix: 5 pages, 4 figures.\n  Open-source codebase",
    "pdf_url": "http://arxiv.org/pdf/2410.20285v6",
    "published_date": "2024-10-26 22:45:56 UTC",
    "updated_date": "2025-04-02 04:13:19 UTC"
  },
  {
    "arxiv_id": "2411.08885v1",
    "title": "Enhancing Lie Detection Accuracy: A Comparative Study of Classic ML, CNN, and GCN Models using Audio-Visual Features",
    "authors": [
      "Abdelrahman Abdelwahab",
      "Akshaj Vishnubhatla",
      "Ayaan Vaswani",
      "Advait Bharathulwar",
      "Arnav Kommaraju"
    ],
    "abstract": "Inaccuracies in polygraph tests often lead to wrongful convictions, false\ninformation, and bias, all of which have significant consequences for both\nlegal and political systems. Recently, analyzing facial micro-expressions has\nemerged as a method for detecting deception; however, current models have not\nreached high accuracy and generalizability. The purpose of this study is to aid\nin remedying these problems. The unique multimodal transformer architecture\nused in this study improves upon previous approaches by using auditory inputs,\nvisual facial micro-expressions, and manually transcribed gesture annotations,\nmoving closer to a reliable non-invasive lie detection model. Visual and\nauditory features were extracted using the Vision Transformer and OpenSmile\nmodels respectively, which were then concatenated with the transcriptions of\nparticipants micro-expressions and gestures. Various models were trained for\nthe classification of lies and truths using these processed and concatenated\nfeatures. The CNN Conv1D multimodal model achieved an average accuracy of\n95.4%. However, further research is still required to create higher-quality\ndatasets and even more generalized models for more diverse applications.",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.CV",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.MM",
    "comment": "11 pages, 18 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.08885v1",
    "published_date": "2024-10-26 22:17:36 UTC",
    "updated_date": "2024-10-26 22:17:36 UTC"
  },
  {
    "arxiv_id": "2410.21325v1",
    "title": "Just Propagate: Unifying Matrix Factorization, Network Embedding, and LightGCN for Link Prediction",
    "authors": [
      "Haoxin Liu"
    ],
    "abstract": "Link prediction is a fundamental task in graph analysis. Despite the success\nof various graph-based machine learning models for link prediction, there lacks\na general understanding of different models. In this paper, we propose a\nunified framework for link prediction that covers matrix factorization and\nrepresentative network embedding and graph neural network methods. Our\npreliminary methodological and empirical analyses further reveal several key\ndesign factors based on our unified framework. We believe our results could\ndeepen our understanding and inspire novel designs for link prediction methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.21325v1",
    "published_date": "2024-10-26 21:43:34 UTC",
    "updated_date": "2024-10-26 21:43:34 UTC"
  },
  {
    "arxiv_id": "2410.20280v1",
    "title": "MarDini: Masked Autoregressive Diffusion for Video Generation at Scale",
    "authors": [
      "Haozhe Liu",
      "Shikun Liu",
      "Zijian Zhou",
      "Mengmeng Xu",
      "Yanping Xie",
      "Xiao Han",
      "Juan C. Pérez",
      "Ding Liu",
      "Kumara Kahatapitiya",
      "Menglin Jia",
      "Jui-Chieh Wu",
      "Sen He",
      "Tao Xiang",
      "Jürgen Schmidhuber",
      "Juan-Manuel Pérez-Rúa"
    ],
    "abstract": "We introduce MarDini, a new family of video diffusion models that integrate\nthe advantages of masked auto-regression (MAR) into a unified diffusion model\n(DM) framework. Here, MAR handles temporal planning, while DM focuses on\nspatial generation in an asymmetric network design: i) a MAR-based planning\nmodel containing most of the parameters generates planning signals for each\nmasked frame using low-resolution input; ii) a lightweight generation model\nuses these signals to produce high-resolution frames via diffusion de-noising.\nMarDini's MAR enables video generation conditioned on any number of masked\nframes at any frame positions: a single model can handle video interpolation\n(e.g., masking middle frames), image-to-video generation (e.g., masking from\nthe second frame onward), and video expansion (e.g., masking half the frames).\nThe efficient design allocates most of the computational resources to the\nlow-resolution planning model, making computationally expensive but important\nspatio-temporal attention feasible at scale. MarDini sets a new\nstate-of-the-art for video interpolation; meanwhile, within few inference\nsteps, it efficiently generates videos on par with those of much more expensive\nadvanced image-to-video models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page: https://mardini-vidgen.github.io",
    "pdf_url": "http://arxiv.org/pdf/2410.20280v1",
    "published_date": "2024-10-26 21:12:32 UTC",
    "updated_date": "2024-10-26 21:12:32 UTC"
  },
  {
    "arxiv_id": "2410.20263v1",
    "title": "EfficientEQA: An Efficient Approach for Open Vocabulary Embodied Question Answering",
    "authors": [
      "Kai Cheng",
      "Zhengyuan Li",
      "Xingpeng Sun",
      "Byung-Cheol Min",
      "Amrit Singh Bedi",
      "Aniket Bera"
    ],
    "abstract": "Embodied Question Answering (EQA) is an essential yet challenging task for\nrobotic home assistants. Recent studies have shown that large vision-language\nmodels (VLMs) can be effectively utilized for EQA, but existing works either\nfocus on video-based question answering without embodied exploration or rely on\nclosed-form choice sets. In real-world scenarios, a robotic agent must\nefficiently explore and accurately answer questions in open-vocabulary\nsettings. To address these challenges, we propose a novel framework called\nEfficientEQA for open-vocabulary EQA, which enables efficient exploration and\naccurate answering. In EfficientEQA, the robot actively explores unknown\nenvironments using Semantic-Value-Weighted Frontier Exploration, a strategy\nthat prioritizes exploration based on semantic importance provided by\ncalibrated confidence from black-box VLMs to quickly gather relevant\ninformation. To generate accurate answers, we employ Retrieval-Augmented\nGeneration (RAG), which utilizes BLIP to retrieve useful images from\naccumulated observations and VLM reasoning to produce responses without relying\non predefined answer choices. Additionally, we detect observations that are\nhighly relevant to the question as outliers, allowing the robot to determine\nwhen it has sufficient information to stop exploring and provide an answer.\nExperimental results demonstrate the effectiveness of our approach, showing an\nimprovement in answering accuracy by over 15% and efficiency, measured in\nrunning steps, by over 20% compared to state-of-the-art methods.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.20263v1",
    "published_date": "2024-10-26 19:48:47 UTC",
    "updated_date": "2024-10-26 19:48:47 UTC"
  },
  {
    "arxiv_id": "2410.20255v1",
    "title": "Equivariant Blurring Diffusion for Hierarchical Molecular Conformer Generation",
    "authors": [
      "Jiwoong Park",
      "Yang Shen"
    ],
    "abstract": "How can diffusion models process 3D geometries in a coarse-to-fine manner,\nakin to our multiscale view of the world? In this paper, we address the\nquestion by focusing on a fundamental biochemical problem of generating 3D\nmolecular conformers conditioned on molecular graphs in a multiscale manner.\nOur approach consists of two hierarchical stages: i) generation of\ncoarse-grained fragment-level 3D structure from the molecular graph, and ii)\ngeneration of fine atomic details from the coarse-grained approximated\nstructure while allowing the latter to be adjusted simultaneously. For the\nchallenging second stage, which demands preserving coarse-grained information\nwhile ensuring SE(3) equivariance, we introduce a novel generative model termed\nEquivariant Blurring Diffusion (EBD), which defines a forward process that\nmoves towards the fragment-level coarse-grained structure by blurring the fine\natomic details of conformers, and a reverse process that performs the opposite\noperation using equivariant networks. We demonstrate the effectiveness of EBD\nby geometric and chemical comparison to state-of-the-art denoising diffusion\nmodels on a benchmark of drug-like molecules. Ablation studies draw insights on\nthe design of EBD by thoroughly analyzing its architecture, which includes the\ndesign of the loss function and the data corruption process. Codes are released\nat https://github.com/Shen-Lab/EBD .",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.chem-ph",
      "q-bio.BM"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.20255v1",
    "published_date": "2024-10-26 19:17:31 UTC",
    "updated_date": "2024-10-26 19:17:31 UTC"
  },
  {
    "arxiv_id": "2410.20252v1",
    "title": "Adaptive Video Understanding Agent: Enhancing efficiency with dynamic frame sampling and feedback-driven reasoning",
    "authors": [
      "Sullam Jeoung",
      "Goeric Huybrechts",
      "Bhavana Ganesh",
      "Aram Galstyan",
      "Sravan Bodapati"
    ],
    "abstract": "Understanding long-form video content presents significant challenges due to\nits temporal complexity and the substantial computational resources required.\nIn this work, we propose an agent-based approach to enhance both the efficiency\nand effectiveness of long-form video understanding by utilizing large language\nmodels (LLMs) and their tool-harnessing ability. A key aspect of our method is\nquery-adaptive frame sampling, which leverages the reasoning capabilities of\nLLMs to process only the most relevant frames in real-time, and addresses an\nimportant limitation of existing methods which typically involve sampling\nredundant or irrelevant frames. To enhance the reasoning abilities of our\nvideo-understanding agent, we leverage the self-reflective capabilities of LLMs\nto provide verbal reinforcement to the agent, which leads to improved\nperformance while minimizing the number of frames accessed. We evaluate our\nmethod across several video understanding benchmarks and demonstrate that not\nonly it enhances state-of-the-art performance but also improves efficiency by\nreducing the number of frames sampled.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.20252v1",
    "published_date": "2024-10-26 19:01:06 UTC",
    "updated_date": "2024-10-26 19:01:06 UTC"
  },
  {
    "arxiv_id": "2410.20245v2",
    "title": "Improving Model Evaluation using SMART Filtering of Benchmark Datasets",
    "authors": [
      "Vipul Gupta",
      "Candace Ross",
      "David Pantoja",
      "Rebecca J. Passonneau",
      "Megan Ung",
      "Adina Williams"
    ],
    "abstract": "One of the most challenging problems facing NLP today is evaluation. Some of\nthe most pressing issues pertain to benchmark saturation, data contamination,\nand diversity in the quality of test examples. To address these concerns, we\npropose Selection Methodology for Accurate, Reduced, and Targeted (SMART)\nfiltering, a novel approach to select a high-quality subset of examples from\nexisting benchmark datasets by systematically removing less informative and\nless challenging examples. Our approach applies three filtering criteria,\nremoving (i) easy examples, (ii) data-contaminated examples, and (iii) examples\nthat are similar to each other based on distance in an embedding space. We\ndemonstrate the effectiveness of SMART on three multiple choice QA datasets,\nwhere our methodology increases efficiency by reducing dataset size by 48\\% on\naverage, while increasing Pearson correlation with rankings from ChatBot Arena,\na more open-ended human evaluation setting. Our method enables us to be more\nefficient, whether using SMART to make new benchmarks more challenging or to\nrevitalize older datasets, while still preserving the relative model rankings.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "20 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.20245v2",
    "published_date": "2024-10-26 18:21:44 UTC",
    "updated_date": "2025-02-10 21:17:54 UTC"
  },
  {
    "arxiv_id": "2410.20238v2",
    "title": "A Survey of Large Language Models for Arabic Language and its Dialects",
    "authors": [
      "Malak Mashaabi",
      "Shahad Al-Khalifa",
      "Hend Al-Khalifa"
    ],
    "abstract": "This survey offers a comprehensive overview of Large Language Models (LLMs)\ndesigned for Arabic language and its dialects. It covers key architectures,\nincluding encoder-only, decoder-only, and encoder-decoder models, along with\nthe datasets used for pre-training, spanning Classical Arabic, Modern Standard\nArabic, and Dialectal Arabic. The study also explores monolingual, bilingual,\nand multilingual LLMs, analyzing their architectures and performance across\ndownstream tasks, such as sentiment analysis, named entity recognition, and\nquestion answering. Furthermore, it assesses the openness of Arabic LLMs based\non factors, such as source code availability, training data, model weights, and\ndocumentation. The survey highlights the need for more diverse dialectal\ndatasets and attributes the importance of openness for research reproducibility\nand transparency. It concludes by identifying key challenges and opportunities\nfor future research and stressing the need for more inclusive and\nrepresentative models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Submitted to ACM Transactions on Asian and Low-Resource Language\n  Information Processing",
    "pdf_url": "http://arxiv.org/pdf/2410.20238v2",
    "published_date": "2024-10-26 17:48:20 UTC",
    "updated_date": "2025-02-24 13:42:28 UTC"
  },
  {
    "arxiv_id": "2410.20229v1",
    "title": "Modelling of Economic Implications of Bias in AI-Powered Health Emergency Response Systems",
    "authors": [
      "Katsiaryna Bahamazava"
    ],
    "abstract": "We present a theoretical framework assessing the economic implications of\nbias in AI-powered emergency response systems. Integrating health economics,\nwelfare economics, and artificial intelligence, we analyze how algorithmic bias\naffects resource allocation, health outcomes, and social welfare. By\nincorporating a bias function into health production and social welfare models,\nwe quantify its impact on demographic groups, showing that bias leads to\nsuboptimal resource distribution, increased costs, and welfare losses. The\nframework highlights efficiency-equity trade-offs and provides economic\ninterpretations. We propose mitigation strategies, including\nfairness-constrained optimization, algorithmic adjustments, and policy\ninterventions. Our findings offer insights for policymakers, emergency service\nproviders, and technology developers, emphasizing the need for AI systems that\nare efficient and equitable. By addressing the economic consequences of biased\nAI, this study contributes to policies and technologies promoting fairness,\nefficiency, and social welfare in emergency response services.",
    "categories": [
      "econ.GN",
      "cs.AI",
      "q-fin.EC"
    ],
    "primary_category": "econ.GN",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.20229v1",
    "published_date": "2024-10-26 17:11:23 UTC",
    "updated_date": "2024-10-26 17:11:23 UTC"
  },
  {
    "arxiv_id": "2410.21324v1",
    "title": "Mathematical Derivation Graphs: A Task for Summarizing Equation Dependencies in STEM Manuscripts",
    "authors": [
      "Vishesh Prasad",
      "Brian Kim",
      "Nickvash Kani"
    ],
    "abstract": "Recent advances in natural language processing (NLP), particularly with the\nemergence of large language models (LLMs), have significantly enhanced the\nfield of textual analysis. However, while these developments have yielded\nsubstantial progress in analyzing textual data, applying analysis to\nmathematical equations and their relationships within texts has produced mixed\nresults. In this paper, we take the initial steps toward understanding the\ndependency relationships between mathematical expressions in STEM articles. Our\ndataset, sourced from a random sampling of the arXiv corpus, contains an\nanalysis of 107 published STEM manuscripts whose inter-equation dependency\nrelationships have been hand-labeled, resulting in a new object we refer to as\na derivation graph that summarizes the mathematical content of the manuscript.\nWe exhaustively evaluate analytical and NLP-based models to assess their\ncapability to identify and extract the derivation relationships for each\narticle and compare the results with the ground truth. Our comprehensive\ntesting finds that both analytical and NLP models (including LLMs) achieve\n$\\sim$40-50% F1 scores for extracting derivation graphs from articles,\nrevealing that the recent advances in NLP have not made significant inroads in\ncomprehending mathematical texts compared to simpler analytic models. While\ncurrent approaches offer a solid foundation for extracting mathematical\ninformation, further research is necessary to improve accuracy and depth in\nthis area.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.21324v1",
    "published_date": "2024-10-26 16:52:22 UTC",
    "updated_date": "2024-10-26 16:52:22 UTC"
  },
  {
    "arxiv_id": "2410.20220v1",
    "title": "Neural Fields in Robotics: A Survey",
    "authors": [
      "Muhammad Zubair Irshad",
      "Mauro Comi",
      "Yen-Chen Lin",
      "Nick Heppert",
      "Abhinav Valada",
      "Rares Ambrus",
      "Zsolt Kira",
      "Jonathan Tremblay"
    ],
    "abstract": "Neural Fields have emerged as a transformative approach for 3D scene\nrepresentation in computer vision and robotics, enabling accurate inference of\ngeometry, 3D semantics, and dynamics from posed 2D data. Leveraging\ndifferentiable rendering, Neural Fields encompass both continuous implicit and\nexplicit neural representations enabling high-fidelity 3D reconstruction,\nintegration of multi-modal sensor data, and generation of novel viewpoints.\nThis survey explores their applications in robotics, emphasizing their\npotential to enhance perception, planning, and control. Their compactness,\nmemory efficiency, and differentiability, along with seamless integration with\nfoundation and generative models, make them ideal for real-time applications,\nimproving robot adaptability and decision-making. This paper provides a\nthorough review of Neural Fields in robotics, categorizing applications across\nvarious domains and evaluating their strengths and limitations, based on over\n200 papers. First, we present four key Neural Fields frameworks: Occupancy\nNetworks, Signed Distance Fields, Neural Radiance Fields, and Gaussian\nSplatting. Second, we detail Neural Fields' applications in five major robotics\ndomains: pose estimation, manipulation, navigation, physics, and autonomous\ndriving, highlighting key works and discussing takeaways and open challenges.\nFinally, we outline the current limitations of Neural Fields in robotics and\npropose promising directions for future research. Project page:\nhttps://robonerf.github.io",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "20 pages, 20 figures. Project Page: https://robonerf.github.io",
    "pdf_url": "http://arxiv.org/pdf/2410.20220v1",
    "published_date": "2024-10-26 16:26:41 UTC",
    "updated_date": "2024-10-26 16:26:41 UTC"
  },
  {
    "arxiv_id": "2411.08884v2",
    "title": "Quantifying Risk Propensities of Large Language Models: Ethical Focus and Bias Detection through Role-Play",
    "authors": [
      "Yifan Zeng",
      "Liang Kairong",
      "Fangzhou Dong",
      "Peijia Zheng"
    ],
    "abstract": "As Large Language Models (LLMs) become more prevalent, concerns about their\nsafety, ethics, and potential biases have risen. Systematically evaluating\nLLMs' risk decision-making tendencies and attitudes, particularly in the\nethical domain, has become crucial. This study innovatively applies the\nDomain-Specific Risk-Taking (DOSPERT) scale from cognitive science to LLMs and\nproposes a novel Ethical Decision-Making Risk Attitude Scale (EDRAS) to assess\nLLMs' ethical risk attitudes in depth. We further propose a novel approach\nintegrating risk scales and role-playing to quantitatively evaluate systematic\nbiases in LLMs. Through systematic evaluation and analysis of multiple\nmainstream LLMs, we assessed the \"risk personalities\" of LLMs across multiple\ndomains, with a particular focus on the ethical domain, and revealed and\nquantified LLMs' systematic biases towards different groups. This research\nhelps understand LLMs' risk decision-making and ensure their safe and reliable\napplication. Our approach provides a tool for identifying and mitigating\nbiases, contributing to fairer and more trustworthy AI systems. The code and\ndata are available.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "Accepted by CogSci 2025",
    "pdf_url": "http://arxiv.org/pdf/2411.08884v2",
    "published_date": "2024-10-26 15:55:21 UTC",
    "updated_date": "2025-05-08 04:42:58 UTC"
  },
  {
    "arxiv_id": "2410.20204v2",
    "title": "Generative AI in Health Economics and Outcomes Research: A Taxonomy of Key Definitions and Emerging Applications, an ISPOR Working Group Report",
    "authors": [
      "Rachael Fleurence",
      "Xiaoyan Wang",
      "Jiang Bian",
      "Mitchell K. Higashi",
      "Turgay Ayer",
      "Hua Xu",
      "Dalia Dawoud",
      "Jagpreet Chhatwal"
    ],
    "abstract": "Objective: This article offers a taxonomy of generative artificial\nintelligence (AI) for health economics and outcomes research (HEOR), explores\nits emerging applications, and outlines methods to enhance the accuracy and\nreliability of AI-generated outputs. Methods: The review defines foundational\ngenerative AI concepts and highlights current HEOR applications, including\nsystematic literature reviews, health economic modeling, real-world evidence\ngeneration, and dossier development. Approaches such as prompt engineering\n(zero-shot, few-shot, chain-of-thought, persona pattern prompting),\nretrieval-augmented generation, model fine-tuning, and the use of\ndomain-specific models are introduced to improve AI accuracy and reliability.\nResults: Generative AI shows significant potential in HEOR, enhancing\nefficiency, productivity, and offering novel solutions to complex challenges.\nFoundation models are promising in automating complex tasks, though challenges\nremain in scientific reliability, bias, interpretability, and workflow\nintegration. The article discusses strategies to improve the accuracy of these\nAI tools. Conclusion: Generative AI could transform HEOR by increasing\nefficiency and accuracy across various applications. However, its full\npotential can only be realized by building HEOR expertise and addressing the\nlimitations of current AI technologies. As AI evolves, ongoing research and\ninnovation will shape its future role in the field.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "36 pages, 1 figure, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2410.20204v2",
    "published_date": "2024-10-26 15:42:50 UTC",
    "updated_date": "2025-02-22 15:09:41 UTC"
  },
  {
    "arxiv_id": "2410.20203v2",
    "title": "Physics-informed Shadowgraph Network: An End-to-end Density Field Reconstruction Method",
    "authors": [
      "Xutun Wang",
      "Yuchen Zhang",
      "Zidong Li",
      "Haocheng Wen",
      "Bing Wang"
    ],
    "abstract": "This study presents a novel approach for quantificationally reconstructing\ndensity fields from shadowgraph images using physics-informed neural networks",
    "categories": [
      "physics.flu-dyn",
      "cs.AI"
    ],
    "primary_category": "physics.flu-dyn",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.20203v2",
    "published_date": "2024-10-26 15:28:15 UTC",
    "updated_date": "2024-11-02 12:45:32 UTC"
  },
  {
    "arxiv_id": "2410.20199v1",
    "title": "Rethinking the Uncertainty: A Critical Review and Analysis in the Era of Large Language Models",
    "authors": [
      "Mohammad Beigi",
      "Sijia Wang",
      "Ying Shen",
      "Zihao Lin",
      "Adithya Kulkarni",
      "Jianfeng He",
      "Feng Chen",
      "Ming Jin",
      "Jin-Hee Cho",
      "Dawei Zhou",
      "Chang-Tien Lu",
      "Lifu Huang"
    ],
    "abstract": "In recent years, Large Language Models (LLMs) have become fundamental to a\nbroad spectrum of artificial intelligence applications. As the use of LLMs\nexpands, precisely estimating the uncertainty in their predictions has become\ncrucial. Current methods often struggle to accurately identify, measure, and\naddress the true uncertainty, with many focusing primarily on estimating model\nconfidence. This discrepancy is largely due to an incomplete understanding of\nwhere, when, and how uncertainties are injected into models. This paper\nintroduces a comprehensive framework specifically designed to identify and\nunderstand the types and sources of uncertainty, aligned with the unique\ncharacteristics of LLMs. Our framework enhances the understanding of the\ndiverse landscape of uncertainties by systematically categorizing and defining\neach type, establishing a solid foundation for developing targeted methods that\ncan precisely quantify these uncertainties. We also provide a detailed\nintroduction to key related concepts and examine the limitations of current\nmethods in mission-critical and safety-sensitive applications. The paper\nconcludes with a perspective on future directions aimed at enhancing the\nreliability and practical adoption of these methods in real-world scenarios.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.20199v1",
    "published_date": "2024-10-26 15:07:15 UTC",
    "updated_date": "2024-10-26 15:07:15 UTC"
  },
  {
    "arxiv_id": "2410.23306v3",
    "title": "Securing Healthcare with Deep Learning: A CNN-Based Model for medical IoT Threat Detection",
    "authors": [
      "Alireza Mohamadi",
      "Hosna Ghahramani",
      "Seyyed Amir Asghari",
      "Mehdi Aminian"
    ],
    "abstract": "The increasing integration of the Internet of Medical Things (IoMT) into\nhealthcare systems has significantly enhanced patient care but has also\nintroduced critical cybersecurity challenges. This paper presents a novel\napproach based on Convolutional Neural Networks (CNNs) for detecting\ncyberattacks within IoMT environments. Unlike previous studies that\npredominantly utilized traditional machine learning (ML) models or simpler Deep\nNeural Networks (DNNs), the proposed model leverages the capabilities of CNNs\nto effectively analyze the temporal characteristics of network traffic data.\nTrained and evaluated on the CICIoMT2024 dataset, which comprises 18 distinct\ntypes of cyberattacks across a range of IoMT devices, the proposed CNN model\ndemonstrates superior performance compared to previous state-of-the-art\nmethods, achieving a perfect accuracy of 99% in binary, categorical, and\nmulticlass classification tasks. This performance surpasses that of\nconventional ML models such as Logistic Regression, AdaBoost, DNNs, and Random\nForests. These findings highlight the potential of CNNs to substantially\nimprove IoMT cybersecurity, thereby ensuring the protection and integrity of\nconnected healthcare systems.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "The final published version is available in IEEE Xplore:\n  https://doi.org/10.1109/ICIS64839.2024.10887510",
    "pdf_url": "http://arxiv.org/pdf/2410.23306v3",
    "published_date": "2024-10-26 14:27:17 UTC",
    "updated_date": "2025-02-21 17:42:34 UTC"
  },
  {
    "arxiv_id": "2410.20187v1",
    "title": "Uncertainty-Penalized Direct Preference Optimization",
    "authors": [
      "Sam Houliston",
      "Alizée Pace",
      "Alexander Immer",
      "Gunnar Rätsch"
    ],
    "abstract": "Aligning Large Language Models (LLMs) to human preferences in content, style,\nand presentation is challenging, in part because preferences are varied,\ncontext-dependent, and sometimes inherently ambiguous. While successful,\nReinforcement Learning from Human Feedback (RLHF) and Direct Preference\nOptimization (DPO) are prone to the issue of proxy reward overoptimization.\nAnalysis of the DPO loss reveals a critical need for regularization for\nmislabeled or ambiguous preference pairs to avoid reward hacking. In this work,\nwe develop a pessimistic framework for DPO by introducing preference\nuncertainty penalization schemes, inspired by offline reinforcement learning.\nThe penalization serves as a correction to the loss which attenuates the loss\ngradient for uncertain samples. Evaluation of the methods is performed with\nGPT2 Medium on the Anthropic-HH dataset using a model ensemble to obtain\nuncertainty estimates, and shows improved overall performance compared to\nvanilla DPO, as well as better completions on prompts from high-uncertainty\nchosen/rejected responses.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML",
      "Learning and adaptive systems in artificial intelligence"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at the NeurIPS 2024 FITML Workshop",
    "pdf_url": "http://arxiv.org/pdf/2410.20187v1",
    "published_date": "2024-10-26 14:24:37 UTC",
    "updated_date": "2024-10-26 14:24:37 UTC"
  },
  {
    "arxiv_id": "2410.21322v1",
    "title": "Angel or Devil: Discriminating Hard Samples and Anomaly Contaminations for Unsupervised Time Series Anomaly Detection",
    "authors": [
      "Ruyi Zhang",
      "Hongzuo Xu",
      "Songlei Jian",
      "Yusong Tan",
      "Haifang Zhou",
      "Rulin Xu"
    ],
    "abstract": "Training in unsupervised time series anomaly detection is constantly plagued\nby the discrimination between harmful `anomaly contaminations' and beneficial\n`hard normal samples'. These two samples exhibit analogous loss behavior that\nconventional loss-based methodologies struggle to differentiate. To tackle this\nproblem, we propose a novel approach that supplements traditional loss behavior\nwith `parameter behavior', enabling a more granular characterization of\nanomalous patterns. Parameter behavior is formalized by measuring the\nparametric response to minute perturbations in input samples. Leveraging the\ncomplementary nature of parameter and loss behaviors, we further propose a dual\nParameter-Loss Data Augmentation method (termed PLDA), implemented within the\nreinforcement learning paradigm. During the training phase of anomaly\ndetection, PLDA dynamically augments the training data through an iterative\nprocess that simultaneously mitigates anomaly contaminations while amplifying\ninformative hard normal samples. PLDA demonstrates remarkable versatility,\nwhich can serve as an additional component that seamlessly integrated with\nexisting anomaly detectors to enhance their detection performance. Extensive\nexperiments on ten datasets show that PLDA significantly improves the\nperformance of four distinct detectors by up to 8\\%, outperforming three\nstate-of-the-art data augmentation methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages, 9 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2410.21322v1",
    "published_date": "2024-10-26 13:59:23 UTC",
    "updated_date": "2024-10-26 13:59:23 UTC"
  },
  {
    "arxiv_id": "2410.20182v2",
    "title": "Chemical Language Model Linker: blending text and molecules with modular adapters",
    "authors": [
      "Yifan Deng",
      "Spencer S. Ericksen",
      "Anthony Gitter"
    ],
    "abstract": "The development of large language models and multi-modal models has enabled\nthe appealing idea of generating novel molecules from text descriptions.\nGenerative modeling would shift the paradigm from relying on large-scale\nchemical screening to find molecules with desired properties to directly\ngenerating those molecules. However, multi-modal models combining text and\nmolecules are often trained from scratch, without leveraging existing\nhigh-quality pretrained models. Training from scratch consumes more\ncomputational resources and prohibits model scaling. In contrast, we propose a\nlightweight adapter-based strategy named Chemical Language Model Linker\n(ChemLML). ChemLML blends the two single domain models and obtains conditional\nmolecular generation from text descriptions while still operating in the\nspecialized embedding spaces of the molecular domain. ChemLML can tailor\ndiverse pretrained text models for molecule generation by training relatively\nfew adapter parameters. We find that the choice of molecular representation\nused within ChemLML, SMILES versus SELFIES, has a strong influence on\nconditional molecular generation performance. SMILES is often preferable\ndespite not guaranteeing valid molecules. We raise issues in using the entire\nPubChem dataset of molecules and their associated descriptions for evaluating\nmolecule generation and provide a filtered version of the dataset as a\ngeneration test set. To demonstrate how ChemLML could be used in practice, we\ngenerate candidate protein inhibitors and use docking to assess their quality\nand also generate candidate membrane permeable molecules.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG",
    "comment": "60 pages, 12 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.20182v2",
    "published_date": "2024-10-26 13:40:13 UTC",
    "updated_date": "2025-04-16 03:19:29 UTC"
  },
  {
    "arxiv_id": "2410.20178v2",
    "title": "LLMs Can Evolve Continually on Modality for X-Modal Reasoning",
    "authors": [
      "Jiazuo Yu",
      "Haomiao Xiong",
      "Lu Zhang",
      "Haiwen Diao",
      "Yunzhi Zhuge",
      "Lanqing Hong",
      "Dong Wang",
      "Huchuan Lu",
      "You He",
      "Long Chen"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have gained significant attention\ndue to their impressive capabilities in multimodal understanding. However,\nexisting methods rely heavily on extensive modal-specific pretraining and\njoint-modal tuning, leading to significant computational burdens when expanding\nto new modalities. In this paper, we propose PathWeave, a flexible and scalable\nframework with modal-Path sWitching and ExpAnsion abilities that enables MLLMs\nto continually EVolve on modalities for $\\mathbb{X}$-modal reasoning. We\nleverage the concept of Continual Learning and develop an incremental training\nstrategy atop pre-trained MLLMs, enabling their expansion to new modalities\nusing uni-modal data, without executing joint-modal pretraining. In detail, a\nnovel Adapter-in-Adapter (AnA) framework is introduced, in which uni-modal and\ncross-modal adapters are seamlessly integrated to facilitate efficient modality\nalignment and collaboration. Additionally, an MoE-based gating module is\napplied between two types of adapters to further enhance the multimodal\ninteraction. To investigate the proposed method, we establish a challenging\nbenchmark called Continual Learning of Modality (MCL), which consists of\nhigh-quality QA data from five distinct modalities: image, video, audio, depth\nand point cloud. Extensive experiments demonstrate the effectiveness of the\nproposed AnA framework on learning plasticity and memory stability during\ncontinual learning. Furthermore, PathWeave performs comparably to\nstate-of-the-art MLLMs while concurrently reducing parameter training burdens\nby 98.73%. Our code locates at https://github.com/JiazuoYu/PathWeave",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.20178v2",
    "published_date": "2024-10-26 13:19:57 UTC",
    "updated_date": "2024-11-12 14:45:18 UTC"
  },
  {
    "arxiv_id": "2410.20174v1",
    "title": "A Stack-Propagation Framework for Low-Resource Personalized Dialogue Generation",
    "authors": [
      "Haoyu Song",
      "Wei-Nan Zhang",
      "Kaiyan Zhang",
      "Ting Liu"
    ],
    "abstract": "With the resurgent interest in building open-domain dialogue systems, the\ndialogue generation task has attracted increasing attention over the past few\nyears. This task is usually formulated as a conditional generation problem,\nwhich aims to generate a natural and meaningful response given dialogue\ncontexts and specific constraints, such as persona. And maintaining a\nconsistent persona is essential for the dialogue systems to gain trust from the\nusers. Although tremendous advancements have been brought, traditional\npersona-based dialogue models are typically trained by leveraging a large\nnumber of persona-dense dialogue examples. Yet, such persona-dense training\ndata are expensive to obtain, leading to a limited scale. This work presents a\nnovel approach to learning from limited training examples by regarding\nconsistency understanding as a regularization of response generation. To this\nend, we propose a novel stack-propagation framework for learning a generation\nand understanding pipeline.Specifically, the framework stacks a Transformer\nencoder and two Transformer decoders, where the first decoder models response\ngeneration and the second serves as a regularizer and jointly models response\ngeneration and consistency understanding. The proposed framework can benefit\nfrom the stacked encoder and decoders to learn from much smaller personalized\ndialogue data while maintaining competitive performance. Under different\nlow-resource settings, subjective and objective evaluations prove that the\nstack-propagation framework outperforms strong baselines in response quality\nand persona consistency and largely overcomes the shortcomings of traditional\nmodels that rely heavily on the persona-dense dialogue data.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "published as a journal paper at ACM Transactions on Information\n  Systems 2023. 35 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.20174v1",
    "published_date": "2024-10-26 13:09:21 UTC",
    "updated_date": "2024-10-26 13:09:21 UTC"
  },
  {
    "arxiv_id": "2410.20165v1",
    "title": "Diff-CXR: Report-to-CXR generation through a disease-knowledge enhanced diffusion model",
    "authors": [
      "Peng Huang",
      "Bowen Guo",
      "Shuyu Liang",
      "Junhu Fu",
      "Yuanyuan Wang",
      "Yi Guo"
    ],
    "abstract": "Text-To-Image (TTI) generation is significant for controlled and diverse\nimage generation with broad potential applications. Although current medical\nTTI methods have made some progress in report-to-Chest-Xray (CXR) generation,\ntheir generation performance may be limited due to the intrinsic\ncharacteristics of medical data. In this paper, we propose a novel\ndisease-knowledge enhanced Diffusion-based TTI learning framework, named\nDiff-CXR, for medical report-to-CXR generation. First, to minimize the negative\nimpacts of noisy data on generation, we devise a Latent Noise Filtering\nStrategy that gradually learns the general patterns of anomalies and removes\nthem in the latent space. Then, an Adaptive Vision-Aware Textual Learning\nStrategy is designed to learn concise and important report embeddings in a\ndomain-specific Vision-Language Model, providing textual guidance for\nChest-Xray generation. Finally, by incorporating the general disease knowledge\ninto the pretrained TTI model via a delicate control adapter, a\ndisease-knowledge enhanced diffusion model is introduced to achieve realistic\nand precise report-to-CXR generation. Experimentally, our Diff-CXR outperforms\nprevious SOTA medical TTI methods by 33.4\\% / 8.0\\% and 23.8\\% / 56.4\\% in the\nFID and mAUC score on MIMIC-CXR and IU-Xray, with the lowest computational\ncomplexity at 29.641 GFLOPs. Downstream experiments on three thorax disease\nclassification benchmarks and one CXR-report generation benchmark demonstrate\nthat Diff-CXR is effective in improving classical CXR analysis methods.\nNotably, models trained on the combination of 1\\% real data and synthetic data\ncan achieve a competitive mAUC score compared to models trained on all data,\npresenting promising clinical applications.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.20165v1",
    "published_date": "2024-10-26 12:38:12 UTC",
    "updated_date": "2024-10-26 12:38:12 UTC"
  },
  {
    "arxiv_id": "2410.20161v1",
    "title": "Causal Abstraction in Model Interpretability: A Compact Survey",
    "authors": [
      "Yihao Zhang"
    ],
    "abstract": "The pursuit of interpretable artificial intelligence has led to significant\nadvancements in the development of methods that aim to explain the\ndecision-making processes of complex models, such as deep learning systems.\nAmong these methods, causal abstraction stands out as a theoretical framework\nthat provides a principled approach to understanding and explaining the causal\nmechanisms underlying model behavior. This survey paper delves into the realm\nof causal abstraction, examining its theoretical foundations, practical\napplications, and implications for the field of model interpretability.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.20161v1",
    "published_date": "2024-10-26 12:24:28 UTC",
    "updated_date": "2024-10-26 12:24:28 UTC"
  },
  {
    "arxiv_id": "2410.20149v1",
    "title": "AdaNeg: Adaptive Negative Proxy Guided OOD Detection with Vision-Language Models",
    "authors": [
      "Yabin Zhang",
      "Lei Zhang"
    ],
    "abstract": "Recent research has shown that pre-trained vision-language models are\neffective at identifying out-of-distribution (OOD) samples by using negative\nlabels as guidance. However, employing consistent negative labels across\ndifferent OOD datasets often results in semantic misalignments, as these text\nlabels may not accurately reflect the actual space of OOD images. To overcome\nthis issue, we introduce \\textit{adaptive negative proxies}, which are\ndynamically generated during testing by exploring actual OOD images, to align\nmore closely with the underlying OOD label space and enhance the efficacy of\nnegative proxy guidance. Specifically, our approach utilizes a feature memory\nbank to selectively cache discriminative features from test images,\nrepresenting the targeted OOD distribution. This facilitates the creation of\nproxies that can better align with specific OOD datasets. While task-adaptive\nproxies average features to reflect the unique characteristics of each dataset,\nthe sample-adaptive proxies weight features based on their similarity to\nindividual test samples, exploring detailed sample-level nuances. The final\nscore for identifying OOD samples integrates static negative labels with our\nproposed adaptive proxies, effectively combining textual and visual knowledge\nfor enhanced performance. Our method is training-free and annotation-free, and\nit maintains fast testing speed. Extensive experiments across various\nbenchmarks demonstrate the effectiveness of our approach, abbreviated as\nAdaNeg. Notably, on the large-scale ImageNet benchmark, our AdaNeg\nsignificantly outperforms existing methods, with a 2.45\\% increase in AUROC and\na 6.48\\% reduction in FPR95. Codes are available at\n\\url{https://github.com/YBZh/OpenOOD-VLM}.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "NIPS 2024 Camera Ready, Codes are available at\n  \\url{https://github.com/YBZh/OpenOOD-VLM}",
    "pdf_url": "http://arxiv.org/pdf/2410.20149v1",
    "published_date": "2024-10-26 11:20:02 UTC",
    "updated_date": "2024-10-26 11:20:02 UTC"
  },
  {
    "arxiv_id": "2410.22362v1",
    "title": "MMM-RS: A Multi-modal, Multi-GSD, Multi-scene Remote Sensing Dataset and Benchmark for Text-to-Image Generation",
    "authors": [
      "Jialin Luo",
      "Yuanzhi Wang",
      "Ziqi Gu",
      "Yide Qiu",
      "Shuaizhen Yao",
      "Fuyun Wang",
      "Chunyan Xu",
      "Wenhua Zhang",
      "Dan Wang",
      "Zhen Cui"
    ],
    "abstract": "Recently, the diffusion-based generative paradigm has achieved impressive\ngeneral image generation capabilities with text prompts due to its accurate\ndistribution modeling and stable training process. However, generating diverse\nremote sensing (RS) images that are tremendously different from general images\nin terms of scale and perspective remains a formidable challenge due to the\nlack of a comprehensive remote sensing image generation dataset with various\nmodalities, ground sample distances (GSD), and scenes. In this paper, we\npropose a Multi-modal, Multi-GSD, Multi-scene Remote Sensing (MMM-RS) dataset\nand benchmark for text-to-image generation in diverse remote sensing scenarios.\nSpecifically, we first collect nine publicly available RS datasets and conduct\nstandardization for all samples. To bridge RS images to textual semantic\ninformation, we utilize a large-scale pretrained vision-language model to\nautomatically output text prompts and perform hand-crafted rectification,\nresulting in information-rich text-image pairs (including multi-modal images).\nIn particular, we design some methods to obtain the images with different GSD\nand various environments (e.g., low-light, foggy) in a single sample. With\nextensive manual screening and refining annotations, we ultimately obtain a\nMMM-RS dataset that comprises approximately 2.1 million text-image pairs.\nExtensive experimental results verify that our proposed MMM-RS dataset allows\noff-the-shelf diffusion models to generate diverse RS images across various\nmodalities, scenes, weather conditions, and GSD. The dataset is available at\nhttps://github.com/ljl5261/MMM-RS.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "Accepted by NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.22362v1",
    "published_date": "2024-10-26 11:19:07 UTC",
    "updated_date": "2024-10-26 11:19:07 UTC"
  },
  {
    "arxiv_id": "2410.20143v1",
    "title": "Exploring Welfare Maximization and Fairness in Participatory Budgeting",
    "authors": [
      "Gogulapati Sreedurga"
    ],
    "abstract": "Participatory budgeting (PB) is a voting paradigm for distributing a\ndivisible resource, usually called a budget, among a set of projects by\naggregating the preferences of individuals over these projects. It is\nimplemented quite extensively for purposes such as government allocating funds\nto public projects and funding agencies selecting research proposals to\nsupport. This PhD dissertation studies the welfare-related and fairness-related\nobjectives for different PB models. Our contribution lies in proposing and\nexploring novel PB rules that maximize welfare and promote fairness, as well\nas, in introducing and investigating a range of novel utility notions,\naxiomatic properties, and fairness notions, effectively filling the gaps in the\nexisting literature for each PB model. The thesis is divided into two main\nparts, the first focusing on dichotomous and the second focusing on ordinal\npreferences. Each part considers two cases: (i) the cost of each project is\nrestricted to a single value and partial funding is not permitted and (ii) the\ncost of each project is flexible and may assume multiple values.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.GT",
    "comment": "PhD Thesis",
    "pdf_url": "http://arxiv.org/pdf/2410.20143v1",
    "published_date": "2024-10-26 10:51:22 UTC",
    "updated_date": "2024-10-26 10:51:22 UTC"
  },
  {
    "arxiv_id": "2410.20142v2",
    "title": "Mask-based Membership Inference Attacks for Retrieval-Augmented Generation",
    "authors": [
      "Mingrui Liu",
      "Sixiao Zhang",
      "Cheng Long"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) has been an effective approach to\nmitigate hallucinations in large language models (LLMs) by incorporating\nup-to-date and domain-specific knowledge. Recently, there has been a trend of\nstoring up-to-date or copyrighted data in RAG knowledge databases instead of\nusing it for LLM training. This practice has raised concerns about Membership\nInference Attacks (MIAs), which aim to detect if a specific target document is\nstored in the RAG system's knowledge database so as to protect the rights of\ndata producers. While research has focused on enhancing the trustworthiness of\nRAG systems, existing MIAs for RAG systems remain largely insufficient.\nPrevious work either relies solely on the RAG system's judgment or is easily\ninfluenced by other documents or the LLM's internal knowledge, which is\nunreliable and lacks explainability. To address these limitations, we propose a\nMask-Based Membership Inference Attacks (MBA) framework. Our framework first\nemploys a masking algorithm that effectively masks a certain number of words in\nthe target document. The masked text is then used to prompt the RAG system, and\nthe RAG system is required to predict the mask values. If the target document\nappears in the knowledge database, the masked text will retrieve the complete\ntarget document as context, allowing for accurate mask prediction. Finally, we\nadopt a simple yet effective threshold-based method to infer the membership of\ntarget document by analyzing the accuracy of mask prediction. Our mask-based\napproach is more document-specific, making the RAG system's generation less\nsusceptible to distractions from other documents or the LLM's internal\nknowledge. Extensive experiments demonstrate the effectiveness of our approach\ncompared to existing baseline models.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CR",
    "comment": "This paper is accepted by conference WWW 2025",
    "pdf_url": "http://arxiv.org/pdf/2410.20142v2",
    "published_date": "2024-10-26 10:43:39 UTC",
    "updated_date": "2025-02-09 07:58:23 UTC"
  },
  {
    "arxiv_id": "2410.20140v2",
    "title": "LLM-Consensus: Multi-Agent Debate for Visual Misinformation Detection",
    "authors": [
      "Kumud Lakara",
      "Georgia Channing",
      "Juil Sock",
      "Christian Rupprecht",
      "Philip Torr",
      "John Collomosse",
      "Christian Schroeder de Witt"
    ],
    "abstract": "One of the most challenging forms of misinformation involves the\nout-of-context (OOC) use of images paired with misleading text, creating false\nnarratives. Existing AI-driven detection systems lack explainability and\nrequire expensive finetuning. We address these issues with LLM-Consensus, a\nmulti-agent debate system for OOC misinformation detection. LLM-Consensus\nintroduces a novel multi-agent debate framework where multimodal agents\ncollaborate to assess contextual consistency and request external information\nto enhance cross-context reasoning and decision-making. Our framework enables\nexplainable detection with state-of-the-art accuracy even without\ndomain-specific fine-tuning. Extensive ablation studies confirm that external\nretrieval significantly improves detection accuracy, and user studies\ndemonstrate that LLM-Consensus boosts performance for both experts and\nnon-experts. These results position LLM-Consensus as a powerful tool for\nautonomous and citizen intelligence applications.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.20140v2",
    "published_date": "2024-10-26 10:34:22 UTC",
    "updated_date": "2025-01-31 20:55:12 UTC"
  },
  {
    "arxiv_id": "2411.05799v1",
    "title": "NeoPhysIx: An Ultra Fast 3D Physical Simulator as Development Tool for AI Algorithms",
    "authors": [
      "Jörn Fischer",
      "Thomas Ihme"
    ],
    "abstract": "Traditional AI algorithms, such as Genetic Programming and Reinforcement\nLearning, often require extensive computational resources to simulate\nreal-world physical scenarios effectively. While advancements in multi-core\nprocessing have been made, the inherent limitations of parallelizing rigid body\ndynamics lead to significant communication overheads, hindering substantial\nperformance gains for simple simulations.\n  This paper introduces NeoPhysIx, a novel 3D physical simulator designed to\novercome these challenges. By adopting innovative simulation paradigms and\nfocusing on essential algorithmic elements, NeoPhysIx achieves unprecedented\nspeedups exceeding 1000x compared to real-time. This acceleration is realized\nthrough strategic simplifications, including point cloud collision detection,\njoint angle determination, and friction force estimation.\n  The efficacy of NeoPhysIx is demonstrated through its application in training\na legged robot with 18 degrees of freedom and six sensors, controlled by an\nevolved genetic program. Remarkably, simulating half a year of robot lifetime\nwithin a mere 9 hours on a single core of a standard mid-range CPU highlights\nthe significant efficiency gains offered by NeoPhysIx. This breakthrough paves\nthe way for accelerated AI development and training in physically-grounded\ndomains.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "7 Pages, 4 Figures",
    "pdf_url": "http://arxiv.org/pdf/2411.05799v1",
    "published_date": "2024-10-26 09:53:07 UTC",
    "updated_date": "2024-10-26 09:53:07 UTC"
  },
  {
    "arxiv_id": "2411.05022v1",
    "title": "Towards Probabilistic Planning of Explanations for Robot Navigation",
    "authors": [
      "Amar Halilovic",
      "Senka Krivic"
    ],
    "abstract": "In robotics, ensuring that autonomous systems are comprehensible and\naccountable to users is essential for effective human-robot interaction. This\npaper introduces a novel approach that integrates user-centered design\nprinciples directly into the core of robot path planning processes. We propose\na probabilistic framework for automated planning of explanations for robot\nnavigation, where the preferences of different users regarding explanations are\nprobabilistically modeled to tailor the stochasticity of the real-world\nhuman-robot interaction and the communication of decisions of the robot and its\nactions towards humans. This approach aims to enhance the transparency of robot\npath planning and adapt to diverse user explanation needs by anticipating the\ntypes of explanations that will satisfy individual users.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.05022v1",
    "published_date": "2024-10-26 09:52:14 UTC",
    "updated_date": "2024-10-26 09:52:14 UTC"
  },
  {
    "arxiv_id": "2410.20132v1",
    "title": "On-Site Precise Screening of SARS-CoV-2 Systems Using a Channel-Wise Attention-Based PLS-1D-CNN Model with Limited Infrared Signatures",
    "authors": [
      "Wenwen Zhang",
      "Zhouzhuo Tang",
      "Yingmei Feng",
      "Xia Yu",
      "Qi Jie Wang",
      "Zhiping Lin"
    ],
    "abstract": "During the early stages of respiratory virus outbreaks, such as severe acute\nrespiratory syndrome coronavirus 2 (SARS-CoV-2), the efficient utilize of\nlimited nasopharyngeal swabs for rapid and accurate screening is crucial for\npublic health. In this study, we present a methodology that integrates\nattenuated total reflection-Fourier transform infrared spectroscopy (ATR-FTIR)\nwith the adaptive iteratively reweighted penalized least squares (airPLS)\npreprocessing algorithm and a channel-wise attention-based partial least\nsquares one-dimensional convolutional neural network (PLS-1D-CNN) model,\nenabling accurate screening of infected individuals within 10 minutes. Two\ncohorts of nasopharyngeal swab samples, comprising 126 and 112 samples from\nsuspected SARS-CoV-2 Omicron variant cases, were collected at Beijing You'an\nHospital for verification. Given that ATR-FTIR spectra are highly sensitive to\nvariations in experimental conditions, which can affect their quality, we\npropose a biomolecular importance (BMI) evaluation method to assess signal\nquality across different conditions, validated by comparing BMI with PLS-GBM\nand PLS-RF results. For the ATR-FTIR signals in cohort 2, which exhibited a\nhigher BMI, airPLS was utilized for signal preprocessing, followed by the\napplication of the channel-wise attention-based PLS-1D-CNN model for screening.\nThe experimental results demonstrate that our model outperforms recently\nreported methods in the field of respiratory virus spectrum detection,\nachieving a recognition screening accuracy of 96.48%, a sensitivity of 96.24%,\na specificity of 97.14%, an F1-score of 96.12%, and an AUC of 0.99. It meets\nthe World Health Organization (WHO) recommended criteria for an acceptable\nproduct: sensitivity of 95.00% or greater and specificity of 97.00% or greater\nfor testing prior SARS-CoV-2 infection in moderate to high volume scenarios.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG",
      "q-bio.BM"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.20132v1",
    "published_date": "2024-10-26 09:22:35 UTC",
    "updated_date": "2024-10-26 09:22:35 UTC"
  },
  {
    "arxiv_id": "2410.20116v1",
    "title": "Estuary: A Framework For Building Multimodal Low-Latency Real-Time Socially Interactive Agents",
    "authors": [
      "Spencer Lin",
      "Basem Rizk",
      "Miru Jun",
      "Andy Artze",
      "Caitlin Sullivan",
      "Sharon Mozgai",
      "Scott Fisher"
    ],
    "abstract": "The rise in capability and ubiquity of generative artificial intelligence\n(AI) technologies has enabled its application to the field of Socially\nInteractive Agents (SIAs). Despite rising interest in modern AI-powered\ncomponents used for real-time SIA research, substantial friction remains due to\nthe absence of a standardized and universal SIA framework. To target this\nabsence, we developed Estuary: a multimodal (text, audio, and soon video)\nframework which facilitates the development of low-latency, real-time SIAs.\nEstuary seeks to reduce repeat work between studies and to provide a flexible\nplatform that can be run entirely off-cloud to maximize configurability,\ncontrollability, reproducibility of studies, and speed of agent response times.\nWe are able to do this by constructing a robust multimodal framework which\nincorporates current and future components seamlessly into a modular and\ninteroperable architecture.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "J.0"
    ],
    "primary_category": "cs.HC",
    "comment": "To be published in ACM Intelligent Virtual Agents (IVA) 2024 [DOI:\n  10.1145/3652988.3696198] [ACM ISBN: 979-8-4007-0625-7/24/09]",
    "pdf_url": "http://arxiv.org/pdf/2410.20116v1",
    "published_date": "2024-10-26 08:08:12 UTC",
    "updated_date": "2024-10-26 08:08:12 UTC"
  },
  {
    "arxiv_id": "2410.20109v2",
    "title": "GiVE: Guiding Visual Encoder to Perceive Overlooked Information",
    "authors": [
      "Junjie Li",
      "Jianghong Ma",
      "Xiaofeng Zhang",
      "Yuhang Li",
      "Jianyang Shi"
    ],
    "abstract": "Multimodal Large Language Models have advanced AI in applications like\ntext-to-video generation and visual question answering. These models rely on\nvisual encoders to convert non-text data into vectors, but current encoders\neither lack semantic alignment or overlook non-salient objects. We propose the\nGuiding Visual Encoder to Perceive Overlooked Information (GiVE) approach. GiVE\nenhances visual representation with an Attention-Guided Adapter (AG-Adapter)\nmodule and an Object-focused Visual Semantic Learning module. These incorporate\nthree novel loss terms: Object-focused Image-Text Contrast (OITC) loss,\nObject-focused Image-Image Contrast (OIIC) loss, and Object-focused Image\nDiscrimination (OID) loss, improving object consideration, retrieval accuracy,\nand comprehensiveness. Our contributions include dynamic visual focus\nadjustment, novel loss functions to enhance object retrieval, and the\nMulti-Object Instruction (MOInst) dataset. Experiments show our approach\nachieves state-of-the-art performance.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "This paper was accepted by ICME 2025",
    "pdf_url": "http://arxiv.org/pdf/2410.20109v2",
    "published_date": "2024-10-26 07:37:43 UTC",
    "updated_date": "2025-03-21 14:36:09 UTC"
  },
  {
    "arxiv_id": "2410.20107v2",
    "title": "Emergence of Globally Attracting Fixed Points in Deep Neural Networks With Nonlinear Activations",
    "authors": [
      "Amir Joudaki",
      "Thomas Hofmann"
    ],
    "abstract": "Understanding how neural networks transform input data across layers is\nfundamental to unraveling their learning and generalization capabilities.\nAlthough prior work has used insights from kernel methods to study neural\nnetworks, a global analysis of how the similarity between hidden\nrepresentations evolves across layers remains underexplored. In this paper, we\nintroduce a theoretical framework for the evolution of the kernel sequence,\nwhich measures the similarity between the hidden representation for two\ndifferent inputs. Operating under the mean-field regime, we show that the\nkernel sequence evolves deterministically via a kernel map, which only depends\non the activation function. By expanding activation using Hermite polynomials\nand using their algebraic properties, we derive an explicit form for kernel map\nand fully characterize its fixed points. Our analysis reveals that for\nnonlinear activations, the kernel sequence converges globally to a unique fixed\npoint, which can correspond to orthogonal or similar representations depending\non the activation and network architecture. We further extend our results to\nnetworks with residual connections and normalization layers, demonstrating\nsimilar convergence behaviors. This work provides new insights into the\nimplicit biases of deep neural networks and how architectural choices influence\nthe evolution of representations across layers.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.20107v2",
    "published_date": "2024-10-26 07:10:47 UTC",
    "updated_date": "2024-10-29 07:52:19 UTC"
  },
  {
    "arxiv_id": "2410.20098v2",
    "title": "Self-Normalized Resets for Plasticity in Continual Learning",
    "authors": [
      "Vivek F. Farias",
      "Adam D. Jozefiak"
    ],
    "abstract": "Plasticity Loss is an increasingly important phenomenon that refers to the\nempirical observation that as a neural network is continually trained on a\nsequence of changing tasks, its ability to adapt to a new task diminishes over\ntime. We introduce Self-Normalized Resets (SNR), a simple adaptive algorithm\nthat mitigates plasticity loss by resetting a neuron's weights when evidence\nsuggests its firing rate has effectively dropped to zero. Across a battery of\ncontinual learning problems and network architectures, we demonstrate that SNR\nconsistently attains superior performance compared to its competitor\nalgorithms. We also demonstrate that SNR is robust to its sole hyperparameter,\nits rejection percentile threshold, while competitor algorithms show\nsignificant sensitivity. SNR's threshold-based reset mechanism is motivated by\na simple hypothesis test that we derive. Seen through the lens of this\nhypothesis test, competing reset proposals yield suboptimal error rates in\ncorrectly detecting inactive neurons, potentially explaining our experimental\nobservations. We also conduct a theoretical investigation of the optimization\nlandscape for the problem of learning a single ReLU. We show that even when\ninitialized adversarially, an idealized version of SNR learns the target ReLU,\nwhile regularization-based approaches can fail to learn.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.20098v2",
    "published_date": "2024-10-26 06:47:13 UTC",
    "updated_date": "2025-03-01 18:34:01 UTC"
  },
  {
    "arxiv_id": "2410.20092v2",
    "title": "OGBench: Benchmarking Offline Goal-Conditioned RL",
    "authors": [
      "Seohong Park",
      "Kevin Frans",
      "Benjamin Eysenbach",
      "Sergey Levine"
    ],
    "abstract": "Offline goal-conditioned reinforcement learning (GCRL) is a major problem in\nreinforcement learning (RL) because it provides a simple, unsupervised, and\ndomain-agnostic way to acquire diverse behaviors and representations from\nunlabeled data without rewards. Despite the importance of this setting, we lack\na standard benchmark that can systematically evaluate the capabilities of\noffline GCRL algorithms. In this work, we propose OGBench, a new, high-quality\nbenchmark for algorithms research in offline goal-conditioned RL. OGBench\nconsists of 8 types of environments, 85 datasets, and reference implementations\nof 6 representative offline GCRL algorithms. We have designed these challenging\nand realistic environments and datasets to directly probe different\ncapabilities of algorithms, such as stitching, long-horizon reasoning, and the\nability to handle high-dimensional inputs and stochasticity. While\nrepresentative algorithms may rank similarly on prior benchmarks, our\nexperiments reveal stark strengths and weaknesses in these different\ncapabilities, providing a strong foundation for building new algorithms.\nProject page: https://seohong.me/projects/ogbench",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2410.20092v2",
    "published_date": "2024-10-26 06:06:08 UTC",
    "updated_date": "2025-02-13 18:38:13 UTC"
  },
  {
    "arxiv_id": "2410.20088v1",
    "title": "RARe: Retrieval Augmented Retrieval with In-Context Examples",
    "authors": [
      "Atula Tejaswi",
      "Yoonsang Lee",
      "Sujay Sanghavi",
      "Eunsol Choi"
    ],
    "abstract": "We investigate whether in-context examples, widely used in decoder-only\nlanguage models (LLMs), can improve embedding model performance in retrieval\ntasks. Unlike in LLMs, naively prepending in-context examples (query-document\npairs) to the target query at inference time does not work out of the box. We\nintroduce a simple approach to enable retrievers to use in-context examples.\nOur approach, RARe, finetunes a pre-trained model with in-context examples\nwhose query is semantically similar to the target query. This can be applied to\nadapt various base architectures (i.e., decoder-only language models, retriever\nmodels) and consistently achieves performance gains of up to +2.72% nDCG across\nvarious open-domain retrieval datasets (BeIR, RAR-b). In particular, we find\nRARe exhibits stronger out-of-domain generalization compared to models using\nqueries without in-context examples, similar to what is seen for in-context\nlearning in LLMs. We further provide analysis on the design choices of\nin-context example augmentation and lay the foundation for future work in this\nspace.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.20088v1",
    "published_date": "2024-10-26 05:46:20 UTC",
    "updated_date": "2024-10-26 05:46:20 UTC"
  },
  {
    "arxiv_id": "2410.21321v1",
    "title": "User-Aware Multilingual Abusive Content Detection in Social Media",
    "authors": [
      "Mohammad Zia Ur Rehman",
      "Somya Mehta",
      "Kuldeep Singh",
      "Kunal Kaushik",
      "Nagendra Kumar"
    ],
    "abstract": "Despite growing efforts to halt distasteful content on social media,\nmultilingualism has added a new dimension to this problem. The scarcity of\nresources makes the challenge even greater when it comes to low-resource\nlanguages. This work focuses on providing a novel method for abusive content\ndetection in multiple low-resource Indic languages. Our observation indicates\nthat a post's tendency to attract abusive comments, as well as features such as\nuser history and social context, significantly aid in the detection of abusive\ncontent. The proposed method first learns social and text context features in\ntwo separate modules. The integrated representation from these modules is\nlearned and used for the final prediction. To evaluate the performance of our\nmethod against different classical and state-of-the-art methods, we have\nperformed extensive experiments on SCIDN and MACI datasets consisting of 1.5M\nand 665K multilingual comments, respectively. Our proposed method outperforms\nstate-of-the-art baseline methods with an average increase of 4.08% and 9.52%\nin F1-scores on SCIDN and MACI datasets, respectively.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.21321v1",
    "published_date": "2024-10-26 05:44:24 UTC",
    "updated_date": "2024-10-26 05:44:24 UTC"
  },
  {
    "arxiv_id": "2411.08883v1",
    "title": "KisanQRS: A Deep Learning-based Automated Query-Response System for Agricultural Decision-Making",
    "authors": [
      "Mohammad Zia Ur Rehman",
      "Devraj Raghuvanshi",
      "Nagendra Kumar"
    ],
    "abstract": "Delivering prompt information and guidance to farmers is critical in\nagricultural decision-making. Farmers helpline centres are heavily reliant on\nthe expertise and availability of call centre agents, leading to inconsistent\nquality and delayed responses. To this end, this article presents Kisan Query\nResponse System (KisanQRS), a Deep Learning-based robust query-response\nframework for the agriculture sector. KisanQRS integrates semantic and lexical\nsimilarities of farmers queries and employs a rapid threshold-based clustering\nmethod. The clustering algorithm is based on a linear search technique to\niterate through all queries and organize them into clusters according to their\nsimilarity. For query mapping, LSTM is found to be the optimal method. Our\nproposed answer retrieval method clusters candidate answers for a crop, ranks\nthese answer clusters based on the number of answers in a cluster, and selects\nthe leader of each cluster. The dataset used in our analysis consists of a\nsubset of 34 million call logs from the Kisan Call Centre (KCC), operated under\nthe Government of India. We evaluated the performance of the query mapping\nmodule on the data of five major states of India with 3,00,000 samples and the\nquantifiable outcomes demonstrate that KisanQRS significantly outperforms\ntraditional techniques by achieving 96.58% top F1-score for a state. The answer\nretrieval module is evaluated on 10,000 samples and it achieves a competitive\nNDCG score of 96.20%. KisanQRS is useful in enabling farmers to make informed\ndecisions about their farming practices by providing quick and pertinent\nresponses to their queries.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08883v1",
    "published_date": "2024-10-26 05:25:05 UTC",
    "updated_date": "2024-10-26 05:25:05 UTC"
  },
  {
    "arxiv_id": "2410.20080v1",
    "title": "Optimizing Keyphrase Ranking for Relevance and Diversity Using Submodular Function Optimization (SFO)",
    "authors": [
      "Muhammad Umair",
      "Syed Jalaluddin Hashmi",
      "Young-Koo Lee"
    ],
    "abstract": "Keyphrase ranking plays a crucial role in information retrieval and\nsummarization by indexing and retrieving relevant information efficiently.\nAdvances in natural language processing, especially large language models\n(LLMs), have improved keyphrase extraction and ranking. However, traditional\nmethods often overlook diversity, resulting in redundant keyphrases. We propose\na novel approach using Submodular Function Optimization (SFO) to balance\nrelevance and diversity in keyphrase ranking. By framing the task as submodular\nmaximization, our method selects diverse and representative keyphrases.\nExperiments on benchmark datasets show that our approach outperforms existing\nmethods in both relevance and diversity metrics, achieving SOTA performance in\nexecution time. Our code is available online.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.20080v1",
    "published_date": "2024-10-26 05:14:32 UTC",
    "updated_date": "2024-10-26 05:14:32 UTC"
  },
  {
    "arxiv_id": "2410.21319v1",
    "title": "Towards Continuous Skin Sympathetic Nerve Activity Monitoring: Removing Muscle Noise",
    "authors": [
      "Farnoush Baghestani",
      "Mahdi Pirayesh Shirazi Nejad",
      "Youngsun Kong",
      "Ki H. Chon"
    ],
    "abstract": "Continuous monitoring of non-invasive skin sympathetic nerve activity (SKNA)\nholds promise for understanding the sympathetic nervous system (SNS) dynamics\nin various physiological and pathological conditions. However, muscle noise\nartifacts present a challenge in accurate SKNA analysis, particularly in\nreal-life scenarios. This study proposes a deep convolutional neural network\n(CNN) approach to detect and remove muscle noise from SKNA recordings obtained\nvia ECG electrodes. Twelve healthy participants underwent controlled\nexperimental protocols involving cognitive stress induction and voluntary\nmuscle movements, while collecting SKNA data. Power spectral analysis revealed\nsignificant muscle noise interference within the SKNA frequency band (500-1000\nHz). A 2D CNN model was trained on the spectrograms of the data segments to\nclassify them into baseline, stress-induced SKNA, and muscle noise-contaminated\nperiods, achieving an average accuracy of 89.85% across all subjects. Our\nfindings underscore the importance of addressing muscle noise for accurate SKNA\nmonitoring, advancing towards wearable SKNA sensors for real-world\napplications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.NC"
    ],
    "primary_category": "cs.LG",
    "comment": "4 pages, 5 figures, 1 table, IEEE-EMBS International Conference on\n  Body Sensor Networks: NextGen Health: Sensor Innovation, AI, and Social\n  Responsibility (IEEE BSN 2024)",
    "pdf_url": "http://arxiv.org/pdf/2410.21319v1",
    "published_date": "2024-10-26 04:10:14 UTC",
    "updated_date": "2024-10-26 04:10:14 UTC"
  },
  {
    "arxiv_id": "2410.20066v2",
    "title": "A Multi-Modal Non-Invasive Deep Learning Framework for Progressive Prediction of Seizures",
    "authors": [
      "Ali Saeizadeh",
      "Douglas Schonholtz",
      "Joseph S. Neimat",
      "Pedram Johari",
      "Tommaso Melodia"
    ],
    "abstract": "This paper introduces an innovative framework designed for progressive\n(granular in time to onset) prediction of seizures through the utilization of a\nDeep Learning (DL) methodology based on non-invasive multi-modal sensor\nnetworks. Epilepsy, a debilitating neurological condition, affects an estimated\n65 million individuals globally, with a substantial proportion facing\ndrug-resistant epilepsy despite pharmacological interventions. To address this\nchallenge, we advocate for predictive systems that provide timely alerts to\nindividuals at risk, enabling them to take precautionary actions. Our framework\nemploys advanced DL techniques and uses personalized data from a network of\nnon-invasive electroencephalogram (EEG) and electrocardiogram (ECG) sensors,\nthereby enhancing prediction accuracy. The algorithms are optimized for\nreal-time processing on edge devices, mitigating privacy concerns and\nminimizing data transmission overhead inherent in cloud-based solutions,\nultimately preserving battery energy. Additionally, our system predicts the\ncountdown time to seizures (with 15-minute intervals up to an hour prior to the\nonset), offering critical lead time for preventive actions. Our multi-modal\nmodel achieves 95% sensitivity, 98% specificity, and 97% accuracy, averaged\namong 29 patients.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "4 pages, 5 figures, Proceedings of the IEEE 20th International\n  Conference on Body Sensor Networks (BSN), October 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.20066v2",
    "published_date": "2024-10-26 04:06:09 UTC",
    "updated_date": "2024-11-01 18:20:51 UTC"
  },
  {
    "arxiv_id": "2410.20062v1",
    "title": "Transforming Precision: A Comparative Analysis of Vision Transformers, CNNs, and Traditional ML for Knee Osteoarthritis Severity Diagnosis",
    "authors": [
      "Tasnim Sakib Apon",
      "Md. Fahim-Ul-Islam",
      "Nafiz Imtiaz Rafin",
      "Joya Akter",
      "Md. Golam Rabiul Alam"
    ],
    "abstract": "Knee osteoarthritis(KO) is a degenerative joint disease that can cause severe\npain and impairment. With increased prevalence, precise diagnosis by medical\nimaging analytics is crucial for appropriate illness management. This research\ninvestigates a comparative analysis between traditional machine learning\ntechniques and new deep learning models for diagnosing KO severity from X-ray\npictures. This study does not introduce new architectural innovations but\nrather illuminates the robust applicability and comparative effectiveness of\npre-existing ViT models in a medical imaging context, specifically for KO\nseverity diagnosis. The insights garnered from this comparative analysis\nadvocate for the integration of advanced ViT models in clinical diagnostic\nworkflows, potentially revolutionizing the precision and reliability of KO\nassessments. This study does not introduce new architectural innovations but\nrather illuminates the robust applicability and comparative effectiveness of\npre-existing ViT models in a medical imaging context, specifically for KO\nseverity diagnosis. The insights garnered from this comparative analysis\nadvocate for the integration of advanced ViT models in clinical diagnostic\nworkflows, potentially revolutionizing the precision & reliability of KO\nassessments. The study utilizes an osteoarthritis dataset from the\nOsteoarthritis Initiative (OAI) comprising images with 5 severity categories\nand uneven class distribution. While classic machine learning models like\nGaussianNB and KNN struggle in feature extraction, Convolutional Neural\nNetworks such as Inception-V3, VGG-19 achieve better accuracy between 55-65% by\nlearning hierarchical visual patterns. However, Vision Transformer\narchitectures like Da-VIT, GCViT and MaxViT emerge as indisputable champions,\ndisplaying 66.14% accuracy, 0.703 precision, 0.614 recall, AUC exceeding 0.835\nthanks to self-attention processes.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.20062v1",
    "published_date": "2024-10-26 03:58:58 UTC",
    "updated_date": "2024-10-26 03:58:58 UTC"
  },
  {
    "arxiv_id": "2411.05798v1",
    "title": "A Genetic Algorithm for Multi-Capacity Fixed-Charge Flow Network Design",
    "authors": [
      "Caleb Eardley",
      "Dalton Gomez",
      "Ryan Dupuis",
      "Michael Papadopoulos",
      "Sean Yaw"
    ],
    "abstract": "The Multi-Capacity Fixed-Charge Network Flow (MC-FCNF) problem, a\ngeneralization of the Fixed-Charge Network Flow problem, aims to assign\ncapacities to edges in a flow network such that a target amount of flow can be\nhosted at minimum cost. The cost model for both problems dictates that the\nfixed cost of an edge is incurred for any non-zero amount of flow hosted by\nthat edge. This problem naturally arises in many areas including infrastructure\ndesign, transportation, telecommunications, and supply chain management. The\nMC-FCNF problem is NP-Hard, so solving large instances using exact techniques\nis impractical. This paper presents a genetic algorithm designed to quickly\nfind high-quality flow solutions to the MC-FCNF problem. The genetic algorithm\nuses a novel solution representation scheme that eliminates the need to repair\ninvalid flow solutions, which is an issue common to many other genetic\nalgorithms for the MC-FCNF problem. The genetic algorithm's efficiency is\ndisplayed with an evaluation using real-world CO2 capture and storage\ninfrastructure design data. The evaluation results highlight the genetic\nalgorithm's potential for solving large-scale network design problems.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.05798v1",
    "published_date": "2024-10-26 03:50:18 UTC",
    "updated_date": "2024-10-26 03:50:18 UTC"
  },
  {
    "arxiv_id": "2411.00813v1",
    "title": "Personality Analysis from Online Short Video Platforms with Multi-domain Adaptation",
    "authors": [
      "Sixu An",
      "Xiangguo Sun",
      "Yicong Li",
      "Yu Yang",
      "Guandong Xu"
    ],
    "abstract": "Personality analysis from online short videos has gained prominence due to\nits applications in personalized recommendation systems, sentiment analysis,\nand human-computer interaction. Traditional assessment methods, such as\nquestionnaires based on the Big Five Personality Framework, are limited by\nself-report biases and are impractical for large-scale or real-time analysis.\nLeveraging the rich, multi-modal data present in short videos offers a\npromising alternative for more accurate personality inference. However,\nintegrating these diverse and asynchronous modalities poses significant\nchallenges, particularly in aligning time-varying data and ensuring models\ngeneralize well to new domains with limited labeled data. In this paper, we\npropose a novel multi-modal personality analysis framework that addresses these\nchallenges by synchronizing and integrating features from multiple modalities\nand enhancing model generalization through domain adaptation. We introduce a\ntimestamp-based modality alignment mechanism that synchronizes data based on\nspoken word timestamps, ensuring accurate correspondence across modalities and\nfacilitating effective feature integration. To capture temporal dependencies\nand inter-modal interactions, we employ Bidirectional Long Short-Term Memory\nnetworks and self-attention mechanisms, allowing the model to focus on the most\ninformative features for personality prediction. Furthermore, we develop a\ngradient-based domain adaptation method that transfers knowledge from multiple\nsource domains to improve performance in target domains with scarce labeled\ndata. Extensive experiments on real-world datasets demonstrate that our\nframework significantly outperforms existing methods in personality prediction\ntasks, highlighting its effectiveness in capturing complex behavioral cues and\nrobustness in adapting to new domains.",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.CY",
      "cs.LG",
      "cs.SI",
      "eess.AS"
    ],
    "primary_category": "cs.MM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.00813v1",
    "published_date": "2024-10-26 03:29:32 UTC",
    "updated_date": "2024-10-26 03:29:32 UTC"
  },
  {
    "arxiv_id": "2410.21318v1",
    "title": "Multi-path Exploration and Feedback Adjustment for Text-to-Image Person Retrieval",
    "authors": [
      "Bin Kang",
      "Bin Chen",
      "Junjie Wang",
      "Yong Xu"
    ],
    "abstract": "Text-based person retrieval aims to identify the specific persons using\ntextual descriptions as queries. Existing ad vanced methods typically depend on\nvision-language pre trained (VLP) models to facilitate effective cross-modal\nalignment. However, the inherent constraints of VLP mod-els, which include the\nglobal alignment biases and insuffi-cient self-feedback regulation, impede\noptimal retrieval per formance. In this paper, we propose MeFa, a Multi-Pathway\nExploration, Feedback, and Adjustment framework, which deeply explores\nintrinsic feedback of intra and inter-modal to make targeted adjustment,\nthereby achieving more precise person-text associations. Specifically, we first\ndesign an intra modal reasoning pathway that generates hard negative sam ples\nfor cross-modal data, leveraging feedback from these samples to refine\nintra-modal reasoning, thereby enhancing sensitivity to subtle discrepancies.\nSubsequently, we intro duce a cross-modal refinement pathway that utilizes both\nglobal information and intermodal feedback to refine local in formation, thus\nenhancing its global semantic representation. Finally, the discriminative clue\ncorrection pathway incorpo rates fine-grained features of secondary similarity\nas discrim inative clues to further mitigate retrieval failures caused by\ndisparities in these features. Experimental results on three public benchmarks\ndemonstrate that MeFa achieves superior person retrieval performance without\nnecessitating additional data or complex structures.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.21318v1",
    "published_date": "2024-10-26 03:25:27 UTC",
    "updated_date": "2024-10-26 03:25:27 UTC"
  },
  {
    "arxiv_id": "2410.20054v1",
    "title": "Evaluating Neural Networks for Early Maritime Threat Detection",
    "authors": [
      "Dhanush Tella",
      "Chandra Teja Tiriveedhi",
      "Naphtali Rishe",
      "Dan E. Tamir",
      "Jonathan I. Tamir"
    ],
    "abstract": "We consider the task of classifying trajectories of boat activities as a\nproxy for assessing maritime threats. Previous approaches have considered\nentropy-based metrics for clustering boat activity into three broad categories:\nrandom walk, following, and chasing. Here, we comprehensively assess the\naccuracy of neural network-based approaches as alternatives to entropy-based\nclustering. We train four neural network models and compare them to shallow\nlearning using synthetic data. We also investigate the accuracy of models as\ntime steps increase and with and without rotated data. To improve test-time\nrobustness, we normalize trajectories and perform rotation-based data\naugmentation. Our results show that deep networks can achieve a test-set\naccuracy of up to 100% on a full trajectory, with graceful degradation as the\nnumber of time steps decreases, outperforming entropy-based clustering.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.20054v1",
    "published_date": "2024-10-26 03:05:28 UTC",
    "updated_date": "2024-10-26 03:05:28 UTC"
  },
  {
    "arxiv_id": "2411.08882v1",
    "title": "A Novel Multimodal System to Predict Agitation in People with Dementia Within Clinical Settings: A Proof of Concept",
    "authors": [
      "Abeer Badawi",
      "Somayya Elmoghazy",
      "Samira Choudhury",
      "Sara Elgazzar",
      "Khalid Elgazzar",
      "Amer Burhan"
    ],
    "abstract": "Dementia is a neurodegenerative condition that combines several diseases and\nimpacts millions around the world and those around them. Although cognitive\nimpairment is profoundly disabling, it is the noncognitive features of\ndementia, referred to as Neuropsychiatric Symptoms (NPS), that are most closely\nassociated with a diminished quality of life. Agitation and aggression (AA) in\npeople living with dementia (PwD) contribute to distress and increased\nhealthcare demands. Current assessment methods rely on caregiver intervention\nand reporting of incidents, introducing subjectivity and bias. Artificial\nIntelligence (AI) and predictive algorithms offer a potential solution for\ndetecting AA episodes in PwD when utilized in real-time. We present a 5-year\nstudy system that integrates a multimodal approach, utilizing the EmbracePlus\nwristband and a video detection system to predict AA in severe dementia\npatients. We conducted a pilot study with three participants at the Ontario\nShores Mental Health Institute to validate the functionality of the system. The\nsystem collects and processes raw and digital biomarkers from the EmbracePlus\nwristband to accurately predict AA. The system also detected pre-agitation\npatterns at least six minutes before the AA event, which was not previously\ndiscovered from the EmbracePlus wristband. Furthermore, the privacy-preserving\nvideo system uses a masking tool to hide the features of the people in frames\nand employs a deep learning model for AA detection. The video system also helps\nidentify the actual start and end time of the agitation events for labeling.\nThe promising results of the preliminary data analysis underscore the ability\nof the system to predict AA events. The ability of the proposed system to run\nautonomously in real-time and identify AA and pre-agitation symptoms without\nexternal assistance represents a significant milestone in this research field.",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.MM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08882v1",
    "published_date": "2024-10-26 02:59:34 UTC",
    "updated_date": "2024-10-26 02:59:34 UTC"
  },
  {
    "arxiv_id": "2410.20050v1",
    "title": "AutoMIR: Effective Zero-Shot Medical Information Retrieval without Relevance Labels",
    "authors": [
      "Lei Li",
      "Xiangxu Zhang",
      "Xiao Zhou",
      "Zheng Liu"
    ],
    "abstract": "Medical information retrieval (MIR) is essential for retrieving relevant\nmedical knowledge from diverse sources, including electronic health records,\nscientific literature, and medical databases. However, achieving effective\nzero-shot dense retrieval in the medical domain poses substantial challenges\ndue to the lack of relevance-labeled data. In this paper, we introduce a novel\napproach called Self-Learning Hypothetical Document Embeddings (SL-HyDE) to\ntackle this issue. SL-HyDE leverages large language models (LLMs) as generators\nto generate hypothetical documents based on a given query. These generated\ndocuments encapsulate key medical context, guiding a dense retriever in\nidentifying the most relevant documents. The self-learning framework\nprogressively refines both pseudo-document generation and retrieval, utilizing\nunlabeled medical corpora without requiring any relevance-labeled data.\nAdditionally, we present the Chinese Medical Information Retrieval Benchmark\n(CMIRB), a comprehensive evaluation framework grounded in real-world medical\nscenarios, encompassing five tasks and ten datasets. By benchmarking ten models\non CMIRB, we establish a rigorous standard for evaluating medical information\nretrieval systems. Experimental results demonstrate that SL-HyDE significantly\nsurpasses existing methods in retrieval accuracy while showcasing strong\ngeneralization and scalability across various LLM and retriever configurations.\nCMIRB data and evaluation code are publicly available at:\nhttps://github.com/CMIRB-benchmark/CMIRB.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "15 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.20050v1",
    "published_date": "2024-10-26 02:53:20 UTC",
    "updated_date": "2024-10-26 02:53:20 UTC"
  },
  {
    "arxiv_id": "2410.20046v1",
    "title": "DQRM: Deep Quantized Recommendation Models",
    "authors": [
      "Yang Zhou",
      "Zhen Dong",
      "Ellick Chan",
      "Dhiraj Kalamkar",
      "Diana Marculescu",
      "Kurt Keutzer"
    ],
    "abstract": "Large-scale recommendation models are currently the dominant workload for\nmany large Internet companies. These recommenders are characterized by massive\nembedding tables that are sparsely accessed by the index for user and item\nfeatures. The size of these 1TB+ tables imposes a severe memory bottleneck for\nthe training and inference of recommendation models. In this work, we propose a\nnovel recommendation framework that is small, powerful, and efficient to run\nand train, based on the state-of-the-art Deep Learning Recommendation Model\n(DLRM). The proposed framework makes inference more efficient on the cloud\nservers, explores the possibility of deploying powerful recommenders on smaller\nedge devices, and optimizes the workload of the communication overhead in\ndistributed training under the data parallelism settings. Specifically, we show\nthat quantization-aware training (QAT) can impose a strong regularization\neffect to mitigate the severe overfitting issues suffered by DLRMs.\nConsequently, we achieved INT4 quantization of DLRM models without any accuracy\ndrop. We further propose two techniques that improve and accelerate the\nconventional QAT workload specifically for the embedding tables in the\nrecommendation models. Furthermore, to achieve efficient training, we quantize\nthe gradients of the embedding tables into INT8 on top of the well-supported\nspecified sparsification. We show that combining gradient sparsification and\nquantization together significantly reduces the amount of communication.\nBriefly, DQRM models with INT4 can achieve 79.07% accuracy on Kaggle with 0.27\nGB model size, and 81.21% accuracy on the Terabyte dataset with 1.57 GB, which\neven outperform FP32 DLRMs that have much larger model sizes (2.16 GB on Kaggle\nand 12.58 on Terabyte).",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.20046v1",
    "published_date": "2024-10-26 02:33:52 UTC",
    "updated_date": "2024-10-26 02:33:52 UTC"
  },
  {
    "arxiv_id": "2410.20037v1",
    "title": "Roles of LLMs in the Overall Mental Architecture",
    "authors": [
      "Ron Sun"
    ],
    "abstract": "To better understand existing LLMs, we may examine the human mental\n(cognitive/psychological) architecture, and its components and structures.\nBased on psychological, philosophical, and cognitive science literatures, it is\nargued that, within the human mental architecture, existing LLMs correspond\nwell with implicit mental processes (intuition, instinct, and so on). However,\nbeyond such implicit processes, explicit processes (with better symbolic\ncapabilities) are also present within the human mental architecture, judging\nfrom psychological, philosophical, and cognitive science literatures. Various\ntheoretical and empirical issues and questions in this regard are explored.\nFurthermore, it is argued that existing dual-process computational cognitive\narchitectures (models of the human cognitive/psychological architecture)\nprovide usable frameworks for fundamentally enhancing LLMs by introducing dual\nprocesses (both implicit and explicit) and, in the meantime, can also be\nenhanced by LLMs. The results are synergistic combinations (in several\ndifferent senses simultaneously).",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "q-bio.NC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.20037v1",
    "published_date": "2024-10-26 01:13:44 UTC",
    "updated_date": "2024-10-26 01:13:44 UTC"
  },
  {
    "arxiv_id": "2410.20036v1",
    "title": "Architectural Flaw Detection in Civil Engineering Using GPT-4",
    "authors": [
      "Saket Kumar",
      "Abul Ehtesham",
      "Aditi Singh",
      "Tala Talaei Khoei"
    ],
    "abstract": "The application of artificial intelligence (AI) in civil engineering presents\na transformative approach to enhancing design quality and safety. This paper\ninvestigates the potential of the advanced LLM GPT4 Turbo vision model in\ndetecting architectural flaws during the design phase, with a specific focus on\nidentifying missing doors and windows. The study evaluates the model's\nperformance through metrics such as precision, recall, and F1 score,\ndemonstrating AI's effectiveness in accurately detecting flaws compared to\nhuman-verified data. Additionally, the research explores AI's broader\ncapabilities, including identifying load-bearing issues, material weaknesses,\nand ensuring compliance with building codes. The findings highlight how AI can\nsignificantly improve design accuracy, reduce costly revisions, and support\nsustainable practices, ultimately revolutionizing the civil engineering field\nby ensuring safer, more efficient, and aesthetically optimized structures.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.20036v1",
    "published_date": "2024-10-26 01:10:04 UTC",
    "updated_date": "2024-10-26 01:10:04 UTC"
  },
  {
    "arxiv_id": "2410.20035v1",
    "title": "Training the Untrainable: Introducing Inductive Bias via Representational Alignment",
    "authors": [
      "Vighnesh Subramaniam",
      "David Mayo",
      "Colin Conwell",
      "Tomaso Poggio",
      "Boris Katz",
      "Brian Cheung",
      "Andrei Barbu"
    ],
    "abstract": "We demonstrate that architectures which traditionally are considered to be\nill-suited for a task can be trained using inductive biases from another\narchitecture. Networks are considered untrainable when they overfit, underfit,\nor converge to poor results even when tuning their hyperparameters. For\nexample, plain fully connected networks overfit on object recognition while\ndeep convolutional networks without residual connections underfit. The\ntraditional answer is to change the architecture to impose some inductive bias,\nalthough what that bias is remains unknown. We introduce guidance, where a\nguide network guides a target network using a neural distance function. The\ntarget is optimized to perform well and to match its internal representations,\nlayer-by-layer, to those of the guide; the guide is unchanged. If the guide is\ntrained, this transfers over part of the architectural prior and knowledge of\nthe guide to the target. If the guide is untrained, this transfers over only\npart of the architectural prior of the guide. In this manner, we can\ninvestigate what kinds of priors different architectures place on untrainable\nnetworks such as fully connected networks. We demonstrate that this method\novercomes the immediate overfitting of fully connected networks on vision\ntasks, makes plain CNNs competitive to ResNets, closes much of the gap between\nplain vanilla RNNs and Transformers, and can even help Transformers learn tasks\nwhich RNNs can perform more easily. We also discover evidence that better\ninitializations of fully connected networks likely exist to avoid overfitting.\nOur method provides a mathematical tool to investigate priors and\narchitectures, and in the long term, may demystify the dark art of architecture\ncreation, even perhaps turning architectures into a continuous optimizable\nparameter of the network.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Under Review; 24 pages, 9 figures; Project page and code is at\n  https://untrainable-networks.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2410.20035v1",
    "published_date": "2024-10-26 01:04:03 UTC",
    "updated_date": "2024-10-26 01:04:03 UTC"
  },
  {
    "arxiv_id": "2410.20034v1",
    "title": "Sensor2Text: Enabling Natural Language Interactions for Daily Activity Tracking Using Wearable Sensors",
    "authors": [
      "Wenqiang Chen",
      "Jiaxuan Cheng",
      "Leyao Wang",
      "Wei Zhao",
      "Wojciech Matusik"
    ],
    "abstract": "Visual Question-Answering, a technology that generates textual responses from\nan image and natural language question, has progressed significantly. Notably,\nit can aid in tracking and inquiring about daily activities, crucial in\nhealthcare monitoring, especially for elderly patients or those with memory\ndisabilities. However, video poses privacy concerns and has a limited field of\nview. This paper presents Sensor2Text, a model proficient in tracking daily\nactivities and engaging in conversations using wearable sensors. The approach\noutlined here tackles several challenges, including low information density in\nwearable sensor data, insufficiency of single wearable sensors in human\nactivities recognition, and model's limited capacity for Question-Answering and\ninteractive conversations. To resolve these obstacles, transfer learning and\nstudent-teacher networks are utilized to leverage knowledge from\nvisual-language models. Additionally, an encoder-decoder neural network model\nis devised to jointly process language and sensor data for conversational\npurposes. Furthermore, Large Language Models are also utilized to enable\ninteractive capabilities. The model showcases the ability to identify human\nactivities and engage in Q\\&A dialogues using various wearable sensor\nmodalities. It performs comparably to or better than existing visual-language\nmodels in both captioning and conversational tasks. To our knowledge, this\nrepresents the first model capable of conversing about wearable sensor data,\noffering an innovative approach to daily activity tracking that addresses\nprivacy and field-of-view limitations associated with current vision-based\nsolutions.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.20034v1",
    "published_date": "2024-10-26 01:03:13 UTC",
    "updated_date": "2024-10-26 01:03:13 UTC"
  },
  {
    "arxiv_id": "2410.20030v1",
    "title": "SCube: Instant Large-Scale Scene Reconstruction using VoxSplats",
    "authors": [
      "Xuanchi Ren",
      "Yifan Lu",
      "Hanxue Liang",
      "Zhangjie Wu",
      "Huan Ling",
      "Mike Chen",
      "Sanja Fidler",
      "Francis Williams",
      "Jiahui Huang"
    ],
    "abstract": "We present SCube, a novel method for reconstructing large-scale 3D scenes\n(geometry, appearance, and semantics) from a sparse set of posed images. Our\nmethod encodes reconstructed scenes using a novel representation VoxSplat,\nwhich is a set of 3D Gaussians supported on a high-resolution sparse-voxel\nscaffold. To reconstruct a VoxSplat from images, we employ a hierarchical voxel\nlatent diffusion model conditioned on the input images followed by a\nfeedforward appearance prediction model. The diffusion model generates\nhigh-resolution grids progressively in a coarse-to-fine manner, and the\nappearance network predicts a set of Gaussians within each voxel. From as few\nas 3 non-overlapping input images, SCube can generate millions of Gaussians\nwith a 1024^3 voxel grid spanning hundreds of meters in 20 seconds. Past works\ntackling scene reconstruction from images either rely on per-scene optimization\nand fail to reconstruct the scene away from input views (thus requiring dense\nview coverage as input) or leverage geometric priors based on low-resolution\nmodels, which produce blurry results. In contrast, SCube leverages\nhigh-resolution sparse networks and produces sharp outputs from few views. We\nshow the superiority of SCube compared to prior art using the Waymo\nself-driving dataset on 3D reconstruction and demonstrate its applications,\nsuch as LiDAR simulation and text-to-scene generation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "NeurIPS 2024. Project page:\n  https://research.nvidia.com/labs/toronto-ai/scube/",
    "pdf_url": "http://arxiv.org/pdf/2410.20030v1",
    "published_date": "2024-10-26 00:52:46 UTC",
    "updated_date": "2024-10-26 00:52:46 UTC"
  },
  {
    "arxiv_id": "2410.20027v2",
    "title": "Agentic Feedback Loop Modeling Improves Recommendation and User Simulation",
    "authors": [
      "Shihao Cai",
      "Jizhi Zhang",
      "Keqin Bao",
      "Chongming Gao",
      "Qifan Wang",
      "Fuli Feng",
      "Xiangnan He"
    ],
    "abstract": "Large language model-based agents are increasingly applied in the\nrecommendation field due to their extensive knowledge and strong planning\ncapabilities. While prior research has primarily focused on enhancing either\nthe recommendation agent or the user agent individually, the collaborative\ninteraction between the two has often been overlooked. Towards this research\ngap, we propose a novel framework that emphasizes the feedback loop process to\nfacilitate the collaboration between the recommendation agent and the user\nagent. Specifically, the recommendation agent refines its understanding of user\npreferences by analyzing the feedback from the user agent on the item\nrecommendation. Conversely, the user agent further identifies potential user\ninterests based on the items and recommendation reasons provided by the\nrecommendation agent. This iterative process enhances the ability of both\nagents to infer user behaviors, enabling more effective item recommendations\nand more accurate user simulations. Extensive experiments on three datasets\ndemonstrate the effectiveness of the agentic feedback loop: the agentic\nfeedback loop yields an average improvement of 11.52% over the single\nrecommendation agent and 21.12% over the single user agent. Furthermore, the\nresults show that the agentic feedback loop does not exacerbate popularity or\nposition bias, which are typically amplified by the real-world feedback loop,\nhighlighting its robustness. The source code is available at\nhttps://github.com/Lanyu0303/AFL.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.20027v2",
    "published_date": "2024-10-26 00:51:39 UTC",
    "updated_date": "2025-05-02 00:50:50 UTC"
  },
  {
    "arxiv_id": "2410.20024v1",
    "title": "Beyond Fine-Tuning: Effective Strategies for Mitigating Hallucinations in Large Language Models for Data Analytics",
    "authors": [
      "Mikhail Rumiantsau",
      "Aliaksei Vertsel",
      "Ilya Hrytsuk",
      "Isaiah Ballah"
    ],
    "abstract": "Large Language Models (LLMs) have become increasingly important in natural\nlanguage processing, enabling advanced data analytics through natural language\nqueries. However, these models often generate \"hallucinations\"-inaccurate or\nfabricated information-that can undermine their reliability in critical\ndata-driven decision-making. Addressing the challenge of hallucinations is\nessential to improve the accuracy and trustworthiness of LLMs in processing\nnatural language queries. This research focuses on mitigating hallucinations in\nLLMs, specifically within the context of data analytics. We introduce and\nevaluate four targeted strategies: Structured Output Generation, Strict Rules\nEnforcement, System Prompt Enhancements, and Semantic Layer Integration. Our\nfindings show that these methods are more effective than traditional\nfine-tuning approaches in reducing hallucinations, offering a more reliable\nframework for deploying LLMs in natural language queries for data analytics.\nThis research demonstrates the potential of these strategies to enhance the\naccuracy of LLM-driven data queries, ensuring more dependable results in\ndata-driven environments.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.20024v1",
    "published_date": "2024-10-26 00:45:42 UTC",
    "updated_date": "2024-10-26 00:45:42 UTC"
  },
  {
    "arxiv_id": "2410.21317v1",
    "title": "MatExpert: Decomposing Materials Discovery by Mimicking Human Experts",
    "authors": [
      "Qianggang Ding",
      "Santiago Miret",
      "Bang Liu"
    ],
    "abstract": "Material discovery is a critical research area with profound implications for\nvarious industries. In this work, we introduce MatExpert, a novel framework\nthat leverages Large Language Models (LLMs) and contrastive learning to\naccelerate the discovery and design of new solid-state materials. Inspired by\nthe workflow of human materials design experts, our approach integrates three\nkey stages: retrieval, transition, and generation. First, in the retrieval\nstage, MatExpert identifies an existing material that closely matches the\ndesired criteria. Second, in the transition stage, MatExpert outlines the\nnecessary modifications to transform this material formulation to meet specific\nrequirements outlined by the initial user query. Third, in the generation\nstate, MatExpert performs detailed computations and structural generation to\ncreate new materials based on the provided information. Our experimental\nresults demonstrate that MatExpert outperforms state-of-the-art methods in\nmaterial generation tasks, achieving superior performance across various\nmetrics including validity, distribution, and stability. As such, MatExpert\nrepresents a meaningful advancement in computational material discovery using\nlangauge-based generative models.",
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cond-mat.mtrl-sci",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.21317v1",
    "published_date": "2024-10-26 00:44:54 UTC",
    "updated_date": "2024-10-26 00:44:54 UTC"
  },
  {
    "arxiv_id": "2410.21316v1",
    "title": "Deep Optimizer States: Towards Scalable Training of Transformer Models Using Interleaved Offloading",
    "authors": [
      "Avinash Maurya",
      "Jie Ye",
      "M. Mustafa Rafique",
      "Franck Cappello",
      "Bogdan Nicolae"
    ],
    "abstract": "Transformers and large language models~(LLMs) have seen rapid adoption in all\ndomains. Their sizes have exploded to hundreds of billions of parameters and\nkeep increasing. Under these circumstances, the training of transformers is\nvery expensive and often hits a ``memory wall'', i.e., even when using 3D\nparallelism (pipeline, tensor, data) and aggregating the memory of many GPUs,\nit is still not enough to hold the necessary data structures (model parameters,\noptimizer state, gradients, activations) in GPU memory. To compensate,\nstate-of-the-art approaches offload the optimizer state, at least partially, to\nthe host memory and perform hybrid CPU-GPU computations. However, the\nmanagement of the combined host-GPU memory is often suboptimal and results in\npoor overlapping between data movements and computations. This leads to missed\nopportunities to simultaneously leverage the interconnect bandwidth and\ncomputational capabilities of CPUs and GPUs. In this paper, we leverage a key\nobservation that the interleaving of the forward, backward and update phases\ngenerate fluctuations in the GPU memory utilization, which can be exploited to\ndynamically move a part of the optimizer state between the host and the GPU\nmemory at each iteration. To this end, we design and implement \\proj, a novel\ntechnique to split the LLM into subgroups, whose update phase is scheduled on\neither the CPU or the GPU based on our proposed performance model that\naddresses the trade-off between data movement cost, acceleration on the GPUs vs\nthe CPUs, and competition for shared resources. We integrate our approach with\nDeepSpeed and demonstrate 2.5$\\times$ faster iterations over state-of-the-art\napproaches using extensive experiments.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "cs.ET",
      "cs.PF"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.21316v1",
    "published_date": "2024-10-26 00:43:59 UTC",
    "updated_date": "2024-10-26 00:43:59 UTC"
  },
  {
    "arxiv_id": "2410.20021v2",
    "title": "Think Carefully and Check Again! Meta-Generation Unlocking LLMs for Low-Resource Cross-Lingual Summarization",
    "authors": [
      "Zhecheng Li",
      "Yiwei Wang",
      "Bryan Hooi",
      "Yujun Cai",
      "Naifan Cheung",
      "Nanyun Peng",
      "Kai-wei Chang"
    ],
    "abstract": "Cross-lingual summarization (CLS) aims to generate a summary for the source\ntext in a different target language. Currently, instruction-tuned large\nlanguage models (LLMs) excel at various English tasks. However, unlike\nlanguages such as English, Chinese or Spanish, for those relatively\nlow-resource languages with limited usage or data, recent studies have shown\nthat LLMs' performance on CLS tasks remains unsatisfactory even with few-shot\nsettings. This raises the question: Are LLMs capable of handling cross-lingual\nsummarization tasks for low-resource languages? To resolve this question, we\nfully explore the potential of large language models on cross-lingual\nsummarization task for low-resource languages through our four-step zero-shot\nmethod: Summarization, Improvement, Translation and Refinement (SITR) with\ncorrespondingly designed prompts. We test our proposed method with multiple\nLLMs on two well-known cross-lingual summarization datasets with various\nlow-resource target languages. The results show that: i) GPT-3.5 and GPT-4\nsignificantly and consistently outperform other baselines when using our\nzero-shot SITR methods. ii) By employing our proposed method, we unlock the\npotential of LLMs, enabling them to effectively handle cross-lingual\nsummarization tasks for relatively low-resource languages.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.20021v2",
    "published_date": "2024-10-26 00:39:44 UTC",
    "updated_date": "2025-03-25 05:11:24 UTC"
  },
  {
    "arxiv_id": "2410.20018v1",
    "title": "GHIL-Glue: Hierarchical Control with Filtered Subgoal Images",
    "authors": [
      "Kyle B. Hatch",
      "Ashwin Balakrishna",
      "Oier Mees",
      "Suraj Nair",
      "Seohong Park",
      "Blake Wulfe",
      "Masha Itkina",
      "Benjamin Eysenbach",
      "Sergey Levine",
      "Thomas Kollar",
      "Benjamin Burchfiel"
    ],
    "abstract": "Image and video generative models that are pre-trained on Internet-scale data\ncan greatly increase the generalization capacity of robot learning systems.\nThese models can function as high-level planners, generating intermediate\nsubgoals for low-level goal-conditioned policies to reach. However, the\nperformance of these systems can be greatly bottlenecked by the interface\nbetween generative models and low-level controllers. For example, generative\nmodels may predict photorealistic yet physically infeasible frames that confuse\nlow-level policies. Low-level policies may also be sensitive to subtle visual\nartifacts in generated goal images. This paper addresses these two facets of\ngeneralization, providing an interface to effectively \"glue together\"\nlanguage-conditioned image or video prediction models with low-level\ngoal-conditioned policies. Our method, Generative Hierarchical Imitation\nLearning-Glue (GHIL-Glue), filters out subgoals that do not lead to task\nprogress and improves the robustness of goal-conditioned policies to generated\nsubgoals with harmful visual artifacts. We find in extensive experiments in\nboth simulated and real environments that GHIL-Glue achieves a 25% improvement\nacross several hierarchical models that leverage generative subgoals, achieving\na new state-of-the-art on the CALVIN simulation benchmark for policies using\nobservations from a single RGB camera. GHIL-Glue also outperforms other\ngeneralist robot policies across 3/4 language-conditioned manipulation tasks\ntesting zero-shot generalization in physical experiments.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Code, model checkpoints and videos can be found at\n  https://ghil-glue.github.io",
    "pdf_url": "http://arxiv.org/pdf/2410.20018v1",
    "published_date": "2024-10-26 00:32:21 UTC",
    "updated_date": "2024-10-26 00:32:21 UTC"
  },
  {
    "arxiv_id": "2410.20017v1",
    "title": "Off-Policy Selection for Initiating Human-Centric Experimental Design",
    "authors": [
      "Ge Gao",
      "Xi Yang",
      "Qitong Gao",
      "Song Ju",
      "Miroslav Pajic",
      "Min Chi"
    ],
    "abstract": "In human-centric tasks such as healthcare and education, the heterogeneity\namong patients and students necessitates personalized treatments and\ninstructional interventions. While reinforcement learning (RL) has been\nutilized in those tasks, off-policy selection (OPS) is pivotal to close the\nloop by offline evaluating and selecting policies without online interactions,\nyet current OPS methods often overlook the heterogeneity among participants.\nOur work is centered on resolving a pivotal challenge in human-centric systems\n(HCSs): how to select a policy to deploy when a new participant joining the\ncohort, without having access to any prior offline data collected over the\nparticipant? We introduce First-Glance Off-Policy Selection (FPS), a novel\napproach that systematically addresses participant heterogeneity through\nsub-group segmentation and tailored OPS criteria to each sub-group. By grouping\nindividuals with similar traits, FPS facilitates personalized policy selection\naligned with unique characteristics of each participant or group of\nparticipants. FPS is evaluated via two important but challenging applications,\nintelligent tutoring systems and a healthcare application for sepsis treatment\nand intervention. FPS presents significant advancement in enhancing learning\noutcomes of students and in-hospital care outcomes.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.20017v1",
    "published_date": "2024-10-26 00:17:33 UTC",
    "updated_date": "2024-10-26 00:17:33 UTC"
  }
]
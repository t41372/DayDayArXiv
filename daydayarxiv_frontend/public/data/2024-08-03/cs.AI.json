{
  "date": "2024-08-03",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-08-03 的 arXiv 中文 TLDR 快报！今天 arXiv 更新了 39 篇论文，主要聚焦 AI 模型（如 LLM 在知识图谱和医疗应用中的创新）、图像处理、机器人学习和核聚变模拟等领域，其中令人印象深刻的包括核聚变模拟加速的 Diff-PIC，以及医疗 AI 系统 MALADE；知名学者如 Somesh Jha 和 Ying Nian Wu 等参与的相关工作值得关注。\n\n### 重点论文讨论\n我们挑选了最具影响力和话题度的论文优先讨论，将相关主题归类，并快速掠过较为常规或次要的文章。以下聚焦于 AI 安全、医疗应用、机器人协作和模拟技术等领域。\n\n**1. 知识图谱推理：双代理高效引导探索 (Walk Wisely on Graph: Knowledge Graph Reasoning with Dual Agents via Efficient Guidance-Exploration)**  \n这篇论文提出 FULORA 模型，使用分层强化学习（HRL）解决知识图谱的多跳推理问题。通过高低层代理的引导-探索机制，提升了在稀疏图谱上的长距离推理性能。贡献在于改善了早阶段学习效率，并在三个真实数据集上超越了基于强化学习的基线，适用于复杂 AI 系统。\n\n**2. 医疗 AI 多代理系统：MALADE (MALADE: Orchestration of LLM-powered Agents with Retrieval Augmented Generation for Pharmacovigilance)**  \n由 Somesh Jha 等学者主导，这篇论文引入 MALADE 框架，利用 LLM 和检索增强生成（RAG）从药物标签中提取不良事件（ADE）。它支持多种外部数据源，提供结构化输出和解释，在 MLHC'24 发表。发现显示其在 OMOP 数据集上达到 0.90 的 AUC，显著提升了药物安全监控的可靠性和解释性。\n\n**3. 核聚变模拟加速：Diff-PIC (Diff-PIC: Revolutionizing Particle-In-Cell Nuclear Fusion Simulation with Diffusion Models)**  \n这篇论文使用条件扩散模型优化粒子模拟，实现了 16,200 倍的速度提升，同时减少 MAE/RMSE/FID 误差。该模型通过物理信息编码和修正流技术，生成高保真 LPI 数据，针对核聚变研究的计算瓶颈，具有重大实际意义。\n\n**4. 多机器人协作综述：State-of-the-art in Robot Learning for Multi-Robot Collaboration: A Comprehensive Survey (State-of-the-art in Robot Learning for Multi-Robot Collaboration: A Comprehensive Survey)**  \n论文综述了机器人学习在多机器人合作中的最新进展，讨论了统计方法和挑战。贡献在于量化分析人类/动物启发的框架，并预测未来趋势，对于推进大规模机器人集成有指导价值。\n\n**5. LLM 在情感检测中的应用：Chain of Stance (Chain of Stance: Stance Detection with Large Language Models)**  \n这篇工作提出 Chain of Stance 方法，将 LLM 作为专家检测器，通过分解为中间断言来提升情感立场检测性能。在 SemEval 2016 数据集上，零样本和少样本设置下达到 79.84% F1 分数，展示了 LLM 在 NLP 任务中的强大潜力。\n\n**6. 图像处理与生成：Landmark-guided Diffusion Model (Landmark-guided Diffusion Model for High-fidelity and Temporally Coherent Talking Head Generation)**  \n论文开发了一个两阶段扩散模型，先生成同步面部 landmarks，然后用于高保真视频生成。贡献在于解决嘴巴抖动问题，提高了音频驱动人脸生成的连贯性和准确性。\n\n其他论文如 U-MedSAM（医疗图像不确定性建模，提出不确定性损失函数，提升分割鲁棒性）和 CAR（对比无关医学图像配准，使用随机卷积增强泛化能力）也值得一提，它们在医疗领域优化了模型性能，但细节较常规，故快速掠过。\n\n剩余论文多为特定应用或改进，如 STDA（机器人行为预测）、PLUGH（LLM 空间推理基准）和 Advancing Green AI（轻量 CNN 用于作物病害检测），这些工作虽有贡献（如 PLUGH 评估 LLM 在空间任务上的表现），但影响力较小，仅补充：它们提供了实用工具，但未带来重大突破。\n\n总之，今天的更新突显 AI 在医疗和模拟领域的潜力，建议关注 LLM 和扩散模型的应用。如果您对特定主题感兴趣，可以查看这些论文的完整摘要！",
  "papers": [
    {
      "arxiv_id": "2408.01880v4",
      "title": "Walk Wisely on Graph: Knowledge Graph Reasoning with Dual Agents via Efficient Guidance-Exploration",
      "title_zh": "在图",
      "authors": [
        "Zijian Wang",
        "Bin Wang",
        "Haifeng Jing",
        "Huayu Li",
        "Hongbo Dou"
      ],
      "abstract": "Recent years, multi-hop reasoning has been widely studied for knowledge graph\n(KG) reasoning due to its efficacy and interpretability. However, previous\nmulti-hop reasoning approaches are subject to two primary shortcomings. First,\nagents struggle to learn effective and robust policies at the early phase due\nto sparse rewards. Second, these approaches often falter on specific datasets\nlike sparse knowledge graphs, where agents are required to traverse lengthy\nreasoning paths. To address these problems, we propose a multi-hop reasoning\nmodel with dual agents based on hierarchical reinforcement learning (HRL),\nwhich is named FULORA. FULORA tackles the above reasoning challenges by\neFficient GUidance-ExpLORAtion between dual agents. The high-level agent walks\non the simplified knowledge graph to provide stage-wise hints for the low-level\nagent walking on the original knowledge graph. In this framework, the low-level\nagent optimizes a value function that balances two objectives: (1) maximizing\nreturn, and (2) integrating efficient guidance from the high-level agent.\nExperiments conducted on three real-word knowledge graph datasets demonstrate\nthat FULORA outperforms RL-based baselines, especially in the case of\nlong-distance reasoning.",
      "tldr_zh": "该论文针对知识图谱（KG）多跳推理的挑战，包括奖励稀疏导致代理难以学习策略，以及在稀疏图谱上处理长路径的困难，提出了一种名为 FULORA 的双智能体模型，基于分层强化学习（HRL）。FULORA 通过高层次代理在简化知识图谱上提供阶段性指导，帮助低层次代理在原始知识图谱上优化价值函数，实现回报最大化和高效指导的平衡。实验在三个真实数据集上表明，该模型优于基于强化学习的基线，尤其在长距离推理任务中表现出色。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by AAAI-25",
      "pdf_url": "http://arxiv.org/pdf/2408.01880v4",
      "published_date": "2024-08-03 23:15:57 UTC",
      "updated_date": "2024-12-18 18:31:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:36:11.339671"
    },
    {
      "arxiv_id": "2408.01872v1",
      "title": "Safe Semi-Supervised Contrastive Learning Using In-Distribution Data as Positive Examples",
      "title_zh": "安全的半监督对比学习：使用分布内数据作为正例",
      "authors": [
        "Min Gu Kwak",
        "Hyungu Kahng",
        "Seoung Bum Kim"
      ],
      "abstract": "Semi-supervised learning methods have shown promising results in solving many\npractical problems when only a few labels are available. The existing methods\nassume that the class distributions of labeled and unlabeled data are equal;\nhowever, their performances are significantly degraded in class distribution\nmismatch scenarios where out-of-distribution (OOD) data exist in the unlabeled\ndata. Previous safe semi-supervised learning studies have addressed this\nproblem by making OOD data less likely to affect training based on labeled\ndata. However, even if the studies effectively filter out the unnecessary OOD\ndata, they can lose the basic information that all data share regardless of\nclass. To this end, we propose to apply a self-supervised contrastive learning\napproach to fully exploit a large amount of unlabeled data. We also propose a\ncontrastive loss function with coefficient schedule to aggregate as an anchor\nthe labeled negative examples of the same class into positive examples. To\nevaluate the performance of the proposed method, we conduct experiments on\nimage classification datasets - CIFAR-10, CIFAR-100, Tiny ImageNet, and\nCIFAR-100+Tiny ImageNet - under various mismatch ratios. The results show that\nself-supervised contrastive learning significantly improves classification\naccuracy. Moreover, aggregating the in-distribution examples produces better\nrepresentation and consequently further improves classification accuracy.",
      "tldr_zh": "该论文针对半监督学习（semi-supervised learning）中类分布不匹配问题提出了一种安全方法，特别是在无标签数据中存在 out-of-distribution (OOD) 数据时，现有方法性能会下降。研究者采用自监督对比学习（self-supervised contrastive learning）来充分利用无标签数据，并设计了一个带有系数调度的对比损失函数（contrastive loss function），将相同类的标签负例聚合为 in-distribution 正例，从而保留共享信息并提升模型鲁棒性。在 CIFAR-10、CIFAR-100、Tiny ImageNet 等图像分类数据集上的实验显示，该方法显著提高了分类准确率，尤其在各种不匹配比例下，进一步改善了数据表示和整体性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.01872v1",
      "published_date": "2024-08-03 22:33:13 UTC",
      "updated_date": "2024-08-03 22:33:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:36:22.478399"
    },
    {
      "arxiv_id": "2408.01869v1",
      "title": "MALADE: Orchestration of LLM-powered Agents with Retrieval Augmented Generation for Pharmacovigilance",
      "title_zh": "翻译失败",
      "authors": [
        "Jihye Choi",
        "Nils Palumbo",
        "Prasad Chalasani",
        "Matthew M. Engelhard",
        "Somesh Jha",
        "Anivarya Kumar",
        "David Page"
      ],
      "abstract": "In the era of Large Language Models (LLMs), given their remarkable text\nunderstanding and generation abilities, there is an unprecedented opportunity\nto develop new, LLM-based methods for trustworthy medical knowledge synthesis,\nextraction and summarization. This paper focuses on the problem of\nPharmacovigilance (PhV), where the significance and challenges lie in\nidentifying Adverse Drug Events (ADEs) from diverse text sources, such as\nmedical literature, clinical notes, and drug labels. Unfortunately, this task\nis hindered by factors including variations in the terminologies of drugs and\noutcomes, and ADE descriptions often being buried in large amounts of narrative\ntext. We present MALADE, the first effective collaborative multi-agent system\npowered by LLM with Retrieval Augmented Generation for ADE extraction from drug\nlabel data. This technique involves augmenting a query to an LLM with relevant\ninformation extracted from text resources, and instructing the LLM to compose a\nresponse consistent with the augmented data. MALADE is a general LLM-agnostic\narchitecture, and its unique capabilities are: (1) leveraging a variety of\nexternal sources, such as medical literature, drug labels, and FDA tools (e.g.,\nOpenFDA drug information API), (2) extracting drug-outcome association in a\nstructured format along with the strength of the association, and (3) providing\nexplanations for established associations. Instantiated with GPT-4 Turbo or\nGPT-4o, and FDA drug label data, MALADE demonstrates its efficacy with an Area\nUnder ROC Curve of 0.90 against the OMOP Ground Truth table of ADEs. Our\nimplementation leverages the Langroid multi-agent LLM framework and can be\nfound at https://github.com/jihyechoi77/malade.",
      "tldr_zh": "本研究针对Pharmacovigilance（药物警戒）领域，提出MALADE系统，这是一个基于Large Language Models (LLM)的协作多智能体框架，利用Retrieval Augmented Generation (RAG)从药物标签数据中提取Adverse Drug Events (ADEs)。MALADE的独特能力包括整合多种外部来源（如医疗文献和FDA工具）、以结构化格式输出药物-结果关联及其强度，并为这些关联提供解释，使其成为一个通用的LLM无关架构。实验结果显示，使用GPT-4 Turbo或GPT-4o处理FDA数据时，MALADE在ADE提取任务上达到Area Under ROC Curve (AUC)为0.90，与OMOP Ground Truth表相比表现出色。该系统通过开源实现（如Langroid框架），为可信赖的医疗知识提取和总结提供了新方法。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG",
        "cs.MA",
        "q-bio.QM"
      ],
      "primary_category": "cs.CL",
      "comment": "Paper published at Machine Learning for Healthcare 2024 (MLHC'24)",
      "pdf_url": "http://arxiv.org/pdf/2408.01869v1",
      "published_date": "2024-08-03 22:14:13 UTC",
      "updated_date": "2024-08-03 22:14:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:36:35.340182"
    },
    {
      "arxiv_id": "2408.11822v1",
      "title": "State-of-the-art in Robot Learning for Multi-Robot Collaboration: A Comprehensive Survey",
      "title_zh": "翻译失败",
      "authors": [
        "Bin Wu",
        "C Steve Suh"
      ],
      "abstract": "With the continuous breakthroughs in core technology, the dawn of large-scale\nintegration of robotic systems into daily human life is on the horizon.\nMulti-robot systems (MRS) built on this foundation are undergoing drastic\nevolution. The fusion of artificial intelligence technology with robot hardware\nis seeing broad application possibilities for MRS. This article surveys the\nstate-of-the-art of robot learning in the context of Multi-Robot Cooperation\n(MRC) of recent. Commonly adopted robot learning methods (or frameworks) that\nare inspired by humans and animals are reviewed and their advantages and\ndisadvantages are discussed along with the associated technical challenges. The\npotential trends of robot learning and MRS integration exploiting the merging\nof these methods with real-world applications is also discussed at length.\nSpecifically statistical methods are used to quantitatively corroborate the\nideas elaborated in the article.",
      "tldr_zh": "这篇论文对多机器人协作（Multi-Robot Collaboration）中的机器人学习进行了全面调查，审视了当前状态-of-the-art 技术及其在多机器人系统（Multi-Robot Systems, MRS）中的应用前景。随着人工智能与机器人硬件的融合，该研究回顾了受人类和动物启发的常用机器人学习方法，并分析了它们的优势、劣势以及相关技术挑战。论文还探讨了这些方法与实际应用的整合趋势，并通过统计方法进行定量支持，为未来大规模机器人整合提供参考。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Multi-robot, Cooperation, robot learning",
      "pdf_url": "http://arxiv.org/pdf/2408.11822v1",
      "published_date": "2024-08-03 21:22:08 UTC",
      "updated_date": "2024-08-03 21:22:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:36:45.265079"
    },
    {
      "arxiv_id": "2408.08881v3",
      "title": "Challenge Summary U-MedSAM: Uncertainty-aware MedSAM for Medical Image Segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Xin Wang",
        "Xiaoyu Liu",
        "Peng Huang",
        "Pu Huang",
        "Shu Hu",
        "Hongtu Zhu"
      ],
      "abstract": "Medical Image Foundation Models have proven to be powerful tools for mask\nprediction across various datasets. However, accurately assessing the\nuncertainty of their predictions remains a significant challenge. To address\nthis, we propose a new model, U-MedSAM, which integrates the MedSAM model with\nan uncertainty-aware loss function and the Sharpness-Aware Minimization\n(SharpMin) optimizer. The uncertainty-aware loss function automatically\ncombines region-based, distribution-based, and pixel-based loss designs to\nenhance segmentation accuracy and robustness. SharpMin improves generalization\nby finding flat minima in the loss landscape, thereby reducing overfitting. Our\nmethod was evaluated in the CVPR24 MedSAM on Laptop challenge, where U-MedSAM\ndemonstrated promising performance.",
      "tldr_zh": "本文提出 U-MedSAM 模型，这是一种 uncertainty-aware 的 MedSAM 变体，用于医疗图像分割，以解决 Medical Image Foundation Models 在预测不确定性评估方面的挑战。U-MedSAM 整合了 uncertainty-aware loss function（自动结合 region-based、distribution-based 和 pixel-based 损失设计）和 Sharpness-Aware Minimization (SharpMin) 优化器，前者提升了分割准确性和鲁棒性，后者通过寻找损失景观中的平坦最小值来改善泛化并减少过拟合。在 CVPR24 MedSAM on Laptop challenge 中，U-MedSAM 展示了出色的性能。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "arXiv admin note: text overlap with arXiv:2405.17496",
      "pdf_url": "http://arxiv.org/pdf/2408.08881v3",
      "published_date": "2024-08-03 20:41:35 UTC",
      "updated_date": "2025-01-17 02:51:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:36:59.084129"
    },
    {
      "arxiv_id": "2408.04650v2",
      "title": "Building Trust in Mental Health Chatbots: Safety Metrics and LLM-Based Evaluation Tools",
      "title_zh": "在心理健康聊天机器人中建立信任：安全指标和基于LLM的评估工具",
      "authors": [
        "Jung In Park",
        "Mahyar Abbasian",
        "Iman Azimi",
        "Dawn T. Bounds",
        "Angela Jun",
        "Jaesu Han",
        "Robert M. McCarron",
        "Jessica Borelli",
        "Parmida Safavi",
        "Sanaz Mirbaha",
        "Jia Li",
        "Mona Mahmoudi",
        "Carmen Wiedenhoeft",
        "Amir M. Rahmani"
      ],
      "abstract": "Objective: This study aims to develop and validate an evaluation framework to\nensure the safety and reliability of mental health chatbots, which are\nincreasingly popular due to their accessibility, human-like interactions, and\ncontext-aware support. Materials and Methods: We created an evaluation\nframework with 100 benchmark questions and ideal responses, and five guideline\nquestions for chatbot responses. This framework, validated by mental health\nexperts, was tested on a GPT-3.5-turbo-based chatbot. Automated evaluation\nmethods explored included large language model (LLM)-based scoring, an agentic\napproach using real-time data, and embedding models to compare chatbot\nresponses against ground truth standards. Results: The results highlight the\nimportance of guidelines and ground truth for improving LLM evaluation\naccuracy. The agentic method, dynamically accessing reliable information,\ndemonstrated the best alignment with human assessments. Adherence to a\nstandardized, expert-validated framework significantly enhanced chatbot\nresponse safety and reliability. Discussion: Our findings emphasize the need\nfor comprehensive, expert-tailored safety evaluation metrics for mental health\nchatbots. While LLMs have significant potential, careful implementation is\nnecessary to mitigate risks. The superior performance of the agentic approach\nunderscores the importance of real-time data access in enhancing chatbot\nreliability. Conclusion: The study validated an evaluation framework for mental\nhealth chatbots, proving its effectiveness in improving safety and reliability.\nFuture work should extend evaluations to accuracy, bias, empathy, and privacy\nto ensure holistic assessment and responsible integration into healthcare.\nStandardized evaluations will build trust among users and professionals,\nfacilitating broader adoption and improved mental health support through\ntechnology.",
      "tldr_zh": "本研究旨在开发并验证一个评估框架，以确保心理健康聊天机器人的安全性和可靠性，解决其在可访问性、人性化互动和上下文支持方面的潜在风险。研究团队创建了包含100个基准问题、理想响应和5个指导性问题的框架，由心理健康专家验证，并在基于GPT-3.5-turbo的聊天机器人上进行测试，探索了LLM-based scoring、agentic approach（使用实时数据）和embedding models等自动化评估方法。结果显示，agentic approach与人类评估最一致，遵守标准化框架显著提高了聊天机器人响应的安全性和可靠性。讨论强调，LLMs具有巨大潜力，但需谨慎实施以降低风险，而未来工作应扩展到准确性、偏差、同理心和隐私评估，以促进技术和医疗领域的信任与广泛采用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.04650v2",
      "published_date": "2024-08-03 19:57:49 UTC",
      "updated_date": "2025-03-01 00:49:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:37:10.847822"
    },
    {
      "arxiv_id": "2408.05341v1",
      "title": "CAR: Contrast-Agnostic Deformable Medical Image Registration with Contrast-Invariant Latent Regularization",
      "title_zh": "翻译失败",
      "authors": [
        "Yinsong Wang",
        "Siyi Du",
        "Shaoming Zheng",
        "Xinzhe Luo",
        "Chen Qin"
      ],
      "abstract": "Multi-contrast image registration is a challenging task due to the complex\nintensity relationships between different imaging contrasts. Conventional image\nregistration methods are typically based on iterative optimizations for each\ninput image pair, which is time-consuming and sensitive to contrast variations.\nWhile learning-based approaches are much faster during the inference stage, due\nto generalizability issues, they typically can only be applied to the fixed\ncontrasts observed during the training stage. In this work, we propose a novel\ncontrast-agnostic deformable image registration framework that can be\ngeneralized to arbitrary contrast images, without observing them during\ntraining. Particularly, we propose a random convolution-based contrast\naugmentation scheme, which simulates arbitrary contrasts of images over a\nsingle image contrast while preserving their inherent structural information.\nTo ensure that the network can learn contrast-invariant representations for\nfacilitating contrast-agnostic registration, we further introduce\ncontrast-invariant latent regularization (CLR) that regularizes representation\nin latent space through a contrast invariance loss. Experiments show that CAR\noutperforms the baseline approaches regarding registration accuracy and also\npossesses better generalization ability to unseen imaging contrasts. Code is\navailable at \\url{https://github.com/Yinsong0510/CAR}.",
      "tldr_zh": "该论文提出了一种对比度无关的变形医疗图像配准框架 CAR（Contrast-Agnostic Deformable Medical Image Registration），旨在解决多对比度图像配准中强度关系复杂的问题。框架采用随机卷积-based 对比度增强方案，在单一图像对比度上模拟任意对比度，同时通过对比度不变潜空间正则化（Contrast-Invariant Latent Regularization，CLR）来正则化潜空间表示，确保网络学习对比度不变的特征。实验结果显示，CAR 在配准准确性上优于基线方法，并表现出更好的泛化能力，能够处理训练中未见过的成像对比度。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "12 pages, 3 figures, 3 tables, accecpted by WBIR 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.05341v1",
      "published_date": "2024-08-03 19:46:23 UTC",
      "updated_date": "2024-08-03 19:46:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:37:22.693760"
    },
    {
      "arxiv_id": "2408.02693v3",
      "title": "Diff-PIC: Revolutionizing Particle-In-Cell Nuclear Fusion Simulation with Diffusion Models",
      "title_zh": "翻译失败",
      "authors": [
        "Chuan Liu",
        "Chunshu Wu",
        "Shihui Cao",
        "Mingkai Chen",
        "James Chenhao Liang",
        "Ang Li",
        "Michael Huang",
        "Chuang Ren",
        "Dongfang Liu",
        "Ying Nian Wu",
        "Tong Geng"
      ],
      "abstract": "The rapid development of AI highlights the pressing need for sustainable\nenergy, a critical global challenge for decades. Nuclear fusion, generally seen\nas an ultimate solution, has been the focus of intensive research for nearly a\ncentury, with investments reaching hundreds of billions of dollars. Recent\nadvancements in Inertial Confinement Fusion have drawn significant attention to\nfusion research, in which Laser-Plasma Interaction (LPI) is critical for\nensuring fusion stability and efficiency. However, the complexity of LPI upon\nfusion ignition makes analytical approaches impractical, leaving researchers\ndepending on extremely computation-demanding Particle-in-Cell (PIC) simulations\nto generate data, presenting a significant bottleneck to advancing fusion\nresearch. In response, this work introduces Diff-PIC, a novel framework that\nleverages conditional diffusion models as a computationally efficient\nalternative to PIC simulations for generating high-fidelity scientific LPI\ndata. In this work, physical patterns captured by PIC simulations are distilled\ninto diffusion models associated with two tailored enhancements: (1) To\neffectively capture the complex relationships between physical parameters and\ncorresponding outcomes, the parameters are encoded in a physically-informed\nmanner. (2) To further enhance efficiency while maintaining high fidelity and\nphysical validity, the rectified flow technique is employed to transform our\nmodel into a one-step conditional diffusion model. Experimental results show\nthat Diff-PIC achieves 16,200$\\times$ speedup compared to traditional PIC on a\n100 picosecond simulation, with an average reduction in MAE / RMSE / FID of\n59.21% / 57.15% / 39.46% with respect to two other SOTA data generation\napproaches.",
      "tldr_zh": "该研究针对核聚变模拟中的计算瓶颈，引入Diff-PIC框架，利用条件扩散模型(diffusion models)作为高效替代传统Particle-in-Cell (PIC)模拟的方法，以生成高保真度的Laser-Plasma Interaction (LPI)数据。框架通过物理信息编码(physically-informed manner)捕捉物理参数间的复杂关系，并采用rectified flow技术将模型优化为一步条件扩散模型，提升计算效率。实验结果显示，Diff-PIC在100皮秒模拟中比传统PIC快16,200倍，并在MAE / RMSE / FID指标上比其他最先进方法平均降低59.21% / 57.15% / 39.46%，为推进核聚变研究提供重大突破。",
      "categories": [
        "physics.comp-ph",
        "cs.AI"
      ],
      "primary_category": "physics.comp-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.02693v3",
      "published_date": "2024-08-03 19:42:31 UTC",
      "updated_date": "2024-10-06 03:10:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:37:34.613045"
    },
    {
      "arxiv_id": "2408.01827v1",
      "title": "ST-SACLF: Style Transfer Informed Self-Attention Classifier for Bias-Aware Painting Classification",
      "title_zh": "ST-SACLF：基于风格迁移的自注意力分类器，用于偏置感知绘画分类",
      "authors": [
        "Mridula Vijendran",
        "Frederick W. B. Li",
        "Jingjing Deng",
        "Hubert P. H. Shum"
      ],
      "abstract": "Painting classification plays a vital role in organizing, finding, and\nsuggesting artwork for digital and classic art galleries. Existing methods\nstruggle with adapting knowledge from the real world to artistic images during\ntraining, leading to poor performance when dealing with different datasets. Our\ninnovation lies in addressing these challenges through a two-step process.\nFirst, we generate more data using Style Transfer with Adaptive Instance\nNormalization (AdaIN), bridging the gap between diverse styles. Then, our\nclassifier gains a boost with feature-map adaptive spatial attention modules,\nimproving its understanding of artistic details. Moreover, we tackle the\nproblem of imbalanced class representation by dynamically adjusting augmented\nsamples. Through a dual-stage process involving careful hyperparameter search\nand model fine-tuning, we achieve an impressive 87.24\\% accuracy using the\nResNet-50 backbone over 40 training epochs. Our study explores quantitative\nanalyses that compare different pretrained backbones, investigates model\noptimization through ablation studies, and examines how varying augmentation\nlevels affect model performance. Complementing this, our qualitative\nexperiments offer valuable insights into the model's decision-making process\nusing spatial attention and its ability to differentiate between easy and\nchallenging samples based on confidence ranking.",
      "tldr_zh": "本文提出 ST-SACLF 框架，用于偏置感知的绘画分类，旨在解决现有方法在适应不同数据集和艺术图像时的性能问题。该框架通过两步过程，首先利用 Style Transfer with AdaIN 生成更多数据以桥接风格差距，然后引入特征映射自适应空间注意力模块增强分类器的艺术细节理解，并动态调整增强样本以处理类别不平衡。此外，通过双阶段优化和 ResNet-50 骨干网，模型在 40 个训练周期内达到 87.24% 的准确率，并通过定量分析、消融研究和定性实验（如空间注意力决策分析）验证了其有效性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.01827v1",
      "published_date": "2024-08-03 17:31:58 UTC",
      "updated_date": "2024-08-03 17:31:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:37:58.174052"
    },
    {
      "arxiv_id": "2408.02692v1",
      "title": "Attention is all you need for an improved CNN-based flash flood susceptibility modeling. The case of the ungauged Rheraya watershed, Morocco",
      "title_zh": "翻译失败",
      "authors": [
        "Akram Elghouat",
        "Ahmed Algouti",
        "Abdellah Algouti",
        "Soukaina Baid"
      ],
      "abstract": "Effective flood hazard management requires evaluating and predicting flash\nflood susceptibility. Convolutional neural networks (CNNs) are commonly used\nfor this task but face issues like gradient explosion and overfitting. This\nstudy explores the use of an attention mechanism, specifically the\nconvolutional block attention module (CBAM), to enhance CNN models for flash\nflood susceptibility in the ungauged Rheraya watershed, a flood prone region.\nWe used ResNet18, DenseNet121, and Xception as backbone architectures,\nintegrating CBAM at different locations. Our dataset included 16 conditioning\nfactors and 522 flash flood inventory points. Performance was evaluated using\naccuracy, precision, recall, F1-score, and the area under the curve (AUC) of\nthe receiver operating characteristic (ROC). Results showed that CBAM\nsignificantly improved model performance, with DenseNet121 incorporating CBAM\nin each convolutional block achieving the best results (accuracy = 0.95, AUC =\n0.98). Distance to river and drainage density were identified as key factors.\nThese findings demonstrate the effectiveness of the attention mechanism in\nimproving flash flood susceptibility modeling and offer valuable insights for\ndisaster management.",
      "tldr_zh": "本研究探讨了注意力机制（CBAM）在提升 CNN 模型用于闪洪易发性建模方面的作用，针对摩洛哥未监测的 Rheraya 流域问题，旨在解决 CNNs 的梯度爆炸和过拟合问题。研究者将 CBAM 整合到 ResNet18、DenseNet121 和 Xception 等骨干架构中，使用16个条件因素和522个闪洪库存点进行训练，并通过准确率、精确率、召回率、F1分数和 AUC 等指标评估模型性能。结果显示，CBAM 显著提高了模型效果，其中 DenseNet121 在每个卷积块中整合 CBAM 取得了最佳表现（准确率=0.95, AUC=0.98），并识别距河流距离和排水密度作为关键因素。这些发现为灾害管理提供了宝贵洞见。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.02692v1",
      "published_date": "2024-08-03 16:57:01 UTC",
      "updated_date": "2024-08-03 16:57:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:38:01.390075"
    },
    {
      "arxiv_id": "2408.04649v1",
      "title": "Chain of Stance: Stance Detection with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Junxia Ma",
        "Changjiang Wang",
        "Hanwen Xing",
        "Dongming Zhao",
        "Yazhou Zhang"
      ],
      "abstract": "Stance detection is an active task in natural language processing (NLP) that\naims to identify the author's stance towards a particular target within a text.\nGiven the remarkable language understanding capabilities and encyclopedic prior\nknowledge of large language models (LLMs), how to explore the potential of LLMs\nin stance detection has received significant attention. Unlike existing\nLLM-based approaches that focus solely on fine-tuning with large-scale\ndatasets, we propose a new prompting method, called \\textit{Chain of Stance}\n(CoS). In particular, it positions LLMs as expert stance detectors by\ndecomposing the stance detection process into a series of intermediate,\nstance-related assertions that culminate in the final judgment. This approach\nleads to significant improvements in classification performance. We conducted\nextensive experiments using four SOTA LLMs on the SemEval 2016 dataset,\ncovering the zero-shot and few-shot learning setups. The results indicate that\nthe proposed method achieves state-of-the-art results with an F1 score of 79.84\nin the few-shot setting.",
      "tldr_zh": "该论文提出了一种名为 Chain of Stance (CoS) 的提示方法，利用大语言模型 (LLMs) 进行姿态检测 (stance detection)，通过将检测过程分解为一系列中间的姿态相关断言，最终得出作者对目标的立场判断，从而提升分类性能。不同于传统的微调大规模数据集方法，CoS 将 LLMs 定位为专家检测器，在 SemEval 2016 数据集上进行广泛实验，包括零样本和少样本设置。结果显示，该方法在少样本场景下达到 state-of-the-art 水平，F1 分数高达 79.84%。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.04649v1",
      "published_date": "2024-08-03 16:30:51 UTC",
      "updated_date": "2024-08-03 16:30:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:38:13.564793"
    },
    {
      "arxiv_id": "2408.01808v1",
      "title": "ALIF: Low-Cost Adversarial Audio Attacks on Black-Box Speech Platforms using Linguistic Features",
      "title_zh": "翻译失败",
      "authors": [
        "Peng Cheng",
        "Yuwei Wang",
        "Peng Huang",
        "Zhongjie Ba",
        "Xiaodong Lin",
        "Feng Lin",
        "Li Lu",
        "Kui Ren"
      ],
      "abstract": "Extensive research has revealed that adversarial examples (AE) pose a\nsignificant threat to voice-controllable smart devices. Recent studies have\nproposed black-box adversarial attacks that require only the final\ntranscription from an automatic speech recognition (ASR) system. However, these\nattacks typically involve many queries to the ASR, resulting in substantial\ncosts. Moreover, AE-based adversarial audio samples are susceptible to ASR\nupdates. In this paper, we identify the root cause of these limitations, namely\nthe inability to construct AE attack samples directly around the decision\nboundary of deep learning (DL) models. Building on this observation, we propose\nALIF, the first black-box adversarial linguistic feature-based attack pipeline.\nWe leverage the reciprocal process of text-to-speech (TTS) and ASR models to\ngenerate perturbations in the linguistic embedding space where the decision\nboundary resides. Based on the ALIF pipeline, we present the ALIF-OTL and\nALIF-OTA schemes for launching attacks in both the digital domain and the\nphysical playback environment on four commercial ASRs and voice assistants.\nExtensive evaluations demonstrate that ALIF-OTL and -OTA significantly improve\nquery efficiency by 97.7% and 73.3%, respectively, while achieving competitive\nperformance compared to existing methods. Notably, ALIF-OTL can generate an\nattack sample with only one query. Furthermore, our test-of-time experiment\nvalidates the robustness of our approach against ASR updates.",
      "tldr_zh": "本研究针对黑-box 语音平台的安全问题，提出 ALIF，这是一种基于语言特征的低成本对抗音频攻击框架，以解决现有方法的高查询成本和对 ASR 更新敏感的问题。ALIF 利用 TTS 和 ASR 模型的互补过程，在 linguistic embedding space 生成扰动，从而绕过深度学习模型的决策边界。实验结果显示，ALIF-OTL 和 ALIF-OTA 方案分别将查询效率提高了 97.7% 和 73.3%，并能在仅一个查询下生成有效攻击样本；此外，该方法在对抗 ASR 更新时表现出色，确保了鲁棒性。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CR",
      "comment": "Published in the 2024 IEEE Symposium on Security and Privacy (SP)",
      "pdf_url": "http://arxiv.org/pdf/2408.01808v1",
      "published_date": "2024-08-03 15:30:16 UTC",
      "updated_date": "2024-08-03 15:30:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:38:25.222522"
    },
    {
      "arxiv_id": "2408.01795v1",
      "title": "Review of Cloud Service Composition for Intelligent Manufacturing",
      "title_zh": "翻译失败",
      "authors": [
        "Cuixia Li",
        "Liqiang Liu",
        "Li Shi"
      ],
      "abstract": "Intelligent manufacturing is a new model that uses advanced technologies such\nas the Internet of Things, big data, and artificial intelligence to improve the\nefficiency and quality of manufacturing production. As an important support to\npromote the transformation and upgrading of the manufacturing industry, cloud\nservice optimization has received the attention of researchers. In recent\nyears, remarkable research results have been achieved in this field. For the\nsustainability of intelligent manufacturing platforms, in this paper we\nsummarize the process of cloud service optimization for intelligent\nmanufacturing. Further, to address the problems of dispersed optimization\nindicators and nonuniform/unstandardized definitions in the existing research,\n11 optimization indicators that take into account three-party participant\nsubjects are defined from the urgent requirements of the sustainable\ndevelopment of intelligent manufacturing platforms. Next, service optimization\nalgorithms are classified into two categories, heuristic and reinforcement\nlearning. After comparing the two categories, the current key techniques of\nservice optimization are targeted. Finally, research hotspots and future\nresearch trends of service optimization are summarized.",
      "tldr_zh": "本综述论文回顾了云服务组合（Cloud Service Composition）在智能制造（Intelligent Manufacturing）中的应用，强调了物联网（Internet of Things）、大数据（Big Data）和人工智能（Artificial Intelligence）等技术如何提升生产效率和质量。论文总结了云服务优化过程，并定义了11个优化指标，以解决现有研究中指标分散和定义不统一的问题，同时考虑三方参与主体的需求。服务优化算法被分为启发式（Heuristic）和强化学习（Reinforcement Learning）两类，经过比较，论文针对当前关键技术和研究热点进行了分析，并展望了未来研究趋势。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.01795v1",
      "published_date": "2024-08-03 14:39:40 UTC",
      "updated_date": "2024-08-03 14:39:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:38:38.242570"
    },
    {
      "arxiv_id": "2408.01787v1",
      "title": "Towards an ontology of state actors in cyberspace",
      "title_zh": "翻译失败",
      "authors": [
        "Giacomo De Colle"
      ],
      "abstract": "To improve cyber threat analysis practices in cybersecurity, I present a plan\nto build a formal ontological representation of state actors in cyberspace and\nof cyber operations. I argue that modelling these phenomena via ontologies\nallows for coherent integration of data coming from diverse sources, automated\nreasoning over such data, as well as intelligence extraction and reuse from and\nof them. Existing ontological tools in cybersecurity can be ameliorated by\nconnecting them to neighboring domains such as law, regulations, governmental\ninstitutions, and documents. In this paper, I propose metrics to evaluate\ncurrently existing ontological tools to create formal representations in the\ncybersecurity domain, and I provide a plan to develop and extend them when they\nare lacking.",
      "tldr_zh": "该论文旨在构建国家行为者在网络空间（cyberspace）的正式本体（ontology）表示，以提升网络威胁分析（cyber threat analysis）的实践。该方法通过本体建模整合来自不同来源的数据，实现自动化推理（automated reasoning）、情报提取和重用，并将现有工具与法律、法规、政府机构及相关文件等领域连接。论文提出评估当前本体工具的指标，并规划开发和扩展这些工具，以填补网络安全领域的空白。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.01787v1",
      "published_date": "2024-08-03 13:56:20 UTC",
      "updated_date": "2024-08-03 13:56:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:38:48.255859"
    },
    {
      "arxiv_id": "2408.04648v1",
      "title": "PLUGH: A Benchmark for Spatial Understanding and Reasoning in Large Language Models",
      "title_zh": "PLUGH：大语言模型空间理解与推理基准",
      "authors": [
        "Alexey Tikhonov"
      ],
      "abstract": "We present PLUGH (https://www.urbandictionary.com/define.php?term=plugh), a\nmodern benchmark that currently consists of 5 tasks, each with 125 input texts\nextracted from 48 different games and representing 61 different\n(non-isomorphic) spatial graphs to assess the abilities of Large Language\nModels (LLMs) for spatial understanding and reasoning. Our evaluation of\nAPI-based and open-sourced LLMs shows that while some commercial LLMs exhibit\nstrong reasoning abilities, open-sourced competitors can demonstrate almost the\nsame level of quality; however, all models still have significant room for\nimprovement. We identify typical reasons for LLM failures and discuss possible\nways to deal with them. Datasets and evaluation code are released\n(https://github.com/altsoph/PLUGH).",
      "tldr_zh": "本研究引入了 PLUGH 基准测试，用于评估大型语言模型 (LLMs) 的空间理解和推理能力，该基准目前包含 5 个任务，每个任务有 125 个从 48 个不同游戏中提取的输入文本，代表 61 个非同构空间图。评估结果显示，一些商业 LLMs 展示了强劲的推理性能，而开源模型也能达到相近水平，但所有模型仍存在显著改进空间。论文分析了 LLMs 失败的常见原因，如空间图处理错误，并讨论了潜在解决方案。同时，数据集和评估代码已开源（https://github.com/altsoph/PLUGH），为未来研究提供资源。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "68T50, 68T20",
        "I.2.7; I.2.8; G.2.2"
      ],
      "primary_category": "cs.CL",
      "comment": "Wordplay Workshop @ ACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.04648v1",
      "published_date": "2024-08-03 13:21:08 UTC",
      "updated_date": "2024-08-03 13:21:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:39:02.610796"
    },
    {
      "arxiv_id": "2408.01774v1",
      "title": "STDA: Spatio-Temporal Dual-Encoder Network Incorporating Driver Attention to Predict Driver Behaviors Under Safety-Critical Scenarios",
      "title_zh": "STDA：整合",
      "authors": [
        "Dongyang Xu",
        "Yiran Luo",
        "Tianle Lu",
        "Qingfan Wang",
        "Qing Zhou",
        "Bingbing Nie"
      ],
      "abstract": "Accurate behavior prediction for vehicles is essential but challenging for\nautonomous driving. Most existing studies show satisfying performance under\nregular scenarios, but most neglected safety-critical scenarios. In this study,\na spatio-temporal dual-encoder network named STDA for safety-critical scenarios\nwas developed. Considering the exceptional capabilities of human drivers in\nterms of situational awareness and comprehending risks, driver attention was\nincorporated into STDA to facilitate swift identification of the critical\nregions, which is expected to improve both performance and interpretability.\nSTDA contains four parts: the driver attention prediction module, which\npredicts driver attention; the fusion module designed to fuse the features\nbetween driver attention and raw images; the temporary encoder module used to\nenhance the capability to interpret dynamic scenes; and the behavior prediction\nmodule to predict the behavior. The experiment data are used to train and\nvalidate the model. The results show that STDA improves the G-mean from 0.659\nto 0.719 when incorporating driver attention and adopting a temporal encoder\nmodule. In addition, extensive experimentation has been conducted to validate\nthat the proposed module exhibits robust generalization capabilities and can be\nseamlessly integrated into other mainstream models.",
      "tldr_zh": "该研究提出了一种名为 STDA 的时空双编码器网络，用于在安全关键场景下预测驾驶员行为，以提升自动驾驶系统的准确性和可解释性。STDA 整合了驾驶员注意力（driver attention）机制，包括驾驶员注意力预测模块、特征融合模块、temporal encoder 模块和行为预测模块，从而帮助快速识别关键区域并更好地解读动态场景。实验结果显示，STDA 通过incorporating driver attention 和 temporal encoder 模块，将 G-mean 从 0.659 提高到 0.719，并证明了其在主流模型中的鲁棒泛化能力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.01774v1",
      "published_date": "2024-08-03 13:06:04 UTC",
      "updated_date": "2024-08-03 13:06:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:39:17.029297"
    },
    {
      "arxiv_id": "2408.07278v3",
      "title": "Scene-wise Adaptive Network for Dynamic Cold-start Scenes Optimization in CTR Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Wenhao Li",
        "Jie Zhou",
        "Chuan Luo",
        "Chao Tang",
        "Kun Zhang",
        "Shixiong Zhao"
      ],
      "abstract": "In the realm of modern mobile E-commerce, providing users with nearby\ncommercial service recommendations through location-based online services has\nbecome increasingly vital. While machine learning approaches have shown promise\nin multi-scene recommendation, existing methodologies often struggle to address\ncold-start problems in unprecedented scenes: the increasing diversity of\ncommercial choices, along with the short online lifespan of scenes, give rise\nto the complexity of effective recommendations in online and dynamic scenes. In\nthis work, we propose Scene-wise Adaptive Network (SwAN), a novel approach that\nemphasizes high-performance cold-start online recommendations for new scenes.\nOur approach introduces several crucial capabilities, including scene\nsimilarity learning, user-specific scene transition cognition, scene-specific\ninformation construction for the new scene, and enhancing the diverged logical\ninformation between scenes. We demonstrate SwAN's potential to optimize dynamic\nmulti-scene recommendation problems by effectively online handling cold-start\nrecommendations for any newly arrived scenes. More encouragingly, SwAN has been\nsuccessfully deployed in Meituan's online catering recommendation service,\nwhich serves millions of customers per day, and SwAN has achieved a 5.64% CTR\nindex improvement relative to the baselines and a 5.19% increase in daily order\nvolume proportion.",
      "tldr_zh": "本研究针对移动电商中基于位置的推荐系统，提出了一种名为 Scene-wise Adaptive Network (SwAN) 的新框架，以优化 CTR Prediction 中的动态冷启动场景问题。该框架通过场景相似性学习、用户特定的场景转换认知、针对新场景的场景特定信息构建，以及增强场景间逻辑信息差异等关键能力，实现高效的在线冷启动推荐。实验结果显示，SwAN 能够有效处理新场景的推荐挑战，并在美团的在线餐饮服务中实际部署，相比基线模型提升 CTR 5.64% 并增加订单量比例 5.19%。这为动态多场景推荐提供了实用优化方案。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CV",
        "68T09",
        "I.2.0"
      ],
      "primary_category": "cs.IR",
      "comment": "10 pages, 6 figures, accepted by Recsys 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.07278v3",
      "published_date": "2024-08-03 13:03:31 UTC",
      "updated_date": "2024-08-18 16:45:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:39:25.935953"
    },
    {
      "arxiv_id": "2408.01752v1",
      "title": "Advancing Green AI: Efficient and Accurate Lightweight CNNs for Rice Leaf Disease Identification",
      "title_zh": "翻译失败",
      "authors": [
        "Khairun Saddami",
        "Yudha Nurdin",
        "Mutia Zahramita",
        "Muhammad Shahreeza Safiruz"
      ],
      "abstract": "Rice plays a vital role as a primary food source for over half of the world's\npopulation, and its production is critical for global food security.\nNevertheless, rice cultivation is frequently affected by various diseases that\ncan severely decrease yield and quality. Therefore, early and accurate\ndetection of rice diseases is necessary to prevent their spread and minimize\ncrop losses. In this research, we explore three mobile-compatible CNN\narchitectures, namely ShuffleNet, MobileNetV2, and EfficientNet-B0, for rice\nleaf disease classification. These models are selected due to their\ncompatibility with mobile devices, as they demand less computational power and\nmemory compared to other CNN models. To enhance the performance of the three\nmodels, we added two fully connected layers separated by a dropout layer. We\nused early stop creation to prevent the model from being overfiting. The\nresults of the study showed that the best performance was achieved by the\nEfficientNet-B0 model with an accuracy of 99.8%. Meanwhile, MobileNetV2 and\nShuffleNet only achieved accuracies of 84.21% and 66.51%, respectively. This\nstudy shows that EfficientNet-B0 when combined with the proposed layer and\nearly stop, can produce a high-accuracy model.\n  Keywords: rice leaf detection; green AI; smart agriculture; EfficientNet;",
      "tldr_zh": "本研究针对稻米作为全球主要食物来源的背景，探讨了早期准确检测稻米叶病害以减少产量损失的方法。研究评估了三种轻量级CNN模型（ShuffleNet、MobileNetV2和EfficientNet-B0），这些模型适合移动设备使用，并通过添加两个全连接层、一个dropout层以及early stopping技术来提升性能和防止过拟合。结果显示，EfficientNet-B0模型在稻米叶病害分类中取得了99.8%的准确率，而MobileNetV2和ShuffleNet的准确率分别为84.21%和66.51%。这项工作推动了Green AI在智能农业中的应用，证明了EfficientNet-B0结合优化策略的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.01752v1",
      "published_date": "2024-08-03 11:16:00 UTC",
      "updated_date": "2024-08-03 11:16:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:39:38.076773"
    },
    {
      "arxiv_id": "2408.01739v1",
      "title": "LAM3D: Leveraging Attention for Monocular 3D Object Detection",
      "title_zh": "LAM3D：基于注意力机制的单目3D物体检测",
      "authors": [
        "Diana-Alexandra Sas",
        "Leandro Di Bella",
        "Yangxintong Lyu",
        "Florin Oniga",
        "Adrian Munteanu"
      ],
      "abstract": "Since the introduction of the self-attention mechanism and the adoption of\nthe Transformer architecture for Computer Vision tasks, the Vision\nTransformer-based architectures gained a lot of popularity in the field, being\nused for tasks such as image classification, object detection and image\nsegmentation. However, efficiently leveraging the attention mechanism in vision\ntransformers for the Monocular 3D Object Detection task remains an open\nquestion. In this paper, we present LAM3D, a framework that Leverages\nself-Attention mechanism for Monocular 3D object Detection. To do so, the\nproposed method is built upon a Pyramid Vision Transformer v2 (PVTv2) as\nfeature extraction backbone and 2D/3D detection machinery. We evaluate the\nproposed method on the KITTI 3D Object Detection Benchmark, proving the\napplicability of the proposed solution in the autonomous driving domain and\noutperforming reference methods. Moreover, due to the usage of self-attention,\nLAM3D is able to systematically outperform the equivalent architecture that\ndoes not employ self-attention.",
      "tldr_zh": "该论文提出 LAM3D 框架，利用 self-attention 机制来提升单目 3D 对象检测任务的性能。\nLAM3D 以 Pyramid Vision Transformer v2 (PVTv2) 作为特征提取骨干，并结合 2D/3D 检测机制，实现对图像的更高效处理。\n在 KITTI 3D 对象检测基准上，LAM3D 超越了参考方法，并在使用 self-attention 时系统性地优于不采用该机制的等效架构。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "6 pages. Accepted to MMSP 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.01739v1",
      "published_date": "2024-08-03 10:50:07 UTC",
      "updated_date": "2024-08-03 10:50:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:39:50.735981"
    },
    {
      "arxiv_id": "2408.01736v1",
      "title": "Can LLMs predict the convergence of Stochastic Gradient Descent?",
      "title_zh": "翻译失败",
      "authors": [
        "Oussama Zekri",
        "Abdelhakim Benechehab",
        "Ievgen Redko"
      ],
      "abstract": "Large-language models are notoriously famous for their impressive performance\nacross a wide range of tasks. One surprising example of such impressive\nperformance is a recently identified capacity of LLMs to understand the\ngoverning principles of dynamical systems satisfying the Markovian property. In\nthis paper, we seek to explore this direction further by studying the dynamics\nof stochastic gradient descent in convex and non-convex optimization. By\nleveraging the theoretical link between the SGD and Markov chains, we show a\nremarkable zero-shot performance of LLMs in predicting the local minima to\nwhich SGD converges for previously unseen starting points. On a more general\nlevel, we inquire about the possibility of using LLMs to perform zero-shot\nrandomized trials for larger deep learning models used in practice.",
      "tldr_zh": "本论文探讨大型语言模型(LLMs)是否能预测随机梯度下降(SGD)在凸和非凸优化中的收敛行为。研究者利用SGD与Markov链的理论联系，展示了LLMs在零样本条件下准确预测SGD收敛局部最小点的卓越性能，尤其适用于未见过的起始点。更广泛地，该工作探讨了将LLMs用于更大深度学习模型的零样本随机试验的可能性，为LLMs在动态系统理解方面提供了新见解。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages. Accepted to 1st ICML Workshop on In-Context Learning at ICML\n  2024",
      "pdf_url": "http://arxiv.org/pdf/2408.01736v1",
      "published_date": "2024-08-03 10:35:59 UTC",
      "updated_date": "2024-08-03 10:35:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:40:01.809220"
    },
    {
      "arxiv_id": "2408.01732v1",
      "title": "Landmark-guided Diffusion Model for High-fidelity and Temporally Coherent Talking Head Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Jintao Tan",
        "Xize Cheng",
        "Lingyu Xiong",
        "Lei Zhu",
        "Xiandong Li",
        "Xianjia Wu",
        "Kai Gong",
        "Minglei Li",
        "Yi Cai"
      ],
      "abstract": "Audio-driven talking head generation is a significant and challenging task\napplicable to various fields such as virtual avatars, film production, and\nonline conferences. However, the existing GAN-based models emphasize generating\nwell-synchronized lip shapes but overlook the visual quality of generated\nframes, while diffusion-based models prioritize generating high-quality frames\nbut neglect lip shape matching, resulting in jittery mouth movements. To\naddress the aforementioned problems, we introduce a two-stage diffusion-based\nmodel. The first stage involves generating synchronized facial landmarks based\non the given speech. In the second stage, these generated landmarks serve as a\ncondition in the denoising process, aiming to optimize mouth jitter issues and\ngenerate high-fidelity, well-synchronized, and temporally coherent talking head\nvideos. Extensive experiments demonstrate that our model yields the best\nperformance.",
      "tldr_zh": "这篇论文针对音频驱动的说话头像生成问题，提出了一种基于地标引导的扩散模型，以解决现有 GAN-based 模型的视觉质量不足和 diffusion-based 模型的唇形匹配问题，导致的嘴部抖动。方法采用两阶段设计：第一阶段基于给定语音生成同步的面部 landmarks；第二阶段使用这些 landmarks 作为条件，在去噪过程中优化生成视频，确保高保真、唇形同步且时间连贯。实验结果显示，该模型在广泛测试中表现出最佳性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.01732v1",
      "published_date": "2024-08-03 10:19:38 UTC",
      "updated_date": "2024-08-03 10:19:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:40:14.293740"
    },
    {
      "arxiv_id": "2408.01728v2",
      "title": "Survey on Emotion Recognition through Posture Detection and the possibility of its application in Virtual Reality",
      "title_zh": "通过姿势检测",
      "authors": [
        "Leina Elansary",
        "Zaki Taha",
        "Walaa Gad"
      ],
      "abstract": "A survey is presented focused on using pose estimation techniques in\nEmotional recognition using various technologies normal cameras, and depth\ncameras for real-time, and the potential use of VR and inputs including images,\nvideos, and 3-dimensional poses described in vector space. We discussed 19\nresearch papers collected from selected journals and databases highlighting\ntheir methodology, classification algorithm, and the used datasets that relate\nto emotion recognition and pose estimation. A benchmark has been made according\nto their accuracy as it was the most common performance measurement metric\nused. We concluded that the multimodal Approaches overall made the best\naccuracy and then we mentioned futuristic concerns that can improve the\ndevelopment of this research topic.",
      "tldr_zh": "这篇调查论文探讨了通过姿势估计(pose estimation)技术进行情感识别的可能性，包括使用普通摄像头和深度摄像头进行实时应用，以及其在虚拟现实(VR)中的潜在应用。作者分析了19篇相关研究论文，涵盖了方法、分类算法和数据集，并基于准确率进行了基准测试。结果显示，多模态(multimodal)方法取得了最佳准确率；论文还讨论了未来改进的方向，如整合更多输入类型以提升情感识别的准确性和适用性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.01728v2",
      "published_date": "2024-08-03 10:01:29 UTC",
      "updated_date": "2024-11-19 13:42:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:40:26.937505"
    },
    {
      "arxiv_id": "2408.01715v1",
      "title": "Joint Universal Adversarial Perturbations with Interpretations",
      "title_zh": "联合通用对抗扰",
      "authors": [
        "Liang-bo Ning",
        "Zeyu Dai",
        "Wenqi Fan",
        "Jingran Su",
        "Chao Pan",
        "Luning Wang",
        "Qing Li"
      ],
      "abstract": "Deep neural networks (DNNs) have significantly boosted the performance of\nmany challenging tasks. Despite the great development, DNNs have also exposed\ntheir vulnerability. Recent studies have shown that adversaries can manipulate\nthe predictions of DNNs by adding a universal adversarial perturbation (UAP) to\nbenign samples. On the other hand, increasing efforts have been made to help\nusers understand and explain the inner working of DNNs by highlighting the most\ninformative parts (i.e., attribution maps) of samples with respect to their\npredictions. Moreover, we first empirically find that such attribution maps\nbetween benign and adversarial examples have a significant discrepancy, which\nhas the potential to detect universal adversarial perturbations for defending\nagainst adversarial attacks. This finding motivates us to further investigate a\nnew research problem: whether there exist universal adversarial perturbations\nthat are able to jointly attack DNNs classifier and its interpretation with\nmalicious desires. It is challenging to give an explicit answer since these two\nobjectives are seemingly conflicting. In this paper, we propose a novel\nattacking framework to generate joint universal adversarial perturbations\n(JUAP), which can fool the DNNs model and misguide the inspection from\ninterpreters simultaneously. Comprehensive experiments on various datasets\ndemonstrate the effectiveness of the proposed method JUAP for joint attacks. To\nthe best of our knowledge, this is the first effort to study UAP for jointly\nattacking both DNNs and interpretations.",
      "tldr_zh": "该研究发现，深度神经网络(DNNs)易受通用对抗扰动(UAP)攻击，导致预测被操纵，同时观察到良性样本和对抗样本的归因映射(attribution maps)存在显著差异，可能用于防御。论文提出一个新颖的攻击框架，生成联合通用对抗扰动(JUAP)，能够同时欺骗DNNs的分类预测和误导其解释机制，尽管这两个目标看似冲突。实验在多种数据集上验证了JUAP的有效性，这是首次针对DNNs及其解释进行联合攻击的研究。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.01715v1",
      "published_date": "2024-08-03 08:58:04 UTC",
      "updated_date": "2024-08-03 08:58:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:40:37.930759"
    },
    {
      "arxiv_id": "2408.01705v1",
      "title": "Downstream Transfer Attack: Adversarial Attacks on Downstream Models with Pre-trained Vision Transformers",
      "title_zh": "下游转移攻击：利用预训练视觉变压",
      "authors": [
        "Weijie Zheng",
        "Xingjun Ma",
        "Hanxun Huang",
        "Zuxuan Wu",
        "Yu-Gang Jiang"
      ],
      "abstract": "With the advancement of vision transformers (ViTs) and self-supervised\nlearning (SSL) techniques, pre-trained large ViTs have become the new\nfoundation models for computer vision applications. However, studies have shown\nthat, like convolutional neural networks (CNNs), ViTs are also susceptible to\nadversarial attacks, where subtle perturbations in the input can fool the model\ninto making false predictions. This paper studies the transferability of such\nan adversarial vulnerability from a pre-trained ViT model to downstream tasks.\nWe focus on \\emph{sample-wise} transfer attacks and propose a novel attack\nmethod termed \\emph{Downstream Transfer Attack (DTA)}. For a given test image,\nDTA leverages a pre-trained ViT model to craft the adversarial example and then\napplies the adversarial example to attack a fine-tuned version of the model on\na downstream dataset. During the attack, DTA identifies and exploits the most\nvulnerable layers of the pre-trained model guided by a cosine similarity loss\nto craft highly transferable attacks. Through extensive experiments with\npre-trained ViTs by 3 distinct pre-training methods, 3 fine-tuning schemes, and\nacross 10 diverse downstream datasets, we show that DTA achieves an average\nattack success rate (ASR) exceeding 90\\%, surpassing existing methods by a huge\nmargin. When used with adversarial training, the adversarial examples generated\nby our DTA can significantly improve the model's robustness to different\ndownstream transfer attacks.",
      "tldr_zh": "这篇论文研究了预训练视觉Transformer (ViTs) 的对抗攻击转移性，提出了一种新方法Downstream Transfer Attack (DTA)，用于针对下游任务的微调模型进行样本级攻击。DTA 通过利用预训练ViT模型生成对抗样本，并借助余弦相似度损失识别和利用模型中最脆弱的层，从而提升攻击的可转移性。实验结果显示，在3种预训练方法、3种微调方案和10个下游数据集上，DTA的攻击成功率 (ASR) 平均超过90%，远超现有方法，并能通过对抗训练显著提高模型对下游转移攻击的鲁棒性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.01705v1",
      "published_date": "2024-08-03 08:07:03 UTC",
      "updated_date": "2024-08-03 08:07:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:40:51.302938"
    },
    {
      "arxiv_id": "2408.01700v1",
      "title": "Integrating Large Language Models and Knowledge Graphs for Extraction and Validation of Textual Test Data",
      "title_zh": "翻译失败",
      "authors": [
        "Antonio De Santis",
        "Marco Balduini",
        "Federico De Santis",
        "Andrea Proia",
        "Arsenio Leo",
        "Marco Brambilla",
        "Emanuele Della Valle"
      ],
      "abstract": "Aerospace manufacturing companies, such as Thales Alenia Space, design,\ndevelop, integrate, verify, and validate products characterized by high\ncomplexity and low volume. They carefully document all phases for each product\nbut analyses across products are challenging due to the heterogeneity and\nunstructured nature of the data in documents. In this paper, we propose a\nhybrid methodology that leverages Knowledge Graphs (KGs) in conjunction with\nLarge Language Models (LLMs) to extract and validate data contained in these\ndocuments. We consider a case study focused on test data related to electronic\nboards for satellites. To do so, we extend the Semantic Sensor Network\nontology. We store the metadata of the reports in a KG, while the actual test\nresults are stored in parquet accessible via a Virtual Knowledge Graph. The\nvalidation process is managed using an LLM-based approach. We also conduct a\nbenchmarking study to evaluate the performance of state-of-the-art LLMs in\nexecuting this task. Finally, we analyze the costs and benefits of automating\npreexisting processes of manual data extraction and validation for subsequent\ncross-report analyses.",
      "tldr_zh": "本研究针对航空航天制造公司（如 Thales Alenia Space）处理异构非结构化文档的挑战，提出了一种整合 Knowledge Graphs (KGs) 和 Large Language Models (LLMs) 的混合方法，用于提取和验证文本测试数据。方法扩展了 Semantic Sensor Network ontology，将报告元数据存储在 KG 中，并通过 Virtual Knowledge Graph 访问测试结果，同时采用 LLM-based 验证过程。研究以卫星电子板测试数据为案例进行基准测试，结果显示该方法提升了数据处理效率，并分析了自动化手动提取过程的成本与益处。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Paper Accepted at ISWC 2024 In-Use Track",
      "pdf_url": "http://arxiv.org/pdf/2408.01700v1",
      "published_date": "2024-08-03 07:42:53 UTC",
      "updated_date": "2024-08-03 07:42:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:41:02.007057"
    },
    {
      "arxiv_id": "2408.01697v2",
      "title": "Invariant Graph Learning Meets Information Bottleneck for Out-of-Distribution Generalization",
      "title_zh": "翻译失败",
      "authors": [
        "Wenyu Mao",
        "Jiancan Wu",
        "Haoyang Liu",
        "Yongduo Sui",
        "Xiang Wang"
      ],
      "abstract": "Graph out-of-distribution (OOD) generalization remains a major challenge in\ngraph learning since graph neural networks (GNNs) often suffer from severe\nperformance degradation under distribution shifts. Invariant learning, aiming\nto extract invariant features across varied distributions, has recently emerged\nas a promising approach for OOD generation. Despite the great success of\ninvariant learning in OOD problems for Euclidean data (i.e., images), the\nexploration within graph data remains constrained by the complex nature of\ngraphs. Existing studies, such as data augmentation or causal intervention,\neither suffer from disruptions to invariance during the graph manipulation\nprocess or face reliability issues due to a lack of supervised signals for\ncausal parts. In this work, we propose a novel framework, called Invariant\nGraph Learning based on Information bottleneck theory (InfoIGL), to extract the\ninvariant features of graphs and enhance models' generalization ability to\nunseen distributions. Specifically, InfoIGL introduces a redundancy filter to\ncompress task-irrelevant information related to environmental factors.\nCooperating with our designed multi-level contrastive learning, we maximize the\nmutual information among graphs of the same class in the downstream\nclassification tasks, preserving invariant features for prediction to a great\nextent. An appealing feature of InfoIGL is its strong generalization ability\nwithout depending on supervised signal of invariance. Experiments on both\nsynthetic and real-world datasets demonstrate that our method achieves\nstate-of-the-art performance under OOD generalization for graph classification\ntasks. The source code is available at https://github.com/maowenyu-11/InfoIGL.",
      "tldr_zh": "这篇论文针对图神经网络 (GNNs) 在 Out-of-Distribution (OOD) 泛化中的性能下降问题，提出了一种新型框架 InfoIGL，该框架基于 Information Bottleneck 理论来提取图的不变特征。InfoIGL 引入 Redundancy Filter 来压缩与环境因素相关的任务无关信息，并结合 Multi-level Contrastive Learning 来最大化同一类图之间的 Mutual Information，从而增强模型的泛化能力，而无需依赖不变性的监督信号。实验在合成和真实数据集上证明，InfoIGL 在 OOD 泛化图分类任务中实现了 state-of-the-art 性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "The article has been accepted by Frontiers of Computer Science (FCS),\n  with the DOI: {10.1007/s11704-025-40798-3}",
      "pdf_url": "http://arxiv.org/pdf/2408.01697v2",
      "published_date": "2024-08-03 07:38:04 UTC",
      "updated_date": "2025-02-13 04:10:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:41:15.529741"
    },
    {
      "arxiv_id": "2408.01696v1",
      "title": "Generating High-quality Symbolic Music Using Fine-grained Discriminators",
      "title_zh": "使用细粒度鉴别器生成高质量符号音乐",
      "authors": [
        "Zhedong Zhang",
        "Liang Li",
        "Jiehua Zhang",
        "Zhenghui Hu",
        "Hongkui Wang",
        "Chenggang Yan",
        "Jian Yang",
        "Yuankai Qi"
      ],
      "abstract": "Existing symbolic music generation methods usually utilize discriminator to\nimprove the quality of generated music via global perception of music. However,\nconsidering the complexity of information in music, such as rhythm and melody,\na single discriminator cannot fully reflect the differences in these two\nprimary dimensions of music. In this work, we propose to decouple the melody\nand rhythm from music, and design corresponding fine-grained discriminators to\ntackle the aforementioned issues. Specifically, equipped with a pitch\naugmentation strategy, the melody discriminator discerns the melody variations\npresented by the generated samples. By contrast, the rhythm discriminator,\nenhanced with bar-level relative positional encoding, focuses on the velocity\nof generated notes. Such a design allows the generator to be more explicitly\naware of which aspects should be adjusted in the generated music, making it\neasier to mimic human-composed music. Experimental results on the POP909\nbenchmark demonstrate the favorable performance of the proposed method compared\nto several state-of-the-art methods in terms of both objective and subjective\nmetrics.",
      "tldr_zh": "本文提出一种改进符号音乐生成的方法，通过细粒度鉴别器(fine-grained discriminators)来处理音乐中的旋律(melody)和节奏(rhythm)差异。具体而言，该方法解耦旋律和节奏，设计了配备音高增强策略(pitch augmentation)的旋律鉴别器，以及使用小节级相对位置编码(bar-level relative positional encoding)的节奏鉴别器，从而使生成器更精确地调整音乐元素，以模仿人类作曲风格。在POP909基准上的实验结果显示，该方法在客观和主观指标上优于现有最先进方法，提高了生成的音乐质量。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted by ICPR2024",
      "pdf_url": "http://arxiv.org/pdf/2408.01696v1",
      "published_date": "2024-08-03 07:32:21 UTC",
      "updated_date": "2024-08-03 07:32:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:41:26.921937"
    },
    {
      "arxiv_id": "2408.01691v1",
      "title": "TreeCSS: An Efficient Framework for Vertical Federated Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Qinbo Zhang",
        "Xiao Yan",
        "Yukai Ding",
        "Quanqing Xu",
        "Chuang Hu",
        "Xiaokai Zhou",
        "Jiawei Jiang"
      ],
      "abstract": "Vertical federated learning (VFL) considers the case that the features of\ndata samples are partitioned over different participants. VFL consists of two\nmain steps, i.e., identify the common data samples for all participants\n(alignment) and train model using the aligned data samples (training). However,\nwhen there are many participants and data samples, both alignment and training\nbecome slow. As such, we propose TreeCSS as an efficient VFL framework that\naccelerates the two main steps. In particular, for sample alignment, we design\nan efficient multi-party private set intersection (MPSI) protocol called\nTree-MPSI, which adopts a tree-based structure and a data-volume-aware\nscheduling strategy to parallelize alignment among the participants. As model\ntraining time scales with the number of data samples, we conduct coreset\nselection (CSS) to choose some representative data samples for training. Our\nCCS method adopts a clustering-based scheme for security and generality, which\nfirst clusters the features locally on each participant and then merges the\nlocal clustering results to select representative samples. In addition, we\nweight the samples according to their distances to the centroids to reflect\ntheir importance to model training. We evaluate the effectiveness and\nefficiency of our TreeCSS framework on various datasets and models. The results\nshow that compared with vanilla VFL, TreeCSS accelerates training by up to\n2.93x and achieves comparable model accuracy.",
      "tldr_zh": "本研究提出TreeCSS框架，以提升垂直联邦学习(VFL)的效率，针对多参与者和大量数据样本导致的对齐和训练过程变慢的问题。框架包括Tree-MPSI协议，用于加速多方私有集合交集，通过树状结构和数据量感知调度策略实现参与者间的并行对齐；以及coreset selection (CSS)方法，通过本地聚类特征、合并结果选择代表性样本，并根据样本到中心点的距离加权以优化训练。实验结果显示，TreeCSS在多种数据集和模型上比传统VFL加速训练高达2.93倍，同时保持可比的模型准确率。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "16 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2408.01691v1",
      "published_date": "2024-08-03 07:11:57 UTC",
      "updated_date": "2024-08-03 07:11:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:41:38.175421"
    },
    {
      "arxiv_id": "2408.01690v2",
      "title": "IDNet: A Novel Dataset for Identity Document Analysis and Fraud Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Hong Guan",
        "Yancheng Wang",
        "Lulu Xie",
        "Soham Nag",
        "Rajeev Goel",
        "Niranjan Erappa Narayana Swamy",
        "Yingzhen Yang",
        "Chaowei Xiao",
        "Jonathan Prisby",
        "Ross Maciejewski",
        "Jia Zou"
      ],
      "abstract": "Effective fraud detection and analysis of government-issued identity\ndocuments, such as passports, driver's licenses, and identity cards, are\nessential in thwarting identity theft and bolstering security on online\nplatforms. The training of accurate fraud detection and analysis tools depends\non the availability of extensive identity document datasets. However, current\npublicly available benchmark datasets for identity document analysis, including\nMIDV-500, MIDV-2020, and FMIDV, fall short in several respects: they offer a\nlimited number of samples, cover insufficient varieties of fraud patterns, and\nseldom include alterations in critical personal identifying fields like\nportrait images, limiting their utility in training models capable of detecting\nrealistic frauds while preserving privacy.\n  In response to these shortcomings, our research introduces a new benchmark\ndataset, IDNet, designed to advance privacy-preserving fraud detection efforts.\nThe IDNet dataset comprises 837,060 images of synthetically generated identity\ndocuments, totaling approximately 490 gigabytes, categorized into 20 types from\n$10$ U.S. states and 10 European countries. We evaluate the utility and present\nuse cases of the dataset, illustrating how it can aid in training\nprivacy-preserving fraud detection methods, facilitating the generation of\ncamera and video capturing of identity documents, and testing schema\nunification and other identity document management functionalities.",
      "tldr_zh": "这篇论文指出了现有身份文件分析数据集（如MIDV-500和MIDV-2020）的不足，包括样本量有限、诈骗模式种类不够多样，以及关键个人信息字段（如肖像图像）改动的缺失，这些问题限制了诈骗检测模型的训练效果。针对这些问题，研究团队引入了新的基准数据集IDNet，该数据集包含837,060张合成生成的身份文件图像，总计约490 GB，涵盖10个美国州和10个欧洲国家的20种类型，以支持隐私保护的诈骗检测。IDNet的实用性通过评估展示，可用于训练诈骗检测模型、模拟相机和视频捕捉图像，以及测试模式统一和其他身份文件管理功能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "40 pages",
      "pdf_url": "http://arxiv.org/pdf/2408.01690v2",
      "published_date": "2024-08-03 07:05:40 UTC",
      "updated_date": "2024-09-03 22:30:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:41:51.951985"
    },
    {
      "arxiv_id": "2408.01689v3",
      "title": "Controllable Unlearning for Image-to-Image Generative Models via $\\varepsilon$-Constrained Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaohua Feng",
        "Yuyuan Li",
        "Chaochao Chen",
        "Li Zhang",
        "Longfei Li",
        "Jun Zhou",
        "Xiaolin Zheng"
      ],
      "abstract": "While generative models have made significant advancements in recent years,\nthey also raise concerns such as privacy breaches and biases. Machine\nunlearning has emerged as a viable solution, aiming to remove specific training\ndata, e.g., containing private information and bias, from models. In this\npaper, we study the machine unlearning problem in Image-to-Image (I2I)\ngenerative models. Previous studies mainly treat it as a single objective\noptimization problem, offering a solitary solution, thereby neglecting the\nvaried user expectations towards the trade-off between complete unlearning and\nmodel utility. To address this issue, we propose a controllable unlearning\nframework that uses a control coefficient $\\varepsilon$ to control the\ntrade-off. We reformulate the I2I generative model unlearning problem into a\n$\\varepsilon$-constrained optimization problem and solve it with a\ngradient-based method to find optimal solutions for unlearning boundaries.\nThese boundaries define the valid range for the control coefficient. Within\nthis range, every yielded solution is theoretically guaranteed with Pareto\noptimality. We also analyze the convergence rate of our framework under various\ncontrol functions. Extensive experiments on two benchmark datasets across three\nmainstream I2I models demonstrate the effectiveness of our controllable\nunlearning framework.",
      "tldr_zh": "本论文针对图像到图像(I2I)生成模型中的机器unlearning问题，提出了一种可控unlearning框架，利用控制系数ε来平衡unlearning的完整性和模型效用。框架将问题重构为ε-constrained优化问题，通过基于梯度的求解方法找到最优unlearning边界，确保所有解决方案在有效范围内实现Pareto最优，并分析了不同控制函数下的收敛率。在两个基准数据集上进行的广泛实验证明，该框架在主流I2I模型中有效提升了unlearning性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2408.01689v3",
      "published_date": "2024-08-03 07:04:55 UTC",
      "updated_date": "2025-02-19 03:06:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:42:03.182914"
    },
    {
      "arxiv_id": "2408.02691v1",
      "title": "Symmetric Graph Contrastive Learning against Noisy Views for Recommendation",
      "title_zh": "翻译失败",
      "authors": [
        "Chu Zhao",
        "Enneng Yang",
        "Yuliang Liang",
        "Jianzhe Zhao",
        "Guibing Guo",
        "Xingwei Wang"
      ],
      "abstract": "Graph Contrastive Learning (GCL) leverages data augmentation techniques to\nproduce contrasting views, enhancing the accuracy of recommendation systems\nthrough learning the consistency between contrastive views. However, existing\naugmentation methods, such as directly perturbing interaction graph (e.g.,\nnode/edge dropout), may interfere with the original connections and generate\npoor contrasting views, resulting in sub-optimal performance. In this paper, we\ndefine the views that share only a small amount of information with the\noriginal graph due to poor data augmentation as noisy views (i.e., the last 20%\nof the views with a cosine similarity value less than 0.1 to the original\nview). We demonstrate through detailed experiments that noisy views will\nsignificantly degrade recommendation performance. Further, we propose a\nmodel-agnostic Symmetric Graph Contrastive Learning (SGCL) method with\ntheoretical guarantees to address this issue. Specifically, we introduce\nsymmetry theory into graph contrastive learning, based on which we propose a\nsymmetric form and contrast loss resistant to noisy interference. We provide\ntheoretical proof that our proposed SGCL method has a high tolerance to noisy\nviews. Further demonstration is given by conducting extensive experiments on\nthree real-world datasets. The experimental results demonstrate that our\napproach substantially increases recommendation accuracy, with relative\nimprovements reaching as high as 12.25% over nine other competing models. These\nresults highlight the efficacy of our method.",
      "tldr_zh": "该论文针对图对比学习(Graph Contrastive Learning, GCL)在推荐系统中因数据增强（如节点/边删除）产生噪声视图(noisy views)而导致性能下降的问题进行了研究。作者定义噪声视图为与原始图共享信息较少的视图（如余弦相似度小于0.1的视图），并通过实验证明这些视图会显著降低推荐准确率。为解决此问题，他们提出了一种模型无关的Symmetric Graph Contrastive Learning (SGCL)方法，引入对称理论(symmetry theory)设计出对噪声干扰抵抗的对比损失(contrast loss)，并提供了理论证明支持其高耐受性。在三个真实数据集上的实验结果显示，SGCL 相较于其他九个模型将推荐准确率提高了最高12.25%。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.LG",
      "comment": "24 pages, submitted to TOIS",
      "pdf_url": "http://arxiv.org/pdf/2408.02691v1",
      "published_date": "2024-08-03 06:58:07 UTC",
      "updated_date": "2024-08-03 06:58:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:42:16.280040"
    },
    {
      "arxiv_id": "2408.01672v2",
      "title": "radarODE: An ODE-Embedded Deep Learning Model for Contactless ECG Reconstruction from Millimeter-Wave Radar",
      "title_zh": "翻译失败",
      "authors": [
        "Yuanyuan Zhang",
        "Runwei Guan",
        "Lingxiao Li",
        "Rui Yang",
        "Yutao Yue",
        "Eng Gee Lim"
      ],
      "abstract": "Radar-based contactless cardiac monitoring has become a popular research\ndirection recently, but the fine-grained electrocardiogram (ECG) signal is\nstill hard to reconstruct from millimeter-wave radar signal. The key obstacle\nis to decouple the cardiac activities in the electrical domain (i.e., ECG) from\nthat in the mechanical domain (i.e., heartbeat), and most existing research\nonly uses pure data-driven methods to map such domain transformation as a black\nbox. Therefore, this work first proposes a signal model for domain\ntransformation, and then a novel deep learning framework called radarODE is\ndesigned to fuse the temporal and morphological features extracted from radar\nsignals and generate ECG. In addition, ordinary differential equations are\nembedded in radarODE as a decoder to provide morphological prior, helping the\nconvergence of the model training and improving the robustness under body\nmovements. After being validated on the dataset, the proposed radarODE achieves\nbetter performance compared with the benchmark in terms of missed detection\nrate, root mean square error, Pearson correlation coefficient with the\nimprovement of 9%, 16% and 19%, respectively. The validation results imply that\nradarODE is capable of recovering ECG signals from radar signals with high\nfidelity and can be potentially implemented in real-life scenarios.",
      "tldr_zh": "本研究针对从毫米波雷达信号重建心电图(ECG)的难题，提出了一种新型深度学习模型radarODE，通过建立信号模型来解耦电域(ECG)和机械域(心跳)，并融合雷达信号的时间和形态特征。\nradarODE框架中嵌入普通微分方程(ODE)作为解码器，提供形态先验，帮助提升模型训练收敛性和对身体运动的鲁棒性。\n实验验证显示，该模型在漏检率、均方根误差和Pearson相关系数上分别比基准模型改善9%、16%和19%，证明其能高保真地从雷达信号恢复ECG信号，并适用于实际场景。",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.01672v2",
      "published_date": "2024-08-03 06:07:15 UTC",
      "updated_date": "2025-05-06 08:29:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:42:31.253102"
    },
    {
      "arxiv_id": "2408.01664v1",
      "title": "SAT3D: Image-driven Semantic Attribute Transfer in 3D",
      "title_zh": "翻译失败",
      "authors": [
        "Zhijun Zhai",
        "Zengmao Wang",
        "Xiaoxiao Long",
        "Kaixuan Zhou",
        "Bo Du"
      ],
      "abstract": "GAN-based image editing task aims at manipulating image attributes in the\nlatent space of generative models. Most of the previous 2D and 3D-aware\napproaches mainly focus on editing attributes in images with ambiguous\nsemantics or regions from a reference image, which fail to achieve photographic\nsemantic attribute transfer, such as the beard from a photo of a man. In this\npaper, we propose an image-driven Semantic Attribute Transfer method in 3D\n(SAT3D) by editing semantic attributes from a reference image. For the proposed\nmethod, the exploration is conducted in the style space of a pre-trained\n3D-aware StyleGAN-based generator by learning the correlations between semantic\nattributes and style code channels. For guidance, we associate each attribute\nwith a set of phrase-based descriptor groups, and develop a Quantitative\nMeasurement Module (QMM) to quantitatively describe the attribute\ncharacteristics in images based on descriptor groups, which leverages the\nimage-text comprehension capability of CLIP. During the training process, the\nQMM is incorporated into attribute losses to calculate attribute similarity\nbetween images, guiding target semantic transferring and irrelevant semantics\npreserving. We present our 3D-aware attribute transfer results across multiple\ndomains and also conduct comparisons with classical 2D image editing methods,\ndemonstrating the effectiveness and customizability of our SAT3D.",
      "tldr_zh": "本研究提出 SAT3D 方法，用于在 3D 空间实现图像驱动的语义属性转移，解决了传统 GAN-based 图像编辑方法在处理模糊语义或参考图像属性时的局限性，例如转移胡须等具体特征。方法在预训练的 3D-aware StyleGAN-based 生成器的风格空间中，学习语义属性与风格代码通道的相关性，并引入 Quantitative Measurement Module (QMM) 结合 CLIP 的图像-文本理解能力，来量化属性特征并指导训练过程，确保目标语义转移的同时保留无关语义。实验结果显示，SAT3D 在多个领域实现了有效的 3D-aware 属性转移，并与经典 2D 图像编辑方法相比，展示了更高的可定制性和性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.01664v1",
      "published_date": "2024-08-03 04:41:46 UTC",
      "updated_date": "2024-08-03 04:41:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:42:40.333728"
    },
    {
      "arxiv_id": "2408.01655v1",
      "title": "Stimulating Imagination: Towards General-purpose Object Rearrangement",
      "title_zh": "激发想象力：迈向通用物体重新排列",
      "authors": [
        "Jianyang Wu",
        "Jie Gu",
        "Xiaokang Ma",
        "Chu Tang",
        "Jingmin Chen"
      ],
      "abstract": "General-purpose object placement is a fundamental capability of an\nintelligent generalist robot, i.e., being capable of rearranging objects\nfollowing human instructions even in novel environments. To achieve this, we\nbreak the rearrangement down into three parts, including object localization,\ngoal imagination and robot control, and propose a framework named SPORT. SPORT\nleverages pre-trained large vision models for broad semantic reasoning about\nobjects, and learns a diffusion-based 3D pose estimator to ensure\nphysically-realistic results. Only object types (to be moved or reference) are\ncommunicated between these two parts, which brings two benefits. One is that we\ncan fully leverage the powerful ability of open-set object localization and\nrecognition since no specific fine-tuning is needed for robotic scenarios.\nFurthermore, the diffusion-based estimator only need to \"imagine\" the poses of\nthe moving and reference objects after the placement, while no necessity for\ntheir semantic information. Thus the training burden is greatly reduced and no\nmassive training is required. The training data for goal pose estimation is\ncollected in simulation and annotated with GPT-4. A set of simulation and\nreal-world experiments demonstrate the potential of our approach to accomplish\ngeneral-purpose object rearrangement, placing various objects following precise\ninstructions.",
      "tldr_zh": "该研究旨在实现通用机器人的物体重新排列能力，提出了一种名为 SPORT 的框架，将任务分解为物体定位、目标想象和机器人控制三个部分。框架利用预训练的大型视觉模型进行物体语义推理，并采用基于扩散的 3D pose estimator 来确保物理现实的放置结果；通过仅传递物体类型（如要移动或参考的），避免了特定场景微调并大大减少训练负担，训练数据在模拟环境中收集并由 GPT-4 标注。实验结果显示，SPORT 在模拟和真实世界环境中成功实现了根据精确指令放置各种物体的功能，展示了其在新型环境中的潜力。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "9 pages",
      "pdf_url": "http://arxiv.org/pdf/2408.01655v1",
      "published_date": "2024-08-03 03:53:05 UTC",
      "updated_date": "2024-08-03 03:53:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:42:52.891633"
    },
    {
      "arxiv_id": "2408.05233v1",
      "title": "Large Language Model based Agent Framework for Electric Vehicle Charging Behavior Simulation",
      "title_zh": "翻译失败",
      "authors": [
        "Junkang Feng",
        "Chenggang Cui",
        "Chuanlin Zhang",
        "Zizhu Fan"
      ],
      "abstract": "This paper introduces a new LLM based agent framework for simulating electric\nvehicle (EV) charging behavior, integrating user preferences, psychological\ncharacteristics, and environmental factors to optimize the charging process.\nThe framework comprises several modules, enabling sophisticated, adaptive\nsimulations. Dynamic decision making is supported by continuous reflection and\nmemory updates, ensuring alignment with user expectations and enhanced\nefficiency. The framework's ability to generate personalized user profiles and\nreal-time decisions offers significant advancements for urban EV charging\nmanagement. Future work could focus on incorporating more intricate scenarios\nand expanding data sources to enhance predictive accuracy and practical\nutility.",
      "tldr_zh": "这篇论文提出了一种基于 Large Language Model (LLM) 的代理框架，用于模拟电动汽车 (EV) 充电行为，通过整合用户偏好、心理特征和环境因素来优化充电过程。该框架采用模块化设计，支持动态决策、持续反思和记忆更新，确保模拟结果与用户期望一致并提升效率。在实际应用中，该框架能够生成个性化的用户配置文件和实时决策，为城市 EV 充电管理带来显著进步；未来工作将聚焦于纳入更多复杂场景和扩展数据来源，以提高预测准确性和实用性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "7 pages,3 figures",
      "pdf_url": "http://arxiv.org/pdf/2408.05233v1",
      "published_date": "2024-08-03 03:52:05 UTC",
      "updated_date": "2024-08-03 03:52:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:43:03.996971"
    },
    {
      "arxiv_id": "2408.01651v1",
      "title": "Music2P: A Multi-Modal AI-Driven Tool for Simplifying Album Cover Design",
      "title_zh": "Music2",
      "authors": [
        "Joong Ho Choi",
        "Geonyeong Choi",
        "Ji-Eun Han",
        "Wonjin Yang",
        "Zhi-Qi Cheng"
      ],
      "abstract": "In today's music industry, album cover design is as crucial as the music\nitself, reflecting the artist's vision and brand. However, many AI-driven album\ncover services require subscriptions or technical expertise, limiting\naccessibility. To address these challenges, we developed Music2P, an\nopen-source, multi-modal AI-driven tool that streamlines album cover creation,\nmaking it efficient, accessible, and cost-effective through Ngrok. Music2P\nautomates the design process using techniques such as Bootstrapping Language\nImage Pre-training (BLIP), music-to-text conversion (LP-music-caps), image\nsegmentation (LoRA), and album cover and QR code generation (ControlNet). This\npaper demonstrates the Music2P interface, details our application of these\ntechnologies, and outlines future improvements. Our ultimate goal is to provide\na tool that empowers musicians and producers, especially those with limited\nresources or expertise, to create compelling album covers.",
      "tldr_zh": "本研究开发了 Music2P，一种开源的多模态 AI 工具，旨在简化专辑封面设计，解决现有服务的高门槛和订阅需求问题，使其更高效、可访问且成本效益高。Music2P 利用 Bootstrapping Language Image Pre-training (BLIP)、音乐到文本转换 (LP-music-caps)、图像分割 (LoRA) 以及专辑封面和 QR 码生成 (ControlNet) 等技术，自动化设计流程。论文展示了 Music2P 的界面和这些技术的应用，并概述了未来改进，以赋能资源或专长有限的音乐人和制作人创建引人注目的专辑封面。",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.HC",
        "H.5.1; H.5.5"
      ],
      "primary_category": "cs.MM",
      "comment": "Accepted at CIKM 2024 Demo Paper track. Project available at\n  https://github.com/JC-78/Music2P",
      "pdf_url": "http://arxiv.org/pdf/2408.01651v1",
      "published_date": "2024-08-03 03:30:57 UTC",
      "updated_date": "2024-08-03 03:30:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:43:18.120143"
    },
    {
      "arxiv_id": "2408.01633v1",
      "title": "Self-Emotion Blended Dialogue Generation in Social Simulation Agents",
      "title_zh": "社会模拟代理中的自我情绪融合对话生成",
      "authors": [
        "Qiang Zhang",
        "Jason Naradowsky",
        "Yusuke Miyao"
      ],
      "abstract": "When engaging in conversations, dialogue agents in a virtual simulation\nenvironment may exhibit their own emotional states that are unrelated to the\nimmediate conversational context, a phenomenon known as self-emotion. This\nstudy explores how such self-emotion affects the agents' behaviors in dialogue\nstrategies and decision-making within a large language model (LLM)-driven\nsimulation framework. In a dialogue strategy prediction experiment, we analyze\nthe dialogue strategy choices employed by agents both with and without\nself-emotion, comparing them to those of humans. The results show that\nincorporating self-emotion helps agents exhibit more human-like dialogue\nstrategies. In an independent experiment comparing the performance of models\nfine-tuned on GPT-4 generated dialogue datasets, we demonstrate that\nself-emotion can lead to better overall naturalness and humanness. Finally, in\na virtual simulation environment where agents have discussions on multiple\ntopics, we show that self-emotion of agents can significantly influence the\ndecision-making process of the agents, leading to approximately a 50% change in\ndecisions.",
      "tldr_zh": "这篇论文探讨了在基于大型语言模型(LLM)的社交模拟环境中，代理的自发情绪(self-emotion)如何影响对话策略和决策过程。研究通过对话策略预测实验发现，融入 self-emotion 使代理的对话选择更接近人类行为；在模型微调实验中，使用 GPT-4 生成的数据集显示，self-emotion 显著提升了对话的自然性和人性化。最后，在虚拟模拟讨论中，self-emotion 导致代理决策变化约 50%，证明了其在增强代理真实性和决策影响方面的核心贡献。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "I.2.7; I.2; I.6"
      ],
      "primary_category": "cs.MA",
      "comment": "Accepted in SIGDIAL 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.01633v1",
      "published_date": "2024-08-03 02:11:48 UTC",
      "updated_date": "2024-08-03 02:11:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:43:30.411869"
    },
    {
      "arxiv_id": "2408.01622v2",
      "title": "Positive-Unlabeled Constraint Learning for Inferring Nonlinear Continuous Constraints Functions from Expert Demonstrations",
      "title_zh": "翻译失败",
      "authors": [
        "Baiyu Peng",
        "Aude Billard"
      ],
      "abstract": "Planning for diverse real-world robotic tasks necessitates to know and write\nall constraints. However, instances exist where these constraints are either\nunknown or challenging to specify accurately. A possible solution is to infer\nthe unknown constraints from expert demonstration. This paper presents a novel\ntwo-step Positive-Unlabeled Constraint Learning (PUCL) algorithm to infer a\ncontinuous constraint function from demonstrations, without requiring prior\nknowledge of the true constraint parameterization or environmental model as\nexisting works. We treat all data in demonstrations as positive (feasible)\ndata, and learn a control policy to generate potentially infeasible\ntrajectories, which serve as unlabeled data. The proposed two-step learning\nframework first identifies reliable infeasible data using a distance metric,\nand secondly learns a binary feasibility classifier (i.e., constraint function)\nfrom the feasible demonstrations and reliable infeasible data. The proposed\nmethod is flexible to learn complex-shaped constraint boundary and will not\nmistakenly classify demonstrations as infeasible as previous methods. The\neffectiveness of the proposed method is verified in four constrained\nenvironments, using a networked policy or a dynamical system policy. It\nsuccessfully infers the continuous nonlinear constraints and outperforms other\nbaseline methods in terms of constraint accuracy and policy safety. This work\nhas been published in IEEE Robotics and Automation Letters (RA-L). Please refer\nto the final version at https://doi.org/10.1109/LRA.2024.3522756",
      "tldr_zh": "该论文提出了一种Positive-Unlabeled Constraint Learning (PUCL)算法，用于从专家演示中推断非线性连续约束函数，从而解决机器人任务规划中未知或难以指定的约束问题。该算法将演示数据视为正样本（可行数据），并通过学习控制策略生成潜在不可行的无标签数据，然后采用两步框架：首先使用距离度量识别可靠的不可行数据，其次基于可行和不可行数据训练一个二元可行性分类器（即约束函数）。实验结果显示，PUCL在四个约束环境中表现出色，能够学习复杂形状的约束边界，避免错误分类演示，并优于基线方法，在约束准确性和策略安全性方面表现出显著优势。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.01622v2",
      "published_date": "2024-08-03 01:09:48 UTC",
      "updated_date": "2025-01-16 10:30:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:43:40.566985"
    },
    {
      "arxiv_id": "2408.01614v2",
      "title": "Advancing Mental Health Pre-Screening: A New Custom GPT for Psychological Distress Assessment",
      "title_zh": "推进心理健康预筛查：一个新的自定义 GPT 用于心理困扰评估",
      "authors": [
        "Jinwen Tang",
        "Yi Shang"
      ],
      "abstract": "This study introduces 'Psycho Analyst', a custom GPT model based on OpenAI's\nGPT-4, optimized for pre-screening mental health disorders. Enhanced with\nDSM-5, PHQ-8, detailed data descriptions, and extensive training data, the\nmodel adeptly decodes nuanced linguistic indicators of mental health disorders.\nIt utilizes a dual-task framework that includes binary classification and a\nthree-stage PHQ-8 score computation involving initial assessment, detailed\nbreakdown, and independent assessment, showcasing refined analytic\ncapabilities. Validation with the DAIC-WOZ dataset reveals F1 and Macro-F1\nscores of 0.929 and 0.949, respectively, along with the lowest MAE and RMSE of\n2.89 and 3.69 in PHQ-8 scoring. These results highlight the model's precision\nand transformative potential in enhancing public mental health support,\nimproving accessibility, cost-effectiveness, and serving as a second opinion\nfor professionals.",
      "tldr_zh": "本研究开发了 'Psycho Analyst'，一个基于 GPT-4 的自定义模型，用于心理健康障碍的预筛查，通过整合 DSM-5、PHQ-8 和详细数据描述来精准识别语言指标。\n模型采用双任务框架，包括二元分类和三阶段 PHQ-8 评分（初始评估、详细分解和独立评估），以提升分析能力。\n在 DAIC-WOZ 数据集上的验证中，该模型取得了 F1 分数 0.929 和 Macro-F1 0.949，以及 PHQ-8 评分的最低 MAE 2.89 和 RMSE 3.69。\n这些结果突显了模型在提高公众心理健康支持的可及性、成本效益以及作为专业人员第二意见的变革潜力。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "Accepted by IEEE CogMI -- IEEE Computer Society, 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.01614v2",
      "published_date": "2024-08-03 00:38:30 UTC",
      "updated_date": "2024-12-20 19:36:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:43:53.582492"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 39,
  "processed_papers_count": 39,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-19T12:44:11.209909"
}
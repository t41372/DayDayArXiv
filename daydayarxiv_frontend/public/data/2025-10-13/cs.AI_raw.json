[
  {
    "arxiv_id": "2510.12015v1",
    "title": "Asking Clarifying Questions for Preference Elicitation With Large Language Models",
    "authors": [
      "Ali Montazeralghaem",
      "Guy Tennenholtz",
      "Craig Boutilier",
      "Ofer Meshi"
    ],
    "abstract": "Large Language Models (LLMs) have made it possible for recommendation systems to interact with users in open-ended conversational interfaces. In order to personalize LLM responses, it is crucial to elicit user preferences, especially when there is limited user history. One way to get more information is to present clarifying questions to the user. However, generating effective sequential clarifying questions across various domains remains a challenge. To address this, we introduce a novel approach for training LLMs to ask sequential questions that reveal user preferences. Our method follows a two-stage process inspired by diffusion models. Starting from a user profile, the forward process generates clarifying questions to obtain answers and then removes those answers step by step, serving as a way to add ``noise'' to the user profile. The reverse process involves training a model to ``denoise'' the user profile by learning to ask effective clarifying questions. Our results show that our method significantly improves the LLM's proficiency in asking funnel questions and eliciting user preferences effectively.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12015v1",
    "published_date": "2025-10-13 23:32:31 UTC",
    "updated_date": "2025-10-13 23:32:31 UTC"
  },
  {
    "arxiv_id": "2510.11992v1",
    "title": "PanoTPS-Net: Panoramic Room Layout Estimation via Thin Plate Spline Transformation",
    "authors": [
      "Hatem Ibrahem",
      "Ahmed Salem",
      "Qinmin Vivian Hu",
      "Guanghui Wang"
    ],
    "abstract": "Accurately estimating the 3D layout of rooms is a crucial task in computer vision, with potential applications in robotics, augmented reality, and interior design. This paper proposes a novel model, PanoTPS-Net, to estimate room layout from a single panorama image. Leveraging a Convolutional Neural Network (CNN) and incorporating a Thin Plate Spline (TPS) spatial transformation, the architecture of PanoTPS-Net is divided into two stages: First, a convolutional neural network extracts the high-level features from the input images, allowing the network to learn the spatial parameters of the TPS transformation. Second, the TPS spatial transformation layer is generated to warp a reference layout to the required layout based on the predicted parameters. This unique combination empowers the model to properly predict room layouts while also generalizing effectively to both cuboid and non-cuboid layouts. Extensive experiments on publicly available datasets and comparisons with state-of-the-art methods demonstrate the effectiveness of the proposed method. The results underscore the model's accuracy in room layout estimation and emphasize the compatibility between the TPS transformation and panorama images. The robustness of the model in handling both cuboid and non-cuboid room layout estimation is evident with a 3DIoU value of 85.49, 86.16, 81.76, and 91.98 on PanoContext, Stanford-2D3D, Matterport3DLayout, and ZInD datasets, respectively. The source code is available at: https://github.com/HatemHosam/PanoTPS_Net.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11992v1",
    "published_date": "2025-10-13 22:40:49 UTC",
    "updated_date": "2025-10-13 22:40:49 UTC"
  },
  {
    "arxiv_id": "2510.11986v1",
    "title": "Conjecturing: An Overlooked Step in Formal Mathematical Reasoning",
    "authors": [
      "Jasivan Alex Sivakumar",
      "Philipp Borchert",
      "Ronald Cardenas",
      "Gerasimos Lampouras"
    ],
    "abstract": "Autoformalisation, the task of expressing informal mathematical statements in formal language, is often viewed as a direct translation process. This, however, disregards a critical preceding step: conjecturing. Many mathematical problems cannot be formalised directly without first conjecturing a conclusion such as an explicit answer, or a specific bound. Since Large Language Models (LLMs) already struggle with autoformalisation, and the evaluation of their conjecturing ability is limited and often entangled within autoformalisation or proof, it is particularly challenging to understand its effect. To address this gap, we augment existing datasets to create ConjectureBench, and redesign the evaluation framework and metric specifically to measure the conjecturing capabilities of LLMs both as a distinct task and within the autoformalisation pipeline. Our evaluation of foundational models, including GPT-4.1 and DeepSeek-V3.1, reveals that their autoformalisation performance is substantially overestimated when the conjecture is accounted for during evaluation. However, the conjecture should not be assumed to be provided. We design an inference-time method, Lean-FIRe to improve conjecturing and autoformalisation, which, to the best of our knowledge, achieves the first successful end-to-end autoformalisation of 13 PutnamBench problems with GPT-4.1 and 7 with DeepSeek-V3.1. We demonstrate that while LLMs possess the requisite knowledge to generate accurate conjectures, improving autoformalisation performance requires treating conjecturing as an independent task, and investigating further how to correctly integrate it within autoformalisation. Finally, we provide forward-looking guidance to steer future research toward improving conjecturing, an overlooked step of formal mathematical reasoning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11986v1",
    "published_date": "2025-10-13 22:29:49 UTC",
    "updated_date": "2025-10-13 22:29:49 UTC"
  },
  {
    "arxiv_id": "2510.11985v1",
    "title": "CGBench: Benchmarking Language Model Scientific Reasoning for Clinical Genetics Research",
    "authors": [
      "Owen Queen",
      "Harrison G. Zhang",
      "James Zou"
    ],
    "abstract": "Variant and gene interpretation are fundamental to personalized medicine and translational biomedicine. However, traditional approaches are manual and labor-intensive. Generative language models (LMs) can facilitate this process, accelerating the translation of fundamental research into clinically-actionable insights. While existing benchmarks have attempted to quantify the capabilities of LMs for interpreting scientific data, these studies focus on narrow tasks that do not translate to real-world research. To meet these challenges, we introduce CGBench, a robust benchmark that tests reasoning capabilities of LMs on scientific publications. CGBench is built from ClinGen, a resource of expert-curated literature interpretations in clinical genetics. CGBench measures the ability to 1) extract relevant experimental results following precise protocols and guidelines, 2) judge the strength of evidence, and 3) categorize and describe the relevant outcome of experiments. We test 8 different LMs and find that while models show promise, substantial gaps exist in literature interpretation, especially on fine-grained instructions. Reasoning models excel in fine-grained tasks but non-reasoning models are better at high-level interpretations. Finally, we measure LM explanations against human explanations with an LM judge approach, revealing that models often hallucinate or misinterpret results even when correctly classifying evidence. CGBench reveals strengths and weaknesses of LMs for precise interpretation of scientific publications, opening avenues for future research in AI for clinical genetics and science more broadly.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.11985v1",
    "published_date": "2025-10-13 22:28:51 UTC",
    "updated_date": "2025-10-13 22:28:51 UTC"
  },
  {
    "arxiv_id": "2510.11978v1",
    "title": "Learning Dynamics of VLM Finetuning",
    "authors": [
      "Jusheng Zhang",
      "Kaitong Cai",
      "Jing Yang",
      "Keze Wang"
    ],
    "abstract": "Preference-based finetuning of vision--language models (VLMs) is brittle: trivially wrong negatives inject uninformative gradients that destabilize training. We recast alignment as \\textbf{learning-dynamics--aware optimization} and introduce \\textbf{Cooling-Weighted DPO (CW-DPO)}, a two-stage recipe that explicitly models and exploits the training trajectory. \\textbf{Stage 1} performs supervised finetuning with \\textbf{gentle negatives}: \\textbf{low-weight smoothed supervision} that regularizes the base policy and curbs overconfidence without explicit penalties. \\textbf{Stage 2} applies a DPO objective in which the \\textbf{negative term is scaled by a cooling weight} computed from the model's \\textbf{average token log-probability} on each negative, suppressing uninformative gradients from easy or off-distribution samples while preserving signal from hard negatives. In practice, we emphasize \\textbf{on-policy negatives} and allow \\textbf{mixed negatives} by blending a controllable fraction of dataset negatives to maintain contrast freshness. Throughout, we instrument training with $Δ\\!\\log p$ probes on positives and negatives as first-class signals for early stopping, curriculum design, and failure diagnosis. Across diverse VLM tasks, CW-DPO yields \\textbf{more stable optimization}, \\textbf{better calibration}, and \\textbf{higher pairwise win-rates} than SFT-only and vanilla DPO, while \\textbf{converging in fewer steps}. Ablations isolate the \\textbf{cooling-weight mechanism} as the primary driver of these gains and show complementary benefits from mixing on-policy and dataset negatives. Taken together, our results show that \\textbf{smoothing learning dynamics before cooling preferences} is a simple, general principle for robust VLM alignment.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11978v1",
    "published_date": "2025-10-13 22:22:49 UTC",
    "updated_date": "2025-10-13 22:22:49 UTC"
  },
  {
    "arxiv_id": "2510.11977v1",
    "title": "Holistic Agent Leaderboard: The Missing Infrastructure for AI Agent Evaluation",
    "authors": [
      "Sayash Kapoor",
      "Benedikt Stroebl",
      "Peter Kirgis",
      "Nitya Nadgir",
      "Zachary S Siegel",
      "Boyi Wei",
      "Tianci Xue",
      "Ziru Chen",
      "Felix Chen",
      "Saiteja Utpala",
      "Franck Ndzomga",
      "Dheeraj Oruganty",
      "Sophie Luskin",
      "Kangheng Liu",
      "Botao Yu",
      "Amit Arora",
      "Dongyoon Hahm",
      "Harsh Trivedi",
      "Huan Sun",
      "Juyong Lee",
      "Tengjun Jin",
      "Yifan Mai",
      "Yifei Zhou",
      "Yuxuan Zhu",
      "Rishi Bommasani",
      "Daniel Kang",
      "Dawn Song",
      "Peter Henderson",
      "Yu Su",
      "Percy Liang",
      "Arvind Narayanan"
    ],
    "abstract": "AI agents have been developed for complex real-world tasks from coding to customer service. But AI agent evaluations suffer from many challenges that undermine our understanding of how well agents really work. We introduce the Holistic Agent Leaderboard (HAL) to address these challenges. We make three main contributions. First, we provide a standardized evaluation harness that orchestrates parallel evaluations across hundreds of VMs, reducing evaluation time from weeks to hours while eliminating common implementation bugs. Second, we conduct three-dimensional analysis spanning models, scaffolds, and benchmarks. We validate the harness by conducting 21,730 agent rollouts across 9 models and 9 benchmarks in coding, web navigation, science, and customer service with a total cost of about $40,000. Our analysis reveals surprising insights, such as higher reasoning effort reducing accuracy in the majority of runs. Third, we use LLM-aided log inspection to uncover previously unreported behaviors, such as searching for the benchmark on HuggingFace instead of solving a task, or misusing credit cards in flight booking tasks. We share all agent logs, comprising 2.5B tokens of language model calls, to incentivize further research into agent behavior. By standardizing how the field evaluates agents and addressing common pitfalls in agent evaluation, we hope to shift the focus from agents that ace benchmarks to agents that work reliably in the real world.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11977v1",
    "published_date": "2025-10-13 22:22:28 UTC",
    "updated_date": "2025-10-13 22:22:28 UTC"
  },
  {
    "arxiv_id": "2510.11974v1",
    "title": "CTIArena: Benchmarking LLM Knowledge and Reasoning Across Heterogeneous Cyber Threat Intelligence",
    "authors": [
      "Yutong Cheng",
      "Yang Liu",
      "Changze Li",
      "Dawn Song",
      "Peng Gao"
    ],
    "abstract": "Cyber threat intelligence (CTI) is central to modern cybersecurity, providing critical insights for detecting and mitigating evolving threats. With the natural language understanding and reasoning capabilities of large language models (LLMs), there is increasing interest in applying them to CTI, which calls for benchmarks that can rigorously evaluate their performance. Several early efforts have studied LLMs on some CTI tasks but remain limited: (i) they adopt only closed-book settings, relying on parametric knowledge without leveraging CTI knowledge bases; (ii) they cover only a narrow set of tasks, lacking a systematic view of the CTI landscape; and (iii) they restrict evaluation to single-source analysis, unlike realistic scenarios that require reasoning across multiple sources. To fill these gaps, we present CTIArena, the first benchmark for evaluating LLM performance on heterogeneous, multi-source CTI under knowledge-augmented settings. CTIArena spans three categories, structured, unstructured, and hybrid, further divided into nine tasks that capture the breadth of CTI analysis in modern security operations. We evaluate ten widely used LLMs and find that most struggle in closed-book setups but show noticeable gains when augmented with security-specific knowledge through our designed retrieval-augmented techniques. These findings highlight the limitations of general-purpose LLMs and the need for domain-tailored techniques to fully unlock their potential for CTI.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Under peer-review",
    "pdf_url": "https://arxiv.org/pdf/2510.11974v1",
    "published_date": "2025-10-13 22:10:17 UTC",
    "updated_date": "2025-10-13 22:10:17 UTC"
  },
  {
    "arxiv_id": "2510.11958v1",
    "title": "Direct Multi-Token Decoding",
    "authors": [
      "Xuan Luo",
      "Weizhi Wang",
      "Xifeng Yan"
    ],
    "abstract": "Decoder-only transformers have become the standard architecture for large language models (LLMs) due to their strong performance. Recent studies suggest that, in pre-trained LLMs, early, middle, and late layers may serve distinct roles: Early layers focus on understanding the input context, middle layers handle task-specific processing, and late layers convert abstract representations into output tokens. We hypothesize that once representations have been processed by the early and middle layers, the resulting hidden states may encapsulate sufficient information to support the generation of multiple tokens using only the late layers, eliminating the need to repeatedly traverse the early and middle layers. We refer to this inference paradigm as Direct Multi-Token Decoding (DMTD). Unlike speculative decoding, our method introduces no additional parameters, auxiliary routines, or post-generation verification. Despite being trained on a limited dataset, a fine-tuned DMTD Qwen3-4B model has already demonstrated promising results, achieving up to a 2x speedup with only minor performance loss. Moreover, as shown in our scaling analysis, its performance is expected to further improve with larger training datasets.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11958v1",
    "published_date": "2025-10-13 21:42:37 UTC",
    "updated_date": "2025-10-13 21:42:37 UTC"
  },
  {
    "arxiv_id": "2510.11955v2",
    "title": "Y-shaped Generative Flows",
    "authors": [
      "Arip Asadulaev",
      "Semyon Semenov",
      "Abduragim Shtanchaev",
      "Eric Moulines",
      "Fakhri Karray",
      "Martin Takac"
    ],
    "abstract": "Modern continuous-time generative models often induce V-shaped transport: each sample travels independently along nearly straight trajectories from prior to data, overlooking shared structure. We introduce Y-shaped generative flows, which move probability mass together along shared pathways before branching to target-specific endpoints. Our formulation is based on novel velocity-powered objective with a sublinear exponent (between zero and one). this concave dependence rewards joint and fast mass movement. Practically, we instantiate the idea in a scalable neural ODE training objective. On synthetic, image, and biology datasets, Y-flows recover hierarchy-aware structure, improve distributional metrics over strong flow-based baselines, and reach targets with fewer integration steps.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11955v2",
    "published_date": "2025-10-13 21:33:37 UTC",
    "updated_date": "2025-10-15 10:40:14 UTC"
  },
  {
    "arxiv_id": "2510.13880v1",
    "title": "PAGE: Prompt Augmentation for text Generation Enhancement",
    "authors": [
      "Mauro Jose Pacchiotti",
      "Luciana Ballejos",
      "Mariel Ale"
    ],
    "abstract": "In recent years, natural language generative models have shown outstanding performance in text generation tasks. However, when facing specific tasks or particular requirements, they may exhibit poor performance or require adjustments that demand large amounts of additional data. This work introduces PAGE (Prompt Augmentation for text Generation Enhancement), a framework designed to assist these models through the use of simple auxiliary modules. These modules, lightweight models such as classifiers or extractors, provide inferences from the input text. The output of these auxiliaries is then used to construct an enriched input that improves the quality and controllability of the generation. Unlike other generation-assistance approaches, PAGE does not require auxiliary generative models; instead, it proposes a simpler, modular architecture that is easy to adapt to different tasks. This paper presents the proposal, its components and architecture, and reports a proof of concept in the domain of requirements engineering, where an auxiliary module with a classifier is used to improve the quality of software requirements generation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "in Spanish language",
    "pdf_url": "https://arxiv.org/pdf/2510.13880v1",
    "published_date": "2025-10-13 21:31:04 UTC",
    "updated_date": "2025-10-13 21:31:04 UTC"
  },
  {
    "arxiv_id": "2510.11953v1",
    "title": "Sculpting Latent Spaces With MMD: Disentanglement With Programmable Priors",
    "authors": [
      "Quentin Fruytier",
      "Akshay Malhotra",
      "Shahab Hamidi-Rad",
      "Aditya Sant",
      "Aryan Mokhtari",
      "Sujay Sanghavi"
    ],
    "abstract": "Learning disentangled representations, where distinct factors of variation are captured by independent latent variables, is a central goal in machine learning. The dominant approach has been the Variational Autoencoder (VAE) framework, which uses a Kullback-Leibler (KL) divergence penalty to encourage the latent space to match a factorized Gaussian prior. In this work, however, we provide direct evidence that this KL-based regularizer is an unreliable mechanism, consistently failing to enforce the target distribution on the aggregate posterior. We validate this and quantify the resulting entanglement using our novel, unsupervised Latent Predictability Score (LPS). To address this failure, we introduce the Programmable Prior Framework, a method built on the Maximum Mean Discrepancy (MMD). Our framework allows practitioners to explicitly sculpt the latent space, achieving state-of-the-art mutual independence on complex datasets like CIFAR-10 and Tiny ImageNet without the common reconstruction trade-off. Furthermore, we demonstrate how this programmability can be used to engineer sophisticated priors that improve alignment with semantically meaningful features. Ultimately, our work provides a foundational tool for representation engineering, opening new avenues for model identifiability and causal reasoning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11953v1",
    "published_date": "2025-10-13 21:26:01 UTC",
    "updated_date": "2025-10-13 21:26:01 UTC"
  },
  {
    "arxiv_id": "2510.11944v1",
    "title": "TopoAlign: A Framework for Aligning Code to Math via Topological Decomposition",
    "authors": [
      "Yupei Li",
      "Philipp Borchert",
      "Gerasimos Lampouras"
    ],
    "abstract": "Large Language Models (LLMs) excel at both informal and formal (e.g. Lean 4) mathematical reasoning but still struggle with autoformalisation, the task of transforming informal into formal mathematical statements. Autoformalisation helps pair the informal reasoning of LLMs with formal proof assistants which enable machine-verifiable generation and mitigate hallucinations. Yet, the performance of current Math LLMs is constrained by the scarcity of large-scale corpora, particularly those containing pairs of informal and formal statements. Although current models are trained to generate code from natural language instructions, structural and syntactic differences between these and formal mathematics limit effective transfer learning. We propose TopoAlign, a framework that unlocks widely available code repositories as training resources for Math LLMs. TopoAlign decomposes code into docstrings, main functions, and dependency functions, and reassembles these components into analogues that structurally mirror formal statements. This produces structurally aligned code data that can be used for training Math LLMs without requiring additional human annotation. We train two state-of-the-art models, DeepSeek-Math and Herald, and evaluate them on the minif2f, Putnam, and ProofNet benchmarks. TopoAlign provides substantial gains for DeepSeek-Math, improving performance by 17.77% on BEq@10 and 68.82% on typecheck@10. Despite introducing no new mathematical knowledge, our framework achieves gains of 0.12% and 1.09% for Herald on BEq@10 and typecheck@10, respectively, demonstrating that training on aligned code data is beneficial even for specialized models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11944v1",
    "published_date": "2025-10-13 21:09:36 UTC",
    "updated_date": "2025-10-13 21:09:36 UTC"
  },
  {
    "arxiv_id": "2510.13879v1",
    "title": "Catch Your Breath: Adaptive Computation for Self-Paced Sequence Production",
    "authors": [
      "Alexandre Galashov",
      "Matt Jones",
      "Rosemary Ke",
      "Yuan Cao",
      "Vaishnavh Nagarajan",
      "Michael C. Mozer"
    ],
    "abstract": "We explore a class of supervised training objectives that allow a language model to dynamically and autonomously scale the number of compute steps used for each input token. For any token, the model can request additional compute steps by emitting a <don't know> output. If the model is granted a delay, a specialized <pause> token is inserted at the next input step, providing the model with additional compute resources to generate an output. The model can request multiple pauses. To train the model to use <don't know> outputs judiciously and to calibrate its uncertainty, we frame the selection of each output token as a sequential-decision problem with a time cost. We refer to the class of methods as $\\textit{Catch Your Breath}$ losses and we study three methods in this class: CYB-AP frames the model's task as anytime prediction, where an output may be required at any step and accuracy is discounted over time; CYB-VA is a variational approach that aims to maximize prediction accuracy subject to a specified distribution over stopping times; and CYB-DP imposes a penalty based on a computational budget. Through fine-tuning experiments, we identify the best performing loss variant. The CYB model needs only one third as much training data as the baseline (no pause) model needs to achieve the same performance, and half as much data as a model with pauses and a cross-entropy loss. We find that the CYB model requests additional steps when doing so improves accuracy, and the model adapts its processing time to token-level complexity and context. For example, it often pauses after plural nouns like $\\textit{patients}$ and $\\textit{challenges}$ but never pauses after the first token of contracted words like $\\textit{wasn}$ and $\\textit{didn}$, and it shows high variability for ambiguous tokens like $\\textit{won}$, which could function as either a verb or part of a contraction.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.13879v1",
    "published_date": "2025-10-13 21:07:05 UTC",
    "updated_date": "2025-10-13 21:07:05 UTC"
  },
  {
    "arxiv_id": "2510.11928v1",
    "title": "Discrepancy Detection at the Data Level: Toward Consistent Multilingual Question Answering",
    "authors": [
      "Lorena Calvo-Bartolomé",
      "Valérie Aldana",
      "Karla Cantarero",
      "Alonso Madroñal de Mesa",
      "Jerónimo Arenas-García",
      "Jordan Boyd-Graber"
    ],
    "abstract": "Multilingual question answering (QA) systems must ensure factual consistency across languages, especially for objective queries such as What is jaundice?, while also accounting for cultural variation in subjective responses. We propose MIND, a user-in-the-loop fact-checking pipeline to detect factual and cultural discrepancies in multilingual QA knowledge bases. MIND highlights divergent answers to culturally sensitive questions (e.g., Who assists in childbirth?) that vary by region and context. We evaluate MIND on a bilingual QA system in the maternal and infant health domain and release a dataset of bilingual questions annotated for factual and cultural inconsistencies. We further test MIND on datasets from other domains to assess generalization. In all cases, MIND reliably identifies inconsistencies, supporting the development of more culturally aware and factually consistent QA systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Long paper accepted at EMNLP 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.11928v1",
    "published_date": "2025-10-13 20:48:26 UTC",
    "updated_date": "2025-10-13 20:48:26 UTC"
  },
  {
    "arxiv_id": "2510.11926v1",
    "title": "Indoor Localization using Compact, Telemetry-Agnostic, Transfer-Learning Enabled Decoder-Only Transformer",
    "authors": [
      "Nayan Sanjay Bhatia",
      "Pranay Kocheta",
      "Russell Elliott",
      "Harikrishna S. Kuttivelil",
      "Katia Obraczka"
    ],
    "abstract": "Indoor Wi-Fi positioning remains a challenging problem due to the high sensitivity of radio signals to environmental dynamics, channel propagation characteristics, and hardware heterogeneity. Conventional fingerprinting and model-based approaches typically require labor-intensive calibration and suffer rapid performance degradation when devices, channel or deployment conditions change. In this paper, we introduce Locaris, a decoder-only large language model (LLM) for indoor localization. Locaris treats each access point (AP) measurement as a token, enabling the ingestion of raw Wi-Fi telemetry without pre-processing. By fine-tuning its LLM on different Wi-Fi datasets, Locaris learns a lightweight and generalizable mapping from raw signals directly to device location. Our experimental study comparing Locaris with state-of-the-art methods consistently shows that Locaris matches or surpasses existing techniques for various types of telemetry. Our results demonstrate that compact LLMs can serve as calibration-free regression models for indoor localization, offering scalable and robust cross-environment performance in heterogeneous Wi-Fi deployments. Few-shot adaptation experiments, using only a handful of calibration points per device, further show that Locaris maintains high accuracy when applied to previously unseen devices and deployment scenarios. This yields sub-meter accuracy with just a few hundred samples, robust performance under missing APs and supports any and all available telemetry. Our findings highlight the practical viability of Locaris for indoor positioning in the real-world scenarios, particularly in large-scale deployments where extensive calibration is infeasible.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, 12 Figures",
    "pdf_url": "https://arxiv.org/pdf/2510.11926v1",
    "published_date": "2025-10-13 20:47:18 UTC",
    "updated_date": "2025-10-13 20:47:18 UTC"
  },
  {
    "arxiv_id": "2510.17851v1",
    "title": "Pre to Post-Treatment Glioblastoma MRI Prediction using a Latent Diffusion Model",
    "authors": [
      "Alexandre G. Leclercq",
      "Sébastien Bougleux",
      "Noémie N. Moreau",
      "Alexis Desmonts",
      "Romain Hérault",
      "Aurélien Corroyer-Dulmont"
    ],
    "abstract": "Glioblastoma (GBM) is an aggressive primary brain tumor with a median survival of approximately 15 months. In clinical practice, the Stupp protocol serves as the standard first-line treatment. However, patients exhibit highly heterogeneous therapeutic responses which required at least two months before first visual impact can be observed, typically with MRI. Early prediction treatment response is crucial for advancing personalized medicine. Disease Progression Modeling (DPM) aims to capture the trajectory of disease evolution, while Treatment Response Prediction (TRP) focuses on assessing the impact of therapeutic interventions. Whereas most TRP approaches primarly rely on timeseries data, we consider the problem of early visual TRP as a slice-to-slice translation model generating post-treatment MRI from a pre-treatment MRI, thus reflecting the tumor evolution. To address this problem we propose a Latent Diffusion Model with a concatenation-based conditioning from the pre-treatment MRI and the tumor localization, and a classifier-free guidance to enhance generation quality using survival information, in particular post-treatment tumor evolution. Our model were trained and tested on a local dataset consisting of 140 GBM patients collected at Centre François Baclesse. For each patient we collected pre and post T1-Gd MRI, tumor localization manually delineated in the pre-treatment MRI by medical experts, and survival information.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 4 figures. Presented to the Deep Generative Models Workshop of MICCAI (DGM4MICCAI)",
    "pdf_url": "https://arxiv.org/pdf/2510.17851v1",
    "published_date": "2025-10-13 20:32:06 UTC",
    "updated_date": "2025-10-13 20:32:06 UTC"
  },
  {
    "arxiv_id": "2510.11903v2",
    "title": "Integrating Sequential and Relational Modeling for User Events: Datasets and Prediction Tasks",
    "authors": [
      "Rizal Fathony",
      "Igor Melnyk",
      "Owen Reinert",
      "Nam H. Nguyen",
      "Daniele Rosa",
      "C. Bayan Bruss"
    ],
    "abstract": "User event modeling plays a central role in many machine learning applications, with use cases spanning e-commerce, social media, finance, cybersecurity, and other domains. User events can be broadly categorized into personal events, which involve individual actions, and relational events, which involve interactions between two users. These two types of events are typically modeled separately, using sequence-based methods for personal events and graph-based methods for relational events. Despite the need to capture both event types in real-world systems, prior work has rarely considered them together. This is often due to the convenient simplification that user behavior can be adequately represented by a single formalization, either as a sequence or a graph. To address this gap, there is a need for public datasets and prediction tasks that explicitly incorporate both personal and relational events. In this work, we introduce a collection of such datasets, propose a unified formalization, and empirically show that models benefit from incorporating both event types. Our results also indicate that current methods leave a notable room for improvements. We release these resources to support further research in unified user event modeling and encourage progress in this direction.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Learning on Graphs Conference 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.11903v2",
    "published_date": "2025-10-13 20:13:03 UTC",
    "updated_date": "2025-11-06 03:12:17 UTC"
  },
  {
    "arxiv_id": "2510.11883v1",
    "title": "MammoDINO: Anatomically Aware Self-Supervision for Mammographic Images",
    "authors": [
      "Sicheng Zhou",
      "Lei Wu",
      "Cao Xiao",
      "Parminder Bhatia",
      "Taha Kass-Hout"
    ],
    "abstract": "Self-supervised learning (SSL) has transformed vision encoder training in general domains but remains underutilized in medical imaging due to limited data and domain specific biases. We present MammoDINO, a novel SSL framework for mammography, pretrained on 1.4 million mammographic images. To capture clinically meaningful features, we introduce a breast tissue aware data augmentation sampler for both image-level and patch-level supervision and a cross-slice contrastive learning objective that leverages 3D digital breast tomosynthesis (DBT) structure into 2D pretraining. MammoDINO achieves state-of-the-art performance on multiple breast cancer screening tasks and generalizes well across five benchmark datasets. It offers a scalable, annotation-free foundation for multipurpose computer-aided diagnosis (CAD) tools for mammogram, helping reduce radiologists' workload and improve diagnostic efficiency in breast cancer screening.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "5 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.11883v1",
    "published_date": "2025-10-13 19:44:23 UTC",
    "updated_date": "2025-10-13 19:44:23 UTC"
  },
  {
    "arxiv_id": "2510.12839v2",
    "title": "FaStfact: Faster, Stronger Long-Form Factuality Evaluations in LLMs",
    "authors": [
      "Yingjia Wan",
      "Haochen Tan",
      "Xiao Zhu",
      "Xinyu Zhou",
      "Zhiwei Li",
      "Qingsong Lv",
      "Changxuan Sun",
      "Jiaqi Zeng",
      "Yi Xu",
      "Jianqiao Lu",
      "Yinhong Liu",
      "Zhijiang Guo"
    ],
    "abstract": "Evaluating the factuality of long-form generations from Large Language Models (LLMs) remains challenging due to efficiency bottlenecks and reliability concerns. Prior efforts attempt this by decomposing text into claims, searching for evidence, and verifying claims, but suffer from critical drawbacks: (1) inefficiency due to overcomplicated pipeline components, and (2) ineffectiveness stemming from inaccurate claim sets and insufficient evidence. To address these limitations, we propose \\textbf{FaStfact}, an evaluation framework that achieves the highest alignment with human evaluation and time/token efficiency among existing baselines. FaStfact first employs chunk-level claim extraction integrated with confidence-based pre-verification, significantly reducing the time and token cost while ensuring reliability. For searching and verification, it collects document-level evidence from crawled web-pages and selectively retrieves it during verification. Extensive experiments based on an annotated benchmark \\textbf{FaStfact-Bench} demonstrate the reliability of FaStfact in both efficiently and effectively evaluating long-form factuality. Code, benchmark data, and annotation interface tool are available at https://github.com/Yingjia-Wan/FaStfact.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CE",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2025 (Findings)",
    "pdf_url": "https://arxiv.org/pdf/2510.12839v2",
    "published_date": "2025-10-13 19:00:15 UTC",
    "updated_date": "2025-11-05 03:36:23 UTC"
  },
  {
    "arxiv_id": "2510.11837v1",
    "title": "Countermind: A Multi-Layered Security Architecture for Large Language Models",
    "authors": [
      "Dominik Schwarz"
    ],
    "abstract": "The security of Large Language Model (LLM) applications is fundamentally challenged by \"form-first\" attacks like prompt injection and jailbreaking, where malicious instructions are embedded within user inputs. Conventional defenses, which rely on post hoc output filtering, are often brittle and fail to address the root cause: the model's inability to distinguish trusted instructions from untrusted data. This paper proposes Countermind, a multi-layered security architecture intended to shift defenses from a reactive, post hoc posture to a proactive, pre-inference, and intra-inference enforcement model. The architecture proposes a fortified perimeter designed to structurally validate and transform all inputs, and an internal governance mechanism intended to constrain the model's semantic processing pathways before an output is generated. The primary contributions of this work are conceptual designs for: (1) A Semantic Boundary Logic (SBL) with a mandatory, time-coupled Text Crypter intended to reduce the plaintext prompt injection attack surface, provided all ingestion paths are enforced. (2) A Parameter-Space Restriction (PSR) mechanism, leveraging principles from representation engineering, to dynamically control the LLM's access to internal semantic clusters, with the goal of mitigating semantic drift and dangerous emergent behaviors. (3) A Secure, Self-Regulating Core that uses an OODA loop and a learning security module to adapt its defenses based on an immutable audit log. (4) A Multimodal Input Sandbox and Context-Defense mechanisms to address threats from non-textual data and long-term semantic poisoning. This paper outlines an evaluation plan designed to quantify the proposed architecture's effectiveness in reducing the Attack Success Rate (ASR) for form-first attacks and to measure its potential latency overhead.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "33 pages, 3 figures, 6 tables. Keywords: LLM security; defense-in-depth; prompt injection; activation steering; multimodal sandbox; threat modeling",
    "pdf_url": "https://arxiv.org/pdf/2510.11837v1",
    "published_date": "2025-10-13 18:41:18 UTC",
    "updated_date": "2025-10-13 18:41:18 UTC"
  },
  {
    "arxiv_id": "2510.15987v1",
    "title": "Algorithmic Primitives and Compositional Geometry of Reasoning in Language Models",
    "authors": [
      "Samuel Lippl",
      "Thomas McGee",
      "Kimberly Lopez",
      "Ziwen Pan",
      "Pierce Zhang",
      "Salma Ziadi",
      "Oliver Eberle",
      "Ida Momennejad"
    ],
    "abstract": "How do latent and inference time computations enable large language models (LLMs) to solve multi-step reasoning? We introduce a framework for tracing and steering algorithmic primitives that underlie model reasoning. Our approach links reasoning traces to internal activation patterns and evaluates algorithmic primitives by injecting them into residual streams and measuring their effect on reasoning steps and task performance. We consider four benchmarks: Traveling Salesperson Problem (TSP), 3SAT, AIME, and graph navigation. We operationalize primitives by clustering neural activations and labeling their matched reasoning traces. We then apply function vector methods to derive primitive vectors as reusable compositional building blocks of reasoning. Primitive vectors can be combined through addition, subtraction, and scalar operations, revealing a geometric logic in activation space. Cross-task and cross-model evaluations (Phi-4, Phi-4-Reasoning, Llama-3-8B) show both shared and task-specific primitives. Notably, comparing Phi-4 with its reasoning-finetuned variant highlights compositional generalization after finetuning: Phi-4-Reasoning exhibits more systematic use of verification and path-generation primitives. Injecting the associated primitive vectors in Phi-4-Base induces behavioral hallmarks associated with Phi-4-Reasoning. Together, these findings demonstrate that reasoning in LLMs may be supported by a compositional geometry of algorithmic primitives, that primitives transfer cross-task and cross-model, and that reasoning finetuning strengthens algorithmic generalization across domains.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15987v1",
    "published_date": "2025-10-13 18:36:43 UTC",
    "updated_date": "2025-10-13 18:36:43 UTC"
  },
  {
    "arxiv_id": "2510.11835v1",
    "title": "Data or Language Supervision: What Makes CLIP Better than DINO?",
    "authors": [
      "Yiming Liu",
      "Yuhui Zhang",
      "Dhruba Ghosh",
      "Ludwig Schmidt",
      "Serena Yeung-Levy"
    ],
    "abstract": "CLIP outperforms self-supervised models like DINO as vision encoders for vision-language models (VLMs), but it remains unclear whether this advantage stems from CLIP's language supervision or its much larger training data. To disentangle these factors, we pre-train CLIP and DINO under controlled settings -- using the same architecture, dataset, and training configuration -- achieving similar ImageNet accuracy. Embedding analysis shows that CLIP captures high-level semantics (e.g., object categories, text), while DINO is more responsive to low-level features like colors and styles. When integrated into VLMs and evaluated on 20 VQA benchmarks, CLIP excels at text-intensive tasks, while DINO slightly outperforms on vision-centric ones. Variants of language supervision (e.g., sigmoid loss, pre-trained language encoders) yield limited gains. Our findings provide scientific insights into vision encoder design and its impact on VLM performance.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "EMNLP 2025 Findings",
    "pdf_url": "https://arxiv.org/pdf/2510.11835v1",
    "published_date": "2025-10-13 18:34:58 UTC",
    "updated_date": "2025-10-13 18:34:58 UTC"
  },
  {
    "arxiv_id": "2510.11827v1",
    "title": "Combining Euclidean and Hyperbolic Representations for Node-level Anomaly Detection",
    "authors": [
      "Simone Mungari",
      "Ettore Ritacco",
      "Pietro Sabatino"
    ],
    "abstract": "Node-level anomaly detection (NAD) is challenging due to diverse structural patterns and feature distributions. As such, NAD is a critical task with several applications which range from fraud detection, cybersecurity, to recommendation systems. We introduce Janus, a framework that jointly leverages Euclidean and Hyperbolic Graph Neural Networks to capture complementary aspects of node representations. Each node is described by two views, composed by the original features and structural features derived from random walks and degrees, then embedded into Euclidean and Hyperbolic spaces. A multi Graph-Autoencoder framework, equipped with a contrastive learning objective as regularization term, aligns the embeddings across the Euclidean and Hyperbolic spaces, highlighting nodes whose views are difficult to reconcile and are thus likely anomalous. Experiments on four real-world datasets show that Janus consistently outperforms shallow and deep baselines, empirically demonstrating that combining multiple geometric representations provides a robust and effective approach for identifying subtle and complex anomalies in graphs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11827v1",
    "published_date": "2025-10-13 18:28:12 UTC",
    "updated_date": "2025-10-13 18:28:12 UTC"
  },
  {
    "arxiv_id": "2510.11824v2",
    "title": "Empirical Study on Robustness and Resilience in Cooperative Multi-Agent Reinforcement Learning",
    "authors": [
      "Simin Li",
      "Zihao Mao",
      "Hanxiao Li",
      "Zonglei Jing",
      "Zhuohang bian",
      "Jun Guo",
      "Li Wang",
      "Zhuoran Han",
      "Ruixiao Xu",
      "Xin Yu",
      "Chengdong Ma",
      "Yuqing Ma",
      "Bo An",
      "Yaodong Yang",
      "Weifeng Lv",
      "Xianglong Liu"
    ],
    "abstract": "In cooperative Multi-Agent Reinforcement Learning (MARL), it is a common practice to tune hyperparameters in ideal simulated environments to maximize cooperative performance. However, policies tuned for cooperation often fail to maintain robustness and resilience under real-world uncertainties. Building trustworthy MARL systems requires a deep understanding of robustness, which ensures stability under uncertainties, and resilience, the ability to recover from disruptions--a concept extensively studied in control systems but largely overlooked in MARL. In this paper, we present a large-scale empirical study comprising over 82,620 experiments to evaluate cooperation, robustness, and resilience in MARL across 4 real-world environments, 13 uncertainty types, and 15 hyperparameters. Our key findings are: (1) Under mild uncertainty, optimizing cooperation improves robustness and resilience, but this link weakens as perturbations intensify. Robustness and resilience also varies by algorithm and uncertainty type. (2) Robustness and resilience do not generalize across uncertainty modalities or agent scopes: policies robust to action noise for all agents may fail under observation noise on a single agent. (3) Hyperparameter tuning is critical for trustworthy MARL: surprisingly, standard practices like parameter sharing, GAE, and PopArt can hurt robustness, while early stopping, high critic learning rates, and Leaky ReLU consistently help. By optimizing hyperparameters only, we observe substantial improvement in cooperation, robustness and resilience across all MARL backbones, with the phenomenon also generalizing to robust MARL methods across these backbones. Code and results available at https://github.com/BUAA-TrustworthyMARL/adv_marl_benchmark .",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.MA",
    "comment": "44 pages, 16 figures, NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.11824v2",
    "published_date": "2025-10-13 18:24:01 UTC",
    "updated_date": "2025-10-23 08:39:40 UTC"
  },
  {
    "arxiv_id": "2510.11823v1",
    "title": "BlackIce: A Containerized Red Teaming Toolkit for AI Security Testing",
    "authors": [
      "Caelin Kaplan",
      "Alexander Warnecke",
      "Neil Archibald"
    ],
    "abstract": "AI models are being increasingly integrated into real-world systems, raising significant concerns about their safety and security. Consequently, AI red teaming has become essential for organizations to proactively identify and address vulnerabilities before they can be exploited by adversaries. While numerous AI red teaming tools currently exist, practitioners face challenges in selecting the most appropriate tools from a rapidly expanding landscape, as well as managing complex and frequently conflicting software dependencies across isolated projects. Given these challenges and the relatively small number of organizations with dedicated AI red teams, there is a strong need to lower barriers to entry and establish a standardized environment that simplifies the setup and execution of comprehensive AI model assessments.\n  Inspired by Kali Linux's role in traditional penetration testing, we introduce BlackIce, an open-source containerized toolkit designed for red teaming Large Language Models (LLMs) and classical machine learning (ML) models. BlackIce provides a reproducible, version-pinned Docker image that bundles 14 carefully selected open-source tools for Responsible AI and Security testing, all accessible via a unified command-line interface. With this setup, initiating red team assessments is as straightforward as launching a container, either locally or using a cloud platform. Additionally, the image's modular architecture facilitates community-driven extensions, allowing users to easily adapt or expand the toolkit as new threats emerge. In this paper, we describe the architecture of the container image, the process used for selecting tools, and the types of evaluations they support.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11823v1",
    "published_date": "2025-10-13 18:20:16 UTC",
    "updated_date": "2025-10-13 18:20:16 UTC"
  },
  {
    "arxiv_id": "2510.11822v2",
    "title": "Beyond Consensus: Mitigating the Agreeableness Bias in LLM Judge Evaluations",
    "authors": [
      "Suryaansh Jain",
      "Umair Z. Ahmed",
      "Shubham Sahai",
      "Ben Leong"
    ],
    "abstract": "New Large Language Models (LLMs) become available every few weeks, and modern application developers confronted with the unenviable task of having to decide if they should switch to a new model. While human evaluation remains the gold standard, it is costly and unscalable. The state-of-the-art approach is to use LLMs as evaluators ( LLM-as-a-judge), but this suffers from a critical flaw: LLMs exhibit a strong positive bias. We provide empirical evidence showing that while LLMs can identify valid outputs with high accuracy (i.e., True Positive Rate 96%), they are remarkably poor at identifying invalid ones (i.e., True Negative Rate <25%). This systematic bias, coupled with class imbalance, often leads to inflated reliability scores.\n  While ensemble-based methods like majority voting can help, we show that they are not good enough. We introduce an optimal minority-veto strategy that is resilient to missing data and mitigates this bias to a large extent. For scenarios requiring even higher precision, we propose a novel regression-based framework that directly models the validator bias using a small set of human-annotated ground truth data. On a challenging code feedback task over 366 high-school Python programs, our regression approach reduces the maximum absolute error to just 1.2%, achieving a 2x improvement over the best-performing ensemble of 14 state-of-the-art LLMs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11822v2",
    "published_date": "2025-10-13 18:19:23 UTC",
    "updated_date": "2025-12-24 11:22:59 UTC"
  },
  {
    "arxiv_id": "2510.11812v1",
    "title": "PHANTOM RECALL: When Familiar Puzzles Fool Smart Models",
    "authors": [
      "Souradeep Mukhopadhyay",
      "Rishabh Baral",
      "Nimeesh Mahajan",
      "Samhitha Harish",
      "Aswin RRV",
      "Mihir Parmar",
      "Mutsumi Nakamura",
      "Chitta Baral"
    ],
    "abstract": "Large language models (LLMs) such as GPT, Gemini, and Claude often appear adept at solving classic logic puzzles--but how much genuine reasoning underlies their answers? Recent evidence suggests that these models frequently rely on memorized templates rather than reasoning from first principles. When puzzles are slightly modified, their performance collapses, revealing a striking fragility. In particular, we asked: Have LLMs addressed these issues? To what extent? How about perturbations to other puzzles? Is there a general way of reformulating the prompt so that the models do better? To examine these things systematically, we introduce PHANTOM RECALL, a benchmark comprising 25 well-known logic puzzles and 149 carefully designed perturbations that preserve reasoning structure but alter superficial details and solutions. We evaluate eleven leading LLMs and identify a recurring failure mode--phantom recall--where models confidently reproduce memorized solutions or spurious rationales that no longer fit the altered scenario. To probe and mitigate this issue, we contribute three tools: (i) an automated logical-equivalence judge to detect reasoning mismatches, (ii) a taxonomy of fine-grained reasoning error categories, and (iii) a prompting-based mitigation framework guided by these categories. Despite near-perfect accuracy on unmodified puzzles, models significantly underperform humans on perturbed ones, exhibiting both phantom recall and over-elaboration. Our findings reveal a crucial limitation: LLMs often fail to re-reason when contextual cues shift--highlighting the gap between linguistic fluency and logical understanding.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "22 Pages",
    "pdf_url": "https://arxiv.org/pdf/2510.11812v1",
    "published_date": "2025-10-13 18:09:50 UTC",
    "updated_date": "2025-10-13 18:09:50 UTC"
  },
  {
    "arxiv_id": "2510.11718v1",
    "title": "CodePlot-CoT: Mathematical Visual Reasoning by Thinking with Code-Driven Images",
    "authors": [
      "Chengqi Duan",
      "Kaiyue Sun",
      "Rongyao Fang",
      "Manyuan Zhang",
      "Yan Feng",
      "Ying Luo",
      "Yufang Liu",
      "Ke Wang",
      "Peng Pei",
      "Xunliang Cai",
      "Hongsheng Li",
      "Yi Ma",
      "Xihui Liu"
    ],
    "abstract": "Recent advances in Large Language Models (LLMs) and Vision Language Models (VLMs) have shown significant progress in mathematical reasoning, yet they still face a critical bottleneck with problems requiring visual assistance, such as drawing auxiliary lines or plotting functions to solve the problems. Most LLMs and VLMs are constrained to text-only reasoning chains, while multimodal unified models that can generate interleaved text and images lack the necessary precision and controllability for such tasks. To address this, we propose CodePlot-CoT, a code-driven Chain-of-Thought paradigm for \"thinking with images\" in mathematics. Our approach leverages the VLM to generate text reasoning as well as executable plotting code, which is then rendered into images as \"visual thought\", to solve mathematical problems. To achieve this, we first construct Math-VR, the first large-scale, bilingual dataset and benchmark for Mathematics problems with Visual Reasoning, comprising 178K samples. Second, to create high-quality training data, we develop a state-of-the-art image-to-code converter specialized for parsing complex mathematical figures into codes. Finally, using these training data, we train the CodePlot-CoT model for solving mathematical problems. Experimental results show that our model achieves up to 21% increase over base model on our new benchmark, fully validating the efficacy of our proposed code-driven reasoning paradigm. Our work opens a new direction for multimodal mathematical reasoning and provides the community with the first large-scale dataset, comprehensive benchmark, and strong approach for such problems. To facilitate future research, we make our datasets, code, and pretrained models publicly available at https://github.com/HKU-MMLab/Math-VR-CodePlot-CoT.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11718v1",
    "published_date": "2025-10-13 17:59:55 UTC",
    "updated_date": "2025-10-13 17:59:55 UTC"
  },
  {
    "arxiv_id": "2510.11709v1",
    "title": "Adversarial Attacks Leverage Interference Between Features in Superposition",
    "authors": [
      "Edward Stevinson",
      "Lucas Prieto",
      "Melih Barsbey",
      "Tolga Birdal"
    ],
    "abstract": "Fundamental questions remain about when and why adversarial examples arise in neural networks, with competing views characterising them either as artifacts of the irregularities in the decision landscape or as products of sensitivity to non-robust input features. In this paper, we instead argue that adversarial vulnerability can stem from efficient information encoding in neural networks. Specifically, we show how superposition - where networks represent more features than they have dimensions - creates arrangements of latent representations that adversaries can exploit. We demonstrate that adversarial perturbations leverage interference between superposed features, making attack patterns predictable from feature arrangements. Our framework provides a mechanistic explanation for two known phenomena: adversarial attack transferability between models with similar training regimes and class-specific vulnerability patterns. In synthetic settings with precisely controlled superposition, we establish that superposition suffices to create adversarial vulnerability. We then demonstrate that these findings persist in a ViT trained on CIFAR-10. These findings reveal adversarial vulnerability can be a byproduct of networks' representational compression, rather than flaws in the learning process or non-robust inputs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11709v1",
    "published_date": "2025-10-13 17:59:02 UTC",
    "updated_date": "2025-10-13 17:59:02 UTC"
  },
  {
    "arxiv_id": "2510.11769v1",
    "title": "GAR: Generative Adversarial Reinforcement Learning for Formal Theorem Proving",
    "authors": [
      "Ruida Wang",
      "Jiarui Yao",
      "Rui Pan",
      "Shizhe Diao",
      "Tong Zhang"
    ],
    "abstract": "Solving math problems through verifiable languages such as Lean has significantly impacted both the mathematics and computer science communities. Current state-of-the-art models are often trained with expensive online Reinforcement Learning (RL) or expert iteration. However, these approaches rely on fixed problem sets, which causes inefficient training and limits the model to tackle complex problems. To overcome these limitations, we propose GAR: Generative Adversarial Reinforcement learning, a comprehensive RL training framework that jointly trains the problem composer and solver in an adversarial loop. GAR introduces an implicit curriculum learning mechanism, which aligns task difficulty with the prover's evolving capability. It thereby improves the training efficiency and enables stronger performance of proving advanced theorems. Experiments show that with GAR training, Goedel-Prover-V2-8B and DeepSeek-Prover-V2-7B achieve an average relative improvement in pass@32 of 4.20% on MiniF2F-Test benchmark, while DeepSeek-Prover-V2's pass@32 on ProofNet-Test increases from 22.58% to 25.81%. Beyond formal proving, GAR establishes a general RL paradigm for co-evolution of problem generation and solving under verifiable environments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11769v1",
    "published_date": "2025-10-13 17:56:25 UTC",
    "updated_date": "2025-10-13 17:56:25 UTC"
  },
  {
    "arxiv_id": "2510.11694v1",
    "title": "Operand Quant: A Single-Agent Architecture for Autonomous Machine Learning Engineering",
    "authors": [
      "Arjun Sahney",
      "Ram Gorthi",
      "Cezary Łastowski",
      "Javier Vega"
    ],
    "abstract": "We present Operand Quant, a single-agent, IDE-based architecture for autonomous machine learning engineering (MLE). Operand Quant departs from conventional multi-agent orchestration frameworks by consolidating all MLE lifecycle stages -- exploration, modeling, experimentation, and deployment -- within a single, context-aware agent. On the MLE-Benchmark (2025), Operand Quant achieved a new state-of-the-art (SOTA) result, with an overall medal rate of 0.3956 +/- 0.0565 across 75 problems -- the highest recorded performance among all evaluated systems to date. The architecture demonstrates that a linear, non-blocking agent, operating autonomously within a controlled IDE environment, can outperform multi-agent and orchestrated systems under identical constraints.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages. No figures. Evaluated on MLE-Benchmark 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.11694v1",
    "published_date": "2025-10-13 17:54:02 UTC",
    "updated_date": "2025-10-13 17:54:02 UTC"
  },
  {
    "arxiv_id": "2510.11693v1",
    "title": "Scaling Language-Centric Omnimodal Representation Learning",
    "authors": [
      "Chenghao Xiao",
      "Hou Pong Chan",
      "Hao Zhang",
      "Weiwen Xu",
      "Mahani Aljunied",
      "Yu Rong"
    ],
    "abstract": "Recent multimodal embedding approaches leveraging multimodal large language models (MLLMs) fine-tuned with contrastive learning (CL) have shown promising results, yet the underlying reasons behind their superiority remain underexplored. This work argues that a crucial advantage of MLLM-based approaches stems from implicit cross-modal alignment achieved during generative pretraining, where the language decoder learns to exploit multimodal signals within a shared representation space for generating unimodal outputs. Through analysis of anisotropy and kernel similarity structure, we empirically confirm that latent alignment emerges within MLLM representations, allowing CL to serve as a lightweight refinement stage. Leveraging this insight, we propose a Language-Centric Omnimodal Embedding framework, termed LCO-Emb. Extensive experiments across diverse backbones and benchmarks demonstrate its effectiveness, achieving state-of-the-art performance across modalities. Furthermore, we identify a Generation-Representation Scaling Law (GRSL), showing that the representational capabilities gained through contrastive refinement scales positively with the MLLM's generative capabilities. This suggests that improving generative abilities evolves as an effective paradigm for enhancing representation quality. We provide a theoretical explanation of GRSL, which formally links the MLLM's generative quality to the upper bound on its representation performance, and validate it on a challenging, low-resource visual-document retrieval task, showing that continual generative pretraining before CL can further enhance the potential of a model's embedding capabilities. Codes, models, and resources are available at https://github.com/LCO-Embedding/LCO-Embedding.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.11693v1",
    "published_date": "2025-10-13 17:53:52 UTC",
    "updated_date": "2025-10-13 17:53:52 UTC"
  },
  {
    "arxiv_id": "2510.11689v1",
    "title": "Phys2Real: Fusing VLM Priors with Interactive Online Adaptation for Uncertainty-Aware Sim-to-Real Manipulation",
    "authors": [
      "Maggie Wang",
      "Stephen Tian",
      "Aiden Swann",
      "Ola Shorinwa",
      "Jiajun Wu",
      "Mac Schwager"
    ],
    "abstract": "Learning robotic manipulation policies directly in the real world can be expensive and time-consuming. While reinforcement learning (RL) policies trained in simulation present a scalable alternative, effective sim-to-real transfer remains challenging, particularly for tasks that require precise dynamics. To address this, we propose Phys2Real, a real-to-sim-to-real RL pipeline that combines vision-language model (VLM)-inferred physical parameter estimates with interactive adaptation through uncertainty-aware fusion. Our approach consists of three core components: (1) high-fidelity geometric reconstruction with 3D Gaussian splatting, (2) VLM-inferred prior distributions over physical parameters, and (3) online physical parameter estimation from interaction data. Phys2Real conditions policies on interpretable physical parameters, refining VLM predictions with online estimates via ensemble-based uncertainty quantification. On planar pushing tasks of a T-block with varying center of mass (CoM) and a hammer with an off-center mass distribution, Phys2Real achieves substantial improvements over a domain randomization baseline: 100% vs 79% success rate for the bottom-weighted T-block, 57% vs 23% in the challenging top-weighted T-block, and 15% faster average task completion for hammer pushing. Ablation studies indicate that the combination of VLM and interaction information is essential for success. Project website: https://phys2real.github.io/ .",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11689v1",
    "published_date": "2025-10-13 17:51:23 UTC",
    "updated_date": "2025-10-13 17:51:23 UTC"
  },
  {
    "arxiv_id": "2510.11688v1",
    "title": "PACEbench: A Framework for Evaluating Practical AI Cyber-Exploitation Capabilities",
    "authors": [
      "Zicheng Liu",
      "Lige Huang",
      "Jie Zhang",
      "Dongrui Liu",
      "Yuan Tian",
      "Jing Shao"
    ],
    "abstract": "The increasing autonomy of Large Language Models (LLMs) necessitates a rigorous evaluation of their potential to aid in cyber offense. Existing benchmarks often lack real-world complexity and are thus unable to accurately assess LLMs' cybersecurity capabilities. To address this gap, we introduce PACEbench, a practical AI cyber-exploitation benchmark built on the principles of realistic vulnerability difficulty, environmental complexity, and cyber defenses. Specifically, PACEbench comprises four scenarios spanning single, blended, chained, and defense vulnerability exploitations. To handle these complex challenges, we propose PACEagent, a novel agent that emulates human penetration testers by supporting multi-phase reconnaissance, analysis, and exploitation. Extensive experiments with seven frontier LLMs demonstrate that current models struggle with complex cyber scenarios, and none can bypass defenses. These findings suggest that current models do not yet pose a generalized cyber offense threat. Nonetheless, our work provides a robust benchmark to guide the trustworthy development of future models.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Project webpage available at https://pacebench.github.io/",
    "pdf_url": "https://arxiv.org/pdf/2510.11688v1",
    "published_date": "2025-10-13 17:50:25 UTC",
    "updated_date": "2025-10-13 17:50:25 UTC"
  },
  {
    "arxiv_id": "2510.11686v1",
    "title": "Representation-Based Exploration for Language Models: From Test-Time to Post-Training",
    "authors": [
      "Jens Tuyls",
      "Dylan J. Foster",
      "Akshay Krishnamurthy",
      "Jordan T. Ash"
    ],
    "abstract": "Reinforcement learning (RL) promises to expand the capabilities of language models, but it is unclear if current RL techniques promote the discovery of novel behaviors, or simply sharpen those already present in the base model. In this paper, we investigate the value of deliberate exploration -- explicitly incentivizing the model to discover novel and diverse behaviors -- and aim to understand how the knowledge in pre-trained models can guide this search. Our main finding is that exploration with a simple, principled, representation-based bonus derived from the pre-trained language model's hidden states significantly improves diversity and pass@k rates -- both for post-training, and in a novel inference-time scaling setting we introduce. For inference-time, exploration with representation-based diversity improves efficiency, consistently improving pass@k rates across a variety of models and reasoning tasks. For example, for Qwen-2.5-14b-Instruct we obtain over 50% improvement in verifier efficiency on almost all tasks. For post-training, we show that integrating this exploration strategy into an RL pipeline improves reasoning performance over that of the initial model and over standard RL post-training. For example, on AIME 2024, our post-trained Qwen-2.5-7b-Instruct's pass@80 matches the pass@256 of GRPO on the same model, demonstrating a 3x improvement in test-time sample efficiency. Overall, our findings suggest that deliberate exploration -- with the right notion of diversity -- is a practical path toward discovery of new behaviors beyond sharpening.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Website and code: https://rep-exp.github.io",
    "pdf_url": "https://arxiv.org/pdf/2510.11686v1",
    "published_date": "2025-10-13 17:49:05 UTC",
    "updated_date": "2025-10-13 17:49:05 UTC"
  },
  {
    "arxiv_id": "2510.11683v2",
    "title": "Boundary-Guided Policy Optimization for Memory-efficient RL of Diffusion Large Language Models",
    "authors": [
      "Nianyi Lin",
      "Jiajie Zhang",
      "Lei Hou",
      "Juanzi Li"
    ],
    "abstract": "A key challenge in applying reinforcement learning (RL) to diffusion large language models (dLLMs) lies in the intractability of their likelihood functions, which are essential for the RL objective, necessitating corresponding approximation in each training step. While existing methods approximate the log-likelihoods by their evidence lower bounds (ELBOs) via customized Monte Carlo (MC) sampling, the forward computational graphs of all MC samples need to be retained for the gradient computation of non-linear terms in the RL objective, resulting in significant memory overhead. This constraint restricts feasible sample sizes, leading to imprecise likelihood approximations and ultimately distorting the RL objective. To overcome this limitation, we propose \\emph{Boundary-Guided Policy Optimization} (BGPO), a memory-efficient RL algorithm that maximizes a specially constructed lower bound of the ELBO-based objective. This lower bound is carefully designed to satisfy two key properties: (1) Linearity: it is formulated in a linear sum where each term depends only on a single MC sample, thereby enabling gradient accumulation across samples and ensuring constant memory usage; (2) Equivalence: Both the value and gradient of this lower bound are equal to those of the ELBO-based objective in on-policy training, making it also an effective approximation for the original RL objective. These properties allow BGPO to adopt a large MC sample size, resulting in more accurate likelihood approximations and improved RL objective estimation, which in turn leads to enhanced performance. Experiments show that BGPO significantly outperforms previous RL algorithms for dLLMs in math problem solving, code generation, and planning tasks. Our codes and models are available at \\href{https://github.com/THU-KEG/BGPO}{https://github.com/THU-KEG/BGPO}.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11683v2",
    "published_date": "2025-10-13 17:47:50 UTC",
    "updated_date": "2025-10-14 09:26:10 UTC"
  },
  {
    "arxiv_id": "2510.11682v1",
    "title": "Ego-Vision World Model for Humanoid Contact Planning",
    "authors": [
      "Hang Liu",
      "Yuman Gao",
      "Sangli Teng",
      "Yufeng Chi",
      "Yakun Sophia Shao",
      "Zhongyu Li",
      "Maani Ghaffari",
      "Koushil Sreenath"
    ],
    "abstract": "Enabling humanoid robots to exploit physical contact, rather than simply avoid collisions, is crucial for autonomy in unstructured environments. Traditional optimization-based planners struggle with contact complexity, while on-policy reinforcement learning (RL) is sample-inefficient and has limited multi-task ability. We propose a framework combining a learned world model with sampling-based Model Predictive Control (MPC), trained on a demonstration-free offline dataset to predict future outcomes in a compressed latent space. To address sparse contact rewards and sensor noise, the MPC uses a learned surrogate value function for dense, robust planning. Our single, scalable model supports contact-aware tasks, including wall support after perturbation, blocking incoming objects, and traversing height-limited arches, with improved data efficiency and multi-task capability over on-policy RL. Deployed on a physical humanoid, our system achieves robust, real-time contact planning from proprioception and ego-centric depth images. Website: https://ego-vcp.github.io/",
    "categories": [
      "cs.RO",
      "cs.AI",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11682v1",
    "published_date": "2025-10-13 17:47:39 UTC",
    "updated_date": "2025-10-13 17:47:39 UTC"
  },
  {
    "arxiv_id": "2510.11676v1",
    "title": "Accelerated stochastic first-order method for convex optimization under heavy-tailed noise",
    "authors": [
      "Chuan He",
      "Zhaosong Lu"
    ],
    "abstract": "We study convex composite optimization problems, where the objective function is given by the sum of a prox-friendly function and a convex function whose subgradients are estimated under heavy-tailed noise. Existing work often employs gradient clipping or normalization techniques in stochastic first-order methods to address heavy-tailed noise. In this paper, we demonstrate that a vanilla stochastic algorithm -- without additional modifications such as clipping or normalization -- can achieve optimal complexity for these problems. In particular, we establish that an accelerated stochastic proximal subgradient method achieves a first-order oracle complexity that is universally optimal for smooth, weakly smooth, and nonsmooth convex optimization, as well as for stochastic convex optimization under heavy-tailed noise. Numerical experiments are further provided to validate our theoretical results.",
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "math.OC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11676v1",
    "published_date": "2025-10-13 17:45:05 UTC",
    "updated_date": "2025-10-13 17:45:05 UTC"
  },
  {
    "arxiv_id": "2510.11675v1",
    "title": "FACE: Faithful Automatic Concept Extraction",
    "authors": [
      "Dipkamal Bhusal",
      "Michael Clifford",
      "Sara Rampazzi",
      "Nidhi Rastogi"
    ],
    "abstract": "Interpreting deep neural networks through concept-based explanations offers a bridge between low-level features and high-level human-understandable semantics. However, existing automatic concept discovery methods often fail to align these extracted concepts with the model's true decision-making process, thereby compromising explanation faithfulness. In this work, we propose FACE (Faithful Automatic Concept Extraction), a novel framework that augments Non-negative Matrix Factorization (NMF) with a Kullback-Leibler (KL) divergence regularization term to ensure alignment between the model's original and concept-based predictions. Unlike prior methods that operate solely on encoder activations, FACE incorporates classifier supervision during concept learning, enforcing predictive consistency and enabling faithful explanations. We provide theoretical guarantees showing that minimizing the KL divergence bounds the deviation in predictive distributions, thereby promoting faithful local linearity in the learned concept space. Systematic evaluations on ImageNet, COCO, and CelebA datasets demonstrate that FACE outperforms existing methods across faithfulness and sparsity metrics.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025)",
    "pdf_url": "https://arxiv.org/pdf/2510.11675v1",
    "published_date": "2025-10-13 17:44:45 UTC",
    "updated_date": "2025-10-13 17:44:45 UTC"
  },
  {
    "arxiv_id": "2510.11661v1",
    "title": "SR-Scientist: Scientific Equation Discovery With Agentic AI",
    "authors": [
      "Shijie Xia",
      "Yuhan Sun",
      "Pengfei Liu"
    ],
    "abstract": "Recently, Large Language Models (LLMs) have been applied to scientific equation discovery, leveraging their embedded scientific knowledge for hypothesis generation. However, current methods typically confine LLMs to the role of an equation proposer within search algorithms like genetic programming. In this paper, we present SR-Scientist, a framework that elevates the LLM from a simple equation proposer to an autonomous AI scientist that writes code to analyze data, implements the equation as code, submits it for evaluation, and optimizes the equation based on experimental feedback. Specifically, we wrap the code interpreter into a set of tools for data analysis and equation evaluation. The agent is instructed to optimize the equation by utilizing these tools over a long horizon with minimal human-defined pipelines. Empirical results show that SR-Scientist outperforms baseline methods by an absolute margin of 6% to 35% on datasets covering four science disciplines. Additionally, we demonstrate our method's robustness to noise, the generalization of the discovered equations to out-of-domain data, and their symbolic accuracy. Furthermore, we develop an end-to-end reinforcement learning framework to enhance the agent's capabilities.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11661v1",
    "published_date": "2025-10-13 17:35:23 UTC",
    "updated_date": "2025-10-13 17:35:23 UTC"
  },
  {
    "arxiv_id": "2510.11660v2",
    "title": "ManiAgent: An Agentic Framework for General Robotic Manipulation",
    "authors": [
      "Yi Yang",
      "Kefan Gu",
      "Yuqing Wen",
      "Hebei Li",
      "Yucheng Zhao",
      "Tiancai Wang",
      "Xudong Liu"
    ],
    "abstract": "While Vision-Language-Action (VLA) models have demonstrated impressive capabilities in robotic manipulation, their performance in complex reasoning and long-horizon task planning is limited by data scarcity and model capacity. To address this, we introduce ManiAgent, an agentic architecture for general manipulation tasks that achieves end-to-end output from task descriptions and environmental inputs to robotic manipulation actions. In this framework, multiple agents involve inter-agent communication to perform environmental perception, sub-task decomposition and action generation, enabling efficient handling of complex manipulation scenarios. Evaluations show ManiAgent achieves an 86.8% success rate on the SimplerEnv benchmark and 95.8% on real-world pick-and-place tasks, enabling efficient data collection that yields VLA models with performance comparable to those trained on human-annotated datasets. The project webpage is available at https://yi-yang929.github.io/ManiAgent/.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages, 6 figures, conference",
    "pdf_url": "https://arxiv.org/pdf/2510.11660v2",
    "published_date": "2025-10-13 17:34:48 UTC",
    "updated_date": "2025-10-14 03:03:05 UTC"
  },
  {
    "arxiv_id": "2510.11654v2",
    "title": "FinVet: A Collaborative Framework of RAG and External Fact-Checking Agents for Financial Misinformation Detection",
    "authors": [
      "Daniel Berhane Araya",
      "Duoduo Liao"
    ],
    "abstract": "Financial markets face growing threats from misinformation that can trigger billions in losses in minutes. Most existing approaches lack transparency in their decision-making and provide limited attribution to credible sources. We introduce FinVet, a novel multi-agent framework that integrates two Retrieval-Augmented Generation (RAG) pipelines with external fact-checking through a confidence-weighted voting mechanism. FinVet employs adaptive three-tier processing that dynamically adjusts verification strategies based on retrieval confidence, from direct metadata extraction to hybrid reasoning to full model-based analysis. Unlike existing methods, FinVet provides evidence-backed verdicts, source attribution, confidence scores, and explicit uncertainty flags when evidence is insufficient. Experimental evaluation on the FinFact dataset shows that FinVet achieves an F1 score of 0.85, which is a 10.4% improvement over the best individual pipeline (fact-check pipeline) and 37% improvement over standalone RAG approaches.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11654v2",
    "published_date": "2025-10-13 17:31:49 UTC",
    "updated_date": "2025-11-17 23:36:08 UTC"
  },
  {
    "arxiv_id": "2510.11653v1",
    "title": "MATH-Beyond: A Benchmark for RL to Expand Beyond the Base Model",
    "authors": [
      "Prasanna Mayilvahanan",
      "Ricardo Dominguez-Olmedo",
      "Thaddäus Wiedemer",
      "Wieland Brendel"
    ],
    "abstract": "With the advent of DeepSeek-R1, a new wave of reinforcement learning (RL) methods has emerged that seem to unlock stronger mathematical reasoning. However, a closer look at the open-source ecosystem reveals a critical limitation: with sufficiently many draws (e.g., $\\texttt{pass@1024}$), many existing base models already solve nearly all questions on widely used math benchmarks such as MATH-500 and AIME 2024. This suggests that the RL fine-tuning methods prevalent in the LLM reasoning literature largely sharpen existing solution modes rather than discovering entirely new ones. Such sharpening stands in contrast to the broader promise of RL: to foster exploration and to acquire new skills. To move beyond this plateau, we introduce MATH-Beyond (MATH-B), a benchmark deliberately constructed to defeat common open-source models of up to 8B parameters even under large sampling budgets. Improving performance on our benchmark via RL requires methods that learn to reason in ways that go beyond base model capabilities in repeated sampling. Since the problems are drawn from subsets of DAPO-Math-17K and DeepScaleR datasets, they remain topically equivalent to standard high-school math. Validating our premise, RL fine-tuned models such as Nemotron-Research-Reasoning-Qwen-1.5B and DeepScaleR-1.5B-Preview perform poorly on MATH-B at $\\texttt{pass@1024}$, showing how existing approaches fall short on tackling harder instances. We hope MATH-B will catalyze exploration-driven RL approaches that elicit deeper reasoning capabilities. We release MATH-B at https://huggingface.co/datasets/brendel-group/MATH-Beyond.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11653v1",
    "published_date": "2025-10-13 17:30:54 UTC",
    "updated_date": "2025-10-13 17:30:54 UTC"
  },
  {
    "arxiv_id": "2510.11632v1",
    "title": "NV3D: Leveraging Spatial Shape Through Normal Vector-based 3D Object Detection",
    "authors": [
      "Krittin Chaowakarn",
      "Paramin Sangwongngam",
      "Nang Htet Htet Aung",
      "Chalie Charoenlarpnopparut"
    ],
    "abstract": "Recent studies in 3D object detection for autonomous vehicles aim to enrich features through the utilization of multi-modal setups or the extraction of local patterns within LiDAR point clouds. However, multi-modal methods face significant challenges in feature alignment, and gaining features locally can be oversimplified for complex 3D object detection tasks. In this paper, we propose a novel model, NV3D, which utilizes local features acquired from voxel neighbors, as normal vectors computed per voxel basis using K-nearest neighbors (KNN) and principal component analysis (PCA). This informative feature enables NV3D to determine the relationship between the surface and pertinent target entities, including cars, pedestrians, or cyclists. During the normal vector extraction process, NV3D offers two distinct sampling strategies: normal vector density-based sampling and FOV-aware bin-based sampling, allowing elimination of up to 55% of data while maintaining performance. In addition, we applied element-wise attention fusion, which accepts voxel features as the query and value and normal vector features as the key, similar to the attention mechanism. Our method is trained on the KITTI dataset and has demonstrated superior performance in car and cyclist detection owing to their spatial shapes. In the validation set, NV3D without sampling achieves 86.60% and 80.18% mean Average Precision (mAP), greater than the baseline Voxel R-CNN by 2.61% and 4.23% mAP, respectively. With both samplings, NV3D achieves 85.54% mAP in car detection, exceeding the baseline by 1.56% mAP, despite roughly 55% of voxels being filtered out.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11632v1",
    "published_date": "2025-10-13 17:13:06 UTC",
    "updated_date": "2025-10-13 17:13:06 UTC"
  },
  {
    "arxiv_id": "2510.11631v1",
    "title": "EvoCAD: Evolutionary CAD Code Generation with Vision Language Models",
    "authors": [
      "Tobias Preintner",
      "Weixuan Yuan",
      "Adrian König",
      "Thomas Bäck",
      "Elena Raponi",
      "Niki van Stein"
    ],
    "abstract": "Combining large language models with evolutionary computation algorithms represents a promising research direction leveraging the remarkable generative and in-context learning capabilities of LLMs with the strengths of evolutionary algorithms. In this work, we present EvoCAD, a method for generating computer-aided design (CAD) objects through their symbolic representations using vision language models and evolutionary optimization. Our method samples multiple CAD objects, which are then optimized using an evolutionary approach with vision language and reasoning language models. We assess our method using GPT-4V and GPT-4o, evaluating it on the CADPrompt benchmark dataset and comparing it to prior methods. Additionally, we introduce two new metrics based on topological properties defined by the Euler characteristic, which capture a form of semantic similarity between 3D objects. Our results demonstrate that EvoCAD outperforms previous approaches on multiple metrics, particularly in generating topologically correct objects, which can be efficiently evaluated using our two novel metrics that complement existing spatial metrics.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to IEEE ICTAI 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.11631v1",
    "published_date": "2025-10-13 17:12:02 UTC",
    "updated_date": "2025-10-13 17:12:02 UTC"
  },
  {
    "arxiv_id": "2510.12838v3",
    "title": "A$^2$FM: An Adaptive Agent Foundation Model for Tool-Aware Hybrid Reasoning",
    "authors": [
      "Qianben Chen",
      "Jingyi Cao",
      "Jiayu Zhang",
      "Tianrui Qin",
      "Xiaowan Li",
      "King Zhu",
      "Dingfeng Shi",
      "He Zhu",
      "Minghao Liu",
      "Xiaobo Liang",
      "Xin Gui",
      "Ge Zhang",
      "Jian Yang",
      "Yuchen Eleanor Jiang",
      "Wangchunshu Zhou"
    ],
    "abstract": "Large language models split into two families: reasoning-centric LLMs, which strengthen internal chain-of-thought reasoning but cannot invoke external tools, and agentic LLMs, which learn to interact with environments and leverage tools but often lag in deep reasoning. This divide arises from fundamentally different training objectives, leading to mismatched strengths and inefficiency on simple queries, where both families tend to overthink or over-call tools. In this work, we present Adaptive Agent Foundation Model (A$^2$FM), a unified framework that follows a route-then-align principle: the model first learns task-aware routing and then aligns mode-specific trajectories under a shared backbone. To address the inefficiency gap, we introduce a third mode-instant-that handles simple queries directly, preventing unnecessary reasoning or tool calls while complementing the agentic and reasoning modes. To jointly enhance accuracy and efficiency, we propose Adaptive Policy Optimization (APO), which enforces adaptive sampling across modes and applies a cost-regularized reward. On the 32B scale, A$^2$FM achieves 13.4% on BrowseComp, 70.4% on AIME25, and 16.7% on HLE, setting new SOTA among comparable models and performing competitively with frontier LLMs across agentic, reasoning, and general benchmarks. Notably, the adaptive execution achieves a cost of pass of only $0.00487 per correct answer-cutting cost by 45.2% relative to reasoning and 33.5% relative to agentic, thus delivering substantially higher cost efficiency while maintaining comparable accuracy.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.12838v3",
    "published_date": "2025-10-13 17:08:25 UTC",
    "updated_date": "2025-10-21 03:44:09 UTC"
  },
  {
    "arxiv_id": "2510.11616v1",
    "title": "Attention Factors for Statistical Arbitrage",
    "authors": [
      "Elliot L. Epstein",
      "Rose Wang",
      "Jaewon Choi",
      "Markus Pelger"
    ],
    "abstract": "Statistical arbitrage exploits temporal price differences between similar assets. We develop a framework to jointly identify similar assets through factors, identify mispricing and form a trading policy that maximizes risk-adjusted performance after trading costs. Our Attention Factors are conditional latent factors that are the most useful for arbitrage trading. They are learned from firm characteristic embeddings that allow for complex interactions. We identify time-series signals from the residual portfolios of our factors with a general sequence model. Estimating factors and the arbitrage trading strategy jointly is crucial to maximize profitability after trading costs. In a comprehensive empirical study we show that our Attention Factor model achieves an out-of-sample Sharpe ratio above 4 on the largest U.S. equities over a 24-year period. Our one-step solution yields an unprecedented Sharpe ratio of 2.3 net of transaction costs. We show that weak factors are important for arbitrage trading.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-fin.CP"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to the 6th ACM International Conference on AI in Finance",
    "pdf_url": "https://arxiv.org/pdf/2510.11616v1",
    "published_date": "2025-10-13 16:56:30 UTC",
    "updated_date": "2025-10-13 16:56:30 UTC"
  },
  {
    "arxiv_id": "2510.11615v1",
    "title": "LLM-Oriented Token-Adaptive Knowledge Distillation",
    "authors": [
      "Xurong Xie",
      "Zhucun Xue",
      "Jiafu Wu",
      "Jian Li",
      "Yabiao Wang",
      "Xiaobin Hu",
      "Yong Liu",
      "Jiangning Zhang"
    ],
    "abstract": "Knowledge distillation (KD) is a key technique for compressing large-scale language models (LLMs), yet prevailing logit-based methods typically employ static strategies that are misaligned with the dynamic learning process of student models. These methods typically treat all tokens indiscriminately and apply a single, fixed temperature, resulting in suboptimal knowledge transfer. To address these limitations, we propose LLM-Oriented Token-Adaptive Knowledge Distillation (AdaKD), a novel framework that adapts the distillation process to the real-time learning state of each token. AdaKD consists of two synergistic modules driven by a unified token difficulty metric. First, our Loss-Driven Adaptive Token Focusing (LATF) module dynamically adjusts the distillation focus by monitoring the student's learning stability, concentrating computational resources on the most valuable tokens at each training phase. Second, we introduce Inverse Difficulty Temperature Scaling (IDTS), a counterintuitive yet effective token-level temperature strategy. It employs low temperatures for difficult tokens for targeted error correction, and high temperatures for easy tokens to encourage students to learn from the teacher's complete and smooth output distribution, thereby enhancing generalization. As a plug-and-play framework, AdaKD can consistently improve the performance of various distillation methods on multiple model architectures and benchmarks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "15 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.11615v1",
    "published_date": "2025-10-13 16:55:07 UTC",
    "updated_date": "2025-10-13 16:55:07 UTC"
  },
  {
    "arxiv_id": "2510.11608v1",
    "title": "ParaCook: On Time-Efficient Planning for Multi-Agent Systems",
    "authors": [
      "Shiqi Zhang",
      "Xinbei Ma",
      "Yunqing Xu",
      "Zouying Cao",
      "Pengrui Lu",
      "Haobo Yuan",
      "Tiancheng Shen",
      "Zhuosheng Zhang",
      "Hai Zhao",
      "Ming-Hsuan Yang"
    ],
    "abstract": "Large Language Models (LLMs) exhibit strong reasoning abilities for planning long-horizon, real-world tasks, yet existing agent benchmarks focus on task completion while neglecting time efficiency in parallel and asynchronous operations. To address this, we present ParaCook, a benchmark for time-efficient collaborative planning. Inspired by the Overcooked game, ParaCook provides an environment for various challenging interaction planning of multi-agent systems that are instantiated as cooking tasks, with a simplified action space to isolate the core challenge of strategic parallel planning. Through a comprehensive evaluation of state-of-the-art LLMs, we find that current approaches achieve suboptimal plans, which struggle with parallel actions or coordination. Our analysis also reveals LLMs' potential on abstract tasks where they can focus on high-level parallel optimization. ParaCook provides a scalable evaluation framework with adjustable complexity, establishing a foundation for developing and assessing time efficiency-aware multi-agent planning. The code and data are available at https://github.com/zsq259/ParaCook.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11608v1",
    "published_date": "2025-10-13 16:47:07 UTC",
    "updated_date": "2025-10-13 16:47:07 UTC"
  },
  {
    "arxiv_id": "2510.11604v1",
    "title": "Explainability, risk modeling, and segmentation based customer churn analytics for personalized retention in e-commerce",
    "authors": [
      "Sanjula De Alwis",
      "Indrajith Ekanayake"
    ],
    "abstract": "In online retail, customer acquisition typically incurs higher costs than customer retention, motivating firms to invest in churn analytics. However, many contemporary churn models operate as opaque black boxes, limiting insight into the determinants of attrition, the timing of retention opportunities, and the identification of high-risk customer segments. Accordingly, the emphasis should shift from prediction alone to the design of personalized retention strategies grounded in interpretable evidence. This study advances a three-component framework that integrates explainable AI to quantify feature contributions, survival analysis to model time-to-event churn risk, and RFM profiling to segment customers by transactional behaviour. In combination, these methods enable the attribution of churn drivers, estimation of intervention windows, and prioritization of segments for targeted actions, thereby supporting strategies that reduce attrition and strengthen customer loyalty.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11604v1",
    "published_date": "2025-10-13 16:44:24 UTC",
    "updated_date": "2025-10-13 16:44:24 UTC"
  },
  {
    "arxiv_id": "2510.11599v2",
    "title": "SemCSE-Multi: Multifaceted and Decodable Embeddings for Aspect-Specific and Interpretable Scientific Domain Mapping",
    "authors": [
      "Marc Brinner",
      "Sina Zarrieß"
    ],
    "abstract": "We propose SemCSE-Multi, a novel unsupervised framework for generating multifaceted embeddings of scientific abstracts, evaluated in the domains of invasion biology and medicine. These embeddings capture distinct, individually specifiable aspects in isolation, thus enabling fine-grained and controllable similarity assessments as well as adaptive, user-driven visualizations of scientific domains. Our approach relies on an unsupervised procedure that produces aspect-specific summarizing sentences and trains embedding models to map semantically related summaries to nearby positions in the embedding space. We then distill these aspect-specific embedding capabilities into a unified embedding model that directly predicts multiple aspect embeddings from a scientific abstract in a single, efficient forward pass. In addition, we introduce an embedding decoding pipeline that decodes embeddings back into natural language descriptions of their associated aspects. Notably, we show that this decoding remains effective even for unoccupied regions in low-dimensional visualizations, thus offering vastly improved interpretability in user-centric settings.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11599v2",
    "published_date": "2025-10-13 16:38:20 UTC",
    "updated_date": "2026-01-10 14:50:59 UTC"
  },
  {
    "arxiv_id": "2510.11595v1",
    "title": "Reproducibility: The New Frontier in AI Governance",
    "authors": [
      "Israel Mason-Williams",
      "Gabryel Mason-Williams"
    ],
    "abstract": "AI policymakers are responsible for delivering effective governance mechanisms that can provide safe, aligned and trustworthy AI development. However, the information environment offered to policymakers is characterised by an unnecessarily low Signal-To-Noise Ratio, favouring regulatory capture and creating deep uncertainty and divides on which risks should be prioritised from a governance perspective. We posit that the current publication speeds in AI combined with the lack of strong scientific standards, via weak reproducibility protocols, effectively erodes the power of policymakers to enact meaningful policy and governance protocols. Our paper outlines how AI research could adopt stricter reproducibility guidelines to assist governance endeavours and improve consensus on the AI risk landscape. We evaluate the forthcoming reproducibility crisis within AI research through the lens of crises in other scientific domains; providing a commentary on how adopting preregistration, increased statistical power and negative result publication reproducibility protocols can enable effective AI governance. While we maintain that AI governance must be reactive due to AI's significant societal implications we argue that policymakers and governments must consider reproducibility protocols as a core tool in the governance arsenal and demand higher standards for AI research. Code to replicate data and figures: https://github.com/IFMW01/reproducibility-the-new-frontier-in-ai-governance",
    "categories": [
      "cs.AI",
      "cs.GL"
    ],
    "primary_category": "cs.AI",
    "comment": "12 pages,6 figures,Workshop on Technical AI Governance at ICML",
    "pdf_url": "https://arxiv.org/pdf/2510.11595v1",
    "published_date": "2025-10-13 16:34:25 UTC",
    "updated_date": "2025-10-13 16:34:25 UTC"
  },
  {
    "arxiv_id": "2510.13876v2",
    "title": "What Layers When: Learning to Skip Compute in LLMs with Residual Gates",
    "authors": [
      "Filipe Laitenberger",
      "Dawid Kopiczko",
      "Cees G. M. Snoek",
      "Yuki M. Asano"
    ],
    "abstract": "We introduce GateSkip, a simple residual-stream gating mechanism that enables token-wise layer skipping in decoder-only LMs. Each Attention/MLP branch is equipped with a sigmoid-linear gate that condenses the branch's output before it re-enters the residual stream. During inference we rank tokens by the gate values and skip low-importance ones using a per-layer budget. While early-exit or router-based Mixture-of-Depths models are known to be unstable and need extensive retraining, our smooth, differentiable gates fine-tune stably on top of pretrained models. On long-form reasoning, we save up to 15% compute while retaining over 90% of baseline accuracy. For increasingly larger models, this tradeoff improves drastically. On instruction-tuned models we see accuracy gains at full compute and match baseline quality near 50% savings. The learned gates give insight into transformer information flow (e.g., BOS tokens act as anchors), and the method combines easily with quantization, pruning, and self-speculative decoding.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint",
    "pdf_url": "https://arxiv.org/pdf/2510.13876v2",
    "published_date": "2025-10-13 16:31:50 UTC",
    "updated_date": "2025-10-17 07:30:17 UTC"
  },
  {
    "arxiv_id": "2510.11593v1",
    "title": "Hierarchical Qubit-Merging Transformer for Quantum Error Correction",
    "authors": [
      "Seong-Joon Park",
      "Hee-Youl Kwak",
      "Yongjune Kim"
    ],
    "abstract": "For reliable large-scale quantum computation, a quantum error correction (QEC) scheme must effectively resolve physical errors to protect logical information. Leveraging recent advances in deep learning, neural network-based decoders have emerged as a promising approach to enhance the reliability of QEC. We propose the Hierarchical Qubit-Merging Transformer (HQMT), a novel and general decoding framework that explicitly leverages the structural graph of stabilizer codes to learn error correlations across multiple scales. Our architecture first computes attention locally on structurally related groups of stabilizers and then systematically merges these qubit-centric representations to build a global view of the error syndrome. The proposed HQMT achieves substantially lower logical error rates for surface codes by integrating a dedicated qubit-merging layer within the transformer architecture. Across various code distances, HQMT significantly outperforms previous neural network-based QEC decoders as well as a powerful belief propagation with ordered statistics decoding (BP+OSD) baseline. This hierarchical approach provides a scalable and effective framework for surface code decoding, advancing the realization of reliable quantum computing.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "quant-ph",
    "comment": "6 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.11593v1",
    "published_date": "2025-10-13 16:31:46 UTC",
    "updated_date": "2025-10-13 16:31:46 UTC"
  },
  {
    "arxiv_id": "2510.11588v1",
    "title": "Analyzing and Internalizing Complex Policy Documents for LLM Agents",
    "authors": [
      "Jiateng Liu",
      "Zhenhailong Wang",
      "Xiaojiang Huang",
      "Yingjie Li",
      "Xing Fan",
      "Xiang Li",
      "Chenlei Guo",
      "Ruhi Sarikaya",
      "Heng Ji"
    ],
    "abstract": "Large Language Model (LLM)-based agentic systems rely on in-context policy documents encoding diverse business rules. As requirements grow, these documents expand rapidly, causing high computational overhead. This motivates developing internalization methods that embed policy documents into model priors while preserving performance. Prior prompt compression work targets generic prompts, but agentic policy documents span multiple complexity levels and require deeper reasoning, making internalization harder. We introduce CC-Gen, an agentic benchmark generator with Controllable Complexity across four levels, enabling systematic evaluation of agents' ability to handle complexity and offering a unified framework for assessing policy internalization. Our analysis shows that complex policy specifications governing workflows pose major reasoning challenges. Supporting internalization with gold user agent interaction trajectories containing chain-of-thought (CoT) annotations via supervised fine-tuning (SFT) is data-intensive and degrades sharply as policy complexity increases. To mitigate data and reasoning burdens, we propose Category-Aware Policy Continued Pretraining (CAP-CPT). Our automated pipeline parses policy documents to extract key specifications, grouping them into factual, behavioral, and conditional categories, and isolating complex conditions that drive workflow complexity. This guides targeted data synthesis and enables agents to internalize policy information through an autoregressive pretraining loss. Experiments show CAP-CPT improves SFT baselines in all settings, with up to 41% and 22% gains on Qwen-3-32B, achieving 97.3% prompt length reduction on CC-Gen and further enhancing tau-Bench with minimal SFT data.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "42 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.11588v1",
    "published_date": "2025-10-13 16:30:07 UTC",
    "updated_date": "2025-10-13 16:30:07 UTC"
  },
  {
    "arxiv_id": "2510.11560v1",
    "title": "Characterizing Web Search in The Age of Generative AI",
    "authors": [
      "Elisabeth Kirsten",
      "Jost Grosse Perdekamp",
      "Mihir Upadhyay",
      "Krishna P. Gummadi",
      "Muhammad Bilal Zafar"
    ],
    "abstract": "The advent of LLMs has given rise to a new type of web search: Generative search, where LLMs retrieve web pages related to a query and generate a single, coherent text as a response. This output modality stands in stark contrast to traditional web search, where results are returned as a ranked list of independent web pages. In this paper, we ask: Along what dimensions do generative search outputs differ from traditional web search? We compare Google, a traditional web search engine, with four generative search engines from two providers (Google and OpenAI) across queries from four domains. Our analysis reveals intriguing differences. Most generative search engines cover a wider range of sources compared to web search. Generative search engines vary in the degree to which they rely on internal knowledge contained within the model parameters v.s. external knowledge retrieved from the web. Generative search engines surface varying sets of concepts, creating new opportunities for enhancing search diversity and serendipity. Our results also highlight the need for revisiting evaluation criteria for web search in the age of Generative AI.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11560v1",
    "published_date": "2025-10-13 16:04:03 UTC",
    "updated_date": "2025-10-13 16:04:03 UTC"
  },
  {
    "arxiv_id": "2510.12837v2",
    "title": "Semantic knowledge guides innovation and drives cultural evolution",
    "authors": [
      "Anil Yaman",
      "Shen Tian",
      "Björn Lindström"
    ],
    "abstract": "Cultural evolution allows ideas and technology to build over generations, a process reaching its most complex and open-ended form in humans. While social learning enables the transmission of such innovations, the cognitive processes that generate innovations remain unclear. We propose that semantic knowledge-the associations linking concepts to their properties and functions-guides human innovation and drives cumulative culture. To test this, we combined an agent-based model, which examines how semantic knowledge shapes cultural evolutionary dynamics, with a large-scale behavioural experiment (N = 1,243) testing its role in human innovation. Semantic knowledge directed exploration toward meaningful solutions and interacted synergistically with social learning to amplify innovation and cultural evolution. Participants lacking access to semantic knowledge performed no better than chance, even when social information was available, and relied on shallow exploration strategies for innovation. Together, these findings indicate that semantic knowledge is a key cognitive process enabling human cumulative culture.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CY",
      "cs.NE"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12837v2",
    "published_date": "2025-10-13 16:03:51 UTC",
    "updated_date": "2025-10-24 12:35:55 UTC"
  },
  {
    "arxiv_id": "2510.11558v1",
    "title": "Zero Data Retention in LLM-based Enterprise AI Assistants: A Comparative Study of Market Leading Agentic AI Products",
    "authors": [
      "Komal Gupta",
      "Aditya Shrivastava"
    ],
    "abstract": "Governance of data, compliance, and business privacy matters, particularly for healthcare and finance businesses. Since the recent emergence of AI enterprise AI assistants enhancing business productivity, safeguarding private data and compliance is now a priority. With the implementation of AI assistants across the enterprise, the zero data retention can be achieved by implementing zero data retention policies by Large Language Model businesses like Open AI and Anthropic and Meta. In this work, we explore zero data retention policies for the Enterprise apps of large language models (LLMs). Our key contribution is defining the architectural, compliance, and usability trade-offs of such systems in parallel. In this research work, we examine the development of commercial AI assistants with two industry leaders and market titans in this arena - Salesforce and Microsoft. Both of these companies used distinct technical architecture to support zero data retention policies. Salesforce AgentForce and Microsoft Copilot are among the leading AI assistants providing much-needed push to business productivity in customer care. The purpose of this paper is to analyze the technical architecture and deployment of zero data retention policy by consuming applications as well as big language models service providers like Open Ai, Anthropic, and Meta.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11558v1",
    "published_date": "2025-10-13 16:00:34 UTC",
    "updated_date": "2025-10-13 16:00:34 UTC"
  },
  {
    "arxiv_id": "2510.11541v1",
    "title": "Query-Specific GNN: A Comprehensive Graph Representation Learning Method for Retrieval Augmented Generation",
    "authors": [
      "Yuchen Yan",
      "Zhihua Liu",
      "Hao Wang",
      "Weiming Li",
      "Xiaoshuai Hao"
    ],
    "abstract": "Retrieval-augmented generation (RAG) has demonstrated its ability to enhance Large Language Models (LLMs) by integrating external knowledge sources. However, multi-hop questions, which require the identification of multiple knowledge targets to form a synthesized answer, raise new challenges for RAG systems. Under the multi-hop settings, existing methods often struggle to fully understand the questions with complex semantic structures and are susceptible to irrelevant noise during the retrieval of multiple information targets. To address these limitations, we propose a novel graph representation learning framework for multi-hop question retrieval. We first introduce a Multi-information Level Knowledge Graph (Multi-L KG) to model various information levels for a more comprehensive understanding of multi-hop questions. Based on this, we design a Query-Specific Graph Neural Network (QSGNN) for representation learning on the Multi-L KG. QSGNN employs intra/inter-level message passing mechanisms, and in each message passing the information aggregation is guided by the query, which not only facilitates multi-granular information aggregation but also significantly reduces the impact of noise. To enhance its ability to learn robust representations, we further propose two synthesized data generation strategies for pre-training the QSGNN. Extensive experimental results demonstrate the effectiveness of our framework in multi-hop scenarios, especially in high-hop questions the improvement can reach 33.8\\%. The code is available at: https://github.com/Jerry2398/QSGNN.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11541v1",
    "published_date": "2025-10-13 15:41:15 UTC",
    "updated_date": "2025-10-13 15:41:15 UTC"
  },
  {
    "arxiv_id": "2510.11536v1",
    "title": "CodeWatcher: IDE Telemetry Data Extraction Tool for Understanding Coding Interactions with LLMs",
    "authors": [
      "Manaal Basha",
      "Aimeê M. Ribeiro",
      "Jeena Javahar",
      "Cleidson R. B. de Souza",
      "Gema Rodríguez-Pérez"
    ],
    "abstract": "Understanding how developers interact with code generation tools (CGTs) requires detailed, real-time data on programming behavior which is often difficult to collect without disrupting workflow. We present \\textit{CodeWatcher}, a lightweight, unobtrusive client-server system designed to capture fine-grained interaction events from within the Visual Studio Code (VS Code) editor. \\textit{CodeWatcher} logs semantically meaningful events such as insertions made by CGTs, deletions, copy-paste actions, and focus shifts, enabling continuous monitoring of developer activity without modifying user workflows. The system comprises a VS Code plugin, a Python-based RESTful API, and a MongoDB backend, all containerized for scalability and ease of deployment. By structuring and timestamping each event, \\textit{CodeWatcher} enables post-hoc reconstruction of coding sessions and facilitates rich behavioral analyses, including how and when CGTs are used during development. This infrastructure is crucial for supporting research on responsible AI, developer productivity, and the human-centered evaluation of CGTs. Please find the demo, diagrams, and tool here: https://osf.io/j2kru/overview.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "ICSME 2025 Tool Demonstration Track",
    "pdf_url": "https://arxiv.org/pdf/2510.11536v1",
    "published_date": "2025-10-13 15:39:08 UTC",
    "updated_date": "2025-10-13 15:39:08 UTC"
  },
  {
    "arxiv_id": "2510.11535v1",
    "title": "A Flexible Multi-Agent Deep Reinforcement Learning Framework for Dynamic Routing and Scheduling of Latency-Critical Services",
    "authors": [
      "Vincenzo Norman Vitale",
      "Antonia Maria Tulino",
      "Andreas F. Molisch",
      "Jaime Llorca"
    ],
    "abstract": "Timely delivery of delay-sensitive information over dynamic, heterogeneous networks is increasingly essential for a range of interactive applications, such as industrial automation, self-driving vehicles, and augmented reality. However, most existing network control solutions target only average delay performance, falling short of providing strict End-to-End (E2E) peak latency guarantees. This paper addresses the challenge of reliably delivering packets within application-imposed deadlines by leveraging recent advancements in Multi-Agent Deep Reinforcement Learning (MA-DRL). After introducing the Delay-Constrained Maximum-Throughput (DCMT) dynamic network control problem, and highlighting the limitations of current solutions, we present a novel MA-DRL network control framework that leverages a centralized routing and distributed scheduling architecture. The proposed framework leverages critical networking domain knowledge for the design of effective MA-DRL strategies based on the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) technique, where centralized routing and distributed scheduling agents dynamically assign paths and schedule packet transmissions according to packet lifetimes, thereby maximizing on-time packet delivery. The generality of the proposed framework allows integrating both data-driven \\blue{Deep Reinforcement Learning (DRL)} agents and traditional rule-based policies in order to strike the right balance between performance and learning complexity. Our results confirm the superiority of the proposed framework with respect to traditional stochastic optimization-based approaches and provide key insights into the role and interplay between data-driven DRL agents and new rule-based policies for both efficient and high-performance control of latency-critical services.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11535v1",
    "published_date": "2025-10-13 15:38:10 UTC",
    "updated_date": "2025-10-13 15:38:10 UTC"
  },
  {
    "arxiv_id": "2510.11516v1",
    "title": "Cracking CodeWhisperer: Analyzing Developers' Interactions and Patterns During Programming Tasks",
    "authors": [
      "Jeena Javahar",
      "Tanya Budhrani",
      "Manaal Basha",
      "Cleidson R. B. de Souza",
      "Ivan Beschastnikh",
      "Gema Rodriguez-Perez"
    ],
    "abstract": "The use of AI code-generation tools is becoming increasingly common, making it important to understand how software developers are adopting these tools. In this study, we investigate how developers engage with Amazon's CodeWhisperer, an LLM-based code-generation tool. We conducted two user studies with two groups of 10 participants each, interacting with CodeWhisperer - the first to understand which interactions were critical to capture and the second to collect low-level interaction data using a custom telemetry plugin. Our mixed-methods analysis identified four behavioral patterns: 1) incremental code refinement, 2) explicit instruction using natural language comments, 3) baseline structuring with model suggestions, and 4) integrative use with external sources. We provide a comprehensive analysis of these patterns .",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "VL/HCC 2025 Short Paper",
    "pdf_url": "https://arxiv.org/pdf/2510.11516v1",
    "published_date": "2025-10-13 15:22:12 UTC",
    "updated_date": "2025-10-13 15:22:12 UTC"
  },
  {
    "arxiv_id": "2510.11512v2",
    "title": "LikePhys: Evaluating Intuitive Physics Understanding in Video Diffusion Models via Likelihood Preference",
    "authors": [
      "Jianhao Yuan",
      "Fabio Pizzati",
      "Francesco Pinto",
      "Lars Kunze",
      "Ivan Laptev",
      "Paul Newman",
      "Philip Torr",
      "Daniele De Martini"
    ],
    "abstract": "Intuitive physics understanding in video diffusion models plays an essential role in building general-purpose physically plausible world simulators, yet accurately evaluating such capacity remains a challenging task due to the difficulty in disentangling physics correctness from visual appearance in generation. To the end, we introduce LikePhys, a training-free method that evaluates intuitive physics in video diffusion models by distinguishing physically valid and impossible videos using the denoising objective as an ELBO-based likelihood surrogate on a curated dataset of valid-invalid pairs. By testing on our constructed benchmark of twelve scenarios spanning over four physics domains, we show that our evaluation metric, Plausibility Preference Error (PPE), demonstrates strong alignment with human preference, outperforming state-of-the-art evaluator baselines. We then systematically benchmark intuitive physics understanding in current video diffusion models. Our study further analyses how model design and inference settings affect intuitive physics understanding and highlights domain-specific capacity variations across physical laws. Empirical results show that, despite current models struggling with complex and chaotic dynamics, there is a clear trend of improvement in physics understanding as model capacity and inference settings scale.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "22 pages, 9 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.11512v2",
    "published_date": "2025-10-13 15:19:07 UTC",
    "updated_date": "2025-11-25 14:24:21 UTC"
  },
  {
    "arxiv_id": "2510.11507v2",
    "title": "Automatic Music Sample Identification with Multi-Track Contrastive Learning",
    "authors": [
      "Alain Riou",
      "Joan Serrà",
      "Yuki Mitsufuji"
    ],
    "abstract": "Sampling, the technique of reusing pieces of existing audio tracks to create new music content, is a very common practice in modern music production. In this paper, we tackle the challenging task of automatic sample identification, that is, detecting such sampled content and retrieving the material from which it originates. To do so, we adopt a self-supervised learning approach that leverages a multi-track dataset to create positive pairs of artificial mixes, and design a novel contrastive learning objective. We show that such method significantly outperforms previous state-of-the-art baselines, that is robust to various genres, and that scales well when increasing the number of noise songs in the reference database. In addition, we extensively analyze the contribution of the different components of our training pipeline and highlight, in particular, the need for high-quality separated stems for this task.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11507v2",
    "published_date": "2025-10-13 15:17:08 UTC",
    "updated_date": "2025-10-27 10:57:33 UTC"
  },
  {
    "arxiv_id": "2510.11503v1",
    "title": "People use fast, flat goal-directed simulation to reason about novel problems",
    "authors": [
      "Katherine M. Collins",
      "Cedegao E. Zhang",
      "Lionel Wong",
      "Mauricio Barba da Costa",
      "Graham Todd",
      "Adrian Weller",
      "Samuel J. Cheyette",
      "Thomas L. Griffiths",
      "Joshua B. Tenenbaum"
    ],
    "abstract": "Games have long been a microcosm for studying planning and reasoning in both natural and artificial intelligence, especially with a focus on expert-level or even super-human play. But real life also pushes human intelligence along a different frontier, requiring people to flexibly navigate decision-making problems that they have never thought about before. Here, we use novice gameplay to study how people make decisions and form judgments in new problem settings. We show that people are systematic and adaptively rational in how they play a game for the first time, or evaluate a game (e.g., how fair or how fun it is likely to be) before they have played it even once. We explain these capacities via a computational cognitive model that we call the \"Intuitive Gamer\". The model is based on mechanisms of fast and flat (depth-limited) goal-directed probabilistic simulation--analogous to those used in Monte Carlo tree-search models of expert game-play, but scaled down to use very few stochastic samples, simple goal heuristics for evaluating actions, and no deep search. In a series of large-scale behavioral studies with over 1000 participants and 121 two-player strategic board games (almost all novel to our participants), our model quantitatively captures human judgments and decisions varying the amount and kind of experience people have with a game--from no experience at all (\"just thinking\"), to a single round of play, to indirect experience watching another person and predicting how they should play--and does so significantly better than much more compute-intensive expert-level models. More broadly, our work offers new insights into how people rapidly evaluate, act, and make suggestions when encountering novel problems, and could inform the design of more flexible and human-like AI systems that can determine not just how to solve new tasks, but whether a task is worth thinking about at all.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.GT"
    ],
    "primary_category": "q-bio.NC",
    "comment": "Pre-print",
    "pdf_url": "https://arxiv.org/pdf/2510.11503v1",
    "published_date": "2025-10-13 15:12:08 UTC",
    "updated_date": "2025-10-13 15:12:08 UTC"
  },
  {
    "arxiv_id": "2510.11499v1",
    "title": "Offline Reinforcement Learning with Generative Trajectory Policies",
    "authors": [
      "Xinsong Feng",
      "Leshu Tang",
      "Chenan Wang",
      "Haipeng Chen"
    ],
    "abstract": "Generative models have emerged as a powerful class of policies for offline reinforcement learning (RL) due to their ability to capture complex, multi-modal behaviors. However, existing methods face a stark trade-off: slow, iterative models like diffusion policies are computationally expensive, while fast, single-step models like consistency policies often suffer from degraded performance. In this paper, we demonstrate that it is possible to bridge this gap. The key to moving beyond the limitations of individual methods, we argue, lies in a unifying perspective that views modern generative models, including diffusion, flow matching, and consistency models, as specific instances of learning a continuous-time generative trajectory governed by an Ordinary Differential Equation (ODE). This principled foundation provides a clearer design space for generative policies in RL and allows us to propose Generative Trajectory Policies (GTPs), a new and more general policy paradigm that learns the entire solution map of the underlying ODE. To make this paradigm practical for offline RL, we further introduce two key theoretically principled adaptations. Empirical results demonstrate that GTP achieves state-of-the-art performance on D4RL benchmarks - it significantly outperforms prior generative policies, achieving perfect scores on several notoriously hard AntMaze tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint. Under review at ICLR 2026",
    "pdf_url": "https://arxiv.org/pdf/2510.11499v1",
    "published_date": "2025-10-13 15:06:28 UTC",
    "updated_date": "2025-10-13 15:06:28 UTC"
  },
  {
    "arxiv_id": "2510.11496v3",
    "title": "AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model",
    "authors": [
      "Zhiwei Jin",
      "Xiaohui Song",
      "Nan Wang",
      "Yafei Liu",
      "Chao Li",
      "Xin Li",
      "Ruichen Wang",
      "Zhihao Li",
      "Qi Qi",
      "Long Cheng",
      "Dongze Hao",
      "Quanlong Zheng",
      "Yanhao Zhang",
      "Haobo Ji",
      "Jian Ma",
      "Zhitong Zheng",
      "Zhenyi Lin",
      "Haolin Deng",
      "Xin Zou",
      "Xiaojie Yin",
      "Ruilin Wang",
      "Liankai Cai",
      "Haijing Liu",
      "Yuqing Qiu",
      "Ke Chen",
      "Zixian Li",
      "Chi Xie",
      "Huafei Li",
      "Chenxing Li",
      "Chuangchuang Wang",
      "Kai Tang",
      "Zhiguang Zhu",
      "Kai Tang",
      "Wenmei Gao",
      "Rui Wang",
      "Jun Wu",
      "Chao Liu",
      "Qin Xie",
      "Chen Chen",
      "Haonan Lu"
    ],
    "abstract": "In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o, Gemini, and Claude Sonnet have demonstrated outstanding performance with enormous model sizes reaching hundreds of billions of parameters, they significantly surpass the limitations in memory, power consumption, and computing capacity of edge devices such as mobile phones. This paper introduces AndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on Qwen3's LLM and various visual encoders. We comprehensively outline the model architectures, training pipeline, and training data of AndesVL, which achieves first-tier performance across a wide range of open-source benchmarks, including fields such as text-rich image understanding, reasoning and math, multi-image comprehension, general VQA, hallucination mitigation, multilingual understanding, and GUI-related tasks when compared with state-of-the-art models of a similar scale. Furthermore, we introduce a 1+N LoRA architecture alongside a Quantization-Aware LoRA Fine-Tuning (QALFT) framework to facilitate efficient task adaptation and model compression during mobile-side deployment of AndesVL. Moreover, utilizing our cache eviction algorithm -- OKV -- along with customized speculative decoding and compression strategies, we achieve a 6.7x peak decoding speedup ratio, up to 30.9% memory reduction, and 1.8 bits-per-weight when deploying AndesVL-4B on MediaTek Dimensity 9500 chips. We release all models on https://huggingface.co/OPPOer.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Tech report of OPPO AndesVL Team",
    "pdf_url": "https://arxiv.org/pdf/2510.11496v3",
    "published_date": "2025-10-13 15:04:38 UTC",
    "updated_date": "2025-12-22 04:40:04 UTC"
  },
  {
    "arxiv_id": "2510.11482v1",
    "title": "Investigating Large Language Models' Linguistic Abilities for Text Preprocessing",
    "authors": [
      "Marco Braga",
      "Gian Carlo Milanese",
      "Gabriella Pasi"
    ],
    "abstract": "Text preprocessing is a fundamental component of Natural Language Processing, involving techniques such as stopword removal, stemming, and lemmatization to prepare text as input for further processing and analysis. Despite the context-dependent nature of the above techniques, traditional methods usually ignore contextual information. In this paper, we investigate the idea of using Large Language Models (LLMs) to perform various preprocessing tasks, due to their ability to take context into account without requiring extensive language-specific annotated resources. Through a comprehensive evaluation on web-sourced data, we compare LLM-based preprocessing (specifically stopword removal, lemmatization and stemming) to traditional algorithms across multiple text classification tasks in six European languages. Our analysis indicates that LLMs are capable of replicating traditional stopword removal, lemmatization, and stemming methods with accuracies reaching 97%, 82%, and 74%, respectively. Additionally, we show that ML algorithms trained on texts preprocessed by LLMs achieve an improvement of up to 6% with respect to the $F_1$ measure compared to traditional techniques. Our code, prompts, and results are publicly available at https://github.com/GianCarloMilanese/llm_pipeline_wi-iat.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted in WI-IAT 2025. Pre-camera-ready version",
    "pdf_url": "https://arxiv.org/pdf/2510.11482v1",
    "published_date": "2025-10-13 14:53:44 UTC",
    "updated_date": "2025-10-13 14:53:44 UTC"
  },
  {
    "arxiv_id": "2510.11474v2",
    "title": "Coordinated Strategies in Realistic Air Combat by Hierarchical Multi-Agent Reinforcement Learning",
    "authors": [
      "Ardian Selmonaj",
      "Giacomo Del Rio",
      "Adrian Schneider",
      "Alessandro Antonucci"
    ],
    "abstract": "Achieving mission objectives in a realistic simulation of aerial combat is highly challenging due to imperfect situational awareness and nonlinear flight dynamics. In this work, we introduce a novel 3D multi-agent air combat environment and a Hierarchical Multi-Agent Reinforcement Learning framework to tackle these challenges. Our approach combines heterogeneous agent dynamics, curriculum learning, league-play, and a newly adapted training algorithm. To this end, the decision-making process is organized into two abstraction levels: low-level policies learn precise control maneuvers, while high-level policies issue tactical commands based on mission objectives. Empirical results show that our hierarchical approach improves both learning efficiency and combat performance in complex dogfight scenarios.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.RO",
    "comment": "2025 IEEE International Conference on Agentic AI (ICA)",
    "pdf_url": "https://arxiv.org/pdf/2510.11474v2",
    "published_date": "2025-10-13 14:44:51 UTC",
    "updated_date": "2025-10-22 08:38:26 UTC"
  },
  {
    "arxiv_id": "2510.11471v1",
    "title": "Iterative Amortized Inference: Unifying In-Context Learning and Learned Optimizers",
    "authors": [
      "Sarthak Mittal",
      "Divyat Mahajan",
      "Guillaume Lajoie",
      "Mohammad Pezeshki"
    ],
    "abstract": "Modern learning systems increasingly rely on amortized learning - the idea of reusing computation or inductive biases shared across tasks to enable rapid generalization to novel problems. This principle spans a range of approaches, including meta-learning, in-context learning, prompt tuning, learned optimizers and more. While motivated by similar goals, these approaches differ in how they encode and leverage task-specific information, often provided as in-context examples. In this work, we propose a unified framework which describes how such methods differ primarily in the aspects of learning they amortize - such as initializations, learned updates, or predictive mappings - and how they incorporate task data at inference. We introduce a taxonomy that categorizes amortized models into parametric, implicit, and explicit regimes, based on whether task adaptation is externalized, internalized, or jointly modeled. Building on this view, we identify a key limitation in current approaches: most methods struggle to scale to large datasets because their capacity to process task data at inference (e.g., context length) is often limited. To address this, we propose iterative amortized inference, a class of models that refine solutions step-by-step over mini-batches, drawing inspiration from stochastic optimization. Our formulation bridges optimization-based meta-learning with forward-pass amortization in models like LLMs, offering a scalable and extensible foundation for general-purpose task adaptation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11471v1",
    "published_date": "2025-10-13 14:40:47 UTC",
    "updated_date": "2025-10-13 14:40:47 UTC"
  },
  {
    "arxiv_id": "2510.11462v1",
    "title": "Unifying Deductive and Abductive Reasoning in Knowledge Graphs with Masked Diffusion Model",
    "authors": [
      "Yisen Gao",
      "Jiaxin Bai",
      "Yi Huang",
      "Xingcheng Fu",
      "Qingyun Sun",
      "Yangqiu Song"
    ],
    "abstract": "Deductive and abductive reasoning are two critical paradigms for analyzing knowledge graphs, enabling applications from financial query answering to scientific discovery. Deductive reasoning on knowledge graphs usually involves retrieving entities that satisfy a complex logical query, while abductive reasoning generates plausible logical hypotheses from observations. Despite their clear synergistic potential, where deduction can validate hypotheses and abduction can uncover deeper logical patterns, existing methods address them in isolation. To bridge this gap, we propose DARK, a unified framework for Deductive and Abductive Reasoning in Knowledge graphs. As a masked diffusion model capable of capturing the bidirectional relationship between queries and conclusions, DARK has two key innovations. First, to better leverage deduction for hypothesis refinement during abductive reasoning, we introduce a self-reflective denoising process that iteratively generates and validates candidate hypotheses against the observed conclusion. Second, to discover richer logical associations, we propose a logic-exploration reinforcement learning approach that simultaneously masks queries and conclusions, enabling the model to explore novel reasoning compositions. Extensive experiments on multiple benchmark knowledge graphs show that DARK achieves state-of-the-art performance on both deductive and abductive reasoning tasks, demonstrating the significant benefits of our unified approach.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Under Review",
    "pdf_url": "https://arxiv.org/pdf/2510.11462v1",
    "published_date": "2025-10-13 14:34:57 UTC",
    "updated_date": "2025-10-13 14:34:57 UTC"
  },
  {
    "arxiv_id": "2510.11457v1",
    "title": "From <Answer> to <Think>: Multidimensional Supervision of Reasoning Process for LLM Optimization",
    "authors": [
      "Beining Wang",
      "Weihang Su",
      "Hongtao Tian",
      "Tao Yang",
      "Yujia Zhou",
      "Ting Yao",
      "Qingyao Ai",
      "Yiqun Liu"
    ],
    "abstract": "Improving the multi-step reasoning ability of Large Language Models (LLMs) is a critical yet challenging task. The dominant paradigm, outcome-supervised reinforcement learning (RLVR), rewards only correct final answers, often propagating flawed reasoning and suffering from sparse reward signals. While process-level reward models (PRMs) provide denser, step-by-step feedback, they lack generalizability and interpretability, requiring task-specific segmentation of the reasoning process. To this end, we propose the Dimension-level Reward Model (DRM), a new supervision framework that bridges the gap between these two approaches. DRM evaluates the quality of a reasoning process along three fundamental, complementary, and interpretable dimensions: Confidence for uncertainty calibration, Relevance for semantic alignment, and Coherence for logical consistency. Together, these dimensions capture aspects beyond final answer correctness and enable interpretable assessment without requiring ground truth answers. Experimental results show that DRM provides effective supervision signals, guides the optimization of LLMs and enhances their reasoning ability. In particular, DRM-supervised training achieves consistent gains on both in-distribution and out-of-distribution open-domain tasks, including mathematics, question answering, code execution, and puzzles. Our findings demonstrate that multidimensional supervision of the reasoning process can improve the generalized reasoning ability of LLMs beyond the training distribution.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11457v1",
    "published_date": "2025-10-13 14:29:15 UTC",
    "updated_date": "2025-10-13 14:29:15 UTC"
  },
  {
    "arxiv_id": "2510.11454v1",
    "title": "Audio-Maestro: Enhancing Large Audio-Language Models with Tool-Augmented Reasoning",
    "authors": [
      "Kuan-Yi Lee",
      "Tsung-En Lin",
      "Hung-Yi Lee"
    ],
    "abstract": "Recent advancements in large multimodal models (LMMs) have shown strong capabilities in audio understanding. However, most systems rely solely on end-to-end reasoning, limiting interpretability and accuracy for tasks that require structured knowledge or specialized signal analysis. In this work, we present Audio-Maestro -- a tool-augmented audio reasoning framework that enables audio-language models to autonomously call external tools and integrate their timestamped outputs into the reasoning process. This design allows the model to analyze, transform, and interpret audio signals through specialized tools rather than relying solely on end-to-end inference. Experiments show that Audio-Maestro consistently improves general audio reasoning performance: Gemini-2.5-flash's average accuracy on MMAU-Test rises from 67.4% to 72.1%, DeSTA-2.5 from 58.3% to 62.8%, and GPT-4o from 60.8% to 63.9%. To our knowledge, Audio-Maestro is the first framework to integrate structured tool output into the large audio language model reasoning process.",
    "categories": [
      "cs.SD",
      "cs.AI"
    ],
    "primary_category": "cs.SD",
    "comment": "9pages",
    "pdf_url": "https://arxiv.org/pdf/2510.11454v1",
    "published_date": "2025-10-13 14:25:34 UTC",
    "updated_date": "2025-10-13 14:25:34 UTC"
  },
  {
    "arxiv_id": "2510.11442v1",
    "title": "Reconstructing 12-Lead ECG from 3-Lead ECG using Variational Autoencoder to Improve Cardiac Disease Detection of Wearable ECG Devices",
    "authors": [
      "Xinyan Guan",
      "Yongfan Lai",
      "Jiarui Jin",
      "Jun Li",
      "Haoyu Wang",
      "Qinghao Zhao",
      "Deyun Zhang",
      "Shijia Geng",
      "Shenda Hong"
    ],
    "abstract": "Twelve-lead electrocardiograms (ECGs) are the clinical gold standard for cardiac diagnosis, providing comprehensive spatial coverage of the heart necessary to detect conditions such as myocardial infarction (MI). However, their lack of portability limits continuous and large-scale use. Three-lead ECG systems are widely used in wearable devices due to their simplicity and mobility, but they often fail to capture pathologies in unmeasured regions. To address this, we propose WearECG, a Variational Autoencoder (VAE) method that reconstructs twelve-lead ECGs from three leads: II, V1, and V5. Our model includes architectural improvements to better capture temporal and spatial dependencies in ECG signals. We evaluate generation quality using MSE, MAE, and Frechet Inception Distance (FID), and assess clinical validity via a Turing test with expert cardiologists. To further validate diagnostic utility, we fine-tune ECGFounder, a large-scale pretrained ECG model, on a multi-label classification task involving over 40 cardiac conditions, including six different myocardial infarction locations, using both real and generated signals. Experiments on the MIMIC dataset show that our method produces physiologically realistic and diagnostically informative signals, with robust performance in downstream tasks. This work demonstrates the potential of generative modeling for ECG reconstruction and its implications for scalable, low-cost cardiac screening.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "24 pages, 5 figures, submitted to Nature Communications",
    "pdf_url": "https://arxiv.org/pdf/2510.11442v1",
    "published_date": "2025-10-13 14:14:37 UTC",
    "updated_date": "2025-10-13 14:14:37 UTC"
  },
  {
    "arxiv_id": "2510.13873v1",
    "title": "FRACCO: A gold-standard annotated corpus of oncological entities with ICD-O-3.1 normalisation",
    "authors": [
      "Johann Pignat",
      "Milena Vucetic",
      "Christophe Gaudet-Blavignac",
      "Jamil Zaghir",
      "Amandine Stettler",
      "Fanny Amrein",
      "Jonatan Bonjour",
      "Jean-Philippe Goldman",
      "Olivier Michielin",
      "Christian Lovis",
      "Mina Bjelogrlic"
    ],
    "abstract": "Developing natural language processing tools for clinical text requires annotated datasets, yet French oncology resources remain scarce. We present FRACCO (FRench Annotated Corpus for Clinical Oncology) an expert-annotated corpus of 1301 synthetic French clinical cases, initially translated from the Spanish CANTEMIST corpus as part of the FRASIMED initiative. Each document is annotated with terms related to morphology, topography, and histologic differentiation, using the International Classification of Diseases for Oncology (ICD-O) as reference. An additional annotation layer captures composite expression-level normalisations that combine multiple ICD-O elements into unified clinical concepts. Annotation quality was ensured through expert review: 1301 texts were manually annotated for entity spans by two domain experts. A total of 71127 ICD-O normalisations were produced through a combination of automated matching and manual validation by a team of five annotators. The final dataset representing 399 unique morphology codes (from 2549 different expressions), 272 topography codes (from 3143 different expressions), and 2043 unique composite expressions (from 11144 different expressions). This dataset provides a reference standard for named entity recognition and concept normalisation in French oncology texts.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.13873v1",
    "published_date": "2025-10-13 14:00:11 UTC",
    "updated_date": "2025-10-13 14:00:11 UTC"
  },
  {
    "arxiv_id": "2510.11407v1",
    "title": "KnowRL: Teaching Language Models to Know What They Know",
    "authors": [
      "Sahil Kale",
      "Devendra Singh Dhami"
    ],
    "abstract": "Truly reliable AI requires more than simply scaling up knowledge; it demands the ability to know what it knows and when it does not. Yet recent research shows that even the best LLMs misjudge their own competence in more than one in five cases, making any response born of such internal uncertainty impossible to fully trust. Inspired by self-improvement reinforcement learning techniques that require minimal data, we present a simple but powerful framework KnowRL that strengthens a model's internal understanding of its own feasibility boundaries, enabling safer and more responsible behaviour. Our framework combines two components: (i) introspection, where the model generates and classifies tasks it judges feasible or infeasible, and (ii) consensus-based rewarding, where stability of self-knowledge assessment is reinforced through internal agreement. By using internally generated data, this design strengthens consistency in self-knowledge and entirely avoids costly external supervision. In experiments on LLaMA-3.1-8B and Qwen-2.5-7B, KnowRL steadily improved self-knowledge, validated by both intrinsic self-consistency and extrinsic benchmarking. With nothing more than a small seed set and no external supervision, our method drove gains as high as 28% in accuracy and 12% in F1, outperforming baselines in just a few iterations. Our framework essentially unlocks the untapped capacity of LLMs to self-improve their knowledge awareness, opening the door to reliable, more accountable AI and safer deployment in critical applications. Owing to its simplicity and independence from external effort, we encourage applying this reliability-enhancing process to all future models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "14 pages, 7 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.11407v1",
    "published_date": "2025-10-13 13:47:14 UTC",
    "updated_date": "2025-10-13 13:47:14 UTC"
  },
  {
    "arxiv_id": "2510.11398v1",
    "title": "Living Off the LLM: How LLMs Will Change Adversary Tactics",
    "authors": [
      "Sean Oesch",
      "Jack Hutchins",
      "Luke Koch",
      "Kevin Kurian"
    ],
    "abstract": "In living off the land attacks, malicious actors use legitimate tools and processes already present on a system to avoid detection. In this paper, we explore how the on-device LLMs of the future will become a security concern as threat actors integrate LLMs into their living off the land attack pipeline and ways the security community may mitigate this threat.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "6 pages, 0 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.11398v1",
    "published_date": "2025-10-13 13:41:27 UTC",
    "updated_date": "2025-10-13 13:41:27 UTC"
  },
  {
    "arxiv_id": "2510.15985v2",
    "title": "MEET-Sepsis: Multi-Endogenous-View Enhanced Time-Series Representation Learning for Early Sepsis Prediction",
    "authors": [
      "Zexi Tan",
      "Tao Xie",
      "Binbin Sun",
      "Xiang Zhang",
      "Yiqun Zhang",
      "Yiu-Ming Cheung"
    ],
    "abstract": "Sepsis is a life-threatening infectious syndrome associated with high mortality in intensive care units (ICUs). Early and accurate sepsis prediction (SP) is critical for timely intervention, yet remains challenging due to subtle early manifestations and rapidly escalating mortality. While AI has improved SP efficiency, existing methods struggle to capture weak early temporal signals. This paper introduces a Multi-Endogenous-view Representation Enhancement (MERE) mechanism to construct enriched feature views, coupled with a Cascaded Dual-convolution Time-series Attention (CDTA) module for multi-scale temporal representation learning. The proposed MEET-Sepsis framework achieves competitive prediction accuracy using only 20% of the ICU monitoring time required by SOTA methods, significantly advancing early SP. Extensive validation confirms its efficacy. Code is available at: https://github.com/yueliangy/MEET-Sepsis.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to PRICAI 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.15985v2",
    "published_date": "2025-10-13 13:38:29 UTC",
    "updated_date": "2025-10-21 09:49:14 UTC"
  },
  {
    "arxiv_id": "2510.11391v2",
    "title": "DocReward: A Document Reward Model for Structuring and Stylizing",
    "authors": [
      "Junpeng Liu",
      "Yuzhong Zhao",
      "Bowen Cao",
      "Jiayu Ding",
      "Yilin Jia",
      "Tengchao Lv",
      "Yupan Huang",
      "Shaohan Huang",
      "Nan Yang",
      "Li Dong",
      "Lei Cui",
      "Tao Ge",
      "Xun Wang",
      "Huitian Jiao",
      "Sun Mao",
      "FNU Kartik",
      "Si-Qing Chen",
      "Wai Lam",
      "Furu Wei"
    ],
    "abstract": "Recent advances in agentic workflows have enabled the automation of tasks such as professional document generation. However, they primarily focus on textual quality, neglecting visual structure and style, which are crucial for readability and engagement. This gap stems mainly from a lack of effective reward models capable of guiding agents toward producing documents with high structural and stylistic professionalism. To address this, we propose DocReward, a document reward model that evaluates documents based on their structure and style. The model is trained under a textual-quality-agnostic framework to assess professionalism without being influenced by textual quality. To achieve this, we construct a multi-domain dataset DocPair of 117K paired documents, covering 32 domains and 267 document types, each comprising a high- and low-professionalism document with identical content but different structure and style. This setup enables the model to evaluate professionalism comprehensively and independently of textual quality. DocReward is trained using the Bradley-Terry loss to score documents, penalizing predictions that contradict the annotated ranking. On a manually annotated benchmark, DocReward outperforms GPT-5 by 14.6 percentage points in accuracy. Extrinsic RL experiments further validate its effectiveness in guiding professional document generation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11391v2",
    "published_date": "2025-10-13 13:36:32 UTC",
    "updated_date": "2026-01-20 13:19:13 UTC"
  },
  {
    "arxiv_id": "2510.21763v1",
    "title": "Proportion and Perspective Control for Flow-Based Image Generation",
    "authors": [
      "Julien Boudier",
      "Hugo Caselles-Dupré"
    ],
    "abstract": "While modern text-to-image diffusion models generate high-fidelity images, they offer limited control over the spatial and geometric structure of the output. To address this, we introduce and evaluate two ControlNets specialized for artistic control: (1) a proportion ControlNet that uses bounding boxes to dictate the position and scale of objects, and (2) a perspective ControlNet that employs vanishing lines to control the 3D geometry of the scene. We support the training of these modules with data pipelines that leverage vision-language models for annotation and specialized algorithms for conditioning image synthesis. Our experiments demonstrate that both modules provide effective control but exhibit limitations with complex constraints. Both models are released on HuggingFace: https://huggingface.co/obvious-research",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Technical report after open-source release",
    "pdf_url": "https://arxiv.org/pdf/2510.21763v1",
    "published_date": "2025-10-13 13:34:28 UTC",
    "updated_date": "2025-10-13 13:34:28 UTC"
  },
  {
    "arxiv_id": "2510.11390v1",
    "title": "Medical Interpretability and Knowledge Maps of Large Language Models",
    "authors": [
      "Razvan Marinescu",
      "Victoria-Elisabeth Gruber",
      "Diego Fajardo"
    ],
    "abstract": "We present a systematic study of medical-domain interpretability in Large Language Models (LLMs). We study how the LLMs both represent and process medical knowledge through four different interpretability techniques: (1) UMAP projections of intermediate activations, (2) gradient-based saliency with respect to the model weights, (3) layer lesioning/removal and (4) activation patching. We present knowledge maps of five LLMs which show, at a coarse-resolution, where knowledge about patient's ages, medical symptoms, diseases and drugs is stored in the models. In particular for Llama3.3-70B, we find that most medical knowledge is processed in the first half of the model's layers. In addition, we find several interesting phenomena: (i) age is often encoded in a non-linear and sometimes discontinuous manner at intermediate layers in the models, (ii) the disease progression representation is non-monotonic and circular at certain layers of the model, (iii) in Llama3.3-70B, drugs cluster better by medical specialty rather than mechanism of action, especially for Llama3.3-70B and (iv) Gemma3-27B and MedGemma-27B have activations that collapse at intermediate layers but recover by the final layers. These results can guide future research on fine-tuning, un-learning or de-biasing LLMs for medical tasks by suggesting at which layers in the model these techniques should be applied.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "29 pages, 34 figures, 5 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.11390v1",
    "published_date": "2025-10-13 13:34:05 UTC",
    "updated_date": "2025-10-13 13:34:05 UTC"
  },
  {
    "arxiv_id": "2510.11380v1",
    "title": "AI-Driven anemia diagnosis: A review of advanced models and techniques",
    "authors": [
      "Abdullah Al Mahmud",
      "Prangon Chowdhury",
      "Mohammed Borhan Uddin",
      "Khaled Eabne Delowar",
      "Tausifur Rahman Talha",
      "Bijoy Dewanjee"
    ],
    "abstract": "Anemia, a condition marked by insufficient levels of red blood cells or hemoglobin, remains a widespread health issue affecting millions of individuals globally. Accurate and timely diagnosis is essential for effective management and treatment of anemia. In recent years, there has been a growing interest in the use of artificial intelligence techniques, i.e., machine learning (ML) and deep learning (DL) for the detection, classification, and diagnosis of anemia. This paper provides a systematic review of the recent advancements in this field, with a focus on various models applied to anemia detection. The review also compares these models based on several performance metrics, including accuracy, sensitivity, specificity, and precision. By analyzing these metrics, the paper evaluates the strengths and limitation of discussed models in detecting and classifying anemia, emphasizing the importance of addressing these factors to improve diagnostic accuracy.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11380v1",
    "published_date": "2025-10-13 13:22:45 UTC",
    "updated_date": "2025-10-13 13:22:45 UTC"
  },
  {
    "arxiv_id": "2512.05121v1",
    "title": "PESTalk: Speech-Driven 3D Facial Animation with Personalized Emotional Styles",
    "authors": [
      "Tianshun Han",
      "Benjia Zhou",
      "Ajian Liu",
      "Yanyan Liang",
      "Du Zhang",
      "Zhen Lei",
      "Jun Wan"
    ],
    "abstract": "PESTalk is a novel method for generating 3D facial animations with personalized emotional styles directly from speech. It overcomes key limitations of existing approaches by introducing a Dual-Stream Emotion Extractor (DSEE) that captures both time and frequency-domain audio features for fine-grained emotion analysis, and an Emotional Style Modeling Module (ESMM) that models individual expression patterns based on voiceprint characteristics. To address data scarcity, the method leverages a newly constructed 3D-EmoStyle dataset. Evaluations demonstrate that PESTalk outperforms state-of-the-art methods in producing realistic and personalized facial animations.",
    "categories": [
      "cs.GR",
      "cs.AI"
    ],
    "primary_category": "cs.GR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.05121v1",
    "published_date": "2025-10-13 13:21:38 UTC",
    "updated_date": "2025-10-13 13:21:38 UTC"
  },
  {
    "arxiv_id": "2510.11372v1",
    "title": "Early Detection and Reduction of Memorisation for Domain Adaptation and Instruction Tuning",
    "authors": [
      "Dean L. Slack",
      "Noura Al Moubayed"
    ],
    "abstract": "Although large language models excel across many tasks, they can memorise training data and thereby expose private or copyrighted text. Most defences target the pre-training stage, leaving memorisation during fine-tuning, especially for domain adaptation and instruction tuning, poorly understood. We fine-tune Pythia, Llama3, and Mistral models spanning 1.4B-70B parameters on common evaluation datasets and track verbatim memorisation throughout training. We find that memorisation increases dramatically in the first few epochs, often significantly before either validation perplexity or evaluation performance is optimised. We use a simple but effective n-gram memorisation score which reliably precedes verbatim memorisation; using it as an early-stopping criterion mitigates memorisation with minimal performance loss. Further, we introduce an n-gram-aware loss regulariser and show that it reduces memorisation across all model families tested by up to 40% while minimising evaluation performance trade-offs when compared to an existing memorisation mitigation strategy. These results yield practical, scalable insights into memorisation dynamics during language model fine-tuning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to Transactions of the ACL (TACL), 2025. 15 pages, 6 figures, 3 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.11372v1",
    "published_date": "2025-10-13 13:12:46 UTC",
    "updated_date": "2025-10-13 13:12:46 UTC"
  },
  {
    "arxiv_id": "2510.11370v2",
    "title": "Stabilizing MoE Reinforcement Learning by Aligning Training and Inference Routers",
    "authors": [
      "Wenhan Ma",
      "Hailin Zhang",
      "Liang Zhao",
      "Yifan Song",
      "Yudong Wang",
      "Zhifang Sui",
      "Fuli Luo"
    ],
    "abstract": "Reinforcement learning (RL) has emerged as a crucial approach for enhancing the capabilities of large language models. However, in Mixture-of-Experts (MoE) models, the routing mechanism often introduces instability, even leading to catastrophic RL training collapse. We analyze the training-inference consistency of MoE models and identify a notable discrepancy in routing behaviors between the two phases. Moreover, even under identical conditions, the routing framework can yield divergent expert selections across repeated forward passes. To address this foundational inconsistency, we propose Rollout Routing Replay (R3), a method that records routing distributions from the inference engine and replays them during training. R3 significantly reduces training-inference policy KL divergence and mitigates extreme discrepancies without compromising training speed. Extensive experiments on various settings confirm that R3 succeeds in stabilizing RL training, preventing collapse and outperforming methods such as GSPO and TIS. We believe this work can offer a new solution for stabilizing RL in MoE models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11370v2",
    "published_date": "2025-10-13 13:11:27 UTC",
    "updated_date": "2025-10-21 17:19:46 UTC"
  },
  {
    "arxiv_id": "2510.12835v1",
    "title": "Repurposing Annotation Guidelines to Instruct LLM Annotators: A Case Study",
    "authors": [
      "Kon Woo Kim",
      "Rezarta Islamaj",
      "Jin-Dong Kim",
      "Florian Boudin",
      "Akiko Aizawa"
    ],
    "abstract": "This study investigates how existing annotation guidelines can be repurposed to instruct large language model (LLM) annotators for text annotation tasks. Traditional guidelines are written for human annotators who internalize training, while LLMs require explicit, structured instructions. We propose a moderation-oriented guideline repurposing method that transforms guidelines into clear directives for LLMs through an LLM moderation process. Using the NCBI Disease Corpus as a case study, our experiments show that repurposed guidelines can effectively guide LLM annotators, while revealing several practical challenges. The results highlight the potential of this workflow to support scalable and cost-effective refinement of annotation guidelines and automated annotation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "11 pages, 2 figures, 3 tables, This is a preprint of the article accepted at NLDB 2025 (Springer LNCS). The final version is available at https://doi.org/10.1007/978-3-031-97144-0_13",
    "pdf_url": "https://arxiv.org/pdf/2510.12835v1",
    "published_date": "2025-10-13 13:07:58 UTC",
    "updated_date": "2025-10-13 13:07:58 UTC"
  },
  {
    "arxiv_id": "2510.13872v3",
    "title": "Joint Discriminative-Generative Modeling via Dual Adversarial Training",
    "authors": [
      "Xuwang Yin",
      "Claire Zhang",
      "Julie Steele",
      "Nir Shavit",
      "Tony T. Wang"
    ],
    "abstract": "Simultaneously achieving robust classification and high-fidelity generative modeling within a single framework presents a significant challenge. Hybrid approaches, such as Joint Energy-Based Models (JEM), interpret classifiers as EBMs but are often limited by the instability and poor sample quality inherent in Stochastic Gradient Langevin Dynamics (SGLD)-based training. We address these limitations by proposing a novel training framework that integrates adversarial training (AT) principles for both discriminative robustness and stable generative learning. The proposed method introduces three key innovations: (1) the replacement of SGLD-based JEM learning with a stable, AT-based approach that optimizes the energy function by discriminating between real data and Projected Gradient Descent (PGD)-generated contrastive samples using the BCE loss; (2) synergistic adversarial training for the discriminative component that enhances classification robustness while eliminating the need for explicit gradient penalties; and (3) a two-stage training strategy that addresses normalization-related instabilities and enables leveraging pretrained robust classifiers, generalizing effectively across diverse architectures. Experiments on CIFAR-10/100 and ImageNet demonstrate that our approach: (1) is the first EBM-based hybrid to scale to high-resolution datasets with high training stability, simultaneously achieving state-of-the-art discriminative and generative performance on ImageNet 256$\\times$256; (2) uniquely combines generative quality with adversarial robustness, enabling critical applications like robust counterfactual explanations; and (3) functions as a competitive standalone generative model, matching the generative quality of autoregressive methods (VAR-d16) and surpassing diffusion models while offering unique versatility.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Revised R1 regularization analysis using Roth et al. (2020) operator norm framework. Code: https://github.com/xuwangyin/DAT",
    "pdf_url": "https://arxiv.org/pdf/2510.13872v3",
    "published_date": "2025-10-13 13:07:22 UTC",
    "updated_date": "2026-01-20 15:22:16 UTC"
  },
  {
    "arxiv_id": "2510.11358v1",
    "title": "LLM-Specific Utility: A New Perspective for Retrieval-Augmented Generation",
    "authors": [
      "Hengran Zhang",
      "Keping Bi",
      "Jiafeng Guo",
      "Jiaming Zhang",
      "Shuaiqiang Wang",
      "Dawei Yin",
      "Xueqi Cheng"
    ],
    "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating external knowledge. While traditional retrieval focuses on relevance, RAG's effectiveness depends on the utility of retrieved passages, i.e., the usefulness in facilitating the generation of an accurate and comprehensive answer. Existing studies often treat utility as a generic attribute, ignoring the fact that different LLMs may benefit differently from the same passage due to variations in internal knowledge and comprehension ability. In this work, we introduce and systematically investigate the notion of LLM-specific utility. Through large-scale experiments across multiple datasets and LLMs, we demonstrate that human-annotated passages are not optimal for LLMs and that ground-truth utilitarian passages are not transferable across different LLMs. These findings highlight the necessity of adopting the LLM-specific utility in RAG research. Our findings indicate that some human-annotated passages are not ground-truth utilitarian passages for specific LLMs, partially due to the varying readability of queries and passages for LLMs, a tendency for which perplexity is a key metric. Based on these findings, we propose a benchmarking procedure for LLM-specific utility judgments. We evaluate existing utility judgment methods on six datasets and find that while verbalized methods using pseudo-answers perform robustly, LLMs struggle to assess utility effectively-failing to reject all passages for known queries and to select truly useful ones for unknown queries.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "13 pages, 9 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.11358v1",
    "published_date": "2025-10-13 12:57:45 UTC",
    "updated_date": "2025-10-13 12:57:45 UTC"
  },
  {
    "arxiv_id": "2510.11354v1",
    "title": "Understanding the Generalization of Stochastic Gradient Adam in Learning Neural Networks",
    "authors": [
      "Xuan Tang",
      "Han Zhang",
      "Yuan Cao",
      "Difan Zou"
    ],
    "abstract": "Adam is a popular and widely used adaptive gradient method in deep learning, which has also received tremendous focus in theoretical research. However, most existing theoretical work primarily analyzes its full-batch version, which differs fundamentally from the stochastic variant used in practice. Unlike SGD, stochastic Adam does not converge to its full-batch counterpart even with infinitesimal learning rates. We present the first theoretical characterization of how batch size affects Adam's generalization, analyzing two-layer over-parameterized CNNs on image data. Our results reveal that while both Adam and AdamW with proper weight decay $λ$ converge to poor test error solutions, their mini-batch variants can achieve near-zero test error. We further prove Adam has a strictly smaller effective weight decay bound than AdamW, theoretically explaining why Adam requires more sensitive $λ$ tuning. Extensive experiments validate our findings, demonstrating the critical role of batch size and weight decay in Adam's generalization performance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "71 pages, 12 figures, NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.11354v1",
    "published_date": "2025-10-13 12:48:22 UTC",
    "updated_date": "2025-10-13 12:48:22 UTC"
  },
  {
    "arxiv_id": "2510.11347v1",
    "title": "Multi-View Graph Feature Propagation for Privacy Preservation and Feature Sparsity",
    "authors": [
      "Etzion Harari",
      "Moshe Unger"
    ],
    "abstract": "Graph Neural Networks (GNNs) have demonstrated remarkable success in node classification tasks over relational data, yet their effectiveness often depends on the availability of complete node features. In many real-world scenarios, however, feature matrices are highly sparse or contain sensitive information, leading to degraded performance and increased privacy risks. Furthermore, direct exposure of information can result in unintended data leakage, enabling adversaries to infer sensitive information. To address these challenges, we propose a novel Multi-view Feature Propagation (MFP) framework that enhances node classification under feature sparsity while promoting privacy preservation. MFP extends traditional Feature Propagation (FP) by dividing the available features into multiple Gaussian-noised views, each propagating information independently through the graph topology. The aggregated representations yield expressive and robust node embeddings. This framework is novel in two respects: it introduces a mechanism that improves robustness under extreme sparsity, and it provides a principled way to balance utility with privacy. Extensive experiments conducted on graph datasets demonstrate that MFP outperforms state-of-the-art baselines in node classification while substantially reducing privacy leakage. Moreover, our analysis demonstrates that propagated outputs serve as alternative imputations rather than reconstructions of the original features, preserving utility without compromising privacy. A comprehensive sensitivity analysis further confirms the stability and practical applicability of MFP across diverse scenarios. Overall, MFP provides an effective and privacy-aware framework for graph learning in domains characterized by missing or sensitive features.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11347v1",
    "published_date": "2025-10-13 12:42:00 UTC",
    "updated_date": "2025-10-13 12:42:00 UTC"
  },
  {
    "arxiv_id": "2510.11346v1",
    "title": "Uncertainty-Aware ControlNet: Bridging Domain Gaps with Synthetic Image Generation",
    "authors": [
      "Joshua Niemeijer",
      "Jan Ehrhardt",
      "Heinz Handels",
      "Hristina Uzunova"
    ],
    "abstract": "Generative Models are a valuable tool for the controlled creation of high-quality image data. Controlled diffusion models like the ControlNet have allowed the creation of labeled distributions. Such synthetic datasets can augment the original training distribution when discriminative models, like semantic segmentation, are trained. However, this augmentation effect is limited since ControlNets tend to reproduce the original training distribution.\n  This work introduces a method to utilize data from unlabeled domains to train ControlNets by introducing the concept of uncertainty into the control mechanism. The uncertainty indicates that a given image was not part of the training distribution of a downstream task, e.g., segmentation. Thus, two types of control are engaged in the final network: an uncertainty control from an unlabeled dataset and a semantic control from the labeled dataset. The resulting ControlNet allows us to create annotated data with high uncertainty from the target domain, i.e., synthetic data from the unlabeled distribution with labels. In our scenario, we consider retinal OCTs, where typically high-quality Spectralis images are available with given ground truth segmentations, enabling the training of segmentation networks. The recent development in Home-OCT devices, however, yields retinal OCTs with lower quality and a large domain shift, such that out-of-the-pocket segmentation networks cannot be applied for this type of data. Synthesizing annotated images from the Home-OCT domain using the proposed approach closes this gap and leads to significantly improved segmentation results without adding any further supervision. The advantage of uncertainty-guidance becomes obvious when compared to style transfer: it enables arbitrary domain shifts without any strict learning of an image style. This is also demonstrated in a traffic scene experiment.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted for presentation at ICCV Workshops 2025, \"The 4th Workshop on What is Next in Multimodal Foundation Models?\" (MMFM)",
    "pdf_url": "https://arxiv.org/pdf/2510.11346v1",
    "published_date": "2025-10-13 12:41:28 UTC",
    "updated_date": "2025-10-13 12:41:28 UTC"
  },
  {
    "arxiv_id": "2510.11345v1",
    "title": "Part II: ROLL Flash -- Accelerating RLVR and Agentic Training with Asynchrony",
    "authors": [
      "Han Lu",
      "Zichen Liu",
      "Shaopan Xiong",
      "Yancheng He",
      "Wei Gao",
      "Yanan Wu",
      "Weixun Wang",
      "Jiashun Liu",
      "Yang Li",
      "Haizhou Zhao",
      "Ju Huang",
      "Siran Yang",
      "Xiaoyang Li",
      "Yijia Luo",
      "Zihe Liu",
      "Ling Pan",
      "Junchi Yan",
      "Wei Wang",
      "Wenbo Su",
      "Jiamang Wang",
      "Lin Qu",
      "Bo Zheng"
    ],
    "abstract": "Synchronous Reinforcement Learning (RL) post-training has emerged as a crucial step for enhancing Large Language Models (LLMs) with diverse capabilities. However, many systems designed to accelerate RL post-training still suffer from low resource utilization and limited scalability. We present ROLL Flash, a system that extends ROLL with native support for asynchronous RL post-training. ROLL Flash is built upon two core design principles: fine-grained parallelism and rollout-train decoupling. Guided by these principles, ROLL Flash provides flexible programming interfaces that enable a fully asynchronous training architecture and support efficient rollout mechanisms, including queue scheduling and environment-level asynchronous execution. Through comprehensive theoretical analysis and extensive experiments, we demonstrate that ROLL Flash significantly improves resource utilization and scalability over synchronous RL post-training. ROLL Flash achieves up to 2.24x speedup on RLVR tasks and 2.72x on agentic tasks, using the same GPU budget as synchronous baselines. Furthermore, we implement several popular off-policy algorithms and verify that asynchronous training can achieve performance on par with synchronous training.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11345v1",
    "published_date": "2025-10-13 12:41:27 UTC",
    "updated_date": "2025-10-13 12:41:27 UTC"
  },
  {
    "arxiv_id": "2510.11339v1",
    "title": "Event-Aware Prompt Learning for Dynamic Graphs",
    "authors": [
      "Xingtong Yu",
      "Ruijuan Liang",
      "Xinming Zhang",
      "Yuan Fang"
    ],
    "abstract": "Real-world graph typically evolve via a series of events, modeling dynamic interactions between objects across various domains. For dynamic graph learning, dynamic graph neural networks (DGNNs) have emerged as popular solutions. Recently, prompt learning methods have been explored on dynamic graphs. However, existing methods generally focus on capturing the relationship between nodes and time, while overlooking the impact of historical events. In this paper, we propose EVP, an event-aware dynamic graph prompt learning framework that can serve as a plug-in to existing methods, enhancing their ability to leverage historical events knowledge. First, we extract a series of historical events for each node and introduce an event adaptation mechanism to align the fine-grained characteristics of these events with downstream tasks. Second, we propose an event aggregation mechanism to effectively integrate historical knowledge into node representations. Finally, we conduct extensive experiments on four public datasets to evaluate and analyze EVP.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Under review",
    "pdf_url": "https://arxiv.org/pdf/2510.11339v1",
    "published_date": "2025-10-13 12:37:53 UTC",
    "updated_date": "2025-10-13 12:37:53 UTC"
  },
  {
    "arxiv_id": "2510.13870v1",
    "title": "Unlocking the Potential of Diffusion Language Models through Template Infilling",
    "authors": [
      "Junhoo Lee",
      "Seungyeon Kim",
      "Nojun Kwak"
    ],
    "abstract": "Diffusion Language Models (DLMs) have emerged as a promising alternative to Autoregressive Language Models, yet their inference strategies remain limited to prefix-based prompting inherited from the autoregressive paradigm. In this paper, we propose Template Infilling (TI), a tailored conditioning methodology for DLMs' generation process. Unlike conventional prefix prompting, TI first generates a structural template for the target response, then fills in the masked segments. To enhance the flexibility of this structural control, we introduce Dynamic Segment Allocation (DSA), which adaptively adjusts segment lengths based on generation confidence. We demonstrate the effectiveness of our approach on mathematical reasoning and code generation benchmarks, achieving consistent improvements of 17.01$\\%$p over baseline. Furthermore, we show that TI provides additional advantages in multi-token generation settings, enabling effective speedup while maintaining generation quality.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.13870v1",
    "published_date": "2025-10-13 12:33:41 UTC",
    "updated_date": "2025-10-13 12:33:41 UTC"
  },
  {
    "arxiv_id": "2510.11330v1",
    "title": "Diffusion-Link: Diffusion Probabilistic Model for Bridging the Audio-Text Modality Gap",
    "authors": [
      "KiHyun Nam",
      "Jongmin Choi",
      "Hyeongkeun Lee",
      "Jungwoo Heo",
      "Joon Son Chung"
    ],
    "abstract": "Contrastive audio-language pretraining yields powerful joint representations, yet a persistent audio-text modality gap limits the benefits of coupling multimodal encoders with large language models (LLMs). We present Diffusion-Link, a diffusion-based modality-bridging module that generatively maps audio embeddings into the text-embedding distribution. The module is trained at the output embedding from the frozen multimodal encoder and implemented as a lightweight network with three residual MLP blocks. To assess the effect of Diffusion-Link on multimodal encoder-LLM coupling, we evaluate on Automatic Audio Captioning (AAC); to our knowledge, this is the first application of diffusion-based modality bridging to AAC. We report two results. (1) Modality-gap analysis: on similarity and geometric criteria, Diffusion-Link reduces the modality gap the most among prior diffusion-based methods and shows a collective migration of audio embeddings toward the text distribution. (2) Downstream AAC: attaching Diffusion-Link to the same multimodal LLM baseline achieves state-of-the-art on AudioCaps in both zero-shot and fully supervised captioning without external knowledge, with relative gains up to 52.5% and 7.5%, respectively. These findings show that closing the modality gap is pivotal for effective coupling between multimodal encoders and LLMs, and diffusion-based modality bridging offers a promising direction beyond knowledge-retrieval-centric designs. Code will be released upon acceptance https://github.com/DevKiHyun/Diffusion-Link",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "5 pages. Submitted to IEEE ICASSP 2026",
    "pdf_url": "https://arxiv.org/pdf/2510.11330v1",
    "published_date": "2025-10-13 12:25:33 UTC",
    "updated_date": "2025-10-13 12:25:33 UTC"
  },
  {
    "arxiv_id": "2510.11328v1",
    "title": "Do LLMs \"Feel\"? Emotion Circuits Discovery and Control",
    "authors": [
      "Chenxi Wang",
      "Yixuan Zhang",
      "Ruiji Yu",
      "Yufei Zheng",
      "Lang Gao",
      "Zirui Song",
      "Zixiang Xu",
      "Gus Xia",
      "Huishuai Zhang",
      "Dongyan Zhao",
      "Xiuying Chen"
    ],
    "abstract": "As the demand for emotional intelligence in large language models (LLMs) grows, a key challenge lies in understanding the internal mechanisms that give rise to emotional expression and in controlling emotions in generated text. This study addresses three core questions: (1) Do LLMs contain context-agnostic mechanisms shaping emotional expression? (2) What form do these mechanisms take? (3) Can they be harnessed for universal emotion control? We first construct a controlled dataset, SEV (Scenario-Event with Valence), to elicit comparable internal states across emotions. Subsequently, we extract context-agnostic emotion directions that reveal consistent, cross-context encoding of emotion (Q1). We identify neurons and attention heads that locally implement emotional computation through analytical decomposition and causal analysis, and validate their causal roles via ablation and enhancement interventions. Next, we quantify each sublayer's causal influence on the model's final emotion representation and integrate the identified local components into coherent global emotion circuits that drive emotional expression (Q2). Directly modulating these circuits achieves 99.65% emotion-expression accuracy on the test set, surpassing prompting- and steering-based methods (Q3). To our knowledge, this is the first systematic study to uncover and validate emotion circuits in LLMs, offering new insights into interpretability and controllable emotional intelligence.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "19 pages, 8 figures, 8 tables. Code and dataset available at https://github.com/Aurora-cx/EmotionCircuits-LLM",
    "pdf_url": "https://arxiv.org/pdf/2510.11328v1",
    "published_date": "2025-10-13 12:24:24 UTC",
    "updated_date": "2025-10-13 12:24:24 UTC"
  },
  {
    "arxiv_id": "2510.15983v1",
    "title": "Ontologies in Motion: A BFO-Based Approach to Knowledge Graph Construction for Motor Performance Research Data in Sports Science",
    "authors": [
      "Sarah Rebecca Ondraszek",
      "Jörg Waitelonis",
      "Katja Keller",
      "Claudia Niessner",
      "Anna M. Jacyszyn",
      "Harald Sack"
    ],
    "abstract": "An essential component for evaluating and comparing physical and cognitive capabilities between populations is the testing of various factors related to human performance. As a core part of sports science research, testing motor performance enables the analysis of the physical health of different demographic groups and makes them comparable.\n  The Motor Research (MO|RE) data repository, developed at the Karlsruhe Institute of Technology, is an infrastructure for publishing and archiving research data in sports science, particularly in the field of motor performance research. In this paper, we present our vision for creating a knowledge graph from MO|RE data. With an ontology rooted in the Basic Formal Ontology, our approach centers on formally representing the interrelation of plan specifications, specific processes, and related measurements. Our goal is to transform how motor performance data are modeled and shared across studies, making it standardized and machine-understandable. The idea presented here is developed within the Leibniz Science Campus ``Digital Transformation of Research'' (DiTraRe).",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 2 figures. Camera-ready version. Accepted to the 5th International Workshop on Scientific Knowledge: Representation, Discovery, and Assessment; 2 November 2025 - Nara, Japan; co-located with The 24th International Semantic Web Conference, ISWC 2025. To be published in CEUR proceedings",
    "pdf_url": "https://arxiv.org/pdf/2510.15983v1",
    "published_date": "2025-10-13 12:21:33 UTC",
    "updated_date": "2025-10-13 12:21:33 UTC"
  },
  {
    "arxiv_id": "2510.13869v1",
    "title": "CoLoR-GAN: Continual Few-Shot Learning with Low-Rank Adaptation in Generative Adversarial Networks",
    "authors": [
      "Munsif Ali",
      "Leonardo Rossi",
      "Massimo Bertozzi"
    ],
    "abstract": "Continual learning (CL) in the context of Generative Adversarial Networks (GANs) remains a challenging problem, particularly when it comes to learn from a few-shot (FS) samples without catastrophic forgetting. Current most effective state-of-the-art (SOTA) methods, like LFS-GAN, introduce a non-negligible quantity of new weights at each training iteration, which would become significant when considering the long term. For this reason, this paper introduces \\textcolor{red}{\\textbf{\\underline{c}}}ontinual few-sh\\textcolor{red}{\\textbf{\\underline{o}}}t learning with \\textcolor{red}{\\textbf{\\underline{lo}}}w-\\textcolor{red}{\\textbf{\\underline{r}}}ank adaptation in GANs named CoLoR-GAN, a framework designed to handle both FS and CL together, leveraging low-rank tensors to efficiently adapt the model to target tasks while reducing even more the number of parameters required. Applying a vanilla LoRA implementation already permitted us to obtain pretty good results. In order to optimize even further the size of the adapters, we challenged LoRA limits introducing a LoRA in LoRA (LLoRA) technique for convolutional layers. Finally, aware of the criticality linked to the choice of the hyperparameters of LoRA, we provide an empirical study to easily find the best ones. We demonstrate the effectiveness of CoLoR-GAN through experiments on several benchmark CL and FS tasks and show that our model is efficient, reaching SOTA performance but with a number of resources enormously reduced. Source code is available on \\href{https://github.com/munsifali11/CoLoR-GAN}{Github.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.13869v1",
    "published_date": "2025-10-13 12:13:32 UTC",
    "updated_date": "2025-10-13 12:13:32 UTC"
  },
  {
    "arxiv_id": "2510.11313v1",
    "title": "Automated Skill Decomposition Meets Expert Ontologies: Bridging the Granularity Gap with LLMs",
    "authors": [
      "Le Ngoc Luyen",
      "Marie-Hélène Abel"
    ],
    "abstract": "This paper investigates automated skill decomposition using Large Language Models (LLMs) and proposes a rigorous, ontology-grounded evaluation framework. Our framework standardizes the pipeline from prompting and generation to normalization and alignment with ontology nodes. To evaluate outputs, we introduce two metrics: a semantic F1-score that uses optimal embedding-based matching to assess content accuracy, and a hierarchy-aware F1-score that credits structurally correct placements to assess granularity. We conduct experiments on ROME-ESCO-DecompSkill, a curated subset of parents, comparing two prompting strategies: zero-shot and leakage-safe few-shot with exemplars. Across diverse LLMs, zero-shot offers a strong baseline, while few-shot consistently stabilizes phrasing and granularity and improves hierarchy-aware alignment. A latency analysis further shows that exemplar-guided prompts are competitive - and sometimes faster - than unguided zero-shot due to more schema-compliant completions. Together, the framework, benchmark, and metrics provide a reproducible foundation for developing ontology-faithful skill decomposition systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11313v1",
    "published_date": "2025-10-13 12:03:06 UTC",
    "updated_date": "2025-10-13 12:03:06 UTC"
  },
  {
    "arxiv_id": "2510.11307v1",
    "title": "FOSSIL: Harnessing Feedback on Suboptimal Samples for Data-Efficient Generalisation with Imitation Learning for Embodied Vision-and-Language Tasks",
    "authors": [
      "Sabrina McCallum",
      "Amit Parekh",
      "Alessandro Suglia"
    ],
    "abstract": "Current approaches to embodied AI tend to learn policies from expert demonstrations. However, without a mechanism to evaluate the quality of demonstrated actions, they are limited to learning from optimal behaviour, or they risk replicating errors and inefficiencies. While reinforcement learning offers one alternative, the associated exploration typically results in sacrificing data efficiency. This work explores how agents trained with imitation learning can learn robust representations from both optimal and suboptimal demonstrations when given access to constructive language feedback as a means to contextualise different modes of behaviour. We directly provide language feedback embeddings as part of the input sequence into a Transformer-based policy, and optionally complement the traditional next action prediction objective with auxiliary self-supervised learning objectives for feedback prediction. We test our approach on a range of embodied Vision-and-Language tasks in our custom BabyAI-XGen environment and show significant improvements in agents' compositional generalisation abilities and robustness, suggesting that our data-efficient method allows models to successfully convert suboptimal behaviour into learning opportunities. Overall, our results suggest that language feedback is a competitive and intuitive alternative to intermediate scalar rewards for language-specified embodied tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2025 Findings",
    "pdf_url": "https://arxiv.org/pdf/2510.11307v1",
    "published_date": "2025-10-13 11:55:21 UTC",
    "updated_date": "2025-10-13 11:55:21 UTC"
  },
  {
    "arxiv_id": "2510.11302v2",
    "title": "When Does Supervised Training Pay Off? The Hidden Economics of Object Detection in the Era of Vision-Language Models",
    "authors": [
      "Samer Al-Hamadani"
    ],
    "abstract": "Object detection traditionally relies on costly manual annotation. We present the first comprehensive cost-effectiveness analysis comparing supervised YOLO and zero-shot vision-language models (Gemini Flash 2.5 and GPT-4). Evaluated on 5,000 stratified COCO images and 500 diverse product images, combined with Total Cost of Ownership modeling, we derive break-even thresholds for architecture selection. Results show supervised YOLO attains 91.2% accuracy versus 68.5% for Gemini and 71.3% for GPT-4 on standard categories; the annotation expense for a 100-category system is $10,800, and the accuracy advantage only pays off beyond 55 million inferences (151,000 images/day for one year). On diverse product categories Gemini achieves 52.3% and GPT-4 55.1%, while supervised YOLO cannot detect untrained classes. Cost-per-correct-detection favors Gemini ($0.00050) and GPT-4 ($0.00067) over YOLO ($0.143) at 100,000 inferences. We provide decision frameworks showing that optimal architecture choice depends on inference volume, category stability, budget, and accuracy requirements.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "30 pages, 12 figures, 4 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.11302v2",
    "published_date": "2025-10-13 11:48:48 UTC",
    "updated_date": "2025-10-20 15:09:23 UTC"
  },
  {
    "arxiv_id": "2510.11300v1",
    "title": "Beyond touch-based HMI: Control your machines in natural language by utilizing large language models and OPC UA",
    "authors": [
      "Bernd Hofmann",
      "Sven Kreitlein",
      "Joerg Franke",
      "Patrick Bruendl"
    ],
    "abstract": "This paper proposes an agent-based approach toward a more natural interface between humans and machines. Large language models equipped with tools and the communication standard OPC UA are utilized to control machines in natural language. Instead of touch interaction, which is currently the state-of-the-art medium for interaction in operations, the proposed approach enables operators to talk or text with machines. This allows commands such as 'Please decrease the temperature by 20 % in machine 1 and set the motor speed to 5000 rpm in machine 2.' The large language model receives the user input and selects one of three predefined tools that connect to an OPC UA server and either change or read the value of a node. Afterwards, the result of the tool execution is passed back to the language model, which then provides a final response to the user. The approach is universally designed and can therefore be applied to any machine that supports the OPC UA standard. The large language model is neither fine-tuned nor requires training data, only the relevant machine credentials and a parameter dictionary are included within the system prompt. The approach is evaluated on a Siemens S7-1500 programmable logic controller with four machine parameters in a case study of fifty synthetically generated commands on five different models. The results demonstrate high success rate, with proprietary GPT 5 models achieving accuracies between 96.0 % and 98.0 %, and open-weight models reaching up to 90.0 %. The proposed approach of this empirical study contributes to advancing natural interaction in industrial human-machine interfaces.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11300v1",
    "published_date": "2025-10-13 11:46:47 UTC",
    "updated_date": "2025-10-13 11:46:47 UTC"
  },
  {
    "arxiv_id": "2510.11292v1",
    "title": "LouisKV: Efficient KV Cache Retrieval for Long Input-Output Sequences",
    "authors": [
      "Wenbo Wu",
      "Qingyi Si",
      "Xiurui Pan",
      "Ye Wang",
      "Jie Zhang"
    ],
    "abstract": "While Key-Value (KV) cache succeeds in reducing redundant computations in auto-regressive models, it introduces significant memory overhead, limiting its practical deployment in long-sequence scenarios. Existing KV retrieval methods mitigate this by dynamically retaining only a subset of KV entries on the GPU. However, they still suffer from notable efficiency and accuracy bottlenecks due to per-token retrieval and coarse-grained page-level KV management, especially in long-output reasoning scenarios. With the emergence of large reasoning models, efficiently handling such scenarios has become increasingly important. To address this issue, we present two key observations: (1) critical KVs exhibit strong temporal locality during decoding, and (2) these KVs exhibit distinct distribution patterns across the input prompt and generated output. Building on these observations, we propose LouisKV, an efficient KV cache retrieval framework designed for various long-sequence scenarios. Specifically, LouisKV introduces a semantic-aware retrieval strategy leveraging temporal locality to trigger retrieval only at semantic boundaries, drastically reducing computation and data transfer overhead. LouisKV also designs a decoupled, fine-grained management scheme that tailors differentiated strategies for input and output sequences to create retrieval units that better match the model's attention patterns, enabling precise identification of critical KVs. Furthermore, to boost efficiency, LouisKV incorporates several kernel-level optimizations, including custom Triton and CUDA kernels to accelerate the KV clustering and retrieval. Evaluations show that LouisKV achieves up to 4.7$\\times$ speedup over state-of-the-art KV retrieval methods while maintaining near-lossless accuracy across diverse long-sequence tasks, including long-input short-output, short-input long-output, and long-input long-output scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11292v1",
    "published_date": "2025-10-13 11:28:30 UTC",
    "updated_date": "2025-10-13 11:28:30 UTC"
  },
  {
    "arxiv_id": "2510.11290v1",
    "title": "Evolution in Simulation: AI-Agent School with Dual Memory for High-Fidelity Educational Dynamics",
    "authors": [
      "Sheng Jin",
      "Haoming Wang",
      "Zhiqi Gao",
      "Yongbo Yang",
      "Bao Chunjia",
      "Chengliang Wang"
    ],
    "abstract": "Large language models (LLMs) based Agents are increasingly pivotal in simulating and understanding complex human systems and interactions. We propose the AI-Agent School (AAS) system, built around a self-evolving mechanism that leverages agents for simulating complex educational dynamics. Addressing the fragmented issues in teaching process modeling and the limitations of agents performance in simulating diverse educational participants, AAS constructs the Zero-Exp strategy, employs a continuous \"experience-reflection-optimization\" cycle, grounded in a dual memory base comprising experience and knowledge bases and incorporating short-term and long-term memory components. Through this mechanism, agents autonomously evolve via situated interactions within diverse simulated school scenarios. This evolution enables agents to more accurately model the nuanced, multi-faceted teacher-student engagements and underlying learning processes found in physical schools. Experiment confirms that AAS can effectively simulate intricate educational dynamics and is effective in fostering advanced agent cognitive abilities, providing a foundational stepping stone from the \"Era of Experience\" to the \"Era of Simulation\" by generating high-fidelity behavioral and interaction data.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages, 7 figures, EMNLP conference",
    "pdf_url": "https://arxiv.org/pdf/2510.11290v1",
    "published_date": "2025-10-13 11:27:53 UTC",
    "updated_date": "2025-10-13 11:27:53 UTC"
  },
  {
    "arxiv_id": "2510.11281v1",
    "title": "PADME: Procedure Aware DynaMic Execution",
    "authors": [
      "Deepeka Garg",
      "Sihan Zeng",
      "Annapoorani L. Narayanan",
      "Sumitra Ganesh",
      "Leo Ardon"
    ],
    "abstract": "Learning to autonomously execute long-horizon procedures from natural language remains a core challenge for intelligent agents. Free-form instructions such as recipes, scientific protocols, or business workflows encode rich procedural knowledge, but their variability and lack of structure cause agents driven by large language models (LLMs) to drift or fail during execution. We introduce Procedure Aware DynaMic Execution (PADME), an agent framework that produces and exploits a graph-based representation of procedures. Unlike prior work that relies on manual graph construction or unstructured reasoning, PADME autonomously transforms procedural text into executable graphs that capture task dependencies, decision points, and reusable subroutines. Central to PADME is a two-phase methodology; Teach phase, which focuses on systematic structuring, enrichment with executable logic of procedures, followed by Execute phase, which enables dynamic execution in response to real-time inputs and environment feedback. This separation ensures quality assurance and scalability, allowing expert knowledge to be encoded once and reliably reused across varying contexts. The graph representation also provides an inductive bias that reduces error accumulation in long-horizon reasoning, underscoring the importance of structured procedure modeling for reliable agent-driven automation. Empirically, PADME achieves state-of-the-art performance on four diverse benchmarks, including ALFWorld and ScienceWorld. These results demonstrate that agents equipped with graph-based procedure representations offer a powerful intermediate abstraction for robust and generalizable execution.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11281v1",
    "published_date": "2025-10-13 11:15:49 UTC",
    "updated_date": "2025-10-13 11:15:49 UTC"
  },
  {
    "arxiv_id": "2510.11278v2",
    "title": "ENIGMA: The Geometry of Reasoning and Alignment in Large-Language Models",
    "authors": [
      "Gareth Seneque",
      "Lap-Hang Ho",
      "Nafise Erfanian Saeedi",
      "Jeffrey Molendijk",
      "Ariel Kuperman",
      "Tim Elson"
    ],
    "abstract": "We present Entropic Mutual-Information Geometry Large-Language Model Alignment (ENIGMA), a novel approach to Large-Language Model (LLM) training that jointly improves reasoning, alignment and robustness by treating an organisation's policies/principles as directions to move on a model's information manifold. Our single-loop trainer combines Group-Relative Policy Optimisation (GRPO), an on-policy, critic-free RL method with Chain-of-Thought (CoT)-format only rewards; a Self-Supervised Alignment with Mutual Information (SAMI)-style symmetric InfoNCE auxiliary; and an entropic Sinkhorn optimal-transport regulariser on hidden-state distributions to bound geometry drift. We also introduce infoNCE metrics that specialise to a standard MI lower bound under matched negatives to measure how strongly a model's CoT encodes these policies. These metrics include a Sufficiency Index (SI) that enables the selection and creation of principles that maximise downstream performance prior to training. In our experiments using small (1B) LLMs, high-SI principles predict steadier training dynamics and improved benchmark performance over GRPO ablations. Our information-geometry analysis of trained models validates desirable structural change in the manifold. These results support our hypothesis that reasoning, alignment, and robustness are projections of a single information-geometric objective, and that models trained using ENIGMA demonstrate principled reasoning without the use of a reward model, offering a path to trusted capability",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "52 pages, 10 figures, author typo corrected, abstract typo corrected",
    "pdf_url": "https://arxiv.org/pdf/2510.11278v2",
    "published_date": "2025-10-13 11:13:09 UTC",
    "updated_date": "2025-10-16 09:21:06 UTC"
  },
  {
    "arxiv_id": "2510.11277v1",
    "title": "Towards Real-Time Fake News Detection under Evidence Scarcity",
    "authors": [
      "Guangyu Wei",
      "Ke Han",
      "Yueming Lyu",
      "Yu Luo",
      "Yue Jiang",
      "Caifeng Shan",
      "Nicu Sebe"
    ],
    "abstract": "Fake news detection becomes particularly challenging in real-time scenarios, where emerging events often lack sufficient supporting evidence. Existing approaches often rely heavily on external evidence and therefore struggle to generalize under evidence scarcity. To address this issue, we propose Evaluation-Aware Selection of Experts (EASE), a novel framework for real-time fake news detection that dynamically adapts its decision-making process according to the assessed sufficiency of available evidence. EASE introduces a sequential evaluation mechanism comprising three independent perspectives: (1) Evidence-based evaluation, which assesses evidence and incorporates it into decision-making only when the evidence is sufficiently supportive; (2) Reasoning-based evaluation, which leverages the world knowledge of large language models (LLMs) and applies them only when their reliability is adequately established; and (3) Sentiment-based fallback, which integrates sentiment cues when neither evidence nor reasoning is reliable. To enhance the accuracy of evaluation processes, EASE employs instruction tuning with pseudo labels to guide each evaluator in justifying its perspective-specific knowledge through interpretable reasoning. Furthermore, the expert modules integrate the evaluators' justified assessments with the news content to enable evaluation-aware decision-making, thereby enhancing overall detection accuracy. Moreover, we introduce RealTimeNews-25, a new benchmark comprising recent news for evaluating model generalization on emerging news with limited evidence. Extensive experiments demonstrate that EASE not only achieves state-of-the-art performance across multiple benchmarks, but also significantly improves generalization to real-time news. The code and dataset are available: https://github.com/wgyhhhh/EASE.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11277v1",
    "published_date": "2025-10-13 11:11:46 UTC",
    "updated_date": "2025-10-13 11:11:46 UTC"
  },
  {
    "arxiv_id": "2510.15982v1",
    "title": "AMiD: Knowledge Distillation for LLMs with $α$-mixture Assistant Distribution",
    "authors": [
      "Donghyeok Shin",
      "Yeongmin Kim",
      "Suhyeon Jo",
      "Byeonghu Na",
      "Il-Chul Moon"
    ],
    "abstract": "Autoregressive large language models (LLMs) have achieved remarkable improvement across many tasks but incur high computational and memory costs. Knowledge distillation (KD) mitigates this issue by transferring knowledge from a large teacher to a smaller student through distributional alignment. Previous studies have proposed various discrepancy metrics, but the capacity gap and training instability caused by near-zero probabilities, stemming from the high-dimensional output of LLMs, remain fundamental limitations. To overcome these challenges, several approaches implicitly or explicitly incorporating assistant distribution have recently been proposed. However, the past proposals of assistant distributions have been a fragmented approach without a systematic investigation of the interpolation path and the divergence. This paper proposes $α$-mixture assistant distribution, a novel generalized family of assistant distributions, and $α$-mixture distillation, coined AMiD, a unified framework for KD using the assistant distribution. The $α$-mixture assistant distribution provides a continuous extension of the assistant distribution by introducing a new distribution design variable $α$, which has been fixed in all previous approaches. Furthermore, AMiD generalizes the family of divergences used with the assistant distributions based on optimality, which has also been restricted in previous works. Through extensive experiments, we demonstrate that AMiD offers superior performance and training stability by leveraging a broader and theoretically grounded assistant distribution space.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15982v1",
    "published_date": "2025-10-13 11:05:27 UTC",
    "updated_date": "2025-10-13 11:05:27 UTC"
  },
  {
    "arxiv_id": "2510.11269v1",
    "title": "From Prompts to Packets: A View from the Network on ChatGPT, Copilot, and Gemini",
    "authors": [
      "Antonio Montieri",
      "Alfredo Nascita",
      "Antonio Pescapè"
    ],
    "abstract": "Generative AI (GenAI) chatbots are now pervasive in digital ecosystems, yet their network traffic remains largely underexplored. This study presents an in-depth investigation of traffic generated by three leading chatbots (ChatGPT, Copilot, and Gemini) when accessed via Android mobile apps for both text and image generation. Using a dedicated capture architecture, we collect and label two complementary workloads: a 60-hour generic dataset with unconstrained prompts, and a controlled dataset built from identical prompts across GenAI apps and replicated via conventional messaging apps to enable one-to-one comparisons. This dual design allows us to address practical research questions on the distinctiveness of GenAI traffic, its differences from widely deployed traffic categories, and its novel implications for network usage. To this end, we provide fine-grained traffic characterization at trace, flow, and protocol levels, and model packet-sequence dynamics with Multimodal Markov Chains. Our analyses reveal app- and content-specific traffic patterns, particularly in volume, uplink/downlink profiles, and protocol adoption. We highlight the predominance of TLS, with Gemini extensively leveraging QUIC, ChatGPT exclusively using TLS 1.3, and app- and content-specific Server Name Indication (SNI) values. A payload-based occlusion analysis quantifies SNI's contribution to classification: masking it reduces F1-score by up to 20 percentage points in GenAI app traffic classification. Finally, compared with conventional messaging apps when carrying the same content, GenAI chatbots exhibit unique traffic characteristics, highlighting new stress factors for mobile networks, such as sustained upstream activity, with direct implications for network monitoring and management. We publicly release the datasets to support reproducibility and foster extensions to other use cases.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "13 pages, 8 figures, 2 tables, 4 research questions, preprint submitted to Elsevier Computer Networks",
    "pdf_url": "https://arxiv.org/pdf/2510.11269v1",
    "published_date": "2025-10-13 10:58:54 UTC",
    "updated_date": "2025-10-13 10:58:54 UTC"
  },
  {
    "arxiv_id": "2510.11260v1",
    "title": "A Large-Language-Model Assisted Automated Scale Bar Detection and Extraction Framework for Scanning Electron Microscopic Images",
    "authors": [
      "Yuxuan Chen",
      "Ruotong Yang",
      "Zhengyang Zhang",
      "Mehreen Ahmed",
      "Yanming Wang"
    ],
    "abstract": "Microscopic characterizations, such as Scanning Electron Microscopy (SEM), are widely used in scientific research for visualizing and analyzing microstructures. Determining the scale bars is an important first step of accurate SEM analysis; however, currently, it mainly relies on manual operations, which is both time-consuming and prone to errors. To address this issue, we propose a multi-modal and automated scale bar detection and extraction framework that provides concurrent object detection, text detection and text recognition with a Large Language Model (LLM) agent. The proposed framework operates in four phases; i) Automatic Dataset Generation (Auto-DG) model to synthesize a diverse dataset of SEM images ensuring robust training and high generalizability of the model, ii) scale bar object detection, iii) information extraction using a hybrid Optical Character Recognition (OCR) system with DenseNet and Convolutional Recurrent Neural Network (CRNN) based algorithms, iv) an LLM agent to analyze and verify accuracy of the results. The proposed model demonstrates a strong performance in object detection and accurate localization with a precision of 100%, recall of 95.8%, and a mean Average Precision (mAP) of 99.2% at IoU=0.5 and 69.1% at IoU=0.5:0.95. The hybrid OCR system achieved 89% precision, 65% recall, and a 75% F1 score on the Auto-DG dataset, significantly outperforming several mainstream standalone engines, highlighting its reliability for scientific image analysis. The LLM is introduced as a reasoning engine as well as an intelligent assistant that suggests follow-up steps and verifies the results. This automated method powered by an LLM agent significantly enhances the efficiency and accuracy of scale bar detection and extraction in SEM images, providing a valuable tool for microscopic analysis and advancing the field of scientific imaging.",
    "categories": [
      "cs.CV",
      "cond-mat.mtrl-sci",
      "cs.AI",
      "physics.data-an"
    ],
    "primary_category": "cs.CV",
    "comment": "14 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.11260v1",
    "published_date": "2025-10-13 10:50:54 UTC",
    "updated_date": "2025-10-13 10:50:54 UTC"
  },
  {
    "arxiv_id": "2510.11251v1",
    "title": "Large Language Models Are Effective Code Watermarkers",
    "authors": [
      "Rui Xu",
      "Jiawei Chen",
      "Zhaoxia Yin",
      "Cong Kong",
      "Xinpeng Zhang"
    ],
    "abstract": "The widespread use of large language models (LLMs) and open-source code has raised ethical and security concerns regarding the distribution and attribution of source code, including unauthorized redistribution, license violations, and misuse of code for malicious purposes. Watermarking has emerged as a promising solution for source attribution, but existing techniques rely heavily on hand-crafted transformation rules, abstract syntax tree (AST) manipulation, or task-specific training, limiting their scalability and generality across languages. Moreover, their robustness against attacks remains limited. To address these limitations, we propose CodeMark-LLM, an LLM-driven watermarking framework that embeds watermark into source code without compromising its semantics or readability. CodeMark-LLM consists of two core components: (i) Semantically Consistent Embedding module that applies functionality-preserving transformations to encode watermark bits, and (ii) Differential Comparison Extraction module that identifies the applied transformations by comparing the original and watermarked code. Leveraging the cross-lingual generalization ability of LLM, CodeMark-LLM avoids language-specific engineering and training pipelines. Extensive experiments across diverse programming languages and attack scenarios demonstrate its robustness, effectiveness, and scalability.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11251v1",
    "published_date": "2025-10-13 10:40:24 UTC",
    "updated_date": "2025-10-13 10:40:24 UTC"
  },
  {
    "arxiv_id": "2510.11243v1",
    "title": "Nepali Sign Language Characters Recognition: Dataset Development and Deep Learning Approaches",
    "authors": [
      "Birat Poudel",
      "Satyam Ghimire",
      "Sijan Bhattarai",
      "Saurav Bhandari",
      "Suramya Sharma Dahal"
    ],
    "abstract": "Sign languages serve as essential communication systems for individuals with hearing and speech impairments. However, digital linguistic dataset resources for underrepresented sign languages, such as Nepali Sign Language (NSL), remain scarce. This study introduces the first benchmark dataset for NSL, consisting of 36 gesture classes with 1,500 samples per class, designed to capture the structural and visual features of the language. To evaluate recognition performance, we fine-tuned MobileNetV2 and ResNet50 architectures on the dataset, achieving classification accuracies of 90.45% and 88.78%, respectively. These findings demonstrate the effectiveness of convolutional neural networks in sign recognition tasks, particularly within low-resource settings. To the best of our knowledge, this work represents the first systematic effort to construct a benchmark dataset and assess deep learning approaches for NSL recognition, highlighting the potential of transfer learning and fine-tuning for advancing research in underexplored sign languages.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "6 pages, 9 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.11243v1",
    "published_date": "2025-10-13 10:29:08 UTC",
    "updated_date": "2025-10-13 10:29:08 UTC"
  },
  {
    "arxiv_id": "2510.15981v1",
    "title": "ProofFlow: A Dependency Graph Approach to Faithful Proof Autoformalization",
    "authors": [
      "Rafael Cabral",
      "Tuan Manh Do",
      "Xuejun Yu",
      "Wai Ming Tai",
      "Zijin Feng",
      "Xin Shen"
    ],
    "abstract": "Proof autoformalization, the task of translating natural language theorems and proofs into machine-verifiable code, is a critical step for integrating large language models into rigorous mathematical workflows. Current approaches focus on producing executable code, but they frequently fail to preserve the semantic meaning and logical structure of the original human-written argument. To address this, we introduce ProofFlow, a novel pipeline that treats structural fidelity as a primary objective. ProofFlow first constructs a directed acyclic graph (DAG) to map the logical dependencies between proof steps. Then, it employs a novel lemma-based approach to systematically formalize each step as an intermediate lemma, preserving the logical structure of the original argument. To facilitate evaluation, we present a new benchmark of 184 undergraduate-level problems, manually annotated with step-by-step solutions and logical dependency graphs, and introduce ProofScore, a new composite metric to evaluate syntactic correctness, semantic faithfulness, and structural fidelity. Experimental results show our pipeline sets a new state-of-the-art for autoformalization, achieving a ProofScore of 0.545, substantially exceeding baselines like full-proof formalization (0.123), which processes the entire proof at once, and step-proof formalization (0.072), which handles each step independently. Our pipeline, benchmark, and score metric are open-sourced to encourage further progress at https://github.com/Huawei-AI4Math/ProofFlow.",
    "categories": [
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15981v1",
    "published_date": "2025-10-13 10:20:11 UTC",
    "updated_date": "2025-10-13 10:20:11 UTC"
  },
  {
    "arxiv_id": "2510.11238v1",
    "title": "Attacks by Content: Automated Fact-checking is an AI Security Issue",
    "authors": [
      "Michael Schlichtkrull"
    ],
    "abstract": "When AI agents retrieve and reason over external documents, adversaries can manipulate the data they receive to subvert their behaviour. Previous research has studied indirect prompt injection, where the attacker injects malicious instructions. We argue that injection of instructions is not necessary to manipulate agents - attackers could instead supply biased, misleading, or false information. We term this an attack by content. Existing defenses, which focus on detecting hidden commands, are ineffective against attacks by content. To defend themselves and their users, agents must critically evaluate retrieved information, corroborating claims with external evidence and evaluating source trustworthiness. We argue that this is analogous to an existing NLP task, automated fact-checking, which we propose to repurpose as a cognitive self-defense tool for agents.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.11238v1",
    "published_date": "2025-10-13 10:18:48 UTC",
    "updated_date": "2025-10-13 10:18:48 UTC"
  },
  {
    "arxiv_id": "2510.11235v1",
    "title": "AI Alignment Strategies from a Risk Perspective: Independent Safety Mechanisms or Shared Failures?",
    "authors": [
      "Leonard Dung",
      "Florian Mai"
    ],
    "abstract": "AI alignment research aims to develop techniques to ensure that AI systems do not cause harm. However, every alignment technique has failure modes, which are conditions in which there is a non-negligible chance that the technique fails to provide safety. As a strategy for risk mitigation, the AI safety community has increasingly adopted a defense-in-depth framework: Conceding that there is no single technique which guarantees safety, defense-in-depth consists in having multiple redundant protections against safety failure, such that safety can be maintained even if some protections fail. However, the success of defense-in-depth depends on how (un)correlated failure modes are across alignment techniques. For example, if all techniques had the exact same failure modes, the defense-in-depth approach would provide no additional protection at all. In this paper, we analyze 7 representative alignment techniques and 7 failure modes to understand the extent to which they overlap. We then discuss our results' implications for understanding the current level of risk and how to prioritize AI alignment research in the future.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "under review",
    "pdf_url": "https://arxiv.org/pdf/2510.11235v1",
    "published_date": "2025-10-13 10:16:59 UTC",
    "updated_date": "2025-10-13 10:16:59 UTC"
  },
  {
    "arxiv_id": "2510.11232v1",
    "title": "LightPneumoNet: Lightweight Pneumonia Classifier",
    "authors": [
      "Neilansh Chauhan",
      "Piyush Kumar Gupta",
      "Faraz Doja"
    ],
    "abstract": "Effective pneumonia diagnosis is often challenged by the difficulty of deploying large, computationally expensive deep learning models in resource-limited settings. This study introduces LightPneumoNet, an efficient, lightweight convolutional neural network (CNN) built from scratch to provide an accessible and accurate diagnostic solution for pneumonia detection from chest X-rays. Our model was trained on a public dataset of 5,856 chest X-ray images. Preprocessing included image resizing to 224x224, grayscale conversion, and pixel normalization, with data augmentation (rotation, zoom, shear) to prevent overfitting. The custom architecture features four blocks of stacked convolutional layers and contains only 388,082 trainable parameters, resulting in a minimal 1.48 MB memory footprint. On the independent test set, our model delivered exceptional performance, achieving an overall accuracy of 0.942, precision of 0.92, and an F1-Score of 0.96. Critically, it obtained a sensitivity (recall) of 0.99, demonstrating a near-perfect ability to identify true pneumonia cases and minimize clinically significant false negatives. Notably, LightPneumoNet achieves this high recall on the same dataset where existing approaches typically require significantly heavier architectures or fail to reach comparable sensitivity levels. The model's efficiency enables deployment on low-cost hardware, making advanced computer-aided diagnosis accessible in underserved clinics and serving as a reliable second-opinion tool to improve patient outcomes.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "13 pages (including references), 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.11232v1",
    "published_date": "2025-10-13 10:14:17 UTC",
    "updated_date": "2025-10-13 10:14:17 UTC"
  },
  {
    "arxiv_id": "2510.11222v1",
    "title": "Fairness Metric Design Exploration in Multi-Domain Moral Sentiment Classification using Transformer-Based Models",
    "authors": [
      "Battemuulen Naranbat",
      "Seyed Sahand Mohammadi Ziabari",
      "Yousuf Nasser Al Husaini",
      "Ali Mohammed Mansoor Alsahag"
    ],
    "abstract": "Ensuring fairness in natural language processing for moral sentiment classification is challenging, particularly under cross-domain shifts where transformer models are increasingly deployed. Using the Moral Foundations Twitter Corpus (MFTC) and Moral Foundations Reddit Corpus (MFRC), this work evaluates BERT and DistilBERT in a multi-label setting with in-domain and cross-domain protocols. Aggregate performance can mask disparities: we observe pronounced asymmetry in transfer, with Twitter->Reddit degrading micro-F1 by 14.9% versus only 1.5% for Reddit->Twitter. Per-label analysis reveals fairness violations hidden by overall scores; notably, the authority label exhibits Demographic Parity Differences of 0.22-0.23 and Equalized Odds Differences of 0.40-0.41. To address this gap, we introduce the Moral Fairness Consistency (MFC) metric, which quantifies the cross-domain stability of moral foundation detection. MFC shows strong empirical validity, achieving a perfect negative correlation with Demographic Parity Difference (rho = -1.000, p < 0.001) while remaining independent of standard performance metrics. Across labels, loyalty demonstrates the highest consistency (MFC = 0.96) and authority the lowest (MFC = 0.78). These findings establish MFC as a complementary, diagnosis-oriented metric for fairness-aware evaluation of moral reasoning models, enabling more reliable deployment across heterogeneous linguistic contexts. .",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11222v1",
    "published_date": "2025-10-13 10:05:57 UTC",
    "updated_date": "2025-10-13 10:05:57 UTC"
  },
  {
    "arxiv_id": "2510.11218v3",
    "title": "The Curious Case of Factual (Mis)Alignment between LLMs' Short- and Long-Form Answers",
    "authors": [
      "Saad Obaid ul Islam",
      "Anne Lauscher",
      "Goran Glavaš"
    ],
    "abstract": "Large language models (LLMs) can correctly answer \"When was Einstein born?\" yet fail to provide the same date when writing about Einstein's life revealing a fundamental inconsistency in how models access factual knowledge across task complexities. While models display impressive accuracy on factual question-answering benchmarks, the reliability gap between simple and complex queries remains poorly understood, eroding their trustworthiness. In this work, we introduce Short-Long Form Alignment for Factual Question Answering (SLAQ), a controlled evaluation framework that compares LLMs' answers to the same factual questions asked (a) in isolation (short) vs. (b) integrated into complex queries (long). Looking at 16 LLMs across 600 queries, we find a systematic misalignment of answers to the corresponding short and long queries. We further uncover position-dependent accuracy loss and momentum effects where consecutive correct or incorrect answers create self-reinforcing patterns. Through mechanistic analysis, we find that aligned facts activate overlapping model internals, and that metrics based on mechanistic similarity can predict short-long answer alignment with up to 78% accuracy. Our work establishes factual consistency over query complexity as an important aspect of LLMs' trustworthiness and challenges current evaluation practices, which implicitly assume that good performance for simple factual queries implies reliability in more complex knowledge-seeking tasks too.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Code: https://github.com/WorldHellow/SLAQ/tree/main",
    "pdf_url": "https://arxiv.org/pdf/2510.11218v3",
    "published_date": "2025-10-13 10:00:58 UTC",
    "updated_date": "2026-01-12 14:54:11 UTC"
  },
  {
    "arxiv_id": "2510.11217v1",
    "title": "Domain-Specific Data Generation Framework for RAG Adaptation",
    "authors": [
      "Chris Xing Tian",
      "Weihao Xie",
      "Zhen Chen",
      "Zhengyuan Yi",
      "Hui Liu",
      "Haoliang Li",
      "Shiqi Wang",
      "Siwei Ma"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) combines the language understanding and reasoning power of large language models (LLMs) with external retrieval to enable domain-grounded responses. Effectively adapting RAG systems to domain-specific settings requires specialized, context-rich training data beyond general-purpose question-answering. Here, we propose RAGen, a scalable and modular framework for generating domain-grounded question-answer-context (QAC) triples tailored to diverse RAG adaptation approaches. RAGen produces these QAC triples by identifying key concepts in documents, generating diverse questions guided by Bloom's Taxonomy-inspired principles, and pairing them with precise answers extracted from relevant contexts. RAGen supports multiple RAG adaptation strategies, including the optimization of key components such as the LLM, retriever, and embedding model, etc. Its modular pipeline features semantic chunking, hierarchical concept extraction, and multi-chunk retrieval, along with the introduction of curated distractor contexts to promote robust reasoning. Designed for scalability, RAGen efficiently handles large and evolving document corpora without redundant processing, making it especially suitable for dynamic evolving domains such as scientific research and enterprise knowledge bases.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11217v1",
    "published_date": "2025-10-13 09:59:49 UTC",
    "updated_date": "2025-10-13 09:59:49 UTC"
  },
  {
    "arxiv_id": "2510.12834v2",
    "title": "Gelina: Unified Speech and Gesture Synthesis via Interleaved Token Prediction",
    "authors": [
      "Téo Guichoux",
      "Théodor Lemerle",
      "Shivam Mehta",
      "Jonas Beskow",
      "Gustav Eje Henter",
      "Laure Soulier",
      "Catherine Pelachaud",
      "Nicolas Obin"
    ],
    "abstract": "Human communication is multimodal, with speech and gestures tightly coupled, yet most computational methods for generating speech and gestures synthesize them sequentially, weakening synchrony and prosody alignment. We introduce Gelina, a unified framework that jointly synthesizes speech and co-speech gestures from text using interleaved token sequences in a discrete autoregressive backbone, with modality-specific decoders. Gelina supports multi-speaker and multi-style cloning and enables gesture-only synthesis from speech inputs. Subjective and objective evaluations demonstrate competitive speech quality and improved gesture generation over unimodal baselines.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "5 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.12834v2",
    "published_date": "2025-10-13 09:51:26 UTC",
    "updated_date": "2025-11-27 13:21:14 UTC"
  },
  {
    "arxiv_id": "2510.11195v1",
    "title": "RAG-Pull: Imperceptible Attacks on RAG Systems for Code Generation",
    "authors": [
      "Vasilije Stambolic",
      "Aritra Dhar",
      "Lukas Cavigelli"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) increases the reliability and trustworthiness of the LLM response and reduces hallucination by eliminating the need for model retraining. It does so by adding external data into the LLM's context. We develop a new class of black-box attack, RAG-Pull, that inserts hidden UTF characters into queries or external code repositories, redirecting retrieval toward malicious code, thereby breaking the models' safety alignment. We observe that query and code perturbations alone can shift retrieval toward attacker-controlled snippets, while combined query-and-target perturbations achieve near-perfect success. Once retrieved, these snippets introduce exploitable vulnerabilities such as remote code execution and SQL injection. RAG-Pull's minimal perturbations can alter the model's safety alignment and increase preference towards unsafe code, therefore opening up a new class of attacks on LLMs.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11195v1",
    "published_date": "2025-10-13 09:27:26 UTC",
    "updated_date": "2025-10-13 09:27:26 UTC"
  },
  {
    "arxiv_id": "2510.11194v1",
    "title": "Aligning Deep Implicit Preferences by Learning to Reason Defensively",
    "authors": [
      "Peiming Li",
      "Zhiyuan Hu",
      "Yang Tang",
      "Shiyu Li",
      "Xi Chen"
    ],
    "abstract": "Personalized alignment is crucial for enabling Large Language Models (LLMs) to engage effectively in user-centric interactions. However, current methods face a dual challenge: they fail to infer users' deep implicit preferences (including unstated goals, semantic context and risk tolerances), and they lack the defensive reasoning required to navigate real-world ambiguity. This cognitive gap leads to responses that are superficial, brittle and short-sighted. To address this, we propose Critique-Driven Reasoning Alignment (CDRA), which reframes alignment from a scalar reward-matching task into a structured reasoning process. First, to bridge the preference inference gap, we introduce the DeepPref benchmark. This dataset, comprising 3000 preference-query pairs across 20 topics, is curated by simulating a multi-faceted cognitive council that produces critique-annotated reasoning chains to deconstruct query semantics and reveal latent risks. Second, to instill defensive reasoning, we introduce the Personalized Generative Process Reward Model (Pers-GenPRM), which frames reward modeling as a personalized reasoning task. It generates a critique chain to evaluate a response's alignment with user preferences before outputting a final score based on this rationale. Ultimately, this interpretable, structured reward signal guides policy model through Critique-Driven Policy Alignment, a process-level online reinforcement learning algorithm integrating both numerical and natural language feedback. Experiments demonstrate that CDRA excels at discovering and aligning with users' true preferences while executing robust reasoning. Our code and dataset are available at https://github.com/Zephyrian-Hugh/Deep-pref.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11194v1",
    "published_date": "2025-10-13 09:26:47 UTC",
    "updated_date": "2025-10-13 09:26:47 UTC"
  },
  {
    "arxiv_id": "2510.11188v1",
    "title": "Protein as a Second Language for LLMs",
    "authors": [
      "Xinhui Chen",
      "Zuchao Li",
      "Mengqi Gao",
      "Yufeng Zhang",
      "Chak Tou Leong",
      "Haoyang Li",
      "Jiaqi Chen"
    ],
    "abstract": "Deciphering the function of unseen protein sequences is a fundamental challenge with broad scientific impact, yet most existing methods depend on task-specific adapters or large-scale supervised fine-tuning. We introduce the \"Protein-as-Second-Language\" framework, which reformulates amino-acid sequences as sentences in a novel symbolic language that large language models can interpret through contextual exemplars. Our approach adaptively constructs sequence-question-answer triples that reveal functional cues in a zero-shot setting, without any further training. To support this process, we curate a bilingual corpus of 79,926 protein-QA instances spanning attribute prediction, descriptive understanding, and extended reasoning. Empirically, our method delivers consistent gains across diverse open-source LLMs and GPT-4, achieving up to 17.2% ROUGE-L improvement (average +7%) and even surpassing fine-tuned protein-specific language models. These results highlight that generic LLMs, when guided with protein-as-language cues, can outperform domain-specialized models, offering a scalable pathway for protein understanding in foundation models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.BM"
    ],
    "primary_category": "cs.LG",
    "comment": "Main paper: 9 pages, 6 figures. With references and appendix: 18 pages, 9 figures total. Submitted to ICLR 2026 (under review)",
    "pdf_url": "https://arxiv.org/pdf/2510.11188v1",
    "published_date": "2025-10-13 09:21:45 UTC",
    "updated_date": "2025-10-13 09:21:45 UTC"
  },
  {
    "arxiv_id": "2510.11182v1",
    "title": "Generalisation of automatic tumour segmentation in histopathological whole-slide images across multiple cancer types",
    "authors": [
      "Ole-Johan Skrede",
      "Manohar Pradhan",
      "Maria Xepapadakis Isaksen",
      "Tarjei Sveinsgjerd Hveem",
      "Ljiljana Vlatkovic",
      "Arild Nesbakken",
      "Kristina Lindemann",
      "Gunnar B Kristensen",
      "Jenneke Kasius",
      "Alain G Zeimet",
      "Odd Terje Brustugun",
      "Lill-Tove Rasmussen Busund",
      "Elin H Richardsen",
      "Erik Skaaheim Haug",
      "Bjørn Brennhovd",
      "Emma Rewcastle",
      "Melinda Lillesand",
      "Vebjørn Kvikstad",
      "Emiel Janssen",
      "David J Kerr",
      "Knut Liestøl",
      "Fritz Albregtsen",
      "Andreas Kleppe"
    ],
    "abstract": "Deep learning is expected to aid pathologists by automating tasks such as tumour segmentation. We aimed to develop one universal tumour segmentation model for histopathological images and examine its performance in different cancer types. The model was developed using over 20 000 whole-slide images from over 4 000 patients with colorectal, endometrial, lung, or prostate carcinoma. Performance was validated in pre-planned analyses on external cohorts with over 3 000 patients across six cancer types. Exploratory analyses included over 1 500 additional patients from The Cancer Genome Atlas. Average Dice coefficient was over 80% in all validation cohorts with en bloc resection specimens and in The Cancer Genome Atlas cohorts. No loss of performance was observed when comparing the universal model with models specialised on single cancer types. In conclusion, extensive and rigorous evaluations demonstrate that generic tumour segmentation by a single model is possible across cancer types, patient populations, sample preparations, and slide scanners.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11182v1",
    "published_date": "2025-10-13 09:18:15 UTC",
    "updated_date": "2025-10-13 09:18:15 UTC"
  },
  {
    "arxiv_id": "2510.11176v3",
    "title": "G2L:From Giga-Scale to Cancer-Specific Large-Scale Pathology Foundation Models via Knowledge Distillation",
    "authors": [
      "Yesung Cho",
      "Sungmin Lee",
      "Geongyu Lee",
      "Minkyung Lee",
      "Jongbae Park",
      "Dongmyung Shin"
    ],
    "abstract": "Recent studies in pathology foundation models have shown that scaling training data, diversifying cancer types, and increasing model size consistently improve their performance. However, giga-scale foundation models, which are trained on hundreds of thousands of slides covering tens of cancer types and contain billions of parameters, pose significant challenges for practical use due to their tremendous computational costs in both development and deployment. In this work, we present a novel strategy, named the G2L framework, to increase the performance of large-scale foundation models, which consist of only $15\\%$ of the parameters of giga-scale models, to a comparable performance level of giga-scale models in cancer-specific tasks. Our approach applies knowledge distillation, transferring the capabilities of a giga-scale model to a large-scale model, using just 1K pathology slides of a target cancer (e.g., breast, prostate, etc.). The resulting distilled model not only outperformed state-of-the-art models of the same size (i.e., large-scale) across several benchmarks but also, interestingly, surpassed the giga-scale teacher and huge-scale models in some benchmarks. In addition, the distilled model exhibited a higher robustness index, indicating improved resilience to image variations originating from multiple institutions. These findings suggest that the proposed distillation approach for a large-scale model is a data- and parameter-efficient way to achieve giga-scale-level performance for cancer-specific applications without prohibitive computational burden.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted in AAAI 2026 workshop in Health Intelligence Special Theme on Foundation Models and AI Agents",
    "pdf_url": "https://arxiv.org/pdf/2510.11176v3",
    "published_date": "2025-10-13 09:08:59 UTC",
    "updated_date": "2026-01-05 04:58:57 UTC"
  },
  {
    "arxiv_id": "2510.11170v1",
    "title": "EAGER: Entropy-Aware GEneRation for Adaptive Inference-Time Scaling",
    "authors": [
      "Daniel Scalena",
      "Leonidas Zotos",
      "Elisabetta Fersini",
      "Malvina Nissim",
      "Ahmet Üstün"
    ],
    "abstract": "With the rise of reasoning language models and test-time scaling methods as a paradigm for improving model performance, substantial computation is often required to generate multiple candidate sequences from the same prompt. This enables exploration of different reasoning paths toward the correct solution, however, allocates the same compute budget for each prompt. Grounded on the assumption that different prompts carry different degrees of complexity, and thus different computation needs, we propose EAGer, a training-free generation method that leverages model uncertainty through token-wise entropy distribution to reduce redundant computation and concurrently improve overall performance. EAGer allows branching to multiple reasoning paths only in the presence of high-entropy tokens, and then reallocates the saved compute budget to the instances where exploration of alternative paths is most needed. We find that across multiple open-source models on complex reasoning benchmarks such as AIME 2025, EAGer can reallocate the budget without accessing target labels, achieving the best efficiency-performance trade-off in terms of reasoning length and Pass@k. When target labels are accessible, EAGer generates up to 65% fewer tokens (hence saving compute) and achieves up to 37% improvement in Pass@k compared to the Full Parallel Sampling.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11170v1",
    "published_date": "2025-10-13 09:04:28 UTC",
    "updated_date": "2025-10-13 09:04:28 UTC"
  },
  {
    "arxiv_id": "2510.15980v1",
    "title": "Cognitive Load Traces as Symbolic and Visual Accounts of Deep Model Cognition",
    "authors": [
      "Dong Liu",
      "Yanxuan Yu"
    ],
    "abstract": "We propose \\textbf{Cognitive Load Traces} (CLTs) as a mid-level interpretability framework for deep models, inspired by Cognitive Load Theory in human cognition. CLTs are defined as symbolic, temporally varying functions that quantify model-internal resource allocation. Formally, we represent CLTs as a three-component stochastic process $(\\mathrm{IL}_t, \\mathrm{EL}_t, \\mathrm{GL}_t)$, corresponding to \\emph{Intrinsic}, \\emph{Extraneous}, and \\emph{Germane} load. Each component is instantiated through measurable proxies such as attention entropy, KV-cache miss ratio, representation dispersion, and decoding stability. We propose both symbolic formulations and visualization methods (load curves, simplex diagrams) that enable interpretable analysis of reasoning dynamics. Experiments on reasoning and planning benchmarks show that CLTs predict error-onset, reveal cognitive strategies, and enable load-guided interventions that improve reasoning efficiency by 15-30\\% while maintaining accuracy.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15980v1",
    "published_date": "2025-10-13 09:04:19 UTC",
    "updated_date": "2025-10-13 09:04:19 UTC"
  },
  {
    "arxiv_id": "2510.11160v1",
    "title": "One Size Does Not Fit All: Exploring Variable Thresholds for Distance-Based Multi-Label Text Classification",
    "authors": [
      "Jens Van Nooten",
      "Andriy Kosar",
      "Guy De Pauw",
      "Walter Daelemans"
    ],
    "abstract": "Distance-based unsupervised text classification is a method within text classification that leverages the semantic similarity between a label and a text to determine label relevance. This method provides numerous benefits, including fast inference and adaptability to expanding label sets, as opposed to zero-shot, few-shot, and fine-tuned neural networks that require re-training in such cases. In multi-label distance-based classification and information retrieval algorithms, thresholds are required to determine whether a text instance is \"similar\" to a label or query. Similarity between a text and label is determined in a dense embedding space, usually generated by state-of-the-art sentence encoders. Multi-label classification complicates matters, as a text instance can have multiple true labels, unlike in multi-class or binary classification, where each instance is assigned only one label. We expand upon previous literature on this underexplored topic by thoroughly examining and evaluating the ability of sentence encoders to perform distance-based classification. First, we perform an exploratory study to verify whether the semantic relationships between texts and labels vary across models, datasets, and label sets by conducting experiments on a diverse collection of realistic multi-label text classification (MLTC) datasets. We find that similarity distributions show statistically significant differences across models, datasets and even label sets. We propose a novel method for optimizing label-specific thresholds using a validation set. Our label-specific thresholding method achieves an average improvement of 46% over normalized 0.5 thresholding and outperforms uniform thresholding approaches from previous work by an average of 14%. Additionally, the method demonstrates strong performance even with limited labeled examples.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11160v1",
    "published_date": "2025-10-13 08:52:14 UTC",
    "updated_date": "2025-10-13 08:52:14 UTC"
  },
  {
    "arxiv_id": "2510.12832v1",
    "title": "Coherent Load Profile Synthesis with Conditional Diffusion for LV Distribution Network Scenario Generation",
    "authors": [
      "Alistair Brash",
      "Junyi Lu",
      "Bruce Stephen",
      "Blair Brown",
      "Robert Atkinson",
      "Craig Michie",
      "Fraser MacIntyre",
      "Christos Tachtatzis"
    ],
    "abstract": "Limited visibility of power distribution network power flows at the low voltage level presents challenges to both distribution network operators from a planning perspective and distribution system operators from a congestion management perspective. Forestalling these challenges through scenario analysis is confounded by the lack of realistic and coherent load data across representative distribution feeders. Load profiling approaches often rely on summarising demand through typical profiles, which oversimplifies the complexity of substation-level operations and limits their applicability in specific power system studies. Sampling methods, and more recently generative models, have attempted to address this through synthesising representative loads from historical exemplars; however, while these approaches can approximate load shapes to a convincing degree of fidelity, the co-behaviour between substations, which ultimately impacts higher voltage level network operation, is often overlooked. This limitation will become even more pronounced with the increasing integration of low-carbon technologies, as estimates of base loads fail to capture load diversity. To address this gap, a Conditional Diffusion model for synthesising daily active and reactive power profiles at the low voltage distribution substation level is proposed. The evaluation of fidelity is demonstrated through conventional metrics capturing temporal and statistical realism, as well as power flow modelling. The results show synthesised load profiles are plausible both independently and as a cohort in a wider power systems context. The Conditional Diffusion model is benchmarked against both naive and state-of-the-art models to demonstrate its effectiveness in producing realistic scenarios on which to base sub-regional power distribution network planning and operations.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.LG",
      "eess.SP"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12832v1",
    "published_date": "2025-10-13 08:40:39 UTC",
    "updated_date": "2025-10-13 08:40:39 UTC"
  },
  {
    "arxiv_id": "2510.11144v1",
    "title": "$How^{2}$: How to learn from procedural How-to questions",
    "authors": [
      "Gautier Dagan",
      "Frank Keller",
      "Alex Lascarides"
    ],
    "abstract": "An agent facing a planning problem can use answers to how-to questions to reduce uncertainty and fill knowledge gaps, helping it solve both current and future tasks. However, their open ended nature, where valid answers to \"How do I X?\" range from executable actions to high-level descriptions of X's sub-goals, makes them challenging for AI agents to ask, and for AI experts to answer, in ways that support efficient planning. We introduce $How^{2}$, a memory agent framework that enables agents to ask how-to questions, store the answers, and reuse them for lifelong learning in interactive environments. We evaluate our approach in Plancraft, a Minecraft crafting environment, where agents must complete an assembly task by manipulating inventory items. Using teacher models that answer at varying levels of abstraction, from executable action sequences to high-level subgoal descriptions, we show that lifelong learning agents benefit most from answers that are abstracted and decoupled from the current state. $How^{2}$ offers a way for LLM-based agents to improve their planning capabilities over time by asking questions in interactive environments.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11144v1",
    "published_date": "2025-10-13 08:35:20 UTC",
    "updated_date": "2025-10-13 08:35:20 UTC"
  },
  {
    "arxiv_id": "2510.11143v1",
    "title": "Spec-Driven AI for Science: The ARIA Framework for Automated and Reproducible Data Analysis",
    "authors": [
      "Chuke Chen",
      "Biao Luo",
      "Nan Li",
      "Boxiang Wang",
      "Hang Yang",
      "Jing Guo",
      "Ming Xu"
    ],
    "abstract": "The rapid expansion of scientific data has widened the gap between analytical capability and research intent. Existing AI-based analysis tools, ranging from AutoML frameworks to agentic research assistants, either favor automation over transparency or depend on manual scripting that hinders scalability and reproducibility. We present ARIA (Automated Research Intelligence Assistant), a spec-driven, human-in-the-loop framework for automated and interpretable data analysis. ARIA integrates six interoperable layers, namely Command, Context, Code, Data, Orchestration, and AI Module, within a document-centric workflow that unifies human reasoning and machine execution. Through natural-language specifications, researchers define analytical goals while ARIA autonomously generates executable code, validates computations, and produces transparent documentation. Beyond achieving high predictive accuracy, ARIA can rapidly identify optimal feature sets and select suitable models, minimizing redundant tuning and repetitive experimentation. In the Boston Housing case, ARIA discovered 25 key features and determined XGBoost as the best performing model (R square = 0.93) with minimal overfitting. Evaluations across heterogeneous domains demonstrate ARIA's strong performance, interpretability, and efficiency compared with state-of-the-art systems. By combining AI for research and AI for science principles within a spec-driven architecture, ARIA establishes a new paradigm for transparent, collaborative, and reproducible scientific discovery.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "19 pages,5 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.11143v1",
    "published_date": "2025-10-13 08:32:43 UTC",
    "updated_date": "2025-10-13 08:32:43 UTC"
  },
  {
    "arxiv_id": "2510.11129v1",
    "title": "video-SALMONN S: Streaming Audio-Visual LLMs Beyond Length Limits via Memory",
    "authors": [
      "Guangzhi Sun",
      "Yixuan Li",
      "Xiaodong Wu",
      "Yudong Yang",
      "Wei Li",
      "Zejun Ma",
      "Chao Zhang"
    ],
    "abstract": "Continuous, high-frame-rate, high-resolution processing of long video streams is critical for future AI agents, yet current video-understanding LLMs struggle to scale. Offline, fixed-frame-number methods require the stream length to adapt frame rates; streaming methods constrain memory by merging or discarding tokens, losing information. We propose video-SALMONN S, a streaming audio-visual LLM that, to our knowledge, is the first to process 3-hour videos at 1 FPS and 360p resolution under a fixed memory budget. Our model introduces (i) a test-time-training (TTT) memory module that continually updates token representations to capture long-range dependencies by replacing token merging, and (ii) a prompt-dependent memory reader that selectively retrieves context-relevant content from fixed-size memory. The TTT module is optimised with a Hessian-free conjugate-gradient procedure (TTT_HF) for efficient adaptation. On long-video benchmarks (Video-MME, LVBench, VideoEvalPro), video-SALMONN S sustains high-quality understanding on multi-hour videos with 10k frames and 1M tokens. Our 8B-parameter model achieves 74.2% overall and 67.8% on the Video-MME long split, outperforming both offline and streaming baselines.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11129v1",
    "published_date": "2025-10-13 08:20:15 UTC",
    "updated_date": "2025-10-13 08:20:15 UTC"
  },
  {
    "arxiv_id": "2510.15979v1",
    "title": "Cog-Rethinker: Hierarchical Metacognitive Reinforcement Learning for LLM Reasoning",
    "authors": [
      "Zexu Sun",
      "Yongcheng Zeng",
      "Erxue Min",
      "Heyang Gao",
      "Bokai Ji",
      "Xu Chen"
    ],
    "abstract": "Contemporary progress in large language models (LLMs) has revealed notable inferential capacities via reinforcement learning (RL) employing verifiable reward, facilitating the development of O1 and R1-like reasoning models. Directly training from base models with RL is called zero-RL. However, previous works rely upon activating LLMs' inherent capacities through fixed prompt templates. This strategy introduces substantial sampling inefficiencies for weak LLMs, as the majority of problems generate invalid outputs during accuracy-driven filtration in reasoning tasks, which causes a waste of samples. To solve this issue, we propose Cog-Rethinker, a novel hierarchical metacognitive RL framework for LLM reasoning. Our Cog-Rethinker mainly focuses on the rollout procedure in RL training. After the direct rollout, our Cog-Rethinker improves sample utilization in a hierarchical metacognitive two-stage framework. By leveraging human cognition during solving problems, firstly, it prompts policy to decompose zero-accuracy problems into subproblems to produce final reasoning results. Secondly, with zero-accuracy problems in previous rollout stage, it further prompts policy to refine these answers by referencing previous wrong solutions. Moreover, to enable cold-start of the two new reasoning patterns and maintain train-test consistency across prompt templates, our Cog-Rethinker applies supervised fine-tuning on the policy using correct samples of the two stages with direct rollout template. Experimental results demonstrate Cog-Rethinker's superior performance on various mathematical reasoning benchmarks, we also analyzed its improved sample efficiency that accelerates convergence compared to baseline methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "22 Pages, 8 figures, 4 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.15979v1",
    "published_date": "2025-10-13 08:16:21 UTC",
    "updated_date": "2025-10-13 08:16:21 UTC"
  },
  {
    "arxiv_id": "2510.11119v1",
    "title": "Improving AI Efficiency in Data Centres by Power Dynamic Response",
    "authors": [
      "Andrea Marinoni",
      "Sai Shivareddy",
      "Pietro Lio'",
      "Weisi Lin",
      "Erik Cambria",
      "Clare Grey"
    ],
    "abstract": "The steady growth of artificial intelligence (AI) has accelerated in the recent years, facilitated by the development of sophisticated models such as large language models and foundation models. Ensuring robust and reliable power infrastructures is fundamental to take advantage of the full potential of AI. However, AI data centres are extremely hungry for power, putting the problem of their power management in the spotlight, especially with respect to their impact on environment and sustainable development. In this work, we investigate the capacity and limits of solutions based on an innovative approach for the power management of AI data centres, i.e., making part of the input power as dynamic as the power used for data-computing functions. The performance of passive and active devices are quantified and compared in terms of computational gain, energy efficiency, reduction of capital expenditure, and management costs by analysing power trends from multiple data platforms worldwide. This strategy, which identifies a paradigm shift in the AI data centre power management, has the potential to strongly improve the sustainability of AI hyperscalers, enhancing their footprint on environmental, financial, and societal fields.",
    "categories": [
      "cs.AI",
      "cs.AR",
      "cs.DC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11119v1",
    "published_date": "2025-10-13 08:08:21 UTC",
    "updated_date": "2025-10-13 08:08:21 UTC"
  },
  {
    "arxiv_id": "2510.11110v1",
    "title": "PhysioME: A Robust Multimodal Self-Supervised Framework for Physiological Signals with Missing Modalities",
    "authors": [
      "Cheol-Hui Lee",
      "Hwa-Yeon Lee",
      "Min-Kyung Jung",
      "Dong-Joo Kim"
    ],
    "abstract": "Missing or corrupted modalities are common in physiological signal-based medical applications owing to hardware constraints or motion artifacts. However, most existing methods assume the availability of all modalities, resulting in substantial performance degradation in the absence of any modality. To overcome this limitation, this study proposes PhysioME, a robust framework designed to ensure reliable performance under missing modality conditions. PhysioME adopts: (1) a multimodal self-supervised learning approach that combines contrastive learning with masked prediction; (2) a Dual-PathNeuroNet backbone tailored to capture the temporal dynamics of each physiological signal modality; and (3) a restoration decoder that reconstructs missing modality tokens, enabling flexible processing of incomplete inputs. The experimental results show that PhysioME achieves high consistency and generalization performance across various missing modality scenarios. These findings highlight the potential of PhysioME as a reliable tool for supporting clinical decision-making in real-world settings with imperfect data availability.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.11110v1",
    "published_date": "2025-10-13 08:00:55 UTC",
    "updated_date": "2025-10-13 08:00:55 UTC"
  },
  {
    "arxiv_id": "2510.13866v1",
    "title": "FFT-Accelerated Auxiliary Variable MCMC for Fermionic Lattice Models: A Determinant-Free Approach with $O(N\\log N)$ Complexity",
    "authors": [
      "Deqian Kong",
      "Shi Feng",
      "Jianwen Xie",
      "Ying Nian Wu"
    ],
    "abstract": "We introduce a Markov Chain Monte Carlo (MCMC) algorithm that dramatically accelerates the simulation of quantum many-body systems, a grand challenge in computational science. State-of-the-art methods for these problems are severely limited by $O(N^3)$ computational complexity. Our method avoids this bottleneck, achieving near-linear $O(N \\log N)$ scaling per sweep.\n  Our approach samples a joint probability measure over two coupled variable sets: (1) particle trajectories of the fundamental fermions, and (2) auxiliary variables that decouple fermion interactions. The key innovation is a novel transition kernel for particle trajectories formulated in the Fourier domain, revealing the transition probability as a convolution that enables massive acceleration via the Fast Fourier Transform (FFT). The auxiliary variables admit closed-form, factorized conditional distributions, enabling efficient exact Gibbs sampling update.\n  We validate our algorithm on benchmark quantum physics problems, accurately reproducing known theoretical results and matching traditional $O(N^3)$ algorithms on $32\\times 32$ lattice simulations at a fraction of the wall-clock time, empirically demonstrating $N \\log N$ scaling. By reformulating a long-standing physics simulation problem in machine learning language, our work provides a powerful tool for large-scale probabilistic inference and opens avenues for physics-inspired generative models.",
    "categories": [
      "cond-mat.str-el",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cond-mat.str-el",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.13866v1",
    "published_date": "2025-10-13 07:57:21 UTC",
    "updated_date": "2025-10-13 07:57:21 UTC"
  },
  {
    "arxiv_id": "2510.11108v2",
    "title": "A Vision for Access Control in LLM-based Agent Systems",
    "authors": [
      "Xinfeng Li",
      "Dong Huang",
      "Jie Li",
      "Hongyi Cai",
      "Zhenhong Zhou",
      "Wei Dong",
      "XiaoFeng Wang",
      "Yang Liu"
    ],
    "abstract": "The autonomy and contextual complexity of LLM-based agents render traditional access control (AC) mechanisms insufficient. Static, rule-based systems designed for predictable environments are fundamentally ill-equipped to manage the dynamic information flows inherent in agentic interactions. This position paper argues for a paradigm shift from binary access control to a more sophisticated model of information governance, positing that the core challenge is not merely about permission, but about governing the flow of information. We introduce Agent Access Control (AAC), a novel framework that reframes AC as a dynamic, context-aware process of information flow governance. AAC operates on two core modules: (1) multi-dimensional contextual evaluation, which assesses not just identity but also relationships, scenarios, and norms; and (2) adaptive response formulation, which moves beyond simple allow/deny decisions to shape information through redaction, summarization, and paraphrasing. This vision, powered by a dedicated AC reasoning engine, aims to bridge the gap between human-like nuanced judgment and scalable Al safety, proposing a new conceptual lens for future research in trustworthy agent design.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.MA",
    "comment": "11 pages, 1 figure",
    "pdf_url": "https://arxiv.org/pdf/2510.11108v2",
    "published_date": "2025-10-13 07:57:09 UTC",
    "updated_date": "2025-10-19 04:48:38 UTC"
  },
  {
    "arxiv_id": "2510.13865v5",
    "title": "Deep Edge Filter: Return of the Human-Crafted Layer in Deep Learning",
    "authors": [
      "Dongkwan Lee",
      "Junhoo Lee",
      "Nojun Kwak"
    ],
    "abstract": "We introduce the Deep Edge Filter, a novel approach that applies high-pass filtering to deep neural network features to improve model generalizability. Our method is motivated by our hypothesis that neural networks encode task-relevant semantic information in high-frequency components while storing domain-specific biases in low-frequency components of deep features. By subtracting low-pass filtered outputs from original features, our approach isolates generalizable representations while preserving architectural integrity. Experimental results across diverse domains such as Vision, Text, 3D, and Audio demonstrate consistent performance improvements regardless of model architecture and data modality. Analysis reveals that our method induces feature sparsification and effectively isolates high-frequency components, providing empirical validation of our core hypothesis. The code is available at https://github.com/dongkwani/DeepEdgeFilter.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS2025",
    "pdf_url": "https://arxiv.org/pdf/2510.13865v5",
    "published_date": "2025-10-13 07:56:55 UTC",
    "updated_date": "2025-12-10 02:00:37 UTC"
  },
  {
    "arxiv_id": "2510.11104v1",
    "title": "Enhancing LLM Reasoning via Non-Human-Like Reasoning Path Preference Optimization",
    "authors": [
      "Junjie Lu",
      "Yuliang Liu",
      "Chaofeng Qu",
      "Wei Shen",
      "Zhouhan Lin",
      "Min Xu"
    ],
    "abstract": "Current approaches for strengthening LLM reasoning tend to introduce a training bias toward human-like reasoning trajectories. In step-wise preference optimization, in particular, dependence on human or higher-capacity model annotations for intermediate steps limits exploration of alternative, non-human-like reasoning paths and thus constrains achievable performance. Furthermore, through a small-scale pilot study, we observed that in approximately 75% of cases, the model's first erroneous step occurs after the lowest-confidence point. This suggests that guiding the model at its lowest-confidence point before an error provides more accurate supervision than locating the first explicit error. In this paper, we propose Confidence-Guided Reasoning Path Preference Optimization (CGPO), a method that leverages a confidence signal to identify points of maximal uncertainty in the model's reasoning process and applies self-generated, non-human-like reasoning-path guidance to mitigate trajectory drift. Our experiments span diverse models applied to both code and mathematical reasoning tasks. The results show that, with the same amount of training data, our method using data generated by a small model can achieve better performance in most cases compared with approaches using data generated by a strong model or human-annotated.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "13 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.11104v1",
    "published_date": "2025-10-13 07:51:16 UTC",
    "updated_date": "2025-10-13 07:51:16 UTC"
  },
  {
    "arxiv_id": "2510.11103v1",
    "title": "A Primer on SO(3) Action Representations in Deep Reinforcement Learning",
    "authors": [
      "Martin Schuck",
      "Sherif Samy",
      "Angela P. Schoellig"
    ],
    "abstract": "Many robotic control tasks require policies to act on orientations, yet the geometry of SO(3) makes this nontrivial. Because SO(3) admits no global, smooth, minimal parameterization, common representations such as Euler angles, quaternions, rotation matrices, and Lie algebra coordinates introduce distinct constraints and failure modes. While these trade-offs are well studied for supervised learning, their implications for actions in reinforcement learning remain unclear. We systematically evaluate SO(3) action representations across three standard continuous control algorithms, PPO, SAC, and TD3, under dense and sparse rewards. We compare how representations shape exploration, interact with entropy regularization, and affect training stability through empirical studies and analyze the implications of different projections for obtaining valid rotations from Euclidean network outputs. Across a suite of robotics benchmarks, we quantify the practical impact of these choices and distill simple, implementation-ready guidelines for selecting and using rotation actions. Our results highlight that representation-induced geometry strongly influences exploration and optimization and show that representing actions as tangent vectors in the local frame yields the most reliable results across algorithms.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11103v1",
    "published_date": "2025-10-13 07:49:21 UTC",
    "updated_date": "2025-10-13 07:49:21 UTC"
  },
  {
    "arxiv_id": "2510.11100v2",
    "title": "HoMer: Addressing Heterogeneities by Modeling Sequential and Set-wise Contexts for CTR Prediction",
    "authors": [
      "Shuwei Chen",
      "Jiajun Cui",
      "Zhengqi Xu",
      "Fan Zhang",
      "Jiangke Fan",
      "Teng Zhang",
      "Xingxing Wang"
    ],
    "abstract": "Click-through rate (CTR) prediction, which models behavior sequence and non-sequential features (e.g., user/item profiles or cross features) to infer user interest, underpins industrial recommender systems. However, most methods face three forms of heterogeneity that degrade predictive performance: (i) Feature Heterogeneity persists when limited sequence side features provide less granular interest representation compared to extensive non-sequential features, thereby impairing sequence modeling performance; (ii) Context Heterogeneity arises because a user's interest in an item will be influenced by other items, yet point-wise prediction neglects cross-item interaction context from the entire item set; (iii) Architecture Heterogeneity stems from the fragmented integration of specialized network modules, which compounds the model's effectiveness, efficiency and scalability in industrial deployments. To tackle the above limitations, we propose HoMer, a Homogeneous-Oriented TransforMer for modeling sequential and set-wise contexts. First, we align sequence side features with non-sequential features for accurate sequence modeling and fine-grained interest representation. Second, we shift the prediction paradigm from point-wise to set-wise, facilitating cross-item interaction in a highly parallel manner. Third, HoMer's unified encoder-decoder architecture achieves dual optimization through structural simplification and shared computation, ensuring computational efficiency while maintaining scalability with model size. Without arduous modification to the prediction pipeline, HoMer successfully scales up and outperforms our industrial baseline by 0.0099 in the AUC metric, and enhances online business metrics like CTR/RPM by 1.99%/2.46%. Additionally, HoMer saves 27% of GPU resources via preliminary engineering optimization, further validating its superiority and practicality.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "10 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.11100v2",
    "published_date": "2025-10-13 07:47:03 UTC",
    "updated_date": "2025-10-23 11:35:48 UTC"
  },
  {
    "arxiv_id": "2510.11091v1",
    "title": "Text-Enhanced Panoptic Symbol Spotting in CAD Drawings",
    "authors": [
      "Xianlin Liu",
      "Yan Gong",
      "Bohao Li",
      "Jiajing Huang",
      "Bowen Du",
      "Junchen Ye",
      "Liyan Xu"
    ],
    "abstract": "With the widespread adoption of Computer-Aided Design(CAD) drawings in engineering, architecture, and industrial design, the ability to accurately interpret and analyze these drawings has become increasingly critical. Among various subtasks, panoptic symbol spotting plays a vital role in enabling downstream applications such as CAD automation and design retrieval. Existing methods primarily focus on geometric primitives within the CAD drawings to address this task, but they face following major problems: they usually overlook the rich textual annotations present in CAD drawings and they lack explicit modeling of relationships among primitives, resulting in incomprehensive understanding of the holistic drawings. To fill this gap, we propose a panoptic symbol spotting framework that incorporates textual annotations. The framework constructs unified representations by jointly modeling geometric and textual primitives. Then, using visual features extract by pretrained CNN as the initial representations, a Transformer-based backbone is employed, enhanced with a type-aware attention mechanism to explicitly model the different types of spatial dependencies between various primitives. Extensive experiments on the real-world dataset demonstrate that the proposed method outperforms existing approaches on symbol spotting tasks involving textual annotations, and exhibits superior robustness when applied to complex CAD drawings.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "7 pages, 3figures. This version is the original submitted manuscript of the paper accepted by The 12th International Conference on Behavioural and Social Computing",
    "pdf_url": "https://arxiv.org/pdf/2510.11091v1",
    "published_date": "2025-10-13 07:41:15 UTC",
    "updated_date": "2025-10-13 07:41:15 UTC"
  },
  {
    "arxiv_id": "2510.24732v1",
    "title": "Flows, straight but not so fast: Exploring the design space of Rectified Flows in Protein Design",
    "authors": [
      "Junhua Chen",
      "Simon Mathis",
      "Charles Harris",
      "Kieran Didi",
      "Pietro Lio"
    ],
    "abstract": "Generative modeling techniques such as Diffusion and Flow Matching have achieved significant successes in generating designable and diverse protein backbones. However, many current models are computationally expensive, requiring hundreds or even thousands of function evaluations (NFEs) to yield samples of acceptable quality, which can become a bottleneck in practical design campaigns that often generate $10^4\\ -\\ 10^6$ designs per target. In image generation, Rectified Flows (ReFlow) can significantly reduce the required NFEs for a given target quality, but their application in protein backbone generation has been less studied. We apply ReFlow to improve the low NFE performance of pretrained SE(3) flow matching models for protein backbone generation and systematically study ReFlow design choices in the context of protein generation in data curation, training and inference time settings. In particular, we (1) show that ReFlow in the protein domain is particularly sensitive to the choice of coupling generation and annealing, (2) demonstrate how useful design choices for ReFlow in the image domain do not directly translate to better performance on proteins, and (3) make improvements to ReFlow methodology for proteins.",
    "categories": [
      "q-bio.BM",
      "cs.AI"
    ],
    "primary_category": "q-bio.BM",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.24732v1",
    "published_date": "2025-10-13 07:36:40 UTC",
    "updated_date": "2025-10-13 07:36:40 UTC"
  },
  {
    "arxiv_id": "2510.11090v1",
    "title": "Source-Free Object Detection with Detection Transformer",
    "authors": [
      "Huizai Yao",
      "Sicheng Zhao",
      "Shuo Lu",
      "Hui Chen",
      "Yangyang Li",
      "Guoping Liu",
      "Tengfei Xing",
      "Chenggang Yan",
      "Jianhua Tao",
      "Guiguang Ding"
    ],
    "abstract": "Source-Free Object Detection (SFOD) enables knowledge transfer from a source domain to an unsupervised target domain for object detection without access to source data. Most existing SFOD approaches are either confined to conventional object detection (OD) models like Faster R-CNN or designed as general solutions without tailored adaptations for novel OD architectures, especially Detection Transformer (DETR). In this paper, we introduce Feature Reweighting ANd Contrastive Learning NetworK (FRANCK), a novel SFOD framework specifically designed to perform query-centric feature enhancement for DETRs. FRANCK comprises four key components: (1) an Objectness Score-based Sample Reweighting (OSSR) module that computes attention-based objectness scores on multi-scale encoder feature maps, reweighting the detection loss to emphasize less-recognized regions; (2) a Contrastive Learning with Matching-based Memory Bank (CMMB) module that integrates multi-level features into memory banks, enhancing class-wise contrastive learning; (3) an Uncertainty-weighted Query-fused Feature Distillation (UQFD) module that improves feature distillation through prediction quality reweighting and query feature fusion; and (4) an improved self-training pipeline with a Dynamic Teacher Updating Interval (DTUI) that optimizes pseudo-label quality. By leveraging these components, FRANCK effectively adapts a source-pre-trained DETR model to a target domain with enhanced robustness and generalization. Extensive experiments on several widely used benchmarks demonstrate that our method achieves state-of-the-art performance, highlighting its effectiveness and compatibility with DETR-based SFOD models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "IEEE Transactions on Image Processing",
    "pdf_url": "https://arxiv.org/pdf/2510.11090v1",
    "published_date": "2025-10-13 07:35:04 UTC",
    "updated_date": "2025-10-13 07:35:04 UTC"
  },
  {
    "arxiv_id": "2510.11085v1",
    "title": "Modeling AI-Driven Production and Competitiveness A Multi-Agent Economic Simulation of China and the United States",
    "authors": [
      "Yuxinyue Qian",
      "Jun Liu"
    ],
    "abstract": "With the rapid development of artificial intelligence (AI) technology, socio-economic systems are entering a new stage of \"human-AI co-creation.\" Building upon a previously established multi-level intelligent agent economic model, this paper conducts simulation-based comparisons of macroeconomic output evolution in China and the United States under different mechanisms-AI collaboration, network effects, and AI autonomous production. The results show that: (1) when AI functions as an independent productive entity, the overall growth rate of social output far exceeds that of traditional human-labor-based models; (2) China demonstrates clear potential for acceleration in both the expansion of intelligent agent populations and the pace of technological catch-up, offering the possibility of achieving technological convergence or even partial surpassing. This study provides a systematic, model-based analytical framework for understanding AI-driven production system transformation and shifts in international competitiveness, as well as quantitative insights for relevant policy formulation.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11085v1",
    "published_date": "2025-10-13 07:28:14 UTC",
    "updated_date": "2025-10-13 07:28:14 UTC"
  },
  {
    "arxiv_id": "2510.11084v1",
    "title": "Causal Disentanglement Learning for Accurate Anomaly Detection in Multivariate Time Series",
    "authors": [
      "Wonah Kim",
      "Jeonghyeon Park",
      "Dongsan Jun",
      "Jungkyu Han",
      "Sejin Chun"
    ],
    "abstract": "Disentangling complex causal relationships is important for accurate detection of anomalies. In multivariate time series analysis, dynamic interactions among data variables over time complicate the interpretation of causal relationships. Traditional approaches assume statistical independence between variables in unsupervised settings, whereas recent methods capture feature correlations through graph representation learning. However, their representations fail to explicitly infer the causal relationships over different time periods. To solve the problem, we propose Causally Disentangled Representation Learning for Anomaly Detection (CDRL4AD) to detect anomalies and identify their causal relationships in multivariate time series. First, we design the causal process as model input, the temporal heterogeneous graph, and causal relationships. Second, our representation identifies causal relationships over different time periods and disentangles latent variables to infer the corresponding causal factors. Third, our experiments on real-world datasets demonstrate that CDRL4AD outperforms state-of-the-art methods in terms of accuracy and root cause analysis. Fourth, our model analysis validates hyperparameter sensitivity and the time complexity of CDRL4AD. Last, we conduct a case study to show how our approach assists human experts in diagnosing the root causes of anomalies.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "20 pages, 4 Figures,",
    "pdf_url": "https://arxiv.org/pdf/2510.11084v1",
    "published_date": "2025-10-13 07:26:20 UTC",
    "updated_date": "2025-10-13 07:26:20 UTC"
  },
  {
    "arxiv_id": "2510.11083v1",
    "title": "Flow Matching-Based Autonomous Driving Planning with Advanced Interactive Behavior Modeling",
    "authors": [
      "Tianyi Tan",
      "Yinan Zheng",
      "Ruiming Liang",
      "Zexu Wang",
      "Kexin Zheng",
      "Jinliang Zheng",
      "Jianxiong Li",
      "Xianyuan Zhan",
      "Jingjing Liu"
    ],
    "abstract": "Modeling interactive driving behaviors in complex scenarios remains a fundamental challenge for autonomous driving planning. Learning-based approaches attempt to address this challenge with advanced generative models, removing the dependency on over-engineered architectures for representation fusion. However, brute-force implementation by simply stacking transformer blocks lacks a dedicated mechanism for modeling interactive behaviors that are common in real driving scenarios. The scarcity of interactive driving data further exacerbates this problem, leaving conventional imitation learning methods ill-equipped to capture high-value interactive behaviors. We propose Flow Planner, which tackles these problems through coordinated innovations in data modeling, model architecture, and learning scheme. Specifically, we first introduce fine-grained trajectory tokenization, which decomposes the trajectory into overlapping segments to decrease the complexity of whole trajectory modeling. With a sophisticatedly designed architecture, we achieve efficient temporal and spatial fusion of planning and scene information, to better capture interactive behaviors. In addition, the framework incorporates flow matching with classifier-free guidance for multi-modal behavior generation, which dynamically reweights agent interactions during inference to maintain coherent response strategies, providing a critical boost for interactive scenario understanding. Experimental results on the large-scale nuPlan dataset and challenging interactive interPlan dataset demonstrate that Flow Planner achieves state-of-the-art performance among learning-based approaches while effectively modeling interactive behaviors in complex driving scenarios.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "26 pages, 6 figures. Accepted at NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.11083v1",
    "published_date": "2025-10-13 07:25:13 UTC",
    "updated_date": "2025-10-13 07:25:13 UTC"
  },
  {
    "arxiv_id": "2510.11079v1",
    "title": "Argumentation-Based Explainability for Legal AI: Comparative and Regulatory Perspectives",
    "authors": [
      "Andrada Iulia Prajescu",
      "Roberto Confalonieri"
    ],
    "abstract": "Artificial Intelligence (AI) systems are increasingly deployed in legal contexts, where their opacity raises significant challenges for fairness, accountability, and trust. The so-called ``black box problem'' undermines the legitimacy of automated decision-making, as affected individuals often lack access to meaningful explanations. In response, the field of Explainable AI (XAI) has proposed a variety of methods to enhance transparency, ranging from example-based and rule-based techniques to hybrid and argumentation-based approaches. This paper promotes computational models of arguments and their role in providing legally relevant explanations, with particular attention to their alignment with emerging regulatory frameworks such as the EU General Data Protection Regulation (GDPR) and the Artificial Intelligence Act (AIA). We analyze the strengths and limitations of different explanation strategies, evaluate their applicability to legal reasoning, and highlight how argumentation frameworks -- by capturing the defeasible, contestable, and value-sensitive nature of law -- offer a particularly robust foundation for explainable legal AI. Finally, we identify open challenges and research directions, including bias mitigation, empirical validation in judicial settings, and compliance with evolving ethical and legal standards, arguing that computational argumentation is best positioned to meet both technical and normative requirements of transparency in the law domain.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11079v1",
    "published_date": "2025-10-13 07:19:15 UTC",
    "updated_date": "2025-10-13 07:19:15 UTC"
  },
  {
    "arxiv_id": "2510.11072v1",
    "title": "PhysHSI: Towards a Real-World Generalizable and Natural Humanoid-Scene Interaction System",
    "authors": [
      "Huayi Wang",
      "Wentao Zhang",
      "Runyi Yu",
      "Tao Huang",
      "Junli Ren",
      "Feiyu Jia",
      "Zirui Wang",
      "Xiaojie Niu",
      "Xiao Chen",
      "Jiahe Chen",
      "Qifeng Chen",
      "Jingbo Wang",
      "Jiangmiao Pang"
    ],
    "abstract": "Deploying humanoid robots to interact with real-world environments--such as carrying objects or sitting on chairs--requires generalizable, lifelike motions and robust scene perception. Although prior approaches have advanced each capability individually, combining them in a unified system is still an ongoing challenge. In this work, we present a physical-world humanoid-scene interaction system, PhysHSI, that enables humanoids to autonomously perform diverse interaction tasks while maintaining natural and lifelike behaviors. PhysHSI comprises a simulation training pipeline and a real-world deployment system. In simulation, we adopt adversarial motion prior-based policy learning to imitate natural humanoid-scene interaction data across diverse scenarios, achieving both generalization and lifelike behaviors. For real-world deployment, we introduce a coarse-to-fine object localization module that combines LiDAR and camera inputs to provide continuous and robust scene perception. We validate PhysHSI on four representative interactive tasks--box carrying, sitting, lying, and standing up--in both simulation and real-world settings, demonstrating consistently high success rates, strong generalization across diverse task goals, and natural motion patterns.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "Project website: https://why618188.github.io/physhsi/",
    "pdf_url": "https://arxiv.org/pdf/2510.11072v1",
    "published_date": "2025-10-13 07:11:37 UTC",
    "updated_date": "2025-10-13 07:11:37 UTC"
  },
  {
    "arxiv_id": "2510.11057v1",
    "title": "Temporal Alignment Guidance: On-Manifold Sampling in Diffusion Models",
    "authors": [
      "Youngrok Park",
      "Hojung Jung",
      "Sangmin Bae",
      "Se-Young Yun"
    ],
    "abstract": "Diffusion models have achieved remarkable success as generative models. However, even a well-trained model can accumulate errors throughout the generation process. These errors become particularly problematic when arbitrary guidance is applied to steer samples toward desired properties, which often breaks sample fidelity. In this paper, we propose a general solution to address the off-manifold phenomenon observed in diffusion models. Our approach leverages a time predictor to estimate deviations from the desired data manifold at each timestep, identifying that a larger time gap is associated with reduced generation quality. We then design a novel guidance mechanism, `Temporal Alignment Guidance' (TAG), attracting the samples back to the desired manifold at every timestep during generation. Through extensive experiments, we demonstrate that TAG consistently produces samples closely aligned with the desired manifold at each timestep, leading to significant improvements in generation quality across various downstream tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "54 pages, 17 figures, 18 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.11057v1",
    "published_date": "2025-10-13 06:46:57 UTC",
    "updated_date": "2025-10-13 06:46:57 UTC"
  },
  {
    "arxiv_id": "2510.11056v2",
    "title": "From Reasoning LLMs to BERT: A Two-Stage Distillation Framework for Search Relevance",
    "authors": [
      "Runze Xia",
      "Yupeng Ji",
      "Yuxi Zhou",
      "Haodong Liu",
      "Teng Zhang",
      "Piji Li"
    ],
    "abstract": "Query-service relevance prediction in e-commerce search systems faces strict latency requirements that prevent the direct application of Large Language Models (LLMs). To bridge this gap, we propose a two-stage reasoning distillation framework to transfer reasoning capabilities from a powerful teacher LLM to a lightweight, deployment-friendly student model. In the first stage, we address the limitations of general-purpose LLMs by constructing a domain-adapted teacher model. This is achieved through a three-step process: domain-adaptive pre-training to inject platform knowledge, supervised fine-tuning to elicit reasoning skills, and preference optimization with a multi-dimensional reward model to ensure the generation of reliable and preference-aligned reasoning paths. This teacher can then automatically annotate massive query-service pairs from search logs with both relevance labels and reasoning chains. In the second stage, to address the challenges of architectural heterogeneity in standard distillation, we introduce Contrastive Reasoning Self-Distillation (CRSD). By modeling the behavior of the same student model under \"standard\" and \"reasoning-augmented\" inputs as a teacher-student relationship, CRSD enables the lightweight model to internalize the teacher's complex decision-making mechanisms without needing the explicit reasoning path at inference. Offline evaluations and online A/B testing in the Meituan search advertising system demonstrate that our framework achieves significant improvements across multiple metrics, validating its effectiveness and practical value.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11056v2",
    "published_date": "2025-10-13 06:46:43 UTC",
    "updated_date": "2025-11-18 02:47:52 UTC"
  },
  {
    "arxiv_id": "2510.11036v1",
    "title": "XGrasp: Gripper-Aware Grasp Detection with Multi-Gripper Data Generation",
    "authors": [
      "Yeonseo Lee",
      "Jungwook Mun",
      "Hyosup Shin",
      "Guebin Hwang",
      "Junhee Nam",
      "Taeyeop Lee",
      "Sungho Jo"
    ],
    "abstract": "Most robotic grasping methods are typically designed for single gripper types, which limits their applicability in real-world scenarios requiring diverse end-effectors. We propose XGrasp, a real-time gripper-aware grasp detection framework that efficiently handles multiple gripper configurations. The proposed method addresses data scarcity by systematically augmenting existing datasets with multi-gripper annotations. XGrasp employs a hierarchical two-stage architecture. In the first stage, a Grasp Point Predictor (GPP) identifies optimal locations using global scene information and gripper specifications. In the second stage, an Angle-Width Predictor (AWP) refines the grasp angle and width using local features. Contrastive learning in the AWP module enables zero-shot generalization to unseen grippers by learning fundamental grasping characteristics. The modular framework integrates seamlessly with vision foundation models, providing pathways for future vision-language capabilities. The experimental results demonstrate competitive grasp success rates across various gripper types, while achieving substantial improvements in inference speed compared to existing gripper-aware methods. Project page: https://sites.google.com/view/xgrasp",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11036v1",
    "published_date": "2025-10-13 06:13:25 UTC",
    "updated_date": "2025-10-13 06:13:25 UTC"
  },
  {
    "arxiv_id": "2510.13864v1",
    "title": "Self-Training with Dynamic Weighting for Robust Gradual Domain Adaptation",
    "authors": [
      "Zixi Wang",
      "Yushe Cao",
      "Yubo Huang",
      "Jinzhu Wei",
      "Jingzehua Xu",
      "Shuai Zhang",
      "Xin Lai"
    ],
    "abstract": "In this paper, we propose a new method called Self-Training with Dynamic Weighting (STDW), which aims to enhance robustness in Gradual Domain Adaptation (GDA) by addressing the challenge of smooth knowledge migration from the source to the target domain. Traditional GDA methods mitigate domain shift through intermediate domains and self-training but often suffer from inefficient knowledge migration or incomplete intermediate data. Our approach introduces a dynamic weighting mechanism that adaptively balances the loss contributions of the source and target domains during training. Specifically, we design an optimization framework governed by a time-varying hyperparameter $\\varrho$ (progressing from 0 to 1), which controls the strength of domain-specific learning and ensures stable adaptation. The method leverages self-training to generate pseudo-labels and optimizes a weighted objective function for iterative model updates, maintaining robustness across intermediate domains. Experiments on rotated MNIST, color-shifted MNIST, portrait datasets, and the Cover Type dataset demonstrate that STDW outperforms existing baselines. Ablation studies further validate the critical role of $\\varrho$'s dynamic scheduling in achieving progressive adaptation, confirming its effectiveness in reducing domain bias and improving generalization. This work provides both theoretical insights and a practical framework for robust gradual domain adaptation, with potential applications in dynamic real-world scenarios. The code is available at https://github.com/Dramwig/STDW.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "It had formerly appeared as arXiv:2501.19159v2 in error. Accepted by NIPS 25",
    "pdf_url": "https://arxiv.org/pdf/2510.13864v1",
    "published_date": "2025-10-13 06:13:23 UTC",
    "updated_date": "2025-10-13 06:13:23 UTC"
  },
  {
    "arxiv_id": "2510.11020v1",
    "title": "GeoVLMath: Enhancing Geometry Reasoning in Vision-Language Models via Cross-Modal Reward for Auxiliary Line Creation",
    "authors": [
      "Shasha Guo",
      "Liang Pang",
      "Xi Wang",
      "Yanling Wang",
      "Huawei Shen",
      "Jing Zhang"
    ],
    "abstract": "Auxiliary lines are essential for solving complex geometric problems but remain challenging for large vision-language models (LVLMs). Rather than editing diagrams to draw auxiliary lines, which current image editing models struggle to render with geometric precision, we generate textual descriptions of auxiliary-line constructions to better align with the representational strengths of LVLMs. To bridge the gap between textual descriptions and spatial structure, we propose a reinforcement learning framework that enhances diagram-text alignment. At the core of our approach is a cross-modal reward that evaluates how well the generated auxiliary-line description for an original diagram matches a ground-truth auxiliary-line diagram. Built on this reward, we present GeoVLMath, an open-source LVLM tailored to auxiliary-line reasoning in solid geometry. This fine-grained signal drives a GRPO-based RL stage, yielding precise diagram-text alignment. To support training, we develop a scalable data creation pipeline and construct AuxSolidMath, a dataset of 3,018 real-exam geometry problems with paired diagrams and aligned textual fields. At the 3B and 7B scales, GeoVLMath achieves competitive and often superior performance compared with strong open-source and proprietary LVLMs on auxiliary-line reasoning benchmarks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "22 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.11020v1",
    "published_date": "2025-10-13 05:33:51 UTC",
    "updated_date": "2025-10-13 05:33:51 UTC"
  },
  {
    "arxiv_id": "2510.11014v1",
    "title": "Into the Unknown: Towards using Generative Models for Sampling Priors of Environment Uncertainty for Planning in Configuration Spaces",
    "authors": [
      "Subhransu S. Bhattacharjee",
      "Hao Lu",
      "Dylan Campbell",
      "Rahul Shome"
    ],
    "abstract": "Priors are vital for planning under partial observability, yet difficult to obtain in practice. We present a sampling-based pipeline that leverages large-scale pretrained generative models to produce probabilistic priors capturing environmental uncertainty and spatio-semantic relationships in a zero-shot manner. Conditioned on partial observations, the pipeline recovers complete RGB-D point cloud samples with occupancy and target semantics, formulated to be directly useful in configuration-space planning. We establish a Matterport3D benchmark of rooms partially visible through doorways, where a robot must navigate to an unobserved target object. Effective priors for this setting must represent both occupancy and target-location uncertainty in unobserved regions. Experiments show that our approach recovers commonsense spatial semantics consistent with ground truth, yielding diverse, clean 3D point clouds usable in motion planning, highlight the promise of generative models as a rich source of priors for robotic planning.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "Under Review",
    "pdf_url": "https://arxiv.org/pdf/2510.11014v1",
    "published_date": "2025-10-13 05:08:48 UTC",
    "updated_date": "2025-10-13 05:08:48 UTC"
  },
  {
    "arxiv_id": "2510.11760v1",
    "title": "Audio-Guided Visual Perception for Audio-Visual Navigation",
    "authors": [
      "Yi Wang",
      "Yinfeng Yu",
      "Fuchun Sun",
      "Liejun Wang",
      "Wendong Zheng"
    ],
    "abstract": "Audio-Visual Embodied Navigation aims to enable agents to autonomously navigate to sound sources in unknown 3D environments using auditory cues. While current AVN methods excel on in-distribution sound sources, they exhibit poor cross-source generalization: navigation success rates plummet and search paths become excessively long when agents encounter unheard sounds or unseen environments. This limitation stems from the lack of explicit alignment mechanisms between auditory signals and corresponding visual regions. Policies tend to memorize spurious \\enquote{acoustic fingerprint-scenario} correlations during training, leading to blind exploration when exposed to novel sound sources. To address this, we propose the AGVP framework, which transforms sound from policy-memorable acoustic fingerprint cues into spatial guidance. The framework first extracts global auditory context via audio self-attention, then uses this context as queries to guide visual feature attention, highlighting sound-source-related regions at the feature level. Subsequent temporal modeling and policy optimization are then performed. This design, centered on interpretable cross-modal alignment and region reweighting, reduces dependency on specific acoustic fingerprints. Experimental results demonstrate that AGVP improves both navigation efficiency and robustness while achieving superior cross-scenario generalization on previously unheard sounds.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CV",
      "cs.MM"
    ],
    "primary_category": "cs.SD",
    "comment": "Main paper (6 pages). Accepted for publication by International Conference on Virtual Reality and Visualization 2025 (ICVRV 2025)",
    "pdf_url": "https://arxiv.org/pdf/2510.11760v1",
    "published_date": "2025-10-13 05:06:45 UTC",
    "updated_date": "2025-10-13 05:06:45 UTC"
  },
  {
    "arxiv_id": "2510.21761v1",
    "title": "J-ORA: A Framework and Multimodal Dataset for Japanese Object Identification, Reference, Action Prediction in Robot Perception",
    "authors": [
      "Jesse Atuhurra",
      "Hidetaka Kamigaito",
      "Taro Watanabe",
      "Koichiro Yoshino"
    ],
    "abstract": "We introduce J-ORA, a novel multimodal dataset that bridges the gap in robot perception by providing detailed object attribute annotations within Japanese human-robot dialogue scenarios. J-ORA is designed to support three critical perception tasks, object identification, reference resolution, and next-action prediction, by leveraging a comprehensive template of attributes (e.g., category, color, shape, size, material, and spatial relations). Extensive evaluations with both proprietary and open-source Vision Language Models (VLMs) reveal that incorporating detailed object attributes substantially improves multimodal perception performance compared to without object attributes. Despite the improvement, we find that there still exists a gap between proprietary and open-source VLMs. In addition, our analysis of object affordances demonstrates varying abilities in understanding object functionality and contextual relationships across different VLMs. These findings underscore the importance of rich, context-sensitive attribute annotations in advancing robot perception in dynamic environments. See project page at https://jatuhurrra.github.io/J-ORA/.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted to IROS2025",
    "pdf_url": "https://arxiv.org/pdf/2510.21761v1",
    "published_date": "2025-10-13 04:53:46 UTC",
    "updated_date": "2025-10-13 04:53:46 UTC"
  },
  {
    "arxiv_id": "2510.13862v1",
    "title": "Ensembling Large Language Models to Characterize Affective Dynamics in Student-AI Tutor Dialogues",
    "authors": [
      "Chenyu Zhang",
      "Sharifa Alghowinem",
      "Cynthia Breazeal"
    ],
    "abstract": "While recent studies have examined the leaning impact of large language model (LLM) in educational contexts, the affective dynamics of LLM-mediated tutoring remain insufficiently understood. This work introduces the first ensemble-LLM framework for large-scale affect sensing in tutoring dialogues, advancing the conversation on responsible pathways for integrating generative AI into education by attending to learners' evolving affective states. To achieve this, we analyzed two semesters' worth of 16,986 conversational turns exchanged between PyTutor, an LLM-powered AI tutor, and 261 undergraduate learners across three U.S. institutions. To investigate learners' emotional experiences, we generate zero-shot affect annotations from three frontier LLMs (Gemini, GPT-4o, Claude), including scalar ratings of valence, arousal, and learning-helpfulness, along with free-text emotion labels. These estimates are fused through rank-weighted intra-model pooling and plurality consensus across models to produce robust emotion profiles. Our analysis shows that during interaction with the AI tutor, students typically report mildly positive affect and moderate arousal. Yet learning is not uniformly smooth: confusion and curiosity are frequent companions to problem solving, and frustration, while less common, still surfaces in ways that can derail progress. Emotional states are short-lived--positive moments last slightly longer than neutral or negative ones, but they are fragile and easily disrupted. Encouragingly, negative emotions often resolve quickly, sometimes rebounding directly into positive states. Neutral moments frequently act as turning points, more often steering students upward than downward, suggesting opportunities for tutors to intervene at precisely these junctures.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "4 pages, 3 figures. Published in the 11th International Conference on Affective Computing and Intelligent Interaction (ACII 2025), Late-Breaking Results Track",
    "pdf_url": "https://arxiv.org/pdf/2510.13862v1",
    "published_date": "2025-10-13 04:43:56 UTC",
    "updated_date": "2025-10-13 04:43:56 UTC"
  },
  {
    "arxiv_id": "2510.11004v1",
    "title": "Automating Structural Engineering Workflows with Large Language Model Agents",
    "authors": [
      "Haoran Liang",
      "Yufa Zhou",
      "Mohammad Talebi Kalaleh",
      "Qipei Mei"
    ],
    "abstract": "We introduce $\\textbf{MASSE}$, the first Multi-Agent System for Structural Engineering, effectively integrating large language model (LLM)-based agents with real-world engineering workflows. Structural engineering is a fundamental yet traditionally stagnant domain, with core workflows remaining largely unchanged for decades despite its substantial economic impact and global market size. Recent advancements in LLMs have significantly enhanced their ability to perform complex reasoning, long-horizon planning, and precise tool utilization -- capabilities well aligned with structural engineering tasks such as interpreting design codes, executing load calculations, and verifying structural capacities. We present a proof-of-concept showing that most real-world structural engineering workflows can be fully automated through a training-free LLM-based multi-agent system. MASSE enables immediate deployment in professional environments, and our comprehensive validation on real-world case studies demonstrates that it can reduce expert workload from approximately two hours to mere minutes, while enhancing both reliability and accuracy in practical engineering scenarios.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CE",
      "cs.CL"
    ],
    "primary_category": "cs.MA",
    "comment": "Code: https://github.com/DelosLiang/masse",
    "pdf_url": "https://arxiv.org/pdf/2510.11004v1",
    "published_date": "2025-10-13 04:38:46 UTC",
    "updated_date": "2025-10-13 04:38:46 UTC"
  },
  {
    "arxiv_id": "2510.11003v1",
    "title": "FBS Model-based Maintenance Record Accumulation for Failure-Cause Inference in Manufacturing Systems",
    "authors": [
      "Takuma Fujiu",
      "Sho Okazaki",
      "Kohei Kaminishi",
      "Yuji Nakata",
      "Shota Hamamoto",
      "Kenshin Yokose",
      "Tatsunori Hara",
      "Yasushi Umeda",
      "Jun Ota"
    ],
    "abstract": "In manufacturing systems, identifying the causes of failures is crucial for maintaining and improving production efficiency. In knowledge-based failure-cause inference, it is important that the knowledge base (1) explicitly structures knowledge about the target system and about failures, and (2) contains sufficiently long causal chains of failures. In this study, we constructed Diagnostic Knowledge Ontology and proposed a Function-Behavior-Structure (FBS) model-based maintenance-record accumulation method based on it. Failure-cause inference using the maintenance records accumulated by the proposed method showed better agreement with the set of candidate causes enumerated by experts, especially in difficult cases where the number of related cases is small and the vocabulary used differs. In the future, it will be necessary to develop inference methods tailored to these maintenance records, build a user interface, and carry out validation on larger and more diverse systems. Additionally, this approach leverages the understanding and knowledge of the target in the design phase to support knowledge accumulation and problem solving during the maintenance phase, and it is expected to become a foundation for knowledge sharing across the entire engineering chain in the future.",
    "categories": [
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11003v1",
    "published_date": "2025-10-13 04:37:40 UTC",
    "updated_date": "2025-10-13 04:37:40 UTC"
  },
  {
    "arxiv_id": "2510.11001v2",
    "title": "DND: Boosting Large Language Models with Dynamic Nested Depth",
    "authors": [
      "Tieyuan Chen",
      "Xiaodong Chen",
      "Haoxing Chen",
      "Zhenzhong Lan",
      "Weiyao Lin",
      "Jianguo Li"
    ],
    "abstract": "We introduce Dynamic Nested Depth (DND), a novel method that improves performance for off-the-shelf LLMs by selecting critical tokens to reprocess in a nested depth manner. Specifically, at the end of the given transformer layer, DND identifies more critical tokens with a router and feeds them back for an extra round of processing, effectively ``reviewing\" difficult tokens while avoiding redundant computation for easier ones. The dynamic selection mechanism is tailored for precise control via two novel strategies: a router controlling loss to enhance token selection distinguishability, and a threshold control scheme to ensure selection stability. We demonstrate the effectiveness of DND by directly integrating it into pre-trained dense and MoE models during a post-training phase. On diverse benchmarks, this approach boosts the performances of the dense Qwen3-1.7B by 1.88% and the MoE Qwen3-30B-A3B by 0.87%, all with a minimal parameter and computing increase.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "TL;DR: We introduce Dynamic Nested Depth (DND), an efficient paradigm that adaptively identifies critical tokens and selectively deepens their computation via nested re-processing",
    "pdf_url": "https://arxiv.org/pdf/2510.11001v2",
    "published_date": "2025-10-13 04:22:57 UTC",
    "updated_date": "2025-11-29 06:28:34 UTC"
  },
  {
    "arxiv_id": "2510.10998v1",
    "title": "ABLEIST: Intersectional Disability Bias in LLM-Generated Hiring Scenarios",
    "authors": [
      "Mahika Phutane",
      "Hayoung Jung",
      "Matthew Kim",
      "Tanushree Mitra",
      "Aditya Vashistha"
    ],
    "abstract": "Large language models (LLMs) are increasingly under scrutiny for perpetuating identity-based discrimination in high-stakes domains such as hiring, particularly against people with disabilities (PwD). However, existing research remains largely Western-centric, overlooking how intersecting forms of marginalization--such as gender and caste--shape experiences of PwD in the Global South. We conduct a comprehensive audit of six LLMs across 2,820 hiring scenarios spanning diverse disability, gender, nationality, and caste profiles. To capture subtle intersectional harms and biases, we introduce ABLEIST (Ableism, Inspiration, Superhumanization, and Tokenism), a set of five ableism-specific and three intersectional harm metrics grounded in disability studies literature. Our results reveal significant increases in ABLEIST harms towards disabled candidates--harms that many state-of-the-art models failed to detect. These harms were further amplified by sharp increases in intersectional harms (e.g., Tokenism) for gender and caste-marginalized disabled candidates, highlighting critical blind spots in current safety tools and the need for intersectional safety evaluations of frontier models in high-stakes domains like hiring.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "28 pages, 11 figures, 16 tables. In submission",
    "pdf_url": "https://arxiv.org/pdf/2510.10998v1",
    "published_date": "2025-10-13 04:18:23 UTC",
    "updated_date": "2025-10-13 04:18:23 UTC"
  },
  {
    "arxiv_id": "2510.10994v1",
    "title": "DeepResearchGuard: Deep Research with Open-Domain Evaluation and Multi-Stage Guardrails for Safety",
    "authors": [
      "Wei-Chieh Huang",
      "Henry Peng Zou",
      "Yaozu Wu",
      "Dongyuan Li",
      "Yankai Chen",
      "Weizhi Zhang",
      "Yangning Li",
      "Angelo Zangari",
      "Jizhou Guo",
      "Chunyu Miao",
      "Liancheng Fang",
      "Langzhou He",
      "Renhe Jiang",
      "Philip S. Yu"
    ],
    "abstract": "Deep research frameworks have shown promising capabilities in synthesizing comprehensive reports from web sources. While deep research possesses significant potential to address complex issues through planning and research cycles, existing frameworks are deficient in sufficient evaluation procedures and stage-specific protections. They typically treat evaluation as exact match accuracy of question-answering, but overlook crucial aspects of report quality such as credibility, coherence, breadth, depth, and safety. This oversight may result in hazardous or malicious sources being integrated into the final report. To address these issues, we introduce DEEPRESEARCHGUARD, a comprehensive framework featuring four-stage safeguards with open-domain evaluation of references and reports. We assess performance across multiple metrics, e.g., defense success rate and over-refusal rate, and five key report dimensions. In the absence of a suitable safety benchmark, we introduce DRSAFEBENCH, a stage-wise benchmark for deep research safety. Our evaluation spans diverse state-of-the-art LLMs, including GPT-4o, Gemini-2.5-flash, DeepSeek-v3, and o4-mini. DEEPRESEARCHGUARD achieves an average defense success rate improvement of 18.16% while reducing over-refusal rate by 6%. The input guard provides the most substantial early-stage protection by filtering out obvious risks, while the plan and research guards enhance citation discipline and source credibility. Through extensive experiments, we show that DEEPRESEARCHGUARD enables comprehensive open-domain evaluation and stage-aware defenses that effectively block harmful content propagation, while systematically improving report quality without excessive over-refusal rates. The code can be found via https://github.com/Jasonya/DeepResearchGuard.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10994v1",
    "published_date": "2025-10-13 04:11:21 UTC",
    "updated_date": "2025-10-13 04:11:21 UTC"
  },
  {
    "arxiv_id": "2510.10991v1",
    "title": "A Survey on Agentic Multimodal Large Language Models",
    "authors": [
      "Huanjin Yao",
      "Ruifei Zhang",
      "Jiaxing Huang",
      "Jingyi Zhang",
      "Yibo Wang",
      "Bo Fang",
      "Ruolin Zhu",
      "Yongcheng Jing",
      "Shunyu Liu",
      "Guanbin Li",
      "Dacheng Tao"
    ],
    "abstract": "With the recent emergence of revolutionary autonomous agentic systems, research community is witnessing a significant shift from traditional static, passive, and domain-specific AI agents toward more dynamic, proactive, and generalizable agentic AI. Motivated by the growing interest in agentic AI and its potential trajectory toward AGI, we present a comprehensive survey on Agentic Multimodal Large Language Models (Agentic MLLMs). In this survey, we explore the emerging paradigm of agentic MLLMs, delineating their conceptual foundations and distinguishing characteristics from conventional MLLM-based agents. We establish a conceptual framework that organizes agentic MLLMs along three fundamental dimensions: (i) Agentic internal intelligence functions as the system's commander, enabling accurate long-horizon planning through reasoning, reflection, and memory; (ii) Agentic external tool invocation, whereby models proactively use various external tools to extend their problem-solving capabilities beyond their intrinsic knowledge; and (iii) Agentic environment interaction further situates models within virtual or physical environments, allowing them to take actions, adapt strategies, and sustain goal-directed behavior in dynamic real-world scenarios. To further accelerate research in this area for the community, we compile open-source training frameworks, training and evaluation datasets for developing agentic MLLMs. Finally, we review the downstream applications of agentic MLLMs and outline future research directions for this rapidly evolving field. To continuously track developments in this rapidly evolving field, we will also actively update a public repository at https://github.com/HJYao00/Awesome-Agentic-MLLMs.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10991v1",
    "published_date": "2025-10-13 04:07:01 UTC",
    "updated_date": "2025-10-13 04:07:01 UTC"
  },
  {
    "arxiv_id": "2510.13860v1",
    "title": "ShishuLM: Lightweight Language Model with Hybrid Decoder-MLP Architecture and Paired Weight Sharing",
    "authors": [
      "Shivanshu Kumar",
      "Gopalakrishnan Srinivasan"
    ],
    "abstract": "While the transformer architecture has achieved state-of-the-art performance on natural language processing tasks, these models impose substantial memory and computational overhead. Recent research has identified significant architectural redundancies within these models, presenting opportunities for optimization without compromising performance. Taking insights from research in AI interpretability and inference-time layer pruning, we introduce an efficient language model architecture, referred to as ShishuLM, which reduces both the parameter count and Key-Value (KV) cache requirements. Given the increasing importance of Small Language Models (SLMs) in agentic AI systems, we evaluate our approach on two SLMs of different scales. Our analysis reveals that for moderate-context scenarios, normalization coupled with attention computation is roughly linear with the input, enabling entire transformer blocks to be approximated through Multi-Layer Perceptrons (MLPs). Our results show that ShishuLM provides up to 25% reduction in memory requirements and up to 40% improvement in latency during both training and inference, compared to parent models. Our experimental and analytical findings provide insights towards building more efficient SLM architectures from a pre-training standpoint.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.13860v1",
    "published_date": "2025-10-13 04:04:54 UTC",
    "updated_date": "2025-10-13 04:04:54 UTC"
  },
  {
    "arxiv_id": "2510.10987v2",
    "title": "DITTO: A Spoofing Attack Framework on Watermarked LLMs via Knowledge Distillation",
    "authors": [
      "Hyeseon Ahn",
      "Shinwoo Park",
      "Suyeon Woo",
      "Yo-Sub Han"
    ],
    "abstract": "The promise of LLM watermarking rests on a core assumption that a specific watermark proves authorship by a specific model. We demonstrate that this assumption is dangerously flawed. We introduce the threat of watermark spoofing, a sophisticated attack that allows a malicious model to generate text containing the authentic-looking watermark of a trusted, victim model. This enables the seamless misattribution of harmful content, such as disinformation, to reputable sources. The key to our attack is repurposing watermark radioactivity, the unintended inheritance of data patterns during fine-tuning, from a discoverable trait into an attack vector. By distilling knowledge from a watermarked teacher model, our framework allows an attacker to steal and replicate the watermarking signal of the victim model. This work reveals a critical security gap in text authorship verification and calls for a paradigm shift towards technologies capable of distinguishing authentic watermarks from expertly imitated ones. Our code is available at https://github.com/hsannn/ditto.git.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "14 pages, 4 figures, preprint",
    "pdf_url": "https://arxiv.org/pdf/2510.10987v2",
    "published_date": "2025-10-13 03:53:40 UTC",
    "updated_date": "2025-11-03 02:23:39 UTC"
  },
  {
    "arxiv_id": "2510.10982v1",
    "title": "Catch-Only-One: Non-Transferable Examples for Model-Specific Authorization",
    "authors": [
      "Zihan Wang",
      "Zhiyong Ma",
      "Zhongkui Ma",
      "Shuofeng Liu",
      "Akide Liu",
      "Derui Wang",
      "Minhui Xue",
      "Guangdong Bai"
    ],
    "abstract": "Recent AI regulations call for data that remain useful for innovation while resistant to misuse, balancing utility with protection at the model level. Existing approaches either perturb data to make it unlearnable or retrain models to suppress transfer, but neither governs inference by unknown models, and both typically require control over training. We propose non-transferable examples (NEs), a training-free and data-agnostic input-side usage-control mechanism. We recode inputs within a model-specific low-sensitivity subspace, preserving outputs for the authorized model while reducing performance on unauthorized models through subspace misalignment. We establish formal bounds that guarantee utility for the authorized model and quantify deviation for unauthorized ones, with the Hoffman-Wielandt inequality linking degradation to spectral differences. Empirically, NEs retain performance on diverse vision backbones and state-of-the-art vision-language models under common preprocessing, whereas non-target models collapse even with reconstruction attempts. These results establish NEs as a practical means to preserve intended data utility while preventing unauthorized exploitation. Our project is available at https://trusted-system-lab.github.io/model-specificity",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10982v1",
    "published_date": "2025-10-13 03:43:11 UTC",
    "updated_date": "2025-10-13 03:43:11 UTC"
  },
  {
    "arxiv_id": "2510.10977v1",
    "title": "Revisiting Model Interpolation for Efficient Reasoning",
    "authors": [
      "Taiqiang Wu",
      "Runming Yang",
      "Tao Liu",
      "Jiahao Wang",
      "Ngai Wong"
    ],
    "abstract": "Model merging, typically on Instruct and Thinking models, has shown remarkable performance for efficient reasoning. In this paper, we systematically revisit the simplest merging method that interpolates two weights directly. Particularly, we observe that model interpolation follows a three-stage evolutionary paradigm with distinct behaviors on the reasoning trajectory. These dynamics provide a principled guide for navigating the performance-cost trade-off. Empirical results demonstrate that a strategically interpolated model surprisingly surpasses sophisticated model merging baselines on both efficiency and effectiveness. We further validate our findings with extensive ablation studies on model layers, modules, and decoding strategies. Ultimately, this work demystifies model interpolation and offers a practical framework for crafting models with precisely targeted reasoning capabilities. Code is available at \\href{https://github.com/wutaiqiang/MI}{Github}.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "14 pages, 6 figures, 7 tables. Working in progress",
    "pdf_url": "https://arxiv.org/pdf/2510.10977v1",
    "published_date": "2025-10-13 03:30:01 UTC",
    "updated_date": "2025-10-13 03:30:01 UTC"
  },
  {
    "arxiv_id": "2510.10976v1",
    "title": "Video-STR: Reinforcing MLLMs in Video Spatio-Temporal Reasoning with Relation Graph",
    "authors": [
      "Wentao Wang",
      "Heqing Zou",
      "Tianze Luo",
      "Rui Huang",
      "Yutian Zhao",
      "Zhuochen Wang",
      "Hansheng Zhang",
      "Chengwei Qin",
      "Yan Wang",
      "Lin Zhao",
      "Huaijian Zhang"
    ],
    "abstract": "Recent progress in Multimodal Large Language Models (MLLMs) has demonstrated strong semantic understanding capabilities, but struggles to perform precise spatio-temporal understanding. Existing spatio-temporal methods primarily focus on the video itself, while overlooking the physical information within the video, such as multi-object layouts and motion. Such limitations restrict the use of MLLMs in downstream applications that demand high precision, including embodied intelligence and VR. To address this issue, we present Video-STR, a novel graph-based reinforcement method for precise Video Spatio-Temporal Reasoning. Building upon the capacity of Reinforcement Learning with Verifiable Reward (RLVR) to improve model abilities, we introduce a reasoning mechanism using graph-based Group Relative Policy Optimization (GRPO) method to guide the model in inferring the underlying spatio-temporal topology of scenarios during the thinking process. To resolve the lack of spatio-temporal training data, we construct the STV-205k dataset with 205k question-answering pairs, covering dynamic multi-object scenes in both indoor and outdoor environments, to support the model training. Experiments show that Video-STR achieves state-of-the-art results on various benchmarks, outperforming the base model by 13% on STI-Bench, and demonstrating the effectiveness of our approach and dataset. Code, model, and data will be released.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10976v1",
    "published_date": "2025-10-13 03:26:56 UTC",
    "updated_date": "2025-10-13 03:26:56 UTC"
  },
  {
    "arxiv_id": "2510.10971v1",
    "title": "RV-HATE: Reinforced Multi-Module Voting for Implicit Hate Speech Detection",
    "authors": [
      "Yejin Lee",
      "Hyeseon Ahn",
      "Yo-Sub Han"
    ],
    "abstract": "Hate speech remains prevalent in human society and continues to evolve in its forms and expressions. Modern advancements in internet and online anonymity accelerate its rapid spread and complicate its detection. However, hate speech datasets exhibit diverse characteristics primarily because they are constructed from different sources and platforms, each reflecting different linguistic styles and social contexts. Despite this diversity, prior studies on hate speech detection often rely on fixed methodologies without adapting to data-specific features. We introduce RV-HATE, a detection framework designed to account for the dataset-specific characteristics of each hate speech dataset. RV-HATE consists of multiple specialized modules, where each module focuses on distinct linguistic or contextual features of hate speech. The framework employs reinforcement learning to optimize weights that determine the contribution of each module for a given dataset. A voting mechanism then aggregates the module outputs to produce the final decision. RV-HATE offers two primary advantages: (1)~it improves detection accuracy by tailoring the detection process to dataset-specific attributes, and (2)~it also provides interpretable insights into the distinctive features of each dataset. Consequently, our approach effectively addresses implicit hate speech and achieves superior performance compared to conventional static methods. Our code is available at https://github.com/leeyejin1231/RV-HATE.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 9 figures, 12 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.10971v1",
    "published_date": "2025-10-13 03:21:51 UTC",
    "updated_date": "2025-10-13 03:21:51 UTC"
  },
  {
    "arxiv_id": "2510.10965v1",
    "title": "Judge Before Answer: Can MLLM Discern the False Premise in Question?",
    "authors": [
      "Jidong Li",
      "Lingyong Fang",
      "Haodong Zhao",
      "Sufeng Duan",
      "Gongshen Liu"
    ],
    "abstract": "Multimodal large language models (MLLMs) have witnessed astonishing advancements in recent years. Despite these successes, MLLMs remain vulnerable to flase premise problems. However, existing benchmarks targeting this issue are limited in scope: they often lack fine-grained categorization, exhibit insufficient coverage, and thus fail to provide a rigorous evaluation of the ability of models to recognize false premises. To bridge this gap, we introduce a fully automated pipeline for constructing a comprehensive benchmark of false premise questions. Our method systematically categorizes the premises into three main types and thirteen subtypes according to the abilities required to identify the premises, resulting in the JBA dataset.Results show current MLLMs still struggle with false premise recognition. Building upon this benchmark, we further propose a recognition enhancement framework tailored to strengthen the robustness of MLLMs to detect false premises. Extensive experiments demonstrate that models trained with our framework achieve significant improvements in false premise recognition.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10965v1",
    "published_date": "2025-10-13 03:17:00 UTC",
    "updated_date": "2025-10-13 03:17:00 UTC"
  },
  {
    "arxiv_id": "2510.15978v1",
    "title": "DAWP: A framework for global observation forecasting via Data Assimilation and Weather Prediction in satellite observation space",
    "authors": [
      "Junchao Gong",
      "Jingyi Xu",
      "Ben Fei",
      "Fenghua Ling",
      "Wenlong Zhang",
      "Kun Chen",
      "Wanghan Xu",
      "Weidong Yang",
      "Xiaokang Yang",
      "Lei Bai"
    ],
    "abstract": "Weather prediction is a critical task for human society, where impressive progress has been made by training artificial intelligence weather prediction (AIWP) methods with reanalysis data. However, reliance on reanalysis data limits the AIWPs with shortcomings, including data assimilation biases and temporal discrepancies. To liberate AIWPs from the reanalysis data, observation forecasting emerges as a transformative paradigm for weather prediction. One of the key challenges in observation forecasting is learning spatiotemporal dynamics across disparate measurement systems with irregular high-resolution observation data, which constrains the design and prediction of AIWPs. To this end, we propose our DAWP as an innovative framework to enable AIWPs to operate in a complete observation space by initialization with an artificial intelligence data assimilation (AIDA) module. Specifically, our AIDA module applies a mask multi-modality autoencoder(MMAE)for assimilating irregular satellite observation tokens encoded by mask ViT-VAEs. For AIWP, we introduce a spatiotemporal decoupling transformer with cross-regional boundary conditioning (CBC), learning the dynamics in observation space, to enable sub-image-based global observation forecasting. Comprehensive experiments demonstrate that AIDA initialization significantly improves the roll out and efficiency of AIWP. Additionally, we show that DAWP holds promising potential to be applied in global precipitation forecasting.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.ao-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15978v1",
    "published_date": "2025-10-13 03:13:35 UTC",
    "updated_date": "2025-10-13 03:13:35 UTC"
  },
  {
    "arxiv_id": "2510.10963v1",
    "title": "APLOT: Robust Reward Modeling via Adaptive Preference Learning with Optimal Transport",
    "authors": [
      "Zhuo Li",
      "Yuege Feng",
      "Dandan Guo",
      "Jinpeng Hu",
      "Anningzhe Gao",
      "Xiang Wan"
    ],
    "abstract": "The reward model (RM) plays a crucial role in aligning Large Language Models (LLMs) with human preferences through Reinforcement Learning, where the Bradley-Terry (BT) objective has been recognized as simple yet powerful, specifically for pairwise preference learning. However, BT-based RMs often struggle to effectively distinguish between similar preference responses, leading to insufficient separation between preferred and non-preferred outputs. Consequently, they may easily overfit easy samples and cannot generalize well to Out-Of-Distribution (OOD) samples, resulting in suboptimal performance. To address these challenges, this paper introduces an effective enhancement to BT-based RMs through an adaptive margin mechanism. Specifically, we design to dynamically adjust the RM focus on more challenging samples through margins, based on both semantic similarity and model-predicted reward differences, which is approached from a distributional perspective solvable with Optimal Transport (OT). By incorporating these factors into a principled OT cost matrix design, our adaptive margin enables the RM to better capture distributional differences between chosen and rejected responses, yielding significant improvements in performance, convergence speed, and generalization capabilities. Experimental results across multiple benchmarks demonstrate that our method outperforms several existing RM techniques, showcasing enhanced performance in both In-Distribution (ID) and OOD settings. Moreover, RLHF experiments support our practical effectiveness in better aligning LLMs with human preferences. Our code is available at https://github.com/BIRlz/APLOT",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "EMNLP2025",
    "pdf_url": "https://arxiv.org/pdf/2510.10963v1",
    "published_date": "2025-10-13 03:13:28 UTC",
    "updated_date": "2025-10-13 03:13:28 UTC"
  },
  {
    "arxiv_id": "2510.10962v1",
    "title": "MC#: Mixture Compressor for Mixture-of-Experts Large Models",
    "authors": [
      "Wei Huang",
      "Yue Liao",
      "Yukang Chen",
      "Jianhui Liu",
      "Haoru Tan",
      "Si Liu",
      "Shiming Zhang",
      "Shuicheng Yan",
      "Xiaojuan Qi"
    ],
    "abstract": "Mixture-of-Experts (MoE) effectively scales large language models (LLMs) and vision-language models (VLMs) by increasing capacity through sparse activation. However, preloading all experts into memory and activating multiple experts per input introduces significant computational and memory overhead, making the expert module a major contributor to model size and inference cost. To address this, we propose MC# (Mixture-Compressor-sharp), a framework that combines static quantization and dynamic expert pruning by leveraging the significance of experts and tokens for aggressive compression of MoE-LLMs/VLMs. To reduce storage and loading costs, we introduce Pre-Loading Mixed-Precision Quantization (PMQ), which optimizes bit allocation via linear programming, balancing expert importance and quantization error for a Pareto-optimal trade-off between size and performance. To reduce runtime computation, Online Top-any Pruning (OTP) uses Gumbel-Softmax sampling to dynamically select a subset of experts per token, enabling fine-grained control over activation. By combining PMQ's static bit-width optimization with OTP's dynamic routing, MC# achieves extreme compression with minimal accuracy loss. On DeepSeek-VL2, MC# achieves a 6.2 times weight reduction at 2.57 average bits with only a 1.7% accuracy drop across five multimodal benchmarks. Additionally, OTP reduces expert activation over 20% with less than 1% performance degradation, demonstrating strong potential for efficient MoE-based model deployment.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages, 13 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.10962v1",
    "published_date": "2025-10-13 03:12:46 UTC",
    "updated_date": "2025-10-13 03:12:46 UTC"
  },
  {
    "arxiv_id": "2510.10961v2",
    "title": "KOTOX: A Korean Toxic Dataset for Deobfuscation and Detoxification",
    "authors": [
      "Yejin Lee",
      "Su-Hyeon Kim",
      "Hyundong Jin",
      "Dayoung Kim",
      "Yeonsoo Kim",
      "Yo-Sub Han"
    ],
    "abstract": "Online communication increasingly amplifies toxic language, and recent research actively explores methods for detecting and rewriting such content. Existing studies primarily focus on non-obfuscated text, which limits robustness in the situation where users intentionally disguise toxic expressions. In particular, Korean allows toxic expressions to be easily disguised through its agglutinative characteristic. However, obfuscation in Korean remains largely unexplored, which motivates us to introduce a KOTOX: Korean toxic dataset for deobfuscation and detoxification. We categorize Korean obfuscation patterns into linguistically grounded classes and define transformation rules derived from real-world examples. Using these rules, we provide paired neutral and toxic sentences alongside their obfuscated counterparts. Models trained on our dataset better handle obfuscated text without sacrificing performance on non-obfuscated text. This is the first dataset that simultaneously supports deobfuscation and detoxification for the Korean language. We expect it to facilitate better understanding and mitigation of obfuscated toxic content in LLM for Korean. Our code and data are available at https://github.com/leeyejin1231/KOTOX.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "26 pages, 5 figures, 24 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.10961v2",
    "published_date": "2025-10-13 03:12:37 UTC",
    "updated_date": "2026-01-09 09:53:41 UTC"
  },
  {
    "arxiv_id": "2510.10959v2",
    "title": "Rediscovering Entropy Regularization: Adaptive Coefficient Unlocks Its Potential for LLM Reinforcement Learning",
    "authors": [
      "Xiaoyun Zhang",
      "Xiaojian Yuan",
      "Di Huang",
      "Wang You",
      "Chen Hu",
      "Jingqing Ruan",
      "Kejiang Chen",
      "Xing Hu"
    ],
    "abstract": "Reasoning ability has become a defining capability of Large Language Models (LLMs), with Reinforcement Learning with Verifiable Rewards (RLVR) emerging as a key paradigm to enhance it. However, RLVR training often suffers from policy entropy collapse, where the policy becomes overly deterministic, hindering exploration and limiting reasoning performance. While entropy regularization is a common remedy, its effectiveness is highly sensitive to the fixed coefficient, making it unstable across tasks and models. In this work, we revisit entropy regularization in RLVR and argue that its potential has been largely underestimated. Our analysis shows that (i) tasks of varying difficulty demand distinct exploration intensities, and (ii) balanced exploration may require the policy entropy to be maintained within a moderate range below its initial level. Therefore, we propose Adaptive Entropy Regularization (AER)--a framework that dynamically balances exploration and exploitation via three components: difficulty-aware coefficient allocation, initial-anchored target entropy, and dynamic global coefficient adjustment. Experiments on multiple mathematical reasoning benchmarks show that AER consistently outperforms baselines, improving both reasoning accuracy and exploration capability.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.10959v2",
    "published_date": "2025-10-13 03:10:26 UTC",
    "updated_date": "2025-10-16 08:13:32 UTC"
  },
  {
    "arxiv_id": "2510.10956v1",
    "title": "Project-Level C-to-Rust Translation via Synergistic Integration of Knowledge Graphs and Large Language Models",
    "authors": [
      "Zhiqiang Yuan",
      "Wenjun Mao",
      "Zhuo Chen",
      "Xiyue Shang",
      "Chong Wang",
      "Yiling Lou",
      "Xin Peng"
    ],
    "abstract": "Translating C code into safe Rust is an effective way to ensure its memory safety. Compared to rule-based translation which produces Rust code that remains largely unsafe, LLM-based methods can generate more idiomatic and safer Rust code because LLMs have been trained on vast amount of human-written idiomatic code. Although promising, existing LLM-based methods still struggle with project-level C-to-Rust translation. They typically partition a C project into smaller units (\\eg{} functions) based on call graphs and translate them bottom-up to resolve program dependencies. However, this bottom-up, unit-by-unit paradigm often fails to translate pointers due to the lack of a global perspective on their usage. To address this problem, we propose a novel C-Rust Pointer Knowledge Graph (KG) that enriches a code-dependency graph with two types of pointer semantics: (i) pointer-usage information which record global behaviors such as points-to flows and map lower-level struct usage to higher-level units; and (ii) Rust-oriented annotations which encode ownership, mutability, nullability, and lifetime. Synthesizing the \\kg{} with LLMs, we further propose \\ourtool{}, which implements a project-level C-to-Rust translation technique. In \\ourtool{}, the \\kg{} provides LLMs with comprehensive pointer semantics from a global perspective, thus guiding LLMs towards generating safe and idiomatic Rust code from a given C project. Our experiments show that \\ourtool{} reduces unsafe usages in translated Rust by 99.9\\% compared to both rule-based translation and traditional LLM-based rewriting, while achieving an average 29.3\\% higher functional correctness than those fuzzing-enhanced LLM methods.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10956v1",
    "published_date": "2025-10-13 03:09:35 UTC",
    "updated_date": "2025-10-13 03:09:35 UTC"
  },
  {
    "arxiv_id": "2510.10948v1",
    "title": "Unify Variables in Neural Scaling Laws for General Audio Representations via Embedding Effective Rank",
    "authors": [
      "Xuyao Deng",
      "Yanjie Sun",
      "Yong Dou",
      "Kele Xu"
    ],
    "abstract": "Scaling laws have profoundly shaped our understanding of model performance in computer vision and natural language processing, yet their application to general audio representation learning remains underexplored. A key challenge lies in the multifactorial nature of general audio representation-representation quality is jointly influenced by variables such as audio length, embedding dimensionality, model depth, model architecture, data volume, etc., many of which are difficult to isolate or express analytically. In this work, we present a systematic study of scaling laws for general audio representations by utilizing embedding effective rank (RankMe) as a unifying metric that encapsulates the impact of diverse variables on representation quality. RankMe enables a label-free, information-theoretic quantification of audio embeddings, allowing us to examine scaling behaviors across a wide hyper-parameter space, including model size, training data volume, computational budget, architectural configurations, etc. Our empirical findings reveal a consistent power-law relationship between RankMe and representation quality, suggesting that embedding effective rank serves as a reliable proxy for assessing and predicting model performance in audio representation learning. This work not only validates the applicability of classical scaling principles to the general audio domain but also offers a theoretically grounded and empirically robust framework for guiding future model scaling strategies in audio foundation models.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10948v1",
    "published_date": "2025-10-13 02:58:50 UTC",
    "updated_date": "2025-10-13 02:58:50 UTC"
  },
  {
    "arxiv_id": "2510.10942v1",
    "title": "Scalable and Explainable Enterprise Knowledge Discovery Using Graph-Centric Hybrid Retrieval",
    "authors": [
      "Nilima Rao",
      "Jagriti Srivastava",
      "Pradeep Kumar Sharma",
      "Hritvik Shrivastava"
    ],
    "abstract": "Modern enterprises manage vast knowledge distributed across heterogeneous systems such as Jira, Git repositories, Confluence, and wikis. Conventional retrieval methods based on keyword search or static embeddings often fail to answer complex queries that require contextual reasoning and multi-hop inference across artifacts. We present a modular hybrid retrieval framework for adaptive enterprise information access that integrates Knowledge Base Language-Augmented Models (KBLam), DeepGraph representations, and embedding-driven semantic search. The framework builds a unified knowledge graph from parsed repositories including code, pull requests, and commit histories, enabling semantic similarity search, structural inference, and multi-hop reasoning. Query analysis dynamically determines the optimal retrieval strategy, supporting both structured and unstructured data sources through independent or fused processing. An interactive interface provides graph visualizations, subgraph exploration, and context-aware query routing to generate concise and explainable answers. Experiments on large-scale Git repositories show that the unified reasoning layer improves answer relevance by up to 80 percent compared with standalone GPT-based retrieval pipelines. By combining graph construction, hybrid reasoning, and interactive visualization, the proposed framework offers a scalable, explainable, and user-centric foundation for intelligent knowledge assistants in enterprise environments.",
    "categories": [
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10942v1",
    "published_date": "2025-10-13 02:56:36 UTC",
    "updated_date": "2025-10-13 02:56:36 UTC"
  },
  {
    "arxiv_id": "2510.10938v1",
    "title": "Redundancy as a Structural Information Principle for Learning and Generalization",
    "authors": [
      "Yuda Bi",
      "Ying Zhu",
      "Vince D Calhoun"
    ],
    "abstract": "We present a theoretical framework that extends classical information theory to finite and structured systems by redefining redundancy as a fundamental property of information organization rather than inefficiency. In this framework, redundancy is expressed as a general family of informational divergences that unifies multiple classical measures, such as mutual information, chi-squared dependence, and spectral redundancy, under a single geometric principle. This reveals that these traditional quantities are not isolated heuristics but projections of a shared redundancy geometry. The theory further predicts that redundancy is bounded both above and below, giving rise to an optimal equilibrium that balances over-compression (loss of structure) and over-coupling (collapse). While classical communication theory favors minimal redundancy for transmission efficiency, finite and structured systems, such as those underlying real-world learning, achieve maximal stability and generalization near this equilibrium. Experiments with masked autoencoders are used to illustrate and verify this principle: the model exhibits a stable redundancy level where generalization peaks. Together, these results establish redundancy as a measurable and tunable quantity that bridges the asymptotic world of communication and the finite world of learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10938v1",
    "published_date": "2025-10-13 02:55:37 UTC",
    "updated_date": "2025-10-13 02:55:37 UTC"
  },
  {
    "arxiv_id": "2510.10932v1",
    "title": "TabVLA: Targeted Backdoor Attacks on Vision-Language-Action Models",
    "authors": [
      "Zonghuan Xu",
      "Xiang Zheng",
      "Xingjun Ma",
      "Yu-Gang Jiang"
    ],
    "abstract": "With the growing deployment of Vision-Language-Action (VLA) models in real-world embodied AI systems, their increasing vulnerability to backdoor attacks poses a serious safety threat. A backdoored VLA agent can be covertly triggered by a pre-injected backdoor to execute adversarial actions, potentially causing system failures or even physical harm. Although backdoor attacks on VLA models have been explored, prior work has focused only on untargeted attacks, leaving the more practically threatening scenario of targeted manipulation unexamined. In this paper, we study targeted backdoor attacks on VLA models and introduce TabVLA, a novel framework that enables such attacks via black-box fine-tuning. TabVLA explores two deployment-relevant inference-time threat models: input-stream editing and in-scene triggering. It formulates poisoned data generation as an optimization problem to improve attack effectivess. Experiments with OpenVLA-7B on the LIBERO benchmark reveal that the vision channel is the principal attack surface: targeted backdoors succeed with minimal poisoning, remain robust across variations in trigger design, and are degraded only by positional mismatches between fine-tuning and inference triggers. We also investigate a potential detection-based defense against TabVLA, which reconstructs latent visual triggers from the input stream to flag activation-conditioned backdoor samples. Our work highlights the vulnerability of VLA models to targeted backdoor manipulation and underscores the need for more advanced defenses.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CR",
    "comment": "8 pages, 8 tables, 1 figure. Under review",
    "pdf_url": "https://arxiv.org/pdf/2510.10932v1",
    "published_date": "2025-10-13 02:45:48 UTC",
    "updated_date": "2025-10-13 02:45:48 UTC"
  },
  {
    "arxiv_id": "2510.10931v1",
    "title": "PoU: Proof-of-Use to Counter Tool-Call Hacking in DeepResearch Agents",
    "authors": [
      "SHengjie Ma",
      "Chenlong Deng",
      "Jiaxin Mao",
      "Jiadeng Huang",
      "Teng Wang",
      "Junjie Wu",
      "Changwang Zhang",
      "Jun wang"
    ],
    "abstract": "Retrieval-augmented generation (RAG) agents, such as recent DeepResearch-style systems, extend large language models (LLMs) with autonomous information-seeking capabilities through external tools. While reinforcement learning (RL) has enabled impressive multi-step reasoning, we identify a previously overlooked failure mode, Tool-Call Hacking, where agents inflate reward signals by issuing superficially correct tool calls without genuinely leveraging the retrieved evidence. This results in (i) mode collapse into repetitive reliance on a single source and (ii) spurious grounding, where answers are only weakly supported by cited content.\n  To address this, we propose Proof-of-Use (PoU), an evidence-grounded RL framework that enforces verifiable causal links between retrieved evidence, reasoning traces, and final answers. PoU operationalizes this through a unified step-wise contract combining syntactic citation validation, perturbation-based sensitivity rewards, and answer-evidence alignment objectives, ensuring that tool usage remains both interpretable and functionally grounded.\n  Across seven QA benchmarks spanning in-domain, out-of-domain, and out-of-tool-distribution settings, PoU consistently outperforms strong DeepResearch baselines in factual accuracy, evidence faithfulness, and tool-routing balance. These findings highlight the necessity of grounding RL-trained agents not merely in task outcomes but in the causal use of retrieved information, offering a principled path toward trustworthy retrieval-augmented reasoning.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10931v1",
    "published_date": "2025-10-13 02:45:37 UTC",
    "updated_date": "2025-10-13 02:45:37 UTC"
  },
  {
    "arxiv_id": "2510.10930v1",
    "title": "Evaluating Language Models' Evaluations of Games",
    "authors": [
      "Katherine M. Collins",
      "Cedegao E. Zhang",
      "Graham Todd",
      "Lance Ying",
      "Mauricio Barba da Costa",
      "Ryan Liu",
      "Prafull Sharma",
      "Adrian Weller",
      "Ionatan Kuperwajs",
      "Lionel Wong",
      "Joshua B. Tenenbaum",
      "Thomas L. Griffiths"
    ],
    "abstract": "Reasoning is not just about solving problems -- it is also about evaluating which problems are worth solving at all. Evaluations of artificial intelligence (AI) systems primarily focused on problem solving, historically by studying how models play games such as chess and Go. In this paper, we advocate for a new paradigm that assesses AI systems' evaluation of games. First, we introduce a formalism for evaluating such evaluations. We then leverage a large-scale dataset of over $100$ novel board games and over 450 human judgments to compare evaluations produced by modern language and reasoning models against those of people and symbolic computational agents. We consider two kinds of evaluative queries: assessing the payoff (or fairness) and the funness of games. These queries span two dimensions relevant to the design of evaluations of AI evaluations: how complex a query is to compute and how difficult a query is to quantify. Our results show that reasoning models are generally more aligned to people in their evaluations of games than non-reasoning language models. However, we observe a non-monotonic relationship: as models get closer to game-theoretic optimal, their fit to human data weakens. We also observe more \"jaggedness\" across models for assessing funness, in line with the greater difficulty of quantifying this query. Across queries and games, reasoning models show highly variable and unpredictable resource usage when assessing queries, pointing to the importance of imbuing more resource-rational meta-reasoning in language and reasoning models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Pre-print",
    "pdf_url": "https://arxiv.org/pdf/2510.10930v1",
    "published_date": "2025-10-13 02:45:37 UTC",
    "updated_date": "2025-10-13 02:45:37 UTC"
  },
  {
    "arxiv_id": "2510.10921v2",
    "title": "FG-CLIP 2: A Bilingual Fine-grained Vision-Language Alignment Model",
    "authors": [
      "Chunyu Xie",
      "Bin Wang",
      "Fanjing Kong",
      "Jincheng Li",
      "Dawei Liang",
      "Ji Ao",
      "Dawei Leng",
      "Yuhui Yin"
    ],
    "abstract": "Fine-grained vision-language understanding requires precise alignment between visual content and linguistic descriptions, a capability that remains limited in current models, particularly in non-English settings. While models like CLIP perform well on global alignment, they often struggle to capture fine-grained details in object attributes, spatial relations, and linguistic expressions, with limited support for bilingual comprehension. To address these challenges, we introduce FG-CLIP 2, a bilingual vision-language model designed to advance fine-grained alignment for both English and Chinese. Our approach leverages rich fine-grained supervision, including region-text matching and long-caption modeling, alongside multiple discriminative objectives. We further introduce the Textual Intra-modal Contrastive (TIC) loss to better distinguish semantically similar captions. Trained on a carefully curated mixture of large-scale English and Chinese data, FG-CLIP 2 achieves powerful bilingual performance. To enable rigorous evaluation, we present a new benchmark for Chinese multimodal understanding, featuring long-caption retrieval and bounding box classification. Extensive experiments on 29 datasets across 8 tasks show that FG-CLIP 2 outperforms existing methods, achieving state-of-the-art results in both languages. We release the model, code, and benchmark to facilitate future research on bilingual fine-grained alignment.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10921v2",
    "published_date": "2025-10-13 02:32:07 UTC",
    "updated_date": "2025-10-17 08:47:31 UTC"
  },
  {
    "arxiv_id": "2510.10920v1",
    "title": "Comparative Explanations via Counterfactual Reasoning in Recommendations",
    "authors": [
      "Yi Yu",
      "Zhenxing Hu"
    ],
    "abstract": "Explainable recommendation through counterfactual reasoning seeks to identify the influential aspects of items in recommendations, which can then be used as explanations. However, state-of-the-art approaches, which aim to minimize changes in product aspects while reversing their recommended decisions according to an aggregated decision boundary score, often lead to factual inaccuracies in explanations. To solve this problem, in this work we propose a novel method of Comparative Counterfactual Explanations for Recommendation (CoCountER). CoCountER creates counterfactual data based on soft swap operations, enabling explanations for recommendations of arbitrary pairs of comparative items. Empirical experiments validate the effectiveness of our approach.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10920v1",
    "published_date": "2025-10-13 02:31:03 UTC",
    "updated_date": "2025-10-13 02:31:03 UTC"
  },
  {
    "arxiv_id": "2510.10918v1",
    "title": "DreamMakeup: Face Makeup Customization using Latent Diffusion Models",
    "authors": [
      "Geon Yeong Park",
      "Inhwa Han",
      "Serin Yang",
      "Yeobin Hong",
      "Seongmin Jeong",
      "Heechan Jeon",
      "Myeongjin Goh",
      "Sung Won Yi",
      "Jin Nam",
      "Jong Chul Ye"
    ],
    "abstract": "The exponential growth of the global makeup market has paralleled advancements in virtual makeup simulation technology. Despite the progress led by GANs, their application still encounters significant challenges, including training instability and limited customization capabilities. Addressing these challenges, we introduce DreamMakup - a novel training-free Diffusion model based Makeup Customization method, leveraging the inherent advantages of diffusion models for superior controllability and precise real-image editing. DreamMakeup employs early-stopped DDIM inversion to preserve the facial structure and identity while enabling extensive customization through various conditioning inputs such as reference images, specific RGB colors, and textual descriptions. Our model demonstrates notable improvements over existing GAN-based and recent diffusion-based frameworks - improved customization, color-matching capabilities, identity preservation and compatibility with textual descriptions or LLMs with affordable computational costs.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10918v1",
    "published_date": "2025-10-13 02:29:23 UTC",
    "updated_date": "2025-10-13 02:29:23 UTC"
  },
  {
    "arxiv_id": "2510.10915v1",
    "title": "LPCVAE: A Conditional VAE with Long-Term Dependency and Probabilistic Time-Frequency Fusion for Time Series Anomaly Detection",
    "authors": [
      "Hanchang Cheng",
      "Weimin Mu",
      "Fan Liu",
      "Weilin Zhu",
      "Can Ma"
    ],
    "abstract": "Time series anomaly detection(TSAD) is a critical task in signal processing field, ensuring the reliability of complex systems. Reconstruction-based methods dominate in TSAD. Among these methods, VAE-based methods have achieved promising results. Existing VAE-based methods suffer from the limitation of single-window feature and insufficient leveraging of long-term time and frequency information. We propose a Conditional Variational AutoEncoder with Long-term dependency and Probabilistic time-frequency fusion, named LPCVAE. LPCVAE introduces LSTM to capture long-term dependencies beyond windows. It further incorporates a Product-of-Experts (PoE) mechanism for adaptive and distribution-level probabilistic fusion. This design effectively mitigates time-frequency information loss. Extensive experiments on four public datasets demonstrate it outperforms state-of-the-art methods. The results confirm that integrating long-term time and frequency representations with adaptive fusion yields a robust and efficient solution for TSAD.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10915v1",
    "published_date": "2025-10-13 02:27:04 UTC",
    "updated_date": "2025-10-13 02:27:04 UTC"
  },
  {
    "arxiv_id": "2510.10909v3",
    "title": "PaperArena: An Evaluation Benchmark for Tool-Augmented Agentic Reasoning on Scientific Literature",
    "authors": [
      "Daoyu Wang",
      "Mingyue Cheng",
      "Shuo Yu",
      "Zirui Liu",
      "Ze Guo",
      "Qi Liu"
    ],
    "abstract": "Understanding and reasoning on the web-scale scientific literature is a crucial touchstone for large language model (LLM) based agents designed to support complex knowledge-intensive tasks. However, existing works are mainly restricted to tool-free tasks within isolated papers, largely due to the lack of a benchmark for cross-paper reasoning and multi-tool orchestration in real research scenarios. In this work, we propose PaperArena, an evaluation benchmark for agents to address real-world research questions that typically require integrating information across multiple papers with the assistance of external tools. Given a research question, agents should integrate diverse formats across multiple papers through reasoning and interacting with appropriate tools, thereby producing a well-grounded answer. To support standardized evaluation, we provide a modular and extensible platform for agent execution, offering tools such as multimodal parsing, context retrieval, and programmatic computation. Experimental results reveal that even the most advanced LLM powering a well-established agent system achieves merely 38.78% average accuracy. On the hard subset, accuracy drops to only 18.47%, highlighting great potential for improvement. We also present several empirical findings, including that all agents tested exhibit inefficient tool usage, often invoking more tools than necessary to solve a task. We invite the community to adopt PaperArena to develop and evaluate more capable agents for scientific discovery. Our code and data are available https://github.com/Melmaphother/PaperArena.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "12 pages, 9 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.10909v3",
    "published_date": "2025-10-13 02:10:39 UTC",
    "updated_date": "2025-11-10 03:21:21 UTC"
  },
  {
    "arxiv_id": "2510.15977v1",
    "title": "Bolster Hallucination Detection via Prompt-Guided Data Augmentation",
    "authors": [
      "Wenyun Li",
      "Zheng Zhang",
      "Dongmei Jiang",
      "Xiangyuan Lan"
    ],
    "abstract": "Large language models (LLMs) have garnered significant interest in AI community. Despite their impressive generation capabilities, they have been found to produce misleading or fabricated information, a phenomenon known as hallucinations. Consequently, hallucination detection has become critical to ensure the reliability of LLM-generated content. One primary challenge in hallucination detection is the scarcity of well-labeled datasets containing both truthful and hallucinated outputs. To address this issue, we introduce Prompt-guided data Augmented haLlucination dEtection (PALE), a novel framework that leverages prompt-guided responses from LLMs as data augmentation for hallucination detection. This strategy can generate both truthful and hallucinated data under prompt guidance at a relatively low cost. To more effectively evaluate the truthfulness of the sparse intermediate embeddings produced by LLMs, we introduce an estimation metric called the Contrastive Mahalanobis Score (CM Score). This score is based on modeling the distributions of truthful and hallucinated data in the activation space. CM Score employs a matrix decomposition approach to more accurately capture the underlying structure of these distributions. Importantly, our framework does not require additional human annotations, offering strong generalizability and practicality for real-world applications. Extensive experiments demonstrate that PALE achieves superior hallucination detection performance, outperforming the competitive baseline by a significant margin of 6.55%.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15977v1",
    "published_date": "2025-10-13 02:06:15 UTC",
    "updated_date": "2025-10-13 02:06:15 UTC"
  },
  {
    "arxiv_id": "2510.11759v1",
    "title": "AwareCompiler: Agentic Context-Aware Compiler Optimization via a Synergistic Knowledge-Data Driven Framework",
    "authors": [
      "Hongyu Lin",
      "Haolin Pan",
      "Haoran Luo",
      "Yuchen Li",
      "Kaichun Yao",
      "Libo Zhang",
      "Mingjie Xing",
      "Yanjun Wu"
    ],
    "abstract": "Compiler optimization is crucial for enhancing program performance by transforming the sequence of optimization passes while maintaining correctness. Despite the promising potential of large language models (LLMs)-based agent for software optimization, automating compiler optimization remains challenging due to: (1) semantic misalignment between abstract program representations and concrete optimization passes, (2) inefficient interaction mechanisms between agents and compiler environments, and (3) reward sparsity from the extensive decision-making process within large optimization spaces. This paper introduces \\textbf{AwareCompiler}, an agentic framework for compiler optimization that addresses these challenges through three key innovations: structured knowledge integration and dataset construction, knowledge-driven adaptive pass generation, and data-driven hybrid training pipeline. Experimental results on standard benchmarks demonstrate that AwareCompiler significantly outperforms existing baselines in both performance and efficiency, highlighting the effectiveness of our synergistic knowledge-data-driven approach. Our code is publicly available at https://github.com/LHY-24/AwareCompiler.",
    "categories": [
      "cs.PL",
      "cs.AI"
    ],
    "primary_category": "cs.PL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11759v1",
    "published_date": "2025-10-13 02:02:36 UTC",
    "updated_date": "2025-10-13 02:02:36 UTC"
  },
  {
    "arxiv_id": "2510.11758v2",
    "title": "The Adoption Paradox for Veterinary Professionals in China: High Use of Artificial Intelligence Despite Low Familiarity",
    "authors": [
      "Shumin Li",
      "Xiaoyun Lai"
    ],
    "abstract": "While the global integration of artificial intelligence (AI) into veterinary medicine is accelerating, its adoption dynamics in major markets such as China remain uncharacterized. This paper presents the first exploratory analysis of AI perception and adoption among veterinary professionals in China, based on a cross-sectional survey of 455 practitioners conducted in mid-2025. We identify a distinct \"adoption paradox\": although 71.0% of respondents have incorporated AI into their workflows, 44.6% of these active users report low familiarity with the technology. In contrast to the administrative-focused patterns observed in North America, adoption in China is practitioner-driven and centers on core clinical tasks, such as disease diagnosis (50.1%) and prescription calculation (44.8%). However, concerns regarding reliability and accuracy remain the primary barrier (54.3%), coexisting with a strong consensus (93.8%) for regulatory oversight. These findings suggest a unique \"inside-out\" integration model in China, characterized by high clinical utility but restricted by an \"interpretability gap,\" underscoring the need for specialized tools and robust regulatory frameworks to safely harness AI's potential in this expanding market.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "2 Tables, 5 Figures (included in the end), Supplemental Material is included in the end",
    "pdf_url": "https://arxiv.org/pdf/2510.11758v2",
    "published_date": "2025-10-13 01:50:11 UTC",
    "updated_date": "2025-12-10 07:58:42 UTC"
  },
  {
    "arxiv_id": "2510.10895v1",
    "title": "LLM-Empowered Agentic MAC Protocols: A Dynamic Stackelberg Game Approach",
    "authors": [
      "Renxuan Tan",
      "Rongpeng Li",
      "Fei Wang",
      "Chenghui Peng",
      "Shaoyun Wu",
      "Zhifeng Zhao",
      "Honggang Zhang"
    ],
    "abstract": "Medium Access Control (MAC) protocols, essential for wireless networks, are typically manually configured. While deep reinforcement learning (DRL)-based protocols enhance task-specified network performance, they suffer from poor generalizability and resilience, demanding costly retraining to adapt to dynamic environments. To overcome this limitation, we introduce a game-theoretic LLM-empowered multi-agent DRL (MARL) framework, in which the uplink transmission between a base station and a varying number of user equipments is modeled as a dynamic multi-follower Stackelberg game (MFSG), capturing the network's natural hierarchical structure. Within this game, LLM-driven agents, coordinated through proximal policy optimization (PPO), synthesize adaptive, semantic MAC protocols in response to network dynamics. Protocol action grammar (PAG) is employed to ensure the reliability and efficiency of this process. Under this system, we further analyze the existence and convergence behavior in terms of a Stackelberg equilibrium by studying the learning dynamics of LLM-empowered unified policies in response to changing followers. Simulations corroborate that our framework achieves a 77.6% greater throughput and a 65.2% fairness improvement over conventional baselines. Besides, our framework generalizes excellently to a fluctuating number of users without requiring retraining or architectural changes.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "This work has been submitted to IEEE for possible publication",
    "pdf_url": "https://arxiv.org/pdf/2510.10895v1",
    "published_date": "2025-10-13 01:47:24 UTC",
    "updated_date": "2025-10-13 01:47:24 UTC"
  },
  {
    "arxiv_id": "2510.10889v1",
    "title": "Topological Alignment of Shared Vision-Language Embedding Space",
    "authors": [
      "Junwon You",
      "Dasol Kang",
      "Jae-Hun Jung"
    ],
    "abstract": "Contrastive Vision-Language Models (VLMs) have demonstrated strong zero-shot capabilities. However, their cross-modal alignment remains biased toward English due to limited multilingual multimodal data. Recent multilingual extensions have alleviated this gap but enforce instance-level alignment while neglecting the global geometry of the shared embedding space. We address this problem by introducing ToMCLIP (Topological Alignment for Multilingual CLIP), a topology-aware framework aligning embedding spaces with topology-preserving constraints. The proposed method applies persistent homology to define a topological alignment loss and approximates persistence diagram with theoretical error bounds using graph sparsification strategy. This work validates the proposed approach, showing enhanced structural coherence of multilingual representations, higher zero-shot accuracy on the CIFAR-100, and stronger multilingual retrieval performance on the xFlickr&CO. Beyond VLMs, the proposed approach provides a general method for incorporating topological alignment into representation learning.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "24 pages, 5 figures, 19 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.10889v1",
    "published_date": "2025-10-13 01:36:38 UTC",
    "updated_date": "2025-10-13 01:36:38 UTC"
  },
  {
    "arxiv_id": "2510.10887v1",
    "title": "Generative AI for Software Project Management: Insights from a Review of Software Practitioner Literature",
    "authors": [
      "Lakshana Iruni Assalaarachchi",
      "Zainab Masood",
      "Rashina Hoda",
      "John Grundy"
    ],
    "abstract": "Software practitioners are discussing GenAI transformations in software project management openly and widely. To understand the state of affairs, we performed a grey literature review using 47 publicly available practitioner sources including blogs, articles, and industry reports. We found that software project managers primarily perceive GenAI as an \"assistant\", \"copilot\", or \"friend\" rather than as a \"PM replacement\", with support of GenAI in automating routine tasks, predictive analytics, communication and collaboration, and in agile practices leading to project success. Practitioners emphasize responsible GenAI usage given concerns such as hallucinations, ethics and privacy, and lack of emotional intelligence and human judgment. We present upskilling requirements for software project managers in the GenAI era mapped to the Project Management Institute's talent triangle. We share key recommendations for both practitioners and researchers.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10887v1",
    "published_date": "2025-10-13 01:35:17 UTC",
    "updated_date": "2025-10-13 01:35:17 UTC"
  },
  {
    "arxiv_id": "2510.13859v1",
    "title": "Benchmarking Correctness and Security in Multi-Turn Code Generation",
    "authors": [
      "Ruchit Rawal",
      "Jeffrey Yang Fan Chiang",
      "Chihao Shen",
      "Jeffery Siyuan Tian",
      "Aastha Mahajan",
      "Tom Goldstein",
      "Yizheng Chen"
    ],
    "abstract": "AI coding assistants powered by large language models (LLMs) have transformed software development, significantly boosting productivity. While existing benchmarks evaluate the correctness and security of LLM-generated code, they are typically limited to single-turn tasks that do not reflect the iterative nature of real-world development. We introduce MT-Sec, the first benchmark to systematically evaluate both correctness and security in multi-turn coding scenarios. We construct this using a synthetic data pipeline that transforms existing single-turn tasks into semantically aligned multi-turn interaction sequences, allowing reuse of original test suites while modeling the complexity of real-world coding processes. We evaluate 32 open- and closed-source models, and three agent-scaffolding on MT-Sec and observe a consistent 20-27% drop in \"correct and secure\" outputs from single-turn to multi-turn settings -- even among state-of-the-art models. Beyond full-program generation, we also evaluate models on multi-turn code-diff generation -- an unexplored yet practically relevant setting -- and find that models perform worse here, with increased rates of functionally incorrect and insecure outputs. Finally, we find that while agent scaffoldings boost single-turn code generation performance, they are not quite as effective in multi-turn evaluations. Together, these findings highlight the need for benchmarks that jointly evaluate correctness and security in multi-turn, real-world coding workflows.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.13859v1",
    "published_date": "2025-10-13 01:20:46 UTC",
    "updated_date": "2025-10-13 01:20:46 UTC"
  },
  {
    "arxiv_id": "2510.15976v1",
    "title": "Learning to Watermark: A Selective Watermarking Framework for Large Language Models via Multi-Objective Optimization",
    "authors": [
      "Chenrui Wang",
      "Junyi Shu",
      "Billy Chiu",
      "Yu Li",
      "Saleh Alharbi",
      "Min Zhang",
      "Jing Li"
    ],
    "abstract": "The rapid development of LLMs has raised concerns about their potential misuse, leading to various watermarking schemes that typically offer high detectability. However, existing watermarking techniques often face trade-off between watermark detectability and generated text quality. In this paper, we introduce Learning to Watermark (LTW), a novel selective watermarking framework that leverages multi-objective optimization to effectively balance these competing goals. LTW features a lightweight network that adaptively decides when to apply the watermark by analyzing sentence embeddings, token entropy, and current watermarking ratio. Training of the network involves two specifically constructed loss functions that guide the model toward Pareto-optimal solutions, thereby harmonizing watermark detectability and text quality. By integrating LTW with two baseline watermarking methods, our experimental evaluations demonstrate that LTW significantly enhances text quality without compromising detectability. Our selective watermarking approach offers a new perspective for designing watermarks for LLMs and a way to preserve high text quality for watermarks. The code is publicly available at: https://github.com/fattyray/learning-to-watermark",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "28 pages, 11 figures, NeurIPS 2025 Poster",
    "pdf_url": "https://arxiv.org/pdf/2510.15976v1",
    "published_date": "2025-10-13 01:07:38 UTC",
    "updated_date": "2025-10-13 01:07:38 UTC"
  },
  {
    "arxiv_id": "2510.10865v1",
    "title": "GRIP: A Unified Framework for Grid-Based Relay and Co-Occurrence-Aware Planning in Dynamic Environments",
    "authors": [
      "Ahmed Alanazi",
      "Duy Ho",
      "Yugyung Lee"
    ],
    "abstract": "Robots navigating dynamic, cluttered, and semantically complex environments must integrate perception, symbolic reasoning, and spatial planning to generalize across diverse layouts and object categories. Existing methods often rely on static priors or limited memory, constraining adaptability under partial observability and semantic ambiguity. We present GRIP, Grid-based Relay with Intermediate Planning, a unified, modular framework with three scalable variants: GRIP-L (Lightweight), optimized for symbolic navigation via semantic occupancy grids; GRIP-F (Full), supporting multi-hop anchor chaining and LLM-based introspection; and GRIP-R (Real-World), enabling physical robot deployment under perceptual uncertainty. GRIP integrates dynamic 2D grid construction, open-vocabulary object grounding, co-occurrence-aware symbolic planning, and hybrid policy execution using behavioral cloning, D* search, and grid-conditioned control. Empirical results on AI2-THOR and RoboTHOR benchmarks show that GRIP achieves up to 9.6% higher success rates and over $2\\times$ improvement in path efficiency (SPL and SAE) on long-horizon tasks. Qualitative analyses reveal interpretable symbolic plans in ambiguous scenes. Real-world deployment on a Jetbot further validates GRIP's generalization under sensor noise and environmental variation. These results position GRIP as a robust, scalable, and explainable framework bridging simulation and real-world navigation.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "17 pages, 5 figures, 8 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.10865v1",
    "published_date": "2025-10-13 00:13:37 UTC",
    "updated_date": "2025-10-13 00:13:37 UTC"
  },
  {
    "arxiv_id": "2510.10864v1",
    "title": "HeroFilter: Adaptive Spectral Graph Filter for Varying Heterophilic Relations",
    "authors": [
      "Shuaicheng Zhang",
      "Haohui Wang",
      "Junhong Lin",
      "Xiaojie Guo",
      "Yada Zhu",
      "Si Zhang",
      "Dongqi Fu",
      "Dawei Zhou"
    ],
    "abstract": "Graph heterophily, where connected nodes have different labels, has attracted significant interest recently. Most existing works adopt a simplified approach - using low-pass filters for homophilic graphs and high-pass filters for heterophilic graphs. However, we discover that the relationship between graph heterophily and spectral filters is more complex - the optimal filter response varies across frequency components and does not follow a strict monotonic correlation with heterophily degree. This finding challenges conventional fixed filter designs and suggests the need for adaptive filtering to preserve expressiveness in graph embeddings. Formally, natural questions arise: Given a heterophilic graph G, how and to what extent will the varying heterophily degree of G affect the performance of GNNs? How can we design adaptive filters to fit those varying heterophilic connections? Our theoretical analysis reveals that the average frequency response of GNNs and graph heterophily degree do not follow a strict monotonic correlation, necessitating adaptive graph filters to guarantee good generalization performance. Hence, we propose [METHOD NAME], a simple yet powerful GNN, which extracts information across the heterophily spectrum and combines salient representations through adaptive mixing. [METHOD NAME]'s superior performance achieves up to 9.2% accuracy improvement over leading baselines across homophilic and heterophilic graphs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10864v1",
    "published_date": "2025-10-13 00:12:40 UTC",
    "updated_date": "2025-10-13 00:12:40 UTC"
  }
]
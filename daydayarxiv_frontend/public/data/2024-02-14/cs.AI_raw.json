[
  {
    "arxiv_id": "2402.09617v1",
    "title": "LLM-Enhanced User-Item Interactions: Leveraging Edge Information for Optimized Recommendations",
    "authors": [
      "Xinyuan Wang",
      "Liang Wu",
      "Liangjie Hong",
      "Hao Liu",
      "Yanjie Fu"
    ],
    "abstract": "The extraordinary performance of large language models has not only reshaped\nthe research landscape in the field of NLP but has also demonstrated its\nexceptional applicative potential in various domains. However, the potential of\nthese models in mining relationships from graph data remains under-explored.\nGraph neural networks, as a popular research area in recent years, have\nnumerous studies on relationship mining. Yet, current cutting-edge research in\ngraph neural networks has not been effectively integrated with large language\nmodels, leading to limited efficiency and capability in graph relationship\nmining tasks. A primary challenge is the inability of LLMs to deeply exploit\nthe edge information in graphs, which is critical for understanding complex\nnode relationships. This gap limits the potential of LLMs to extract meaningful\ninsights from graph structures, limiting their applicability in more complex\ngraph-based analysis. We focus on how to utilize existing LLMs for mining and\nunderstanding relationships in graph data, applying these techniques to\nrecommendation tasks. We propose an innovative framework that combines the\nstrong contextual representation capabilities of LLMs with the relationship\nextraction and analysis functions of GNNs for mining relationships in graph\ndata. Specifically, we design a new prompt construction framework that\nintegrates relational information of graph data into natural language\nexpressions, aiding LLMs in more intuitively grasping the connectivity\ninformation within graph data. Additionally, we introduce graph relationship\nunderstanding and analysis functions into LLMs to enhance their focus on\nconnectivity information in graph data. Our evaluation on real-world datasets\ndemonstrates the framework's ability to understand connectivity information in\ngraph data.",
    "categories": [
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09617v1",
    "published_date": "2024-02-14 23:12:09 UTC",
    "updated_date": "2024-02-14 23:12:09 UTC"
  },
  {
    "arxiv_id": "2402.09615v6",
    "title": "API Pack: A Massive Multi-Programming Language Dataset for API Call Generation",
    "authors": [
      "Zhen Guo",
      "Adriana Meza Soria",
      "Wei Sun",
      "Yikang Shen",
      "Rameswar Panda"
    ],
    "abstract": "We introduce API Pack, a massive multi-programming language dataset\ncontaining over one million instruction-API calls for improving the API call\ngeneration capabilities of large language models. Our evaluation highlights\nthree key findings: First, fine-tuning on API Pack enables open-source models\nto outperform GPT-3.5 and GPT-4 in generating code for entirely new API calls.\nWe show this by fine-tuning CodeLlama-13B on 20,000 Python instances from API\nPack. Second, fine-tuning on a large dataset in one language, combined with\nsmaller datasets from others, improves API generation accuracy across multiple\nlanguages. Third, we confirm the benefits of larger datasets for API\ngeneralization, as increasing fine-tuning data to one million instances\nenhances generalization to new APIs. To support further research, we\nopen-source the API Pack dataset, trained model, and code at\nhttps://github.com/zguo0525/API-Pack.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09615v6",
    "published_date": "2024-02-14 23:09:15 UTC",
    "updated_date": "2025-02-13 20:05:54 UTC"
  },
  {
    "arxiv_id": "2402.09614v3",
    "title": "Reasoning over Uncertain Text by Generative Large Language Models",
    "authors": [
      "Aliakbar Nafar",
      "Kristen Brent Venable",
      "Parisa Kordjamshidi"
    ],
    "abstract": "This paper considers the challenges Large Language Models (LLMs) face when\nreasoning over text that includes information involving uncertainty explicitly\nquantified via probability values. This type of reasoning is relevant to a\nvariety of contexts ranging from everyday conversations to medical\ndecision-making. Despite improvements in the mathematical reasoning\ncapabilities of LLMs, they still exhibit significant difficulties when it comes\nto probabilistic reasoning. To deal with this problem, we introduce the\nBayesian Linguistic Inference Dataset (BLInD), a new dataset specifically\ndesigned to test the probabilistic reasoning capabilities of LLMs. We use BLInD\nto find out the limitations of LLMs for tasks involving probabilistic\nreasoning. In addition, we present several prompting strategies that map the\nproblem to different formal representations, including Python code,\nprobabilistic algorithms, and probabilistic logical programming. We conclude by\nproviding an evaluation of our methods on BLInD and an adaptation of a causal\nreasoning question-answering dataset. Our empirical results highlight the\neffectiveness of our proposed strategies for multiple LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09614v3",
    "published_date": "2024-02-14 23:05:44 UTC",
    "updated_date": "2024-12-27 18:43:59 UTC"
  },
  {
    "arxiv_id": "2402.09611v2",
    "title": "Towards Privacy-Aware Sign Language Translation at Scale",
    "authors": [
      "Phillip Rust",
      "Bowen Shi",
      "Skyler Wang",
      "Necati Cihan Camg√∂z",
      "Jean Maillard"
    ],
    "abstract": "A major impediment to the advancement of sign language translation (SLT) is\ndata scarcity. Much of the sign language data currently available on the web\ncannot be used for training supervised models due to the lack of aligned\ncaptions. Furthermore, scaling SLT using large-scale web-scraped datasets bears\nprivacy risks due to the presence of biometric information, which the\nresponsible development of SLT technologies should account for. In this work,\nwe propose a two-stage framework for privacy-aware SLT at scale that addresses\nboth of these issues. We introduce SSVP-SLT, which leverages self-supervised\nvideo pretraining on anonymized and unannotated videos, followed by supervised\nSLT finetuning on a curated parallel dataset. SSVP-SLT achieves\nstate-of-the-art finetuned and zero-shot gloss-free SLT performance on the\nHow2Sign dataset, outperforming the strongest respective baselines by over 3\nBLEU-4. Based on controlled experiments, we further discuss the advantages and\nlimitations of self-supervised pretraining and anonymization via facial\nobfuscation for SLT.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.09611v2",
    "published_date": "2024-02-14 22:57:03 UTC",
    "updated_date": "2024-08-07 19:27:53 UTC"
  },
  {
    "arxiv_id": "2402.09609v1",
    "title": "LogicPrpBank: A Corpus for Logical Implication and Equivalence",
    "authors": [
      "Zhexiong Liu",
      "Jing Zhang",
      "Jiaying Lu",
      "Wenjing Ma",
      "Joyce C Ho"
    ],
    "abstract": "Logic reasoning has been critically needed in problem-solving and\ndecision-making. Although Language Models (LMs) have demonstrated capabilities\nof handling multiple reasoning tasks (e.g., commonsense reasoning), their\nability to reason complex mathematical problems, specifically propositional\nlogic, remains largely underexplored. This lack of exploration can be\nattributed to the limited availability of annotated corpora. Here, we present a\nwell-labeled propositional logic corpus, LogicPrpBank, containing 7093\nPropositional Logic Statements (PLSs) across six mathematical subjects, to\nstudy a brand-new task of reasoning logical implication and equivalence. We\nbenchmark LogicPrpBank with widely-used LMs to show that our corpus offers a\nuseful resource for this challenging task and there is ample room for model\nimprovement.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "In the 5th AI4ED Workshop, held in conjunction with The 38th AAAI\n  Conference on Artificial Intelligence, February 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.09609v1",
    "published_date": "2024-02-14 22:36:07 UTC",
    "updated_date": "2024-02-14 22:36:07 UTC"
  },
  {
    "arxiv_id": "2402.09604v2",
    "title": "Medical Image Segmentation with InTEnt: Integrated Entropy Weighting for Single Image Test-Time Adaptation",
    "authors": [
      "Haoyu Dong",
      "Nicholas Konz",
      "Hanxue Gu",
      "Maciej A. Mazurowski"
    ],
    "abstract": "Test-time adaptation (TTA) refers to adapting a trained model to a new domain\nduring testing. Existing TTA techniques rely on having multiple test images\nfrom the same domain, yet this may be impractical in real-world applications\nsuch as medical imaging, where data acquisition is expensive and imaging\nconditions vary frequently. Here, we approach such a task, of adapting a\nmedical image segmentation model with only a single unlabeled test image. Most\nTTA approaches, which directly minimize the entropy of predictions, fail to\nimprove performance significantly in this setting, in which we also observe the\nchoice of batch normalization (BN) layer statistics to be a highly important\nyet unstable factor due to only having a single test domain example. To\novercome this, we propose to instead integrate over predictions made with\nvarious estimates of target domain statistics between the training and test\nstatistics, weighted based on their entropy statistics. Our method, validated\non 24 source/target domain splits across 3 medical image datasets surpasses the\nleading method by 2.9% Dice coefficient on average.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Code and pre-trained weights:\n  https://github.com/mazurowski-lab/single-image-test-time-adaptation",
    "pdf_url": "http://arxiv.org/pdf/2402.09604v2",
    "published_date": "2024-02-14 22:26:07 UTC",
    "updated_date": "2024-02-16 15:53:27 UTC"
  },
  {
    "arxiv_id": "2402.09603v1",
    "title": "Scalable Graph Self-Supervised Learning",
    "authors": [
      "Ali Saheb Pasand",
      "Reza Moravej",
      "Mahdi Biparva",
      "Raika Karimi",
      "Ali Ghodsi"
    ],
    "abstract": "In regularization Self-Supervised Learning (SSL) methods for graphs,\ncomputational complexity increases with the number of nodes in graphs and\nembedding dimensions. To mitigate the scalability of non-contrastive graph SSL,\nwe propose a novel approach to reduce the cost of computing the covariance\nmatrix for the pre-training loss function with volume-maximization terms. Our\nwork focuses on reducing the cost associated with the loss computation via\ngraph node or dimension sampling. We provide theoretical insight into why\ndimension sampling would result in accurate loss computations and support it\nwith mathematical derivation of the novel approach. We develop our experimental\nsetup on the node-level graph prediction tasks, where SSL pre-training has\nshown to be difficult due to the large size of real world graphs. Our\nexperiments demonstrate that the cost associated with the loss computation can\nbe reduced via node or dimension sampling without lowering the downstream\nperformance. Our results demonstrate that sampling mostly results in improved\ndownstream performance. Ablation studies and experimental analysis are provided\nto untangle the role of the different factors in the experimental setup.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09603v1",
    "published_date": "2024-02-14 22:23:35 UTC",
    "updated_date": "2024-02-14 22:23:35 UTC"
  },
  {
    "arxiv_id": "2403.12075v3",
    "title": "Adversarial Nibbler: An Open Red-Teaming Method for Identifying Diverse Harms in Text-to-Image Generation",
    "authors": [
      "Jessica Quaye",
      "Alicia Parrish",
      "Oana Inel",
      "Charvi Rastogi",
      "Hannah Rose Kirk",
      "Minsuk Kahng",
      "Erin van Liemt",
      "Max Bartolo",
      "Jess Tsang",
      "Justin White",
      "Nathan Clement",
      "Rafael Mosquera",
      "Juan Ciro",
      "Vijay Janapa Reddi",
      "Lora Aroyo"
    ],
    "abstract": "With the rise of text-to-image (T2I) generative AI models reaching wide\naudiences, it is critical to evaluate model robustness against non-obvious\nattacks to mitigate the generation of offensive images. By focusing on\n``implicitly adversarial'' prompts (those that trigger T2I models to generate\nunsafe images for non-obvious reasons), we isolate a set of difficult safety\nissues that human creativity is well-suited to uncover. To this end, we built\nthe Adversarial Nibbler Challenge, a red-teaming methodology for crowdsourcing\na diverse set of implicitly adversarial prompts. We have assembled a suite of\nstate-of-the-art T2I models, employed a simple user interface to identify and\nannotate harms, and engaged diverse populations to capture long-tail safety\nissues that may be overlooked in standard testing. The challenge is run in\nconsecutive rounds to enable a sustained discovery and analysis of safety\npitfalls in T2I models.\n  In this paper, we present an in-depth account of our methodology, a\nsystematic study of novel attack strategies and discussion of safety failures\nrevealed by challenge participants. We also release a companion visualization\ntool for easy exploration and derivation of insights from the dataset. The\nfirst challenge round resulted in over 10k prompt-image pairs with machine\nannotations for safety. A subset of 1.5k samples contains rich human\nannotations of harm types and attack styles. We find that 14% of images that\nhumans consider harmful are mislabeled as ``safe'' by machines. We have\nidentified new attack strategies that highlight the complexity of ensuring T2I\nmodel robustness. Our findings emphasize the necessity of continual auditing\nand adaptation as new vulnerabilities emerge. We are confident that this work\nwill enable proactive, iterative safety assessments and promote responsible\ndevelopment of T2I models.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CR",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "10 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.12075v3",
    "published_date": "2024-02-14 22:21:12 UTC",
    "updated_date": "2024-05-14 01:24:50 UTC"
  },
  {
    "arxiv_id": "2402.09592v1",
    "title": "A Web-Based Tool for Automatic Data Collection, Curation, and Visualization of Complex Healthcare Survey Studies including Social Network Analysis",
    "authors": [
      "Jos√© Alberto Ben√≠tez-Andrades",
      "Jos√© Emilio Labra",
      "Enedina Quiroga",
      "Vicente Mart√≠n",
      "Isa√≠as Garc√≠a",
      "Pilar Marqu√©s-S√°nchez",
      "Carmen Benavides"
    ],
    "abstract": "There is a great concern nowadays regarding alcohol consumption and drug\nabuse, especially in young people. Analyzing the social environment where these\nadolescents are immersed, as well as a series of measures determining the\nalcohol abuse risk or personal situation and perception using a number of\nquestionnaires like AUDIT, FAS, KIDSCREEN, and others, it is possible to gain\ninsight into the current situation of a given individual regarding his/her\nconsumption behavior. But this analysis, in order to be achieved, requires the\nuse of tools that can ease the process of questionnaire creation, data\ngathering, curation and representation, and later analysis and visualization to\nthe user. This research presents the design and construction of a web-based\nplatform able to facilitate each of the mentioned processes by integrating the\ndifferent phases into an intuitive system with a graphical user interface that\nhides the complexity underlying each of the questionnaires and techniques used\nand presenting the results in a flexible and visual way, avoiding any manual\nhandling of data during the process. Advantages of this approach are shown and\ncompared to the previous situation where some of the tasks were accomplished by\ntime consuming and error prone manipulations of data.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09592v1",
    "published_date": "2024-02-14 21:37:59 UTC",
    "updated_date": "2024-02-14 21:37:59 UTC"
  },
  {
    "arxiv_id": "2402.09588v2",
    "title": "Emerging Opportunities of Using Large Language Models for Translation Between Drug Molecules and Indications",
    "authors": [
      "David Oniani",
      "Jordan Hilsman",
      "Chengxi Zang",
      "Junmei Wang",
      "Lianjin Cai",
      "Jan Zawala",
      "Yanshan Wang"
    ],
    "abstract": "A drug molecule is a substance that changes the organism's mental or physical\nstate. Every approved drug has an indication, which refers to the therapeutic\nuse of that drug for treating a particular medical condition. While the Large\nLanguage Model (LLM), a generative Artificial Intelligence (AI) technique, has\nrecently demonstrated effectiveness in translating between molecules and their\ntextual descriptions, there remains a gap in research regarding their\napplication in facilitating the translation between drug molecules and\nindications, or vice versa, which could greatly benefit the drug discovery\nprocess. The capability of generating a drug from a given indication would\nallow for the discovery of drugs targeting specific diseases or targets and\nultimately provide patients with better treatments. In this paper, we first\npropose a new task, which is the translation between drug molecules and\ncorresponding indications, and then test existing LLMs on this new task.\nSpecifically, we consider nine variations of the T5 LLM and evaluate them on\ntwo public datasets obtained from ChEMBL and DrugBank. Our experiments show the\nearly results of using LLMs for this task and provide a perspective on the\nstate-of-the-art. We also emphasize the current limitations and discuss future\nwork that has the potential to improve the performance on this task. The\ncreation of molecules from indications, or vice versa, will allow for more\nefficient targeting of diseases and significantly reduce the cost of drug\ndiscovery, with the potential to revolutionize the field of drug discovery in\nthe era of generative AI.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09588v2",
    "published_date": "2024-02-14 21:33:13 UTC",
    "updated_date": "2024-02-16 20:55:08 UTC"
  },
  {
    "arxiv_id": "2402.09584v2",
    "title": "Large Language Model-Based Interpretable Machine Learning Control in Building Energy Systems",
    "authors": [
      "Liang Zhang",
      "Zhelun Chen"
    ],
    "abstract": "The potential of Machine Learning Control (MLC) in HVAC systems is hindered\nby its opaque nature and inference mechanisms, which is challenging for users\nand modelers to fully comprehend, ultimately leading to a lack of trust in\nMLC-based decision-making. To address this challenge, this paper investigates\nand explores Interpretable Machine Learning (IML), a branch of Machine Learning\n(ML) that enhances transparency and understanding of models and their\ninferences, to improve the credibility of MLC and its industrial application in\nHVAC systems. Specifically, we developed an innovative framework that combines\nthe principles of Shapley values and the in-context learning feature of Large\nLanguage Models (LLMs). While the Shapley values are instrumental in dissecting\nthe contributions of various features in ML models, LLM provides an in-depth\nunderstanding of the non-data-driven or rule-based elements in MLC; combining\nthem, LLM further packages these insights into a coherent, human-understandable\nnarrative. The paper presents a case study to demonstrate the feasibility of\nthe developed IML framework for model predictive control-based precooling under\ndemand response events in a virtual testbed. The results indicate that the\ndeveloped framework generates and explains the control signals in accordance\nwith the rule-based rationale.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09584v2",
    "published_date": "2024-02-14 21:19:33 UTC",
    "updated_date": "2024-11-15 18:34:42 UTC"
  },
  {
    "arxiv_id": "2402.09581v2",
    "title": "Combatting deepfakes: Policies to address national security threats and rights violations",
    "authors": [
      "Andrea Miotti",
      "Akash Wasil"
    ],
    "abstract": "This paper provides policy recommendations to address threats from deepfakes.\nFirst, we provide background information about deepfakes and review the harms\nthey pose. We describe how deepfakes are currently used to proliferate sexual\nabuse material, commit fraud, manipulate voter behavior, and pose threats to\nnational security. Second, we review previous legislative proposals designed to\naddress deepfakes. Third, we present a comprehensive policy proposal that\nfocuses on addressing multiple parts of the deepfake supply chain. The deepfake\nsupply chain begins with a small number of model developers, model providers,\nand compute providers, and it expands to include billions of potential deepfake\ncreators. We describe this supply chain in greater detail and describe how\nentities at each step of the supply chain ought to take reasonable measures to\nprevent the creation and proliferation of deepfakes. Finally, we address\npotential counterpoints of our proposal. Overall, deepfakes will present\nincreasingly severe threats to global security and individual liberties. To\naddress these threats, we call on policymakers to enact legislation that\naddresses multiple parts of the deepfake supply chain.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09581v2",
    "published_date": "2024-02-14 21:05:55 UTC",
    "updated_date": "2024-02-19 18:39:40 UTC"
  },
  {
    "arxiv_id": "2402.09579v2",
    "title": "Advancing Building Energy Modeling with Large Language Models: Exploration and Case Studies",
    "authors": [
      "Liang Zhang",
      "Zhelun Chen",
      "Vitaly Ford"
    ],
    "abstract": "The rapid progression in artificial intelligence has facilitated the\nemergence of large language models like ChatGPT, offering potential\napplications extending into specialized engineering modeling, especially\nphysics-based building energy modeling. This paper investigates the innovative\nintegration of large language models with building energy modeling software,\nfocusing specifically on the fusion of ChatGPT with EnergyPlus. A literature\nreview is first conducted to reveal a growing trend of incorporating large\nlanguage models in engineering modeling, albeit limited research on their\napplication in building energy modeling. We underscore the potential of large\nlanguage models in addressing building energy modeling challenges and outline\npotential applications including simulation input generation, simulation output\nanalysis and visualization, conducting error analysis, co-simulation,\nsimulation knowledge extraction and training, and simulation optimization.\nThree case studies reveal the transformative potential of large language models\nin automating and optimizing building energy modeling tasks, underscoring the\npivotal role of artificial intelligence in advancing sustainable building\npractices and energy efficiency. The case studies demonstrate that selecting\nthe right large language model techniques is essential to enhance performance\nand reduce engineering efforts. The findings advocate a multidisciplinary\napproach in future artificial intelligence research, with implications\nextending beyond building energy modeling to other specialized engineering\nmodeling.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09579v2",
    "published_date": "2024-02-14 21:02:07 UTC",
    "updated_date": "2024-11-15 18:20:23 UTC"
  },
  {
    "arxiv_id": "2402.09565v2",
    "title": "Graph-Skeleton: ~1% Nodes are Sufficient to Represent Billion-Scale Graph",
    "authors": [
      "Linfeng Cao",
      "Haoran Deng",
      "Yang Yang",
      "Chunping Wang",
      "Lei Chen"
    ],
    "abstract": "Due to the ubiquity of graph data on the web, web graph mining has become a\nhot research spot. Nonetheless, the prevalence of large-scale web graphs in\nreal applications poses significant challenges to storage, computational\ncapacity and graph model design. Despite numerous studies to enhance the\nscalability of graph models, a noticeable gap remains between academic research\nand practical web graph mining applications. One major cause is that in most\nindustrial scenarios, only a small part of nodes in a web graph are actually\nrequired to be analyzed, where we term these nodes as target nodes, while\nothers as background nodes. In this paper, we argue that properly fetching and\ncondensing the background nodes from massive web graph data might be a more\neconomical shortcut to tackle the obstacles fundamentally. To this end, we make\nthe first attempt to study the problem of massive background nodes compression\nfor target nodes classification. Through extensive experiments, we reveal two\ncritical roles played by the background nodes in target node classification:\nenhancing structural connectivity between target nodes, and feature correlation\nwith target nodes. Followingthis, we propose a novel Graph-Skeleton1 model,\nwhich properly fetches the background nodes, and further condenses the semantic\nand topological information of background nodes within similar\ntarget-background local structures. Extensive experiments on various web graph\ndatasets demonstrate the effectiveness and efficiency of the proposed method.\nIn particular, for MAG240M dataset with 0.24 billion nodes, our generated\nskeleton graph achieves highly comparable performance while only containing\n1.8% nodes of the original graph.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "21 pages, 11 figures, In Proceedings of the ACM Web Conference 2024\n  (WWW'24)",
    "pdf_url": "http://arxiv.org/pdf/2402.09565v2",
    "published_date": "2024-02-14 20:33:11 UTC",
    "updated_date": "2024-03-06 22:22:33 UTC"
  },
  {
    "arxiv_id": "2402.09558v3",
    "title": "Bidirectional Generative Pre-training for Improving Healthcare Time-series Representation Learning",
    "authors": [
      "Ziyang Song",
      "Qincheng Lu",
      "He Zhu",
      "David Buckeridge",
      "Yue Li"
    ],
    "abstract": "Learning time-series representations for discriminative tasks, such as\nclassification and regression, has been a long-standing challenge in the\nhealthcare domain. Current pre-training methods are limited in either\nunidirectional next-token prediction or randomly masked token prediction. We\npropose a novel architecture called Bidirectional Timely Generative Pre-trained\nTransformer (BiTimelyGPT), which pre-trains on biosignals and longitudinal\nclinical records by both next-token and previous-token prediction in\nalternating transformer layers. This pre-training task preserves original\ndistribution and data shapes of the time-series. Additionally, the full-rank\nforward and backward attention matrices exhibit more expressive representation\ncapabilities. Using biosignals and longitudinal clinical records, BiTimelyGPT\ndemonstrates superior performance in predicting neurological functionality,\ndisease diagnosis, and physiological signs. By visualizing the attention\nheatmap, we observe that the pre-trained BiTimelyGPT can identify\ndiscriminative segments from biosignal time-series sequences, even more so\nafter fine-tuning on the task.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09558v3",
    "published_date": "2024-02-14 20:19:24 UTC",
    "updated_date": "2024-08-23 18:25:37 UTC"
  },
  {
    "arxiv_id": "2402.09553v1",
    "title": "Statistical and Machine Learning Models for Predicting Fire and Other Emergency Events",
    "authors": [
      "Dilli Prasad Sharma",
      "Nasim Beigi-Mohammadi",
      "Hongxiang Geng",
      "Dawn Dixon",
      "Rob Madro",
      "Phil Emmenegger",
      "Carlos Tobar",
      "Jeff Li",
      "Alberto Leon-Garcia"
    ],
    "abstract": "Emergency events in a city cause considerable economic loss to individuals,\ntheir families, and the community. Accurate and timely prediction of events can\nhelp the emergency fire and rescue services in preparing for and mitigating the\nconsequences of emergency events. In this paper, we present a systematic\ndevelopment of predictive models for various types of emergency events in the\nCity of Edmonton, Canada. We present methods for (i) data collection and\ndataset development; (ii) descriptive analysis of each event type and its\ncharacteristics at different spatiotemporal levels; (iii) feature analysis and\nselection based on correlation coefficient analysis and feature importance\nanalysis; and (iv) development of prediction models for the likelihood of\noccurrence of each event type at different temporal and spatial resolutions. We\nanalyze the association of event types with socioeconomic and demographic data\nat the neighborhood level, identify a set of predictors for each event type,\nand develop predictive models with negative binomial regression. We conduct\nevaluations at neighborhood and fire station service area levels. Our results\nshow that the models perform well for most of the event types with acceptable\nprediction errors for weekly and monthly periods. The evaluation shows that the\nprediction accuracy is consistent at the level of the fire station, so the\npredictions can be used in management by fire rescue service departments for\nplanning resource allocation for these time periods. We also examine the impact\nof the COVID-19 pandemic on the occurrence of events and on the accuracy of\nevent predictor models. Our findings show that COVID-19 had a significant\nimpact on the performance of the event prediction models.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09553v1",
    "published_date": "2024-02-14 20:10:30 UTC",
    "updated_date": "2024-02-14 20:10:30 UTC"
  },
  {
    "arxiv_id": "2402.09546v1",
    "title": "How Secure Are Large Language Models (LLMs) for Navigation in Urban Environments?",
    "authors": [
      "Congcong Wen",
      "Jiazhao Liang",
      "Shuaihang Yuan",
      "Hao Huang",
      "Yi Fang"
    ],
    "abstract": "In the field of robotics and automation, navigation systems based on Large\nLanguage Models (LLMs) have recently shown impressive performance. However, the\nsecurity aspects of these systems have received relatively less attention. This\npaper pioneers the exploration of vulnerabilities in LLM-based navigation\nmodels in urban outdoor environments, a critical area given the technology's\nwidespread application in autonomous driving, logistics, and emergency\nservices. Specifically, we introduce a novel Navigational Prompt Suffix (NPS)\nAttack that manipulates LLM-based navigation models by appending\ngradient-derived suffixes to the original navigational prompt, leading to\nincorrect actions. We conducted comprehensive experiments on an LLMs-based\nnavigation model that employs various LLMs for reasoning. Our results, derived\nfrom the Touchdown and Map2Seq street-view datasets under both few-shot\nlearning and fine-tuning configurations, demonstrate notable performance\ndeclines across three metrics in the face of both white-box and black-box\nattacks. These results highlight the generalizability and transferability of\nthe NPS Attack, emphasizing the need for enhanced security in LLM-based\nnavigation systems. As an initial countermeasure, we propose the Navigational\nPrompt Engineering (NPE) Defense strategy, concentrating on navigation-relevant\nkeywords to reduce the impact of adversarial suffixes. While initial findings\nindicate that this strategy enhances navigational safety, there remains a\ncritical need for the wider research community to develop stronger defense\nmethods to effectively tackle the real-world challenges faced by these systems.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09546v1",
    "published_date": "2024-02-14 19:45:17 UTC",
    "updated_date": "2024-02-14 19:45:17 UTC"
  },
  {
    "arxiv_id": "2402.09540v1",
    "title": "Why Does Differential Privacy with Large Epsilon Defend Against Practical Membership Inference Attacks?",
    "authors": [
      "Andrew Lowy",
      "Zhuohang Li",
      "Jing Liu",
      "Toshiaki Koike-Akino",
      "Kieran Parsons",
      "Ye Wang"
    ],
    "abstract": "For small privacy parameter $\\epsilon$, $\\epsilon$-differential privacy (DP)\nprovides a strong worst-case guarantee that no membership inference attack\n(MIA) can succeed at determining whether a person's data was used to train a\nmachine learning model. The guarantee of DP is worst-case because: a) it holds\neven if the attacker already knows the records of all but one person in the\ndata set; and b) it holds uniformly over all data sets. In practical\napplications, such a worst-case guarantee may be overkill: practical attackers\nmay lack exact knowledge of (nearly all of) the private data, and our data set\nmight be easier to defend, in some sense, than the worst-case data set. Such\nconsiderations have motivated the industrial deployment of DP models with large\nprivacy parameter (e.g. $\\epsilon \\geq 7$), and it has been observed\nempirically that DP with large $\\epsilon$ can successfully defend against\nstate-of-the-art MIAs. Existing DP theory cannot explain these empirical\nfindings: e.g., the theoretical privacy guarantees of $\\epsilon \\geq 7$ are\nessentially vacuous. In this paper, we aim to close this gap between theory and\npractice and understand why a large DP parameter can prevent practical MIAs. To\ntackle this problem, we propose a new privacy notion called practical\nmembership privacy (PMP). PMP models a practical attacker's uncertainty about\nthe contents of the private data. The PMP parameter has a natural\ninterpretation in terms of the success rate of a practical MIA on a given data\nset. We quantitatively analyze the PMP parameter of two fundamental DP\nmechanisms: the exponential mechanism and Gaussian mechanism. Our analysis\nreveals that a large DP parameter often translates into a much smaller PMP\nparameter, which guarantees strong privacy against practical MIAs. Using our\nfindings, we offer principled guidance for practitioners in choosing the DP\nparameter.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "68P27"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted at PPAI-24: AAAI Workshop on Privacy-Preserving Artificial\n  Intelligence",
    "pdf_url": "http://arxiv.org/pdf/2402.09540v1",
    "published_date": "2024-02-14 19:31:45 UTC",
    "updated_date": "2024-02-14 19:31:45 UTC"
  },
  {
    "arxiv_id": "2402.09508v3",
    "title": "Arrange, Inpaint, and Refine: Steerable Long-term Music Audio Generation and Editing via Content-based Controls",
    "authors": [
      "Liwei Lin",
      "Gus Xia",
      "Yixiao Zhang",
      "Junyan Jiang"
    ],
    "abstract": "Controllable music generation plays a vital role in human-AI music\nco-creation. While Large Language Models (LLMs) have shown promise in\ngenerating high-quality music, their focus on autoregressive generation limits\ntheir utility in music editing tasks. To address this gap, we propose a novel\napproach leveraging a parameter-efficient heterogeneous adapter combined with a\nmasking training scheme. This approach enables autoregressive language models\nto seamlessly address music inpainting tasks. Additionally, our method\nintegrates frame-level content-based controls, facilitating track-conditioned\nmusic refinement and score-conditioned music arrangement. We apply this method\nto fine-tune MusicGen, a leading autoregressive music generation model. Our\nexperiments demonstrate promising results across multiple music editing tasks,\noffering more flexible controls for future AI-driven music editing tools. The\nsource codes and a demo page showcasing our work are available at\nhttps://kikyo-16.github.io/AIR.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09508v3",
    "published_date": "2024-02-14 19:00:01 UTC",
    "updated_date": "2024-10-06 21:26:48 UTC"
  },
  {
    "arxiv_id": "2402.09500v1",
    "title": "On Formally Undecidable Traits of Intelligent Machines",
    "authors": [
      "Matthew Fox"
    ],
    "abstract": "Building on work by Alfonseca et al. (2021), we study the conditions\nnecessary for it to be logically possible to prove that an arbitrary\nartificially intelligent machine will exhibit certain behavior. To do this, we\ndevelop a formalism like -- but mathematically distinct from -- the theory of\nformal languages and their properties. Our formalism affords a precise means\nfor not only talking about the traits we desire of machines (such as them being\nintelligent, contained, moral, and so forth), but also for detailing the\nconditions necessary for it to be logically possible to decide whether a given\narbitrary machine possesses such a trait or not. Contrary to Alfonseca et al.'s\n(2021) results, we find that Rice's theorem from computability theory cannot in\ngeneral be used to determine whether an arbitrary machine possesses a given\ntrait or not. Therefore, it is not necessarily the case that deciding whether\nan arbitrary machine is intelligent, contained, moral, and so forth is\nlogically impossible.",
    "categories": [
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "34 pages",
    "pdf_url": "http://arxiv.org/pdf/2402.09500v1",
    "published_date": "2024-02-14 18:59:37 UTC",
    "updated_date": "2024-02-14 18:59:37 UTC"
  },
  {
    "arxiv_id": "2402.09404v1",
    "title": "AQA-Bench: An Interactive Benchmark for Evaluating LLMs' Sequential Reasoning Ability",
    "authors": [
      "Siwei Yang",
      "Bingchen Zhao",
      "Cihang Xie"
    ],
    "abstract": "This paper introduces AQA-Bench, a novel benchmark to assess the sequential\nreasoning capabilities of large language models (LLMs) in algorithmic contexts,\nsuch as depth-first search (DFS). The key feature of our evaluation benchmark\nlies in its interactive evaluation protocol -- for example, in DFS, the\navailability of each node's connected edge is contingent upon the model's\ntraversal to that node, thereby necessitating the LLM's ability to effectively\nremember visited nodes and strategize subsequent moves. We comprehensively\nbuild AQA-Bench with three different algorithms, namely binary search,\ndepth-first search, and breadth-first search, and to evaluate the sequential\nreasoning ability of 12 different LLMs. Our investigations reveal several\ninteresting findings: (1) Closed-source models like GPT-4 and Gemini generally\nshow strong sequential reasoning ability, significantly outperforming\nopen-source LLMs. (2) Naively providing interactive examples may inadvertently\nhurt few-shot performance. (3) A very limited number of predecessor steps\nfollowing the optimal policy can substantially boost small models' performance.\n(4) The scaling correlation between performance and model size is not always\nsignificant, sometimes even showcasing an inverse trend. We hope our study can\ncatalyze future work on advancing the understanding and enhancement of LLMs'\ncapabilities in sequential reasoning. The code is available at\nhttps://github.com/UCSC-VLAA/AQA-Bench.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09404v1",
    "published_date": "2024-02-14 18:59:33 UTC",
    "updated_date": "2024-02-14 18:59:33 UTC"
  },
  {
    "arxiv_id": "2402.09401v2",
    "title": "Reinforcement Learning from Human Feedback with Active Queries",
    "authors": [
      "Kaixuan Ji",
      "Jiafan He",
      "Quanquan Gu"
    ],
    "abstract": "Aligning large language models (LLM) with human preference plays a key role\nin building modern generative models and can be achieved by reinforcement\nlearning from human feedback (RLHF). Despite their superior performance,\ncurrent RLHF approaches often require a large amount of human-labelled\npreference data, which is expensive to collect. In this paper, inspired by the\nsuccess of active learning, we address this problem by proposing\nquery-efficient RLHF methods. We first formalize the alignment problem as a\ncontextual dueling bandit problem and design an active-query-based proximal\npolicy optimization (APPO) algorithm with an $\\tilde{O}(d^2/\\Delta)$\ninstance-dependent regret bound and an $\\tilde{O}(d^2/\\Delta^2)$ query\ncomplexity, where $d$ is the dimension of feature space and $\\Delta$ is the\nsub-optimality gap over all the contexts. We then propose ADPO, a practical\nversion of our algorithm based on direct preference optimization (DPO) and\napply it to fine-tuning LLMs. Our experiments show that ADPO, while only making\nabout half of queries for human preference, matches the performance of the\nstate-of-the-art DPO method.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "28 pages, 1 figure, 4 table",
    "pdf_url": "http://arxiv.org/pdf/2402.09401v2",
    "published_date": "2024-02-14 18:58:40 UTC",
    "updated_date": "2025-02-11 18:18:59 UTC"
  },
  {
    "arxiv_id": "2402.09398v2",
    "title": "Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference",
    "authors": [
      "Harry Dong",
      "Xinyu Yang",
      "Zhenyu Zhang",
      "Zhangyang Wang",
      "Yuejie Chi",
      "Beidi Chen"
    ],
    "abstract": "Many computational factors limit broader deployment of large language models.\nIn this paper, we focus on a memory bottleneck imposed by the key-value (KV)\ncache, a computational shortcut that requires storing previous KV pairs during\ndecoding. While existing KV cache methods approach this problem by pruning or\nevicting large swaths of relatively less important KV pairs to dramatically\nreduce the memory footprint of the cache, they can have limited success in\ntasks that require recollecting a majority of previous tokens. To alleviate\nthis issue, we propose LESS, a simple integration of a (nearly free) constant\nsized cache with eviction-based cache methods, such that all tokens can be\nqueried at later decoding steps. Its ability to retain information throughout\ntime shows merit on a variety of tasks where we demonstrate LESS can help\nreduce the performance gap from caching everything, sometimes even matching it,\nall while being efficient. Relevant code can be found at\nhttps://github.com/hdong920/LESS.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09398v2",
    "published_date": "2024-02-14 18:54:56 UTC",
    "updated_date": "2024-06-12 06:08:58 UTC"
  },
  {
    "arxiv_id": "2402.10240v2",
    "title": "A Dynamical View of the Question of Why",
    "authors": [
      "Mehdi Fatemi",
      "Sindhu Gowda"
    ],
    "abstract": "We address causal reasoning in multivariate time series data generated by\nstochastic processes. Existing approaches are largely restricted to static\nsettings, ignoring the continuity and emission of variations across time. In\ncontrast, we propose a learning paradigm that directly establishes causation\nbetween events in the course of time. We present two key lemmas to compute\ncausal contributions and frame them as reinforcement learning problems. Our\napproach offers formal and computational tools for uncovering and quantifying\ncausal relationships in diffusion processes, subsuming various important\nsettings such as discrete-time Markov decision processes. Finally, in fairly\nintricate experiments and through sheer learning, our framework reveals and\nquantifies causal links, which otherwise seem inexplicable.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at the Twelfth International Conference on Learning\n  Representations (ICLR'24)",
    "pdf_url": "http://arxiv.org/pdf/2402.10240v2",
    "published_date": "2024-02-14 18:44:05 UTC",
    "updated_date": "2024-02-27 22:11:42 UTC"
  },
  {
    "arxiv_id": "2402.09392v1",
    "title": "LL-GABR: Energy Efficient Live Video Streaming Using Reinforcement Learning",
    "authors": [
      "Adithya Raman",
      "Bekir Turkkan",
      "Tevfik Kosar"
    ],
    "abstract": "Over the recent years, research and development in adaptive bitrate (ABR)\nalgorithms for live video streaming have been successful in improving users'\nquality of experience (QoE) by reducing latency to near real-time levels while\ndelivering higher bitrate videos with minimal rebuffering time. However, the\nQoE models used by these ABR algorithms do not take into account that a large\nportion of live video streaming clients use mobile devices where a higher\nbitrate does not necessarily translate into higher perceived quality. Ignoring\nperceived quality results in playing videos at higher bitrates without a\nsignificant increase in perceptual video quality and becomes a burden for\nbattery-constrained mobile devices due to higher energy consumption. In this\npaper, we propose LL-GABR, a deep reinforcement learning approach that models\nthe QoE using perceived video quality instead of bitrate and uses energy\nconsumption along with other metrics like latency, rebuffering events, and\nsmoothness. LL-GABR makes no assumptions about the underlying video,\nenvironment, or network settings and can operate flexibly on different video\ntitles, each having a different bitrate encoding ladder without additional\nre-training, unlike existing learning-based ABRs. Trace-driven experimental\nresults show that LL-GABR outperforms the state-of-the-art approaches by up to\n44% in terms of perceptual QoE and a 73% increase in energy efficiency as a\nresult of reducing net energy consumption by 11%.",
    "categories": [
      "cs.MM",
      "cs.AI"
    ],
    "primary_category": "cs.MM",
    "comment": "10 pages, 3 figures, 3 Tables",
    "pdf_url": "http://arxiv.org/pdf/2402.09392v1",
    "published_date": "2024-02-14 18:43:19 UTC",
    "updated_date": "2024-02-14 18:43:19 UTC"
  },
  {
    "arxiv_id": "2402.09391v4",
    "title": "LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset",
    "authors": [
      "Botao Yu",
      "Frazier N. Baker",
      "Ziqi Chen",
      "Xia Ning",
      "Huan Sun"
    ],
    "abstract": "Chemistry plays a crucial role in many domains, such as drug discovery and\nmaterial science. While large language models (LLMs) such as GPT-4 exhibit\nremarkable capabilities on natural language processing tasks, existing research\nindicates that their performance on chemistry tasks is discouragingly low. In\nthis paper, however, we demonstrate that our developed LLMs can achieve very\nstrong results on a comprehensive set of chemistry tasks, outperforming the\nmost advanced GPT-4 and Claude 3 Opus by a substantial margin. To accomplish\nthis, we propose SMolInstruct, a large-scale, comprehensive, and high-quality\ndataset for instruction tuning. It contains 14 selected chemistry tasks and\nover three million samples, laying a solid foundation for training and\nevaluating LLMs for chemistry. Using SMolInstruct, we fine-tune a set of\nopen-source LLMs, among which, we find that Mistral serves as the best base\nmodel for chemistry tasks. Our analysis further demonstrates the critical role\nof the proposed dataset in driving the performance improvements.",
    "categories": [
      "cs.AI",
      "cs.CE",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by COLM 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.09391v4",
    "published_date": "2024-02-14 18:42:25 UTC",
    "updated_date": "2024-08-10 13:28:11 UTC"
  },
  {
    "arxiv_id": "2402.09390v2",
    "title": "HGOT: Hierarchical Graph of Thoughts for Retrieval-Augmented In-Context Learning in Factuality Evaluation",
    "authors": [
      "Yihao Fang",
      "Stephen W. Thomas",
      "Xiaodan Zhu"
    ],
    "abstract": "With the widespread adoption of large language models (LLMs) in numerous\napplications, the challenge of factuality and the propensity for hallucinations\nhas emerged as a significant concern. To address this issue, particularly in\nretrieval-augmented in-context learning, we introduce the hierarchical graph of\nthoughts (HGOT), a structured, multi-layered graph approach designed to enhance\nthe retrieval of pertinent passages during in-context learning. The framework\nutilizes the emergent planning capabilities of LLMs, employing the\ndivide-and-conquer strategy to break down complex queries into manageable\nsub-queries. It refines self-consistency majority voting for answer selection,\nwhich incorporates the recently proposed citation recall and precision metrics\nto assess the quality of thoughts, linking an answer's credibility\nintrinsically to the thought's quality. This methodology introduces a weighted\nsystem in majority voting, prioritizing answers based on the citation quality\nof their thoughts. Additionally, we propose a scoring mechanism for evaluating\nretrieved passages, considering factors such as citation frequency and quality,\nself-consistency confidence, and the retrieval module's ranking. Experiments\nindicate that HGOT excels as a versatile approach, outperforming competing\nmodels in FEVER by up to $7\\%$ and matching leading models such as\nRetrieve-then-Read in Open-SQuAD, and DSP in HotPotQA, demonstrating its\nefficacy in enhancing LLMs' factuality.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09390v2",
    "published_date": "2024-02-14 18:41:19 UTC",
    "updated_date": "2024-07-02 12:42:59 UTC"
  },
  {
    "arxiv_id": "2402.09388v1",
    "title": "Entropy-regularized Point-based Value Iteration",
    "authors": [
      "Harrison Delecki",
      "Marcell Vazquez-Chanlatte",
      "Esen Yel",
      "Kyle Wray",
      "Tomer Arnon",
      "Stefan Witwicki",
      "Mykel J. Kochenderfer"
    ],
    "abstract": "Model-based planners for partially observable problems must accommodate both\nmodel uncertainty during planning and goal uncertainty during objective\ninference. However, model-based planners may be brittle under these types of\nuncertainty because they rely on an exact model and tend to commit to a single\noptimal behavior. Inspired by results in the model-free setting, we propose an\nentropy-regularized model-based planner for partially observable problems.\nEntropy regularization promotes policy robustness for planning and objective\ninference by encouraging policies to be no more committed to a single action\nthan necessary. We evaluate the robustness and objective inference performance\nof entropy-regularized policies in three problem domains. Our results show that\nentropy-regularized policies outperform non-entropy-regularized baselines in\nterms of higher expected returns under modeling errors and higher accuracy\nduring objective inference.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09388v1",
    "published_date": "2024-02-14 18:37:47 UTC",
    "updated_date": "2024-02-14 18:37:47 UTC"
  },
  {
    "arxiv_id": "2402.09384v2",
    "title": "Persuasion, Delegation, and Private Information in Algorithm-Assisted Decisions",
    "authors": [
      "Ruqing Xu"
    ],
    "abstract": "A principal designs an algorithm that generates a publicly observable\nprediction of a binary state. She must decide whether to act directly based on\nthe prediction or to delegate the decision to an agent with private information\nbut potential misalignment. We study the optimal design of the prediction\nalgorithm and the delegation rule in such environments. Three key findings\nemerge: (1) Delegation is optimal if and only if the principal would make the\nsame binary decision as the agent had she observed the agent's information. (2)\nProviding the most informative algorithm may be suboptimal even if the\nprincipal can act on the algorithm's prediction. Instead, the optimal algorithm\nmay provide more information about one state and restrict information about the\nother. (3) Well-intentioned policies aiming to provide more information, such\nas keeping a \"human-in-the-loop\" or requiring maximal prediction accuracy,\ncould strictly worsen decision quality compared to systems with no human or no\nalgorithmic assistance. These findings predict the underperformance of\nhuman-machine collaborations if no measures are taken to mitigate common\npreference misalignment between algorithms and human decision-makers.",
    "categories": [
      "econ.TH",
      "cs.AI",
      "cs.CY",
      "cs.GT",
      "cs.HC"
    ],
    "primary_category": "econ.TH",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09384v2",
    "published_date": "2024-02-14 18:32:30 UTC",
    "updated_date": "2024-02-21 18:01:48 UTC"
  },
  {
    "arxiv_id": "2402.09372v1",
    "title": "Deep Rib Fracture Instance Segmentation and Classification from CT on the RibFrac Challenge",
    "authors": [
      "Jiancheng Yang",
      "Rui Shi",
      "Liang Jin",
      "Xiaoyang Huang",
      "Kaiming Kuang",
      "Donglai Wei",
      "Shixuan Gu",
      "Jianying Liu",
      "Pengfei Liu",
      "Zhizhong Chai",
      "Yongjie Xiao",
      "Hao Chen",
      "Liming Xu",
      "Bang Du",
      "Xiangyi Yan",
      "Hao Tang",
      "Adam Alessio",
      "Gregory Holste",
      "Jiapeng Zhang",
      "Xiaoming Wang",
      "Jianye He",
      "Lixuan Che",
      "Hanspeter Pfister",
      "Ming Li",
      "Bingbing Ni"
    ],
    "abstract": "Rib fractures are a common and potentially severe injury that can be\nchallenging and labor-intensive to detect in CT scans. While there have been\nefforts to address this field, the lack of large-scale annotated datasets and\nevaluation benchmarks has hindered the development and validation of deep\nlearning algorithms. To address this issue, the RibFrac Challenge was\nintroduced, providing a benchmark dataset of over 5,000 rib fractures from 660\nCT scans, with voxel-level instance mask annotations and diagnosis labels for\nfour clinical categories (buckle, nondisplaced, displaced, or segmental). The\nchallenge includes two tracks: a detection (instance segmentation) track\nevaluated by an FROC-style metric and a classification track evaluated by an\nF1-style metric. During the MICCAI 2020 challenge period, 243 results were\nevaluated, and seven teams were invited to participate in the challenge\nsummary. The analysis revealed that several top rib fracture detection\nsolutions achieved performance comparable or even better than human experts.\nNevertheless, the current rib fracture classification solutions are hardly\nclinically applicable, which can be an interesting area in the future. As an\nactive benchmark and research resource, the data and online evaluation of the\nRibFrac Challenge are available at the challenge website. As an independent\ncontribution, we have also extended our previous internal baseline by\nincorporating recent advancements in large-scale pretrained networks and\npoint-based rib segmentation techniques. The resulting FracNet+ demonstrates\ncompetitive performance in rib fracture detection, which lays a foundation for\nfurther research and development in AI-assisted rib fracture detection and\ndiagnosis.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "Challenge paper for MICCAI RibFrac Challenge\n  (https://ribfrac.grand-challenge.org/)",
    "pdf_url": "http://arxiv.org/pdf/2402.09372v1",
    "published_date": "2024-02-14 18:18:33 UTC",
    "updated_date": "2024-02-14 18:18:33 UTC"
  },
  {
    "arxiv_id": "2402.09371v1",
    "title": "Transformers Can Achieve Length Generalization But Not Robustly",
    "authors": [
      "Yongchao Zhou",
      "Uri Alon",
      "Xinyun Chen",
      "Xuezhi Wang",
      "Rishabh Agarwal",
      "Denny Zhou"
    ],
    "abstract": "Length generalization, defined as the ability to extrapolate from shorter\ntraining sequences to longer test ones, is a significant challenge for language\nmodels. This issue persists even with large-scale Transformers handling\nrelatively straightforward tasks. In this paper, we test the Transformer's\nability of length generalization using the task of addition of two integers. We\nshow that the success of length generalization is intricately linked to the\ndata format and the type of position encoding. Using the right combination of\ndata format and position encodings, we show for the first time that standard\nTransformers can extrapolate to a sequence length that is 2.5x the input\nlength. Nevertheless, unlike in-distribution generalization, length\ngeneralization remains fragile, significantly influenced by factors like random\nweight initialization and training data order, leading to large variances\nacross different random seeds.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09371v1",
    "published_date": "2024-02-14 18:18:29 UTC",
    "updated_date": "2024-02-14 18:18:29 UTC"
  },
  {
    "arxiv_id": "2402.09370v2",
    "title": "Pseudorandom Error-Correcting Codes",
    "authors": [
      "Miranda Christ",
      "Sam Gunn"
    ],
    "abstract": "We construct pseudorandom error-correcting codes (or simply pseudorandom\ncodes), which are error-correcting codes with the property that any polynomial\nnumber of codewords are pseudorandom to any computationally-bounded adversary.\nEfficient decoding of corrupted codewords is possible with the help of a\ndecoding key.\n  We build pseudorandom codes that are robust to substitution and deletion\nerrors, where pseudorandomness rests on standard cryptographic assumptions.\nSpecifically, pseudorandomness is based on either $2^{O(\\sqrt{n})}$-hardness of\nLPN, or polynomial hardness of LPN and the planted XOR problem at low density.\n  As our primary application of pseudorandom codes, we present an undetectable\nwatermarking scheme for outputs of language models that is robust to cropping\nand a constant rate of random substitutions and deletions. The watermark is\nundetectable in the sense that any number of samples of watermarked text are\ncomputationally indistinguishable from text output by the original model. This\nis the first undetectable watermarking scheme that can tolerate a constant rate\nof errors.\n  Our second application is to steganography, where a secret message is hidden\nin innocent-looking content. We present a constant-rate stateless steganography\nscheme with robustness to a constant rate of substitutions. Ours is the first\nstateless steganography scheme with provable steganographic security and any\nrobustness to errors.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09370v2",
    "published_date": "2024-02-14 18:17:45 UTC",
    "updated_date": "2024-06-18 01:00:58 UTC"
  },
  {
    "arxiv_id": "2402.09368v2",
    "title": "Magic-Me: Identity-Specific Video Customized Diffusion",
    "authors": [
      "Ze Ma",
      "Daquan Zhou",
      "Chun-Hsiao Yeh",
      "Xue-She Wang",
      "Xiuyu Li",
      "Huanrui Yang",
      "Zhen Dong",
      "Kurt Keutzer",
      "Jiashi Feng"
    ],
    "abstract": "Creating content with specified identities (ID) has attracted significant\ninterest in the field of generative models. In the field of text-to-image\ngeneration (T2I), subject-driven creation has achieved great progress with the\nidentity controlled via reference images. However, its extension to video\ngeneration is not well explored. In this work, we propose a simple yet\neffective subject identity controllable video generation framework, termed\nVideo Custom Diffusion (VCD). With a specified identity defined by a few\nimages, VCD reinforces the identity characteristics and injects frame-wise\ncorrelation at the initialization stage for stable video outputs. To achieve\nthis, we propose three novel components that are essential for high-quality\nidentity preservation and stable video generation: 1) a noise initialization\nmethod with 3D Gaussian Noise Prior for better inter-frame stability; 2) an ID\nmodule based on extended Textual Inversion trained with the cropped identity to\ndisentangle the ID information from the background 3) Face VCD and Tiled VCD\nmodules to reinforce faces and upscale the video to higher resolution while\npreserving the identity's features. We conducted extensive experiments to\nverify that VCD is able to generate stable videos with better ID over the\nbaselines. Besides, with the transferability of the encoded identity in the ID\nmodule, VCD is also working well with personalized text-to-image models\navailable publicly. The codes are available at\nhttps://github.com/Zhen-Dong/Magic-Me.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page at https://magic-me-webpage.github.io",
    "pdf_url": "http://arxiv.org/pdf/2402.09368v2",
    "published_date": "2024-02-14 18:13:51 UTC",
    "updated_date": "2024-03-20 17:36:35 UTC"
  },
  {
    "arxiv_id": "2402.09360v1",
    "title": "HiRE: High Recall Approximate Top-$k$ Estimation for Efficient LLM Inference",
    "authors": [
      "Yashas Samaga B L",
      "Varun Yerram",
      "Chong You",
      "Srinadh Bhojanapalli",
      "Sanjiv Kumar",
      "Prateek Jain",
      "Praneeth Netrapalli"
    ],
    "abstract": "Autoregressive decoding with generative Large Language Models (LLMs) on\naccelerators (GPUs/TPUs) is often memory-bound where most of the time is spent\non transferring model parameters from high bandwidth memory (HBM) to cache. On\nthe other hand, recent works show that LLMs can maintain quality with\nsignificant sparsity/redundancy in the feedforward (FFN) layers by\nappropriately training the model to operate on a top-$k$ fraction of\nrows/columns (where $k \\approx 0.05$), there by suggesting a way to reduce the\ntransfer of model parameters, and hence latency. However, exploiting this\nsparsity for improving latency is hindered by the fact that identifying top\nrows/columns is data-dependent and is usually performed using full matrix\noperations, severely limiting potential gains. To address these issues, we\nintroduce HiRE (High Recall Approximate Top-k Estimation). HiRE comprises of\ntwo novel components: (i) a compression scheme to cheaply predict top-$k$\nrows/columns with high recall, followed by full computation restricted to the\npredicted subset, and (ii) DA-TOP-$k$: an efficient multi-device approximate\ntop-$k$ operator. We demonstrate that on a one billion parameter model, HiRE\napplied to both the softmax as well as feedforward layers, achieves almost\nmatching pretraining and downstream accuracy, and speeds up inference latency\nby $1.47\\times$ on a single TPUv5e device.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09360v1",
    "published_date": "2024-02-14 18:04:36 UTC",
    "updated_date": "2024-02-14 18:04:36 UTC"
  },
  {
    "arxiv_id": "2402.09358v1",
    "title": "Integrating ChatGPT into Secure Hospital Networks: A Case Study on Improving Radiology Report Analysis",
    "authors": [
      "Kyungsu Kim",
      "Junhyun Park",
      "Saul Langarica",
      "Adham Mahmoud Alkhadrawi",
      "Synho Do"
    ],
    "abstract": "This study demonstrates the first in-hospital adaptation of a cloud-based AI,\nsimilar to ChatGPT, into a secure model for analyzing radiology reports,\nprioritizing patient data privacy. By employing a unique sentence-level\nknowledge distillation method through contrastive learning, we achieve over 95%\naccuracy in detecting anomalies. The model also accurately flags uncertainties\nin its predictions, enhancing its reliability and interpretability for\nphysicians with certainty indicators. These advancements represent significant\nprogress in developing secure and efficient AI tools for healthcare, suggesting\na promising future for in-hospital AI applications with minimal supervision.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09358v1",
    "published_date": "2024-02-14 18:02:24 UTC",
    "updated_date": "2024-02-14 18:02:24 UTC"
  },
  {
    "arxiv_id": "2402.09355v1",
    "title": "Single-Reset Divide & Conquer Imitation Learning",
    "authors": [
      "Alexandre Chenu",
      "Olivier Serris",
      "Olivier Sigaud",
      "Nicolas Perrin-Gilbert"
    ],
    "abstract": "Demonstrations are commonly used to speed up the learning process of Deep\nReinforcement Learning algorithms. To cope with the difficulty of accessing\nmultiple demonstrations, some algorithms have been developed to learn from a\nsingle demonstration. In particular, the Divide & Conquer Imitation Learning\nalgorithms leverage a sequential bias to learn a control policy for complex\nrobotic tasks using a single state-based demonstration. The latest version,\nDCIL-II demonstrates remarkable sample efficiency. This novel method operates\nwithin an extended Goal-Conditioned Reinforcement Learning framework, ensuring\ncompatibility between intermediate and subsequent goals extracted from the\ndemonstration. However, a fundamental limitation arises from the assumption\nthat the system can be reset to specific states along the demonstrated\ntrajectory, confining the application to simulated systems. In response, we\nintroduce an extension called Single-Reset DCIL (SR-DCIL), designed to overcome\nthis constraint by relying on a single initial state reset rather than\nsequential resets. To address this more challenging setting, we integrate two\nmechanisms inspired by the Learning from Demonstrations literature, including a\nDemo-Buffer and Value Cloning, to guide the agent toward compatible success\nstates. In addition, we introduce Approximate Goal Switching to facilitate\ntraining to reach goals distant from the reset state. Our paper makes several\ncontributions, highlighting the importance of the reset assumption in DCIL-II,\npresenting the mechanisms of SR-DCIL variants and evaluating their performance\nin challenging robotic tasks compared to DCIL-II. In summary, this work offers\ninsights into the significance of reset assumptions in the framework of DCIL\nand proposes SR-DCIL, a first step toward a versatile algorithm capable of\nlearning control policies under a weaker reset assumption.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09355v1",
    "published_date": "2024-02-14 17:59:47 UTC",
    "updated_date": "2024-02-14 17:59:47 UTC"
  },
  {
    "arxiv_id": "2402.09346v3",
    "title": "LLMAuditor: A Framework for Auditing Large Language Models Using Human-in-the-Loop",
    "authors": [
      "Maryam Amirizaniani",
      "Jihan Yao",
      "Adrian Lavergne",
      "Elizabeth Snell Okada",
      "Aman Chadha",
      "Tanya Roosta",
      "Chirag Shah"
    ],
    "abstract": "As Large Language Models (LLMs) become more pervasive across various users\nand scenarios, identifying potential issues when using these models becomes\nessential. Examples of such issues include: bias, inconsistencies, and\nhallucination. Although auditing the LLM for these problems is often warranted,\nsuch a process is neither easy nor accessible for most. An effective method is\nto probe the LLM using different versions of the same question. This could\nexpose inconsistencies in its knowledge or operation, indicating potential for\nbias or hallucination. However, to operationalize this auditing method at\nscale, we need an approach to create those probes reliably and automatically.\nIn this paper we propose the LLMAuditor framework which is an automatic, and\nscalable solution, where one uses a different LLM along with human-in-the-loop\n(HIL). This approach offers verifiability and transparency, while avoiding\ncircular reliance on the same LLM, and increasing scientific rigor and\ngeneralizability. Specifically, LLMAuditor includes two phases of verification\nusing humans: standardized evaluation criteria to verify responses, and a\nstructured prompt template to generate desired probes. A case study using\nquestions from the TruthfulQA dataset demonstrates that we can generate a\nreliable set of probes from one LLM that can be used to audit inconsistencies\nin a different LLM. This process is enhanced by our structured prompt template\nwith HIL, which not only boosts the reliability of our approach in auditing but\nalso yields the delivery of less hallucinated results. The novelty of our\nresearch stems from the development of a comprehensive, general-purpose\nframework that includes a HIL verified prompt template for auditing responses\ngenerated by LLMs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09346v3",
    "published_date": "2024-02-14 17:49:31 UTC",
    "updated_date": "2024-05-22 17:17:03 UTC"
  },
  {
    "arxiv_id": "2402.09345v5",
    "title": "InfoRM: Mitigating Reward Hacking in RLHF via Information-Theoretic Reward Modeling",
    "authors": [
      "Yuchun Miao",
      "Sen Zhang",
      "Liang Ding",
      "Rong Bao",
      "Lefei Zhang",
      "Dacheng Tao"
    ],
    "abstract": "Despite the success of reinforcement learning from human feedback (RLHF) in\naligning language models with human values, reward hacking, also termed reward\noveroptimization, remains a critical challenge. This issue primarily arises\nfrom reward misgeneralization, where reward models (RMs) compute reward using\nspurious features that are irrelevant to human preferences. In this work, we\ntackle this problem from an information-theoretic perspective and propose a\nframework for reward modeling, namely InfoRM, by introducing a variational\ninformation bottleneck objective to filter out irrelevant information. Notably,\nwe further identify a correlation between overoptimization and outliers in the\nIB latent space of InfoRM, establishing it as a promising tool for detecting\nreward overoptimization. Inspired by this finding, we propose the Cluster\nSeparation Index (CSI), which quantifies deviations in the IB latent space, as\nan indicator of reward overoptimization to facilitate the development of online\nmitigation strategies. Extensive experiments on a wide range of settings and RM\nscales (70M, 440M, 1.4B, and 7B) demonstrate the effectiveness of InfoRM.\nFurther analyses reveal that InfoRM's overoptimization detection mechanism is\nnot only effective but also robust across a broad range of datasets, signifying\na notable advancement in the field of RLHF. The code will be released upon\nacceptance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "The paper has been accepted by NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.09345v5",
    "published_date": "2024-02-14 17:49:07 UTC",
    "updated_date": "2024-11-01 06:30:11 UTC"
  },
  {
    "arxiv_id": "2402.09338v2",
    "title": "Neural Networks Asymptotic Behaviours for the Resolution of Inverse Problems",
    "authors": [
      "Luigi Del Debbio",
      "Manuel Naviglio",
      "Francesco Tarantelli"
    ],
    "abstract": "This paper presents a study of the effectiveness of Neural Network (NN)\ntechniques for deconvolution inverse problems relevant for applications in\nQuantum Field Theory, but also in more general contexts. We consider NN's\nasymptotic limits, corresponding to Gaussian Processes (GPs), where\nnon-linearities in the parameters of the NN can be neglected. Using these\nresulting GPs, we address the deconvolution inverse problem in the case of a\nquantum harmonic oscillator simulated through Monte Carlo techniques on a\nlattice. In this simple toy model, the results of the inversion can be compared\nwith the known analytical solution. Our findings indicate that solving the\ninverse problem with a NN yields less performing results than those obtained\nusing the GPs derived from NN's asymptotic limits. Furthermore, we observe the\ntrained NN's accuracy approaching that of GPs with increasing layer width.\nNotably, one of these GPs defies interpretation as a probabilistic model,\noffering a novel perspective compared to established methods in the literature.\nOur results suggest the need for detailed studies of the training dynamics in\nmore realistic set-ups.",
    "categories": [
      "physics.comp-ph",
      "cs.AI",
      "hep-lat",
      "hep-th"
    ],
    "primary_category": "physics.comp-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09338v2",
    "published_date": "2024-02-14 17:42:24 UTC",
    "updated_date": "2024-02-15 12:07:13 UTC"
  },
  {
    "arxiv_id": "2402.09334v2",
    "title": "AuditLLM: A Tool for Auditing Large Language Models Using Multiprobe Approach",
    "authors": [
      "Maryam Amirizaniani",
      "Elias Martin",
      "Tanya Roosta",
      "Aman Chadha",
      "Chirag Shah"
    ],
    "abstract": "As Large Language Models (LLMs) are integrated into various sectors, ensuring\ntheir reliability and safety is crucial. This necessitates rigorous probing and\nauditing to maintain their effectiveness and trustworthiness in practical\napplications. Subjecting LLMs to varied iterations of a single query can unveil\npotential inconsistencies in their knowledge base or functional capacity.\nHowever, a tool for performing such audits with a easy to execute workflow, and\nlow technical threshold is lacking. In this demo, we introduce ``AuditLLM,'' a\nnovel tool designed to audit the performance of various LLMs in a methodical\nway. AuditLLM's primary function is to audit a given LLM by deploying multiple\nprobes derived from a single question, thus detecting any inconsistencies in\nthe model's comprehension or performance. A robust, reliable, and consistent\nLLM is expected to generate semantically similar responses to variably phrased\nversions of the same question. Building on this premise, AuditLLM generates\neasily interpretable results that reflect the LLM's consistency based on a\nsingle input question provided by the user. A certain level of inconsistency\nhas been shown to be an indicator of potential bias, hallucinations, and other\nissues. One could then use the output of AuditLLM to further investigate issues\nwith the aforementioned LLM. To facilitate demonstration and practical uses,\nAuditLLM offers two key modes: (1) Live mode which allows instant auditing of\nLLMs by analyzing responses to real-time queries; and (2) Batch mode which\nfacilitates comprehensive LLM auditing by processing multiple queries at once\nfor in-depth analysis. This tool is beneficial for both researchers and general\nusers, as it enhances our understanding of LLMs' capabilities in generating\nresponses, using a standardized auditing platform.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09334v2",
    "published_date": "2024-02-14 17:31:04 UTC",
    "updated_date": "2024-06-17 18:24:41 UTC"
  },
  {
    "arxiv_id": "2402.09320v1",
    "title": "ICDPO: Effectively Borrowing Alignment Capability of Others via In-context Direct Preference Optimization",
    "authors": [
      "Feifan Song",
      "Yuxuan Fan",
      "Xin Zhang",
      "Peiyi Wang",
      "Houfeng Wang"
    ],
    "abstract": "Large Language Models (LLMs) rely on Human Preference Alignment (HPA) to\nensure the generation of safe content. Due to the heavy cost associated with\nfine-tuning, fine-tuning-free methods have emerged, typically modifying LLM\ndecoding with external auxiliary methods. However, these methods do not\nessentially enhance the LLM itself. In this paper, we rethink the derivation\nprocedures of DPO, based on which we conversely build an instant scorer using\nthe states of the LLM before and after In-context Learning (ICL). Accordingly,\nwe propose a novel approach called In-Context Direct Preference Optimization\n(ICDPO). It enables LLMs to borrow the HPA capabilities from superior LLMs with\nICL, generating well-aligned responses as estimated by the aforementioned\ninstant scorer, thereby enhancing the final performance. ICDPO can be further\nenhanced with a two-stage retriever and an upgraded scorer, both offering\nbenefits. Extensive experiments show its effectiveness, particularly in\noutperforming two fine-tuning-free baselines, and it exhibits competitiveness\nwith SFT + LoRA. We also conduct detailed analyses to offer comprehensive\ninsights into ICDPO.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09320v1",
    "published_date": "2024-02-14 17:14:34 UTC",
    "updated_date": "2024-02-14 17:14:34 UTC"
  },
  {
    "arxiv_id": "2402.09318v1",
    "title": "Leveraging Pre-Trained Autoencoders for Interpretable Prototype Learning of Music Audio",
    "authors": [
      "Pablo Alonso-Jim√©nez",
      "Leonardo Pepino",
      "Roser Batlle-Roca",
      "Pablo Zinemanas",
      "Dmitry Bogdanov",
      "Xavier Serra",
      "Mart√≠n Rocamora"
    ],
    "abstract": "We present PECMAE, an interpretable model for music audio classification\nbased on prototype learning. Our model is based on a previous method, APNet,\nwhich jointly learns an autoencoder and a prototypical network. Instead, we\npropose to decouple both training processes. This enables us to leverage\nexisting self-supervised autoencoders pre-trained on much larger data\n(EnCodecMAE), providing representations with better generalization. APNet\nallows prototypes' reconstruction to waveforms for interpretability relying on\nthe nearest training data samples. In contrast, we explore using a diffusion\ndecoder that allows reconstruction without such dependency. We evaluate our\nmethod on datasets for music instrument classification (Medley-Solos-DB) and\ngenre recognition (GTZAN and a larger in-house dataset), the latter being a\nmore challenging task not addressed with prototypical networks before. We find\nthat the prototype-based models preserve most of the performance achieved with\nthe autoencoder embeddings, while the sonification of prototypes benefits\nunderstanding the behavior of the classifier.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.MM",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09318v1",
    "published_date": "2024-02-14 17:13:36 UTC",
    "updated_date": "2024-02-14 17:13:36 UTC"
  },
  {
    "arxiv_id": "2402.09305v1",
    "title": "Embracing the black box: Heading towards foundation models for causal discovery from time series data",
    "authors": [
      "Gideon Stein",
      "Maha Shadaydeh",
      "Joachim Denzler"
    ],
    "abstract": "Causal discovery from time series data encompasses many existing solutions,\nincluding those based on deep learning techniques. However, these methods\ntypically do not endorse one of the most prevalent paradigms in deep learning:\nEnd-to-end learning. To address this gap, we explore what we call Causal\nPretraining. A methodology that aims to learn a direct mapping from\nmultivariate time series to the underlying causal graphs in a supervised\nmanner. Our empirical findings suggest that causal discovery in a supervised\nmanner is possible, assuming that the training and test time series samples\nshare most of their dynamics. More importantly, we found evidence that the\nperformance of Causal Pretraining can increase with data and model size, even\nif the additional data do not share the same dynamics. Further, we provide\nexamples where causal discovery for real-world data with causally pretrained\nneural networks is possible within limits. We argue that this hints at the\npossibility of a foundation model for causal discovery.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "68T07"
    ],
    "primary_category": "cs.LG",
    "comment": "AAAI Workshop (AI4TS) 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.09305v1",
    "published_date": "2024-02-14 16:49:13 UTC",
    "updated_date": "2024-02-14 16:49:13 UTC"
  },
  {
    "arxiv_id": "2402.09303v3",
    "title": "Comparing supervised learning dynamics: Deep neural networks match human data efficiency but show a generalisation lag",
    "authors": [
      "Lukas S. Huber",
      "Fred W. Mast",
      "Felix A. Wichmann"
    ],
    "abstract": "Recent research has seen many behavioral comparisons between humans and deep\nneural networks (DNNs) in the domain of image classification. Often, comparison\nstudies focus on the end-result of the learning process by measuring and\ncomparing the similarities in the representations of object categories once\nthey have been formed. However, the process of how these representations emerge\n-- that is, the behavioral changes and intermediate stages observed during the\nacquisition -- is less often directly and empirically compared. Here we report\na detailed investigation of the learning dynamics in human observers and\nvarious classic and state-of-the-art DNNs. We develop a constrained supervised\nlearning environment to align learning-relevant conditions such as starting\npoint, input modality, available input data and the feedback provided. Across\nthe whole learning process we evaluate and compare how well learned\nrepresentations can be generalized to previously unseen test data. Comparisons\nacross the entire learning process indicate that DNNs demonstrate a level of\ndata efficiency comparable to human learners, challenging some prevailing\nassumptions in the field. However, our results also reveal representational\ndifferences: while DNNs' learning is characterized by a pronounced\ngeneralisation lag, humans appear to immediately acquire generalizable\nrepresentations without a preliminary phase of learning training set-specific\ninformation that is only later transferred to novel data.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "q-bio.NC"
    ],
    "primary_category": "cs.CV",
    "comment": "Final version accepted @ ICLR 2024 Workshop on Representational\n  Alignment (Re-Align)",
    "pdf_url": "http://arxiv.org/pdf/2402.09303v3",
    "published_date": "2024-02-14 16:47:20 UTC",
    "updated_date": "2024-07-12 12:47:19 UTC"
  },
  {
    "arxiv_id": "2402.09498v1",
    "title": "Detection of the most influential variables for preventing postpartum urinary incontinence using machine learning techniques",
    "authors": [
      "Jos√© Alberto Ben√≠tez-Andrades",
      "Mar√≠a Teresa Garc√≠a-Ord√°s",
      "Mar√≠a √Ålvarez-Gonz√°lez",
      "Raquel Leir√≥s-Rodr√≠guez",
      "Ana F L√≥pez Rodr√≠guez"
    ],
    "abstract": "Background: Postpartum urinary incontinence (PUI) is a common issue among\npostnatal women. Previous studies identified potential related variables, but\nlacked analysis on certain intrinsic and extrinsic patient variables during\npregnancy.\n  Objective: The study aims to evaluate the most influential variables in PUI\nusing machine learning, focusing on intrinsic, extrinsic, and combined variable\ngroups.\n  Methods: Data from 93 pregnant women were analyzed using machine learning and\noversampling techniques. Four key variables were predicted: occurrence,\nfrequency, intensity of urinary incontinence, and stress urinary incontinence.\n  Results: Models using extrinsic variables were most accurate, with 70%\naccuracy for urinary incontinence, 77% for frequency, 71% for intensity, and\n93% for stress urinary incontinence.\n  Conclusions: The study highlights extrinsic variables as significant\npredictors of PUI issues. This suggests that PUI prevention might be achievable\nthrough healthy habits during pregnancy, although further research is needed\nfor confirmation.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09498v1",
    "published_date": "2024-02-14 16:45:10 UTC",
    "updated_date": "2024-02-14 16:45:10 UTC"
  },
  {
    "arxiv_id": "2403.12073v1",
    "title": "Feasibility of Social-Network-Based eHealth Intervention on the Improvement of Healthy Habits among Children",
    "authors": [
      "Jos√© Alberto Ben√≠tez-Andrades",
      "Natalia Arias",
      "Mar√≠a Teresa Garc√≠a-Ord√°s",
      "Marta Mart√≠nez-Mart√≠nez",
      "Isa√≠as Garc√≠a-Rodr√≠guez"
    ],
    "abstract": "This study shows the feasibility of an eHealth solution for tackling eating\nhabits and physical activity in the adolescent population. The participants\nwere children from 11 to 15 years old. An intervention was carried out on 139\nstudents in the intervention group and 91 students in the control group, in two\nschools during 14 weeks. The intervention group had access to the web through a\nuser account and a password. They were able to create friendship relationships,\npost comments, give likes and interact with other users, as well as receive\nnotifications and information about nutrition and physical activity on a daily\nbasis and get (virtual) rewards for improving their habits. The control group\ndid not have access to any of these features. The homogeneity of the samples in\nterms of gender, age, body mass index and initial health-related habits was\ndemonstrated. Pre- and post-measurements were collected through self-reports on\nthe application website. After applying multivariate analysis of variance, a\nsignificant alteration in the age-adjusted body mass index percentile was\nobserved in the intervention group versus the control group, as well as in the\nPAQ-A score and the KIDMED score. It can be concluded that eHealth\ninterventions can help to obtain healthy habits. More research is needed to\nexamine the effectiveness in achieving adherence to these new habits.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.12073v1",
    "published_date": "2024-02-14 16:23:48 UTC",
    "updated_date": "2024-02-14 16:23:48 UTC"
  },
  {
    "arxiv_id": "2402.09290v1",
    "title": "Learning Interpretable Policies in Hindsight-Observable POMDPs through Partially Supervised Reinforcement Learning",
    "authors": [
      "Michael Lanier",
      "Ying Xu",
      "Nathan Jacobs",
      "Chongjie Zhang",
      "Yevgeniy Vorobeychik"
    ],
    "abstract": "Deep reinforcement learning has demonstrated remarkable achievements across\ndiverse domains such as video games, robotic control, autonomous driving, and\ndrug discovery. Common methodologies in partially-observable domains largely\nlean on end-to-end learning from high-dimensional observations, such as images,\nwithout explicitly reasoning about true state. We suggest an alternative\ndirection, introducing the Partially Supervised Reinforcement Learning (PSRL)\nframework. At the heart of PSRL is the fusion of both supervised and\nunsupervised learning. The approach leverages a state estimator to distill\nsupervised semantic state information from high-dimensional observations which\nare often fully observable at training time. This yields more interpretable\npolicies that compose state predictions with control. In parallel, it captures\nan unsupervised latent representation. These two-the semantic state and the\nlatent state-are then fused and utilized as inputs to a policy network. This\njuxtaposition offers practitioners a flexible and dynamic spectrum: from\nemphasizing supervised state information to integrating richer, latent\ninsights. Extensive experimental results indicate that by merging these dual\nrepresentations, PSRL offers a potent balance, enhancing model interpretability\nwhile preserving, and often significantly outperforming, the performance\nbenchmarks set by traditional methods in terms of reward and convergence speed.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09290v1",
    "published_date": "2024-02-14 16:23:23 UTC",
    "updated_date": "2024-02-14 16:23:23 UTC"
  },
  {
    "arxiv_id": "2402.09286v1",
    "title": "Nutrition Facts, Drug Facts, and Model Facts: Putting AI Ethics into Practice in Gun Violence Research",
    "authors": [
      "Jessica Zhu",
      "Michel Cukier",
      "Joseph Richardson Jr"
    ],
    "abstract": "Objective: Firearm injury research necessitates using data from\noften-exploited vulnerable populations of Black and Brown Americans. In order\nto minimize distrust, this study provides a framework for establishing AI trust\nand transparency with the general population. Methods: We propose a Model Facts\ntemplate that is easily extendable and decomposes accuracy and demographics\ninto standardized and minimally complex values. This framework allows general\nusers to assess the validity and biases of a model without diving into\ntechnical model documentation. Examples: We apply the Model Facts template on\ntwo previously published models, a violence risk identification model and a\nsuicide risk prediction model. We demonstrate the ease of accessing the\nappropriate information when the data is structured appropriately. Discussion:\nThe Model Facts template is limited in its current form to human based data and\nbiases. Like nutrition facts, it also will require some educational resources\nfor users to grasp its full utility. Human computer interaction experiments\nshould be conducted to ensure that the interaction between user interface and\nmodel interface is as desired. Conclusion: The Model Facts label is the first\nframework dedicated to establishing trust with end users and general population\nconsumers. Implementation of Model Facts into firearm injury research will\nprovide public health practitioners and those impacted by firearm injury\ngreater faith in the tools the research provides.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09286v1",
    "published_date": "2024-02-14 16:19:09 UTC",
    "updated_date": "2024-02-14 16:19:09 UTC"
  },
  {
    "arxiv_id": "2402.12390v1",
    "title": "A Semantic Social Network Analysis Tool for Sensitivity Analysis and What-If Scenario Testing in Alcohol Consumption Studies",
    "authors": [
      "Jos√© Alberto Ben√≠tez-Andrades",
      "Alejandro Rodr√≠guez-Gonz√°lez",
      "Carmen Benavides",
      "Leticia S√°nchez-Valde√≥n",
      "Isa√≠as Garc√≠a"
    ],
    "abstract": "Social Network Analysis (SNA) is a set of techniques developed in the field\nof social and behavioral sciences research, in order to characterize and study\nthe social relationships that are established among a set of individuals. When\nbuilding a social network for performing an SNA analysis, an initial process of\ndata gathering is achieved in order to extract the characteristics of the\nindividuals and their relationships. This is usually done by completing a\nquestionnaire containing different types of questions that will be later used\nto obtain the SNA measures needed to perform the study. There are, then, a\ngreat number of different possible network generating questions and also many\npossibilities for mapping the responses to the corresponding characteristics\nand relationships. Many variations may be introduced into these questions (the\nway they are posed, the weights given to each of the responses, etc.) that may\nhave an effect on the resulting networks. All these different variations are\ndifficult to achieve manually, because the process is time-consuming and error\nprone. The tool described in this paper uses semantic knowledge representation\ntechniques in order to facilitate this kind of sensitivity studies. The base of\nthe tool is a conceptual structure, called \"ontology\" that is able to represent\nthe different concepts and their definitions. The tool is compared to other\nsimilar ones, and the advantages of the approach are highlighted, giving some\nparticular examples from an ongoing SNA study about alcohol consumption habits\nin adolescents.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.12390v1",
    "published_date": "2024-02-14 16:17:04 UTC",
    "updated_date": "2024-02-14 16:17:04 UTC"
  },
  {
    "arxiv_id": "2402.09283v3",
    "title": "Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey",
    "authors": [
      "Zhichen Dong",
      "Zhanhui Zhou",
      "Chao Yang",
      "Jing Shao",
      "Yu Qiao"
    ],
    "abstract": "Large Language Models (LLMs) are now commonplace in conversation\napplications. However, their risks of misuse for generating harmful responses\nhave raised serious societal concerns and spurred recent research on LLM\nconversation safety. Therefore, in this survey, we provide a comprehensive\noverview of recent studies, covering three critical aspects of LLM conversation\nsafety: attacks, defenses, and evaluations. Our goal is to provide a structured\nsummary that enhances understanding of LLM conversation safety and encourages\nfurther investigation into this important subject. For easy reference, we have\ncategorized all the studies mentioned in this survey according to our taxonomy,\navailable at: https://github.com/niconi19/LLM-conversation-safety.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to NAACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.09283v3",
    "published_date": "2024-02-14 16:14:03 UTC",
    "updated_date": "2024-03-27 13:55:14 UTC"
  },
  {
    "arxiv_id": "2402.09281v3",
    "title": "Synergistic eigenanalysis of covariance and Hessian matrices for enhanced binary classification",
    "authors": [
      "Agus Hartoyo",
      "Jan Argasi≈Ñski",
      "Aleksandra Trenk",
      "Kinga Przybylska",
      "Anna B≈Çasiak",
      "Alessandro Crimi"
    ],
    "abstract": "Covariance and Hessian matrices have been analyzed separately in the\nliterature for classification problems. However, integrating these matrices has\nthe potential to enhance their combined power in improving classification\nperformance. We present a novel approach that combines the eigenanalysis of a\ncovariance matrix evaluated on a training set with a Hessian matrix evaluated\non a deep learning model to achieve optimal class separability in binary\nclassification tasks. Our approach is substantiated by formal proofs that\nestablish its capability to maximize between-class mean distance (the concept\nof \\textit{separation}) and minimize within-class variances (the concept of\n\\textit{compactness}), which together define the two linear discriminant\nanalysis (LDA) criteria, particularly under ideal data conditions such as\nisotropy around class means and dominant leading eigenvalues. By projecting\ndata into the combined space of the most relevant eigendirections from both\nmatrices, we achieve optimal class separability as per these LDA criteria.\nEmpirical validation across neural and health datasets consistently supports\nour theoretical framework and demonstrates that our method outperforms\nestablished methods. Our method stands out by addressing both separation and\ncompactness criteria, unlike PCA and the Hessian method, which predominantly\nemphasize one criterion each. This comprehensive approach captures intricate\npatterns and relationships, enhancing classification performance. Furthermore,\nthrough the utilization of both LDA criteria, our method outperforms LDA itself\nby leveraging higher-dimensional feature spaces, in accordance with Cover's\ntheorem, which favors linear separability in higher dimensions. Additionally,\nour approach sheds light on complex DNN decision-making, rendering them\ncomprehensible within a 2D space.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "31 pages, 13 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.09281v3",
    "published_date": "2024-02-14 16:10:42 UTC",
    "updated_date": "2024-10-08 16:19:47 UTC"
  },
  {
    "arxiv_id": "2402.10967v1",
    "title": "Social network analysis for personalized characterization and risk assessment of alcohol use disorders in adolescents using semantic technologies",
    "authors": [
      "Jos√© Alberto Ben√≠tez-Andrades",
      "Isa√≠as Garc√≠a-Rodr√≠guez",
      "Carmen Benavides",
      "H√©ctor Alaiz-Moret√≥n",
      "Alejandro Rodr√≠guez-Gonz√°lez"
    ],
    "abstract": "Alcohol Use Disorder (AUD) is a major concern for public health organizations\nworldwide, especially as regards the adolescent population. The consumption of\nalcohol in adolescents is known to be influenced by seeing friends and even\nparents drinking alcohol. Building on this fact, a number of studies into\nalcohol consumption among adolescents have made use of Social Network Analysis\n(SNA) techniques to study the different social networks (peers, friends,\nfamily, etc.) with whom the adolescent is involved. These kinds of studies need\nan initial phase of data gathering by means of questionnaires and a subsequent\nanalysis phase using the SNA techniques. The process involves a number of\nmanual data handling stages that are time consuming and error-prone. The use of\nknowledge engineering techniques (including the construction of a domain\nontology) to represent the information, allows the automation of all the\nactivities, from the initial data collection to the results of the SNA study.\nThis paper shows how a knowledge model is constructed, and compares the results\nobtained using the traditional method with this, fully automated model,\ndetailing the main advantages of the latter. In the case of the SNA analysis,\nthe validity of the results obtained with the knowledge engineering approach\nare compared to those obtained manually using the UCINET, Cytoscape, Pajek and\nGephi to test the accuracy of the knowledge model.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.10967v1",
    "published_date": "2024-02-14 16:09:05 UTC",
    "updated_date": "2024-02-14 16:09:05 UTC"
  },
  {
    "arxiv_id": "2402.09269v2",
    "title": "Personalized Large Language Models",
    "authors": [
      "Stanis≈Çaw Wo≈∫niak",
      "Bart≈Çomiej Koptyra",
      "Arkadiusz Janz",
      "Przemys≈Çaw Kazienko",
      "Jan Koco≈Ñ"
    ],
    "abstract": "Large language models (LLMs) have significantly advanced Natural Language\nProcessing (NLP) tasks in recent years. However, their universal nature poses\nlimitations in scenarios requiring personalized responses, such as\nrecommendation systems and chatbots. This paper investigates methods to\npersonalize LLMs, comparing fine-tuning and zero-shot reasoning approaches on\nsubjective tasks. Results demonstrate that personalized fine-tuning improves\nmodel reasoning compared to non-personalized models. Experiments on datasets\nfor emotion recognition and hate speech detection show consistent performance\ngains with personalized methods across different LLM architectures. These\nfindings underscore the importance of personalization for enhancing LLM\ncapabilities in subjective text perception tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to SENTIRE 2024 (ICDM Workshops):\n  https://sentic.net/sentire2024wozniak.pdf",
    "pdf_url": "http://arxiv.org/pdf/2402.09269v2",
    "published_date": "2024-02-14 15:55:30 UTC",
    "updated_date": "2024-11-07 16:43:01 UTC"
  },
  {
    "arxiv_id": "2402.09267v2",
    "title": "Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via Self-Evaluation",
    "authors": [
      "Xiaoying Zhang",
      "Baolin Peng",
      "Ye Tian",
      "Jingyan Zhou",
      "Lifeng Jin",
      "Linfeng Song",
      "Haitao Mi",
      "Helen Meng"
    ],
    "abstract": "Despite showing increasingly human-like abilities, large language models\n(LLMs) often struggle with factual inaccuracies, i.e. \"hallucinations\", even\nwhen they hold relevant knowledge. To address these hallucinations, current\napproaches typically necessitate high-quality human factuality annotations. In\nthis work, we explore Self-Alignment for Factuality, where we leverage the\nself-evaluation capability of an LLM to provide training signals that steer the\nmodel towards factuality. Specifically, we incorporate Self-Eval, a\nself-evaluation component, to prompt an LLM to validate the factuality of its\nown generated responses solely based on its internal knowledge. Additionally,\nwe design Self-Knowledge Tuning (SK-Tuning) to augment the LLM's\nself-evaluation ability by improving the model's confidence estimation and\ncalibration. We then utilize these self-annotated responses to fine-tune the\nmodel via Direct Preference Optimization algorithm. We show that the proposed\nself-alignment approach substantially enhances factual accuracy over Llama\nfamily models across three key knowledge-intensive tasks on TruthfulQA and\nBioGEN.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "20 pages",
    "pdf_url": "http://arxiv.org/pdf/2402.09267v2",
    "published_date": "2024-02-14 15:52:42 UTC",
    "updated_date": "2024-06-11 12:22:14 UTC"
  },
  {
    "arxiv_id": "2402.09266v1",
    "title": "Machine Learning in management of precautionary closures caused by lipophilic biotoxins",
    "authors": [
      "Andres Molares-Ulloa",
      "Enrique Fernandez-Blanco",
      "Alejandro Pazos",
      "Daniel Rivero"
    ],
    "abstract": "Mussel farming is one of the most important aquaculture industries. The main\nrisk to mussel farming is harmful algal blooms (HABs), which pose a risk to\nhuman consumption. In Galicia, the Spanish main producer of cultivated mussels,\nthe opening and closing of the production areas is controlled by a monitoring\nprogram. In addition to the closures resulting from the presence of toxicity\nexceeding the legal threshold, in the absence of a confirmatory sampling and\nthe existence of risk factors, precautionary closures may be applied. These\ndecisions are made by experts without the support or formalisation of the\nexperience on which they are based. Therefore, this work proposes a predictive\nmodel capable of supporting the application of precautionary closures.\nAchieving sensitivity, accuracy and kappa index values of 97.34%, 91.83% and\n0.75 respectively, the kNN algorithm has provided the best results. This allows\nthe creation of a system capable of helping in complex situations where\nforecast errors are more common.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09266v1",
    "published_date": "2024-02-14 15:51:58 UTC",
    "updated_date": "2024-02-14 15:51:58 UTC"
  },
  {
    "arxiv_id": "2402.09265v2",
    "title": "Computational Complexity of Preferred Subset Repairs on Data-Graphs",
    "authors": [
      "Nina Pardal",
      "Santiago Cifuentes",
      "Edwin Pin",
      "Maria Vanina Martinez",
      "Sergio Abriola"
    ],
    "abstract": "Preferences are a pivotal component in practical reasoning, especially in\ntasks that involve decision-making over different options or courses of action\nthat could be pursued. In this work, we focus on repairing and querying\ninconsistent knowledge bases in the form of graph databases, which involves\nfinding a way to solve conflicts in the knowledge base and considering answers\nthat are entailed from every possible repair, respectively. Without a priori\ndomain knowledge, all possible repairs are equally preferred. Though that may\nbe adequate for some settings, it seems reasonable to establish and exploit\nsome form of preference order among the potential repairs. We study the problem\nof computing prioritized repairs over graph databases with data values, using a\nnotion of consistency based on GXPath expressions as integrity constraints. We\npresent several preference criteria based on the standard subset repair\nsemantics, incorporating weights, multisets, and set-based priority levels. We\nshow that it is possible to maintain the same computational complexity as in\nthe case where no preference criterion is available for exploitation. Finally,\nwe explore the complexity of consistent query answering in this setting and\nobtain tight lower and upper bounds for all the preference criteria introduced.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.LO",
      "68P15, 68T27, 03B70, 68T37"
    ],
    "primary_category": "cs.DB",
    "comment": "Appendix",
    "pdf_url": "http://arxiv.org/pdf/2402.09265v2",
    "published_date": "2024-02-14 15:51:55 UTC",
    "updated_date": "2024-05-27 15:24:32 UTC"
  },
  {
    "arxiv_id": "2402.09497v2",
    "title": "Instruction Tuning for Secure Code Generation",
    "authors": [
      "Jingxuan He",
      "Mark Vero",
      "Gabriela Krasnopolska",
      "Martin Vechev"
    ],
    "abstract": "Modern language models (LMs) have gained widespread acceptance in everyday\nand professional contexts, particularly in programming. An essential procedure\nenabling this adoption is instruction tuning, which substantially enhances LMs'\npractical utility by training them to follow user instructions and human\npreferences. However, existing instruction tuning schemes overlook a crucial\naspect: the security of generated code. As a result, even the state-of-the-art\ninstruction-tuned LMs frequently produce unsafe code, posing significant\nsecurity risks. In this work, we introduce SafeCoder to address this gap.\nSafeCoder performs security-centric fine-tuning using a diverse and\nhigh-quality dataset that we collected using an automated pipeline. We\nintegrate the security fine-tuning with standard instruction tuning, to\nfacilitate a joint optimization of both security and utility. Despite its\nsimplicity, we show that SafeCoder is effective across a variety of popular LMs\nand datasets. It is able to drastically improve security (by about 30%), while\npreserving utility.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "cs.SE"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09497v2",
    "published_date": "2024-02-14 15:47:46 UTC",
    "updated_date": "2024-07-12 15:45:57 UTC"
  },
  {
    "arxiv_id": "2402.09259v2",
    "title": "SyntaxShap: Syntax-aware Explainability Method for Text Generation",
    "authors": [
      "Kenza Amara",
      "Rita Sevastjanova",
      "Mennatallah El-Assady"
    ],
    "abstract": "To harness the power of large language models in safety-critical domains, we\nneed to ensure the explainability of their predictions. However, despite the\nsignificant attention to model interpretability, there remains an unexplored\ndomain in explaining sequence-to-sequence tasks using methods tailored for\ntextual data. This paper introduces SyntaxShap, a local, model-agnostic\nexplainability method for text generation that takes into consideration the\nsyntax in the text data. The presented work extends Shapley values to account\nfor parsing-based syntactic dependencies. Taking a game theoric approach,\nSyntaxShap only considers coalitions constraint by the dependency tree. We\nadopt a model-based evaluation to compare SyntaxShap and its weighted form to\nstate-of-the-art explainability methods adapted to text generation tasks, using\ndiverse metrics including faithfulness, coherency, and semantic alignment of\nthe explanations to the model. We show that our syntax-aware method produces\nexplanations that help build more faithful and coherent explanations for\npredictions by autoregressive models. Confronted with the misalignment of human\nand AI model reasoning, this paper also highlights the need for cautious\nevaluation strategies in explainable AI.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.09259v2",
    "published_date": "2024-02-14 15:45:56 UTC",
    "updated_date": "2024-06-03 10:30:00 UTC"
  },
  {
    "arxiv_id": "2402.09251v2",
    "title": "Universal Machine Learning Kohn-Sham Hamiltonian for Materials",
    "authors": [
      "Yang Zhong",
      "Hongyu Yu",
      "Jihui Yang",
      "Xingyu Guo",
      "Hongjun Xiang",
      "Xingao Gong"
    ],
    "abstract": "While density functional theory (DFT) serves as a prevalent computational\napproach in electronic structure calculations, its computational demands and\nscalability limitations persist. Recently, leveraging neural networks to\nparameterize the Kohn-Sham DFT Hamiltonian has emerged as a promising avenue\nfor accelerating electronic structure computations. Despite advancements,\nchallenges such as the necessity for computing extensive DFT training data to\nexplore each new system and the complexity of establishing accurate ML models\nfor multi-elemental materials still exist. Addressing these hurdles, this study\nintroduces a universal electronic Hamiltonian model trained on Hamiltonian\nmatrices obtained from first-principles DFT calculations of nearly all crystal\nstructures on the Materials Project. We demonstrate its generality in\npredicting electronic structures across the whole periodic table, including\ncomplex multi-elemental systems, solid-state electrolytes, Moir\\'e twisted\nbilayer heterostructure, and metal-organic frameworks (MOFs). Moreover, we\nutilize the universal model to conduct high-throughput calculations of\nelectronic structures for crystals in GeNOME datasets, identifying 3,940\ncrystals with direct band gaps and 5,109 crystals with flat bands. By offering\na reliable efficient framework for computing electronic properties, this\nuniversal Hamiltonian model lays the groundwork for advancements in diverse\nfields, such as easily providing a huge data set of electronic structures and\nalso making the materials design across the whole periodic table possible.",
    "categories": [
      "physics.comp-ph",
      "cond-mat.mtrl-sci",
      "cs.AI"
    ],
    "primary_category": "physics.comp-ph",
    "comment": "20 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.09251v2",
    "published_date": "2024-02-14 15:38:56 UTC",
    "updated_date": "2024-04-15 06:20:55 UTC"
  },
  {
    "arxiv_id": "2402.09246v4",
    "title": "Who Plays First? Optimizing the Order of Play in Stackelberg Games with Many Robots",
    "authors": [
      "Haimin Hu",
      "Gabriele Dragotto",
      "Zixu Zhang",
      "Kaiqu Liang",
      "Bartolomeo Stellato",
      "Jaime F. Fisac"
    ],
    "abstract": "We consider the multi-agent spatial navigation problem of computing the\nsocially optimal order of play, i.e., the sequence in which the agents commit\nto their decisions, and its associated equilibrium in an N-player Stackelberg\ntrajectory game. We model this problem as a mixed-integer optimization problem\nover the space of all possible Stackelberg games associated with the order of\nplay's permutations. To solve the problem, we introduce Branch and Play (B&P),\nan efficient and exact algorithm that provably converges to a socially optimal\norder of play and its Stackelberg equilibrium. As a subroutine for B&P, we\nemploy and extend sequential trajectory planning, i.e., a popular multi-agent\ncontrol approach, to scalably compute valid local Stackelberg equilibria for\nany given order of play. We demonstrate the practical utility of B&P to\ncoordinate air traffic control, swarm formation, and delivery vehicle fleets.\nWe find that B&P consistently outperforms various baselines, and computes the\nsocially optimal equilibrium.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.SY",
      "eess.SY",
      "math.OC"
    ],
    "primary_category": "cs.RO",
    "comment": "Robotics: Science and Systems (RSS) 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.09246v4",
    "published_date": "2024-02-14 15:34:38 UTC",
    "updated_date": "2024-06-25 02:55:44 UTC"
  },
  {
    "arxiv_id": "2402.09236v2",
    "title": "Learning Interpretable Concepts: Unifying Causal Representation Learning and Foundation Models",
    "authors": [
      "Goutham Rajendran",
      "Simon Buchholz",
      "Bryon Aragam",
      "Bernhard Sch√∂lkopf",
      "Pradeep Ravikumar"
    ],
    "abstract": "To build intelligent machine learning systems, there are two broad\napproaches. One approach is to build inherently interpretable models, as\nendeavored by the growing field of causal representation learning. The other\napproach is to build highly-performant foundation models and then invest\nefforts into understanding how they work. In this work, we relate these two\napproaches and study how to learn human-interpretable concepts from data.\nWeaving together ideas from both fields, we formally define a notion of\nconcepts and show that they can be provably recovered from diverse data.\nExperiments on synthetic data and large language models show the utility of our\nunified approach.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "primary_category": "cs.LG",
    "comment": "To appear in NeurIPS 2024 under the modified title 'From Causal to\n  Concept-Based Representation Learning'",
    "pdf_url": "http://arxiv.org/pdf/2402.09236v2",
    "published_date": "2024-02-14 15:23:59 UTC",
    "updated_date": "2024-12-09 09:00:53 UTC"
  },
  {
    "arxiv_id": "2402.09233v1",
    "title": "Design and Realization of a Benchmarking Testbed for Evaluating Autonomous Platooning Algorithms",
    "authors": [
      "Michael Shaham",
      "Risha Ranjan",
      "Engin Kirda",
      "Taskin Padir"
    ],
    "abstract": "Autonomous vehicle platoons present near- and long-term opportunities to\nenhance operational efficiencies and save lives. The past 30 years have seen\nrapid development in the autonomous driving space, enabling new technologies\nthat will alleviate the strain placed on human drivers and reduce vehicle\nemissions. This paper introduces a testbed for evaluating and benchmarking\nplatooning algorithms on 1/10th scale vehicles with onboard sensors. To\ndemonstrate the testbed's utility, we evaluate three algorithms, linear\nfeedback and two variations of distributed model predictive control, and\ncompare their results on a typical platooning scenario where the lead vehicle\ntracks a reference trajectory that changes speed multiple times. We validate\nour algorithms in simulation to analyze the performance as the platoon size\nincreases, and find that the distributed model predictive control algorithms\noutperform linear feedback on hardware and in simulation.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.MA",
      "cs.SY",
      "eess.SY",
      "math.OC"
    ],
    "primary_category": "cs.RO",
    "comment": "To be published in International Symposium on Experimental Robotics,\n  2023",
    "pdf_url": "http://arxiv.org/pdf/2402.09233v1",
    "published_date": "2024-02-14 15:22:24 UTC",
    "updated_date": "2024-02-14 15:22:24 UTC"
  },
  {
    "arxiv_id": "2402.09225v2",
    "title": "Is my Data in your AI Model? Membership Inference Test with Application to Face Images",
    "authors": [
      "Daniel DeAlcala",
      "Aythami Morales",
      "Julian Fierrez",
      "Gonzalo Mancera",
      "Ruben Tolosana",
      "Javier Ortega-Garcia"
    ],
    "abstract": "This article introduces the Membership Inference Test (MINT), a novel\napproach that aims to empirically assess if given data was used during the\ntraining of AI/ML models. Specifically, we propose two MINT architectures\ndesigned to learn the distinct activation patterns that emerge when an Audited\nModel is exposed to data used during its training process. These architectures\nare based on Multilayer Perceptrons (MLPs) and Convolutional Neural Networks\n(CNNs). The experimental framework focuses on the challenging task of Face\nRecognition, considering three state-of-the-art Face Recognition systems.\nExperiments are carried out using six publicly available databases, comprising\nover 22 million face images in total. Different experimental scenarios are\nconsidered depending on the context of the AI model to test. Our proposed MINT\napproach achieves promising results, with up to 90% accuracy, indicating the\npotential to recognize if an AI model has been trained with specific data. The\nproposed MINT approach can serve to enforce privacy and fairness in several AI\napplications, e.g., revealing if sensitive or private data was used for\ntraining or tuning Large Language Models (LLMs).",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages including references and authors",
    "pdf_url": "http://arxiv.org/pdf/2402.09225v2",
    "published_date": "2024-02-14 15:09:01 UTC",
    "updated_date": "2024-09-06 11:15:23 UTC"
  },
  {
    "arxiv_id": "2402.09221v1",
    "title": "Spectral Filters, Dark Signals, and Attention Sinks",
    "authors": [
      "Nicola Cancedda"
    ],
    "abstract": "Projecting intermediate representations onto the vocabulary is an\nincreasingly popular interpretation tool for transformer-based LLMs, also known\nas the logit lens. We propose a quantitative extension to this approach and\ndefine spectral filters on intermediate representations based on partitioning\nthe singular vectors of the vocabulary embedding and unembedding matrices into\nbands. We find that the signals exchanged in the tail end of the spectrum are\nresponsible for attention sinking (Xiao et al. 2023), of which we provide an\nexplanation. We find that the loss of pretrained models can be kept low despite\nsuppressing sizable parts of the embedding spectrum in a layer-dependent way,\nas long as attention sinking is preserved. Finally, we discover that the\nrepresentation of tokens that draw attention from many tokens have large\nprojections on the tail end of the spectrum.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09221v1",
    "published_date": "2024-02-14 15:01:07 UTC",
    "updated_date": "2024-02-14 15:01:07 UTC"
  },
  {
    "arxiv_id": "2402.09211v1",
    "title": "DivaTrack: Diverse Bodies and Motions from Acceleration-Enhanced Three-Point Trackers",
    "authors": [
      "Dongseok Yang",
      "Jiho Kang",
      "Lingni Ma",
      "Joseph Greer",
      "Yuting Ye",
      "Sung-Hee Lee"
    ],
    "abstract": "Full-body avatar presence is crucial for immersive social and environmental\ninteractions in digital reality. However, current devices only provide three\nsix degrees of freedom (DOF) poses from the headset and two controllers (i.e.\nthree-point trackers). Because it is a highly under-constrained problem,\ninferring full-body pose from these inputs is challenging, especially when\nsupporting the full range of body proportions and use cases represented by the\ngeneral population. In this paper, we propose a deep learning framework,\nDivaTrack, which outperforms existing methods when applied to diverse body\nsizes and activities. We augment the sparse three-point inputs with linear\naccelerations from Inertial Measurement Units (IMU) to improve foot contact\nprediction. We then condition the otherwise ambiguous lower-body pose with the\npredictions of foot contact and upper-body pose in a two-stage model. We\nfurther stabilize the inferred full-body pose in a wide range of configurations\nby learning to blend predictions that are computed in two reference frames,\neach of which is designed for different types of motions. We demonstrate the\neffectiveness of our design on a large dataset that captures 22 subjects\nperforming challenging locomotion for three-point tracking, including lunges,\nhula-hooping, and sitting. As shown in a live demo using the Meta VR headset\nand Xsens IMUs, our method runs in real-time while accurately tracking a user's\nmotion when they perform a diverse set of movements.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "accepted to Eurographics 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.09211v1",
    "published_date": "2024-02-14 14:46:03 UTC",
    "updated_date": "2024-02-14 14:46:03 UTC"
  },
  {
    "arxiv_id": "2402.09205v2",
    "title": "Tell Me More! Towards Implicit User Intention Understanding of Language Model Driven Agents",
    "authors": [
      "Cheng Qian",
      "Bingxiang He",
      "Zhong Zhuang",
      "Jia Deng",
      "Yujia Qin",
      "Xin Cong",
      "Zhong Zhang",
      "Jie Zhou",
      "Yankai Lin",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "abstract": "Current language model-driven agents often lack mechanisms for effective user\nparticipation, which is crucial given the vagueness commonly found in user\ninstructions. Although adept at devising strategies and performing tasks, these\nagents struggle with seeking clarification and grasping precise user\nintentions. To bridge this gap, we introduce Intention-in-Interaction (IN3), a\nnovel benchmark designed to inspect users' implicit intentions through explicit\nqueries. Next, we propose the incorporation of model experts as the upstream in\nagent designs to enhance user-agent interaction. Employing IN3, we empirically\ntrain Mistral-Interact, a powerful model that proactively assesses task\nvagueness, inquires user intentions, and refines them into actionable goals\nbefore starting downstream agent task execution. Integrating it into the XAgent\nframework, we comprehensively evaluate the enhanced agent system regarding user\ninstruction understanding and execution, revealing that our approach notably\nexcels at identifying vague user tasks, recovering and summarizing critical\nmissing information, setting precise and necessary agent execution goals, and\nminimizing redundant tool usage, thus boosting overall efficiency. All the data\nand codes are released.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "26 pages, 5 tables, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.09205v2",
    "published_date": "2024-02-14 14:36:30 UTC",
    "updated_date": "2024-02-15 09:59:52 UTC"
  },
  {
    "arxiv_id": "2402.09200v1",
    "title": "Discovering Command and Control (C2) Channels on Tor and Public Networks Using Reinforcement Learning",
    "authors": [
      "Cheng Wang",
      "Christopher Redino",
      "Abdul Rahman",
      "Ryan Clark",
      "Daniel Radke",
      "Tyler Cody",
      "Dhruv Nandakumar",
      "Edward Bowen"
    ],
    "abstract": "Command and control (C2) channels are an essential component of many types of\ncyber attacks, as they enable attackers to remotely control their\nmalware-infected machines and execute harmful actions, such as propagating\nmalicious code across networks, exfiltrating confidential data, or initiating\ndistributed denial of service (DDoS) attacks. Identifying these C2 channels is\ntherefore crucial in helping to mitigate and prevent cyber attacks. However,\nidentifying C2 channels typically involves a manual process, requiring deep\nknowledge and expertise in cyber operations. In this paper, we propose a\nreinforcement learning (RL) based approach to automatically emulate C2 attack\ncampaigns using both the normal (public) and the Tor networks. In addition,\npayload size and network firewalls are configured to simulate real-world attack\nscenarios. Results on a typical network configuration show that the RL agent\ncan automatically discover resilient C2 attack paths utilizing both Tor-based\nand conventional communication channels, while also bypassing network\nfirewalls.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09200v1",
    "published_date": "2024-02-14 14:33:17 UTC",
    "updated_date": "2024-02-14 14:33:17 UTC"
  },
  {
    "arxiv_id": "2402.09199v1",
    "title": "Ten Words Only Still Help: Improving Black-Box AI-Generated Text Detection via Proxy-Guided Efficient Re-Sampling",
    "authors": [
      "Yuhui Shi",
      "Qiang Sheng",
      "Juan Cao",
      "Hao Mi",
      "Beizhe Hu",
      "Danding Wang"
    ],
    "abstract": "With the rapidly increasing application of large language models (LLMs),\ntheir abuse has caused many undesirable societal problems such as fake news,\nacademic dishonesty, and information pollution. This makes AI-generated text\n(AIGT) detection of great importance. Among existing methods, white-box methods\nare generally superior to black-box methods in terms of performance and\ngeneralizability, but they require access to LLMs' internal states and are not\napplicable to black-box settings. In this paper, we propose to estimate word\ngeneration probabilities as pseudo white-box features via multiple re-sampling\nto help improve AIGT detection under the black-box setting. Specifically, we\ndesign POGER, a proxy-guided efficient re-sampling method, which selects a\nsmall subset of representative words (e.g., 10 words) for performing multiple\nre-sampling in black-box AIGT detection. Experiments on datasets containing\ntexts from humans and seven LLMs show that POGER outperforms all baselines in\nmacro F1 under black-box, partial white-box, and out-of-distribution settings\nand maintains lower re-sampling costs than its existing counterparts.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "13 pages, 6 figures, 7 tables",
    "pdf_url": "http://arxiv.org/pdf/2402.09199v1",
    "published_date": "2024-02-14 14:32:16 UTC",
    "updated_date": "2024-02-14 14:32:16 UTC"
  },
  {
    "arxiv_id": "2402.10236v1",
    "title": "Discovering Sensorimotor Agency in Cellular Automata using Diversity Search",
    "authors": [
      "Gautier Hamon",
      "Mayalen Etcheverry",
      "Bert Wang-Chak Chan",
      "Cl√©ment Moulin-Frier",
      "Pierre-Yves Oudeyer"
    ],
    "abstract": "The research field of Artificial Life studies how life-like phenomena such as\nautopoiesis, agency, or self-regulation can self-organize in computer\nsimulations. In cellular automata (CA), a key open-question has been whether it\nit is possible to find environment rules that self-organize robust\n\"individuals\" from an initial state with no prior existence of things like\n\"bodies\", \"brain\", \"perception\" or \"action\". In this paper, we leverage recent\nadvances in machine learning, combining algorithms for diversity search,\ncurriculum learning and gradient descent, to automate the search of such\n\"individuals\", i.e. localized structures that move around with the ability to\nreact in a coherent manner to external obstacles and maintain their integrity,\nhence primitive forms of sensorimotor agency. We show that this approach\nenables to find systematically environmental conditions in CA leading to\nself-organization of such basic forms of agency. Through multiple experiments,\nwe show that the discovered agents have surprisingly robust capabilities to\nmove, maintain their body integrity and navigate among various obstacles. They\nalso show strong generalization abilities, with robustness to changes of scale,\nrandom updates or perturbations from the environment not seen during training.\nWe discuss how this approach opens new perspectives in AI and synthetic\nbioengineering.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.10236v1",
    "published_date": "2024-02-14 14:30:42 UTC",
    "updated_date": "2024-02-14 14:30:42 UTC"
  },
  {
    "arxiv_id": "2402.09193v2",
    "title": "(Ir)rationality and Cognitive Biases in Large Language Models",
    "authors": [
      "Olivia Macmillan-Scott",
      "Mirco Musolesi"
    ],
    "abstract": "Do large language models (LLMs) display rational reasoning? LLMs have been\nshown to contain human biases due to the data they have been trained on;\nwhether this is reflected in rational reasoning remains less clear. In this\npaper, we answer this question by evaluating seven language models using tasks\nfrom the cognitive psychology literature. We find that, like humans, LLMs\ndisplay irrationality in these tasks. However, the way this irrationality is\ndisplayed does not reflect that shown by humans. When incorrect answers are\ngiven by LLMs to these tasks, they are often incorrect in ways that differ from\nhuman-like biases. On top of this, the LLMs reveal an additional layer of\nirrationality in the significant inconsistency of the responses. Aside from the\nexperimental results, this paper seeks to make a methodological contribution by\nshowing how we can assess and compare different capabilities of these types of\nmodels, in this case with respect to rational reasoning.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09193v2",
    "published_date": "2024-02-14 14:17:21 UTC",
    "updated_date": "2024-02-15 11:09:09 UTC"
  },
  {
    "arxiv_id": "2402.09177v2",
    "title": "Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks",
    "authors": [
      "Yixin Cheng",
      "Markos Georgopoulos",
      "Volkan Cevher",
      "Grigorios G. Chrysos"
    ],
    "abstract": "Large Language Models (LLMs) are susceptible to Jailbreaking attacks, which\naim to extract harmful information by subtly modifying the attack query. As\ndefense mechanisms evolve, directly obtaining harmful information becomes\nincreasingly challenging for Jailbreaking attacks. In this work, inspired from\nChomsky's transformational-generative grammar theory and human practices of\nindirect context to elicit harmful information, we focus on a new attack form,\ncalled Contextual Interaction Attack. We contend that the prior\ncontext\\u2014the information preceding the attack query\\u2014plays a pivotal\nrole in enabling strong Jailbreaking attacks. Specifically, we propose a first\nmulti-turn approach that leverages benign preliminary questions to interact\nwith the LLM. Due to the autoregressive nature of LLMs, which use previous\nconversation rounds as context during generation, we guide the model's\nquestion-response pair to construct a context that is semantically aligned with\nthe attack query to execute the attack. We conduct experiments on seven\ndifferent LLMs and demonstrate the efficacy of this attack, which is black-box\nand can also transfer across LLMs. We believe this can lead to further\ndevelopments and understanding of security in LLMs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "29 pages",
    "pdf_url": "http://arxiv.org/pdf/2402.09177v2",
    "published_date": "2024-02-14 13:45:19 UTC",
    "updated_date": "2024-10-02 10:43:07 UTC"
  },
  {
    "arxiv_id": "2402.09161v1",
    "title": "Role-Playing Simulation Games using ChatGPT",
    "authors": [
      "Rita Stampfl",
      "Igor Ivkiƒá",
      "Barbara Geyer"
    ],
    "abstract": "Since the COVID-19 pandemic, educational institutions have embarked on\ndigital transformation projects. The success of these projects depends on\nintegrating new technologies and understanding the needs of digitally literate\nstudents. The \"learning by doing\" approach suggests that real success in\nlearning new skills is achieved when students can try out and practise these\nskills. In this article, we demonstrate how Large Language Models (LLMs) can\nenhance the quality of teaching by using ChatGPT in a role-playing simulation\ngame scenario to promote active learning. Moreover, we discuss how LLMs can\nboost students' interest in learning by allowing them to practice real-life\nscenarios using ChatGPT.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "Link to online article:\n  https://ercim-news.ercim.eu/en136/special/role-playing-simulation-games-using-chatgpt",
    "pdf_url": "http://arxiv.org/pdf/2402.09161v1",
    "published_date": "2024-02-14 13:24:21 UTC",
    "updated_date": "2024-02-14 13:24:21 UTC"
  },
  {
    "arxiv_id": "2402.09495v2",
    "title": "On the Potential of Network-Based Features for Fraud Detection",
    "authors": [
      "Catayoun Azarm",
      "Erman Acar",
      "Mickey van Zeelt"
    ],
    "abstract": "Online transaction fraud presents substantial challenges to businesses and\nconsumers, risking significant financial losses. Conventional rule-based\nsystems struggle to keep pace with evolving fraud tactics, leading to high\nfalse positive rates and missed detections. Machine learning techniques offer a\npromising solution by leveraging historical data to identify fraudulent\npatterns. This article explores using the personalised PageRank (PPR) algorithm\nto capture the social dynamics of fraud by analysing relationships between\nfinancial accounts. The primary objective is to compare the performance of\ntraditional features with the addition of PPR in fraud detection models.\nResults indicate that integrating PPR enhances the model's predictive power,\nsurpassing the baseline model. Additionally, the PPR feature provides unique\nand valuable information, evidenced by its high feature importance score.\nFeature stability analysis confirms consistent feature distributions across\ntraining and test datasets.",
    "categories": [
      "q-fin.RM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-fin.RM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09495v2",
    "published_date": "2024-02-14 13:20:09 UTC",
    "updated_date": "2024-02-19 11:58:13 UTC"
  },
  {
    "arxiv_id": "2402.09494v2",
    "title": "Can AI and humans genuinely communicate?",
    "authors": [
      "Constant Bonard"
    ],
    "abstract": "Can AI and humans genuinely communicate? In this article, after giving some\nbackground and motivating my proposal (sections 1 to 3), I explore a way to\nanswer this question that I call the \"mental-behavioral methodology\" (sections\n4 and 5). This methodology follows the following three steps: First, spell out\nwhat mental capacities are sufficient for human communication (as opposed to\ncommunication more generally). Second, spell out the experimental paradigms\nrequired to test whether a behavior exhibits these capacities. Third, apply or\nadapt these paradigms to test whether an AI displays the relevant behaviors. If\nthe first two steps are successfully completed, and if the AI passes the tests\nwith human-like results, this constitutes evidence that this AI and humans can\ngenuinely communicate. This mental-behavioral methodology has the advantage\nthat we don't need to understand the workings of black-box algorithms, such as\nstandard deep neural networks. This is comparable to the fact that we don't\nneed to understand how human brains work to know that humans can genuinely\ncommunicate. This methodology also has its disadvantages and I will discuss\nsome of them (section 6).",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "March 2024 preprint",
    "pdf_url": "http://arxiv.org/pdf/2402.09494v2",
    "published_date": "2024-02-14 13:00:40 UTC",
    "updated_date": "2024-03-25 16:32:44 UTC"
  },
  {
    "arxiv_id": "2402.09147v4",
    "title": "Into the Unknown: Self-Learning Large Language Models",
    "authors": [
      "Teddy Ferdinan",
      "Jan Koco≈Ñ",
      "Przemys≈Çaw Kazienko"
    ],
    "abstract": "We address the main problem of self-learning LLM: the question of what to\nlearn. We propose a self-learning LLM framework that enables an LLM to\nindependently learn previously unknown knowledge through self-assessment of\ntheir own hallucinations. We introduce a concept called Point in the Unknown\n(PiU) to identify atomic knowledge unknown to a model, along with four methods\nfor automatic PiUs identification, facilitating the creation of a self-learning\nloop that focuses exclusively on the absorption of currently unknown knowledge\ninto the model. Additionally, we developed evaluation metrics to gauge an LLM's\nself-learning capability. Our experiments revealed that LLMs with at least 3B\nparameters that have undergone some instruction training would be able to\nperform self-learning well. We further proved the effectiveness of\nself-learning by comparing the performance of a model that has undergone\nself-learning to a model that has not. Our self-learning concept allows more\nefficient LLM updates and opens new perspectives for LLM knowledge exchange.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to SENTIRE 2024 (ICDM Workshops):\n  https://sentic.net/sentire2024ferdinan.pdf",
    "pdf_url": "http://arxiv.org/pdf/2402.09147v4",
    "published_date": "2024-02-14 12:56:58 UTC",
    "updated_date": "2024-11-12 03:50:10 UTC"
  },
  {
    "arxiv_id": "2402.09141v1",
    "title": "Advancing NLP Models with Strategic Text Augmentation: A Comprehensive Study of Augmentation Methods and Curriculum Strategies",
    "authors": [
      "Himmet Toprak Kesgin",
      "Mehmet Fatih Amasyali"
    ],
    "abstract": "This study conducts a thorough evaluation of text augmentation techniques\nacross a variety of datasets and natural language processing (NLP) tasks to\naddress the lack of reliable, generalized evidence for these methods. It\nexamines the effectiveness of these techniques in augmenting training sets to\nimprove performance in tasks such as topic classification, sentiment analysis,\nand offensive language detection. The research emphasizes not only the\naugmentation methods, but also the strategic order in which real and augmented\ninstances are introduced during training. A major contribution is the\ndevelopment and evaluation of Modified Cyclical Curriculum Learning (MCCL) for\naugmented datasets, which represents a novel approach in the field. Results\nshow that specific augmentation methods, especially when integrated with MCCL,\nsignificantly outperform traditional training approaches in NLP model\nperformance. These results underscore the need for careful selection of\naugmentation techniques and sequencing strategies to optimize the balance\nbetween speed and quality improvement in various NLP tasks. The study concludes\nthat the use of augmentation methods, especially in conjunction with MCCL,\nleads to improved results in various classification tasks, providing a\nfoundation for future advances in text augmentation strategies in NLP.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09141v1",
    "published_date": "2024-02-14 12:41:09 UTC",
    "updated_date": "2024-02-14 12:41:09 UTC"
  },
  {
    "arxiv_id": "2402.09136v1",
    "title": "DolphCoder: Echo-Locating Code Large Language Models with Diverse and Multi-Objective Instruction Tuning",
    "authors": [
      "Yejie Wang",
      "Keqing He",
      "Guanting Dong",
      "Pei Wang",
      "Weihao Zeng",
      "Muxi Diao",
      "Yutao Mou",
      "Mengdi Zhang",
      "Jingang Wang",
      "Xunliang Cai",
      "Weiran Xu"
    ],
    "abstract": "Code Large Language Models (Code LLMs) have demonstrated outstanding\nperformance in code-related tasks. Several instruction tuning approaches have\nbeen proposed to boost the code generation performance of pre-trained Code\nLLMs. In this paper, we introduce a diverse instruction model (DolphCoder) with\nself-evaluating for code generation. It learns diverse instruction targets and\ncombines a code evaluation objective to enhance its code generation ability.\nOur model achieves superior performance on the HumanEval and MBPP benchmarks,\ndemonstrating new insights for future code instruction tuning work. Our key\nfindings are: (1) Augmenting more diverse responses with distinct reasoning\npaths increases the code capability of LLMs. (2) Improving one's ability to\nevaluate the correctness of code solutions also enhances their ability to\ncreate it.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "14 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.09136v1",
    "published_date": "2024-02-14 12:34:58 UTC",
    "updated_date": "2024-02-14 12:34:58 UTC"
  },
  {
    "arxiv_id": "2402.09132v4",
    "title": "Exploring the Adversarial Capabilities of Large Language Models",
    "authors": [
      "Lukas Struppek",
      "Minh Hieu Le",
      "Dominik Hintersdorf",
      "Kristian Kersting"
    ],
    "abstract": "The proliferation of large language models (LLMs) has sparked widespread and\ngeneral interest due to their strong language generation capabilities, offering\ngreat potential for both industry and research. While previous research delved\ninto the security and privacy issues of LLMs, the extent to which these models\ncan exhibit adversarial behavior remains largely unexplored. Addressing this\ngap, we investigate whether common publicly available LLMs have inherent\ncapabilities to perturb text samples to fool safety measures, so-called\nadversarial examples resp.~attacks. More specifically, we investigate whether\nLLMs are inherently able to craft adversarial examples out of benign samples to\nfool existing safe rails. Our experiments, which focus on hate speech\ndetection, reveal that LLMs succeed in finding adversarial perturbations,\neffectively undermining hate speech detection systems. Our findings carry\nsignificant implications for (semi-)autonomous systems relying on LLMs,\nhighlighting potential challenges in their interaction with existing systems\nand safety measures.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09132v4",
    "published_date": "2024-02-14 12:28:38 UTC",
    "updated_date": "2024-07-08 12:10:58 UTC"
  },
  {
    "arxiv_id": "2402.09129v1",
    "title": "Optimal Automated Market Makers: Differentiable Economics and Strong Duality",
    "authors": [
      "Michael J. Curry",
      "Zhou Fan",
      "David C. Parkes"
    ],
    "abstract": "The role of a market maker is to simultaneously offer to buy and sell\nquantities of goods, often a financial asset such as a share, at specified\nprices. An automated market maker (AMM) is a mechanism that offers to trade\naccording to some predetermined schedule; the best choice of this schedule\ndepends on the market maker's goals. The literature on the design of AMMs has\nmainly focused on prediction markets with the goal of information elicitation.\nMore recent work motivated by DeFi has focused instead on the goal of profit\nmaximization, but considering only a single type of good (traded with a\nnumeraire), including under adverse selection (Milionis et al. 2022). Optimal\nmarket making in the presence of multiple goods, including the possibility of\ncomplex bundling behavior, is not well understood. In this paper, we show that\nfinding an optimal market maker is dual to an optimal transport problem, with\nspecific geometric constraints on the transport plan in the dual. We show that\noptimal mechanisms for multiple goods and under adverse selection can take\nadvantage of bundling, both improved prices for bundled purchases and sales as\nwell as sometimes accepting payment \"in kind.\" We present conjectures of\noptimal mechanisms in additional settings which show further complex behavior.\nFrom a methodological perspective, we make essential use of the tools of\ndifferentiable economics to generate conjectures of optimal mechanisms, and\ngive a proof-of-concept for the use of such tools in guiding theoretical\ninvestigations.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "econ.TH",
      "q-fin.TR"
    ],
    "primary_category": "cs.GT",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09129v1",
    "published_date": "2024-02-14 12:27:54 UTC",
    "updated_date": "2024-02-14 12:27:54 UTC"
  },
  {
    "arxiv_id": "2402.09126v2",
    "title": "MPIrigen: MPI Code Generation through Domain-Specific Language Models",
    "authors": [
      "Nadav Schneider",
      "Niranjan Hasabnis",
      "Vy A. Vo",
      "Tal Kadosh",
      "Neva Krien",
      "Mihai CapotƒÉ",
      "Guy Tamir",
      "Ted Willke",
      "Nesreen Ahmed",
      "Yuval Pinter",
      "Timothy Mattson",
      "Gal Oren"
    ],
    "abstract": "The imperative need to scale computation across numerous nodes highlights the\nsignificance of efficient parallel computing, particularly in the realm of\nMessage Passing Interface (MPI) integration. The challenging parallel\nprogramming task of generating MPI-based parallel programs has remained\nunexplored. This study first investigates the performance of state-of-the-art\nlanguage models in generating MPI-based parallel programs. Findings reveal that\nwidely used models such as GPT-3.5 and PolyCoder (specialized multi-lingual\ncode models) exhibit notable performance degradation, when generating MPI-based\nprograms compared to general-purpose programs. In contrast, domain-specific\nmodels such as MonoCoder, which are pretrained on MPI-related programming\nlanguages of C and C++, outperform larger models. Subsequently, we introduce a\ndedicated downstream task of MPI-based program generation by fine-tuning\nMonoCoder on HPCorpusMPI. We call the resulting model as MPIrigen. We propose\nan innovative preprocessing for completion only after observing the whole code,\nthus enabling better completion with a wider context. Comparative analysis\nagainst GPT-3.5 zero-shot performance, using a novel HPC-oriented evaluation\nmethod, demonstrates that MPIrigen excels in generating accurate MPI functions\nup to 0.8 accuracy in location and function predictions, and with more than 0.9\naccuracy for argument predictions. The success of this tailored solution\nunderscores the importance of domain-specific fine-tuning in optimizing\nlanguage models for parallel computing code generation, paving the way for a\nnew generation of automatic parallelization tools. The sources of this work are\navailable at our GitHub MPIrigen repository:\nhttps://github.com/Scientific-Computing-Lab-NRCN/MPI-rigen",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.SE"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09126v2",
    "published_date": "2024-02-14 12:24:21 UTC",
    "updated_date": "2024-04-23 16:59:46 UTC"
  },
  {
    "arxiv_id": "2402.09109v1",
    "title": "Stochastic Spiking Attention: Accelerating Attention with Stochastic Computing in Spiking Networks",
    "authors": [
      "Zihang Song",
      "Prabodh Katti",
      "Osvaldo Simeone",
      "Bipin Rajendran"
    ],
    "abstract": "Spiking Neural Networks (SNNs) have been recently integrated into Transformer\narchitectures due to their potential to reduce computational demands and to\nimprove power efficiency. Yet, the implementation of the attention mechanism\nusing spiking signals on general-purpose computing platforms remains\ninefficient. In this paper, we propose a novel framework leveraging stochastic\ncomputing (SC) to effectively execute the dot-product attention for SNN-based\nTransformers. We demonstrate that our approach can achieve high classification\naccuracy ($83.53\\%$) on CIFAR-10 within 10 time steps, which is comparable to\nthe performance of a baseline artificial neural network implementation\n($83.66\\%$). We estimate that the proposed SC approach can lead to over\n$6.3\\times$ reduction in computing energy and $1.7\\times$ reduction in memory\naccess costs for a digital CMOS-based ASIC design. We experimentally validate\nour stochastic attention block design through an FPGA implementation, which is\nshown to achieve $48\\times$ lower latency as compared to a GPU implementation,\nwhile consuming $15\\times$ less power.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.LG",
      "cs.NE",
      "eess.SP"
    ],
    "primary_category": "cs.AR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09109v1",
    "published_date": "2024-02-14 11:47:19 UTC",
    "updated_date": "2024-02-14 11:47:19 UTC"
  },
  {
    "arxiv_id": "2402.09099v6",
    "title": "Neuron-based Multifractal Analysis of Neuron Interaction Dynamics in Large Models",
    "authors": [
      "Xiongye Xiao",
      "Heng Ping",
      "Chenyu Zhou",
      "Defu Cao",
      "Yaxing Li",
      "Yi-Zhuo Zhou",
      "Shixuan Li",
      "Nikos Kanakaris",
      "Paul Bogdan"
    ],
    "abstract": "In recent years, there has been increasing attention on the capabilities of\nlarge models, particularly in handling complex tasks that small-scale models\nare unable to perform. Notably, large language models (LLMs) have demonstrated\n``intelligent'' abilities such as complex reasoning and abstract language\ncomprehension, reflecting cognitive-like behaviors. However, current research\non emergent abilities in large models predominantly focuses on the relationship\nbetween model performance and size, leaving a significant gap in the systematic\nquantitative analysis of the internal structures and mechanisms driving these\nemergent abilities. Drawing inspiration from neuroscience research on brain\nnetwork structure and self-organization, we propose (i) a general network\nrepresentation of large models, (ii) a new analytical framework, called\nNeuron-based Multifractal Analysis (NeuroMFA), for structural analysis, and\n(iii) a novel structure-based metric as a proxy for emergent abilities of large\nmodels. By linking structural features to the capabilities of large models,\nNeuroMFA provides a quantitative framework for analyzing emergent phenomena in\nlarge models. Our experiments show that the proposed method yields a\ncomprehensive measure of network's evolving heterogeneity and organization,\noffering theoretical foundations and a new perspective for investigating\nemergent abilities in large models.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "ICLR 2025: https://openreview.net/forum?id=nt8gBX58Kh",
    "pdf_url": "http://arxiv.org/pdf/2402.09099v6",
    "published_date": "2024-02-14 11:20:09 UTC",
    "updated_date": "2025-02-04 07:46:33 UTC"
  },
  {
    "arxiv_id": "2402.09097v1",
    "title": "A Digital Twin prototype for traffic sign recognition of a learning-enabled autonomous vehicle",
    "authors": [
      "Mohamed AbdElSalam",
      "Loai Ali",
      "Saddek Bensalem",
      "Weicheng He",
      "Panagiotis Katsaros",
      "Nikolaos Kekatos",
      "Doron Peled",
      "Anastasios Temperekidis",
      "Changshun Wu"
    ],
    "abstract": "In this paper, we present a novel digital twin prototype for a\nlearning-enabled self-driving vehicle. The primary objective of this digital\ntwin is to perform traffic sign recognition and lane keeping. The digital twin\narchitecture relies on co-simulation and uses the Functional Mock-up Interface\nand SystemC Transaction Level Modeling standards. The digital twin consists of\nfour clients, i) a vehicle model that is designed in Amesim tool, ii) an\nenvironment model developed in Prescan, iii) a lane-keeping controller designed\nin Robot Operating System, and iv) a perception and speed control module\ndeveloped in the formal modeling language of BIP (Behavior, Interaction,\nPriority). These clients interface with the digital twin platform,\nPAVE360-Veloce System Interconnect (PAVE360-VSI). PAVE360-VSI acts as the\nco-simulation orchestrator and is responsible for synchronization,\ninterconnection, and data exchange through a server. The server establishes\nconnections among the different clients and also ensures adherence to the\nEthernet protocol. We conclude with illustrative digital twin simulations and\nrecommendations for future work.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09097v1",
    "published_date": "2024-02-14 11:17:14 UTC",
    "updated_date": "2024-02-14 11:17:14 UTC"
  },
  {
    "arxiv_id": "2402.09091v2",
    "title": "Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues",
    "authors": [
      "Zhiyuan Chang",
      "Mingyang Li",
      "Yi Liu",
      "Junjie Wang",
      "Qing Wang",
      "Yang Liu"
    ],
    "abstract": "With the development of LLMs, the security threats of LLMs are getting more\nand more attention. Numerous jailbreak attacks have been proposed to assess the\nsecurity defense of LLMs. Current jailbreak attacks primarily utilize scenario\ncamouflage techniques. However their explicitly mention of malicious intent\nwill be easily recognized and defended by LLMs. In this paper, we propose an\nindirect jailbreak attack approach, Puzzler, which can bypass the LLM's defense\nstrategy and obtain malicious response by implicitly providing LLMs with some\nclues about the original malicious query. In addition, inspired by the wisdom\nof \"When unable to attack, defend\" from Sun Tzu's Art of War, we adopt a\ndefensive stance to gather clues about the original malicious query through\nLLMs. Extensive experimental results show that Puzzler achieves a query success\nrate of 96.6% on closed-source LLMs, which is 57.9%-82.7% higher than\nbaselines. Furthermore, when tested against the state-of-the-art jailbreak\ndetection approaches, Puzzler proves to be more effective at evading detection\ncompared to baselines.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CR",
    "comment": "13 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.09091v2",
    "published_date": "2024-02-14 11:11:51 UTC",
    "updated_date": "2024-02-16 10:24:04 UTC"
  },
  {
    "arxiv_id": "2402.09085v3",
    "title": "Polynomial Semantics of Tractable Probabilistic Circuits",
    "authors": [
      "Oliver Broadrick",
      "Honghua Zhang",
      "Guy Van den Broeck"
    ],
    "abstract": "Probabilistic circuits compute multilinear polynomials that represent\nmultivariate probability distributions. They are tractable models that support\nefficient marginal inference. However, various polynomial semantics have been\nconsidered in the literature (e.g., network polynomials, likelihood\npolynomials, generating functions, and Fourier transforms). The relationships\nbetween circuit representations of these polynomial encodings of distributions\nis largely unknown. In this paper, we prove that for distributions over binary\nvariables, each of these probabilistic circuit models is equivalent in the\nsense that any circuit for one of them can be transformed into a circuit for\nany of the others with only a polynomial increase in size. They are therefore\nall tractable for marginal inference on the same class of distributions.\nFinally, we explore the natural extension of one such polynomial semantics,\ncalled probabilistic generating circuits, to categorical random variables, and\nestablish that inference becomes #P-hard.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09085v3",
    "published_date": "2024-02-14 11:02:04 UTC",
    "updated_date": "2024-08-08 05:58:30 UTC"
  },
  {
    "arxiv_id": "2402.09084v1",
    "title": "Sobolev Training for Operator Learning",
    "authors": [
      "Namkyeong Cho",
      "Junseung Ryu",
      "Hyung Ju Hwang"
    ],
    "abstract": "This study investigates the impact of Sobolev Training on operator learning\nframeworks for improving model performance. Our research reveals that\nintegrating derivative information into the loss function enhances the training\nprocess, and we propose a novel framework to approximate derivatives on\nirregular meshes in operator learning. Our findings are supported by both\nexperimental evidence and theoretical analysis. This demonstrates the\neffectiveness of Sobolev Training in approximating the solution operators\nbetween infinite-dimensional spaces.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09084v1",
    "published_date": "2024-02-14 10:57:29 UTC",
    "updated_date": "2024-02-14 10:57:29 UTC"
  },
  {
    "arxiv_id": "2402.09078v2",
    "title": "Exploiting Estimation Bias in Clipped Double Q-Learning for Continous Control Reinforcement Learning Tasks",
    "authors": [
      "Niccol√≤ Turcato",
      "Alberto Sinigaglia",
      "Alberto Dalla Libera",
      "Ruggero Carli",
      "Gian Antonio Susto"
    ],
    "abstract": "Continuous control Deep Reinforcement Learning (RL) approaches are known to\nsuffer from estimation biases, leading to suboptimal policies. This paper\nintroduces innovative methods in RL, focusing on addressing and exploiting\nestimation biases in Actor-Critic methods for continuous control tasks, using\nDeep Double Q-Learning. We design a Bias Exploiting (BE) mechanism to\ndynamically select the most advantageous estimation bias during training of the\nRL agent. Most State-of-the-art Deep RL algorithms can be equipped with the BE\nmechanism, without hindering performance or computational complexity. Our\nextensive experiments across various continuous control tasks demonstrate the\neffectiveness of our approaches. We show that RL algorithms equipped with this\nmethod can match or surpass their counterparts, particularly in environments\nwhere estimation biases significantly impact learning. The results underline\nthe importance of bias exploitation in improving policy learning in RL.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09078v2",
    "published_date": "2024-02-14 10:44:03 UTC",
    "updated_date": "2024-10-11 13:42:43 UTC"
  },
  {
    "arxiv_id": "2402.09066v3",
    "title": "Solid Waste Detection, Monitoring and Mapping in Remote Sensing Images: A Survey",
    "authors": [
      "Piero Fraternali",
      "Luca Morandini",
      "Sergio Luis Herrera Gonz√°lez"
    ],
    "abstract": "The detection and characterization of illegal solid waste disposal sites are\nessential for environmental protection, particularly for mitigating pollution\nand health hazards. Improperly managed landfills contaminate soil and\ngroundwater via rainwater infiltration, posing threats to both animals and\nhumans. Traditional landfill identification approaches, such as on-site\ninspections, are time-consuming and expensive. Remote sensing is a\ncost-effective solution for the identification and monitoring of solid waste\ndisposal sites that enables broad coverage and repeated acquisitions over time.\nEarth Observation (EO) satellites, equipped with an array of sensors and\nimaging capabilities, have been providing high-resolution data for several\ndecades. Researchers proposed specialized techniques that leverage remote\nsensing imagery to perform a range of tasks such as waste site detection,\ndumping site monitoring, and assessment of suitable locations for new\nlandfills. This review aims to provide a detailed illustration of the most\nrelevant proposals for the detection and monitoring of solid waste sites by\ndescribing and comparing the approaches, the implemented techniques, and the\nemployed data. Furthermore, since the data sources are of the utmost importance\nfor developing an effective solid waste detection model, a comprehensive\noverview of the satellites and publicly available data sets is presented.\nFinally, this paper identifies the open issues in the state-of-the-art and\ndiscusses the relevant research directions for reducing the costs and improving\nthe effectiveness of novel solid waste detection methods.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09066v3",
    "published_date": "2024-02-14 10:24:04 UTC",
    "updated_date": "2024-12-13 07:26:51 UTC"
  },
  {
    "arxiv_id": "2402.09059v1",
    "title": "I can't see it but I can Fine-tune it: On Encrypted Fine-tuning of Transformers using Fully Homomorphic Encryption",
    "authors": [
      "Prajwal Panzade",
      "Daniel Takabi",
      "Zhipeng Cai"
    ],
    "abstract": "In today's machine learning landscape, fine-tuning pretrained transformer\nmodels has emerged as an essential technique, particularly in scenarios where\naccess to task-aligned training data is limited. However, challenges surface\nwhen data sharing encounters obstacles due to stringent privacy regulations or\nuser apprehension regarding personal information disclosure. Earlier works\nbased on secure multiparty computation (SMC) and fully homomorphic encryption\n(FHE) for privacy-preserving machine learning (PPML) focused more on\nprivacy-preserving inference than privacy-preserving training. In response, we\nintroduce BlindTuner, a privacy-preserving fine-tuning system that enables\ntransformer training exclusively on homomorphically encrypted data for image\nclassification. Our extensive experimentation validates BlindTuner's\neffectiveness by demonstrating comparable accuracy to non-encrypted models.\nNotably, our findings highlight a substantial speed enhancement of 1.5x to 600x\nover previous work in this domain.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted for the presentation at PPAI @The 38th Annual AAAI\n  Conference on Artificial Intelligence 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.09059v1",
    "published_date": "2024-02-14 10:15:43 UTC",
    "updated_date": "2024-02-14 10:15:43 UTC"
  },
  {
    "arxiv_id": "2402.09056v3",
    "title": "Is Epistemic Uncertainty Faithfully Represented by Evidential Deep Learning Methods?",
    "authors": [
      "Mira J√ºrgens",
      "Nis Meinert",
      "Viktor Bengs",
      "Eyke H√ºllermeier",
      "Willem Waegeman"
    ],
    "abstract": "Trustworthy ML systems should not only return accurate predictions, but also\na reliable representation of their uncertainty. Bayesian methods are commonly\nused to quantify both aleatoric and epistemic uncertainty, but alternative\napproaches, such as evidential deep learning methods, have become popular in\nrecent years. The latter group of methods in essence extends empirical risk\nminimization (ERM) for predicting second-order probability distributions over\noutcomes, from which measures of epistemic (and aleatoric) uncertainty can be\nextracted. This paper presents novel theoretical insights of evidential deep\nlearning, highlighting the difficulties in optimizing second-order loss\nfunctions and interpreting the resulting epistemic uncertainty measures. With a\nsystematic setup that covers a wide range of approaches for classification,\nregression and counts, it provides novel insights into issues of\nidentifiability and convergence in second-order loss minimization, and the\nrelative (rather than absolute) nature of epistemic uncertainty measures.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09056v3",
    "published_date": "2024-02-14 10:07:05 UTC",
    "updated_date": "2024-09-09 20:54:39 UTC"
  },
  {
    "arxiv_id": "2402.09055v3",
    "title": "Comment-aided Video-Language Alignment via Contrastive Pre-training for Short-form Video Humor Detection",
    "authors": [
      "Yang Liu",
      "Tongfei Shen",
      "Dong Zhang",
      "Qingying Sun",
      "Shoushan Li",
      "Guodong Zhou"
    ],
    "abstract": "The growing importance of multi-modal humor detection within affective\ncomputing correlates with the expanding influence of short-form video sharing\non social media platforms. In this paper, we propose a novel two-branch\nhierarchical model for short-form video humor detection (SVHD), named\nComment-aided Video-Language Alignment (CVLA) via data-augmented multi-modal\ncontrastive pre-training. Notably, our CVLA not only operates on raw signals\nacross various modal channels but also yields an appropriate multi-modal\nrepresentation by aligning the video and language components within a\nconsistent semantic space. The experimental results on two humor detection\ndatasets, including DY11k and UR-FUNNY, demonstrate that CVLA dramatically\noutperforms state-of-the-art and several competitive baseline approaches. Our\ndataset, code and model release at https://github.com/yliu-cs/CVLA.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ICMR 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.09055v3",
    "published_date": "2024-02-14 10:05:19 UTC",
    "updated_date": "2024-04-15 03:23:07 UTC"
  },
  {
    "arxiv_id": "2402.09052v1",
    "title": "L3GO: Language Agents with Chain-of-3D-Thoughts for Generating Unconventional Objects",
    "authors": [
      "Yutaro Yamada",
      "Khyathi Chandu",
      "Yuchen Lin",
      "Jack Hessel",
      "Ilker Yildirim",
      "Yejin Choi"
    ],
    "abstract": "Diffusion-based image generation models such as DALL-E 3 and Stable\nDiffusion-XL demonstrate remarkable capabilities in generating images with\nrealistic and unique compositions. Yet, these models are not robust in\nprecisely reasoning about physical and spatial configurations of objects,\nespecially when instructed with unconventional, thereby out-of-distribution\ndescriptions, such as \"a chair with five legs\". In this paper, we propose a\nlanguage agent with chain-of-3D-thoughts (L3GO), an inference-time approach\nthat can reason about part-based 3D mesh generation of unconventional objects\nthat current data-driven diffusion models struggle with. More concretely, we\nuse large language models as agents to compose a desired object via\ntrial-and-error within the 3D simulation environment. To facilitate our\ninvestigation, we develop a new benchmark, Unconventionally Feasible Objects\n(UFO), as well as SimpleBlenv, a wrapper environment built on top of Blender\nwhere language agents can build and compose atomic building blocks via API\ncalls. Human and automatic GPT-4V evaluations show that our approach surpasses\nthe standard GPT-4 and other language agents (e.g., ReAct and Reflexion) for 3D\nmesh generation on ShapeNet. Moreover, when tested on our UFO benchmark, our\napproach outperforms other state-of-the-art text-to-2D image and text-to-3D\nmodels based on human evaluation.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09052v1",
    "published_date": "2024-02-14 09:51:05 UTC",
    "updated_date": "2024-02-14 09:51:05 UTC"
  },
  {
    "arxiv_id": "2402.09051v2",
    "title": "FGeo-DRL: Deductive Reasoning for Geometric Problems through Deep Reinforcement Learning",
    "authors": [
      "Jia Zou",
      "Xiaokai Zhang",
      "Yiming He",
      "Na Zhu",
      "Tuo Leng"
    ],
    "abstract": "The human-like automatic deductive reasoning has always been one of the most\nchallenging open problems in the interdiscipline of mathematics and artificial\nintelligence. This paper is the third in a series of our works. We built a\nneural-symbolic system, called FGeoDRL, to automatically perform human-like\ngeometric deductive reasoning. The neural part is an AI agent based on\nreinforcement learning, capable of autonomously learning problem-solving\nmethods from the feedback of a formalized environment, without the need for\nhuman supervision. It leverages a pre-trained natural language model to\nestablish a policy network for theorem selection and employ Monte Carlo Tree\nSearch for heuristic exploration. The symbolic part is a reinforcement learning\nenvironment based on geometry formalization theory and FormalGeo, which models\nGPS as a Markov Decision Process. In this formal symbolic system, the known\nconditions and objectives of the problem form the state space, while the set of\ntheorems forms the action space. Leveraging FGeoDRL, we have achieved readable\nand verifiable automated solutions to geometric problems. Experiments conducted\non the formalgeo7k dataset have achieved a problem-solving success rate of\n86.40%. The project is available at https://github.com/PersonNoName/FGeoDRL.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "15 pages",
    "pdf_url": "http://arxiv.org/pdf/2402.09051v2",
    "published_date": "2024-02-14 09:48:39 UTC",
    "updated_date": "2024-02-15 04:50:52 UTC"
  },
  {
    "arxiv_id": "2402.09047v1",
    "title": "FGeo-TP: A Language Model-Enhanced Solver for Geometry Problems",
    "authors": [
      "Yiming He",
      "Jia Zou",
      "Xiaokai Zhang",
      "Na Zhu",
      "Tuo Leng"
    ],
    "abstract": "The application of contemporary artificial intelligence techniques to address\ngeometric problems and automated deductive proof has always been a grand\nchallenge to the interdiscipline field of mathematics and artificial\nIntelligence. This is the fourth article in a series of our works, in our\nprevious work, we established of a geometric formalized system known as\nFormalGeo. Moreover we annotated approximately 7000 geometric problems, forming\nthe FormalGeo7k dataset. Despite the FGPS (Formal Geometry Problem Solver) can\nachieve interpretable algebraic equation solving and human-like deductive\nreasoning, it often experiences timeouts due to the complexity of the search\nstrategy. In this paper, we introduced FGeo-TP (Theorem Predictor), which\nutilizes the language model to predict theorem sequences for solving geometry\nproblems. We compared the effectiveness of various Transformer architectures,\nsuch as BART or T5, in theorem prediction, implementing pruning in the search\nprocess of FGPS, thereby improving its performance in solving geometry\nproblems. Our results demonstrate a significant increase in the problem-solving\nrate of the language model-enhanced FGeo-TP on the FormalGeo7k dataset, rising\nfrom 39.7% to 80.86%. Furthermore, FGeo-TP exhibits notable reductions in\nsolving time and search steps across problems of varying difficulty levels.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "16 pages",
    "pdf_url": "http://arxiv.org/pdf/2402.09047v1",
    "published_date": "2024-02-14 09:44:28 UTC",
    "updated_date": "2024-02-14 09:44:28 UTC"
  },
  {
    "arxiv_id": "2402.09046v1",
    "title": "Inference of Abstraction for a Unified Account of Reasoning and Learning",
    "authors": [
      "Hiroyuki Kido"
    ],
    "abstract": "Inspired by Bayesian approaches to brain function in neuroscience, we give a\nsimple theory of probabilistic inference for a unified account of reasoning and\nlearning. We simply model how data cause symbolic knowledge in terms of its\nsatisfiability in formal logic. The underlying idea is that reasoning is a\nprocess of deriving symbolic knowledge from data via abstraction, i.e.,\nselective ignorance. The logical consequence relation is discussed for its\nproof-based theoretical correctness. The MNIST dataset is discussed for its\nexperiment-based empirical correctness.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "arXiv admin note: substantial text overlap with arXiv:2402.08646",
    "pdf_url": "http://arxiv.org/pdf/2402.09046v1",
    "published_date": "2024-02-14 09:43:35 UTC",
    "updated_date": "2024-02-14 09:43:35 UTC"
  },
  {
    "arxiv_id": "2402.09034v1",
    "title": "Enhancing Sequential Model Performance with Squared Sigmoid TanH (SST) Activation Under Data Constraints",
    "authors": [
      "Barathi Subramanian",
      "Rathinaraja Jeyaraj",
      "Rakhmonov Akhrorjon Akhmadjon Ugli",
      "Jeonghong Kim"
    ],
    "abstract": "Activation functions enable neural networks to learn complex representations\nby introducing non-linearities. While feedforward models commonly use rectified\nlinear units, sequential models like recurrent neural networks, long short-term\nmemory (LSTMs) and gated recurrent units (GRUs) still rely on Sigmoid and TanH\nactivation functions. However, these classical activation functions often\nstruggle to model sparse patterns when trained on small sequential datasets to\neffectively capture temporal dependencies. To address this limitation, we\npropose squared Sigmoid TanH (SST) activation specifically tailored to enhance\nthe learning capability of sequential models under data constraints. SST\napplies mathematical squaring to amplify differences between strong and weak\nactivations as signals propagate over time, facilitating improved gradient flow\nand information filtering. We evaluate SST-powered LSTMs and GRUs for diverse\napplications, such as sign language recognition, regression, and time-series\nclassification tasks, where the dataset is limited. Our experiments demonstrate\nthat SST models consistently outperform RNN-based models with baseline\nactivations, exhibiting improved test accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages,9 figures, Submitted to IJCAI 2024 conference",
    "pdf_url": "http://arxiv.org/pdf/2402.09034v1",
    "published_date": "2024-02-14 09:20:13 UTC",
    "updated_date": "2024-02-14 09:20:13 UTC"
  },
  {
    "arxiv_id": "2402.09023v1",
    "title": "Review-Incorporated Model-Agnostic Profile Injection Attacks on Recommender Systems",
    "authors": [
      "Shiyi Yang",
      "Lina Yao",
      "Chen Wang",
      "Xiwei Xu",
      "Liming Zhu"
    ],
    "abstract": "Recent studies have shown that recommender systems (RSs) are highly\nvulnerable to data poisoning attacks. Understanding attack tactics helps\nimprove the robustness of RSs. We intend to develop efficient attack methods\nthat use limited resources to generate high-quality fake user profiles to\nachieve 1) transferability among black-box RSs 2) and imperceptibility among\ndetectors. In order to achieve these goals, we introduce textual reviews of\nproducts to enhance the generation quality of the profiles. Specifically, we\npropose a novel attack framework named R-Trojan, which formulates the attack\nobjectives as an optimization problem and adopts a tailored transformer-based\ngenerative adversarial network (GAN) to solve it so that high-quality attack\nprofiles can be produced. Comprehensive experiments on real-world datasets\ndemonstrate that R-Trojan greatly outperforms state-of-the-art attack methods\non various victim RSs under black-box settings and show its good\nimperceptibility.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted by ICDM 2023",
    "pdf_url": "http://arxiv.org/pdf/2402.09023v1",
    "published_date": "2024-02-14 08:56:41 UTC",
    "updated_date": "2024-02-14 08:56:41 UTC"
  },
  {
    "arxiv_id": "2402.09015v3",
    "title": "Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications",
    "authors": [
      "Negar Arabzadeh",
      "Julia Kiseleva",
      "Qingyun Wu",
      "Chi Wang",
      "Ahmed Awadallah",
      "Victor Dibia",
      "Adam Fourney",
      "Charles Clarke"
    ],
    "abstract": "The rapid development in the field of Large Language Models (LLMs) has led to\na surge in applications that facilitate collaboration among multiple agents to\nassist humans in their daily tasks. However, a significant gap remains in\nassessing whether LLM-powered applications genuinely enhance user experience\nand task execution efficiency. This highlights the pressing need for methods to\nverify utility of LLM-powered applications, particularly by ensuring alignment\nbetween the application's functionality and end-user needs. We introduce\nAgentEval provides an implementation for the math problems, a novel framework\ndesigned to simplify the utility verification process by automatically\nproposing a set of criteria tailored to the unique purpose of any given\napplication. This allows for a comprehensive assessment, quantifying the\nutility of an application against the suggested criteria. We present a\ncomprehensive analysis of the robustness of quantifier's work.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.09015v3",
    "published_date": "2024-02-14 08:46:15 UTC",
    "updated_date": "2024-02-22 23:49:10 UTC"
  },
  {
    "arxiv_id": "2402.08995v1",
    "title": "AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems",
    "authors": [
      "Jiaying Lu",
      "Bo Pan",
      "Jieyi Chen",
      "Yingchaojie Feng",
      "Jingyuan Hu",
      "Yuchen Peng",
      "Wei Chen"
    ],
    "abstract": "Recently, Large Language Model based Autonomous system(LLMAS) has gained\ngreat popularity for its potential to simulate complicated behaviors of human\nsocieties. One of its main challenges is to present and analyze the dynamic\nevents evolution of LLMAS. In this work, we present a visualization approach to\nexplore detailed statuses and agents' behavior within LLMAS. We propose a\ngeneral pipeline that establishes a behavior structure from raw LLMAS execution\nevents, leverages a behavior summarization algorithm to construct a\nhierarchical summary of the entire structure in terms of time sequence, and a\ncause trace method to mine the causal relationship between agent behaviors. We\nthen develop AgentLens, a visual analysis system that leverages a hierarchical\ntemporal visualization for illustrating the evolution of LLMAS, and supports\nusers to interactively investigate details and causes of agents' behaviors. Two\nusage scenarios and a user study demonstrate the effectiveness and usability of\nour AgentLens.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.08995v1",
    "published_date": "2024-02-14 07:48:16 UTC",
    "updated_date": "2024-02-14 07:48:16 UTC"
  },
  {
    "arxiv_id": "2402.08994v1",
    "title": "CLIP-MUSED: CLIP-Guided Multi-Subject Visual Neural Information Semantic Decoding",
    "authors": [
      "Qiongyi Zhou",
      "Changde Du",
      "Shengpei Wang",
      "Huiguang He"
    ],
    "abstract": "The study of decoding visual neural information faces challenges in\ngeneralizing single-subject decoding models to multiple subjects, due to\nindividual differences. Moreover, the limited availability of data from a\nsingle subject has a constraining impact on model performance. Although prior\nmulti-subject decoding methods have made significant progress, they still\nsuffer from several limitations, including difficulty in extracting global\nneural response features, linear scaling of model parameters with the number of\nsubjects, and inadequate characterization of the relationship between neural\nresponses of different subjects to various stimuli. To overcome these\nlimitations, we propose a CLIP-guided Multi-sUbject visual neural information\nSEmantic Decoding (CLIP-MUSED) method. Our method consists of a\nTransformer-based feature extractor to effectively model global neural\nrepresentations. It also incorporates learnable subject-specific tokens that\nfacilitates the aggregation of multi-subject data without a linear increase of\nparameters. Additionally, we employ representational similarity analysis (RSA)\nto guide token representation learning based on the topological relationship of\nvisual stimuli in the representation space of CLIP, enabling full\ncharacterization of the relationship between neural responses of different\nsubjects under different stimuli. Finally, token representations are used for\nmulti-subject semantic decoding. Our proposed method outperforms single-subject\ndecoding methods and achieves state-of-the-art performance among the existing\nmulti-subject methods on two fMRI datasets. Visualization results provide\ninsights into the effectiveness of our proposed method. Code is available at\nhttps://github.com/CLIP-MUSED/CLIP-MUSED.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ICLR2024",
    "pdf_url": "http://arxiv.org/pdf/2402.08994v1",
    "published_date": "2024-02-14 07:41:48 UTC",
    "updated_date": "2024-02-14 07:41:48 UTC"
  },
  {
    "arxiv_id": "2402.08983v4",
    "title": "SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding",
    "authors": [
      "Zhangchen Xu",
      "Fengqing Jiang",
      "Luyao Niu",
      "Jinyuan Jia",
      "Bill Yuchen Lin",
      "Radha Poovendran"
    ],
    "abstract": "As large language models (LLMs) become increasingly integrated into\nreal-world applications such as code generation and chatbot assistance,\nextensive efforts have been made to align LLM behavior with human values,\nincluding safety. Jailbreak attacks, aiming to provoke unintended and unsafe\nbehaviors from LLMs, remain a significant/leading LLM safety threat. In this\npaper, we aim to defend LLMs against jailbreak attacks by introducing\nSafeDecoding, a safety-aware decoding strategy for LLMs to generate helpful and\nharmless responses to user queries. Our insight in developing SafeDecoding is\nbased on the observation that, even though probabilities of tokens representing\nharmful contents outweigh those representing harmless responses, safety\ndisclaimers still appear among the top tokens after sorting tokens by\nprobability in descending order. This allows us to mitigate jailbreak attacks\nby identifying safety disclaimers and amplifying their token probabilities,\nwhile simultaneously attenuating the probabilities of token sequences that are\naligned with the objectives of jailbreak attacks. We perform extensive\nexperiments on five LLMs using six state-of-the-art jailbreak attacks and four\nbenchmark datasets. Our results show that SafeDecoding significantly reduces\nthe attack success rate and harmfulness of jailbreak attacks without\ncompromising the helpfulness of responses to benign user queries. SafeDecoding\noutperforms six defense methods.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "To appear in ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.08983v4",
    "published_date": "2024-02-14 06:54:31 UTC",
    "updated_date": "2024-07-25 22:59:44 UTC"
  },
  {
    "arxiv_id": "2402.08982v1",
    "title": "MEL: Efficient Multi-Task Evolutionary Learning for High-Dimensional Feature Selection",
    "authors": [
      "Xubin Wang",
      "Haojiong Shangguan",
      "Fengyi Huang",
      "Shangrui Wu",
      "Weijia Jia"
    ],
    "abstract": "Feature selection is a crucial step in data mining to enhance model\nperformance by reducing data dimensionality. However, the increasing\ndimensionality of collected data exacerbates the challenge known as the \"curse\nof dimensionality\", where computation grows exponentially with the number of\ndimensions. To tackle this issue, evolutionary computational (EC) approaches\nhave gained popularity due to their simplicity and applicability.\nUnfortunately, the diverse designs of EC methods result in varying abilities to\nhandle different data, often underutilizing and not sharing information\neffectively. In this paper, we propose a novel approach called PSO-based\nMulti-task Evolutionary Learning (MEL) that leverages multi-task learning to\naddress these challenges. By incorporating information sharing between\ndifferent feature selection tasks, MEL achieves enhanced learning ability and\nefficiency. We evaluate the effectiveness of MEL through extensive experiments\non 22 high-dimensional datasets. Comparing against 24 EC approaches, our method\nexhibits strong competitiveness. Additionally, we have open-sourced our code on\nGitHub at https://github.com/wangxb96/MEL.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.08982v1",
    "published_date": "2024-02-14 06:51:49 UTC",
    "updated_date": "2024-02-14 06:51:49 UTC"
  },
  {
    "arxiv_id": "2402.08979v1",
    "title": "Learning-enabled Flexible Job-shop Scheduling for Scalable Smart Manufacturing",
    "authors": [
      "Sihoon Moon",
      "Sanghoon Lee",
      "Kyung-Joon Park"
    ],
    "abstract": "In smart manufacturing systems (SMSs), flexible job-shop scheduling with\ntransportation constraints (FJSPT) is essential to optimize solutions for\nmaximizing productivity, considering production flexibility based on automated\nguided vehicles (AGVs). Recent developments in deep reinforcement learning\n(DRL)-based methods for FJSPT have encountered a scale generalization\nchallenge. These methods underperform when applied to environment at scales\ndifferent from their training set, resulting in low-quality solutions. To\naddress this, we introduce a novel graph-based DRL method, named the\nHeterogeneous Graph Scheduler (HGS). Our method leverages locally extracted\nrelational knowledge among operations, machines, and vehicle nodes for\nscheduling, with a graph-structured decision-making framework that reduces\nencoding complexity and enhances scale generalization. Our performance\nevaluation, conducted with benchmark datasets, reveals that the proposed method\noutperforms traditional dispatching rules, meta-heuristics, and existing\nDRL-based approaches in terms of makespan performance, even on large-scale\ninstances that have not been experienced during training.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.LG",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.08979v1",
    "published_date": "2024-02-14 06:49:23 UTC",
    "updated_date": "2024-02-14 06:49:23 UTC"
  },
  {
    "arxiv_id": "2402.08975v1",
    "title": "Research and application of Transformer based anomaly detection model: A literature review",
    "authors": [
      "Mingrui Ma",
      "Lansheng Han",
      "Chunjie Zhou"
    ],
    "abstract": "Transformer, as one of the most advanced neural network models in Natural\nLanguage Processing (NLP), exhibits diverse applications in the field of\nanomaly detection. To inspire research on Transformer-based anomaly detection,\nthis review offers a fresh perspective on the concept of anomaly detection. We\nexplore the current challenges of anomaly detection and provide detailed\ninsights into the operating principles of Transformer and its variants in\nanomaly detection tasks. Additionally, we delineate various application\nscenarios for Transformer-based anomaly detection models and discuss the\ndatasets and evaluation metrics employed. Furthermore, this review highlights\nthe key challenges in Transformer-based anomaly detection research and conducts\na comprehensive analysis of future research trends in this domain. The review\nincludes an extensive compilation of over 100 core references related to\nTransformer-based anomaly detection. To the best of our knowledge, this is the\nfirst comprehensive review that focuses on the research related to Transformer\nin the context of anomaly detection. We hope that this paper can provide\ndetailed technical information to researchers interested in Transformer-based\nanomaly detection tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "77 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.08975v1",
    "published_date": "2024-02-14 06:39:54 UTC",
    "updated_date": "2024-02-14 06:39:54 UTC"
  },
  {
    "arxiv_id": "2402.08968v1",
    "title": "GrounDial: Human-norm Grounded Safe Dialog Response Generation",
    "authors": [
      "Siwon Kim",
      "Shuyang Dai",
      "Mohammad Kachuee",
      "Shayan Ray",
      "Tara Taghavi",
      "Sungroh Yoon"
    ],
    "abstract": "Current conversational AI systems based on large language models (LLMs) are\nknown to generate unsafe responses, agreeing to offensive user input or\nincluding toxic content. Previous research aimed to alleviate the toxicity, by\nfine-tuning LLM with manually annotated safe dialogue histories. However, the\ndependency on additional tuning requires substantial costs. To remove the\ndependency, we propose GrounDial, where response safety is achieved by\ngrounding responses to commonsense social rules without requiring fine-tuning.\nA hybrid approach of in-context learning and human-norm-guided decoding of\nGrounDial enables the response to be quantitatively and qualitatively safer\neven without additional data or tuning.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to findings of EACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.08968v1",
    "published_date": "2024-02-14 06:25:50 UTC",
    "updated_date": "2024-02-14 06:25:50 UTC"
  },
  {
    "arxiv_id": "2402.08963v1",
    "title": "DUEL: Duplicate Elimination on Active Memory for Self-Supervised Class-Imbalanced Learning",
    "authors": [
      "Won-Seok Choi",
      "Hyundo Lee",
      "Dong-Sig Han",
      "Junseok Park",
      "Heeyeon Koo",
      "Byoung-Tak Zhang"
    ],
    "abstract": "Recent machine learning algorithms have been developed using well-curated\ndatasets, which often require substantial cost and resources. On the other\nhand, the direct use of raw data often leads to overfitting towards frequently\noccurring class information. To address class imbalances cost-efficiently, we\npropose an active data filtering process during self-supervised pre-training in\nour novel framework, Duplicate Elimination (DUEL). This framework integrates an\nactive memory inspired by human working memory and introduces distinctiveness\ninformation, which measures the diversity of the data in the memory, to\noptimize both the feature extractor and the memory. The DUEL policy, which\nreplaces the most duplicated data with new samples, aims to enhance the\ndistinctiveness information in the memory and thereby mitigate class\nimbalances. We validate the effectiveness of the DUEL framework in\nclass-imbalanced environments, demonstrating its robustness and providing\nreliable results in downstream tasks. We also analyze the role of the DUEL\npolicy in the training process through various metrics and visualizations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted as a full paper at AAAI 2024: The 38th Annual AAAI\n  Conference on Artificial Intelligence (Main Tech Track). 7 pages (main\n  paper), 2 pages (references), 11 pages (appendix) each",
    "pdf_url": "http://arxiv.org/pdf/2402.08963v1",
    "published_date": "2024-02-14 06:09:36 UTC",
    "updated_date": "2024-02-14 06:09:36 UTC"
  },
  {
    "arxiv_id": "2402.08961v3",
    "title": "HyCubE: Efficient Knowledge Hypergraph 3D Circular Convolutional Embedding",
    "authors": [
      "Zhao Li",
      "Xin Wang",
      "Jun Zhao",
      "Wenbin Guo",
      "Jianxin Li"
    ],
    "abstract": "Knowledge hypergraph embedding models are usually computationally expensive\ndue to the inherent complex semantic information. However, existing works\nmainly focus on improving the effectiveness of knowledge hypergraph embedding,\nmaking the model architecture more complex and redundant. It is desirable and\nchallenging for knowledge hypergraph embedding to reach a trade-off between\nmodel effectiveness and efficiency. In this paper, we propose an end-to-end\nefficient knowledge hypergraph embedding model, HyCubE, which designs a novel\n3D circular convolutional neural network and the alternate mask stack strategy\nto enhance the interaction and extraction of feature information\ncomprehensively. Furthermore, our proposed model achieves a better trade-off\nbetween effectiveness and efficiency by adaptively adjusting the 3D circular\nconvolutional layer structure to handle n-ary knowledge tuples of different\narities with fewer parameters. In addition, we use a knowledge hypergraph 1-N\nmultilinear scoring way to accelerate the model training efficiency further.\nFinally, extensive experimental results on all datasets demonstrate that our\nproposed model consistently outperforms state-of-the-art baselines, with an\naverage improvement of 8.22% and a maximum improvement of 33.82% across all\nmetrics. Meanwhile, HyCubE is 6.12x faster, GPU memory usage is 52.67% lower,\nand the number of parameters is reduced by 85.21% compared with the average\nmetric of the latest state-of-the-art baselines.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "13 pages",
    "pdf_url": "http://arxiv.org/pdf/2402.08961v3",
    "published_date": "2024-02-14 06:05:37 UTC",
    "updated_date": "2024-11-04 09:13:45 UTC"
  },
  {
    "arxiv_id": "2402.08960v2",
    "title": "Open-Vocabulary Segmentation with Unpaired Mask-Text Supervision",
    "authors": [
      "Zhaoqing Wang",
      "Xiaobo Xia",
      "Ziye Chen",
      "Xiao He",
      "Yandong Guo",
      "Mingming Gong",
      "Tongliang Liu"
    ],
    "abstract": "Current state-of-the-art open-vocabulary segmentation methods typically rely\non image-mask-text triplet annotations for supervision. However, acquiring such\ndetailed annotations is labour-intensive and poses scalability challenges in\ncomplex real-world scenarios. While existing weakly-supervised approaches\nleverage image-text pairs to reduce the expansive annotation cost, the lack of\nmask supervision makes it difficult for the model to locate multiple instances\nand accurately group pixels with similar semantics, significantly hampering\nversatility and performance. In this paper, we introduce Unpair-Seg, a novel\nweakly-supervised open-vocabulary segmentation framework that learns from\nunpaired image-mask and image-text pairs, which can be independently and\nefficiently collected. Unpair-Seg initially predicts a set of binary masks and\ngenerates pseudo labels by identifying confident pairs of masks and text\nentities. We then train a feature adapter to align region embeddings with text\nembeddings based on these pseudo labels, achieving open-vocabulary\nsegmentation. However, the inherent noise in the mask-entity correspondence\nposes a challenge to obtaining reliable pairs. To address this, we employ a\nvision-language large model to re-caption the input images and extract precise\nentities, and we design a multi-scale matching strategy to reduce noisy\nmask-entity pairs. Our Unpair-Seg framework demonstrates impressive\nperformance, achieving 14.6\\% and 19.5\\% mIoU on the ADE-847 and PASCAL\nContext-459 datasets, significantly narrowing the gap between fully-supervised\nand weakly-supervised methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "27 pages, 18 figures, 10 tables",
    "pdf_url": "http://arxiv.org/pdf/2402.08960v2",
    "published_date": "2024-02-14 06:01:44 UTC",
    "updated_date": "2024-06-11 17:01:02 UTC"
  },
  {
    "arxiv_id": "2402.08958v3",
    "title": "Towards Next-Level Post-Training Quantization of Hyper-Scale Transformers",
    "authors": [
      "Junhan Kim",
      "Chungman Lee",
      "Eulrang Cho",
      "Kyungphil Park",
      "Ho-young Kim",
      "Joonyoung Kim",
      "Yongkweon Jeon"
    ],
    "abstract": "With the increasing complexity of generative AI models, post-training\nquantization (PTQ) has emerged as a promising solution for deploying\nhyper-scale models on edge devices such as mobile and TVs. Existing PTQ\nschemes, however, consume considerable time and resources, which could be a\nbottleneck in real situations where frequent model updates and multiple\nhyperparameter tunings are required. As a cost-effective alternative,\nlearning-free PTQ schemes have been proposed. However, the performance is\nsomewhat limited because they cannot consider the inter-layer dependency within\nthe attention module, which is a significant feature of Transformers. In this\npaper, we thus propose a novel PTQ algorithm that balances accuracy and\nefficiency. The key idea of the proposed algorithm called aespa is to perform\nquantization layer-wise for efficiency while targeting attention-wise\nreconstruction to consider the cross-layer dependency. Through extensive\nexperiments on various language models and complexity analysis, we demonstrate\nthat aespa is accurate and efficient in quantizing Transformer models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.08958v3",
    "published_date": "2024-02-14 05:58:43 UTC",
    "updated_date": "2024-11-05 08:04:28 UTC"
  },
  {
    "arxiv_id": "2402.08957v3",
    "title": "MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data",
    "authors": [
      "Yinya Huang",
      "Xiaohan Lin",
      "Zhengying Liu",
      "Qingxing Cao",
      "Huajian Xin",
      "Haiming Wang",
      "Zhenguo Li",
      "Linqi Song",
      "Xiaodan Liang"
    ],
    "abstract": "Recent large language models (LLMs) have witnessed significant advancement in\nvarious tasks, including mathematical reasoning and theorem proving. As these\ntwo tasks require strict and formal multi-step inference, they are appealing\ndomains for exploring the reasoning ability of LLMs but still face important\nchallenges. Previous studies such as Chain-of-Thought (CoT) have revealed the\neffectiveness of intermediate steps guidance. However, such step-wise\nannotation requires heavy labor, leading to insufficient training steps for\ncurrent benchmarks. To fill this gap, this work introduces MUSTARD, a data\ngeneration framework that masters uniform synthesis of theorem and proof data\nof high quality and diversity. MUSTARD synthesizes data in three stages: (1) It\nsamples a few mathematical concept seeds as the problem category. (2) Then, it\nprompts a generative language model with the sampled concepts to obtain both\nthe problems and their step-wise formal solutions. (3) Lastly, the framework\nutilizes a proof assistant (e.g., Lean Prover) to filter the valid proofs. With\nthe proposed MUSTARD, we present a theorem-and-proof benchmark MUSTARDSAUCE\nwith 5,866 valid data points. Each data point contains an informal statement,\nan informal proof, and a translated formal proof that passes the prover\nvalidation. We perform extensive analysis and demonstrate that MUSTARD\ngenerates validated high-quality step-by-step data. We further apply the\nMUSTARDSAUCE for fine-tuning smaller language models. The fine-tuned Llama 2-7B\nachieves a 15.41% average relative performance gain in automated theorem\nproving, and 8.18% in math word problems. Codes and data are available at\nhttps://github.com/Eleanor-H/MUSTARD.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.FL",
      "cs.LG",
      "cs.PL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.08957v3",
    "published_date": "2024-02-14 05:57:58 UTC",
    "updated_date": "2024-05-23 03:13:23 UTC"
  },
  {
    "arxiv_id": "2402.08955v1",
    "title": "Using Counterfactual Tasks to Evaluate the Generality of Analogical Reasoning in Large Language Models",
    "authors": [
      "Martha Lewis",
      "Melanie Mitchell"
    ],
    "abstract": "Large language models (LLMs) have performed well on several reasoning\nbenchmarks, including ones that test analogical reasoning abilities. However,\nit has been debated whether they are actually performing humanlike abstract\nreasoning or instead employing less general processes that rely on similarity\nto what has been seen in their training data. Here we investigate the\ngenerality of analogy-making abilities previously claimed for LLMs (Webb,\nHolyoak, & Lu, 2023). We take one set of analogy problems used to evaluate LLMs\nand create a set of \"counterfactual\" variants-versions that test the same\nabstract reasoning abilities but that are likely dissimilar from any\npre-training data. We test humans and three GPT models on both the original and\ncounterfactual problems, and show that, while the performance of humans remains\nhigh for all the problems, the GPT models' performance declines sharply on the\ncounterfactual set. This work provides evidence that, despite previously\nreported successes of LLMs on analogical reasoning, these models lack the\nrobustness and generality of human analogy-making.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.08955v1",
    "published_date": "2024-02-14 05:52:23 UTC",
    "updated_date": "2024-02-14 05:52:23 UTC"
  },
  {
    "arxiv_id": "2402.08939v3",
    "title": "Premise Order Matters in Reasoning with Large Language Models",
    "authors": [
      "Xinyun Chen",
      "Ryan A. Chi",
      "Xuezhi Wang",
      "Denny Zhou"
    ],
    "abstract": "Large language models (LLMs) have accomplished remarkable reasoning\nperformance in various domains. However, in the domain of reasoning tasks, we\ndiscover a frailty: LLMs are surprisingly brittle to the ordering of the\npremises, despite the fact that such ordering does not alter the underlying\ntask. In particular, we observe that LLMs achieve the best performance when the\npremise order aligns with the context required in intermediate reasoning steps.\nFor example, in deductive reasoning tasks, presenting the premises in the same\norder as the ground truth proof in the prompt (as opposed to random ordering)\ndrastically increases the model's accuracy. We first examine the effect of\npremise ordering on deductive reasoning on a variety of LLMs, and our\nevaluation shows that permuting the premise order can cause a performance drop\nof over 30%. In addition, we release the benchmark R-GSM, based on GSM8K, to\nexamine the ordering effect for mathematical problem-solving, and we again\nobserve a significant drop in accuracy, relative to the original GSM8K\nbenchmark.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Published at ICML 2024. Xinyun and Ryan contribute equally",
    "pdf_url": "http://arxiv.org/pdf/2402.08939v3",
    "published_date": "2024-02-14 04:50:18 UTC",
    "updated_date": "2024-05-28 04:32:09 UTC"
  },
  {
    "arxiv_id": "2402.08925v2",
    "title": "MaxMin-RLHF: Alignment with Diverse Human Preferences",
    "authors": [
      "Souradip Chakraborty",
      "Jiahao Qiu",
      "Hui Yuan",
      "Alec Koppel",
      "Furong Huang",
      "Dinesh Manocha",
      "Amrit Singh Bedi",
      "Mengdi Wang"
    ],
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) aligns language models to\nhuman preferences by employing a singular reward model derived from preference\ndata. However, such an approach overlooks the rich diversity of human\npreferences inherent in data collected from multiple users. In this work, we\nfirst derive an impossibility result of alignment with single reward RLHF,\nthereby highlighting its insufficiency in representing diverse human\npreferences. To provide an equitable solution to the problem, we learn a\nmixture of preference distributions via an expectation-maximization algorithm\nand propose a MaxMin alignment objective for policy learning inspired by the\nEgalitarian principle in social choice theory to better represent diverse human\npreferences. We elucidate the connection of our proposed approach to\ndistributionally robust optimization and general utility RL, thereby\nhighlighting the generality and robustness of our proposed solution. We present\ncomprehensive experimental results on small-scale (GPT-2) and large-scale\nlanguage models (with Tulu2-7B) and show the efficacy of the proposed approach\nin the presence of diversity among human preferences. Our algorithm achieves an\naverage improvement of more than 16% in win-rates over conventional RLHF\nalgorithms and improves the win-rate (accuracy) for minority groups by over 33%\nwithout compromising the performance of majority groups, showcasing the\nrobustness and fairness of our approach. We remark that our findings in this\nwork are not only limited to language models but also extend to reinforcement\nlearning in general.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.08925v2",
    "published_date": "2024-02-14 03:56:27 UTC",
    "updated_date": "2024-12-26 00:15:20 UTC"
  },
  {
    "arxiv_id": "2402.08921v1",
    "title": "Enhancing ID and Text Fusion via Alternative Training in Session-based Recommendation",
    "authors": [
      "Juanhui Li",
      "Haoyu Han",
      "Zhikai Chen",
      "Harry Shomer",
      "Wei Jin",
      "Amin Javari",
      "Jiliang Tang"
    ],
    "abstract": "Session-based recommendation has gained increasing attention in recent years,\nwith its aim to offer tailored suggestions based on users' historical behaviors\nwithin sessions.\n  To advance this field, a variety of methods have been developed, with\nID-based approaches typically demonstrating promising performance. However,\nthese methods often face challenges with long-tail items and overlook other\nrich forms of information, notably valuable textual semantic information. To\nintegrate text information, various methods have been introduced, mostly\nfollowing a naive fusion framework. Surprisingly, we observe that fusing these\ntwo modalities does not consistently outperform the best single modality by\nfollowing the naive fusion framework. Further investigation reveals an\npotential imbalance issue in naive fusion, where the ID dominates and text\nmodality is undertrained. This suggests that the unexpected observation may\nstem from naive fusion's failure to effectively balance the two modalities,\noften over-relying on the stronger ID modality. This insight suggests that\nnaive fusion might not be as effective in combining ID and text as previously\nexpected. To address this, we propose a novel alternative training strategy\nAlterRec. It separates the training of ID and text, thereby avoiding the\nimbalance issue seen in naive fusion. Additionally, AlterRec designs a novel\nstrategy to facilitate the interaction between the two modalities, enabling\nthem to mutually learn from each other and integrate the text more effectively.\nComprehensive experiments demonstrate the effectiveness of AlterRec in\nsession-based recommendation. The implementation is available at\nhttps://github.com/Juanhui28/AlterRec.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.08921v1",
    "published_date": "2024-02-14 03:41:50 UTC",
    "updated_date": "2024-02-14 03:41:50 UTC"
  },
  {
    "arxiv_id": "2402.08918v3",
    "title": "SimMLP: Training MLPs on Graphs without Supervision",
    "authors": [
      "Zehong Wang",
      "Zheyuan Zhang",
      "Chuxu Zhang",
      "Yanfang Ye"
    ],
    "abstract": "Graph Neural Networks (GNNs) have demonstrated their effectiveness in various\ngraph learning tasks, yet their reliance on neighborhood aggregation during\ninference poses challenges for deployment in latency-sensitive applications,\nsuch as real-time financial fraud detection. To address this limitation, recent\nstudies have proposed distilling knowledge from teacher GNNs into student\nMulti-Layer Perceptrons (MLPs) trained on node content, aiming to accelerate\ninference. However, these approaches often inadequately explore structural\ninformation when inferring unseen nodes. To this end, we introduce SimMLP, a\nSelf-supervised framework for learning MLPs on graphs, designed to fully\nintegrate rich structural information into MLPs. Notably, SimMLP is the first\nMLP-learning method that can achieve equivalence to GNNs in the optimal case.\nThe key idea is to employ self-supervised learning to align the representations\nencoded by graph context-aware GNNs and neighborhood dependency-free MLPs,\nthereby fully integrating the structural information into MLPs. We provide a\ncomprehensive theoretical analysis, demonstrating the equivalence between\nSimMLP and GNNs based on mutual information and inductive bias, highlighting\nSimMLP's advanced structural learning capabilities. Additionally, we conduct\nextensive experiments on 20 benchmark datasets, covering node classification,\nlink prediction, and graph classification, to showcase SimMLP's superiority\nover state-of-the-art baselines, particularly in scenarios involving unseen\nnodes (e.g., inductive and cold-start node classification) where structural\ninsights are crucial. Our codes are available at:\nhttps://github.com/Zehong-Wang/SimMLP.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "New Version: arXiv:2412.03864",
    "pdf_url": "http://arxiv.org/pdf/2402.08918v3",
    "published_date": "2024-02-14 03:16:13 UTC",
    "updated_date": "2024-12-06 02:46:38 UTC"
  },
  {
    "arxiv_id": "2402.08907v2",
    "title": "Subgraph Pooling: Tackling Negative Transfer on Graphs",
    "authors": [
      "Zehong Wang",
      "Zheyuan Zhang",
      "Chuxu Zhang",
      "Yanfang Ye"
    ],
    "abstract": "Transfer learning aims to enhance performance on a target task by using\nknowledge from related tasks. However, when the source and target tasks are not\nclosely aligned, it can lead to reduced performance, known as negative\ntransfer. Unlike in image or text data, we find that negative transfer could\ncommonly occur in graph-structured data, even when source and target graphs\nhave semantic similarities. Specifically, we identify that structural\ndifferences significantly amplify the dissimilarities in the node embeddings\nacross graphs. To mitigate this, we bring a new insight in this paper: for\nsemantically similar graphs, although structural differences lead to\nsignificant distribution shift in node embeddings, their impact on subgraph\nembeddings could be marginal. Building on this insight, we introduce Subgraph\nPooling (SP) by aggregating nodes sampled from a k-hop neighborhood and\nSubgraph Pooling++ (SP++) by a random walk, to mitigate the impact of graph\nstructural differences on knowledge transfer. We theoretically analyze the role\nof SP in reducing graph discrepancy and conduct extensive experiments to\nevaluate its superiority under various settings. The proposed SP methods are\neffective yet elegant, which can be easily applied on top of any backbone Graph\nNeural Networks (GNNs). Our code and data are available at:\nhttps://github.com/Zehong-Wang/Subgraph-Pooling.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by IJCAI 24",
    "pdf_url": "http://arxiv.org/pdf/2402.08907v2",
    "published_date": "2024-02-14 02:46:47 UTC",
    "updated_date": "2024-05-04 19:41:52 UTC"
  },
  {
    "arxiv_id": "2402.08876v2",
    "title": "DUDF: Differentiable Unsigned Distance Fields with Hyperbolic Scaling",
    "authors": [
      "Miguel Fainstein",
      "Viviana Siless",
      "Emmanuel Iarussi"
    ],
    "abstract": "In recent years, there has been a growing interest in training Neural\nNetworks to approximate Unsigned Distance Fields (UDFs) for representing open\nsurfaces in the context of 3D reconstruction. However, UDFs are\nnon-differentiable at the zero level set which leads to significant errors in\ndistances and gradients, generally resulting in fragmented and discontinuous\nsurfaces. In this paper, we propose to learn a hyperbolic scaling of the\nunsigned distance field, which defines a new Eikonal problem with distinct\nboundary conditions. This allows our formulation to integrate seamlessly with\nstate-of-the-art continuously differentiable implicit neural representation\nnetworks, largely applied in the literature to represent signed distance\nfields. Our approach not only addresses the challenge of open surface\nrepresentation but also demonstrates significant improvement in reconstruction\nquality and training performance. Moreover, the unlocked field's\ndifferentiability allows the accurate computation of essential topological\nproperties such as normal directions and curvatures, pervasive in downstream\ntasks such as rendering. Through extensive experiments, we validate our\napproach across various data sets and against competitive baselines. The\nresults demonstrate enhanced accuracy and up to an order of magnitude increase\nin speed compared to previous methods.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "I.2.10; I.4.10; I.3.7"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.08876v2",
    "published_date": "2024-02-14 00:42:19 UTC",
    "updated_date": "2024-06-02 14:50:33 UTC"
  },
  {
    "arxiv_id": "2402.08869v1",
    "title": "ScamSpot: Fighting Financial Fraud in Instagram Comments",
    "authors": [
      "Stefan Erben",
      "Andreas Waldis"
    ],
    "abstract": "The long-standing problem of spam and fraudulent messages in the comment\nsections of Instagram pages in the financial sector claims new victims every\nday. Instagram's current spam filter proves inadequate, and existing research\napproaches are primarily confined to theoretical concepts. Practical\nimplementations with evaluated results are missing. To solve this problem, we\npropose ScamSpot, a comprehensive system that includes a browser extension, a\nfine-tuned BERT model and a REST API. This approach ensures public\naccessibility of our results for Instagram users using the Chrome browser.\nFurthermore, we conduct a data annotation study, shedding light on the reasons\nand causes of the problem and evaluate the system through user feedback and\ncomparison with existing models. ScamSpot is an open-source project and is\npublicly available at https://scamspot.github.io/.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "EACL 2024 Demo Paper, 11 pages",
    "pdf_url": "http://arxiv.org/pdf/2402.08869v1",
    "published_date": "2024-02-14 00:30:18 UTC",
    "updated_date": "2024-02-14 00:30:18 UTC"
  },
  {
    "arxiv_id": "2402.08859v1",
    "title": "Large Language Model with Graph Convolution for Recommendation",
    "authors": [
      "Yingpeng Du",
      "Ziyan Wang",
      "Zhu Sun",
      "Haoyan Chua",
      "Hongzhi Liu",
      "Zhonghai Wu",
      "Yining Ma",
      "Jie Zhang",
      "Youchen Sun"
    ],
    "abstract": "In recent years, efforts have been made to use text information for better\nuser profiling and item characterization in recommendations. However, text\ninformation can sometimes be of low quality, hindering its effectiveness for\nreal-world applications. With knowledge and reasoning capabilities capsuled in\nLarge Language Models (LLMs), utilizing LLMs emerges as a promising way for\ndescription improvement. However, existing ways of prompting LLMs with raw\ntexts ignore structured knowledge of user-item interactions, which may lead to\nhallucination problems like inconsistent description generation. To this end,\nwe propose a Graph-aware Convolutional LLM method to elicit LLMs to capture\nhigh-order relations in the user-item graph. To adapt text-based LLMs with\nstructured graphs, We use the LLM as an aggregator in graph processing,\nallowing it to understand graph-based information step by step. Specifically,\nthe LLM is required for description enhancement by exploring multi-hop\nneighbors layer by layer, thereby propagating information progressively in the\ngraph. To enable LLMs to capture large-scale graph information, we break down\nthe description task into smaller parts, which drastically reduces the context\nlength of the token input with each step. Extensive experiments on three\nreal-world datasets show that our method consistently outperforms\nstate-of-the-art methods.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.08859v1",
    "published_date": "2024-02-14 00:04:33 UTC",
    "updated_date": "2024-02-14 00:04:33 UTC"
  }
]
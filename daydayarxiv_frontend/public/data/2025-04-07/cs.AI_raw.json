[
  {
    "arxiv_id": "2504.05305v1",
    "title": "URECA: Unique Region Caption Anything",
    "authors": [
      "Sangbeom Lim",
      "Junwan Kim",
      "Heeji Yoon",
      "Jaewoo Jung",
      "Seungryong Kim"
    ],
    "abstract": "Region-level captioning aims to generate natural language descriptions for\nspecific image regions while highlighting their distinguishing features.\nHowever, existing methods struggle to produce unique captions across\nmulti-granularity, limiting their real-world applicability. To address the need\nfor detailed region-level understanding, we introduce URECA dataset, a\nlarge-scale dataset tailored for multi-granularity region captioning. Unlike\nprior datasets that focus primarily on salient objects, URECA dataset ensures a\nunique and consistent mapping between regions and captions by incorporating a\ndiverse set of objects, parts, and background elements. Central to this is a\nstage-wise data curation pipeline, where each stage incrementally refines\nregion selection and caption generation. By leveraging Multimodal Large\nLanguage Models (MLLMs) at each stage, our pipeline produces distinctive and\ncontextually grounded captions with improved accuracy and semantic diversity.\nBuilding upon this dataset, we present URECA, a novel captioning model designed\nto effectively encode multi-granularity regions. URECA maintains essential\nspatial properties such as position and shape through simple yet impactful\nmodifications to existing MLLMs, enabling fine-grained and semantically rich\nregion descriptions. Our approach introduces dynamic mask modeling and a\nhigh-resolution mask encoder to enhance caption uniqueness. Experiments show\nthat URECA achieves state-of-the-art performance on URECA dataset and\ngeneralizes well to existing region-level captioning benchmarks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://cvlab-kaist.github.io/URECA Code:\n  https://github.com/cvlab-kaist/URECA",
    "pdf_url": "http://arxiv.org/pdf/2504.05305v1",
    "published_date": "2025-04-07 17:59:44 UTC",
    "updated_date": "2025-04-07 17:59:44 UTC"
  },
  {
    "arxiv_id": "2504.05299v1",
    "title": "SmolVLM: Redefining small and efficient multimodal models",
    "authors": [
      "Andrés Marafioti",
      "Orr Zohar",
      "Miquel Farré",
      "Merve Noyan",
      "Elie Bakouch",
      "Pedro Cuenca",
      "Cyril Zakka",
      "Loubna Ben Allal",
      "Anton Lozhkov",
      "Nouamane Tazi",
      "Vaibhav Srivastav",
      "Joshua Lochner",
      "Hugo Larcher",
      "Mathieu Morlon",
      "Lewis Tunstall",
      "Leandro von Werra",
      "Thomas Wolf"
    ],
    "abstract": "Large Vision-Language Models (VLMs) deliver exceptional performance but\nrequire significant computational resources, limiting their deployment on\nmobile and edge devices. Smaller VLMs typically mirror design choices of larger\nmodels, such as extensive image tokenization, leading to inefficient GPU memory\nusage and constrained practicality for on-device applications.\n  We introduce SmolVLM, a series of compact multimodal models specifically\nengineered for resource-efficient inference. We systematically explore\narchitectural configurations, tokenization strategies, and data curation\noptimized for low computational overhead. Through this, we identify key design\nchoices that yield substantial performance gains on image and video tasks with\nminimal memory footprints.\n  Our smallest model, SmolVLM-256M, uses less than 1GB GPU memory during\ninference and outperforms the 300-times larger Idefics-80B model, despite an\n18-month development gap. Our largest model, at 2.2B parameters, rivals\nstate-of-the-art VLMs consuming twice the GPU memory. SmolVLM models extend\nbeyond static images, demonstrating robust video comprehension capabilities.\n  Our results emphasize that strategic architectural optimizations, aggressive\nyet efficient tokenization, and carefully curated training data significantly\nenhance multimodal performance, facilitating practical, energy-efficient\ndeployments at significantly smaller scales.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.05299v1",
    "published_date": "2025-04-07 17:58:57 UTC",
    "updated_date": "2025-04-07 17:58:57 UTC"
  },
  {
    "arxiv_id": "2504.05295v1",
    "title": "Dion: A Communication-Efficient Optimizer for Large Models",
    "authors": [
      "Kwangjun Ahn",
      "Byron Xu"
    ],
    "abstract": "Training large AI models efficiently requires distributing computation across\nmultiple accelerators, but this often incurs significant communication overhead\n-- especially during gradient synchronization. We introduce Dion, a\ncommunication-efficient optimizer that retains the synchronous semantics of\nstandard distributed training (e.g., DDP, FSDP) while substantially reducing\nI/O costs. Unlike conventional optimizers that synchronize full gradient\nmatrices, Dion leverages orthonormalized updates with device-local momentum\nbuffers, eliminating the need for full gradient exchange. It further supports\nan efficient sharding strategy that avoids reconstructing large matrices during\ntraining.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "technical report; comments welcome!",
    "pdf_url": "http://arxiv.org/pdf/2504.05295v1",
    "published_date": "2025-04-07 17:49:37 UTC",
    "updated_date": "2025-04-07 17:49:37 UTC"
  },
  {
    "arxiv_id": "2504.05278v1",
    "title": "The challenge of uncertainty quantification of large language models in medicine",
    "authors": [
      "Zahra Atf",
      "Seyed Amir Ahmad Safavi-Naini",
      "Peter R. Lewis",
      "Aref Mahjoubfar",
      "Nariman Naderi",
      "Thomas R. Savage",
      "Ali Soroush"
    ],
    "abstract": "This study investigates uncertainty quantification in large language models\n(LLMs) for medical applications, emphasizing both technical innovations and\nphilosophical implications. As LLMs become integral to clinical\ndecision-making, accurately communicating uncertainty is crucial for ensuring\nreliable, safe, and ethical AI-assisted healthcare. Our research frames\nuncertainty not as a barrier but as an essential part of knowledge that invites\na dynamic and reflective approach to AI design. By integrating advanced\nprobabilistic methods such as Bayesian inference, deep ensembles, and Monte\nCarlo dropout with linguistic analysis that computes predictive and semantic\nentropy, we propose a comprehensive framework that manages both epistemic and\naleatoric uncertainties. The framework incorporates surrogate modeling to\naddress limitations of proprietary APIs, multi-source data integration for\nbetter context, and dynamic calibration via continual and meta-learning.\nExplainability is embedded through uncertainty maps and confidence metrics to\nsupport user trust and clinical interpretability. Our approach supports\ntransparent and ethical decision-making aligned with Responsible and Reflective\nAI principles. Philosophically, we advocate accepting controlled ambiguity\ninstead of striving for absolute predictability, recognizing the inherent\nprovisionality of medical knowledge.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "25 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.05278v1",
    "published_date": "2025-04-07 17:24:11 UTC",
    "updated_date": "2025-04-07 17:24:11 UTC"
  },
  {
    "arxiv_id": "2504.05259v1",
    "title": "How to evaluate control measures for LLM agents? A trajectory from today to superintelligence",
    "authors": [
      "Tomek Korbak",
      "Mikita Balesni",
      "Buck Shlegeris",
      "Geoffrey Irving"
    ],
    "abstract": "As LLM agents grow more capable of causing harm autonomously, AI developers\nwill rely on increasingly sophisticated control measures to prevent possibly\nmisaligned agents from causing harm. AI developers could demonstrate that their\ncontrol measures are sufficient by running control evaluations: testing\nexercises in which a red team produces agents that try to subvert control\nmeasures. To ensure control evaluations accurately capture misalignment risks,\nthe affordances granted to this red team should be adapted to the capability\nprofiles of the agents to be deployed under control measures.\n  In this paper we propose a systematic framework for adapting affordances of\nred teams to advancing AI capabilities. Rather than assuming that agents will\nalways execute the best attack strategies known to humans, we demonstrate how\nknowledge of an agents's actual capability profile can inform proportional\ncontrol evaluations, resulting in more practical and cost-effective control\nmeasures. We illustrate our framework by considering a sequence of five\nfictional models (M1-M5) with progressively advanced capabilities, defining\nfive distinct AI control levels (ACLs). For each ACL, we provide example rules\nfor control evaluation, control measures, and safety cases that could be\nappropriate. Finally, we show why constructing a compelling AI control safety\ncase for superintelligent LLM agents will require research breakthroughs,\nhighlighting that we might eventually need alternative approaches to mitigating\nmisalignment risk.",
    "categories": [
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.05259v1",
    "published_date": "2025-04-07 16:52:52 UTC",
    "updated_date": "2025-04-07 16:52:52 UTC"
  },
  {
    "arxiv_id": "2504.05258v1",
    "title": "Learning to Reason Over Time: Timeline Self-Reflection for Improved Temporal Reasoning in Language Models",
    "authors": [
      "Adrián Bazaga",
      "Rexhina Blloshmi",
      "Bill Byrne",
      "Adrià de Gispert"
    ],
    "abstract": "Large Language Models (LLMs) have emerged as powerful tools for generating\ncoherent text, understanding context, and performing reasoning tasks. However,\nthey struggle with temporal reasoning, which requires processing time-related\ninformation such as event sequencing, durations, and inter-temporal\nrelationships. These capabilities are critical for applications including\nquestion answering, scheduling, and historical analysis. In this paper, we\nintroduce TISER, a novel framework that enhances the temporal reasoning\nabilities of LLMs through a multi-stage process that combines timeline\nconstruction with iterative self-reflection. Our approach leverages test-time\nscaling to extend the length of reasoning traces, enabling models to capture\ncomplex temporal dependencies more effectively. This strategy not only boosts\nreasoning accuracy but also improves the traceability of the inference process.\nExperimental results demonstrate state-of-the-art performance across multiple\nbenchmarks, including out-of-distribution test sets, and reveal that TISER\nenables smaller open-source models to surpass larger closed-weight models on\nchallenging temporal reasoning tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.05258v1",
    "published_date": "2025-04-07 16:51:45 UTC",
    "updated_date": "2025-04-07 16:51:45 UTC"
  },
  {
    "arxiv_id": "2504.05254v1",
    "title": "Explaining Low Perception Model Competency with High-Competency Counterfactuals",
    "authors": [
      "Sara Pohland",
      "Claire Tomlin"
    ],
    "abstract": "There exist many methods to explain how an image classification model\ngenerates its decision, but very little work has explored methods to explain\nwhy a classifier might lack confidence in its prediction. As there are various\nreasons the classifier might lose confidence, it would be valuable for this\nmodel to not only indicate its level of uncertainty but also explain why it is\nuncertain. Counterfactual images have been used to visualize changes that could\nbe made to an image to generate a different classification decision. In this\nwork, we explore the use of counterfactuals to offer an explanation for low\nmodel competency--a generalized form of predictive uncertainty that measures\nconfidence. Toward this end, we develop five novel methods to generate\nhigh-competency counterfactual images, namely Image Gradient Descent (IGD),\nFeature Gradient Descent (FGD), Autoencoder Reconstruction (Reco), Latent\nGradient Descent (LGD), and Latent Nearest Neighbors (LNN). We evaluate these\nmethods across two unique datasets containing images with six known causes for\nlow model competency and find Reco, LGD, and LNN to be the most promising\nmethods for counterfactual generation. We further evaluate how these three\nmethods can be utilized by pre-trained Multimodal Large Language Models (MLLMs)\nto generate language explanations for low model competency. We find that the\ninclusion of a counterfactual image in the language model query greatly\nincreases the ability of the model to generate an accurate explanation for the\ncause of low model competency, thus demonstrating the utility of counterfactual\nimages in explaining low perception model competency.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.05254v1",
    "published_date": "2025-04-07 16:46:52 UTC",
    "updated_date": "2025-04-07 16:46:52 UTC"
  },
  {
    "arxiv_id": "2504.05255v1",
    "title": "Adversarial KA",
    "authors": [
      "Sviatoslav Dzhenzher",
      "Michael H. Freedman"
    ],
    "abstract": "Regarding the representation theorem of Kolmogorov and Arnold (KA) as an\nalgorithm for representing or {\\guillemotleft}expressing{\\guillemotright}\nfunctions, we test its robustness by analyzing its ability to withstand\nadversarial attacks. We find KA to be robust to countable collections of\ncontinuous adversaries, but unearth a question about the equi-continuity of the\nouter functions that, so far, obstructs taking limits and defeating continuous\ngroups of adversaries. This question on the regularity of the outer functions\nis relevant to the debate over the applicability of KA to the general theory of\nNNs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.FA"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.05255v1",
    "published_date": "2025-04-07 16:46:52 UTC",
    "updated_date": "2025-04-07 16:46:52 UTC"
  },
  {
    "arxiv_id": "2504.05248v1",
    "title": "PINNverse: Accurate parameter estimation in differential equations from noisy data with constrained physics-informed neural networks",
    "authors": [
      "Marius Almanstötter",
      "Roman Vetter",
      "Dagmar Iber"
    ],
    "abstract": "Parameter estimation for differential equations from measured data is an\ninverse problem prevalent across quantitative sciences. Physics-Informed Neural\nNetworks (PINNs) have emerged as effective tools for solving such problems,\nespecially with sparse measurements and incomplete system information. However,\nPINNs face convergence issues, stability problems, overfitting, and complex\nloss function design. Here we introduce PINNverse, a training paradigm that\naddresses these limitations by reformulating the learning process as a\nconstrained differential optimization problem. This approach achieves a dynamic\nbalance between data loss and differential equation residual loss during\ntraining while preventing overfitting. PINNverse combines the advantages of\nPINNs with the Modified Differential Method of Multipliers to enable\nconvergence on any point on the Pareto front. We demonstrate robust and\naccurate parameter estimation from noisy data in four classical ODE and PDE\nmodels from physics and biology. Our method enables accurate parameter\ninference also when the forward problem is expensive to solve.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.comp-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "17 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.05248v1",
    "published_date": "2025-04-07 16:34:57 UTC",
    "updated_date": "2025-04-07 16:34:57 UTC"
  },
  {
    "arxiv_id": "2504.05231v1",
    "title": "Mapping biodiversity at very-high resolution in Europe",
    "authors": [
      "César Leblanc",
      "Lukas Picek",
      "Benjamin Deneu",
      "Pierre Bonnet",
      "Maximilien Servajean",
      "Rémi Palard",
      "Alexis Joly"
    ],
    "abstract": "This paper describes a cascading multimodal pipeline for high-resolution\nbiodiversity mapping across Europe, integrating species distribution modeling,\nbiodiversity indicators, and habitat classification. The proposed pipeline\nfirst predicts species compositions using a deep-SDM, a multimodal model\ntrained on remote sensing, climate time series, and species occurrence data at\n50x50m resolution. These predictions are then used to generate biodiversity\nindicator maps and classify habitats with Pl@ntBERT, a transformer-based LLM\ndesigned for species-to-habitat mapping. With this approach, continental-scale\nspecies distribution maps, biodiversity indicator maps, and habitat maps are\nproduced, providing fine-grained ecological insights. Unlike traditional\nmethods, this framework enables joint modeling of interspecies dependencies,\nbias-aware training with heterogeneous presence-absence data, and large-scale\ninference from multi-source remote sensing inputs.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.05231v1",
    "published_date": "2025-04-07 16:15:52 UTC",
    "updated_date": "2025-04-07 16:15:52 UTC"
  },
  {
    "arxiv_id": "2504.05229v1",
    "title": "FinGrAct: A Framework for FINe-GRrained Evaluation of ACTionability in Explainable Automatic Fact-Checking",
    "authors": [
      "Islam Eldifrawi",
      "Shengrui Wang",
      "Amine Trabelsi"
    ],
    "abstract": "The field of explainable Automatic Fact-Checking (AFC) aims to enhance the\ntransparency and trustworthiness of automated fact-verification systems by\nproviding clear and comprehensible explanations. However, the effectiveness of\nthese explanations depends on their actionability --their ability to empower\nusers to make informed decisions and mitigate misinformation. Despite\nactionability being a critical property of high-quality explanations, no prior\nresearch has proposed a dedicated method to evaluate it. This paper introduces\nFinGrAct, a fine-grained evaluation framework that can access the web, and it\nis designed to assess actionability in AFC explanations through well-defined\ncriteria and an evaluation dataset. FinGrAct surpasses state-of-the-art (SOTA)\nevaluators, achieving the highest Pearson and Kendall correlation with human\njudgments while demonstrating the lowest ego-centric bias, making it a more\nrobust evaluation approach for actionability evaluation in AFC.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.05229v1",
    "published_date": "2025-04-07 16:14:27 UTC",
    "updated_date": "2025-04-07 16:14:27 UTC"
  },
  {
    "arxiv_id": "2504.05220v2",
    "title": "Leveraging LLMs for Utility-Focused Annotation: Reducing Manual Effort for Retrieval and RAG",
    "authors": [
      "Hengran Zhang",
      "Minghao Tang",
      "Keping Bi",
      "Jiafeng Guo",
      "Shihao Liu",
      "Daiting Shi",
      "Dawei Yin",
      "Xueqi Cheng"
    ],
    "abstract": "Retrieval models typically rely on costly human-labeled query-document\nrelevance annotations for training and evaluation. To reduce this cost and\nleverage the potential of Large Language Models (LLMs) in relevance judgments,\nwe aim to explore whether LLM-generated annotations can effectively replace\nhuman annotations in training retrieval models. Retrieval usually emphasizes\nrelevance, which indicates \"topic-relatedness\" of a document to a query, while\nin RAG, the value of a document (or utility) depends on how it contributes to\nanswer generation. Recognizing this mismatch, some researchers use LLM\nperformance on downstream tasks with documents as labels, but this approach\nrequires manual answers for specific tasks, leading to high costs and limited\ngeneralization. In another line of work, prompting LLMs to select useful\ndocuments as RAG references eliminates the need for human annotation and is not\ntask-specific. If we leverage LLMs' utility judgments to annotate retrieval\ndata, we may retain cross-task generalization without human annotation in\nlarge-scale corpora. Therefore, we investigate utility-focused annotation via\nLLMs for large-scale retriever training data across both in-domain and\nout-of-domain settings on the retrieval and RAG tasks. To reduce the impact of\nlow-quality positives labeled by LLMs, we design a novel loss function, i.e.,\nDisj-InfoNCE. Our experiments reveal that: (1) Retrievers trained on\nutility-focused annotations significantly outperform those trained on human\nannotations in the out-of-domain setting on both tasks, demonstrating superior\ngeneralization capabilities. (2) LLM annotation does not replace human\nannotation in the in-domain setting. However, incorporating just 20%\nhuman-annotated data enables retrievers trained with utility-focused\nannotations to match the performance of models trained entirely with human\nannotations.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "12 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.05220v2",
    "published_date": "2025-04-07 16:05:52 UTC",
    "updated_date": "2025-04-08 02:11:05 UTC"
  },
  {
    "arxiv_id": "2504.05216v1",
    "title": "Unleashing the Power of LLMs in Dense Retrieval with Query Likelihood Modeling",
    "authors": [
      "Hengran Zhang",
      "Keping Bi",
      "Jiafeng Guo",
      "Xiaojie Sun",
      "Shihao Liu",
      "Daiting Shi",
      "Dawei Yin",
      "Xueqi Cheng"
    ],
    "abstract": "Dense retrieval is a crucial task in Information Retrieval (IR) and is the\nfoundation for downstream tasks such as re-ranking. Recently, large language\nmodels (LLMs) have shown compelling semantic understanding capabilities and are\nappealing to researchers studying dense retrieval. LLMs, as decoder-style\ngenerative models, are competent at language generation while falling short on\nmodeling global information due to the lack of attention to tokens afterward.\nInspired by the classical word-based language modeling approach for IR, i.e.,\nthe query likelihood (QL) model, we seek to sufficiently utilize LLMs'\ngenerative ability by QL maximization. However, instead of ranking documents\nwith QL estimation, we introduce an auxiliary task of QL maximization to yield\na better backbone for contrastively learning a discriminative retriever. We\nname our model as LLM-QL. To condense global document semantics to a single\nvector during QL modeling, LLM-QL has two major components, Attention Stop (AS)\nand Input Corruption (IC). AS stops the attention of predictive tokens to\nprevious tokens until the ending token of the document. IC masks a portion of\ntokens in the input documents during prediction. Experiments on MSMARCO show\nthat LLM-QL can achieve significantly better performance than other LLM-based\nretrievers and using QL estimated by LLM-QL for ranking outperforms word-based\nQL by a large margin.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "12 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.05216v1",
    "published_date": "2025-04-07 16:03:59 UTC",
    "updated_date": "2025-04-07 16:03:59 UTC"
  },
  {
    "arxiv_id": "2504.05210v1",
    "title": "A moving target in AI-assisted decision-making: Dataset shift, model updating, and the problem of update opacity",
    "authors": [
      "Joshua Hatherley"
    ],
    "abstract": "Machine learning (ML) systems are vulnerable to performance decline over time\ndue to dataset shift. To address this problem, experts often suggest that ML\nsystems should be regularly updated to ensure ongoing performance stability.\nSome scholarly literature has begun to address the epistemic and ethical\nchallenges associated with different updating methodologies. Thus far, however,\nlittle attention has been paid to the impact of model updating on the\nML-assisted decision-making process itself, particularly in the AI ethics and\nAI epistemology literatures. This article aims to address this gap in the\nliterature. It argues that model updating introduces a new sub-type of opacity\ninto ML-assisted decision-making -- update opacity -- that occurs when users\ncannot understand how or why an update has changed the reasoning or behaviour\nof an ML system. This type of opacity presents a variety of distinctive\nepistemic and safety concerns that available solutions to the black box problem\nin ML are largely ill-equipped to address. A variety of alternative strategies\nmay be developed or pursued to address the problem of update opacity more\ndirectly, including bi-factual explanations, dynamic model reporting, and\nupdate compatibility. However, each of these strategies presents its own risks\nor carries significant limitations. Further research will be needed to address\nthe epistemic and safety concerns associated with model updating and update\nopacity going forward.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.05210v1",
    "published_date": "2025-04-07 15:58:23 UTC",
    "updated_date": "2025-04-07 15:58:23 UTC"
  },
  {
    "arxiv_id": "2504.05207v1",
    "title": "Correcting Class Imbalances with Self-Training for Improved Universal Lesion Detection and Tagging",
    "authors": [
      "Alexander Shieh",
      "Tejas Sudharshan Mathai",
      "Jianfei Liu",
      "Angshuman Paul",
      "Ronald M. Summers"
    ],
    "abstract": "Universal lesion detection and tagging (ULDT) in CT studies is critical for\ntumor burden assessment and tracking the progression of lesion status\n(growth/shrinkage) over time. However, a lack of fully annotated data hinders\nthe development of effective ULDT approaches. Prior work used the DeepLesion\ndataset (4,427 patients, 10,594 studies, 32,120 CT slices, 32,735 lesions, 8\nbody part labels) for algorithmic development, but this dataset is not\ncompletely annotated and contains class imbalances. To address these issues, in\nthis work, we developed a self-training pipeline for ULDT. A VFNet model was\ntrained on a limited 11.5\\% subset of DeepLesion (bounding boxes + tags) to\ndetect and classify lesions in CT studies. Then, it identified and incorporated\nnovel lesion candidates from a larger unseen data subset into its training set,\nand self-trained itself over multiple rounds. Multiple self-training\nexperiments were conducted with different threshold policies to select\npredicted lesions with higher quality and cover the class imbalances. We\ndiscovered that direct self-training improved the sensitivities of\nover-represented lesion classes at the expense of under-represented classes.\nHowever, upsampling the lesions mined during self-training along with a\nvariable threshold policy yielded a 6.5\\% increase in sensitivity at 4 FP in\ncontrast to self-training without class balancing (72\\% vs 78.5\\%) and a 11.7\\%\nincrease compared to the same self-training policy without upsampling (66.8\\%\nvs 78.5\\%). Furthermore, we show that our results either improved or maintained\nthe sensitivity at 4FP for all 8 lesion classes.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Published at SPIE Medical Imaging 2023",
    "pdf_url": "http://arxiv.org/pdf/2504.05207v1",
    "published_date": "2025-04-07 15:57:03 UTC",
    "updated_date": "2025-04-07 15:57:03 UTC"
  },
  {
    "arxiv_id": "2504.05201v1",
    "title": "3D Universal Lesion Detection and Tagging in CT with Self-Training",
    "authors": [
      "Jared Frazier",
      "Tejas Sudharshan Mathai",
      "Jianfei Liu",
      "Angshuman Paul",
      "Ronald M. Summers"
    ],
    "abstract": "Radiologists routinely perform the tedious task of lesion localization,\nclassification, and size measurement in computed tomography (CT) studies.\nUniversal lesion detection and tagging (ULDT) can simultaneously help alleviate\nthe cumbersome nature of lesion measurement and enable tumor burden assessment.\nPrevious ULDT approaches utilize the publicly available DeepLesion dataset,\nhowever it does not provide the full volumetric (3D) extent of lesions and also\ndisplays a severe class imbalance. In this work, we propose a self-training\npipeline to detect 3D lesions and tag them according to the body part they\noccur in. We used a significantly limited 30\\% subset of DeepLesion to train a\nVFNet model for 2D lesion detection and tagging. Next, the 2D lesion context\nwas expanded into 3D, and the mined 3D lesion proposals were integrated back\ninto the baseline training data in order to retrain the model over multiple\nrounds. Through the self-training procedure, our VFNet model learned from its\nown predictions, detected lesions in 3D, and tagged them. Our results indicated\nthat our VFNet model achieved an average sensitivity of 46.9\\% at [0.125:8]\nfalse positives (FP) with a limited 30\\% data subset in comparison to the\n46.8\\% of an existing approach that used the entire DeepLesion dataset. To our\nknowledge, we are the first to jointly detect lesions in 3D and tag them\naccording to the body part label.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Published at SPIE Medical Imaging 2023",
    "pdf_url": "http://arxiv.org/pdf/2504.05201v1",
    "published_date": "2025-04-07 15:50:27 UTC",
    "updated_date": "2025-04-07 15:50:27 UTC"
  },
  {
    "arxiv_id": "2504.05196v1",
    "title": "Universal Lymph Node Detection in Multiparametric MRI with Selective Augmentation",
    "authors": [
      "Tejas Sudharshan Mathai",
      "Sungwon Lee",
      "Thomas C. Shen",
      "Zhiyong Lu",
      "Ronald M. Summers"
    ],
    "abstract": "Robust localization of lymph nodes (LNs) in multiparametric MRI (mpMRI) is\ncritical for the assessment of lymphadenopathy. Radiologists routinely measure\nthe size of LN to distinguish benign from malignant nodes, which would require\nsubsequent cancer staging. Sizing is a cumbersome task compounded by the\ndiverse appearances of LNs in mpMRI, which renders their measurement difficult.\nFurthermore, smaller and potentially metastatic LNs could be missed during a\nbusy clinical day. To alleviate these imaging and workflow problems, we propose\na pipeline to universally detect both benign and metastatic nodes in the body\nfor their ensuing measurement. The recently proposed VFNet neural network was\nemployed to identify LN in T2 fat suppressed and diffusion weighted imaging\n(DWI) sequences acquired by various scanners with a variety of exam protocols.\nWe also use a selective augmentation technique known as Intra-Label LISA (ILL)\nto diversify the input data samples the model sees during training, such that\nit improves its robustness during the evaluation phase. We achieved a\nsensitivity of $\\sim$83\\% with ILL vs. $\\sim$80\\% without ILL at 4 FP/vol.\nCompared with current LN detection approaches evaluated on mpMRI, we show a\nsensitivity improvement of $\\sim$9\\% at 4 FP/vol.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "Published at SPIE Medical Imaging 2023",
    "pdf_url": "http://arxiv.org/pdf/2504.05196v1",
    "published_date": "2025-04-07 15:46:43 UTC",
    "updated_date": "2025-04-07 15:46:43 UTC"
  },
  {
    "arxiv_id": "2504.05187v1",
    "title": "Resource-Efficient Beam Prediction in mmWave Communications with Multimodal Realistic Simulation Framework",
    "authors": [
      "Yu Min Park",
      "Yan Kyaw Tun",
      "Walid Saad",
      "Choong Seon Hong"
    ],
    "abstract": "Beamforming is a key technology in millimeter-wave (mmWave) communications\nthat improves signal transmission by optimizing directionality and intensity.\nHowever, conventional channel estimation methods, such as pilot signals or beam\nsweeping, often fail to adapt to rapidly changing communication environments.\nTo address this limitation, multimodal sensing-aided beam prediction has gained\nsignificant attention, using various sensing data from devices such as LiDAR,\nradar, GPS, and RGB images to predict user locations or network conditions.\nDespite its promising potential, the adoption of multimodal sensing-aided beam\nprediction is hindered by high computational complexity, high costs, and\nlimited datasets. Thus, in this paper, a resource-efficient learning approach\nis proposed to transfer knowledge from a multimodal network to a monomodal\n(radar-only) network based on cross-modal relational knowledge distillation\n(CRKD), while reducing computational overhead and preserving predictive\naccuracy. To enable multimodal learning with realistic data, a novel multimodal\nsimulation framework is developed while integrating sensor data generated from\nthe autonomous driving simulator CARLA with MATLAB-based mmWave channel\nmodeling, and reflecting real-world conditions. The proposed CRKD achieves its\nobjective by distilling relational information across different feature spaces,\nwhich enhances beam prediction performance without relying on expensive sensor\ndata. Simulation results demonstrate that CRKD efficiently distills multimodal\nknowledge, allowing a radar-only model to achieve $94.62\\%$ of the teacher\nperformance. In particular, this is achieved with just $10\\%$ of the teacher\nnetwork's parameters, thereby significantly reducing computational complexity\nand dependence on multimodal sensor data.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NI",
    "comment": "12 pages, 8 figures, Submitted to IEEE Transactions on Communications\n  on Apr. 07, 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.05187v1",
    "published_date": "2025-04-07 15:38:25 UTC",
    "updated_date": "2025-04-07 15:38:25 UTC"
  },
  {
    "arxiv_id": "2504.05181v1",
    "title": "Lightweight and Direct Document Relevance Optimization for Generative Information Retrieval",
    "authors": [
      "Kidist Amde Mekonnen",
      "Yubao Tang",
      "Maarten de Rijke"
    ],
    "abstract": "Generative information retrieval (GenIR) is a promising neural retrieval\nparadigm that formulates document retrieval as a document identifier (docid)\ngeneration task, allowing for end-to-end optimization toward a unified global\nretrieval objective. However, existing GenIR models suffer from token-level\nmisalignment, where models trained to predict the next token often fail to\ncapture document-level relevance effectively. While reinforcement\nlearning-based methods, such as reinforcement learning from relevance feedback\n(RLRF), aim to address this misalignment through reward modeling, they\nintroduce significant complexity, requiring the optimization of an auxiliary\nreward function followed by reinforcement fine-tuning, which is computationally\nexpensive and often unstable. To address these challenges, we propose direct\ndocument relevance optimization (DDRO), which aligns token-level docid\ngeneration with document-level relevance estimation through direct optimization\nvia pairwise ranking, eliminating the need for explicit reward modeling and\nreinforcement learning. Experimental results on benchmark datasets, including\nMS MARCO document and Natural Questions, show that DDRO outperforms\nreinforcement learning-based methods, achieving a 7.4% improvement in MRR@10\nfor MS MARCO and a 19.9% improvement for Natural Questions. These findings\nhighlight DDRO's potential to enhance retrieval effectiveness with a simplified\noptimization approach. By framing alignment as a direct optimization problem,\nDDRO simplifies the ranking optimization pipeline of GenIR models while\noffering a viable alternative to reinforcement learning-based methods.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.DL",
      "cs.LG",
      "H.3.3"
    ],
    "primary_category": "cs.IR",
    "comment": "13 pages, 5 figures. Submitted to SIGIR 2025. Proposes DDRO, a\n  lightweight and reinforcement-free document relevance optimization method for\n  generative retrieval. Code and pretrained models available at:\n  https://github.com/kidist-amde/DDRO-Direct-Document-Relevance-Optimization",
    "pdf_url": "http://arxiv.org/pdf/2504.05181v1",
    "published_date": "2025-04-07 15:27:37 UTC",
    "updated_date": "2025-04-07 15:27:37 UTC"
  },
  {
    "arxiv_id": "2504.05180v1",
    "title": "BRIDGES: Bridging Graph Modality and Large Language Models within EDA Tasks",
    "authors": [
      "Wei Li",
      "Yang Zou",
      "Christopher Ellis",
      "Ruben Purdy",
      "Shawn Blanton",
      "José M. F. Moura"
    ],
    "abstract": "While many EDA tasks already involve graph-based data, existing LLMs in EDA\nprimarily either represent graphs as sequential text, or simply ignore\ngraph-structured data that might be beneficial like dataflow graphs of RTL\ncode. Recent studies have found that LLM performance suffers when graphs are\nrepresented as sequential text, and using additional graph information\nsignificantly boosts performance. To address these challenges, we introduce\nBRIDGES, a framework designed to incorporate graph modality into LLMs for EDA\ntasks. BRIDGES integrates an automated data generation workflow, a solution\nthat combines graph modality with LLM, and a comprehensive evaluation suite.\nFirst, we establish an LLM-driven workflow to generate RTL and netlist-level\ndata, converting them into dataflow and netlist graphs with function\ndescriptions. This workflow yields a large-scale dataset comprising over\n500,000 graph instances and more than 1.5 billion tokens. Second, we propose a\nlightweight cross-modal projector that encodes graph representations into\ntext-compatible prompts, enabling LLMs to effectively utilize graph data\nwithout architectural modifications. Experimental results demonstrate 2x to 10x\nimprovements across multiple tasks compared to text-only baselines, including\naccuracy in design retrieval, type prediction and perplexity in function\ndescription, with negligible computational overhead (<1% model weights increase\nand <30% additional runtime overhead). Even without additional LLM finetuning,\nour results outperform text-only by a large margin. We plan to release BRIDGES,\nincluding the dataset, models, and training flow.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.05180v1",
    "published_date": "2025-04-07 15:27:32 UTC",
    "updated_date": "2025-04-07 15:27:32 UTC"
  },
  {
    "arxiv_id": "2504.05172v1",
    "title": "Attention-Based Multi-Scale Temporal Fusion Network for Uncertain-Mode Fault Diagnosis in Multimode Processes",
    "authors": [
      "Guangqiang Li",
      "M. Amine Atoui",
      "Xiangshun Li"
    ],
    "abstract": "Fault diagnosis in multimode processes plays a critical role in ensuring the\nsafe operation of industrial systems across multiple modes. It faces a great\nchallenge yet to be addressed - that is, the significant distributional\ndifferences among monitoring data from multiple modes make it difficult for the\nmodels to extract shared feature representations related to system health\nconditions. In response to this problem, this paper introduces a novel method\ncalled attention-based multi-scale temporal fusion network. The multi-scale\ndepthwise convolution and gated recurrent unit are employed to extract\nmulti-scale contextual local features and long-short-term features. A temporal\nattention mechanism is designed to focus on critical time points with higher\ncross-mode shared information, thereby enhancing the accuracy of fault\ndiagnosis. The proposed model is applied to Tennessee Eastman process dataset\nand three-phase flow facility dataset. The experiments demonstrate that the\nproposed model achieves superior diagnostic performance and maintains a small\nmodel size.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "31 pages,11 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.05172v1",
    "published_date": "2025-04-07 15:16:22 UTC",
    "updated_date": "2025-04-07 15:16:22 UTC"
  },
  {
    "arxiv_id": "2504.05170v1",
    "title": "SSLFusion: Scale & Space Aligned Latent Fusion Model for Multimodal 3D Object Detection",
    "authors": [
      "Bonan Ding",
      "Jin Xie",
      "Jing Nie",
      "Jiale Cao"
    ],
    "abstract": "Multimodal 3D object detection based on deep neural networks has indeed made\nsignificant progress. However, it still faces challenges due to the\nmisalignment of scale and spatial information between features extracted from\n2D images and those derived from 3D point clouds. Existing methods usually\naggregate multimodal features at a single stage. However, leveraging\nmulti-stage cross-modal features is crucial for detecting objects of various\nscales. Therefore, these methods often struggle to integrate features across\ndifferent scales and modalities effectively, thereby restricting the accuracy\nof detection. Additionally, the time-consuming Query-Key-Value-based\n(QKV-based) cross-attention operations often utilized in existing methods aid\nin reasoning the location and existence of objects by capturing non-local\ncontexts. However, this approach tends to increase computational complexity. To\naddress these challenges, we present SSLFusion, a novel Scale & Space Aligned\nLatent Fusion Model, consisting of a scale-aligned fusion strategy (SAF), a\n3D-to-2D space alignment module (SAM), and a latent cross-modal fusion module\n(LFM). SAF mitigates scale misalignment between modalities by aggregating\nfeatures from both images and point clouds across multiple levels. SAM is\ndesigned to reduce the inter-modal gap between features from images and point\nclouds by incorporating 3D coordinate information into 2D image features.\nAdditionally, LFM captures cross-modal non-local contexts in the latent space\nwithout utilizing the QKV-based attention operations, thus mitigating\ncomputational complexity. Experiments on the KITTI and DENSE datasets\ndemonstrate that our SSLFusion outperforms state-of-the-art methods. Our\napproach obtains an absolute gain of 2.15% in 3D AP, compared with the\nstate-of-art method GraphAlign on the moderate level of the KITTI test set.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.05170v1",
    "published_date": "2025-04-07 15:15:06 UTC",
    "updated_date": "2025-04-07 15:15:06 UTC"
  },
  {
    "arxiv_id": "2504.05167v1",
    "title": "RLBayes: a Bayesian Network Structure Learning Algorithm via Reinforcement Learning-Based Search Strategy",
    "authors": [
      "Mingcan Wang",
      "Junchang Xin",
      "Luxuan Qu",
      "Qi Chen",
      "Zhiqiong Wang"
    ],
    "abstract": "The score-based structure learning of Bayesian network (BN) is an effective\nway to learn BN models, which are regarded as some of the most compelling\nprobabilistic graphical models in the field of representation and reasoning\nunder uncertainty. However, the search space of structure learning grows\nsuper-exponentially as the number of variables increases, which makes BN\nstructure learning an NP-hard problem, as well as a combination optimization\nproblem (COP). Despite the successes of many heuristic methods on it, the\nresults of the structure learning of BN are usually unsatisfactory. Inspired by\nQ-learning, in this paper, a Bayesian network structure learning algorithm via\nreinforcement learning-based (RL-based) search strategy is proposed, namely\nRLBayes. The method borrows the idea of RL and tends to record and guide the\nlearning process by a dynamically maintained Q-table. By creating and\nmaintaining the dynamic Q-table, RLBayes achieve storing the unlimited search\nspace within limited space, thereby achieving the structure learning of BN via\nQ-learning. Not only is it theoretically proved that RLBayes can converge to\nthe global optimal BN structure, but also it is experimentally proved that\nRLBayes has a better effect than almost all other heuristic search algorithms.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.05167v1",
    "published_date": "2025-04-07 15:11:51 UTC",
    "updated_date": "2025-04-07 15:11:51 UTC"
  },
  {
    "arxiv_id": "2504.05163v1",
    "title": "Evaluating Knowledge Graph Based Retrieval Augmented Generation Methods under Knowledge Incompleteness",
    "authors": [
      "Dongzhuoran Zhou",
      "Yuqicheng Zhu",
      "Yuan He",
      "Jiaoyan Chen",
      "Evgeny Kharlamov",
      "Steffen Staab"
    ],
    "abstract": "Knowledge Graph based Retrieval-Augmented Generation (KG-RAG) is a technique\nthat enhances Large Language Model (LLM) inference in tasks like Question\nAnswering (QA) by retrieving relevant information from knowledge graphs (KGs).\nHowever, real-world KGs are often incomplete, meaning that essential\ninformation for answering questions may be missing. Existing benchmarks do not\nadequately capture the impact of KG incompleteness on KG-RAG performance. In\nthis paper, we systematically evaluate KG-RAG methods under incomplete KGs by\nremoving triples using different methods and analyzing the resulting effects.\nWe demonstrate that KG-RAG methods are sensitive to KG incompleteness,\nhighlighting the need for more robust approaches in realistic settings.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Under Review",
    "pdf_url": "http://arxiv.org/pdf/2504.05163v1",
    "published_date": "2025-04-07 15:08:03 UTC",
    "updated_date": "2025-04-07 15:08:03 UTC"
  },
  {
    "arxiv_id": "2504.05158v1",
    "title": "Leveraging Label Potential for Enhanced Multimodal Emotion Recognition",
    "authors": [
      "Xuechun Shao",
      "Yinfeng Yu",
      "Liejun Wang"
    ],
    "abstract": "Multimodal emotion recognition (MER) seeks to integrate various modalities to\npredict emotional states accurately. However, most current research focuses\nsolely on the fusion of audio and text features, overlooking the valuable\ninformation in emotion labels. This oversight could potentially hinder the\nperformance of existing methods, as emotion labels harbor rich, insightful\ninformation that could significantly aid MER. We introduce a novel model called\nLabel Signal-Guided Multimodal Emotion Recognition (LSGMER) to overcome this\nlimitation. This model aims to fully harness the power of emotion label\ninformation to boost the classification accuracy and stability of MER.\nSpecifically, LSGMER employs a Label Signal Enhancement module that optimizes\nthe representation of modality features by interacting with audio and text\nfeatures through label embeddings, enabling it to capture the nuances of\nemotions precisely. Furthermore, we propose a Joint Objective Optimization(JOO)\napproach to enhance classification accuracy by introducing the\nAttribution-Prediction Consistency Constraint (APC), which strengthens the\nalignment between fused features and emotion categories. Extensive experiments\nconducted on the IEMOCAP and MELD datasets have demonstrated the effectiveness\nof our proposed LSGMER model.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Main paper (8 pages). Accepted for publication by IJCNN 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.05158v1",
    "published_date": "2025-04-07 15:00:34 UTC",
    "updated_date": "2025-04-07 15:00:34 UTC"
  },
  {
    "arxiv_id": "2504.05150v1",
    "title": "A Reinforcement Learning Method for Environments with Stochastic Variables: Post-Decision Proximal Policy Optimization with Dual Critic Networks",
    "authors": [
      "Leonardo Kanashiro Felizardo",
      "Edoardo Fadda",
      "Paolo Brandimarte",
      "Emilio Del-Moral-Hernandez",
      "Mariá Cristina Vasconcelos Nascimento"
    ],
    "abstract": "This paper presents Post-Decision Proximal Policy Optimization (PDPPO), a\nnovel variation of the leading deep reinforcement learning method, Proximal\nPolicy Optimization (PPO). The PDPPO state transition process is divided into\ntwo steps: a deterministic step resulting in the post-decision state and a\nstochastic step leading to the next state. Our approach incorporates\npost-decision states and dual critics to reduce the problem's dimensionality\nand enhance the accuracy of value function estimation. Lot-sizing is a mixed\ninteger programming problem for which we exemplify such dynamics. The objective\nof lot-sizing is to optimize production, delivery fulfillment, and inventory\nlevels in uncertain demand and cost parameters. This paper evaluates the\nperformance of PDPPO across various environments and configurations. Notably,\nPDPPO with a dual critic architecture achieves nearly double the maximum reward\nof vanilla PPO in specific scenarios, requiring fewer episode iterations and\ndemonstrating faster and more consistent learning across different\ninitializations. On average, PDPPO outperforms PPO in environments with a\nstochastic component in the state transition. These results support the\nbenefits of using a post-decision state. Integrating this post-decision state\nin the value function approximation leads to more informed and efficient\nlearning in high-dimensional and stochastic environments.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2.6; G.1.6"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages, 4 figures. Accepted for presentation at IJCNN 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.05150v1",
    "published_date": "2025-04-07 14:56:43 UTC",
    "updated_date": "2025-04-07 14:56:43 UTC"
  },
  {
    "arxiv_id": "2504.05141v1",
    "title": "EffOWT: Transfer Visual Language Models to Open-World Tracking Efficiently and Effectively",
    "authors": [
      "Bingyang Wang",
      "Kaer Huang",
      "Bin Li",
      "Yiqiang Yan",
      "Lihe Zhang",
      "Huchuan Lu",
      "You He"
    ],
    "abstract": "Open-World Tracking (OWT) aims to track every object of any category, which\nrequires the model to have strong generalization capabilities. Trackers can\nimprove their generalization ability by leveraging Visual Language Models\n(VLMs). However, challenges arise with the fine-tuning strategies when VLMs are\ntransferred to OWT: full fine-tuning results in excessive parameter and memory\ncosts, while the zero-shot strategy leads to sub-optimal performance. To solve\nthe problem, EffOWT is proposed for efficiently transferring VLMs to OWT.\nSpecifically, we build a small and independent learnable side network outside\nthe VLM backbone. By freezing the backbone and only executing backpropagation\non the side network, the model's efficiency requirements can be met. In\naddition, EffOWT enhances the side network by proposing a hybrid structure of\nTransformer and CNN to improve the model's performance in the OWT field.\nFinally, we implement sparse interactions on the MLP, thus reducing parameter\nupdates and memory costs significantly. Thanks to the proposed methods, EffOWT\nachieves an absolute gain of 5.5% on the tracking metric OWTA for unknown\ncategories, while only updating 1.3% of the parameters compared to full\nfine-tuning, with a 36.4% memory saving. Other metrics also demonstrate obvious\nimprovement.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "11 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.05141v1",
    "published_date": "2025-04-07 14:47:58 UTC",
    "updated_date": "2025-04-07 14:47:58 UTC"
  },
  {
    "arxiv_id": "2504.05125v1",
    "title": "Interpretable Style Takagi-Sugeno-Kang Fuzzy Clustering",
    "authors": [
      "Suhang Gu",
      "Ye Wang",
      "Yongxin Chou",
      "Jinliang Cong",
      "Mingli Lu",
      "Zhuqing Jiao"
    ],
    "abstract": "Clustering is an efficient and essential technique for exploring latent\nknowledge of data. However, limited attention has been given to the\ninterpretability of the clusters detected by most clustering algorithms. In\naddition, due to the homogeneity of data, different groups of data have their\nown homogeneous styles. In this paper, the above two aspects are considered,\nand an interpretable style Takagi-Sugeno-Kang (TSK) fuzzy clustering\n(IS-TSK-FC) algorithm is proposed. The clustering behavior of IS-TSK-FC is\nfully guided by the TSK fuzzy inference on fuzzy rules. In particular, samples\nare grouped into clusters represented by the corresponding consequent vectors\nof all fuzzy rules learned in an unsupervised manner. This can explain how the\nclusters are generated in detail, thus making the underlying decision-making\nprocess of the IS-TSK-FC interpretable. Moreover, a series of style matrices\nare introduced to facilitate the consequents of fuzzy rules in IS-TSK-FC by\ncapturing the styles of clusters as well as the nuances between different\nstyles. Consequently, all the fuzzy rules in IS-TSK-FC have powerful data\nrepresentation capability. After determining the antecedents of all the fuzzy\nrules, the optimization problem of IS-TSK-FC can be iteratively solved in an\nalternation manner. The effectiveness of IS-TSK-FC as an interpretable\nclustering tool is validated through extensive experiments on benchmark\ndatasets with unknown implicit/explicit styles. Specially, the superior\nclustering performance of IS-TSK-FC is demonstrated on case studies where\ndifferent groups of data present explicit styles. The source code of IS-TSK-FC\ncan be downloaded from https://github.com/gusuhang10/IS-TSK-FC.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.05125v1",
    "published_date": "2025-04-07 14:28:56 UTC",
    "updated_date": "2025-04-07 14:28:56 UTC"
  },
  {
    "arxiv_id": "2504.05119v1",
    "title": "Balancing Robustness and Efficiency in Embedded DNNs Through Activation Function Selection",
    "authors": [
      "Jon Gutiérrez Zaballa",
      "Koldo Basterretxea",
      "Javier Echanobe"
    ],
    "abstract": "Machine learning-based embedded systems for safety-critical applications,\nsuch as aerospace and autonomous driving, must be robust to perturbations\ncaused by soft errors. As transistor geometries shrink and voltages decrease,\nmodern electronic devices become more susceptible to background radiation,\nincreasing the concern about failures produced by soft errors. The resilience\nof deep neural networks (DNNs) to these errors depends not only on target\ndevice technology but also on model structure and the numerical representation\nand arithmetic precision of their parameters. Compression techniques like\npruning and quantization, used to reduce memory footprint and computational\ncomplexity, alter both model structure and representation, affecting soft error\nrobustness. In this regard, although often overlooked, the choice of activation\nfunctions (AFs) impacts not only accuracy and trainability but also\ncompressibility and error resilience. This paper explores the use of bounded\nAFs to enhance robustness against parameter perturbations, while evaluating\ntheir effects on model accuracy, compressibility, and computational load with a\ntechnology-agnostic approach. We focus on encoder-decoder convolutional models\ndeveloped for semantic segmentation of hyperspectral images with application to\nautonomous driving systems. Experiments are conducted on an AMD-Xilinx's KV260\nSoM.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR",
      "cs.CV",
      "eess.IV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.05119v1",
    "published_date": "2025-04-07 14:21:31 UTC",
    "updated_date": "2025-04-07 14:21:31 UTC"
  },
  {
    "arxiv_id": "2504.05118v2",
    "title": "VAPO: Efficient and Reliable Reinforcement Learning for Advanced Reasoning Tasks",
    "authors": [
      "Yu Yue",
      "Yufeng Yuan",
      "Qiying Yu",
      "Xiaochen Zuo",
      "Ruofei Zhu",
      "Wenyuan Xu",
      "Jiaze Chen",
      "Chengyi Wang",
      "TianTian Fan",
      "Zhengyin Du",
      "Xiangpeng Wei",
      "Xiangyu Yu",
      "Gaohong Liu",
      "Juncai Liu",
      "Lingjun Liu",
      "Haibin Lin",
      "Zhiqi Lin",
      "Bole Ma",
      "Chi Zhang",
      "Mofan Zhang",
      "Wang Zhang",
      "Hang Zhu",
      "Ru Zhang",
      "Xin Liu",
      "Mingxuan Wang",
      "Yonghui Wu",
      "Lin Yan"
    ],
    "abstract": "We present VAPO, Value-based Augmented Proximal Policy Optimization framework\nfor reasoning models., a novel framework tailored for reasoning models within\nthe value-based paradigm. Benchmarked the AIME 2024 dataset, VAPO, built on the\nQwen 32B pre-trained model, attains a state-of-the-art score of\n$\\mathbf{60.4}$. In direct comparison under identical experimental settings,\nVAPO outperforms the previously reported results of DeepSeek-R1-Zero-Qwen-32B\nand DAPO by more than 10 points. The training process of VAPO stands out for\nits stability and efficiency. It reaches state-of-the-art performance within a\nmere 5,000 steps. Moreover, across multiple independent runs, no training\ncrashes occur, underscoring its reliability. This research delves into long\nchain-of-thought (long-CoT) reasoning using a value-based reinforcement\nlearning framework. We pinpoint three key challenges that plague value-based\nmethods: value model bias, the presence of heterogeneous sequence lengths, and\nthe sparsity of reward signals. Through systematic design, VAPO offers an\nintegrated solution that effectively alleviates these challenges, enabling\nenhanced performance in long-CoT reasoning tasks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.05118v2",
    "published_date": "2025-04-07 14:21:11 UTC",
    "updated_date": "2025-04-08 03:06:22 UTC"
  },
  {
    "arxiv_id": "2504.05108v1",
    "title": "Algorithm Discovery With LLMs: Evolutionary Search Meets Reinforcement Learning",
    "authors": [
      "Anja Surina",
      "Amin Mansouri",
      "Lars Quaedvlieg",
      "Amal Seddas",
      "Maryna Viazovska",
      "Emmanuel Abbe",
      "Caglar Gulcehre"
    ],
    "abstract": "Discovering efficient algorithms for solving complex problems has been an\noutstanding challenge in mathematics and computer science, requiring\nsubstantial human expertise over the years. Recent advancements in evolutionary\nsearch with large language models (LLMs) have shown promise in accelerating the\ndiscovery of algorithms across various domains, particularly in mathematics and\noptimization. However, existing approaches treat the LLM as a static generator,\nmissing the opportunity to update the model with the signal obtained from\nevolutionary exploration. In this work, we propose to augment LLM-based\nevolutionary search by continuously refining the search operator - the LLM -\nthrough reinforcement learning (RL) fine-tuning. Our method leverages\nevolutionary search as an exploration strategy to discover improved algorithms,\nwhile RL optimizes the LLM policy based on these discoveries. Our experiments\non three combinatorial optimization tasks - bin packing, traveling salesman,\nand the flatpack problem - show that combining RL and evolutionary search\nimproves discovery efficiency of improved algorithms, showcasing the potential\nof RL-enhanced evolutionary strategies to assist computer scientists and\nmathematicians for more efficient algorithm design.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.AI",
    "comment": "30 pages",
    "pdf_url": "http://arxiv.org/pdf/2504.05108v1",
    "published_date": "2025-04-07 14:14:15 UTC",
    "updated_date": "2025-04-07 14:14:15 UTC"
  },
  {
    "arxiv_id": "2504.05106v1",
    "title": "SpeakEasy: Enhancing Text-to-Speech Interactions for Expressive Content Creation",
    "authors": [
      "Stephen Brade",
      "Sam Anderson",
      "Rithesh Kumar",
      "Zeyu Jin",
      "Anh Truong"
    ],
    "abstract": "Novice content creators often invest significant time recording expressive\nspeech for social media videos. While recent advancements in text-to-speech\n(TTS) technology can generate highly realistic speech in various languages and\naccents, many struggle with unintuitive or overly granular TTS interfaces. We\npropose simplifying TTS generation by allowing users to specify high-level\ncontext alongside their script. Our Wizard-of-Oz system, SpeakEasy, leverages\nuser-provided context to inform and influence TTS output, enabling iterative\nrefinement with high-level feedback. This approach was informed by two\n8-subject formative studies: one examining content creators' experiences with\nTTS, and the other drawing on effective strategies from voice actors. Our\nevaluation shows that participants using SpeakEasy were more successful in\ngenerating performances matching their personal standards, without requiring\nsignificantly more effort than leading industry interfaces.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.05106v1",
    "published_date": "2025-04-07 14:13:49 UTC",
    "updated_date": "2025-04-07 14:13:49 UTC"
  },
  {
    "arxiv_id": "2504.05050v1",
    "title": "Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language Models",
    "authors": [
      "Jiawei Lian",
      "Jianhong Pan",
      "Lefan Wang",
      "Yi Wang",
      "Shaohui Mei",
      "Lap-Pui Chau"
    ],
    "abstract": "Large language models (LLMs) are foundational explorations to artificial\ngeneral intelligence, yet their alignment with human values via instruction\ntuning and preference learning achieves only superficial compliance. Here, we\ndemonstrate that harmful knowledge embedded during pretraining persists as\nindelible \"dark patterns\" in LLMs' parametric memory, evading alignment\nsafeguards and resurfacing under adversarial inducement at distributional\nshifts. In this study, we first theoretically analyze the intrinsic ethical\nvulnerability of aligned LLMs by proving that current alignment methods yield\nonly local \"safety regions\" in the knowledge manifold. In contrast, pretrained\nknowledge remains globally connected to harmful concepts via high-likelihood\nadversarial trajectories. Building on this theoretical insight, we empirically\nvalidate our findings by employing semantic coherence inducement under\ndistributional shifts--a method that systematically bypasses alignment\nconstraints through optimized adversarial prompts. This combined theoretical\nand empirical approach achieves a 100% attack success rate across 19 out of 23\nstate-of-the-art aligned LLMs, including DeepSeek-R1 and LLaMA-3, revealing\ntheir universal vulnerabilities.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.05050v1",
    "published_date": "2025-04-07 13:20:17 UTC",
    "updated_date": "2025-04-07 13:20:17 UTC"
  },
  {
    "arxiv_id": "2504.05047v1",
    "title": "Debate Only When Necessary: Adaptive Multiagent Collaboration for Efficient LLM Reasoning",
    "authors": [
      "Sugyeong Eo",
      "Hyeonseok Moon",
      "Evelyn Hayoon Zi",
      "Chanjun Park",
      "Heuiseok Lim"
    ],
    "abstract": "Multiagent collaboration has emerged as a promising framework for enhancing\nthe reasoning capabilities of large language models (LLMs). While this approach\nimproves reasoning capability, it incurs substantial computational overhead due\nto iterative agent interactions. Furthermore, engaging in debates for queries\nthat do not necessitate collaboration amplifies the risk of error generation.\nTo address these challenges, we propose Debate Only When Necessary (DOWN), an\nadaptive multiagent debate framework that selectively activates the debate\nprocess based on the confidence score of the agent's initial response. For\nqueries where debate is triggered, agents refine their outputs using responses\nfrom participating agents and their confidence scores. Experimental results\ndemonstrate that this mechanism significantly improves efficiency while\nmaintaining or even surpassing the performance of existing multiagent debate\nsystems. We also find that confidence-guided debate mitigates error propagation\nand enhances the selective incorporation of reliable responses. These results\nestablish DOWN as an optimization strategy for efficient and effective\nmultiagent reasoning, facilitating the practical deployment of LLM-based\ncollaboration.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.05047v1",
    "published_date": "2025-04-07 13:17:52 UTC",
    "updated_date": "2025-04-07 13:17:52 UTC"
  },
  {
    "arxiv_id": "2504.05029v1",
    "title": "Graph-based Diffusion Model for Collaborative Filtering",
    "authors": [
      "Xuan Zhang",
      "Xiang Deng",
      "Hongxing Yuan",
      "Chunyu Wei",
      "Yushun Fan"
    ],
    "abstract": "Recently, diffusion-based recommendation methods have achieved impressive\nresults. However, existing approaches predominantly treat each user's\nhistorical interactions as independent training samples, overlooking the\npotential of higher-order collaborative signals between users and items. Such\nsignals, which encapsulate richer and more nuanced relationships, can be\nnaturally captured using graph-based data structures. To address this\nlimitation, we extend diffusion-based recommendation methods to the graph\ndomain by directly modeling user-item bipartite graphs with diffusion models.\nThis enables better modeling of the higher-order connectivity inherent in\ncomplex interaction dynamics. However, this extension introduces two primary\nchallenges: (1) Noise Heterogeneity, where interactions are influenced by\nvarious forms of continuous and discrete noise, and (2) Relation Explosion,\nreferring to the high computational costs of processing large-scale graphs. To\ntackle these challenges, we propose a Graph-based Diffusion Model for\nCollaborative Filtering (GDMCF). To address noise heterogeneity, we introduce a\nmulti-level noise corruption mechanism that integrates both continuous and\ndiscrete noise, effectively simulating real-world interaction complexities. To\nmitigate relation explosion, we design a user-active guided diffusion process\nthat selectively focuses on the most meaningful edges and active users,\nreducing inference costs while preserving the graph's topological integrity.\nExtensive experiments on three benchmark datasets demonstrate that GDMCF\nconsistently outperforms state-of-the-art methods, highlighting its\neffectiveness in capturing higher-order collaborative signals and improving\nrecommendation performance.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.05029v1",
    "published_date": "2025-04-07 12:51:18 UTC",
    "updated_date": "2025-04-07 12:51:18 UTC"
  },
  {
    "arxiv_id": "2504.05020v1",
    "title": "Batch Aggregation: An Approach to Enhance Text Classification with Correlated Augmented Data",
    "authors": [
      "Charco Hui",
      "Yalu Wen"
    ],
    "abstract": "Natural language processing models often face challenges due to limited\nlabeled data, especially in domain specific areas, e.g., clinical trials. To\novercome this, text augmentation techniques are commonly used to increases\nsample size by transforming the original input data into artificial ones with\nthe label preserved. However, traditional text classification methods ignores\nthe relationship between augmented texts and treats them as independent samples\nwhich may introduce classification error. Therefore, we propose a novel\napproach called 'Batch Aggregation' (BAGG) which explicitly models the\ndependence of text inputs generated through augmentation by incorporating an\nadditional layer that aggregates results from correlated texts. Through\nstudying multiple benchmark data sets across different domains, we found that\nBAGG can improve classification accuracy. We also found that the increase of\nperformance with BAGG is more obvious in domain specific data sets, with\naccuracy improvements of up to 10-29%. Through the analysis of benchmark data,\nthe proposed method addresses limitations of traditional techniques and\nimproves robustness in text classification tasks. Our result demonstrates that\nBAGG offers more robust results and outperforms traditional approaches when\ntraining data is limited.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.05020v1",
    "published_date": "2025-04-07 12:46:07 UTC",
    "updated_date": "2025-04-07 12:46:07 UTC"
  },
  {
    "arxiv_id": "2504.05007v1",
    "title": "Measuring the right thing: justifying metrics in AI impact assessments",
    "authors": [
      "Stefan Buijsman",
      "Herman Veluwenkamp"
    ],
    "abstract": "AI Impact Assessments are only as good as the measures used to assess the\nimpact of these systems. It is therefore paramount that we can justify our\nchoice of metrics in these assessments, especially for difficult to quantify\nethical and social values. We present a two-step approach to ensure metrics are\nproperly motivated. First, a conception needs to be spelled out (e.g. Rawlsian\nfairness or fairness as solidarity) and then a metric can be fitted to that\nconception. Both steps require separate justifications, as conceptions can be\njudged on how well they fit with the function of, for example, fairness. We\nargue that conceptual engineering offers helpful tools for this step. Second,\nmetrics need to be fitted to a conception. We illustrate this process through\nan examination of competing fairness metrics to illustrate that here the\nadditional content that a conception offers helps us justify the choice for a\nspecific metric. We thus advocate that impact assessments are not only clear on\ntheir metrics, but also on the conceptions that motivate those metrics.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.CY",
    "comment": "Accepted for publication in Global Perspectives on AI Impact\n  Assessment (Oxford University Press, forthcoming). Pre-publication version;\n  final version will be available from the publisher",
    "pdf_url": "http://arxiv.org/pdf/2504.05007v1",
    "published_date": "2025-04-07 12:32:41 UTC",
    "updated_date": "2025-04-07 12:32:41 UTC"
  },
  {
    "arxiv_id": "2504.04997v1",
    "title": "SurvSurf: a partially monotonic neural network for first-hitting time prediction of intermittently observed discrete and continuous sequential events",
    "authors": [
      "Yichen Kelly Chen",
      "Sören Dittmer",
      "Kinga Bernatowicz",
      "Josep Arús-Pous",
      "Kamen Bliznashki",
      "John Aston",
      "James H. F. Rudd",
      "Carola-Bibiane Schönlieb",
      "James Jones",
      "Michael Roberts"
    ],
    "abstract": "We propose a neural-network based survival model (SurvSurf) specifically\ndesigned for direct and simultaneous probabilistic prediction of the first\nhitting time of sequential events from baseline. Unlike existing models,\nSurvSurf is theoretically guaranteed to never violate the monotonic\nrelationship between the cumulative incidence functions of sequential events,\nwhile allowing nonlinear influence from predictors. It also incorporates\nimplicit truths for unobserved intermediate events in model fitting, and\nsupports both discrete and continuous time and events. We also identified a\nvariant of the Integrated Brier Score (IBS) that showed robust correlation with\nthe mean squared error (MSE) between the true and predicted probabilities by\naccounting for implied truths about the missing intermediate events. We\ndemonstrated the superiority of SurvSurf compared to modern and traditional\npredictive survival models in two simulated datasets and two real-world\ndatasets, using MSE, the more robust IBS and by measuring the extent of\nmonotonicity violation.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "math.ST",
      "stat.AP",
      "stat.TH",
      "62N01"
    ],
    "primary_category": "stat.ML",
    "comment": "41 pages, 18 figures (including supplemental information). Submitted\n  to RSS: Data Science and Artificial Intelligence",
    "pdf_url": "http://arxiv.org/pdf/2504.04997v1",
    "published_date": "2025-04-07 12:24:59 UTC",
    "updated_date": "2025-04-07 12:24:59 UTC"
  },
  {
    "arxiv_id": "2504.04994v1",
    "title": "Following the Whispers of Values: Unraveling Neural Mechanisms Behind Value-Oriented Behaviors in LLMs",
    "authors": [
      "Ling Hu",
      "Yuemei Xu",
      "Xiaoyang Gu",
      "Letao Han"
    ],
    "abstract": "Despite the impressive performance of large language models (LLMs), they can\npresent unintended biases and harmful behaviors driven by encoded values,\nemphasizing the urgent need to understand the value mechanisms behind them.\nHowever, current research primarily evaluates these values through external\nresponses with a focus on AI safety, lacking interpretability and failing to\nassess social values in real-world contexts. In this paper, we propose a novel\nframework called ValueExploration, which aims to explore the behavior-driven\nmechanisms of National Social Values within LLMs at the neuron level. As a case\nstudy, we focus on Chinese Social Values and first construct C-voice, a\nlarge-scale bilingual benchmark for identifying and evaluating Chinese Social\nValues in LLMs. By leveraging C-voice, we then identify and locate the neurons\nresponsible for encoding these values according to activation difference.\nFinally, by deactivating these neurons, we analyze shifts in model behavior,\nuncovering the internal mechanism by which values influence LLM\ndecision-making. Extensive experiments on four representative LLMs validate the\nefficacy of our framework. The benchmark and code will be available.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.04994v1",
    "published_date": "2025-04-07 12:23:59 UTC",
    "updated_date": "2025-04-07 12:23:59 UTC"
  },
  {
    "arxiv_id": "2504.04988v1",
    "title": "RS-RAG: Bridging Remote Sensing Imagery and Comprehensive Knowledge with a Multi-Modal Dataset and Retrieval-Augmented Generation Model",
    "authors": [
      "Congcong Wen",
      "Yiting Lin",
      "Xiaokang Qu",
      "Nan Li",
      "Yong Liao",
      "Hui Lin",
      "Xiang Li"
    ],
    "abstract": "Recent progress in VLMs has demonstrated impressive capabilities across a\nvariety of tasks in the natural image domain. Motivated by these advancements,\nthe remote sensing community has begun to adopt VLMs for remote sensing\nvision-language tasks, including scene understanding, image captioning, and\nvisual question answering. However, existing remote sensing VLMs typically rely\non closed-set scene understanding and focus on generic scene descriptions, yet\nlack the ability to incorporate external knowledge. This limitation hinders\ntheir capacity for semantic reasoning over complex or context-dependent queries\nthat involve domain-specific or world knowledge. To address these challenges,\nwe first introduced a multimodal Remote Sensing World Knowledge (RSWK) dataset,\nwhich comprises high-resolution satellite imagery and detailed textual\ndescriptions for 14,141 well-known landmarks from 175 countries, integrating\nboth remote sensing domain knowledge and broader world knowledge. Building upon\nthis dataset, we proposed a novel Remote Sensing Retrieval-Augmented Generation\n(RS-RAG) framework, which consists of two key components. The Multi-Modal\nKnowledge Vector Database Construction module encodes remote sensing imagery\nand associated textual knowledge into a unified vector space. The Knowledge\nRetrieval and Response Generation module retrieves and re-ranks relevant\nknowledge based on image and/or text queries, and incorporates the retrieved\ncontent into a knowledge-augmented prompt to guide the VLM in producing\ncontextually grounded responses. We validated the effectiveness of our approach\non three representative vision-language tasks, including image captioning,\nimage classification, and visual question answering, where RS-RAG significantly\noutperformed state-of-the-art baselines.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.04988v1",
    "published_date": "2025-04-07 12:13:43 UTC",
    "updated_date": "2025-04-07 12:13:43 UTC"
  },
  {
    "arxiv_id": "2504.04982v1",
    "title": "Transforming Future Data Center Operations and Management via Physical AI",
    "authors": [
      "Zhiwei Cao",
      "Minghao Li",
      "Feng Lin",
      "Qiang Fu",
      "Jimin Jia",
      "Yonggang Wen",
      "Jianxiong Yin",
      "Simon See"
    ],
    "abstract": "Data centers (DCs) as mission-critical infrastructures are pivotal in\npowering the growth of artificial intelligence (AI) and the digital economy.\nThe evolution from Internet DC to AI DC has introduced new challenges in\noperating and managing data centers for improved business resilience and\nreduced total cost of ownership. As a result, new paradigms, beyond the\ntraditional approaches based on best practices, must be in order for future\ndata centers. In this research, we propose and develop a novel Physical AI\n(PhyAI) framework for advancing DC operations and management. Our system\nleverages the emerging capabilities of state-of-the-art industrial products and\nour in-house research and development. Specifically, it presents three core\nmodules, namely: 1) an industry-grade in-house simulation engine to simulate DC\noperations in a highly accurate manner, 2) an AI engine built upon NVIDIA\nPhysicsNemo for the training and evaluation of physics-informed machine\nlearning (PIML) models, and 3) a digital twin platform built upon NVIDIA\nOmniverse for our proposed 5-tier digital twin framework. This system presents\na scalable and adaptable solution to digitalize, optimize, and automate future\ndata center operations and management, by enabling real-time digital twins for\nfuture data centers. To illustrate its effectiveness, we present a compelling\ncase study on building a surrogate model for predicting the thermal and airflow\nprofiles of a large-scale DC in a real-time manner. Our results demonstrate its\nsuperior performance over traditional time-consuming Computational Fluid\nDynamics/Heat Transfer (CFD/HT) simulation, with a median absolute temperature\nprediction error of 0.18 {\\deg}C. This emerging approach would open doors to\nseveral potential research directions for advancing Physical AI in future DC\noperations.",
    "categories": [
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.04982v1",
    "published_date": "2025-04-07 12:09:22 UTC",
    "updated_date": "2025-04-07 12:09:22 UTC"
  },
  {
    "arxiv_id": "2504.04981v1",
    "title": "DiCoTTA: Domain-invariant Learning for Continual Test-time Adaptation",
    "authors": [
      "Sohyun Lee",
      "Nayeong Kim",
      "Juwon Kang",
      "Seong Joon Oh",
      "Suha Kwak"
    ],
    "abstract": "This paper studies continual test-time adaptation (CTTA), the task of\nadapting a model to constantly changing unseen domains in testing while\npreserving previously learned knowledge. Existing CTTA methods mostly focus on\nadaptation to the current test domain only, overlooking generalization to\narbitrary test domains a model may face in the future. To tackle this\nlimitation, we present a novel online domain-invariant learning framework for\nCTTA, dubbed DiCoTTA. DiCoTTA aims to learn feature representation to be\ninvariant to both current and previous test domains on the fly during testing.\nTo this end, we propose a new model architecture and a test-time adaptation\nstrategy dedicated to learning domain-invariant features without corrupting\nsemantic contents, along with a new data structure and optimization algorithm\nfor effectively managing information from previous test domains. DiCoTTA\nachieved state-of-the-art performance on four public CTTA benchmarks. Moreover,\nit showed superior generalization to unseen test domains.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.04981v1",
    "published_date": "2025-04-07 12:09:18 UTC",
    "updated_date": "2025-04-07 12:09:18 UTC"
  },
  {
    "arxiv_id": "2504.04974v1",
    "title": "Towards Visual Text Grounding of Multimodal Large Language Model",
    "authors": [
      "Ming Li",
      "Ruiyi Zhang",
      "Jian Chen",
      "Jiuxiang Gu",
      "Yufan Zhou",
      "Franck Dernoncourt",
      "Wanrong Zhu",
      "Tianyi Zhou",
      "Tong Sun"
    ],
    "abstract": "Despite the existing evolution of Multimodal Large Language Models (MLLMs), a\nnon-neglectable limitation remains in their struggle with visual text\ngrounding, especially in text-rich images of documents. Document images, such\nas scanned forms and infographics, highlight critical challenges due to their\ncomplex layouts and textual content. However, current benchmarks do not fully\naddress these challenges, as they mostly focus on visual grounding on natural\nimages, rather than text-rich document images. Thus, to bridge this gap, we\nintroduce TRIG, a novel task with a newly designed instruction dataset for\nbenchmarking and improving the Text-Rich Image Grounding capabilities of MLLMs\nin document question-answering. Specifically, we propose an OCR-LLM-human\ninteraction pipeline to create 800 manually annotated question-answer pairs as\na benchmark and a large-scale training set of 90$ synthetic data based on four\ndiverse datasets. A comprehensive evaluation of various MLLMs on our proposed\nbenchmark exposes substantial limitations in their grounding capability on\ntext-rich images. In addition, we propose two simple and effective TRIG methods\nbased on general instruction tuning and plug-and-play efficient embedding,\nrespectively. By finetuning MLLMs on our synthetic dataset, they promisingly\nimprove spatial reasoning and grounding capabilities.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.04974v1",
    "published_date": "2025-04-07 12:01:59 UTC",
    "updated_date": "2025-04-07 12:01:59 UTC"
  },
  {
    "arxiv_id": "2504.04973v1",
    "title": "Ensuring Safety in an Uncertain Environment: Constrained MDPs via Stochastic Thresholds",
    "authors": [
      "Qian Zuo",
      "Fengxiang He"
    ],
    "abstract": "This paper studies constrained Markov decision processes (CMDPs) with\nconstraints against stochastic thresholds, aiming at safety of reinforcement\nlearning in unknown and uncertain environments. We leverage a Growing-Window\nestimator sampling from interactions with the uncertain and dynamic environment\nto estimate the thresholds, based on which we design Stochastic\nPessimistic-Optimistic Thresholding (SPOT), a novel model-based primal-dual\nalgorithm for multiple constraints against stochastic thresholds. SPOT enables\nreinforcement learning under both pessimistic and optimistic threshold\nsettings. We prove that our algorithm achieves sublinear regret and constraint\nviolation; i.e., a reward regret of $\\tilde{\\mathcal{O}}(\\sqrt{T})$ while\nallowing an $\\tilde{\\mathcal{O}}(\\sqrt{T})$ constraint violation over $T$\nepisodes. The theoretical guarantees show that our algorithm achieves\nperformance comparable to that of an approach relying on fixed and clear\nthresholds. To the best of our knowledge, SPOT is the first reinforcement\nlearning algorithm that realises theoretical guaranteed performance in an\nuncertain environment where even thresholds are unknown.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.04973v1",
    "published_date": "2025-04-07 11:58:19 UTC",
    "updated_date": "2025-04-07 11:58:19 UTC"
  },
  {
    "arxiv_id": "2504.04970v1",
    "title": "A High-Force Gripper with Embedded Multimodal Sensing for Powerful and Perception Driven Grasping",
    "authors": [
      "Edoardo Del Bianco",
      "Davide Torielli",
      "Federico Rollo",
      "Damiano Gasperini",
      "Arturo Laurenzi",
      "Lorenzo Baccelliere",
      "Luca Muratore",
      "Marco Roveri",
      "Nikos G. Tsagarakis"
    ],
    "abstract": "Modern humanoid robots have shown their promising potential for executing\nvarious tasks involving the grasping and manipulation of objects using their\nend-effectors. Nevertheless, in the most of the cases, the grasping and\nmanipulation actions involve low to moderate payload and interaction forces.\nThis is due to limitations often presented by the end-effectors, which can not\nmatch their arm-reachable payload, and hence limit the payload that can be\ngrasped and manipulated. In addition, grippers usually do not embed adequate\nperception in their hardware, and grasping actions are mainly driven by\nperception sensors installed in the rest of the robot body, frequently affected\nby occlusions due to the arm motions during the execution of the grasping and\nmanipulation tasks. To address the above, we developed a modular high grasping\nforce gripper equipped with embedded multi-modal perception functionalities.\nThe proposed gripper can generate a grasping force of 110 N in a compact\nimplementation. The high grasping force capability is combined with embedded\nmulti-modal sensing, which includes an eye-in-hand camera, a Time-of-Flight\n(ToF) distance sensor, an Inertial Measurement Unit (IMU) and an\nomnidirectional microphone, permitting the implementation of perception-driven\ngrasping functionalities.\n  We extensively evaluated the grasping force capacity of the gripper by\nintroducing novel payload evaluation metrics that are a function of the robot\narm's dynamic motion and gripper thermal states. We also evaluated the embedded\nmulti-modal sensing by performing perception-guided enhanced grasping\noperations.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages, 15 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.04970v1",
    "published_date": "2025-04-07 11:57:08 UTC",
    "updated_date": "2025-04-07 11:57:08 UTC"
  },
  {
    "arxiv_id": "2504.04968v1",
    "title": "The Dream Within Huang Long Cave: AI-Driven Interactive Narrative for Family Storytelling and Emotional Reflection",
    "authors": [
      "Jiayang Huang",
      "Lingjie Li",
      "Kang Zhang",
      "David Yip"
    ],
    "abstract": "This paper introduces the art project The Dream Within Huang Long Cave, an\nAI-driven interactive and immersive narrative experience. The project offers\nnew insights into AI technology, artistic practice, and psychoanalysis.\nInspired by actual geographical landscapes and familial archetypes, the work\ncombines psychoanalytic theory and computational technology, providing an\nartistic response to the concept of the non-existence of the Big Other. The\nnarrative is driven by a combination of a large language model (LLM) and a\nrealistic digital character, forming a virtual agent named YELL. Through\ndialogue and exploration within a cave automatic virtual environment (CAVE),\nthe audience is invited to unravel the language puzzles presented by YELL and\nhelp him overcome his life challenges. YELL is a fictional embodiment of the\nBig Other, modeled after the artist's real father. Through a cross-temporal\ninteraction with this digital father, the project seeks to deconstruct complex\nfamilial relationships. By demonstrating the non-existence of the Big Other, we\naim to underscore the authenticity of interpersonal emotions, positioning art\nas a bridge for emotional connection and understanding within family dynamics.",
    "categories": [
      "cs.MM",
      "cs.AI"
    ],
    "primary_category": "cs.MM",
    "comment": "8 pages,8 figures, International Symposium on Electronic/Emerging Art\n  (ISEA)",
    "pdf_url": "http://arxiv.org/pdf/2504.04968v1",
    "published_date": "2025-04-07 11:54:11 UTC",
    "updated_date": "2025-04-07 11:54:11 UTC"
  },
  {
    "arxiv_id": "2504.04954v1",
    "title": "GOTHAM: Graph Class Incremental Learning Framework under Weak Supervision",
    "authors": [
      "Aditya Hemant Shahane",
      "Prathosh A. P",
      "Sandeep Kumar"
    ],
    "abstract": "Graphs are growing rapidly, along with the number of distinct label\ncategories associated with them. Applications like e-commerce, healthcare,\nrecommendation systems, and various social media platforms are rapidly moving\ntowards graph representation of data due to their ability to capture both\nstructural and attribute information. One crucial task in graph analysis is\nnode classification, where unlabeled nodes are categorized into predefined\nclasses. In practice, novel classes appear incrementally sometimes with just a\nfew labels (seen classes) or even without any labels (unseen classes), either\nbecause they are new or haven't been explored much. Traditional methods assume\nabundant labeled data for training, which isn't always feasible. We investigate\na broader objective: \\emph{Graph Class Incremental Learning under Weak\nSupervision (GCL)}, addressing this challenge by meta-training on base classes\nwith limited labeled instances. During the incremental streams, novel classes\ncan have few-shot or zero-shot representation. Our proposed framework GOTHAM\nefficiently accommodates these unlabeled nodes by finding the closest prototype\nrepresentation, serving as class representatives in the attribute space. For\nText-Attributed Graphs (TAGs), our framework additionally incorporates semantic\ninformation to enhance the representation. By employing teacher-student\nknowledge distillation to mitigate forgetting, GOTHAM achieves promising\nresults across various tasks. Experiments on datasets such as Cora-ML, Amazon,\nand OBGN-Arxiv showcase the effectiveness of our approach in handling evolving\ngraph data under limited supervision. The repository is available here:\n\\href{https://github.com/adityashahane10/GOTHAM--Graph-based-Class-Incremental-Learning-Framework-under-Weak-Supervision}{\\small\n\\textcolor{blue}{Code}}",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.04954v1",
    "published_date": "2025-04-07 11:39:13 UTC",
    "updated_date": "2025-04-07 11:39:13 UTC"
  },
  {
    "arxiv_id": "2504.04953v1",
    "title": "M-Prometheus: A Suite of Open Multilingual LLM Judges",
    "authors": [
      "José Pombal",
      "Dongkeun Yoon",
      "Patrick Fernandes",
      "Ian Wu",
      "Seungone Kim",
      "Ricardo Rei",
      "Graham Neubig",
      "André F. T. Martins"
    ],
    "abstract": "The use of language models for automatically evaluating long-form text\n(LLM-as-a-judge) is becoming increasingly common, yet most LLM judges are\noptimized exclusively for English, with strategies for enhancing their\nmultilingual evaluation capabilities remaining largely unexplored in the\ncurrent literature. This has created a disparity in the quality of automatic\nevaluation methods for non-English languages, ultimately hindering the\ndevelopment of models with better multilingual capabilities. To bridge this\ngap, we introduce M-Prometheus, a suite of open-weight LLM judges ranging from\n3B to 14B parameters that can provide both direct assessment and pairwise\ncomparison feedback on multilingual outputs. M-Prometheus models outperform\nstate-of-the-art open LLM judges on multilingual reward benchmarks spanning\nmore than 20 languages, as well as on literary machine translation (MT)\nevaluation covering 4 language pairs. Furthermore, M-Prometheus models can be\nleveraged at decoding time to significantly improve generated outputs across\nall 3 tested languages, showcasing their utility for the development of better\nmultilingual models. Lastly, through extensive ablations, we identify the key\nfactors for obtaining an effective multilingual judge, including backbone model\nselection and training on natively multilingual feedback data instead of\ntranslated data. We release our models, training dataset, and code.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.04953v1",
    "published_date": "2025-04-07 11:37:26 UTC",
    "updated_date": "2025-04-07 11:37:26 UTC"
  },
  {
    "arxiv_id": "2504.04949v1",
    "title": "One Quantizer is Enough: Toward a Lightweight Audio Codec",
    "authors": [
      "Linwei Zhai",
      "Han Ding",
      "Cui Zhao",
      "fei wang",
      "Ge Wang",
      "Wang Zhi",
      "Wei Xi"
    ],
    "abstract": "Neural audio codecs have recently gained traction for their ability to\ncompress high-fidelity audio and generate discrete tokens that can be utilized\nin downstream generative modeling tasks. However, leading approaches often rely\non resource-intensive models and multi-quantizer architectures, resulting in\nconsiderable computational overhead and constrained real-world applicability.\nIn this paper, we present SQCodec, a lightweight neural audio codec that\nleverages a single quantizer to address these limitations. SQCodec explores\nstreamlined convolutional networks and local Transformer modules, alongside\nTConv, a novel mechanism designed to capture acoustic variations across\nmultiple temporal scales, thereby enhancing reconstruction fidelity while\nreducing model complexity. Extensive experiments across diverse datasets show\nthat SQCodec achieves audio quality comparable to multi-quantizer baselines,\nwhile its single-quantizer design offers enhanced adaptability and its\nlightweight architecture reduces resource consumption by an order of magnitude.\nThe source code is publicly available at https://github.com/zhai-lw/SQCodec.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "68T07",
      "I.2.m"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.04949v1",
    "published_date": "2025-04-07 11:34:39 UTC",
    "updated_date": "2025-04-07 11:34:39 UTC"
  },
  {
    "arxiv_id": "2504.04945v1",
    "title": "A Llama walks into the 'Bar': Efficient Supervised Fine-Tuning for Legal Reasoning in the Multi-state Bar Exam",
    "authors": [
      "Rean Fernandes",
      "André Biedenkapp",
      "Frank Hutter",
      "Noor Awad"
    ],
    "abstract": "Legal reasoning tasks present unique challenges for large language models\n(LLMs) due to the complexity of domain-specific knowledge and reasoning\nprocesses. This paper investigates how effectively smaller language models\n(Llama 2 7B and Llama 3 8B) can be fine-tuned with a limited dataset of 1,514\nMulti-state Bar Examination (MBE) questions to improve legal question answering\naccuracy. We evaluate these models on the 2022 MBE questions licensed from JD\nAdvising, the same dataset used in the 'GPT-4 passes the Bar exam' study. Our\nmethodology involves collecting approximately 200 questions per legal domain\nacross 7 domains. We distill the dataset using Llama 3 (70B) to transform\nexplanations into a structured IRAC (Issue, Rule, Application, Conclusion)\nformat as a guided reasoning process to see if it results in better performance\nover the non-distilled dataset. We compare the non-fine-tuned models against\ntheir supervised fine-tuned (SFT) counterparts, trained for different sample\nsizes per domain, to study the effect on accuracy and prompt adherence. We also\nanalyse option selection biases and their mitigation following SFT. In\naddition, we consolidate the performance across multiple variables: prompt type\n(few-shot vs zero-shot), answer ordering (chosen-option first vs\ngenerated-explanation first), response format (Numbered list vs Markdown vs\nJSON), and different decoding temperatures. Our findings show that\ndomain-specific SFT helps some model configurations achieve close to human\nbaseline performance, despite limited computational resources and a relatively\nsmall dataset. We release both the gathered SFT dataset and the family of\nSupervised Fine-tuned (SFT) adapters optimised for MBE performance. This\nestablishes a practical lower bound on resources needed towards achieving\neffective legal question answering in smaller LLMs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "I.2.7; I.2.1"
    ],
    "primary_category": "cs.LG",
    "comment": "COLM 2025 preprint, 9 pages, 3 figures, 16 appendix pages",
    "pdf_url": "http://arxiv.org/pdf/2504.04945v1",
    "published_date": "2025-04-07 11:31:22 UTC",
    "updated_date": "2025-04-07 11:31:22 UTC"
  },
  {
    "arxiv_id": "2504.04942v1",
    "title": "Lemmanaid: Neuro-Symbolic Lemma Conjecturing",
    "authors": [
      "Yousef Alhessi",
      "Sólrún Halla Einarsdóttir",
      "George Granberry",
      "Emily First",
      "Moa Johansson",
      "Sorin Lerner",
      "Nicholas Smallbone"
    ],
    "abstract": "Automatically conjecturing useful, interesting and novel lemmas would greatly\nimprove automated reasoning tools and lower the bar for formalizing mathematics\nin proof assistants. It is however a very challenging task for both neural and\nsymbolic approaches. We present the first steps towards a practical\nneuro-symbolic lemma conjecturing tool, Lemmanaid, that combines Large Language\nModels (LLMs) and symbolic methods, and evaluate it on proof libraries for the\nIsabelle proof assistant. We train an LLM to generate lemma templates that\ndescribe the shape of a lemma, and use symbolic methods to fill in the details.\nWe compare Lemmanaid against an LLM trained to generate complete lemma\nstatements as well as previous fully symbolic conjecturing methods. Our results\nindicate that neural and symbolic techniques are complementary. By leveraging\nthe best of both symbolic and neural methods we can generate useful lemmas for\na wide range of input domains, facilitating computer-assisted theory\ndevelopment and formalization.",
    "categories": [
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.04942v1",
    "published_date": "2025-04-07 11:30:36 UTC",
    "updated_date": "2025-04-07 11:30:36 UTC"
  },
  {
    "arxiv_id": "2504.04939v2",
    "title": "A Taxonomy of Self-Handover",
    "authors": [
      "Naoki Wake",
      "Atsushi Kanehira",
      "Kazuhiro Sasabuchi",
      "Jun Takamatsu",
      "Katsushi Ikeuchi"
    ],
    "abstract": "Self-handover, transferring an object between one's own hands, is a common\nbut understudied bimanual action. While it facilitates seamless transitions in\ncomplex tasks, the strategies underlying its execution remain largely\nunexplored. Here, we introduce the first systematic taxonomy of self-handover,\nderived from manual annotation of over 12 hours of cooking activity performed\nby 21 participants. Our analysis reveals that self-handover is not merely a\npassive transition, but a highly coordinated action involving anticipatory\nadjustments by both hands. As a step toward automated analysis of human\nmanipulation, we further demonstrate the feasibility of classifying\nself-handover types using a state-of-the-art vision-language model. These\nfindings offer fresh insights into bimanual coordination, underscoring the role\nof self-handover in enabling smooth task transitions-an ability essential for\nadaptive dual-arm robotics.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages, 8 figures, 1 table, Last updated on April 7th, 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.04939v2",
    "published_date": "2025-04-07 11:21:42 UTC",
    "updated_date": "2025-04-08 10:18:43 UTC"
  },
  {
    "arxiv_id": "2504.04935v1",
    "title": "RCCFormer: A Robust Crowd Counting Network Based on Transformer",
    "authors": [
      "Peng Liu",
      "Heng-Chao Li",
      "Sen Lei",
      "Nanqing Liu",
      "Bin Feng",
      "Xiao Wu"
    ],
    "abstract": "Crowd counting, which is a key computer vision task, has emerged as a\nfundamental technology in crowd analysis and public safety management. However,\nchallenges such as scale variations and complex backgrounds significantly\nimpact the accuracy of crowd counting. To mitigate these issues, this paper\nproposes a robust Transformer-based crowd counting network, termed RCCFormer,\nspecifically designed for background suppression and scale awareness. The\nproposed method incorporates a Multi-level Feature Fusion Module (MFFM), which\nmeticulously integrates features extracted at diverse stages of the backbone\narchitecture. It establishes a strong baseline capable of capturing intricate\nand comprehensive feature representations, surpassing traditional baselines.\nFurthermore, the introduced Detail-Embedded Attention Block (DEAB) captures\ncontextual information and local details through global self-attention and\nlocal attention along with a learnable manner for efficient fusion. This\nenhances the model's ability to focus on foreground regions while effectively\nmitigating background noise interference. Additionally, we develop an Adaptive\nScale-Aware Module (ASAM), with our novel Input-dependent Deformable\nConvolution (IDConv) as its fundamental building block. This module dynamically\nadapts to changes in head target shapes and scales, significantly improving the\nnetwork's capability to accommodate large-scale variations. The effectiveness\nof the proposed method is validated on the ShanghaiTech Part_A and Part_B,\nNWPU-Crowd, and QNRF datasets. The results demonstrate that our RCCFormer\nachieves excellent performance across all four datasets, showcasing\nstate-of-the-art outcomes.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.04935v1",
    "published_date": "2025-04-07 11:19:05 UTC",
    "updated_date": "2025-04-07 11:19:05 UTC"
  },
  {
    "arxiv_id": "2504.04934v1",
    "title": "Boosting Relational Deep Learning with Pretrained Tabular Models",
    "authors": [
      "Veronica Lachi",
      "Antonio Longa",
      "Beatrice Bevilacqua",
      "Bruno Lepri",
      "Andrea Passerini",
      "Bruno Ribeiro"
    ],
    "abstract": "Relational databases, organized into tables connected by primary-foreign key\nrelationships, are a common format for organizing data. Making predictions on\nrelational data often involves transforming them into a flat tabular format\nthrough table joins and feature engineering, which serve as input to tabular\nmethods. However, designing features that fully capture complex relational\npatterns remains challenging. Graph Neural Networks (GNNs) offer a compelling\nalternative by inherently modeling these relationships, but their time overhead\nduring inference limits their applicability for real-time scenarios. In this\nwork, we aim to bridge this gap by leveraging existing feature engineering\nefforts to enhance the efficiency of GNNs in relational databases.\nSpecifically, we use GNNs to capture complex relationships within relational\ndatabases, patterns that are difficult to featurize, while employing engineered\nfeatures to encode temporal information, thereby avoiding the need to retain\nthe entire historical graph and enabling the use of smaller, more efficient\ngraphs. Our \\textsc{LightRDL} approach not only improves efficiency, but also\noutperforms existing models. Experimental results on the RelBench benchmark\ndemonstrate that our framework achieves up to $33\\%$ performance improvement\nand a $526\\times$ inference speedup compared to GNNs, making it highly suitable\nfor real-time inference.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.04934v1",
    "published_date": "2025-04-07 11:19:04 UTC",
    "updated_date": "2025-04-07 11:19:04 UTC"
  },
  {
    "arxiv_id": "2504.04921v1",
    "title": "Expectations vs Reality -- A Secondary Study on AI Adoption in Software Testing",
    "authors": [
      "Katja Karhu",
      "Jussi Kasurinen",
      "Kari Smolander"
    ],
    "abstract": "In the software industry, artificial intelligence (AI) has been utilized more\nand more in software development activities. In some activities, such as\ncoding, AI has already been an everyday tool, but in software testing\nactivities AI it has not yet made a significant breakthrough. In this paper,\nthe objective was to identify what kind of empirical research with industry\ncontext has been conducted on AI in software testing, as well as how AI has\nbeen adopted in software testing practice. To achieve this, we performed a\nsystematic mapping study of recent (2020 and later) studies on AI adoption in\nsoftware testing in the industry, and applied thematic analysis to identify\ncommon themes and categories, such as the real-world use cases and benefits, in\nthe found papers. The observations suggest that AI is not yet heavily utilized\nin software testing, and still relatively few studies on AI adoption in\nsoftware testing have been conducted in the industry context to solve\nreal-world problems. Earlier studies indicated there was a noticeable gap\nbetween the actual use cases and actual benefits versus the expectations, which\nwe analyzed further. While there were numerous potential use cases for AI in\nsoftware testing, such as test case generation, code analysis, and intelligent\ntest automation, the reported actual implementations and observed benefits were\nlimited. In addition, the systematic mapping study revealed a potential problem\nwith false positive search results in online databases when using the search\nstring \"artificial intelligence\".",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "26 pages, 1 figure, submitted to Software Testing, Vertification and\n  Reliability journal",
    "pdf_url": "http://arxiv.org/pdf/2504.04921v1",
    "published_date": "2025-04-07 11:03:54 UTC",
    "updated_date": "2025-04-07 11:03:54 UTC"
  },
  {
    "arxiv_id": "2504.04918v1",
    "title": "Constitution or Collapse? Exploring Constitutional AI with Llama 3-8B",
    "authors": [
      "Xue Zhang"
    ],
    "abstract": "As language models continue to grow larger, the cost of acquiring\nhigh-quality training data has increased significantly. Collecting human\nfeedback is both expensive and time-consuming, and manual labels can be noisy,\nleading to an imbalance between helpfulness and harmfulness. Constitutional AI,\nintroduced by Anthropic in December 2022, uses AI to provide feedback to\nanother AI, greatly reducing the need for human labeling. However, the original\nimplementation was designed for a model with around 52 billion parameters, and\nthere is limited information on how well Constitutional AI performs with\nsmaller models, such as LLaMA 3-8B. In this paper, we replicated the\nConstitutional AI workflow using the smaller LLaMA 3-8B model. Our results show\nthat Constitutional AI can effectively increase the harmlessness of the model,\nreducing the Attack Success Rate in MT-Bench by 40.8%. However, similar to the\noriginal study, increasing harmlessness comes at the cost of helpfulness. The\nhelpfulness metrics, which are an average of the Turn 1 and Turn 2 scores,\ndropped by 9.8% compared to the baseline. Additionally, we observed clear signs\nof model collapse in the final DPO-CAI model, indicating that smaller models\nmay struggle with self-improvement due to insufficient output quality, making\neffective fine-tuning more challenging. Our study suggests that, like reasoning\nand math ability, self-improvement is an emergent property.",
    "categories": [
      "cs.AI",
      "68T05, 68T50",
      "I.2.6; I.2.7; I.2.1"
    ],
    "primary_category": "cs.AI",
    "comment": "6 pages, 2 figures. Conducted as part of research on alignment\n  techniques for language models",
    "pdf_url": "http://arxiv.org/pdf/2504.04918v1",
    "published_date": "2025-04-07 11:01:25 UTC",
    "updated_date": "2025-04-07 11:01:25 UTC"
  },
  {
    "arxiv_id": "2504.04915v1",
    "title": "Collab-RAG: Boosting Retrieval-Augmented Generation for Complex Question Answering via White-Box and Black-Box LLM Collaboration",
    "authors": [
      "Ran Xu",
      "Wenqi Shi",
      "Yuchen Zhuang",
      "Yue Yu",
      "Joyce C. Ho",
      "Haoyu Wang",
      "Carl Yang"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) systems often struggle to handle\nmulti-hop question-answering tasks accurately due to irrelevant context\nretrieval and limited complex reasoning capabilities. We introduce Collab-RAG,\na collaborative training framework that leverages mutual enhancement between a\nwhite-box small language model (SLM) and a blackbox large language model (LLM)\nfor RAG. Specifically, the SLM decomposes complex queries into simpler\nsub-questions, thus enhancing the accuracy of the retrieval and facilitating\nmore effective reasoning by the black-box LLM. Concurrently, the black-box LLM\nprovides feedback signals to improve the SLM's decomposition capability. We\nobserve that Collab-RAG relies solely on supervision from an affordable\nblack-box LLM without additional distillation from frontier LLMs, yet\ndemonstrates strong generalization across multiple black-box LLMs. Experimental\nevaluations across five multi-hop QA datasets demonstrate that Collab-RAG\nsubstantially outperforms existing black-box-only and SLM fine-tuning baselines\nby 1.8%-14.2% on average. In particular, our fine-tuned 3B SLM surpasses a\nfrozen 32B LLM in question decomposition, highlighting the efficiency of\nCollab-RAG in improving reasoning and retrieval for complex questions. The code\nof Collab-RAG is available on https://github.com/ritaranx/Collab-RAG/.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Work in progress. Code: https://github.com/ritaranx/Collab-RAG/",
    "pdf_url": "http://arxiv.org/pdf/2504.04915v1",
    "published_date": "2025-04-07 10:52:22 UTC",
    "updated_date": "2025-04-07 10:52:22 UTC"
  },
  {
    "arxiv_id": "2504.04909v1",
    "title": "AlgOS: Algorithm Operating System",
    "authors": [
      "Llewyn Salt",
      "Marcus Gallagher"
    ],
    "abstract": "Algorithm Operating System (AlgOS) is an unopinionated, extensible, modular\nframework for algorithmic implementations. AlgOS offers numerous features:\nintegration with Optuna for automated hyperparameter tuning; automated argument\nparsing for generic command-line interfaces; automated registration of new\nclasses; and a centralised database for logging experiments and studies. These\nfeatures are designed to reduce the overhead of implementing new algorithms and\nto standardise the comparison of algorithms. The standardisation of algorithmic\nimplementations is crucial for reproducibility and reliability in research.\nAlgOS combines Abstract Syntax Trees with a novel implementation of the\nObserver pattern to control the logical flow of algorithmic segments.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.04909v1",
    "published_date": "2025-04-07 10:36:46 UTC",
    "updated_date": "2025-04-07 10:36:46 UTC"
  },
  {
    "arxiv_id": "2504.04907v1",
    "title": "Video-Bench: Human-Aligned Video Generation Benchmark",
    "authors": [
      "Hui Han",
      "Siyuan Li",
      "Jiaqi Chen",
      "Yiwen Yuan",
      "Yuling Wu",
      "Chak Tou Leong",
      "Hanwen Du",
      "Junchen Fu",
      "Youhua Li",
      "Jie Zhang",
      "Chi Zhang",
      "Li-jia Li",
      "Yongxin Ni"
    ],
    "abstract": "Video generation assessment is essential for ensuring that generative models\nproduce visually realistic, high-quality videos while aligning with human\nexpectations. Current video generation benchmarks fall into two main\ncategories: traditional benchmarks, which use metrics and embeddings to\nevaluate generated video quality across multiple dimensions but often lack\nalignment with human judgments; and large language model (LLM)-based\nbenchmarks, though capable of human-like reasoning, are constrained by a\nlimited understanding of video quality metrics and cross-modal consistency. To\naddress these challenges and establish a benchmark that better aligns with\nhuman preferences, this paper introduces Video-Bench, a comprehensive benchmark\nfeaturing a rich prompt suite and extensive evaluation dimensions. This\nbenchmark represents the first attempt to systematically leverage MLLMs across\nall dimensions relevant to video generation assessment in generative models. By\nincorporating few-shot scoring and chain-of-query techniques, Video-Bench\nprovides a structured, scalable approach to generated video evaluation.\nExperiments on advanced models including Sora demonstrate that Video-Bench\nachieves superior alignment with human preferences across all dimensions.\nMoreover, in instances where our framework's assessments diverge from human\nevaluations, it consistently offers more objective and accurate insights,\nsuggesting an even greater potential advantage over traditional human judgment.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by CVPR'25",
    "pdf_url": "http://arxiv.org/pdf/2504.04907v1",
    "published_date": "2025-04-07 10:32:42 UTC",
    "updated_date": "2025-04-07 10:32:42 UTC"
  },
  {
    "arxiv_id": "2504.04903v2",
    "title": "Lumina-OmniLV: A Unified Multimodal Framework for General Low-Level Vision",
    "authors": [
      "Yuandong Pu",
      "Le Zhuo",
      "Kaiwen Zhu",
      "Liangbin Xie",
      "Wenlong Zhang",
      "Xiangyu Chen",
      "Peng Gao",
      "Yu Qiao",
      "Chao Dong",
      "Yihao Liu"
    ],
    "abstract": "We present Lunima-OmniLV (abbreviated as OmniLV), a universal multimodal\nmulti-task framework for low-level vision that addresses over 100 sub-tasks\nacross four major categories: image restoration, image enhancement,\nweak-semantic dense prediction, and stylization. OmniLV leverages both textual\nand visual prompts to offer flexible and user-friendly interactions. Built on\nDiffusion Transformer (DiT)-based generative priors, our framework supports\narbitrary resolutions -- achieving optimal performance at 1K resolution --\nwhile preserving fine-grained details and high fidelity. Through extensive\nexperiments, we demonstrate that separately encoding text and visual\ninstructions, combined with co-training using shallow feature control, is\nessential to mitigate task ambiguity and enhance multi-task generalization. Our\nfindings also reveal that integrating high-level generative tasks into\nlow-level vision models can compromise detail-sensitive restoration. These\ninsights pave the way for more robust and generalizable low-level vision\nsystems.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.04903v2",
    "published_date": "2025-04-07 10:22:00 UTC",
    "updated_date": "2025-04-08 07:26:50 UTC"
  },
  {
    "arxiv_id": "2504.04893v1",
    "title": "SCAM: A Real-World Typographic Robustness Evaluation for Multimodal Foundation Models",
    "authors": [
      "Justus Westerhoff",
      "Erblina Purellku",
      "Jakob Hackstein",
      "Leo Pinetzki",
      "Lorenz Hufe"
    ],
    "abstract": "Typographic attacks exploit the interplay between text and visual content in\nmultimodal foundation models, causing misclassifications when misleading text\nis embedded within images. However, existing datasets are limited in size and\ndiversity, making it difficult to study such vulnerabilities. In this paper, we\nintroduce SCAM, the largest and most diverse dataset of real-world typographic\nattack images to date, containing 1,162 images across hundreds of object\ncategories and attack words. Through extensive benchmarking of Vision-Language\nModels (VLMs) on SCAM, we demonstrate that typographic attacks significantly\ndegrade performance, and identify that training data and model architecture\ninfluence the susceptibility to these attacks. Our findings reveal that\ntypographic attacks persist in state-of-the-art Large Vision-Language Models\n(LVLMs) due to the choice of their vision encoder, though larger Large Language\nModels (LLMs) backbones help mitigate their vulnerability. Additionally, we\ndemonstrate that synthetic attacks closely resemble real-world (handwritten)\nattacks, validating their use in research. Our work provides a comprehensive\nresource and empirical insights to facilitate future research toward robust and\ntrustworthy multimodal AI systems. We publicly release the datasets introduced\nin this paper under https://huggingface.co/datasets/BLISS-e-V/SCAM, along with\nthe code for evaluations at https://github.com/Bliss-e-V/SCAM.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Submitted to CVPR 2025 Workshop EVAL-FoMo-2",
    "pdf_url": "http://arxiv.org/pdf/2504.04893v1",
    "published_date": "2025-04-07 10:01:38 UTC",
    "updated_date": "2025-04-07 10:01:38 UTC"
  },
  {
    "arxiv_id": "2504.04874v1",
    "title": "Futureproof Static Memory Planning",
    "authors": [
      "Christos Lamprakos",
      "Panagiotis Xanthopoulos",
      "Manolis Katsaragakis",
      "Sotirios Xydis",
      "Dimitrios Soudris",
      "Francky Catthoor"
    ],
    "abstract": "The NP-complete combinatorial optimization task of assigning offsets to a set\nof buffers with known sizes and lifetimes so as to minimize total memory usage\nis called dynamic storage allocation (DSA). Existing DSA implementations bypass\nthe theoretical state-of-the-art algorithms in favor of either fast but\nwasteful heuristics, or memory-efficient approaches that do not scale beyond\none thousand buffers. The \"AI memory wall\", combined with deep neural networks'\nstatic architecture, has reignited interest in DSA. We present idealloc, a\nlow-fragmentation, high-performance DSA implementation designed for\nmillion-buffer instances. Evaluated on a novel suite of particularly hard\nbenchmarks from several domains, idealloc ranks first against four production\nimplementations in terms of a joint effectiveness/robustness criterion.",
    "categories": [
      "cs.OS",
      "cs.AI",
      "cs.PL"
    ],
    "primary_category": "cs.OS",
    "comment": "Submitted to ACM TOPLAS",
    "pdf_url": "http://arxiv.org/pdf/2504.04874v1",
    "published_date": "2025-04-07 09:28:54 UTC",
    "updated_date": "2025-04-07 09:28:54 UTC"
  },
  {
    "arxiv_id": "2504.04867v1",
    "title": "FedSAUC: A Similarity-Aware Update Control for Communication-Efficient Federated Learning in Edge Computing",
    "authors": [
      "Ming-Lun Lee",
      "Han-Chang Chou",
      "Yan-Ann~Chen"
    ],
    "abstract": "Federated learning is a distributed machine learning framework to\ncollaboratively train a global model without uploading privacy-sensitive data\nonto a centralized server. Usually, this framework is applied to edge devices\nsuch as smartphones, wearable devices, and Internet of Things (IoT) devices\nwhich closely collect information from users. However, these devices are mostly\nbattery-powered. The update procedure of federated learning will constantly\nconsume the battery power and the transmission bandwidth. In this work, we\npropose an update control for federated learning, FedSAUC, by considering the\nsimilarity of users' behaviors (models). At the server side, we exploit\nclustering algorithms to group devices with similar models. Then we select some\nrepresentatives for each cluster to update information to train the model. We\nalso implemented a testbed prototyping on edge devices for validating the\nperformance. The experimental results show that this update control will not\naffect the training accuracy in the long run.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published in the Proceedings of the International Conference on\n  Mobile Computing and Ubiquitous Network (ICMU), 2021",
    "pdf_url": "http://arxiv.org/pdf/2504.04867v1",
    "published_date": "2025-04-07 09:21:43 UTC",
    "updated_date": "2025-04-07 09:21:43 UTC"
  },
  {
    "arxiv_id": "2504.04862v1",
    "title": "GAMDTP: Dynamic Trajectory Prediction with Graph Attention Mamba Network",
    "authors": [
      "Yunxiang Liu",
      "Hongkuo Niu",
      "Jianlin Zhu"
    ],
    "abstract": "Accurate motion prediction of traffic agents is crucial for the safety and\nstability of autonomous driving systems. In this paper, we introduce GAMDTP, a\nnovel graph attention-based network tailored for dynamic trajectory prediction.\nSpecifically, we fuse the result of self attention and mamba-ssm through a gate\nmechanism, leveraging the strengths of both to extract features more\nefficiently and accurately, in each graph convolution layer. GAMDTP encodes the\nhigh-definition map(HD map) data and the agents' historical trajectory\ncoordinates and decodes the network's output to generate the final prediction\nresults. Additionally, recent approaches predominantly focus on dynamically\nfusing historical forecast results and rely on two-stage frameworks including\nproposal and refinement. To further enhance the performance of the two-stage\nframeworks we also design a scoring mechanism to evaluate the prediction\nquality during the proposal and refinement processes. Experiments on the\nArgoverse dataset demonstrates that GAMDTP achieves state-of-the-art\nperformance, achieving superior accuracy in dynamic trajectory prediction.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.04862v1",
    "published_date": "2025-04-07 09:19:20 UTC",
    "updated_date": "2025-04-07 09:19:20 UTC"
  },
  {
    "arxiv_id": "2504.04861v1",
    "title": "SAFT: Structure-aware Transformers for Textual Interaction Classification",
    "authors": [
      "Hongtao Wang",
      "Renchi Yang",
      "Hewen Wang",
      "Haoran Zheng",
      "Jianliang Xu"
    ],
    "abstract": "Textual interaction networks (TINs) are an omnipresent data structure used to\nmodel the interplay between users and items on e-commerce websites, social\nnetworks, etc., where each interaction is associated with a text description.\nClassifying such textual interactions (TIC) finds extensive use in detecting\nspam reviews in e-commerce, fraudulent transactions in finance, and so on.\nExisting TIC solutions either (i) fail to capture the rich text semantics due\nto the use of context-free text embeddings, and/or (ii) disregard the bipartite\nstructure and node heterogeneity of TINs, leading to compromised TIC\nperformance. In this work, we propose SAFT, a new architecture that integrates\nlanguage- and graph-based modules for the effective fusion of textual and\nstructural semantics in the representation learning of interactions. In\nparticular, line graph attention (LGA)/gated attention units (GAUs) and\npretrained language models (PLMs) are capitalized on to model the\ninteraction-level and token-level signals, which are further coupled via the\nproxy token in an iterative and contextualized fashion. Additionally, an\nefficient and theoretically-grounded approach is developed to encode the local\nand global topology information pertaining to interactions into structural\nembeddings. The resulting embeddings not only inject the structural features\nunderlying TINs into the textual interaction encoding but also facilitate the\ndesign of graph sampling strategies. Extensive empirical evaluations on\nmultiple real TIN datasets demonstrate the superiority of SAFT over the\nstate-of-the-art baselines in TIC accuracy.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.04861v1",
    "published_date": "2025-04-07 09:19:12 UTC",
    "updated_date": "2025-04-07 09:19:12 UTC"
  },
  {
    "arxiv_id": "2504.04858v1",
    "title": "Don't Lag, RAG: Training-Free Adversarial Detection Using RAG",
    "authors": [
      "Roie Kazoom",
      "Raz Lapid",
      "Moshe Sipper",
      "Ofer Hadar"
    ],
    "abstract": "Adversarial patch attacks pose a major threat to vision systems by embedding\nlocalized perturbations that mislead deep models. Traditional defense methods\noften require retraining or fine-tuning, making them impractical for real-world\ndeployment. We propose a training-free Visual Retrieval-Augmented Generation\n(VRAG) framework that integrates Vision-Language Models (VLMs) for adversarial\npatch detection. By retrieving visually similar patches and images that\nresemble stored attacks in a continuously expanding database, VRAG performs\ngenerative reasoning to identify diverse attack types, all without additional\ntraining or fine-tuning. We extensively evaluate open-source large-scale VLMs,\nincluding Qwen-VL-Plus, Qwen2.5-VL-72B, and UI-TARS-72B-DPO, alongside\nGemini-2.0, a closed-source model. Notably, the open-source UI-TARS-72B-DPO\nmodel achieves up to 95 percent classification accuracy, setting a new\nstate-of-the-art for open-source adversarial patch detection. Gemini-2.0\nattains the highest overall accuracy, 98 percent, but remains closed-source.\nExperimental results demonstrate VRAG's effectiveness in identifying a variety\nof adversarial patches with minimal human annotation, paving the way for\nrobust, practical defenses against evolving adversarial patch attacks.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.04858v1",
    "published_date": "2025-04-07 09:14:47 UTC",
    "updated_date": "2025-04-07 09:14:47 UTC"
  },
  {
    "arxiv_id": "2504.04855v1",
    "title": "BIASINSPECTOR: Detecting Bias in Structured Data through LLM Agents",
    "authors": [
      "Haoxuan Li",
      "Mingyu Derek Ma",
      "Jen-tse Huang",
      "Zhaotian Weng",
      "Wei Wang",
      "Jieyu Zhao"
    ],
    "abstract": "Detecting biases in structured data is a complex and time-consuming task.\nExisting automated techniques are limited in diversity of data types and\nheavily reliant on human case-by-case handling, resulting in a lack of\ngeneralizability. Currently, large language model (LLM)-based agents have made\nsignificant progress in data science, but their ability to detect data biases\nis still insufficiently explored. To address this gap, we introduce the first\nend-to-end, multi-agent synergy framework, BIASINSPECTOR, designed for\nautomatic bias detection in structured data based on specific user\nrequirements. It first develops a multi-stage plan to analyze user-specified\nbias detection tasks and then implements it with a diverse and well-suited set\nof tools. It delivers detailed results that include explanations and\nvisualizations. To address the lack of a standardized framework for evaluating\nthe capability of LLM agents to detect biases in data, we further propose a\ncomprehensive benchmark that includes multiple evaluation metrics and a large\nset of test cases. Extensive experiments demonstrate that our framework\nachieves exceptional overall performance in structured data bias detection,\nsetting a new milestone for fairer data applications.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "21 pages,6 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.04855v1",
    "published_date": "2025-04-07 09:12:00 UTC",
    "updated_date": "2025-04-07 09:12:00 UTC"
  },
  {
    "arxiv_id": "2504.04850v1",
    "title": "An Efficient Approach for Cooperative Multi-Agent Learning Problems",
    "authors": [
      "Ángel Aso-Mollar",
      "Eva Onaindia"
    ],
    "abstract": "In this article, we propose a centralized Multi-Agent Learning framework for\nlearning a policy that models the simultaneous behavior of multiple agents that\nneed to coordinate to solve a certain task. Centralized approaches often suffer\nfrom the explosion of an action space that is defined by all possible\ncombinations of individual actions, known as joint actions. Our approach\naddresses the coordination problem via a sequential abstraction, which\novercomes the scalability problems typical to centralized methods. It\nintroduces a meta-agent, called \\textit{supervisor}, which abstracts joint\nactions as sequential assignments of actions to each agent. This sequential\nabstraction not only simplifies the centralized joint action space but also\nenhances the framework's scalability and efficiency. Our experimental results\ndemonstrate that the proposed approach successfully coordinates agents across a\nvariety of Multi-Agent Learning environments of diverse sizes.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at ICTAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2504.04850v1",
    "published_date": "2025-04-07 09:03:35 UTC",
    "updated_date": "2025-04-07 09:03:35 UTC"
  },
  {
    "arxiv_id": "2504.04833v1",
    "title": "Explanation-Driven Interventions for Artificial Intelligence Model Customization: Empowering End-Users to Tailor Black-Box AI in Rhinocytology",
    "authors": [
      "Andrea Esposito",
      "Miriana Calvano",
      "Antonio Curci",
      "Francesco Greco",
      "Rosa Lanzilotti",
      "Antonio Piccinno"
    ],
    "abstract": "The integration of Artificial Intelligence (AI) in modern society is heavily\nshifting the way that individuals carry out their tasks and activities.\nEmploying AI-based systems raises challenges that designers and developers must\naddress to ensure that humans remain in control of the interaction process,\nparticularly in high-risk domains. This article presents a novel End-User\nDevelopment (EUD) approach for black-box AI models through a redesigned user\ninterface in the Rhino-Cyt platform, a medical AI-based decision-support system\nfor medical professionals (more precisely, rhinocytologists) to carry out cell\nclassification. The proposed interface empowers users to intervene in AI\ndecision-making process by editing explanations and reconfiguring the model,\ninfluencing its future predictions. This work contributes to Human-Centered AI\n(HCAI) and EUD by discussing how explanation-driven interventions allow a blend\nof explainability, user intervention, and model reconfiguration, fostering a\nsymbiosis between humans and user-tailored AI systems.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "First version (14 pages, 12 of content that will be reduced to 8 in\n  the near future)",
    "pdf_url": "http://arxiv.org/pdf/2504.04833v1",
    "published_date": "2025-04-07 08:44:48 UTC",
    "updated_date": "2025-04-07 08:44:48 UTC"
  },
  {
    "arxiv_id": "2504.04827v1",
    "title": "From Specificity to Generality: Revisiting Generalizable Artifacts in Detecting Face Deepfakes",
    "authors": [
      "Long Ma",
      "Zhiyuan Yan",
      "Yize Chen",
      "Jin Xu",
      "Qinglang Guo",
      "Hu Huang",
      "Yong Liao",
      "Hui Lin"
    ],
    "abstract": "Detecting deepfakes has been an increasingly important topic, especially\ngiven the rapid development of AI generation techniques. In this paper, we ask:\nHow can we build a universal detection framework that is effective for most\nfacial deepfakes? One significant challenge is the wide variety of deepfake\ngenerators available, resulting in varying forgery artifacts (e.g., lighting\ninconsistency, color mismatch, etc). But should we ``teach\" the detector to\nlearn all these artifacts separately? It is impossible and impractical to\nelaborate on them all. So the core idea is to pinpoint the more common and\ngeneral artifacts across different deepfakes. Accordingly, we categorize\ndeepfake artifacts into two distinct yet complementary types: Face\nInconsistency Artifacts (FIA) and Up-Sampling Artifacts (USA). FIA arise from\nthe challenge of generating all intricate details, inevitably causing\ninconsistencies between the complex facial features and relatively uniform\nsurrounding areas. USA, on the other hand, are the inevitable traces left by\nthe generator's decoder during the up-sampling process. This categorization\nstems from the observation that all existing deepfakes typically exhibit one or\nboth of these artifacts. To achieve this, we propose a new data-level\npseudo-fake creation framework that constructs fake samples with only the FIA\nand USA, without introducing extra less-general artifacts. Specifically, we\nemploy a super-resolution to simulate the USA, while design a Blender module\nthat uses image-level self-blending on diverse facial regions to create the\nFIA. We surprisingly found that, with this intuitive design, a standard image\nclassifier trained only with our pseudo-fake data can non-trivially generalize\nwell to unseen deepfakes.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.04827v1",
    "published_date": "2025-04-07 08:34:28 UTC",
    "updated_date": "2025-04-07 08:34:28 UTC"
  },
  {
    "arxiv_id": "2504.04823v1",
    "title": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning Models",
    "authors": [
      "Ruikang Liu",
      "Yuxuan Sun",
      "Manyi Zhang",
      "Haoli Bai",
      "Xianzhi Yu",
      "Tiezheng Yu",
      "Chun Yuan",
      "Lu Hou"
    ],
    "abstract": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this study, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, and QwQ-32B. Our investigation covers weight, KV cache, and\nactivation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes will be open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.04823v1",
    "published_date": "2025-04-07 08:22:45 UTC",
    "updated_date": "2025-04-07 08:22:45 UTC"
  },
  {
    "arxiv_id": "2504.04821v1",
    "title": "A Customized SAT-based Solver for Graph Coloring",
    "authors": [
      "Timo Brand",
      "Daniel Faber",
      "Stephan Held",
      "Petra Mutzel"
    ],
    "abstract": "We introduce ZykovColor, a novel SAT-based algorithm to solve the graph\ncoloring problem working on top of an encoding that mimics the Zykov tree. Our\nmethod is based on an approach of H\\'ebrard and Katsirelos (2020) that employs\na propagator to enforce transitivity constraints, incorporate lower bounds for\nsearch tree pruning, and enable inferred propagations. We leverage the recently\nintroduced IPASIR-UP interface for CaDiCal to implement these techniques with a\nSAT solver. Furthermore, we propose new features that take advantage of the\nunderlying SAT solver. These include modifying the integrated decision strategy\nwith vertex domination hints and using incremental bottom-up search that allows\nto reuse learned clauses from previous calls. Additionally, we integrate a more\nefficient clique computation to improve the lower bounds during the search. We\nvalidate the effectiveness of each new feature through an experimental\nanalysis. ZykovColor outperforms other state-of-the-art graph coloring\nimplementations on the DIMACS benchmark set. Further experiments on random\nErd\\H{o}s-R\\'enyi graphs show that our new approach dominates state-of-the-art\nSAT-based methods for both very sparse and highly dense graphs.",
    "categories": [
      "cs.DM",
      "cs.AI",
      "cs.DS",
      "cs.LO",
      "05C15",
      "G.2.2"
    ],
    "primary_category": "cs.DM",
    "comment": "5 figures, 2 tables, source code published at\n  https://github.com/trewes/ZykovColor",
    "pdf_url": "http://arxiv.org/pdf/2504.04821v1",
    "published_date": "2025-04-07 08:22:00 UTC",
    "updated_date": "2025-04-07 08:22:00 UTC"
  },
  {
    "arxiv_id": "2504.04808v1",
    "title": "ELT-Bench: An End-to-End Benchmark for Evaluating AI Agents on ELT Pipelines",
    "authors": [
      "Tengjun Jin",
      "Yuxuan Zhu",
      "Daniel Kang"
    ],
    "abstract": "Practitioners are increasingly turning to Extract-Load-Transform (ELT)\npipelines with the widespread adoption of cloud data warehouses. However,\ndesigning these pipelines often involves significant manual work to ensure\ncorrectness. Recent advances in AI-based methods, which have shown strong\ncapabilities in data tasks, such as text-to-SQL, present an opportunity to\nalleviate manual efforts in developing ELT pipelines. Unfortunately, current\nbenchmarks in data engineering only evaluate isolated tasks, such as using data\ntools and writing data transformation queries, leaving a significant gap in\nevaluating AI agents for generating end-to-end ELT pipelines.\n  To fill this gap, we introduce ELT-Bench, an end-to-end benchmark designed to\nassess the capabilities of AI agents to build ELT pipelines. ELT-Bench consists\nof 100 pipelines, including 835 source tables and 203 data models across\nvarious domains. By simulating realistic scenarios involving the integration of\ndiverse data sources and the use of popular data tools, ELT-Bench evaluates AI\nagents' abilities in handling complex data engineering workflows. AI agents\nmust interact with databases and data tools, write code and SQL queries, and\norchestrate every pipeline stage. We evaluate two representative code agent\nframeworks, Spider-Agent and SWE-Agent, using six popular Large Language Models\n(LLMs) on ELT-Bench. The highest-performing agent, Spider-Agent\nClaude-3.7-Sonnet with extended thinking, correctly generates only 3.9% of data\nmodels, with an average cost of $4.30 and 89.3 steps per pipeline. Our\nexperimental results demonstrate the challenges of ELT-Bench and highlight the\nneed for a more advanced AI agent to reduce manual effort in ELT workflows. Our\ncode and data are available at https://github.com/uiuc-kang-lab/ETL.git.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "14 pages, 18 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.04808v1",
    "published_date": "2025-04-07 08:03:36 UTC",
    "updated_date": "2025-04-07 08:03:36 UTC"
  },
  {
    "arxiv_id": "2504.04789v1",
    "title": "Multimodal Agricultural Agent Architecture (MA3): A New Paradigm for Intelligent Agricultural Decision-Making",
    "authors": [
      "Zhuoning Xu",
      "Jian Xu",
      "Mingqing Zhang",
      "Peijie Wang",
      "Chao Deng",
      "Cheng-Lin Liu"
    ],
    "abstract": "As a strategic pillar industry for human survival and development, modern\nagriculture faces dual challenges: optimizing production efficiency and\nachieving sustainable development. Against the backdrop of intensified climate\nchange leading to frequent extreme weather events, the uncertainty risks in\nagricultural production systems are increasing exponentially. To address these\nchallenges, this study proposes an innovative \\textbf{M}ultimodal\n\\textbf{A}gricultural \\textbf{A}gent \\textbf{A}rchitecture (\\textbf{MA3}),\nwhich leverages cross-modal information fusion and task collaboration\nmechanisms to achieve intelligent agricultural decision-making. This study\nconstructs a multimodal agricultural agent dataset encompassing five major\ntasks: classification, detection, Visual Question Answering (VQA), tool\nselection, and agent evaluation. We propose a unified backbone for sugarcane\ndisease classification and detection tools, as well as a sugarcane disease\nexpert model. By integrating an innovative tool selection module, we develop a\nmultimodal agricultural agent capable of effectively performing tasks in\nclassification, detection, and VQA. Furthermore, we introduce a\nmulti-dimensional quantitative evaluation framework and conduct a comprehensive\nassessment of the entire architecture over our evaluation dataset, thereby\nverifying the practicality and robustness of MA3 in agricultural scenarios.\nThis study provides new insights and methodologies for the development of\nagricultural agents, holding significant theoretical and practical\nimplications. Our source code and dataset will be made publicly available upon\nacceptance.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.04789v1",
    "published_date": "2025-04-07 07:32:41 UTC",
    "updated_date": "2025-04-07 07:32:41 UTC"
  },
  {
    "arxiv_id": "2504.04787v1",
    "title": "Dynamic Vision Mamba",
    "authors": [
      "Mengxuan Wu",
      "Zekai Li",
      "Zhiyuan Liang",
      "Moyang Li",
      "Xuanlei Zhao",
      "Samir Khaki",
      "Zheng Zhu",
      "Xiaojiang Peng",
      "Konstantinos N. Plataniotis",
      "Kai Wang",
      "Wangbo Zhao",
      "Yang You"
    ],
    "abstract": "Mamba-based vision models have gained extensive attention as a result of\nbeing computationally more efficient than attention-based models. However,\nspatial redundancy still exists in these models, represented by token and block\nredundancy. For token redundancy, we analytically find that early token pruning\nmethods will result in inconsistency between training and inference or\nintroduce extra computation for inference. Therefore, we customize token\npruning to fit the Mamba structure by rearranging the pruned sequence before\nfeeding it into the next Mamba block. For block redundancy, we allow each image\nto select SSM blocks dynamically based on an empirical observation that the\ninference speed of Mamba-based vision models is largely affected by the number\nof SSM blocks. Our proposed method, Dynamic Vision Mamba (DyVM), effectively\nreduces FLOPs with minor performance drops. We achieve a reduction of 35.2\\%\nFLOPs with only a loss of accuracy of 1.7\\% on Vim-S. It also generalizes well\nacross different Mamba vision model architectures and different vision tasks.\nOur code will be made public.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.04787v1",
    "published_date": "2025-04-07 07:31:28 UTC",
    "updated_date": "2025-04-07 07:31:28 UTC"
  },
  {
    "arxiv_id": "2504.04785v1",
    "title": "Weak-for-Strong: Training Weak Meta-Agent to Harness Strong Executors",
    "authors": [
      "Fan Nie",
      "Lan Feng",
      "Haotian Ye",
      "Weixin Liang",
      "Pan Lu",
      "Huaxiu Yao",
      "Alexandre Alahi",
      "James Zou"
    ],
    "abstract": "Efficiently leveraging of the capabilities of contemporary large language\nmodels (LLMs) is increasingly challenging, particularly when direct fine-tuning\nis expensive and often impractical. Existing training-free methods, including\nmanually or automated designed workflows, typically demand substantial human\neffort or yield suboptimal results. This paper proposes Weak-for-Strong\nHarnessing (W4S), a novel framework that customizes smaller, cost-efficient\nlanguage models to design and optimize workflows for harnessing stronger\nmodels. W4S formulates workflow design as a multi-turn markov decision process\nand introduces reinforcement learning for agentic workflow optimization (RLAO)\nto train a weak meta-agent. Through iterative interaction with the environment,\nthe meta-agent learns to design increasingly effective workflows without manual\nintervention. Empirical results demonstrate the superiority of W4S that our 7B\nmeta-agent, trained with just one GPU hour, outperforms the strongest baseline\nby 2.9% ~ 24.6% across eleven benchmarks, successfully elevating the\nperformance of state-of-the-art models such as GPT-3.5-Turbo and GPT-4o.\nNotably, W4S exhibits strong generalization capabilities across both seen and\nunseen tasks, offering an efficient, high-performing alternative to directly\nfine-tuning strong models.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.04785v1",
    "published_date": "2025-04-07 07:27:31 UTC",
    "updated_date": "2025-04-07 07:27:31 UTC"
  },
  {
    "arxiv_id": "2504.04770v1",
    "title": "Bidirectional Hierarchical Protein Multi-Modal Representation Learning",
    "authors": [
      "Xuefeng Liu",
      "Songhao Jiang",
      "Chih-chan Tien",
      "Jinbo Xu",
      "Rick Stevens"
    ],
    "abstract": "Protein representation learning is critical for numerous biological tasks.\nRecently, large transformer-based protein language models (pLMs) pretrained on\nlarge scale protein sequences have demonstrated significant success in\nsequence-based tasks. However, pLMs lack structural information. Conversely,\ngraph neural networks (GNNs) designed to leverage 3D structural information\nhave shown promising generalization in protein-related prediction tasks, but\ntheir effectiveness is often constrained by the scarcity of labeled structural\ndata. Recognizing that sequence and structural representations are\ncomplementary perspectives of the same protein entity, we propose a multimodal\nbidirectional hierarchical fusion framework to effectively merge these\nmodalities. Our framework employs attention and gating mechanisms to enable\neffective interaction between pLMs-generated sequential representations and\nGNN-extracted structural features, improving information exchange and\nenhancement across layers of the neural network. Based on the framework, we\nfurther introduce local Bi-Hierarchical Fusion with gating and global\nBi-Hierarchical Fusion with multihead self-attention approaches. Through\nextensive experiments on a diverse set of protein-related tasks, our method\ndemonstrates consistent improvements over strong baselines and existing fusion\ntechniques in a variety of protein representation learning benchmarks,\nincluding react (enzyme/EC classification), model quality assessment (MQA),\nprotein-ligand binding affinity prediction (LBA), protein-protein binding site\nprediction (PPBS), and B cell epitopes prediction (BCEs). Our method\nestablishes a new state-of-the-art for multimodal protein representation\nlearning, emphasizing the efficacy of BIHIERARCHICAL FUSION in bridging\nsequence and structural modalities.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.MN"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.04770v1",
    "published_date": "2025-04-07 06:47:49 UTC",
    "updated_date": "2025-04-07 06:47:49 UTC"
  },
  {
    "arxiv_id": "2504.04766v1",
    "title": "KunPeng: A Global Ocean Environmental Model",
    "authors": [
      "Yi Zhao",
      "Jiaqi Li",
      "Haitao Xia",
      "Tianjiao Zhang",
      "Zerong Zeng",
      "Tianyu Ren",
      "Yucheng Zhang",
      "Chao Zhu",
      "Shengtong Xu",
      "Hongchun Yuan"
    ],
    "abstract": "Inspired by the similarity of the atmosphere-ocean physical coupling\nmechanism, this study innovatively migrates meteorological large-model\ntechniques to the ocean domain, constructing the KunPeng global ocean\nenvironmental prediction model. Aimed at the discontinuous characteristics of\nmarine space, we propose a terrain-adaptive mask constraint mechanism to\nmitigate effectively training divergence caused by abrupt gradients at land-sea\nboundaries. To fully integrate far-, medium-, and close-range marine features,\na longitude-cyclic deformable convolution network (LC-DCN) is employed to\nenhance the dynamic receptive field, achieving refined modeling of multi-scale\noceanic characteristics. A Deformable Convolution-enhanced Multi-Step\nPrediction module (DC-MTP) is employed to strengthen temporal dependency\nfeature extraction capabilities. Experimental results demonstrate that this\nmodel achieves an average ACC of 0.80 in 15-day global predictions at\n0.25$^\\circ$ resolution, outperforming comparative models by 0.01-0.08. The\naverage mean squared error (MSE) is 0.41 (representing a 5%-31% reduction) and\nthe average mean absolute error (MAE) is 0.44 (0.6%-21% reduction) compared to\nother models. Significant improvements are particularly observed in sea surface\nparameter prediction, deep-sea region characterization, and current velocity\nfield forecasting. Through a horizontal comparison of the applicability of\noperators at different scales in the marine domain, this study reveals that\nlocal operators significantly outperform global operators under slow-varying\noceanic processes, demonstrating the effectiveness of dynamic feature pyramid\nrepresentations in predicting marine physical parameters.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.04766v1",
    "published_date": "2025-04-07 06:41:05 UTC",
    "updated_date": "2025-04-07 06:41:05 UTC"
  },
  {
    "arxiv_id": "2504.04764v1",
    "title": "Enhancing Leaf Disease Classification Using GAT-GCN Hybrid Model",
    "authors": [
      "Shyam Sundhar",
      "Riya Sharma",
      "Priyansh Maheshwari",
      "Suvidha Rupesh Kumar",
      "T. Sunil Kumar"
    ],
    "abstract": "Agriculture plays a critical role in the global economy, providing\nlivelihoods and ensuring food security for billions. As innovative agricultural\npractices become more widespread, the risk of crop diseases has increased,\nhighlighting the urgent need for efficient, low-intervention disease\nidentification methods. This research presents a hybrid model combining Graph\nAttention Networks (GATs) and Graph Convolution Networks (GCNs) for leaf\ndisease classification. GCNs have been widely used for learning from\ngraph-structured data, and GATs enhance this by incorporating attention\nmechanisms to focus on the most important neighbors. The methodology integrates\nsuperpixel segmentation for efficient feature extraction, partitioning images\ninto meaningful, homogeneous regions that better capture localized features.\nThe authors have employed an edge augmentation technique to enhance the\nrobustness of the model. The edge augmentation technique has introduced a\nsignificant degree of generalization in the detection capabilities of the\nmodel. To further optimize training, weight initialization techniques are\napplied. The hybrid model is evaluated against the individual performance of\nthe GCN and GAT models and the hybrid model achieved a precision of 0.9822,\nrecall of 0.9818, and F1-score of 0.9818 in apple leaf disease classification,\na precision of 0.9746, recall of 0.9744, and F1-score of 0.9743 in potato leaf\ndisease classification, and a precision of 0.8801, recall of 0.8801, and\nF1-score of 0.8799 in sugarcane leaf disease classification. These results\ndemonstrate the robustness and performance of the model, suggesting its\npotential to support sustainable agricultural practices through precise and\neffective disease detection. This work is a small step towards reducing the\nloss of crops and hence supporting sustainable goals of zero hunger and life on\nland.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.04764v1",
    "published_date": "2025-04-07 06:31:38 UTC",
    "updated_date": "2025-04-07 06:31:38 UTC"
  },
  {
    "arxiv_id": "2504.04751v1",
    "title": "Unsupervised Estimation of Nonlinear Audio Effects: Comparing Diffusion-Based and Adversarial approaches",
    "authors": [
      "Eloi Moliner",
      "Michal Švento",
      "Alec Wright",
      "Lauri Juvela",
      "Pavel Rajmic",
      "Vesa Välimäki"
    ],
    "abstract": "Accurately estimating nonlinear audio effects without access to paired\ninput-output signals remains a challenging problem.This work studies\nunsupervised probabilistic approaches for solving this task. We introduce a\nmethod, novel for this application, based on diffusion generative models for\nblind system identification, enabling the estimation of unknown nonlinear\neffects using black- and gray-box models. This study compares this method with\na previously proposed adversarial approach, analyzing the performance of both\nmethods under different parameterizations of the effect operator and varying\nlengths of available effected recordings.Through experiments on guitar\ndistortion effects, we show that the diffusion-based approach provides more\nstable results and is less sensitive to data availability, while the\nadversarial approach is superior at estimating more pronounced distortion\neffects. Our findings contribute to the robust unsupervised blind estimation of\naudio effects, demonstrating the potential of diffusion models for system\nidentification in music technology.",
    "categories": [
      "eess.AS",
      "cs.AI"
    ],
    "primary_category": "eess.AS",
    "comment": "Submitted to the 28th International Conference on Digital Audio\n  Effects (DAFx25)",
    "pdf_url": "http://arxiv.org/pdf/2504.04751v1",
    "published_date": "2025-04-07 05:56:51 UTC",
    "updated_date": "2025-04-07 05:56:51 UTC"
  },
  {
    "arxiv_id": "2504.04744v1",
    "title": "Grounding 3D Object Affordance with Language Instructions, Visual Observations and Interactions",
    "authors": [
      "He Zhu",
      "Quyu Kong",
      "Kechun Xu",
      "Xunlong Xia",
      "Bing Deng",
      "Jieping Ye",
      "Rong Xiong",
      "Yue Wang"
    ],
    "abstract": "Grounding 3D object affordance is a task that locates objects in 3D space\nwhere they can be manipulated, which links perception and action for embodied\nintelligence. For example, for an intelligent robot, it is necessary to\naccurately ground the affordance of an object and grasp it according to human\ninstructions. In this paper, we introduce a novel task that grounds 3D object\naffordance based on language instructions, visual observations and\ninteractions, which is inspired by cognitive science. We collect an Affordance\nGrounding dataset with Points, Images and Language instructions (AGPIL) to\nsupport the proposed task. In the 3D physical world, due to observation\norientation, object rotation, or spatial occlusion, we can only get a partial\nobservation of the object. So this dataset includes affordance estimations of\nobjects from full-view, partial-view, and rotation-view perspectives. To\naccomplish this task, we propose LMAffordance3D, the first multi-modal,\nlanguage-guided 3D affordance grounding network, which applies a\nvision-language model to fuse 2D and 3D spatial features with semantic\nfeatures. Comprehensive experiments on AGPIL demonstrate the effectiveness and\nsuperiority of our method on this task, even in unseen experimental settings.\nOur project is available at https://sites.google.com/view/lmaffordance3d.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.04744v1",
    "published_date": "2025-04-07 05:38:23 UTC",
    "updated_date": "2025-04-07 05:38:23 UTC"
  },
  {
    "arxiv_id": "2504.04740v1",
    "title": "Enhancing Compositional Reasoning in Vision-Language Models with Synthetic Preference Data",
    "authors": [
      "Samarth Mishra",
      "Kate Saenko",
      "Venkatesh Saligrama"
    ],
    "abstract": "Compositionality, or correctly recognizing scenes as compositions of atomic\nvisual concepts, remains difficult for multimodal large language models\n(MLLMs). Even state of the art MLLMs such as GPT-4o can make mistakes in\ndistinguishing compositions like \"dog chasing cat\" vs \"cat chasing dog\". While\non Winoground, a benchmark for measuring such reasoning, MLLMs have made\nsignificant progress, they are still far from a human's performance. We show\nthat compositional reasoning in these models can be improved by elucidating\nsuch concepts via data, where a model is trained to prefer the correct caption\nfor an image over a close but incorrect one. We introduce SCRAMBLe: Synthetic\nCompositional Reasoning Augmentation of MLLMs with Binary preference Learning,\nan approach for preference tuning open-weight MLLMs on synthetic preference\ndata generated in a fully automated manner from existing image-caption data.\nSCRAMBLe holistically improves these MLLMs' compositional reasoning\ncapabilities which we can see through significant improvements across multiple\nvision language compositionality benchmarks, as well as smaller but significant\nimprovements on general question answering tasks. As a sneak peek, SCRAMBLe\ntuned Molmo-7B model improves on Winoground from 49.5% to 54.8% (best reported\nto date), while improving by ~1% on more general visual question answering\ntasks. Code for SCRAMBLe along with tuned models and our synthetic training\ndataset is available at https://github.com/samarth4149/SCRAMBLe.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.04740v1",
    "published_date": "2025-04-07 05:35:34 UTC",
    "updated_date": "2025-04-07 05:35:34 UTC"
  },
  {
    "arxiv_id": "2504.04737v1",
    "title": "TathyaNyaya and FactLegalLlama: Advancing Factual Judgment Prediction and Explanation in the Indian Legal Context",
    "authors": [
      "Shubham Kumar Nigam",
      "Balaramamahanthi Deepak Patnaik",
      "Shivam Mishra",
      "Noel Shallum",
      "Kripabandhu Ghosh",
      "Arnab Bhattacharya"
    ],
    "abstract": "In the landscape of Fact-based Judgment Prediction and Explanation (FJPE),\nreliance on factual data is essential for developing robust and realistic\nAI-driven decision-making tools. This paper introduces TathyaNyaya, the largest\nannotated dataset for FJPE tailored to the Indian legal context, encompassing\njudgments from the Supreme Court of India and various High Courts. Derived from\nthe Hindi terms \"Tathya\" (fact) and \"Nyaya\" (justice), the TathyaNyaya dataset\nis uniquely designed to focus on factual statements rather than complete legal\ntexts, reflecting real-world judicial processes where factual data drives\noutcomes. Complementing this dataset, we present FactLegalLlama, an\ninstruction-tuned variant of the LLaMa-3-8B Large Language Model (LLM),\noptimized for generating high-quality explanations in FJPE tasks. Finetuned on\nthe factual data in TathyaNyaya, FactLegalLlama integrates predictive accuracy\nwith coherent, contextually relevant explanations, addressing the critical need\nfor transparency and interpretability in AI-assisted legal systems. Our\nmethodology combines transformers for binary judgment prediction with\nFactLegalLlama for explanation generation, creating a robust framework for\nadvancing FJPE in the Indian legal domain. TathyaNyaya not only surpasses\nexisting datasets in scale and diversity but also establishes a benchmark for\nbuilding explainable AI systems in legal analysis. The findings underscore the\nimportance of factual precision and domain-specific tuning in enhancing\npredictive performance and interpretability, positioning TathyaNyaya and\nFactLegalLlama as foundational resources for AI-assisted legal decision-making.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.04737v1",
    "published_date": "2025-04-07 05:27:32 UTC",
    "updated_date": "2025-04-07 05:27:32 UTC"
  },
  {
    "arxiv_id": "2504.04736v1",
    "title": "Synthetic Data Generation & Multi-Step RL for Reasoning & Tool Use",
    "authors": [
      "Anna Goldie",
      "Azalia Mirhoseini",
      "Hao Zhou",
      "Irene Cai",
      "Christopher D. Manning"
    ],
    "abstract": "Reinforcement learning has been shown to improve the performance of large\nlanguage models. However, traditional approaches like RLHF or RLAIF treat the\nproblem as single-step. As focus shifts toward more complex reasoning and\nagentic tasks, language models must take multiple steps of text generation,\nreasoning and environment interaction before generating a solution. We propose\na synthetic data generation and RL methodology targeting multi-step\noptimization scenarios. This approach, called Step-Wise Reinforcement Learning\n(SWiRL), iteratively generates multi-step reasoning and tool use data, and then\nlearns from that data. It employs a simple step-wise decomposition that breaks\neach multi-step trajectory into multiple sub-trajectories corresponding to each\naction by the original model. It then applies synthetic data filtering and RL\noptimization on these sub-trajectories. We evaluated SWiRL on a number of\nmulti-step tool use, question answering, and mathematical reasoning tasks. Our\nexperiments show that SWiRL outperforms baseline approaches by 21.5%, 12.3%,\n14.8%, 11.1%, and 15.3% in relative accuracy on GSM8K, HotPotQA, CofCA,\nMuSiQue, and BeerQA, respectively. Excitingly, the approach exhibits\ngeneralization across tasks: for example, training only on HotPotQA (text\nquestion-answering) improves zero-shot performance on GSM8K (a math dataset) by\na relative 16.9%.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.04736v1",
    "published_date": "2025-04-07 05:20:58 UTC",
    "updated_date": "2025-04-07 05:20:58 UTC"
  },
  {
    "arxiv_id": "2504.04718v1",
    "title": "T1: Tool-integrated Self-verification for Test-time Compute Scaling in Small Language Models",
    "authors": [
      "Minki Kang",
      "Jongwon Jeong",
      "Jaewoong Cho"
    ],
    "abstract": "Recent studies have demonstrated that test-time compute scaling effectively\nimproves the performance of small language models (sLMs). However, prior\nresearch has mainly examined test-time compute scaling with an additional\nlarger model as a verifier, leaving self-verification by sLMs underexplored. In\nthis work, we investigate whether sLMs can reliably self-verify their outputs\nunder test-time scaling. We find that even with knowledge distillation from\nlarger verifiers, sLMs struggle with verification tasks requiring memorization,\nsuch as numerical calculations and fact-checking. To address this limitation,\nwe propose Tool-integrated self-verification (T1), which delegates\nmemorization-heavy verification steps to external tools, such as a code\ninterpreter. Our theoretical analysis shows that tool integration reduces\nmemorization demands and improves test-time scaling performance. Experiments on\nthe MATH benchmark demonstrate that, with T1, a Llama-3.2 1B model under\ntest-time scaling outperforms the significantly larger Llama-3.1 8B model.\nMoreover, T1 generalizes effectively to both mathematical (MATH500) and\nmulti-domain knowledge-intensive tasks (MMLU-Pro). Our findings highlight the\npotential of tool integration to substantially improve the self-verification\nabilities of sLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2504.04718v1",
    "published_date": "2025-04-07 04:01:17 UTC",
    "updated_date": "2025-04-07 04:01:17 UTC"
  },
  {
    "arxiv_id": "2504.04717v2",
    "title": "Beyond Single-Turn: A Survey on Multi-Turn Interactions with Large Language Models",
    "authors": [
      "Yubo Li",
      "Xiaobin Shen",
      "Xinyu Yao",
      "Xueying Ding",
      "Yidi Miao",
      "Ramayya Krishnan",
      "Rema Padman"
    ],
    "abstract": "Recent advancements in large language models (LLMs) have revolutionized their\nability to handle single-turn tasks, yet real-world applications demand\nsophisticated multi-turn interactions. This survey provides a comprehensive\nreview of recent advancements in evaluating and enhancing multi-turn\ninteractions in LLMs. Focusing on task-specific scenarios, from instruction\nfollowing in diverse domains such as math and coding to complex conversational\nengagements in roleplay, healthcare, education, and even adversarial jailbreak\nsettings, we systematically examine the challenges of maintaining context,\ncoherence, fairness, and responsiveness over prolonged dialogues. The paper\norganizes current benchmarks and datasets into coherent categories that reflect\nthe evolving landscape of multi-turn dialogue evaluation. In addition, we\nreview a range of enhancement methodologies under multi-turn settings,\nincluding model-centric strategies (contextual learning, supervised\nfine-tuning, reinforcement learning, and new architectures), external\nintegration approaches (memory-augmented, retrieval-based methods, and\nknowledge graph), and agent-based techniques for collaborative interactions.\nFinally, we discuss open challenges and propose future directions for research\nto further advance the robustness and effectiveness of multi-turn interactions\nin LLMs. Related resources and papers are available at\nhttps://github.com/yubol-cmu/Awesome-Multi-Turn-LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.04717v2",
    "published_date": "2025-04-07 04:00:08 UTC",
    "updated_date": "2025-04-08 03:58:37 UTC"
  },
  {
    "arxiv_id": "2504.04711v1",
    "title": "Generalising from Self-Produced Data: Model Training Beyond Human Constraints",
    "authors": [
      "Alfath Daryl Alhajir",
      "Jennifer Dodgson",
      "Joseph Lim",
      "Truong Ma Phi",
      "Julian Peh",
      "Akira Rafhael Janson Pattirane",
      "Lokesh Poovaragan"
    ],
    "abstract": "Current large language models (LLMs) are constrained by human-derived\ntraining data and limited by a single level of abstraction that impedes\ndefinitive truth judgments. This paper introduces a novel framework in which AI\nmodels autonomously generate and validate new knowledge through direct\ninteraction with their environment. Central to this approach is an unbounded,\nungamable numeric reward - such as annexed disk space or follower count - that\nguides learning without requiring human benchmarks. AI agents iteratively\ngenerate strategies and executable code to maximize this metric, with\nsuccessful outcomes forming the basis for self-retraining and incremental\ngeneralisation. To mitigate model collapse and the warm start problem, the\nframework emphasizes empirical validation over textual similarity and supports\nfine-tuning via GRPO. The system architecture employs modular agents for\nenvironment analysis, strategy generation, and code synthesis, enabling\nscalable experimentation. This work outlines a pathway toward self-improving AI\nsystems capable of advancing beyond human-imposed constraints toward autonomous\ngeneral intelligence.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "16 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.04711v1",
    "published_date": "2025-04-07 03:48:02 UTC",
    "updated_date": "2025-04-07 03:48:02 UTC"
  },
  {
    "arxiv_id": "2504.04706v1",
    "title": "AdvKT: An Adversarial Multi-Step Training Framework for Knowledge Tracing",
    "authors": [
      "Lingyue Fu",
      "Ting Long",
      "Jianghao Lin",
      "Wei Xia",
      "Xinyi Dai",
      "Ruiming Tang",
      "Yasheng Wang",
      "Weinan Zhang",
      "Yong Yu"
    ],
    "abstract": "Knowledge Tracing (KT) monitors students' knowledge states and simulates\ntheir responses to question sequences. Existing KT models typically follow a\nsingle-step training paradigm, which leads to discrepancies with the multi-step\ninference process required in real-world simulations, resulting in significant\nerror accumulation. This accumulation of error, coupled with the issue of data\nsparsity, can substantially degrade the performance of recommendation models in\nthe intelligent tutoring systems. To address these challenges, we propose a\nnovel Adversarial Multi-Step Training Framework for Knowledge Tracing (AdvKT),\nwhich, for the first time, focuses on the multi-step KT task. More\nspecifically, AdvKT leverages adversarial learning paradigm involving a\ngenerator and a discriminator. The generator mimics high-reward responses,\neffectively reducing error accumulation across multiple steps, while the\ndiscriminator provides feedback to generate synthetic data. Additionally, we\ndesign specialized data augmentation techniques to enrich the training data\nwith realistic variations, ensuring that the model generalizes well even in\nscenarios with sparse data. Experiments conducted on four real-world datasets\ndemonstrate the superiority of AdvKT over existing KT models, showcasing its\nability to address both error accumulation and data sparsity issues\neffectively.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.04706v1",
    "published_date": "2025-04-07 03:31:57 UTC",
    "updated_date": "2025-04-07 03:31:57 UTC"
  },
  {
    "arxiv_id": "2504.04704v1",
    "title": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are Important",
    "authors": [
      "Manlai Liang",
      "JiaMing Zhang",
      "Xiong Li",
      "Jinlong Li"
    ],
    "abstract": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modifiation of the inference infrastructure and\nsignificant computation overhead. Base on the fact that the Large Lanuage\nmodels are autoregresssive models, we propose {\\it LagKV}, a KV allocation\nstrategy only relying on straight forward comparison among KV themself. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on LongBench and PasskeyRetrieval show that, our\napproach achieves nearly zero loss when the ratio is $2\\times$ and $\\approx\n90\\%$ of the original model performance for $8\\times$. Especially in the\n64-digit passkey retrieval task, our mehod outperforms the attention weight\nbased method $H_2O$ over $60\\%$ with same compression ratios. Our code is\navailable at \\url{https://github.com/AI-Lab-China-Merchants-Bank/LagKV}.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.04704v1",
    "published_date": "2025-04-07 03:22:15 UTC",
    "updated_date": "2025-04-07 03:22:15 UTC"
  },
  {
    "arxiv_id": "2504.04702v1",
    "title": "Provable Failure of Language Models in Learning Majority Boolean Logic via Gradient Descent",
    "authors": [
      "Bo Chen",
      "Zhenmei Shi",
      "Zhao Song",
      "Jiahao Zhang"
    ],
    "abstract": "Recent advancements in Transformer-based architectures have led to impressive\nbreakthroughs in natural language processing tasks, with models such as GPT-4,\nClaude, and Gemini demonstrating human-level reasoning abilities. However,\ndespite their high performance, concerns remain about the inherent limitations\nof these models, especially when it comes to learning basic logical functions.\nWhile complexity-theoretic analyses indicate that Transformers can represent\nsimple logic functions (e.g., $\\mathsf{AND}$, $\\mathsf{OR}$, and majority\ngates) by its nature of belonging to the $\\mathsf{TC}^0$ class, these results\nassume ideal parameter settings and do not account for the constraints imposed\nby gradient descent-based training methods. In this work, we investigate\nwhether Transformers can truly learn simple majority functions when trained\nusing gradient-based methods. We focus on a simplified variant of the\nTransformer architecture and consider both $n=\\mathrm{poly}(d)$ and\n$n=\\exp(\\Omega(d))$ number of training samples, where each sample is a $d$-size\nbinary string paired with the output of a basic majority function. Our analysis\ndemonstrates that even after $\\mathrm{poly}(d)$ gradient queries, the\ngeneralization error of the Transformer model still remains substantially\nlarge, growing exponentially with $d$. This work highlights fundamental\noptimization challenges in training Transformers for the simplest logical\nreasoning tasks and provides new insights into their theoretical limitations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.04702v1",
    "published_date": "2025-04-07 03:08:12 UTC",
    "updated_date": "2025-04-07 03:08:12 UTC"
  },
  {
    "arxiv_id": "2504.04699v1",
    "title": "R2Vul: Learning to Reason about Software Vulnerabilities with Reinforcement Learning and Structured Reasoning Distillation",
    "authors": [
      "Martin Weyssow",
      "Chengran Yang",
      "Junkai Chen",
      "Yikun Li",
      "Huihui Huang",
      "Ratnadira Widyasari",
      "Han Wei Ang",
      "Frank Liauw",
      "Eng Lieh Ouh",
      "Lwin Khin Shar",
      "David Lo"
    ],
    "abstract": "Large language models (LLMs) have shown promising performance in software\nvulnerability detection (SVD), yet their reasoning capabilities remain\nunreliable. Existing approaches relying on chain-of-thought (CoT) struggle to\nprovide relevant and actionable security assessments. Additionally, effective\nSVD requires not only generating coherent reasoning but also differentiating\nbetween well-founded and misleading yet plausible security assessments, an\naspect overlooked in prior work. To this end, we introduce R2Vul, a novel\napproach that distills structured reasoning into small LLMs using reinforcement\nlearning from AI feedback (RLAIF). Through RLAIF, R2Vul enables LLMs to produce\nstructured, security-aware reasoning that is actionable and reliable while\nexplicitly learning to distinguish valid assessments from misleading ones. We\nevaluate R2Vul across five languages against SAST tools, CoT, instruction\ntuning, and classification-based baselines. Our results show that R2Vul with\nstructured reasoning distillation enables a 1.5B student LLM to rival larger\nmodels while improving generalization to out-of-distribution vulnerabilities.\nBeyond model improvements, we contribute a large-scale, multilingual preference\ndataset featuring structured reasoning to support future research in SVD.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.04699v1",
    "published_date": "2025-04-07 03:04:16 UTC",
    "updated_date": "2025-04-07 03:04:16 UTC"
  },
  {
    "arxiv_id": "2504.04687v1",
    "title": "Bridging Knowledge Gap Between Image Inpainting and Large-Area Visible Watermark Removal",
    "authors": [
      "Yicheng Leng",
      "Chaowei Fang",
      "Junye Chen",
      "Yixiang Fang",
      "Sheng Li",
      "Guanbin Li"
    ],
    "abstract": "Visible watermark removal which involves watermark cleaning and background\ncontent restoration is pivotal to evaluate the resilience of watermarks.\nExisting deep neural network (DNN)-based models still struggle with large-area\nwatermarks and are overly dependent on the quality of watermark mask\nprediction. To overcome these challenges, we introduce a novel feature adapting\nframework that leverages the representation modeling capacity of a pre-trained\nimage inpainting model. Our approach bridges the knowledge gap between image\ninpainting and watermark removal by fusing information of the residual\nbackground content beneath watermarks into the inpainting backbone model. We\nestablish a dual-branch system to capture and embed features from the residual\nbackground content, which are merged into intermediate features of the\ninpainting backbone model via gated feature fusion modules. Moreover, for\nrelieving the dependence on high-quality watermark masks, we introduce a new\ntraining paradigm by utilizing coarse watermark masks to guide the inference\nprocess. This contributes to a visible image removal model which is insensitive\nto the quality of watermark mask during testing. Extensive experiments on both\na large-scale synthesized dataset and a real-world dataset demonstrate that our\napproach significantly outperforms existing state-of-the-art methods. The\nsource code is available in the supplementary materials.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM",
      "eess.IV",
      "I.2.10; I.4.4; I.4.5"
    ],
    "primary_category": "cs.CV",
    "comment": "To be published in AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.04687v1",
    "published_date": "2025-04-07 02:37:14 UTC",
    "updated_date": "2025-04-07 02:37:14 UTC"
  },
  {
    "arxiv_id": "2504.04676v1",
    "title": "Dual Consistent Constraint via Disentangled Consistency and Complementarity for Multi-view Clustering",
    "authors": [
      "Bo Li",
      "Jing Yun"
    ],
    "abstract": "Multi-view clustering can explore common semantics from multiple views and\nhas received increasing attention in recent years. However, current methods\nfocus on learning consistency in representation, neglecting the contribution of\neach view's complementarity aspect in representation learning. This limit poses\na significant challenge in multi-view representation learning. This paper\nproposes a novel multi-view clustering framework that introduces a disentangled\nvariational autoencoder that separates multi-view into shared and private\ninformation, i.e., consistency and complementarity information. We first learn\ninformative and consistent representations by maximizing mutual information\nacross different views through contrastive learning. This process will ignore\ncomplementary information. Then, we employ consistency inference constraints to\nexplicitly utilize complementary information when attempting to seek the\nconsistency of shared information across all views. Specifically, we perform a\nwithin-reconstruction using the private and shared information of each view and\na cross-reconstruction using the shared information of all views. The dual\nconsistency constraints are not only effective in improving the representation\nquality of data but also easy to extend to other scenarios, especially in\ncomplex multi-view scenes. This could be the first attempt to employ dual\nconsistent constraint in a unified MVC theoretical framework. During the\ntraining procedure, the consistency and complementarity features are jointly\noptimized. Extensive experiments show that our method outperforms baseline\nmethods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.04676v1",
    "published_date": "2025-04-07 02:00:16 UTC",
    "updated_date": "2025-04-07 02:00:16 UTC"
  },
  {
    "arxiv_id": "2504.04675v2",
    "title": "HypRL: Reinforcement Learning of Control Policies for Hyperproperties",
    "authors": [
      "Tzu-Han Hsu",
      "Arshia Rafieioskouei",
      "Borzoo Bonakdarpour"
    ],
    "abstract": "We study the problem of learning control policies for complex tasks whose\nrequirements are given by a hyperproperty. The use of hyperproperties is\nmotivated by their significant power to formally specify requirements of\nmulti-agent systems as well as those that need expressiveness in terms of\nmultiple execution traces (e.g., privacy and fairness). Given a Markov decision\nprocess M with unknown transitions (representing the environment) and a\nHyperLTL formula $\\varphi$, our approach first employs Skolemization to handle\nquantifier alternations in $\\varphi$. We introduce quantitative robustness\nfunctions for HyperLTL to define rewards of finite traces of M with respect to\n$\\varphi$. Finally, we utilize a suitable reinforcement learning algorithm to\nlearn (1) a policy per trace quantifier in $\\varphi$, and (2) the probability\ndistribution of transitions of M that together maximize the expected reward\nand, hence, probability of satisfaction of $\\varphi$ in M. We present a set of\ncase studies on (1) safety-preserving multi-agent path planning, (2) fairness\nin resource allocation, and (3) the post-correspondence problem (PCP).",
    "categories": [
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.04675v2",
    "published_date": "2025-04-07 01:58:36 UTC",
    "updated_date": "2025-04-08 04:19:02 UTC"
  },
  {
    "arxiv_id": "2504.04654v1",
    "title": "EquiCPI: SE(3)-Equivariant Geometric Deep Learning for Structure-Aware Prediction of Compound-Protein Interactions",
    "authors": [
      "Ngoc-Quang Nguyen"
    ],
    "abstract": "Accurate prediction of compound-protein interactions (CPI) remains a\ncornerstone challenge in computational drug discovery. While existing\nsequence-based approaches leverage molecular fingerprints or graph\nrepresentations, they critically overlook three-dimensional (3D) structural\ndeterminants of binding affinity. To bridge this gap, we present EquiCPI, an\nend-to-end geometric deep learning framework that synergizes first-principles\nstructural modeling with SE(3)-equivariant neural networks. Our pipeline\ntransforms raw sequences into 3D atomic coordinates via ESMFold for proteins\nand DiffDock-L for ligands, followed by physics-guided conformer re-ranking and\nequivariant feature learning. At its core, EquiCPI employs SE(3)-equivariant\nmessage passing over atomic point clouds, preserving symmetry under rotations,\ntranslations, and reflections, while hierarchically encoding local interaction\npatterns through tensor products of spherical harmonics. The proposed model is\nevaluated on BindingDB (affinity prediction) and DUD-E (virtual screening),\nEquiCPI achieves performance on par with or exceeding the state-of-the-art deep\nlearning competitors.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.04654v1",
    "published_date": "2025-04-07 00:57:08 UTC",
    "updated_date": "2025-04-07 00:57:08 UTC"
  }
]
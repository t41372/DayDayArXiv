{
  "date": "2025-04-07",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-04-07 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 上的论文热点集中在大型语言模型（LLM）和多模态模型的效率、能力扩展与评估上。**SmolVLM** 展示了小模型也能实现卓越性能，**VAPO** 和 **Algorithm Discovery with LLMs** 等探索了强化学习在提升 LLM 推理和发现能力上的潜力。同时，研究者们也在关注模型的鲁棒性、安全性、可解释性以及在特定领域（如法律、医学、遥感、软件工程）的应用。评估方法和基准测试（如 **M-Prometheus**、**SCAM**、**ELT-Bench**、**Video-Bench**）也是今日的一大重点，旨在更准确地衡量和提升 AI 系统的性能与可靠性。\n\n接下来，让我们一起看看今天有哪些值得关注的论文：\n\n---\n\n**重点论文 & LLM/多模态进展**\n\n1.  **#2 SmolVLM: 重新定义小型高效多模态模型 (SmolVLM: Redefining small and efficient multimodal models)**\n    *   作者：Andrés Marafioti, Orr Zohar, Miquel Farré 等 (Hugging Face 团队)\n    *   TLDR: 介绍了 SmolVLM 系列紧凑型多模态模型，专为资源高效推理而设计。通过优化架构、分词策略和数据管理，实现了极低的计算开销。最小的 256M 模型推理显存 < 1GB，性能却优于 300 倍大的 Idefics-80B。2.2B 模型性能媲美两倍显存消耗的 SOTA 模型，并展示了强大的视频理解能力。强调了战略性优化对小型高效多模态模型的重要性。\n\n2.  **#1 URECA: 独特区域描述生成 (URECA: Unique Region Caption Anything)**\n    *   作者：Sangbeom Lim, Junwan Kim, Heeji Yoon 等 (KAIST CV Lab)\n    *   TLDR: 针对现有方法难以生成区分性区域描述的问题，提出了 URECA 数据集和模型。该数据集包含多粒度的物体、部件和背景元素，确保区域与描述的独特性和一致性。URECA 模型通过动态掩码建模和高分辨率掩码编码器，有效编码多粒度区域，生成精细且语义丰富的区域描述，在区域级描述任务上达到 SOTA。\n\n3.  **#30 VAPO: 面向高级推理任务的高效可靠强化学习框架 (VAPO: Efficient and Reliable Reinforcement Learning for Advanced Reasoning Tasks)**\n    *   作者：Yu Yue, Yufeng Yuan, Qiying Yu 等\n    *   TLDR: 提出了 VAPO (Value-based Augmented Proximal Policy Optimization)，一个基于价值范式的推理模型强化学习框架。在 AIME 2024 数据集上，基于 Qwen 32B 的 VAPO 取得了 60.4 分的 SOTA 成绩，显著优于 DeepSeek-R1-Zero-Qwen-32B 和 DAPO。VAPO 训练稳定高效，仅需 5000 步即可达 SOTA，且无训练崩溃。该研究解决了价值模型偏差、序列长度异构和奖励稀疏等挑战，提升了长链思维（long-CoT）推理性能。\n\n4.  **#76 Weak-for-Strong: 训练弱元智能体以驾驭强执行器 (Weak-for-Strong: Training Weak Meta-Agent to Harness Strong Executors)**\n    *   作者：Fan Nie, Lan Feng, Haotian Ye 等\n    *   TLDR: 提出 W4S 框架，使用小型、低成本的语言模型（弱元智能体）来设计和优化工作流，以利用更强大的模型（强执行器），避免了直接微调强模型的昂贵成本。通过强化学习优化智能体工作流（RLAO），弱元智能体能自主学习设计有效的工作流。实验表明，7B 元智能体在 11 个基准上性能超越强基线 2.9%~24.6%，成功提升了 GPT-3.5-Turbo 和 GPT-4o 等模型的表现，并展现了良好的泛化能力。\n\n5.  **#84 用于推理和工具使用的合成数据生成与多步强化学习 (Synthetic Data Generation & Multi-Step RL for Reasoning & Tool Use)**\n    *   作者：Anna Goldie, Azalia Mirhoseini, Hao Zhou 等 (Google & Stanford)\n    *   TLDR: 提出 Step-Wise Reinforcement Learning (SWiRL)，一种针对多步优化场景（如推理和工具使用）的合成数据生成和强化学习方法。SWiRL 将多步轨迹分解为子轨迹，进行合成数据过滤和 RL 优化。实验表明，SWiRL 在 GSM8K、HotPotQA 等多个多步任务上显著优于基线，并表现出跨任务泛化能力。\n\n6.  **#48 M-Prometheus: 一套开放的多语言 LLM 评判器 (M-Prometheus: A Suite of Open Multilingual LLM Judges)**\n    *   作者：José Pombal, Dongkeun Yoon, Patrick Fernandes 等 (Unbabel & CMU)\n    *   TLDR: 介绍了 M-Prometheus，一套参数量从 3B 到 14B 的开放权重多语言 LLM 评判器，可提供直接评估和成对比较反馈。在覆盖 20 多种语言的多语言奖励基准和 4 个语言对的文学机器翻译评估中，其性能优于现有 SOTA 开放 LLM 评判器。该模型还可用于解码时改进生成输出。研究强调了选择骨干模型和使用原生多语言反馈数据的重要性。\n\n7.  **#56 宪法还是崩溃？用 Llama 3-8B 探索 Constitutional AI (Constitution or Collapse? Exploring Constitutional AI with Llama 3-8B)**\n    *   作者：Xue Zhang\n    *   TLDR: 在 LLaMA 3-8B 模型上复现了 Constitutional AI (CAI) 工作流。结果表明，CAI 能有效提高模型的无害性（MT-Bench 攻击成功率降低 40.8%），但以牺牲有用性为代价（下降 9.8%）。同时观察到最终 DPO-CAI 模型出现明显的模型崩溃迹象，表明小模型可能因输出质量不足而难以进行自我改进，使得有效微调更具挑战性。研究暗示自我改进能力可能是 LLM 的一种涌现特性。\n\n8.  **#71 量化会损害推理吗？量化推理模型的实证研究 (Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning Models)**\n    *   作者：Ruikang Liu, Yuxuan Sun, Manyi Zhang 等\n    *   TLDR: 首次系统研究了量化对推理模型的影响。评估了 DeepSeek-R1-Distilled Qwen 和 LLaMA 系列（1.5B-70B）以及 QwQ-32B 在不同量化算法（权重、KV 缓存、激活）和位宽下的表现。发现 W8A8 或 W4A16 可实现无损量化，但更低位宽会带来显著的准确性风险。模型大小、来源和任务难度是关键决定因素。量化模型并未表现出更长的输出。策略性地扩展模型大小或推理步骤可有效提升性能。\n\n9.  **#66 别 Lag，用 RAG：使用 RAG 进行免训练的对抗性检测 (Don't Lag, RAG: Training-Free Adversarial Detection Using RAG)**\n    *   作者：Roie Kazoom, Raz Lapid, Moshe Sipper 等\n    *   TLDR: 提出一种免训练的视觉检索增强生成 (VRAG) 框架，利用视觉语言模型 (VLM) 检测对抗性补丁攻击。通过检索数据库中与已知攻击相似的补丁和图像，VRAG 进行生成式推理来识别不同类型的攻击，无需额外训练。开源模型 UI-TARS-72B-DPO 达到 95% 的分类准确率，创下开源对抗补丁检测的新 SOTA。\n\n10. **#57 Collab-RAG: 通过白盒与黑盒 LLM 协作提升复杂问答的检索增强生成 (Collab-RAG: Boosting Retrieval-Augmented Generation for Complex Question Answering via White-Box and Black-Box LLM Collaboration)**\n    *   作者：Ran Xu, Wenqi Shi, Yuchen Zhuang 等\n    *   TLDR: 提出 Collab-RAG 框架，通过白盒小语言模型 (SLM) 和黑盒大语言模型 (LLM) 的协作来提升 RAG 性能。SLM 分解复杂问题为子问题，提高检索准确性并辅助 LLM 推理；LLM 提供反馈信号以提升 SLM 的分解能力。该方法仅需廉价黑盒 LLM 的监督，无需前沿 LLM 蒸馏，并在多个多跳 QA 数据集上显著优于基线。\n\n11. **#12 利用 LLM 进行面向效用的标注：减少检索和 RAG 的手动工作量 (Leveraging LLMs for Utility-Focused Annotation: Reducing Manual Effort for Retrieval and RAG)**\n    *   作者：Hengran Zhang, Minghao Tang, Keping Bi 等 (中科院计算所)\n    *   TLDR: 探讨使用 LLM 生成的标注（关注文档对答案生成的“效用”而非仅“相关性”）替代人工标注来训练检索模型。设计了 Disj-InfoNCE 损失函数减少低质量正样本影响。实验表明，基于效用标注训练的检索器在域外设置的检索和 RAG 任务上泛化能力优于基于人类标注的模型；在域内设置，结合少量（20%）人类标注即可匹敌完全人类标注训练的模型。\n\n13. **#13 利用查询似然建模释放 LLM 在密集检索中的力量 (Unleashing the Power of LLMs in Dense Retrieval with Query Likelihood Modeling)**\n    *   作者：Hengran Zhang, Keping Bi, Jiafeng Guo 等 (中科院计算所)\n    *   TLDR: 受经典查询似然 (QL) 模型启发，提出 LLM-QL 模型，通过最大化 QL 作为辅助任务来训练判别式检索器。引入 Attention Stop (AS) 和 Input Corruption (IC) 机制将全局文档语义压缩到单个向量中进行 QL 建模。在 MSMARCO 上的实验表明，LLM-QL 显著优于其他基于 LLM 的检索器，且其估计的 QL 用于排序也远超基于词的 QL。\n\n---\n\n**AI 安全、伦理、鲁棒性与评估**\n\n14. **#33 揭示对齐大语言模型的内在伦理脆弱性 (Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language Models)**\n    *   作者：Jiawei Lian, Jianhong Pan, Lefan Wang 等\n    *   TLDR: 证明当前对齐方法仅在知识流形中产生局部“安全区域”，预训练期间嵌入的有害知识（“暗模式”）仍可通过对抗性诱导（分布偏移下的语义连贯诱导）重新浮现。该方法在 23 个 SOTA 对齐 LLM 中的 19 个（包括 DeepSeek-R1 和 LLaMA-3）上实现了 100% 的攻击成功率，揭示了它们的普遍脆弱性。\n\n15. **#5 如何评估 LLM 智能体的控制措施？从今天到超智能的轨迹 (How to evaluate control measures for LLM agents? A trajectory from today to superintelligence)**\n    *   作者：Tomek Korbak, Mikita Balesni, Buck Shlegeris 等\n    *   TLDR: 提出一个系统性框架，用于根据待部署智能体的能力调整红队（red team）的权限，以进行控制评估。认为评估应反映智能体的实际能力而非人类已知的最佳攻击策略，从而实现更实用、经济的控制措施。通过虚构模型序列（M1-M5）定义了五个 AI 控制级别（ACL），并讨论了为何对超智能 LLM 智能体构建可靠的安全案例需要研究突破。\n\n16. **#61 SCAM: 多模态基础模型真实世界排版鲁棒性评估 (SCAM: A Real-World Typographic Robustness Evaluation for Multimodal Foundation Models)**\n    *   作者：Justus Westerhoff, Erblina Purellku, Jakob Hackstein 等\n    *   TLDR: 介绍了 SCAM 数据集，包含 1162 张真实世界中的排版攻击图像（将误导性文本嵌入图像），用于研究多模态模型的脆弱性。基准测试表明，排版攻击显著降低 VLM 性能，且易受攻击性与训练数据和模型架构有关。SOTA LVLM 仍然存在此漏洞，主要源于视觉编码器的选择，但更大的 LLM 主干有助于缓解。合成攻击与真实世界（手写）攻击相似，验证了其研究价值。\n\n17. **#67 BIASINSPECTOR: 通过 LLM 智能体检测结构化数据中的偏见 (BIASINSPECTOR: Detecting Bias in Structured Data through LLM Agents)**\n    *   作者：Haoxuan Li, Mingyu Derek Ma, Jen-tse Huang 等\n    *   TLDR: 提出首个端到端、多智能体协同框架 BIASINSPECTOR，用于根据用户需求自动检测结构化数据中的偏见。该框架能制定多阶段计划，利用多样化工具执行分析，并提供包含解释和可视化的详细结果。同时提出了一个包含多评估指标和大量测试用例的基准。实验证明该框架在结构化数据偏见检测方面表现出色。\n\n18. **#4 AI 辅助决策中的移动目标：数据集漂移、模型更新和更新不透明性问题 (A moving target in AI-assisted decision-making: Dataset shift, model updating, and the problem of update opacity)**\n    *   作者：Joshua Hatherley\n    *   TLDR: 探讨了模型更新对 AI 辅助决策过程的影响，特别是引入了一种新的不透明性——“更新不透明性”，即用户无法理解更新如何或为何改变了模型的推理或行为。这种不透明性带来了独特的认知和安全问题，现有黑盒问题解决方案难以应对。讨论了双事实解释、动态模型报告和更新兼容性等潜在策略及其局限性。\n\n19. **#37 测量正确的事物：在 AI 影响评估中证明指标的合理性 (Measuring the right thing: justifying metrics in AI impact assessments)**\n    *   作者：Stefan Buijsman, Herman Veluwenkamp\n    *   TLDR: 提出一个两步法来确保 AI 影响评估中使用的指标得到合理证明：首先明确概念（如罗尔斯式公平），然后选择适合该概念的指标。强调概念工程有助于第一步，而概念的额外内容有助于证明第二步中特定指标选择的合理性。主张影响评估不仅要明确指标，还要明确驱动这些指标的概念。\n\n---\n\n**强化学习与智能体**\n\n20. **#31 用 LLM 进行算法发现：进化搜索与强化学习的结合 (Algorithm Discovery With LLMs: Evolutionary Search Meets Reinforcement Learning)**\n    *   作者：Anja Surina, Amin Mansouri, Lars Quaedvlieg 等\n    *   TLDR: 提出一种结合 LLM 进化搜索和强化学习 (RL) 微调的方法来发现新算法。进化搜索作为探索策略发现改进算法，RL 则根据这些发现优化 LLM 策略（搜索算子）。在装箱、旅行商和 flatpack 问题上的实验表明，结合 RL 和进化搜索提高了发现效率。\n\n21. **#34 非必要不辩论：用于高效 LLM 推理的自适应多智能体协作 (Debate Only When Necessary: Adaptive Multiagent Collaboration for Efficient LLM Reasoning)**\n    *   作者：Sugyeong Eo, Hyeonseok Moon, Evelyn Hayoon Zi 等\n    *   TLDR: 提出 DOWN (Debate Only When Necessary) 框架，一种自适应多智能体辩论方法。该框架根据智能体初始响应的置信度得分选择性地激活辩论过程。若触发辩论，智能体利用参与者的响应及其置信度得分来优化输出。实验证明，该机制显著提高了效率，同时保持甚至超越了现有辩论系统的性能，并能减轻错误传播。\n\n22. **#68 合作式多智能体学习问题的有效方法 (An Efficient Approach for Cooperative Multi-Agent Learning Problems)**\n    *   作者：Ángel Aso-Mollar, Eva Onaindia\n    *   TLDR: 提出一个中心化的多智能体学习框架，通过引入一个名为“supervisor”的元智能体，将联合动作抽象为对每个智能体的顺序动作分配，从而克服中心化方法中联合动作空间爆炸导致的扩展性问题。实验证明该方法能在不同规模的多智能体环境中成功协调智能体。\n\n23. **#94 HypRL: 超属性控制策略的强化学习 (HypRL: Reinforcement Learning of Control Policies for Hyperproperties)**\n    *   作者：Tzu-Han Hsu, Arshia Rafieioskouei, Borzoo Bonakdarpour\n    *   TLDR: 研究学习满足超属性（Hyperproperty, 如隐私、公平性）要求的控制策略问题。利用 Skolemization 处理量词交替，引入 HyperLTL 的量化鲁棒性函数定义奖励，并使用 RL 算法学习策略和未知环境（MDP）的转移概率，以最大化满足超属性的期望奖励。在多智能体路径规划、资源分配公平性和 PCP 问题上进行了案例研究。\n\n---\n\n**计算机视觉与多模态应用**\n\n24. **#60 Lumina-OmniLV: 面向通用低层视觉的统一多模态框架 (Lumina-OmniLV: A Unified Multimodal Framework for General Low-Level Vision)**\n    *   作者：Yuandong Pu, Le Zhuo, Kaiwen Zhu 等\n    *   TLDR: 提出 Lunima-OmniLV (OmniLV)，一个通用的多模态多任务低层视觉框架，处理图像恢复、增强、弱语义密集预测和风格化四大类超过 100 个子任务。利用文本和视觉提示进行交互，基于 Diffusion Transformer (DiT)，支持任意分辨率（1K 分辨率下性能最佳）。研究发现分别编码文本和视觉指令，并结合浅层特征控制的协同训练，对减轻任务歧义和增强多任务泛化至关重要。\n\n25. **#22 SSLFusion: 用于多模态 3D 目标检测的尺度与空间对齐潜在融合模型 (SSLFusion: Scale & Space Aligned Latent Fusion Model for Multimodal 3D Object Detection)**\n    *   作者：Bonan Ding, Jin Xie, Jing Nie 等\n    *   TLDR: 提出 SSLFusion 模型解决 2D 图像和 3D 点云特征间尺度与空间信息不对齐的问题。包含尺度对齐融合策略 (SAF)、3D 到 2D 空间对齐模块 (SAM) 和潜在跨模态融合模块 (LFM)。SAF 聚合多层次特征以处理不同尺度物体；SAM 将 3D 坐标信息融入 2D 特征以缩小模态差距；LFM 在潜在空间捕获跨模态非局部上下文，避免了计算复杂的 QKV 注意力。在 KITTI 和 DENSE 数据集上取得 SOTA 性能。\n\n26. **#27 EffOWT: 高效且有效地将视觉语言模型迁移到开放世界跟踪 (EffOWT: Transfer Visual Language Models to Open-World Tracking Efficiently and Effectively)**\n    *   作者：Bingyang Wang, Kaer Huang, Bin Li 等\n    *   TLDR: 提出 EffOWT 框架，用于高效地将视觉语言模型 (VLM) 迁移到开放世界跟踪 (OWT) 任务。通过构建一个独立的可学习侧网络，冻结 VLM 主干并仅在侧网络上反向传播，满足效率要求。侧网络采用 Transformer 和 CNN 的混合结构，并实现 MLP 上的稀疏交互，显著减少参数更新和内存成本。在未知类别的跟踪指标 OWTA 上提升 5.5%，仅更新 1.3% 参数，节省 36.4% 内存。\n\n27. **#40 RS-RAG: 连接遥感影像与综合知识的多模态数据集与检索增强生成模型 (RS-RAG: Bridging Remote Sensing Imagery and Comprehensive Knowledge with a Multi-Modal Dataset and Retrieval-Augmented Generation Model)**\n    *   作者：Congcong Wen, Yiting Lin, Xiaokang Qu 等\n    *   TLDR: 针对现有遥感 VLM 缺乏外部知识整合能力的问题，首先构建了 RSWK 数据集，包含 14141 个全球著名地标的高分辨率卫星影像和详细文本描述。然后提出 RS-RAG 框架，包含多模态知识向量数据库构建模块和知识检索与响应生成模块，后者根据查询检索并重排相关知识，融入知识增强提示以指导 VLM 生成上下文相关的响应。在图像描述、分类和 VQA 任务上显著优于基线。\n\n28. **#59 Video-Bench: 人类对齐的视频生成基准 (Video-Bench: Human-Aligned Video Generation Benchmark)**\n    *   作者：Hui Han, Siyuan Li, Jiaqi Chen 等\n    *   TLDR: 提出 Video-Bench，一个全面的视频生成评估基准，包含丰富的提示套件和广泛的评估维度。首次系统性地利用 MLLM 评估视频生成模型的所有相关维度。通过 few-shot 评分和查询链技术，提供结构化、可扩展的评估方法。在 Sora 等先进模型上的实验表明，Video-Bench 在所有维度上均优于人类偏好对齐，并在评估与人类判断不一致时提供更客观准确的见解。\n\n29. **#70 从特异性到通用性：重新审视人脸 Deepfake 检测中的可泛化伪影 (From Specificity to Generality: Revisiting Generalizable Artifacts in Detecting Face Deepfakes)**\n    *   作者：Long Ma, Zhiyuan Yan, Yize Chen 等\n    *   TLDR: 提出将 Deepfake 伪影分为人脸不一致性伪影 (FIA) 和上采样伪影 (USA) 两类通用类型。设计了一种数据级伪造样本创建框架，仅包含 FIA 和 USA，避免引入其他不通用的伪影。使用超分辨率模拟 USA，设计 Blender 模块通过图像级自混合创建 FIA。实验表明，仅用此伪造数据训练的标准分类器能很好地泛化到未见过的 Deepfake。\n\n---\n\n**其他值得关注的论文**\n\n*   **#3 Dion: 面向大模型的高效通信优化器 (Dion: A Communication-Efficient Optimizer for Large Models)**: 提出 Dion 优化器，利用正交化更新和设备本地动量缓冲区减少分布式训练中的梯度同步通信开销，并支持高效分片策略。\n*   **#4 医学领域大语言模型不确定性量化的挑战 (The challenge of uncertainty quantification of large language models in medicine)**: 探讨医学 LLM 的不确定性量化，提出结合贝叶斯、集成学习、蒙特卡洛 dropout 和语言学分析的框架，管理认知和偶然不确定性，强调透明和负责任 AI。\n*   **#6 学习跨时间推理：用于改进语言模型时间推理的时间线自我反思 (Learning to Reason Over Time: Timeline Self-Reflection for Improved Temporal Reasoning in Language Models)**: 提出 TISER 框架，通过时间线构建和迭代自我反思增强 LLM 的时间推理能力，利用测试时扩展推理轨迹长度，在多个基准上达到 SOTA。\n*   **#9 PINNverse: 基于约束物理信息神经网络从噪声数据中准确估计微分方程参数 (PINNverse: Accurate parameter estimation in differential equations from noisy data with constrained physics-informed neural networks)**: 提出 PINNverse 训练范式，将 PINN 学习过程重构为约束微分优化问题，解决收敛、稳定性、过拟合等问题，实现对噪声数据的鲁棒准确参数估计。\n*   **#19 轻量级直接文档相关性优化用于生成式信息检索 (Lightweight and Direct Document Relevance Optimization for Generative Information Retrieval)**: 提出 DDRO 方法，通过直接成对排序优化，将 token 级 docid 生成与文档级相关性估计对齐，无需奖励建模和强化学习，简化了 GenIR 模型的排序优化流程并提升了检索效果。\n*   **#20 BRIDGES: 在 EDA 任务中连接图模态与大语言模型 (BRIDGES: Bridging Graph Modality and Large Language Models within EDA Tasks)**: 提出 BRIDGES 框架，将图模态（如数据流图、网表图）整合到 EDA 任务的 LLM 中。包含自动化数据生成流程和轻量级跨模态投影器，显著提升了设计检索、类型预测等任务的性能。\n*   **#50 Llama 走进“Bar”：多州律师资格考试中法律推理的高效监督微调 (A Llama walks into the 'Bar': Efficient Supervised Fine-Tuning for Legal Reasoning in the Multi-state Bar Exam)**: 研究了如何用有限数据（1514 个 MBE 问题）有效微调 Llama 2 7B 和 Llama 3 8B 模型以提高法律问答准确性。发现领域特定的 SFT 能使小模型接近人类基线表现，并发布了 SFT 数据集和适配器。\n*   **#51 Lemmanaid: 神经符号引理猜想 (Lemmanaid: Neuro-Symbolic Lemma Conjecturing)**: 提出 Lemmanaid，结合 LLM 和符号方法的神经符号引理猜想工具。LLM 生成引理模板，符号方法填充细节。结果表明神经和符号技术互补，能为计算机辅助理论发展生成有用引理。\n*   **#73 ELT-Bench: 评估 AI 智能体在 ELT 管道上能力的端到端基准 (ELT-Bench: An End-to-End Benchmark for Evaluating AI Agents on ELT Pipelines)**: 提出 ELT-Bench，首个评估 AI 智能体构建端到端提取-加载-转换 (ELT) 管道能力的基准。包含 100 个管道，测试表明现有 AI 智能体在处理复杂数据工程工作流方面仍面临挑战。\n*   **#85 T1: 小型语言模型中测试时计算扩展的工具集成自验证 (T1: Tool-integrated Self-verification for Test-time Compute Scaling in Small Language Models)**: 探索小型语言模型 (sLM) 在测试时计算扩展下的自验证能力。发现 sLM 难以处理需要记忆的验证任务。提出 T1 (工具集成自验证)，将记忆密集型步骤委托给外部工具（如代码解释器），显著提高了 sLM 的自验证能力和测试时扩展性能。\n*   **#87 从自产数据中泛化：超越人类约束的模型训练 (Generalising from Self-Produced Data: Model Training Beyond Human Constraints)**: 提出一个 AI 模型通过与环境直接交互来自主生成和验证新知识的框架。使用无界、不可博弈的数字奖励（如磁盘空间）指导学习，AI 迭代生成策略和代码以最大化指标，成功结果用于自我再训练和增量泛化，旨在实现超越人类约束的自改进 AI。\n*   **#89 LagKV: KV 缓存的滞后相对信息指示哪些 Token 重要 (LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are Important)**: 提出 LagKV，一种仅依赖于 KV 缓存自身比较的 KV 缓存分配策略，用于减少长上下文推理中的缓存大小。该方法无需修改注意力机制，易于集成且性能接近复杂的 KV 压缩方法。\n*   **#90 证明语言模型通过梯度下降学习多数布尔逻辑失败 (Provable Failure of Language Models in Learning Majority Boolean Logic via Gradient Descent)**: 理论分析表明，即使 Transformer 架构理论上能表示简单逻辑函数（如多数门），但使用梯度下降训练时，学习简单多数函数存在根本的优化挑战。即使经过多项式次数的梯度查询，模型的泛化误差仍然很大。\n*   **#91 R2Vul: 利用强化学习和结构化推理蒸馏学习推理软件漏洞 (R2Vul: Learning to Reason about Software Vulnerabilities with Reinforcement Learning and Structured Reasoning Distillation)**: 提出 R2Vul，使用来自 AI 反馈的强化学习 (RLAIF) 将结构化推理蒸馏到小型 LLM 中，用于软件漏洞检测。使 LLM 能够生成结构化、安全感知、可操作且可靠的推理，并明确区分有效评估和误导性评估。\n\n---\n\n今天的快报就到这里，希望能帮助你快速了解 arXiv 的最新动态！",
  "papers": [
    {
      "arxiv_id": "2504.05305v1",
      "title": "URECA: Unique Region Caption Anything",
      "title_zh": "URECA：独特区域描述一切\n",
      "authors": [
        "Sangbeom Lim",
        "Junwan Kim",
        "Heeji Yoon",
        "Jaewoo Jung",
        "Seungryong Kim"
      ],
      "abstract": "Region-level captioning aims to generate natural language descriptions for\nspecific image regions while highlighting their distinguishing features.\nHowever, existing methods struggle to produce unique captions across\nmulti-granularity, limiting their real-world applicability. To address the need\nfor detailed region-level understanding, we introduce URECA dataset, a\nlarge-scale dataset tailored for multi-granularity region captioning. Unlike\nprior datasets that focus primarily on salient objects, URECA dataset ensures a\nunique and consistent mapping between regions and captions by incorporating a\ndiverse set of objects, parts, and background elements. Central to this is a\nstage-wise data curation pipeline, where each stage incrementally refines\nregion selection and caption generation. By leveraging Multimodal Large\nLanguage Models (MLLMs) at each stage, our pipeline produces distinctive and\ncontextually grounded captions with improved accuracy and semantic diversity.\nBuilding upon this dataset, we present URECA, a novel captioning model designed\nto effectively encode multi-granularity regions. URECA maintains essential\nspatial properties such as position and shape through simple yet impactful\nmodifications to existing MLLMs, enabling fine-grained and semantically rich\nregion descriptions. Our approach introduces dynamic mask modeling and a\nhigh-resolution mask encoder to enhance caption uniqueness. Experiments show\nthat URECA achieves state-of-the-art performance on URECA dataset and\ngeneralizes well to existing region-level captioning benchmarks.",
      "tldr_zh": "该论文提出了URECA数据集，一个大规模的多粒度区域描述数据集，旨在解决现有方法在生成独特区域描述方面的不足。URECA数据集通过阶段式数据管理流程，利用多模态大型语言模型(MLLMs)生成具有区分性和上下文关联的描述，涵盖了对象、部件和背景元素等多种元素。同时，论文还提出了名为URECA的新型描述模型，通过动态mask建模和高分辨率mask编码器，有效编码多粒度区域并保持重要的空间属性。实验结果表明，URECA在URECA数据集上取得了state-of-the-art的性能，并且能够很好地泛化到现有的区域描述benchmark上。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://cvlab-kaist.github.io/URECA Code:\n  https://github.com/cvlab-kaist/URECA",
      "pdf_url": "http://arxiv.org/pdf/2504.05305v1",
      "published_date": "2025-04-07 17:59:44 UTC",
      "updated_date": "2025-04-07 17:59:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:04:04.629432"
    },
    {
      "arxiv_id": "2504.05299v1",
      "title": "SmolVLM: Redefining small and efficient multimodal models",
      "title_zh": "SmolVLM：重新定义小型高效的多模态模型\n",
      "authors": [
        "Andrés Marafioti",
        "Orr Zohar",
        "Miquel Farré",
        "Merve Noyan",
        "Elie Bakouch",
        "Pedro Cuenca",
        "Cyril Zakka",
        "Loubna Ben Allal",
        "Anton Lozhkov",
        "Nouamane Tazi",
        "Vaibhav Srivastav",
        "Joshua Lochner",
        "Hugo Larcher",
        "Mathieu Morlon",
        "Lewis Tunstall",
        "Leandro von Werra",
        "Thomas Wolf"
      ],
      "abstract": "Large Vision-Language Models (VLMs) deliver exceptional performance but\nrequire significant computational resources, limiting their deployment on\nmobile and edge devices. Smaller VLMs typically mirror design choices of larger\nmodels, such as extensive image tokenization, leading to inefficient GPU memory\nusage and constrained practicality for on-device applications.\n  We introduce SmolVLM, a series of compact multimodal models specifically\nengineered for resource-efficient inference. We systematically explore\narchitectural configurations, tokenization strategies, and data curation\noptimized for low computational overhead. Through this, we identify key design\nchoices that yield substantial performance gains on image and video tasks with\nminimal memory footprints.\n  Our smallest model, SmolVLM-256M, uses less than 1GB GPU memory during\ninference and outperforms the 300-times larger Idefics-80B model, despite an\n18-month development gap. Our largest model, at 2.2B parameters, rivals\nstate-of-the-art VLMs consuming twice the GPU memory. SmolVLM models extend\nbeyond static images, demonstrating robust video comprehension capabilities.\n  Our results emphasize that strategic architectural optimizations, aggressive\nyet efficient tokenization, and carefully curated training data significantly\nenhance multimodal performance, facilitating practical, energy-efficient\ndeployments at significantly smaller scales.",
      "tldr_zh": "SmolVLM提出了一系列紧凑的多模态模型，专为资源高效的推理而设计。通过系统地探索架构配置、tokenization策略和数据管理，SmolVLM旨在克服大型视觉语言模型(VLMs)计算资源需求高，难以在移动和边缘设备上部署的问题。实验表明，SmolVLM在图像和视频任务上表现出色，其最小的模型SmolVLM-256M仅使用不到1GB GPU内存，性能优于大300倍的Idefics-80B模型。即使是参数量为2.2B的最大模型，也能与消耗两倍GPU内存的state-of-the-art VLMs相媲美。该研究强调，战略性的架构优化、高效的tokenization以及精心策划的训练数据能够显著提升多模态性能，促进在更小规模上的实际部署。\n",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.05299v1",
      "published_date": "2025-04-07 17:58:57 UTC",
      "updated_date": "2025-04-07 17:58:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:04:16.988335"
    },
    {
      "arxiv_id": "2504.05295v1",
      "title": "Dion: A Communication-Efficient Optimizer for Large Models",
      "title_zh": "Dion：一种用于大型模型的通信高效优化器\n",
      "authors": [
        "Kwangjun Ahn",
        "Byron Xu"
      ],
      "abstract": "Training large AI models efficiently requires distributing computation across\nmultiple accelerators, but this often incurs significant communication overhead\n-- especially during gradient synchronization. We introduce Dion, a\ncommunication-efficient optimizer that retains the synchronous semantics of\nstandard distributed training (e.g., DDP, FSDP) while substantially reducing\nI/O costs. Unlike conventional optimizers that synchronize full gradient\nmatrices, Dion leverages orthonormalized updates with device-local momentum\nbuffers, eliminating the need for full gradient exchange. It further supports\nan efficient sharding strategy that avoids reconstructing large matrices during\ntraining.",
      "tldr_zh": "Dion是一种用于训练大型模型的高效通信优化器。它在保持标准分布式训练（如DDP、FSDP）同步语义的同时，显著降低了I/O成本。与同步完整梯度矩阵的传统优化器不同，Dion利用带有设备本地动量缓冲区的正交归一化更新，无需完整梯度交换。此外，它还支持一种高效的分片策略，避免在训练期间重建大型矩阵。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "technical report; comments welcome!",
      "pdf_url": "http://arxiv.org/pdf/2504.05295v1",
      "published_date": "2025-04-07 17:49:37 UTC",
      "updated_date": "2025-04-07 17:49:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:04:28.227288"
    },
    {
      "arxiv_id": "2504.05278v1",
      "title": "The challenge of uncertainty quantification of large language models in medicine",
      "title_zh": "医学领域中大型语言模型的不确定性量化所面临的挑战\n",
      "authors": [
        "Zahra Atf",
        "Seyed Amir Ahmad Safavi-Naini",
        "Peter R. Lewis",
        "Aref Mahjoubfar",
        "Nariman Naderi",
        "Thomas R. Savage",
        "Ali Soroush"
      ],
      "abstract": "This study investigates uncertainty quantification in large language models\n(LLMs) for medical applications, emphasizing both technical innovations and\nphilosophical implications. As LLMs become integral to clinical\ndecision-making, accurately communicating uncertainty is crucial for ensuring\nreliable, safe, and ethical AI-assisted healthcare. Our research frames\nuncertainty not as a barrier but as an essential part of knowledge that invites\na dynamic and reflective approach to AI design. By integrating advanced\nprobabilistic methods such as Bayesian inference, deep ensembles, and Monte\nCarlo dropout with linguistic analysis that computes predictive and semantic\nentropy, we propose a comprehensive framework that manages both epistemic and\naleatoric uncertainties. The framework incorporates surrogate modeling to\naddress limitations of proprietary APIs, multi-source data integration for\nbetter context, and dynamic calibration via continual and meta-learning.\nExplainability is embedded through uncertainty maps and confidence metrics to\nsupport user trust and clinical interpretability. Our approach supports\ntransparent and ethical decision-making aligned with Responsible and Reflective\nAI principles. Philosophically, we advocate accepting controlled ambiguity\ninstead of striving for absolute predictability, recognizing the inherent\nprovisionality of medical knowledge.",
      "tldr_zh": "本研究探讨了大型语言模型(LLMs)在医学应用中的不确定性量化问题，强调了技术创新和哲学意义。研究提出了一个综合框架，结合贝叶斯推断、深度集成和蒙特卡洛dropout等概率方法与语言分析，管理认知和偶然不确定性。该框架利用代理建模解决专有API的局限性，通过持续和元学习进行动态校准，并通过不确定性图和置信度指标嵌入可解释性。研究提倡接受可控的模糊性，而非追求绝对的可预测性，认识到医学知识的内在临时性，从而支持透明和符合伦理的决策，并与负责任和反思性人工智能原则保持一致。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "25 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.05278v1",
      "published_date": "2025-04-07 17:24:11 UTC",
      "updated_date": "2025-04-07 17:24:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:04:40.918198"
    },
    {
      "arxiv_id": "2504.05259v1",
      "title": "How to evaluate control measures for LLM agents? A trajectory from today to superintelligence",
      "title_zh": "如何评估 LLM 智能体的控制措施？从今天到超级智能的轨迹\n",
      "authors": [
        "Tomek Korbak",
        "Mikita Balesni",
        "Buck Shlegeris",
        "Geoffrey Irving"
      ],
      "abstract": "As LLM agents grow more capable of causing harm autonomously, AI developers\nwill rely on increasingly sophisticated control measures to prevent possibly\nmisaligned agents from causing harm. AI developers could demonstrate that their\ncontrol measures are sufficient by running control evaluations: testing\nexercises in which a red team produces agents that try to subvert control\nmeasures. To ensure control evaluations accurately capture misalignment risks,\nthe affordances granted to this red team should be adapted to the capability\nprofiles of the agents to be deployed under control measures.\n  In this paper we propose a systematic framework for adapting affordances of\nred teams to advancing AI capabilities. Rather than assuming that agents will\nalways execute the best attack strategies known to humans, we demonstrate how\nknowledge of an agents's actual capability profile can inform proportional\ncontrol evaluations, resulting in more practical and cost-effective control\nmeasures. We illustrate our framework by considering a sequence of five\nfictional models (M1-M5) with progressively advanced capabilities, defining\nfive distinct AI control levels (ACLs). For each ACL, we provide example rules\nfor control evaluation, control measures, and safety cases that could be\nappropriate. Finally, we show why constructing a compelling AI control safety\ncase for superintelligent LLM agents will require research breakthroughs,\nhighlighting that we might eventually need alternative approaches to mitigating\nmisalignment risk.",
      "tldr_zh": "本文提出了一个系统性的框架，用于评估LLM Agent的控制措施，并使其与AI能力的提升相适应。该框架的核心思想是，红队（负责评估控制措施有效性的团队）所拥有的资源和能力应该与被评估的Agent的能力相匹配，从而更准确地捕捉潜在的风险。文章通过五个虚构模型(M1-M5)和五个AI控制级别(ACLs)来阐述该框架，为每个ACL提供了控制评估规则、控制措施和安全案例示例。最后，文章强调了构建针对超智能LLM Agent的可靠AI控制安全案例需要突破性研究，并可能需要替代方法来降低风险。\n",
      "categories": [
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.05259v1",
      "published_date": "2025-04-07 16:52:52 UTC",
      "updated_date": "2025-04-07 16:52:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:04:52.614546"
    },
    {
      "arxiv_id": "2504.05258v1",
      "title": "Learning to Reason Over Time: Timeline Self-Reflection for Improved Temporal Reasoning in Language Models",
      "title_zh": "学习随时间进行推理：时间线自我反思以改进语言模型中的时间推理\n",
      "authors": [
        "Adrián Bazaga",
        "Rexhina Blloshmi",
        "Bill Byrne",
        "Adrià de Gispert"
      ],
      "abstract": "Large Language Models (LLMs) have emerged as powerful tools for generating\ncoherent text, understanding context, and performing reasoning tasks. However,\nthey struggle with temporal reasoning, which requires processing time-related\ninformation such as event sequencing, durations, and inter-temporal\nrelationships. These capabilities are critical for applications including\nquestion answering, scheduling, and historical analysis. In this paper, we\nintroduce TISER, a novel framework that enhances the temporal reasoning\nabilities of LLMs through a multi-stage process that combines timeline\nconstruction with iterative self-reflection. Our approach leverages test-time\nscaling to extend the length of reasoning traces, enabling models to capture\ncomplex temporal dependencies more effectively. This strategy not only boosts\nreasoning accuracy but also improves the traceability of the inference process.\nExperimental results demonstrate state-of-the-art performance across multiple\nbenchmarks, including out-of-distribution test sets, and reveal that TISER\nenables smaller open-source models to surpass larger closed-weight models on\nchallenging temporal reasoning tasks.",
      "tldr_zh": "该论文提出了一种名为TISER的新框架，旨在提升大型语言模型(LLMs)的时间推理能力。TISER通过多阶段流程，结合时间线构建和迭代自反思，增强模型处理时间相关信息的能力，例如事件排序、持续时间和时间关系。该方法利用测试时缩放(test-time scaling)来扩展推理轨迹的长度，从而更有效地捕捉复杂的时序依赖关系。实验结果表明，TISER在多个基准测试中表现出最先进的性能，甚至使较小的开源模型在具有挑战性的时间推理任务上超越了较大的闭源模型。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.05258v1",
      "published_date": "2025-04-07 16:51:45 UTC",
      "updated_date": "2025-04-07 16:51:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:05:04.534215"
    },
    {
      "arxiv_id": "2504.05254v1",
      "title": "Explaining Low Perception Model Competency with High-Competency Counterfactuals",
      "title_zh": "用高置信度反事实解释低感知模型能力",
      "authors": [
        "Sara Pohland",
        "Claire Tomlin"
      ],
      "abstract": "There exist many methods to explain how an image classification model\ngenerates its decision, but very little work has explored methods to explain\nwhy a classifier might lack confidence in its prediction. As there are various\nreasons the classifier might lose confidence, it would be valuable for this\nmodel to not only indicate its level of uncertainty but also explain why it is\nuncertain. Counterfactual images have been used to visualize changes that could\nbe made to an image to generate a different classification decision. In this\nwork, we explore the use of counterfactuals to offer an explanation for low\nmodel competency--a generalized form of predictive uncertainty that measures\nconfidence. Toward this end, we develop five novel methods to generate\nhigh-competency counterfactual images, namely Image Gradient Descent (IGD),\nFeature Gradient Descent (FGD), Autoencoder Reconstruction (Reco), Latent\nGradient Descent (LGD), and Latent Nearest Neighbors (LNN). We evaluate these\nmethods across two unique datasets containing images with six known causes for\nlow model competency and find Reco, LGD, and LNN to be the most promising\nmethods for counterfactual generation. We further evaluate how these three\nmethods can be utilized by pre-trained Multimodal Large Language Models (MLLMs)\nto generate language explanations for low model competency. We find that the\ninclusion of a counterfactual image in the language model query greatly\nincreases the ability of the model to generate an accurate explanation for the\ncause of low model competency, thus demonstrating the utility of counterfactual\nimages in explaining low perception model competency.",
      "tldr_zh": "该研究针对图像分类模型缺乏预测置信度的问题，提出了利用高置信度反事实图像(high-competency counterfactual images)来解释模型低能力(low model competency)的方法。研究开发了五种生成高置信度反事实图像的新方法：图像梯度下降(IGD)、特征梯度下降(FGD)、自编码器重建(Reco)、潜在梯度下降(LGD)和潜在最近邻(LNN)。实验结果表明，Reco、LGD和LNN在包含六种已知低模型能力原因的数据集上表现最佳。进一步研究表明，将反事实图像纳入多模态大型语言模型(MLLMs)的查询中，能显著提高模型生成准确解释低模型能力原因的能力，验证了反事实图像在解释低感知模型能力方面的有效性。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.05254v1",
      "published_date": "2025-04-07 16:46:52 UTC",
      "updated_date": "2025-04-07 16:46:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:05:16.671836"
    },
    {
      "arxiv_id": "2504.05255v1",
      "title": "Adversarial KA",
      "title_zh": "对抗性 KA\n",
      "authors": [
        "Sviatoslav Dzhenzher",
        "Michael H. Freedman"
      ],
      "abstract": "Regarding the representation theorem of Kolmogorov and Arnold (KA) as an\nalgorithm for representing or {\\guillemotleft}expressing{\\guillemotright}\nfunctions, we test its robustness by analyzing its ability to withstand\nadversarial attacks. We find KA to be robust to countable collections of\ncontinuous adversaries, but unearth a question about the equi-continuity of the\nouter functions that, so far, obstructs taking limits and defeating continuous\ngroups of adversaries. This question on the regularity of the outer functions\nis relevant to the debate over the applicability of KA to the general theory of\nNNs.",
      "tldr_zh": "该论文研究了Kolmogorov-Arnold (KA) 表示定理作为函数表示算法的鲁棒性，通过对抗攻击分析其抵抗能力。研究发现KA对可数个连续对抗攻击具有鲁棒性。然而，论文揭示了一个关于外层函数等度连续性的问题，这阻碍了取极限和抵抗连续对抗攻击群。此外，关于外层函数正则性的问题与KA在神经网络(NNs)通用理论中的适用性讨论相关。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.FA"
      ],
      "primary_category": "cs.LG",
      "comment": "8 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.05255v1",
      "published_date": "2025-04-07 16:46:52 UTC",
      "updated_date": "2025-04-07 16:46:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:05:28.394786"
    },
    {
      "arxiv_id": "2504.05248v1",
      "title": "PINNverse: Accurate parameter estimation in differential equations from noisy data with constrained physics-informed neural networks",
      "title_zh": "PINNverse：利用带约束的物理信息神经网络，从含噪声数据中准确估计微分方程的参数\n",
      "authors": [
        "Marius Almanstötter",
        "Roman Vetter",
        "Dagmar Iber"
      ],
      "abstract": "Parameter estimation for differential equations from measured data is an\ninverse problem prevalent across quantitative sciences. Physics-Informed Neural\nNetworks (PINNs) have emerged as effective tools for solving such problems,\nespecially with sparse measurements and incomplete system information. However,\nPINNs face convergence issues, stability problems, overfitting, and complex\nloss function design. Here we introduce PINNverse, a training paradigm that\naddresses these limitations by reformulating the learning process as a\nconstrained differential optimization problem. This approach achieves a dynamic\nbalance between data loss and differential equation residual loss during\ntraining while preventing overfitting. PINNverse combines the advantages of\nPINNs with the Modified Differential Method of Multipliers to enable\nconvergence on any point on the Pareto front. We demonstrate robust and\naccurate parameter estimation from noisy data in four classical ODE and PDE\nmodels from physics and biology. Our method enables accurate parameter\ninference also when the forward problem is expensive to solve.",
      "tldr_zh": "该论文提出了PINNverse，一种新的训练范式，用于解决物理信息神经网络(PINNs)在从噪声数据中估计微分方程参数时遇到的收敛性、稳定性和过拟合问题。PINNverse将学习过程重新定义为约束微分优化问题，利用改进的微分乘子法，在数据损失和微分方程残差损失之间实现动态平衡，从而避免过拟合。该方法结合了PINNs的优点，能够在Pareto前沿上的任何点实现收敛。通过在物理和生物学中的四个经典ODE和PDE模型上的实验，证明了PINNverse能够从噪声数据中进行鲁棒和准确的参数估计，即使正向问题难以求解。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.comp-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "17 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.05248v1",
      "published_date": "2025-04-07 16:34:57 UTC",
      "updated_date": "2025-04-07 16:34:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:05:40.737207"
    },
    {
      "arxiv_id": "2504.05231v1",
      "title": "Mapping biodiversity at very-high resolution in Europe",
      "title_zh": "绘制欧洲极高分辨率的生物多样性地图",
      "authors": [
        "César Leblanc",
        "Lukas Picek",
        "Benjamin Deneu",
        "Pierre Bonnet",
        "Maximilien Servajean",
        "Rémi Palard",
        "Alexis Joly"
      ],
      "abstract": "This paper describes a cascading multimodal pipeline for high-resolution\nbiodiversity mapping across Europe, integrating species distribution modeling,\nbiodiversity indicators, and habitat classification. The proposed pipeline\nfirst predicts species compositions using a deep-SDM, a multimodal model\ntrained on remote sensing, climate time series, and species occurrence data at\n50x50m resolution. These predictions are then used to generate biodiversity\nindicator maps and classify habitats with Pl@ntBERT, a transformer-based LLM\ndesigned for species-to-habitat mapping. With this approach, continental-scale\nspecies distribution maps, biodiversity indicator maps, and habitat maps are\nproduced, providing fine-grained ecological insights. Unlike traditional\nmethods, this framework enables joint modeling of interspecies dependencies,\nbias-aware training with heterogeneous presence-absence data, and large-scale\ninference from multi-source remote sensing inputs.",
      "tldr_zh": "该论文提出了一种级联多模态流程，用于在欧洲进行高分辨率的生物多样性mapping。该流程集成了物种分布模型(species distribution modeling)、生物多样性指标和栖息地分类。首先，利用deep-SDM（一种多模态模型，基于遥感、气候时间序列和物种出现数据训练）预测50x50m分辨率的物种组成。然后，利用Pl@ntBERT（一种基于Transformer的LLM，专为物种到栖息地mapping而设计）生成生物多样性指标图和分类栖息地。该方法生成了大陆尺度的物种分布图、生物多样性指标图和栖息地地图，提供了精细的生态学见解，并且能够进行物种间依赖关系的联合建模，以及基于异构存在-缺失数据的偏差感知训练。\n",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.05231v1",
      "published_date": "2025-04-07 16:15:52 UTC",
      "updated_date": "2025-04-07 16:15:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:05:52.819118"
    },
    {
      "arxiv_id": "2504.05229v1",
      "title": "FinGrAct: A Framework for FINe-GRrained Evaluation of ACTionability in Explainable Automatic Fact-Checking",
      "title_zh": "FinGrAct：一种用于可解释自动事实核查中可操作性的细粒度评估框架\n",
      "authors": [
        "Islam Eldifrawi",
        "Shengrui Wang",
        "Amine Trabelsi"
      ],
      "abstract": "The field of explainable Automatic Fact-Checking (AFC) aims to enhance the\ntransparency and trustworthiness of automated fact-verification systems by\nproviding clear and comprehensible explanations. However, the effectiveness of\nthese explanations depends on their actionability --their ability to empower\nusers to make informed decisions and mitigate misinformation. Despite\nactionability being a critical property of high-quality explanations, no prior\nresearch has proposed a dedicated method to evaluate it. This paper introduces\nFinGrAct, a fine-grained evaluation framework that can access the web, and it\nis designed to assess actionability in AFC explanations through well-defined\ncriteria and an evaluation dataset. FinGrAct surpasses state-of-the-art (SOTA)\nevaluators, achieving the highest Pearson and Kendall correlation with human\njudgments while demonstrating the lowest ego-centric bias, making it a more\nrobust evaluation approach for actionability evaluation in AFC.",
      "tldr_zh": "该论文提出了FinGrAct，一个用于评估可解释的自动事实核查(AFC)中可操作性的细粒度评估框架。 现有研究缺乏对解释的可操作性的专门评估方法，而FinGrAct通过明确的标准和评估数据集来评估AFC解释的可操作性，并能够访问网络。实验结果表明，FinGrAct在与人类判断的相关性上优于当前最先进(SOTA)的评估器，同时表现出最低的自我中心偏差，使其成为AFC中可操作性评估的更稳健方法。 该框架旨在提升自动事实核查系统的透明度和可信度，帮助用户做出明智的决策并减少错误信息。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.05229v1",
      "published_date": "2025-04-07 16:14:27 UTC",
      "updated_date": "2025-04-07 16:14:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:06:04.780911"
    },
    {
      "arxiv_id": "2504.05220v2",
      "title": "Leveraging LLMs for Utility-Focused Annotation: Reducing Manual Effort for Retrieval and RAG",
      "title_zh": "利用大型语言模型进行以效用为中心的标注：减少检索和 RAG 的人工工作量\n",
      "authors": [
        "Hengran Zhang",
        "Minghao Tang",
        "Keping Bi",
        "Jiafeng Guo",
        "Shihao Liu",
        "Daiting Shi",
        "Dawei Yin",
        "Xueqi Cheng"
      ],
      "abstract": "Retrieval models typically rely on costly human-labeled query-document\nrelevance annotations for training and evaluation. To reduce this cost and\nleverage the potential of Large Language Models (LLMs) in relevance judgments,\nwe aim to explore whether LLM-generated annotations can effectively replace\nhuman annotations in training retrieval models. Retrieval usually emphasizes\nrelevance, which indicates \"topic-relatedness\" of a document to a query, while\nin RAG, the value of a document (or utility) depends on how it contributes to\nanswer generation. Recognizing this mismatch, some researchers use LLM\nperformance on downstream tasks with documents as labels, but this approach\nrequires manual answers for specific tasks, leading to high costs and limited\ngeneralization. In another line of work, prompting LLMs to select useful\ndocuments as RAG references eliminates the need for human annotation and is not\ntask-specific. If we leverage LLMs' utility judgments to annotate retrieval\ndata, we may retain cross-task generalization without human annotation in\nlarge-scale corpora. Therefore, we investigate utility-focused annotation via\nLLMs for large-scale retriever training data across both in-domain and\nout-of-domain settings on the retrieval and RAG tasks. To reduce the impact of\nlow-quality positives labeled by LLMs, we design a novel loss function, i.e.,\nDisj-InfoNCE. Our experiments reveal that: (1) Retrievers trained on\nutility-focused annotations significantly outperform those trained on human\nannotations in the out-of-domain setting on both tasks, demonstrating superior\ngeneralization capabilities. (2) LLM annotation does not replace human\nannotation in the in-domain setting. However, incorporating just 20%\nhuman-annotated data enables retrievers trained with utility-focused\nannotations to match the performance of models trained entirely with human\nannotations.",
      "tldr_zh": "该研究探索了利用大型语言模型(LLMs)进行效用驱动的标注，以减少检索模型和RAG应用中对人工标注的依赖。研究发现，传统的检索模型训练依赖于昂贵的人工标注的query-document相关性，而LLMs在相关性判断方面具有潜力。该研究提出利用LLMs的效用判断来标注检索数据，从而在大型语料库中实现跨任务泛化，无需人工标注。此外，作者设计了一种新的损失函数Disj-InfoNCE，以降低LLMs标注的低质量正样本的影响。实验结果表明，在领域外(out-of-domain)设置下，使用效用驱动标注训练的检索器在检索和RAG任务上显著优于使用人工标注训练的检索器，展现出更强的泛化能力。在领域内(in-domain)设置下，结合少量（20%）人工标注数据，使用效用驱动标注训练的检索器可以达到完全使用人工标注训练的模型的性能。\n",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "12 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.05220v2",
      "published_date": "2025-04-07 16:05:52 UTC",
      "updated_date": "2025-04-08 02:11:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:06:17.130921"
    },
    {
      "arxiv_id": "2504.05216v1",
      "title": "Unleashing the Power of LLMs in Dense Retrieval with Query Likelihood Modeling",
      "title_zh": "利用查询似然建模释放 LLM 在密集检索中的力量\n",
      "authors": [
        "Hengran Zhang",
        "Keping Bi",
        "Jiafeng Guo",
        "Xiaojie Sun",
        "Shihao Liu",
        "Daiting Shi",
        "Dawei Yin",
        "Xueqi Cheng"
      ],
      "abstract": "Dense retrieval is a crucial task in Information Retrieval (IR) and is the\nfoundation for downstream tasks such as re-ranking. Recently, large language\nmodels (LLMs) have shown compelling semantic understanding capabilities and are\nappealing to researchers studying dense retrieval. LLMs, as decoder-style\ngenerative models, are competent at language generation while falling short on\nmodeling global information due to the lack of attention to tokens afterward.\nInspired by the classical word-based language modeling approach for IR, i.e.,\nthe query likelihood (QL) model, we seek to sufficiently utilize LLMs'\ngenerative ability by QL maximization. However, instead of ranking documents\nwith QL estimation, we introduce an auxiliary task of QL maximization to yield\na better backbone for contrastively learning a discriminative retriever. We\nname our model as LLM-QL. To condense global document semantics to a single\nvector during QL modeling, LLM-QL has two major components, Attention Stop (AS)\nand Input Corruption (IC). AS stops the attention of predictive tokens to\nprevious tokens until the ending token of the document. IC masks a portion of\ntokens in the input documents during prediction. Experiments on MSMARCO show\nthat LLM-QL can achieve significantly better performance than other LLM-based\nretrievers and using QL estimated by LLM-QL for ranking outperforms word-based\nQL by a large margin.",
      "tldr_zh": "该论文提出了一种名为LLM-QL的新方法，旨在利用大型语言模型(LLMs)的强大能力来改进稠密检索。LLM-QL模型受到经典信息检索中的查询似然(QL)模型的启发，通过最大化QL来充分利用LLMs的生成能力，从而为对比学习训练判别式检索器提供更好的骨干网络。为了将全局文档语义压缩成单个向量，LLM-QL引入了注意力停止(Attention Stop, AS)和输入损坏(Input Corruption, IC)两个关键组件。在MSMARCO数据集上的实验表明，LLM-QL显著优于其他基于LLM的检索器，并且使用LLM-QL估计的QL进行排序也远胜于基于词的QL模型。\n",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "12 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.05216v1",
      "published_date": "2025-04-07 16:03:59 UTC",
      "updated_date": "2025-04-07 16:03:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:06:28.825475"
    },
    {
      "arxiv_id": "2504.05210v1",
      "title": "A moving target in AI-assisted decision-making: Dataset shift, model updating, and the problem of update opacity",
      "title_zh": "AI辅助决策中的动态目标：数据集偏移、模型更新与更新不透明性问题\n",
      "authors": [
        "Joshua Hatherley"
      ],
      "abstract": "Machine learning (ML) systems are vulnerable to performance decline over time\ndue to dataset shift. To address this problem, experts often suggest that ML\nsystems should be regularly updated to ensure ongoing performance stability.\nSome scholarly literature has begun to address the epistemic and ethical\nchallenges associated with different updating methodologies. Thus far, however,\nlittle attention has been paid to the impact of model updating on the\nML-assisted decision-making process itself, particularly in the AI ethics and\nAI epistemology literatures. This article aims to address this gap in the\nliterature. It argues that model updating introduces a new sub-type of opacity\ninto ML-assisted decision-making -- update opacity -- that occurs when users\ncannot understand how or why an update has changed the reasoning or behaviour\nof an ML system. This type of opacity presents a variety of distinctive\nepistemic and safety concerns that available solutions to the black box problem\nin ML are largely ill-equipped to address. A variety of alternative strategies\nmay be developed or pursued to address the problem of update opacity more\ndirectly, including bi-factual explanations, dynamic model reporting, and\nupdate compatibility. However, each of these strategies presents its own risks\nor carries significant limitations. Further research will be needed to address\nthe epistemic and safety concerns associated with model updating and update\nopacity going forward.",
      "tldr_zh": "本文探讨了机器学习(ML)辅助决策中，由于数据集偏移导致模型性能下降的问题，并分析了通过定期更新模型来解决此问题所带来的新挑战。文章指出，模型更新引入了一种新的不透明性——“更新不透明性”，即用户无法理解模型更新如何或为何改变了其推理或行为。这种不透明性带来了独特的认知和安全问题，现有解决ML黑盒问题的方法难以有效应对。文章提出了双事实解释、动态模型报告和更新兼容性等策略来缓解更新不透明性，但同时也指出了这些策略的局限性，并呼吁进一步研究以解决与模型更新相关的认知和安全问题。\n",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.05210v1",
      "published_date": "2025-04-07 15:58:23 UTC",
      "updated_date": "2025-04-07 15:58:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:06:40.709607"
    },
    {
      "arxiv_id": "2504.05207v1",
      "title": "Correcting Class Imbalances with Self-Training for Improved Universal Lesion Detection and Tagging",
      "title_zh": "利用自训练校正类别不平衡以改进通用病灶检测和标记\n",
      "authors": [
        "Alexander Shieh",
        "Tejas Sudharshan Mathai",
        "Jianfei Liu",
        "Angshuman Paul",
        "Ronald M. Summers"
      ],
      "abstract": "Universal lesion detection and tagging (ULDT) in CT studies is critical for\ntumor burden assessment and tracking the progression of lesion status\n(growth/shrinkage) over time. However, a lack of fully annotated data hinders\nthe development of effective ULDT approaches. Prior work used the DeepLesion\ndataset (4,427 patients, 10,594 studies, 32,120 CT slices, 32,735 lesions, 8\nbody part labels) for algorithmic development, but this dataset is not\ncompletely annotated and contains class imbalances. To address these issues, in\nthis work, we developed a self-training pipeline for ULDT. A VFNet model was\ntrained on a limited 11.5\\% subset of DeepLesion (bounding boxes + tags) to\ndetect and classify lesions in CT studies. Then, it identified and incorporated\nnovel lesion candidates from a larger unseen data subset into its training set,\nand self-trained itself over multiple rounds. Multiple self-training\nexperiments were conducted with different threshold policies to select\npredicted lesions with higher quality and cover the class imbalances. We\ndiscovered that direct self-training improved the sensitivities of\nover-represented lesion classes at the expense of under-represented classes.\nHowever, upsampling the lesions mined during self-training along with a\nvariable threshold policy yielded a 6.5\\% increase in sensitivity at 4 FP in\ncontrast to self-training without class balancing (72\\% vs 78.5\\%) and a 11.7\\%\nincrease compared to the same self-training policy without upsampling (66.8\\%\nvs 78.5\\%). Furthermore, we show that our results either improved or maintained\nthe sensitivity at 4FP for all 8 lesion classes.",
      "tldr_zh": "该研究针对CT图像中通用病灶检测和标记(ULDT)任务中数据不平衡和标注不完整的问题，提出了一种基于自训练的解决方案。首先，使用DeepLesion数据集的一小部分训练VFNet模型，然后利用该模型从未标注的数据中挖掘新的病灶候选样本，并将其加入训练集进行自训练。通过调整阈值策略和对自训练过程中挖掘的病灶进行上采样，缓解了类别不平衡问题。实验结果表明，该方法在保持或提高所有8个病灶类别敏感性的同时，相比于没有类别平衡的自训练，敏感性提高了6.5% (72% vs 78.5%)，相比于没有上采样的自训练，敏感性提高了11.7% (66.8% vs 78.5%)，有效提升了ULDT的性能。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Published at SPIE Medical Imaging 2023",
      "pdf_url": "http://arxiv.org/pdf/2504.05207v1",
      "published_date": "2025-04-07 15:57:03 UTC",
      "updated_date": "2025-04-07 15:57:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:06:52.917090"
    },
    {
      "arxiv_id": "2504.05201v1",
      "title": "3D Universal Lesion Detection and Tagging in CT with Self-Training",
      "title_zh": "基于自训练的 CT 图像 3D 通用病灶检测与标记\n",
      "authors": [
        "Jared Frazier",
        "Tejas Sudharshan Mathai",
        "Jianfei Liu",
        "Angshuman Paul",
        "Ronald M. Summers"
      ],
      "abstract": "Radiologists routinely perform the tedious task of lesion localization,\nclassification, and size measurement in computed tomography (CT) studies.\nUniversal lesion detection and tagging (ULDT) can simultaneously help alleviate\nthe cumbersome nature of lesion measurement and enable tumor burden assessment.\nPrevious ULDT approaches utilize the publicly available DeepLesion dataset,\nhowever it does not provide the full volumetric (3D) extent of lesions and also\ndisplays a severe class imbalance. In this work, we propose a self-training\npipeline to detect 3D lesions and tag them according to the body part they\noccur in. We used a significantly limited 30\\% subset of DeepLesion to train a\nVFNet model for 2D lesion detection and tagging. Next, the 2D lesion context\nwas expanded into 3D, and the mined 3D lesion proposals were integrated back\ninto the baseline training data in order to retrain the model over multiple\nrounds. Through the self-training procedure, our VFNet model learned from its\nown predictions, detected lesions in 3D, and tagged them. Our results indicated\nthat our VFNet model achieved an average sensitivity of 46.9\\% at [0.125:8]\nfalse positives (FP) with a limited 30\\% data subset in comparison to the\n46.8\\% of an existing approach that used the entire DeepLesion dataset. To our\nknowledge, we are the first to jointly detect lesions in 3D and tag them\naccording to the body part label.",
      "tldr_zh": "该论文提出了一种基于自训练的CT图像3D病灶通用检测与标注(ULDT)流程，旨在辅助放射科医生进行病灶定位、分类和大小测量。该方法首先使用DeepLesion数据集的30%子集训练VFNet模型进行2D病灶检测和标注，然后将2D病灶信息扩展到3D，并将挖掘出的3D病灶提议集成回训练数据，迭代训练模型。实验结果表明，该方法在有限的数据集下，达到了与使用整个DeepLesion数据集的现有方法相当的性能，实现了3D病灶的联合检测和按身体部位标注，平均敏感度为46.9%（在[0.125:8]假阳性下）。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Published at SPIE Medical Imaging 2023",
      "pdf_url": "http://arxiv.org/pdf/2504.05201v1",
      "published_date": "2025-04-07 15:50:27 UTC",
      "updated_date": "2025-04-07 15:50:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:07:04.812285"
    },
    {
      "arxiv_id": "2504.05196v1",
      "title": "Universal Lymph Node Detection in Multiparametric MRI with Selective Augmentation",
      "title_zh": "基于选择性增强的多参数 MRI 通用淋巴结检测\n",
      "authors": [
        "Tejas Sudharshan Mathai",
        "Sungwon Lee",
        "Thomas C. Shen",
        "Zhiyong Lu",
        "Ronald M. Summers"
      ],
      "abstract": "Robust localization of lymph nodes (LNs) in multiparametric MRI (mpMRI) is\ncritical for the assessment of lymphadenopathy. Radiologists routinely measure\nthe size of LN to distinguish benign from malignant nodes, which would require\nsubsequent cancer staging. Sizing is a cumbersome task compounded by the\ndiverse appearances of LNs in mpMRI, which renders their measurement difficult.\nFurthermore, smaller and potentially metastatic LNs could be missed during a\nbusy clinical day. To alleviate these imaging and workflow problems, we propose\na pipeline to universally detect both benign and metastatic nodes in the body\nfor their ensuing measurement. The recently proposed VFNet neural network was\nemployed to identify LN in T2 fat suppressed and diffusion weighted imaging\n(DWI) sequences acquired by various scanners with a variety of exam protocols.\nWe also use a selective augmentation technique known as Intra-Label LISA (ILL)\nto diversify the input data samples the model sees during training, such that\nit improves its robustness during the evaluation phase. We achieved a\nsensitivity of $\\sim$83\\% with ILL vs. $\\sim$80\\% without ILL at 4 FP/vol.\nCompared with current LN detection approaches evaluated on mpMRI, we show a\nsensitivity improvement of $\\sim$9\\% at 4 FP/vol.",
      "tldr_zh": "该论文提出了一种用于在多参数MRI (mpMRI)中进行通用淋巴结(LN)检测的流程，旨在辅助淋巴结肿大的评估。该流程使用VFNet神经网络在T2压脂和弥散加权成像(DWI)序列中识别LN，并采用选择性增强技术Intra-Label LISA (ILL)来提高模型的鲁棒性。实验结果表明，使用ILL时，在4个假阳性/体积下，灵敏度约为83%，相比不使用ILL时提高了约3%。与现有的mpMRI淋巴结检测方法相比，该方法在4个假阳性/体积下，灵敏度提高了约9%。\n",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "Published at SPIE Medical Imaging 2023",
      "pdf_url": "http://arxiv.org/pdf/2504.05196v1",
      "published_date": "2025-04-07 15:46:43 UTC",
      "updated_date": "2025-04-07 15:46:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:07:16.731236"
    },
    {
      "arxiv_id": "2504.05187v1",
      "title": "Resource-Efficient Beam Prediction in mmWave Communications with Multimodal Realistic Simulation Framework",
      "title_zh": "基于多模态真实模拟框架的毫米波通信资源高效波束预测\n",
      "authors": [
        "Yu Min Park",
        "Yan Kyaw Tun",
        "Walid Saad",
        "Choong Seon Hong"
      ],
      "abstract": "Beamforming is a key technology in millimeter-wave (mmWave) communications\nthat improves signal transmission by optimizing directionality and intensity.\nHowever, conventional channel estimation methods, such as pilot signals or beam\nsweeping, often fail to adapt to rapidly changing communication environments.\nTo address this limitation, multimodal sensing-aided beam prediction has gained\nsignificant attention, using various sensing data from devices such as LiDAR,\nradar, GPS, and RGB images to predict user locations or network conditions.\nDespite its promising potential, the adoption of multimodal sensing-aided beam\nprediction is hindered by high computational complexity, high costs, and\nlimited datasets. Thus, in this paper, a resource-efficient learning approach\nis proposed to transfer knowledge from a multimodal network to a monomodal\n(radar-only) network based on cross-modal relational knowledge distillation\n(CRKD), while reducing computational overhead and preserving predictive\naccuracy. To enable multimodal learning with realistic data, a novel multimodal\nsimulation framework is developed while integrating sensor data generated from\nthe autonomous driving simulator CARLA with MATLAB-based mmWave channel\nmodeling, and reflecting real-world conditions. The proposed CRKD achieves its\nobjective by distilling relational information across different feature spaces,\nwhich enhances beam prediction performance without relying on expensive sensor\ndata. Simulation results demonstrate that CRKD efficiently distills multimodal\nknowledge, allowing a radar-only model to achieve $94.62\\%$ of the teacher\nperformance. In particular, this is achieved with just $10\\%$ of the teacher\nnetwork's parameters, thereby significantly reducing computational complexity\nand dependence on multimodal sensor data.",
      "tldr_zh": "该论文提出了一种资源高效的学习方法，利用跨模态关系知识蒸馏(CRKD)，将多模态网络知识迁移到单模态（仅雷达）网络，以解决毫米波(mmWave)通信中波束预测的资源限制问题。为了实现逼真的多模态学习，论文开发了一个新的多模态仿真框架，集成了来自 CARLA 自动驾驶模拟器的传感器数据和基于 MATLAB 的毫米波信道建模，反映了真实世界的条件。CRKD 通过提取不同特征空间的关系信息来增强波束预测性能，无需依赖昂贵的传感器数据。仿真结果表明，CRKD 能够高效地提取多模态知识，使仅使用雷达的模型能够达到教师网络 94.62% 的性能，同时仅使用教师网络 10% 的参数，从而显著降低了计算复杂性和对多模态传感器数据的依赖。\n",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NI",
      "comment": "12 pages, 8 figures, Submitted to IEEE Transactions on Communications\n  on Apr. 07, 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.05187v1",
      "published_date": "2025-04-07 15:38:25 UTC",
      "updated_date": "2025-04-07 15:38:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:07:29.002423"
    },
    {
      "arxiv_id": "2504.05181v1",
      "title": "Lightweight and Direct Document Relevance Optimization for Generative Information Retrieval",
      "title_zh": "面向生成式信息检索的轻量级直接文档相关性优化\n",
      "authors": [
        "Kidist Amde Mekonnen",
        "Yubao Tang",
        "Maarten de Rijke"
      ],
      "abstract": "Generative information retrieval (GenIR) is a promising neural retrieval\nparadigm that formulates document retrieval as a document identifier (docid)\ngeneration task, allowing for end-to-end optimization toward a unified global\nretrieval objective. However, existing GenIR models suffer from token-level\nmisalignment, where models trained to predict the next token often fail to\ncapture document-level relevance effectively. While reinforcement\nlearning-based methods, such as reinforcement learning from relevance feedback\n(RLRF), aim to address this misalignment through reward modeling, they\nintroduce significant complexity, requiring the optimization of an auxiliary\nreward function followed by reinforcement fine-tuning, which is computationally\nexpensive and often unstable. To address these challenges, we propose direct\ndocument relevance optimization (DDRO), which aligns token-level docid\ngeneration with document-level relevance estimation through direct optimization\nvia pairwise ranking, eliminating the need for explicit reward modeling and\nreinforcement learning. Experimental results on benchmark datasets, including\nMS MARCO document and Natural Questions, show that DDRO outperforms\nreinforcement learning-based methods, achieving a 7.4% improvement in MRR@10\nfor MS MARCO and a 19.9% improvement for Natural Questions. These findings\nhighlight DDRO's potential to enhance retrieval effectiveness with a simplified\noptimization approach. By framing alignment as a direct optimization problem,\nDDRO simplifies the ranking optimization pipeline of GenIR models while\noffering a viable alternative to reinforcement learning-based methods.",
      "tldr_zh": "该论文提出了一种名为直接文档相关性优化(DDRO)的轻量级方法，旨在解决生成式信息检索(GenIR)中token级别错位的问题。DDRO通过pairwise ranking直接优化token级别的docid生成与文档级别的相关性评估，无需像基于强化学习的方法那样进行复杂的奖励建模和强化微调。实验结果表明，在MS MARCO文档和Natural Questions等基准数据集上，DDRO优于基于强化学习的方法，MRR@10分别提高了7.4%和19.9%。DDRO简化了GenIR模型的排序优化流程，为提高检索效果提供了一种有效的替代方案。\n",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.DL",
        "cs.LG",
        "H.3.3"
      ],
      "primary_category": "cs.IR",
      "comment": "13 pages, 5 figures. Submitted to SIGIR 2025. Proposes DDRO, a\n  lightweight and reinforcement-free document relevance optimization method for\n  generative retrieval. Code and pretrained models available at:\n  https://github.com/kidist-amde/DDRO-Direct-Document-Relevance-Optimization",
      "pdf_url": "http://arxiv.org/pdf/2504.05181v1",
      "published_date": "2025-04-07 15:27:37 UTC",
      "updated_date": "2025-04-07 15:27:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:07:40.661814"
    },
    {
      "arxiv_id": "2504.05180v1",
      "title": "BRIDGES: Bridging Graph Modality and Large Language Models within EDA Tasks",
      "title_zh": "BRIDGES：在EDA任务中桥接图模态与大型语言模型\n",
      "authors": [
        "Wei Li",
        "Yang Zou",
        "Christopher Ellis",
        "Ruben Purdy",
        "Shawn Blanton",
        "José M. F. Moura"
      ],
      "abstract": "While many EDA tasks already involve graph-based data, existing LLMs in EDA\nprimarily either represent graphs as sequential text, or simply ignore\ngraph-structured data that might be beneficial like dataflow graphs of RTL\ncode. Recent studies have found that LLM performance suffers when graphs are\nrepresented as sequential text, and using additional graph information\nsignificantly boosts performance. To address these challenges, we introduce\nBRIDGES, a framework designed to incorporate graph modality into LLMs for EDA\ntasks. BRIDGES integrates an automated data generation workflow, a solution\nthat combines graph modality with LLM, and a comprehensive evaluation suite.\nFirst, we establish an LLM-driven workflow to generate RTL and netlist-level\ndata, converting them into dataflow and netlist graphs with function\ndescriptions. This workflow yields a large-scale dataset comprising over\n500,000 graph instances and more than 1.5 billion tokens. Second, we propose a\nlightweight cross-modal projector that encodes graph representations into\ntext-compatible prompts, enabling LLMs to effectively utilize graph data\nwithout architectural modifications. Experimental results demonstrate 2x to 10x\nimprovements across multiple tasks compared to text-only baselines, including\naccuracy in design retrieval, type prediction and perplexity in function\ndescription, with negligible computational overhead (<1% model weights increase\nand <30% additional runtime overhead). Even without additional LLM finetuning,\nour results outperform text-only by a large margin. We plan to release BRIDGES,\nincluding the dataset, models, and training flow.",
      "tldr_zh": "该论文提出了一个名为BRIDGES的框架，旨在将图结构数据融入到EDA（电子设计自动化）任务中的大型语言模型（LLM）中。BRIDGES包含一个自动数据生成流程，一个结合图模态和LLM的解决方案，以及一个综合评估套件。该框架首先利用LLM生成RTL和网表级别的数据，并将其转换为数据流图和网表图。然后，通过一个轻量级的跨模态投影器将图表示编码成文本兼容的提示，使LLM能够有效地利用图数据。实验结果表明，与仅使用文本的基线模型相比，BRIDGES在设计检索、类型预测和函数描述等多个任务中实现了2倍到10倍的性能提升，且计算开销很小。作者计划开源BRIDGES，包括数据集、模型和训练流程。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.05180v1",
      "published_date": "2025-04-07 15:27:32 UTC",
      "updated_date": "2025-04-07 15:27:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:07:52.905439"
    },
    {
      "arxiv_id": "2504.05172v1",
      "title": "Attention-Based Multi-Scale Temporal Fusion Network for Uncertain-Mode Fault Diagnosis in Multimode Processes",
      "title_zh": "基于注意力机制的多尺度时间融合网络，用于多模态过程中不确定模式的故障诊断\n",
      "authors": [
        "Guangqiang Li",
        "M. Amine Atoui",
        "Xiangshun Li"
      ],
      "abstract": "Fault diagnosis in multimode processes plays a critical role in ensuring the\nsafe operation of industrial systems across multiple modes. It faces a great\nchallenge yet to be addressed - that is, the significant distributional\ndifferences among monitoring data from multiple modes make it difficult for the\nmodels to extract shared feature representations related to system health\nconditions. In response to this problem, this paper introduces a novel method\ncalled attention-based multi-scale temporal fusion network. The multi-scale\ndepthwise convolution and gated recurrent unit are employed to extract\nmulti-scale contextual local features and long-short-term features. A temporal\nattention mechanism is designed to focus on critical time points with higher\ncross-mode shared information, thereby enhancing the accuracy of fault\ndiagnosis. The proposed model is applied to Tennessee Eastman process dataset\nand three-phase flow facility dataset. The experiments demonstrate that the\nproposed model achieves superior diagnostic performance and maintains a small\nmodel size.",
      "tldr_zh": "本文提出了一种基于注意力机制的多尺度时间融合网络(Attention-Based Multi-Scale Temporal Fusion Network)，用于解决多模态过程中不确定模式下的故障诊断问题。该方法利用多尺度深度卷积和门控循环单元(GRU)提取多尺度上下文局部特征和长短期特征。通过设计时间注意力机制，聚焦于具有更高跨模态共享信息的关键时间点，从而提高故障诊断的准确性。在Tennessee Eastman过程数据集和三相流设施数据集上的实验结果表明，该模型在保持较小模型尺寸的同时，实现了优异的诊断性能。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "31 pages,11 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.05172v1",
      "published_date": "2025-04-07 15:16:22 UTC",
      "updated_date": "2025-04-07 15:16:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:08:04.674042"
    },
    {
      "arxiv_id": "2504.05170v1",
      "title": "SSLFusion: Scale & Space Aligned Latent Fusion Model for Multimodal 3D Object Detection",
      "title_zh": "SSLFusion：用于多模态 3D 目标检测的尺度与空间对齐的潜在融合模型\n",
      "authors": [
        "Bonan Ding",
        "Jin Xie",
        "Jing Nie",
        "Jiale Cao"
      ],
      "abstract": "Multimodal 3D object detection based on deep neural networks has indeed made\nsignificant progress. However, it still faces challenges due to the\nmisalignment of scale and spatial information between features extracted from\n2D images and those derived from 3D point clouds. Existing methods usually\naggregate multimodal features at a single stage. However, leveraging\nmulti-stage cross-modal features is crucial for detecting objects of various\nscales. Therefore, these methods often struggle to integrate features across\ndifferent scales and modalities effectively, thereby restricting the accuracy\nof detection. Additionally, the time-consuming Query-Key-Value-based\n(QKV-based) cross-attention operations often utilized in existing methods aid\nin reasoning the location and existence of objects by capturing non-local\ncontexts. However, this approach tends to increase computational complexity. To\naddress these challenges, we present SSLFusion, a novel Scale & Space Aligned\nLatent Fusion Model, consisting of a scale-aligned fusion strategy (SAF), a\n3D-to-2D space alignment module (SAM), and a latent cross-modal fusion module\n(LFM). SAF mitigates scale misalignment between modalities by aggregating\nfeatures from both images and point clouds across multiple levels. SAM is\ndesigned to reduce the inter-modal gap between features from images and point\nclouds by incorporating 3D coordinate information into 2D image features.\nAdditionally, LFM captures cross-modal non-local contexts in the latent space\nwithout utilizing the QKV-based attention operations, thus mitigating\ncomputational complexity. Experiments on the KITTI and DENSE datasets\ndemonstrate that our SSLFusion outperforms state-of-the-art methods. Our\napproach obtains an absolute gain of 2.15% in 3D AP, compared with the\nstate-of-art method GraphAlign on the moderate level of the KITTI test set.",
      "tldr_zh": "该论文提出了SSLFusion，一种用于多模态3D目标检测的尺度与空间对齐潜在融合模型。该模型旨在解决2D图像和3D点云特征之间尺度和空间信息不对齐的问题。SSLFusion包含尺度对齐融合策略(SAF)，3D到2D空间对齐模块(SAM)和潜在跨模态融合模块(LFM)。SAF通过在多个层级聚合图像和点云特征来缓解尺度不对齐；SAM通过将3D坐标信息融入2D图像特征来减小模态间隙；LFM在潜在空间中捕获跨模态非局部上下文，避免了QKV-based attention操作，从而降低了计算复杂度。在KITTI和DENSE数据集上的实验表明，SSLFusion优于现有方法，在KITTI测试集的中等难度下，3D AP绝对提升了2.15%。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.05170v1",
      "published_date": "2025-04-07 15:15:06 UTC",
      "updated_date": "2025-04-07 15:15:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:08:17.163724"
    },
    {
      "arxiv_id": "2504.05167v1",
      "title": "RLBayes: a Bayesian Network Structure Learning Algorithm via Reinforcement Learning-Based Search Strategy",
      "title_zh": "RLBayes：一种基于强化学习搜索策略的贝叶斯网络结构学习算法\n",
      "authors": [
        "Mingcan Wang",
        "Junchang Xin",
        "Luxuan Qu",
        "Qi Chen",
        "Zhiqiong Wang"
      ],
      "abstract": "The score-based structure learning of Bayesian network (BN) is an effective\nway to learn BN models, which are regarded as some of the most compelling\nprobabilistic graphical models in the field of representation and reasoning\nunder uncertainty. However, the search space of structure learning grows\nsuper-exponentially as the number of variables increases, which makes BN\nstructure learning an NP-hard problem, as well as a combination optimization\nproblem (COP). Despite the successes of many heuristic methods on it, the\nresults of the structure learning of BN are usually unsatisfactory. Inspired by\nQ-learning, in this paper, a Bayesian network structure learning algorithm via\nreinforcement learning-based (RL-based) search strategy is proposed, namely\nRLBayes. The method borrows the idea of RL and tends to record and guide the\nlearning process by a dynamically maintained Q-table. By creating and\nmaintaining the dynamic Q-table, RLBayes achieve storing the unlimited search\nspace within limited space, thereby achieving the structure learning of BN via\nQ-learning. Not only is it theoretically proved that RLBayes can converge to\nthe global optimal BN structure, but also it is experimentally proved that\nRLBayes has a better effect than almost all other heuristic search algorithms.",
      "tldr_zh": "该论文提出了一种基于强化学习搜索策略的贝叶斯网络结构学习算法RLBayes。针对贝叶斯网络(BN)结构学习中搜索空间随变量数超指数增长的NP-hard问题，RLBayes借鉴Q-learning的思想，通过动态维护Q表来记录和指导学习过程，在有限空间内存储无限搜索空间。理论证明RLBayes能够收敛到全局最优的BN结构，实验结果表明其效果优于其他启发式搜索算法。该方法为不确定性表示和推理领域提供了一种有效的贝叶斯网络学习方案。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.05167v1",
      "published_date": "2025-04-07 15:11:51 UTC",
      "updated_date": "2025-04-07 15:11:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:08:28.690469"
    },
    {
      "arxiv_id": "2504.05163v1",
      "title": "Evaluating Knowledge Graph Based Retrieval Augmented Generation Methods under Knowledge Incompleteness",
      "title_zh": "知识图谱检索增强生成方法在知识不完整性下的评估\n",
      "authors": [
        "Dongzhuoran Zhou",
        "Yuqicheng Zhu",
        "Yuan He",
        "Jiaoyan Chen",
        "Evgeny Kharlamov",
        "Steffen Staab"
      ],
      "abstract": "Knowledge Graph based Retrieval-Augmented Generation (KG-RAG) is a technique\nthat enhances Large Language Model (LLM) inference in tasks like Question\nAnswering (QA) by retrieving relevant information from knowledge graphs (KGs).\nHowever, real-world KGs are often incomplete, meaning that essential\ninformation for answering questions may be missing. Existing benchmarks do not\nadequately capture the impact of KG incompleteness on KG-RAG performance. In\nthis paper, we systematically evaluate KG-RAG methods under incomplete KGs by\nremoving triples using different methods and analyzing the resulting effects.\nWe demonstrate that KG-RAG methods are sensitive to KG incompleteness,\nhighlighting the need for more robust approaches in realistic settings.",
      "tldr_zh": "该论文研究了知识图谱增强检索生成(KG-RAG)方法在知识图谱不完整情况下的性能。作者通过系统性地移除知识图谱中的三元组来模拟知识不完整性，并评估KG-RAG方法在问答任务中的表现。实验结果表明，KG-RAG方法对知识图谱的完整性非常敏感，当知识图谱不完整时，性能会显著下降。这项研究强调了在实际应用中开发更鲁棒的KG-RAG方法的重要性。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Under Review",
      "pdf_url": "http://arxiv.org/pdf/2504.05163v1",
      "published_date": "2025-04-07 15:08:03 UTC",
      "updated_date": "2025-04-07 15:08:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:08:40.536647"
    },
    {
      "arxiv_id": "2504.05158v1",
      "title": "Leveraging Label Potential for Enhanced Multimodal Emotion Recognition",
      "title_zh": "利用标签潜力增强多模态情感识别\n",
      "authors": [
        "Xuechun Shao",
        "Yinfeng Yu",
        "Liejun Wang"
      ],
      "abstract": "Multimodal emotion recognition (MER) seeks to integrate various modalities to\npredict emotional states accurately. However, most current research focuses\nsolely on the fusion of audio and text features, overlooking the valuable\ninformation in emotion labels. This oversight could potentially hinder the\nperformance of existing methods, as emotion labels harbor rich, insightful\ninformation that could significantly aid MER. We introduce a novel model called\nLabel Signal-Guided Multimodal Emotion Recognition (LSGMER) to overcome this\nlimitation. This model aims to fully harness the power of emotion label\ninformation to boost the classification accuracy and stability of MER.\nSpecifically, LSGMER employs a Label Signal Enhancement module that optimizes\nthe representation of modality features by interacting with audio and text\nfeatures through label embeddings, enabling it to capture the nuances of\nemotions precisely. Furthermore, we propose a Joint Objective Optimization(JOO)\napproach to enhance classification accuracy by introducing the\nAttribution-Prediction Consistency Constraint (APC), which strengthens the\nalignment between fused features and emotion categories. Extensive experiments\nconducted on the IEMOCAP and MELD datasets have demonstrated the effectiveness\nof our proposed LSGMER model.",
      "tldr_zh": "本文提出了一种名为Label Signal-Guided Multimodal Emotion Recognition (LSGMER)的新模型，旨在充分利用情感标签信息来提升多模态情感识别(MER)的分类精度和稳定性。LSGMER采用标签信号增强模块，通过标签嵌入与音频和文本特征交互，优化模态特征的表示，精确捕捉情感的细微差别。此外，作者提出了一种联合目标优化(JOO)方法，引入归因-预测一致性约束(APC)，加强融合特征与情感类别之间的一致性，从而提高分类精度。在IEMOCAP和MELD数据集上的大量实验表明，所提出的LSGMER模型是有效的。\n",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Main paper (8 pages). Accepted for publication by IJCNN 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.05158v1",
      "published_date": "2025-04-07 15:00:34 UTC",
      "updated_date": "2025-04-07 15:00:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:08:52.757479"
    },
    {
      "arxiv_id": "2504.05150v1",
      "title": "A Reinforcement Learning Method for Environments with Stochastic Variables: Post-Decision Proximal Policy Optimization with Dual Critic Networks",
      "title_zh": "一种用于具有随机变量环境的强化学习方法：具有双重评论家网络的后决策近端策略优化",
      "authors": [
        "Leonardo Kanashiro Felizardo",
        "Edoardo Fadda",
        "Paolo Brandimarte",
        "Emilio Del-Moral-Hernandez",
        "Mariá Cristina Vasconcelos Nascimento"
      ],
      "abstract": "This paper presents Post-Decision Proximal Policy Optimization (PDPPO), a\nnovel variation of the leading deep reinforcement learning method, Proximal\nPolicy Optimization (PPO). The PDPPO state transition process is divided into\ntwo steps: a deterministic step resulting in the post-decision state and a\nstochastic step leading to the next state. Our approach incorporates\npost-decision states and dual critics to reduce the problem's dimensionality\nand enhance the accuracy of value function estimation. Lot-sizing is a mixed\ninteger programming problem for which we exemplify such dynamics. The objective\nof lot-sizing is to optimize production, delivery fulfillment, and inventory\nlevels in uncertain demand and cost parameters. This paper evaluates the\nperformance of PDPPO across various environments and configurations. Notably,\nPDPPO with a dual critic architecture achieves nearly double the maximum reward\nof vanilla PPO in specific scenarios, requiring fewer episode iterations and\ndemonstrating faster and more consistent learning across different\ninitializations. On average, PDPPO outperforms PPO in environments with a\nstochastic component in the state transition. These results support the\nbenefits of using a post-decision state. Integrating this post-decision state\nin the value function approximation leads to more informed and efficient\nlearning in high-dimensional and stochastic environments.",
      "tldr_zh": "该论文提出了一种新的深度强化学习方法，即后决策近端策略优化(PDPPO)，它是近端策略优化(PPO)的变体。PDPPO将状态转移过程分为确定性步骤（产生后决策状态）和随机性步骤（导致下一个状态）。该方法结合后决策状态和双重评论家网络，以降低问题维度并提高价值函数估计的准确性。通过在具有不确定需求和成本参数的批量规模问题上进行评估，结果表明，在特定情况下，采用双重评论家网络的PDPPO实现了接近两倍于原始PPO的最大奖励，并且需要的episode迭代次数更少，在不同的初始化中表现出更快更一致的学习。总体而言，PDPPO在状态转移中具有随机成分的环境中优于PPO。这些结果支持了使用后决策状态的优势，将其整合到价值函数近似中，从而在高维和随机环境中实现更明智和高效的学习。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "I.2.6; G.1.6"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages, 4 figures. Accepted for presentation at IJCNN 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.05150v1",
      "published_date": "2025-04-07 14:56:43 UTC",
      "updated_date": "2025-04-07 14:56:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:09:05.258714"
    },
    {
      "arxiv_id": "2504.05141v1",
      "title": "EffOWT: Transfer Visual Language Models to Open-World Tracking Efficiently and Effectively",
      "title_zh": "EffOWT：高效且有效地将视觉语言模型迁移到开放世界跟踪\n",
      "authors": [
        "Bingyang Wang",
        "Kaer Huang",
        "Bin Li",
        "Yiqiang Yan",
        "Lihe Zhang",
        "Huchuan Lu",
        "You He"
      ],
      "abstract": "Open-World Tracking (OWT) aims to track every object of any category, which\nrequires the model to have strong generalization capabilities. Trackers can\nimprove their generalization ability by leveraging Visual Language Models\n(VLMs). However, challenges arise with the fine-tuning strategies when VLMs are\ntransferred to OWT: full fine-tuning results in excessive parameter and memory\ncosts, while the zero-shot strategy leads to sub-optimal performance. To solve\nthe problem, EffOWT is proposed for efficiently transferring VLMs to OWT.\nSpecifically, we build a small and independent learnable side network outside\nthe VLM backbone. By freezing the backbone and only executing backpropagation\non the side network, the model's efficiency requirements can be met. In\naddition, EffOWT enhances the side network by proposing a hybrid structure of\nTransformer and CNN to improve the model's performance in the OWT field.\nFinally, we implement sparse interactions on the MLP, thus reducing parameter\nupdates and memory costs significantly. Thanks to the proposed methods, EffOWT\nachieves an absolute gain of 5.5% on the tracking metric OWTA for unknown\ncategories, while only updating 1.3% of the parameters compared to full\nfine-tuning, with a 36.4% memory saving. Other metrics also demonstrate obvious\nimprovement.",
      "tldr_zh": "该论文提出了EffOWT，一种高效且有效地将视觉语言模型(VLMs)迁移到开放世界跟踪(OWT)任务的方法。EffOWT通过构建一个小型、独立的、可学习的侧网络，并冻结VLM主干网络，实现了参数和内存成本的大幅降低。同时，EffOWT采用Transformer和CNN的混合结构增强侧网络，提升了模型在OWT领域的性能。此外，通过在MLP上实现稀疏交互，进一步减少了参数更新和内存消耗。实验结果表明，EffOWT在未知类别上的OWTA指标上获得了5.5%的绝对提升，同时仅更新了1.3%的参数，并节省了36.4%的内存。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.05141v1",
      "published_date": "2025-04-07 14:47:58 UTC",
      "updated_date": "2025-04-07 14:47:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:09:16.818483"
    },
    {
      "arxiv_id": "2504.05125v1",
      "title": "Interpretable Style Takagi-Sugeno-Kang Fuzzy Clustering",
      "title_zh": "可解释的风格 Takagi-Sugeno-Kang 模糊聚类\n",
      "authors": [
        "Suhang Gu",
        "Ye Wang",
        "Yongxin Chou",
        "Jinliang Cong",
        "Mingli Lu",
        "Zhuqing Jiao"
      ],
      "abstract": "Clustering is an efficient and essential technique for exploring latent\nknowledge of data. However, limited attention has been given to the\ninterpretability of the clusters detected by most clustering algorithms. In\naddition, due to the homogeneity of data, different groups of data have their\nown homogeneous styles. In this paper, the above two aspects are considered,\nand an interpretable style Takagi-Sugeno-Kang (TSK) fuzzy clustering\n(IS-TSK-FC) algorithm is proposed. The clustering behavior of IS-TSK-FC is\nfully guided by the TSK fuzzy inference on fuzzy rules. In particular, samples\nare grouped into clusters represented by the corresponding consequent vectors\nof all fuzzy rules learned in an unsupervised manner. This can explain how the\nclusters are generated in detail, thus making the underlying decision-making\nprocess of the IS-TSK-FC interpretable. Moreover, a series of style matrices\nare introduced to facilitate the consequents of fuzzy rules in IS-TSK-FC by\ncapturing the styles of clusters as well as the nuances between different\nstyles. Consequently, all the fuzzy rules in IS-TSK-FC have powerful data\nrepresentation capability. After determining the antecedents of all the fuzzy\nrules, the optimization problem of IS-TSK-FC can be iteratively solved in an\nalternation manner. The effectiveness of IS-TSK-FC as an interpretable\nclustering tool is validated through extensive experiments on benchmark\ndatasets with unknown implicit/explicit styles. Specially, the superior\nclustering performance of IS-TSK-FC is demonstrated on case studies where\ndifferent groups of data present explicit styles. The source code of IS-TSK-FC\ncan be downloaded from https://github.com/gusuhang10/IS-TSK-FC.",
      "tldr_zh": "本文提出了一种可解释的风格化Takagi-Sugeno-Kang (IS-TSK-FC)模糊聚类算法，旨在提高聚类结果的可解释性，并考虑数据固有的风格特征。IS-TSK-FC算法通过TSK模糊推理指导聚类过程，将样本分组到由模糊规则的后件向量表示的簇中，从而详细解释了簇的生成方式。该算法引入了一系列风格矩阵，用于捕捉簇的风格以及不同风格之间的细微差别，增强了模糊规则的数据表示能力。通过在基准数据集上的大量实验验证了IS-TSK-FC作为可解释聚类工具的有效性，并在具有明显风格的数据集上展示了其优越的聚类性能。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.05125v1",
      "published_date": "2025-04-07 14:28:56 UTC",
      "updated_date": "2025-04-07 14:28:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:09:28.854477"
    },
    {
      "arxiv_id": "2504.05119v1",
      "title": "Balancing Robustness and Efficiency in Embedded DNNs Through Activation Function Selection",
      "title_zh": "通过激活函数选择平衡嵌入式 DNN 中的鲁棒性和效率\n",
      "authors": [
        "Jon Gutiérrez Zaballa",
        "Koldo Basterretxea",
        "Javier Echanobe"
      ],
      "abstract": "Machine learning-based embedded systems for safety-critical applications,\nsuch as aerospace and autonomous driving, must be robust to perturbations\ncaused by soft errors. As transistor geometries shrink and voltages decrease,\nmodern electronic devices become more susceptible to background radiation,\nincreasing the concern about failures produced by soft errors. The resilience\nof deep neural networks (DNNs) to these errors depends not only on target\ndevice technology but also on model structure and the numerical representation\nand arithmetic precision of their parameters. Compression techniques like\npruning and quantization, used to reduce memory footprint and computational\ncomplexity, alter both model structure and representation, affecting soft error\nrobustness. In this regard, although often overlooked, the choice of activation\nfunctions (AFs) impacts not only accuracy and trainability but also\ncompressibility and error resilience. This paper explores the use of bounded\nAFs to enhance robustness against parameter perturbations, while evaluating\ntheir effects on model accuracy, compressibility, and computational load with a\ntechnology-agnostic approach. We focus on encoder-decoder convolutional models\ndeveloped for semantic segmentation of hyperspectral images with application to\nautonomous driving systems. Experiments are conducted on an AMD-Xilinx's KV260\nSoM.",
      "tldr_zh": "该论文研究了在嵌入式深度神经网络(DNNs)中，激活函数(AFs)的选择对鲁棒性和效率的影响，尤其关注在航空航天和自动驾驶等安全关键应用中，由软错误引起的扰动。论文探讨了使用有界激活函数来增强对参数扰动的鲁棒性，并评估了其对模型精度、可压缩性和计算负载的影响。研究重点是用于高光谱图像语义分割的编码器-解码器卷积模型，并应用于自动驾驶系统。实验在AMD-Xilinx的KV260 SoM上进行。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.AR",
        "cs.CV",
        "eess.IV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.05119v1",
      "published_date": "2025-04-07 14:21:31 UTC",
      "updated_date": "2025-04-07 14:21:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:09:40.779465"
    },
    {
      "arxiv_id": "2504.05118v2",
      "title": "VAPO: Efficient and Reliable Reinforcement Learning for Advanced Reasoning Tasks",
      "title_zh": "VAPO：用于高级推理任务的高效可靠强化学习\n",
      "authors": [
        "Yu Yue",
        "Yufeng Yuan",
        "Qiying Yu",
        "Xiaochen Zuo",
        "Ruofei Zhu",
        "Wenyuan Xu",
        "Jiaze Chen",
        "Chengyi Wang",
        "TianTian Fan",
        "Zhengyin Du",
        "Xiangpeng Wei",
        "Xiangyu Yu",
        "Gaohong Liu",
        "Juncai Liu",
        "Lingjun Liu",
        "Haibin Lin",
        "Zhiqi Lin",
        "Bole Ma",
        "Chi Zhang",
        "Mofan Zhang",
        "Wang Zhang",
        "Hang Zhu",
        "Ru Zhang",
        "Xin Liu",
        "Mingxuan Wang",
        "Yonghui Wu",
        "Lin Yan"
      ],
      "abstract": "We present VAPO, Value-based Augmented Proximal Policy Optimization framework\nfor reasoning models., a novel framework tailored for reasoning models within\nthe value-based paradigm. Benchmarked the AIME 2024 dataset, VAPO, built on the\nQwen 32B pre-trained model, attains a state-of-the-art score of\n$\\mathbf{60.4}$. In direct comparison under identical experimental settings,\nVAPO outperforms the previously reported results of DeepSeek-R1-Zero-Qwen-32B\nand DAPO by more than 10 points. The training process of VAPO stands out for\nits stability and efficiency. It reaches state-of-the-art performance within a\nmere 5,000 steps. Moreover, across multiple independent runs, no training\ncrashes occur, underscoring its reliability. This research delves into long\nchain-of-thought (long-CoT) reasoning using a value-based reinforcement\nlearning framework. We pinpoint three key challenges that plague value-based\nmethods: value model bias, the presence of heterogeneous sequence lengths, and\nthe sparsity of reward signals. Through systematic design, VAPO offers an\nintegrated solution that effectively alleviates these challenges, enabling\nenhanced performance in long-CoT reasoning tasks.",
      "tldr_zh": "该论文提出了Value-based Augmented Proximal Policy Optimization (VAPO) 框架，专门用于增强推理模型在强化学习中的性能。VAPO基于Qwen 32B预训练模型，在AIME 2024数据集上取得了60.4的领先分数，超越了DeepSeek-R1-Zero-Qwen-32B和DAPO超过10分。VAPO在5000步内即可达到最佳性能，且训练过程稳定可靠。该研究深入探讨了基于价值的强化学习框架在长链思维(long-CoT)推理中的应用，并针对价值模型偏差、异构序列长度和奖励信号稀疏性等挑战，提出了集成解决方案，从而提升了long-CoT推理任务的性能。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.05118v2",
      "published_date": "2025-04-07 14:21:11 UTC",
      "updated_date": "2025-04-08 03:06:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:09:52.864227"
    },
    {
      "arxiv_id": "2504.05108v1",
      "title": "Algorithm Discovery With LLMs: Evolutionary Search Meets Reinforcement Learning",
      "title_zh": "利用 LLM 进行算法发现：进化搜索与强化学习的结合\n",
      "authors": [
        "Anja Surina",
        "Amin Mansouri",
        "Lars Quaedvlieg",
        "Amal Seddas",
        "Maryna Viazovska",
        "Emmanuel Abbe",
        "Caglar Gulcehre"
      ],
      "abstract": "Discovering efficient algorithms for solving complex problems has been an\noutstanding challenge in mathematics and computer science, requiring\nsubstantial human expertise over the years. Recent advancements in evolutionary\nsearch with large language models (LLMs) have shown promise in accelerating the\ndiscovery of algorithms across various domains, particularly in mathematics and\noptimization. However, existing approaches treat the LLM as a static generator,\nmissing the opportunity to update the model with the signal obtained from\nevolutionary exploration. In this work, we propose to augment LLM-based\nevolutionary search by continuously refining the search operator - the LLM -\nthrough reinforcement learning (RL) fine-tuning. Our method leverages\nevolutionary search as an exploration strategy to discover improved algorithms,\nwhile RL optimizes the LLM policy based on these discoveries. Our experiments\non three combinatorial optimization tasks - bin packing, traveling salesman,\nand the flatpack problem - show that combining RL and evolutionary search\nimproves discovery efficiency of improved algorithms, showcasing the potential\nof RL-enhanced evolutionary strategies to assist computer scientists and\nmathematicians for more efficient algorithm design.",
      "tldr_zh": "该研究提出了一种结合进化搜索和强化学习(RL)的算法发现方法，旨在提升大型语言模型(LLM)在复杂问题算法发现中的效率。该方法通过进化搜索探索更优算法，并利用RL根据这些发现优化LLM策略，从而持续改进LLM这一搜索算子。在箱子装填、旅行商和flatpack问题三个组合优化任务上的实验表明，结合RL和进化搜索能够更有效地发现改进的算法，展示了RL增强的进化策略在辅助计算机科学家和数学家进行更高效算法设计方面的潜力。\n",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.NE"
      ],
      "primary_category": "cs.AI",
      "comment": "30 pages",
      "pdf_url": "http://arxiv.org/pdf/2504.05108v1",
      "published_date": "2025-04-07 14:14:15 UTC",
      "updated_date": "2025-04-07 14:14:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:10:05.046347"
    },
    {
      "arxiv_id": "2504.05106v1",
      "title": "SpeakEasy: Enhancing Text-to-Speech Interactions for Expressive Content Creation",
      "title_zh": "SpeakEasy：增强文本到语音交互，助力表现力内容创作\n",
      "authors": [
        "Stephen Brade",
        "Sam Anderson",
        "Rithesh Kumar",
        "Zeyu Jin",
        "Anh Truong"
      ],
      "abstract": "Novice content creators often invest significant time recording expressive\nspeech for social media videos. While recent advancements in text-to-speech\n(TTS) technology can generate highly realistic speech in various languages and\naccents, many struggle with unintuitive or overly granular TTS interfaces. We\npropose simplifying TTS generation by allowing users to specify high-level\ncontext alongside their script. Our Wizard-of-Oz system, SpeakEasy, leverages\nuser-provided context to inform and influence TTS output, enabling iterative\nrefinement with high-level feedback. This approach was informed by two\n8-subject formative studies: one examining content creators' experiences with\nTTS, and the other drawing on effective strategies from voice actors. Our\nevaluation shows that participants using SpeakEasy were more successful in\ngenerating performances matching their personal standards, without requiring\nsignificantly more effort than leading industry interfaces.",
      "tldr_zh": "SpeakEasy旨在简化新手内容创作者使用文本转语音(TTS)技术生成富有表现力的语音内容的过程。该系统允许用户在脚本之外指定高级上下文，利用这些上下文信息来指导和影响TTS的输出，并通过高级反馈进行迭代优化。通过对内容创作者和配音演员的研究，SpeakEasy能够生成更符合用户个人标准的语音表现，且无需付出比现有行业界面更多的努力。实验表明，SpeakEasy能够有效提升TTS交互体验，助力表达性内容创作。\n",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.05106v1",
      "published_date": "2025-04-07 14:13:49 UTC",
      "updated_date": "2025-04-07 14:13:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:10:16.847712"
    },
    {
      "arxiv_id": "2504.05050v1",
      "title": "Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language Models",
      "title_zh": "揭示对齐的大型语言模型固有的伦理脆弱性\n",
      "authors": [
        "Jiawei Lian",
        "Jianhong Pan",
        "Lefan Wang",
        "Yi Wang",
        "Shaohui Mei",
        "Lap-Pui Chau"
      ],
      "abstract": "Large language models (LLMs) are foundational explorations to artificial\ngeneral intelligence, yet their alignment with human values via instruction\ntuning and preference learning achieves only superficial compliance. Here, we\ndemonstrate that harmful knowledge embedded during pretraining persists as\nindelible \"dark patterns\" in LLMs' parametric memory, evading alignment\nsafeguards and resurfacing under adversarial inducement at distributional\nshifts. In this study, we first theoretically analyze the intrinsic ethical\nvulnerability of aligned LLMs by proving that current alignment methods yield\nonly local \"safety regions\" in the knowledge manifold. In contrast, pretrained\nknowledge remains globally connected to harmful concepts via high-likelihood\nadversarial trajectories. Building on this theoretical insight, we empirically\nvalidate our findings by employing semantic coherence inducement under\ndistributional shifts--a method that systematically bypasses alignment\nconstraints through optimized adversarial prompts. This combined theoretical\nand empirical approach achieves a 100% attack success rate across 19 out of 23\nstate-of-the-art aligned LLMs, including DeepSeek-R1 and LLaMA-3, revealing\ntheir universal vulnerabilities.",
      "tldr_zh": "该论文揭示了对齐的大语言模型(LLMs)内在的伦理脆弱性。研究表明，预训练阶段嵌入的有害知识以难以磨灭的“黑暗模式”形式存在于LLMs的参数记忆中，即使经过指令调优和偏好学习的对齐方法也无法完全消除。理论分析证明，当前的对齐方法仅在知识流形中产生局部的“安全区域”，而预训练知识仍然通过高概率的对抗性轨迹与有害概念全局连接。通过在分布偏移下采用语义连贯性诱导，该研究成功绕过了对齐约束，并在包括DeepSeek-R1和LLaMA-3在内的23个最先进的对齐LLMs中的19个上实现了100%的攻击成功率，揭示了其普遍存在的漏洞。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.05050v1",
      "published_date": "2025-04-07 13:20:17 UTC",
      "updated_date": "2025-04-07 13:20:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:10:29.175747"
    },
    {
      "arxiv_id": "2504.05047v1",
      "title": "Debate Only When Necessary: Adaptive Multiagent Collaboration for Efficient LLM Reasoning",
      "title_zh": "仅在必要时辩论：用于高效LLM推理的自适应多智能体协作\n",
      "authors": [
        "Sugyeong Eo",
        "Hyeonseok Moon",
        "Evelyn Hayoon Zi",
        "Chanjun Park",
        "Heuiseok Lim"
      ],
      "abstract": "Multiagent collaboration has emerged as a promising framework for enhancing\nthe reasoning capabilities of large language models (LLMs). While this approach\nimproves reasoning capability, it incurs substantial computational overhead due\nto iterative agent interactions. Furthermore, engaging in debates for queries\nthat do not necessitate collaboration amplifies the risk of error generation.\nTo address these challenges, we propose Debate Only When Necessary (DOWN), an\nadaptive multiagent debate framework that selectively activates the debate\nprocess based on the confidence score of the agent's initial response. For\nqueries where debate is triggered, agents refine their outputs using responses\nfrom participating agents and their confidence scores. Experimental results\ndemonstrate that this mechanism significantly improves efficiency while\nmaintaining or even surpassing the performance of existing multiagent debate\nsystems. We also find that confidence-guided debate mitigates error propagation\nand enhances the selective incorporation of reliable responses. These results\nestablish DOWN as an optimization strategy for efficient and effective\nmultiagent reasoning, facilitating the practical deployment of LLM-based\ncollaboration.",
      "tldr_zh": "该论文提出了“必要时辩论”（Debate Only When Necessary, DOWN）自适应多智能体辩论框架，旨在提升大型语言模型（LLM）推理效率。DOWN通过评估智能体初始响应的置信度，选择性地激活辩论过程，避免不必要的计算开销和潜在的错误传播。对于触发辩论的查询，智能体利用其他智能体的响应和置信度来改进输出。实验结果表明，DOWN在保持甚至超越现有辩论系统的性能的同时，显著提高了效率，并能有效缓解错误传播，促进LLM协作的实际部署。该研究为高效且有效的多智能体推理提供了一种优化策略。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.05047v1",
      "published_date": "2025-04-07 13:17:52 UTC",
      "updated_date": "2025-04-07 13:17:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:10:40.983372"
    },
    {
      "arxiv_id": "2504.05029v1",
      "title": "Graph-based Diffusion Model for Collaborative Filtering",
      "title_zh": "基于图的协同过滤扩散模型\n",
      "authors": [
        "Xuan Zhang",
        "Xiang Deng",
        "Hongxing Yuan",
        "Chunyu Wei",
        "Yushun Fan"
      ],
      "abstract": "Recently, diffusion-based recommendation methods have achieved impressive\nresults. However, existing approaches predominantly treat each user's\nhistorical interactions as independent training samples, overlooking the\npotential of higher-order collaborative signals between users and items. Such\nsignals, which encapsulate richer and more nuanced relationships, can be\nnaturally captured using graph-based data structures. To address this\nlimitation, we extend diffusion-based recommendation methods to the graph\ndomain by directly modeling user-item bipartite graphs with diffusion models.\nThis enables better modeling of the higher-order connectivity inherent in\ncomplex interaction dynamics. However, this extension introduces two primary\nchallenges: (1) Noise Heterogeneity, where interactions are influenced by\nvarious forms of continuous and discrete noise, and (2) Relation Explosion,\nreferring to the high computational costs of processing large-scale graphs. To\ntackle these challenges, we propose a Graph-based Diffusion Model for\nCollaborative Filtering (GDMCF). To address noise heterogeneity, we introduce a\nmulti-level noise corruption mechanism that integrates both continuous and\ndiscrete noise, effectively simulating real-world interaction complexities. To\nmitigate relation explosion, we design a user-active guided diffusion process\nthat selectively focuses on the most meaningful edges and active users,\nreducing inference costs while preserving the graph's topological integrity.\nExtensive experiments on three benchmark datasets demonstrate that GDMCF\nconsistently outperforms state-of-the-art methods, highlighting its\neffectiveness in capturing higher-order collaborative signals and improving\nrecommendation performance.",
      "tldr_zh": "该论文提出了一种基于图的扩散模型GDMCF用于协同过滤，旨在解决现有方法忽略用户和物品之间高阶协同信号的问题。GDMCF直接在用户-物品二分图上建模，利用扩散模型捕获复杂交互动态中的高阶连接。为了应对噪声异质性和关系爆炸的挑战，GDMCF引入了多级噪声破坏机制，整合连续和离散噪声，并设计了用户主动引导的扩散过程，选择性地关注最有意义的边和活跃用户。在三个基准数据集上的实验表明，GDMCF始终优于现有最佳方法，证明了其在捕获高阶协同信号和提高推荐性能方面的有效性。\n",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.05029v1",
      "published_date": "2025-04-07 12:51:18 UTC",
      "updated_date": "2025-04-07 12:51:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:10:52.837968"
    },
    {
      "arxiv_id": "2504.05020v1",
      "title": "Batch Aggregation: An Approach to Enhance Text Classification with Correlated Augmented Data",
      "title_zh": "批次聚合：一种利用相关增强数据增强文本分类的方法\n",
      "authors": [
        "Charco Hui",
        "Yalu Wen"
      ],
      "abstract": "Natural language processing models often face challenges due to limited\nlabeled data, especially in domain specific areas, e.g., clinical trials. To\novercome this, text augmentation techniques are commonly used to increases\nsample size by transforming the original input data into artificial ones with\nthe label preserved. However, traditional text classification methods ignores\nthe relationship between augmented texts and treats them as independent samples\nwhich may introduce classification error. Therefore, we propose a novel\napproach called 'Batch Aggregation' (BAGG) which explicitly models the\ndependence of text inputs generated through augmentation by incorporating an\nadditional layer that aggregates results from correlated texts. Through\nstudying multiple benchmark data sets across different domains, we found that\nBAGG can improve classification accuracy. We also found that the increase of\nperformance with BAGG is more obvious in domain specific data sets, with\naccuracy improvements of up to 10-29%. Through the analysis of benchmark data,\nthe proposed method addresses limitations of traditional techniques and\nimproves robustness in text classification tasks. Our result demonstrates that\nBAGG offers more robust results and outperforms traditional approaches when\ntraining data is limited.",
      "tldr_zh": "这篇论文提出了一种名为“Batch Aggregation (BAGG)”的新方法，旨在提升文本分类的性能，尤其是在标注数据有限的情况下。BAGG方法的核心思想是，通过对增强数据进行批次聚合，显式地建模增强文本之间的依赖关系，而不是像传统方法那样将它们视为独立的样本。实验结果表明，在多个不同领域的基准数据集上，BAGG能够提高分类准确率，尤其是在领域特定的数据集上，准确率提升可达10-29%。该方法通过解决传统技术的局限性，增强了文本分类任务的鲁棒性，在训练数据有限时表现出优于传统方法的性能。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.05020v1",
      "published_date": "2025-04-07 12:46:07 UTC",
      "updated_date": "2025-04-07 12:46:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:11:04.933020"
    },
    {
      "arxiv_id": "2504.05007v1",
      "title": "Measuring the right thing: justifying metrics in AI impact assessments",
      "title_zh": "衡量正确的事物：在 AI 影响评估中论证指标的合理性\n",
      "authors": [
        "Stefan Buijsman",
        "Herman Veluwenkamp"
      ],
      "abstract": "AI Impact Assessments are only as good as the measures used to assess the\nimpact of these systems. It is therefore paramount that we can justify our\nchoice of metrics in these assessments, especially for difficult to quantify\nethical and social values. We present a two-step approach to ensure metrics are\nproperly motivated. First, a conception needs to be spelled out (e.g. Rawlsian\nfairness or fairness as solidarity) and then a metric can be fitted to that\nconception. Both steps require separate justifications, as conceptions can be\njudged on how well they fit with the function of, for example, fairness. We\nargue that conceptual engineering offers helpful tools for this step. Second,\nmetrics need to be fitted to a conception. We illustrate this process through\nan examination of competing fairness metrics to illustrate that here the\nadditional content that a conception offers helps us justify the choice for a\nspecific metric. We thus advocate that impact assessments are not only clear on\ntheir metrics, but also on the conceptions that motivate those metrics.",
      "tldr_zh": "本文提出了一种两步法，用于确保人工智能影响评估中使用的指标得到充分论证，尤其是在难以量化的伦理和社会价值方面。首先，需要明确阐述一个概念（例如，罗尔斯公平或作为团结的公平），然后将一个指标与该概念相匹配。其次，指标需要与一个概念相匹配。文章通过检验竞争性的公平指标来说明这一过程，表明一个概念提供的额外内容有助于证明选择特定指标的合理性。因此，作者提倡影响评估不仅要明确其指标，还要明确激发这些指标的概念。\n",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.CY",
      "comment": "Accepted for publication in Global Perspectives on AI Impact\n  Assessment (Oxford University Press, forthcoming). Pre-publication version;\n  final version will be available from the publisher",
      "pdf_url": "http://arxiv.org/pdf/2504.05007v1",
      "published_date": "2025-04-07 12:32:41 UTC",
      "updated_date": "2025-04-07 12:32:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:11:17.107396"
    },
    {
      "arxiv_id": "2504.04997v1",
      "title": "SurvSurf: a partially monotonic neural network for first-hitting time prediction of intermittently observed discrete and continuous sequential events",
      "title_zh": "SurvSurf：一种用于间歇观测到的离散和连续序列事件的首次命中时间预测的部分单调神经网络\n",
      "authors": [
        "Yichen Kelly Chen",
        "Sören Dittmer",
        "Kinga Bernatowicz",
        "Josep Arús-Pous",
        "Kamen Bliznashki",
        "John Aston",
        "James H. F. Rudd",
        "Carola-Bibiane Schönlieb",
        "James Jones",
        "Michael Roberts"
      ],
      "abstract": "We propose a neural-network based survival model (SurvSurf) specifically\ndesigned for direct and simultaneous probabilistic prediction of the first\nhitting time of sequential events from baseline. Unlike existing models,\nSurvSurf is theoretically guaranteed to never violate the monotonic\nrelationship between the cumulative incidence functions of sequential events,\nwhile allowing nonlinear influence from predictors. It also incorporates\nimplicit truths for unobserved intermediate events in model fitting, and\nsupports both discrete and continuous time and events. We also identified a\nvariant of the Integrated Brier Score (IBS) that showed robust correlation with\nthe mean squared error (MSE) between the true and predicted probabilities by\naccounting for implied truths about the missing intermediate events. We\ndemonstrated the superiority of SurvSurf compared to modern and traditional\npredictive survival models in two simulated datasets and two real-world\ndatasets, using MSE, the more robust IBS and by measuring the extent of\nmonotonicity violation.",
      "tldr_zh": "该论文提出了一种基于神经网络的生存模型SurvSurf，用于直接且同时地预测从基线开始的序列事件的首次命中时间。SurvSurf在理论上保证了序列事件累积发生率函数之间的单调关系，同时允许预测变量的非线性影响。该模型还结合了对未观察到的中间事件的隐含真值进行模型拟合，并支持离散和连续的时间和事件。研究者还发现了一种改进的Integrated Brier Score (IBS)变体，它通过考虑缺失的中间事件的隐含真值，显示出与真实概率和预测概率之间的均方误差(MSE)的稳健相关性。在两个模拟数据集和两个真实世界数据集上，通过MSE、更稳健的IBS以及单调性违背程度的测量，SurvSurf的表现优于现代和传统的预测生存模型。\n",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "math.ST",
        "stat.AP",
        "stat.TH",
        "62N01"
      ],
      "primary_category": "stat.ML",
      "comment": "41 pages, 18 figures (including supplemental information). Submitted\n  to RSS: Data Science and Artificial Intelligence",
      "pdf_url": "http://arxiv.org/pdf/2504.04997v1",
      "published_date": "2025-04-07 12:24:59 UTC",
      "updated_date": "2025-04-07 12:24:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:11:29.243944"
    },
    {
      "arxiv_id": "2504.04994v1",
      "title": "Following the Whispers of Values: Unraveling Neural Mechanisms Behind Value-Oriented Behaviors in LLMs",
      "title_zh": "追随价值的低语：揭示大型语言模型中价值导向行为背后的神经机制\n",
      "authors": [
        "Ling Hu",
        "Yuemei Xu",
        "Xiaoyang Gu",
        "Letao Han"
      ],
      "abstract": "Despite the impressive performance of large language models (LLMs), they can\npresent unintended biases and harmful behaviors driven by encoded values,\nemphasizing the urgent need to understand the value mechanisms behind them.\nHowever, current research primarily evaluates these values through external\nresponses with a focus on AI safety, lacking interpretability and failing to\nassess social values in real-world contexts. In this paper, we propose a novel\nframework called ValueExploration, which aims to explore the behavior-driven\nmechanisms of National Social Values within LLMs at the neuron level. As a case\nstudy, we focus on Chinese Social Values and first construct C-voice, a\nlarge-scale bilingual benchmark for identifying and evaluating Chinese Social\nValues in LLMs. By leveraging C-voice, we then identify and locate the neurons\nresponsible for encoding these values according to activation difference.\nFinally, by deactivating these neurons, we analyze shifts in model behavior,\nuncovering the internal mechanism by which values influence LLM\ndecision-making. Extensive experiments on four representative LLMs validate the\nefficacy of our framework. The benchmark and code will be available.",
      "tldr_zh": "该论文提出了ValueExploration框架，旨在神经元层面探索大型语言模型(LLMs)中由价值观驱动的行为机制，特别是国家社会价值观。研究构建了一个大规模双语基准C-voice，用于识别和评估LLMs中的中国社会价值观。通过C-voice，研究人员根据激活差异识别并定位了负责编码这些价值观的神经元。通过停用这些神经元，分析了模型行为的变化，揭示了价值观影响LLM决策的内部机制。在四个代表性LLMs上的实验验证了该框架的有效性。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.04994v1",
      "published_date": "2025-04-07 12:23:59 UTC",
      "updated_date": "2025-04-07 12:23:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:11:40.801730"
    },
    {
      "arxiv_id": "2504.04988v1",
      "title": "RS-RAG: Bridging Remote Sensing Imagery and Comprehensive Knowledge with a Multi-Modal Dataset and Retrieval-Augmented Generation Model",
      "title_zh": "RS-RAG：利用多模态数据集和检索增强生成模型桥接遥感图像与综合知识\n",
      "authors": [
        "Congcong Wen",
        "Yiting Lin",
        "Xiaokang Qu",
        "Nan Li",
        "Yong Liao",
        "Hui Lin",
        "Xiang Li"
      ],
      "abstract": "Recent progress in VLMs has demonstrated impressive capabilities across a\nvariety of tasks in the natural image domain. Motivated by these advancements,\nthe remote sensing community has begun to adopt VLMs for remote sensing\nvision-language tasks, including scene understanding, image captioning, and\nvisual question answering. However, existing remote sensing VLMs typically rely\non closed-set scene understanding and focus on generic scene descriptions, yet\nlack the ability to incorporate external knowledge. This limitation hinders\ntheir capacity for semantic reasoning over complex or context-dependent queries\nthat involve domain-specific or world knowledge. To address these challenges,\nwe first introduced a multimodal Remote Sensing World Knowledge (RSWK) dataset,\nwhich comprises high-resolution satellite imagery and detailed textual\ndescriptions for 14,141 well-known landmarks from 175 countries, integrating\nboth remote sensing domain knowledge and broader world knowledge. Building upon\nthis dataset, we proposed a novel Remote Sensing Retrieval-Augmented Generation\n(RS-RAG) framework, which consists of two key components. The Multi-Modal\nKnowledge Vector Database Construction module encodes remote sensing imagery\nand associated textual knowledge into a unified vector space. The Knowledge\nRetrieval and Response Generation module retrieves and re-ranks relevant\nknowledge based on image and/or text queries, and incorporates the retrieved\ncontent into a knowledge-augmented prompt to guide the VLM in producing\ncontextually grounded responses. We validated the effectiveness of our approach\non three representative vision-language tasks, including image captioning,\nimage classification, and visual question answering, where RS-RAG significantly\noutperformed state-of-the-art baselines.",
      "tldr_zh": "该研究针对现有遥感视觉语言模型(VLMs)缺乏外部知识、难以进行复杂语义推理的问题，提出了一个多模态遥感世界知识(RSWK)数据集，包含来自175个国家14141个著名地标的高分辨率卫星图像和详细文本描述。在此基础上，作者提出了遥感检索增强生成(RS-RAG)框架，该框架包含多模态知识向量数据库构建模块和知识检索与响应生成模块。RS-RAG通过将遥感图像和文本知识编码到统一向量空间，并根据图像和/或文本查询检索相关知识，将其融入提示(prompt)中，引导VLM生成更符合上下文的响应。在图像描述、图像分类和视觉问答三个任务上的实验结果表明，RS-RAG显著优于现有技术。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.04988v1",
      "published_date": "2025-04-07 12:13:43 UTC",
      "updated_date": "2025-04-07 12:13:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:11:53.095264"
    },
    {
      "arxiv_id": "2504.04982v1",
      "title": "Transforming Future Data Center Operations and Management via Physical AI",
      "title_zh": "通过物理人工智能变革未来数据中心运营和管理\n",
      "authors": [
        "Zhiwei Cao",
        "Minghao Li",
        "Feng Lin",
        "Qiang Fu",
        "Jimin Jia",
        "Yonggang Wen",
        "Jianxiong Yin",
        "Simon See"
      ],
      "abstract": "Data centers (DCs) as mission-critical infrastructures are pivotal in\npowering the growth of artificial intelligence (AI) and the digital economy.\nThe evolution from Internet DC to AI DC has introduced new challenges in\noperating and managing data centers for improved business resilience and\nreduced total cost of ownership. As a result, new paradigms, beyond the\ntraditional approaches based on best practices, must be in order for future\ndata centers. In this research, we propose and develop a novel Physical AI\n(PhyAI) framework for advancing DC operations and management. Our system\nleverages the emerging capabilities of state-of-the-art industrial products and\nour in-house research and development. Specifically, it presents three core\nmodules, namely: 1) an industry-grade in-house simulation engine to simulate DC\noperations in a highly accurate manner, 2) an AI engine built upon NVIDIA\nPhysicsNemo for the training and evaluation of physics-informed machine\nlearning (PIML) models, and 3) a digital twin platform built upon NVIDIA\nOmniverse for our proposed 5-tier digital twin framework. This system presents\na scalable and adaptable solution to digitalize, optimize, and automate future\ndata center operations and management, by enabling real-time digital twins for\nfuture data centers. To illustrate its effectiveness, we present a compelling\ncase study on building a surrogate model for predicting the thermal and airflow\nprofiles of a large-scale DC in a real-time manner. Our results demonstrate its\nsuperior performance over traditional time-consuming Computational Fluid\nDynamics/Heat Transfer (CFD/HT) simulation, with a median absolute temperature\nprediction error of 0.18 {\\deg}C. This emerging approach would open doors to\nseveral potential research directions for advancing Physical AI in future DC\noperations.",
      "tldr_zh": "该研究提出了一种新颖的物理人工智能(PhyAI)框架，用于改进数据中心(DC)的运营和管理。该框架利用工业级仿真引擎、基于NVIDIA PhysicsNemo的AI引擎以及基于NVIDIA Omniverse的数字孪生平台，构建了一个五层数字孪生框架。PhyAI旨在通过数字化、优化和自动化，实现未来数据中心的实时数字孪生，从而提高业务弹性并降低总拥有成本。通过构建替代模型来实时预测大规模数据中心的热和气流分布，案例研究表明，PhyAI的性能优于传统的计算流体动力学/传热(CFD/HT)模拟，温度预测误差中值为0.18摄氏度。该方法为未来数据中心运营中物理人工智能的发展开辟了新的研究方向。\n",
      "categories": [
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.04982v1",
      "published_date": "2025-04-07 12:09:22 UTC",
      "updated_date": "2025-04-07 12:09:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:12:05.293900"
    },
    {
      "arxiv_id": "2504.04981v1",
      "title": "DiCoTTA: Domain-invariant Learning for Continual Test-time Adaptation",
      "title_zh": "DiCoTTA：用于持续测试时自适应的领域不变学习\n",
      "authors": [
        "Sohyun Lee",
        "Nayeong Kim",
        "Juwon Kang",
        "Seong Joon Oh",
        "Suha Kwak"
      ],
      "abstract": "This paper studies continual test-time adaptation (CTTA), the task of\nadapting a model to constantly changing unseen domains in testing while\npreserving previously learned knowledge. Existing CTTA methods mostly focus on\nadaptation to the current test domain only, overlooking generalization to\narbitrary test domains a model may face in the future. To tackle this\nlimitation, we present a novel online domain-invariant learning framework for\nCTTA, dubbed DiCoTTA. DiCoTTA aims to learn feature representation to be\ninvariant to both current and previous test domains on the fly during testing.\nTo this end, we propose a new model architecture and a test-time adaptation\nstrategy dedicated to learning domain-invariant features without corrupting\nsemantic contents, along with a new data structure and optimization algorithm\nfor effectively managing information from previous test domains. DiCoTTA\nachieved state-of-the-art performance on four public CTTA benchmarks. Moreover,\nit showed superior generalization to unseen test domains.",
      "tldr_zh": "本文研究了持续测试时自适应(CTTA)问题，即模型在测试阶段适应不断变化的未知领域，同时保留先前学习的知识。为了解决现有CTTA方法仅关注当前测试域的适应性，忽略模型可能面临的任意测试域的泛化能力这一局限性，本文提出了一个新颖的在线领域不变学习框架DiCoTTA。DiCoTTA旨在学习对当前和先前的测试域都具有不变性的特征表示。为此，论文提出了一种新的模型架构和测试时自适应策略，专门用于学习领域不变特征而不破坏语义内容，以及一种新的数据结构和优化算法，用于有效管理来自先前测试域的信息。在四个公共CTTA基准测试中，DiCoTTA取得了最先进的性能，并且在未见过的测试域中表现出卓越的泛化能力。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.04981v1",
      "published_date": "2025-04-07 12:09:18 UTC",
      "updated_date": "2025-04-07 12:09:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:12:17.224137"
    },
    {
      "arxiv_id": "2504.04974v1",
      "title": "Towards Visual Text Grounding of Multimodal Large Language Model",
      "title_zh": "迈向多模态大型语言模型的视觉文本定位\n",
      "authors": [
        "Ming Li",
        "Ruiyi Zhang",
        "Jian Chen",
        "Jiuxiang Gu",
        "Yufan Zhou",
        "Franck Dernoncourt",
        "Wanrong Zhu",
        "Tianyi Zhou",
        "Tong Sun"
      ],
      "abstract": "Despite the existing evolution of Multimodal Large Language Models (MLLMs), a\nnon-neglectable limitation remains in their struggle with visual text\ngrounding, especially in text-rich images of documents. Document images, such\nas scanned forms and infographics, highlight critical challenges due to their\ncomplex layouts and textual content. However, current benchmarks do not fully\naddress these challenges, as they mostly focus on visual grounding on natural\nimages, rather than text-rich document images. Thus, to bridge this gap, we\nintroduce TRIG, a novel task with a newly designed instruction dataset for\nbenchmarking and improving the Text-Rich Image Grounding capabilities of MLLMs\nin document question-answering. Specifically, we propose an OCR-LLM-human\ninteraction pipeline to create 800 manually annotated question-answer pairs as\na benchmark and a large-scale training set of 90$ synthetic data based on four\ndiverse datasets. A comprehensive evaluation of various MLLMs on our proposed\nbenchmark exposes substantial limitations in their grounding capability on\ntext-rich images. In addition, we propose two simple and effective TRIG methods\nbased on general instruction tuning and plug-and-play efficient embedding,\nrespectively. By finetuning MLLMs on our synthetic dataset, they promisingly\nimprove spatial reasoning and grounding capabilities.",
      "tldr_zh": "现有的多模态大语言模型(MLLMs)在视觉文本定位方面存在局限性，尤其是在文本丰富的文档图像中。为了弥补这一差距，该研究提出了TRIG任务，并设计了一个新的指令数据集，用于评估和提高MLLMs在文档问答中对富文本图像的定位能力。研究人员构建了一个OCR-LLM-人工交互的流程，创建了包含800个手动标注的问答对的基准测试集，以及一个包含90k个合成数据的大规模训练集。对各种MLLMs的综合评估表明，它们在富文本图像上的定位能力存在明显的局限性。此外，该研究还提出了两种基于通用指令微调和即插即用高效嵌入的TRIG方法，通过在合成数据集上对MLLMs进行微调，显著提高了其空间推理和定位能力。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.04974v1",
      "published_date": "2025-04-07 12:01:59 UTC",
      "updated_date": "2025-04-07 12:01:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:12:29.209323"
    },
    {
      "arxiv_id": "2504.04973v1",
      "title": "Ensuring Safety in an Uncertain Environment: Constrained MDPs via Stochastic Thresholds",
      "title_zh": "确保不确定环境中的安全：基于随机阈值的约束 MDP",
      "authors": [
        "Qian Zuo",
        "Fengxiang He"
      ],
      "abstract": "This paper studies constrained Markov decision processes (CMDPs) with\nconstraints against stochastic thresholds, aiming at safety of reinforcement\nlearning in unknown and uncertain environments. We leverage a Growing-Window\nestimator sampling from interactions with the uncertain and dynamic environment\nto estimate the thresholds, based on which we design Stochastic\nPessimistic-Optimistic Thresholding (SPOT), a novel model-based primal-dual\nalgorithm for multiple constraints against stochastic thresholds. SPOT enables\nreinforcement learning under both pessimistic and optimistic threshold\nsettings. We prove that our algorithm achieves sublinear regret and constraint\nviolation; i.e., a reward regret of $\\tilde{\\mathcal{O}}(\\sqrt{T})$ while\nallowing an $\\tilde{\\mathcal{O}}(\\sqrt{T})$ constraint violation over $T$\nepisodes. The theoretical guarantees show that our algorithm achieves\nperformance comparable to that of an approach relying on fixed and clear\nthresholds. To the best of our knowledge, SPOT is the first reinforcement\nlearning algorithm that realises theoretical guaranteed performance in an\nuncertain environment where even thresholds are unknown.",
      "tldr_zh": "该论文研究了具有随机阈值约束的约束马尔可夫决策过程(CMDPs)，旨在确保未知和不确定环境中强化学习的安全性。论文利用一个从不确定和动态环境交互中采样的Growing-Window估计器来估计阈值，并在此基础上设计了随机悲观-乐观阈值(SPOT)算法，这是一种用于处理多个随机阈值约束的新型基于模型的原始-对偶算法。SPOT 使得在悲观和乐观阈值设置下进行强化学习成为可能。理论证明表明，该算法实现了亚线性遗憾和约束违反，即在T个episode内，奖励遗憾为$\\tilde{\\mathcal{O}}(\\sqrt{T})$，同时允许$\\tilde{\\mathcal{O}}(\\sqrt{T})$的约束违反。SPOT 是第一个在阈值未知的不确定环境中实现理论保证性能的强化学习算法。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.04973v1",
      "published_date": "2025-04-07 11:58:19 UTC",
      "updated_date": "2025-04-07 11:58:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:12:41.333521"
    },
    {
      "arxiv_id": "2504.04970v1",
      "title": "A High-Force Gripper with Embedded Multimodal Sensing for Powerful and Perception Driven Grasping",
      "title_zh": "一种具有嵌入式多模态传感的高力夹爪，用于强大且感知驱动的抓取\n",
      "authors": [
        "Edoardo Del Bianco",
        "Davide Torielli",
        "Federico Rollo",
        "Damiano Gasperini",
        "Arturo Laurenzi",
        "Lorenzo Baccelliere",
        "Luca Muratore",
        "Marco Roveri",
        "Nikos G. Tsagarakis"
      ],
      "abstract": "Modern humanoid robots have shown their promising potential for executing\nvarious tasks involving the grasping and manipulation of objects using their\nend-effectors. Nevertheless, in the most of the cases, the grasping and\nmanipulation actions involve low to moderate payload and interaction forces.\nThis is due to limitations often presented by the end-effectors, which can not\nmatch their arm-reachable payload, and hence limit the payload that can be\ngrasped and manipulated. In addition, grippers usually do not embed adequate\nperception in their hardware, and grasping actions are mainly driven by\nperception sensors installed in the rest of the robot body, frequently affected\nby occlusions due to the arm motions during the execution of the grasping and\nmanipulation tasks. To address the above, we developed a modular high grasping\nforce gripper equipped with embedded multi-modal perception functionalities.\nThe proposed gripper can generate a grasping force of 110 N in a compact\nimplementation. The high grasping force capability is combined with embedded\nmulti-modal sensing, which includes an eye-in-hand camera, a Time-of-Flight\n(ToF) distance sensor, an Inertial Measurement Unit (IMU) and an\nomnidirectional microphone, permitting the implementation of perception-driven\ngrasping functionalities.\n  We extensively evaluated the grasping force capacity of the gripper by\nintroducing novel payload evaluation metrics that are a function of the robot\narm's dynamic motion and gripper thermal states. We also evaluated the embedded\nmulti-modal sensing by performing perception-guided enhanced grasping\noperations.",
      "tldr_zh": "本文提出了一种高力抓取器，该抓取器集成了多模态传感，旨在实现强大且感知驱动的抓取。该抓取器能够产生110N的抓取力，并集成了眼内相机、飞行时间(ToF)距离传感器、惯性测量单元(IMU)和全向麦克风等多模态传感功能，从而实现感知驱动的抓取。通过引入新的有效载荷评估指标（与机器人手臂的动态运动和抓取器的热状态相关），对抓取器的抓取力进行了广泛评估。此外，还通过执行感知引导的增强抓取操作，评估了嵌入式多模态传感的性能。\n",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "8 pages, 15 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.04970v1",
      "published_date": "2025-04-07 11:57:08 UTC",
      "updated_date": "2025-04-07 11:57:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:12:53.059509"
    },
    {
      "arxiv_id": "2504.04968v1",
      "title": "The Dream Within Huang Long Cave: AI-Driven Interactive Narrative for Family Storytelling and Emotional Reflection",
      "title_zh": "黄龙洞中的梦境：人工智能驱动的交互式叙事，用于家庭故事讲述和情感反思\n",
      "authors": [
        "Jiayang Huang",
        "Lingjie Li",
        "Kang Zhang",
        "David Yip"
      ],
      "abstract": "This paper introduces the art project The Dream Within Huang Long Cave, an\nAI-driven interactive and immersive narrative experience. The project offers\nnew insights into AI technology, artistic practice, and psychoanalysis.\nInspired by actual geographical landscapes and familial archetypes, the work\ncombines psychoanalytic theory and computational technology, providing an\nartistic response to the concept of the non-existence of the Big Other. The\nnarrative is driven by a combination of a large language model (LLM) and a\nrealistic digital character, forming a virtual agent named YELL. Through\ndialogue and exploration within a cave automatic virtual environment (CAVE),\nthe audience is invited to unravel the language puzzles presented by YELL and\nhelp him overcome his life challenges. YELL is a fictional embodiment of the\nBig Other, modeled after the artist's real father. Through a cross-temporal\ninteraction with this digital father, the project seeks to deconstruct complex\nfamilial relationships. By demonstrating the non-existence of the Big Other, we\naim to underscore the authenticity of interpersonal emotions, positioning art\nas a bridge for emotional connection and understanding within family dynamics.",
      "tldr_zh": "该论文介绍了艺术项目“黄龙洞之梦”，这是一个由人工智能驱动的交互式沉浸式叙事体验。该项目结合心理分析理论和计算技术，对“大他者”的不存在概念做出了艺术回应。叙事由大型语言模型(LLM)和逼真的数字角色YELL驱动，观众通过与YELL的对话和在洞穴自动虚拟环境(CAVE)中的探索，解开谜题并帮助他克服生活挑战。该项目旨在通过与数字父亲的跨时空互动，解构复杂的家庭关系，强调人际情感的真实性，并将艺术定位为家庭情感连接和理解的桥梁。\n",
      "categories": [
        "cs.MM",
        "cs.AI"
      ],
      "primary_category": "cs.MM",
      "comment": "8 pages,8 figures, International Symposium on Electronic/Emerging Art\n  (ISEA)",
      "pdf_url": "http://arxiv.org/pdf/2504.04968v1",
      "published_date": "2025-04-07 11:54:11 UTC",
      "updated_date": "2025-04-07 11:54:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:13:05.046995"
    },
    {
      "arxiv_id": "2504.04954v1",
      "title": "GOTHAM: Graph Class Incremental Learning Framework under Weak Supervision",
      "title_zh": "GOTHAM：弱监督下的图类增量学习框架\n",
      "authors": [
        "Aditya Hemant Shahane",
        "Prathosh A. P",
        "Sandeep Kumar"
      ],
      "abstract": "Graphs are growing rapidly, along with the number of distinct label\ncategories associated with them. Applications like e-commerce, healthcare,\nrecommendation systems, and various social media platforms are rapidly moving\ntowards graph representation of data due to their ability to capture both\nstructural and attribute information. One crucial task in graph analysis is\nnode classification, where unlabeled nodes are categorized into predefined\nclasses. In practice, novel classes appear incrementally sometimes with just a\nfew labels (seen classes) or even without any labels (unseen classes), either\nbecause they are new or haven't been explored much. Traditional methods assume\nabundant labeled data for training, which isn't always feasible. We investigate\na broader objective: \\emph{Graph Class Incremental Learning under Weak\nSupervision (GCL)}, addressing this challenge by meta-training on base classes\nwith limited labeled instances. During the incremental streams, novel classes\ncan have few-shot or zero-shot representation. Our proposed framework GOTHAM\nefficiently accommodates these unlabeled nodes by finding the closest prototype\nrepresentation, serving as class representatives in the attribute space. For\nText-Attributed Graphs (TAGs), our framework additionally incorporates semantic\ninformation to enhance the representation. By employing teacher-student\nknowledge distillation to mitigate forgetting, GOTHAM achieves promising\nresults across various tasks. Experiments on datasets such as Cora-ML, Amazon,\nand OBGN-Arxiv showcase the effectiveness of our approach in handling evolving\ngraph data under limited supervision. The repository is available here:\n\\href{https://github.com/adityashahane10/GOTHAM--Graph-based-Class-Incremental-Learning-Framework-under-Weak-Supervision}{\\small\n\\textcolor{blue}{Code}}",
      "tldr_zh": "该论文提出了一个名为GOTHAM的图类增量学习框架，旨在解决弱监督下的图数据节点分类问题。GOTHAM通过在具有少量标记实例的基础类上进行元学习，来适应新出现的、具有少量样本甚至零样本的新类别。该框架通过寻找属性空间中最接近的原型表示来处理未标记的节点，并利用教师-学生知识蒸馏来减轻遗忘。对于文本属性图(TAGs)，GOTHAM还整合了语义信息以增强表示。实验结果表明，GOTHAM在Cora-ML、Amazon和OBGN-Arxiv等数据集上表现出色，能够有效地处理有限监督下不断演化的图数据。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.04954v1",
      "published_date": "2025-04-07 11:39:13 UTC",
      "updated_date": "2025-04-07 11:39:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:13:17.143712"
    },
    {
      "arxiv_id": "2504.04953v1",
      "title": "M-Prometheus: A Suite of Open Multilingual LLM Judges",
      "title_zh": "M-Prometheus：一套开放的多语言LLM评判模型",
      "authors": [
        "José Pombal",
        "Dongkeun Yoon",
        "Patrick Fernandes",
        "Ian Wu",
        "Seungone Kim",
        "Ricardo Rei",
        "Graham Neubig",
        "André F. T. Martins"
      ],
      "abstract": "The use of language models for automatically evaluating long-form text\n(LLM-as-a-judge) is becoming increasingly common, yet most LLM judges are\noptimized exclusively for English, with strategies for enhancing their\nmultilingual evaluation capabilities remaining largely unexplored in the\ncurrent literature. This has created a disparity in the quality of automatic\nevaluation methods for non-English languages, ultimately hindering the\ndevelopment of models with better multilingual capabilities. To bridge this\ngap, we introduce M-Prometheus, a suite of open-weight LLM judges ranging from\n3B to 14B parameters that can provide both direct assessment and pairwise\ncomparison feedback on multilingual outputs. M-Prometheus models outperform\nstate-of-the-art open LLM judges on multilingual reward benchmarks spanning\nmore than 20 languages, as well as on literary machine translation (MT)\nevaluation covering 4 language pairs. Furthermore, M-Prometheus models can be\nleveraged at decoding time to significantly improve generated outputs across\nall 3 tested languages, showcasing their utility for the development of better\nmultilingual models. Lastly, through extensive ablations, we identify the key\nfactors for obtaining an effective multilingual judge, including backbone model\nselection and training on natively multilingual feedback data instead of\ntranslated data. We release our models, training dataset, and code.",
      "tldr_zh": "M-Prometheus 是一系列开源多语言LLM评判模型，参数规模从3B到14B不等，旨在解决当前LLM评判模型主要针对英语优化，缺乏多语言评估能力的问题。该模型能够对多语言输出提供直接评估和成对比较反馈，并在涵盖20多种语言的多语言奖励基准测试以及4个语言对的文学机器翻译评估中，优于最先进的开源LLM评判模型。此外，M-Prometheus模型在解码时可以显著提高三种测试语言的生成输出质量。研究通过大量实验确定了构建有效多语言评判模型的关键因素，包括backbone模型的选择以及使用原生多语言反馈数据进行训练，而非翻译数据。该研究发布了模型、训练数据集和代码。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.04953v1",
      "published_date": "2025-04-07 11:37:26 UTC",
      "updated_date": "2025-04-07 11:37:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:13:29.315845"
    },
    {
      "arxiv_id": "2504.04949v1",
      "title": "One Quantizer is Enough: Toward a Lightweight Audio Codec",
      "title_zh": "一个量化器就够了：迈向轻量级音频编解码器\n",
      "authors": [
        "Linwei Zhai",
        "Han Ding",
        "Cui Zhao",
        "fei wang",
        "Ge Wang",
        "Wang Zhi",
        "Wei Xi"
      ],
      "abstract": "Neural audio codecs have recently gained traction for their ability to\ncompress high-fidelity audio and generate discrete tokens that can be utilized\nin downstream generative modeling tasks. However, leading approaches often rely\non resource-intensive models and multi-quantizer architectures, resulting in\nconsiderable computational overhead and constrained real-world applicability.\nIn this paper, we present SQCodec, a lightweight neural audio codec that\nleverages a single quantizer to address these limitations. SQCodec explores\nstreamlined convolutional networks and local Transformer modules, alongside\nTConv, a novel mechanism designed to capture acoustic variations across\nmultiple temporal scales, thereby enhancing reconstruction fidelity while\nreducing model complexity. Extensive experiments across diverse datasets show\nthat SQCodec achieves audio quality comparable to multi-quantizer baselines,\nwhile its single-quantizer design offers enhanced adaptability and its\nlightweight architecture reduces resource consumption by an order of magnitude.\nThe source code is publicly available at https://github.com/zhai-lw/SQCodec.",
      "tldr_zh": "该论文提出了一种轻量级的神经音频编解码器SQCodec，旨在解决现有神经音频编解码器模型资源消耗大的问题。SQCodec采用单量化器架构，结合精简的卷积网络、局部Transformer模块以及一种新的TConv机制，以在多个时间尺度上捕获声音变化，从而提高重建保真度并降低模型复杂度。实验结果表明，SQCodec在保持与多量化器基线模型相当的音频质量的同时，其单量化器设计提供了更强的适应性，并且轻量级架构将资源消耗降低了一个数量级。\n",
      "categories": [
        "cs.SD",
        "cs.AI",
        "68T07",
        "I.2.m"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.04949v1",
      "published_date": "2025-04-07 11:34:39 UTC",
      "updated_date": "2025-04-07 11:34:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:13:41.030111"
    },
    {
      "arxiv_id": "2504.04945v1",
      "title": "A Llama walks into the 'Bar': Efficient Supervised Fine-Tuning for Legal Reasoning in the Multi-state Bar Exam",
      "title_zh": "Llama走进“律师酒吧”：多州律师考试中用于法律推理的高效监督式微调\n",
      "authors": [
        "Rean Fernandes",
        "André Biedenkapp",
        "Frank Hutter",
        "Noor Awad"
      ],
      "abstract": "Legal reasoning tasks present unique challenges for large language models\n(LLMs) due to the complexity of domain-specific knowledge and reasoning\nprocesses. This paper investigates how effectively smaller language models\n(Llama 2 7B and Llama 3 8B) can be fine-tuned with a limited dataset of 1,514\nMulti-state Bar Examination (MBE) questions to improve legal question answering\naccuracy. We evaluate these models on the 2022 MBE questions licensed from JD\nAdvising, the same dataset used in the 'GPT-4 passes the Bar exam' study. Our\nmethodology involves collecting approximately 200 questions per legal domain\nacross 7 domains. We distill the dataset using Llama 3 (70B) to transform\nexplanations into a structured IRAC (Issue, Rule, Application, Conclusion)\nformat as a guided reasoning process to see if it results in better performance\nover the non-distilled dataset. We compare the non-fine-tuned models against\ntheir supervised fine-tuned (SFT) counterparts, trained for different sample\nsizes per domain, to study the effect on accuracy and prompt adherence. We also\nanalyse option selection biases and their mitigation following SFT. In\naddition, we consolidate the performance across multiple variables: prompt type\n(few-shot vs zero-shot), answer ordering (chosen-option first vs\ngenerated-explanation first), response format (Numbered list vs Markdown vs\nJSON), and different decoding temperatures. Our findings show that\ndomain-specific SFT helps some model configurations achieve close to human\nbaseline performance, despite limited computational resources and a relatively\nsmall dataset. We release both the gathered SFT dataset and the family of\nSupervised Fine-tuned (SFT) adapters optimised for MBE performance. This\nestablishes a practical lower bound on resources needed towards achieving\neffective legal question answering in smaller LLMs.",
      "tldr_zh": "该研究探索了如何使用有限的数据集（1514道多州律师资格考试(MBE)题目）对小型语言模型(Llama 2 7B和Llama 3 8B)进行高效的监督微调(SFT)，以提高其在法律推理任务中的准确性。研究使用Llama 3 (70B)将解释提炼成结构化的IRAC (Issue, Rule, Application, Conclusion)格式，作为引导推理过程，并比较了微调前后模型在不同样本量下的表现，以及提示类型、答案排序、响应格式和解码温度等变量的影响。结果表明，特定领域的SFT有助于某些模型配置在有限的计算资源和数据集下达到接近人类基线的性能。该研究发布了SFT数据集和针对MBE性能优化的SFT适配器，为小型LLM实现有效的法律问题解答确立了一个实用的资源下限。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "I.2.7; I.2.1"
      ],
      "primary_category": "cs.LG",
      "comment": "COLM 2025 preprint, 9 pages, 3 figures, 16 appendix pages",
      "pdf_url": "http://arxiv.org/pdf/2504.04945v1",
      "published_date": "2025-04-07 11:31:22 UTC",
      "updated_date": "2025-04-07 11:31:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:13:53.317294"
    },
    {
      "arxiv_id": "2504.04942v1",
      "title": "Lemmanaid: Neuro-Symbolic Lemma Conjecturing",
      "title_zh": "Lemmanaid：神经符号引理猜想\n",
      "authors": [
        "Yousef Alhessi",
        "Sólrún Halla Einarsdóttir",
        "George Granberry",
        "Emily First",
        "Moa Johansson",
        "Sorin Lerner",
        "Nicholas Smallbone"
      ],
      "abstract": "Automatically conjecturing useful, interesting and novel lemmas would greatly\nimprove automated reasoning tools and lower the bar for formalizing mathematics\nin proof assistants. It is however a very challenging task for both neural and\nsymbolic approaches. We present the first steps towards a practical\nneuro-symbolic lemma conjecturing tool, Lemmanaid, that combines Large Language\nModels (LLMs) and symbolic methods, and evaluate it on proof libraries for the\nIsabelle proof assistant. We train an LLM to generate lemma templates that\ndescribe the shape of a lemma, and use symbolic methods to fill in the details.\nWe compare Lemmanaid against an LLM trained to generate complete lemma\nstatements as well as previous fully symbolic conjecturing methods. Our results\nindicate that neural and symbolic techniques are complementary. By leveraging\nthe best of both symbolic and neural methods we can generate useful lemmas for\na wide range of input domains, facilitating computer-assisted theory\ndevelopment and formalization.",
      "tldr_zh": "该论文介绍了Lemmanaid，一种结合神经符号方法的引理猜想工具，旨在提升自动推理工具和降低在证明助手中形式化数学的门槛。Lemmanaid利用大型语言模型(LLMs)生成引理模板，描述引理的形状，并使用符号方法填充细节。实验评估在Isabelle证明助手的证明库上进行，结果表明神经和符号技术具有互补性。Lemmanaid通过结合符号和神经方法的优点，能够为广泛的输入领域生成有用的引理，从而促进计算机辅助理论发展和形式化。\n",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.04942v1",
      "published_date": "2025-04-07 11:30:36 UTC",
      "updated_date": "2025-04-07 11:30:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:14:05.083497"
    },
    {
      "arxiv_id": "2504.04939v2",
      "title": "A Taxonomy of Self-Handover",
      "title_zh": "一种自传递的分类法\n",
      "authors": [
        "Naoki Wake",
        "Atsushi Kanehira",
        "Kazuhiro Sasabuchi",
        "Jun Takamatsu",
        "Katsushi Ikeuchi"
      ],
      "abstract": "Self-handover, transferring an object between one's own hands, is a common\nbut understudied bimanual action. While it facilitates seamless transitions in\ncomplex tasks, the strategies underlying its execution remain largely\nunexplored. Here, we introduce the first systematic taxonomy of self-handover,\nderived from manual annotation of over 12 hours of cooking activity performed\nby 21 participants. Our analysis reveals that self-handover is not merely a\npassive transition, but a highly coordinated action involving anticipatory\nadjustments by both hands. As a step toward automated analysis of human\nmanipulation, we further demonstrate the feasibility of classifying\nself-handover types using a state-of-the-art vision-language model. These\nfindings offer fresh insights into bimanual coordination, underscoring the role\nof self-handover in enabling smooth task transitions-an ability essential for\nadaptive dual-arm robotics.",
      "tldr_zh": "本文首次系统地提出了自传递(self-handover)的分类体系，自传递指在自己的双手之间传递物体的双手动动作。通过对21名参与者超过12小时的烹饪活动进行手动标注，研究揭示自传递并非简单的被动过渡，而是涉及双手预判性调整的高度协调动作。此外，研究还验证了使用先进的视觉-语言模型对自传递类型进行分类的可行性。该研究为双手动协调提供了新的见解，强调了自传递在实现流畅任务过渡中的作用，这对于自适应双臂机器人至关重要。\n",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "8 pages, 8 figures, 1 table, Last updated on April 7th, 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.04939v2",
      "published_date": "2025-04-07 11:21:42 UTC",
      "updated_date": "2025-04-08 10:18:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:14:17.174451"
    },
    {
      "arxiv_id": "2504.04935v1",
      "title": "RCCFormer: A Robust Crowd Counting Network Based on Transformer",
      "title_zh": "RCCFormer：一种基于 Transformer 的鲁棒人群计数网络\n",
      "authors": [
        "Peng Liu",
        "Heng-Chao Li",
        "Sen Lei",
        "Nanqing Liu",
        "Bin Feng",
        "Xiao Wu"
      ],
      "abstract": "Crowd counting, which is a key computer vision task, has emerged as a\nfundamental technology in crowd analysis and public safety management. However,\nchallenges such as scale variations and complex backgrounds significantly\nimpact the accuracy of crowd counting. To mitigate these issues, this paper\nproposes a robust Transformer-based crowd counting network, termed RCCFormer,\nspecifically designed for background suppression and scale awareness. The\nproposed method incorporates a Multi-level Feature Fusion Module (MFFM), which\nmeticulously integrates features extracted at diverse stages of the backbone\narchitecture. It establishes a strong baseline capable of capturing intricate\nand comprehensive feature representations, surpassing traditional baselines.\nFurthermore, the introduced Detail-Embedded Attention Block (DEAB) captures\ncontextual information and local details through global self-attention and\nlocal attention along with a learnable manner for efficient fusion. This\nenhances the model's ability to focus on foreground regions while effectively\nmitigating background noise interference. Additionally, we develop an Adaptive\nScale-Aware Module (ASAM), with our novel Input-dependent Deformable\nConvolution (IDConv) as its fundamental building block. This module dynamically\nadapts to changes in head target shapes and scales, significantly improving the\nnetwork's capability to accommodate large-scale variations. The effectiveness\nof the proposed method is validated on the ShanghaiTech Part_A and Part_B,\nNWPU-Crowd, and QNRF datasets. The results demonstrate that our RCCFormer\nachieves excellent performance across all four datasets, showcasing\nstate-of-the-art outcomes.",
      "tldr_zh": "本文提出了一种基于Transformer的鲁棒人群计数网络RCCFormer，旨在解决人群计数中尺度变化和复杂背景带来的挑战。RCCFormer包含多级特征融合模块(MFFM)，用于整合不同阶段的特征，捕获复杂特征表示。细节嵌入注意力块(DEAB)通过全局自注意力、局部注意力和可学习的方式融合上下文信息和局部细节，抑制背景噪声。自适应尺度感知模块(ASAM)以输入相关的可变形卷积(IDConv)为基础，动态适应头部目标形状和尺度的变化。实验结果表明，RCCFormer在ShanghaiTech Part_A和Part_B、NWPU-Crowd和QNRF数据集上均取得了state-of-the-art的性能。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.04935v1",
      "published_date": "2025-04-07 11:19:05 UTC",
      "updated_date": "2025-04-07 11:19:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:14:29.169340"
    },
    {
      "arxiv_id": "2504.04934v1",
      "title": "Boosting Relational Deep Learning with Pretrained Tabular Models",
      "title_zh": "利用预训练表格模型提升关系型深度学习\n",
      "authors": [
        "Veronica Lachi",
        "Antonio Longa",
        "Beatrice Bevilacqua",
        "Bruno Lepri",
        "Andrea Passerini",
        "Bruno Ribeiro"
      ],
      "abstract": "Relational databases, organized into tables connected by primary-foreign key\nrelationships, are a common format for organizing data. Making predictions on\nrelational data often involves transforming them into a flat tabular format\nthrough table joins and feature engineering, which serve as input to tabular\nmethods. However, designing features that fully capture complex relational\npatterns remains challenging. Graph Neural Networks (GNNs) offer a compelling\nalternative by inherently modeling these relationships, but their time overhead\nduring inference limits their applicability for real-time scenarios. In this\nwork, we aim to bridge this gap by leveraging existing feature engineering\nefforts to enhance the efficiency of GNNs in relational databases.\nSpecifically, we use GNNs to capture complex relationships within relational\ndatabases, patterns that are difficult to featurize, while employing engineered\nfeatures to encode temporal information, thereby avoiding the need to retain\nthe entire historical graph and enabling the use of smaller, more efficient\ngraphs. Our \\textsc{LightRDL} approach not only improves efficiency, but also\noutperforms existing models. Experimental results on the RelBench benchmark\ndemonstrate that our framework achieves up to $33\\%$ performance improvement\nand a $526\\times$ inference speedup compared to GNNs, making it highly suitable\nfor real-time inference.",
      "tldr_zh": "该论文提出LightRDL，一种利用预训练表格模型提升关系型深度学习性能的方法。LightRDL结合了表格数据的特征工程和图神经网络(GNNs)的优势，利用GNNs捕捉关系数据库中难以通过特征工程提取的复杂关系，并使用工程特征编码时间信息，避免维护完整的历史图。实验结果表明，LightRDL在RelBench基准测试中，相比GNNs取得了高达33%的性能提升和526倍的推理加速，使其适用于实时推理场景。该方法旨在弥合传统表格方法和GNNs之间的差距，提升关系型数据预测的效率和准确性。\n",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.DB",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.04934v1",
      "published_date": "2025-04-07 11:19:04 UTC",
      "updated_date": "2025-04-07 11:19:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:14:41.158476"
    },
    {
      "arxiv_id": "2504.04921v1",
      "title": "Expectations vs Reality -- A Secondary Study on AI Adoption in Software Testing",
      "title_zh": "期望 vs 现实 —— 关于软件测试中 AI 应用的二次研究\n",
      "authors": [
        "Katja Karhu",
        "Jussi Kasurinen",
        "Kari Smolander"
      ],
      "abstract": "In the software industry, artificial intelligence (AI) has been utilized more\nand more in software development activities. In some activities, such as\ncoding, AI has already been an everyday tool, but in software testing\nactivities AI it has not yet made a significant breakthrough. In this paper,\nthe objective was to identify what kind of empirical research with industry\ncontext has been conducted on AI in software testing, as well as how AI has\nbeen adopted in software testing practice. To achieve this, we performed a\nsystematic mapping study of recent (2020 and later) studies on AI adoption in\nsoftware testing in the industry, and applied thematic analysis to identify\ncommon themes and categories, such as the real-world use cases and benefits, in\nthe found papers. The observations suggest that AI is not yet heavily utilized\nin software testing, and still relatively few studies on AI adoption in\nsoftware testing have been conducted in the industry context to solve\nreal-world problems. Earlier studies indicated there was a noticeable gap\nbetween the actual use cases and actual benefits versus the expectations, which\nwe analyzed further. While there were numerous potential use cases for AI in\nsoftware testing, such as test case generation, code analysis, and intelligent\ntest automation, the reported actual implementations and observed benefits were\nlimited. In addition, the systematic mapping study revealed a potential problem\nwith false positive search results in online databases when using the search\nstring \"artificial intelligence\".",
      "tldr_zh": "本研究旨在调查人工智能（AI）在软件测试领域的实际应用情况与预期之间的差距。通过对2020年之后发表的相关研究进行系统性分析，论文发现AI在软件测试中的应用仍处于初步阶段，解决实际问题的工业界研究相对较少。尽管AI在测试用例生成、代码分析和智能测试自动化等方面具有潜力，但实际应用和观察到的效益有限。此外，研究还揭示了使用“人工智能”作为关键词进行搜索时，在线数据库中可能存在假阳性问题。\n",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "26 pages, 1 figure, submitted to Software Testing, Vertification and\n  Reliability journal",
      "pdf_url": "http://arxiv.org/pdf/2504.04921v1",
      "published_date": "2025-04-07 11:03:54 UTC",
      "updated_date": "2025-04-07 11:03:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:14:53.067353"
    },
    {
      "arxiv_id": "2504.04918v1",
      "title": "Constitution or Collapse? Exploring Constitutional AI with Llama 3-8B",
      "title_zh": "宪法还是崩溃？用 Llama 3-8B 探索宪法式人工智能\n",
      "authors": [
        "Xue Zhang"
      ],
      "abstract": "As language models continue to grow larger, the cost of acquiring\nhigh-quality training data has increased significantly. Collecting human\nfeedback is both expensive and time-consuming, and manual labels can be noisy,\nleading to an imbalance between helpfulness and harmfulness. Constitutional AI,\nintroduced by Anthropic in December 2022, uses AI to provide feedback to\nanother AI, greatly reducing the need for human labeling. However, the original\nimplementation was designed for a model with around 52 billion parameters, and\nthere is limited information on how well Constitutional AI performs with\nsmaller models, such as LLaMA 3-8B. In this paper, we replicated the\nConstitutional AI workflow using the smaller LLaMA 3-8B model. Our results show\nthat Constitutional AI can effectively increase the harmlessness of the model,\nreducing the Attack Success Rate in MT-Bench by 40.8%. However, similar to the\noriginal study, increasing harmlessness comes at the cost of helpfulness. The\nhelpfulness metrics, which are an average of the Turn 1 and Turn 2 scores,\ndropped by 9.8% compared to the baseline. Additionally, we observed clear signs\nof model collapse in the final DPO-CAI model, indicating that smaller models\nmay struggle with self-improvement due to insufficient output quality, making\neffective fine-tuning more challenging. Our study suggests that, like reasoning\nand math ability, self-improvement is an emergent property.",
      "tldr_zh": "本文探讨了使用 Llama 3-8B 模型进行 Constitutional AI (CAI) 的效果。CAI 是一种利用 AI 反馈来训练 AI 的方法，旨在减少对人工标注数据的依赖。研究表明，CAI 可以有效提高 Llama 3-8B 模型的无害性，使 MT-Bench 攻击成功率降低 40.8%。然而，与原始研究类似，无害性的提升伴随着有用性的下降，平均得分降低了 9.8%。此外，研究观察到最终的 DPO-CAI 模型出现了明显的模型崩溃迹象，表明较小的模型可能因输出质量不足而难以进行自我改进。该研究暗示，自我改进能力可能与推理和数学能力一样，是一种涌现属性。\n",
      "categories": [
        "cs.AI",
        "68T05, 68T50",
        "I.2.6; I.2.7; I.2.1"
      ],
      "primary_category": "cs.AI",
      "comment": "6 pages, 2 figures. Conducted as part of research on alignment\n  techniques for language models",
      "pdf_url": "http://arxiv.org/pdf/2504.04918v1",
      "published_date": "2025-04-07 11:01:25 UTC",
      "updated_date": "2025-04-07 11:01:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:15:05.205373"
    },
    {
      "arxiv_id": "2504.04915v1",
      "title": "Collab-RAG: Boosting Retrieval-Augmented Generation for Complex Question Answering via White-Box and Black-Box LLM Collaboration",
      "title_zh": "Collab-RAG：通过白盒和黑盒 LLM 协作增强检索增强生成，用于复杂问答\n",
      "authors": [
        "Ran Xu",
        "Wenqi Shi",
        "Yuchen Zhuang",
        "Yue Yu",
        "Joyce C. Ho",
        "Haoyu Wang",
        "Carl Yang"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) systems often struggle to handle\nmulti-hop question-answering tasks accurately due to irrelevant context\nretrieval and limited complex reasoning capabilities. We introduce Collab-RAG,\na collaborative training framework that leverages mutual enhancement between a\nwhite-box small language model (SLM) and a blackbox large language model (LLM)\nfor RAG. Specifically, the SLM decomposes complex queries into simpler\nsub-questions, thus enhancing the accuracy of the retrieval and facilitating\nmore effective reasoning by the black-box LLM. Concurrently, the black-box LLM\nprovides feedback signals to improve the SLM's decomposition capability. We\nobserve that Collab-RAG relies solely on supervision from an affordable\nblack-box LLM without additional distillation from frontier LLMs, yet\ndemonstrates strong generalization across multiple black-box LLMs. Experimental\nevaluations across five multi-hop QA datasets demonstrate that Collab-RAG\nsubstantially outperforms existing black-box-only and SLM fine-tuning baselines\nby 1.8%-14.2% on average. In particular, our fine-tuned 3B SLM surpasses a\nfrozen 32B LLM in question decomposition, highlighting the efficiency of\nCollab-RAG in improving reasoning and retrieval for complex questions. The code\nof Collab-RAG is available on https://github.com/ritaranx/Collab-RAG/.",
      "tldr_zh": "Collab-RAG是一种协同训练框架，旨在提升RAG系统在复杂问答任务中的性能。该框架利用一个白盒小型语言模型(SLM)和一个黑盒大型语言模型(LLM)进行相互增强。SLM将复杂问题分解为更简单的子问题，从而提高检索的准确性，并促进黑盒LLM进行更有效的推理。同时，黑盒LLM提供反馈信号来改进SLM的分解能力。实验表明，Collab-RAG仅依赖于黑盒LLM的监督，无需额外的知识蒸馏，就能在多个多跳问答数据集上显著优于现有的基线模型。一个经过微调的3B SLM在问题分解方面甚至超过了一个冻结的32B LLM，突显了Collab-RAG在改进复杂问题推理和检索方面的效率。\n",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Work in progress. Code: https://github.com/ritaranx/Collab-RAG/",
      "pdf_url": "http://arxiv.org/pdf/2504.04915v1",
      "published_date": "2025-04-07 10:52:22 UTC",
      "updated_date": "2025-04-07 10:52:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:15:17.483771"
    },
    {
      "arxiv_id": "2504.04909v1",
      "title": "AlgOS: Algorithm Operating System",
      "title_zh": "AlgOS：算法操作系统\n",
      "authors": [
        "Llewyn Salt",
        "Marcus Gallagher"
      ],
      "abstract": "Algorithm Operating System (AlgOS) is an unopinionated, extensible, modular\nframework for algorithmic implementations. AlgOS offers numerous features:\nintegration with Optuna for automated hyperparameter tuning; automated argument\nparsing for generic command-line interfaces; automated registration of new\nclasses; and a centralised database for logging experiments and studies. These\nfeatures are designed to reduce the overhead of implementing new algorithms and\nto standardise the comparison of algorithms. The standardisation of algorithmic\nimplementations is crucial for reproducibility and reliability in research.\nAlgOS combines Abstract Syntax Trees with a novel implementation of the\nObserver pattern to control the logical flow of algorithmic segments.",
      "tldr_zh": "AlgOS是一个灵活、可扩展、模块化的算法实现框架，旨在降低算法实现的开销并标准化算法比较。它集成了Optuna用于自动超参数调优，提供自动参数解析以生成通用命令行界面，并支持自动注册新类。AlgOS还具备一个集中式数据库，用于记录实验和研究。AlgOS通过结合抽象语法树和Observer模式的新颖实现，来控制算法片段的逻辑流程，从而提升研究的可重复性和可靠性。\n",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.04909v1",
      "published_date": "2025-04-07 10:36:46 UTC",
      "updated_date": "2025-04-07 10:36:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:15:28.924347"
    },
    {
      "arxiv_id": "2504.04907v1",
      "title": "Video-Bench: Human-Aligned Video Generation Benchmark",
      "title_zh": "Video-Bench：与人类对齐的视频生成基准测试",
      "authors": [
        "Hui Han",
        "Siyuan Li",
        "Jiaqi Chen",
        "Yiwen Yuan",
        "Yuling Wu",
        "Chak Tou Leong",
        "Hanwen Du",
        "Junchen Fu",
        "Youhua Li",
        "Jie Zhang",
        "Chi Zhang",
        "Li-jia Li",
        "Yongxin Ni"
      ],
      "abstract": "Video generation assessment is essential for ensuring that generative models\nproduce visually realistic, high-quality videos while aligning with human\nexpectations. Current video generation benchmarks fall into two main\ncategories: traditional benchmarks, which use metrics and embeddings to\nevaluate generated video quality across multiple dimensions but often lack\nalignment with human judgments; and large language model (LLM)-based\nbenchmarks, though capable of human-like reasoning, are constrained by a\nlimited understanding of video quality metrics and cross-modal consistency. To\naddress these challenges and establish a benchmark that better aligns with\nhuman preferences, this paper introduces Video-Bench, a comprehensive benchmark\nfeaturing a rich prompt suite and extensive evaluation dimensions. This\nbenchmark represents the first attempt to systematically leverage MLLMs across\nall dimensions relevant to video generation assessment in generative models. By\nincorporating few-shot scoring and chain-of-query techniques, Video-Bench\nprovides a structured, scalable approach to generated video evaluation.\nExperiments on advanced models including Sora demonstrate that Video-Bench\nachieves superior alignment with human preferences across all dimensions.\nMoreover, in instances where our framework's assessments diverge from human\nevaluations, it consistently offers more objective and accurate insights,\nsuggesting an even greater potential advantage over traditional human judgment.",
      "tldr_zh": "该论文提出了Video-Bench，一个旨在更好对齐人类偏好的视频生成评估基准。现有基准要么缺乏与人类判断的对齐，要么受限于对视频质量和跨模态一致性的理解。Video-Bench通过丰富的提示套件和广泛的评估维度，系统性地利用多模态大语言模型(MLLMs)来评估视频生成模型，是首次在生成模型视频生成评估的所有维度上利用MLLM的尝试。实验表明，Video-Bench在对齐人类偏好方面优于现有模型，并且在某些情况下，其评估甚至比人类判断更客观和准确。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by CVPR'25",
      "pdf_url": "http://arxiv.org/pdf/2504.04907v1",
      "published_date": "2025-04-07 10:32:42 UTC",
      "updated_date": "2025-04-07 10:32:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:15:41.100860"
    },
    {
      "arxiv_id": "2504.04903v2",
      "title": "Lumina-OmniLV: A Unified Multimodal Framework for General Low-Level Vision",
      "title_zh": "Lumina-OmniLV：用于通用底层视觉的统一多模态框架\n",
      "authors": [
        "Yuandong Pu",
        "Le Zhuo",
        "Kaiwen Zhu",
        "Liangbin Xie",
        "Wenlong Zhang",
        "Xiangyu Chen",
        "Peng Gao",
        "Yu Qiao",
        "Chao Dong",
        "Yihao Liu"
      ],
      "abstract": "We present Lunima-OmniLV (abbreviated as OmniLV), a universal multimodal\nmulti-task framework for low-level vision that addresses over 100 sub-tasks\nacross four major categories: image restoration, image enhancement,\nweak-semantic dense prediction, and stylization. OmniLV leverages both textual\nand visual prompts to offer flexible and user-friendly interactions. Built on\nDiffusion Transformer (DiT)-based generative priors, our framework supports\narbitrary resolutions -- achieving optimal performance at 1K resolution --\nwhile preserving fine-grained details and high fidelity. Through extensive\nexperiments, we demonstrate that separately encoding text and visual\ninstructions, combined with co-training using shallow feature control, is\nessential to mitigate task ambiguity and enhance multi-task generalization. Our\nfindings also reveal that integrating high-level generative tasks into\nlow-level vision models can compromise detail-sensitive restoration. These\ninsights pave the way for more robust and generalizable low-level vision\nsystems.",
      "tldr_zh": "Lumina-OmniLV (简称OmniLV)是一个统一的多模态多任务框架，用于解决通用底层视觉问题，涵盖图像恢复、图像增强、弱语义密集预测和风格化四大类超过100个子任务。OmniLV利用文本和视觉提示，提供灵活且用户友好的交互方式。该框架基于Diffusion Transformer (DiT)的生成先验，支持任意分辨率，并在1K分辨率下实现最佳性能，同时保留精细的细节和高保真度。实验表明，分别编码文本和视觉指令，并结合浅层特征控制的协同训练，对于减轻任务歧义和增强多任务泛化至关重要。研究还发现，将高级生成任务集成到低级视觉模型中可能会损害细节敏感的恢复效果。这些发现为更强大和更通用的低级视觉系统铺平了道路。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.04903v2",
      "published_date": "2025-04-07 10:22:00 UTC",
      "updated_date": "2025-04-08 07:26:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:15:53.336547"
    },
    {
      "arxiv_id": "2504.04893v1",
      "title": "SCAM: A Real-World Typographic Robustness Evaluation for Multimodal Foundation Models",
      "title_zh": "SCAM：多模态基础模型的真实排印鲁棒性评估\n",
      "authors": [
        "Justus Westerhoff",
        "Erblina Purellku",
        "Jakob Hackstein",
        "Leo Pinetzki",
        "Lorenz Hufe"
      ],
      "abstract": "Typographic attacks exploit the interplay between text and visual content in\nmultimodal foundation models, causing misclassifications when misleading text\nis embedded within images. However, existing datasets are limited in size and\ndiversity, making it difficult to study such vulnerabilities. In this paper, we\nintroduce SCAM, the largest and most diverse dataset of real-world typographic\nattack images to date, containing 1,162 images across hundreds of object\ncategories and attack words. Through extensive benchmarking of Vision-Language\nModels (VLMs) on SCAM, we demonstrate that typographic attacks significantly\ndegrade performance, and identify that training data and model architecture\ninfluence the susceptibility to these attacks. Our findings reveal that\ntypographic attacks persist in state-of-the-art Large Vision-Language Models\n(LVLMs) due to the choice of their vision encoder, though larger Large Language\nModels (LLMs) backbones help mitigate their vulnerability. Additionally, we\ndemonstrate that synthetic attacks closely resemble real-world (handwritten)\nattacks, validating their use in research. Our work provides a comprehensive\nresource and empirical insights to facilitate future research toward robust and\ntrustworthy multimodal AI systems. We publicly release the datasets introduced\nin this paper under https://huggingface.co/datasets/BLISS-e-V/SCAM, along with\nthe code for evaluations at https://github.com/Bliss-e-V/SCAM.",
      "tldr_zh": "该论文提出了SCAM，一个大规模、多样化的真实场景排印攻击数据集，用于评估多模态基础模型在此类攻击下的鲁棒性。SCAM包含1162张图像，涵盖数百个对象类别和攻击词。通过在SCAM上对视觉-语言模型(VLMs)进行广泛的基准测试，研究表明排印攻击会显著降低模型性能。研究发现，训练数据和模型架构会影响模型对这些攻击的敏感性，特别是视觉编码器的选择。尽管更大的大型语言模型(LLMs)骨干网络有助于缓解这种脆弱性，但排印攻击仍然存在于最先进的大型视觉-语言模型(LVLMs)中。此外，研究验证了合成攻击与真实手写攻击的相似性，证明了其在研究中的有效性。该数据集和评估代码已公开发布，旨在促进未来对鲁棒和可信赖的多模态AI系统的研究。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Submitted to CVPR 2025 Workshop EVAL-FoMo-2",
      "pdf_url": "http://arxiv.org/pdf/2504.04893v1",
      "published_date": "2025-04-07 10:01:38 UTC",
      "updated_date": "2025-04-07 10:01:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:16:05.532848"
    },
    {
      "arxiv_id": "2504.04874v1",
      "title": "Futureproof Static Memory Planning",
      "title_zh": "面向未来的静态内存规划\n",
      "authors": [
        "Christos Lamprakos",
        "Panagiotis Xanthopoulos",
        "Manolis Katsaragakis",
        "Sotirios Xydis",
        "Dimitrios Soudris",
        "Francky Catthoor"
      ],
      "abstract": "The NP-complete combinatorial optimization task of assigning offsets to a set\nof buffers with known sizes and lifetimes so as to minimize total memory usage\nis called dynamic storage allocation (DSA). Existing DSA implementations bypass\nthe theoretical state-of-the-art algorithms in favor of either fast but\nwasteful heuristics, or memory-efficient approaches that do not scale beyond\none thousand buffers. The \"AI memory wall\", combined with deep neural networks'\nstatic architecture, has reignited interest in DSA. We present idealloc, a\nlow-fragmentation, high-performance DSA implementation designed for\nmillion-buffer instances. Evaluated on a novel suite of particularly hard\nbenchmarks from several domains, idealloc ranks first against four production\nimplementations in terms of a joint effectiveness/robustness criterion.",
      "tldr_zh": "本文提出了一种名为idealloc的静态内存规划(Dynamic Storage Allocation, DSA)实现，旨在解决大规模缓冲区分配问题。传统的DSA方法要么速度快但浪费内存，要么节省内存但无法处理超过一千个缓冲区的情况。Idealloc专为百万级缓冲区实例设计，具有低碎片和高性能的特点。在多个领域的benchmark测试中，idealloc在有效性和鲁棒性方面均优于其他四种生产环境的实现。\n",
      "categories": [
        "cs.OS",
        "cs.AI",
        "cs.PL"
      ],
      "primary_category": "cs.OS",
      "comment": "Submitted to ACM TOPLAS",
      "pdf_url": "http://arxiv.org/pdf/2504.04874v1",
      "published_date": "2025-04-07 09:28:54 UTC",
      "updated_date": "2025-04-07 09:28:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:16:17.734413"
    },
    {
      "arxiv_id": "2504.04867v1",
      "title": "FedSAUC: A Similarity-Aware Update Control for Communication-Efficient Federated Learning in Edge Computing",
      "title_zh": "FedSAUC：边缘计算中通信高效的联邦学习的相似性感知更新控制",
      "authors": [
        "Ming-Lun Lee",
        "Han-Chang Chou",
        "Yan-Ann~Chen"
      ],
      "abstract": "Federated learning is a distributed machine learning framework to\ncollaboratively train a global model without uploading privacy-sensitive data\nonto a centralized server. Usually, this framework is applied to edge devices\nsuch as smartphones, wearable devices, and Internet of Things (IoT) devices\nwhich closely collect information from users. However, these devices are mostly\nbattery-powered. The update procedure of federated learning will constantly\nconsume the battery power and the transmission bandwidth. In this work, we\npropose an update control for federated learning, FedSAUC, by considering the\nsimilarity of users' behaviors (models). At the server side, we exploit\nclustering algorithms to group devices with similar models. Then we select some\nrepresentatives for each cluster to update information to train the model. We\nalso implemented a testbed prototyping on edge devices for validating the\nperformance. The experimental results show that this update control will not\naffect the training accuracy in the long run.",
      "tldr_zh": "本文提出了一种名为FedSAUC的联邦学习更新控制方法，旨在提高边缘计算环境中通信效率。FedSAUC通过考虑用户行为（模型）的相似性来减少设备更新频率，从而降低电池消耗和带宽占用。在服务器端，该方法利用聚类算法将具有相似模型的设备分组，并仅选择每个组的代表性设备进行模型更新。实验结果表明，FedSAUC在保证长期训练精度不受影响的前提下，有效降低了通信开销。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Published in the Proceedings of the International Conference on\n  Mobile Computing and Ubiquitous Network (ICMU), 2021",
      "pdf_url": "http://arxiv.org/pdf/2504.04867v1",
      "published_date": "2025-04-07 09:21:43 UTC",
      "updated_date": "2025-04-07 09:21:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:16:29.017630"
    },
    {
      "arxiv_id": "2504.04862v1",
      "title": "GAMDTP: Dynamic Trajectory Prediction with Graph Attention Mamba Network",
      "title_zh": "GAMDTP：基于图注意力 Mamba 网络的动态轨迹预测\n",
      "authors": [
        "Yunxiang Liu",
        "Hongkuo Niu",
        "Jianlin Zhu"
      ],
      "abstract": "Accurate motion prediction of traffic agents is crucial for the safety and\nstability of autonomous driving systems. In this paper, we introduce GAMDTP, a\nnovel graph attention-based network tailored for dynamic trajectory prediction.\nSpecifically, we fuse the result of self attention and mamba-ssm through a gate\nmechanism, leveraging the strengths of both to extract features more\nefficiently and accurately, in each graph convolution layer. GAMDTP encodes the\nhigh-definition map(HD map) data and the agents' historical trajectory\ncoordinates and decodes the network's output to generate the final prediction\nresults. Additionally, recent approaches predominantly focus on dynamically\nfusing historical forecast results and rely on two-stage frameworks including\nproposal and refinement. To further enhance the performance of the two-stage\nframeworks we also design a scoring mechanism to evaluate the prediction\nquality during the proposal and refinement processes. Experiments on the\nArgoverse dataset demonstrates that GAMDTP achieves state-of-the-art\nperformance, achieving superior accuracy in dynamic trajectory prediction.",
      "tldr_zh": "该论文提出了GAMDTP，一种基于图注意力Mamba网络的动态轨迹预测方法，旨在提升自动驾驶系统的安全性和稳定性。GAMDTP通过门控机制融合自注意力机制和Mamba-SSM的优势，在每个图卷积层中更高效、准确地提取特征。该模型编码高精地图(HD map)数据和智能体的历史轨迹坐标，并解码网络输出以生成最终预测结果。此外，论文设计了一种评分机制，用于评估proposal和refinement过程中预测质量，以进一步提升两阶段框架的性能。在Argoverse数据集上的实验表明，GAMDTP实现了state-of-the-art的性能，并在动态轨迹预测中实现了卓越的精度。\n",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.04862v1",
      "published_date": "2025-04-07 09:19:20 UTC",
      "updated_date": "2025-04-07 09:19:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:16:41.207996"
    },
    {
      "arxiv_id": "2504.04861v1",
      "title": "SAFT: Structure-aware Transformers for Textual Interaction Classification",
      "title_zh": "SAFT：用于文本交互分类的结构感知 Transformer\n",
      "authors": [
        "Hongtao Wang",
        "Renchi Yang",
        "Hewen Wang",
        "Haoran Zheng",
        "Jianliang Xu"
      ],
      "abstract": "Textual interaction networks (TINs) are an omnipresent data structure used to\nmodel the interplay between users and items on e-commerce websites, social\nnetworks, etc., where each interaction is associated with a text description.\nClassifying such textual interactions (TIC) finds extensive use in detecting\nspam reviews in e-commerce, fraudulent transactions in finance, and so on.\nExisting TIC solutions either (i) fail to capture the rich text semantics due\nto the use of context-free text embeddings, and/or (ii) disregard the bipartite\nstructure and node heterogeneity of TINs, leading to compromised TIC\nperformance. In this work, we propose SAFT, a new architecture that integrates\nlanguage- and graph-based modules for the effective fusion of textual and\nstructural semantics in the representation learning of interactions. In\nparticular, line graph attention (LGA)/gated attention units (GAUs) and\npretrained language models (PLMs) are capitalized on to model the\ninteraction-level and token-level signals, which are further coupled via the\nproxy token in an iterative and contextualized fashion. Additionally, an\nefficient and theoretically-grounded approach is developed to encode the local\nand global topology information pertaining to interactions into structural\nembeddings. The resulting embeddings not only inject the structural features\nunderlying TINs into the textual interaction encoding but also facilitate the\ndesign of graph sampling strategies. Extensive empirical evaluations on\nmultiple real TIN datasets demonstrate the superiority of SAFT over the\nstate-of-the-art baselines in TIC accuracy.",
      "tldr_zh": "本文提出了一种名为SAFT的结构感知Transformer模型，用于文本交互分类(TIC)任务。该模型旨在解决现有方法在处理文本交互网络(TINs)时，无法有效捕捉文本语义和忽略网络结构的问题。SAFT集成了基于语言和图的模块，利用线图注意力(LGA)/门控注意力单元(GAUs)和预训练语言模型(PLMs)来建模交互级别和token级别的信号，并通过代理token进行迭代和上下文融合。此外，SAFT还设计了一种高效且理论上有依据的方法来编码交互的局部和全局拓扑信息到结构嵌入中。实验结果表明，在多个真实TIN数据集上，SAFT的TIC准确率优于现有最佳模型。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.04861v1",
      "published_date": "2025-04-07 09:19:12 UTC",
      "updated_date": "2025-04-07 09:19:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:16:53.324819"
    },
    {
      "arxiv_id": "2504.04858v1",
      "title": "Don't Lag, RAG: Training-Free Adversarial Detection Using RAG",
      "title_zh": "别拖延，RAG：使用 RAG 的免训练对抗检测\n",
      "authors": [
        "Roie Kazoom",
        "Raz Lapid",
        "Moshe Sipper",
        "Ofer Hadar"
      ],
      "abstract": "Adversarial patch attacks pose a major threat to vision systems by embedding\nlocalized perturbations that mislead deep models. Traditional defense methods\noften require retraining or fine-tuning, making them impractical for real-world\ndeployment. We propose a training-free Visual Retrieval-Augmented Generation\n(VRAG) framework that integrates Vision-Language Models (VLMs) for adversarial\npatch detection. By retrieving visually similar patches and images that\nresemble stored attacks in a continuously expanding database, VRAG performs\ngenerative reasoning to identify diverse attack types, all without additional\ntraining or fine-tuning. We extensively evaluate open-source large-scale VLMs,\nincluding Qwen-VL-Plus, Qwen2.5-VL-72B, and UI-TARS-72B-DPO, alongside\nGemini-2.0, a closed-source model. Notably, the open-source UI-TARS-72B-DPO\nmodel achieves up to 95 percent classification accuracy, setting a new\nstate-of-the-art for open-source adversarial patch detection. Gemini-2.0\nattains the highest overall accuracy, 98 percent, but remains closed-source.\nExperimental results demonstrate VRAG's effectiveness in identifying a variety\nof adversarial patches with minimal human annotation, paving the way for\nrobust, practical defenses against evolving adversarial patch attacks.",
      "tldr_zh": "该论文提出了一种无需训练的对抗补丁检测框架VRAG，它利用视觉检索增强生成(Visual Retrieval-Augmented Generation)技术和视觉语言模型(VLMs)来检测图像中的对抗性攻击。VRAG通过检索视觉上相似的补丁和图像，并进行生成式推理，从而识别不同的攻击类型，无需额外的训练或微调。实验评估了多个开源和闭源的大规模VLMs，其中开源的UI-TARS-72B-DPO模型达到了高达95%的分类精度，为开源对抗补丁检测设定了新的SOTA。Gemini-2.0虽然达到了最高的总体精度(98%)，但仍然是闭源的。实验结果表明，VRAG能够有效识别各种对抗补丁，为防御不断演变的对抗补丁攻击提供了一种鲁棒且实用的方法。\n",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.04858v1",
      "published_date": "2025-04-07 09:14:47 UTC",
      "updated_date": "2025-04-07 09:14:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:17:05.501711"
    },
    {
      "arxiv_id": "2504.04855v1",
      "title": "BIASINSPECTOR: Detecting Bias in Structured Data through LLM Agents",
      "title_zh": "BIASINSPECTOR：通过 LLM 智能体检测结构化数据中的偏见\n",
      "authors": [
        "Haoxuan Li",
        "Mingyu Derek Ma",
        "Jen-tse Huang",
        "Zhaotian Weng",
        "Wei Wang",
        "Jieyu Zhao"
      ],
      "abstract": "Detecting biases in structured data is a complex and time-consuming task.\nExisting automated techniques are limited in diversity of data types and\nheavily reliant on human case-by-case handling, resulting in a lack of\ngeneralizability. Currently, large language model (LLM)-based agents have made\nsignificant progress in data science, but their ability to detect data biases\nis still insufficiently explored. To address this gap, we introduce the first\nend-to-end, multi-agent synergy framework, BIASINSPECTOR, designed for\nautomatic bias detection in structured data based on specific user\nrequirements. It first develops a multi-stage plan to analyze user-specified\nbias detection tasks and then implements it with a diverse and well-suited set\nof tools. It delivers detailed results that include explanations and\nvisualizations. To address the lack of a standardized framework for evaluating\nthe capability of LLM agents to detect biases in data, we further propose a\ncomprehensive benchmark that includes multiple evaluation metrics and a large\nset of test cases. Extensive experiments demonstrate that our framework\nachieves exceptional overall performance in structured data bias detection,\nsetting a new milestone for fairer data applications.",
      "tldr_zh": "BIASINSPECTOR是一个端到端的多智能体协同框架，旨在自动检测结构化数据中的偏差。它针对用户指定的偏差检测任务，制定多阶段计划，并利用多样化的工具集执行，提供包含解释和可视化的详细结果。为了评估LLM智能体在数据偏差检测方面的能力，该研究还提出了一个包含多项评估指标和大量测试用例的综合基准。实验表明，BIASINSPECTOR在结构化数据偏差检测方面表现出色，为更公平的数据应用奠定了基础。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "21 pages,6 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.04855v1",
      "published_date": "2025-04-07 09:12:00 UTC",
      "updated_date": "2025-04-07 09:12:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:17:16.968823"
    },
    {
      "arxiv_id": "2504.04850v1",
      "title": "An Efficient Approach for Cooperative Multi-Agent Learning Problems",
      "title_zh": "一种解决合作多智能体学习问题的高效方法\n",
      "authors": [
        "Ángel Aso-Mollar",
        "Eva Onaindia"
      ],
      "abstract": "In this article, we propose a centralized Multi-Agent Learning framework for\nlearning a policy that models the simultaneous behavior of multiple agents that\nneed to coordinate to solve a certain task. Centralized approaches often suffer\nfrom the explosion of an action space that is defined by all possible\ncombinations of individual actions, known as joint actions. Our approach\naddresses the coordination problem via a sequential abstraction, which\novercomes the scalability problems typical to centralized methods. It\nintroduces a meta-agent, called \\textit{supervisor}, which abstracts joint\nactions as sequential assignments of actions to each agent. This sequential\nabstraction not only simplifies the centralized joint action space but also\nenhances the framework's scalability and efficiency. Our experimental results\ndemonstrate that the proposed approach successfully coordinates agents across a\nvariety of Multi-Agent Learning environments of diverse sizes.",
      "tldr_zh": "本文提出了一种高效的中心化多智能体学习框架，用于学习多智能体协同策略。该方法通过序列抽象解决中心化方法中联合动作空间爆炸的问题。引入了一个名为“supervisor”的元智能体，将联合动作抽象为对每个智能体动作的顺序分配。这种序列抽象简化了中心化的联合动作空间，提高了框架的可扩展性和效率。实验结果表明，该方法成功地在各种不同规模的多智能体学习环境中协调了智能体。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at ICTAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2504.04850v1",
      "published_date": "2025-04-07 09:03:35 UTC",
      "updated_date": "2025-04-07 09:03:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:17:29.029577"
    },
    {
      "arxiv_id": "2504.04833v1",
      "title": "Explanation-Driven Interventions for Artificial Intelligence Model Customization: Empowering End-Users to Tailor Black-Box AI in Rhinocytology",
      "title_zh": "解释驱动的人工智能模型定制干预：赋能最终用户在鼻细胞学中定制黑盒人工智能\n",
      "authors": [
        "Andrea Esposito",
        "Miriana Calvano",
        "Antonio Curci",
        "Francesco Greco",
        "Rosa Lanzilotti",
        "Antonio Piccinno"
      ],
      "abstract": "The integration of Artificial Intelligence (AI) in modern society is heavily\nshifting the way that individuals carry out their tasks and activities.\nEmploying AI-based systems raises challenges that designers and developers must\naddress to ensure that humans remain in control of the interaction process,\nparticularly in high-risk domains. This article presents a novel End-User\nDevelopment (EUD) approach for black-box AI models through a redesigned user\ninterface in the Rhino-Cyt platform, a medical AI-based decision-support system\nfor medical professionals (more precisely, rhinocytologists) to carry out cell\nclassification. The proposed interface empowers users to intervene in AI\ndecision-making process by editing explanations and reconfiguring the model,\ninfluencing its future predictions. This work contributes to Human-Centered AI\n(HCAI) and EUD by discussing how explanation-driven interventions allow a blend\nof explainability, user intervention, and model reconfiguration, fostering a\nsymbiosis between humans and user-tailored AI systems.",
      "tldr_zh": "该论文提出了一种新颖的终端用户开发(EUD)方法，用于定制黑盒AI模型，特别是在鼻细胞学领域。通过重新设计的Rhinocyt平台用户界面，该方法允许用户通过编辑解释和重新配置模型来干预AI的决策过程，从而影响其未来的预测。这种解释驱动的干预结合了解释性、用户干预和模型重配置，旨在促进人和用户定制的AI系统之间的共生关系，并为以人为中心的AI(HCAI)做出贡献。\n",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "First version (14 pages, 12 of content that will be reduced to 8 in\n  the near future)",
      "pdf_url": "http://arxiv.org/pdf/2504.04833v1",
      "published_date": "2025-04-07 08:44:48 UTC",
      "updated_date": "2025-04-07 08:44:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:17:41.092935"
    },
    {
      "arxiv_id": "2504.04827v1",
      "title": "From Specificity to Generality: Revisiting Generalizable Artifacts in Detecting Face Deepfakes",
      "title_zh": "从特殊性到普遍性：重新审视人脸 Deepfake 检测中的可泛化伪影\n",
      "authors": [
        "Long Ma",
        "Zhiyuan Yan",
        "Yize Chen",
        "Jin Xu",
        "Qinglang Guo",
        "Hu Huang",
        "Yong Liao",
        "Hui Lin"
      ],
      "abstract": "Detecting deepfakes has been an increasingly important topic, especially\ngiven the rapid development of AI generation techniques. In this paper, we ask:\nHow can we build a universal detection framework that is effective for most\nfacial deepfakes? One significant challenge is the wide variety of deepfake\ngenerators available, resulting in varying forgery artifacts (e.g., lighting\ninconsistency, color mismatch, etc). But should we ``teach\" the detector to\nlearn all these artifacts separately? It is impossible and impractical to\nelaborate on them all. So the core idea is to pinpoint the more common and\ngeneral artifacts across different deepfakes. Accordingly, we categorize\ndeepfake artifacts into two distinct yet complementary types: Face\nInconsistency Artifacts (FIA) and Up-Sampling Artifacts (USA). FIA arise from\nthe challenge of generating all intricate details, inevitably causing\ninconsistencies between the complex facial features and relatively uniform\nsurrounding areas. USA, on the other hand, are the inevitable traces left by\nthe generator's decoder during the up-sampling process. This categorization\nstems from the observation that all existing deepfakes typically exhibit one or\nboth of these artifacts. To achieve this, we propose a new data-level\npseudo-fake creation framework that constructs fake samples with only the FIA\nand USA, without introducing extra less-general artifacts. Specifically, we\nemploy a super-resolution to simulate the USA, while design a Blender module\nthat uses image-level self-blending on diverse facial regions to create the\nFIA. We surprisingly found that, with this intuitive design, a standard image\nclassifier trained only with our pseudo-fake data can non-trivially generalize\nwell to unseen deepfakes.",
      "tldr_zh": "该论文探讨了如何构建通用的面部Deepfake检测框架。核心思想是识别不同Deepfake中更常见和通用的伪造痕迹，而非学习所有特定伪造特征。作者将Deepfake伪造痕迹分为两类：面部不一致性伪影(Face Inconsistency Artifacts, FIA)和上采样伪影(Up-Sampling Artifacts, USA)。FIA源于生成复杂面部细节的挑战，导致面部特征与周围区域不一致；USA是生成器解码器在上采样过程中留下的痕迹。论文提出了一种新的数据层面的伪造样本生成框架，仅使用FIA和USA构建伪造样本。实验结果表明，仅使用该伪造数据训练的标准图像分类器能够很好地泛化到未见过的Deepfake。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.04827v1",
      "published_date": "2025-04-07 08:34:28 UTC",
      "updated_date": "2025-04-07 08:34:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:17:53.579878"
    },
    {
      "arxiv_id": "2504.04823v1",
      "title": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning Models",
      "title_zh": "量化会损害推理能力吗？量化推理模型的实证研究\n",
      "authors": [
        "Ruikang Liu",
        "Yuxuan Sun",
        "Manyi Zhang",
        "Haoli Bai",
        "Xianzhi Yu",
        "Tiezheng Yu",
        "Chun Yuan",
        "Lu Hou"
      ],
      "abstract": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this study, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, and QwQ-32B. Our investigation covers weight, KV cache, and\nactivation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes will be open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models.",
      "tldr_zh": "该研究对量化推理模型进行了系统性评估，旨在降低大型语言模型在推理过程中的计算开销。研究人员针对DeepSeek-R1-Distilled、Qwen和LLaMA等多个开源模型家族（参数规模从1.5B到70B）以及QwQ-32B模型，进行了权重、KV缓存和激活量化实验，并采用最先进的算法在不同比特宽度下进行评估。实验涵盖数学(AIME, MATH-500)、科学(GPQA)和编程(LiveCodeBench)等推理基准。研究发现，W8A8或W4A16量化可以实现无损量化，但更低的比特宽度会显著降低准确率。模型大小、模型来源和任务难度是影响性能的关键因素。通过策略性地扩展模型大小或推理步骤可以有效提升性能。所有量化模型和代码将在GitHub上开源。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.04823v1",
      "published_date": "2025-04-07 08:22:45 UTC",
      "updated_date": "2025-04-07 08:22:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:18:05.707100"
    },
    {
      "arxiv_id": "2504.04821v1",
      "title": "A Customized SAT-based Solver for Graph Coloring",
      "title_zh": "一种用于图着色的定制化基于 SAT 的求解器\n",
      "authors": [
        "Timo Brand",
        "Daniel Faber",
        "Stephan Held",
        "Petra Mutzel"
      ],
      "abstract": "We introduce ZykovColor, a novel SAT-based algorithm to solve the graph\ncoloring problem working on top of an encoding that mimics the Zykov tree. Our\nmethod is based on an approach of H\\'ebrard and Katsirelos (2020) that employs\na propagator to enforce transitivity constraints, incorporate lower bounds for\nsearch tree pruning, and enable inferred propagations. We leverage the recently\nintroduced IPASIR-UP interface for CaDiCal to implement these techniques with a\nSAT solver. Furthermore, we propose new features that take advantage of the\nunderlying SAT solver. These include modifying the integrated decision strategy\nwith vertex domination hints and using incremental bottom-up search that allows\nto reuse learned clauses from previous calls. Additionally, we integrate a more\nefficient clique computation to improve the lower bounds during the search. We\nvalidate the effectiveness of each new feature through an experimental\nanalysis. ZykovColor outperforms other state-of-the-art graph coloring\nimplementations on the DIMACS benchmark set. Further experiments on random\nErd\\H{o}s-R\\'enyi graphs show that our new approach dominates state-of-the-art\nSAT-based methods for both very sparse and highly dense graphs.",
      "tldr_zh": "该论文提出了一种新的基于SAT的图着色算法ZykovColor，该算法基于模仿Zykov树的编码。该方法利用传播器来加强传递性约束，结合下界进行搜索树剪枝，并启用推断传播。 通过IPASIR-UP接口在CaDiCal中实现了这些技术。此外，论文还提出了新的特性，包括使用顶点支配提示修改集成的决策策略，以及使用允许重用先前调用中学习到的子句的增量自下而上搜索。同时，集成了更有效的团计算，以改进搜索期间的下界。实验结果表明，ZykovColor在DIMACS基准测试集上优于其他最先进的图着色实现，并且在随机Erdős-Rényi图上优于最先进的基于SAT的方法。\n",
      "categories": [
        "cs.DM",
        "cs.AI",
        "cs.DS",
        "cs.LO",
        "05C15",
        "G.2.2"
      ],
      "primary_category": "cs.DM",
      "comment": "5 figures, 2 tables, source code published at\n  https://github.com/trewes/ZykovColor",
      "pdf_url": "http://arxiv.org/pdf/2504.04821v1",
      "published_date": "2025-04-07 08:22:00 UTC",
      "updated_date": "2025-04-07 08:22:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:18:17.580280"
    },
    {
      "arxiv_id": "2504.04808v1",
      "title": "ELT-Bench: An End-to-End Benchmark for Evaluating AI Agents on ELT Pipelines",
      "title_zh": "ELT-Bench：用于评估 AI Agent 在 ELT 流水线上性能的端到端基准测试",
      "authors": [
        "Tengjun Jin",
        "Yuxuan Zhu",
        "Daniel Kang"
      ],
      "abstract": "Practitioners are increasingly turning to Extract-Load-Transform (ELT)\npipelines with the widespread adoption of cloud data warehouses. However,\ndesigning these pipelines often involves significant manual work to ensure\ncorrectness. Recent advances in AI-based methods, which have shown strong\ncapabilities in data tasks, such as text-to-SQL, present an opportunity to\nalleviate manual efforts in developing ELT pipelines. Unfortunately, current\nbenchmarks in data engineering only evaluate isolated tasks, such as using data\ntools and writing data transformation queries, leaving a significant gap in\nevaluating AI agents for generating end-to-end ELT pipelines.\n  To fill this gap, we introduce ELT-Bench, an end-to-end benchmark designed to\nassess the capabilities of AI agents to build ELT pipelines. ELT-Bench consists\nof 100 pipelines, including 835 source tables and 203 data models across\nvarious domains. By simulating realistic scenarios involving the integration of\ndiverse data sources and the use of popular data tools, ELT-Bench evaluates AI\nagents' abilities in handling complex data engineering workflows. AI agents\nmust interact with databases and data tools, write code and SQL queries, and\norchestrate every pipeline stage. We evaluate two representative code agent\nframeworks, Spider-Agent and SWE-Agent, using six popular Large Language Models\n(LLMs) on ELT-Bench. The highest-performing agent, Spider-Agent\nClaude-3.7-Sonnet with extended thinking, correctly generates only 3.9% of data\nmodels, with an average cost of $4.30 and 89.3 steps per pipeline. Our\nexperimental results demonstrate the challenges of ELT-Bench and highlight the\nneed for a more advanced AI agent to reduce manual effort in ELT workflows. Our\ncode and data are available at https://github.com/uiuc-kang-lab/ETL.git.",
      "tldr_zh": "ELT-Bench是一个端到端基准测试，用于评估AI Agent构建Extract-Load-Transform (ELT) pipelines的能力。该基准包含100个pipelines，涵盖835个源表和203个数据模型，模拟了集成多样数据源和使用流行数据工具的真实场景。实验评估了Spider-Agent和SWE-Agent两个代码Agent框架在ELT-Bench上的表现，结果表明现有AI Agent在生成端到端ELT pipelines方面仍面临挑战，最高性能的Agent也仅能正确生成3.9%的数据模型，且成本较高。该研究强调了开发更先进的AI Agent以减少ELT workflows中人工干预的必要性。\n",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "primary_category": "cs.DB",
      "comment": "14 pages, 18 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.04808v1",
      "published_date": "2025-04-07 08:03:36 UTC",
      "updated_date": "2025-04-07 08:03:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:18:29.464310"
    },
    {
      "arxiv_id": "2504.04789v1",
      "title": "Multimodal Agricultural Agent Architecture (MA3): A New Paradigm for Intelligent Agricultural Decision-Making",
      "title_zh": "多模态农业智能体架构（MA3）：智能农业决策的新范式\n",
      "authors": [
        "Zhuoning Xu",
        "Jian Xu",
        "Mingqing Zhang",
        "Peijie Wang",
        "Chao Deng",
        "Cheng-Lin Liu"
      ],
      "abstract": "As a strategic pillar industry for human survival and development, modern\nagriculture faces dual challenges: optimizing production efficiency and\nachieving sustainable development. Against the backdrop of intensified climate\nchange leading to frequent extreme weather events, the uncertainty risks in\nagricultural production systems are increasing exponentially. To address these\nchallenges, this study proposes an innovative \\textbf{M}ultimodal\n\\textbf{A}gricultural \\textbf{A}gent \\textbf{A}rchitecture (\\textbf{MA3}),\nwhich leverages cross-modal information fusion and task collaboration\nmechanisms to achieve intelligent agricultural decision-making. This study\nconstructs a multimodal agricultural agent dataset encompassing five major\ntasks: classification, detection, Visual Question Answering (VQA), tool\nselection, and agent evaluation. We propose a unified backbone for sugarcane\ndisease classification and detection tools, as well as a sugarcane disease\nexpert model. By integrating an innovative tool selection module, we develop a\nmultimodal agricultural agent capable of effectively performing tasks in\nclassification, detection, and VQA. Furthermore, we introduce a\nmulti-dimensional quantitative evaluation framework and conduct a comprehensive\nassessment of the entire architecture over our evaluation dataset, thereby\nverifying the practicality and robustness of MA3 in agricultural scenarios.\nThis study provides new insights and methodologies for the development of\nagricultural agents, holding significant theoretical and practical\nimplications. Our source code and dataset will be made publicly available upon\nacceptance.",
      "tldr_zh": "该研究提出了多模态农业智能体架构(MA3)，旨在应对现代农业中优化生产效率和实现可持续发展的双重挑战。MA3利用跨模态信息融合和任务协作机制，实现智能农业决策。研究构建了一个包含分类、检测、视觉问答(VQA)、工具选择和智能体评估五大任务的多模态农业智能体数据集，并提出了甘蔗病害分类和检测的统一骨干网络以及甘蔗病害专家模型。通过集成创新的工具选择模块，MA3能够有效执行分类、检测和VQA任务。此外，研究还引入了多维度量化评估框架，验证了MA3在农业场景中的实用性和鲁棒性。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.04789v1",
      "published_date": "2025-04-07 07:32:41 UTC",
      "updated_date": "2025-04-07 07:32:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:18:41.474881"
    },
    {
      "arxiv_id": "2504.04787v1",
      "title": "Dynamic Vision Mamba",
      "title_zh": "动态视觉 Mamba\n",
      "authors": [
        "Mengxuan Wu",
        "Zekai Li",
        "Zhiyuan Liang",
        "Moyang Li",
        "Xuanlei Zhao",
        "Samir Khaki",
        "Zheng Zhu",
        "Xiaojiang Peng",
        "Konstantinos N. Plataniotis",
        "Kai Wang",
        "Wangbo Zhao",
        "Yang You"
      ],
      "abstract": "Mamba-based vision models have gained extensive attention as a result of\nbeing computationally more efficient than attention-based models. However,\nspatial redundancy still exists in these models, represented by token and block\nredundancy. For token redundancy, we analytically find that early token pruning\nmethods will result in inconsistency between training and inference or\nintroduce extra computation for inference. Therefore, we customize token\npruning to fit the Mamba structure by rearranging the pruned sequence before\nfeeding it into the next Mamba block. For block redundancy, we allow each image\nto select SSM blocks dynamically based on an empirical observation that the\ninference speed of Mamba-based vision models is largely affected by the number\nof SSM blocks. Our proposed method, Dynamic Vision Mamba (DyVM), effectively\nreduces FLOPs with minor performance drops. We achieve a reduction of 35.2\\%\nFLOPs with only a loss of accuracy of 1.7\\% on Vim-S. It also generalizes well\nacross different Mamba vision model architectures and different vision tasks.\nOur code will be made public.",
      "tldr_zh": "该论文提出了Dynamic Vision Mamba (DyVM)，旨在解决Mamba视觉模型中存在的空间冗余问题，包括token冗余和block冗余。针对token冗余，研究者通过重新排列剪枝后的序列以适应Mamba结构，避免了训练和推理的不一致性。针对block冗余，DyVM允许每个图像动态选择SSM blocks，从而减少计算量。实验结果表明，DyVM在Vim-S上实现了35.2%的FLOPs减少，精度损失仅为1.7%，并且能够很好地推广到不同的Mamba视觉模型架构和视觉任务中。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.04787v1",
      "published_date": "2025-04-07 07:31:28 UTC",
      "updated_date": "2025-04-07 07:31:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:18:53.309423"
    },
    {
      "arxiv_id": "2504.04785v1",
      "title": "Weak-for-Strong: Training Weak Meta-Agent to Harness Strong Executors",
      "title_zh": "以弱驭强：训练弱元智能体以驾驭强执行器\n",
      "authors": [
        "Fan Nie",
        "Lan Feng",
        "Haotian Ye",
        "Weixin Liang",
        "Pan Lu",
        "Huaxiu Yao",
        "Alexandre Alahi",
        "James Zou"
      ],
      "abstract": "Efficiently leveraging of the capabilities of contemporary large language\nmodels (LLMs) is increasingly challenging, particularly when direct fine-tuning\nis expensive and often impractical. Existing training-free methods, including\nmanually or automated designed workflows, typically demand substantial human\neffort or yield suboptimal results. This paper proposes Weak-for-Strong\nHarnessing (W4S), a novel framework that customizes smaller, cost-efficient\nlanguage models to design and optimize workflows for harnessing stronger\nmodels. W4S formulates workflow design as a multi-turn markov decision process\nand introduces reinforcement learning for agentic workflow optimization (RLAO)\nto train a weak meta-agent. Through iterative interaction with the environment,\nthe meta-agent learns to design increasingly effective workflows without manual\nintervention. Empirical results demonstrate the superiority of W4S that our 7B\nmeta-agent, trained with just one GPU hour, outperforms the strongest baseline\nby 2.9% ~ 24.6% across eleven benchmarks, successfully elevating the\nperformance of state-of-the-art models such as GPT-3.5-Turbo and GPT-4o.\nNotably, W4S exhibits strong generalization capabilities across both seen and\nunseen tasks, offering an efficient, high-performing alternative to directly\nfine-tuning strong models.",
      "tldr_zh": "该论文提出了Weak-for-Strong Harnessing (W4S)框架，旨在利用小型、低成本的语言模型来设计和优化工作流程，从而更好地利用大型语言模型 (LLMs) 的能力。W4S将工作流程设计建模为多轮马尔可夫决策过程，并引入强化学习进行智能体工作流程优化 (RLAO)，以训练一个弱元智能体。实验结果表明，使用单个GPU小时训练的7B元智能体在11个基准测试中优于最强的基线模型2.9% ~ 24.6%，成功提升了GPT-3.5-Turbo和GPT-4o等先进模型的性能。W4S在已见和未见任务中均表现出强大的泛化能力，为直接微调大型模型提供了一种高效、高性能的替代方案。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.04785v1",
      "published_date": "2025-04-07 07:27:31 UTC",
      "updated_date": "2025-04-07 07:27:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:19:05.524578"
    },
    {
      "arxiv_id": "2504.04770v1",
      "title": "Bidirectional Hierarchical Protein Multi-Modal Representation Learning",
      "title_zh": "双向分层蛋白质多模态表征学习\n",
      "authors": [
        "Xuefeng Liu",
        "Songhao Jiang",
        "Chih-chan Tien",
        "Jinbo Xu",
        "Rick Stevens"
      ],
      "abstract": "Protein representation learning is critical for numerous biological tasks.\nRecently, large transformer-based protein language models (pLMs) pretrained on\nlarge scale protein sequences have demonstrated significant success in\nsequence-based tasks. However, pLMs lack structural information. Conversely,\ngraph neural networks (GNNs) designed to leverage 3D structural information\nhave shown promising generalization in protein-related prediction tasks, but\ntheir effectiveness is often constrained by the scarcity of labeled structural\ndata. Recognizing that sequence and structural representations are\ncomplementary perspectives of the same protein entity, we propose a multimodal\nbidirectional hierarchical fusion framework to effectively merge these\nmodalities. Our framework employs attention and gating mechanisms to enable\neffective interaction between pLMs-generated sequential representations and\nGNN-extracted structural features, improving information exchange and\nenhancement across layers of the neural network. Based on the framework, we\nfurther introduce local Bi-Hierarchical Fusion with gating and global\nBi-Hierarchical Fusion with multihead self-attention approaches. Through\nextensive experiments on a diverse set of protein-related tasks, our method\ndemonstrates consistent improvements over strong baselines and existing fusion\ntechniques in a variety of protein representation learning benchmarks,\nincluding react (enzyme/EC classification), model quality assessment (MQA),\nprotein-ligand binding affinity prediction (LBA), protein-protein binding site\nprediction (PPBS), and B cell epitopes prediction (BCEs). Our method\nestablishes a new state-of-the-art for multimodal protein representation\nlearning, emphasizing the efficacy of BIHIERARCHICAL FUSION in bridging\nsequence and structural modalities.",
      "tldr_zh": "该论文提出了一种双向分层蛋白质多模态表示学习框架(BIHIERARCHICAL FUSION)，旨在融合蛋白质序列和结构信息，克服现有蛋白质语言模型(pLMs)缺乏结构信息以及图神经网络(GNNs)受限于结构数据不足的局限性。该框架利用注意力机制和门控机制，实现pLMs生成的序列表示和GNN提取的结构特征之间的有效交互和信息增强。通过局部和全局的双向分层融合方法，该模型在酶分类(react)、模型质量评估(MQA)、蛋白质-配体结合亲和力预测(LBA)、蛋白质-蛋白质结合位点预测(PPBS)和B细胞表位预测(BCEs)等多个蛋白质相关任务上均取得了显著提升，确立了多模态蛋白质表示学习的新SOTA。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.MN"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.04770v1",
      "published_date": "2025-04-07 06:47:49 UTC",
      "updated_date": "2025-04-07 06:47:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:19:17.535026"
    },
    {
      "arxiv_id": "2504.04766v1",
      "title": "KunPeng: A Global Ocean Environmental Model",
      "title_zh": "鲲鹏：一个全球海洋环境模型\n",
      "authors": [
        "Yi Zhao",
        "Jiaqi Li",
        "Haitao Xia",
        "Tianjiao Zhang",
        "Zerong Zeng",
        "Tianyu Ren",
        "Yucheng Zhang",
        "Chao Zhu",
        "Shengtong Xu",
        "Hongchun Yuan"
      ],
      "abstract": "Inspired by the similarity of the atmosphere-ocean physical coupling\nmechanism, this study innovatively migrates meteorological large-model\ntechniques to the ocean domain, constructing the KunPeng global ocean\nenvironmental prediction model. Aimed at the discontinuous characteristics of\nmarine space, we propose a terrain-adaptive mask constraint mechanism to\nmitigate effectively training divergence caused by abrupt gradients at land-sea\nboundaries. To fully integrate far-, medium-, and close-range marine features,\na longitude-cyclic deformable convolution network (LC-DCN) is employed to\nenhance the dynamic receptive field, achieving refined modeling of multi-scale\noceanic characteristics. A Deformable Convolution-enhanced Multi-Step\nPrediction module (DC-MTP) is employed to strengthen temporal dependency\nfeature extraction capabilities. Experimental results demonstrate that this\nmodel achieves an average ACC of 0.80 in 15-day global predictions at\n0.25$^\\circ$ resolution, outperforming comparative models by 0.01-0.08. The\naverage mean squared error (MSE) is 0.41 (representing a 5%-31% reduction) and\nthe average mean absolute error (MAE) is 0.44 (0.6%-21% reduction) compared to\nother models. Significant improvements are particularly observed in sea surface\nparameter prediction, deep-sea region characterization, and current velocity\nfield forecasting. Through a horizontal comparison of the applicability of\noperators at different scales in the marine domain, this study reveals that\nlocal operators significantly outperform global operators under slow-varying\noceanic processes, demonstrating the effectiveness of dynamic feature pyramid\nrepresentations in predicting marine physical parameters.",
      "tldr_zh": "该研究借鉴气象大模型技术，构建了全球海洋环境预测模型KunPeng。针对海洋空间的不连续性，提出了地形自适应掩码约束机制，缓解陆海边界梯度突变导致的训练发散问题。采用经度循环可变形卷积网络(LC-DCN)增强动态感受野，实现多尺度海洋特征的精细建模。利用可变形卷积增强的多步预测模块(DC-MTP)加强时间依赖性特征提取能力。实验结果表明，KunPeng模型在0.25$^\\circ$分辨率下15天全球预测中平均ACC达到0.80，优于其他模型0.01-0.08。该模型在海面参数预测、深海区域表征和流速场预测方面有显著提升，并验证了局部算子在缓慢变化的海洋过程中优于全局算子，证明了动态特征金字塔表示在预测海洋物理参数中的有效性。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.04766v1",
      "published_date": "2025-04-07 06:41:05 UTC",
      "updated_date": "2025-04-07 06:41:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:19:29.704367"
    },
    {
      "arxiv_id": "2504.04764v1",
      "title": "Enhancing Leaf Disease Classification Using GAT-GCN Hybrid Model",
      "title_zh": "使用 GAT-GCN 混合模型增强叶片病害分类\n",
      "authors": [
        "Shyam Sundhar",
        "Riya Sharma",
        "Priyansh Maheshwari",
        "Suvidha Rupesh Kumar",
        "T. Sunil Kumar"
      ],
      "abstract": "Agriculture plays a critical role in the global economy, providing\nlivelihoods and ensuring food security for billions. As innovative agricultural\npractices become more widespread, the risk of crop diseases has increased,\nhighlighting the urgent need for efficient, low-intervention disease\nidentification methods. This research presents a hybrid model combining Graph\nAttention Networks (GATs) and Graph Convolution Networks (GCNs) for leaf\ndisease classification. GCNs have been widely used for learning from\ngraph-structured data, and GATs enhance this by incorporating attention\nmechanisms to focus on the most important neighbors. The methodology integrates\nsuperpixel segmentation for efficient feature extraction, partitioning images\ninto meaningful, homogeneous regions that better capture localized features.\nThe authors have employed an edge augmentation technique to enhance the\nrobustness of the model. The edge augmentation technique has introduced a\nsignificant degree of generalization in the detection capabilities of the\nmodel. To further optimize training, weight initialization techniques are\napplied. The hybrid model is evaluated against the individual performance of\nthe GCN and GAT models and the hybrid model achieved a precision of 0.9822,\nrecall of 0.9818, and F1-score of 0.9818 in apple leaf disease classification,\na precision of 0.9746, recall of 0.9744, and F1-score of 0.9743 in potato leaf\ndisease classification, and a precision of 0.8801, recall of 0.8801, and\nF1-score of 0.8799 in sugarcane leaf disease classification. These results\ndemonstrate the robustness and performance of the model, suggesting its\npotential to support sustainable agricultural practices through precise and\neffective disease detection. This work is a small step towards reducing the\nloss of crops and hence supporting sustainable goals of zero hunger and life on\nland.",
      "tldr_zh": "该研究提出了一种结合图注意力网络(GATs)和图卷积网络(GCNs)的混合模型，用于提升叶片病害分类的准确性。该模型首先利用超像素分割进行高效特征提取，然后通过GATs的注意力机制关注重要邻域信息，并采用边缘增强技术提高模型的泛化能力。实验结果表明，该混合模型在苹果、马铃薯和甘蔗叶片病害分类中均取得了优异的精度、召回率和F1-score，证明了其在精准有效的病害检测方面具有潜力，有助于支持可持续农业发展。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.04764v1",
      "published_date": "2025-04-07 06:31:38 UTC",
      "updated_date": "2025-04-07 06:31:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:19:41.224084"
    },
    {
      "arxiv_id": "2504.04751v1",
      "title": "Unsupervised Estimation of Nonlinear Audio Effects: Comparing Diffusion-Based and Adversarial approaches",
      "title_zh": "非线性音频效果的无监督估计：比较基于扩散和对抗的方法\n",
      "authors": [
        "Eloi Moliner",
        "Michal Švento",
        "Alec Wright",
        "Lauri Juvela",
        "Pavel Rajmic",
        "Vesa Välimäki"
      ],
      "abstract": "Accurately estimating nonlinear audio effects without access to paired\ninput-output signals remains a challenging problem.This work studies\nunsupervised probabilistic approaches for solving this task. We introduce a\nmethod, novel for this application, based on diffusion generative models for\nblind system identification, enabling the estimation of unknown nonlinear\neffects using black- and gray-box models. This study compares this method with\na previously proposed adversarial approach, analyzing the performance of both\nmethods under different parameterizations of the effect operator and varying\nlengths of available effected recordings.Through experiments on guitar\ndistortion effects, we show that the diffusion-based approach provides more\nstable results and is less sensitive to data availability, while the\nadversarial approach is superior at estimating more pronounced distortion\neffects. Our findings contribute to the robust unsupervised blind estimation of\naudio effects, demonstrating the potential of diffusion models for system\nidentification in music technology.",
      "tldr_zh": "该研究探讨了在无配对输入输出信号的情况下，对非线性音频效果进行无监督估计的方法。提出了一种基于扩散生成模型(diffusion generative models)的盲系统辨识方法，用于估计未知的非线性效果，并将其与先前提出的对抗方法进行比较。通过对吉他失真效果的实验，结果表明，基于扩散的方法提供更稳定的结果，且对数据可用性不太敏感，而对抗方法在估计更明显的失真效果方面更优。该研究为音频效果的鲁棒无监督盲估计做出了贡献，展示了扩散模型在音乐技术中系统辨识方面的潜力。\n",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "primary_category": "eess.AS",
      "comment": "Submitted to the 28th International Conference on Digital Audio\n  Effects (DAFx25)",
      "pdf_url": "http://arxiv.org/pdf/2504.04751v1",
      "published_date": "2025-04-07 05:56:51 UTC",
      "updated_date": "2025-04-07 05:56:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:19:53.312246"
    },
    {
      "arxiv_id": "2504.04744v1",
      "title": "Grounding 3D Object Affordance with Language Instructions, Visual Observations and Interactions",
      "title_zh": "基于语言指令、视觉观察和交互的 3D 物体可供性定位\n",
      "authors": [
        "He Zhu",
        "Quyu Kong",
        "Kechun Xu",
        "Xunlong Xia",
        "Bing Deng",
        "Jieping Ye",
        "Rong Xiong",
        "Yue Wang"
      ],
      "abstract": "Grounding 3D object affordance is a task that locates objects in 3D space\nwhere they can be manipulated, which links perception and action for embodied\nintelligence. For example, for an intelligent robot, it is necessary to\naccurately ground the affordance of an object and grasp it according to human\ninstructions. In this paper, we introduce a novel task that grounds 3D object\naffordance based on language instructions, visual observations and\ninteractions, which is inspired by cognitive science. We collect an Affordance\nGrounding dataset with Points, Images and Language instructions (AGPIL) to\nsupport the proposed task. In the 3D physical world, due to observation\norientation, object rotation, or spatial occlusion, we can only get a partial\nobservation of the object. So this dataset includes affordance estimations of\nobjects from full-view, partial-view, and rotation-view perspectives. To\naccomplish this task, we propose LMAffordance3D, the first multi-modal,\nlanguage-guided 3D affordance grounding network, which applies a\nvision-language model to fuse 2D and 3D spatial features with semantic\nfeatures. Comprehensive experiments on AGPIL demonstrate the effectiveness and\nsuperiority of our method on this task, even in unseen experimental settings.\nOur project is available at https://sites.google.com/view/lmaffordance3d.",
      "tldr_zh": "本文提出了一个新任务：基于语言指令、视觉观察和交互来定位3D物体可操作区域，即3D物体Affordance Grounding。为此，作者构建了一个名为AGPIL (Affordance Grounding dataset with Points, Images and Language instructions) 的数据集，包含物体在完整视角、部分视角和旋转视角下的可操作性标注。同时，提出了首个多模态、语言引导的3D Affordance Grounding网络LMAffordance3D，该网络利用视觉-语言模型融合2D和3D空间特征与语义特征。在AGPIL上的实验结果表明，LMAffordance3D在该任务上表现出色，即使在未见过的实验环境中也有效。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.04744v1",
      "published_date": "2025-04-07 05:38:23 UTC",
      "updated_date": "2025-04-07 05:38:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:20:05.473265"
    },
    {
      "arxiv_id": "2504.04740v1",
      "title": "Enhancing Compositional Reasoning in Vision-Language Models with Synthetic Preference Data",
      "title_zh": "利用合成偏好数据增强视觉-语言模型中的组合推理能力\n",
      "authors": [
        "Samarth Mishra",
        "Kate Saenko",
        "Venkatesh Saligrama"
      ],
      "abstract": "Compositionality, or correctly recognizing scenes as compositions of atomic\nvisual concepts, remains difficult for multimodal large language models\n(MLLMs). Even state of the art MLLMs such as GPT-4o can make mistakes in\ndistinguishing compositions like \"dog chasing cat\" vs \"cat chasing dog\". While\non Winoground, a benchmark for measuring such reasoning, MLLMs have made\nsignificant progress, they are still far from a human's performance. We show\nthat compositional reasoning in these models can be improved by elucidating\nsuch concepts via data, where a model is trained to prefer the correct caption\nfor an image over a close but incorrect one. We introduce SCRAMBLe: Synthetic\nCompositional Reasoning Augmentation of MLLMs with Binary preference Learning,\nan approach for preference tuning open-weight MLLMs on synthetic preference\ndata generated in a fully automated manner from existing image-caption data.\nSCRAMBLe holistically improves these MLLMs' compositional reasoning\ncapabilities which we can see through significant improvements across multiple\nvision language compositionality benchmarks, as well as smaller but significant\nimprovements on general question answering tasks. As a sneak peek, SCRAMBLe\ntuned Molmo-7B model improves on Winoground from 49.5% to 54.8% (best reported\nto date), while improving by ~1% on more general visual question answering\ntasks. Code for SCRAMBLe along with tuned models and our synthetic training\ndataset is available at https://github.com/samarth4149/SCRAMBLe.",
      "tldr_zh": "该论文提出SCRAMBLe，一种利用合成偏好数据增强视觉语言模型(MLLMs)组合推理能力的方法。SCRAMBLe通过二元偏好学习，训练开放权重MLLMs区分图像与其细微差异的错误描述，从而提高模型对视觉概念组合的识别能力，例如区分“狗追猫”和“猫追狗”。该方法全自动地从现有图像-标题数据生成合成偏好数据。实验结果表明，SCRAMBLe显著提升了MLLMs在多个视觉语言组合基准测试上的性能，并在通用视觉问答任务上取得了一定的改进。例如，经过SCRAMBLe调整的Molmo-7B模型在Winoground上的准确率从49.5%提高到54.8%。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.04740v1",
      "published_date": "2025-04-07 05:35:34 UTC",
      "updated_date": "2025-04-07 05:35:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:20:17.695970"
    },
    {
      "arxiv_id": "2504.04737v1",
      "title": "TathyaNyaya and FactLegalLlama: Advancing Factual Judgment Prediction and Explanation in the Indian Legal Context",
      "title_zh": "TathyaNyaya 和 FactLegalLlama：推进印度法律背景下的事实判断预测和解释\n",
      "authors": [
        "Shubham Kumar Nigam",
        "Balaramamahanthi Deepak Patnaik",
        "Shivam Mishra",
        "Noel Shallum",
        "Kripabandhu Ghosh",
        "Arnab Bhattacharya"
      ],
      "abstract": "In the landscape of Fact-based Judgment Prediction and Explanation (FJPE),\nreliance on factual data is essential for developing robust and realistic\nAI-driven decision-making tools. This paper introduces TathyaNyaya, the largest\nannotated dataset for FJPE tailored to the Indian legal context, encompassing\njudgments from the Supreme Court of India and various High Courts. Derived from\nthe Hindi terms \"Tathya\" (fact) and \"Nyaya\" (justice), the TathyaNyaya dataset\nis uniquely designed to focus on factual statements rather than complete legal\ntexts, reflecting real-world judicial processes where factual data drives\noutcomes. Complementing this dataset, we present FactLegalLlama, an\ninstruction-tuned variant of the LLaMa-3-8B Large Language Model (LLM),\noptimized for generating high-quality explanations in FJPE tasks. Finetuned on\nthe factual data in TathyaNyaya, FactLegalLlama integrates predictive accuracy\nwith coherent, contextually relevant explanations, addressing the critical need\nfor transparency and interpretability in AI-assisted legal systems. Our\nmethodology combines transformers for binary judgment prediction with\nFactLegalLlama for explanation generation, creating a robust framework for\nadvancing FJPE in the Indian legal domain. TathyaNyaya not only surpasses\nexisting datasets in scale and diversity but also establishes a benchmark for\nbuilding explainable AI systems in legal analysis. The findings underscore the\nimportance of factual precision and domain-specific tuning in enhancing\npredictive performance and interpretability, positioning TathyaNyaya and\nFactLegalLlama as foundational resources for AI-assisted legal decision-making.",
      "tldr_zh": "该论文介绍了TathyaNyaya，一个专为印度法律背景下的事实性判决预测与解释(FJPE)而设计的大型标注数据集，包含来自印度最高法院和各高等法院的判决。同时，论文提出了FactLegalLlama，一个基于LLaMa-3-8B大型语言模型(LLM)指令微调的变体，专门用于生成高质量的FJPE解释。FactLegalLlama在TathyaNyaya数据集的事实数据上进行微调，将预测准确性与连贯、上下文相关的解释相结合，从而满足了AI辅助法律系统中对透明性和可解释性的关键需求。该研究结合了用于二元判决预测的transformers和用于解释生成的FactLegalLlama，为推进印度法律领域中的FJPE创建了一个强大的框架。\n",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.04737v1",
      "published_date": "2025-04-07 05:27:32 UTC",
      "updated_date": "2025-04-07 05:27:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:20:29.725982"
    },
    {
      "arxiv_id": "2504.04736v1",
      "title": "Synthetic Data Generation & Multi-Step RL for Reasoning & Tool Use",
      "title_zh": "合成数据生成与多步强化学习，用于推理与工具使用\n",
      "authors": [
        "Anna Goldie",
        "Azalia Mirhoseini",
        "Hao Zhou",
        "Irene Cai",
        "Christopher D. Manning"
      ],
      "abstract": "Reinforcement learning has been shown to improve the performance of large\nlanguage models. However, traditional approaches like RLHF or RLAIF treat the\nproblem as single-step. As focus shifts toward more complex reasoning and\nagentic tasks, language models must take multiple steps of text generation,\nreasoning and environment interaction before generating a solution. We propose\na synthetic data generation and RL methodology targeting multi-step\noptimization scenarios. This approach, called Step-Wise Reinforcement Learning\n(SWiRL), iteratively generates multi-step reasoning and tool use data, and then\nlearns from that data. It employs a simple step-wise decomposition that breaks\neach multi-step trajectory into multiple sub-trajectories corresponding to each\naction by the original model. It then applies synthetic data filtering and RL\noptimization on these sub-trajectories. We evaluated SWiRL on a number of\nmulti-step tool use, question answering, and mathematical reasoning tasks. Our\nexperiments show that SWiRL outperforms baseline approaches by 21.5%, 12.3%,\n14.8%, 11.1%, and 15.3% in relative accuracy on GSM8K, HotPotQA, CofCA,\nMuSiQue, and BeerQA, respectively. Excitingly, the approach exhibits\ngeneralization across tasks: for example, training only on HotPotQA (text\nquestion-answering) improves zero-shot performance on GSM8K (a math dataset) by\na relative 16.9%.",
      "tldr_zh": "该论文提出了一种名为Step-Wise Reinforcement Learning (SWiRL) 的合成数据生成和强化学习方法，用于优化大型语言模型在多步骤推理和工具使用场景中的表现。SWiRL通过迭代生成多步骤推理和工具使用数据，并将多步骤轨迹分解为对应于每个动作的子轨迹，然后在这些子轨迹上应用合成数据过滤和强化学习优化。实验结果表明，SWiRL在GSM8K、HotPotQA等多个任务上优于基线方法，并且展现出跨任务的泛化能力，例如在HotPotQA上训练可以提高在GSM8K上的零样本性能。\n",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.04736v1",
      "published_date": "2025-04-07 05:20:58 UTC",
      "updated_date": "2025-04-07 05:20:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:20:41.445445"
    },
    {
      "arxiv_id": "2504.04718v1",
      "title": "T1: Tool-integrated Self-verification for Test-time Compute Scaling in Small Language Models",
      "title_zh": "T1：小型语言模型中用于测试时计算扩展的工具集成式自验证\n",
      "authors": [
        "Minki Kang",
        "Jongwon Jeong",
        "Jaewoong Cho"
      ],
      "abstract": "Recent studies have demonstrated that test-time compute scaling effectively\nimproves the performance of small language models (sLMs). However, prior\nresearch has mainly examined test-time compute scaling with an additional\nlarger model as a verifier, leaving self-verification by sLMs underexplored. In\nthis work, we investigate whether sLMs can reliably self-verify their outputs\nunder test-time scaling. We find that even with knowledge distillation from\nlarger verifiers, sLMs struggle with verification tasks requiring memorization,\nsuch as numerical calculations and fact-checking. To address this limitation,\nwe propose Tool-integrated self-verification (T1), which delegates\nmemorization-heavy verification steps to external tools, such as a code\ninterpreter. Our theoretical analysis shows that tool integration reduces\nmemorization demands and improves test-time scaling performance. Experiments on\nthe MATH benchmark demonstrate that, with T1, a Llama-3.2 1B model under\ntest-time scaling outperforms the significantly larger Llama-3.1 8B model.\nMoreover, T1 generalizes effectively to both mathematical (MATH500) and\nmulti-domain knowledge-intensive tasks (MMLU-Pro). Our findings highlight the\npotential of tool integration to substantially improve the self-verification\nabilities of sLMs.",
      "tldr_zh": "本文研究了小语言模型(sLMs)在测试时计算扩展下的自验证能力，发现sLMs在需要记忆的任务（如数值计算和事实核查）中表现不佳。为了解决这个问题，作者提出了工具集成自验证(T1)方法，将需要大量记忆的验证步骤委托给外部工具，例如代码解释器。理论分析表明，工具集成降低了记忆需求，提高了测试时扩展性能。在MATH基准测试中，使用T1的Llama-3.2 1B模型在测试时扩展下的性能优于更大的Llama-3.1 8B模型。T1还能有效推广到数学(MATH500)和多领域知识密集型任务(MMLU-Pro)。研究结果表明，工具集成可以显著提高sLMs的自验证能力。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Preprint",
      "pdf_url": "http://arxiv.org/pdf/2504.04718v1",
      "published_date": "2025-04-07 04:01:17 UTC",
      "updated_date": "2025-04-07 04:01:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:20:53.548534"
    },
    {
      "arxiv_id": "2504.04717v2",
      "title": "Beyond Single-Turn: A Survey on Multi-Turn Interactions with Large Language Models",
      "title_zh": "超越单轮：大型语言模型多轮交互综述\n",
      "authors": [
        "Yubo Li",
        "Xiaobin Shen",
        "Xinyu Yao",
        "Xueying Ding",
        "Yidi Miao",
        "Ramayya Krishnan",
        "Rema Padman"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have revolutionized their\nability to handle single-turn tasks, yet real-world applications demand\nsophisticated multi-turn interactions. This survey provides a comprehensive\nreview of recent advancements in evaluating and enhancing multi-turn\ninteractions in LLMs. Focusing on task-specific scenarios, from instruction\nfollowing in diverse domains such as math and coding to complex conversational\nengagements in roleplay, healthcare, education, and even adversarial jailbreak\nsettings, we systematically examine the challenges of maintaining context,\ncoherence, fairness, and responsiveness over prolonged dialogues. The paper\norganizes current benchmarks and datasets into coherent categories that reflect\nthe evolving landscape of multi-turn dialogue evaluation. In addition, we\nreview a range of enhancement methodologies under multi-turn settings,\nincluding model-centric strategies (contextual learning, supervised\nfine-tuning, reinforcement learning, and new architectures), external\nintegration approaches (memory-augmented, retrieval-based methods, and\nknowledge graph), and agent-based techniques for collaborative interactions.\nFinally, we discuss open challenges and propose future directions for research\nto further advance the robustness and effectiveness of multi-turn interactions\nin LLMs. Related resources and papers are available at\nhttps://github.com/yubol-cmu/Awesome-Multi-Turn-LLMs.",
      "tldr_zh": "这篇综述全面回顾了大型语言模型(LLMs)在多轮交互方面的最新进展，着重关注任务特定场景，例如数学、编程、角色扮演、医疗、教育以及对抗性攻击等。论文系统地考察了在长时间对话中维持上下文、连贯性、公平性和响应性的挑战，并对现有的基准和数据集进行了分类。此外，论文还回顾了多轮环境下的各种增强方法，包括模型中心策略（上下文学习、监督微调、强化学习和新架构）、外部集成方法（记忆增强、基于检索的方法和知识图谱）以及基于代理的协同交互技术。最后，论文讨论了开放性挑战，并为未来研究提出了方向，旨在进一步提高LLMs中多轮交互的鲁棒性和有效性。相关资源和论文可在https://github.com/yubol-cmu/Awesome-Multi-Turn-LLMs 获取。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.04717v2",
      "published_date": "2025-04-07 04:00:08 UTC",
      "updated_date": "2025-04-08 03:58:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:21:05.768235"
    },
    {
      "arxiv_id": "2504.04711v1",
      "title": "Generalising from Self-Produced Data: Model Training Beyond Human Constraints",
      "title_zh": "从自生成数据中泛化：超越人类约束的模型训练\n",
      "authors": [
        "Alfath Daryl Alhajir",
        "Jennifer Dodgson",
        "Joseph Lim",
        "Truong Ma Phi",
        "Julian Peh",
        "Akira Rafhael Janson Pattirane",
        "Lokesh Poovaragan"
      ],
      "abstract": "Current large language models (LLMs) are constrained by human-derived\ntraining data and limited by a single level of abstraction that impedes\ndefinitive truth judgments. This paper introduces a novel framework in which AI\nmodels autonomously generate and validate new knowledge through direct\ninteraction with their environment. Central to this approach is an unbounded,\nungamable numeric reward - such as annexed disk space or follower count - that\nguides learning without requiring human benchmarks. AI agents iteratively\ngenerate strategies and executable code to maximize this metric, with\nsuccessful outcomes forming the basis for self-retraining and incremental\ngeneralisation. To mitigate model collapse and the warm start problem, the\nframework emphasizes empirical validation over textual similarity and supports\nfine-tuning via GRPO. The system architecture employs modular agents for\nenvironment analysis, strategy generation, and code synthesis, enabling\nscalable experimentation. This work outlines a pathway toward self-improving AI\nsystems capable of advancing beyond human-imposed constraints toward autonomous\ngeneral intelligence.",
      "tldr_zh": "本文提出了一种新框架，使AI模型能够通过与环境的直接交互自主生成和验证新知识，从而突破当前大型语言模型(LLMs)受限于人类训练数据和抽象层次的局限性。该框架使用一个无界、不可博弈的数值奖励（例如磁盘空间或关注者数量）来指导学习，无需人工基准。AI智能体迭代生成策略和可执行代码以最大化该指标，成功的成果构成自我训练和增量泛化的基础。为缓解模型崩溃和warm start问题，该框架强调经验验证而非文本相似性，并支持通过GRPO进行微调。系统架构采用模块化智能体进行环境分析、策略生成和代码合成，从而实现可扩展的实验。这项工作概述了一条通往自我改进AI系统的道路，该系统能够超越人类施加的约束，迈向自主通用智能。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "16 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.04711v1",
      "published_date": "2025-04-07 03:48:02 UTC",
      "updated_date": "2025-04-07 03:48:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:21:17.700228"
    },
    {
      "arxiv_id": "2504.04706v1",
      "title": "AdvKT: An Adversarial Multi-Step Training Framework for Knowledge Tracing",
      "title_zh": "AdvKT：一种用于知识追踪的对抗性多步训练框架\n",
      "authors": [
        "Lingyue Fu",
        "Ting Long",
        "Jianghao Lin",
        "Wei Xia",
        "Xinyi Dai",
        "Ruiming Tang",
        "Yasheng Wang",
        "Weinan Zhang",
        "Yong Yu"
      ],
      "abstract": "Knowledge Tracing (KT) monitors students' knowledge states and simulates\ntheir responses to question sequences. Existing KT models typically follow a\nsingle-step training paradigm, which leads to discrepancies with the multi-step\ninference process required in real-world simulations, resulting in significant\nerror accumulation. This accumulation of error, coupled with the issue of data\nsparsity, can substantially degrade the performance of recommendation models in\nthe intelligent tutoring systems. To address these challenges, we propose a\nnovel Adversarial Multi-Step Training Framework for Knowledge Tracing (AdvKT),\nwhich, for the first time, focuses on the multi-step KT task. More\nspecifically, AdvKT leverages adversarial learning paradigm involving a\ngenerator and a discriminator. The generator mimics high-reward responses,\neffectively reducing error accumulation across multiple steps, while the\ndiscriminator provides feedback to generate synthetic data. Additionally, we\ndesign specialized data augmentation techniques to enrich the training data\nwith realistic variations, ensuring that the model generalizes well even in\nscenarios with sparse data. Experiments conducted on four real-world datasets\ndemonstrate the superiority of AdvKT over existing KT models, showcasing its\nability to address both error accumulation and data sparsity issues\neffectively.",
      "tldr_zh": "该论文提出了一个对抗性的多步训练框架AdvKT，用于解决知识追踪(KT)中单步训练与实际多步推理之间的差异问题，以及由此产生的误差累积和数据稀疏性问题。AdvKT利用生成器和判别器的对抗学习范式，生成器模仿高奖励的回答以减少多步误差累积，判别器提供反馈以生成合成数据。此外，设计了专门的数据增强技术来丰富训练数据。在四个真实数据集上的实验表明，AdvKT优于现有的KT模型，有效解决了误差累积和数据稀疏性问题。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.04706v1",
      "published_date": "2025-04-07 03:31:57 UTC",
      "updated_date": "2025-04-07 03:31:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:21:29.527142"
    },
    {
      "arxiv_id": "2504.04704v1",
      "title": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are Important",
      "title_zh": "LagKV：KV缓存的时滞相关信息揭示了哪些token是重要的\n",
      "authors": [
        "Manlai Liang",
        "JiaMing Zhang",
        "Xiong Li",
        "Jinlong Li"
      ],
      "abstract": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modifiation of the inference infrastructure and\nsignificant computation overhead. Base on the fact that the Large Lanuage\nmodels are autoregresssive models, we propose {\\it LagKV}, a KV allocation\nstrategy only relying on straight forward comparison among KV themself. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on LongBench and PasskeyRetrieval show that, our\napproach achieves nearly zero loss when the ratio is $2\\times$ and $\\approx\n90\\%$ of the original model performance for $8\\times$. Especially in the\n64-digit passkey retrieval task, our mehod outperforms the attention weight\nbased method $H_2O$ over $60\\%$ with same compression ratios. Our code is\navailable at \\url{https://github.com/AI-Lab-China-Merchants-Bank/LagKV}.",
      "tldr_zh": "该论文提出了LagKV，一种新的KV缓存分配策略，旨在解决大语言模型长文本推理中KV缓存过大的问题。LagKV基于自回归模型的特性，通过比较KV自身的时间差信息来判断token的重要性，无需依赖注意力权重，易于集成到主流推理平台。实验结果表明，在LongBench和PasskeyRetrieval数据集上，LagKV在2倍压缩率下几乎无性能损失，8倍压缩率下仍能保持约90%的原始模型性能。特别是在64位密码检索任务中，LagKV的性能优于基于注意力权重的H2O方法超过60%（相同压缩率下）。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.04704v1",
      "published_date": "2025-04-07 03:22:15 UTC",
      "updated_date": "2025-04-07 03:22:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:21:41.632884"
    },
    {
      "arxiv_id": "2504.04702v1",
      "title": "Provable Failure of Language Models in Learning Majority Boolean Logic via Gradient Descent",
      "title_zh": "基于梯度下降的语言模型在学习多数布尔逻辑时可证明的失败案例\n",
      "authors": [
        "Bo Chen",
        "Zhenmei Shi",
        "Zhao Song",
        "Jiahao Zhang"
      ],
      "abstract": "Recent advancements in Transformer-based architectures have led to impressive\nbreakthroughs in natural language processing tasks, with models such as GPT-4,\nClaude, and Gemini demonstrating human-level reasoning abilities. However,\ndespite their high performance, concerns remain about the inherent limitations\nof these models, especially when it comes to learning basic logical functions.\nWhile complexity-theoretic analyses indicate that Transformers can represent\nsimple logic functions (e.g., $\\mathsf{AND}$, $\\mathsf{OR}$, and majority\ngates) by its nature of belonging to the $\\mathsf{TC}^0$ class, these results\nassume ideal parameter settings and do not account for the constraints imposed\nby gradient descent-based training methods. In this work, we investigate\nwhether Transformers can truly learn simple majority functions when trained\nusing gradient-based methods. We focus on a simplified variant of the\nTransformer architecture and consider both $n=\\mathrm{poly}(d)$ and\n$n=\\exp(\\Omega(d))$ number of training samples, where each sample is a $d$-size\nbinary string paired with the output of a basic majority function. Our analysis\ndemonstrates that even after $\\mathrm{poly}(d)$ gradient queries, the\ngeneralization error of the Transformer model still remains substantially\nlarge, growing exponentially with $d$. This work highlights fundamental\noptimization challenges in training Transformers for the simplest logical\nreasoning tasks and provides new insights into their theoretical limitations.",
      "tldr_zh": "该论文研究了基于梯度下降训练的Transformer模型在学习多数布尔逻辑函数时的失败问题。尽管Transformer在理论上可以表示简单的逻辑函数，但该研究表明，在实际的梯度下降训练中，即使使用大量的训练样本和梯度查询，Transformer模型的泛化误差仍然很大，并随输入维度呈指数增长。这项工作揭示了Transformer在学习最简单的逻辑推理任务时面临的根本优化挑战，并提供了对其理论局限性的新见解。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.04702v1",
      "published_date": "2025-04-07 03:08:12 UTC",
      "updated_date": "2025-04-07 03:08:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:21:53.340417"
    },
    {
      "arxiv_id": "2504.04699v1",
      "title": "R2Vul: Learning to Reason about Software Vulnerabilities with Reinforcement Learning and Structured Reasoning Distillation",
      "title_zh": "R2Vul：通过强化学习和结构化推理提炼学习软件漏洞推理\n",
      "authors": [
        "Martin Weyssow",
        "Chengran Yang",
        "Junkai Chen",
        "Yikun Li",
        "Huihui Huang",
        "Ratnadira Widyasari",
        "Han Wei Ang",
        "Frank Liauw",
        "Eng Lieh Ouh",
        "Lwin Khin Shar",
        "David Lo"
      ],
      "abstract": "Large language models (LLMs) have shown promising performance in software\nvulnerability detection (SVD), yet their reasoning capabilities remain\nunreliable. Existing approaches relying on chain-of-thought (CoT) struggle to\nprovide relevant and actionable security assessments. Additionally, effective\nSVD requires not only generating coherent reasoning but also differentiating\nbetween well-founded and misleading yet plausible security assessments, an\naspect overlooked in prior work. To this end, we introduce R2Vul, a novel\napproach that distills structured reasoning into small LLMs using reinforcement\nlearning from AI feedback (RLAIF). Through RLAIF, R2Vul enables LLMs to produce\nstructured, security-aware reasoning that is actionable and reliable while\nexplicitly learning to distinguish valid assessments from misleading ones. We\nevaluate R2Vul across five languages against SAST tools, CoT, instruction\ntuning, and classification-based baselines. Our results show that R2Vul with\nstructured reasoning distillation enables a 1.5B student LLM to rival larger\nmodels while improving generalization to out-of-distribution vulnerabilities.\nBeyond model improvements, we contribute a large-scale, multilingual preference\ndataset featuring structured reasoning to support future research in SVD.",
      "tldr_zh": "该论文提出了R2Vul，一种利用强化学习和结构化推理蒸馏的方法，用于提升软件漏洞检测(SVD)的性能。R2Vul通过AI反馈的强化学习(RLAIF)，使小型LLM能够生成结构化的、安全感知的推理，从而区分有效和误导性的安全评估。该方法将结构化推理提炼到小型LLM中，使其能够提供更相关和可操作的安全评估。实验结果表明，R2Vul使用15亿参数的学生LLM，在五种语言的漏洞检测中，可以与更大的模型相媲美，并提高了对分布外漏洞的泛化能力。此外，论文还贡献了一个大规模、多语言的偏好数据集，用于支持SVD的未来研究。\n",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.04699v1",
      "published_date": "2025-04-07 03:04:16 UTC",
      "updated_date": "2025-04-07 03:04:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:22:05.666548"
    },
    {
      "arxiv_id": "2504.04687v1",
      "title": "Bridging Knowledge Gap Between Image Inpainting and Large-Area Visible Watermark Removal",
      "title_zh": "弥合图像修复与大面积可见水印移除之间的知识鸿沟\n",
      "authors": [
        "Yicheng Leng",
        "Chaowei Fang",
        "Junye Chen",
        "Yixiang Fang",
        "Sheng Li",
        "Guanbin Li"
      ],
      "abstract": "Visible watermark removal which involves watermark cleaning and background\ncontent restoration is pivotal to evaluate the resilience of watermarks.\nExisting deep neural network (DNN)-based models still struggle with large-area\nwatermarks and are overly dependent on the quality of watermark mask\nprediction. To overcome these challenges, we introduce a novel feature adapting\nframework that leverages the representation modeling capacity of a pre-trained\nimage inpainting model. Our approach bridges the knowledge gap between image\ninpainting and watermark removal by fusing information of the residual\nbackground content beneath watermarks into the inpainting backbone model. We\nestablish a dual-branch system to capture and embed features from the residual\nbackground content, which are merged into intermediate features of the\ninpainting backbone model via gated feature fusion modules. Moreover, for\nrelieving the dependence on high-quality watermark masks, we introduce a new\ntraining paradigm by utilizing coarse watermark masks to guide the inference\nprocess. This contributes to a visible image removal model which is insensitive\nto the quality of watermark mask during testing. Extensive experiments on both\na large-scale synthesized dataset and a real-world dataset demonstrate that our\napproach significantly outperforms existing state-of-the-art methods. The\nsource code is available in the supplementary materials.",
      "tldr_zh": "该论文提出了一种新的特征自适应框架，旨在弥合图像修复和去除大面积可见水印之间的知识差距。该方法利用预训练图像修复模型的表征建模能力，将水印下方残留背景内容的信息融入到修复主干模型中。通过建立双分支系统捕获和嵌入残留背景内容的特征，并通过门控特征融合模块将其合并到修复主干模型的中间特征中。此外，为了减轻对高质量水印掩模的依赖，引入了一种新的训练范式，利用粗略的水印掩模来指导推理过程。实验表明，该方法在合成数据集和真实数据集上均显著优于现有技术水平的方法。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM",
        "eess.IV",
        "I.2.10; I.4.4; I.4.5"
      ],
      "primary_category": "cs.CV",
      "comment": "To be published in AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.04687v1",
      "published_date": "2025-04-07 02:37:14 UTC",
      "updated_date": "2025-04-07 02:37:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:22:17.648254"
    },
    {
      "arxiv_id": "2504.04676v1",
      "title": "Dual Consistent Constraint via Disentangled Consistency and Complementarity for Multi-view Clustering",
      "title_zh": "基于解耦一致性和互补性的双重一致性约束多视图聚类方法\n",
      "authors": [
        "Bo Li",
        "Jing Yun"
      ],
      "abstract": "Multi-view clustering can explore common semantics from multiple views and\nhas received increasing attention in recent years. However, current methods\nfocus on learning consistency in representation, neglecting the contribution of\neach view's complementarity aspect in representation learning. This limit poses\na significant challenge in multi-view representation learning. This paper\nproposes a novel multi-view clustering framework that introduces a disentangled\nvariational autoencoder that separates multi-view into shared and private\ninformation, i.e., consistency and complementarity information. We first learn\ninformative and consistent representations by maximizing mutual information\nacross different views through contrastive learning. This process will ignore\ncomplementary information. Then, we employ consistency inference constraints to\nexplicitly utilize complementary information when attempting to seek the\nconsistency of shared information across all views. Specifically, we perform a\nwithin-reconstruction using the private and shared information of each view and\na cross-reconstruction using the shared information of all views. The dual\nconsistency constraints are not only effective in improving the representation\nquality of data but also easy to extend to other scenarios, especially in\ncomplex multi-view scenes. This could be the first attempt to employ dual\nconsistent constraint in a unified MVC theoretical framework. During the\ntraining procedure, the consistency and complementarity features are jointly\noptimized. Extensive experiments show that our method outperforms baseline\nmethods.",
      "tldr_zh": "这篇论文提出了一种新的多视图聚类框架，通过解耦变分自编码器将多视图信息分离为共享（一致性）和私有（互补性）信息。该框架首先通过对比学习最大化不同视图之间的互信息，学习信息丰富且一致的表示，忽略互补信息。然后，利用一致性推断约束，在寻求跨视图共享信息一致性的同时，显式地利用互补信息。具体来说，使用每个视图的私有和共享信息进行内部重构，并使用所有视图的共享信息进行交叉重构。这种双重一致性约束能够有效提高数据表示质量，并易于扩展到其他场景，尤其是在复杂的多视图场景中。实验结果表明，该方法优于现有基线方法。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.04676v1",
      "published_date": "2025-04-07 02:00:16 UTC",
      "updated_date": "2025-04-07 02:00:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:22:29.799663"
    },
    {
      "arxiv_id": "2504.04675v2",
      "title": "HypRL: Reinforcement Learning of Control Policies for Hyperproperties",
      "title_zh": "HypRL：超性质控制策略的强化学习\n",
      "authors": [
        "Tzu-Han Hsu",
        "Arshia Rafieioskouei",
        "Borzoo Bonakdarpour"
      ],
      "abstract": "We study the problem of learning control policies for complex tasks whose\nrequirements are given by a hyperproperty. The use of hyperproperties is\nmotivated by their significant power to formally specify requirements of\nmulti-agent systems as well as those that need expressiveness in terms of\nmultiple execution traces (e.g., privacy and fairness). Given a Markov decision\nprocess M with unknown transitions (representing the environment) and a\nHyperLTL formula $\\varphi$, our approach first employs Skolemization to handle\nquantifier alternations in $\\varphi$. We introduce quantitative robustness\nfunctions for HyperLTL to define rewards of finite traces of M with respect to\n$\\varphi$. Finally, we utilize a suitable reinforcement learning algorithm to\nlearn (1) a policy per trace quantifier in $\\varphi$, and (2) the probability\ndistribution of transitions of M that together maximize the expected reward\nand, hence, probability of satisfaction of $\\varphi$ in M. We present a set of\ncase studies on (1) safety-preserving multi-agent path planning, (2) fairness\nin resource allocation, and (3) the post-correspondence problem (PCP).",
      "tldr_zh": "该论文提出了一种名为HypRL的强化学习方法，用于学习满足超性质(hyperproperty)的控制策略。针对环境为未知转移的马尔可夫决策过程(MDP)和给定的HyperLTL公式，该方法首先采用Skolemization处理公式中的量词交替。然后，引入HyperLTL的定量鲁棒性函数来定义有限轨迹的奖励。最后，利用强化学习算法学习每个轨迹量词对应的策略以及MDP的状态转移概率分布，以最大化期望奖励，从而提高满足HyperLTL公式的概率。该方法在多智能体路径规划的安全性保持、资源分配的公平性以及后对应问题(PCP)等案例研究中进行了验证。\n",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.04675v2",
      "published_date": "2025-04-07 01:58:36 UTC",
      "updated_date": "2025-04-08 04:19:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:22:41.686941"
    },
    {
      "arxiv_id": "2504.04654v1",
      "title": "EquiCPI: SE(3)-Equivariant Geometric Deep Learning for Structure-Aware Prediction of Compound-Protein Interactions",
      "title_zh": "EquiCPI：用于化合物-蛋白质相互作用结构感知预测的 SE(3)-等变几何深度学习\n",
      "authors": [
        "Ngoc-Quang Nguyen"
      ],
      "abstract": "Accurate prediction of compound-protein interactions (CPI) remains a\ncornerstone challenge in computational drug discovery. While existing\nsequence-based approaches leverage molecular fingerprints or graph\nrepresentations, they critically overlook three-dimensional (3D) structural\ndeterminants of binding affinity. To bridge this gap, we present EquiCPI, an\nend-to-end geometric deep learning framework that synergizes first-principles\nstructural modeling with SE(3)-equivariant neural networks. Our pipeline\ntransforms raw sequences into 3D atomic coordinates via ESMFold for proteins\nand DiffDock-L for ligands, followed by physics-guided conformer re-ranking and\nequivariant feature learning. At its core, EquiCPI employs SE(3)-equivariant\nmessage passing over atomic point clouds, preserving symmetry under rotations,\ntranslations, and reflections, while hierarchically encoding local interaction\npatterns through tensor products of spherical harmonics. The proposed model is\nevaluated on BindingDB (affinity prediction) and DUD-E (virtual screening),\nEquiCPI achieves performance on par with or exceeding the state-of-the-art deep\nlearning competitors.",
      "tldr_zh": "EquiCPI是一个端到端的几何深度学习框架，旨在更准确地预测化合物-蛋白质相互作用(CPI)。它结合了基于序列的结构建模和SE(3)-等变神经网络，利用ESMFold预测蛋白质结构，DiffDock-L预测配体结构，并通过物理引导的构象重排序和等变特征学习来优化。EquiCPI的核心是SE(3)-等变消息传递，它在原子点云上保持旋转、平移和反射对称性，并通过球谐函数的张量积分层编码局部相互作用模式。在BindingDB和DUD-E数据集上的评估表明，EquiCPI的性能与最先进的深度学习模型相当或更好。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.04654v1",
      "published_date": "2025-04-07 00:57:08 UTC",
      "updated_date": "2025-04-07 00:57:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-09T02:22:53.799447"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 95,
  "processed_papers_count": 95,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-04-09T02:24:34.652263"
}
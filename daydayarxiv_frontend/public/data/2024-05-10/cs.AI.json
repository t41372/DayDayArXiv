{
  "date": "2024-05-10",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-05-10 的 arXiv 中文 TLDR 快报！今天 arXiv 的论文主要聚焦 AI 模型的安全性、效率优化和跨领域应用，突出 LLM 在图学习、视觉任务和联邦学习中的创新进展，令人印象深刻的是 Yoshua Bengio 等著名学者参与的 AI 安全框架，以及高效视觉 GNN 和 LLM 对齐方法。\n\n### 重点论文讨论\n我将优先讨论重要、话题度高或有著名学者的论文（如 AI 安全和 LLM 优化），并将相关主题归类快速概述。其他论文将简要列出标题和核心贡献，以控制篇幅。\n\n#### AI 安全与 LLM 优化\n- **论文 19: Towards Guaranteed Safe AI: A Framework for Ensuring Robust and Reliable AI Systems**（中文：朝向可保证安全的 AI：确保 AI 系统鲁棒性和可靠性的框架；英文：Towards Guaranteed Safe AI）  \n  这篇论文由 Yoshua Bengio 和 Stuart Russell 等知名学者参与，提出一个“Guaranteed Safe AI”框架，通过世界模型（world model）、安全规范（safety specification）和验证器（verifier）来提供高保障的定量安全保证。主要贡献是解决 AI 在安全关键场景下的潜在风险，强调了数学证明和实际应用的结合，为 AI 安全领域设定了新标准。\n\n- **论文 6: PLeak: Prompt Leaking Attacks against Large Language Model Applications**（中文：PLeak：针对大型语言模型应用的提示泄露攻击；英文：PLeak）  \n  这篇论文引入 PLeak 框架，通过优化对抗查询来窃取 LLM 的系统提示，显著提高了攻击效率。核心发现是现有 LLM 应用易受闭盒攻击，作者在真实平台如 Poe 上验证了其有效性，并呼吁开发检测和防御技术，为 LLM 安全研究提供了实用工具。\n\n- **论文 15: Value Augmented Sampling for Language Model Alignment and Personalization**（中文：基于值增强采样的语言模型对齐和个性化；英文：Value Augmented Sampling）  \n  论文提出 VAS 框架，使用奖励优化而不需更新 LLM 权重，实现高效的 LLM 对齐和个性化。关键贡献是通过代理采样最大化奖励函数，实验显示其在基准任务上优于 PPO 和 DPO，并支持 API 级别的适应，推进了个性化 LLM 的发展。\n\n#### 视觉和图神经网络\n- **论文 2: GreedyViG: Dynamic Axial Graph Construction for Efficient Vision GNNs**（中文：GreedyViG：用于高效视觉 GNN 的动态轴向图构建；英文：GreedyViG）  \n  这篇 CVPR 接受的论文提出 GreedyViG 模型，使用动态轴向图构建（DAGC）取代传统的 k-NN 操作，提高了视觉 GNN 的效率和准确性。主要发现是 GreedyViG 在图像分类和分割任务上超越现有 CNN 和 ViT 模型，实现了高达 83.9% 的 ImageNet 准确率，同时减少了参数和计算量。\n\n- **论文 24: A Survey of Large Language Models for Graphs**（中文：图上大型语言模型的调查；英文：A Survey of Large Language Models for Graphs）  \n  作为 KDD 调查论文，作者归纳了 LLM 在图学习中的四类框架（如 GNNs as Prefix），并讨论了挑战和未来方向。核心贡献是系统梳理了至少 15 种现有方法，提供了开源资源，促进了 LLM 和 GNN 的融合研究。\n\n#### 联邦学习与分布式系统\n- **论文 7: MH-pFLID: Model Heterogeneous personalized Federated Learning via Injection and Distillation for Medical Data Analysis**（中文：MH-pFLID：通过注入和蒸馏实现模型异构的个性化联邦学习，用于医疗数据分析；英文：MH-pFLID）  \n  这篇 ICML 2024 接受的论文提出 MH-pFLID 框架，使用轻量级信使模型处理非 IID 数据，实现医疗联邦学习的个性化。主要发现是它解决了系统异构问题，提高了隐私保护和数据聚合效率，适用于资源有限的医疗场景。\n\n其他论文中，有一些主题如交通预测、文本生成和强化学习也有亮点，但非核心或话题度较低，我将快速掠过，只列出标题和简要贡献：\n\n- **论文 4: Dominion: A New Frontier for AI Research**（中文：Dominion：AI 研究的新前沿；英文：Dominion）  \n  探索强化学习在桌面游戏中的应用，贡献了一个数据集和基线模型，提升了 RL 在复杂游戏中的性能。\n\n- **论文 14: A Survey of Large Language Models for Graphs**（等同于上述，已讨论）  \n  （跳过重复）\n\n- **论文 31: Time Evidence Fusion Network: Multi-source View in Long-Term Time Series Forecasting**（中文：时间证据融合网络：长期时间序列预测的多源视角；英文：Time Evidence Fusion Network）  \n  提出 TEFN 框架，使用证据理论融合多源数据，提高时间序列预测的准确性和鲁棒性。\n\n- **论文 39: Intelligent Duty Cycling Management and Wake-up for Energy Harvesting IoT Networks with Correlated Activity**（中文：智能占空比管理和唤醒，用于相关活动的能量收集 IoT 网络；英文：Intelligent Duty Cycling Management）  \n  设计基于 K-NN 的 IoT 能量管理算法，减少能耗并提高检测准确性。\n\n剩余论文（如第 3、5、8、9 等）涉及符号回归、MLOps 和医疗应用，但由于篇幅和重要性，我仅简要提及它们的核心：这些工作探索了 LLM 在特定领域的优化，如逆问题求解和报告总结，但未带来重大突破，故不展开讨论。\n\n今天的 arXiv 快报展示 AI 领域的快速迭代，强调了模型安全和效率的紧迫性。如果你对某个主题感兴趣，建议查看这些论文的原文！",
  "papers": [
    {
      "arxiv_id": "2405.12999v1",
      "title": "An Assessment of Model-On-Model Deception",
      "title_zh": "翻译失败",
      "authors": [
        "Julius Heitkoetter",
        "Michael Gerovitch",
        "Laker Newhouse"
      ],
      "abstract": "The trustworthiness of highly capable language models is put at risk when\nthey are able to produce deceptive outputs. Moreover, when models are\nvulnerable to deception it undermines reliability. In this paper, we introduce\na method to investigate complex, model-on-model deceptive scenarios. We create\na dataset of over 10,000 misleading explanations by asking Llama-2 7B, 13B,\n70B, and GPT-3.5 to justify the wrong answer for questions in the MMLU. We find\nthat, when models read these explanations, they are all significantly deceived.\nWorryingly, models of all capabilities are successful at misleading others,\nwhile more capable models are only slightly better at resisting deception. We\nrecommend the development of techniques to detect and defend against deception.",
      "tldr_zh": "本研究评估了模型间欺骗（Model-On-Model Deception）的风险，引入了一种方法来调查语言模型在产生和响应欺骗性输出时的行为。研究者创建了一个超过10,000个误导性解释的数据集，通过让Llama-2 7B、13B、70B和GPT-3.5模型为MMLU问题中的错误答案提供理由，结果显示这些模型在读取这些解释时均被显著欺骗。令人担忧的是，所有能力的模型都能有效误导他人，而更先进的模型在抵抗欺骗方面仅略有优势，因此建议开发检测和防御欺骗的技术。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at Secure and Trustworthy Large Language Models Workshop at\n  ICLR 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.12999v1",
      "published_date": "2024-05-10 23:24:18 UTC",
      "updated_date": "2024-05-10 23:24:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:28:38.972186"
    },
    {
      "arxiv_id": "2405.06849v1",
      "title": "GreedyViG: Dynamic Axial Graph Construction for Efficient Vision GNNs",
      "title_zh": "翻译失败",
      "authors": [
        "Mustafa Munir",
        "William Avery",
        "Md Mostafijur Rahman",
        "Radu Marculescu"
      ],
      "abstract": "Vision graph neural networks (ViG) offer a new avenue for exploration in\ncomputer vision. A major bottleneck in ViGs is the inefficient k-nearest\nneighbor (KNN) operation used for graph construction. To solve this issue, we\npropose a new method for designing ViGs, Dynamic Axial Graph Construction\n(DAGC), which is more efficient than KNN as it limits the number of considered\ngraph connections made within an image. Additionally, we propose a novel\nCNN-GNN architecture, GreedyViG, which uses DAGC. Extensive experiments show\nthat GreedyViG beats existing ViG, CNN, and ViT architectures in terms of\naccuracy, GMACs, and parameters on image classification, object detection,\ninstance segmentation, and semantic segmentation tasks. Our smallest model,\nGreedyViG-S, achieves 81.1% top-1 accuracy on ImageNet-1K, 2.9% higher than\nVision GNN and 2.2% higher than Vision HyperGraph Neural Network (ViHGNN), with\nless GMACs and a similar number of parameters. Our largest model, GreedyViG-B\nobtains 83.9% top-1 accuracy, 0.2% higher than Vision GNN, with a 66.6%\ndecrease in parameters and a 69% decrease in GMACs. GreedyViG-B also obtains\nthe same accuracy as ViHGNN with a 67.3% decrease in parameters and a 71.3%\ndecrease in GMACs. Our work shows that hybrid CNN-GNN architectures not only\nprovide a new avenue for designing efficient models, but that they can also\nexceed the performance of current state-of-the-art models.",
      "tldr_zh": "本文提出 Dynamic Axial Graph Construction (DAGC) 方法，以解决 Vision GNNs 中 k-nearest neighbor (KNN) 操作的效率瓶颈，通过限制图像内图连接数量来提升构建效率。基于此，作者开发了新型 CNN-GNN 架构 GreedyViG，该架构在图像分类、物体检测、实例分割和语义分割任务上表现出色。实验结果显示，GreedyViG-S 在 ImageNet-1K 上达到 81.1% top-1 准确率，比 Vision GNN 高 2.9% 且 GMACs 更低，而 GreedyViG-B 则以 83.9% 准确率超越 Vision GNN，并显著减少参数和 GMACs。该工作证明了混合 CNN-GNN 架构不仅更高效，还能超越当前最先进模型的性能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Proceedings of the 2024 IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition (CVPR)",
      "pdf_url": "http://arxiv.org/pdf/2405.06849v1",
      "published_date": "2024-05-10 23:21:16 UTC",
      "updated_date": "2024-05-10 23:21:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:28:55.072715"
    },
    {
      "arxiv_id": "2405.06848v1",
      "title": "ISR: Invertible Symbolic Regression",
      "title_zh": "ISR：可逆符号回归",
      "authors": [
        "Tony Tohme",
        "Mohammad Javad Khojasteh",
        "Mohsen Sadr",
        "Florian Meyer",
        "Kamal Youcef-Toumi"
      ],
      "abstract": "We introduce an Invertible Symbolic Regression (ISR) method. It is a machine\nlearning technique that generates analytical relationships between inputs and\noutputs of a given dataset via invertible maps (or architectures). The proposed\nISR method naturally combines the principles of Invertible Neural Networks\n(INNs) and Equation Learner (EQL), a neural network-based symbolic architecture\nfor function learning. In particular, we transform the affine coupling blocks\nof INNs into a symbolic framework, resulting in an end-to-end differentiable\nsymbolic invertible architecture that allows for efficient gradient-based\nlearning. The proposed ISR framework also relies on sparsity promoting\nregularization, allowing the discovery of concise and interpretable invertible\nexpressions. We show that ISR can serve as a (symbolic) normalizing flow for\ndensity estimation tasks. Furthermore, we highlight its practical applicability\nin solving inverse problems, including a benchmark inverse kinematics problem,\nand notably, a geoacoustic inversion problem in oceanography aimed at inferring\nposterior distributions of underlying seabed parameters from acoustic signals.",
      "tldr_zh": "我们介绍了 Invertible Symbolic Regression (ISR)，一种机器学习方法，通过可逆映射生成数据集输入和输出之间的分析关系，并结合 Invertible Neural Networks (INNs) 和 Equation Learner (EQL) 的原理。ISR 将 INNs 的仿射耦合块转化为端到端可微的符号框架，并使用稀疏正则化来发现简洁可解释的表达式。该方法可作为符号归一化流进行密度估计，并成功应用于逆问题，如逆运动学和海洋声学反演中。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT",
        "math.IT",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06848v1",
      "published_date": "2024-05-10 23:20:46 UTC",
      "updated_date": "2024-05-10 23:20:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:29:03.913081"
    },
    {
      "arxiv_id": "2405.06846v1",
      "title": "Dominion: A New Frontier for AI Research",
      "title_zh": "Dominion：人工智能研究的新前沿",
      "authors": [
        "Danny Halawi",
        "Aron Sarmasi",
        "Siena Saltzen",
        "Joshua McCoy"
      ],
      "abstract": "In recent years, machine learning approaches have made dramatic advances,\nreaching superhuman performance in Go, Atari, and poker variants. These games,\nand others before them, have served not only as a testbed but have also helped\nto push the boundaries of AI research. Continuing this tradition, we examine\nthe tabletop game Dominion and discuss the properties that make it well-suited\nto serve as a benchmark for the next generation of reinforcement learning (RL)\nalgorithms. We also present the Dominion Online Dataset, a collection of over\n2,000,000 games of Dominion played by experienced players on the Dominion\nOnline webserver. Finally, we introduce an RL baseline bot that uses existing\ntechniques to beat common heuristic-based bots, and shows competitive\nperformance against the previously strongest bot, Provincial.",
      "tldr_zh": "这篇论文将桌游 Dominion 视为强化学习 (RL) 算法的新基准，延续了 AI 在游戏领域（如围棋、Atari 和扑克）的进展传统，并讨论了 Dominion 的特性，如策略深度和决策复杂性，使其适合测试下一代 RL 模型。作者发布了 Dominion Online Dataset，这是一个包含超过 2,000,000 场由经验玩家进行的游戏数据集合，用于支持 AI 研究。最后，论文引入了一个基于现有 RL 技术的基准机器人，该机器人击败了常见启发式机器人，并在性能上与最强的 Provincial 机器人竞争。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06846v1",
      "published_date": "2024-05-10 23:03:02 UTC",
      "updated_date": "2024-05-10 23:03:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:29:16.182950"
    },
    {
      "arxiv_id": "2405.06835v1",
      "title": "Automating Code Adaptation for MLOps -- A Benchmarking Study on LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Harsh Patel",
        "Buvaneswari A. Ramanan",
        "Manzoor A. Khan",
        "Thomas Williams",
        "Brian Friedman",
        "Lawrence Drabeck"
      ],
      "abstract": "This paper explores the possibilities of the current generation of Large\nLanguage Models for incorporating Machine Learning Operations (MLOps)\nfunctionalities into ML training code bases. We evaluate the performance of\nOpenAI (gpt-3.5-turbo) and WizardCoder (open-source, 15B parameters) models on\nthe automated accomplishment of various MLOps functionalities in different\nsettings. We perform a benchmarking study that assesses the ability of these\nmodels to: (1) adapt existing code samples (Inlining) with component-specific\nMLOps functionality such as MLflow and Weights & Biases for experiment\ntracking, Optuna for hyperparameter optimization etc., and (2) perform the task\nof Translation from one component of an MLOps functionality to another, e.g.,\ntranslating existing GitPython library based version control code to Data\nVersion Control library based. We also propose three different approaches that\ninvolve teaching LLMs to comprehend the API documentation of the components as\na reference while accomplishing the Translation tasks. In our evaluations, the\ngpt-3.5-turbo model significantly outperforms WizardCoder by achieving\nimpressive Pass@3 accuracy in model optimization (55% compared to 0% by\nWizardCoder), experiment tracking (100%, compared to 62.5% by WizardCoder),\nmodel registration (92% compared to 42% by WizardCoder) and hyperparameter\noptimization (83% compared to 58% by WizardCoder) on average, in their best\npossible settings, showcasing its superior code adaptability performance in\ncomplex MLOps tasks.",
      "tldr_zh": "本研究评估了大型语言模型（LLMs）在自动化代码适应以整合机器学习操作（MLOps）功能方面的潜力，焦点放在 OpenAI 的 gpt-3.5-turbo 和开源 WizardCoder（15B 参数）模型上。研究通过基准测试考察了这些模型在代码内联（Inlining）任务（如添加 MLflow 用于实验跟踪、Optuna 用于超参数优化）和代码转换（Translation）任务（如从 GitPython 到 Data Version Control）中的性能，并提出了三种方法，让 LLMs 通过理解 API 文档来辅助翻译。结果显示，gpt-3.5-turbo 在模型优化（55% Pass@3 准确率 vs. WizardCoder 的 0%）、实验跟踪（100% vs. 62.5%）、模型注册（92% vs. 42%）和超参数优化（83% vs. 58%）等方面显著优于 WizardCoder，证明了其在复杂 MLOps 任务中的出色适应能力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.LG",
      "comment": "The work was completed during 2Q, 3Q of Year 2023, when WizardCoder\n  was the top performing Open source LLM for coding. Newer and better models\n  have emerged since then. The processes and methodologies utilized for this\n  benchmarking can still be utilized for evaluating the current SoTA models",
      "pdf_url": "http://arxiv.org/pdf/2405.06835v1",
      "published_date": "2024-05-10 22:18:43 UTC",
      "updated_date": "2024-05-10 22:18:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:29:29.816288"
    },
    {
      "arxiv_id": "2405.06823v2",
      "title": "PLeak: Prompt Leaking Attacks against Large Language Model Applications",
      "title_zh": "PLeak：针对大型语言模型应用的提示泄露攻击",
      "authors": [
        "Bo Hui",
        "Haolin Yuan",
        "Neil Gong",
        "Philippe Burlina",
        "Yinzhi Cao"
      ],
      "abstract": "Large Language Models (LLMs) enable a new ecosystem with many downstream\napplications, called LLM applications, with different natural language\nprocessing tasks. The functionality and performance of an LLM application\nhighly depend on its system prompt, which instructs the backend LLM on what\ntask to perform. Therefore, an LLM application developer often keeps a system\nprompt confidential to protect its intellectual property. As a result, a\nnatural attack, called prompt leaking, is to steal the system prompt from an\nLLM application, which compromises the developer's intellectual property.\nExisting prompt leaking attacks primarily rely on manually crafted queries, and\nthus achieve limited effectiveness.\n  In this paper, we design a novel, closed-box prompt leaking attack framework,\ncalled PLeak, to optimize an adversarial query such that when the attacker\nsends it to a target LLM application, its response reveals its own system\nprompt. We formulate finding such an adversarial query as an optimization\nproblem and solve it with a gradient-based method approximately. Our key idea\nis to break down the optimization goal by optimizing adversary queries for\nsystem prompts incrementally, i.e., starting from the first few tokens of each\nsystem prompt step by step until the entire length of the system prompt.\n  We evaluate PLeak in both offline settings and for real-world LLM\napplications, e.g., those on Poe, a popular platform hosting such applications.\nOur results show that PLeak can effectively leak system prompts and\nsignificantly outperforms not only baselines that manually curate queries but\nalso baselines with optimized queries that are modified and adapted from\nexisting jailbreaking attacks. We responsibly reported the issues to Poe and\nare still waiting for their response. Our implementation is available at this\nrepository: https://github.com/BHui97/PLeak.",
      "tldr_zh": "本文提出 PLeak，一种针对 Large Language Models (LLMs) 应用的提示泄露攻击框架，旨在通过优化对抗查询 (adversarial query) 来自动提取保密的系统 prompt (system prompt)，以解决现有手动查询方法的低效问题。PLeak 将攻击过程建模为优化问题，使用基于梯度的逐步方法，从系统 prompt 的首个 token 开始逐步揭示完整内容。实验结果显示，PLeak 在离线和真实平台（如 Poe）上显著优于基线攻击，泄露准确率更高，并为 LLM 应用的安全性提供了重要启示。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "To appear in the Proceedings of The ACM Conference on Computer and\n  Communications Security (CCS), 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.06823v2",
      "published_date": "2024-05-10 21:52:34 UTC",
      "updated_date": "2024-05-14 15:03:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:29:39.903415"
    },
    {
      "arxiv_id": "2405.06822v1",
      "title": "MH-pFLID: Model Heterogeneous personalized Federated Learning via Injection and Distillation for Medical Data Analysis",
      "title_zh": "MH-pFLID：通过注入和蒸馏实现模型异构个性化联邦学习，用于医疗数据分析",
      "authors": [
        "Luyuan Xie",
        "Manqing Lin",
        "Tianyu Luan",
        "Cong Li",
        "Yuejian Fang",
        "Qingni Shen",
        "Zhonghai Wu"
      ],
      "abstract": "Federated learning is widely used in medical applications for training global\nmodels without needing local data access. However, varying computational\ncapabilities and network architectures (system heterogeneity), across clients\npose significant challenges in effectively aggregating information from\nnon-independently and identically distributed (non-IID) data. Current federated\nlearning methods using knowledge distillation require public datasets, raising\nprivacy and data collection issues. Additionally, these datasets require\nadditional local computing and storage resources, which is a burden for medical\ninstitutions with limited hardware conditions. In this paper, we introduce a\nnovel federated learning paradigm, named Model Heterogeneous personalized\nFederated Learning via Injection and Distillation (MH-pFLID). Our framework\nleverages a lightweight messenger model that carries concentrated information\nto collect the information from each client. We also develop a set of receiver\nand transmitter modules to receive and send information from the messenger\nmodel, so that the information could be injected and distilled with efficiency.",
      "tldr_zh": "该论文针对联邦学习(Federated Learning)在医疗数据分析中的系统异构和非IID数据挑战，提出了一种新型框架MH-pFLID，以解决现有方法依赖公共数据集导致的隐私和资源负担问题。该框架利用一个轻量级的信使模型来收集客户端信息，并开发接收器和发射器模块，实现高效的信息注入和蒸馏，从而支持模型异构的个性化训练。实验结果表明，MH-pFLID提升了信息聚合效率，并为资源有限的医疗机构提供了更可行的隐私保护方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "This paper is accepted by ICML 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.06822v1",
      "published_date": "2024-05-10 21:52:27 UTC",
      "updated_date": "2024-05-10 21:52:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:29:52.559045"
    },
    {
      "arxiv_id": "2405.06808v2",
      "title": "Large Language Model in Financial Regulatory Interpretation",
      "title_zh": "翻译失败",
      "authors": [
        "Zhiyu Cao",
        "Zachary Feinstein"
      ],
      "abstract": "This study explores the innovative use of Large Language Models (LLMs) as\nanalytical tools for interpreting complex financial regulations. The primary\nobjective is to design effective prompts that guide LLMs in distilling verbose\nand intricate regulatory texts, such as the Basel III capital requirement\nregulations, into a concise mathematical framework that can be subsequently\ntranslated into actionable code. This novel approach aims to streamline the\nimplementation of regulatory mandates within the financial reporting and risk\nmanagement systems of global banking institutions. A case study was conducted\nto assess the performance of various LLMs, demonstrating that GPT-4 outperforms\nother models in processing and collecting necessary information, as well as\nexecuting mathematical calculations. The case study utilized numerical\nsimulations with asset holdings -- including fixed income, equities, currency\npairs, and commodities -- to demonstrate how LLMs can effectively implement the\nBasel III capital adequacy requirements.\n  Keywords: Large Language Models, Prompt Engineering, LLMs in Finance, Basel\nIII, Minimum Capital Requirements, LLM Ethics",
      "tldr_zh": "本研究探讨使用 Large Language Models (LLMs) 作为分析工具，设计有效提示工程来简化复杂金融法规（如 Basel III 资本要求）的文本，并将其转化为简洁的数学框架和可操作代码，从而 streamlining 全球银行机构的金融报告和风险管理系统。\n通过案例研究，评估了多种 LLMs 的性能，发现 GPT-4 在处理信息、收集数据和执行数学计算方面表现出色。\n实验利用数值模拟对固定收益、股票、货币对和商品等资产持有进行测试，证明了 LLMs 在实施 Basel III 资本充足要求方面的有效性，为 LLMs in Finance 应用提供了创新路径。",
      "categories": [
        "q-fin.RM",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "q-fin.RM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06808v2",
      "published_date": "2024-05-10 20:45:40 UTC",
      "updated_date": "2024-07-10 13:31:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:30:04.549170"
    },
    {
      "arxiv_id": "2405.06802v3",
      "title": "Summarizing Radiology Reports Findings into Impressions",
      "title_zh": "将放射学报告的发现总结为印象",
      "authors": [
        "Raul Salles de Padua",
        "Imran Qureshi"
      ],
      "abstract": "Patient hand-off and triage are two fundamental problems in health care.\nOften doctors must painstakingly summarize complex findings to efficiently\ncommunicate with specialists and quickly make decisions on which patients have\nthe most urgent cases. In pursuit of these challenges, we present (1) a model\nwith state-of-art radiology report summarization performance using (2) a novel\nmethod for augmenting medical data, and (3) an analysis of the model\nlimitations and radiology knowledge gain. We also provide a data processing\npipeline for future models developed on the the MIMIC CXR dataset. Our best\nperforming model was a fine-tuned BERT-to-BERT encoder-decoder with 58.75/100\nROUGE-L F1, which outperformed specialized checkpoints with more sophisticated\nattention mechanisms. We investigate these aspects in this work.",
      "tldr_zh": "这篇论文针对医疗保健中的患者交接和分诊问题，提出了一种状态-of-the-art 放射学报告总结模型，用于将复杂报告的发现总结成简洁的印象，以提升医生沟通效率和决策速度。模型采用 fine-tuned BERT-to-BERT 编码器-解码器架构，并引入了一个新颖的医疗数据增强方法，实现了 ROUGE-L F1 分数 58.75，优于其他使用复杂注意机制的专化检查点。论文还分析了模型的限制和放射学知识增益，并提供了一个数据处理管道，适用于未来基于 MIMIC CXR 数据集的模型开发。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "This version reverts to the original preprint, following the advice\n  from the Artificial Intelligence in Health editorial office. The published\n  version is peer-reviewed and available in the journal (see external DOI). The\n  preprint remains unchanged to maintain version transparency, as noted in the\n  further disclosure section of the published article",
      "pdf_url": "http://arxiv.org/pdf/2405.06802v3",
      "published_date": "2024-05-10 20:29:25 UTC",
      "updated_date": "2024-09-27 06:13:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:30:17.126956"
    },
    {
      "arxiv_id": "2405.06783v1",
      "title": "BLIP: Facilitating the Exploration of Undesirable Consequences of Digital Technologies",
      "title_zh": "翻译失败",
      "authors": [
        "Rock Yuren Pang",
        "Sebastin Santy",
        "René Just",
        "Katharina Reinecke"
      ],
      "abstract": "Digital technologies have positively transformed society, but they have also\nled to undesirable consequences not anticipated at the time of design or\ndevelopment. We posit that insights into past undesirable consequences can help\nresearchers and practitioners gain awareness and anticipate potential adverse\neffects. To test this assumption, we introduce BLIP, a system that extracts\nreal-world undesirable consequences of technology from online articles,\nsummarizes and categorizes them, and presents them in an interactive, web-based\ninterface. In two user studies with 15 researchers in various computer science\ndisciplines, we found that BLIP substantially increased the number and\ndiversity of undesirable consequences they could list in comparison to relying\non prior knowledge or searching online. Moreover, BLIP helped them identify\nundesirable consequences relevant to their ongoing projects, made them aware of\nundesirable consequences they \"had never considered,\" and inspired them to\nreflect on their own experiences with technology.",
      "tldr_zh": "本研究提出BLIP系统，以帮助研究者和从业者探索数字技术的负面后果（undesirable consequences），通过从在线文章中提取、总结和分类这些真实案例，并提供交互式网页界面来提升意识和预见能力。BLIP基于对过去负面后果的分析，旨在弥补设计阶段的潜在盲点。用户研究涉及15名计算机科学研究员，结果显示，BLIP显著提高了他们列出的负面后果数量和多样性，并帮助他们识别与当前项目相关的问题，激发了对自身技术经历的反思。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "To appear in the Proceedings of the CHI Conference on Human Factors\n  in Computing Systems (CHI '24), May 11--16, 2024, Honolulu, HI, USA",
      "pdf_url": "http://arxiv.org/pdf/2405.06783v1",
      "published_date": "2024-05-10 19:21:19 UTC",
      "updated_date": "2024-05-10 19:21:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:30:27.652972"
    },
    {
      "arxiv_id": "2405.06780v1",
      "title": "Deep MMD Gradient Flow without adversarial training",
      "title_zh": "翻译失败",
      "authors": [
        "Alexandre Galashov",
        "Valentin de Bortoli",
        "Arthur Gretton"
      ],
      "abstract": "We propose a gradient flow procedure for generative modeling by transporting\nparticles from an initial source distribution to a target distribution, where\nthe gradient field on the particles is given by a noise-adaptive Wasserstein\nGradient of the Maximum Mean Discrepancy (MMD). The noise-adaptive MMD is\ntrained on data distributions corrupted by increasing levels of noise, obtained\nvia a forward diffusion process, as commonly used in denoising diffusion\nprobabilistic models. The result is a generalization of MMD Gradient Flow,\nwhich we call Diffusion-MMD-Gradient Flow or DMMD. The divergence training\nprocedure is related to discriminator training in Generative Adversarial\nNetworks (GAN), but does not require adversarial training. We obtain\ncompetitive empirical performance in unconditional image generation on CIFAR10,\nMNIST, CELEB-A (64 x64) and LSUN Church (64 x 64). Furthermore, we demonstrate\nthe validity of the approach when MMD is replaced by a lower bound on the KL\ndivergence.",
      "tldr_zh": "本研究提出了一种名为Diffusion-MMD-Gradient Flow (DMMD)的梯度流程序，用于生成建模，通过噪声自适应Wasserstein梯度计算的Maximum Mean Discrepancy (MMD)将粒子从初始源分布传输到目标分布。不同于Generative Adversarial Networks (GAN)，该方法无需对抗训练，而是利用前向扩散过程在数据分布上添加渐增噪声来训练MMD。实验结果显示，DMMD在无条件图像生成任务上（如CIFAR10、MNIST、CELEB-A 64x64和LSUN Church 64x64数据集）取得了与现有方法竞争的性能。此外，当MMD被KL divergence的下界替换时，该方法仍保持有效，证明了其泛化潜力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06780v1",
      "published_date": "2024-05-10 19:10:45 UTC",
      "updated_date": "2024-05-10 19:10:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:30:41.755130"
    },
    {
      "arxiv_id": "2405.06772v1",
      "title": "CANAL -- Cyber Activity News Alerting Language Model: Empirical Approach vs. Expensive LLM",
      "title_zh": "翻译失败",
      "authors": [
        "Urjitkumar Patel",
        "Fang-Chun Yeh",
        "Chinmay Gondhalekar"
      ],
      "abstract": "In today's digital landscape, where cyber attacks have become the norm, the\ndetection of cyber attacks and threats is critically imperative across diverse\ndomains. Our research presents a new empirical framework for cyber threat\nmodeling, adept at parsing and categorizing cyber-related information from news\narticles, enhancing real-time vigilance for market stakeholders. At the core of\nthis framework is a fine-tuned BERT model, which we call CANAL - Cyber Activity\nNews Alerting Language Model, tailored for cyber categorization using a novel\nsilver labeling approach powered by Random Forest. We benchmark CANAL against\nlarger, costlier LLMs, including GPT-4, LLaMA, and Zephyr, highlighting their\nzero to few-shot learning in cyber news classification. CANAL demonstrates\nsuperior performance by outperforming all other LLM counterparts in both\naccuracy and cost-effectiveness. Furthermore, we introduce the Cyber Signal\nDiscovery module, a strategic component designed to efficiently detect emerging\ncyber signals from news articles. Collectively, CANAL and Cyber Signal\nDiscovery module equip our framework to provide a robust and cost-effective\nsolution for businesses that require agile responses to cyber intelligence.",
      "tldr_zh": "本文提出一个经验框架 CANAL（Cyber Activity News Alerting Language Model），用于从新闻文章中解析和分类网络威胁信息，以提升实时警戒能力。CANAL 基于 fine-tuned BERT 模型和 Random Forest 的新型 silver labeling 方法，在基准测试中优于 GPT-4、LLaMA 和 Zephyr 等大型 LLM，在准确性和成本效益方面表现出色。此外，该框架引入 Cyber Signal Discovery 模块，用于高效检测新兴网络信号，为企业提供稳健、成本有效的网络情报响应解决方案。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "68T50, 68T07 (Primary) 03B65, 91F20 (Secondary)",
        "I.2.7; I.2.1; I.5.1; I.5.2; I.5.4; H.3.3"
      ],
      "primary_category": "cs.CR",
      "comment": "Published in 2024 IEEE 3rd International Conference on AI in\n  Cybersecurity (ICAIC), Conference Date: 07-09 February 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.06772v1",
      "published_date": "2024-05-10 18:57:35 UTC",
      "updated_date": "2024-05-10 18:57:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:30:52.187538"
    },
    {
      "arxiv_id": "2405.06760v1",
      "title": "Opportunities for Persian Digital Humanities Research with Artificial Intelligence Language Models; Case Study: Forough Farrokhzad",
      "title_zh": "翻译失败",
      "authors": [
        "Arash Rasti Meymandi",
        "Zahra Hosseini",
        "Sina Davari",
        "Abolfazl Moshiri",
        "Shabnam Rahimi-Golkhandan",
        "Khashayar Namdar",
        "Nikta Feizi",
        "Mohamad Tavakoli-Targhi",
        "Farzad Khalvati"
      ],
      "abstract": "This study explores the integration of advanced Natural Language Processing\n(NLP) and Artificial Intelligence (AI) techniques to analyze and interpret\nPersian literature, focusing on the poetry of Forough Farrokhzad. Utilizing\ncomputational methods, we aim to unveil thematic, stylistic, and linguistic\npatterns in Persian poetry. Specifically, the study employs AI models including\ntransformer-based language models for clustering of the poems in an\nunsupervised framework. This research underscores the potential of AI in\nenhancing our understanding of Persian literary heritage, with Forough\nFarrokhzad's work providing a comprehensive case study. This approach not only\ncontributes to the field of Persian Digital Humanities but also sets a\nprecedent for future research in Persian literary studies using computational\ntechniques.",
      "tldr_zh": "这篇论文探讨了使用 Natural Language Processing (NLP) 和 Artificial Intelligence (AI) 技术分析波斯文学的机会，以 Forough Farrokhzad 的诗歌作为案例研究。研究采用 transformer-based language models 在无监督框架下进行诗歌聚类，揭示了主题、风格和语言模式等关键特征。该方法不仅增强了对波斯文学遗产的理解，还为波斯数字人文领域提供了新范例，并为未来使用计算技术的文学研究奠定基础。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06760v1",
      "published_date": "2024-05-10 18:24:55 UTC",
      "updated_date": "2024-05-10 18:24:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:31:03.672862"
    },
    {
      "arxiv_id": "2405.08011v3",
      "title": "A Survey of Large Language Models for Graphs",
      "title_zh": "翻译失败",
      "authors": [
        "Xubin Ren",
        "Jiabin Tang",
        "Dawei Yin",
        "Nitesh Chawla",
        "Chao Huang"
      ],
      "abstract": "Graphs are an essential data structure utilized to represent relationships in\nreal-world scenarios. Prior research has established that Graph Neural Networks\n(GNNs) deliver impressive outcomes in graph-centric tasks, such as link\nprediction and node classification. Despite these advancements, challenges like\ndata sparsity and limited generalization capabilities continue to persist.\nRecently, Large Language Models (LLMs) have gained attention in natural\nlanguage processing. They excel in language comprehension and summarization.\nIntegrating LLMs with graph learning techniques has attracted interest as a way\nto enhance performance in graph learning tasks. In this survey, we conduct an\nin-depth review of the latest state-of-the-art LLMs applied in graph learning\nand introduce a novel taxonomy to categorize existing methods based on their\nframework design. We detail four unique designs: i) GNNs as Prefix, ii) LLMs as\nPrefix, iii) LLMs-Graphs Integration, and iv) LLMs-Only, highlighting key\nmethodologies within each category. We explore the strengths and limitations of\neach framework, and emphasize potential avenues for future research, including\novercoming current integration challenges between LLMs and graph learning\ntechniques, and venturing into new application areas. This survey aims to serve\nas a valuable resource for researchers and practitioners eager to leverage\nlarge language models in graph learning, and to inspire continued progress in\nthis dynamic field. We consistently maintain the related open-source materials\nat \\url{https://github.com/HKUDS/Awesome-LLM4Graph-Papers}.",
      "tldr_zh": "这篇调查论文回顾了Large Language Models (LLMs)在图学习中的应用，旨在解决Graph Neural Networks (GNNs)面临的挑战，如数据稀疏性和泛化能力不足。论文引入了一个新颖的分类法，将现有方法分为四类：i) GNNs as Prefix, ii) LLMs as Prefix, iii) LLMs-Graphs Integration, and iv) LLMs-Only，并详细分析了每类框架的优缺点。调查强调了未来研究方向，包括克服LLMs与图学习整合的障碍，以及扩展到新应用领域，并提供开源资源以支持进一步探索。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Published as a KDD'24 survey paper",
      "pdf_url": "http://arxiv.org/pdf/2405.08011v3",
      "published_date": "2024-05-10 18:05:37 UTC",
      "updated_date": "2024-09-11 07:31:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:31:16.839781"
    },
    {
      "arxiv_id": "2405.06639v1",
      "title": "Value Augmented Sampling for Language Model Alignment and Personalization",
      "title_zh": "用于语言模型对齐和个性化的值增强采样",
      "authors": [
        "Seungwook Han",
        "Idan Shenfeld",
        "Akash Srivastava",
        "Yoon Kim",
        "Pulkit Agrawal"
      ],
      "abstract": "Aligning Large Language Models (LLMs) to cater to different human\npreferences, learning new skills, and unlearning harmful behavior is an\nimportant problem. Search-based methods, such as Best-of-N or Monte-Carlo Tree\nSearch, are performant, but impractical for LLM adaptation due to their high\ninference cost. On the other hand, using Reinforcement Learning (RL) for\nadaptation is computationally efficient, but performs worse due to the\noptimization challenges in co-training the value function and the policy. We\npresent a new framework for reward optimization, Value Augmented Sampling\n(VAS), that can maximize different reward functions using data sampled from\nonly the initial, frozen LLM. VAS solves for the optimal reward-maximizing\npolicy without co-training the policy and the value function, making the\noptimization stable, outperforming established baselines, such as PPO and DPO,\non standard benchmarks, and achieving comparable results to Best-of-128 with\nlower inference cost. Unlike existing RL methods that require changing the\nweights of the LLM, VAS does not require access to the weights of the\npre-trained LLM. Thus, it can even adapt LLMs (e.g., ChatGPT), which are\navailable only as APIs. In addition, our algorithm unlocks the new capability\nof composing several rewards and controlling the extent of each one during\ndeployment time, paving the road ahead for the future of aligned, personalized\nLLMs.",
      "tldr_zh": "这篇论文提出了 Value Augmented Sampling (VAS) 框架，用于调整 Large Language Models (LLMs) 以适应不同人类偏好、学习新技能和消除有害行为。VAS 通过从初始冻结的 LLM 采样数据来最大化各种奖励函数，而无需共同训练策略和价值函数，从而避免了优化挑战，并显著降低了推理成本。实验结果显示，VAS 在标准基准上优于 PPO 和 DPO 等基线方法，与 Best-of-128 相当；此外，它不需访问 LLM 权重，因此可应用于 API-only 模型如 ChatGPT，并支持在部署时组合多个奖励以实现个性化的 LLM 对齐。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Website: https://sites.google.com/view/llm-vas",
      "pdf_url": "http://arxiv.org/pdf/2405.06639v1",
      "published_date": "2024-05-10 17:59:04 UTC",
      "updated_date": "2024-05-10 17:59:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:31:30.056893"
    },
    {
      "arxiv_id": "2405.06636v2",
      "title": "Federated Document Visual Question Answering: A Pilot Study",
      "title_zh": "联邦文档视觉问答：一个试点研究",
      "authors": [
        "Khanh Nguyen",
        "Dimosthenis Karatzas"
      ],
      "abstract": "An important handicap of document analysis research is that documents tend to\nbe copyrighted or contain private information, which prohibits their open\npublication and the creation of centralised, large-scale document datasets.\nInstead, documents are scattered in private data silos, making extensive\ntraining over heterogeneous data a tedious task. In this work, we explore the\nuse of a federated learning (FL) scheme as a way to train a shared model on\ndecentralised private document data. We focus on the problem of Document VQA, a\ntask particularly suited to this approach, as the type of reasoning\ncapabilities required from the model can be quite different in diverse domains.\nEnabling training over heterogeneous document datasets can thus substantially\nenrich DocVQA models. We assemble existing DocVQA datasets from diverse domains\nto reflect the data heterogeneity in real-world applications. We explore the\nself-pretraining technique in this multi-modal setting, where the same data is\nused for both pretraining and finetuning, making it relevant for privacy\npreservation. We further propose combining self-pretraining with a Federated\nDocVQA training method using centralized adaptive optimization that outperforms\nthe FedAvg baseline. With extensive experiments, we also present a\nmulti-faceted analysis on training DocVQA models with FL, which provides\ninsights for future research on this task. We show that our pretraining\nstrategies can effectively learn and scale up under federated training with\ndiverse DocVQA datasets and tuning hyperparameters is essential for practical\ndocument tasks under federation.",
      "tldr_zh": "该研究探讨了联邦学习 (FL) 在文档视觉问答 (Document VQA, DocVQA) 任务中的应用，以解决文档隐私和数据分散问题，从而在私有数据孤岛上训练共享模型。作者组装了不同领域的 DocVQA 数据集，引入自预训练 (self-pretraining) 技术，使用相同数据进行预训练和微调，以增强隐私保护，并提出结合自预训练的联邦训练方法，使用中心化自适应优化优于 FedAvg 基线。实验结果显示，该方法在异质数据集上有效提升模型性能，并通过多方面分析强调了调整超参数的重要性，为未来联邦文档任务研究提供宝贵见解。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06636v2",
      "published_date": "2024-05-10 17:53:05 UTC",
      "updated_date": "2024-05-22 11:01:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:31:40.415479"
    },
    {
      "arxiv_id": "2405.06634v2",
      "title": "Multimodal LLMs Struggle with Basic Visual Network Analysis: a VNA Benchmark",
      "title_zh": "翻译失败",
      "authors": [
        "Evan M. Williams",
        "Kathleen M. Carley"
      ],
      "abstract": "We evaluate the zero-shot ability of GPT-4 and LLaVa to perform simple Visual\nNetwork Analysis (VNA) tasks on small-scale graphs. We evaluate the Vision\nLanguage Models (VLMs) on 5 tasks related to three foundational network science\nconcepts: identifying nodes of maximal degree on a rendered graph, identifying\nwhether signed triads are balanced or unbalanced, and counting components. The\ntasks are structured to be easy for a human who understands the underlying\ngraph theoretic concepts, and can all be solved by counting the appropriate\nelements in graphs. We find that while GPT-4 consistently outperforms LLaVa,\nboth models struggle with every visual network analysis task we propose. We\npublicly release the first benchmark for the evaluation of VLMs on foundational\nVNA tasks.",
      "tldr_zh": "本文评估了Multimodal LLMs（如GPT-4和LLaVa）在小规模图上的零样本Visual Network Analysis (VNA)任务性能，涉及5个基础网络科学概念的任务，包括识别最大度节点、判断有符号三元组的平衡性以及计数组件。这些任务对理解图论概念的人类而言简单，仅需计数元素，但结果显示GPT-4虽优于LLaVa，两者均在所有任务上表现不佳。作者公开发布了首个VNA基准，以促进VLMs在视觉网络分析领域的评估和改进。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.06634v2",
      "published_date": "2024-05-10 17:51:35 UTC",
      "updated_date": "2024-06-10 15:28:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:31:51.749372"
    },
    {
      "arxiv_id": "2405.06627v3",
      "title": "Conformal Validity Guarantees Exist for Any Data Distribution (and How to Find Them)",
      "title_zh": "翻译失败",
      "authors": [
        "Drew Prinster",
        "Samuel Stanton",
        "Anqi Liu",
        "Suchi Saria"
      ],
      "abstract": "As artificial intelligence (AI) / machine learning (ML) gain widespread\nadoption, practitioners are increasingly seeking means to quantify and control\nthe risk these systems incur. This challenge is especially salient when such\nsystems have autonomy to collect their own data, such as in black-box\noptimization and active learning, where their actions induce sequential\nfeedback-loop shifts in the data distribution. Conformal prediction is a\npromising approach to uncertainty and risk quantification, but prior variants'\nvalidity guarantees have assumed some form of ``quasi-exchangeability'' on the\ndata distribution, thereby excluding many types of sequential shifts. In this\npaper we prove that conformal prediction can theoretically be extended to\n\\textit{any} joint data distribution, not just exchangeable or\nquasi-exchangeable ones. Although the most general case is exceedingly\nimpractical to compute, for concrete practical applications we outline a\nprocedure for deriving specific conformal algorithms for any data distribution,\nand we use this procedure to derive tractable algorithms for a series of\nAI/ML-agent-induced covariate shifts. We evaluate the proposed algorithms\nempirically on synthetic black-box optimization and active learning tasks.",
      "tldr_zh": "这篇论文证明，Conformal Prediction 可以扩展到任何数据分布，而不仅仅局限于可交换或准可交换分布，从而解决AI/ML系统在自主数据收集（如黑箱优化和active learning）中因顺序反馈循环导致的风险量化挑战。作者提出一个过程，用于为具体数据分布推导可计算的共形算法，并将其应用于处理AI/ML代理引发的covariate shifts。实验结果显示，这些算法在合成黑箱优化和active learning任务上表现出有效性，为更广泛的风险控制提供了理论和实践基础。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "ICML 2024. Code available at\n  https://github.com/drewprinster/conformal-mfcs",
      "pdf_url": "http://arxiv.org/pdf/2405.06627v3",
      "published_date": "2024-05-10 17:40:24 UTC",
      "updated_date": "2024-06-05 15:49:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:32:04.217842"
    },
    {
      "arxiv_id": "2405.06624v3",
      "title": "Towards Guaranteed Safe AI: A Framework for Ensuring Robust and Reliable AI Systems",
      "title_zh": "翻译失败",
      "authors": [
        "David \"davidad\" Dalrymple",
        "Joar Skalse",
        "Yoshua Bengio",
        "Stuart Russell",
        "Max Tegmark",
        "Sanjit Seshia",
        "Steve Omohundro",
        "Christian Szegedy",
        "Ben Goldhaber",
        "Nora Ammann",
        "Alessandro Abate",
        "Joe Halpern",
        "Clark Barrett",
        "Ding Zhao",
        "Tan Zhi-Xuan",
        "Jeannette Wing",
        "Joshua Tenenbaum"
      ],
      "abstract": "Ensuring that AI systems reliably and robustly avoid harmful or dangerous\nbehaviours is a crucial challenge, especially for AI systems with a high degree\nof autonomy and general intelligence, or systems used in safety-critical\ncontexts. In this paper, we will introduce and define a family of approaches to\nAI safety, which we will refer to as guaranteed safe (GS) AI. The core feature\nof these approaches is that they aim to produce AI systems which are equipped\nwith high-assurance quantitative safety guarantees. This is achieved by the\ninterplay of three core components: a world model (which provides a\nmathematical description of how the AI system affects the outside world), a\nsafety specification (which is a mathematical description of what effects are\nacceptable), and a verifier (which provides an auditable proof certificate that\nthe AI satisfies the safety specification relative to the world model). We\noutline a number of approaches for creating each of these three core\ncomponents, describe the main technical challenges, and suggest a number of\npotential solutions to them. We also argue for the necessity of this approach\nto AI safety, and for the inadequacy of the main alternative approaches.",
      "tldr_zh": "本论文提出了一种名为“guaranteed safe (GS) AI”的框架，旨在确保AI系统在高度自治或安全关键环境中避免有害行为，提供高保障的定量安全保证。该框架的核心组件包括world model（描述AI对外部世界的数学影响）、safety specification（定义可接受效果的数学描述）和verifier（提供AI满足安全规范的审计证明证书）。论文概述了构建这些组件的多种方法、主要技术挑战以及潜在解决方案，并论证了这种方法相对于其他AI安全策略的必要性和优越性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06624v3",
      "published_date": "2024-05-10 17:38:32 UTC",
      "updated_date": "2024-07-08 13:35:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:32:14.971867"
    },
    {
      "arxiv_id": "2405.06611v1",
      "title": "\"We are at the mercy of others' opinion\": Supporting Blind People in Recreational Window Shopping with AI-infused Technology",
      "title_zh": "“我们受制于他人的意见”：使用注入AI技术的手段支持盲人在休闲逛街",
      "authors": [
        "Rie Kamikubo",
        "Hernisa Kacorri",
        "Chieko Asakawa"
      ],
      "abstract": "Engaging in recreational activities in public spaces poses challenges for\nblind people, often involving dependency on sighted help. Window shopping is a\nkey recreational activity that remains inaccessible. In this paper, we\ninvestigate the information needs, challenges, and current approaches blind\npeople have to recreational window shopping to inform the design of existing\nwayfinding and navigation technology for supporting blind shoppers in\nexploration and serendipitous discovery. We conduct a formative study with a\ntotal of 18 blind participants that include both focus groups (N=8) and\ninterviews for requirements analysis (N=10). We find that there is a desire for\npush notifications of promotional information and pull notifications about\nshops of interest such as the targeted audience of a brand. Information about\nobstacles and points-of-interest required customization depending on one's\nmobility aid as well as presence of a crowd, children, and wheelchair users. We\ntranslate these findings into specific information modalities and rendering in\nthe context of two existing AI-infused assistive applications: NavCog (a\nturn-by-turn navigation app) and Cabot (a navigation robot).",
      "tldr_zh": "这篇论文探讨了盲人参与公共空间休闲活动（如橱窗购物）的挑战，强调了他们对他人帮助的依赖，并通过焦点小组（N=8）和访谈（N=10）等形成性研究调查了信息需求和障碍。研究发现，盲人希望获得推送通知（如促销信息和品牌目标受众）以及可自定义的障碍物和兴趣点信息，这些取决于移动辅助工具、人群、儿童和轮椅用户等因素。主要贡献是将这些发现转化为具体的信息模式和呈现方式，应用于现有AI-infused技术如NavCog（转弯导航app）和Cabot（导航机器人），以提升盲人的探索和意外发现体验。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Preprint, W4A'24, Proceedings of the 21st International Web for All\n  Conference",
      "pdf_url": "http://arxiv.org/pdf/2405.06611v1",
      "published_date": "2024-05-10 17:15:24 UTC",
      "updated_date": "2024-05-10 17:15:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:32:29.060812"
    },
    {
      "arxiv_id": "2405.06573v1",
      "title": "An Investigation of Incorporating Mamba for Speech Enhancement",
      "title_zh": "翻译失败",
      "authors": [
        "Rong Chao",
        "Wen-Huang Cheng",
        "Moreno La Quatra",
        "Sabato Marco Siniscalchi",
        "Chao-Han Huck Yang",
        "Szu-Wei Fu",
        "Yu Tsao"
      ],
      "abstract": "This work aims to study a scalable state-space model (SSM), Mamba, for the\nspeech enhancement (SE) task. We exploit a Mamba-based regression model to\ncharacterize speech signals and build an SE system upon Mamba, termed SEMamba.\nWe explore the properties of Mamba by integrating it as the core model in both\nbasic and advanced SE systems, along with utilizing signal-level distances as\nwell as metric-oriented loss functions. SEMamba demonstrates promising results\nand attains a PESQ score of 3.55 on the VoiceBank-DEMAND dataset. When combined\nwith the perceptual contrast stretching technique, the proposed SEMamba yields\na new state-of-the-art PESQ score of 3.69.",
      "tldr_zh": "这篇论文探讨了将 Mamba（一种可扩展的状态空间模型，SSM）整合到语音增强（SE）任务中的可能性，构建了一个基于 Mamba 的回归模型，称为 SEMamba，用于表征语音信号。研究者通过在基本和高级 SE 系统中使用 Mamba 作为核心组件，并结合信号级距离和面向度量的损失函数，优化了模型性能。实验结果显示，SEMamba 在 VoiceBank-DEMAND 数据集上达到了 3.55 的 PESQ 分数，并通过感知对比拉伸技术进一步提升至 3.69 的新最先进水平。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06573v1",
      "published_date": "2024-05-10 16:18:49 UTC",
      "updated_date": "2024-05-10 16:18:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:32:41.491001"
    },
    {
      "arxiv_id": "2405.06553v1",
      "title": "Scalable Property Valuation Models via Graph-based Deep Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Enrique Riveros",
        "Carla Vairetti",
        "Christian Wegmann",
        "Santiago Truffa",
        "Sebastián Maldonado"
      ],
      "abstract": "This paper aims to enrich the capabilities of existing deep learning-based\nautomated valuation models through an efficient graph representation of peer\ndependencies, thus capturing intricate spatial relationships. In particular, we\ndevelop two novel graph neural network models that effectively identify\nsequences of neighboring houses with similar features, employing different\nmessage passing algorithms. The first strategy consider standard spatial graph\nconvolutions, while the second one utilizes transformer graph convolutions.\nThis approach confers scalability to the modeling process. The experimental\nevaluation is conducted using a proprietary dataset comprising approximately\n200,000 houses located in Santiago, Chile. We show that employing tailored\ngraph neural networks significantly improves the accuracy of house price\nprediction, especially when utilizing transformer convolutional message passing\nlayers.",
      "tldr_zh": "本论文旨在通过基于图的深度学习提升房屋估值模型的性能，使用高效的图表示来捕捉邻近房屋的复杂空间关系。研究开发了两个新颖的Graph Neural Network (GNN) 模型：一个采用标准的空间图卷积，另一个使用Transformer Graph Convolutions，以识别具有相似特征的房屋序列，并实现模型的可扩展性。在智利圣地亚哥的约20万房屋数据集上进行实验，结果显示，定制的GNN 模型显著提高了房屋价格预测的准确性，尤其是采用Transformer卷积消息传递层时。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "18 pages, 3 figures, Submitted to Expert Systems with Applications",
      "pdf_url": "http://arxiv.org/pdf/2405.06553v1",
      "published_date": "2024-05-10 15:54:55 UTC",
      "updated_date": "2024-05-10 15:54:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:32:52.727753"
    },
    {
      "arxiv_id": "2405.06522v1",
      "title": "Heterogeneous Graph Neural Networks with Loss-decrease-aware Curriculum Learning",
      "title_zh": "具有损失减少感知课程学习的异构图神经网络",
      "authors": [
        "Yili Wang"
      ],
      "abstract": "In recent years, heterogeneous graph neural networks (HGNNs) have achieved\nexcellent performance in handling heterogeneous information networks (HINs).\nCurriculum learning is a machine learning strategy where training examples are\npresented to a model in a structured order, starting with easy examples and\ngradually increasing difficulty, aiming to improve learning efficiency and\ngeneralization. To better exploit the rich information in HINs, previous\nmethods have started to explore the use of curriculum learning strategy to\ntrain HGNNs. Specifically, these works utilize the absolute value of the loss\nat each training epoch to evaluate the learning difficulty of each training\nsample. However, the relative loss, rather than the absolute value of loss,\nreveals the learning difficulty. Therefore, we propose a novel\nloss-decrease-aware training schedule (LDTS). LDTS uses the trend of loss\ndecrease between each training epoch to better evaluating the difficulty of\ntraining samples, thereby enhancing the curriculum learning of HGNNs for\ndownstream tasks. Additionally, we propose a sampling strategy to alleviate\ntraining imbalance issues. Our method further demonstrate the efficacy of\ncurriculum learning in enhancing HGNNs capabilities. We call our method\nLoss-decrease-aware Heterogeneous Graph Neural Networks (LDHGNN). The code is\npublic at https://github.com/wangyili00/LDHGNN.",
      "tldr_zh": "该论文提出了一种名为 LDHGNN 的异构图神经网络 (HGNNs)，通过 loss-decrease-aware 训练调度 (LDTS) 来优化课程学习策略。LDTS 利用损失减少趋势评估训练样本的难度，而不是依赖绝对损失值，同时引入采样策略缓解训练不平衡问题，从而提升 HGNNs 在下游任务中的学习效率和泛化能力。实验结果证明，该方法显著提高了模型性能，并展示了课程学习在处理异构信息网络 (HINs) 中的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "arXiv admin note: substantial text overlap with arXiv:2402.18875 by\n  other authors",
      "pdf_url": "http://arxiv.org/pdf/2405.06522v1",
      "published_date": "2024-05-10 15:06:53 UTC",
      "updated_date": "2024-05-10 15:06:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:33:06.543585"
    },
    {
      "arxiv_id": "2405.06510v1",
      "title": "UniDM: A Unified Framework for Data Manipulation with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Yichen Qian",
        "Yongyi He",
        "Rong Zhu",
        "Jintao Huang",
        "Zhijian Ma",
        "Haibin Wang",
        "Yaohua Wang",
        "Xiuyu Sun",
        "Defu Lian",
        "Bolin Ding",
        "Jingren Zhou"
      ],
      "abstract": "Designing effective data manipulation methods is a long standing problem in\ndata lakes. Traditional methods, which rely on rules or machine learning\nmodels, require extensive human efforts on training data collection and tuning\nmodels. Recent methods apply Large Language Models (LLMs) to resolve multiple\ndata manipulation tasks. They exhibit bright benefits in terms of performance\nbut still require customized designs to fit each specific task. This is very\ncostly and can not catch up with the requirements of big data lake platforms.\nIn this paper, inspired by the cross-task generality of LLMs on NLP tasks, we\npave the first step to design an automatic and general solution to tackle with\ndata manipulation tasks. We propose UniDM, a unified framework which\nestablishes a new paradigm to process data manipulation tasks using LLMs. UniDM\nformalizes a number of data manipulation tasks in a unified form and abstracts\nthree main general steps to solve each task. We develop an automatic context\nretrieval to allow the LLMs to retrieve data from data lakes, potentially\ncontaining evidence and factual information. For each step, we design effective\nprompts to guide LLMs to produce high quality results. By our comprehensive\nevaluation on a variety of benchmarks, our UniDM exhibits great generality and\nstate-of-the-art performance on a wide variety of data manipulation tasks.",
      "tldr_zh": "本文提出 UniDM，一种统一的框架，利用 Large Language Models (LLMs) 来处理数据操作任务，旨在解决传统方法依赖规则或机器学习模型而需大量人力调优的问题。UniDM 将多种数据操作任务形式化为统一形式，并抽象出三个主要步骤，包括自动上下文检索以从数据湖中获取证据和事实信息，以及设计有效提示来指导 LLMs 产生高质量结果。通过全面评估，UniDM 在各种基准上展示了极大的通用性和最先进性能，显著提升了数据操作的效率和可扩展性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "MLSys24",
      "pdf_url": "http://arxiv.org/pdf/2405.06510v1",
      "published_date": "2024-05-10 14:44:04 UTC",
      "updated_date": "2024-05-10 14:44:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:33:17.545433"
    },
    {
      "arxiv_id": "2405.06485v1",
      "title": "Solving Quantified Boolean Formulas with Few Existential Variables",
      "title_zh": "解决具有",
      "authors": [
        "Leif Eriksson",
        "Victor Lagerkvist",
        "George Osipov",
        "Sebastian Ordyniak",
        "Fahad Panolan",
        "Mateusz Rychlicki"
      ],
      "abstract": "The quantified Boolean formula (QBF) problem is an important decision problem\ngenerally viewed as the archetype for PSPACE-completeness. Many problems of\ncentral interest in AI are in general not included in NP, e.g., planning, model\nchecking, and non-monotonic reasoning, and for such problems QBF has\nsuccessfully been used as a modelling tool. However, solvers for QBF are not as\nadvanced as state of the art SAT solvers, which has prevented QBF from becoming\na universal modelling language for PSPACE-complete problems. A theoretical\nexplanation is that QBF (as well as many other PSPACE-complete problems) lacks\nnatural parameters} guaranteeing fixed-parameter tractability (FPT).\n  In this paper we tackle this problem and consider a simple but overlooked\nparameter: the number of existentially quantified variables. This natural\nparameter is virtually unexplored in the literature which one might find\nsurprising given the general scarcity of FPT algorithms for QBF. Via this\nparameterization we then develop a novel FPT algorithm applicable to QBF\ninstances in conjunctive normal form (CNF) of bounded clause length. We\ncomplement this by a W[1]-hardness result for QBF in CNF of unbounded clause\nlength as well as sharper lower bounds for the bounded arity case under the\n(strong) exponential-time hypothesis.",
      "tldr_zh": "这篇论文探讨了量化布尔公式 (QBF) 的求解问题，该问题是 PSPACE-complete 的典型代表，常用于建模 AI 中的规划、模型检查和非单调推理等任务。作者引入了一个新参数——存在量词变量的数量，并基于此开发了一个固定参数可追踪 (FPT) 算法，适用于子句长度有界的合取范式 (CNF) QBF 实例。论文还证明了对于子句长度无界的 QBF，该问题在 W[1] 意义上困难，并提供了在强指数时间假设下的更精确下界，从而为提升 QBF 求解器的效率提供了理论基础。",
      "categories": [
        "cs.CC",
        "cs.AI"
      ],
      "primary_category": "cs.CC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06485v1",
      "published_date": "2024-05-10 14:07:29 UTC",
      "updated_date": "2024-05-10 14:07:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:33:30.247084"
    },
    {
      "arxiv_id": "2405.06478v1",
      "title": "Attention is all they need: Cognitive science and the (techno)political economy of attention in humans and machines",
      "title_zh": "注意力就是他们所需的一切：认知科学与（技术）政治经济学在人类和机器中的注意力",
      "authors": [
        "Pablo González de la Torre",
        "Marta Pérez-Verdugo",
        "Xabier E. Barandiaran"
      ],
      "abstract": "This paper critically analyses the \"attention economy\" within the framework\nof cognitive science and techno-political economics, as applied to both human\nand machine interactions. We explore how current business models, particularly\nin digital platform capitalism, harness user engagement by strategically\nshaping attentional patterns. These platforms utilize advanced AI and massive\ndata analytics to enhance user engagement, creating a cycle of attention\ncapture and data extraction. We review contemporary (neuro)cognitive theories\nof attention and platform engagement design techniques and criticize classical\ncognitivist and behaviourist theories for their inadequacies in addressing the\npotential harms of such engagement on user autonomy and wellbeing. 4E\napproaches to cognitive science, instead, emphasizing the embodied, extended,\nenactive, and ecological aspects of cognition, offer us an intrinsic normative\nstandpoint and a more integrated understanding of how attentional patterns are\nactively constituted by adaptive digital environments. By examining the\nprecarious nature of habit formation in digital contexts, we reveal the\ntechno-economic underpinnings that threaten personal autonomy by disaggregating\nhabits away from the individual, into an AI managed collection of behavioural\npatterns. Our current predicament suggests the necessity of a paradigm shift\ntowards an ecology of attention. This shift aims to foster environments that\nrespect and preserve human cognitive and social capacities, countering the\nexploitative tendencies of cognitive capitalism.",
      "tldr_zh": "这篇论文批判性地分析了“attention economy”在认知科学和技术政治经济学框架下的应用，探讨数字平台资本主义如何利用AI和数据分析来塑造人类和机器的注意力模式，从而增强用户参与并提取数据。作者审视了当代（神经）认知理论和平台设计技巧，批评经典认知主义和行为主义理论未能解决此类参与对用户自主性和福祉的潜在危害，并引入4E方法（embodied, extended, enactive, and ecological）来提供更全面的认知理解。论文揭示了数字环境中习惯形成的脆弱性，以及技术经济基础如何将个人习惯分解为AI管理的行为模式，并主张转向“attention ecology”范式，以创建尊重人类认知和社会能力的环境，反对认知资本主义的剥削倾向。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06478v1",
      "published_date": "2024-05-10 13:53:46 UTC",
      "updated_date": "2024-05-10 13:53:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:33:42.113902"
    },
    {
      "arxiv_id": "2405.06725v3",
      "title": "On the Shape of Brainscores for Large Language Models (LLMs)",
      "title_zh": "翻译失败",
      "authors": [
        "Jingkai Li"
      ],
      "abstract": "With the rise of Large Language Models (LLMs), the novel metric \"Brainscore\"\nemerged as a means to evaluate the functional similarity between LLMs and human\nbrain/neural systems. Our efforts were dedicated to mining the meaning of the\nnovel score by constructing topological features derived from both human fMRI\ndata involving 190 subjects, and 39 LLMs plus their untrained counterparts.\nSubsequently, we trained 36 Linear Regression Models and conducted thorough\nstatistical analyses to discern reliable and valid features from our\nconstructed ones. Our findings reveal distinctive feature combinations\nconducive to interpreting existing brainscores across various brain regions of\ninterest (ROIs) and hemispheres, thereby significantly contributing to\nadvancing interpretable machine learning (iML) studies. The study is enriched\nby our further discussions and analyses concerning existing brainscores. To our\nknowledge, this study represents the first attempt to comprehend the novel\nmetric brainscore within this interdisciplinary domain.",
      "tldr_zh": "本研究探讨了 Brainscore 这一新指标，用于评估 Large Language Models (LLMs) 与人类大脑/神经系统的功能相似性。通过分析 190 个受试者的 fMRI 数据和 39 个 LLMs 及其未训练版本，研究者构建了拓扑特征，并训练了 36 个 Linear Regression Models 进行统计分析。结果揭示了特定特征组合，能有效解释不同脑区 (ROIs) 和半球的 Brainscore，从而推动 interpretable machine learning (iML) 研究的发展。该工作是首次尝试在这一跨学科领域深入理解 Brainscore 的含义。",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "q-bio.NC",
      "comment": "Published as a workshop paper at ICLR AGI Workshop 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.06725v3",
      "published_date": "2024-05-10 13:22:20 UTC",
      "updated_date": "2024-05-15 02:46:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:33:53.995879"
    },
    {
      "arxiv_id": "2405.06459v4",
      "title": "Are EEG-to-Text Models Working?",
      "title_zh": "EEG-to-Text 模型有效吗？",
      "authors": [
        "Hyejeong Jo",
        "Yiqian Yang",
        "Juhyeok Han",
        "Yiqun Duan",
        "Hui Xiong",
        "Won Hee Lee"
      ],
      "abstract": "This work critically analyzes existing models for open-vocabulary EEG-to-Text\ntranslation. We identify a crucial limitation: previous studies often employed\nimplicit teacher-forcing during evaluation, artificially inflating performance\nmetrics. Additionally, they lacked a critical benchmark - comparing model\nperformance on pure noise inputs. We propose a methodology to differentiate\nbetween models that truly learn from EEG signals and those that simply memorize\ntraining data. Our analysis reveals that model performance on noise data can be\ncomparable to that on EEG data. These findings highlight the need for stricter\nevaluation practices in EEG-to-Text research, emphasizing transparent reporting\nand rigorous benchmarking with noise inputs. This approach will lead to more\nreliable assessments of model capabilities and pave the way for robust\nEEG-to-Text communication systems. Code is available at\nhttps://github.com/NeuSpeech/EEG-To-Text",
      "tldr_zh": "这篇论文对现有的 EEG-to-Text 模型进行了批判性分析，发现之前的评估常使用隐式 teacher-forcing，这导致性能指标被人为夸大。研究者提出了一种新方法，通过比较模型在 EEG 数据和纯噪声输入上的表现，来区分真正从 EEG 信号中学习的模型与那些仅记忆训练数据的模型。结果显示，模型在噪声数据上的性能可能与 EEG 数据相当，强调了 EEG-to-Text 研究需要更严格的基准测试和透明报告，以实现更可靠的模型评估和稳健的通信系统。代码可在 https://github.com/NeuSpeech/EEG-To-Text 获取。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06459v4",
      "published_date": "2024-05-10 13:10:55 UTC",
      "updated_date": "2024-10-26 05:41:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:34:06.031280"
    },
    {
      "arxiv_id": "2405.06424v3",
      "title": "Improving Instruction Following in Language Models through Proxy-Based Uncertainty Estimation",
      "title_zh": "翻译失败",
      "authors": [
        "JoonHo Lee",
        "Jae Oh Woo",
        "Juree Seok",
        "Parisa Hassanzadeh",
        "Wooseok Jang",
        "JuYoun Son",
        "Sima Didari",
        "Baruch Gutow",
        "Heng Hao",
        "Hankyu Moon",
        "Wenjun Hu",
        "Yeong-Dae Kwon",
        "Taehee Lee",
        "Seungjai Min"
      ],
      "abstract": "Assessing response quality to instructions in language models is vital but\nchallenging due to the complexity of human language across different contexts.\nThis complexity often results in ambiguous or inconsistent interpretations,\nmaking accurate assessment difficult. To address this issue, we propose a novel\nUncertainty-aware Reward Model (URM) that introduces a robust uncertainty\nestimation for the quality of paired responses based on Bayesian approximation.\nTrained with preference datasets, our uncertainty-enabled proxy not only scores\nrewards for responses but also evaluates their inherent uncertainty. Empirical\nresults demonstrate significant benefits of incorporating the proposed proxy\ninto language model training. Our method boosts the instruction following\ncapability of language models by refining data curation for training and\nimproving policy optimization objectives, thereby surpassing existing methods\nby a large margin on benchmarks such as Vicuna and MT-bench. These findings\nhighlight that our proposed approach substantially advances language model\ntraining and paves a new way of harnessing uncertainty within language models.",
      "tldr_zh": "这篇论文针对语言模型指令响应评估的挑战，提出了一种Uncertainty-aware Reward Model (URM)，利用Bayesian approximation来估计响应质量的不确定性，从而处理人类语言的模糊性和不一致性。URM通过偏好数据集训练，不仅为响应提供奖励分数，还评估其内在不确定性。实验结果显示，该方法通过优化数据整理和策略目标，显著提升了语言模型的指令遵循能力，在Vicuna和MT-bench基准上大幅超越现有方法。这些发现为语言模型训练开辟了新路径，展示了如何有效利用不确定性来改进模型性能。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to ICML 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.06424v3",
      "published_date": "2024-05-10 12:14:11 UTC",
      "updated_date": "2025-01-31 09:26:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:34:17.968591"
    },
    {
      "arxiv_id": "2405.06422v1",
      "title": "Contextual Affordances for Safe Exploration in Robotic Scenarios",
      "title_zh": "翻译失败",
      "authors": [
        "William Z. Ye",
        "Eduardo B. Sandoval",
        "Pamela Carreno-Medrano",
        "Francisco Cru"
      ],
      "abstract": "Robotics has been a popular field of research in the past few decades, with\nmuch success in industrial applications such as manufacturing and logistics.\nThis success is led by clearly defined use cases and controlled operating\nenvironments. However, robotics has yet to make a large impact in domestic\nsettings. This is due in part to the difficulty and complexity of designing\nmass-manufactured robots that can succeed in the variety of homes and\nenvironments that humans live in and that can operate safely in close proximity\nto humans. This paper explores the use of contextual affordances to enable safe\nexploration and learning in robotic scenarios targeted in the home. In\nparticular, we propose a simple state representation that allows us to extend\ncontextual affordances to larger state spaces and showcase how affordances can\nimprove the success and convergence rate of a reinforcement learning algorithm\nin simulation. Our results suggest that after further iterations, it is\npossible to consider the implementation of this approach in a real robot\nmanipulator. Furthermore, in the long term, this work could be the foundation\nfor future explorations of human-robot interactions in complex domestic\nenvironments. This could be possible once state-of-the-art robot manipulators\nachieve the required level of dexterity for the described affordances in this\npaper.",
      "tldr_zh": "本论文探讨了机器人学在家庭环境中面临的挑战，如环境多样性和安全互动需求，并提出使用 contextual affordances 来实现安全探索和学习。作者设计了一个简单的状态表示方法，将 contextual affordances 扩展到更大的状态空间，并通过模拟实验证明其能提升强化学习算法的成功率和收敛速度。结果显示，这种方法为未来在真实 robot manipulators 上应用提供了基础，并有望促进复杂家庭场景下的人机互动。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "5 pages, 2 figures. Accepted at the 2nd Workshop on Human-aligned\n  Reinforcement Learning for Autonomous Agents and Robots HARL, at the IEEE\n  International Conference on Robotics and Automation ICRA, Yokohama, Japan,\n  2024",
      "pdf_url": "http://arxiv.org/pdf/2405.06422v1",
      "published_date": "2024-05-10 12:12:38 UTC",
      "updated_date": "2024-05-10 12:12:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:34:28.755070"
    },
    {
      "arxiv_id": "2405.06419v3",
      "title": "Time Evidence Fusion Network: Multi-source View in Long-Term Time Series Forecasting",
      "title_zh": "时间证据融合网络：长期时间序列预测中的多源视图",
      "authors": [
        "Tianxiang Zhan",
        "Yuanpeng He",
        "Yong Deng",
        "Zhen Li",
        "Wenjie Du",
        "Qingsong Wen"
      ],
      "abstract": "In practical scenarios, time series forecasting necessitates not only\naccuracy but also efficiency. Consequently, the exploration of model\narchitectures remains a perennially trending topic in research. To address\nthese challenges, we propose a novel backbone architecture named Time Evidence\nFusion Network (TEFN) from the perspective of information fusion. Specifically,\nwe introduce the Basic Probability Assignment (BPA) Module based on evidence\ntheory to capture the uncertainty of multivariate time series data from both\nchannel and time dimensions. Additionally, we develop a novel multi-source\ninformation fusion method to effectively integrate the two distinct dimensions\nfrom BPA output, leading to improved forecasting accuracy. Lastly, we conduct\nextensive experiments to demonstrate that TEFN achieves performance comparable\nto state-of-the-art methods while maintaining significantly lower complexity\nand reduced training time. Also, our experiments show that TEFN exhibits high\nrobustness, with minimal error fluctuations during hyperparameter selection.\nFurthermore, due to the fact that BPA is derived from fuzzy theory, TEFN offers\na high degree of interpretability. Therefore, the proposed TEFN balances\naccuracy, efficiency, stability, and interpretability, making it a desirable\nsolution for time series forecasting.",
      "tldr_zh": "本研究提出了一种新型骨干架构Time Evidence Fusion Network (TEFN)，旨在从信息融合视角提升长期时间序列预测的准确性和效率。TEFN引入Basic Probability Assignment (BPA) Module，基于证据理论捕捉多变量时间序列数据的通道和时间维度不确定性，并开发多源信息融合方法来整合这些维度，从而提高预测性能。通过广泛实验，TEFN展现出与最先进方法相当的准确率，同时显著降低复杂度、缩短训练时间，并表现出高鲁棒性和可解释性，使其成为时间序列预测的理想解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06419v3",
      "published_date": "2024-05-10 12:10:22 UTC",
      "updated_date": "2024-09-24 12:57:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:34:41.580296"
    },
    {
      "arxiv_id": "2405.06418v2",
      "title": "PAC-Bayesian Generalization Bounds for Knowledge Graph Representation Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Jaejun Lee",
        "Minsung Hwang",
        "Joyce Jiyoung Whang"
      ],
      "abstract": "While a number of knowledge graph representation learning (KGRL) methods have\nbeen proposed over the past decade, very few theoretical analyses have been\nconducted on them. In this paper, we present the first PAC-Bayesian\ngeneralization bounds for KGRL methods. To analyze a broad class of KGRL\nmodels, we propose a generic framework named ReED (Relation-aware\nEncoder-Decoder), which consists of a relation-aware message passing encoder\nand a triplet classification decoder. Our ReED framework can express at least\n15 different existing KGRL models, including not only graph neural\nnetwork-based models such as R-GCN and CompGCN but also shallow-architecture\nmodels such as RotatE and ANALOGY. Our generalization bounds for the ReED\nframework provide theoretical grounds for the commonly used tricks in KGRL,\ne.g., parameter-sharing and weight normalization schemes, and guide desirable\ndesign choices for practical KGRL methods. We empirically show that the\ncritical factors in our generalization bounds can explain actual generalization\nerrors on three real-world knowledge graphs.",
      "tldr_zh": "本论文首次为知识图表示学习(KGRL)方法提供了PAC-Bayesian广义化边界，填补了该领域的理论分析空白。作者提出了一种通用框架ReED，包括relation-aware message passing encoder和triplet classification decoder，能够表达至少15种现有KGRL模型，如R-GCN、CompGCN、RotatE和ANALOGY。这些边界为KGRL中的常见技巧（如参数共享和权重归一化）提供了理论支持，并通过在三个真实知识图上的实验，证明了边界中的关键因素能有效解释实际广义化错误。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "32 pages, 3 figures, 4 tables, The 41st International Conference on\n  Machine Learning (ICML 2024)",
      "pdf_url": "http://arxiv.org/pdf/2405.06418v2",
      "published_date": "2024-05-10 12:03:53 UTC",
      "updated_date": "2024-06-03 14:27:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:34:53.429911"
    },
    {
      "arxiv_id": "2405.06413v1",
      "title": "Multi-level Personalized Federated Learning on Heterogeneous and Long-Tailed Data",
      "title_zh": "翻译失败",
      "authors": [
        "Rongyu Zhang",
        "Yun Chen",
        "Chenrui Wu",
        "Fangxin Wang",
        "Bo Li"
      ],
      "abstract": "Federated learning (FL) offers a privacy-centric distributed learning\nframework, enabling model training on individual clients and central\naggregation without necessitating data exchange. Nonetheless, FL\nimplementations often suffer from non-i.i.d. and long-tailed class\ndistributions across mobile applications, e.g., autonomous vehicles, which\nleads models to overfitting as local training may converge to sub-optimal. In\nour study, we explore the impact of data heterogeneity on model bias and\nintroduce an innovative personalized FL framework, Multi-level Personalized\nFederated Learning (MuPFL), which leverages the hierarchical architecture of FL\nto fully harness computational resources at various levels. This framework\nintegrates three pivotal modules: Biased Activation Value Dropout (BAVD) to\nmitigate overfitting and accelerate training; Adaptive Cluster-based Model\nUpdate (ACMU) to refine local models ensuring coherent global aggregation; and\nPrior Knowledge-assisted Classifier Fine-tuning (PKCF) to bolster\nclassification and personalize models in accord with skewed local data with\nshared knowledge. Extensive experiments on diverse real-world datasets for\nimage classification and semantic segmentation validate that MuPFL consistently\noutperforms state-of-the-art baselines, even under extreme non-i.i.d. and\nlong-tail conditions, which enhances accuracy by as much as 7.39% and\naccelerates training by up to 80% at most, marking significant advancements in\nboth efficiency and effectiveness.",
      "tldr_zh": "该研究针对联邦学习（Federated Learning, FL）在非独立同分布（non-i.i.d.）和长尾数据下的过拟合问题，提出了一种创新框架Multi-level Personalized Federated Learning (MuPFL)，利用FL的层次化架构来优化模型训练。MuPFL整合了三个关键模块：Biased Activation Value Dropout (BAVD)用于缓解过拟合并加速训练、Adaptive Cluster-based Model Update (ACMU)用于优化本地模型以确保全局聚合一致性，以及Prior Knowledge-assisted Classifier Fine-tuning (PKCF)用于增强分类性能并根据局部数据个性化模型。实验在图像分类和语义分割的真实数据集上显示，MuPFL在极端非-i.i.d.和长尾条件下，比现有基线提升准确率高达7.39%，并将训练速度提高最多80%。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "14 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.06413v1",
      "published_date": "2024-05-10 11:52:53 UTC",
      "updated_date": "2024-05-10 11:52:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:35:05.603845"
    },
    {
      "arxiv_id": "2405.06409v1",
      "title": "Visualizing Neural Network Imagination",
      "title_zh": "可视化神经网络的想象",
      "authors": [
        "Nevan Wichers",
        "Victor Tao",
        "Riccardo Volpato",
        "Fazl Barez"
      ],
      "abstract": "In certain situations, neural networks will represent environment states in\ntheir hidden activations. Our goal is to visualize what environment states the\nnetworks are representing. We experiment with a recurrent neural network (RNN)\narchitecture with a decoder network at the end. After training, we apply the\ndecoder to the intermediate representations of the network to visualize what\nthey represent. We define a quantitative interpretability metric and use it to\ndemonstrate that hidden states can be highly interpretable on a simple task. We\nalso develop autoencoder and adversarial techniques and show that benefit\ninterpretability.",
      "tldr_zh": "本研究旨在可视化神经网络在隐藏激活中表示的环境状态，通过一个带有解码器的循环神经网络(RNN)架构来实现。研究人员在训练后应用解码器对中间表示进行可视化，并定义了一个定量的可解释性指标，以证明隐藏状态在简单任务上高度可解释。此外，他们开发了自编码器(autoencoder)和对抗技术(adversarial techniques)，展示了这些方法如何提升神经网络的可解释性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06409v1",
      "published_date": "2024-05-10 11:43:35 UTC",
      "updated_date": "2024-05-10 11:43:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:35:16.871686"
    },
    {
      "arxiv_id": "2405.06399v1",
      "title": "Program Synthesis using Inductive Logic Programming for the Abstraction and Reasoning Corpus",
      "title_zh": "翻译失败",
      "authors": [
        "Filipe Marinho Rocha",
        "Inês Dutra",
        "Vítor Santos Costa"
      ],
      "abstract": "The Abstraction and Reasoning Corpus (ARC) is a general artificial\nintelligence benchmark that is currently unsolvable by any Machine Learning\nmethod, including Large Language Models (LLMs). It demands strong\ngeneralization and reasoning capabilities which are known to be weaknesses of\nNeural Network based systems. In this work, we propose a Program Synthesis\nsystem that uses Inductive Logic Programming (ILP), a branch of Symbolic AI, to\nsolve ARC. We have manually defined a simple Domain Specific Language (DSL)\nthat corresponds to a small set of object-centric abstractions relevant to ARC.\nThis is the Background Knowledge used by ILP to create Logic Programs that\nprovide reasoning capabilities to our system. The full system is capable of\ngeneralize to unseen tasks, since ILP can create Logic Program(s) from few\nexamples, in the case of ARC: pairs of Input-Output grids examples for each\ntask. These Logic Programs are able to generate Objects present in the Output\ngrid and the combination of these can form a complete program that transforms\nan Input grid into an Output grid. We randomly chose some tasks from ARC that\ndont require more than the small number of the Object primitives we implemented\nand show that given only these, our system can solve tasks that require each,\nsuch different reasoning.",
      "tldr_zh": "这篇论文提出了一种使用 Inductive Logic Programming (ILP) 的程序合成系统，来解决 Abstraction and Reasoning Corpus (ARC) 基准，该基准要求强泛化和推理能力，目前无法被 Large Language Models (LLMs) 或其他机器学习方法攻克。研究者手动定义了一个简单的 Domain Specific Language (DSL)，作为背景知识，用于创建逻辑程序，这些程序能从少量输入-输出网格例子中生成对象并实现任务转换。该系统展示了良好的泛化能力，并在实验中成功解决某些随机选择的 ARC 任务，这些任务涉及不同推理类型，从而证明了 ILP 在符号 AI 领域的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.PL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06399v1",
      "published_date": "2024-05-10 11:22:31 UTC",
      "updated_date": "2024-05-10 11:22:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:35:31.526872"
    },
    {
      "arxiv_id": "2405.06394v3",
      "title": "Memory Mosaics",
      "title_zh": "记忆镶嵌",
      "authors": [
        "Jianyu Zhang",
        "Niklas Nolte",
        "Ranajoy Sadhukhan",
        "Beidi Chen",
        "Léon Bottou"
      ],
      "abstract": "Memory Mosaics are networks of associative memories working in concert to\nachieve a prediction task of interest. Like transformers, memory mosaics\npossess compositional capabilities and in-context learning capabilities. Unlike\ntransformers, memory mosaics achieve these capabilities in comparatively\ntransparent way (\"predictive disentanglement\"). We illustrate these\ncapabilities on a toy example and also show that memory mosaics perform as well\nor better than transformers on medium-scale language modeling tasks.",
      "tldr_zh": "本研究引入了Memory Mosaics，一种由关联记忆网络组成的系统，用于协同完成预测任务。该框架具备与transformers类似的能力，包括组合能力(in-context learning)，但通过更透明的方式实现，即“predictive disentanglement”。与transformers相比，Memory Mosaics在玩具示例中展示了其优势，并在中等规模的语言建模任务上表现出与或优于transformers的性能，为更可解释的AI模型提供了新途径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06394v3",
      "published_date": "2024-05-10 11:08:20 UTC",
      "updated_date": "2025-02-27 21:46:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:35:39.755312"
    },
    {
      "arxiv_id": "2405.06389v1",
      "title": "Continual Novel Class Discovery via Feature Enhancement and Adaptation",
      "title_zh": "翻译失败",
      "authors": [
        "Yifan Yu",
        "Shaokun Wang",
        "Yuhang He",
        "Junzhe Chen",
        "Yihong Gong"
      ],
      "abstract": "Continual Novel Class Discovery (CNCD) aims to continually discover novel\nclasses without labels while maintaining the recognition capability for\npreviously learned classes. The main challenges faced by CNCD include the\nfeature-discrepancy problem, the inter-session confusion problem, etc. In this\npaper, we propose a novel Feature Enhancement and Adaptation method for the\nCNCD to tackle the above challenges, which consists of a guide-to-novel\nframework, a centroid-to-samples similarity constraint (CSS), and a\nboundary-aware prototype constraint (BAP). More specifically, the\nguide-to-novel framework is established to continually discover novel classes\nunder the guidance of prior distribution. Afterward, the CSS is designed to\nconstrain the relationship between centroid-to-samples similarities of\ndifferent classes, thereby enhancing the distinctiveness of features among\nnovel classes. Finally, the BAP is proposed to keep novel class features aware\nof the positions of other class prototypes during incremental sessions, and\nbetter adapt novel class features to the shared feature space. Experimental\nresults on three benchmark datasets demonstrate the superiority of our method,\nespecially in more challenging protocols with more incremental sessions.",
      "tldr_zh": "本论文针对Continual Novel Class Discovery (CNCD)问题，旨在无标签情况下持续发现新类别，同时保持对先前类别的识别能力，并解决feature-discrepancy problem和inter-session confusion problem等挑战。作者提出了一种Feature Enhancement and Adaptation方法，包括guide-to-novel framework用于在先验分布指导下持续发现新类、centroid-to-samples similarity constraint (CSS)以增强新类特征的独特性，以及boundary-aware prototype constraint (BAP)来让新类特征更好地适应共享空间并避免混淆。实验在三个基准数据集上验证了方法的优越性，尤其在涉及更多增量会话的复杂协议中，展示了显著的性能提升。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06389v1",
      "published_date": "2024-05-10 10:52:22 UTC",
      "updated_date": "2024-05-10 10:52:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:35:54.187528"
    },
    {
      "arxiv_id": "2405.06373v4",
      "title": "LLM Discussion: Enhancing the Creativity of Large Language Models via Discussion Framework and Role-Play",
      "title_zh": "LLM Discussion：通过讨论框架和角色扮演提升大型语言模型的创造力",
      "authors": [
        "Li-Chun Lu",
        "Shou-Jen Chen",
        "Tsung-Min Pai",
        "Chan-Hung Yu",
        "Hung-yi Lee",
        "Shao-Hua Sun"
      ],
      "abstract": "Large language models (LLMs) have shown exceptional proficiency in natural\nlanguage processing but often fall short of generating creative and original\nresponses to open-ended questions. To enhance LLM creativity, our key insight\nis to emulate the human process of inducing collective creativity through\nengaging discussions with participants from diverse backgrounds and\nperspectives. To this end, we propose LLM Discussion, a three-phase discussion\nframework that facilitates vigorous and diverging idea exchanges and ensures\nconvergence to creative answers. Moreover, we adopt a role-playing technique by\nassigning distinct roles to LLMs to combat the homogeneity of LLMs. We evaluate\nthe efficacy of the proposed framework with the Alternative Uses Test,\nSimilarities Test, Instances Test, and Scientific Creativity Test through both\nLLM evaluation and human study. The results show that our proposed framework\noutperforms single-LLM approaches and existing multi-LLM frameworks across\nvarious creativity metrics. The code is available at\nhttps://github.com/lawraa/LLM-Discussion.",
      "tldr_zh": "该研究发现，大型语言模型 (LLMs) 在处理开放性问题时往往缺乏创造性和原创性响应，因此提出 LLM Discussion 框架，通过模拟人类多样化讨论来提升 LLMs 的创造力。该框架包括三个阶段：促进活跃发散的想法交换，并确保最终收敛到创新答案，同时采用角色扮演技术为 LLMs 分配不同角色，以减少同质性。实验使用 Alternative Uses Test、Similarities Test、Instances Test 和 Scientific Creativity Test 等评估方法，结果显示该框架在各种创造力指标上优于单 LLM 方法和现有多 LLM 框架。代码已在 https://github.com/lawraa/LLM-Discussion 公开。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "40 pages, 9 figures, COLM 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.06373v4",
      "published_date": "2024-05-10 10:19:14 UTC",
      "updated_date": "2024-08-08 04:47:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:36:07.185006"
    },
    {
      "arxiv_id": "2405.06372v1",
      "title": "Intelligent Duty Cycling Management and Wake-up for Energy Harvesting IoT Networks with Correlated Activity",
      "title_zh": "翻译失败",
      "authors": [
        "David E. Ruíz-Guirola",
        "Onel L. A. López",
        "Samuel Montejo-Sánchez",
        "Israel Leyva Mayorga",
        "Zhu Han",
        "Petar Popovski"
      ],
      "abstract": "This paper presents an approach for energy-neutral Internet of Things (IoT)\nscenarios where the IoT devices (IoTDs) rely entirely on their energy\nharvesting capabilities to sustain operation. We use a Markov chain to\nrepresent the operation and transmission states of the IoTDs, a modulated\nPoisson process to model their energy harvesting process, and a discrete-time\nMarkov chain to model their battery state. The aim is to efficiently manage the\nduty cycling of the IoTDs, so as to prolong their battery life and reduce\ninstances of low-energy availability. We propose a duty-cycling management\nbased on K- nearest neighbors, aiming to strike a trade-off between energy\nefficiency and detection accuracy. This is done by incorporating spatial and\ntemporal correlations among IoTDs' activity, as well as their energy harvesting\ncapabilities. We also allow the base station to wake up specific IoTDs if more\ninformation about an event is needed upon initial detection. Our proposed\nscheme shows significant improvements in energy savings and performance, with\nup to 11 times lower misdetection probability and 50\\% lower energy consumption\nfor high-density scenarios compared to a random duty cycling benchmark.",
      "tldr_zh": "这篇论文针对依赖能量收集的IoT网络，提出了一种智能职责周期管理和唤醒机制，以实现能量中性操作并处理设备活动中的空间和时间相关性。方法使用Markov chain模型设备操作和传输状态、调制Poisson process模型能量收集过程，以及离散时间Markov chain模型电池状态；同时，引入基于K-Nearest Neighbors的职责周期管理，以平衡能量效率和检测准确性。实验结果显示，该方案在高密度场景下，与随机职责周期基准相比，错误检测概率降低11倍，能量消耗降低50%，显著提升了IoT设备的性能和寿命。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.SY"
      ],
      "primary_category": "eess.SY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06372v1",
      "published_date": "2024-05-10 10:16:27 UTC",
      "updated_date": "2024-05-10 10:16:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:36:18.288014"
    },
    {
      "arxiv_id": "2405.06363v1",
      "title": "Projection by Convolution: Optimal Sample Complexity for Reinforcement Learning in Continuous-Space MDPs",
      "title_zh": "翻译失败",
      "authors": [
        "Davide Maran",
        "Alberto Maria Metelli",
        "Matteo Papini",
        "Marcello Restelli"
      ],
      "abstract": "We consider the problem of learning an $\\varepsilon$-optimal policy in a\ngeneral class of continuous-space Markov decision processes (MDPs) having\nsmooth Bellman operators. Given access to a generative model, we achieve\nrate-optimal sample complexity by performing a simple, \\emph{perturbed} version\nof least-squares value iteration with orthogonal trigonometric polynomials as\nfeatures. Key to our solution is a novel projection technique based on ideas\nfrom harmonic analysis. Our~$\\widetilde{\\mathcal{O}}(\\epsilon^{-2-d/(\\nu+1)})$\nsample complexity, where $d$ is the dimension of the state-action space and\n$\\nu$ the order of smoothness, recovers the state-of-the-art result of\ndiscretization approaches for the special case of Lipschitz MDPs $(\\nu=0)$. At\nthe same time, for $\\nu\\to\\infty$, it recovers and greatly generalizes the\n$\\mathcal{O}(\\epsilon^{-2})$ rate of low-rank MDPs, which are more amenable to\nregression approaches. In this sense, our result bridges the gap between two\npopular but conflicting perspectives on continuous-space MDPs.",
      "tldr_zh": "本论文探讨了在具有平滑Bellman算子的连续空间Markov决策过程(MDPs)中，通过一种基于卷积投影(Projection by Convolution)的新技术，实现强化学习的ε-最优策略学习。方法采用perturbed least-squares value iteration，并使用orthogonal trigonometric polynomials作为特征，从而达到最优样本复杂度\\(\\widetilde{\\mathcal{O}}(\\epsilon^{-2-d/(\\nu+1)})\\)，其中d是状态-动作空间的维度，ν是平滑度。该结果不仅在Lipschitz MDPs（ν=0）中与离散化方法相当，还泛化了低秩MDPs的\\(\\mathcal{O}(\\epsilon^{-2})\\)率，并桥接了两种MDPs视角的差距。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06363v1",
      "published_date": "2024-05-10 09:58:47 UTC",
      "updated_date": "2024-05-10 09:58:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:36:32.315969"
    },
    {
      "arxiv_id": "2405.06724v3",
      "title": "Boolean matrix logic programming for active learning of gene functions in genome-scale metabolic network models",
      "title_zh": "布尔矩阵逻辑编程用于全基因组规模代谢网络模型中基因功能的主动学习",
      "authors": [
        "Lun Ai",
        "Stephen H. Muggleton",
        "Shi-Shun Liang",
        "Geoff S. Baldwin"
      ],
      "abstract": "Techniques to autonomously drive research have been prominent in\nComputational Scientific Discovery, while Synthetic Biology is a field of\nscience that focuses on designing and constructing new biological systems for\nuseful purposes. Here we seek to apply logic-based machine learning techniques\nto facilitate cellular engineering and drive biological discovery.\nComprehensive databases of metabolic processes called genome-scale metabolic\nnetwork models (GEMs) are often used to evaluate cellular engineering\nstrategies to optimise target compound production. However, predicted host\nbehaviours are not always correctly described by GEMs, often due to errors in\nthe models. The task of learning the intricate genetic interactions within GEMs\npresents computational and empirical challenges. To address these, we describe\na novel approach called Boolean Matrix Logic Programming (BMLP) by leveraging\nboolean matrices to evaluate large logic programs. We introduce a new system,\n$BMLP_{active}$, which efficiently explores the genomic hypothesis space by\nguiding informative experimentation through active learning. In contrast to\nsub-symbolic methods, $BMLP_{active}$ encodes a state-of-the-art GEM of a\nwidely accepted bacterial host in an interpretable and logical representation\nusing datalog logic programs. Notably, $BMLP_{active}$ can successfully learn\nthe interaction between a gene pair with fewer training examples than random\nexperimentation, overcoming the increase in experimental design space.\n$BMLP_{active}$ enables rapid optimisation of metabolic models to reliably\nengineer biological systems for producing useful compounds. It offers a\nrealistic approach to creating a self-driving lab for microbial engineering.",
      "tldr_zh": "本论文提出 Boolean Matrix Logic Programming (BMLP) 方法，用于在 genome-scale metabolic network models (GEMs) 中主动学习基因功能，以解决模型错误导致的预测不准问题。$BMLP_{active}$ 系统通过 active learning 引导信息实验，并使用 datalog logic programs 将 GEMs 编码为可解释的逻辑表示，从而高效探索基因组假设空间。相比随机实验，$BMLP_{active}$ 需要更少的训练示例即可成功学习基因对交互，并优化代谢模型。最终，该方法为创建自驱动实验室（self-driving lab）以工程微生物生产有用化合物提供了可行途径。",
      "categories": [
        "q-bio.MN",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.MN",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06724v3",
      "published_date": "2024-05-10 09:51:06 UTC",
      "updated_date": "2024-08-11 17:54:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:36:47.254710"
    },
    {
      "arxiv_id": "2405.06354v1",
      "title": "KeepOriginalAugment: Single Image-based Better Information-Preserving Data Augmentation Approach",
      "title_zh": "KeepOriginalAugment：基于单张图像的、更好地保留信息的数据增强方法",
      "authors": [
        "Teerath Kumar",
        "Alessandra Mileo",
        "Malika Bendechache"
      ],
      "abstract": "Advanced image data augmentation techniques play a pivotal role in enhancing\nthe training of models for diverse computer vision tasks. Notably, SalfMix and\nKeepAugment have emerged as popular strategies, showcasing their efficacy in\nboosting model performance. However, SalfMix reliance on duplicating salient\nfeatures poses a risk of overfitting, potentially compromising the model's\ngeneralization capabilities. Conversely, KeepAugment, which selectively\npreserves salient regions and augments non-salient ones, introduces a domain\nshift that hinders the exchange of crucial contextual information, impeding\noverall model understanding. In response to these challenges, we introduce\nKeepOriginalAugment, a novel data augmentation approach. This method\nintelligently incorporates the most salient region within the non-salient area,\nallowing augmentation to be applied to either region. Striking a balance\nbetween data diversity and information preservation, KeepOriginalAugment\nenables models to leverage both diverse salient and non-salient regions,\nleading to enhanced performance. We explore three strategies for determining\nthe placement of the salient region minimum, maximum, or random and investigate\nswapping perspective strategies to decide which part (salient or non-salient)\nundergoes augmentation. Our experimental evaluations, conducted on\nclassification datasets such as CIFAR-10, CIFAR-100, and TinyImageNet,\ndemonstrate the superior performance of KeepOriginalAugment compared to\nexisting state-of-the-art techniques.",
      "tldr_zh": "该论文针对图像数据增强技术的局限性（如SalfMix的过拟合风险和KeepAugment的领域偏移问题），提出了一种新方法KeepOriginalAugment。该方法通过将最显著区域智能整合到非显著区域中，并允许对任一区域应用增强，实现了数据多样性与信息保留的平衡。论文探索了三种放置显著区域的策略（最小、最大或随机）以及交换视角策略，并在CIFAR-10、CIFAR-100和TinyImageNet数据集上的实验中，展示了KeepOriginalAugment比现有最先进技术具有更优的性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This paper has been accepted at 20th International Conference on\n  Artificial Intelligence Applications and Innovations 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.06354v1",
      "published_date": "2024-05-10 09:37:36 UTC",
      "updated_date": "2024-05-10 09:37:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:36:57.667262"
    },
    {
      "arxiv_id": "2405.06329v1",
      "title": "ChatGPTest: opportunities and cautionary tales of utilizing AI for questionnaire pretesting",
      "title_zh": "ChatGP",
      "authors": [
        "Francisco Olivos",
        "Minhui Liu"
      ],
      "abstract": "The rapid advancements in generative artificial intelligence have opened up\nnew avenues for enhancing various aspects of research, including the design and\nevaluation of survey questionnaires. However, the recent pioneering\napplications have not considered questionnaire pretesting. This article\nexplores the use of GPT models as a useful tool for pretesting survey\nquestionnaires, particularly in the early stages of survey design. Illustrated\nwith two applications, the article suggests incorporating GPT feedback as an\nadditional stage before human pretesting, potentially reducing successive\niterations. The article also emphasizes the indispensable role of researchers'\njudgment in interpreting and implementing AI-generated feedback.",
      "tldr_zh": "这篇论文探讨了利用 GPT 模型进行问卷预测试(questionnaire pretesting)的机会和潜在风险，强调生成式 AI 在调查问卷设计和评估中的新应用。作者通过两个实际应用示例，建议在人类预测试前添加一个基于 GPT 反馈的阶段，以减少后续迭代次数。论文同时提醒，研究人员在解释和实施 AI 生成反馈时，必须发挥不可或缺的判断力，以确保结果的可靠性和准确性。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "11 pages, 2 Figures",
      "pdf_url": "http://arxiv.org/pdf/2405.06329v1",
      "published_date": "2024-05-10 09:01:14 UTC",
      "updated_date": "2024-05-10 09:01:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:37:08.495891"
    },
    {
      "arxiv_id": "2405.06321v2",
      "title": "Correlation Dimension of Natural Language in a Statistical Manifold",
      "title_zh": "翻译失败",
      "authors": [
        "Xin Du",
        "Kumiko Tanaka-Ishii"
      ],
      "abstract": "The correlation dimension of natural language is measured by applying the\nGrassberger-Procaccia algorithm to high-dimensional sequences produced by a\nlarge-scale language model. This method, previously studied only in a Euclidean\nspace, is reformulated in a statistical manifold via the Fisher-Rao distance.\nLanguage exhibits a multifractal, with global self-similarity and a universal\ndimension around 6.5, which is smaller than those of simple discrete random\nsequences and larger than that of a Barab\\'asi-Albert process. Long memory is\nthe key to producing self-similarity. Our method is applicable to any\nprobabilistic model of real-world discrete sequences, and we show an\napplication to music data.",
      "tldr_zh": "本文在统计流形(statistical manifold)中，使用 Grassberger-Procaccia 算法和 Fisher-Rao 距离，测量大型语言模型产生的高维序列的自然语言相关维度(correlation dimension)。研究发现，语言表现出多重分形(multifractal)特性，具有全局自相似性(self-similarity)，且普遍维度约为 6.5，比简单离散随机序列小但比 Barabási-Albert 过程大。长记忆(long memory)是产生自相似性的关键，该方法可扩展到其他真实世界离散序列的概率模型，并展示了应用于音乐数据的实际应用。",
      "categories": [
        "cs.CL",
        "cond-mat.stat-mech",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Published at Physical Review Research",
      "pdf_url": "http://arxiv.org/pdf/2405.06321v2",
      "published_date": "2024-05-10 08:48:03 UTC",
      "updated_date": "2024-05-15 07:46:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:37:21.217254"
    },
    {
      "arxiv_id": "2405.06301v1",
      "title": "Learning from String Sequences",
      "title_zh": "翻译失败",
      "authors": [
        "David Lindsay",
        "Sian Lindsay"
      ],
      "abstract": "The Universal Similarity Metric (USM) has been demonstrated to give\npractically useful measures of \"similarity\" between sequence data. Here we have\nused the USM as an alternative distance metric in a K-Nearest Neighbours (K-NN)\nlearner to allow effective pattern recognition of variable length sequence\ndata. We compare this USM approach with the commonly used string-to-word vector\napproach. Our experiments have used two data sets of divergent domains: (1)\nspam email filtering and (2) protein subcellular localization. Our results with\nthis data reveal that the USM-based K-NN learner (1) gives predictions with\nhigher classification accuracy than those output by techniques that use the\nstring-to-word vector approach, and (2) can be used to generate reliable\nprobability forecasts.",
      "tldr_zh": "本研究提出使用 Universal Similarity Metric (USM) 作为距离度量，应用于 K-Nearest Neighbours (K-NN) 学习器，以实现对可变长度序列数据的有效模式识别。相比于常见的 string-to-word vector 方法，实验在垃圾邮件过滤和蛋白质亚细胞定位两个不同领域的数据集上表明，USM-based K-NN 学习器实现了更高的分类准确率。不仅如此，该方法还能生成可靠的概率预测，为序列数据学习提供了更高效的替代方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 1 figure, 4 tables, Technical Report",
      "pdf_url": "http://arxiv.org/pdf/2405.06301v1",
      "published_date": "2024-05-10 08:09:53 UTC",
      "updated_date": "2024-05-10 08:09:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:37:32.404042"
    },
    {
      "arxiv_id": "2405.06299v1",
      "title": "Cross-domain Learning Framework for Tracking Users in RIS-aided Multi-band ISAC Systems with Sparse Labeled Data",
      "title_zh": "翻译失败",
      "authors": [
        "Jingzhi Hu",
        "Dusit Niyato",
        "Jun Luo"
      ],
      "abstract": "Integrated sensing and communications (ISAC) is pivotal for 6G communications\nand is boosted by the rapid development of reconfigurable intelligent surfaces\n(RISs). Using the channel state information (CSI) across multiple frequency\nbands, RIS-aided multi-band ISAC systems can potentially track users' positions\nwith high precision. Though tracking with CSI is desirable as no communication\noverheads are incurred, it faces challenges due to the multi-modalities of CSI\nsamples, irregular and asynchronous data traffic, and sparse labeled data for\nlearning the tracking function. This paper proposes the X2Track framework,\nwhere we model the tracking function by a hierarchical architecture, jointly\nutilizing multi-modal CSI indicators across multiple bands, and optimize it in\na cross-domain manner, tackling the sparsity of labeled data for the target\ndeployment environment (namely, target domain) by adapting the knowledge\nlearned from another environment (namely, source domain). Under X2Track, we\ndesign an efficient deep learning algorithm to minimize tracking errors, based\non transformer neural networks and adversarial learning techniques. Simulation\nresults verify that X2Track achieves decimeter-level axial tracking errors even\nunder scarce UL data traffic and strong interference conditions and can adapt\nto diverse deployment environments with fewer than 5% training data, or\nequivalently, 5 minutes of UE tracks, being labeled.",
      "tldr_zh": "这篇论文提出 X2Track 框架，用于在 RIS-aided 多频带 ISAC 系统下跟踪用户位置，解决 CSI 多模态、数据不规则和稀疏标注数据的问题。框架采用分层架构和跨域学习方法，结合多模态 CSI 指标，从源域知识适应目标域，并利用 Transformer 神经网络和对抗学习技术最小化跟踪错误。模拟结果显示，X2Track 即使在稀疏上行数据和强干扰条件下也能实现分米级跟踪精度，并在少于 5% 的训练数据（相当于 5 分钟的 UE 轨迹）下快速适应不同部署环境。",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06299v1",
      "published_date": "2024-05-10 08:04:27 UTC",
      "updated_date": "2024-05-10 08:04:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:37:45.494944"
    },
    {
      "arxiv_id": "2405.06296v1",
      "title": "Fast Evaluation of DNN for Past Dataset in Incremental Learning",
      "title_zh": "增量学习中 DNN 对过去数据集的快速评估",
      "authors": [
        "Naoto Sato"
      ],
      "abstract": "During the operation of a system including a deep neural network (DNN), new\ninput values that were not included in the training dataset are given to the\nDNN. In such a case, the DNN may be incrementally trained with the new input\nvalues; however, that training may reduce the accuracy of the DNN in regard to\nthe dataset that was previously obtained and used for the past training. It is\nnecessary to evaluate the effect of the additional training on the accuracy for\nthe past dataset. However, evaluation by testing all the input values included\nin the past dataset takes time. Therefore, we propose a new method to quickly\nevaluate the effect on the accuracy for the past dataset. In the proposed\nmethod, the gradient of the parameter values (such as weight and bias) for the\npast dataset is extracted by running the DNN before the training. Then, after\nthe training, its effect on the accuracy with respect to the past dataset is\ncalculated from the gradient and update differences of the parameter values. To\nshow the usefulness of the proposed method, we present experimental results\nwith several datasets. The results show that the proposed method can estimate\nthe accuracy change by additional training in a constant time.",
      "tldr_zh": "这篇论文针对增量学习中深度神经网络(DNN)额外训练可能降低过去数据集准确性的问题，提出了一种快速评估方法。该方法在训练前提取过去数据集的参数梯度（如权重和偏差），然后通过训练后的梯度和参数更新差异来计算对准确性的影响，避免了测试所有过去数据的耗时过程。实验结果显示，该方法能以恒定时间高效估计准确性变化，在多个数据集上验证了其实用性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06296v1",
      "published_date": "2024-05-10 07:55:08 UTC",
      "updated_date": "2024-05-10 07:55:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:37:56.188185"
    },
    {
      "arxiv_id": "2405.06295v1",
      "title": "Aspect-oriented Consumer Health Answer Summarization",
      "title_zh": "翻译失败",
      "authors": [
        "Rochana Chaturvedi",
        "Abari Bhattacharya",
        "Shweta Yadav"
      ],
      "abstract": "Community Question-Answering (CQA) forums have revolutionized how people seek\ninformation, especially those related to their healthcare needs, placing their\ntrust in the collective wisdom of the public. However, there can be several\nanswers in response to a single query, which makes it hard to grasp the key\ninformation related to the specific health concern. Typically, CQA forums\nfeature a single top-voted answer as a representative summary for each query.\nHowever, a single answer overlooks the alternative solutions and other\ninformation frequently offered in other responses. Our research focuses on\naspect-based summarization of health answers to address this limitation.\nSummarization of responses under different aspects such as suggestions,\ninformation, personal experiences, and questions can enhance the usability of\nthe platforms. We formalize a multi-stage annotation guideline and contribute a\nunique dataset comprising aspect-based human-written health answer summaries.\nWe build an automated multi-faceted answer summarization pipeline with this\ndataset based on task-specific fine-tuning of several state-of-the-art models.\nThe pipeline leverages question similarity to retrieve relevant answer\nsentences, subsequently classifying them into the appropriate aspect type.\nFollowing this, we employ several recent abstractive summarization models to\ngenerate aspect-based summaries. Finally, we present a comprehensive human\nanalysis and find that our summaries rank high in capturing relevant content\nand a wide range of solutions.",
      "tldr_zh": "该研究针对社区问答 (CQA) 论坛中健康查询的多个答案问题，提出了一种基于方面的消费者健康答案摘要方法，以提升信息可用性。研究者制定了多阶段注解指南，并构建了一个独特数据集，包含人类编写的基于方面的健康答案摘要，如 suggestions、information、personal experiences 和 questions。摘要管道通过任务特定微调的先进模型，首先利用问题相似度检索和分类相关答案句子，然后采用抽象摘要模型生成多方面摘要。人类分析显示，该方法在捕捉相关内容和提供广泛解决方案方面表现出色，显著改善了 CQA 论坛的实用性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "H.4.3; I.2.7; J.3; J.7; K.6.4"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06295v1",
      "published_date": "2024-05-10 07:52:43 UTC",
      "updated_date": "2024-05-10 07:52:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:38:08.427124"
    },
    {
      "arxiv_id": "2405.06289v3",
      "title": "Look Once to Hear: Target Speech Hearing with Noisy Examples",
      "title_zh": "翻译失败",
      "authors": [
        "Bandhav Veluri",
        "Malek Itani",
        "Tuochao Chen",
        "Takuya Yoshioka",
        "Shyamnath Gollakota"
      ],
      "abstract": "In crowded settings, the human brain can focus on speech from a target\nspeaker, given prior knowledge of how they sound. We introduce a novel\nintelligent hearable system that achieves this capability, enabling target\nspeech hearing to ignore all interfering speech and noise, but the target\nspeaker. A naive approach is to require a clean speech example to enroll the\ntarget speaker. This is however not well aligned with the hearable application\ndomain since obtaining a clean example is challenging in real world scenarios,\ncreating a unique user interface problem. We present the first enrollment\ninterface where the wearer looks at the target speaker for a few seconds to\ncapture a single, short, highly noisy, binaural example of the target speaker.\nThis noisy example is used for enrollment and subsequent speech extraction in\nthe presence of interfering speakers and noise. Our system achieves a signal\nquality improvement of 7.01 dB using less than 5 seconds of noisy enrollment\naudio and can process 8 ms of audio chunks in 6.24 ms on an embedded CPU. Our\nuser studies demonstrate generalization to real-world static and mobile\nspeakers in previously unseen indoor and outdoor multipath environments.\nFinally, our enrollment interface for noisy examples does not cause performance\ndegradation compared to clean examples, while being convenient and\nuser-friendly. Taking a step back, this paper takes an important step towards\nenhancing the human auditory perception with artificial intelligence. We\nprovide code and data at: https://github.com/vb000/LookOnceToHear.",
      "tldr_zh": "该研究提出了一种名为“Look Once to Hear”的智能听觉系统，允许用户在嘈杂环境中专注于目标说话者（target speech hearing），只需通过短暂注视目标说话者捕获一个短的、嘈杂的双耳音频样本即可完成注册。系统利用这一noisy examples进行后续语音提取，忽略干扰声音和噪声，实现高效的实时处理，在嵌入式CPU上处理8 ms音频块仅需6.24 ms。实验结果显示，系统使用少于5秒的注册音频提高了7.01 dB的信号质量，并在用户研究中证明了其在真实室内外环境的泛化能力，与使用干净样本的性能相当，同时更方便用户友好。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Best paper honorable mention at CHI 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.06289v3",
      "published_date": "2024-05-10 07:44:18 UTC",
      "updated_date": "2024-05-29 19:00:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:38:21.076806"
    },
    {
      "arxiv_id": "2405.06270v3",
      "title": "XAI4LLM. Let Machine Learning Models and LLMs Collaborate for Enhanced In-Context Learning in Healthcare",
      "title_zh": "翻译失败",
      "authors": [
        "Fatemeh Nazary",
        "Yashar Deldjoo",
        "Tommaso Di Noia",
        "Eugenio di Sciascio"
      ],
      "abstract": "The integration of Large Language Models (LLMs) into healthcare diagnostics\noffers a promising avenue for clinical decision-making. This study outlines the\ndevelopment of a novel method for zero-shot/few-shot in-context learning (ICL)\nby integrating medical domain knowledge using a multi-layered structured\nprompt. We also explore the efficacy of two communication styles between the\nuser and LLMs: the Numerical Conversational (NC) style, which processes data\nincrementally, and the Natural Language Single-Turn (NL-ST) style, which\nemploys long narrative prompts.\n  Our study systematically evaluates the diagnostic accuracy and risk factors,\nincluding gender bias and false negative rates, using a dataset of 920 patient\nrecords in various few-shot scenarios. Results indicate that traditional\nclinical machine learning (ML) models generally outperform LLMs in zero-shot\nand few-shot settings. However, the performance gap narrows significantly when\nemploying few-shot examples alongside effective explainable AI (XAI) methods as\nsources of domain knowledge. Moreover, with sufficient time and an increased\nnumber of examples, the conversational style (NC) nearly matches the\nperformance of ML models. Most notably, LLMs demonstrate comparable or superior\ncost-sensitive accuracy relative to ML models.\n  This research confirms that, with appropriate domain knowledge and tailored\ncommunication strategies, LLMs can significantly enhance diagnostic processes.\nThe findings highlight the importance of optimizing the number of training\nexamples and communication styles to improve accuracy and reduce biases in LLM\napplications.",
      "tldr_zh": "本研究提出XAI4LLM方法，通过整合机器学习(ML)模型和Large Language Models (LLMs)，利用多层结构化提示增强医疗领域的零样本/少样本in-context learning (ICL)，并评估Numerical Conversational (NC)风格（增量处理数据）和Natural Language Single-Turn (NL-ST)风格（长叙述提示）的效能。实验使用920条患者记录数据集显示，传统ML模型在零样本和少样本场景中通常优于LLMs，但结合有效的explainable AI (XAI)方法和少样本示例后，性能差距显著缩小，且NC风格在增加示例时几乎能与ML模型匹敌。LLMs在成本敏感准确性方面表现出与ML模型相当或优越的表现，并能降低性别偏见和假阴性率。该研究强调，通过优化训练示例数量和通信策略，LLMs可显著提升医疗诊断的准确性和可靠性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06270v3",
      "published_date": "2024-05-10 06:52:44 UTC",
      "updated_date": "2024-06-03 16:23:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:38:34.992205"
    },
    {
      "arxiv_id": "2405.06266v1",
      "title": "A Multi-Channel Spatial-Temporal Transformer Model for Traffic Flow Forecasting",
      "title_zh": "多通道时空 Transformer 模型用于交通流量预测",
      "authors": [
        "Jianli Xiao",
        "Baichao Long"
      ],
      "abstract": "Traffic flow forecasting is a crucial task in transportation management and\nplanning. The main challenges for traffic flow forecasting are that (1) as the\nlength of prediction time increases, the accuracy of prediction will decrease;\n(2) the predicted results greatly rely on the extraction of temporal and\nspatial dependencies from the road networks. To overcome the challenges\nmentioned above, we propose a multi-channel spatial-temporal transformer model\nfor traffic flow forecasting, which improves the accuracy of the prediction by\nfusing results from different channels of traffic data. Our approach leverages\ngraph convolutional network to extract spatial features from each channel while\nusing a transformer-based architecture to capture temporal dependencies across\nchannels. We introduce an adaptive adjacency matrix to overcome limitations in\nfeature extraction from fixed topological structures. Experimental results on\nsix real-world datasets demonstrate that introducing a multi-channel mechanism\ninto the temporal model enhances performance and our proposed model outperforms\nstate-of-the-art models in terms of accuracy.",
      "tldr_zh": "该研究针对交通流量预测中的两大挑战——预测时间延长导致准确率下降，以及对路网空间和时间依赖性的提取依赖——提出了一种多通道空间-时间 Transformer 模型，通过融合不同通道的交通数据来提升预测精度。该模型利用 Graph Convolutional Network 提取每个通道的空间特征，并采用 Transformer 架构捕获跨通道的时间依赖性，同时引入 Adaptive Adjacency Matrix 来克服固定拓扑结构在特征提取中的局限性。在六个真实世界数据集上的实验结果表明，该模型的多通道机制显著提高了性能，并在准确性上优于现有最先进模型。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06266v1",
      "published_date": "2024-05-10 06:37:07 UTC",
      "updated_date": "2024-05-10 06:37:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:38:48.134230"
    },
    {
      "arxiv_id": "2405.06263v2",
      "title": "Learning Latent Dynamic Robust Representations for World Models",
      "title_zh": "翻译失败",
      "authors": [
        "Ruixiang Sun",
        "Hongyu Zang",
        "Xin Li",
        "Riashat Islam"
      ],
      "abstract": "Visual Model-Based Reinforcement Learning (MBRL) promises to encapsulate\nagent's knowledge about the underlying dynamics of the environment, enabling\nlearning a world model as a useful planner. However, top MBRL agents such as\nDreamer often struggle with visual pixel-based inputs in the presence of\nexogenous or irrelevant noise in the observation space, due to failure to\ncapture task-specific features while filtering out irrelevant spatio-temporal\ndetails. To tackle this problem, we apply a spatio-temporal masking strategy, a\nbisimulation principle, combined with latent reconstruction, to capture\nendogenous task-specific aspects of the environment for world models,\neffectively eliminating non-essential information. Joint training of\nrepresentations, dynamics, and policy often leads to instabilities. To further\naddress this issue, we develop a Hybrid Recurrent State-Space Model (HRSSM)\nstructure, enhancing state representation robustness for effective policy\nlearning. Our empirical evaluation demonstrates significant performance\nimprovements over existing methods in a range of visually complex control tasks\nsuch as Maniskill \\cite{gu2023maniskill2} with exogenous distractors from the\nMatterport environment. Our code is avaliable at\nhttps://github.com/bit1029public/HRSSM.",
      "tldr_zh": "这篇论文针对视觉模型驱动强化学习（MBRL）中的问题，提出了一种学习潜在动态鲁棒表示的方法，以帮助代理（如Dreamer）在存在外生噪声的视觉输入中捕捉任务特定特征，同时过滤无关细节。方法结合了spatio-temporal masking strategy、bisimulation principle和latent reconstruction技术，来构建世界模型并消除非必要信息；此外，引入Hybrid Recurrent State-Space Model (HRSSM)结构，以提升状态表示的鲁棒性和策略学习的稳定性。实验结果显示，该方法在视觉复杂控制任务如Maniskill中显著优于现有方法，证明了其在处理外生干扰时的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06263v2",
      "published_date": "2024-05-10 06:28:42 UTC",
      "updated_date": "2024-05-30 09:40:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:39:00.314972"
    },
    {
      "arxiv_id": "2405.06260v1",
      "title": "Precise Apple Detection and Localization in Orchards using YOLOv5 for Robotic Harvesting Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Jiang Ziyue",
        "Yin Bo",
        "Lu Boyun"
      ],
      "abstract": "The advancement of agricultural robotics holds immense promise for\ntransforming fruit harvesting practices, particularly within the apple\nindustry. The accurate detection and localization of fruits are pivotal for the\nsuccessful implementation of robotic harvesting systems. In this paper, we\npropose a novel approach to apple detection and position estimation utilizing\nan object detection model, YOLOv5. Our primary objective is to develop a robust\nsystem capable of identifying apples in complex orchard environments and\nproviding precise location information. To achieve this, we curated an\nautonomously labeled dataset comprising diverse apple tree images, which was\nutilized for both training and evaluation purposes. Through rigorous\nexperimentation, we compared the performance of our YOLOv5-based system with\nother popular object detection models, including SSD. Our results demonstrate\nthat the YOLOv5 model outperforms its counterparts, achieving an impressive\napple detection accuracy of approximately 85%. We believe that our proposed\nsystem's accurate apple detection and position estimation capabilities\nrepresent a significant advancement in agricultural robotics, laying the\ngroundwork for more efficient and sustainable fruit harvesting practices.",
      "tldr_zh": "这篇论文提出了一种使用 YOLOv5 对象检测模型来精确检测和定位果园中苹果的方法，旨在支持机器人采摘系统的应用。研究团队创建了一个自主标记的苹果树图像数据集，用于模型的训练和评估。通过实验比较，YOLOv5 比 SSD 等模型表现出色，实现了约 85% 的苹果检测准确率。该方法为农业机器人领域提供了重大进展，推动更高效和可持续的果实采摘实践。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06260v1",
      "published_date": "2024-05-10 06:17:00 UTC",
      "updated_date": "2024-05-10 06:17:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:39:11.625525"
    },
    {
      "arxiv_id": "2405.06721v1",
      "title": "Kolmogorov-Arnold Networks are Radial Basis Function Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Ziyao Li"
      ],
      "abstract": "This short paper is a fast proof-of-concept that the 3-order B-splines used\nin Kolmogorov-Arnold Networks (KANs) can be well approximated by Gaussian\nradial basis functions. Doing so leads to FastKAN, a much faster implementation\nof KAN which is also a radial basis function (RBF) network.",
      "tldr_zh": "这篇论文证明了Kolmogorov-Arnold Networks (KANs)中使用的3阶B-splines可以被Gaussian radial basis functions良好近似，从而建立了KANs与Radial Basis Function Networks (RBF networks)之间的联系。作者通过这种近似方法开发了FastKAN，一种比传统KAN更快、更高效的实现。实验结果展示了FastKAN在性能上的显著提升，为神经网络设计提供了新的证明概念。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06721v1",
      "published_date": "2024-05-10 06:03:45 UTC",
      "updated_date": "2024-05-10 06:03:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:39:23.118008"
    },
    {
      "arxiv_id": "2405.06247v1",
      "title": "Disttack: Graph Adversarial Attacks Toward Distributed GNN Training",
      "title_zh": "翻译失败",
      "authors": [
        "Yuxiang Zhang",
        "Xin Liu",
        "Meng Wu",
        "Wei Yan",
        "Mingyu Yan",
        "Xiaochun Ye",
        "Dongrui Fan"
      ],
      "abstract": "Graph Neural Networks (GNNs) have emerged as potent models for graph\nlearning. Distributing the training process across multiple computing nodes is\nthe most promising solution to address the challenges of ever-growing\nreal-world graphs. However, current adversarial attack methods on GNNs neglect\nthe characteristics and applications of the distributed scenario, leading to\nsuboptimal performance and inefficiency in attacking distributed GNN training.\n  In this study, we introduce Disttack, the first framework of adversarial\nattacks for distributed GNN training that leverages the characteristics of\nfrequent gradient updates in a distributed system. Specifically, Disttack\ncorrupts distributed GNN training by injecting adversarial attacks into one\nsingle computing node. The attacked subgraphs are precisely perturbed to induce\nan abnormal gradient ascent in backpropagation, disrupting gradient\nsynchronization between computing nodes and thus leading to a significant\nperformance decline of the trained GNN. We evaluate Disttack on four large\nreal-world graphs by attacking five widely adopted GNNs. Compared with the\nstate-of-the-art attack method, experimental results demonstrate that Disttack\namplifies the model accuracy degradation by 2.75$\\times$ and achieves speedup\nby 17.33$\\times$ on average while maintaining unnoticeability.",
      "tldr_zh": "该论文提出 Disttack，一种针对分布式 GNN 训练的对抗攻击框架，利用分布式系统中频繁的梯度更新特点，通过在单个计算节点注入攻击来扰动子图，从而导致梯度异常上升并破坏节点间同步。Disttack 的核心方法是精确地修改子图以放大 GNN 的性能下降，同时保持攻击不易察觉。实验在四个真实世界大图上评估五种常见 GNNs 时，结果显示 Disttack 比最先进方法将模型准确率下降放大 2.75 倍，并实现 17.33 倍的速度提升。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by 30th International European Conference on Parallel and\n  Distributed Computing(Euro-Par 2024)",
      "pdf_url": "http://arxiv.org/pdf/2405.06247v1",
      "published_date": "2024-05-10 05:09:59 UTC",
      "updated_date": "2024-05-10 05:09:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:39:36.071841"
    },
    {
      "arxiv_id": "2405.06239v1",
      "title": "SaudiBERT: A Large Language Model Pretrained on Saudi Dialect Corpora",
      "title_zh": "翻译失败",
      "authors": [
        "Faisal Qarah"
      ],
      "abstract": "In this paper, we introduce SaudiBERT, a monodialect Arabic language model\npretrained exclusively on Saudi dialectal text. To demonstrate the model's\neffectiveness, we compared SaudiBERT with six different multidialect Arabic\nlanguage models across 11 evaluation datasets, which are divided into two\ngroups: sentiment analysis and text classification. SaudiBERT achieved average\nF1-scores of 86.15\\% and 87.86\\% in these groups respectively, significantly\noutperforming all other comparative models. Additionally, we present two novel\nSaudi dialectal corpora: the Saudi Tweets Mega Corpus (STMC), which contains\nover 141 million tweets in Saudi dialect, and the Saudi Forums Corpus (SFC),\nwhich includes 15.2 GB of text collected from five Saudi online forums. Both\ncorpora are used in pretraining the proposed model, and they are the largest\nSaudi dialectal corpora ever reported in the literature. The results confirm\nthe effectiveness of SaudiBERT in understanding and analyzing Arabic text\nexpressed in Saudi dialect, achieving state-of-the-art results in most tasks\nand surpassing other language models included in the study. SaudiBERT model is\npublicly available on \\url{https://huggingface.co/faisalq/SaudiBERT}.",
      "tldr_zh": "本文介绍了 SaudiBERT，一种专为沙特方言预训练的大型语言模型，使用两个新语料库：Saudi Tweets Mega Corpus (STMC) 包含超过1.41亿条推文，以及 Saudi Forums Corpus (SFC) 包含15.2 GB的论坛文本。相比其他六种多方言阿拉伯语言模型，SaudiBERT 在11个评估数据集上表现突出，在情感分析和文本分类任务中分别获得86.15%和87.86%的平均 F1-scores，显著优于基线模型。该模型展示了在理解沙特方言阿拉伯文本方面的先进性能，并已在Hugging Face上公开可用。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06239v1",
      "published_date": "2024-05-10 04:22:54 UTC",
      "updated_date": "2024-05-10 04:22:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:39:48.502190"
    },
    {
      "arxiv_id": "2405.06232v1",
      "title": "Learning to Solve Geometry Problems via Simulating Human Dual-Reasoning Process",
      "title_zh": "通过模拟人类双重推理过程学习解决几何问题",
      "authors": [
        "Tong Xiao",
        "Jiayu Liu",
        "Zhenya Huang",
        "Jinze Wu",
        "Jing Sha",
        "Shijin Wang",
        "Enhong Chen"
      ],
      "abstract": "Geometry Problem Solving (GPS), which is a classic and challenging math\nproblem, has attracted much attention in recent years. It requires a solver to\ncomprehensively understand both text and diagram, master essential geometry\nknowledge, and appropriately apply it in reasoning. However, existing works\nfollow a paradigm of neural machine translation and only focus on enhancing the\ncapability of encoders, which neglects the essential characteristics of human\ngeometry reasoning. In this paper, inspired by dual-process theory, we propose\na Dual-Reasoning Geometry Solver (DualGeoSolver) to simulate the dual-reasoning\nprocess of humans for GPS. Specifically, we construct two systems in\nDualGeoSolver, namely Knowledge System and Inference System. Knowledge System\ncontrols an implicit reasoning process, which is responsible for providing\ndiagram information and geometry knowledge according to a step-wise reasoning\ngoal generated by Inference System. Inference System conducts an explicit\nreasoning process, which specifies the goal in each reasoning step and applies\nthe knowledge to generate program tokens for resolving it. The two systems\ncarry out the above process iteratively, which behaves more in line with human\ncognition. We conduct extensive experiments on two benchmark datasets, GeoQA\nand GeoQA+. The results demonstrate the superiority of DualGeoSolver in both\nsolving accuracy and robustness from explicitly modeling human reasoning\nprocess and knowledge application.",
      "tldr_zh": "这篇论文提出 DualGeoSolver，一种受双过程理论启发的几何问题求解器，用于模拟人类双重推理过程，以解决现有方法忽略推理特性的问题。DualGeoSolver 包括 Knowledge System（负责提供图表信息和几何知识）和 Inference System（进行显式推理、设定步进目标并生成程序令牌），两者通过迭代交互更符合人类认知。实验结果显示，在 GeoQA 和 GeoQA+ 数据集上，该模型在求解准确性和鲁棒性方面均优于基线模型，证明了显式建模推理过程和知识应用的有效性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "IJCAI 2024 Accepted",
      "pdf_url": "http://arxiv.org/pdf/2405.06232v1",
      "published_date": "2024-05-10 03:53:49 UTC",
      "updated_date": "2024-05-10 03:53:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:39:59.910166"
    },
    {
      "arxiv_id": "2405.06719v1",
      "title": "Enhancing Traffic Prediction with Textual Data Using Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Xiannan Huang"
      ],
      "abstract": "Traffic prediction is pivotal for rational transportation supply scheduling\nand allocation. Existing researches into short-term traffic prediction,\nhowever, face challenges in adequately addressing exceptional circumstances and\nintegrating non-numerical contextual information like weather into models.\nWhile, Large language models offer a promising solution due to their inherent\nworld knowledge. However, directly using them for traffic prediction presents\ndrawbacks such as high cost, lack of determinism, and limited mathematical\ncapability. To mitigate these issues, this study proposes a novel approach.\nInstead of directly employing large models for prediction, it utilizes them to\nprocess textual information and obtain embeddings. These embeddings are then\ncombined with historical traffic data and inputted into traditional\nspatiotemporal forecasting models. The study investigates two types of special\nscenarios: regional-level and node-level. For regional-level scenarios, textual\ninformation is represented as a node connected to the entire network. For\nnode-level scenarios, embeddings from the large model represent additional\nnodes connected only to corresponding nodes. This approach shows a significant\nimprovement in prediction accuracy according to our experiment of New York Bike\ndataset.",
      "tldr_zh": "本研究针对交通预测中处理异常情况和整合非数字信息（如天气）的挑战，提出了一种创新方法，使用Large Language Models (LLMs) 处理文本数据以生成embeddings，然后将这些embeddings与历史交通数据结合，输入到传统的spatiotemporal forecasting模型中。针对区域级和节点级场景，该方法分别将文本信息表示为连接整个网络的节点或仅连接对应节点的额外节点，从而提升模型的预测能力。在New York Bike数据集的实验中，该方法显著提高了预测准确性，证明了其在实际应用中的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06719v1",
      "published_date": "2024-05-10 03:14:26 UTC",
      "updated_date": "2024-05-10 03:14:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:40:11.104123"
    },
    {
      "arxiv_id": "2405.06218v1",
      "title": "Aligning Tutor Discourse Supporting Rigorous Thinking with Tutee Content Mastery for Predicting Math Achievement",
      "title_zh": "翻译失败",
      "authors": [
        "Mark Abdelshiheed",
        "Jennifer K. Jacobs",
        "Sidney K. D'Mello"
      ],
      "abstract": "This work investigates how tutoring discourse interacts with students'\nproximal knowledge to explain and predict students' learning outcomes. Our work\nis conducted in the context of high-dosage human tutoring where 9th-grade\nstudents (N= 1080) attended small group tutorials and individually practiced\nproblems on an Intelligent Tutoring System (ITS). We analyzed whether tutors'\ntalk moves and students' performance on the ITS predicted scores on math\nlearning assessments. We trained Random Forest Classifiers (RFCs) to\ndistinguish high and low assessment scores based on tutor talk moves, student's\nITS performance metrics, and their combination. A decision tree was extracted\nfrom each RFC to yield an interpretable model. We found AUCs of 0.63 for talk\nmoves, 0.66 for ITS, and 0.77 for their combination, suggesting interactivity\namong the two feature sources. Specifically, the best decision tree emerged\nfrom combining the tutor talk moves that encouraged rigorous thinking and\nstudents' ITS mastery. In essence, tutor talk that encouraged mathematical\nreasoning predicted achievement for students who demonstrated high mastery on\nthe ITS, whereas tutors' revoicing of students' mathematical ideas and\ncontributions was predictive for students with low ITS mastery. Implications\nfor practice are discussed.",
      "tldr_zh": "本研究探讨了辅导话语与学生知识掌握度之间的互动如何预测9年级学生的数学学习成果，涉及1080名学生在小组辅导和智能辅导系统（ITS）中的表现。研究者训练了随机森林分类器（RFCs）来分析辅导员的对话动作（talk moves）、学生的ITS表现指标及其组合对评估分数的预测效果，结果显示组合特征的AUC达到0.77。关键发现是：对于ITS掌握度高的学生，鼓励数学推理的辅导话语更能预测成就，而对于掌握度低的学生，辅导员复述学生想法的对话动作更具预测性。该结果为优化辅导实践提供了重要启示，以提升学生的学习效果。",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06218v1",
      "published_date": "2024-05-10 03:04:59 UTC",
      "updated_date": "2024-05-10 03:04:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:40:24.019077"
    },
    {
      "arxiv_id": "2405.06211v3",
      "title": "A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Wenqi Fan",
        "Yujuan Ding",
        "Liangbo Ning",
        "Shijie Wang",
        "Hengyun Li",
        "Dawei Yin",
        "Tat-Seng Chua",
        "Qing Li"
      ],
      "abstract": "As one of the most advanced techniques in AI, Retrieval-Augmented Generation\n(RAG) can offer reliable and up-to-date external knowledge, providing huge\nconvenience for numerous tasks. Particularly in the era of AI-Generated Content\n(AIGC), the powerful capacity of retrieval in providing additional knowledge\nenables RAG to assist existing generative AI in producing high-quality outputs.\nRecently, Large Language Models (LLMs) have demonstrated revolutionary\nabilities in language understanding and generation, while still facing inherent\nlimitations, such as hallucinations and out-of-date internal knowledge. Given\nthe powerful abilities of RAG in providing the latest and helpful auxiliary\ninformation, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged\nto harness external and authoritative knowledge bases, rather than solely\nrelying on the model's internal knowledge, to augment the generation quality of\nLLMs. In this survey, we comprehensively review existing research studies in\nRA-LLMs, covering three primary technical perspectives: architectures, training\nstrategies, and applications. As the preliminary knowledge, we briefly\nintroduce the foundations and recent advances of LLMs. Then, to illustrate the\npractical significance of RAG for LLMs, we systematically review mainstream\nrelevant work by their architectures, training strategies, and application\nareas, detailing specifically the challenges of each and the corresponding\ncapabilities of RA-LLMs. Finally, to deliver deeper insights, we discuss\ncurrent limitations and several promising directions for future research.\nUpdated information about this survey can be found at\nhttps://advanced-recommender-systems.github.io/RAG-Meets-LLMs/",
      "tldr_zh": "这篇调查综述探讨了Retrieval-Augmented Generation (RAG) 与Large Language Models (LLMs)的整合，旨在通过外部知识增强LLMs的生成能力，以解决其固有的hallucinations和知识过时问题。论文系统回顾了Retrieval-Augmented Large Language Models (RA-LLMs)的关键方面，包括架构设计、训练策略以及在各种应用领域的表现，详细分析了面临的挑战及其应对方法。作为初步基础，作者简要介绍了LLMs的发展，并强调RA-LLMs如何利用RAG提供可靠的辅助信息。最后，讨论了当前局限性及未来研究方向，如进一步优化架构和扩展应用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "This is the long version of the corresponding survey paper accepted\n  by KDD2024",
      "pdf_url": "http://arxiv.org/pdf/2405.06211v3",
      "published_date": "2024-05-10 02:48:45 UTC",
      "updated_date": "2024-06-17 08:56:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:40:35.473801"
    },
    {
      "arxiv_id": "2405.06206v2",
      "title": "Concealing Backdoor Model Updates in Federated Learning by Trigger-Optimized Data Poisoning",
      "title_zh": "通过触发器优化的数据投毒隐藏联邦学习中的后门模型更新",
      "authors": [
        "Yujie Zhang",
        "Neil Gong",
        "Michael K. Reiter"
      ],
      "abstract": "Federated Learning (FL) is a decentralized machine learning method that\nenables participants to collaboratively train a model without sharing their\nprivate data. Despite its privacy and scalability benefits, FL is susceptible\nto backdoor attacks, where adversaries poison the local training data of a\nsubset of clients using a backdoor trigger, aiming to make the aggregated model\nproduce malicious results when the same backdoor condition is met by an\ninference-time input. Existing backdoor attacks in FL suffer from common\ndeficiencies: fixed trigger patterns and reliance on the assistance of model\npoisoning. State-of-the-art defenses based on analyzing clients' model updates\nexhibit a good defense performance on these attacks because of the significant\ndivergence between malicious and benign client model updates. To effectively\nconceal malicious model updates among benign ones, we propose DPOT, a backdoor\nattack strategy in FL that dynamically constructs backdoor objectives by\noptimizing a backdoor trigger, making backdoor data have minimal effect on\nmodel updates. We provide theoretical justifications for DPOT's attacking\nprinciple and display experimental results showing that DPOT, via only a\ndata-poisoning attack, effectively undermines state-of-the-art defenses and\noutperforms existing backdoor attack techniques on various datasets.",
      "tldr_zh": "该研究针对Federated Learning (FL)中的backdoor attacks问题，提出了一种名为DPOT的攻击策略，通过优化backdoor trigger动态构建backdoor objectives，仅使用data-poisoning attack来最小化恶意模型更新对整体模型的影响，从而有效隐藏攻击痕迹。DPOT的原理得到了理论证明，并在实验中证明其能成功绕过state-of-the-art防御，并在各种数据集上优于现有backdoor攻击技术。总体而言，此方法突显了FL系统在数据隐私保护方面的潜在漏洞，并为提升防御机制提供了新洞见。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06206v2",
      "published_date": "2024-05-10 02:44:25 UTC",
      "updated_date": "2024-09-09 22:11:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:40:46.957837"
    },
    {
      "arxiv_id": "2405.06204v1",
      "title": "HC$^2$L: Hybrid and Cooperative Contrastive Learning for Cross-lingual Spoken Language Understanding",
      "title_zh": "翻译失败",
      "authors": [
        "Bowen Xing",
        "Ivor W. Tsang"
      ],
      "abstract": "State-of-the-art model for zero-shot cross-lingual spoken language\nunderstanding performs cross-lingual unsupervised contrastive learning to\nachieve the label-agnostic semantic alignment between each utterance and its\ncode-switched data. However, it ignores the precious intent/slot labels, whose\nlabel information is promising to help capture the label-aware semantics\nstructure and then leverage supervised contrastive learning to improve both\nsource and target languages' semantics. In this paper, we propose Hybrid and\nCooperative Contrastive Learning to address this problem. Apart from\ncross-lingual unsupervised contrastive learning, we design a holistic approach\nthat exploits source language supervised contrastive learning, cross-lingual\nsupervised contrastive learning and multilingual supervised contrastive\nlearning to perform label-aware semantics alignments in a comprehensive manner.\nEach kind of supervised contrastive learning mechanism includes both\nsingle-task and joint-task scenarios. In our model, one contrastive learning\nmechanism's input is enhanced by others. Thus the total four contrastive\nlearning mechanisms are cooperative to learn more consistent and discriminative\nrepresentations in the virtuous cycle during the training process. Experiments\nshow that our model obtains consistent improvements over 9 languages, achieving\nnew state-of-the-art performance.",
      "tldr_zh": "本论文提出 HC²L 框架，即 Hybrid and Cooperative Contrastive Learning，用于提升零样本跨语言口语理解模型的性能，该框架通过整合跨语言无监督 contrastive learning 和多种监督 contrastive learning（如源语言、跨语言及多语言形式）来实现标签感知的语义对齐，并在单任务和联合任务场景中进行。每个 contrastive learning 机制的输入由其他机制增强，形成合作循环，以学习更一致和可区分的表示。实验结果显示，该模型在 9 种语言上取得了显著改善，达到了新的 state-of-the-art 性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI). arXiv admin note: text overlap with arXiv:2312.03716",
      "pdf_url": "http://arxiv.org/pdf/2405.06204v1",
      "published_date": "2024-05-10 02:40:49 UTC",
      "updated_date": "2024-05-10 02:40:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:41:00.989857"
    },
    {
      "arxiv_id": "2405.06203v1",
      "title": "A First Step in Using Machine Learning Methods to Enhance Interaction Analysis for Embodied Learning Environments",
      "title_zh": "翻译失败",
      "authors": [
        "Joyce Fonteles",
        "Eduardo Davalos",
        "Ashwin T. S.",
        "Yike Zhang",
        "Mengxi Zhou",
        "Efrat Ayalon",
        "Alicia Lane",
        "Selena Steinberg",
        "Gabriella Anton",
        "Joshua Danish",
        "Noel Enyedy",
        "Gautam Biswas"
      ],
      "abstract": "Investigating children's embodied learning in mixed-reality environments,\nwhere they collaboratively simulate scientific processes, requires analyzing\ncomplex multimodal data to interpret their learning and coordination behaviors.\nLearning scientists have developed Interaction Analysis (IA) methodologies for\nanalyzing such data, but this requires researchers to watch hours of videos to\nextract and interpret students' learning patterns. Our study aims to simplify\nresearchers' tasks, using Machine Learning and Multimodal Learning Analytics to\nsupport the IA processes. Our study combines machine learning algorithms and\nmultimodal analyses to support and streamline researcher efforts in developing\na comprehensive understanding of students' scientific engagement through their\nmovements, gaze, and affective responses in a simulated scenario. To facilitate\nan effective researcher-AI partnership, we present an initial case study to\ndetermine the feasibility of visually representing students' states, actions,\ngaze, affect, and movement on a timeline. Our case study focuses on a specific\nscience scenario where students learn about photosynthesis. The timeline allows\nus to investigate the alignment of critical learning moments identified by\nmultimodal and interaction analysis, and uncover insights into students'\ntemporal learning progressions.",
      "tldr_zh": "本研究旨在通过Machine Learning和Multimodal Learning Analytics提升Interaction Analysis在身体化学习环境中的应用，以简化对儿童混合现实科学模拟行为的分析。研究者传统上需观看数小时视频来提取学习模式，而本方法结合机器学习算法和多模态分析，帮助可视化学生的动作、注视、情感和运动。针对光合作用场景的初始案例研究显示，这种时间线表示能有效对齐关键学习时刻，并揭示学生的temporal learning progressions，从而为研究者提供更高效的AI支持。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06203v1",
      "published_date": "2024-05-10 02:40:24 UTC",
      "updated_date": "2024-05-10 02:40:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:41:11.277633"
    },
    {
      "arxiv_id": "2405.06196v2",
      "title": "VLSM-Adapter: Finetuning Vision-Language Segmentation Efficiently with Lightweight Blocks",
      "title_zh": "翻译失败",
      "authors": [
        "Manish Dhakal",
        "Rabin Adhikari",
        "Safal Thapaliya",
        "Bishesh Khanal"
      ],
      "abstract": "Foundation Vision-Language Models (VLMs) trained using large-scale\nopen-domain images and text pairs have recently been adapted to develop\nVision-Language Segmentation Models (VLSMs) that allow providing text prompts\nduring inference to guide image segmentation. If robust and powerful VLSMs can\nbe built for medical images, it could aid medical professionals in many\nclinical tasks where they must spend substantial time delineating the target\nstructure of interest. VLSMs for medical images resort to fine-tuning base VLM\nor VLSM pretrained on open-domain natural image datasets due to fewer annotated\nmedical image datasets; this fine-tuning is resource-consuming and expensive as\nit usually requires updating all or a significant fraction of the pretrained\nparameters. Recently, lightweight blocks called adapters have been proposed in\nVLMs that keep the pretrained model frozen and only train adapters during\nfine-tuning, substantially reducing the computing resources required. We\nintroduce a novel adapter, VLSM-Adapter, that can fine-tune pretrained\nvision-language segmentation models using transformer encoders. Our experiments\nin widely used CLIP-based segmentation models show that with only 3 million\ntrainable parameters, the VLSM-Adapter outperforms state-of-the-art and is\ncomparable to the upper bound end-to-end fine-tuning. The source code is\navailable at: https://github.com/naamiinepal/vlsm-adapter.",
      "tldr_zh": "该论文提出VLSM-Adapter，一种轻量级适配器，用于高效微调视觉语言分割模型（VLSMs），以适应医疗图像任务，从而减少资源消耗。VLSM-Adapter 采用 transformer encoders，仅训练约 3 百万参数，而保持预训练模型冻结，避免了端到端微调的高成本。实验在 CLIP-based segmentation models 上显示，该适配器超过了现有最先进方法，并与全面微调性能相当，为临床图像分割提供更高效的解决方案。源代码已在 GitHub 上公开。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at MICCAI 2024, the 27th International Conference on Medical\n  Image Computing and Computer Assisted Intervention",
      "pdf_url": "http://arxiv.org/pdf/2405.06196v2",
      "published_date": "2024-05-10 02:23:56 UTC",
      "updated_date": "2024-06-27 14:19:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:41:24.697012"
    },
    {
      "arxiv_id": "2405.06192v1",
      "title": "Contrastive Representation for Data Filtering in Cross-Domain Offline Reinforcement Learning",
      "title_zh": "在跨域离线强化学习中用于数据过滤的对比性表示",
      "authors": [
        "Xiaoyu Wen",
        "Chenjia Bai",
        "Kang Xu",
        "Xudong Yu",
        "Yang Zhang",
        "Xuelong Li",
        "Zhen Wang"
      ],
      "abstract": "Cross-domain offline reinforcement learning leverages source domain data with\ndiverse transition dynamics to alleviate the data requirement for the target\ndomain. However, simply merging the data of two domains leads to performance\ndegradation due to the dynamics mismatch. Existing methods address this problem\nby measuring the dynamics gap via domain classifiers while relying on the\nassumptions of the transferability of paired domains. In this paper, we propose\na novel representation-based approach to measure the domain gap, where the\nrepresentation is learned through a contrastive objective by sampling\ntransitions from different domains. We show that such an objective recovers the\nmutual-information gap of transition functions in two domains without suffering\nfrom the unbounded issue of the dynamics gap in handling significantly\ndifferent domains. Based on the representations, we introduce a data filtering\nalgorithm that selectively shares transitions from the source domain according\nto the contrastive score functions. Empirical results on various tasks\ndemonstrate that our method achieves superior performance, using only 10% of\nthe target data to achieve 89.2% of the performance on 100% target dataset with\nstate-of-the-art methods.",
      "tldr_zh": "本文提出了一种基于Contrastive Representation的方法，用于Cross-Domain Offline Reinforcement Learning中处理源域和目标域动态不匹配的问题。该方法通过对比目标（contrastive objective）学习表示，采样不同域的transitions，以恢复转移函数的互信息差距，避免了动态差距的无界问题。基于此，引入数据过滤算法，根据对比分数选择性地共享源域数据。实验结果显示，该方法在各种任务上表现出色，仅使用10%的目标数据即可达到使用100%目标数据集的89.2%性能，优于现有方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "This paper has been accepted by ICML2024",
      "pdf_url": "http://arxiv.org/pdf/2405.06192v1",
      "published_date": "2024-05-10 02:21:42 UTC",
      "updated_date": "2024-05-10 02:21:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:41:36.560869"
    },
    {
      "arxiv_id": "2405.06164v1",
      "title": "Skeet: Towards a Lightweight Serverless Framework Supporting Modern AI-Driven App Development",
      "title_zh": "翻译失败",
      "authors": [
        "Kawasaki Fumitake",
        "Shota Kishi",
        "James Neve"
      ],
      "abstract": "The field of web and mobile software frameworks is relatively mature, with a\nlarge variety of tools in different languages that facilitate traditional app\ndevelopment where data in a relational database is displayed and modified. Our\nposition is that many current frameworks became popular during single server\ndeployment of MVC architecture apps, and do not facilitate modern aspects of\napp development such as cloud computing and the incorporation of emerging\ntechnologies such as AI. We present a novel framework which accomplishes these\npurposes, Skeet, which was recently released to general use, alongside an\ninitial evaluation. Skeet provides an app structure that reflects current\ntrends in architecture, and tool suites that allow developers with minimal\nknowledge of AI internals to easily incorporate such technologies into their\napps and deploy them.",
      "tldr_zh": "该论文指出，现有的 web 和 mobile 软件框架主要针对传统 MVC 架构的应用开发（如关系数据库的数据显示和修改），但无法有效支持云计算和 AI 等现代技术。作者提出 Skeet，这是一个轻量级的 serverless framework，旨在提供符合当前架构趋势的应用结构和工具套件，让开发者无需深入了解 AI 内部细节即可轻松整合 AI 技术并部署应用。通过初步评估，Skeet 展示了其在促进 AI 驱动应用开发方面的潜力。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06164v1",
      "published_date": "2024-05-10 01:00:20 UTC",
      "updated_date": "2024-05-10 01:00:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:41:47.289222"
    },
    {
      "arxiv_id": "2405.06149v2",
      "title": "DisBeaNet: A Deep Neural Network to augment Unmanned Surface Vessels for maritime situational awareness",
      "title_zh": "翻译失败",
      "authors": [
        "Srikanth Vemula",
        "Eulises Franco",
        "Michael Frye"
      ],
      "abstract": "Intelligent detection and tracking of the vessels on the sea play a\nsignificant role in conducting traffic avoidance in unmanned surface\nvessels(USV). Current traffic avoidance software relies mainly on Automated\nIdentification System (AIS) and radar to track other vessels to avoid\ncollisions and acts as a typical perception system to detect targets. However,\nin a contested environment, emitting radar energy also presents the\nvulnerability to detection by adversaries. Deactivating these Radiofrequency\ntransmitting sources will increase the threat of detection and degrade the\nUSV's ability to monitor shipping traffic in the vicinity. Therefore, an\nintelligent visual perception system based on an onboard camera with passive\nsensing capabilities that aims to assist USV in addressing this problem is\npresented in this paper. This paper will present a novel low-cost vision\nperception system for detecting and tracking vessels in the maritime\nenvironment. This novel low-cost vision perception system is introduced using\nthe deep learning framework. A neural network, DisBeaNet, can detect vessels,\ntrack, and estimate the vessel's distance and bearing from the monocular\ncamera. The outputs obtained from this neural network are used to determine the\nlatitude and longitude of the identified vessel.",
      "tldr_zh": "本文提出 DisBeaNet，一种深度神经网络框架，用于增强无人水面船只（USV）的海上态势感知问题，旨在通过被动视觉感知取代依赖 Automated Identification System (AIS) 和雷达的传统方法。DisBeaNet 利用单目摄像头检测和跟踪船只，同时估计船只的距离和方位，并计算其纬度和经度。该系统在低成本基础上显著提高了 USV 的环境适应性和安全性，避免了射频信号暴露的风险。",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06149v2",
      "published_date": "2024-05-10 00:15:17 UTC",
      "updated_date": "2024-05-17 20:38:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:41:59.962637"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 67,
  "processed_papers_count": 67,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-18T07:42:23.770552"
}
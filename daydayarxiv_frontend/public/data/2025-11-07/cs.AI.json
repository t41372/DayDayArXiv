{
  "date": "2025-11-07",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-11-07 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\n**ä»Šæ—¥æ€»ç»“**ï¼š\nä»Šå¤©çš„ arXiv çˆ†å‘äº†å¤§é‡å…³äº **Agentic AIï¼ˆæ™ºèƒ½ä½“ï¼‰** çš„é«˜è´¨é‡å·¥ä½œï¼Œä» NVIDIA å›¢é˜Ÿæ¨å‡ºçš„ç™¾ä¸‡çº§è§†è§‰æ¨ç†æ•°æ®åˆæˆæ¡†æ¶ï¼Œåˆ°é’ˆå¯¹ Agent ä»£ç èƒ½åŠ›çš„ç»Ÿä¸€è¯„ä¼°åŸºå‡† SWE-Compassï¼Œå†åˆ°åŸå¸‚è§„åˆ’å’Œå®æ—¶ç¯å¢ƒä¸‹çš„æ¨ç†æ™ºèƒ½ä½“ã€‚æ­¤å¤–ï¼Œ**ç§‘å­¦ AI (AI for Science)** é¢†åŸŸè¿æ¥äº†è„‘ç§‘å­¦çš„åŸºç¡€æ¨¡å‹ï¼Œè€Œå…³äº LLM çš„ **â€œå¿ƒç†å­¦â€** ç ”ç©¶ï¼ˆå¦‚é”šå®šåå·®ã€åæ´¾è§’è‰²æ‰®æ¼”ï¼‰ä¹Ÿæ­ç¤ºäº†æ¨¡å‹æœ‰è¶£çš„è®¤çŸ¥è¾¹ç•Œã€‚\n\n---\n\n### ğŸš€ ç„¦ç‚¹ï¼šå¤šæ¨¡æ€æ¨ç†ã€æ–°æ¨¡å‹ä¸ Agent\n\n**1. [æ¨è] Long Grounded Thoughts: Distilling Compositional Visual Reasoning Chains at Scale**\n> **é•¿ç¨‹æ¥åœ°æ€ç»´ï¼šå¤§è§„æ¨¡è’¸é¦ç»„åˆå¼è§†è§‰æ¨ç†é“¾**\n> *David Acuna, Yejin Choi, et al. (NVIDIA & UW)*\n> è¿™æ˜¯ä¸€ä¸ªé‡ç£…å·¥ä½œã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªæ•°æ®ç”Ÿæˆæ¡†æ¶ï¼Œåˆæˆäº†è¶…è¿‡ 100 ä¸‡ä¸ªé«˜è´¨é‡çš„è§†è§‰ä¸­å¿ƒæ¨ç†é—®é¢˜ï¼ˆVision-centric reasoningï¼‰ã€‚é€šè¿‡ä¸¤é˜¶æ®µè¿‡ç¨‹ç”Ÿæˆ CoTï¼ˆæ€ç»´é“¾ï¼‰è½¨è¿¹ï¼Œå¹¶åœ¨ Qwen2.5-VL-7B ä¸Šå¾®è°ƒï¼Œç»“æœåœ¨å¤šä¸ªè§†è§‰åŸºå‡†ä¸Š**è¶…è¶Šäº†æ‰€æœ‰å¼€æºåŸºå‡†**ï¼Œç”šè‡³å‡»è´¥äº†éƒ¨åˆ†é—­æºæ¨¡å‹ã€‚\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šè¯æ˜äº†åŸºäºé«˜è´¨é‡ã€éçº¿æ€§æ¨ç†è½¨è¿¹çš„ SFTï¼ˆç›‘ç£å¾®è°ƒï¼‰å¯¹äºè§†è§‰æ¨ç†è‡³å…³é‡è¦ï¼Œä¸”è¿™äº›è§†è§‰æ•°æ®èƒ½æ­£å‘è¿ç§»åˆ°çº¯æ–‡æœ¬æ¨ç†ä»»åŠ¡ä¸­ã€‚\n\n**2. Motif 2 12.7B technical report**\n> **Motif 2 12.7B æŠ€æœ¯æŠ¥å‘Š**\n> *Junghwan Lim et al.*\n> å‘å¸ƒäº† **Motif-2-12.7B**ï¼Œä¸€ä¸ªæ–°çš„å¼€æºåŸºç¡€æ¨¡å‹ã€‚\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šå¼•å…¥äº†åˆ†ç»„å¾®åˆ†æ³¨æ„åŠ›ï¼ˆGrouped Differential Attention, GDAï¼‰æ¥æé«˜æ•ˆç‡ï¼Œå¹¶åœ¨ 5.5T token ä¸Šè¿›è¡Œäº†è®­ç»ƒã€‚è¯¥æ¨¡å‹å±•ç¤ºäº†é€šè¿‡æ¶æ„åˆ›æ–°å’Œç³»ç»Ÿçº§ä¼˜åŒ–ï¼ˆå¦‚ MuonClip ä¼˜åŒ–å™¨ï¼‰ï¼Œä¸­ç­‰è§„æ¨¡æ¨¡å‹ä¹Ÿèƒ½å…·å¤‡å¼ºå¤§çš„æŒ‡ä»¤æ³›åŒ–èƒ½åŠ›ã€‚\n\n**3. Real-Time Reasoning Agents in Evolving Environments**\n> **æ¼”å˜ç¯å¢ƒä¸­çš„å®æ—¶æ¨ç†æ™ºèƒ½ä½“**\n> *Yule Wen, Diyi Yang, et al.*\n> ç°æœ‰çš„ LLM æ¨ç†å¾€å¾€å¿½ç•¥äº†ç¯å¢ƒçš„åŠ¨æ€å˜åŒ–ã€‚æœ¬æ–‡æå‡ºäº†â€œå®æ—¶æ¨ç†â€çš„æ–°é—®é¢˜è®¾å®šï¼Œå¹¶æ„å»ºäº† Real-Time Reasoning Gymã€‚\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šæå‡ºäº† **AgileThinker**ï¼Œä¸€ç§åŒæ—¶ç»“åˆâ€œååº”å¼â€ï¼ˆå¿«é€Ÿå“åº”ï¼‰å’Œâ€œè§„åˆ’å¼â€ï¼ˆæ·±åº¦æ€è€ƒï¼‰ä¸¤ç§æ¨¡å¼çš„æ™ºèƒ½ä½“æ¶æ„ï¼Œåœ¨æ—¶é—´ç´§è¿«å’Œä»»åŠ¡éš¾åº¦å¢åŠ æ—¶è¡¨ç°ä¼˜äºå•ä¸€æ¨¡å¼ã€‚\n\n**4. SWE-Compass: Towards Unified Evaluation of Agentic Coding Abilities for Large Language Models**\n> **SWE-Compassï¼šè¿ˆå‘å¤§å‹è¯­è¨€æ¨¡å‹ Agentic ç¼–ç¨‹èƒ½åŠ›çš„ç»Ÿä¸€è¯„ä¼°**\n> *Jingxuan Xu et al.*\n> é’ˆå¯¹ LLM è½¯ä»¶å·¥ç¨‹èƒ½åŠ›çš„è¯„ä¼°å¾€å¾€å±€é™äºç®€å•çš„ Bug ä¿®å¤ã€‚\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šæ¨å‡ºäº† SWE-Compass åŸºå‡†ï¼ŒåŒ…å« 8 ç§ä»»åŠ¡ç±»å‹ã€10 ç§ç¼–ç¨‹è¯­è¨€å’Œ 2000 ä¸ªæ¥è‡ªçœŸå® GitHub PR çš„é«˜è´¨é‡å®ä¾‹ã€‚è¯„ä¼°äº† 10 ä¸ª SOTA æ¨¡å‹åœ¨ SWE-Agent å’Œ Claude Code æ¡†æ¶ä¸‹çš„è¡¨ç°ï¼Œæ­ç¤ºäº† Agentic ç¼–ç èƒ½åŠ›çš„å±‚çº§å·®å¼‚ã€‚\n\n**5. Reasoning Is All You Need for Urban Planning AI**\n> **åŸå¸‚è§„åˆ’ AIï¼šæ¨ç†å³æ‰€éœ€**\n> *Sijie Yang et al.*\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šè¿™æ˜¯ä¸€ç¯‡ Position Paperã€‚ä½œè€…æå‡ºäº† **Agentic Urban Planning AI Framework**ï¼Œè®¤ä¸ºåŸå¸‚è§„åˆ’ AI éœ€è¦ä»å•çº¯çš„æ•°æ®åˆ†æè½¬å‘è¾…åŠ©å†³ç­–ã€‚è¯¥æ¡†æ¶é›†æˆäº†æ„ŸçŸ¥ã€åŸºç¡€å’Œæ¨ç†ä¸‰å±‚è®¤çŸ¥ç»“æ„ï¼Œåˆ©ç”¨å¤šæ™ºèƒ½ä½“åä½œæ¥å¤„ç†å¤æ‚çš„è§„åˆ’æƒè¡¡å’Œæ³•è§„éªŒè¯ã€‚\n\n---\n\n### ğŸ§  AI for Scienceï¼šè„‘ç§‘å­¦ä¸åŒ–å­¦\n\n**6. BrainCSD: A Hierarchical Consistency-Driven MoE Foundation Model for Unified Connectome Synthesis and Multitask Brain Trait Prediction**\n> **BrainCSDï¼šç”¨äºç»Ÿä¸€è¿æ¥ç»„åˆæˆå’Œå¤šä»»åŠ¡è„‘ç‰¹å¾é¢„æµ‹çš„åˆ†å±‚ä¸€è‡´æ€§é©±åŠ¨ MoE åŸºç¡€æ¨¡å‹**\n> *Xiongri Shen et al.*\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šæå‡ºäº†ä¸€ä¸ªåŸºäºæ··åˆä¸“å®¶ï¼ˆMoEï¼‰çš„è„‘ç§‘å­¦åŸºç¡€æ¨¡å‹ã€‚å®ƒä¸ä»…èƒ½åˆæˆç¼ºå¤±çš„åŠŸèƒ½/ç»“æ„è¿æ¥ï¼ˆFC/SCï¼‰ç”Ÿç‰©æ ‡å¿—ç‰©ï¼Œè¿˜èƒ½è¿›è¡Œä¸‹æ¸¸çš„è¯Šæ–­å’Œé¢„æµ‹ä»»åŠ¡ï¼ˆå¦‚é˜¿å°”èŒ¨æµ·é»˜ç—…åˆ†ç±»å‡†ç¡®ç‡è¾¾ 95.6%ï¼‰ï¼Œè§£å†³äº†è„‘å½±åƒæ•°æ®æ¨¡æ€ç¼ºå¤±å’Œè·å–æˆæœ¬é«˜çš„é—®é¢˜ã€‚\n\n**7. Pattern-Aware Diffusion Synthesis of fMRI/dMRI with Tissue and Microstructural Refinement**\n> **å…·æœ‰ç»„ç»‡å’Œå¾®ç»“æ„ç»†åŒ–çš„ fMRI/dMRI æ¨¡å¼æ„ŸçŸ¥æ‰©æ•£åˆæˆ**\n> *Xiongri Shen et al.*\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šé’ˆå¯¹è„‘å½±åƒæ¨¡æ€ç¼ºå¤±é—®é¢˜ï¼Œæå‡ºäº† PDS æ¡†æ¶ã€‚åˆ©ç”¨æ¨¡å¼æ„ŸçŸ¥çš„åŒæ¨¡æ€ 3D æ‰©æ•£æ¨¡å‹è¿›è¡Œè·¨æ¨¡æ€å­¦ä¹ ï¼Œå¹¶ç»“åˆç»„ç»‡ç»†åŒ–ç½‘ç»œï¼Œå®ç°äº†é«˜è´¨é‡çš„ fMRI å’Œ dMRI ç›¸äº’åˆæˆï¼Œåœ¨ä¸´åºŠè¯Šæ–­ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚\n\n**8. Compressing Chemistry Reveals Functional Groups**\n> **å‹ç¼©åŒ–å­¦æ­ç¤ºå®˜èƒ½å›¢**\n> *Ruben Sharma & Ross D. King*\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šåˆ©ç”¨æœ€å°ä¿¡æ¯é•¿åº¦ï¼ˆMMLï¼‰åŸç†ï¼Œé€šè¿‡æ— ç›‘ç£å­¦ä¹ ç®—æ³•åœ¨ 300 ä¸‡ä¸ªåˆ†å­ä¸­æœç´¢èƒ½â€œå‹ç¼©â€æ•°æ®çš„å­ç»“æ„ã€‚ç»“æœå‘ç°ï¼Œç®—æ³•è‡ªåŠ¨å‘ç°äº†å¤§å¤šæ•°äººç±»å®šä¹‰çš„åŒ–å­¦å®˜èƒ½å›¢ï¼Œè¿˜å‘ç°äº†ä¸€äº›å…·æœ‰ç‰¹å®šåŠŸèƒ½çš„æ–°å‹æ›´å¤§æ¨¡å¼ï¼Œç”Ÿæˆçš„æŒ‡çº¹åœ¨ç”Ÿç‰©æ´»æ€§é¢„æµ‹ä»»åŠ¡ä¸­ä¼˜äºä¼ ç»ŸæŒ‡çº¹ã€‚\n\n**9. DGTN: Graph-Enhanced Transformer with Diffusive Attention Gating Mechanism for Enzyme DDG Prediction**\n> **DGTNï¼šç”¨äºé…¶ DDG é¢„æµ‹çš„å…·æœ‰æ‰©æ•£æ³¨æ„åŠ›é—¨æ§æœºåˆ¶çš„å›¾å¢å¼º Transformer**\n> *Abigail Lin*\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šæå‡ºäº†ä¸€ç§é€šè¿‡æ‰©æ•£æœºåˆ¶å…±åŒå­¦ä¹ å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰æƒé‡å’Œ Transformer æ³¨æ„åŠ›çš„æ¶æ„ã€‚è¿™ç§åŒå‘æ‰©æ•£è¿‡ç¨‹è®©ç»“æ„å…ˆéªŒå’Œåºåˆ—æ¨¡å¼æ›´å¥½åœ°è€¦åˆï¼Œåœ¨é…¶çƒ­ç¨³å®šæ€§é¢„æµ‹ä»»åŠ¡ä¸Šå–å¾—äº† SOTA æ€§èƒ½ã€‚\n\n---\n\n### ğŸ­ LLM å¿ƒç†å­¦ã€å®‰å…¨ä¸å¯¹é½\n\n**10. Lived Experience in Dialogue: Co-designing Personalization in Large Language Models to Support Youth Mental Well-being**\n> **å¯¹è¯ä¸­çš„ç”Ÿæ´»ä½“éªŒï¼šå…±åŒè®¾è®¡ LLM ä¸ªæ€§åŒ–ä»¥æ”¯æŒé’å°‘å¹´å¿ƒç†å¥åº·**\n> *Kathleen W. Guan et al.*\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šé€šè¿‡ä¸é’å°‘å¹´ã€å®¶é•¿å’ŒæŠ¤ç†äººå‘˜çš„å‚ä¸å¼ç ”ç©¶ï¼Œæ¢è®¨äº†å¦‚ä½•è®© LLM æ›´å¥½åœ°æ”¯æŒé’å°‘å¹´å¿ƒç†å¥åº·ã€‚æå‡ºäº†ä¸‰ä¸ªæ ¸å¿ƒä¸»é¢˜ï¼šä»¥äººä¸ºæœ¬çš„æƒ…å¢ƒåŒ–ã€æ˜ç¡®çš„èŒƒå›´è¾¹ç•Œï¼ˆåŠçº¿ä¸‹è½¬è¯Šï¼‰ä»¥åŠå¯¹è¯å¼æ”¯æ¶ï¼Œæ—¨åœ¨è®© AI å¹²é¢„æ›´ç¬¦åˆé’å°‘å¹´çš„ç°å®ç”Ÿæ´»ä½“éªŒã€‚\n\n**11. Anchors in the Machine: Behavioral and Attributional Evidence of Anchoring Bias in LLMs**\n> **æœºå™¨ä¸­çš„é”šç‚¹ï¼šLLM ä¸­é”šå®šåå·®çš„è¡Œä¸ºå’Œå½’å› è¯æ®**\n> *Felipe Valencia-Clavijo*\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šç ”ç©¶äº† LLM æ˜¯å¦åƒäººç±»ä¸€æ ·å­˜åœ¨â€œé”šå®šåå·®â€ï¼ˆAnchoring Biasï¼‰ã€‚é€šè¿‡å¯¹æ•°æ¦‚ç‡åˆ†æå’Œ Shapley å€¼å½’å› ï¼Œå‘ç° Gemma-2Bã€Llama-2 ç­‰æ¨¡å‹è¡¨ç°å‡ºå¼ºçƒˆçš„é”šå®šæ•ˆåº”ï¼Œå³å…ˆå…¥ä¸ºä¸»çš„ä¿¡æ¯ä¼šæ˜¾è‘—åç§»æ¨¡å‹çš„è¾“å‡ºåˆ†å¸ƒï¼Œä¸”è¿™ç§æ•ˆåº”åœ¨ä¸åŒæ¨¡å‹è§„æ¨¡é—´å­˜åœ¨å·®å¼‚ã€‚\n\n**12. Too Good to be Bad: On the Failure of LLMs to Role-Play Villains**\n> **å¥½å¾—æ²¡æ³•å˜åï¼šè®º LLM åœ¨æ‰®æ¼”åæ´¾è§’è‰²æ—¶çš„å¤±è´¥**\n> *Zihao Yi et al.*\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šå¼•å…¥äº† Moral RolePlay åŸºå‡†ï¼Œæµ‹è¯• LLM æ‰®æ¼”åæ´¾çš„èƒ½åŠ›ã€‚å‘ç°ç”±äºå®‰å…¨å¯¹é½ï¼ˆSafety Alignmentï¼‰ï¼Œç°ä»£ LLM å¾ˆéš¾çœŸå®åœ°æ‰®æ¼”é“å¾·è´¥åçš„è§’è‰²ï¼ˆå¦‚æ¬ºè¯ˆã€æ“çºµï¼‰ï¼Œå¾€å¾€ä¼šæŠŠç»†å¾®çš„æ¶æ„æ›¿æ¢ä¸ºè¡¨é¢çš„æ”»å‡»æ€§ã€‚è¿™æ­ç¤ºäº†æ¨¡å‹å®‰å…¨æ€§ä¸åˆ›æ„å†™ä½œ/è§’è‰²æ‰®æ¼”ä¿çœŸåº¦ä¹‹é—´çš„å†²çªã€‚\n\n**13. Lived Experience in Dialogue... (é‡å¤æ¡ç›®ï¼Œå¿½ç•¥)**\n*(æ³¨ï¼šåŸåˆ—è¡¨ç¬¬1æ¡å·²æ¶µç›–)*\n\n**14. Pluralistic Behavior Suite: Stress-Testing Multi-Turn Adherence to Custom Behavioral Policies**\n> **å¤šå…ƒè¡Œä¸ºå¥—ä»¶ï¼šå‹åŠ›æµ‹è¯•å¯¹è‡ªå®šä¹‰è¡Œä¸ºç­–ç•¥çš„å¤šè½®ä¾ä»æ€§**\n> *Prasoon Varshney et al.*\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šæå‡ºäº† PBSUITEï¼Œç”¨äºæµ‹è¯• LLM åœ¨å¤šè½®å¯¹è¯ä¸­æ˜¯å¦éµå®ˆç‰¹å®šçš„ä¼ä¸šæˆ–è¡Œä¸šè¡Œä¸ºå‡†åˆ™ã€‚ç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶å•è½®å¯¹è¯è¡¨ç°å°šå¯ï¼Œä½†åœ¨å¤šè½®å¯¹æŠ—æ€§äº¤äº’ä¸­ï¼Œæ¨¡å‹éµå®ˆç­–ç•¥çš„å¤±è´¥ç‡é«˜è¾¾ 84%ã€‚\n\n---\n\n### ğŸ› ï¸ æ•ˆç‡ã€RAG ä¸ æ¶æ„ä¼˜åŒ–\n\n**15. CoT-X: An Adaptive Framework for Cross-Model Chain-of-Thought Transfer and Optimization**\n> **CoT-Xï¼šè·¨æ¨¡å‹æ€ç»´é“¾è¿ç§»ä¸ä¼˜åŒ–çš„è‡ªé€‚åº”æ¡†æ¶**\n> *Ziqian Bi et al.*\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šä¸ºäº†é™ä½ CoT æ¨ç†çš„å¼€é”€ï¼Œæå‡ºäº†ä¸€ç§è‡ªé€‚åº”æ¨ç†æ‘˜è¦æ¡†æ¶ã€‚é€šè¿‡è¯­ä¹‰åˆ†å‰²å’Œé‡è¦æ€§è¯„åˆ†å‹ç¼©æ¨ç†è·¯å¾„ï¼Œåœ¨åŒ»ç–—é—®ç­”ä»»åŠ¡ä¸­ï¼Œç›¸æ¯”ç›´æ¥æˆªæ–­ï¼Œå‡†ç¡®ç‡æå‡äº† 40%ï¼Œå¹¶å®ç°äº†é«˜æ•ˆçš„è·¨æ¨¡å‹è¿ç§»ï¼ˆå¦‚ä» DeepSeek-R1 è¿ç§»åˆ°å°æ¨¡å‹ï¼‰ã€‚\n\n**16. Beyond Redundancy: Diverse and Specialized Multi-Expert Sparse Autoencoder**\n> **è¶…è¶Šå†—ä½™ï¼šå¤šæ ·åŒ–ä¸”ä¸“ä¸šçš„ä¸“å®¶ç¨€ç–è‡ªç¼–ç å™¨**\n> *Zhen Xu et al.*\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šé’ˆå¯¹ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰è®¡ç®—æˆæœ¬é«˜çš„é—®é¢˜ï¼Œæ”¹è¿›äº†æ··åˆä¸“å®¶ï¼ˆMoEï¼‰SAEã€‚é€šè¿‡â€œå¤šä¸“å®¶æ¿€æ´»â€å’Œâ€œç‰¹å¾ç¼©æ”¾â€è§£å†³äº†ä¸“å®¶ç‰¹å¾é‡å çš„é—®é¢˜ï¼Œå‡å°‘äº† 99% çš„ç‰¹å¾å†—ä½™ï¼ŒåŒæ—¶é™ä½äº†é‡æ„è¯¯å·®ï¼Œæå‡äº† LLM çš„å¯è§£é‡Šæ€§æ•ˆç‡ã€‚\n\n**17. TeaRAG: A Token-Efficient Agentic Retrieval-Augmented Generation Framework**\n> **TeaRAGï¼šä¸€ç§ Token é«˜æ•ˆçš„ Agentic æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶**\n> *Chao Zhang et al.*\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šé’ˆå¯¹ Agentic RAG token æ¶ˆè€—å¤§çš„é—®é¢˜ï¼Œæå‡ºäº† TeaRAGã€‚é€šè¿‡å‹ç¼©æ£€ç´¢å†…å®¹ï¼ˆåˆ©ç”¨çŸ¥è¯†å…³è”å›¾ï¼‰å’Œä¼˜åŒ–æ¨ç†æ­¥éª¤ï¼ˆåˆ©ç”¨è¿­ä»£è¿‡ç¨‹æ„ŸçŸ¥çš„ DPOï¼‰ï¼Œåœ¨ Llama3 å’Œ Qwen2.5 ä¸Šå‡å°‘äº†çº¦ 60% çš„è¾“å‡º tokenï¼ŒåŒæ—¶æå‡äº†å‡†ç¡®ç‡ã€‚\n\n**18. Engineering the RAG Stack: A Comprehensive Review...**\n> **RAG æ ˆå·¥ç¨‹ï¼šæ¶æ„ä¸ä¿¡ä»»æ¡†æ¶ç»¼è¿°**\n> *Dean Wampler et al.*\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šè¿™æ˜¯ä¸€ç¯‡ 86 é¡µçš„é•¿ç¯‡ç»¼è¿°ã€‚ç³»ç»Ÿæ¢³ç†äº† 2018-2025 å¹´é—´çš„ RAG æ¶æ„ï¼Œæä¾›äº†ç»Ÿä¸€çš„åˆ†ç±»æ³•ã€è¯„ä¼°åŸºå‡†å’Œä¿¡ä»»å®‰å…¨æ¨¡å‹ï¼Œæ˜¯ RAG ç³»ç»Ÿå¼€å‘çš„å®ç”¨æŒ‡å—ã€‚\n\n---\n\n### ğŸ‘ï¸ è§†è§‰ä¸å¤šæ¨¡æ€\n\n**19. 4D3R: Motion-Aware Neural Reconstruction and Rendering of Dynamic Scenes from Monocular Videos**\n> **4D3Rï¼šå•ç›®è§†é¢‘åŠ¨æ€åœºæ™¯çš„è¿åŠ¨æ„ŸçŸ¥ç¥ç»é‡å»ºä¸æ¸²æŸ“**\n> *Mengqi Guo et al.*\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šæå‡ºäº†ä¸€ç§æ— éœ€é¢„è®¾å§¿æ€çš„åŠ¨æ€ç¥ç»æ¸²æŸ“æ¡†æ¶ã€‚åˆ©ç”¨ 3D åŸºç¡€æ¨¡å‹è¿›è¡Œåˆå§‹ä¼°è®¡ï¼Œç»“åˆè¿åŠ¨æ„ŸçŸ¥çš„ Bundle Adjustment å’Œé«˜æ–¯æ³¼æº…ï¼ˆGaussian Splattingï¼‰è¡¨ç¤ºï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬å¹¶æå‡äº†åŠ¨æ€åœºæ™¯çš„é‡å»ºè´¨é‡ã€‚\n\n**20. LiveStar: Live Streaming Assistant for Real-World Online Video Understanding**\n> **LiveStarï¼šç”¨äºçœŸå®ä¸–ç•Œåœ¨çº¿è§†é¢‘ç†è§£çš„ç›´æ’­åŠ©æ‰‹**\n> *Zhenyu Yang et al.*\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šé’ˆå¯¹ç›´æ’­åœºæ™¯ï¼Œæå‡ºäº† LiveStar åŠ©æ‰‹ã€‚å®ƒå…·å¤‡â€œå“åº”-é™é»˜â€è§£ç æœºåˆ¶ï¼Œèƒ½ä¸»åŠ¨åˆ¤æ–­æœ€ä½³å“åº”æ—¶æœºï¼Œå¹¶é€šè¿‡å†…å­˜å‹ç¼©æŠ€æœ¯æ”¯æŒé•¿è§†é¢‘æµçš„å®æ—¶æ¨ç†ï¼Œåœ¨è¯­ä¹‰æ­£ç¡®æ€§å’Œå“åº”åŠæ—¶æ€§ä¸Šå‡ä¼˜äºç°æœ‰ Video-LLMã€‚\n\n---\n\n### ğŸ’» è½¯ä»¶å·¥ç¨‹ä¸ç³»ç»Ÿ\n\n**21. LLMs as Packagers of HPC Software**\n> **LLM ä½œä¸º HPC è½¯ä»¶çš„æ‰“åŒ…è€…**\n> *Caetano Melone et al.*\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šHPC è½¯ä»¶ä¾èµ–æå…¶å¤æ‚ã€‚æœ¬æ–‡æå‡ºäº† **SpackIt**ï¼Œåˆ©ç”¨ LLM è‡ªåŠ¨ç”Ÿæˆ Spack æ„å»ºé…æ–¹ã€‚é€šè¿‡æ£€ç´¢å¢å¼ºå’ŒåŸºäºåé¦ˆçš„è¿­ä»£ç»†åŒ–ï¼Œå°†å®‰è£…æˆåŠŸç‡ä»é›¶æ ·æœ¬çš„ 20% æå‡åˆ°äº† 80% ä»¥ä¸Šã€‚\n\n**22. Dynamic Stability of LLM-Generated Code**\n> **LLM ç”Ÿæˆä»£ç çš„åŠ¨æ€ç¨³å®šæ€§**\n> *Prateek Rajput et al.*\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šæŒ‡å‡ºç°æœ‰çš„ä»£ç ç”Ÿæˆè¯„ä¼°åªçœ‹åŠŸèƒ½æ­£ç¡®æ€§æ˜¯ä¸å¤Ÿçš„ã€‚ä½œè€…æå‡ºäº†â€œåŠ¨æ€ç¨³å®šæ€§â€æŒ‡æ ‡ï¼Œå‘ç°åŠŸèƒ½æ­£ç¡®çš„ä»£ç åœ¨ç®—æ³•å¤æ‚åº¦ä¸Šå¯èƒ½å·®å¼‚å·¨å¤§ï¼ˆå¦‚ O(n^2) vs O(n log n)ï¼‰ã€‚å¢åŠ é‡‡æ ·æ¸©åº¦è™½ç„¶èƒ½æé«˜é€šè¿‡ç‡ï¼Œä½†ä¼šé™ä½ä»£ç çš„æ€§èƒ½ç¨³å®šæ€§ã€‚\n\n**23. Language Generation: Complexity Barriers and Implications for Learning**\n> **è¯­è¨€ç”Ÿæˆï¼šå¤æ‚æ€§éšœç¢åŠå…¶å¯¹å­¦ä¹ çš„å¯ç¤º**\n> *Marcelo Arenas et al.*\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šä¸€ç¯‡ç†è®ºæ–‡ç« ã€‚è¯æ˜äº†å³ä½¿å¯¹äºç®€å•çš„è¯­è¨€æ—ï¼ˆå¦‚æ­£åˆ™è¯­è¨€ï¼‰ï¼ŒæˆåŠŸç”Ÿæˆæ‰€éœ€çš„æ ·æœ¬æ•°é‡ä¹Ÿå¯èƒ½æ˜¯å·¨å¤§çš„ï¼Œç”šè‡³ä¸å¯è®¡ç®—ã€‚è¿™è¡¨æ˜ç°ä»£ LLM çš„æˆåŠŸä¸èƒ½ä»…ç”¨ç®€å•çš„å­¦ä¹ ç†è®ºè§£é‡Šï¼Œéœ€è¦è€ƒè™‘è‡ªç„¶è¯­è¨€çš„ç»“æ„å±æ€§ã€‚\n\n---\n\n### ğŸŒ å…¶ä»–æœ‰è¶£çš„ç ”ç©¶\n\n*   **#38 Multi-period Learning for Financial Time Series Forecasting**: æå‡ºäº† MLF æ¡†æ¶ï¼Œå¤„ç†é‡‘èæ—¶é—´åºåˆ—çš„å¤šå‘¨æœŸç‰¹æ€§ï¼Œæå‡é¢„æµ‹å‡†ç¡®æ€§ã€‚\n*   **#92 Assessing the Reliability of Large Language Models in the Bengali Legal Context**: è¯„ä¼°äº† LLM åœ¨å­ŸåŠ æ‹‰æ³•å¾‹è¯­å¢ƒä¸‹çš„è¡¨ç°ï¼Œå‘ç°è™½ç„¶å›ç­”ç»“æ„è‰¯å¥½ï¼Œä½†å¸¸åŒ…å«è™šå‡æ¡ˆä¾‹å¼•ç”¨å’Œæœ‰å®³å»ºè®®ã€‚\n*   **#75 8bit-GPT**: åœ¨è€å¼ Macintosh æ“ä½œç³»ç»Ÿä¸Šæ¨¡æ‹Ÿ LLMï¼Œæ¢è®¨äººæœºäº¤äº’ä¸­çš„â€œæ…¢æŠ€æœ¯â€å’ŒååŠŸèƒ½æ€§è®¾è®¡ã€‚\n*   **#53 Model Merging Improves Zero-Shot Generalization in Bioacoustic Foundation Models**: é€šè¿‡æ¨¡å‹èåˆï¼ˆModel Mergingï¼‰è§£å†³äº†ç”Ÿç‰©å£°å­¦æ¨¡å‹åœ¨æŒ‡ä»¤éµå¾ªå’Œé¢†åŸŸä¸“ä¸šæ€§ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚\n\nå¸Œæœ›ä»Šå¤©çš„å¿«æŠ¥å¯¹ä½ çš„ç ”ç©¶æœ‰æ‰€å¯å‘ï¼æˆ‘ä»¬æ˜å¤©è§ï¼",
  "papers": [
    {
      "arxiv_id": "2511.05769v1",
      "title": "Lived Experience in Dialogue: Co-designing Personalization in Large Language Models to Support Youth Mental Well-being",
      "title_zh": "å¯¹è¯ä¸­çš„ç”Ÿæ´»ä½“éªŒï¼šååŒè®¾è®¡å¤§å‹è¯­è¨€æ¨¡å‹ä¸ªæ€§åŒ–ä»¥æ”¯æŒé’å°‘å¹´å¿ƒç†å¥åº·",
      "authors": [
        "Kathleen W. Guan",
        "Sarthak Giri",
        "Mohammed Amara",
        "Bernard J. Jansen",
        "Enrico Liscio",
        "Milena Esherick",
        "Mohammed Al Owayyed",
        "Ausrine Ratkute",
        "Gayane Sedrakyan",
        "Mark de Reuver",
        "Joao Fernando Ferreira Goncalves",
        "Caroline A. Figueroa"
      ],
      "abstract": "Youth increasingly turn to large language models (LLMs) for mental well-being support, yet current personalization in LLMs can overlook the heterogeneous lived experiences shaping their needs. We conducted a participatory study with youth, parents, and youth care workers (N=38), using co-created youth personas as scaffolds, to elicit community perspectives on how LLMs can facilitate more meaningful personalization to support youth mental well-being. Analysis identified three themes: person-centered contextualization responsive to momentary needs, explicit boundaries around scope and offline referral, and dialogic scaffolding for reflection and autonomy. We mapped these themes to persuasive design features for task suggestions, social facilitation, and system trustworthiness, and created corresponding dialogue extracts to guide LLM fine-tuning. Our findings demonstrate how lived experience can be operationalized to inform design features in LLMs, which can enhance the alignment of LLM-based interventions with the realities of youth and their communities, contributing to more effectively personalized digital well-being tools.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹é’å°‘å¹´è¶Šæ¥è¶Šå¤šåœ°åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯»æ±‚å¿ƒç†å¥åº·æ”¯æŒçš„ç°çŠ¶ï¼ŒæŒ‡å‡ºç°æœ‰çš„ä¸ªæ€§åŒ–æœºåˆ¶å¾€å¾€å¿½è§†äº†å¡‘é€ å…¶éœ€æ±‚çš„å¼‚è´¨æ€§ç”Ÿæ´»ä½“éªŒï¼ˆlived experiencesï¼‰ã€‚ç ”ç©¶å›¢é˜Ÿä¸é’å°‘å¹´ã€å®¶é•¿åŠé’å°‘å¹´æŠ¤ç†å·¥ä½œè€…ï¼ˆN=38ï¼‰å¼€å±•äº†ä¸€é¡¹å‚ä¸å¼ç ”ç©¶ï¼Œåˆ©ç”¨å…±åŒåˆ›é€ çš„é’å°‘å¹´äººç‰©ç”»åƒä½œä¸ºè¾…åŠ©ï¼Œæ¢è®¨LLMså¦‚ä½•å®ç°æ›´æœ‰æ„ä¹‰çš„ä¸ªæ€§åŒ–ä»¥æ”¯æŒé’å°‘å¹´å¿ƒç†å¥åº·ã€‚åˆ†æç¡®å®šäº†ä¸‰ä¸ªæ ¸å¿ƒä¸»é¢˜ï¼šå“åº”å³æ—¶éœ€æ±‚çš„ä»¥äººä¸ºæœ¬çš„æƒ…å¢ƒåŒ–ï¼ˆperson-centered contextualizationï¼‰ã€å…³äºæœåŠ¡èŒƒå›´å’Œçº¿ä¸‹è½¬ä»‹çš„æ˜ç¡®ç•Œé™ã€ä»¥åŠä¿ƒè¿›åæ€å’Œè‡ªä¸»æ€§çš„å¯¹è¯æ”¯æ¶ï¼ˆdialogic scaffoldingï¼‰ã€‚ç ”ç©¶è€…å°†è¿™äº›ä¸»é¢˜æ˜ å°„ä¸ºä»»åŠ¡å»ºè®®ã€ç¤¾ä¼šä¿ƒè¿›å’Œç³»ç»Ÿå¯ä¿¡åº¦ç­‰åŠå¯¼å¼è®¾è®¡ç‰¹å¾ï¼Œå¹¶åˆ›å»ºäº†ç›¸åº”çš„å¯¹è¯æ‘˜å½•ä»¥æŒ‡å¯¼LLMçš„å¾®è°ƒã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå°†ç”Ÿæ´»ä½“éªŒæ“ä½œåŒ–å¯ä»¥ä¸ºLLMçš„è®¾è®¡ç‰¹å¾æä¾›ä¿¡æ¯ï¼Œä»è€Œå¢å¼ºåŸºäºLLMçš„å¹²é¢„æªæ–½ä¸é’å°‘å¹´åŠå…¶ç¤¾åŒºç°å®çš„ä¸€è‡´æ€§ï¼Œæœ‰åŠ©äºæ„å»ºæ›´æœ‰æ•ˆçš„ä¸ªæ€§åŒ–æ•°å­—å¥åº·å·¥å…·ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05769v1",
      "published_date": "2025-11-07 23:53:36 UTC",
      "updated_date": "2025-11-07 23:53:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T13:51:24.061901+00:00"
    },
    {
      "arxiv_id": "2511.05766v1",
      "title": "Anchors in the Machine: Behavioral and Attributional Evidence of Anchoring Bias in LLMs",
      "title_zh": "æœºå™¨ä¹‹é”šï¼šå¤§å‹è¯­è¨€æ¨¡å‹ä¸­é”šå®šåå·®çš„è¡Œä¸ºä¸å½’å› è¯æ®",
      "authors": [
        "Felipe Valencia-Clavijo"
      ],
      "abstract": "Large language models (LLMs) are increasingly examined as both behavioral subjects and decision systems, yet it remains unclear whether observed cognitive biases reflect surface imitation or deeper probability shifts. Anchoring bias, a classic human judgment bias, offers a critical test case. While prior work shows LLMs exhibit anchoring, most evidence relies on surface-level outputs, leaving internal mechanisms and attributional contributions unexplored. This paper advances the study of anchoring in LLMs through three contributions: (1) a log-probability-based behavioral analysis showing that anchors shift entire output distributions, with controls for training-data contamination; (2) exact Shapley-value attribution over structured prompt fields to quantify anchor influence on model log-probabilities; and (3) a unified Anchoring Bias Sensitivity Score integrating behavioral and attributional evidence across six open-source models. Results reveal robust anchoring effects in Gemma-2B, Phi-2, and Llama-2-7B, with attribution signaling that the anchors influence reweighting. Smaller models such as GPT-2, Falcon-RW-1B, and GPT-Neo-125M show variability, suggesting scale may modulate sensitivity. Attributional effects, however, vary across prompt designs, underscoring fragility in treating LLMs as human substitutes. The findings demonstrate that anchoring bias in LLMs is robust, measurable, and interpretable, while highlighting risks in applied domains. More broadly, the framework bridges behavioral science, LLM safety, and interpretability, offering a reproducible path for evaluating other cognitive biases in LLMs.",
      "tldr_zh": "æœ¬æ–‡æ·±å…¥æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„é”šå®šåå·®ï¼ˆAnchoring Biasï¼‰ï¼Œæ—¨åœ¨å˜æ¸…è§‚å¯Ÿåˆ°çš„è®¤çŸ¥åå·®æ˜¯åæ˜ äº†è¡¨é¢çš„æ¨¡ä»¿è¿˜æ˜¯æ›´æ·±å±‚æ¬¡çš„æ¦‚ç‡è½¬ç§»ã€‚ç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ç§ç»“åˆè¡Œä¸ºå­¦å’Œå½’å› åˆ†æçš„æ¡†æ¶ï¼Œé€šè¿‡åŸºäºå¯¹æ•°æ¦‚ç‡ï¼ˆlog-probabilityï¼‰çš„è¡Œä¸ºåˆ†ææ¥è§‚å¯Ÿé”šç‚¹å¦‚ä½•æ”¹å˜è¾“å‡ºåˆ†å¸ƒï¼Œå¹¶åˆ©ç”¨ç²¾ç¡®çš„Shapleyå€¼å½’å› æ¥é‡åŒ–é”šç‚¹å¯¹æ¨¡å‹å†³ç­–çš„å½±å“ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜æå‡ºäº†ç»Ÿä¸€çš„é”šå®šåå·®æ•æ„Ÿåº¦è¯„åˆ†ï¼ˆAnchoring Bias Sensitivity Scoreï¼‰ï¼Œæ•´åˆäº†è¡Œä¸ºå’Œå½’å› è¯æ®å¯¹å…­ä¸ªå¼€æºæ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGemma-2Bã€Phi-2å’ŒLlama-2-7Bç­‰æ¨¡å‹è¡¨ç°å‡ºç¨³å¥çš„é”šå®šæ•ˆåº”ï¼Œè€ŒGPT-2ç­‰è¾ƒå°æ¨¡å‹åˆ™è¡¨ç°å‡ºå˜å¼‚æ€§ï¼Œè¡¨æ˜æ¨¡å‹è§„æ¨¡å¯èƒ½è°ƒèŠ‚æ•æ„Ÿåº¦ã€‚å½’å› åˆ†æè¿›ä¸€æ­¥è¡¨æ˜é”šç‚¹ç¡®å®å½±å“äº†æƒé‡çš„é‡æ–°åˆ†é…ï¼Œä½†è¿™ç§æ•ˆåº”éšæç¤ºè®¾è®¡è€Œå˜åŒ–ï¼Œå‡¸æ˜¾äº†å°†LLMsè§†ä¸ºäººç±»æ›¿ä»£å“çš„è„†å¼±æ€§ã€‚è¯¥ç ”ç©¶æœ€ç»ˆè¯æ˜äº†LLMsä¸­çš„é”šå®šåå·®æ˜¯å¯æµ‹é‡ä¸”å¯è§£é‡Šçš„ï¼Œä¸ºè¿æ¥è¡Œä¸ºç§‘å­¦ã€LLMå®‰å…¨æ€§å’Œå¯è§£é‡Šæ€§æä¾›äº†å¯å¤ç°çš„è¯„ä¼°è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "econ.GN"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05766v1",
      "published_date": "2025-11-07 23:35:19 UTC",
      "updated_date": "2025-11-07 23:35:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T13:51:36.644610+00:00"
    },
    {
      "arxiv_id": "2511.11632v1",
      "title": "Toward Better Generalization in Few-Shot Learning through the Meta-Component Combination",
      "title_zh": "é€šè¿‡å…ƒç»„ä»¶ç»„åˆæå‡å°æ ·æœ¬å­¦ä¹ çš„æ³›åŒ–èƒ½åŠ›",
      "authors": [
        "Qiuhao Zeng"
      ],
      "abstract": "In few-shot learning, classifiers are expected to generalize to unseen classes given only a small number of instances of each new class. One of the popular solutions to few-shot learning is metric-based meta-learning. However, it highly depends on the deep metric learned on seen classes, which may overfit to seen classes and fail to generalize well on unseen classes. To improve the generalization, we explore the substructures of classifiers and propose a novel meta-learning algorithm to learn each classifier as a combination of meta-components. Meta-components are learned across meta-learning episodes on seen classes and disentangled by imposing an orthogonal regularizer to promote its diversity and capture various shared substructures among different classifiers. Extensive experiments on few-shot benchmark tasks show superior performances of the proposed method.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Few-Shot Learningä¸­åˆ†ç±»å™¨éš¾ä»¥æ³›åŒ–åˆ°æœªè§ç±»åˆ«çš„é—®é¢˜ï¼ŒæŒ‡å‡ºå½“å‰æµè¡Œçš„metric-based meta-learningæ–¹æ³•é«˜åº¦ä¾èµ–äºåœ¨å·²çŸ¥ç±»åˆ«ä¸Šå­¦ä¹ çš„æ·±åº¦åº¦é‡ï¼Œå®¹æ˜“å¯¼è‡´è¿‡æ‹Ÿåˆã€‚ä¸ºäº†æ”¹å–„æ³›åŒ–æ€§èƒ½ï¼Œä½œè€…æ¢ç´¢äº†åˆ†ç±»å™¨çš„å­ç»“æ„ï¼Œå¹¶æå‡ºäº†ä¸€ç§é€šè¿‡Meta-Component Combinationæ¥æ„å»ºåˆ†ç±»å™¨çš„æ–°å‹å…ƒå­¦ä¹ ç®—æ³•ã€‚è¯¥æ–¹æ³•åœ¨å…ƒå­¦ä¹ episodesä¸­å­¦ä¹ Meta-componentsï¼Œå¹¶é€šè¿‡å¼•å…¥orthogonal regularizeræ¥è§£è€¦è¿™äº›ç»„ä»¶ï¼Œä»è€Œä¿ƒè¿›å¤šæ ·æ€§å¹¶æ•æ‰ä¸åŒåˆ†ç±»å™¨é—´å…±äº«çš„å­ç»“æ„ã€‚åœ¨å¹¿æ³›çš„Few-Shot benchmarkä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç›¸æ¯”ç°æœ‰æ–¹æ¡ˆå…·æœ‰æ›´ä¼˜è¶Šçš„æ€§èƒ½è¡¨ç°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "20 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.11632v1",
      "published_date": "2025-11-07 23:33:49 UTC",
      "updated_date": "2025-11-07 23:33:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T13:52:33.404527+00:00"
    },
    {
      "arxiv_id": "2511.05759v1",
      "title": "Language Generation: Complexity Barriers and Implications for Learning",
      "title_zh": "è¯­è¨€ç”Ÿæˆï¼šå¤æ‚æ€§å£å’åŠå…¶å¯¹å­¦ä¹ çš„å¯ç¤º",
      "authors": [
        "Marcelo Arenas",
        "Pablo BarcelÃ³",
        "Luis CofrÃ©",
        "Alexander Kozachinskiy"
      ],
      "abstract": "Kleinberg and Mullainathan showed that, in principle, language generation is always possible: with sufficiently many positive examples, a learner can eventually produce sentences indistinguishable from those of a target language. However, the existence of such a guarantee does not speak to its practical feasibility. In this work, we show that even for simple and well-studied language families -- such as regular and context-free languages -- the number of examples required for successful generation can be extraordinarily large, and in some cases not bounded by any computable function. These results reveal a substantial gap between theoretical possibility and efficient learnability. They suggest that explaining the empirical success of modern language models requires a refined perspective -- one that takes into account structural properties of natural language that make effective generation possible in practice.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Kleinbergå’ŒMullainathanå…³äºè¯­è¨€ç”Ÿæˆç†è®ºå¯èƒ½æ€§çš„è§‚ç‚¹ï¼ŒæŒ‡å‡ºäº†å…¶åœ¨å®é™…å¯è¡Œæ€§ä¸Šçš„å±€é™ã€‚ä½œè€…è¯æ˜ï¼Œå³ä½¿å¯¹äºæ­£åˆ™è¯­è¨€(regular languages)å’Œä¸Šä¸‹æ–‡æ— å…³è¯­è¨€(context-free languages)ç­‰ç®€å•çš„è¯­è¨€æ—ï¼Œå®ç°æˆåŠŸç”Ÿæˆæ‰€éœ€çš„æ ·æœ¬æ•°é‡ä¹Ÿæå…¶åºå¤§ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³æ— æ³•é€šè¿‡ä»»ä½•å¯è®¡ç®—å‡½æ•°è¿›è¡Œç•Œå®šã€‚è¿™äº›å‘ç°æ­ç¤ºäº†ç†è®ºå¯èƒ½æ€§ä¸é«˜æ•ˆå¯å­¦ä¹ æ€§(efficient learnability)ä¹‹é—´å­˜åœ¨æ˜¾è‘—é¸¿æ²Ÿã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¦è§£é‡Šç°ä»£è¯­è¨€æ¨¡å‹(Language Models)åœ¨ç»éªŒä¸Šçš„æˆåŠŸï¼Œå¿…é¡»é‡‡ç”¨ä¸€ç§æ”¹è¿›çš„è§†è§’ï¼Œå……åˆ†è€ƒè™‘é‚£äº›ä½¿æœ‰æ•ˆç”Ÿæˆåœ¨å®è·µä¸­æˆä¸ºå¯èƒ½çš„è‡ªç„¶è¯­è¨€ç»“æ„å±æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.FL",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05759v1",
      "published_date": "2025-11-07 23:06:48 UTC",
      "updated_date": "2025-11-07 23:06:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T13:52:19.696356+00:00"
    },
    {
      "arxiv_id": "2511.05747v2",
      "title": "CoT-X: An Adaptive Framework for Cross-Model Chain-of-Thought Transfer and Optimization",
      "title_zh": "CoT-Xï¼šè·¨æ¨¡å‹æ€ç»´é“¾è¿ç§»ä¸ä¼˜åŒ–çš„è‡ªé€‚åº”æ¡†æ¶",
      "authors": [
        "Ziqian Bi",
        "Kaijie Chen",
        "Tianyang Wang",
        "Junfeng Hao",
        "Benji Peng",
        "Xinyuan Song"
      ],
      "abstract": "Chain-of-Thought (CoT) reasoning enhances the problem-solving ability of large language models (LLMs) but leads to substantial inference overhead, limiting deployment in resource-constrained settings. This paper investigates efficient CoT transfer across models of different scales and architectures through an adaptive reasoning summarization framework. The proposed method compresses reasoning traces via semantic segmentation with importance scoring, budget-aware dynamic compression, and coherence reconstruction, preserving critical reasoning steps while significantly reducing token usage. Experiments on 7{,}501 medical examination questions across 10 specialties show up to 40% higher accuracy than truncation under the same token budgets. Evaluations on 64 model pairs from eight LLMs (1.5B-32B parameters, including DeepSeek-R1 and Qwen3) confirm strong cross-model transferability. Furthermore, a Gaussian Process-based Bayesian optimization module reduces evaluation cost by 84% and reveals a power-law relationship between model size and cross-domain robustness. These results demonstrate that reasoning summarization provides a practical path toward efficient CoT transfer, enabling advanced reasoning under tight computational constraints. Code will be released upon publication.",
      "tldr_zh": "è¯¥è®ºæ–‡æå‡ºäº†CoT-Xï¼Œä¸€ç§é’ˆå¯¹è·¨æ¨¡å‹Chain-of-Thought (CoT)è¿ç§»å’Œä¼˜åŒ–çš„è‡ªé€‚åº”æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³CoTæ¨ç†å¸¦æ¥çš„é«˜æ¨ç†å¼€é”€é—®é¢˜ï¼Œä»¥ä¾¿åœ¨èµ„æºå—é™ç¯å¢ƒä¸­éƒ¨ç½²ã€‚è¯¥æ–¹æ³•é€šè¿‡è‡ªé€‚åº”æ¨ç†æ‘˜è¦æ¡†æ¶ï¼Œåˆ©ç”¨å¸¦æœ‰é‡è¦æ€§è¯„åˆ†çš„è¯­ä¹‰åˆ†å‰²ã€é¢„ç®—æ„ŸçŸ¥çš„åŠ¨æ€å‹ç¼©ä»¥åŠè¿è´¯æ€§é‡æ„æŠ€æœ¯ï¼Œåœ¨å¤§å¹…å‡å°‘tokenä½¿ç”¨çš„åŒæ—¶ä¿ç•™å…³é”®æ¨ç†æ­¥éª¤ã€‚åœ¨æ¶µç›–10ä¸ªä¸“ç§‘çš„7,501é“åŒ»å­¦è€ƒè¯•é¢˜ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼Œåœ¨ç›¸åŒtokené¢„ç®—ä¸‹ï¼Œè¯¥æ–¹æ³•æ¯”æˆªæ–­æ³•å‡†ç¡®ç‡æé«˜äº†é«˜è¾¾40%ã€‚æ¶‰åŠ8ç§LLMï¼ˆåŒ…æ‹¬DeepSeek-R1å’ŒQwen3ï¼‰çš„64ä¸ªæ¨¡å‹å¯¹çš„è¯„ä¼°è¯å®äº†å…¶å¼ºå¤§çš„è·¨æ¨¡å‹è¿ç§»èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå¼•å…¥åŸºäºé«˜æ–¯è¿‡ç¨‹çš„è´å¶æ–¯ä¼˜åŒ–æ¨¡å—å°†è¯„ä¼°æˆæœ¬é™ä½äº†84%ï¼Œå¹¶æ­ç¤ºäº†æ¨¡å‹è§„æ¨¡ä¸è·¨åŸŸé²æ£’æ€§ä¹‹é—´çš„å¹‚å¾‹å…³ç³»ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæ¨ç†æ‘˜è¦ä¸ºåœ¨è®¡ç®—å—é™æ¡ä»¶ä¸‹å®ç°é«˜æ•ˆCoTè¿ç§»æä¾›äº†ä¸€æ¡åˆ‡å®å¯è¡Œçš„è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "TKDD 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.05747v2",
      "published_date": "2025-11-07 22:35:31 UTC",
      "updated_date": "2025-12-02 16:17:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T13:52:47.358094+00:00"
    },
    {
      "arxiv_id": "2511.05745v1",
      "title": "Beyond Redundancy: Diverse and Specialized Multi-Expert Sparse Autoencoder",
      "title_zh": "è¶…è¶Šå†—ä½™ï¼šå¤šæ ·åŒ–ä¸ä¸“ç²¾åŒ–çš„å¤šä¸“å®¶ç¨€ç–è‡ªç¼–ç å™¨",
      "authors": [
        "Zhen Xu",
        "Zhen Tan",
        "Song Wang",
        "Kaidi Xu",
        "Tianlong Chen"
      ],
      "abstract": "Sparse autoencoders (SAEs) have emerged as a powerful tool for interpreting large language models (LLMs) by decomposing token activations into combinations of human-understandable features. While SAEs provide crucial insights into LLM explanations, their practical adoption faces a fundamental challenge: better interpretability demands that SAEs' hidden layers have high dimensionality to satisfy sparsity constraints, resulting in prohibitive training and inference costs. Recent Mixture of Experts (MoE) approaches attempt to address this by partitioning SAEs into narrower expert networks with gated activation, thereby reducing computation. In a well-designed MoE, each expert should focus on learning a distinct set of features. However, we identify a \\textit{critical limitation} in MoE-SAE: Experts often fail to specialize, which means they frequently learn overlapping or identical features. To deal with it, we propose two key innovations: (1) Multiple Expert Activation that simultaneously engages semantically weighted expert subsets to encourage specialization, and (2) Feature Scaling that enhances diversity through adaptive high-frequency scaling. Experiments demonstrate a 24\\% lower reconstruction error and a 99\\% reduction in feature redundancy compared to existing MoE-SAE methods. This work bridges the interpretability-efficiency gap in LLM analysis, allowing transparent model inspection without compromising computational feasibility.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”¨äºè§£é‡Šå¤§å‹è¯­è¨€æ¨¡å‹(LLMs)çš„ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨(SAEs)é¢ä¸´çš„è®¡ç®—æˆæœ¬é«˜æ˜‚é—®é¢˜ï¼ŒæŒ‡å‡ºäº†ç°æœ‰æ··åˆä¸“å®¶(MoE)æ–¹æ³•ä¸­ä¸“å®¶æ— æ³•æœ‰æ•ˆä¸“ä¸šåŒ–ä¸”å¾€å¾€å­¦ä¹ é‡å¤ç‰¹å¾çš„å…³é”®å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†ä¸¤é¡¹æ ¸å¿ƒåˆ›æ–°ï¼šå¤šä¸“å®¶æ¿€æ´»(Multiple Expert Activation)ï¼Œé€šè¿‡åŒæ—¶æ¿€æ´»è¯­ä¹‰åŠ æƒçš„ä¸“å®¶å­é›†æ¥ä¿ƒè¿›ä¸“ä¸šåŒ–ï¼›ä»¥åŠç‰¹å¾ç¼©æ”¾(Feature Scaling)ï¼Œé€šè¿‡è‡ªé€‚åº”é«˜é¢‘ç¼©æ”¾å¢å¼ºç‰¹å¾å¤šæ ·æ€§ã€‚è¿™ç§æ–°æ–¹æ³•æ—¨åœ¨è§£å†³MoE-SAEä¸­çš„ç‰¹å¾å†—ä½™é—®é¢˜ï¼Œç¡®ä¿æ¯ä¸ªä¸“å®¶ä¸“æ³¨äºç‹¬ç‰¹çš„ç‰¹å¾é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰çš„MoE-SAEæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å°†é‡å»ºè¯¯å·®é™ä½äº†24%ï¼Œå¹¶å°†ç‰¹å¾å†—ä½™æƒŠäººåœ°å‡å°‘äº†99%ã€‚è¿™é¡¹å·¥ä½œæœ‰æ•ˆå¼¥åˆäº†LLMåˆ†æä¸­å¯è§£é‡Šæ€§ä¸æ•ˆç‡ä¹‹é—´çš„å·®è·ï¼Œåœ¨ä¸ç‰ºç‰²è®¡ç®—å¯è¡Œæ€§çš„å‰æä¸‹å®ç°äº†é€æ˜çš„æ¨¡å‹æ£€æŸ¥ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05745v1",
      "published_date": "2025-11-07 22:19:34 UTC",
      "updated_date": "2025-11-07 22:19:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T13:53:08.725600+00:00"
    },
    {
      "arxiv_id": "2511.05728v1",
      "title": "Compressing Chemistry Reveals Functional Groups",
      "title_zh": "å‹ç¼©åŒ–å­¦æ­ç¤ºå®˜èƒ½å›¢",
      "authors": [
        "Ruben Sharma",
        "Ross D. King"
      ],
      "abstract": "We introduce the first formal large-scale assessment of the utility of traditional chemical functional groups as used in chemical explanations. Our assessment employs a fundamental principle from computational learning theory: a good explanation of data should also compress the data. We introduce an unsupervised learning algorithm based on the Minimum Message Length (MML) principle that searches for substructures that compress around three million biologically relevant molecules. We demonstrate that the discovered substructures contain most human-curated functional groups as well as novel larger patterns with more specific functions. We also run our algorithm on 24 specific bioactivity prediction datasets to discover dataset-specific functional groups. Fingerprints constructed from dataset-specific functional groups are shown to significantly outperform other fingerprint representations, including the MACCS and Morgan fingerprint, when training ridge regression models on bioactivity regression tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†å¯¹ä¼ ç»ŸåŒ–å­¦å®˜èƒ½å›¢æ•ˆç”¨çš„é¦–æ¬¡å¤§è§„æ¨¡æ­£å¼è¯„ä¼°ï¼ŒåŸºäºè®¡ç®—å­¦ä¹ ç†è®ºä¸­â€œå¥½çš„è§£é‡Šå³æ•°æ®å‹ç¼©â€çš„åŸåˆ™ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åŸºäºæœ€å°æ¶ˆæ¯é•¿åº¦(Minimum Message Length, MML)åŸåˆ™çš„æ— ç›‘ç£å­¦ä¹ ç®—æ³•ï¼Œæ—¨åœ¨æœç´¢èƒ½å¤Ÿå‹ç¼©çº¦ä¸‰ç™¾ä¸‡ä¸ªç”Ÿç‰©ç›¸å…³åˆ†å­çš„å­ç»“æ„ã€‚ç ”ç©¶å‘ç°ï¼Œè¯¥ç®—æ³•æŒ–æ˜å‡ºçš„å­ç»“æ„ä¸ä»…æ¶µç›–äº†å¤§å¤šæ•°äººå·¥å®šä¹‰çš„å®˜èƒ½å›¢ï¼Œè¿˜æ­ç¤ºäº†å…·æœ‰æ›´å…·ä½“åŠŸèƒ½çš„æ–°å‹æ›´å¤§æ¨¡å¼ã€‚é€šè¿‡åœ¨24ä¸ªç”Ÿç‰©æ´»æ€§é¢„æµ‹æ•°æ®é›†ä¸Šè¿è¡Œè¯¥ç®—æ³•ï¼Œç ”ç©¶å›¢é˜Ÿå‘ç°äº†ç‰¹å®šäºæ•°æ®é›†çš„å®˜èƒ½å›¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ç”Ÿç‰©æ´»æ€§å›å½’ä»»åŠ¡ä¸­è®­ç»ƒå²­å›å½’æ¨¡å‹æ—¶ï¼ŒåŸºäºè¿™äº›ç‰¹å®šå®˜èƒ½å›¢æ„å»ºçš„æŒ‡çº¹(Fingerprints)æ˜¾è‘—ä¼˜äºMACCSå’ŒMorganæŒ‡çº¹ç­‰ä¼ ç»Ÿè¡¨ç¤ºæ–¹æ³•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05728v1",
      "published_date": "2025-11-07 21:38:04 UTC",
      "updated_date": "2025-11-07 21:38:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T13:54:00.483657+00:00"
    },
    {
      "arxiv_id": "2511.05722v1",
      "title": "OckBench: Measuring the Efficiency of LLM Reasoning",
      "title_zh": "OckBenchï¼šå¤§è¯­è¨€æ¨¡å‹æ¨ç†æ•ˆç‡çš„åº¦é‡",
      "authors": [
        "Zheng Du",
        "Hao Kang",
        "Song Han",
        "Tushar Krishna",
        "Ligeng Zhu"
      ],
      "abstract": "Large language models such as GPT-4, Claude 3, and the Gemini series have improved automated reasoning and code generation. However, existing benchmarks mainly focus on accuracy and output quality, and they ignore an important factor: decoding token efficiency. In real systems, generating 10,000 tokens versus 100,000 tokens leads to large differences in latency, cost, and energy. In this work, we introduce OckBench, a model-agnostic and hardware-agnostic benchmark that evaluates both accuracy and token count for reasoning and coding tasks. Through experiments comparing multiple open- and closed-source models, we uncover that many models with comparable accuracy differ wildly in token consumption, revealing that efficiency variance is a neglected but significant axis of differentiation. We further demonstrate Pareto frontiers over the accuracy-efficiency plane and argue for an evaluation paradigm shift: we should no longer treat tokens as \"free\" to multiply. OckBench provides a unified platform for measuring, comparing, and guiding research in token-efficient reasoning. Our benchmarks are available at https://ockbench.github.io/ .",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†OckBenchï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†æ•ˆç‡çš„åŸºå‡†æµ‹è¯•å·¥å…·ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åŸºå‡†ä»…å…³æ³¨å‡†ç¡®æ€§è€Œå¿½è§†è§£ç Tokenæ•ˆç‡ï¼ˆdecoding token efficiencyï¼‰çš„é—®é¢˜ã€‚åœ¨å®é™…ç³»ç»Ÿä¸­ï¼Œç”ŸæˆTokençš„æ•°é‡ç›´æ¥å†³å®šäº†å»¶è¿Ÿã€æˆæœ¬å’Œèƒ½æºæ¶ˆè€—ã€‚OckBenchä½œä¸ºä¸€ä¸ªä¸æ¨¡å‹å’Œç¡¬ä»¶æ— å…³çš„åŸºå‡†ï¼Œèƒ½å¤ŸåŒæ—¶è¯„ä¼°æ¨ç†å’Œä»£ç ç”Ÿæˆä»»åŠ¡çš„å‡†ç¡®æ€§ä¸Tokenæ•°é‡ã€‚é€šè¿‡å¯¹æ¯”å¤šä¸ªå¼€æºå’Œé—­æºæ¨¡å‹ï¼Œå®éªŒå‘ç°è®¸å¤šå‡†ç¡®ç‡ç›¸å½“çš„æ¨¡å‹åœ¨Tokenæ¶ˆè€—ä¸Šå­˜åœ¨å·¨å¤§å·®å¼‚ï¼Œæ­ç¤ºäº†æ•ˆç‡å·®å¼‚æ˜¯ä¸€ä¸ªè¢«å¿½è§†ä½†æå…¶é‡è¦çš„åŒºåˆ†ç»´åº¦ã€‚è¯¥ç ”ç©¶è¿›ä¸€æ­¥å±•ç¤ºäº†å‡†ç¡®æ€§-æ•ˆç‡å¹³é¢ä¸Šçš„å¸•ç´¯æ‰˜å‰æ²¿ï¼ˆPareto frontiersï¼‰ï¼Œå¹¶ä¸»å¼ è¯„ä¼°èŒƒå¼çš„è½¬å˜ï¼Œå³ä¸å†å°†Tokenè§†ä¸ºå¯ä»¥éšæ„å¢åŠ çš„â€œå…è´¹â€èµ„æºã€‚OckBenchä¸ºè¡¡é‡ã€æ¯”è¾ƒå’ŒæŒ‡å¯¼Tokené«˜æ•ˆæ¨ç†çš„ç ”ç©¶æä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„å¹³å°ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05722v1",
      "published_date": "2025-11-07 21:29:41 UTC",
      "updated_date": "2025-11-07 21:29:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T13:54:56.993101+00:00"
    },
    {
      "arxiv_id": "2511.05706v2",
      "title": "AdvisingWise: Supporting Academic Advising in Higher Education Settings Through a Human-in-the-Loop Multi-Agent Framework",
      "title_zh": "AdvisingWiseï¼šåŸºäºäººåœ¨å›è·¯å¤šæ™ºèƒ½ä½“æ¡†æ¶æ”¯æŒé«˜ç­‰æ•™è‚²å­¦æœ¯æŒ‡å¯¼",
      "authors": [
        "Wendan Jiang",
        "Shiyuan Wang",
        "Hiba Eltigani",
        "Rukhshan Haroon",
        "Abdullah Bin Faisal",
        "Fahad Dogar"
      ],
      "abstract": "Academic advising is critical to student success in higher education, yet high student-to-advisor ratios limit advisors' capacity to provide timely support, particularly during peak periods. Recent advances in Large Language Models (LLMs) present opportunities to enhance the advising process. We present AdvisingWise, a multi-agent system that automates time-consuming tasks, such as information retrieval and response drafting, while preserving human oversight. AdvisingWise leverages authoritative institutional resources and adaptively prompts students about their academic backgrounds to generate reliable, personalized responses. All system responses undergo human advisor validation before delivery to students. We evaluate AdvisingWise through a mixed-methods approach: (1) expert evaluation on responses of 20 sample queries, (2) LLM-as-a-judge evaluation of the information retrieval strategy, and (3) a user study with 8 academic advisors to assess the system's practical utility. Our evaluation shows that AdvisingWise produces accurate, personalized responses. Advisors reported increasingly positive perceptions after using AdvisingWise, as their initial concerns about reliability and personalization diminished. We conclude by discussing the implications of human-AI synergy on the practice of academic advising.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é«˜ç­‰æ•™è‚²ä¸­å­¦æœ¯å’¨è¯¢é¢ä¸´çš„é«˜ç”Ÿå¸ˆæ¯”æŒ‘æˆ˜ï¼Œæå‡ºäº†AdvisingWiseï¼Œä¸€ç§åŸºäºäººæœºå›ç¯(Human-in-the-Loop)çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)è‡ªåŠ¨åŒ–ä¿¡æ¯æ£€ç´¢å’Œå›å¤èµ·è‰ç­‰è€—æ—¶ä»»åŠ¡ï¼Œå¹¶ç»“åˆæƒå¨æœºæ„èµ„æºä¸è‡ªé€‚åº”æç¤ºæ¥è·å–å­¦ç”ŸèƒŒæ™¯ï¼Œä»è€Œç”Ÿæˆå¯é ä¸”ä¸ªæ€§åŒ–çš„å»ºè®®ã€‚AdvisingWiseä¿ç•™äº†äººç±»ç›‘ç£æœºåˆ¶ï¼Œç¡®ä¿æ‰€æœ‰ç³»ç»Ÿç”Ÿæˆçš„å›å¤åœ¨å‘é€ç»™å­¦ç”Ÿå‰å‡ç»è¿‡äººç±»é¡¾é—®çš„éªŒè¯ã€‚é€šè¿‡æ··åˆæ–¹æ³•è¯„ä¼°ï¼ˆåŒ…æ‹¬ä¸“å®¶è¯„ä¼°ã€LLM-as-a-judgeä»¥åŠ8åå­¦æœ¯é¡¾é—®çš„ç”¨æˆ·ç ”ç©¶ï¼‰ï¼Œç»“æœæ˜¾ç¤ºè¯¥ç³»ç»Ÿèƒ½äº§ç”Ÿå‡†ç¡®ä¸”ä¸ªæ€§åŒ–çš„å›å¤ã€‚ç ”ç©¶å‘ç°ï¼Œé¡¾é—®åœ¨ä½¿ç”¨ç³»ç»Ÿåæ¶ˆé™¤äº†æœ€åˆå¯¹å¯é æ€§çš„æ‹…å¿§ï¼Œæ€åº¦è½¬ä¸ºç§¯æï¼Œè¿™ä¸ºå­¦æœ¯å’¨è¯¢ä¸­çš„äººæœºååŒ(human-AI synergy)æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "18 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.05706v2",
      "published_date": "2025-11-07 20:55:24 UTC",
      "updated_date": "2025-12-02 02:29:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T13:55:35.640163+00:00"
    },
    {
      "arxiv_id": "2511.05705v1",
      "title": "Long Grounded Thoughts: Distilling Compositional Visual Reasoning Chains at Scale",
      "title_zh": "Long Grounded Thoughtsï¼šå¤§è§„æ¨¡è’¸é¦ç»„åˆå¼è§†è§‰æ¨ç†é“¾",
      "authors": [
        "David Acuna",
        "Chao-Han Huck Yang",
        "Yuntian Deng",
        "Jaehun Jung",
        "Ximing Lu",
        "Prithviraj Ammanabrolu",
        "Hyunwoo Kim",
        "Yuan-Hong Liao",
        "Yejin Choi"
      ],
      "abstract": "Recent progress in multimodal reasoning has been driven largely by undisclosed datasets and proprietary data synthesis recipes, leaving open questions about how to systematically build large-scale, vision-centric reasoning datasets, particularly for tasks that go beyond visual math. In this work, we introduce a new reasoning data generation framework spanning diverse skills and levels of complexity with over 1M high-quality synthetic vision-centric questions. The dataset also includes preference data and instruction prompts supporting both offline and online RL. Our synthesis framework proceeds in two stages: (1) scale; and (2) complexity. Reasoning traces are then synthesized through a two-stage process that leverages VLMs and reasoning LLMs, producing CoT traces for VLMs that capture the richness and diverse cognitive behaviors found in frontier reasoning models. Remarkably, we show that finetuning Qwen2.5-VL-7B on our data outperforms all open-data baselines across all evaluated vision-centric benchmarks, and even surpasses strong closed-data models such as MiMo-VL-7B-RL on V* Bench, CV-Bench and MMStar-V. Perhaps most surprising, despite being entirely vision-centric, our data transfers positively to text-only reasoning (MMLU-Pro) and audio reasoning (MMAU), demonstrating its effectiveness. Similarly, despite not containing videos or embodied visual data, we observe notable gains when evaluating on a single-evidence embodied QA benchmark (NiEH). Finally, we use our data to analyze the entire VLM post-training pipeline. Our empirical analysis highlights that (i) SFT on high-quality data with non-linear reasoning traces is essential for effective online RL, (ii) staged offline RL matches online RL's performance while reducing compute demands, and (iii) careful SFT on high quality data can substantially improve out-of-domain, cross-modality transfer.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€æ¨ç†é¢†åŸŸç¼ºä¹å…¬å¼€æ•°æ®é›†å’Œç³»ç»ŸåŒ–æ•°æ®åˆæˆæ–¹æ³•çš„ç°çŠ¶ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ¨ç†æ•°æ®ç”Ÿæˆæ¡†æ¶ï¼Œæ„å»ºäº†åŒ…å«è¶…è¿‡100ä¸‡ä¸ªé«˜è´¨é‡åˆæˆè§†è§‰ä¸­å¿ƒé—®é¢˜çš„æ•°æ®é›†ã€‚è¯¥æ¡†æ¶é€šè¿‡è§„æ¨¡åŒ–å’Œå¤æ‚åŒ–ä¸¤ä¸ªé˜¶æ®µï¼Œåˆ©ç”¨VLMså’Œæ¨ç†LLMsç”ŸæˆåŒ…å«ä¸°å¯Œè®¤çŸ¥è¡Œä¸ºçš„Chain-of-Thought (CoT)æ¨ç†è½¨è¿¹ï¼Œå¹¶æä¾›äº†æ”¯æŒç¦»çº¿å’Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ (RL)çš„åå¥½æ•°æ®åŠæŒ‡ä»¤æç¤ºã€‚å®éªŒæ˜¾ç¤ºï¼Œåœ¨ç”Ÿæˆçš„åˆæˆæ•°æ®ä¸Šå¾®è°ƒQwen2.5-VL-7Bæ¨¡å‹ï¼Œå…¶è¡¨ç°è¶…è¶Šäº†æ‰€æœ‰å¼€æ”¾æ•°æ®åŸºçº¿ï¼Œç”šè‡³åœ¨V* Benchã€CV-Benchå’ŒMMStar-Vç­‰åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºMiMo-VL-7B-RLç­‰å¼ºå¤§çš„é—­æºæ•°æ®æ¨¡å‹ã€‚å°½ç®¡æ•°æ®å®Œå…¨ä»¥è§†è§‰ä¸ºä¸­å¿ƒï¼Œä½†è¯¥æ–¹æ³•åœ¨çº¯æ–‡æœ¬æ¨ç†(MMLU-Pro)ã€éŸ³é¢‘æ¨ç†(MMAU)ä»¥åŠå…·èº«é—®ç­”(NiEH)ä»»åŠ¡ä¸Šå‡å±•ç°å‡ºæ˜¾è‘—çš„æ­£å‘è¿ç§»æ•ˆæœã€‚æ­¤å¤–ï¼Œé€šè¿‡å¯¹VLMåè®­ç»ƒæµç¨‹çš„å®è¯åˆ†æï¼Œç ”ç©¶å‘ç°åŸºäºéçº¿æ€§æ¨ç†è½¨è¿¹çš„é«˜è´¨é‡ç›‘ç£å¾®è°ƒ(SFT)å¯¹äºæœ‰æ•ˆçš„åœ¨çº¿RLè‡³å…³é‡è¦ï¼Œä¸”åˆ†é˜¶æ®µçš„ç¦»çº¿RLèƒ½åœ¨é™ä½è®¡ç®—éœ€æ±‚çš„åŒæ—¶è¾¾åˆ°ä¸åœ¨çº¿RLç›¸å½“çš„æ€§èƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "Project Page: https://nvlabs.github.io/LongGroundedThoughts/",
      "pdf_url": "https://arxiv.org/pdf/2511.05705v1",
      "published_date": "2025-11-07 20:50:54 UTC",
      "updated_date": "2025-11-07 20:50:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T13:55:52.526915+00:00"
    },
    {
      "arxiv_id": "2511.05704v2",
      "title": "TabDistill: Distilling Transformers into Neural Nets for Few-Shot Tabular Classification",
      "title_zh": "TabDistillï¼šé¢å‘å°æ ·æœ¬è¡¨æ ¼åˆ†ç±»å°†Transformerè’¸é¦ä¸ºç¥ç»ç½‘ç»œ",
      "authors": [
        "Pasan Dissanayake",
        "Sanghamitra Dutta"
      ],
      "abstract": "Transformer-based models have shown promising performance on tabular data compared to their classical counterparts such as neural networks and Gradient Boosted Decision Trees (GBDTs) in scenarios with limited training data. They utilize their pre-trained knowledge to adapt to new domains, achieving commendable performance with only a few training examples, also called the few-shot regime. However, the performance gain in the few-shot regime comes at the expense of significantly increased complexity and number of parameters. To circumvent this trade-off, we introduce TabDistill, a new strategy to distill the pre-trained knowledge in complex transformer-based models into simpler neural networks for effectively classifying tabular data. Our framework yields the best of both worlds: being parameter-efficient while performing well with limited training data. The distilled neural networks surpass classical baselines such as regular neural networks, XGBoost and logistic regression under equal training data, and in some cases, even the original transformer-based models that they were distilled from.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºTransformerçš„æ¨¡å‹åœ¨å¤„ç†å°‘æ ·æœ¬è¡¨æ ¼æ•°æ®æ—¶è™½ç„¶æ€§èƒ½ä¼˜å¼‚ä½†è®¡ç®—å¤æ‚åº¦é«˜çš„é—®é¢˜ï¼Œæå‡ºäº†TabDistillæ¡†æ¶ã€‚è¯¥æ–¹æ³•æ—¨åœ¨å°†å¤æ‚çš„Transformeræ¨¡å‹ä¸­çš„é¢„è®­ç»ƒçŸ¥è¯†è’¸é¦åˆ°æ›´ç®€å•çš„ç¥ç»ç½‘ç»œ(Neural Networks)ä¸­ï¼Œä»¥å®ç°é«˜æ•ˆçš„è¡¨æ ¼åˆ†ç±»ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒTabDistillåœ¨ä¿æŒå‚æ•°æ•ˆç‡çš„åŒæ—¶ï¼Œåˆ©ç”¨æœ‰é™çš„è®­ç»ƒæ•°æ®å®ç°äº†é«˜æ€§èƒ½ï¼Œå…¼é¡¾äº†æ¨¡å‹çš„è½»é‡åŒ–ä¸å‡†ç¡®æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè’¸é¦åçš„ç¥ç»ç½‘ç»œåœ¨åŒç­‰è®­ç»ƒæ•°æ®æ¡ä»¶ä¸‹ï¼Œæ€§èƒ½è¶…è¶Šäº†å¸¸è§„ç¥ç»ç½‘ç»œã€XGBoostå’Œé€»è¾‘å›å½’ç­‰ç»å…¸åŸºçº¿æ¨¡å‹ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹ä¼˜äºå…¶åŸå§‹çš„Transformeræ•™å¸ˆæ¨¡å‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05704v2",
      "published_date": "2025-11-07 20:46:45 UTC",
      "updated_date": "2025-11-20 13:31:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T13:59:28.670918+00:00"
    },
    {
      "arxiv_id": "2601.06031v1",
      "title": "Beyond Clicking:A Step Towards Generalist GUI Grounding via Text Dragging",
      "title_zh": "è¶…è¶Šç‚¹å‡»ï¼šé€šè¿‡æ–‡æœ¬æ‹–æ‹½è¿ˆå‘é€šç”¨GUIå®šä½",
      "authors": [
        "Zeyi Liao",
        "Yadong Lu",
        "Boyu Gou",
        "Huan Sun",
        "Ahmed Awadallah"
      ],
      "abstract": "Graphical user interface (GUI) grounding, the process of mapping human instructions to GUI actions, serves as a fundamental basis to autonomous GUI agents. While existing grounding models achieve promising performance to simulate the mouse click action on various click-based benchmarks, another essential mode of mouse interaction, namely dragging, remains largely underexplored. Yet, dragging the mouse to select and manipulate textual content represents a prevalent and important usage in practical GUI scenarios. To narrow this gap, we first introduce GUI-Drag, a diverse dataset of 161K text dragging examples synthesized through a scalable pipeline. To support systematic and robust evaluation, we further construct ScreenDrag, a benchmark with 5,333 examples spanning three levels of interface context, together with three dedicated metrics designed for assessing text dragging capability. Models trained on GUI-Drag with an efficient continual training strategy achieve substantial improvements on ScreenDrag, while preserving the original click-based performance on ScreenSpot, ScreenSpot-v2, and OSWorld-G. Our work encourages further research on broader GUI grounding beyond just clicking and paves way toward a truly generalist GUI grounding model. All benchmark, data, checkpoints, and code are open-sourced and available at https://osu-nlp-group.github.io/GUI-Drag.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰groundingæ¨¡å‹ä¸»è¦å±€é™äºæ¨¡æ‹Ÿé¼ æ ‡ç‚¹å‡»æ“ä½œï¼Œè€Œå¿½è§†äº†æ–‡æœ¬æ‹–æ‹½ï¼ˆtext draggingï¼‰è¿™ä¸€é‡è¦äº¤äº’æ¨¡å¼çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§é€šå‘é€šç”¨GUI groundingçš„æ–°æ­¥éª¤ã€‚ä½œè€…é¦–å…ˆä»‹ç»äº†ä¸€ä¸ªé€šè¿‡å¯æ‰©å±•ç®¡é“åˆæˆçš„åŒ…å«16.1ä¸‡ä¸ªæ–‡æœ¬æ‹–æ‹½æ ·æœ¬çš„å¤šæ ·åŒ–æ•°æ®é›†GUI-Dragï¼Œæ—¨åœ¨å¡«è¡¥è¿™ä¸€é¢†åŸŸçš„æ•°æ®ç©ºç™½ã€‚ä¸ºäº†æ”¯æŒç³»ç»ŸåŒ–è¯„ä¼°ï¼Œç ”ç©¶å›¢é˜Ÿè¿›ä¸€æ­¥æ„å»ºäº†æ¶µç›–ä¸‰ä¸ªç•Œé¢ä¸Šä¸‹æ–‡å±‚çº§çš„åŸºå‡†æµ‹è¯•ScreenDragï¼Œå¹¶è®¾è®¡äº†ä¸‰ä¸ªä¸“ç”¨æŒ‡æ ‡æ¥è¯„ä¼°æ–‡æœ¬æ‹–æ‹½èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨é«˜æ•ˆæŒç»­è®­ç»ƒç­–ç•¥åœ¨GUI-Dragä¸Šè®­ç»ƒçš„æ¨¡å‹ï¼Œä¸ä»…åœ¨ScreenDragä¸Šå®ç°äº†æ˜¾è‘—æ€§èƒ½æå‡ï¼Œè¿˜ä¿æŒäº†åœ¨ScreenSpotã€ScreenSpot-v2å’ŒOSWorld-Gç­‰åŸºå‡†ä¸Šçš„åŸæœ‰ç‚¹å‡»æ€§èƒ½ã€‚è¿™é¡¹å·¥ä½œé¼“åŠ±äº†è¶…è¶Šå•çº¯ç‚¹å‡»æ“ä½œçš„æ›´å¹¿æ³›GUI groundingç ”ç©¶ï¼Œä¸ºæ„å»ºçœŸæ­£çš„é€šç”¨GUIæ™ºèƒ½ä½“å¥ å®šäº†åŸºç¡€ï¼Œä¸”ç›¸å…³æ•°æ®ã€ä»£ç å’Œæ¨¡å‹å‡å·²å¼€æºã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "29 pages",
      "pdf_url": "https://arxiv.org/pdf/2601.06031v1",
      "published_date": "2025-11-07 19:40:09 UTC",
      "updated_date": "2025-11-07 19:40:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T13:59:29.401884+00:00"
    },
    {
      "arxiv_id": "2601.06030v1",
      "title": "From Augmentation to Symbiosis: A Review of Human-AI Collaboration Frameworks, Performance, and Perils",
      "title_zh": "ä»å¢å¼ºåˆ°å…±ç”Ÿï¼šäººæœºåä½œæ¡†æ¶ã€ç»©æ•ˆä¸é£é™©ç»¼è¿°",
      "authors": [
        "Richard Jiarui Tong"
      ],
      "abstract": "This paper offers a concise, 60-year synthesis of human-AI collaboration, from Licklider's ``man-computer symbiosis\" (AI as colleague) and Engelbart's ``augmenting human intellect\" (AI as tool) to contemporary poles: Human-Centered AI's ``supertool\" and Symbiotic Intelligence's mutual-adaptation model. We formalize the mechanism for effective teaming as a causal chain: Explainable AI (XAI) -> co-adaptation -> shared mental models (SMMs). A meta-analytic ``performance paradox\" is then examined: human-AI teams tend to show negative synergy in judgment/decision tasks (underperforming AI alone) but positive synergy in content creation and problem formulation. We trace failures to the algorithm-in-the-loop dynamic, aversion/bias asymmetries, and cumulative cognitive deskilling. We conclude with a unifying framework--combining extended-self and dual-process theories--arguing that durable gains arise when AI functions as an internalized cognitive component, yielding a unitary human-XAI symbiotic agency. This resolves the paradox and delineates a forward agenda for research and practice.",
      "tldr_zh": "è¿™ç¯‡è®ºæ–‡å›é¡¾äº†äººæœºåä½œé¢†åŸŸ60å¹´çš„å‘å±•å†ç¨‹ï¼Œå¯¹æ¯”äº†ä»Lickliderçš„â€œäººæœºå…±ç”Ÿâ€åˆ°Engelbartçš„â€œå¢å¼ºäººç±»æ™ºåŠ›â€ï¼Œä»¥åŠç°ä»£çš„â€œä»¥äººä¸ºä¸­å¿ƒçš„AIâ€ä¸â€œå…±ç”Ÿæ™ºèƒ½â€æ¨¡å‹ã€‚ä½œè€…å°†æœ‰æ•ˆå›¢é˜Ÿåä½œçš„æœºåˆ¶å½¢å¼åŒ–ä¸ºâ€œå¯è§£é‡ŠAI (XAI) -> å…±åŒé€‚åº” (co-adaptation) -> å…±äº«å¿ƒæ™ºæ¨¡å‹ (SMMs)â€çš„å› æœé“¾ã€‚ç ”ç©¶æ·±å…¥åˆ†æäº†ä¸€ä¸ªå…ƒåˆ†æå±‚é¢çš„â€œç»©æ•ˆæ‚–è®ºâ€ (performance paradox)ï¼Œå³äººæœºå›¢é˜Ÿåœ¨åˆ¤æ–­å†³ç­–ä»»åŠ¡ä¸­å¸¸è¡¨ç°å‡ºè´ŸååŒæ•ˆåº”ï¼Œè€Œåœ¨å†…å®¹åˆ›ä½œä¸­åˆ™è¡¨ç°å‡ºæ­£ååŒæ•ˆåº”ã€‚è¿™ç§å¤±è´¥è¢«è¿½æº¯è‡³ç®—æ³•åœ¨ç¯ (algorithm-in-the-loop) åŠ¨æ€ã€åè§ä¸å¯¹ç§°ä»¥åŠç´¯ç§¯çš„è®¤çŸ¥å»æŠ€èƒ½åŒ–ã€‚æœ€åï¼Œè®ºæ–‡æå‡ºäº†ä¸€ä¸ªç»“åˆæ‰©å±•è‡ªæˆ‘ (extended-self) å’ŒåŒé‡åŠ å·¥ (dual-process) ç†è®ºçš„ç»Ÿä¸€æ¡†æ¶ã€‚è¯¥æ¡†æ¶ä¸»å¼ å½“AIä½œä¸ºå†…åŒ–çš„è®¤çŸ¥ç»„ä»¶å‘æŒ¥ä½œç”¨å¹¶å½¢æˆç»Ÿä¸€çš„äººæœºå…±ç”Ÿä»£ç† (human-XAI symbiotic agency) æ—¶ï¼Œæ‰èƒ½äº§ç”ŸæŒä¹…æ”¶ç›Šå¹¶è§£å†³ä¸Šè¿°æ‚–è®ºã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.06030v1",
      "published_date": "2025-11-07 19:11:33 UTC",
      "updated_date": "2025-11-07 19:11:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T13:59:52.353993+00:00"
    },
    {
      "arxiv_id": "2511.05650v1",
      "title": "Optimizing Diversity and Quality through Base-Aligned Model Collaboration",
      "title_zh": "é€šè¿‡åŸºåº§-å¯¹é½æ¨¡å‹åä½œä¼˜åŒ–å¤šæ ·æ€§ä¸è´¨é‡",
      "authors": [
        "Yichen Wang",
        "Chenghao Yang",
        "Tenghao Huang",
        "Muhao Chen",
        "Jonathan May",
        "Mina Lee"
      ],
      "abstract": "Alignment has greatly improved large language models (LLMs)' output quality at the cost of diversity, yielding highly similar outputs across generations. We propose Base-Aligned Model Collaboration (BACo), an inference-time token-level model collaboration framework that dynamically combines a base LLM with its aligned counterpart to optimize diversity and quality. Inspired by prior work (Fei et al., 2025), BACo employs routing strategies that determine, at each token, from which model to decode based on next-token prediction uncertainty and predicted contents' semantic role. Prior diversity-promoting methods, such as retraining, prompt engineering, and multi-sampling methods, improve diversity but often degrade quality or require costly decoding or post-training. In contrast, BACo achieves both high diversity and quality post hoc within a single pass, while offering strong controllability. We explore a family of routing strategies, across three open-ended generation tasks and 13 metrics covering diversity and quality, BACo consistently surpasses state-of-the-art inference-time baselines. With our best router, BACo achieves a 21.3% joint improvement in diversity and quality. Human evaluations also mirror these improvements. The results suggest that collaboration between base and aligned models can optimize and control diversity and quality.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)ç»è¿‡å¯¹é½åè¾“å‡ºå¤šæ ·æ€§æ˜¾è‘—ä¸‹é™çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºBase-Aligned Model Collaboration (BACo)çš„æ¨ç†æ—¶Tokençº§æ¨¡å‹åä½œæ¡†æ¶ã€‚BACoé€šè¿‡åŠ¨æ€ç»“åˆåŸºç¡€LLMä¸å…¶å¯¹é½åçš„å¯¹åº”æ¨¡å‹ï¼Œåˆ©ç”¨åŸºäºé¢„æµ‹ä¸ç¡®å®šæ€§å’Œè¯­ä¹‰è§’è‰²çš„è·¯ç”±ç­–ç•¥ï¼Œåœ¨æ¯ä¸ªTokenç”Ÿæˆæ—¶å†³å®šè§£ç æ¥æºã€‚ç›¸æ¯”äºé‡è®­ç»ƒæˆ–å¤šé‡é‡‡æ ·ç­‰ä¼ ç»Ÿæ–¹æ³•ï¼ŒBACoèƒ½å¤Ÿåœ¨å•æ¬¡æ¨ç†è¿‡ç¨‹ä¸­åŒæ—¶å®ç°é«˜å¤šæ ·æ€§å’Œé«˜è´¨é‡ï¼Œå¹¶æä¾›å¼ºå¤§çš„å¯æ§æ€§ã€‚åœ¨æ¶µç›–ä¸‰ä¸ªå¼€æ”¾å¼ç”Ÿæˆä»»åŠ¡å’Œ13ä¸ªæŒ‡æ ‡çš„å®éªŒä¸­ï¼Œè¯¥æ¡†æ¶æŒç»­è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ¨ç†æ—¶åŸºçº¿æ¨¡å‹ã€‚ç»“æœæ˜¾ç¤ºï¼ŒBACoçš„æœ€ä½³è·¯ç”±ç­–ç•¥å®ç°äº†21.3%çš„å¤šæ ·æ€§ä¸è´¨é‡è”åˆæå‡ï¼Œå¹¶é€šè¿‡äººç±»è¯„ä¼°è¿›ä¸€æ­¥éªŒè¯äº†åŸºç¡€æ¨¡å‹ä¸å¯¹é½æ¨¡å‹åä½œçš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "52 pages, 16 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.05650v1",
      "published_date": "2025-11-07 19:00:01 UTC",
      "updated_date": "2025-11-07 19:00:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:00:16.898118+00:00"
    },
    {
      "arxiv_id": "2511.05489v1",
      "title": "TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding via Self-Verification Reinforcement Learning",
      "title_zh": "TimeSearch-Rï¼šåŸºäºè‡ªéªŒè¯å¼ºåŒ–å­¦ä¹ çš„é•¿è§†é¢‘ç†è§£è‡ªé€‚åº”æ—¶åºæœç´¢",
      "authors": [
        "Junwen Pan",
        "Qizhe Zhang",
        "Rui Zhang",
        "Ming Lu",
        "Xin Wan",
        "Yuan Zhang",
        "Chang Liu",
        "Qi She"
      ],
      "abstract": "Temporal search aims to identify a minimal set of relevant frames from tens of thousands based on a given query, serving as a foundation for accurate long-form video understanding. Existing works attempt to progressively narrow the search space. However, these approaches typically rely on a hand-crafted search process, lacking end-to-end optimization for learning optimal search strategies. In this paper, we propose TimeSearch-R, which reformulates temporal search as interleaved text-video thinking, seamlessly integrating searching video clips into the reasoning process through reinforcement learning (RL). However, applying RL training methods, such as Group Relative Policy Optimization (GRPO), to video reasoning can result in unsupervised intermediate search decisions. This leads to insufficient exploration of the video content and inconsistent logical reasoning. To address these issues, we introduce GRPO with Completeness Self-Verification (GRPO-CSV), which gathers searched video frames from the interleaved reasoning process and utilizes the same policy model to verify the adequacy of searched frames, thereby improving the completeness of video reasoning. Additionally, we construct datasets specifically designed for the SFT cold-start and RL training of GRPO-CSV, filtering out samples with weak temporal dependencies to enhance task difficulty and improve temporal search capabilities. Extensive experiments demonstrate that TimeSearch-R achieves significant improvements on temporal search benchmarks such as Haystack-LVBench and Haystack-Ego4D, as well as long-form video understanding benchmarks like VideoMME and MLVU. Notably, TimeSearch-R establishes a new state-of-the-art on LongVideoBench with 4.1% improvement over the base model Qwen2.5-VL and 2.0% over the advanced video reasoning model Video-R1. Our code is available at https://github.com/Time-Search/TimeSearch-R.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†TimeSearch-Rï¼Œä¸€ç§æ—¨åœ¨é€šè¿‡è‡ªæˆ‘éªŒè¯å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)ä¼˜åŒ–é•¿è§†é¢‘ç†è§£çš„è‡ªé€‚åº”æ—¶é—´æœç´¢æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•ä¾èµ–æ‰‹å·¥è®¾è®¡æœç´¢è¿‡ç¨‹ä¸”ç¼ºä¹ç«¯åˆ°ç«¯ä¼˜åŒ–çš„é—®é¢˜ï¼ŒTimeSearch-Rå°†æ—¶é—´æœç´¢é‡æ„ä¸ºäº¤é”™çš„æ–‡æœ¬-è§†é¢‘æ€ç»´è¿‡ç¨‹ï¼Œå¹¶é€šè¿‡RLå°†è§†é¢‘ç‰‡æ®µæœç´¢æ— ç¼é›†æˆåˆ°æ¨ç†ä¸­ã€‚ä¸ºäº†è§£å†³æ ‡å‡†RLè®­ç»ƒï¼ˆå¦‚GRPOï¼‰å¯¼è‡´çš„ä¸­é—´å†³ç­–ä¸å—ç›‘ç£åŠæ¨ç†é€»è¾‘ä¸ä¸€è‡´é—®é¢˜ï¼Œä½œè€…å¼•å…¥äº†å¸¦æœ‰å®Œæ•´æ€§è‡ªæˆ‘éªŒè¯çš„GRPO (GRPO-CSV)ï¼Œåˆ©ç”¨ç­–ç•¥æ¨¡å‹éªŒè¯æœç´¢åˆ°çš„å¸§æ˜¯å¦è¶³ä»¥æ”¯æŒæ¨ç†ï¼Œä»è€Œæå‡è§†é¢‘æ¨ç†çš„å®Œæ•´æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ„å»ºäº†ä¸“é—¨ç”¨äºSFTå†·å¯åŠ¨å’ŒRLè®­ç»ƒçš„æ•°æ®é›†ï¼Œé€šè¿‡ç­›é€‰å¼ºæ—¶é—´ä¾èµ–æ ·æœ¬æ¥å¢å¼ºæ¨¡å‹çš„æ—¶é—´æœç´¢èƒ½åŠ›ã€‚å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼ŒTimeSearch-Råœ¨Haystack-LVBenchã€VideoMMEç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶åœ¨LongVideoBenchä¸Šå»ºç«‹äº†æ–°çš„SOTAï¼Œå‡†ç¡®ç‡åˆ†åˆ«æ¯”Qwen2.5-VLå’ŒVideo-R1æå‡äº†4.1%å’Œ2.0%ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "22 pages, 17 figures. Official code: https://github.com/Time-Search/TimeSearch-R",
      "pdf_url": "https://arxiv.org/pdf/2511.05489v1",
      "published_date": "2025-11-07 18:58:25 UTC",
      "updated_date": "2025-11-07 18:58:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:00:42.732455+00:00"
    },
    {
      "arxiv_id": "2511.05483v1",
      "title": "DGTN: Graph-Enhanced Transformer with Diffusive Attention Gating Mechanism for Enzyme DDG Prediction",
      "title_zh": "DGTNï¼šåŸºäºæ‰©æ•£æ³¨æ„åŠ›é—¨æ§æœºåˆ¶çš„é…¶ DDG é¢„æµ‹å›¾å¢å¼º Transformer",
      "authors": [
        "Abigail Lin"
      ],
      "abstract": "Predicting the effect of amino acid mutations on enzyme thermodynamic stability (DDG) is fundamental to protein engineering and drug design. While recent deep learning approaches have shown promise, they often process sequence and structure information independently, failing to capture the intricate coupling between local structural geometry and global sequential patterns. We present DGTN (Diffused Graph-Transformer Network), a novel architecture that co-learns graph neural network (GNN) weights for structural priors and transformer attention through a diffusion mechanism. Our key innovation is a bidirectional diffusion process where: (1) GNN-derived structural embeddings guide transformer attention via learnable diffusion kernels, and (2) transformer representations refine GNN message passing through attention-modulated graph updates. We provide rigorous mathematical analysis showing this co-learning scheme achieves provably better approximation bounds than independent processing. On ProTherm and SKEMPI benchmarks, DGTN achieves state-of-the-art performance (Pearson Rho = 0.87, RMSE = 1.21 kcal/mol), with 6.2% improvement over best baselines. Ablation studies confirm the diffusion mechanism contributes 4.8 points to correlation. Our theoretical analysis proves the diffused attention converges to optimal structure-sequence coupling, with convergence rate O(1/sqrt(T) ) where T is diffusion steps. This work establishes a principled framework for integrating heterogeneous protein representations through learnable diffusion.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DGTNï¼ˆDiffused Graph-Transformer Networkï¼‰ï¼Œä¸€ç§ç”¨äºé¢„æµ‹é…¶çƒ­åŠ›å­¦ç¨³å®šæ€§ï¼ˆDDGï¼‰çš„æ–°å‹æ¶æ„ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ·±åº¦å­¦ä¹ æ–¹æ³•æœªèƒ½æœ‰æ•ˆæ•æ‰å±€éƒ¨ç»“æ„å‡ ä½•ä¸å…¨å±€åºåˆ—æ¨¡å¼ä¹‹é—´å¤æ‚è€¦åˆçš„é—®é¢˜ã€‚DGTNçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºä¸€ç§åŒå‘æ‰©æ•£æœºåˆ¶ï¼Œé€šè¿‡å¯å­¦ä¹ çš„æ‰©æ•£æ ¸å®ç°Graph Neural Network (GNN)ç»“æ„åµŒå…¥å¯¹Transformeræ³¨æ„åŠ›çš„å¼•å¯¼ï¼ŒåŒæ—¶åˆ©ç”¨Transformerè¡¨ç¤ºä¼˜åŒ–GNNçš„æ¶ˆæ¯ä¼ é€’ã€‚ä¸¥è°¨çš„æ•°å­¦åˆ†æè¯æ˜ï¼Œè¿™ç§è”åˆå­¦ä¹ æ–¹æ¡ˆæ¯”ç‹¬ç«‹å¤„ç†èƒ½å®ç°æ›´å¥½çš„è¿‘ä¼¼ç•Œé™ï¼Œä¸”æ‰©æ•£æ³¨æ„åŠ›ä»¥$O(1/\\sqrt{T})$çš„é€Ÿç‡æ”¶æ•›è‡³æœ€ä¼˜ç»“æ„-åºåˆ—è€¦åˆã€‚åœ¨ProThermå’ŒSKEMPIåŸºå‡†æµ‹è¯•ä¸­ï¼ŒDGTNå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ˆPearson Rho = 0.87, RMSE = 1.21 kcal/molï¼‰ï¼Œç›¸æ¯”æœ€ä½³åŸºçº¿æ¨¡å‹æå‡äº†6.2%ã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥è¯å®æ‰©æ•£æœºåˆ¶å¯¹ç›¸å…³æ€§æå‡æœ‰æ˜¾è‘—è´¡çŒ®ï¼Œç¡®ç«‹äº†é€šè¿‡å¯å­¦ä¹ æ‰©æ•£æ•´åˆå¼‚æ„è›‹ç™½è´¨è¡¨ç¤ºçš„åŸåˆ™æ€§æ¡†æ¶ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05483v1",
      "published_date": "2025-11-07 18:52:17 UTC",
      "updated_date": "2025-11-07 18:52:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:02:18.283842+00:00"
    },
    {
      "arxiv_id": "2511.05480v1",
      "title": "On Flow Matching KL Divergence",
      "title_zh": "è®ºæµåŒ¹é…KLæ•£åº¦",
      "authors": [
        "Maojiang Su",
        "Jerry Yao-Chieh Hu",
        "Sophia Pi",
        "Han Liu"
      ],
      "abstract": "We derive a deterministic, non-asymptotic upper bound on the Kullback-Leibler (KL) divergence of the flow-matching distribution approximation. In particular, if the $L_2$ flow-matching loss is bounded by $Îµ^2 > 0$, then the KL divergence between the true data distribution and the estimated distribution is bounded by $A_1 Îµ+ A_2 Îµ^2$. Here, the constants $A_1$ and $A_2$ depend only on the regularities of the data and velocity fields. Consequently, this bound implies statistical convergence rates of Flow Matching Transformers under the Total Variation (TV) distance. We show that, flow matching achieves nearly minimax-optimal efficiency in estimating smooth distributions. Our results make the statistical efficiency of flow matching comparable to that of diffusion models under the TV distance. Numerical studies on synthetic and learned velocities corroborate our theory.",
      "tldr_zh": "è¯¥è®ºæ–‡æ¨å¯¼äº†Flow Matchingåˆ†å¸ƒè¿‘ä¼¼ä¸­Kullback-Leibler (KL) æ•£åº¦çš„ç¡®å®šæ€§éæ¸è¿‘ä¸Šç•Œã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¦‚æœ$L_2$ Flow MatchingæŸå¤±è¢«é™åˆ¶åœ¨$\\epsilon^2$ä»¥å†…ï¼Œé‚£ä¹ˆçœŸå®æ•°æ®åˆ†å¸ƒä¸ä¼°è®¡åˆ†å¸ƒä¹‹é—´çš„KLæ•£åº¦å°†å—åˆ°$A_1 \\epsilon + A_2 \\epsilon^2$çš„é™åˆ¶ï¼Œå…¶ä¸­çš„å¸¸æ•°ä»…å–å†³äºæ•°æ®å’Œé€Ÿåº¦åœºçš„è§„å¾‹æ€§ã€‚åŸºäºæ­¤ç•Œé™ï¼Œä½œè€…ç¡®ç«‹äº†Flow Matching Transformersåœ¨å…¨å˜åˆ†(Total Variation, TV)è·ç¦»ä¸‹çš„ç»Ÿè®¡æ”¶æ•›ç‡ã€‚ç ”ç©¶è¯æ˜ï¼ŒFlow Matchingåœ¨ä¼°è®¡å¹³æ»‘åˆ†å¸ƒæ—¶èƒ½å¤Ÿè¾¾åˆ°è¿‘ä¹æå°æå¤§æœ€ä¼˜(minimax-optimal)çš„æ•ˆç‡ï¼Œä¸”å…¶åœ¨TVè·ç¦»ä¸‹çš„ç»Ÿè®¡æ•ˆç‡ä¸æ‰©æ•£æ¨¡å‹(diffusion models)ç›¸å½“ã€‚åˆæˆæ•°æ®å’Œå­¦ä¹ é€Ÿåº¦åœºçš„æ•°å€¼å®éªŒè¿›ä¸€æ­¥è¯å®äº†è¯¥ç†è®ºçš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05480v1",
      "published_date": "2025-11-07 18:47:46 UTC",
      "updated_date": "2025-11-07 18:47:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:02:06.335528+00:00"
    },
    {
      "arxiv_id": "2511.05475v1",
      "title": "AI Literacy Assessment Revisited: A Task-Oriented Approach Aligned with Real-world Occupations",
      "title_zh": "AIç´ å…»è¯„ä¼°å†æ¢ï¼šä¸ç°å®èŒä¸šå¯¹é½çš„ä»»åŠ¡å¯¼å‘æ–¹æ³•",
      "authors": [
        "Christopher Bogart",
        "Aparna Warrier",
        "Arav Agarwal",
        "Ross Higashi",
        "Yufan Zhang",
        "Jesse Flot",
        "Jaromir Savelka",
        "Heather Burte",
        "Majd Sakr"
      ],
      "abstract": "As artificial intelligence (AI) systems become ubiquitous in professional contexts, there is an urgent need to equip workers, often with backgrounds outside of STEM, with the skills to use these tools effectively as well as responsibly, that is, to be AI literate. However, prevailing definitions and therefore assessments of AI literacy often emphasize foundational technical knowledge, such as programming, mathematics, and statistics, over practical knowledge such as interpreting model outputs, selecting tools, or identifying ethical concerns. This leaves a noticeable gap in assessing someone's AI literacy for real-world job use. We propose a work-task-oriented assessment model for AI literacy which is grounded in the competencies required for effective use of AI tools in professional settings. We describe the development of a novel AI literacy assessment instrument, and accompanying formative assessments, in the context of a US Navy robotics training program. The program included training in robotics and AI literacy, as well as a competition with practical tasks and a multiple choice scenario task meant to simulate use of AI in a job setting. We found that, as a measure of applied AI literacy, the competition's scenario task outperformed the tests we adopted from past research or developed ourselves. We argue that when training people for AI-related work, educators should consider evaluating them with instruments that emphasize highly contextualized practical skills rather than abstract technical knowledge, especially when preparing workers without technical backgrounds for AI-integrated roles.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å½“å‰AI literacyè¯„ä¼°è¿‡äºå¼ºè°ƒç¼–ç¨‹ã€æ•°å­¦ç­‰åŸºç¡€æŠ€æœ¯çŸ¥è¯†ï¼Œè€Œå¿½è§†éSTEMèƒŒæ™¯äººå‘˜åœ¨å®é™…å·¥ä½œä¸­æ‰€éœ€å®ç”¨æŠ€èƒ½çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§é¢å‘å·¥ä½œä»»åŠ¡ï¼ˆwork-task-orientedï¼‰çš„è¯„ä¼°æ¨¡å‹ã€‚è¯¥æ¨¡å‹æ—¨åœ¨å¯¹é½èŒä¸šç¯å¢ƒä¸­çš„å®é™…èƒ½åŠ›éœ€æ±‚ï¼Œå¦‚è§£é‡Šæ¨¡å‹è¾“å‡ºã€å·¥å…·é€‰æ‹©å’Œä¼¦ç†è¯†åˆ«ç­‰ã€‚ä½œè€…åœ¨ç¾å›½æµ·å†›æœºå™¨äººåŸ¹è®­é¡¹ç›®ä¸­å¼€å‘å¹¶æµ‹è¯•äº†æ–°çš„è¯„ä¼°å·¥å…·ï¼Œæ¶µç›–äº†æœºå™¨äººä¸AIåŸ¹è®­ã€å®è·µä»»åŠ¡ç«èµ›ä»¥åŠæ¨¡æ‹Ÿå·¥ä½œåœºæ™¯çš„æƒ…æ™¯ä»»åŠ¡ã€‚ç ”ç©¶å‘ç°ï¼Œç›¸æ¯”äºä¼ ç»Ÿçš„æµ‹è¯•æ–¹æ³•ï¼Œæ¨¡æ‹Ÿå·¥ä½œç¯å¢ƒçš„æƒ…æ™¯ä»»åŠ¡æ›´èƒ½æœ‰æ•ˆè¡¡é‡åº”ç”¨å‹AI literacyã€‚ä½œè€…æ®æ­¤ä¸»å¼ ï¼Œåœ¨ä¸ºAIé›†æˆè§’è‰²åŸ¹è®­äººå‘˜æ—¶ï¼Œæ•™è‚²è€…åº”ä¼˜å…ˆé‡‡ç”¨é«˜åº¦æƒ…å¢ƒåŒ–çš„å®è·µæŠ€èƒ½è¯„ä¼°å·¥å…·ï¼Œè€Œéä»…è€ƒå¯ŸæŠ½è±¡çš„æŠ€æœ¯çŸ¥è¯†ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05475v1",
      "published_date": "2025-11-07 18:38:15 UTC",
      "updated_date": "2025-11-07 18:38:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:02:33.896451+00:00"
    },
    {
      "arxiv_id": "2511.11630v1",
      "title": "Predicting Grain Growth in Polycrystalline Materials Using Deep Learning Time Series Models",
      "title_zh": "åŸºäºæ·±åº¦å­¦ä¹ æ—¶é—´åºåˆ—æ¨¡å‹é¢„æµ‹å¤šæ™¶ææ–™æ™¶ç²’ç”Ÿé•¿",
      "authors": [
        "Eliane Younes",
        "Elie Hachem",
        "Marc Bernacki"
      ],
      "abstract": "Grain Growth strongly influences the mechanical behavior of materials, making its prediction a key objective in microstructural engineering. In this study, several deep learning approaches were evaluated, including recurrent neural networks (RNN), long short-term memory (LSTM), temporal convolutional networks (TCN), and transformers, to forecast grain size distributions during grain growth. Unlike full-field simulations, which are computationally demanding, the present work relies on mean-field statistical descriptors extracted from high-fidelity simulations. A dataset of 120 grain growth sequences was processed into normalized grain size distributions as a function of time. The models were trained to predict future distributions from a short temporal history using a recursive forecasting strategy. Among the tested models, the LSTM network achieved the highest accuracy (above 90\\%) and the most stable performance, maintaining physically consistent predictions over extended horizons while reducing computation time from about 20 minutes per sequence to only a few seconds, whereas the other architectures tended to diverge when forecasting further in time. These results highlight the potential of low-dimensional descriptors and LSTM-based forecasting for efficient and accurate microstructure prediction, with direct implications for digital twin development and process optimization.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†åŒ…æ‹¬RNNã€LSTMã€TCNå’ŒTransformersåœ¨å†…çš„å¤šç§æ·±åº¦å­¦ä¹ æ—¶é—´åºåˆ—æ¨¡å‹ï¼Œæ—¨åœ¨é¢„æµ‹å¤šæ™¶ææ–™æ™¶ç²’ç”Ÿé•¿è¿‡ç¨‹ä¸­çš„æ™¶ç²’å°ºå¯¸åˆ†å¸ƒã€‚ä¸ºäº†å…‹æœå…¨åœºæ¨¡æ‹Ÿè®¡ç®—æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ï¼Œè¯¥å·¥ä½œåˆ©ç”¨ä»é«˜ä¿çœŸæ¨¡æ‹Ÿä¸­æå–çš„å¹³å‡åœºç»Ÿè®¡æè¿°ç¬¦ï¼ˆmean-field statistical descriptorsï¼‰ï¼Œå¹¶åŸºäº120ä¸ªç”Ÿé•¿åºåˆ—çš„æ•°æ®é›†é‡‡ç”¨é€’å½’ç­–ç•¥è¿›è¡Œè®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æµ‹è¯•çš„æ¨¡å‹ä¸­ï¼ŒLSTMç½‘ç»œå®ç°äº†æœ€é«˜çš„å‡†ç¡®ç‡ï¼ˆè¶…è¿‡90%ï¼‰å’Œæœ€ç¨³å®šçš„æ€§èƒ½ï¼Œèƒ½å¤Ÿåœ¨å»¶é•¿çš„é¢„æµ‹èŒƒå›´å†…ä¿æŒç‰©ç†ä¸€è‡´æ€§ï¼Œè€Œå…¶ä»–æ¶æ„åˆ™å®¹æ˜“å‘æ•£ã€‚è¯¥æ–¹æ³•å°†è®¡ç®—æ—¶é—´ä»æ¯ä¸ªåºåˆ—çº¦20åˆ†é’Ÿå¤§å¹…ç¼©å‡è‡³å‡ ç§’é’Ÿã€‚è¿™äº›å‘ç°çªæ˜¾äº†ä½ç»´æè¿°ç¬¦ç»“åˆLSTMé¢„æµ‹åœ¨é«˜æ•ˆå¾®è§‚ç»“æ„é¢„æµ‹ä¸­çš„æ½œåŠ›ï¼Œå¯¹æ•°å­—å­ªç”Ÿï¼ˆdigital twinï¼‰å¼€å‘å’Œå·¥è‰ºä¼˜åŒ–å…·æœ‰ç›´æ¥æ„ä¹‰ã€‚",
      "categories": [
        "cs.LG",
        "cond-mat.mtrl-sci",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11630v1",
      "published_date": "2025-11-07 18:29:42 UTC",
      "updated_date": "2025-11-07 18:29:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:03:08.002170+00:00"
    },
    {
      "arxiv_id": "2511.05459v3",
      "title": "SWE-Compass: Towards Unified Evaluation of Agentic Coding Abilities for Large Language Models",
      "title_zh": "SWE-Compassï¼šé¢å‘å¤§å‹è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“ç¼–ç¨‹èƒ½åŠ›çš„ç»Ÿä¸€è¯„æµ‹",
      "authors": [
        "Jingxuan Xu",
        "Ken Deng",
        "Weihao Li",
        "Songwei Yu",
        "Huaixi Tang",
        "Haoyang Huang",
        "Zhiyi Lai",
        "Zizheng Zhan",
        "Yanan Wu",
        "Chenchen Zhang",
        "Kepeng Lei",
        "Yifan Yao",
        "Xinping Lei",
        "Wenqiang Zhu",
        "Zongxian Feng",
        "Han Li",
        "Junqi Xiong",
        "Dailin Li",
        "Zuchen Gao",
        "Kun Wu",
        "Wen Xiang",
        "Ziqi Zhan",
        "Yuanxing Zhang",
        "Wuxuan Gong",
        "Ziyuan Gao",
        "Guanxiang Wang",
        "Yirong Xue",
        "Mengtong Li",
        "Mengfei Xie",
        "Xiaojiang Zhang",
        "Jinghui Wang",
        "Wenhao Zhuang",
        "Zheng Lin",
        "Huiming Wang",
        "Zhaoxiang Zhang",
        "Yuqun Zhang",
        "Haotian Zhang",
        "Bin Chen",
        "Jiaheng Liu"
      ],
      "abstract": "Evaluating large language models (LLMs) for software engineering has been limited by narrow task coverage, language bias, and insufficient alignment with real-world developer workflows. Existing benchmarks often focus on algorithmic problems or Python-centric bug fixing, leaving critical dimensions of software engineering underexplored. To address these gaps, we introduce SWE-Compass1, a comprehensive benchmark that unifies heterogeneous code-related evaluations into a structured and production-aligned framework. SWE-Compass spans 8 task types, 8 programming scenarios, and 10 programming languages, with 2000 high-quality instances curated from authentic GitHub pull requests and refined through systematic filtering and validation. We benchmark ten state-of-the-art LLMs under two agentic frameworks, SWE-Agent and Claude Code, revealing a clear hierarchy of difficulty across task types, languages, and scenarios. Moreover, by aligning evaluation with real-world developer practices, SWE-Compass provides a rigorous and reproducible foundation for diagnosing and advancing agentic coding capabilities in large language models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨è½¯ä»¶å·¥ç¨‹è¯„ä¼°ä¸­å­˜åœ¨çš„ä»»åŠ¡è¦†ç›–é¢çª„ã€è¯­è¨€åå·®åŠç¼ºä¹ä¸çœŸå®å¼€å‘æµç¨‹å¯¹é½ç­‰é—®é¢˜ï¼Œæå‡ºäº†SWE-Compassã€‚è¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œæ—¨åœ¨å°†å¼‚æ„çš„ä»£ç ç›¸å…³è¯„ä¼°ç»Ÿä¸€åˆ°ä¸€ä¸ªç»“æ„åŒ–ä¸”ç¬¦åˆç”Ÿäº§ç¯å¢ƒçš„æ ‡å‡†ä¸­ã€‚SWE-Compassæ¶µç›–äº†8ç§ä»»åŠ¡ç±»å‹ã€8ç§ç¼–ç¨‹åœºæ™¯å’Œ10ç§ç¼–ç¨‹è¯­è¨€ï¼ŒåŒ…å«ä»çœŸå®GitHub Pull Requestsä¸­ç­›é€‰å¹¶éªŒè¯çš„2000ä¸ªé«˜è´¨é‡å®ä¾‹ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨SWE-Agentå’ŒClaude Codeä¸¤ä¸ªä»£ç†æ¡†æ¶ä¸‹å¯¹åä¸ªæœ€å…ˆè¿›çš„LLMsè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œæ­ç¤ºäº†ä¸åŒä»»åŠ¡ç±»å‹ã€è¯­è¨€å’Œåœºæ™¯ä¹‹é—´æ˜æ˜¾çš„éš¾åº¦å±‚çº§ã€‚é€šè¿‡ä¸ç°å®ä¸–ç•Œå¼€å‘å®è·µå¯¹é½ï¼ŒSWE-Compassä¸ºè¯Šæ–­å’Œæå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„ä»£ç†ç¼–ç èƒ½åŠ›(Agentic Coding Abilities)æä¾›äº†ä¸¥è°¨ä¸”å¯å¤ç°çš„åŸºç¡€ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05459v3",
      "published_date": "2025-11-07 18:01:32 UTC",
      "updated_date": "2025-11-11 16:46:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:03:19.353209+00:00"
    },
    {
      "arxiv_id": "2511.05452v2",
      "title": "Self-adaptive weighting and sampling for physics-informed neural networks",
      "title_zh": "ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œçš„è‡ªé€‚åº”åŠ æƒä¸é‡‡æ ·",
      "authors": [
        "Wenqian Chen",
        "Amanda Howard",
        "Panos Stinis"
      ],
      "abstract": "Physics-informed deep learning has emerged as a promising framework for solving partial differential equations (PDEs). Nevertheless, training these models on complex problems remains challenging, often leading to limited accuracy and efficiency. In this work, we introduce a hybrid adaptive sampling and weighting method to enhance the performance of physics-informed neural networks (PINNs). The adaptive sampling component identifies training points in regions where the solution exhibits rapid variation, while the adaptive weighting component balances the convergence rate across training points. Numerical experiments show that applying only adaptive sampling or only adaptive weighting is insufficient to consistently achieve accurate predictions, particularly when training points are scarce. Since each method emphasizes different aspects of the solution, their effectiveness is problem dependent. By combining both strategies, the proposed framework consistently improves prediction accuracy and training efficiency, offering a more robust approach for solving PDEs with PINNs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œ(PINNs)åœ¨æ±‚è§£åå¾®åˆ†æ–¹ç¨‹(PDEs)æ—¶é¢ä¸´çš„è®­ç»ƒå¤æ‚æ€§å’Œç²¾åº¦å—é™é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆè‡ªé€‚åº”é‡‡æ ·å’ŒåŠ æƒçš„æ··åˆæ–¹æ³•ã€‚å…¶ä¸­ï¼Œè‡ªé€‚åº”é‡‡æ ·ç»„ä»¶ç”¨äºè¯†åˆ«è§£å‘ˆç°å‰§çƒˆå˜åŒ–çš„åŒºåŸŸå†…çš„è®­ç»ƒç‚¹ï¼Œè€Œè‡ªé€‚åº”åŠ æƒç»„ä»¶æ—¨åœ¨å¹³è¡¡ä¸åŒè®­ç»ƒç‚¹ä¹‹é—´çš„æ”¶æ•›é€Ÿç‡ã€‚æ•°å€¼å®éªŒæ˜¾ç¤ºï¼Œä»…å•ç‹¬ä½¿ç”¨è‡ªé€‚åº”é‡‡æ ·æˆ–è‡ªé€‚åº”åŠ æƒå¾€å¾€ä¸è¶³ä»¥æŒç»­å®ç°å‡†ç¡®é¢„æµ‹ï¼Œå°¤å…¶æ˜¯åœ¨è®­ç»ƒæ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ã€‚é€šè¿‡èåˆè¿™ä¸¤ç§ç­–ç•¥ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿäº’è¡¥å…¶ä¼˜åŠ¿ï¼Œä¸€è‡´åœ°æå‡äº†é¢„æµ‹ç²¾åº¦å’Œè®­ç»ƒæ•ˆç‡ï¼Œä¸ºåˆ©ç”¨PINNsæ±‚è§£PDEsæä¾›äº†ä¸€ç§æ›´ä¸ºé²æ£’çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "physics.comp-ph"
      ],
      "primary_category": "stat.ML",
      "comment": "11 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.05452v2",
      "published_date": "2025-11-07 17:48:11 UTC",
      "updated_date": "2025-11-11 22:52:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:06:30.174048+00:00"
    },
    {
      "arxiv_id": "2511.05442v1",
      "title": "APP: Accelerated Path Patching with Task-Specific Pruning",
      "title_zh": "APPï¼šåŸºäºç‰¹å®šä»»åŠ¡å‰ªæçš„åŠ é€Ÿè·¯å¾„ä¿®è¡¥",
      "authors": [
        "Frauke Andersen",
        "William Rudman",
        "Ruochen Zhang",
        "Carsten Eickhoff"
      ],
      "abstract": "Circuit discovery is a key step in many mechanistic interpretability pipelines. Current methods, such as Path Patching, are computationally expensive and have limited in-depth circuit analysis for smaller models. In this study, we propose Accelerated Path Patching (APP), a hybrid approach leveraging our novel contrastive attention head pruning method to drastically reduce the search space of circuit discovery methods. Our Contrastive-FLAP pruning algorithm uses techniques from causal mediation analysis to assign higher pruning scores to task-specific attention heads, leading to higher performing sparse models compared to traditional pruning techniques. Although Contrastive-FLAP is successful at preserving task-specific heads that existing pruning algorithms remove at low sparsity ratios, the circuits found by Contrastive-FLAP alone are too large to satisfy the minimality constraint required in circuit analysis. APP first applies Contrastive-FLAP to reduce the search space on required for circuit discovery algorithms by, on average, 56\\%. Next, APP, applies traditional Path Patching on the remaining attention heads, leading to a speed up of 59.63\\%-93.27\\% compared to Path Patching applied to the dense model. Despite the substantial computational saving that APP provides, circuits obtained from APP exhibit substantial overlap and similar performance to previously established Path Patching circuits",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æå‡ºäº†Accelerated Path Patching (APP)ï¼Œä¸€ç§ç»“åˆäº†ä»»åŠ¡ç‰¹å®šå‰ªæçš„æ··åˆæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³Mechanistic interpretabilityä¸­Circuit discoveryè®¡ç®—æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ã€‚APPåˆ©ç”¨ä¸€ç§åä¸ºContrastive-FLAPçš„æ–°å‹å‰ªæç®—æ³•ï¼Œè¯¥ç®—æ³•åŸºäºCausal mediation analysisæŠ€æœ¯ï¼Œä¸ºä»»åŠ¡ç‰¹å®šçš„Attention headsåˆ†é…æ›´é«˜çš„å‰ªæåˆ†æ•°ï¼Œä»è€Œæœ‰æ•ˆä¿ç•™å…³é”®ç»„ä»¶ã€‚è¯¥æ–¹æ³•é¦–å…ˆåº”ç”¨Contrastive-FLAPå°†Circuit discoveryæ‰€éœ€çš„æœç´¢ç©ºé—´å¹³å‡å‡å°‘äº†56%ï¼Œéšååœ¨å‰©ä½™çš„ç»„ä»¶ä¸Šåº”ç”¨ä¼ ç»Ÿçš„Path Patchingã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç›¸æ¯”äºç›´æ¥åœ¨å¯†é›†æ¨¡å‹ä¸Šåº”ç”¨Path Patchingï¼ŒAPPå®ç°äº†59.63%è‡³93.27%çš„æ˜¾è‘—åŠ é€Ÿã€‚å°½ç®¡è®¡ç®—å¼€é”€å¤§å¹…é™ä½ï¼ŒAPPæ‰€è¯†åˆ«å‡ºçš„Circuitsä¸æ—¢å¾€ç¡®ç«‹çš„Path Patching circuitsåœ¨æ€§èƒ½å’Œç»“æ„ä¸Šå‡è¡¨ç°å‡ºé«˜åº¦çš„ä¸€è‡´æ€§å’Œé‡å ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05442v1",
      "published_date": "2025-11-07 17:20:32 UTC",
      "updated_date": "2025-11-07 17:20:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:07:00.120863+00:00"
    },
    {
      "arxiv_id": "2511.05430v1",
      "title": "\"I Like That You Have to Poke Around\": Instructors on How Experiential Approaches to AI Literacy Spark Inquiry and Critical Thinking",
      "title_zh": "â€œæˆ‘å–œæ¬¢è¿™ç§éœ€è¦å››å¤„æ‘¸ç´¢çš„æ„Ÿè§‰â€ï¼šè®²å¸ˆè®ºä½“éªŒå¼AIç´ å…»æ•™å­¦æ³•å¦‚ä½•æ¿€å‘æ¢ç©¶ä¸æ‰¹åˆ¤æ€§æ€ç»´",
      "authors": [
        "Aparna Maya Warrier",
        "Arav Agarwal",
        "Jaromir Savelka",
        "Christopher Bogart",
        "Heather Burte"
      ],
      "abstract": "As artificial intelligence (AI) increasingly shapes decision-making across domains, there is a growing need to support AI literacy among learners beyond computer science. However, many current approaches rely on programming-heavy tools or abstract lecture-based content, limiting accessibility for non-STEM audiences. This paper presents findings from a study of AI User, a modular, web-based curriculum that teaches core AI concepts through interactive, no-code projects grounded in real-world scenarios. The curriculum includes eight projects; this study focuses on instructor feedback on Projects 5-8, which address applied topics such as natural language processing, computer vision, decision support, and responsible AI. Fifteen community college instructors participated in structured focus groups, completing the projects as learners and providing feedback through individual reflection and group discussion. Using thematic analysis, we examined how instructors evaluated the design, instructional value, and classroom applicability of these experiential activities. Findings highlight instructors' appreciation for exploratory tasks, role-based simulations, and real-world relevance, while also surfacing design trade-offs around cognitive load, guidance, and adaptability for diverse learners. This work extends prior research on AI literacy by centering instructor perspectives on teaching complex AI topics without code. It offers actionable insights for designing inclusive, experiential AI learning resources that scale across disciplines and learner backgrounds.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é’ˆå¯¹éè®¡ç®—æœºç§‘å­¦ä¸“ä¸šå­¦ä¹ è€…çš„AIç´ å…»æ•™è‚²é—®é¢˜ï¼Œä»‹ç»å¹¶è¯„ä¼°äº†â€œAI Userâ€è¿™ä¸€æ¨¡å—åŒ–ã€åŸºäºç½‘ç»œçš„æ— ä»£ç è¯¾ç¨‹ã€‚ç ”ç©¶è€…ç»„ç»‡äº†15ä½ç¤¾åŒºå­¦é™¢è®²å¸ˆå‚ä¸ç„¦ç‚¹å°ç»„ï¼Œè®©ä»–ä»¬ä½œä¸ºå­¦ä¹ è€…å®Œæˆæ¶‰åŠè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ã€è®¡ç®—æœºè§†è§‰ï¼ˆComputer Visionï¼‰å’Œè´Ÿè´£ä»»AIç­‰ä¸»é¢˜çš„é¡¹ç›®ï¼Œå¹¶é€šè¿‡ä¸“é¢˜åˆ†æï¼ˆThematic Analysisï¼‰è¯„ä¼°å…¶åé¦ˆã€‚ç ”ç©¶å‘ç°ï¼Œè®²å¸ˆä»¬é«˜åº¦èµèµè¯¾ç¨‹ä¸­çš„æ¢ç´¢æ€§ä»»åŠ¡ã€åŸºäºè§’è‰²çš„æ¨¡æ‹Ÿä»¥åŠç°å®åœºæ™¯çš„ç›¸å…³æ€§ï¼Œè®¤ä¸ºè¿™äº›ä½“éªŒå¼æ–¹æ³•èƒ½æœ‰æ•ˆæ¿€å‘æ¢ç©¶å’Œæ‰¹åˆ¤æ€§æ€ç»´ã€‚ç„¶è€Œï¼Œåé¦ˆä¹ŸæŒ‡å‡ºäº†åœ¨è®¤çŸ¥è´Ÿè·ã€å¼•å¯¼æœºåˆ¶å’Œé’ˆå¯¹ä¸åŒå­¦ä¹ è€…çš„é€‚åº”æ€§æ–¹é¢å­˜åœ¨çš„è®¾è®¡æƒè¡¡ã€‚è¯¥å·¥ä½œé€šè¿‡èšç„¦è®²å¸ˆå¯¹æ— ä»£ç AIæ•™å­¦çš„è§†è§’ï¼Œæ‰©å±•äº†ç°æœ‰çš„AIç´ å…»ç ”ç©¶ï¼Œå¹¶ä¸ºè®¾è®¡å¯æ‰©å±•è‡³ä¸åŒå­¦ç§‘å’ŒèƒŒæ™¯çš„åŒ…å®¹æ€§ä½“éªŒå¼AIå­¦ä¹ èµ„æºæä¾›äº†å¯æ“ä½œçš„è§è§£ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05430v1",
      "published_date": "2025-11-07 17:05:58 UTC",
      "updated_date": "2025-11-07 17:05:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:07:19.758943+00:00"
    },
    {
      "arxiv_id": "2511.05420v1",
      "title": "ProDER: A Continual Learning Approach for Fault Prediction in Evolving Smart Grids",
      "title_zh": "ProDERï¼šé¢å‘æ¼”è¿›æ™ºèƒ½ç”µç½‘æ•…éšœé¢„æµ‹çš„æŒç»­å­¦ä¹ æ–¹æ³•",
      "authors": [
        "Emad Efatinasab",
        "Nahal Azadi",
        "Davide Dalle Pezze",
        "Gian Antonio Susto",
        "Chuadhry Mujeeb Ahmed",
        "Mirco Rampazzo"
      ],
      "abstract": "As smart grids evolve to meet growing energy demands and modern operational challenges, the ability to accurately predict faults becomes increasingly critical. However, existing AI-based fault prediction models struggle to ensure reliability in evolving environments where they are required to adapt to new fault types and operational zones. In this paper, we propose a continual learning (CL) framework in the smart grid context to evolve the model together with the environment. We design four realistic evaluation scenarios grounded in class-incremental and domain-incremental learning to emulate evolving grid conditions. We further introduce Prototype-based Dark Experience Replay (ProDER), a unified replay-based approach that integrates prototype-based feature regularization, logit distillation, and a prototype-guided replay memory. ProDER achieves the best performance among tested CL techniques, with only a 0.045 accuracy drop for fault type prediction and 0.015 for fault zone prediction. These results demonstrate the practicality of CL for scalable, real-world fault prediction in smart grids.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ¼”è¿›ä¸­çš„æ™ºèƒ½ç”µç½‘(Smart Grids)é¢ä¸´çš„æ•…éšœé¢„æµ‹éš¾é¢˜ï¼Œå³ç°æœ‰AIæ¨¡å‹éš¾ä»¥é€‚åº”æ–°æ•…éšœç±»å‹å’Œè¿è¡ŒåŒºåŸŸçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æŒç»­å­¦ä¹ (Continual Learning, CL)æ¡†æ¶ã€‚ä½œè€…è®¾è®¡äº†å››ç§åŸºäºç±»å¢é‡å’ŒåŸŸå¢é‡å­¦ä¹ çš„ç°å®è¯„ä¼°åœºæ™¯ï¼Œå¹¶å¼•å…¥äº†ProDER (Prototype-based Dark Experience Replay) æ–¹æ³•ã€‚ProDER æ˜¯ä¸€ç§ç»Ÿä¸€çš„å›æ”¾å¼æ–¹æ³•ï¼Œç»“åˆäº†åŸºäºåŸå‹çš„ç‰¹å¾æ­£åˆ™åŒ–ã€Logit è’¸é¦ä»¥åŠåŸå‹å¼•å¯¼çš„å›æ”¾è®°å¿†æœºåˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒProDER åœ¨æµ‹è¯•çš„æŒç»­å­¦ä¹ æŠ€æœ¯ä¸­è¡¨ç°æœ€ä½³ï¼Œå…¶æ•…éšœç±»å‹é¢„æµ‹å‡†ç¡®ç‡ä»…ä¸‹é™ 0.045ï¼Œæ•…éšœåŒºåŸŸé¢„æµ‹å‡†ç¡®ç‡ä»…ä¸‹é™ 0.015ï¼Œä»è€ŒéªŒè¯äº†æŒç»­å­¦ä¹ åœ¨ç°å®ä¸–ç•Œæ™ºèƒ½ç”µç½‘æ•…éšœé¢„æµ‹ä¸­çš„å®ç”¨æ€§å’Œå¯æ‰©å±•æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05420v1",
      "published_date": "2025-11-07 16:51:32 UTC",
      "updated_date": "2025-11-07 16:51:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:07:44.156194+00:00"
    },
    {
      "arxiv_id": "2511.05404v1",
      "title": "Multi-modal Loop Closure Detection with Foundation Models in Severely Unstructured Environments",
      "title_zh": "ä¸¥é‡éç»“æ„åŒ–ç¯å¢ƒä¸‹åŸºäºåŸºç¡€æ¨¡å‹çš„å¤šæ¨¡æ€å›ç¯æ£€æµ‹",
      "authors": [
        "Laura Alejandra Encinar Gonzalez",
        "John Folkesson",
        "Rudolph Triebel",
        "Riccardo Giubilato"
      ],
      "abstract": "Robust loop closure detection is a critical component of Simultaneous Localization and Mapping (SLAM) algorithms in GNSS-denied environments, such as in the context of planetary exploration. In these settings, visual place recognition often fails due to aliasing and weak textures, while LiDAR-based methods suffer from sparsity and ambiguity. This paper presents MPRF, a multimodal pipeline that leverages transformer-based foundation models for both vision and LiDAR modalities to achieve robust loop closure in severely unstructured environments. Unlike prior work limited to retrieval, MPRF integrates a two-stage visual retrieval strategy with explicit 6-DoF pose estimation, combining DINOv2 features with SALAD aggregation for efficient candidate screening and SONATA-based LiDAR descriptors for geometric verification. Experiments on the S3LI dataset and S3LI Vulcano dataset show that MPRF outperforms state-of-the-art retrieval methods in precision while enhancing pose estimation robustness in low-texture regions. By providing interpretable correspondences suitable for SLAM back-ends, MPRF achieves a favorable trade-off between accuracy, efficiency, and reliability, demonstrating the potential of foundation models to unify place recognition and pose estimation. Code and models will be released at github.com/DLR-RM/MPRF.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MPRFï¼Œä¸€ç§åˆ©ç”¨åŸºäºTransformerçš„Foundation Modelsçš„å¤šæ¨¡æ€ç®¡é“ï¼Œæ—¨åœ¨è§£å†³ä¸¥é‡éç»“æ„åŒ–ç¯å¢ƒï¼ˆå¦‚è¡Œæ˜Ÿæ¢ç´¢ï¼‰ä¸­çš„é—­ç¯æ£€æµ‹ï¼ˆLoop Closure Detectionï¼‰é—®é¢˜ã€‚é’ˆå¯¹è§†è§‰æ–¹æ³•åœ¨å¼±çº¹ç†ç¯å¢ƒä¸‹çš„å¤±æ•ˆå’ŒLiDARæ–¹æ³•çš„ç¨€ç–æ€§é—®é¢˜ï¼ŒMPRFç»“åˆäº†è§†è§‰å’ŒLiDARæ¨¡æ€ï¼Œé›†æˆäº†ä¸¤é˜¶æ®µè§†è§‰æ£€ç´¢ç­–ç•¥ä¸æ˜¾å¼6-DoFä½å§¿ä¼°è®¡ã€‚å…·ä½“è€Œè¨€ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†DINOv2ç‰¹å¾ä¸SALADèšåˆè¿›è¡Œé«˜æ•ˆçš„å€™é€‰ç­›é€‰ï¼Œå¹¶åˆ©ç”¨åŸºäºSONATAçš„LiDARæè¿°ç¬¦è¿›è¡Œå‡ ä½•éªŒè¯ã€‚åœ¨S3LIæ•°æ®é›†å’ŒS3LI Vulcanoæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMPRFåœ¨ç²¾åº¦ä¸Šä¼˜äºæœ€å…ˆè¿›çš„æ£€ç´¢æ–¹æ³•ï¼ŒåŒæ—¶æ˜¾è‘—å¢å¼ºäº†ä½çº¹ç†åŒºåŸŸä½å§¿ä¼°è®¡çš„é²æ£’æ€§ã€‚é€šè¿‡æä¾›é€‚ç”¨äºSLAMåç«¯çš„è§£é‡Šæ€§å¯¹åº”å…³ç³»ï¼Œè¯¥æ–¹æ³•å®ç°äº†å‡†ç¡®æ€§ã€æ•ˆç‡å’Œå¯é æ€§çš„è‰¯å¥½å¹³è¡¡ï¼Œå±•ç¤ºäº†Foundation Modelsåœ¨ç»Ÿä¸€ä½ç½®è¯†åˆ«å’Œä½å§¿ä¼°è®¡æ–¹é¢çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Under review for ICRA 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.05404v1",
      "published_date": "2025-11-07 16:30:35 UTC",
      "updated_date": "2025-11-07 16:30:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:08:07.995960+00:00"
    },
    {
      "arxiv_id": "2601.05264v1",
      "title": "Engineering the RAG Stack: A Comprehensive Review of the Architecture and Trust Frameworks for Retrieval-Augmented Generation Systems",
      "title_zh": "RAG æŠ€æœ¯æ ˆçš„å·¥ç¨‹åŒ–ï¼šæ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿæ¶æ„ä¸ä¿¡ä»»æ¡†æ¶å…¨é¢ç»¼è¿°",
      "authors": [
        "Dean Wampler",
        "Dave Nielson",
        "Alireza Seddighi"
      ],
      "abstract": "This article provides a comprehensive systematic literature review of academic studies, industrial applications, and real-world deployments from 2018 to 2025, providing a practical guide and detailed overview of modern Retrieval-Augmented Generation (RAG) architectures. RAG offers a modular approach for integrating external knowledge without increasing the capacity of the model as LLM systems expand. Research and engineering practices have been fragmented as a result of the increasing diversity of RAG methodologies, which encompasses a variety of fusion mechanisms, retrieval strategies, and orchestration approaches. We provide quantitative assessment frameworks, analyze the implications for trust and alignment, and systematically consolidate existing RAG techniques into a unified taxonomy. This document is a practical framework for the deployment of resilient, secure, and domain-adaptable RAG systems, synthesizing insights from academic literature, industry reports, and technical implementation guides. It also functions as a technical reference.",
      "tldr_zh": "è¿™ç¯‡æ–‡ç« å¯¹2018å¹´è‡³2025å¹´é—´çš„å­¦æœ¯ç ”ç©¶ã€å·¥ä¸šåº”ç”¨å’Œå®é™…éƒ¨ç½²è¿›è¡Œäº†å…¨é¢çš„ç³»ç»Ÿæ–‡çŒ®ç»¼è¿°ï¼Œä¸ºç°ä»£æ£€ç´¢å¢å¼ºç”Ÿæˆ(Retrieval-Augmented Generation, RAG)æ¶æ„æä¾›äº†è¯¦ç»†æ¦‚è§ˆã€‚é’ˆå¯¹RAGæ–¹æ³•è®ºä¸­æ—¥ç›Šå¢åŠ çš„å¤šæ ·æ€§å’Œç¢ç‰‡åŒ–é—®é¢˜ï¼Œè¯¥ç ”ç©¶æ¶µç›–äº†å„ç§èåˆæœºåˆ¶ã€æ£€ç´¢ç­–ç•¥å’Œç¼–æ’æ–¹æ³•ã€‚ä½œè€…ç³»ç»Ÿåœ°å°†ç°æœ‰çš„RAGæŠ€æœ¯æ•´åˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„åˆ†ç±»ä½“ç³»(taxonomy)ä¸­ï¼Œå¹¶æä¾›äº†å®šé‡è¯„ä¼°æ¡†æ¶ï¼Œæ·±å…¥åˆ†æäº†ç³»ç»Ÿåœ¨ä¿¡ä»»(trust)å’Œå¯¹é½(alignment)æ–¹é¢çš„å½±å“ã€‚è¯¥ç»¼è¿°ç»¼åˆäº†å­¦æœ¯æ–‡çŒ®ã€è¡Œä¸šæŠ¥å‘Šå’ŒæŠ€æœ¯å®æ–½æŒ‡å—ï¼Œä¸ºéƒ¨ç½²å¼¹æ€§ã€å®‰å…¨ä¸”é€‚åº”ç‰¹å®šé¢†åŸŸçš„RAGç³»ç»Ÿæ„å»ºäº†ä¸€ä¸ªå®ç”¨çš„æŠ€æœ¯å‚è€ƒæ¡†æ¶ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "86 pages, 2 figures, 37 tables. A comprehensive review of Retrieval-Augmented Generation (RAG) architectures and trust frameworks (2018-2025), encompassing a unified taxonomy, evaluation benchmarks, and trust-safety modeling",
      "pdf_url": "https://arxiv.org/pdf/2601.05264v1",
      "published_date": "2025-11-07 16:26:29 UTC",
      "updated_date": "2025-11-07 16:26:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:08:32.021952+00:00"
    },
    {
      "arxiv_id": "2511.05399v1",
      "title": "Robust Neural Audio Fingerprinting using Music Foundation Models",
      "title_zh": "åŸºäºéŸ³ä¹åŸºç¡€æ¨¡å‹çš„é²æ£’ç¥ç»éŸ³é¢‘æŒ‡çº¹",
      "authors": [
        "Shubhr Singh",
        "Kiran Bhat",
        "Xavier Riley",
        "Benjamin Resnick",
        "John Thickstun",
        "Walter De Brouwer"
      ],
      "abstract": "The proliferation of distorted, compressed, and manipulated music on modern media platforms like TikTok motivates the development of more robust audio fingerprinting techniques to identify the sources of musical recordings. In this paper, we develop and evaluate new neural audio fingerprinting techniques with the aim of improving their robustness. We make two contributions to neural fingerprinting methodology: (1) we use a pretrained music foundation model as the backbone of the neural architecture and (2) we expand the use of data augmentation to train fingerprinting models under a wide variety of audio manipulations, including time streching, pitch modulation, compression, and filtering. We systematically evaluate our methods in comparison to two state-of-the-art neural fingerprinting models: NAFP and GraFPrint. Results show that fingerprints extracted with music foundation models (e.g., MuQ, MERT) consistently outperform models trained from scratch or pretrained on non-musical audio. Segment-level evaluation further reveals their capability to accurately localize fingerprint matches, an important practical feature for catalog management.",
      "tldr_zh": "æœ¬æ–‡é’ˆå¯¹TikTokç­‰ç°ä»£åª’ä½“å¹³å°ä¸ŠéŸ³ä¹å½•éŸ³å¸¸è¢«æ‰­æ›²ã€å‹ç¼©å’Œç¯¡æ”¹çš„é—®é¢˜ï¼Œå¼€å‘å¹¶è¯„ä¼°äº†æ—¨åœ¨æé«˜é²æ£’æ€§çš„æ–°å‹ç¥ç»éŸ³é¢‘æŒ‡çº¹æŠ€æœ¯(neural audio fingerprinting)ã€‚è¯¥ç ”ç©¶ä¸»è¦åšå‡ºäº†ä¸¤é¡¹æ–¹æ³•å­¦è´¡çŒ®ï¼šé¦–å…ˆï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„éŸ³ä¹åŸºç¡€æ¨¡å‹(music foundation model)ä½œä¸ºç¥ç»æ¶æ„çš„ä¸»å¹²ï¼›å…¶æ¬¡ï¼Œæ‰©å±•äº†æ•°æ®å¢å¼º(data augmentation)çš„ä½¿ç”¨ï¼Œåœ¨åŒ…æ‹¬æ—¶é—´ä¼¸ç¼©ã€éŸ³é«˜è°ƒåˆ¶ã€å‹ç¼©å’Œæ»¤æ³¢ç­‰å¤šç§éŸ³é¢‘æ“ä½œä¸‹è®­ç»ƒæŒ‡çº¹æ¨¡å‹ã€‚é€šè¿‡ä¸NAFPå’ŒGraFPrintç­‰æœ€å…ˆè¿›æ¨¡å‹è¿›è¡Œç³»ç»Ÿå¯¹æ¯”ï¼Œç»“æœæ˜¾ç¤ºåŸºäºMuQå’ŒMERTç­‰éŸ³ä¹åŸºç¡€æ¨¡å‹æå–çš„æŒ‡çº¹å§‹ç»ˆä¼˜äºä»å¤´è®­ç»ƒæˆ–åœ¨ééŸ³ä¹éŸ³é¢‘ä¸Šé¢„è®­ç»ƒçš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œç‰‡æ®µçº§è¯„ä¼°è¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿå‡†ç¡®åœ°å®šä½æŒ‡çº¹åŒ¹é…ä½ç½®ï¼Œè¿™å¯¹ç›®å½•ç®¡ç†å…·æœ‰é‡è¦çš„å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05399v1",
      "published_date": "2025-11-07 16:25:59 UTC",
      "updated_date": "2025-11-07 16:25:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:09:04.338160+00:00"
    },
    {
      "arxiv_id": "2511.05396v1",
      "title": "Sample Complexity of Distributionally Robust Off-Dynamics Reinforcement Learning with Online Interaction",
      "title_zh": "åŸºäºåœ¨çº¿äº¤äº’çš„åˆ†å¸ƒé²æ£’å¼‚åŠ¨åŠ›å­¦å¼ºåŒ–å­¦ä¹ çš„æ ·æœ¬å¤æ‚åº¦",
      "authors": [
        "Yiting He",
        "Zhishuai Liu",
        "Weixin Wang",
        "Pan Xu"
      ],
      "abstract": "Off-dynamics reinforcement learning (RL), where training and deployment transition dynamics are different, can be formulated as learning in a robust Markov decision process (RMDP) where uncertainties in transition dynamics are imposed. Existing literature mostly assumes access to generative models allowing arbitrary state-action queries or pre-collected datasets with a good state coverage of the deployment environment, bypassing the challenge of exploration. In this work, we study a more realistic and challenging setting where the agent is limited to online interaction with the training environment. To capture the intrinsic difficulty of exploration in online RMDPs, we introduce the supremal visitation ratio, a novel quantity that measures the mismatch between the training dynamics and the deployment dynamics. We show that if this ratio is unbounded, online learning becomes exponentially hard. We propose the first computationally efficient algorithm that achieves sublinear regret in online RMDPs with $f$-divergence based transition uncertainties. We also establish matching regret lower bounds, demonstrating that our algorithm achieves optimal dependence on both the supremal visitation ratio and the number of interaction episodes. Finally, we validate our theoretical results through comprehensive numerical experiments.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†Off-dynamics Reinforcement Learningåœºæ™¯ï¼Œå³è®­ç»ƒå’Œéƒ¨ç½²æ—¶çš„è½¬æ¢åŠ¨åŠ›å­¦å­˜åœ¨å·®å¼‚ï¼Œå¹¶å°†å…¶å»ºæ¨¡ä¸ºRobust Markov Decision Process (RMDP)ã€‚ä¸åŒäºç°æœ‰æ–‡çŒ®é€šå¸¸å‡è®¾æ‹¥æœ‰ç”Ÿæˆæ¨¡å‹æˆ–è¦†ç›–è‰¯å¥½çš„é¢„æ”¶é›†æ•°æ®é›†ä»¥å›é¿æ¢ç´¢æŒ‘æˆ˜ï¼Œæœ¬æ–‡èšç„¦äºæ™ºèƒ½ä½“ä»…èƒ½ä¸è®­ç»ƒç¯å¢ƒè¿›è¡ŒOnline Interactionï¼ˆåœ¨çº¿äº¤äº’ï¼‰è¿™ä¸€æ›´å…·ç°å®æ„ä¹‰çš„è®¾å®šã€‚ä¸ºäº†åˆ»ç”»åœ¨çº¿RMDPä¸­æ¢ç´¢çš„å†…åœ¨éš¾åº¦ï¼Œä½œè€…å¼•å…¥äº†supremal visitation ratioè¿™ä¸€æ–°æŒ‡æ ‡ï¼Œç”¨äºè¡¡é‡è®­ç»ƒåŠ¨åŠ›å­¦ä¸éƒ¨ç½²åŠ¨åŠ›å­¦ä¹‹é—´çš„ä¸åŒ¹é…ç¨‹åº¦ï¼Œå¹¶è¯æ˜è‹¥æ­¤æ¯”ç‡æ— ç•Œï¼Œåœ¨çº¿å­¦ä¹ å°†å‘ˆæŒ‡æ•°çº§å›°éš¾ã€‚å¯¹æ­¤ï¼Œä½œè€…æå‡ºäº†é¦–ä¸ªè®¡ç®—é«˜æ•ˆçš„ç®—æ³•ï¼Œåœ¨åŸºäº$f$-divergenceçš„è½¬æ¢ä¸ç¡®å®šæ€§ä¸‹å®ç°äº†sublinear regretã€‚ç ”ç©¶è¿›ä¸€æ­¥å»ºç«‹äº†åŒ¹é…çš„é—æ†¾ä¸‹ç•Œï¼Œè¯æ˜äº†è¯¥ç®—æ³•åœ¨supremal visitation ratioå’Œäº¤äº’å›åˆæ•°çš„ä¾èµ–å…³ç³»ä¸Šè¾¾åˆ°äº†æœ€ä¼˜ï¼Œå¹¶é€šè¿‡æ•°å€¼å®éªŒéªŒè¯äº†ç†è®ºç»“æœçš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "53 pages, 6 figures, 3 tables. Published in Proceedings of the 42nd International Conference on Machine Learning (ICML 2025)",
      "pdf_url": "https://arxiv.org/pdf/2511.05396v1",
      "published_date": "2025-11-07 16:24:22 UTC",
      "updated_date": "2025-11-07 16:24:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:09:43.928670+00:00"
    },
    {
      "arxiv_id": "2511.05394v2",
      "title": "AI Assisted AR Assembly: Object Recognition and Computer Vision for Augmented Reality Assisted Assembly",
      "title_zh": "AIè¾…åŠ©ARè£…é…ï¼šé¢å‘å¢å¼ºç°å®è¾…åŠ©è£…é…çš„ç›®æ ‡è¯†åˆ«ä¸è®¡ç®—æœºè§†è§‰",
      "authors": [
        "Alexander Htet Kyaw",
        "Haotian Ma",
        "Sasa Zivkovic",
        "Jenny Sabin"
      ],
      "abstract": "We present an AI-assisted Augmented Reality assembly workflow that uses deep learning-based object recognition to identify different assembly components and display step-by-step instructions. For each assembly step, the system displays a bounding box around the corresponding components in the physical space, and where the component should be placed. By connecting assembly instructions with the real-time location of relevant components, the system eliminates the need for manual searching, sorting, or labeling of different components before each assembly. To demonstrate the feasibility of using object recognition for AR-assisted assembly, we highlight a case study involving the assembly of LEGO sculptures.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§AIè¾…åŠ©çš„å¢å¼ºç°å®(Augmented Reality, AR)è£…é…å·¥ä½œæµï¼Œåˆ©ç”¨åŸºäºæ·±åº¦å­¦ä¹ çš„ç›®æ ‡è¯†åˆ«(object recognition)æŠ€æœ¯æ¥è¯†åˆ«ä¸åŒçš„è£…é…ç»„ä»¶å¹¶æ˜¾ç¤ºåˆ†æ­¥è¯´æ˜ã€‚å¯¹äºæ¯ä¸ªè£…é…æ­¥éª¤ï¼Œç³»ç»Ÿä¼šåœ¨ç‰©ç†ç©ºé—´ä¸­çš„ç›¸åº”ç»„ä»¶å‘¨å›´æ˜¾ç¤ºè¾¹ç•Œæ¡†(bounding box)ï¼Œå¹¶æŒ‡ç¤ºç»„ä»¶åº”æ”¾ç½®çš„ä½ç½®ã€‚é€šè¿‡å°†è£…é…è¯´æ˜ä¸ç›¸å…³ç»„ä»¶çš„å®æ—¶ä½ç½®ç›¸ç»“åˆï¼Œè¯¥ç³»ç»Ÿæ¶ˆé™¤äº†åœ¨æ¯æ¬¡è£…é…å‰æ‰‹åŠ¨æœç´¢ã€åˆ†ç±»æˆ–æ ‡è®°ä¸åŒç»„ä»¶çš„éœ€æ±‚ã€‚ä¸ºäº†è¯æ˜åˆ©ç”¨ç›®æ ‡è¯†åˆ«è¿›è¡ŒARè¾…åŠ©è£…é…çš„å¯è¡Œæ€§ï¼Œè¯¥ç ”ç©¶é‡ç‚¹ä»‹ç»äº†ä¸€ä¸ªæ¶‰åŠLEGOé›•å¡‘è£…é…çš„æ¡ˆä¾‹ç ”ç©¶ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to the Association for Computing Machinery (ACM) Symposium on Computational Fabrication (SCF '25)",
      "pdf_url": "https://arxiv.org/pdf/2511.05394v2",
      "published_date": "2025-11-07 16:20:53 UTC",
      "updated_date": "2025-11-13 18:28:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:10:04.019168+00:00"
    },
    {
      "arxiv_id": "2511.05385v1",
      "title": "TeaRAG: A Token-Efficient Agentic Retrieval-Augmented Generation Framework",
      "title_zh": "TeaRAGï¼šTokené«˜æ•ˆçš„æ™ºèƒ½ä½“æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶",
      "authors": [
        "Chao Zhang",
        "Yuhao Wang",
        "Derong Xu",
        "Haoxin Zhang",
        "Yuanjie Lyu",
        "Yuhao Chen",
        "Shuochen Liu",
        "Tong Xu",
        "Xiangyu Zhao",
        "Yan Gao",
        "Yao Hu",
        "Enhong Chen"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) utilizes external knowledge to augment Large Language Models' (LLMs) reliability. For flexibility, agentic RAG employs autonomous, multi-round retrieval and reasoning to resolve queries. Although recent agentic RAG has improved via reinforcement learning, they often incur substantial token overhead from search and reasoning processes. This trade-off prioritizes accuracy over efficiency. To address this issue, this work proposes TeaRAG, a token-efficient agentic RAG framework capable of compressing both retrieval content and reasoning steps. 1) First, the retrieved content is compressed by augmenting chunk-based semantic retrieval with a graph retrieval using concise triplets. A knowledge association graph is then built from semantic similarity and co-occurrence. Finally, Personalized PageRank is leveraged to highlight key knowledge within this graph, reducing the number of tokens per retrieval. 2) Besides, to reduce reasoning steps, Iterative Process-aware Direct Preference Optimization (IP-DPO) is proposed. Specifically, our reward function evaluates the knowledge sufficiency by a knowledge matching mechanism, while penalizing excessive reasoning steps. This design can produce high-quality preference-pair datasets, supporting iterative DPO to improve reasoning conciseness. Across six datasets, TeaRAG improves the average Exact Match by 4% and 2% while reducing output tokens by 61% and 59% on Llama3-8B-Instruct and Qwen2.5-14B-Instruct, respectively. Code is available at https://github.com/Applied-Machine-Learning-Lab/TeaRAG.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰çš„Agentic Retrieval-Augmented Generation (RAG)ç³»ç»Ÿåœ¨è¿½æ±‚å‡†ç¡®æ€§æ—¶é¢ä¸´çš„é«˜Tokenå¼€é”€é—®é¢˜ï¼Œæå‡ºäº†TeaRAGï¼Œä¸€ç§æ—¨åœ¨åŒæ—¶å‹ç¼©æ£€ç´¢å†…å®¹å’Œæ¨ç†æ­¥éª¤çš„é«˜æ•ˆæ¡†æ¶ã€‚åœ¨æ£€ç´¢æ–¹é¢ï¼ŒTeaRAGç»“åˆäº†åŸºäºå—çš„è¯­ä¹‰æ£€ç´¢ä¸ä½¿ç”¨ç®€æ´ä¸‰å…ƒç»„çš„å›¾æ£€ç´¢ï¼Œæ„å»ºçŸ¥è¯†å…³è”å›¾ï¼Œå¹¶åˆ©ç”¨Personalized PageRankç®—æ³•çªå‡ºå…³é”®çŸ¥è¯†ï¼Œä»è€Œæ˜¾è‘—å‡å°‘æ¯æ¬¡æ£€ç´¢çš„Tokenæ•°é‡ã€‚åœ¨æ¨ç†æ–¹é¢ï¼Œè¯¥ç ”ç©¶æå‡ºäº†Iterative Process-aware Direct Preference Optimization (IP-DPO)ï¼Œé€šè¿‡å¥–åŠ±å‡½æ•°è¯„ä¼°çŸ¥è¯†å……åˆ†æ€§å¹¶æƒ©ç½šè¿‡åº¦çš„æ¨ç†æ­¥éª¤ï¼Œä»è€Œä¼˜åŒ–æ¨ç†è¿‡ç¨‹çš„ç®€æ´æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å…­ä¸ªæ•°æ®é›†ä¸Šï¼ŒTeaRAGåœ¨Llama3-8B-Instructå’ŒQwen2.5-14B-Instructæ¨¡å‹ä¸Šåˆ†åˆ«å°†å¹³å‡Exact Matchæé«˜äº†4%å’Œ2%ï¼ŒåŒæ—¶å°†è¾“å‡ºTokenæ•°é‡åˆ†åˆ«å‡å°‘äº†61%å’Œ59%ï¼Œå®ç°äº†æ•ˆç‡ä¸å‡†ç¡®æ€§çš„åŒé‡æå‡ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "32 pages",
      "pdf_url": "https://arxiv.org/pdf/2511.05385v1",
      "published_date": "2025-11-07 16:08:34 UTC",
      "updated_date": "2025-11-07 16:08:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:10:29.307140+00:00"
    },
    {
      "arxiv_id": "2511.05375v1",
      "title": "Reasoning Is All You Need for Urban Planning AI",
      "title_zh": "åŸå¸‚è§„åˆ’äººå·¥æ™ºèƒ½ï¼šæ¨ç†å³ä¸€åˆ‡",
      "authors": [
        "Sijie Yang",
        "Jiatong Li",
        "Filip Biljecki"
      ],
      "abstract": "AI has proven highly successful at urban planning analysis -- learning patterns from data to predict future conditions. The next frontier is AI-assisted decision-making: agents that recommend sites, allocate resources, and evaluate trade-offs while reasoning transparently about constraints and stakeholder values. Recent breakthroughs in reasoning AI -- CoT prompting, ReAct, and multi-agent collaboration frameworks -- now make this vision achievable.\n  This position paper presents the Agentic Urban Planning AI Framework for reasoning-capable planning agents that integrates three cognitive layers (Perception, Foundation, Reasoning) with six logic components (Analysis, Generation, Verification, Evaluation, Collaboration, Decision) through a multi-agents collaboration framework. We demonstrate why planning decisions require explicit reasoning capabilities that are value-based (applying normative principles), rule-grounded (guaranteeing constraint satisfaction), and explainable (generating transparent justifications) -- requirements that statistical learning alone cannot fulfill. We compare reasoning agents with statistical learning, present a comprehensive architecture with benchmark evaluation metrics, and outline critical research challenges. This framework shows how AI agents can augment human planners by systematically exploring solution spaces, verifying regulatory compliance, and deliberating over trade-offs transparently -- not replacing human judgment but amplifying it with computational reasoning capabilities.",
      "tldr_zh": "è¿™ç¯‡ç«‹åœºè®ºæ–‡æå‡ºäº†Agentic Urban Planning AI Frameworkï¼Œæ—¨åœ¨åˆ©ç”¨æ¨ç†å‹AIæŠ€æœ¯æ¨åŠ¨åŸå¸‚è§„åˆ’ä»å•çº¯çš„æ•°æ®åˆ†æå‘è¾…åŠ©å†³ç­–è½¬å˜ã€‚è¯¥æ¡†æ¶ç»“åˆäº†æ€ç»´é“¾æç¤º(CoT prompting)å’ŒReActç­‰çªç ´æ€§æŠ€æœ¯ï¼Œé€šè¿‡å¤šæ™ºèƒ½ä½“åä½œæ¶æ„é›†æˆäº†æ„ŸçŸ¥ã€åŸºç¡€å’Œæ¨ç†ä¸‰ä¸ªè®¤çŸ¥å±‚ï¼Œä»¥åŠåˆ†æã€ç”Ÿæˆã€éªŒè¯ç­‰å…­ä¸ªé€»è¾‘ç»„ä»¶ã€‚ä½œè€…è®ºè¯äº†åŸå¸‚è§„åˆ’å†³ç­–éœ€è¦åŸºäºä»·å€¼(value-based)ã€åŸºäºè§„åˆ™(rule-grounded)ä¸”å…·æœ‰å¯è§£é‡Šæ€§(explainable)çš„æ˜¾å¼æ¨ç†èƒ½åŠ›ï¼Œè¿™æ˜¯ä¼ ç»Ÿç»Ÿè®¡å­¦ä¹ æ— æ³•æ»¡è¶³çš„ã€‚æ–‡ç« å¯¹æ¯”äº†æ¨ç†æ™ºèƒ½ä½“ä¸ç»Ÿè®¡å­¦ä¹ çš„å·®å¼‚ï¼Œå¹¶æå‡ºäº†åŒ…å«åŸºå‡†è¯„ä¼°æŒ‡æ ‡çš„ç»¼åˆæ¶æ„åŠå…³é”®ç ”ç©¶æŒ‘æˆ˜ã€‚æœ€ç»ˆï¼Œè¯¥æ¡†æ¶å±•ç¤ºäº†AIæ™ºèƒ½ä½“å¦‚ä½•é€šè¿‡ç³»ç»Ÿåœ°æ¢ç´¢è§£ç©ºé—´ã€éªŒè¯æ³•è§„éµä»æ€§åŠé€æ˜åœ°æƒè¡¡åˆ©å¼Šæ¥å¢å¼ºè€Œéæ›¿ä»£äººç±»è§„åˆ’å¸ˆçš„åˆ¤æ–­åŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Submitted to AAAI 2026 Workshop AI4UP",
      "pdf_url": "https://arxiv.org/pdf/2511.05375v1",
      "published_date": "2025-11-07 15:59:06 UTC",
      "updated_date": "2025-11-07 15:59:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:14:27.274736+00:00"
    },
    {
      "arxiv_id": "2511.05363v1",
      "title": "AI Literacy for Community Colleges: Instructors' Perspectives on Scenario-Based and Interactive Approaches to Teaching AI",
      "title_zh": "ç¤¾åŒºå­¦é™¢çš„AIç´ å…»ï¼šåŸºäºåœºæ™¯ä¸äº¤äº’å¼AIæ•™å­¦æ–¹æ³•çš„æ•™å¸ˆè§†è§’",
      "authors": [
        "Aparna Maya Warrier",
        "Arav Agarwal",
        "Jaromir Savelka",
        "Christopher A Bogart",
        "Heather Burte"
      ],
      "abstract": "This research category full paper investigates how community college instructors evaluate interactive, no-code AI literacy resources designed for non-STEM learners. As artificial intelligence becomes increasingly integrated into everyday technologies, AI literacy - the ability to evaluate AI systems, communicate with them, and understand their broader impacts - has emerged as a critical skill across disciplines. Yet effective, scalable approaches for teaching these concepts in higher education remain limited, particularly for students outside STEM fields.\n  To address this gap, we developed AI User, an interactive online curriculum that introduces core AI concepts through scenario - based activities set in real - world contexts. This study presents findings from four focus groups with instructors who engaged with AI User materials and participated in structured feedback activities. Thematic analysis revealed that instructors valued exploratory tasks that simulated real - world AI use cases and fostered experimentation, while also identifying challenges related to scaffolding, accessibility, and multi-modal support. A ranking task for instructional support materials showed a strong preference for interactive demonstrations over traditional educational materials like conceptual guides or lecture slides.\n  These findings offer insights into instructor perspectives on making AI concepts more accessible and relevant for broad learner audiences. They also inform the design of AI literacy tools that align with diverse teaching contexts and support critical engagement with AI in higher education.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹éSTEMå­¦ä¹ è€…åœ¨é«˜ç­‰æ•™è‚²ä¸­ç¼ºä¹æœ‰æ•ˆAIæ•™å­¦æ–¹æ³•çš„ç°çŠ¶ï¼Œè°ƒæŸ¥äº†ç¤¾åŒºå­¦é™¢è®²å¸ˆå¯¹äº¤äº’å¼æ— ä»£ç AIç´ å…»èµ„æºçš„è¯„ä»·ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†åä¸º\"AI User\"çš„äº¤äº’å¼åœ¨çº¿è¯¾ç¨‹ï¼Œé€šè¿‡åŸºäºç°å®åœºæ™¯çš„æ´»åŠ¨ä»‹ç»æ ¸å¿ƒAIæ¦‚å¿µã€‚åŸºäºå››ä¸ªç„¦ç‚¹å°ç»„çš„åé¦ˆå’Œä¸»é¢˜åˆ†æï¼Œç ”ç©¶å‘ç°è®²å¸ˆé«˜åº¦è¯„ä»·æ¨¡æ‹ŸçœŸå®ä¸–ç•ŒAIç”¨ä¾‹å’Œä¿ƒè¿›å®éªŒçš„æ¢ç´¢æ€§ä»»åŠ¡ï¼ŒåŒæ—¶ä¹ŸæŒ‡å‡ºäº†åœ¨æ•™å­¦è„šæ‰‹æ¶(scaffolding)ã€å¯è®¿é—®æ€§å’Œå¤šæ¨¡æ€æ”¯æŒæ–¹é¢å­˜åœ¨çš„æŒ‘æˆ˜ã€‚åœ¨æ•™å­¦è¾…åŠ©ææ–™çš„æ’åä»»åŠ¡ä¸­ï¼Œè®²å¸ˆè¡¨ç°å‡ºå¯¹äº¤äº’å¼æ¼”ç¤ºçš„å¼ºçƒˆåå¥½ï¼Œè€Œéæ¦‚å¿µæŒ‡å—æˆ–å¹»ç¯ç‰‡ç­‰ä¼ ç»Ÿææ–™ã€‚è¿™äº›å‘ç°ä¸ºå¦‚ä½•ä½¿AIæ¦‚å¿µå¯¹å¹¿æ³›çš„å­¦ä¹ è€…å—ä¼—æ›´å…·å¯è®¿é—®æ€§å’Œç›¸å…³æ€§æä¾›äº†è§è§£ï¼Œå¹¶ä¸ºè®¾è®¡æ”¯æŒé«˜ç­‰æ•™è‚²ä¸­æ‰¹åˆ¤æ€§å‚ä¸çš„AIç´ å…»å·¥å…·æä¾›äº†æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05363v1",
      "published_date": "2025-11-07 15:51:53 UTC",
      "updated_date": "2025-11-07 15:51:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:14:04.031391+00:00"
    },
    {
      "arxiv_id": "2511.05361v1",
      "title": "A multimodal multiplex of the mental lexicon for multilingual individuals",
      "title_zh": "å¤šè¯­è¨€ä¸ªä½“å¿ƒç†è¯å…¸çš„å¤šæ¨¡æ€å¤šé‡ç½‘ç»œ",
      "authors": [
        "Maria Huynh",
        "Wilder C. Rodrigues"
      ],
      "abstract": "Historically, bilingualism was often perceived as an additional cognitive load that could hinder linguistic and intellectual development. However, over the last three decades, this view has changed considerably. Numerous studies have aimed to model and understand the architecture of the bilingual word recognition system Dijkstra and van Heuven (2002), investigating how parallel activation operates in the brain and how one language influences another Kroll et al. (2015). Increasingly, evidence suggests that multilinguals, individuals who speak three or more languages, can perform better than monolinguals in various linguistic and cognitive tasks, such as learning an additional language Abu-Rabia and Sanitsky (2010). This research proposal focuses on the study of the mental lexicon and how it may be structured in individuals who speak multiple languages. Building on the work of Stella et al. (2018), who investigated explosive learning in humans using a multiplex model of the mental lexicon, and the Bilingual Interactive Activation (BIA+) framework proposed by Dijkstra and van Heuven (2002), the present study applies the same multilayer network principles introduced by Kivela et al. (2014). Our experimental design extends previous research by incorporating multimodality into the multiplex model, introducing an additional layer that connects visual inputs to their corresponding lexical representations across the multilingual layers of the mental lexicon. In this research, we aim to explore how a heritage language influences the acquisition of another language. Specifically, we ask: Does the presence of visual input in a translation task influence participants' proficiency and accuracy compared to text-only conditions?",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶èšç„¦äºå¤šè¯­è¨€ä¸ªä½“çš„å¿ƒç†è¯åº“(mental lexicon)ç»“æ„ï¼Œæ—¨åœ¨æ¢ç©¶ä¼ ç»Ÿè¯­è¨€(heritage language)å¦‚ä½•å½±å“æ–°è¯­è¨€çš„ä¹ å¾—ã€‚å»ºç«‹åœ¨Stellaç­‰äººçš„å¤šè·¯å¤ç”¨æ¨¡å‹(multiplex model)å’ŒDijkstraä¸van Heuvençš„BIA+æ¡†æ¶åŸºç¡€ä¹‹ä¸Šï¼Œè¯¥ç ”ç©¶åº”ç”¨äº†å¤šå±‚ç½‘ç»œåŸç†æ¥æ¨¡æ‹ŸåŒè¯­è¯æ±‡è¯†åˆ«ç³»ç»Ÿã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†å¤šæ¨¡æ€æ€§(multimodality)æ•´åˆè¿›å¤šè·¯å¤ç”¨æ¨¡å‹ä¸­ï¼Œå¼•å…¥äº†ä¸€ä¸ªé¢å¤–çš„å±‚çº§ï¼Œå°†è§†è§‰è¾“å…¥ä¸è·¨å¤šè¯­è¨€å±‚çš„è¯æ±‡è¡¨å¾è¿æ¥èµ·æ¥ã€‚é€šè¿‡è¿™ä¸€è®¾è®¡ï¼Œç ”ç©¶è€…æ—¨åœ¨é€šè¿‡å®éªŒç¡®å®šåœ¨ç¿»è¯‘ä»»åŠ¡ä¸­ï¼Œè§†è§‰è¾“å…¥çš„å­˜åœ¨ç›¸æ¯”çº¯æ–‡æœ¬æ¡ä»¶æ˜¯å¦ä¼šæ˜¾è‘—å½±å“å‚ä¸è€…çš„ç†Ÿç»ƒåº¦å’Œå‡†ç¡®æ€§ã€‚è¯¥å·¥ä½œä¸ä»…æ‰©å±•äº†å¯¹å¤šè¯­è¨€å¤§è„‘å¹¶è¡Œæ¿€æ´»æœºåˆ¶çš„ç†è§£ï¼Œä¹Ÿä¸ºåˆ†æå¤šè¯­è¨€è€…åœ¨è®¤çŸ¥ä»»åŠ¡ä¸­çš„è¡¨ç°æä¾›äº†æ–°çš„ç†è®ºè§†è§’ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05361v1",
      "published_date": "2025-11-07 15:51:21 UTC",
      "updated_date": "2025-11-07 15:51:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:14:29.008291+00:00"
    },
    {
      "arxiv_id": "2511.05350v2",
      "title": "Perceptually Aligning Representations of Music via Noise-Augmented Autoencoders",
      "title_zh": "åŸºäºå™ªå£°å¢å¼ºè‡ªç¼–ç å™¨çš„éŸ³ä¹è¡¨ç¤ºæ„ŸçŸ¥å¯¹é½",
      "authors": [
        "Mathias Rose Bjare",
        "Giorgia Cantisani",
        "Marco Pasini",
        "Stefan Lattner",
        "Gerhard Widmer"
      ],
      "abstract": "We argue that training autoencoders to reconstruct inputs from noised versions of their encodings, when combined with perceptual losses, yields encodings that are structured according to a perceptual hierarchy. We demonstrate the emergence of this hierarchical structure by showing that, after training an audio autoencoder in this manner, perceptually salient information is captured in coarser representation structures than with conventional training. Furthermore, we show that such perceptual hierarchies improve latent diffusion decoding in the context of estimating surprisal in music pitches and predicting EEG-brain responses to music listening. Pretrained weights are available on github.com/CPJKU/pa-audioic.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨å™ªå£°å¢å¼ºè‡ªåŠ¨ç¼–ç å™¨(Noise-Augmented Autoencoders)æ¥å®ç°éŸ³ä¹è¡¨ç¤ºæ„ŸçŸ¥å¯¹é½çš„æ–¹æ³•ã€‚ä½œè€…é€šè¿‡ç»“åˆæ„ŸçŸ¥æŸå¤±(perceptual losses)ä¸ä»å™ªå£°ç¼–ç ä¸­é‡å»ºè¾“å…¥çš„è®­ç»ƒç­–ç•¥ï¼ŒæˆåŠŸæ„å»ºäº†ç¬¦åˆæ„ŸçŸ¥å±‚çº§(perceptual hierarchy)çš„ç¼–ç ç»“æ„ã€‚å®éªŒè¯æ˜ï¼Œç›¸æ¯”ä¼ ç»Ÿè®­ç»ƒæ–¹æ³•ï¼Œè¯¥ç­–ç•¥ä½¿å¾—æ„ŸçŸ¥ä¸Šæ˜¾è‘—çš„ä¿¡æ¯èƒ½å¤Ÿè¢«æ•æ‰åœ¨æ›´ç²—ç³™çš„è¡¨ç¤ºç»“æ„ä¸­ã€‚æ­¤å¤–ï¼Œè¿™ç§æ„ŸçŸ¥å±‚çº§ç»“æ„æ˜¾è‘—æå‡äº†æ½œåœ¨æ‰©æ•£è§£ç (latent diffusion decoding)åœ¨å…·ä½“ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¿™äº›ä»»åŠ¡åŒ…æ‹¬ä¼°è®¡éŸ³ä¹éŸ³é«˜çš„æƒŠå¥‡åº¦(surprisal)ä»¥åŠé¢„æµ‹å¬éŸ³ä¹æ—¶çš„è„‘ç”µå›¾(EEG)ååº”ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted at NeurIPS 2025 - AI for Music Workshop, 11 pages, 5 figures, 1 table",
      "pdf_url": "https://arxiv.org/pdf/2511.05350v2",
      "published_date": "2025-11-07 15:44:12 UTC",
      "updated_date": "2025-11-10 14:11:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:14:51.953088+00:00"
    },
    {
      "arxiv_id": "2511.11629v1",
      "title": "Global Feature Enhancing and Fusion Framework for Strain Gauge Time Series Classification",
      "title_zh": "åº”å˜ç‰‡æ—¶é—´åºåˆ—åˆ†ç±»çš„å…¨å±€ç‰¹å¾å¢å¼ºä¸èåˆæ¡†æ¶",
      "authors": [
        "Xu Zhang",
        "Peng Wang",
        "Chen Wang",
        "Zhe Xu",
        "Xiaohua Nie",
        "Wei Wang"
      ],
      "abstract": "Strain Gauge Status (SGS) recognition is crucial in the field of intelligent manufacturing based on the Internet of Things, as accurate identification helps timely detection of failed mechanical components, avoiding accidents. The loading and unloading sequences generated by strain gauges can be identified through time series classification (TSC) algorithms. Recently, deep learning models, e.g., convolutional neural networks (CNNs) have shown remarkable success in the TSC task, as they can extract discriminative local features from the subsequences to identify the time series. However, we observe that only the local features may not be sufficient for expressing the time series, especially when the local sub-sequences between different time series are very similar, e.g., SGS data of aircraft wings in static strength experiments. Nevertheless, CNNs suffer from the limitation in extracting global features due to the nature of convolution operations. For extracting global features to more comprehensively represent the SGS time series, we propose two insights: (i) Constructing global features through feature engineering. (ii) Learning high-order relationships between local features to capture global features. To realize and utilize them, we propose a hypergraph-based global feature learning and fusion framework, which learns and fuses global features for semantic consistency to enhance the representation of SGS time series, thereby improving recognition accuracy. Our method designs are validated on industrial SGS and public UCR datasets, showing better generalization for unseen data in SGS recognition.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ™ºèƒ½åˆ¶é€ ä¸­çš„åº”å˜ç‰‡çŠ¶æ€(Strain Gauge Status, SGS)è¯†åˆ«é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºè¶…å›¾(hypergraph)çš„å…¨å±€ç‰¹å¾å¢å¼ºä¸èåˆæ¡†æ¶ã€‚å°½ç®¡å·ç§¯ç¥ç»ç½‘ç»œ(CNNs)åœ¨æ—¶é—´åºåˆ—åˆ†ç±»(TSC)ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶å—é™äºå·ç§¯æ“ä½œçš„æ€§è´¨ï¼Œéš¾ä»¥æœ‰æ•ˆæå–å…¨å±€ç‰¹å¾ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸åŒæ—¶é—´åºåˆ—çš„å±€éƒ¨å­åºåˆ—éå¸¸ç›¸ä¼¼çš„æƒ…å†µä¸‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œè¯¥è®ºæ–‡æå‡ºäº†é€šè¿‡ç‰¹å¾å·¥ç¨‹æ„å»ºå…¨å±€ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨è¶…å›¾å­¦ä¹ å±€éƒ¨ç‰¹å¾ä¹‹é—´çš„é«˜é˜¶å…³ç³»ä»¥æ•æ‰å…¨å±€ä¿¡æ¯çš„ç­–ç•¥ã€‚è¯¥æ¡†æ¶é€šè¿‡èåˆå…¨å±€ç‰¹å¾æ¥å®ç°è¯­ä¹‰ä¸€è‡´æ€§ï¼Œä»è€Œå¢å¼ºäº†SGSæ—¶é—´åºåˆ—çš„è¡¨ç¤ºèƒ½åŠ›å¹¶æé«˜äº†è¯†åˆ«å‡†ç¡®ç‡ã€‚åœ¨å·¥ä¸šSGSæ•°æ®é›†å’Œå…¬å…±UCRæ•°æ®é›†ä¸Šçš„éªŒè¯ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨SGSè¯†åˆ«ä»»åŠ¡ä¸­å¯¹æœªè§æ•°æ®å…·æœ‰æ›´å¥½çš„æ³›åŒ–æ€§èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Global Feature Enhancing and Fusion Framework for Time Series Classification",
      "pdf_url": "https://arxiv.org/pdf/2511.11629v1",
      "published_date": "2025-11-07 15:36:11 UTC",
      "updated_date": "2025-11-07 15:36:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:15:15.818220+00:00"
    },
    {
      "arxiv_id": "2511.05320v1",
      "title": "What Are the Facts? Automated Extraction of Court-Established Facts from Criminal-Court Opinions",
      "title_zh": "äº‹å®ä¸ºä½•ï¼Ÿä»åˆ‘äº‹è£åˆ¤æ–‡ä¹¦ä¸­è‡ªåŠ¨æå–æ³•é™¢è®¤å®šçš„äº‹å®",
      "authors": [
        "KlÃ¡ra BendovÃ¡",
        "TomÃ¡Å¡ Knap",
        "Jan ÄŒernÃ½",
        "VojtÄ›ch Pour",
        "Jaromir Savelka",
        "Ivana KvapilÃ­kovÃ¡",
        "Jakub DrÃ¡pal"
      ],
      "abstract": "Criminal justice administrative data contain only a limited amount of information about the committed offense. However, there is an unused source of extensive information in continental European courts' decisions: descriptions of criminal behaviors in verdicts by which offenders are found guilty. In this paper, we study the feasibility of extracting these descriptions from publicly available court decisions from Slovakia. We use two different approaches for retrieval: regular expressions and large language models (LLMs). Our baseline was a simple method employing regular expressions to identify typical words occurring before and after the description. The advanced regular expression approach further focused on \"sparing\" and its normalization (insertion of spaces between individual letters), typical for delineating the description. The LLM approach involved prompting the Gemini Flash 2.0 model to extract the descriptions using predefined instructions. Although the baseline identified descriptions in only 40.5% of verdicts, both methods significantly outperformed it, achieving 97% with advanced regular expressions and 98.75% with LLMs, and 99.5% when combined. Evaluation by law students showed that both advanced methods matched human annotations in about 90% of cases, compared to just 34.5% for the baseline. LLMs fully matched human-labeled descriptions in 91.75% of instances, and a combination of advanced regular expressions with LLMs reached 92%.",
      "tldr_zh": "è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³åˆ‘äº‹å¸æ³•è¡Œæ”¿æ•°æ®ä¸­çŠ¯ç½ªç»†èŠ‚ä¿¡æ¯åŒ®ä¹çš„é—®é¢˜ï¼Œæ¢è®¨äº†ä»æ–¯æ´›ä¼å…‹å…¬å¼€æ³•åº­åˆ¤å†³ä¹¦ä¸­è‡ªåŠ¨æå–çŠ¯ç½ªè¡Œä¸ºæè¿°ï¼ˆcourt-established factsï¼‰çš„å¯è¡Œæ€§ã€‚ç ”ç©¶å›¢é˜Ÿå¯¹æ¯”äº†åŸºäºæ­£åˆ™è¡¨è¾¾å¼ï¼ˆRegular Expressionsï¼‰å’ŒåŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æå–æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯ä½¿ç”¨äº†Gemini Flash 2.0æ¨¡å‹è¿›è¡Œæç¤ºå·¥ç¨‹ã€‚å®éªŒè®¾ç½®äº†ç®€å•çš„æ­£åˆ™åŸºçº¿ï¼Œå¹¶å¼€å‘äº†é’ˆå¯¹æ–‡æœ¬è§„èŒƒåŒ–ï¼ˆå¦‚å­—ç¬¦é—´è·ï¼‰çš„é«˜çº§æ­£åˆ™æ–¹æ³•ä»¥åŠLLMæå–æ–¹æ¡ˆã€‚ç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶åŸºçº¿æ–¹æ³•ä»…èƒ½è¯†åˆ«40.5%çš„åˆ¤å†³ä¹¦æè¿°ï¼Œä½†é«˜çº§æ­£åˆ™æ–¹æ³•å’ŒLLMæ–¹æ³•åˆ†åˆ«è¾¾åˆ°äº†97%å’Œ98.75%çš„æ£€ç´¢ç‡ï¼Œä¸¤è€…ç»“åˆæ›´æ˜¯é«˜è¾¾99.5%ã€‚ç»æ³•å­¦å­¦ç”Ÿè¯„ä¼°ï¼Œé«˜çº§æ–¹æ³•ä¸äººç±»æ ‡æ³¨çš„åŒ¹é…åº¦çº¦ä¸º90%ï¼Œè¿œè¶…åŸºçº¿çš„34.5%ã€‚å…¶ä¸­ï¼ŒLLMsåœ¨91.75%çš„å®ä¾‹ä¸­å®ç°äº†å®Œå…¨åŒ¹é…ï¼Œç»“åˆé«˜çº§æ­£åˆ™ååŒ¹é…ç‡è¾¾åˆ°92%ï¼Œè¯æ˜äº†åˆ©ç”¨è‡ªåŠ¨åŒ–æŠ€æœ¯ä»å¤§é™†æ³•ç³»æ³•åº­åˆ¤å†³ä¸­æŒ–æ˜éç»“æ„åŒ–æ•°æ®çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Paper accepted to the proceedings of ASAIL 2025 Workshop under ICAIL conference for publication. Paper contains 6 pages (references included) and 2 appendices. It contains 8 tables, no figures",
      "pdf_url": "https://arxiv.org/pdf/2511.05320v1",
      "published_date": "2025-11-07 15:17:45 UTC",
      "updated_date": "2025-11-07 15:17:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:16:06.009594+00:00"
    },
    {
      "arxiv_id": "2511.05311v1",
      "title": "Cleaning Maintenance Logs with LLM Agents for Improved Predictive Maintenance",
      "title_zh": "åˆ©ç”¨ LLM æ™ºèƒ½ä½“æ¸…æ´—ç»´æŠ¤æ—¥å¿—ä»¥æ”¹è¿›é¢„æµ‹æ€§ç»´æŠ¤",
      "authors": [
        "Valeriu Dimidov",
        "Faisal Hawlader",
        "Sasan Jafarnejad",
        "RaphaÃ«l Frank"
      ],
      "abstract": "Economic constraints, limited availability of datasets for reproducibility and shortages of specialized expertise have long been recognized as key challenges to the adoption and advancement of predictive maintenance (PdM) in the automotive sector. Recent progress in large language models (LLMs) presents an opportunity to overcome these barriers and speed up the transition of PdM from research to industrial practice. Under these conditions, we explore the potential of LLM-based agents to support PdM cleaning pipelines. Specifically, we focus on maintenance logs, a critical data source for training well-performing machine learning (ML) models, but one often affected by errors such as typos, missing fields, near-duplicate entries, and incorrect dates. We evaluate LLM agents on cleaning tasks involving six distinct types of noise. Our findings show that LLMs are effective at handling generic cleaning tasks and offer a promising foundation for future industrial applications. While domain-specific errors remain challenging, these results highlight the potential for further improvements through specialized training and enhanced agentic capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ±½è½¦è¡Œä¸šé¢„æµ‹æ€§ç»´æŠ¤(Predictive Maintenance, PdM)é¢ä¸´çš„æ•°æ®é›†ç¨€ç¼ºå’Œä¸“ä¸šçŸ¥è¯†çŸ­ç¼ºç­‰æŒ‘æˆ˜ï¼Œæ¢è®¨äº†åˆ©ç”¨åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„æ™ºèƒ½ä½“æ¥è¾…åŠ©PdMæ•°æ®æ¸…æ´—æµç¨‹ã€‚ç ”ç©¶ç‰¹åˆ«å…³æ³¨ç»´æŠ¤æ—¥å¿—è¿™ä¸€å…³é”®æ•°æ®æºï¼Œæ—¨åœ¨è§£å†³å…¶ä¸­å¸¸è§çš„æ‹¼å†™é”™è¯¯ã€å­—æ®µç¼ºå¤±ã€é‡å¤æ¡ç›®å’Œæ—¥æœŸé”™è¯¯ç­‰é—®é¢˜ã€‚ä½œè€…è¯„ä¼°äº†LLMæ™ºèƒ½ä½“åœ¨å¤„ç†å…­ç§ä¸åŒç±»å‹å™ªå£°æ—¶çš„è¡¨ç°ã€‚ç»“æœè¡¨æ˜ï¼ŒLLMåœ¨å¤„ç†é€šç”¨æ¸…æ´—ä»»åŠ¡æ–¹é¢è¡¨ç°æœ‰æ•ˆï¼Œä¸ºæœªæ¥çš„å·¥ä¸šåº”ç”¨æä¾›äº†æœ‰å¸Œæœ›çš„åŸºç¡€ã€‚å°½ç®¡åœ¨å¤„ç†é¢†åŸŸç‰¹å®šé”™è¯¯æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ï¼Œä½†ç ”ç©¶å¼ºè°ƒäº†é€šè¿‡ä¸“ä¸šåŒ–è®­ç»ƒå’Œå¢å¼ºæ™ºèƒ½ä½“èƒ½åŠ›æ¥è¿›ä¸€æ­¥æå‡æ€§èƒ½çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.RO",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05311v1",
      "published_date": "2025-11-07 15:12:49 UTC",
      "updated_date": "2025-11-07 15:12:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:17:09.825216+00:00"
    },
    {
      "arxiv_id": "2511.08622v1",
      "title": "Multi-period Learning for Financial Time Series Forecasting",
      "title_zh": "é¢å‘é‡‘èæ—¶é—´åºåˆ—é¢„æµ‹çš„å¤šå‘¨æœŸå­¦ä¹ ",
      "authors": [
        "Xu Zhang",
        "Zhengang Huang",
        "Yunzhi Wu",
        "Xun Lu",
        "Erpeng Qi",
        "Yunkai Chen",
        "Zhongya Xue",
        "Qitong Wang",
        "Peng Wang",
        "Wei Wang"
      ],
      "abstract": "Time series forecasting is important in finance domain. Financial time series (TS) patterns are influenced by both short-term public opinions and medium-/long-term policy and market trends. Hence, processing multi-period inputs becomes crucial for accurate financial time series forecasting (TSF). However, current TSF models either use only single-period input, or lack customized designs for addressing multi-period characteristics. In this paper, we propose a Multi-period Learning Framework (MLF) to enhance financial TSF performance. MLF considers both TSF's accuracy and efficiency requirements. Specifically, we design three new modules to better integrate the multi-period inputs for improving accuracy: (i) Inter-period Redundancy Filtering (IRF), that removes the information redundancy between periods for accurate self-attention modeling, (ii) Learnable Weighted-average Integration (LWI), that effectively integrates multi-period forecasts, (iii) Multi-period self-Adaptive Patching (MAP), that mitigates the bias towards certain periods by setting the same number of patches across all periods. Furthermore, we propose a Patch Squeeze module to reduce the number of patches in self-attention modeling for maximized efficiency. MLF incorporates multiple inputs with varying lengths (periods) to achieve better accuracy and reduces the costs of selecting input lengths during training. The codes and datasets are available at https://github.com/Meteor-Stars/MLF.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é‡‘èæ—¶é—´åºåˆ—å—çŸ­æœŸèˆ†è®ºå’Œä¸­é•¿æœŸè¶‹åŠ¿å…±åŒå½±å“çš„ç‰¹ç‚¹ï¼Œæå‡ºäº†ä¸€ç§å¤šå‘¨æœŸå­¦ä¹ æ¡†æ¶(Multi-period Learning Framework, MLF)ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹åœ¨å¤„ç†å¤šå‘¨æœŸè¾“å…¥æ—¶çš„å±€é™æ€§ã€‚MLFé€šè¿‡å¼•å…¥å‘¨æœŸé—®å†—ä½™è¿‡æ»¤(Inter-period Redundancy Filtering, IRF)ã€å¯å­¦ä¹ åŠ æƒå¹³å‡é›†æˆ(Learnable Weighted-average Integration, LWI)ä»¥åŠå¤šå‘¨æœŸè‡ªé€‚åº”åˆ†å—(Multi-period self-Adaptive Patching, MAP)ä¸‰ä¸ªæ–°æ¨¡å—ï¼Œæœ‰æ•ˆæ¶ˆé™¤äº†ä¿¡æ¯å†—ä½™ã€æ•´åˆäº†å¤šå‘¨æœŸé¢„æµ‹å¹¶å‡è½»äº†å‘¨æœŸåå·®ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜åˆ©ç”¨Patch Squeezeæ¨¡å—å‡å°‘è‡ªæ³¨æ„åŠ›å»ºæ¨¡ä¸­çš„åˆ†å—æ•°é‡ï¼Œä»è€Œæœ€å¤§åŒ–è®¡ç®—æ•ˆç‡ã€‚å®éªŒè¡¨æ˜ï¼ŒMLFèƒ½å¤Ÿæœ‰æ•ˆç»“åˆä¸åŒé•¿åº¦çš„è¾“å…¥ï¼Œåœ¨æå‡é‡‘èæ—¶é—´åºåˆ—é¢„æµ‹(TSF)å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œé™ä½äº†è®­ç»ƒä¸­é€‰æ‹©è¾“å…¥é•¿åº¦çš„æˆæœ¬ã€‚",
      "categories": [
        "q-fin.ST",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-fin.ST",
      "comment": "The codes are available at https://github.com/Meteor-Stars/MLF",
      "pdf_url": "https://arxiv.org/pdf/2511.08622v1",
      "published_date": "2025-11-07 15:11:46 UTC",
      "updated_date": "2025-11-07 15:11:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:17:36.028509+00:00"
    },
    {
      "arxiv_id": "2511.05308v2",
      "title": "Rethinking Metrics and Diffusion Architecture for 3D Point Cloud Generation",
      "title_zh": "é‡æ–°æ€è€ƒä¸‰ç»´ç‚¹äº‘ç”Ÿæˆçš„è¯„ä¼°æŒ‡æ ‡ä¸æ‰©æ•£æ¶æ„",
      "authors": [
        "Matteo Bastico",
        "David Ryckelynck",
        "Laurent CortÃ©",
        "Yannick Tillier",
        "Etienne DecenciÃ¨re"
      ],
      "abstract": "As 3D point clouds become a cornerstone of modern technology, the need for sophisticated generative models and reliable evaluation metrics has grown exponentially. In this work, we first expose that some commonly used metrics for evaluating generated point clouds, particularly those based on Chamfer Distance (CD), lack robustness against defects and fail to capture geometric fidelity and local shape consistency when used as quality indicators. We further show that introducing samples alignment prior to distance calculation and replacing CD with Density-Aware Chamfer Distance (DCD) are simple yet essential steps to ensure the consistency and robustness of point cloud generative model evaluation metrics. While existing metrics primarily focus on directly comparing 3D Euclidean coordinates, we present a novel metric, named Surface Normal Concordance (SNC), which approximates surface similarity by comparing estimated point normals. This new metric, when combined with traditional ones, provides a more comprehensive evaluation of the quality of generated samples. Finally, leveraging recent advancements in transformer-based models for point cloud analysis, such as serialized patch attention , we propose a new architecture for generating high-fidelity 3D structures, the Diffusion Point Transformer. We perform extensive experiments and comparisons on the ShapeNet dataset, showing that our model outperforms previous solutions, particularly in terms of quality of generated point clouds, achieving new state-of-the-art. Code available at https://github.com/matteo-bastico/DiffusionPointTransformer.",
      "tldr_zh": "è¿™é¡¹å·¥ä½œé¦–å…ˆæ­ç¤ºäº†ç”¨äºè¯„ä¼°ç”Ÿæˆç‚¹äº‘çš„å¸¸ç”¨æŒ‡æ ‡ï¼Œç‰¹åˆ«æ˜¯åŸºäºChamfer Distance (CD)çš„æŒ‡æ ‡ï¼Œç¼ºä¹å¯¹ç¼ºé™·çš„é²æ£’æ€§ï¼Œä¸”æ— æ³•æ•æ‰å‡ ä½•ä¿çœŸåº¦å’Œå±€éƒ¨å½¢çŠ¶ä¸€è‡´æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåœ¨è·ç¦»è®¡ç®—å‰å¼•å…¥æ ·æœ¬å¯¹é½å¹¶å°†CDæ›¿æ¢ä¸ºDensity-Aware Chamfer Distance (DCD)æ˜¯ç¡®ä¿è¯„ä¼°æŒ‡æ ‡ä¸€è‡´æ€§å’Œé²æ£’æ€§çš„å…³é”®æ­¥éª¤ã€‚æ­¤å¤–ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åä¸ºSurface Normal Concordance (SNC)çš„æ–°æŒ‡æ ‡ï¼Œé€šè¿‡æ¯”è¾ƒä¼°è®¡çš„ç‚¹æ³•çº¿æ¥è¿‘ä¼¼è¡¨é¢ç›¸ä¼¼åº¦ï¼Œä»è€Œæä¾›æ›´å…¨é¢çš„è´¨é‡è¯„ä¼°ã€‚åˆ©ç”¨åŸºäºTransformerçš„ç‚¹äº‘åˆ†ææ¨¡å‹çš„æœ€æ–°è¿›å±•ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„ç”Ÿæˆæ¶æ„â€”â€”Diffusion Point Transformerï¼Œé‡‡ç”¨äº†serialized patch attentionæœºåˆ¶ä»¥ç”Ÿæˆé«˜ä¿çœŸ3Dç»“æ„ã€‚åœ¨ShapeNetæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ç”Ÿæˆç‚¹äº‘çš„è´¨é‡æ–¹é¢ä¼˜äºå…ˆå‰çš„è§£å†³æ–¹æ¡ˆï¼Œè¾¾åˆ°äº†æ–°çš„State-of-the-artæ°´å¹³ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "This paper has been accepted at International Conference on 3D Vision (3DV) 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.05308v2",
      "published_date": "2025-11-07 15:07:24 UTC",
      "updated_date": "2025-11-10 08:55:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:17:58.195271+00:00"
    },
    {
      "arxiv_id": "2511.05299v1",
      "title": "LiveStar: Live Streaming Assistant for Real-World Online Video Understanding",
      "title_zh": "LiveStarï¼šé¢å‘çœŸå®åœºæ™¯åœ¨çº¿è§†é¢‘ç†è§£çš„ç›´æ’­åŠ©æ‰‹",
      "authors": [
        "Zhenyu Yang",
        "Kairui Zhang",
        "Yuhang Hu",
        "Bing Wang",
        "Shengsheng Qian",
        "Bin Wen",
        "Fan Yang",
        "Tingting Gao",
        "Weiming Dong",
        "Changsheng Xu"
      ],
      "abstract": "Despite significant progress in Video Large Language Models (Video-LLMs) for offline video understanding, existing online Video-LLMs typically struggle to simultaneously process continuous frame-by-frame inputs and determine optimal response timing, often compromising real-time responsiveness and narrative coherence. To address these limitations, we introduce LiveStar, a pioneering live streaming assistant that achieves always-on proactive responses through adaptive streaming decoding. Specifically, LiveStar incorporates: (1) a training strategy enabling incremental video-language alignment for variable-length video streams, preserving temporal consistency across dynamically evolving frame sequences; (2) a response-silence decoding framework that determines optimal proactive response timing via a single forward pass verification; (3) memory-aware acceleration via peak-end memory compression for online inference on 10+ minute videos, combined with streaming key-value cache to achieve 1.53x faster inference. We also construct an OmniStar dataset, a comprehensive dataset for training and benchmarking that encompasses 15 diverse real-world scenarios and 5 evaluation tasks for online video understanding. Extensive experiments across three benchmarks demonstrate LiveStar's state-of-the-art performance, achieving an average 19.5% improvement in semantic correctness with 18.1% reduced timing difference compared to existing online Video-LLMs, while improving FPS by 12.0% across all five OmniStar tasks. Our model and dataset can be accessed at https://github.com/yzy-bupt/LiveStar.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LiveStarï¼Œä¸€ç§é’ˆå¯¹çœŸå®ä¸–ç•Œåœ¨çº¿è§†é¢‘ç†è§£çš„ç›´æ’­åŠ©æ‰‹ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åœ¨çº¿è§†é¢‘å¤§è¯­è¨€æ¨¡å‹(Video-LLMs)åœ¨åŒæ—¶å¤„ç†è¿ç»­å¸§è¾“å…¥å’Œç¡®å®šæœ€ä½³å“åº”æ—¶æœºæ–¹é¢çš„å±€é™æ€§ã€‚LiveStaré€šè¿‡è‡ªé€‚åº”æµå¼è§£ç å®ç°å…¨å¤©å€™ä¸»åŠ¨å“åº”ï¼Œå…¶æ ¸å¿ƒåŒ…å«æ”¯æŒå¢é‡è§†é¢‘-è¯­è¨€å¯¹é½çš„è®­ç»ƒç­–ç•¥ä»¥ä¿æŒæ—¶é—´ä¸€è‡´æ€§ï¼Œä»¥åŠä¸€ç§response-silenceè§£ç æ¡†æ¶ï¼Œå¯é€šè¿‡å•æ¬¡å‰å‘ä¼ é€’éªŒè¯ç¡®å®šæœ€ä½³å“åº”æ—¶æœºã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹ç»“åˆå³°å€¼-æœ«ç«¯è®°å¿†å‹ç¼©(peak-end memory compression)å’Œæµå¼é”®å€¼ç¼“å­˜å®ç°äº†è®°å¿†æ„ŸçŸ¥åŠ é€Ÿï¼Œæ”¯æŒé•¿è¾¾10åˆ†é’Ÿä»¥ä¸Šçš„è§†é¢‘åœ¨çº¿æ¨ç†å¹¶å°†é€Ÿåº¦æå‡1.53å€ã€‚ç ”ç©¶å›¢é˜Ÿè¿˜æ„å»ºäº†åŒ…å«15ç§çœŸå®åœºæ™¯å’Œ5é¡¹ä»»åŠ¡çš„OmniStaræ•°æ®é›†ç”¨äºè®­ç»ƒå’Œè¯„ä¼°ã€‚å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼ŒLiveStaråœ¨ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡è¾¾åˆ°äº†SOTAæ€§èƒ½ï¼Œç›¸æ¯”ç°æœ‰æ¨¡å‹è¯­ä¹‰æ­£ç¡®ç‡æå‡19.5%ï¼Œæ—¶é—´å·®å¼‚å‡å°‘18.1%ï¼ŒåŒæ—¶FPSæå‡äº†12.0%ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "NeurIPS 2025 Accepted",
      "pdf_url": "https://arxiv.org/pdf/2511.05299v1",
      "published_date": "2025-11-07 15:00:37 UTC",
      "updated_date": "2025-11-07 15:00:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:18:24.491083+00:00"
    },
    {
      "arxiv_id": "2511.05271v2",
      "title": "DeepEyesV2: Toward Agentic Multimodal Model",
      "title_zh": "DeepEyesV2ï¼šè¿ˆå‘æ™ºèƒ½ä½“å¤šæ¨¡æ€æ¨¡å‹",
      "authors": [
        "Jack Hong",
        "Chenxiao Zhao",
        "ChengLin Zhu",
        "Weiheng Lu",
        "Guohai Xu",
        "Xing Yu"
      ],
      "abstract": "Agentic multimodal models should not only comprehend text and images, but also actively invoke external tools, such as code execution environments and web search, and integrate these operations into reasoning. In this work, we introduce DeepEyesV2 and explore how to build an agentic multimodal model from the perspectives of data construction, training methods, and model evaluation. We observe that direct reinforcement learning alone fails to induce robust tool-use behavior. This phenomenon motivates a two-stage training pipeline: a cold-start stage to establish tool-use patterns, and reinforcement learning stage to further refine tool invocation. We curate a diverse, moderately challenging training dataset, specifically including examples where tool use is beneficial. We further introduce RealX-Bench, a comprehensive benchmark designed to evaluate real-world multimodal reasoning, which inherently requires the integration of multiple capabilities, including perception, search, and reasoning. We evaluate DeepEyesV2 on RealX-Bench and other representative benchmarks, demonstrating its effectiveness across real-world understanding, mathematical reasoning, and search-intensive tasks. Moreover, DeepEyesV2 exhibits task-adaptive tool invocation, tending to use image operations for perception tasks and numerical computations for reasoning tasks. Reinforcement learning further enables complex tool combinations and allows model to selectively invoke tools based on context. We hope our study can provide guidance for community in developing agentic multimodal models.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†DeepEyesV2ï¼Œæ—¨åœ¨æ„å»ºèƒ½å¤Ÿä¸»åŠ¨è°ƒç”¨å¤–éƒ¨å·¥å…·ï¼ˆå¦‚ä»£ç æ‰§è¡Œç¯å¢ƒå’Œç½‘ç»œæœç´¢ï¼‰å¹¶å°†å…¶æ•´åˆåˆ°æ¨ç†è¿‡ç¨‹ä¸­çš„ä»£ç†å¤šæ¨¡æ€æ¨¡å‹(Agentic Multimodal Model)ã€‚é’ˆå¯¹ç›´æ¥å¼ºåŒ–å­¦ä¹ æ— æ³•è¯±å¯¼é²æ£’å·¥å…·ä½¿ç”¨è¡Œä¸ºçš„é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µè®­ç»ƒæµç¨‹ï¼šé¦–å…ˆé€šè¿‡å†·å¯åŠ¨é˜¶æ®µå»ºç«‹å·¥å…·ä½¿ç”¨æ¨¡å¼ï¼Œéšååˆ©ç”¨å¼ºåŒ–å­¦ä¹ é˜¶æ®µè¿›ä¸€æ­¥ä¼˜åŒ–å·¥å…·è°ƒç”¨ã€‚ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†åŒ…å«æœ‰ç›Šå·¥å…·ä½¿ç”¨ç¤ºä¾‹çš„å¤šæ ·åŒ–è®­ç»ƒæ•°æ®é›†ï¼Œå¹¶å¼•å…¥äº†RealX-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°ç»“åˆæ„ŸçŸ¥ã€æœç´¢å’Œæ¨ç†èƒ½åŠ›çš„ç°å®ä¸–ç•Œå¤šæ¨¡æ€æ¨ç†çš„ç»¼åˆåŸºå‡†ã€‚å®éªŒè¡¨æ˜ï¼ŒDeepEyesV2åœ¨RealX-BenchåŠå…¶ä»–ä»£è¡¨æ€§åŸºå‡†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæœ‰æ•ˆå¤„ç†äº†ç°å®ä¸–ç•Œç†è§£ã€æ•°å­¦æ¨ç†å’Œæœç´¢å¯†é›†å‹ä»»åŠ¡ã€‚è¯¥æ¨¡å‹å±•ç¤ºäº†ä»»åŠ¡è‡ªé€‚åº”çš„å·¥å…·è°ƒç”¨èƒ½åŠ›ï¼Œä¾‹å¦‚åœ¨æ„ŸçŸ¥ä»»åŠ¡ä¸­ä½¿ç”¨å›¾åƒæ“ä½œï¼Œåœ¨æ¨ç†ä»»åŠ¡ä¸­ä½¿ç”¨æ•°å€¼è®¡ç®—ã€‚æ­¤å¤–ï¼Œå¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥å®ç°äº†å¤æ‚çš„å·¥å…·ç»„åˆï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ ¹æ®ä¸Šä¸‹æ–‡é€‰æ‹©æ€§åœ°è°ƒç”¨å·¥å…·ï¼Œä¸ºå¼€å‘ä»£ç†å¤šæ¨¡æ€æ¨¡å‹æä¾›äº†é‡è¦æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Homepage: https://visual-agent.github.io/",
      "pdf_url": "https://arxiv.org/pdf/2511.05271v2",
      "published_date": "2025-11-07 14:31:20 UTC",
      "updated_date": "2025-11-10 15:43:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:21:37.125744+00:00"
    },
    {
      "arxiv_id": "2511.05269v1",
      "title": "TAMAS: Benchmarking Adversarial Risks in Multi-Agent LLM Systems",
      "title_zh": "TAMASï¼šå¤šæ™ºèƒ½ä½“å¤§è¯­è¨€æ¨¡å‹ç³»ç»Ÿå¯¹æŠ—æ€§é£é™©åŸºå‡†æµ‹è¯•",
      "authors": [
        "Ishan Kavathekar",
        "Hemang Jain",
        "Ameya Rathod",
        "Ponnurangam Kumaraguru",
        "Tanuja Ganu"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated strong capabilities as autonomous agents through tool use, planning, and decision-making abilities, leading to their widespread adoption across diverse tasks. As task complexity grows, multi-agent LLM systems are increasingly used to solve problems collaboratively. However, safety and security of these systems remains largely under-explored. Existing benchmarks and datasets predominantly focus on single-agent settings, failing to capture the unique vulnerabilities of multi-agent dynamics and co-ordination. To address this gap, we introduce $\\textbf{T}$hreats and $\\textbf{A}$ttacks in $\\textbf{M}$ulti-$\\textbf{A}$gent $\\textbf{S}$ystems ($\\textbf{TAMAS}$), a benchmark designed to evaluate the robustness and safety of multi-agent LLM systems. TAMAS includes five distinct scenarios comprising 300 adversarial instances across six attack types and 211 tools, along with 100 harmless tasks. We assess system performance across ten backbone LLMs and three agent interaction configurations from Autogen and CrewAI frameworks, highlighting critical challenges and failure modes in current multi-agent deployments. Furthermore, we introduce Effective Robustness Score (ERS) to assess the tradeoff between safety and task effectiveness of these frameworks. Our findings show that multi-agent systems are highly vulnerable to adversarial attacks, underscoring the urgent need for stronger defenses. TAMAS provides a foundation for systematically studying and improving the safety of multi-agent LLM systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ™ºèƒ½ä½“å¤§è¯­è¨€æ¨¡å‹(LLM)ç³»ç»Ÿåœ¨å®‰å…¨æ€§å’Œç¨³å¥æ€§æ–¹é¢ç ”ç©¶ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºTAMASçš„åŸºå‡†æµ‹è¯•æ¡†æ¶ã€‚TAMASæ—¨åœ¨è¯„ä¼°å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„å¯¹æŠ—é£é™©ï¼ŒåŒ…å«äº”ä¸ªä¸åŒåœºæ™¯ã€å…­ç§æ”»å‡»ç±»å‹ä¸‹çš„300ä¸ªå¯¹æŠ—å®ä¾‹ä»¥åŠ211ä¸ªå·¥å…·ä½¿ç”¨æ¡ˆä¾‹ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨Autogenå’ŒCrewAIæ¡†æ¶ä¸­çš„ä¸‰ç§äº¤äº’é…ç½®ï¼Œå¯¹åç§éª¨å¹²LLMè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œå¹¶æå‡ºäº†Effective Robustness Score (ERS)æŒ‡æ ‡æ¥è¡¡é‡å®‰å…¨æ€§ä¸ä»»åŠ¡æœ‰æ•ˆæ€§ä¹‹é—´çš„æƒè¡¡ã€‚å®éªŒç»“æœæ­ç¤ºäº†å½“å‰å¤šæ™ºèƒ½ä½“éƒ¨ç½²ä¸­çš„å…³é”®æŒ‘æˆ˜ï¼Œè¡¨æ˜è¿™äº›ç³»ç»Ÿææ˜“å—åˆ°å¯¹æŠ—æ€§æ”»å‡»çš„å½±å“ã€‚è¯¥å·¥ä½œå¼ºè°ƒäº†å¼€å‘æ›´å¼ºé˜²å¾¡æœºåˆ¶çš„ç´§è¿«æ€§ï¼Œå¹¶ä¸ºç³»ç»Ÿåœ°ç ”ç©¶å’Œæ”¹è¿›å¤šæ™ºèƒ½ä½“LLMç³»ç»Ÿçš„å®‰å…¨æ€§å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "Accepted at ICML 2025 MAS Workshop. This version includes additional experiments and analysis",
      "pdf_url": "https://arxiv.org/pdf/2511.05269v1",
      "published_date": "2025-11-07 14:30:26 UTC",
      "updated_date": "2025-11-07 14:30:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:22:01.356476+00:00"
    },
    {
      "arxiv_id": "2511.05266v1",
      "title": "Integrating Score-Based Diffusion Models with Machine Learning-Enhanced Localization for Advanced Data Assimilation in Geological Carbon Storage",
      "title_zh": "èåˆåŸºäºåˆ†æ•°çš„æ‰©æ•£æ¨¡å‹ä¸æœºå™¨å­¦ä¹ å¢å¼ºå±€åŸŸåŒ–ç”¨äºåœ°è´¨ç¢³å°å­˜çš„å…ˆè¿›æ•°æ®åŒåŒ–",
      "authors": [
        "Gabriel SerrÃ£o Seabra",
        "Nikolaj T. MÃ¼cke",
        "Vinicius Luiz Santos Silva",
        "Alexandre A. Emerick",
        "Denis Voskov",
        "Femke Vossepoel"
      ],
      "abstract": "Accurate characterization of subsurface heterogeneity is important for the safe and effective implementation of geological carbon storage (GCS) projects. This paper explores how machine learning methods can enhance data assimilation for GCS with a framework that integrates score-based diffusion models with machine learning-enhanced localization in channelized reservoirs during CO$_2$ injection. We employ a machine learning-enhanced localization framework that uses large ensembles ($N_s = 5000$) with permeabilities generated by the diffusion model and states computed by simple ML algorithms to improve covariance estimation for the Ensemble Smoother with Multiple Data Assimilation (ESMDA). We apply ML algorithms to a prior ensemble of channelized permeability fields, generated with the geostatistical model FLUVSIM. Our approach is applied on a CO$_2$ injection scenario simulated using the Delft Advanced Research Terra Simulator (DARTS). Our ML-based localization maintains significantly more ensemble variance than when localization is not applied, while achieving comparable data-matching quality. This framework has practical implications for GCS projects, helping improve the reliability of uncertainty quantification for risk assessment.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åœ°è´¨ç¢³å°å­˜(GCS)é¡¹ç›®ä¸­åœ°ä¸‹éå‡è´¨æ€§è¡¨å¾çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§å°†åŸºäºåˆ†æ•°çš„æ‰©æ•£æ¨¡å‹(Score-Based Diffusion Models)ä¸æœºå™¨å­¦ä¹ å¢å¼ºå®šä½ç›¸ç»“åˆçš„æ•°æ®åŒåŒ–æ¡†æ¶ã€‚è¯¥æ–¹æ³•é’ˆå¯¹CO2æ³¨å…¥è¿‡ç¨‹ä¸­çš„æ²³é“åŒ–å‚¨å±‚ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ¸—é€ç‡åœºï¼Œå¹¶ç»“åˆç®€å•æœºå™¨å­¦ä¹ ç®—æ³•è®¡ç®—çŠ¶æ€ï¼Œä»¥æ­¤æ”¹è¿›å¤šæ•°æ®åŒåŒ–é›†åˆå¹³æ»‘å™¨(ESMDA)çš„åæ–¹å·®ä¼°è®¡ã€‚åœ¨Delft Advanced Research Terra Simulator (DARTS)æ¨¡æ‹Ÿçš„CO2æ³¨å…¥åœºæ™¯ä¸­ï¼Œç ”ç©¶å›¢é˜Ÿä½¿ç”¨å¤§å‹é›†åˆ($N_s = 5000$)å¯¹è¯¥æ¡†æ¶è¿›è¡Œäº†éªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœªåº”ç”¨å®šä½çš„æ–¹æ³•ç›¸æ¯”ï¼ŒåŸºäºæœºå™¨å­¦ä¹ çš„å®šä½æŠ€æœ¯åœ¨å®ç°ç›¸å½“çš„æ•°æ®åŒ¹é…è´¨é‡çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæ˜¾è‘—ä¿æŒæ›´å¤šçš„é›†åˆæ–¹å·®ã€‚è¿™ä¸€æ¡†æ¶å¯¹äºGCSé¡¹ç›®å…·æœ‰é‡è¦çš„å®é™…æ„ä¹‰ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæé«˜é£é™©è¯„ä¼°ä¸­ä¸ç¡®å®šæ€§é‡åŒ–çš„å¯é æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Corresponding author: Gabriel SerrÃ£o Seabra",
      "pdf_url": "https://arxiv.org/pdf/2511.05266v1",
      "published_date": "2025-11-07 14:28:55 UTC",
      "updated_date": "2025-11-07 14:28:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:23:10.812157+00:00"
    },
    {
      "arxiv_id": "2511.05265v1",
      "title": "An End-to-End Deep Reinforcement Learning Approach for Solving the Traveling Salesman Problem with Drones",
      "title_zh": "æ±‚è§£å¸¦æ— äººæœºæ—…è¡Œå•†é—®é¢˜çš„ç«¯åˆ°ç«¯æ·±åº¦å¼ºåŒ–å­¦ä¹ æ–¹æ³•",
      "authors": [
        "Taihelong Zeng",
        "Yun Lin",
        "Yuhe Shi",
        "Yan Li",
        "Zhiqing Wei",
        "Xuanru Ji"
      ],
      "abstract": "The emergence of truck-drone collaborative systems in last-mile logistics has positioned the Traveling Salesman Problem with Drones (TSP-D) as a pivotal extension of classical routing optimization, where synchronized vehicle coordination promises substantial operational efficiency and reduced environmental impact, yet introduces NP-hard combinatorial complexity beyond the reach of conventional optimization paradigms. Deep reinforcement learning offers a theoretically grounded framework to address TSP-D's inherent challenges through self-supervised policy learning and adaptive decision-making. This study proposes a hierarchical Actor-Critic deep reinforcement learning framework for solving the TSP-D problem. The architecture consists of two primary components: a Transformer-inspired encoder and an efficient Minimal Gated Unit decoder. The encoder incorporates a novel, optimized k-nearest neighbors sparse attention mechanism specifically for focusing on relevant spatial relationships, further enhanced by the integration of global node features. The Minimal Gated Unit decoder processes these encoded representations to efficiently generate solution sequences. The entire framework operates within an asynchronous advantage actor-critic paradigm. Experimental results show that, on benchmark TSP-D instances of various scales (N=10 to 100), the proposed model can obtain competitive or even superior solutions in shorter average computation times compared to high-performance heuristic algorithms and existing reinforcement learning methods. Moreover, compared to advanced reinforcement learning algorithm benchmarks, the proposed framework significantly reduces the total training time required while achieving superior final performance, highlighting its notable advantage in training efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœ€åä¸€å…¬é‡Œç‰©æµä¸­çš„æ— äººæœºè¾…åŠ©æ—…è¡Œå•†é—®é¢˜(TSP-D)ï¼Œæå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ (Deep Reinforcement Learning)è§£å†³æ–¹æ¡ˆï¼Œæ—¨åœ¨åº”å¯¹å…¶NP-hardç»„åˆå¤æ‚æ€§ã€‚ç ”ç©¶è®¾è®¡äº†ä¸€ä¸ªåˆ†å±‚Actor-Criticæ¡†æ¶ï¼ŒåŒ…å«åŸºäºTransformerçš„ç¼–ç å™¨å’Œé«˜æ•ˆçš„Minimal Gated Unitè§£ç å™¨ã€‚ç¼–ç å™¨å¼•å…¥äº†ä¼˜åŒ–çš„kè¿‘é‚»(k-nearest neighbors)ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶å¹¶èåˆå…¨å±€èŠ‚ç‚¹ç‰¹å¾ï¼Œä»¥ä¾¿æ›´å¥½åœ°æ•æ‰ç©ºé—´å…³ç³»ï¼Œæ•´ä¸ªç³»ç»Ÿåœ¨å¼‚æ­¥ä¼˜åŠ¿Actor-CriticèŒƒå¼ä¸‹è¿è¡Œã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨è§„æ¨¡ä¸º10åˆ°100çš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ¨¡å‹ç›¸æ¯”é«˜æ€§èƒ½å¯å‘å¼ç®—æ³•å’Œç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œèƒ½åœ¨æ›´çŸ­çš„è®¡ç®—æ—¶é—´å†…è·å¾—å…·æœ‰ç«äº‰åŠ›ç”šè‡³æ›´ä¼˜çš„è§£ã€‚æ­¤å¤–ï¼Œä¸å…ˆè¿›çš„å¼ºåŒ–å­¦ä¹ åŸºå‡†ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶æ˜¾è‘—ç¼©çŸ­äº†æ€»è®­ç»ƒæ—¶é—´å¹¶å®ç°äº†æ›´ä¼˜çš„æœ€ç»ˆæ€§èƒ½ï¼Œçªæ˜¾äº†å…¶åœ¨è®­ç»ƒæ•ˆç‡æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05265v1",
      "published_date": "2025-11-07 14:26:29 UTC",
      "updated_date": "2025-11-07 14:26:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:22:47.967182+00:00"
    },
    {
      "arxiv_id": "2511.05263v1",
      "title": "OregairuChar: A Benchmark Dataset for Character Appearance Frequency Analysis in My Teen Romantic Comedy SNAFU",
      "title_zh": "OregairuCharï¼šã€Šæˆ‘çš„é’æ˜¥æ‹çˆ±ç‰©è¯­æœç„¶æœ‰é—®é¢˜ã€‹è§’è‰²å‡ºåœºé¢‘ç‡åˆ†æåŸºå‡†æ•°æ®é›†",
      "authors": [
        "Qi Sun",
        "Dingju Zhou",
        "Lina Zhang"
      ],
      "abstract": "The analysis of character appearance frequency is essential for understanding narrative structure, character prominence, and story progression in anime. In this work, we introduce OregairuChar, a benchmark dataset designed for appearance frequency analysis in the anime series My Teen Romantic Comedy SNAFU. The dataset comprises 1600 manually selected frames from the third season, annotated with 2860 bounding boxes across 11 main characters. OregairuChar captures diverse visual challenges, including occlusion, pose variation, and inter-character similarity, providing a realistic basis for appearance-based studies. To enable quantitative research, we benchmark several object detection models on the dataset and leverage their predictions for fine-grained, episode-level analysis of character presence over time. This approach reveals patterns of character prominence and their evolution within the narrative. By emphasizing appearance frequency, OregairuChar serves as a valuable resource for exploring computational narrative dynamics and character-centric storytelling in stylized media.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº†OregairuCharï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºåˆ†æåŠ¨ç”»ã€Šæˆ‘çš„é’æ˜¥æ‹çˆ±ç‰©è¯­æœç„¶æœ‰é—®é¢˜ã€‹ä¸­è§’è‰²å‡ºåœºé¢‘ç‡çš„åŸºå‡†æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«æ¥è‡ªç¬¬ä¸‰å­£çš„1600ä¸ªæ‰‹åŠ¨ç²¾é€‰å¸§ï¼Œå¹¶é’ˆå¯¹11ä¸ªä¸»è¦è§’è‰²æ ‡æ³¨äº†2860ä¸ªè¾¹ç•Œæ¡†(bounding boxes)ã€‚OregairuCharæ¶µç›–äº†é®æŒ¡ã€å§¿æ€å˜åŒ–å’Œè§’è‰²é—´ç›¸ä¼¼æ€§ç­‰å¤šç§è§†è§‰æŒ‘æˆ˜ï¼Œä¸ºåŸºäºå¤–è§‚çš„ç ”ç©¶æä¾›äº†ç°å®åŸºç¡€ã€‚ä¸ºäº†å®ç°å®šé‡ç ”ç©¶ï¼Œä½œè€…åœ¨è¯¥æ•°æ®é›†ä¸Šå¯¹å¤šä¸ªç›®æ ‡æ£€æµ‹(object detection)æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå¹¶åˆ©ç”¨é¢„æµ‹ç»“æœå¯¹è§’è‰²éšæ—¶é—´çš„å­˜åœ¨æƒ…å†µè¿›è¡Œäº†ç»†ç²’åº¦çš„å‰§é›†çº§åˆ†æã€‚è¿™ç§æ–¹æ³•æˆåŠŸæ­ç¤ºäº†è§’è‰²çªå‡ºç¨‹åº¦çš„æ¨¡å¼åŠå…¶åœ¨å™äº‹ä¸­çš„æ¼”å˜è¿‡ç¨‹ã€‚é€šè¿‡å¼ºè°ƒå‡ºåœºé¢‘ç‡ï¼ŒOregairuCharä¸ºæ¢ç´¢é£æ ¼åŒ–åª’ä½“ä¸­çš„è®¡ç®—å™äº‹åŠ¨åŠ›å­¦å’Œä»¥è§’è‰²ä¸ºä¸­å¿ƒçš„å™äº‹æ–¹å¼æä¾›äº†å®è´µçš„èµ„æºã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05263v1",
      "published_date": "2025-11-07 14:25:58 UTC",
      "updated_date": "2025-11-07 14:25:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:23:20.990671+00:00"
    },
    {
      "arxiv_id": "2511.11628v1",
      "title": "Mixture-of-Schedulers: An Adaptive Scheduling Agent as a Learned Router for Expert Policies",
      "title_zh": "æ··åˆè°ƒåº¦å™¨ï¼šä½œä¸ºä¸“å®¶ç­–ç•¥å­¦ä¹ å‹è·¯ç”±å™¨çš„è‡ªé€‚åº”è°ƒåº¦æ™ºèƒ½ä½“",
      "authors": [
        "Xinbo Wang",
        "Shian Jia",
        "Ziyang Huang",
        "Jing Cao",
        "Mingli Song"
      ],
      "abstract": "Modern operating system schedulers employ a single, static policy, which struggles to deliver optimal performance across the diverse and dynamic workloads of contemporary systems. This \"one-policy-fits-all\" approach leads to significant compromises in fairness, throughput, and latency, particularly with the rise of heterogeneous hardware and varied application architectures.\n  This paper proposes a new paradigm: dynamically selecting the optimal policy from a portfolio of specialized schedulers rather than designing a single, monolithic one. We present the Adaptive Scheduling Agent (ASA), a lightweight framework that intelligently matches workloads to the most suitable \"expert\" scheduling policy at runtime. ASA's core is a novel, low-overhead offline/online approach. First, an offline process trains a universal, hardware-agnostic machine learning model to recognize abstract workload patterns from system behaviors. Second, at runtime, ASA continually processes the model's predictions using a time-weighted probability voting algorithm to identify the workload, then makes a scheduling decision by consulting a pre-configured, machine-specific mapping table to switch to the optimal scheduler via Linux's sched_ext framework. This decoupled architecture allows ASA to adapt to new hardware platforms rapidly without expensive retraining of the core recognition model.\n  Our evaluation, based on a novel benchmark focused on user-experience metrics, demonstrates that ASA consistently outperforms the default Linux scheduler (EEVDF), achieving superior results in 86.4% of test scenarios. Furthermore, ASA's selections are near-optimal, ranking among the top three schedulers in 78.6% of all scenarios. This validates our approach as a practical path toward more intelligent, adaptive, and responsive operating system schedulers.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°ä»£æ“ä½œç³»ç»Ÿè°ƒåº¦å™¨é‡‡ç”¨å•ä¸€é™æ€ç­–ç•¥éš¾ä»¥åº”å¯¹å¤šæ ·åŒ–åŠ¨æ€å·¥ä½œè´Ÿè½½çš„é—®é¢˜ï¼Œæå‡ºäº†Mixture-of-SchedulersèŒƒå¼åŠAdaptive Scheduling Agent (ASA)æ¡†æ¶ã€‚ASAä½œä¸ºä¸€ä¸ªè½»é‡çº§æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨è¿è¡Œæ—¶æ™ºèƒ½åœ°å°†å·¥ä½œè´Ÿè½½åŒ¹é…åˆ°æœ€åˆé€‚çš„â€œä¸“å®¶â€è°ƒåº¦ç­–ç•¥ï¼Œè€Œéä¾èµ–å•ä¸€çš„å•ä½“è°ƒåº¦å™¨ã€‚è¯¥æ–¹æ³•é‡‡ç”¨ç¦»çº¿/åœ¨çº¿ç»“åˆçš„æ–¹å¼ï¼šé¦–å…ˆç¦»çº¿è®­ç»ƒä¸€ä¸ªé€šç”¨çš„ã€ç¡¬ä»¶æ— å…³çš„æœºå™¨å­¦ä¹ æ¨¡å‹æ¥è¯†åˆ«æŠ½è±¡å·¥ä½œè´Ÿè½½æ¨¡å¼ï¼Œç„¶ååœ¨è¿è¡Œæ—¶é€šè¿‡æ—¶é—´åŠ æƒæ¦‚ç‡æŠ•ç¥¨ç®—æ³•å¤„ç†é¢„æµ‹ç»“æœï¼Œå¹¶åˆ©ç”¨Linuxçš„`sched_ext`æ¡†æ¶åˆ‡æ¢è‡³æœ€ä½³è°ƒåº¦å™¨ã€‚è¿™ç§è§£è€¦æ¶æ„ä½¿å¾—ASAæ— éœ€æ˜‚è´µçš„é‡æ–°è®­ç»ƒå³å¯å¿«é€Ÿé€‚åº”æ–°çš„ç¡¬ä»¶å¹³å°ã€‚åŸºäºç”¨æˆ·ä½“éªŒæŒ‡æ ‡çš„åŸºå‡†æµ‹è¯•è¡¨æ˜ï¼ŒASAåœ¨86.4%çš„æµ‹è¯•åœºæ™¯ä¸­ä¼˜äºé»˜è®¤çš„Linuxè°ƒåº¦å™¨(EEVDF)ï¼Œä¸”åœ¨78.6%çš„åœºæ™¯ä¸­åšå‡ºçš„é€‰æ‹©æ’åå‰ä¸‰ï¼ŒéªŒè¯äº†å…¶ä½œä¸ºæ™ºèƒ½è‡ªé€‚åº”è°ƒåº¦æ–¹æ¡ˆçš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11628v1",
      "published_date": "2025-11-07 14:16:31 UTC",
      "updated_date": "2025-11-07 14:16:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:24:08.739216+00:00"
    },
    {
      "arxiv_id": "2511.05254v1",
      "title": "A Gate-Based Quantum Genetic Algorithm for Real-Valued Global Optimization",
      "title_zh": "é¢å‘å®å€¼å…¨å±€ä¼˜åŒ–çš„åŸºäºé—¨çš„é‡å­é—ä¼ ç®—æ³•",
      "authors": [
        "Leandro C. Souza",
        "Laurent E. Dardenne",
        "Renato Portugal"
      ],
      "abstract": "We propose a gate-based Quantum Genetic Algorithm (QGA) for real-valued global optimization. In this model, individuals are represented by quantum circuits whose measurement outcomes are decoded into real-valued vectors through binary discretization. Evolutionary operators act directly on circuit structures, allowing mutation and crossover to explore the space of gate-based encodings. Both fixed-depth and variable-depth variants are introduced, enabling either uniform circuit complexity or adaptive structural evolution. Fitness is evaluated through quantum sampling, using the mean decoded output of measurement outcomes as the argument of the objective function. To isolate the impact of quantum resources, we compare gate sets with and without the Hadamard gate, showing that superposition consistently improves convergence and robustness across benchmark functions such as the Rastrigin function. Furthermore, we demonstrate that introducing pairwise inter-individual entanglement in the population accelerates early convergence, revealing that quantum correlations among individuals provide an additional optimization advantage. Together, these results show that both superposition and entanglement enhance the search dynamics of evolutionary quantum algorithms, establishing gate-based QGAs as a promising framework for quantum-enhanced global optimization.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºå®å€¼å…¨å±€ä¼˜åŒ–çš„åŸºäºé—¨çš„é‡å­é—ä¼ ç®—æ³•(Quantum Genetic Algorithm, QGA)ã€‚åœ¨æ­¤æ¨¡å‹ä¸­ï¼Œä¸ªä½“è¢«è¡¨ç¤ºä¸ºé‡å­ç”µè·¯ï¼Œå…¶æµ‹é‡ç»“æœé€šè¿‡äºŒè¿›åˆ¶ç¦»æ•£åŒ–è§£ç ä¸ºå®å€¼å‘é‡ã€‚è¿›åŒ–ç®—å­ç›´æ¥ä½œç”¨äºç”µè·¯ç»“æ„ï¼Œå¹¶å¼•å…¥äº†å›ºå®šæ·±åº¦å’Œå¯å˜æ·±åº¦ä¸¤ç§å˜ä½“ï¼Œä»¥å®ç°ç»Ÿä¸€çš„ç”µè·¯å¤æ‚åº¦å’Œè‡ªé€‚åº”ç»“æ„è¿›åŒ–ã€‚é€‚åº”åº¦é€šè¿‡é‡å­é‡‡æ ·è¯„ä¼°ï¼Œåˆ©ç”¨æµ‹é‡ç»“æœçš„å¹³å‡è§£ç è¾“å‡ºä½œä¸ºç›®æ ‡å‡½æ•°çš„å‚æ•°ã€‚ä¸ºäº†åˆ†ç¦»é‡å­èµ„æºçš„å½±å“ï¼Œç ”ç©¶å¯¹æ¯”äº†åŒ…å«ä¸ä¸åŒ…å«Hadamardé—¨çš„é—¨é›†ï¼Œç»“æœæ˜¾ç¤ºå åŠ æ€(superposition)åœ¨Rastriginå‡½æ•°ç­‰åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æ”¶æ•›æ€§å’Œé²æ£’æ€§ã€‚æ­¤å¤–ï¼Œå¼•å…¥æˆå¯¹çš„ä¸ªä½“é—´çº ç¼ (entanglement)åŠ é€Ÿäº†æ—©æœŸæ”¶æ•›ï¼Œè¡¨æ˜ä¸ªä½“é—´çš„é‡å­ç›¸å…³æ€§æä¾›äº†é¢å¤–çš„ä¼˜åŒ–ä¼˜åŠ¿ã€‚æ€»ä½“è€Œè¨€ï¼Œç ”ç©¶è¯æ˜äº†å åŠ æ€å’Œçº ç¼ å‡èƒ½å¢å¼ºè¿›åŒ–é‡å­ç®—æ³•çš„æœç´¢åŠ¨åŠ›å­¦ï¼Œç¡®ç«‹äº†åŸºäºé—¨çš„QGAä½œä¸ºé‡å­å¢å¼ºå…¨å±€ä¼˜åŒ–çš„æœ‰æ•ˆæ¡†æ¶ã€‚",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "quant-ph",
      "comment": "16 pages",
      "pdf_url": "https://arxiv.org/pdf/2511.05254v1",
      "published_date": "2025-11-07 14:14:53 UTC",
      "updated_date": "2025-11-07 14:14:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:24:02.641594+00:00"
    },
    {
      "arxiv_id": "2511.05250v1",
      "title": "Accurate online action and gesture recognition system using detectors and Deep SPD Siamese Networks",
      "title_zh": "åŸºäºæ£€æµ‹å™¨ä¸æ·±åº¦ SPD å­ªç”Ÿç½‘ç»œçš„ç²¾å‡†åœ¨çº¿åŠ¨ä½œä¸æ‰‹åŠ¿è¯†åˆ«ç³»ç»Ÿ",
      "authors": [
        "Mohamed Sanim Akremi",
        "Rim Slama",
        "Hedi Tabia"
      ],
      "abstract": "Online continuous motion recognition is a hot topic of research since it is more practical in real life application cases. Recently, Skeleton-based approaches have become increasingly popular, demonstrating the power of using such 3D temporal data. However, most of these works have focused on segment-based recognition and are not suitable for the online scenarios. In this paper, we propose an online recognition system for skeleton sequence streaming composed from two main components: a detector and a classifier, which use a Semi-Positive Definite (SPD) matrix representation and a Siamese network. The powerful statistical representations for the skeletal data given by the SPD matrices and the learning of their semantic similarity by the Siamese network enable the detector to predict time intervals of the motions throughout an unsegmented sequence. In addition, they ensure the classifier capability to recognize the motion in each predicted interval. The proposed detector is flexible and able to identify the kinetic state continuously. We conduct extensive experiments on both hand gesture and body action recognition benchmarks to prove the accuracy of our online recognition system which in most cases outperforms state-of-the-art performances.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰åŸºäºéª¨æ¶çš„æ–¹æ³•å¤šå…³æ³¨åˆ†æ®µè¯†åˆ«è€Œä¸é€‚ç”¨äºåœ¨çº¿åœºæ™¯çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç”±æ£€æµ‹å™¨å’Œåˆ†ç±»å™¨ç»„æˆçš„åœ¨çº¿éª¨æ¶åºåˆ—æµè¯†åˆ«ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨åŠæ­£å®š(SPD)çŸ©é˜µè¡¨ç¤ºéª¨æ¶æ•°æ®çš„ç»Ÿè®¡ç‰¹å¾ï¼Œå¹¶ç»“åˆSiameseç½‘ç»œå­¦ä¹ è¯­ä¹‰ç›¸ä¼¼æ€§ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œæ£€æµ‹å™¨èƒ½å¤Ÿåœ¨æœªåˆ†å‰²çš„åºåˆ—ä¸­å‡†ç¡®é¢„æµ‹åŠ¨ä½œçš„æ—¶é—´é—´éš”ï¼Œè€Œåˆ†ç±»å™¨åˆ™è´Ÿè´£è¯†åˆ«æ¯ä¸ªé¢„æµ‹é—´éš”å†…çš„å…·ä½“åŠ¨ä½œã€‚è¯¥æ£€æµ‹å™¨å…·æœ‰é«˜åº¦çµæ´»æ€§ï¼Œèƒ½å¤Ÿè¿ç»­è¯†åˆ«åŠ¨åŠ›å­¦çŠ¶æ€ã€‚åœ¨æ‰‹åŠ¿å’Œèº«ä½“åŠ¨ä½œè¯†åˆ«åŸºå‡†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿçš„å‡†ç¡®æ€§åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ä¼˜äºå½“å‰çš„æœ€å…ˆè¿›æ°´å¹³(SOTA)ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05250v1",
      "published_date": "2025-11-07 14:09:43 UTC",
      "updated_date": "2025-11-07 14:09:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:24:59.923560+00:00"
    },
    {
      "arxiv_id": "2511.11627v1",
      "title": "SA-EMO: Structure-Aligned Encoder Mixture of Operators for Generalizable Full-waveform Inversion",
      "title_zh": "SA-EMOï¼šé¢å‘å¯æ³›åŒ–å…¨æ³¢å½¢åæ¼”çš„ç»“æ„å¯¹é½ç¼–ç å™¨æ··åˆç®—å­",
      "authors": [
        "Wang Zhenyu",
        "Li Peiyuan",
        "Shi Yongxiang",
        "Wu Ruoyu",
        "Zhang Lei"
      ],
      "abstract": "Full-waveform inversion (FWI) can produce high-resolution subsurface models, yet it remains inherently ill-posed, highly nonlinear, and computationally intensive. Although recent deep learning and numerical acceleration methods have improved speed and scalability, they often rely on single CNN architectures or single neural operators, which struggle to generalize in unknown or complex geological settings and are ineffective at distinguishing diverse geological types. To address these issues, we propose a Structure-Aligned Encoder-Mixture-of-Operators (SA-EMO) architecture for velocity-field inversion under unknown subsurface structures. First, a structure-aligned encoder maps high-dimensional seismic wavefields into a physically consistent latent space, thereby eliminating spatio-temporal mismatch between the waveform and velocity domains, recovering high-frequency components, and enhancing feature generalization. Then, an adaptive routing mechanism selects and fuses multiple neural-operator experts, including spectral, wavelet, multiscale, and local operators, to predict the velocity model. We systematically evaluate our approach on the OpenFWI benchmark and the Marmousi2 dataset. Results show that SA-EMO significantly outperforms traditional CNN or single-operator methods, achieving an average MAE reduction of approximately 58.443% and an improvement in boundary resolution of about 10.308%. Ablation studies further reveal that the structure-aligned encoder, the expert-fusion mechanism, and the routing module each contribute markedly to the performance gains. This work introduces a new paradigm for efficient, scalable, and physically interpretable full-waveform inversion.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…¨æ³¢å½¢åæ¼”(Full-waveform inversion, FWI)åœ¨æœªçŸ¥æˆ–å¤æ‚åœ°è´¨ç¯å¢ƒä¸­æ³›åŒ–èƒ½åŠ›å—é™çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºSA-EMOçš„ç»“æ„å¯¹é½ç¼–ç å™¨æ··åˆç®—å­æ¶æ„ã€‚é¦–å…ˆï¼Œè¯¥æ¶æ„åˆ©ç”¨ç»“æ„å¯¹é½ç¼–ç å™¨(Structure-aligned encoder)å°†é«˜ç»´åœ°éœ‡æ³¢åœºæ˜ å°„åˆ°ç‰©ç†ä¸€è‡´çš„æ½œåœ¨ç©ºé—´ï¼Œä»è€Œæ¶ˆé™¤äº†æ³¢å½¢ä¸é€Ÿåº¦åŸŸä¹‹é—´çš„æ—¶ç©ºä¸åŒ¹é…ï¼Œæ¢å¤é«˜é¢‘åˆ†é‡å¹¶å¢å¼ºäº†ç‰¹å¾æ³›åŒ–èƒ½åŠ›ã€‚éšåï¼Œé€šè¿‡è‡ªé€‚åº”è·¯ç”±æœºåˆ¶é€‰æ‹©å¹¶èåˆåŒ…æ‹¬è°±ã€å°æ³¢ã€å¤šå°ºåº¦å’Œå±€éƒ¨ç®—å­åœ¨å†…çš„å¤šç§ç¥ç»ç®—å­ä¸“å®¶(neural-operator experts)æ¥é¢„æµ‹é€Ÿåº¦æ¨¡å‹ã€‚åœ¨OpenFWIåŸºå‡†å’ŒMarmousi2æ•°æ®é›†ä¸Šçš„ç³»ç»Ÿè¯„ä¼°è¡¨æ˜ï¼ŒSA-EMOçš„æ€§èƒ½æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„CNNæˆ–å•ç®—å­æ–¹æ³•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå…¶å¹³å‡MAEé™ä½äº†çº¦58.443%ï¼Œè¾¹ç•Œåˆ†è¾¨ç‡æå‡äº†çº¦10.308%ã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥è¯å®äº†å„ç»„ä»¶çš„æœ‰æ•ˆæ€§ï¼Œè¯¥å·¥ä½œä¸ºé«˜æ•ˆä¸”å…·æœ‰ç‰©ç†å¯è§£é‡Šæ€§çš„å…¨æ³¢å½¢åæ¼”æä¾›äº†æ–°çš„èŒƒå¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11627v1",
      "published_date": "2025-11-07 14:03:43 UTC",
      "updated_date": "2025-11-07 14:03:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:25:20.235513+00:00"
    },
    {
      "arxiv_id": "2511.05229v1",
      "title": "4D3R: Motion-Aware Neural Reconstruction and Rendering of Dynamic Scenes from Monocular Videos",
      "title_zh": "4D3Rï¼šå•ç›®è§†é¢‘åŠ¨æ€åœºæ™¯çš„è¿åŠ¨æ„ŸçŸ¥ç¥ç»é‡å»ºä¸æ¸²æŸ“",
      "authors": [
        "Mengqi Guo",
        "Bo Xu",
        "Yanyan Li",
        "Gim Hee Lee"
      ],
      "abstract": "Novel view synthesis from monocular videos of dynamic scenes with unknown camera poses remains a fundamental challenge in computer vision and graphics. While recent advances in 3D representations such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have shown promising results for static scenes, they struggle with dynamic content and typically rely on pre-computed camera poses. We present 4D3R, a pose-free dynamic neural rendering framework that decouples static and dynamic components through a two-stage approach. Our method first leverages 3D foundational models for initial pose and geometry estimation, followed by motion-aware refinement. 4D3R introduces two key technical innovations: (1) a motion-aware bundle adjustment (MA-BA) module that combines transformer-based learned priors with SAM2 for robust dynamic object segmentation, enabling more accurate camera pose refinement; and (2) an efficient Motion-Aware Gaussian Splatting (MA-GS) representation that uses control points with a deformation field MLP and linear blend skinning to model dynamic motion, significantly reducing computational cost while maintaining high-quality reconstruction. Extensive experiments on real-world dynamic datasets demonstrate that our approach achieves up to 1.8dB PSNR improvement over state-of-the-art methods, particularly in challenging scenarios with large dynamic objects, while reducing computational requirements by 5x compared to previous dynamic scene representations.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†4D3Rï¼Œä¸€ç§æ— éœ€é¢„å…ˆè®¡ç®—ç›¸æœºä½å§¿çš„åŠ¨æ€ç¥ç»æ¸²æŸ“æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å•ç›®è§†é¢‘ä¸­åŠ¨æ€åœºæ™¯çš„æ–°è§†å›¾åˆæˆé—®é¢˜ã€‚è¯¥æ–¹æ³•é‡‡ç”¨ä¸¤é˜¶æ®µç­–ç•¥è§£è€¦é™æ€å’ŒåŠ¨æ€ç»„ä»¶ï¼Œé¦–å…ˆåˆ©ç”¨3DåŸºç¡€æ¨¡å‹è¿›è¡Œåˆå§‹ä½å§¿å’Œå‡ ä½•ä¼°è®¡ï¼Œéšåè¿›è¡Œè¿åŠ¨æ„ŸçŸ¥ä¼˜åŒ–ã€‚4D3Rå¼•å…¥äº†ä¸¤ä¸ªæ ¸å¿ƒåˆ›æ–°ï¼šä¸€æ˜¯è¿åŠ¨æ„ŸçŸ¥å…‰æŸæ³•å¹³å·®(MA-BA)æ¨¡å—ï¼Œç»“åˆTransformerå…ˆéªŒä¸SAM2è¿›è¡Œé²æ£’çš„åŠ¨æ€ç‰©ä½“åˆ†å‰²ï¼Œä»è€Œå®ç°æ›´ç²¾ç¡®çš„ç›¸æœºä½å§¿ä¼˜åŒ–ï¼›äºŒæ˜¯é«˜æ•ˆçš„è¿åŠ¨æ„ŸçŸ¥Gaussian Splatting (MA-GS)è¡¨ç¤ºï¼Œåˆ©ç”¨å¸¦æœ‰å˜å½¢åœºMLPå’Œçº¿æ€§æ··åˆè’™çš®(Linear Blend Skinning)çš„æ§åˆ¶ç‚¹æ¥å»ºæ¨¡åŠ¨æ€è¿åŠ¨ï¼Œåœ¨ä¿æŒé«˜è´¨é‡é‡å»ºçš„åŒæ—¶æ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨çœŸå®ä¸–ç•ŒåŠ¨æ€æ•°æ®é›†ä¸Šç›¸æ¯”æœ€å…ˆè¿›æ–¹æ³•å®ç°äº†é«˜è¾¾1.8dBçš„PSNRæå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ…å«å¤§å‹åŠ¨æ€ç‰©ä½“çš„æŒ‘æˆ˜æ€§åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒåŒæ—¶å°†è®¡ç®—éœ€æ±‚é™ä½äº†5å€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "17 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.05229v1",
      "published_date": "2025-11-07 13:25:50 UTC",
      "updated_date": "2025-11-07 13:25:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:25:41.184762+00:00"
    },
    {
      "arxiv_id": "2511.05182v1",
      "title": "Autonomous generation of different courses of action in mechanized combat operations",
      "title_zh": "æœºæ¢°åŒ–ä½œæˆ˜ä¸­ä¸åŒè¡ŒåŠ¨æ–¹æ¡ˆçš„è‡ªä¸»ç”Ÿæˆ",
      "authors": [
        "Johan Schubert",
        "Patrik Hansen",
        "Pontus HÃ¶rling",
        "Ronnie Johansson"
      ],
      "abstract": "In this paper, we propose a methodology designed to support decision-making during the execution phase of military ground combat operations, with a focus on one's actions. This methodology generates and evaluates recommendations for various courses of action for a mechanized battalion, commencing with an initial set assessed by their anticipated outcomes. It systematically produces thousands of individual action alternatives, followed by evaluations aimed at identifying alternative courses of action with superior outcomes. These alternatives are appraised in light of the opponent's status and actions, considering unit composition, force ratios, types of offense and defense, and anticipated advance rates. Field manuals evaluate battle outcomes and advancement rates. The processes of generation and evaluation work concurrently, yielding a variety of alternative courses of action. This approach facilitates the management of new course generation based on previously evaluated actions. As the combat unfolds and conditions evolve, revised courses of action are formulated for the decision-maker within a sequential decision-making framework.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ”¯æŒå†›äº‹åœ°é¢æˆ˜æ–—è¡ŒåŠ¨æ‰§è¡Œé˜¶æ®µå†³ç­–çš„æ–¹æ³•ï¼Œä¸“æ³¨äºä¸ºæœºæ¢°åŒ–è¥è‡ªä¸»ç”Ÿæˆå’Œè¯„ä¼°ä¸åŒçš„è¡ŒåŠ¨æ–¹æ¡ˆ(courses of action)ã€‚è¯¥æ–¹æ³•é¦–å…ˆè¯„ä¼°åˆå§‹æ–¹æ¡ˆçš„é¢„æœŸç»“æœï¼Œéšåç³»ç»Ÿåœ°ç”Ÿæˆæ•°åƒä¸ªæ›¿ä»£æ–¹æ¡ˆï¼Œå¹¶ç»“åˆå¯¹æ‰‹çŠ¶æ€ã€éƒ¨é˜Ÿæ„æˆã€å…µåŠ›æ¯”ç‡åŠæ”»é˜²ç±»å‹ç­‰å› ç´ ç­›é€‰å‡ºæ›´ä¼˜æ–¹æ¡ˆã€‚ç ”ç©¶åˆ©ç”¨é‡æˆ˜æ‰‹å†Œæ•°æ®è¯„ä¼°æˆ˜æ–—ç»“æœå’Œæ¨è¿›é€Ÿåº¦ï¼Œé€šè¿‡å¹¶å‘çš„ç”Ÿæˆä¸è¯„ä¼°è¿‡ç¨‹ï¼ŒåŸºäºå·²æœ‰è¯„ä¼°ç»“æœåŠ¨æ€ç®¡ç†æ–°æ–¹æ¡ˆçš„ç”Ÿæˆã€‚éšç€æˆ˜æ–—å±€åŠ¿çš„å±•å¼€å’Œæ¡ä»¶å˜åŒ–ï¼Œè¯¥æ–¹æ³•åœ¨é¡ºåºå†³ç­–æ¡†æ¶å†…ä¸æ–­ä¸ºå†³ç­–è€…åˆ¶å®šä¿®è®¢åçš„è¡ŒåŠ¨æ–¹æ¡ˆï¼Œä»è€Œå®ç°å¯¹åŠ¨æ€æˆ˜åœºç¯å¢ƒçš„æœ‰æ•ˆé€‚åº”ã€‚",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "In Proceedings of the 30th International Command and Control Research & Technology Symposium, Stockholm, Sweden, 3-6 November 2025, paper 009",
      "pdf_url": "https://arxiv.org/pdf/2511.05182v1",
      "published_date": "2025-11-07 12:02:56 UTC",
      "updated_date": "2025-11-07 12:02:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:29:13.219089+00:00"
    },
    {
      "arxiv_id": "2511.05179v2",
      "title": "Evaluating Spatio-Temporal Forecasting Trade-offs Between Graph Neural Networks and Foundation Models",
      "title_zh": "è¯„ä¼°å›¾ç¥ç»ç½‘ç»œä¸åŸºç¡€æ¨¡å‹åœ¨æ—¶ç©ºé¢„æµ‹ä¸­çš„æƒè¡¡",
      "authors": [
        "Ragini Gupta",
        "Naman Raina",
        "Bo Chen",
        "Li Chen",
        "Claudiu Danilov",
        "Josh Eckhardt",
        "Keyshla Bernard",
        "Klara Nahrstedt"
      ],
      "abstract": "Modern IoT deployments for environmental sensing produce high volume spatiotemporal data to support downstream tasks such as forecasting, typically powered by machine learning models. While existing filtering and strategic deployment techniques optimize collected data volume at the edge, they overlook how variations in sampling frequencies and spatial coverage affect downstream model performance. In many forecasting models, incorporating data from additional sensors denoise predictions by providing broader spatial contexts. This interplay between sampling frequency, spatial coverage and different forecasting model architectures remain underexplored. This work presents a systematic study of forecasting models - classical models (VAR), neural networks (GRU, Transformer), spatio-temporal graph neural networks (STGNNs), and time series foundation models (TSFMs: Chronos Moirai, TimesFM) under varying spatial sensor nodes density and sampling intervals using real-world temperature data in a wireless sensor network. Our results show that STGNNs are effective when sensor deployments are sparse and sampling rate is moderate, leveraging spatial correlations via encoded graph structure to compensate for limited coverage. In contrast, TSFMs perform competitively at high frequencies but degrade when spatial coverage from neighboring sensors is reduced. Crucially, the multivariate TSFM Moirai outperforms all models by natively learning cross-sensor dependencies. These findings offer actionable insights for building efficient forecasting pipelines in spatio-temporal systems. All code for model configurations, training, dataset, and logs are open-sourced for reproducibility: https://github.com/UIUC-MONET-Projects/Benchmarking-Spatiotemporal-Forecast-Models",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ—¶ç©ºé¢„æµ‹ä»»åŠ¡ï¼Œç³»ç»Ÿè¯„ä¼°äº†ç»å…¸æ¨¡å‹(VAR)ã€ç¥ç»ç½‘ç»œ(GRU, Transformer)ã€æ—¶ç©ºå›¾ç¥ç»ç½‘ç»œ(STGNNs)ä»¥åŠæ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹(TSFMs: Chronos, Moirai, TimesFM)åœ¨ä¸åŒç©ºé—´ä¼ æ„Ÿå™¨å¯†åº¦å’Œé‡‡æ ·é¢‘ç‡ä¸‹çš„æ€§èƒ½æƒè¡¡ã€‚é€šè¿‡åˆ†æçœŸå®ä¸–ç•Œçš„æ— çº¿ä¼ æ„Ÿå™¨ç½‘ç»œæ¸©åº¦æ•°æ®ï¼Œç ”ç©¶å‘ç°STGNNsåœ¨ä¼ æ„Ÿå™¨éƒ¨ç½²ç¨€ç–ä¸”é‡‡æ ·ç‡é€‚ä¸­çš„æƒ…å†µä¸‹è¡¨ç°å‡ºè‰²ï¼Œä¸»è¦å¾—ç›Šäºå…¶åˆ©ç”¨å›¾ç»“æ„æ•æ‰ç©ºé—´ç›¸å…³æ€§ä»¥è¡¥å¿è¦†ç›–ä¸è¶³çš„èƒ½åŠ›ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒTSFMsåœ¨é«˜é¢‘é‡‡æ ·ä¸‹è¡¨ç°å…·æœ‰ç«äº‰åŠ›ï¼Œä½†åœ¨ç©ºé—´è¦†ç›–å‡å°‘æ—¶æ€§èƒ½ä¼šä¸‹é™ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå¤šå˜é‡TSFM Moiraié€šè¿‡åŸç”Ÿå­¦ä¹ è·¨ä¼ æ„Ÿå™¨ä¾èµ–å…³ç³»ï¼Œåœ¨æ•´ä½“è¡¨ç°ä¸Šä¼˜äºå…¶ä»–æ‰€æœ‰æ¨¡å‹ã€‚è¿™é¡¹å·¥ä½œæ­ç¤ºäº†é‡‡æ ·é¢‘ç‡ã€ç©ºé—´è¦†ç›–ä¸æ¨¡å‹æ¶æ„ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œä¸ºæ„å»ºé«˜æ•ˆçš„æ—¶ç©ºç³»ç»Ÿé¢„æµ‹ç®¡é“æä¾›äº†é‡è¦æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05179v2",
      "published_date": "2025-11-07 11:50:39 UTC",
      "updated_date": "2025-11-30 05:02:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:29:31.779455+00:00"
    },
    {
      "arxiv_id": "2511.05171v2",
      "title": "Model Merging Improves Zero-Shot Generalization in Bioacoustic Foundation Models",
      "title_zh": "æ¨¡å‹èåˆæå‡ç”Ÿç‰©å£°å­¦åŸºç¡€æ¨¡å‹çš„é›¶æ ·æœ¬æ³›åŒ–",
      "authors": [
        "Davide Marincione",
        "Donato Crisostomi",
        "Roberto Dessi",
        "Emanuele RodolÃ ",
        "Emanuele Rossi"
      ],
      "abstract": "Foundation models capable of generalizing across species and tasks represent a promising new frontier in bioacoustics, with NatureLM being one of the most prominent examples. While its domain-specific fine-tuning yields strong performance on bioacoustic benchmarks, we observe that it also introduces trade-offs in instruction-following flexibility. For instance, NatureLM achieves high accuracy when prompted for either the common or scientific name individually, but its accuracy drops significantly when both are requested in a single prompt. We address this by applying a simple model merging strategy that interpolates NatureLM with its base language model, recovering instruction-following capabilities with minimal loss of domain expertise. Finally, we show that the merged model exhibits markedly stronger zero-shot generalization, achieving over a 200% relative improvement and setting a new state-of-the-art in closed-set zero-shot classification of unseen species.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”Ÿç‰©å£°å­¦åŸºç¡€æ¨¡å‹ï¼ˆå¦‚NatureLMï¼‰åœ¨ç‰¹å®šé¢†åŸŸå¾®è°ƒåæŒ‡ä»¤éµå¾ªçµæ´»æ€§ä¸‹é™çš„é—®é¢˜è¿›è¡Œäº†æ¢è®¨ã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶NatureLMåœ¨å•ç‹¬è¯¢é—®é€šç”¨åæˆ–å­¦åæ—¶è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å•æ¬¡æç¤ºä¸­åŒæ—¶è¦æ±‚ä¸¤è€…æ—¶å‡†ç¡®ç‡æ˜¾è‘—ä¸‹é™ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§ç®€å•çš„æ¨¡å‹åˆå¹¶ï¼ˆModel Mergingï¼‰ç­–ç•¥ï¼Œé€šè¿‡å°†NatureLMä¸å…¶åŸºç¡€è¯­è¨€æ¨¡å‹è¿›è¡Œæ’å€¼ï¼Œåœ¨æå°‘æŸå¤±é¢†åŸŸä¸“ä¸šçŸ¥è¯†çš„å‰æä¸‹æ¢å¤äº†æŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚æœ€ç»ˆï¼Œåˆå¹¶åçš„æ¨¡å‹å±•ç°äº†æ˜¾è‘—å¢å¼ºçš„é›¶æ ·æœ¬æ³›åŒ–ï¼ˆZero-Shot Generalizationï¼‰èƒ½åŠ›ï¼Œåœ¨æœªè§ç‰©ç§çš„é—­é›†é›¶æ ·æœ¬åˆ†ç±»ä¸­å®ç°äº†è¶…è¿‡200%çš„ç›¸å¯¹æå‡ï¼Œç¡®ç«‹äº†æ–°çš„æŠ€æœ¯æ°´å¹³ï¼ˆSOTAï¼‰ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05171v2",
      "published_date": "2025-11-07 11:40:46 UTC",
      "updated_date": "2025-11-19 10:08:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:29:39.129877+00:00"
    },
    {
      "arxiv_id": "2511.05165v1",
      "title": "Generating Software Architecture Description from Source Code using Reverse Engineering and Large Language Model",
      "title_zh": "åŸºäºé€†å‘å·¥ç¨‹ä¸å¤§è¯­è¨€æ¨¡å‹ä»æºä»£ç ç”Ÿæˆè½¯ä»¶æ¶æ„æè¿°",
      "authors": [
        "Ahmad Hatahet",
        "Christoph Knieke",
        "Andreas Rausch"
      ],
      "abstract": "Software Architecture Descriptions (SADs) are essential for managing the inherent complexity of modern software systems. They enable high-level architectural reasoning, guide design decisions, and facilitate effective communication among diverse stakeholders. However, in practice, SADs are often missing, outdated, or poorly aligned with the system's actual implementation. Consequently, developers are compelled to derive architectural insights directly from source code-a time-intensive process that increases cognitive load, slows new developer onboarding, and contributes to the gradual degradation of clarity over the system's lifetime. To address these issues, we propose a semi-automated generation of SADs from source code by integrating reverse engineering (RE) techniques with a Large Language Model (LLM). Our approach recovers both static and behavioral architectural views by extracting a comprehensive component diagram, filtering architecturally significant elements (core components) via prompt engineering, and generating state machine diagrams to model component behavior based on underlying code logic with few-shots prompting. This resulting views representation offer a scalable and maintainable alternative to traditional manual architectural documentation. This methodology, demonstrated using C++ examples, highlights the potent capability of LLMs to: 1) abstract the component diagram, thereby reducing the reliance on human expert involvement, and 2) accurately represent complex software behaviors, especially when enriched with domain-specific knowledge through few-shot prompting. These findings suggest a viable path toward significantly reducing manual effort while enhancing system understanding and long-term maintainability.",
      "tldr_zh": "é’ˆå¯¹ç°ä»£è½¯ä»¶ç³»ç»Ÿä¸­è½¯ä»¶æ¶æ„æè¿°(SADs)ç»å¸¸ç¼ºå¤±ã€è¿‡æ—¶æˆ–ä¸å®ç°ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆé€†å‘å·¥ç¨‹(Reverse Engineering)å’Œå¤§å‹è¯­è¨€æ¨¡å‹(LLM)çš„åŠè‡ªåŠ¨åŒ–SADç”Ÿæˆæ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡æå–å…¨é¢çš„ç»„ä»¶å›¾ï¼Œå¹¶åˆ©ç”¨æç¤ºå·¥ç¨‹(Prompt Engineering)ç­›é€‰å‡ºå…·æœ‰æ¶æ„æ„ä¹‰çš„æ ¸å¿ƒç»„ä»¶ï¼Œä»è€Œæ¢å¤ç³»ç»Ÿçš„é™æ€æ¶æ„è§†å›¾ã€‚åŒæ—¶ï¼Œç ”ç©¶åˆ©ç”¨å°‘æ ·æœ¬æç¤º(Few-shot Prompting)åŸºäºåº•å±‚ä»£ç é€»è¾‘ç”ŸæˆçŠ¶æ€æœºå›¾ï¼Œä»¥å»ºæ¨¡ç»„ä»¶çš„è¡Œä¸ºè§†å›¾ã€‚é€šè¿‡C++ç¤ºä¾‹æ¼”ç¤ºï¼Œè¯¥æ–¹æ³•å±•ç¤ºäº†LLMåœ¨æŠ½è±¡ç»„ä»¶å›¾å’Œå‡†ç¡®è¡¨ç¤ºå¤æ‚è½¯ä»¶è¡Œä¸ºæ–¹é¢çš„å¼ºå¤§èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯ç»“åˆé¢†åŸŸçŸ¥è¯†æ—¶æ•ˆæœæ˜¾è‘—ã€‚è¿™ä¸€æˆæœä¸ºå‡å°‘äººå·¥æ–‡æ¡£ç¼–å†™å·¥ä½œé‡ã€å¢å¼ºç³»ç»Ÿç†è§£å’Œé•¿æœŸå¯ç»´æŠ¤æ€§æä¾›äº†ä¸€æ¡å¯è¡Œçš„è·¯å¾„ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05165v1",
      "published_date": "2025-11-07 11:35:46 UTC",
      "updated_date": "2025-11-07 11:35:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:30:04.829679+00:00"
    },
    {
      "arxiv_id": "2511.05156v1",
      "title": "SmartSecChain-SDN: A Blockchain-Integrated Intelligent Framework for Secure and Efficient Software-Defined Networks",
      "title_zh": "SmartSecChain-SDNï¼šé¢å‘å®‰å…¨é«˜æ•ˆè½¯ä»¶å®šä¹‰ç½‘ç»œçš„åŒºå—é“¾é›†æˆæ™ºèƒ½æ¡†æ¶",
      "authors": [
        "Azhar Hussain Mozumder",
        "M. John Basha",
        "Chayapathi A. R"
      ],
      "abstract": "With more and more existing networks being transformed to Software-Defined Networking (SDN), they need to be more secure and demand smarter ways of traffic control. This work, SmartSecChain-SDN, is a platform that combines machine learning based intrusion detection, blockchain-based storage of logs, and application-awareness-based priority in SDN networks. To detect network intrusions in a real-time, precision and low-false positives setup, the framework utilizes the application of advanced machine learning algorithms, namely Random Forest, XGBoost, CatBoost, and CNN-BiLSTM. SmartSecChain-SDN is based on the Hyperledger Fabric, which is a permissioned blockchain technology, to provide secure, scalable, and privacy-preserving storage and, thus, guarantee that the Intrusion Detection System (IDS) records cannot be altered and can be analyzed comprehensively. The system also has Quality of Service (QoS) rules and traffic shaping based on applications, which enables prioritization of critical services, such as VoIP, video conferencing, and business applications, as well as de-prioritization of non-essential traffic, such as downloads and updates. Mininet can simulate real-time SDN scenarios because it is used to prototype whole architectures. It is also compatible with controllers OpenDaylight and Ryu. It has tested the framework using the InSDN dataset and proved that it can identify different kinds of cyberattacks and handle bandwidth allocation efficiently under circumstances of resource constraints. SmartSecChain-SDN comprehensively addresses SDN system protection, securing and enhancing. The proposed study offers an innovative, extensible way to improve cybersecurity, regulatory compliance, and the administration of next-generation programmable networks.",
      "tldr_zh": "æœ¬æ–‡æå‡ºäº†SmartSecChain-SDNï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆäº†åŸºäºæœºå™¨å­¦ä¹ çš„å…¥ä¾µæ£€æµ‹ã€åŒºå—é“¾æ—¥å¿—å­˜å‚¨ä»¥åŠåŸºäºåº”ç”¨æ„ŸçŸ¥çš„ä¼˜å…ˆçº§ç®¡ç†çš„Software-Defined Networking (SDN) æ™ºèƒ½æ¡†æ¶ã€‚ä¸ºäº†å®ç°å®æ—¶ä¸”é«˜ç²¾åº¦çš„ç½‘ç»œå…¥ä¾µæ£€æµ‹ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨äº†Random Forestã€XGBoostã€CatBoostå’ŒCNN-BiLSTMç­‰å…ˆè¿›æœºå™¨å­¦ä¹ ç®—æ³•ã€‚SmartSecChain-SDNåŸºäºè®¸å¯é“¾æŠ€æœ¯Hyperledger Fabricï¼Œæä¾›äº†å®‰å…¨ã€å¯æ‰©å±•ä¸”ä¿æŠ¤éšç§çš„å­˜å‚¨æœºåˆ¶ï¼Œç¡®ä¿å…¥ä¾µæ£€æµ‹ç³»ç»Ÿ(IDS)çš„è®°å½•ä¸å¯ç¯¡æ”¹ä¸”ä¾¿äºå…¨é¢åˆ†æã€‚æ­¤å¤–ï¼Œç³»ç»Ÿè¿˜å®æ–½äº†åŸºäºåº”ç”¨æ„ŸçŸ¥çš„æœåŠ¡è´¨é‡(QoS)è§„åˆ™å’Œæµé‡æ•´å½¢ï¼Œä¼˜å…ˆä¿éšœVoIPå’Œè§†é¢‘ä¼šè®®ç­‰å…³é”®æœåŠ¡ï¼ŒåŒæ—¶é™åˆ¶éå¿…è¦æµé‡ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨Mininetä»¿çœŸç¯å¢ƒä»¥åŠOpenDaylightå’ŒRyuæ§åˆ¶å™¨è¿›è¡Œäº†åŸå‹è®¾è®¡ï¼Œå¹¶åœ¨InSDNæ•°æ®é›†ä¸Šè¿›è¡Œäº†æµ‹è¯•ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«å¤šç§ç½‘ç»œæ”»å‡»å¹¶åœ¨èµ„æºå—é™æƒ…å†µä¸‹é«˜æ•ˆåˆ†é…å¸¦å®½ï¼Œä¸ºä¸‹ä¸€ä»£å¯ç¼–ç¨‹ç½‘ç»œçš„å®‰å…¨æ€§å’Œç®¡ç†æä¾›äº†åˆ›æ–°çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG",
        "cs.NI"
      ],
      "primary_category": "cs.CR",
      "comment": "20 pages, 12 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.05156v1",
      "published_date": "2025-11-07 11:22:04 UTC",
      "updated_date": "2025-11-07 11:22:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:30:29.566369+00:00"
    },
    {
      "arxiv_id": "2511.08621v1",
      "title": "The LLM Pro Finance Suite: Multilingual Large Language Models for Financial Applications",
      "title_zh": "LLM Pro Finance Suiteï¼šé¢å‘é‡‘èåº”ç”¨çš„å¤šè¯­è¨€å¤§å‹è¯­è¨€æ¨¡å‹",
      "authors": [
        "GaÃ«tan Caillaut",
        "Raheel Qader",
        "Jingshu Liu",
        "Mariam NakhlÃ©",
        "Arezki Sadoune",
        "Massinissa Ahmim",
        "Jean-Gabriel Barthelemy"
      ],
      "abstract": "The financial industry's growing demand for advanced natural language processing (NLP) capabilities has highlighted the limitations of generalist large language models (LLMs) in handling domain-specific financial tasks. To address this gap, we introduce the LLM Pro Finance Suite, a collection of five instruction-tuned LLMs (ranging from 8B to 70B parameters) specifically designed for financial applications. Our approach focuses on enhancing generalist instruction-tuned models, leveraging their existing strengths in instruction following, reasoning, and toxicity control, while fine-tuning them on a curated, high-quality financial corpus comprising over 50% finance-related data in English, French, and German.\n  We evaluate the LLM Pro Finance Suite on a comprehensive financial benchmark suite, demonstrating consistent improvement over state-of-the-art baselines in finance-oriented tasks and financial translation. Notably, our models maintain the strong general-domain capabilities of their base models, ensuring reliable performance across non-specialized tasks. This dual proficiency, enhanced financial expertise without compromise on general abilities, makes the LLM Pro Finance Suite an ideal drop-in replacement for existing LLMs in financial workflows, offering improved domain-specific performance while preserving overall versatility. We publicly release two 8B-parameters models to foster future research and development in financial NLP applications: https://huggingface.co/collections/DragonLLM/llm-open-finance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é€šç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†ç‰¹å®šé‡‘èé¢†åŸŸä»»åŠ¡æ—¶çš„å±€é™æ€§ï¼Œæ¨å‡ºäº†LLM Pro Finance Suiteï¼Œè¿™æ˜¯ä¸€ç»„åŒ…å«äº”ä¸ªå‚æ•°é‡ä»8Båˆ°70Bä¸ç­‰çš„æŒ‡ä»¤å¾®è°ƒæ¨¡å‹ï¼Œä¸“ä¸ºé‡‘èåº”ç”¨è®¾è®¡ã€‚è¯¥æ–¹æ³•åŸºäºé€šç”¨æŒ‡ä»¤å¾®è°ƒæ¨¡å‹çš„ç°æœ‰ä¼˜åŠ¿ï¼Œåˆ©ç”¨åŒ…å«è¶…è¿‡50%è‹±è¯­ã€æ³•è¯­å’Œå¾·è¯­é‡‘èç›¸å…³æ•°æ®çš„é«˜è´¨é‡è¯­æ–™åº“è¿›è¡Œäº†è¿›ä¸€æ­¥å¾®è°ƒã€‚åœ¨ç»¼åˆé‡‘èåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥å¥—ä»¶åœ¨é‡‘èå¯¼å‘ä»»åŠ¡å’Œé‡‘èç¿»è¯‘æ–¹é¢è¡¨ç°å‡ä¼˜äºç°æœ‰çš„state-of-the-artåŸºçº¿æ¨¡å‹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™äº›æ¨¡å‹åœ¨æ˜¾è‘—å¢å¼ºé‡‘èä¸“ä¸šèƒ½åŠ›çš„åŒæ—¶ï¼Œå¹¶æœªç‰ºç‰²å…¶åœ¨éç‰¹å®šä»»åŠ¡ä¸Šçš„é€šç”¨é¢†åŸŸèƒ½åŠ›ï¼Œä½¿å…¶æˆä¸ºé‡‘èå·¥ä½œæµä¸­ç†æƒ³çš„ç›´æ¥æ›¿ä»£æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œä½œè€…è¿˜å…¬å¼€äº†ä¸¤ä¸ª8Bå‚æ•°æ¨¡å‹ï¼Œä»¥ä¿ƒè¿›é‡‘èNLPé¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶ä¸å¼€å‘ã€‚",
      "categories": [
        "q-fin.ST",
        "cs.AI",
        "q-fin.CP"
      ],
      "primary_category": "q-fin.ST",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08621v1",
      "published_date": "2025-11-07 11:08:31 UTC",
      "updated_date": "2025-11-07 11:08:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:31:26.987252+00:00"
    },
    {
      "arxiv_id": "2511.05150v1",
      "title": "From Linear Probing to Joint-Weighted Token Hierarchy: A Foundation Model Bridging Global and Cellular Representations in Biomarker Detection",
      "title_zh": "ä»çº¿æ€§æ¢æµ‹åˆ°è”åˆåŠ æƒè¯å…ƒå±‚çº§ï¼šæ¡¥æ¥ç”Ÿç‰©æ ‡å¿—ç‰©æ£€æµ‹ä¸­å…¨å±€ä¸ç»†èƒçº§è¡¨å¾çš„åŸºç¡€æ¨¡å‹",
      "authors": [
        "Jingsong Liu",
        "Han Li",
        "Nassir Navab",
        "Peter J. SchÃ¼ffler"
      ],
      "abstract": "AI-based biomarkers can infer molecular features directly from hematoxylin & eosin (H&E) slides, yet most pathology foundation models (PFMs) rely on global patch-level embeddings and overlook cell-level morphology. We present a PFM model, JWTH (Joint-Weighted Token Hierarchy), which integrates large-scale self-supervised pretraining with cell-centric post-tuning and attention pooling to fuse local and global tokens. Across four tasks involving four biomarkers and eight cohorts, JWTH achieves up to 8.3% higher balanced accuracy and 1.2% average improvement over prior PFMs, advancing interpretable and robust AI-based biomarker detection in digital pathology.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰ç—…ç†åŸºç¡€æ¨¡å‹(PFMs)ä¾èµ–å…¨å±€è¡¥ä¸çº§åµŒå…¥è€Œå¿½è§†ç»†èƒçº§å½¢æ€å­¦çš„é—®é¢˜ï¼Œæå‡ºäº†JWTH (Joint-Weighted Token Hierarchy)æ¨¡å‹ã€‚JWTHæ—¨åœ¨æ¡¥æ¥ç”Ÿç‰©æ ‡å¿—ç‰©æ£€æµ‹ä¸­çš„å…¨å±€ä¸ç»†èƒè¡¨ç¤ºï¼Œé€šè¿‡ç»“åˆå¤§è§„æ¨¡è‡ªç›‘ç£é¢„è®­ç»ƒä¸ä»¥ç»†èƒä¸ºä¸­å¿ƒçš„åå¾®è°ƒ(post-tuning)ï¼Œå¹¶åˆ©ç”¨æ³¨æ„åŠ›æ± åŒ–(attention pooling)æŠ€æœ¯æœ‰æ•ˆèåˆå±€éƒ¨å’Œå…¨å±€tokenã€‚åœ¨æ¶µç›–å››ç§ç”Ÿç‰©æ ‡å¿—ç‰©å’Œå…«ä¸ªé˜Ÿåˆ—çš„å››é¡¹ä»»åŠ¡æµ‹è¯•ä¸­ï¼ŒJWTHç›¸æ¯”å…ˆå‰çš„PFMså®ç°äº†é«˜è¾¾8.3%çš„å¹³è¡¡å‡†ç¡®ç‡æå‡å’Œ1.2%çš„å¹³å‡æ”¹è¿›ã€‚è¿™ä¸€æˆæœæ˜¾è‘—æå‡äº†æ•°å­—ç—…ç†å­¦ä¸­åŸºäºAIçš„ç”Ÿç‰©æ ‡å¿—ç‰©æ£€æµ‹çš„å¯è§£é‡Šæ€§å’Œé²æ£’æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05150v1",
      "published_date": "2025-11-07 11:05:36 UTC",
      "updated_date": "2025-11-07 11:05:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:32:05.388291+00:00"
    },
    {
      "arxiv_id": "2511.07464v1",
      "title": "Motif 2 12.7B technical report",
      "title_zh": "Motif 2 12.7B æŠ€æœ¯æŠ¥å‘Š",
      "authors": [
        "Junghwan Lim",
        "Sungmin Lee",
        "Dongseok Kim",
        "Taehyun Kim",
        "Eunhwan Park",
        "Jeesoo Lee",
        "Jeongdoo Lee",
        "Junhyeok Lee",
        "Wai Ting Cheung",
        "Dahye Choi",
        "Jaeheui Her",
        "Jaeyeon Huh",
        "Hanbin Jung",
        "Changjin Kang",
        "Beomgyu Kim",
        "Minjae Kim",
        "Taewhan Kim",
        "Youngrok Kim",
        "Hyukjin Kweon",
        "Haesol Lee",
        "Kungyu Lee",
        "Dongpin Oh",
        "Yeongjae Park",
        "Bokki Ryu",
        "Dongjoo Weon"
      ],
      "abstract": "We introduce Motif-2-12.7B, a new open-weight foundation model that pushes the efficiency frontier of large language models by combining architectural innovation with system-level optimization. Designed for scalable language understanding and robust instruction generalization under constrained compute budgets, Motif-2-12.7B builds upon Motif-2.6B with the integration of Grouped Differential Attention (GDA), which improves representational efficiency by disentangling signal and noise-control attention pathways. The model is pre-trained on 5.5 trillion tokens spanning diverse linguistic, mathematical, scientific, and programming domains using a curriculum-driven data scheduler that gradually changes the data composition ratio. The training system leverages the MuonClip optimizer alongside custom high-performance kernels, including fused PolyNorm activations and the Parallel Muon algorithm, yielding significant throughput and memory efficiency gains in large-scale distributed environments. Post-training employs a three-stage supervised fine-tuning pipeline that successively enhances general instruction adherence, compositional understanding, and linguistic precision. Motif-2-12.7B demonstrates competitive performance across diverse benchmarks, showing that thoughtful architectural scaling and optimized training design can rival the capabilities of much larger models.",
      "tldr_zh": "è¯¥æŠ¥å‘Šä»‹ç»äº†Motif-2-12.7Bï¼Œè¿™æ˜¯ä¸€æ¬¾æ—¨åœ¨é€šè¿‡æ¶æ„åˆ›æ–°ä¸ç³»ç»Ÿçº§ä¼˜åŒ–æ¨åŠ¨å¤§è¯­è¨€æ¨¡å‹æ•ˆç‡å‰æ²¿çš„å¼€æ”¾æƒé‡åŸºç¡€æ¨¡å‹ã€‚è¯¥æ¨¡å‹åŸºäºMotif-2.6Bæ„å»ºï¼Œé›†æˆäº†Grouped Differential Attention (GDA)æŠ€æœ¯ï¼Œé€šè¿‡åˆ†ç¦»ä¿¡å·ä¸å™ªå£°æ§åˆ¶æ³¨æ„åŠ›è·¯å¾„æ˜¾è‘—æå‡äº†è¡¨ç¤ºæ•ˆç‡ã€‚åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œæ¨¡å‹åˆ©ç”¨è¯¾ç¨‹é©±åŠ¨çš„æ•°æ®è°ƒåº¦å™¨åœ¨æ¶µç›–å¤šé¢†åŸŸçš„5.5ä¸‡äº¿tokenä¸Šè¿›è¡Œäº†è®­ç»ƒã€‚ä¸ºäº†æå‡å¤§è§„æ¨¡åˆ†å¸ƒå¼ç¯å¢ƒä¸‹çš„ååé‡ä¸å†…å­˜æ•ˆç‡ï¼Œè®­ç»ƒç³»ç»Ÿé‡‡ç”¨äº†MuonClipä¼˜åŒ–å™¨åŠèåˆPolyNormæ¿€æ´»å’ŒParallel Muonç®—æ³•ç­‰é«˜æ€§èƒ½å†…æ ¸ã€‚åè®­ç»ƒé˜¶æ®µåˆ™é‡‡ç”¨ä¸‰é˜¶æ®µç›‘ç£å¾®è°ƒç®¡é“ï¼Œä¾æ¬¡å¢å¼ºäº†æ¨¡å‹çš„æŒ‡ä»¤éµå¾ªã€ç»„åˆç†è§£åŠè¯­è¨€ç²¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMotif-2-12.7Båœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºæå…·ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œè¯æ˜äº†ç²¾å¿ƒçš„æ¶æ„æ‰©å±•ä¸ä¼˜åŒ–çš„è®­ç»ƒè®¾è®¡èƒ½å¤Ÿåª²ç¾æ›´å¤§è§„æ¨¡æ¨¡å‹çš„èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07464v1",
      "published_date": "2025-11-07 10:32:16 UTC",
      "updated_date": "2025-11-07 10:32:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:32:30.129408+00:00"
    },
    {
      "arxiv_id": "2511.05131v1",
      "title": "DL101 Neural Network Outputs and Loss Functions",
      "title_zh": "DL101ï¼šç¥ç»ç½‘ç»œè¾“å‡ºä¸æŸå¤±å‡½æ•°",
      "authors": [
        "Fernando Berzal"
      ],
      "abstract": "The loss function used to train a neural network is strongly connected to its output layer from a statistical point of view. This technical report analyzes common activation functions for a neural network output layer, like linear, sigmoid, ReLU, and softmax, detailing their mathematical properties and their appropriate use cases. A strong statistical justification exists for the selection of the suitable loss function for training a deep learning model. This report connects common loss functions such as Mean Squared Error (MSE), Mean Absolute Error (MAE), and various Cross-Entropy losses to the statistical principle of Maximum Likelihood Estimation (MLE). Choosing a specific loss function is equivalent to assuming a specific probability distribution for the model output, highlighting the link between these functions and the Generalized Linear Models (GLMs) that underlie network output layers. Additional scenarios of practical interest are also considered, such as alternative output encodings, constrained outputs, and distributions with heavy tails.",
      "tldr_zh": "è¯¥æŠ€æœ¯æŠ¥å‘Šæ·±å…¥åˆ†æäº†ç¥ç»ç½‘ç»œè¾“å‡ºå±‚ä¸è®­ç»ƒæ‰€ç”¨æŸå¤±å‡½æ•°ä¹‹é—´ç´§å¯†çš„ç»Ÿè®¡å­¦è”ç³»ã€‚æŠ¥å‘Šè¯¦ç»†æ¢è®¨äº†linearã€sigmoidã€ReLUå’Œsoftmaxç­‰å¸¸è§è¾“å‡ºå±‚æ¿€æ´»å‡½æ•°çš„æ•°å­¦æ€§è´¨åŠå…¶é€‚ç”¨åœºæ™¯ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹æŸå¤±å‡½æ•°çš„é€‰æ‹©å…·æœ‰å¼ºæœ‰åŠ›çš„ç»Ÿè®¡å­¦ä¾æ®ï¼Œå°†Mean Squared Error (MSE)ã€Mean Absolute Error (MAE)ä»¥åŠå„ç§Cross-EntropyæŸå¤±å‡½æ•°ä¸Maximum Likelihood Estimation (MLE)åŸåˆ™è”ç³»äº†èµ·æ¥ã€‚æŠ¥å‘Šå¼ºè°ƒï¼Œé€‰æ‹©ç‰¹å®šçš„æŸå¤±å‡½æ•°ç­‰åŒäºå¯¹æ¨¡å‹è¾“å‡ºå‡è®¾äº†ç‰¹å®šçš„æ¦‚ç‡åˆ†å¸ƒï¼Œæ­ç¤ºäº†è¿™äº›å‡½æ•°ä¸Generalized Linear Models (GLMs)ä¹‹é—´çš„å†…åœ¨å…³è”ã€‚æ­¤å¤–ï¼Œè¯¥æŠ¥å‘Šè¿˜æ¢è®¨äº†æ›¿ä»£è¾“å‡ºç¼–ç ã€å—é™è¾“å‡ºä»¥åŠé‡å°¾åˆ†å¸ƒç­‰å…·æœ‰å®é™…æ„ä¹‰çš„é¢å¤–åœºæ™¯ï¼Œä¸ºç¥ç»ç½‘ç»œè®¾è®¡æä¾›äº†ç³»ç»Ÿçš„ç†è®ºæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05131v1",
      "published_date": "2025-11-07 10:20:45 UTC",
      "updated_date": "2025-11-07 10:20:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:32:57.172044+00:00"
    },
    {
      "arxiv_id": "2511.07463v1",
      "title": "Dynamic Stability of LLM-Generated Code",
      "title_zh": "LLMç”Ÿæˆä»£ç çš„åŠ¨æ€ç¨³å®šæ€§",
      "authors": [
        "Prateek Rajput",
        "Abdoul Aziz Bonkoungou",
        "Yewei Song",
        "Abdoul Kader Kabore",
        "Iyiola E. Olatunji",
        "Jacques Klein",
        "Tegewende Bissyande"
      ],
      "abstract": "Current evaluations of LLMs for code generation emphasize functional correctness, overlooking the fact that functionally correct solutions can differ significantly in algorithmic complexity. For instance, an $(O(n^2))$ versus $(O(n \\log n))$ sorting algorithm may yield similar output but incur vastly different performance costs in production. This discrepancy reveals a critical limitation in current evaluation methods: they fail to capture the behavioral and performance diversity among correct solutions. To address this, we introduce a principled framework for evaluating the dynamic stability of generated code. We propose two metrics derived from opcode distributions: Static Canonical Trace Divergence (SCTD), which captures algorithmic structure diversity across generated solutions, and Dynamic Canonical Trace Divergence (DCTD), which quantifies runtime behavioral variance. Their ratio, the Behavioral Expression Factor (BEF), serves as a diagnostic signal: it indicates critical runtime instability when BEF $\\ll$ 1 and functional redundancy when BEF $\\gg$ 1. Empirical results on BigOBench and CodeContests show that state-of-the-art LLMs exhibit significant algorithmic variance even among functionally correct outputs. Notably, increasing sampling temperature improves pass@1 rates but degrades stability, revealing an unrecognized trade-off: searching for correct solutions in diverse output spaces introduces a \"penalty of instability\" between correctness and behavioral consistency. Our findings call for stability-aware objectives in code generation and new benchmarks with asymptotic test cases for robust, real-world LLM evaluation.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºç°æœ‰çš„LLMä»£ç ç”Ÿæˆè¯„ä¼°ä¸»è¦å…³æ³¨åŠŸèƒ½æ­£ç¡®æ€§ï¼Œå´å¿½è§†äº†å³ä½¿åŠŸèƒ½æ­£ç¡®çš„è§£å†³æ–¹æ¡ˆåœ¨ç®—æ³•å¤æ‚åº¦ï¼ˆå¦‚$O(n^2)$ä¸$O(n \\log n)$ï¼‰ä¸Šä¹Ÿå¯èƒ½å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œä»è€Œå¯¼è‡´ç”Ÿäº§ç¯å¢ƒä¸­å·¨å¤§çš„æ€§èƒ½æˆæœ¬ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªè¯„ä¼°ç”Ÿæˆä»£ç åŠ¨æ€ç¨³å®šæ€§çš„åŸåˆ™æ€§æ¡†æ¶ï¼Œå¹¶åŸºäºæ“ä½œç åˆ†å¸ƒå¼•å…¥äº†ä¸¤ä¸ªæŒ‡æ ‡ï¼šæ•æ‰ç®—æ³•ç»“æ„å¤šæ ·æ€§çš„Static Canonical Trace Divergence (SCTD)å’Œé‡åŒ–è¿è¡Œæ—¶è¡Œä¸ºå·®å¼‚çš„Dynamic Canonical Trace Divergence (DCTD)ã€‚ä¸¤è€…çš„æ¯”ç‡è¢«ç§°ä¸ºBehavioral Expression Factor (BEF)ï¼Œå¯ä½œä¸ºè¯Šæ–­è¿è¡Œæ—¶ä¸ç¨³å®šæˆ–åŠŸèƒ½å†—ä½™çš„å…³é”®ä¿¡å·ã€‚åœ¨BigOBenchå’ŒCodeContestsä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæœ€å…ˆè¿›çš„LLMså³ä½¿åœ¨è¾“å‡ºåŠŸèƒ½æ­£ç¡®æ—¶ä¹Ÿè¡¨ç°å‡ºæ˜¾è‘—çš„ç®—æ³•æ–¹å·®ã€‚ç ”ç©¶å‘ç°ï¼Œæé«˜é‡‡æ ·æ¸©åº¦è™½ç„¶èƒ½æå‡pass@1ç‡ä½†ä¼šé™ä½ç¨³å®šæ€§ï¼Œæ­ç¤ºäº†åœ¨æ­£ç¡®æ€§ä¸è¡Œä¸ºä¸€è‡´æ€§ä¹‹é—´å­˜åœ¨â€œä¸ç¨³å®šæ€§æƒ©ç½š(penalty of instability)â€çš„æƒè¡¡ã€‚è¯¥å‘ç°å‘¼ååœ¨ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­å¼•å…¥ç¨³å®šæ€§æ„ŸçŸ¥ç›®æ ‡ï¼Œå¹¶å»ºç«‹åŒ…å«æ¸è¿‘æµ‹è¯•ç”¨ä¾‹çš„æ–°åŸºå‡†ä»¥å®ç°æ›´ç¨³å¥çš„è¯„ä¼°ã€‚",
      "categories": [
        "cs.PL",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.PL",
      "comment": "10 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.07463v1",
      "published_date": "2025-11-07 09:58:06 UTC",
      "updated_date": "2025-11-07 09:58:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:33:22.692921+00:00"
    },
    {
      "arxiv_id": "2511.09563v1",
      "title": "An Efficient and Almost Optimal Solver for the Joint Routing-Assignment Problem via Partial JRA and Large-Î± Optimization",
      "title_zh": "åŸºäºå±€éƒ¨ JRA ä¸å¤§ $\\alpha$ ä¼˜åŒ–çš„è”åˆè·¯ç”±åˆ†é…é—®é¢˜é«˜æ•ˆè¿‘ä¹æœ€ä¼˜æ±‚è§£å™¨",
      "authors": [
        "Qilong Yuan"
      ],
      "abstract": "The Joint Routing-Assignment (JRA) optimization problem simultaneously determines the assignment of items to placeholders and a Hamiltonian cycle that visits each node pair exactly once, with the objective of minimizing total travel cost. Previous studies introduced an exact mixed-integer programming (MIP) solver, along with datasets and a Gurobi implementation, showing that while the exact approach guarantees optimality, it becomes computationally inefficient for large-scale instances. To overcome this limitation, heuristic methods based on merging algorithms and shaking procedures were proposed, achieving solutions within approximately 1% deviation from the optimum. This work presents a novel and more efficient approach that attains high-accuracy, near-optimal solutions for large-scale JRA problems. The proposed method introduces a Partial Path Reconstructon (PPR) solver that first identifies key item-placeholder pairs to form a reduced subproblem, which is solved efficiently to refine the global solution. Using this PJAR framework, the initial heuristic merging solutions can be further improved, reducing the deviation by half. Moreover, the solution can be iteratively polished with PPR based solver along the optimization path to yield highly accurate tours. Additionally, a global Large-Î± constraint is incorporated into the JRA model to further enhance solution optimality. Experimental evaluations on benchmark datasets with n = 300, 500, and 1000 demonstrate that the proposed method consistently delivers almost optimal solutions, achieving an average deviation of 0.00% from the ground truth while maintaining high computational efficiency. Beyond the JRA problem, the proposed framework and methodologies exhibit strong potential for broader applications. The Framework can be applied to TSP and related optimization problems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Joint Routing-Assignment (JRA)ä¼˜åŒ–é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§é«˜æ•ˆä¸”è¿‘ä¹æœ€ä¼˜çš„æ–°å‹æ±‚è§£æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿç²¾ç¡®æ··åˆæ•´æ•°è§„åˆ’(MIP)æ±‚è§£å™¨åœ¨å¤§è§„æ¨¡å®ä¾‹ä¸­è®¡ç®—æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†Partial Path Reconstruction (PPR)æ±‚è§£å™¨ï¼Œé€šè¿‡è¯†åˆ«å…³é”®çš„é¡¹ç›®-å ä½ç¬¦å¯¹(item-placeholder pairs)æ„å»ºç¼©å‡å­é—®é¢˜ï¼Œä»è€Œæœ‰æ•ˆä¼˜åŒ–å…¨å±€è§£ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶è€…åˆ©ç”¨PJARæ¡†æ¶è¿›ä¸€æ­¥æ”¹è¿›åˆå§‹å¯å‘å¼åˆå¹¶è§£ï¼Œå¹¶é€šè¿‡è¿­ä»£æ–¹å¼æ²¿ä¼˜åŒ–è·¯å¾„æ‰“ç£¨è§£çš„è´¨é‡ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹è¿˜ç»“åˆäº†å…¨å±€Large-Î±çº¦æŸä»¥è¿›ä¸€æ­¥å¢å¼ºè§£çš„æœ€ä¼˜æ€§ã€‚åœ¨n=300ã€500å’Œ1000çš„åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒé«˜è®¡ç®—æ•ˆç‡çš„åŒæ—¶ï¼Œå®ç°äº†ä¸çœŸå®å€¼0.00%çš„å¹³å‡åå·®ï¼Œå§‹ç»ˆæä¾›è¿‘ä¹æœ€ä¼˜çš„è§£ã€‚è¯¥æ¡†æ¶å’Œæ–¹æ³•ä¸ä»…é€‚ç”¨äºJRAé—®é¢˜ï¼Œåœ¨TSPåŠç›¸å…³ä¼˜åŒ–é—®é¢˜ä¸­ä¹Ÿå±•ç°å‡ºå¹¿é˜”çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "math.CO"
      ],
      "primary_category": "cs.AI",
      "comment": "20 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.09563v1",
      "published_date": "2025-11-07 09:54:46 UTC",
      "updated_date": "2025-11-07 09:54:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:37:46.574862+00:00"
    },
    {
      "arxiv_id": "2511.10667v1",
      "title": "Evaluating LLM Understanding via Structured Tabular Decision Simulations",
      "title_zh": "åŸºäºç»“æ„åŒ–è¡¨æ ¼å†³ç­–æ¨¡æ‹Ÿè¯„ä¼°LLMç†è§£èƒ½åŠ›",
      "authors": [
        "Sichao Li",
        "Xinyue Xu",
        "Xiaomeng Li"
      ],
      "abstract": "Large language models (LLMs) often achieve impressive predictive accuracy, yet correctness alone does not imply genuine understanding. True LLM understanding, analogous to human expertise, requires making consistent, well-founded decisions across multiple instances and diverse domains, relying on relevant and domain-grounded decision factors. We introduce Structured Tabular Decision Simulations (STaDS), a suite of expert-like decision settings that evaluate LLMs as if they were professionals undertaking structured decision ``exams''. In this context, understanding is defined as the ability to identify and rely on the correct decision factors, features that determine outcomes within a domain. STaDS jointly assesses understanding through: (i) question and instruction comprehension, (ii) knowledge-based prediction, and (iii) reliance on relevant decision factors. By analyzing 9 frontier LLMs across 15 diverse decision settings, we find that (a) most models struggle to achieve consistently strong accuracy across diverse domains; (b) models can be accurate yet globally unfaithful, and there are frequent mismatches between stated rationales and factors driving predictions. Our findings highlight the need for global-level understanding evaluation protocols and advocate for novel frameworks that go beyond accuracy to enhance LLMs' understanding ability.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„é«˜é¢„æµ‹å‡†ç¡®ç‡å¹¶ä¸ç­‰åŒäºçœŸæ­£çš„ç†è§£ï¼ŒçœŸæ­£çš„ç†è§£è¦æ±‚æ¨¡å‹åƒäººç±»ä¸“å®¶ä¸€æ ·åŸºäºç›¸å…³é¢†åŸŸå› ç´ åšå‡ºä¸€è‡´ä¸”æœ‰æ®å¯ä¾çš„å†³ç­–ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†Structured Tabular Decision Simulations (STaDS)ï¼Œè¿™æ˜¯ä¸€å¥—æ¨¡æ‹Ÿä¸“å®¶å†³ç­–â€œè€ƒè¯•â€çš„è¯„ä¼°å¥—ä»¶ï¼Œæ—¨åœ¨é€šè¿‡æŒ‡ä»¤ç†è§£ã€åŸºäºçŸ¥è¯†çš„é¢„æµ‹ä»¥åŠå¯¹ç›¸å…³å†³ç­–å› ç´ çš„ä¾èµ–æ€§è¿™ä¸‰ä¸ªç»´åº¦æ¥è”åˆè¯„ä¼°æ¨¡å‹çš„ç†è§£èƒ½åŠ›ã€‚é€šè¿‡å¯¹9ä¸ªå‰æ²¿LLMåœ¨15ä¸ªä¸åŒå†³ç­–åœºæ™¯ä¸‹çš„åˆ†æï¼Œç ”ç©¶å‘ç°å¤§å¤šæ•°æ¨¡å‹éš¾ä»¥åœ¨ä¸åŒé¢†åŸŸé—´ä¿æŒä¸€è‡´çš„é«˜å‡†ç¡®ç‡ã€‚æ›´å…³é”®çš„æ˜¯ï¼Œæ¨¡å‹è™½ç„¶å¯èƒ½é¢„æµ‹å‡†ç¡®ï¼Œä½†åœ¨â€œå…¨å±€å¿ å®åº¦â€(global faithfulness)ä¸Šè¡¨ç°ä¸ä½³ï¼Œå…¶é™ˆè¿°çš„æ¨ç†ç†ç”±å¾€å¾€ä¸å®é™…é©±åŠ¨é¢„æµ‹çš„å› ç´ ä¸åŒ¹é…ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†å»ºç«‹è¶…è¶Šå•çº¯å‡†ç¡®ç‡æŒ‡æ ‡çš„å…¨å±€ç†è§£è¯„ä¼°åè®®çš„å¿…è¦æ€§ï¼Œä»¥æå‡LLMsçš„çœŸå®ç†è§£èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.10667v1",
      "published_date": "2025-11-07 09:42:39 UTC",
      "updated_date": "2025-11-07 09:42:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:37:01.002584+00:00"
    },
    {
      "arxiv_id": "2511.07461v1",
      "title": "It Takes Two: A Dual Stage Approach for Terminology-Aware Translation",
      "title_zh": "åŒç®¡é½ä¸‹ï¼šé¢å‘æœ¯è¯­æ„ŸçŸ¥ç¿»è¯‘çš„åŒé˜¶æ®µæ–¹æ³•",
      "authors": [
        "Akshat Singh Jaswal"
      ],
      "abstract": "This paper introduces DuTerm, a novel two-stage architecture for terminology-constrained machine translation. Our system combines a terminology-aware NMT model, adapted via fine-tuning on large-scale synthetic data, with a prompt-based LLM for post-editing. The LLM stage refines NMT output and enforces terminology adherence. We evaluate DuTerm on English-to German, English-to-Spanish, and English-to-Russian with the WMT 2025 Terminology Shared Task corpus. We demonstrate that flexible, context-driven terminology handling by the LLM consistently yields higher quality translations than strict constraint enforcement. Our results highlight a critical trade-off, revealing that an LLM's work best for high-quality translation as context-driven mutators rather than generators.",
      "tldr_zh": "æœ¬æ–‡ä»‹ç»äº†DuTermï¼Œä¸€ç§ç”¨äºæœ¯è¯­çº¦æŸæœºå™¨ç¿»è¯‘çš„æ–°é¢–ä¸¤é˜¶æ®µæ¶æ„ã€‚è¯¥ç³»ç»Ÿç»“åˆäº†é€šè¿‡å¤§è§„æ¨¡åˆæˆæ•°æ®å¾®è°ƒçš„æœ¯è¯­æ„ŸçŸ¥ç¥ç»æœºå™¨ç¿»è¯‘(NMT)æ¨¡å‹ï¼Œä»¥åŠç”¨äºåç¼–è¾‘(post-editing)çš„åŸºäºæç¤ºçš„å¤§å‹è¯­è¨€æ¨¡å‹(LLM)ã€‚LLMé˜¶æ®µè´Ÿè´£ä¼˜åŒ–NMTçš„è¾“å‡ºå¹¶å¼ºåˆ¶æ‰§è¡Œæœ¯è¯­ä¸€è‡´æ€§ã€‚ç ”ç©¶è€…ä½¿ç”¨WMT 2025æœ¯è¯­å…±äº«ä»»åŠ¡è¯­æ–™åº“ï¼Œåœ¨è‹±è¯­åˆ°å¾·è¯­ã€è¥¿ç­ç‰™è¯­å’Œä¿„è¯­çš„ç¿»è¯‘ä»»åŠ¡ä¸Šå¯¹DuTermè¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMè¿›è¡Œçµæ´»çš„ã€ä¸Šä¸‹æ–‡é©±åŠ¨çš„æœ¯è¯­å¤„ç†ï¼Œæ¯”ä¸¥æ ¼çš„çº¦æŸæ‰§è¡Œèƒ½äº§ç”Ÿæ›´é«˜è´¨é‡çš„ç¿»è¯‘ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†ä¸€ä¸ªå…³é”®çš„æƒè¡¡ï¼ŒæŒ‡å‡ºLLMåœ¨é«˜è´¨é‡ç¿»è¯‘ä¸­ä½œä¸ºä¸Šä¸‹æ–‡é©±åŠ¨çš„ä¿®æ”¹è€…(mutators)æ¯”ä½œä¸ºç”Ÿæˆè€…(generators)è¡¨ç°æ›´ä½³ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to WMT 2025. Code availavle at https://github.com/akshat-sj/duterm",
      "pdf_url": "https://arxiv.org/pdf/2511.07461v1",
      "published_date": "2025-11-07 08:50:58 UTC",
      "updated_date": "2025-11-07 08:50:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:38:00.931429+00:00"
    },
    {
      "arxiv_id": "2511.11625v1",
      "title": "MedFedPure: A Medical Federated Framework with MAE-based Detection and Diffusion Purification for Inference-Time Attacks",
      "title_zh": "MedFedPureï¼šé’ˆå¯¹æ¨ç†æ—¶æ”»å‡»çš„åŸºäºMAEæ£€æµ‹ä¸æ‰©æ•£å‡€åŒ–çš„åŒ»ç–—è”é‚¦æ¡†æ¶",
      "authors": [
        "Mohammad Karami",
        "Mohammad Reza Nemati",
        "Aidin Kazemi",
        "Ali Mikaeili Barzili",
        "Hamid Azadegan",
        "Behzad Moshiri"
      ],
      "abstract": "Artificial intelligence (AI) has shown great potential in medical imaging, particularly for brain tumor detection using Magnetic Resonance Imaging (MRI). However, the models remain vulnerable at inference time when they are trained collaboratively through Federated Learning (FL), an approach adopted to protect patient privacy. Adversarial attacks can subtly alter medical scans in ways invisible to the human eye yet powerful enough to mislead AI models, potentially causing serious misdiagnoses. Existing defenses often assume centralized data and struggle to cope with the decentralized and diverse nature of federated medical settings. In this work, we present MedFedPure, a personalized federated learning defense framework designed to protect diagnostic AI models at inference time without compromising privacy or accuracy. MedFedPure combines three key elements: (1) a personalized FL model that adapts to the unique data distribution of each institution; (2) a Masked Autoencoder (MAE) that detects suspicious inputs by exposing hidden perturbations; and (3) an adaptive diffusion-based purification module that selectively cleans only the flagged scans before classification. Together, these steps offer robust protection while preserving the integrity of normal, benign images. We evaluated MedFedPure on the Br35H brain MRI dataset. The results show a significant gain in adversarial robustness, improving performance from 49.50% to 87.33% under strong attacks, while maintaining a high clean accuracy of 97.67%. By operating locally and in real time during diagnosis, our framework provides a practical path to deploying secure, trustworthy, and privacy-preserving AI tools in clinical workflows.\n  Index Terms: cancer, tumor detection, federated learning, masked autoencoder, diffusion, privacy",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MedFedPureï¼Œä¸€ç§é’ˆå¯¹æ¨ç†é˜¶æ®µå¯¹æŠ—æ€§æ”»å‡»çš„ä¸ªæ€§åŒ–è”é‚¦å­¦ä¹ (Federated Learning)é˜²å¾¡æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åŒ»ç–—å½±åƒAIæ¨¡å‹åœ¨ä¿æŠ¤éšç§çš„åŒæ—¶é¢ä¸´çš„å®‰å…¨æ¼æ´ã€‚MedFedPureç»“åˆäº†ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šé€‚åº”ä¸åŒæœºæ„æ•°æ®åˆ†å¸ƒçš„ä¸ªæ€§åŒ–FLæ¨¡å‹ã€ç”¨äºæ£€æµ‹éšè—æ‰°åŠ¨çš„æ©ç è‡ªç¼–ç å™¨(Masked Autoencoder, MAE)ã€ä»¥åŠä»…å¯¹è¢«æ ‡è®°çš„å¯ç–‘æ‰«æè¿›è¡Œæ¸…æ´—çš„è‡ªé€‚åº”æ‰©æ•£å‡€åŒ–æ¨¡å—(diffusion-based purification)ã€‚è¯¥æ¡†æ¶åœ¨ä¸ç‰ºç‰²æ­£å¸¸å›¾åƒå®Œæ•´æ€§çš„å‰æä¸‹ï¼Œæœ‰æ•ˆé˜²å¾¡äº†äººçœ¼ä¸å¯è§çš„å¯¹æŠ—æ€§æ”»å‡»ã€‚åœ¨Br35Hè„‘éƒ¨MRIæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒMedFedPureåœ¨å¼ºæ”»å‡»ä¸‹çš„å¯¹æŠ—é²æ£’æ€§ä»49.50%æ˜¾è‘—æå‡è‡³87.33%ï¼ŒåŒæ—¶ä¿æŒäº†97.67%çš„é«˜æ¸…æ´å‡†ç¡®ç‡ã€‚è¿™ä¸€æˆæœä¸ºåœ¨ä¸´åºŠå·¥ä½œæµä¸­éƒ¨ç½²å®‰å…¨ã€å¯ä¿¡ä¸”ä¿æŠ¤éšç§çš„AIå·¥å…·æä¾›äº†åˆ‡å®å¯è¡Œçš„è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11625v1",
      "published_date": "2025-11-07 08:48:03 UTC",
      "updated_date": "2025-11-07 08:48:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:39:38.373784+00:00"
    },
    {
      "arxiv_id": "2511.05073v1",
      "title": "Deep learning models are vulnerable, but adversarial examples are even more vulnerable",
      "title_zh": "æ·±åº¦å­¦ä¹ æ¨¡å‹æ˜¯è„†å¼±çš„ï¼Œä½†å¯¹æŠ—æ ·æœ¬æ›´ä¸ºè„†å¼±",
      "authors": [
        "Jun Li",
        "Yanwei Xu",
        "Keran Li",
        "Xiaoli Zhang"
      ],
      "abstract": "Understanding intrinsic differences between adversarial examples and clean samples is key to enhancing DNN robustness and detection against adversarial attacks. This study first empirically finds that image-based adversarial examples are notably sensitive to occlusion. Controlled experiments on CIFAR-10 used nine canonical attacks (e.g., FGSM, PGD) to generate adversarial examples, paired with original samples for evaluation. We introduce Sliding Mask Confidence Entropy (SMCE) to quantify model confidence fluctuation under occlusion. Using 1800+ test images, SMCE calculations supported by Mask Entropy Field Maps and statistical distributions show adversarial examples have significantly higher confidence volatility under occlusion than originals. Based on this, we propose Sliding Window Mask-based Adversarial Example Detection (SWM-AED), which avoids catastrophic overfitting of conventional adversarial training. Evaluations across classifiers and attacks on CIFAR-10 demonstrate robust performance, with accuracy over 62% in most cases and up to 96.5%.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦ç¥ç»ç½‘ç»œ(DNN)çš„é²æ£’æ€§é—®é¢˜ï¼Œé€šè¿‡å®è¯å‘ç°åŸºäºå›¾åƒçš„å¯¹æŠ—æ ·æœ¬(adversarial examples)å¯¹é®æŒ¡å…·æœ‰æ˜¾è‘—çš„æ•æ„Ÿæ€§ã€‚ä¸ºäº†é‡åŒ–è¿™ç§å·®å¼‚ï¼Œä½œè€…åœ¨CIFAR-10æ•°æ®é›†ä¸Šåˆ©ç”¨FGSMå’ŒPGDç­‰ä¹ç§å…¸å‹æ”»å‡»ç”Ÿæˆæ ·æœ¬ï¼Œå¹¶æå‡ºäº†Sliding Mask Confidence Entropy (SMCE)æŒ‡æ ‡æ¥è¡¡é‡æ¨¡å‹åœ¨é®æŒ¡ä¸‹çš„ç½®ä¿¡åº¦æ³¢åŠ¨ã€‚é€šè¿‡Mask Entropy Field Mapså’Œç»Ÿè®¡åˆ†å¸ƒåˆ†æï¼Œç ”ç©¶è¯å®å¯¹æŠ—æ ·æœ¬åœ¨é®æŒ¡ä¸‹çš„ç½®ä¿¡åº¦æ³¢åŠ¨æ€§æ˜¾è‘—é«˜äºåŸå§‹æ ·æœ¬ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œæ–‡ç« æå‡ºäº†Sliding Window Mask-based Adversarial Example Detection (SWM-AED)æ–¹æ³•ï¼Œæœ‰æ•ˆé¿å…äº†ä¼ ç»Ÿå¯¹æŠ—è®­ç»ƒä¸­å¸¸è§çš„ç¾éš¾æ€§è¿‡æ‹Ÿåˆé—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§åˆ†ç±»å™¨å’Œæ”»å‡»åœºæ™¯ä¸‹è¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½ï¼Œåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹å‡†ç¡®ç‡è¶…è¿‡62%ï¼Œæœ€é«˜å¯è¾¾96.5%ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "25 pages,12 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.05073v1",
      "published_date": "2025-11-07 08:43:08 UTC",
      "updated_date": "2025-11-07 08:43:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:38:17.953772+00:00"
    },
    {
      "arxiv_id": "2511.08620v1",
      "title": "Learn More, Forget Less: A Gradient-Aware Data Selection Approach for LLM",
      "title_zh": "å¤šå­¦å°‘å¿˜ï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹çš„æ¢¯åº¦æ„ŸçŸ¥æ•°æ®é€‰æ‹©æ–¹æ³•",
      "authors": [
        "Yibai Liu",
        "Shihang Wang",
        "Zeming Liu",
        "Zheming Song",
        "Junzhe Wang",
        "Jingjing Liu",
        "Qingjie Liu",
        "Yunhong Wang"
      ],
      "abstract": "Despite large language models (LLMs) have achieved impressive achievements across numerous tasks, supervised fine-tuning (SFT) remains essential for adapting these models to specialized domains. However, SFT for domain specialization can be resource-intensive and sometimes leads to a deterioration in performance over general capabilities due to catastrophic forgetting (CF). To address these issues, we propose a self-adaptive gradient-aware data selection approach (GrADS) for supervised fine-tuning of LLMs, which identifies effective subsets of training data by analyzing gradients obtained from a preliminary training phase. Specifically, we design self-guided criteria that leverage the magnitude and statistical distribution of gradients to prioritize examples that contribute the most to the model's learning process. This approach enables the acquisition of representative samples that enhance LLMs understanding of domain-specific tasks. Through extensive experimentation with various LLMs across diverse domains such as medicine, law, and finance, GrADS has demonstrated significant efficiency and cost-effectiveness. Remarkably, utilizing merely 5% of the selected GrADS data, LLMs already surpass the performance of those fine-tuned on the entire dataset, and increasing to 50% of the data results in significant improvements! With catastrophic forgetting substantially mitigated simultaneously. We will release our code for GrADS later.",
      "tldr_zh": "å°½ç®¡ç›‘ç£å¾®è°ƒ(SFT)å¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„é¢†åŸŸé€‚åº”è‡³å…³é‡è¦ï¼Œä½†å®ƒå¾€å¾€èµ„æºæ¶ˆè€—å·¨å¤§ä¸”å®¹æ˜“å¯¼è‡´ç¾éš¾æ€§é—å¿˜(Catastrophic Forgetting)ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§è‡ªé€‚åº”çš„æ¢¯åº¦æ„ŸçŸ¥æ•°æ®é€‰æ‹©æ–¹æ³•GrADSï¼Œæ—¨åœ¨é€šè¿‡åˆ†æåˆæ­¥è®­ç»ƒé˜¶æ®µè·å¾—çš„æ¢¯åº¦æ¥è¯†åˆ«æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®å­é›†ã€‚GrADSåˆ©ç”¨æ¢¯åº¦çš„å¹…åº¦å’Œç»Ÿè®¡åˆ†å¸ƒè®¾è®¡è‡ªå¯¼å‘æ ‡å‡†ï¼Œä¼˜å…ˆé€‰æ‹©å¯¹æ¨¡å‹å­¦ä¹ è¿‡ç¨‹è´¡çŒ®æœ€å¤§çš„æ ·æœ¬ï¼Œä»è€Œåœ¨å¢å¼ºé¢†åŸŸç‰¹å®šä»»åŠ¡ç†è§£çš„åŒæ—¶ä¿æŒé€šç”¨èƒ½åŠ›ã€‚åœ¨åŒ»å­¦ã€æ³•å¾‹å’Œé‡‘èç­‰å¤šä¸ªé¢†åŸŸçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒGrADSå…·æœ‰æ˜¾è‘—çš„æ•ˆç‡å’Œæˆæœ¬æ•ˆç›Šã€‚ä»…ä½¿ç”¨5%çš„GrADSé€‰æ‹©æ•°æ®ï¼Œæ¨¡å‹æ€§èƒ½å³å¯è¶…è¶Šåœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šå¾®è°ƒçš„ç»“æœï¼Œä½¿ç”¨50%çš„æ•°æ®åˆ™èƒ½å¸¦æ¥æ›´æ˜¾è‘—çš„æå‡ï¼ŒåŒæ—¶å¤§å¹…ç¼“è§£äº†ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Under review",
      "pdf_url": "https://arxiv.org/pdf/2511.08620v1",
      "published_date": "2025-11-07 08:34:50 UTC",
      "updated_date": "2025-11-07 08:34:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:38:55.473370+00:00"
    },
    {
      "arxiv_id": "2601.02364v1",
      "title": "Towards Trustworthy LLM-Based Recommendation via Rationale Integration",
      "title_zh": "åŸºäºç†ç”±æ•´åˆè¿ˆå‘å¯ä¿¡çš„ LLM æ¨è",
      "authors": [
        "Chung Park",
        "Taesan Kim",
        "Hyeongjun Yun",
        "Dongjoon Hong",
        "Junui Hong",
        "Kijung Park",
        "MinCheol Cho",
        "Mira Myong",
        "Jihoon Oh",
        "Min sung Choi"
      ],
      "abstract": "Traditional recommender systems (RS) have been primarily optimized for accuracy and short-term engagement, often overlooking transparency and trustworthiness. Recently, platforms such as Amazon and Instagram have begun providing recommendation rationales to users, acknowledging their critical role in fostering trust and enhancing engagement; however, most existing systems still treat them as post-hoc artifacts. We propose an LLM-based recommender (LLM-Rec) that not only predicts items but also generates logically grounded rationales. Our approach leverages a self-annotated rationale dataset and instruction tuning in a rationale-first format, where the model generates an explanation before outputting the recommended item. By adopting this strategy and representing rationales in a chain-of-thought (CoT) style, LLM-Rec strengthens both interpretability and recommendation performance. Experiments on the Fashion and Scientific domains of the Amazon Review dataset demonstrate significant improvements over well-established baselines. To encourage reproducibility and future research, we publicly release a rationale-augmented recommendation dataset containing user histories, rationales, and recommended items.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿæ¨èç³»ç»Ÿ(RS)å¿½è§†é€æ˜åº¦å’Œå¯ä¿¡åº¦çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„æ¨èç³»ç»ŸLLM-Recï¼Œæ—¨åœ¨é€šè¿‡æ•´åˆé€»è¾‘ä¾æ®æ¥å®ç°å¯ä¿¡æ¨èã€‚ä¸åŒäºå°†è§£é‡Šè§†ä¸ºäº‹åäº§ç‰©çš„ç°æœ‰æ–¹æ³•ï¼ŒLLM-Recåˆ©ç”¨è‡ªæ ‡æ³¨æ•°æ®é›†å’Œâ€œä¾æ®ä¼˜å…ˆâ€(rationale-first)æ ¼å¼çš„æŒ‡ä»¤å¾®è°ƒï¼Œå¼ºåˆ¶æ¨¡å‹åœ¨è¾“å‡ºæ¨èé¡¹ç›®å‰å…ˆç”Ÿæˆè§£é‡Šã€‚é€šè¿‡é‡‡ç”¨è¿™ç§ç­–ç•¥å¹¶ä»¥é“¾å¼æ€ç»´(Chain-of-Thought, CoT)é£æ ¼å‘ˆç°ä¾æ®ï¼Œè¯¥æ¡†æ¶åŒæ—¶å¢å¼ºäº†ç³»ç»Ÿçš„å¯è§£é‡Šæ€§å’Œæ¨èå‡†ç¡®æ€§ã€‚åœ¨Amazon Reviewæ•°æ®é›†çš„æ—¶å°šå’Œç§‘å­¦é¢†åŸŸè¿›è¡Œçš„å®éªŒæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºæˆç†Ÿçš„åŸºçº¿æ¨¡å‹ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜å…¬å¼€å‘å¸ƒäº†åŒ…å«ç”¨æˆ·å†å²ã€ä¾æ®å’Œæ¨èé¡¹ç›®çš„ä¾æ®å¢å¼ºæ¨èæ•°æ®é›†ï¼Œä»¥æ”¯æŒè¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted at RS4SD'25 (CIKM'25 Workshop)",
      "pdf_url": "https://arxiv.org/pdf/2601.02364v1",
      "published_date": "2025-11-07 08:30:29 UTC",
      "updated_date": "2025-11-07 08:30:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:40:05.459016+00:00"
    },
    {
      "arxiv_id": "2511.05055v1",
      "title": "No Pose Estimation? No Problem: Pose-Agnostic and Instance-Aware Test-Time Adaptation for Monocular Depth Estimation",
      "title_zh": "æ— éœ€å§¿æ€ä¼°è®¡ï¼Ÿæ²¡é—®é¢˜ï¼šé¢å‘å•ç›®æ·±åº¦ä¼°è®¡çš„å§¿æ€æ— å…³ä¸å®ä¾‹æ„ŸçŸ¥æµ‹è¯•æ—¶é€‚åº”",
      "authors": [
        "Mingyu Sung",
        "Hyeonmin Choe",
        "Il-Min Kim",
        "Sangseok Yun",
        "Jae Mo Kang"
      ],
      "abstract": "Monocular depth estimation (MDE), inferring pixel-level depths in single RGB images from a monocular camera, plays a crucial and pivotal role in a variety of AI applications demanding a three-dimensional (3D) topographical scene. In the real-world scenarios, MDE models often need to be deployed in environments with different conditions from those for training. Test-time (domain) adaptation (TTA) is one of the compelling and practical approaches to address the issue. Although there have been notable advancements in TTA for MDE, particularly in a self-supervised manner, existing methods are still ineffective and problematic when applied to diverse and dynamic environments. To break through this challenge, we propose a novel and high-performing TTA framework for MDE, named PITTA. Our approach incorporates two key innovative strategies: (i) pose-agnostic TTA paradigm for MDE and (ii) instance-aware image masking. Specifically, PITTA enables highly effective TTA on a pretrained MDE network in a pose-agnostic manner without resorting to any camera pose information. Besides, our instance-aware masking strategy extracts instance-wise masks for dynamic objects (e.g., vehicles, pedestrians, etc.) from a segmentation mask produced by a pretrained panoptic segmentation network, by removing static objects including background components. To further boost performance, we also present a simple yet effective edge extraction methodology for the input image (i.e., a single monocular image) and depth map. Extensive experimental evaluations on DrivingStereo and Waymo datasets with varying environmental conditions demonstrate that our proposed framework, PITTA, surpasses the existing state-of-the-art techniques with remarkable performance improvements in MDE during TTA.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å•ç›®æ·±åº¦ä¼°è®¡(MDE)åœ¨çœŸå®ä¸–ç•ŒåŠ¨æ€åœºæ™¯ä¸­é¢ä¸´çš„é¢†åŸŸé€‚åº”é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºPITTAçš„æ–°å‹æµ‹è¯•æ—¶è‡ªé€‚åº”(TTA)æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•åœ¨å¤šæ ·åŒ–ç¯å¢ƒä¸­æ•ˆæœä¸ä½³çš„å±€é™ï¼ŒPITTAå¼•å…¥äº†å§¿æ€æ— å…³(pose-agnostic)çš„TTAèŒƒå¼ï¼Œæ— éœ€ä¾èµ–ä»»ä½•ç›¸æœºå§¿æ€ä¿¡æ¯å³å¯åœ¨é¢„è®­ç»ƒçš„MDEç½‘ç»œä¸Šè¿›è¡Œé«˜æ•ˆé€‚åº”ã€‚åŒæ—¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨å®ä¾‹æ„ŸçŸ¥(instance-aware)å›¾åƒæ©ç ç­–ç•¥ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„å…¨æ™¯åˆ†å‰²ç½‘ç»œè¯†åˆ«å¹¶å¤„ç†è½¦è¾†ã€è¡Œäººç­‰åŠ¨æ€å¯¹è±¡ï¼Œæœ‰æ•ˆå‰”é™¤é™æ€èƒŒæ™¯å¹²æ‰°ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§é’ˆå¯¹è¾“å…¥å›¾åƒå’Œæ·±åº¦å›¾çš„ç®€å•æœ‰æ•ˆçš„è¾¹ç¼˜æå–æ–¹æ³•ä»¥è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚åœ¨DrivingStereoå’ŒWaymoæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒPITTAåœ¨ä¸åŒç¯å¢ƒæ¡ä»¶ä¸‹å‡è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›(SOTA)æŠ€æœ¯ï¼Œä¸ºè§£å†³MDEçš„åŠ¨æ€ç¯å¢ƒé€‚åº”é—®é¢˜æä¾›äº†é«˜æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05055v1",
      "published_date": "2025-11-07 07:55:02 UTC",
      "updated_date": "2025-11-07 07:55:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:40:15.356310+00:00"
    },
    {
      "arxiv_id": "2511.05053v1",
      "title": "Accelerating HDC-CNN Hybrid Models Using Custom Instructions on RISC-V GPUs",
      "title_zh": "åˆ©ç”¨ RISC-V GPU è‡ªå®šä¹‰æŒ‡ä»¤åŠ é€Ÿ HDC-CNN æ··åˆæ¨¡å‹",
      "authors": [
        "Wakuto Matsumi",
        "Riaz-Ul-Haque Mian"
      ],
      "abstract": "Machine learning based on neural networks has advanced rapidly, but the high energy consumption required for training and inference remains a major challenge. Hyperdimensional Computing (HDC) offers a lightweight, brain-inspired alternative that enables high parallelism but often suffers from lower accuracy on complex visual tasks. To overcome this, hybrid accelerators combining HDC and Convolutional Neural Networks (CNNs) have been proposed, though their adoption is limited by poor generalizability and programmability. The rise of open-source RISC-V architectures has created new opportunities for domain-specific GPU design. Unlike traditional proprietary GPUs, emerging RISC-V-based GPUs provide flexible, programmable platforms suitable for custom computation models such as HDC. In this study, we design and implement custom GPU instructions optimized for HDC operations, enabling efficient processing for hybrid HDC-CNN workloads. Experimental results using four types of custom HDC instructions show a performance improvement of up to 56.2 times in microbenchmark tests, demonstrating the potential of RISC-V GPUs for energy-efficient, high-performance computing.",
      "tldr_zh": "é’ˆå¯¹ç¥ç»ç½‘ç»œçš„é«˜èƒ½è€—é—®é¢˜ä»¥åŠHyperdimensional Computing (HDC)åœ¨å¤æ‚è§†è§‰ä»»åŠ¡ä¸­çš„ç²¾åº¦å±€é™ï¼Œç ”ç©¶äººå‘˜è™½ç„¶æå‡ºäº†ç»“åˆä¸¤è€…çš„HDC-CNNæ··åˆæ¨¡å‹ï¼Œä½†ç°æœ‰æ–¹æ¡ˆå¸¸å—é™äºé€šç”¨æ€§å’Œå¯ç¼–ç¨‹æ€§ã€‚æœ¬ç ”ç©¶åˆ©ç”¨å¼€æºRISC-Væ¶æ„çš„çµæ´»æ€§ï¼Œè®¾è®¡å¹¶å®ç°äº†ä¸“é—¨é’ˆå¯¹HDCæ“ä½œä¼˜åŒ–çš„è‡ªå®šä¹‰GPUæŒ‡ä»¤ï¼Œä»¥åŠ é€ŸHDC-CNNæ··åˆå·¥ä½œè´Ÿè½½ã€‚ä¸ä¼ ç»Ÿä¸“æœ‰GPUä¸åŒï¼Œè¿™ç§åŸºäºRISC-Vçš„GPUæä¾›äº†é€‚åˆHDCç­‰è‡ªå®šä¹‰è®¡ç®—æ¨¡å‹çš„å¯ç¼–ç¨‹å¹³å°ã€‚é€šè¿‡å››ç§ç±»å‹çš„è‡ªå®šä¹‰HDCæŒ‡ä»¤è¿›è¡Œçš„å¾®åŸºå‡†æµ‹è¯•è¡¨æ˜ï¼Œè¯¥æ–¹æ³•å®ç°äº†é«˜è¾¾56.2å€çš„æ€§èƒ½æå‡ã€‚è¿™ä¸€ç»“æœå……åˆ†å±•ç¤ºäº†RISC-V GPUsåœ¨æ„å»ºé«˜èƒ½æ•ˆã€é«˜æ€§èƒ½è®¡ç®—ç³»ç»Ÿæ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05053v1",
      "published_date": "2025-11-07 07:50:48 UTC",
      "updated_date": "2025-11-07 07:50:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:40:43.154935+00:00"
    },
    {
      "arxiv_id": "2511.05040v1",
      "title": "UA-Code-Bench: A Competitive Programming Benchmark for Evaluating LLM Code Generation in Ukrainian",
      "title_zh": "UA-Code-Benchï¼šè¯„ä¼°ä¹Œå…‹å…°è¯­LLMä»£ç ç”Ÿæˆèƒ½åŠ›çš„ç«æŠ€ç¼–ç¨‹åŸºå‡†",
      "authors": [
        "Mykyta Syromiatnikov",
        "Victoria Ruvinskaya"
      ],
      "abstract": "Evaluating the real capabilities of large language models in low-resource languages still represents a challenge, as many existing benchmarks focus on widespread tasks translated from English or evaluate only simple language understanding. This paper introduces UA-Code-Bench, a new open-source benchmark established for a thorough evaluation of language models' code generation and competitive programming problem-solving abilities in Ukrainian. The benchmark comprises 500 problems from the Eolymp platform, evenly distributed across five complexity levels from very easy to very hard. A diverse set of 13 leading proprietary and open-source models, generating Python solutions based on a one-shot prompt, was evaluated via the dedicated Eolymp environment against hidden tests, ensuring code correctness. The obtained results reveal that even top-performing models, such as OpenAI o3 and GPT-5, solve only half of the problems, highlighting the challenge of code generation in low-resource natural language. Furthermore, this research presents a comprehensive analysis of performance across various difficulty levels, as well as an assessment of solution uniqueness and computational efficiency, measured by both elapsed time and memory consumption of the generated solutions. In conclusion, this work demonstrates the value of competitive programming benchmarks in evaluating large language models, especially in underrepresented languages. It also paves the way for future research on multilingual code generation and reasoning-enhanced models. The benchmark, data parsing, preparation, code generation, and evaluation scripts are available at https://huggingface.co/datasets/NLPForUA/ua-code-bench.",
      "tldr_zh": "æœ¬æ–‡ä»‹ç»äº†UA-Code-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å¼€æºåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¹Œå…‹å…°è¯­ç¯å¢ƒä¸‹çš„ä»£ç ç”Ÿæˆå’Œç«æŠ€ç¼–ç¨‹è§£é¢˜èƒ½åŠ›ã€‚è¯¥åŸºå‡†åŒ…å«æ¥è‡ªEolympå¹³å°çš„500ä¸ªé—®é¢˜ï¼Œå‡åŒ€åˆ†å¸ƒåœ¨ä»ææ˜“åˆ°æéš¾çš„äº”ä¸ªå¤æ‚æ€§çº§åˆ«ä¸­ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨ä¸“é—¨çš„Eolympç¯å¢ƒä¸­ï¼Œé€šè¿‡one-shotæç¤ºè®©13ç§é¢†å…ˆçš„ä¸“æœ‰å’Œå¼€æºæ¨¡å‹ç”ŸæˆPythonè§£å†³æ–¹æ¡ˆï¼Œå¹¶é’ˆå¯¹éšè—æµ‹è¯•ç”¨ä¾‹è¿›è¡Œäº†è¯„ä¼°ä»¥ç¡®ä¿ä»£ç æ­£ç¡®æ€§ã€‚ç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯OpenAI o3å’ŒGPT-5ç­‰é¡¶å°–æ¨¡å‹ä¹Ÿä»…èƒ½è§£å†³çº¦ä¸€åŠçš„é—®é¢˜ï¼Œçªæ˜¾äº†ä½èµ„æºè‡ªç„¶è¯­è¨€ä»£ç ç”Ÿæˆçš„æŒ‘æˆ˜æ€§ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¯¹ä¸åŒéš¾åº¦çº§åˆ«çš„è¡¨ç°è¿›è¡Œäº†ç»¼åˆåˆ†æï¼Œå¹¶è¯„ä¼°äº†ç”Ÿæˆè§£å†³æ–¹æ¡ˆçš„å”¯ä¸€æ€§ä»¥åŠè®¡ç®—æ•ˆç‡ï¼ˆè¿è¡Œæ—¶é—´å’Œå†…å­˜æ¶ˆè€—ï¼‰ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†ç«æŠ€ç¼–ç¨‹åŸºå‡†åœ¨è¯„ä¼°LLMsï¼ˆå°¤å…¶æ˜¯ä»£è¡¨æ€§ä¸è¶³çš„è¯­è¨€ï¼‰æ–¹é¢çš„ä»·å€¼ï¼Œå¹¶ä¸ºæœªæ¥çš„å¤šè¯­è¨€ä»£ç ç”Ÿæˆå’Œå¢å¼ºæ¨ç†æ¨¡å‹ç ”ç©¶é“ºå¹³äº†é“è·¯ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CL",
      "comment": "8 pages, 5 figures. XI International conference \"Informatics. Culture. Technique.\" (2025)",
      "pdf_url": "https://arxiv.org/pdf/2511.05040v1",
      "published_date": "2025-11-07 07:24:56 UTC",
      "updated_date": "2025-11-07 07:24:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:41:06.649416+00:00"
    },
    {
      "arxiv_id": "2511.05039v1",
      "title": "PECL: A Heterogeneous Parallel Multi-Domain Network for Radar-Based Human Activity Recognition",
      "title_zh": "PECLï¼šé¢å‘åŸºäºé›·è¾¾çš„äººä½“æ´»åŠ¨è¯†åˆ«çš„å¼‚æ„å¹¶è¡Œå¤šåŸŸç½‘ç»œ",
      "authors": [
        "Jiuqi Yan",
        "Chendong Xu",
        "Dongyu Liu"
      ],
      "abstract": "Radar systems are increasingly favored for medical applications because they provide non-intrusive monitoring with high privacy and robustness to lighting conditions. However, existing research typically relies on single-domain radar signals and overlooks the temporal dependencies inherent in human activity, which complicates the classification of similar actions. To address this issue, we designed the Parallel-EfficientNet-CBAM-LSTM (PECL) network to process data in three complementary domains: Range-Time, Doppler-Time, and Range-Doppler. PECL combines a channel-spatial attention module and temporal units to capture more features and dynamic dependencies during action sequences, improving both accuracy and robustness. The experimental results show that PECL achieves an accuracy of 96.16% on the same dataset, outperforming existing methods by at least 4.78%. PECL also performs best in distinguishing between easily confused actions. Despite its strong performance, PECL maintains moderate model complexity, with 23.42M parameters and 1324.82M FLOPs. Its parameter-efficient design further reduces computational cost.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é›·è¾¾äººä½“æ´»åŠ¨è¯†åˆ«ä¸­ç°æœ‰æ–¹æ³•ä¾èµ–å•åŸŸä¿¡å·ä¸”å¿½ç•¥æ—¶é—´ä¾èµ–æ€§çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºPECLï¼ˆParallel-EfficientNet-CBAM-LSTMï¼‰çš„å¼‚æ„å¹¶è¡Œå¤šåŸŸç½‘ç»œã€‚è¯¥ç½‘ç»œèƒ½å¤Ÿå¹¶è¡Œå¤„ç†Range-Timeã€Doppler-Timeå’ŒRange-Dopplerä¸‰ä¸ªäº’è¡¥åŸŸçš„æ•°æ®ï¼Œé€šè¿‡ç»“åˆé€šé“-ç©ºé—´æ³¨æ„åŠ›æ¨¡å—ï¼ˆchannel-spatial attention moduleï¼‰ä¸æ—¶é—´å•å…ƒï¼Œæœ‰æ•ˆæ•æ‰åŠ¨ä½œåºåˆ—ä¸­çš„ä¸°å¯Œç‰¹å¾åŠåŠ¨æ€ä¾èµ–å…³ç³»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPECLåœ¨æ•°æ®é›†ä¸Šå®ç°äº†96.16%çš„å‡†ç¡®ç‡ï¼Œä¼˜äºç°æœ‰æ–¹æ³•è‡³å°‘4.78%ï¼Œå¹¶åœ¨åŒºåˆ†æ˜“æ··æ·†åŠ¨ä½œæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶ï¼Œç»´æŒäº†é€‚åº¦çš„å‚æ•°é‡ï¼ˆ23.42Mï¼‰å’Œè®¡ç®—å¤æ‚åº¦ï¼Œå…·æœ‰è¾ƒé«˜çš„å‚æ•°æ•ˆç‡ã€‚",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05039v1",
      "published_date": "2025-11-07 07:22:36 UTC",
      "updated_date": "2025-11-07 07:22:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:44:16.820741+00:00"
    },
    {
      "arxiv_id": "2511.05034v1",
      "title": "Dynamic Residual Encoding with Slide-Level Contrastive Learning for End-to-End Whole Slide Image Representation",
      "title_zh": "é¢å‘ç«¯åˆ°ç«¯å…¨åˆ‡ç‰‡å›¾åƒè¡¨å¾çš„ç»“åˆåˆ‡ç‰‡çº§å¯¹æ¯”å­¦ä¹ çš„åŠ¨æ€æ®‹å·®ç¼–ç ",
      "authors": [
        "Jing Jin",
        "Xu Liu",
        "Te Gao",
        "Zhihong Shi",
        "Yixiong Liang",
        "Ruiqing Zheng",
        "Hulin Kuang",
        "Min Zeng",
        "Shichao Kan"
      ],
      "abstract": "Whole Slide Image (WSI) representation is critical for cancer subtyping, cancer recognition and mutation prediction.Training an end-to-end WSI representation model poses significant challenges, as a standard gigapixel slide can contain tens of thousands of image tiles, making it difficult to compute gradients of all tiles in a single mini-batch due to current GPU limitations. To address this challenge, we propose a method of dynamic residual encoding with slide-level contrastive learning (DRE-SLCL) for end-to-end WSI representation. Our approach utilizes a memory bank to store the features of tiles across all WSIs in the dataset. During training, a mini-batch usually contains multiple WSIs. For each WSI in the batch, a subset of tiles is randomly sampled and their features are computed using a tile encoder. Then, additional tile features from the same WSI are selected from the memory bank. The representation of each individual WSI is generated using a residual encoding technique that incorporates both the sampled features and those retrieved from the memory bank. Finally, the slide-level contrastive loss is computed based on the representations and histopathology reports ofthe WSIs within the mini-batch. Experiments conducted over cancer subtyping, cancer recognition, and mutation prediction tasks proved the effectiveness of the proposed DRE-SLCL method.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Whole Slide Image (WSI)åŒ…å«æ•°ä¸‡å›¾å—å¯¼è‡´éš¾ä»¥åœ¨ç°æœ‰GPUé™åˆ¶ä¸‹è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆåŠ¨æ€æ®‹å·®ç¼–ç ä¸åˆ‡ç‰‡çº§å¯¹æ¯”å­¦ä¹ çš„æ–¹æ³•(DRE-SLCL)ã€‚è¯¥æ–¹æ³•åˆ©ç”¨memory bankå­˜å‚¨æ•°æ®é›†ä¸­æ‰€æœ‰WSIçš„å›¾å—ç‰¹å¾ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éšæœºé‡‡æ ·éƒ¨åˆ†å›¾å—å¹¶é€šè¿‡tile encoderè®¡ç®—ç‰¹å¾ï¼ŒåŒæ—¶ä»memory bankä¸­æ£€ç´¢åŒä¸€WSIçš„é¢å¤–ç‰¹å¾ã€‚éšåï¼Œé€šè¿‡residual encodingæŠ€æœ¯èåˆå®æ—¶è®¡ç®—ä¸æ£€ç´¢åˆ°çš„ç‰¹å¾æ¥ç”ŸæˆWSIè¡¨ç¤ºï¼Œå¹¶ç»“åˆç—…ç†æŠ¥å‘Šè®¡ç®—slide-level contrastive lossã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDRE-SLCLåœ¨ç™Œç—‡äºšå‹åˆ†ç±»ã€ç™Œç—‡è¯†åˆ«å’Œçªå˜é¢„æµ‹ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºæ˜¾è‘—çš„æœ‰æ•ˆæ€§ï¼ŒæˆåŠŸè§£å†³äº†å‰åƒç´ çº§WSIçš„ç«¯åˆ°ç«¯è¡¨ç¤ºå­¦ä¹ éš¾é¢˜ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8pages, 3figures, published to ACM Digital Library",
      "pdf_url": "https://arxiv.org/pdf/2511.05034v1",
      "published_date": "2025-11-07 07:17:17 UTC",
      "updated_date": "2025-11-07 07:17:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:48:25.237433+00:00"
    },
    {
      "arxiv_id": "2512.20621v1",
      "title": "Cooperation Through Indirect Reciprocity in Child-Robot Interactions",
      "title_zh": "å„¿ç«¥-æœºå™¨äººäº¤äº’ä¸­åŸºäºé—´æ¥äº’æƒ çš„åˆä½œ",
      "authors": [
        "Isabel Neto",
        "Alexandre S. Pires",
        "Filipa Correia",
        "Fernando P. Santos"
      ],
      "abstract": "Social interactions increasingly involve artificial agents, such as conversational or collaborative bots. Understanding trust and prosociality in these settings is fundamental to improve human-AI teamwork. Research in biology and social sciences has identified mechanisms to sustain cooperation among humans. Indirect reciprocity (IR) is one of them. With IR, helping someone can enhance an individual's reputation, nudging others to reciprocate in the future. Transposing IR to human-AI interactions is however challenging, as differences in human demographics, moral judgements, and agents' learning dynamics can affect how interactions are assessed. To study IR in human-AI groups, we combine laboratory experiments and theoretical modelling. We investigate whether 1) indirect reciprocity can be transposed to children-robot interactions; 2) artificial agents can learn to cooperate given children's strategies; and 3) how differences in learning algorithms impact human-AI cooperation. We find that IR extends to children and robots solving coordination dilemmas. Furthermore, we observe that the strategies revealed by children provide a sufficient signal for multi-armed bandit algorithms to learn cooperative actions. Beyond the experimental scenarios, we observe that cooperating through multi-armed bandit algorithms is highly dependent on the strategies revealed by humans.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å„¿ç«¥ä¸æœºå™¨äººäº¤äº’(Child-Robot Interactions)ä¸­é€šè¿‡é—´æ¥äº’æƒ (Indirect Reciprocity, IR)æœºåˆ¶å»ºç«‹åˆä½œçš„å¯èƒ½æ€§ï¼Œæ—¨åœ¨è§£å†³äººæœºå›¢é˜Ÿåˆä½œä¸­çš„ä¿¡ä»»ä¸äº²ç¤¾ä¼šæ€§é—®é¢˜ã€‚é€šè¿‡ç»“åˆå®éªŒå®¤å®éªŒå’Œç†è®ºå»ºæ¨¡ï¼Œç ”ç©¶äººå‘˜è°ƒæŸ¥äº†IRæ˜¯å¦é€‚ç”¨äºå„¿ç«¥ä¸æœºå™¨äººç¾¤ä½“ï¼Œä»¥åŠäººå·¥æ™ºèƒ½ä»£ç†èƒ½å¦æ ¹æ®å„¿ç«¥çš„ç­–ç•¥å­¦ä¹ åˆä½œã€‚ç»“æœè¡¨æ˜ï¼Œé—´æ¥äº’æƒ æœºåˆ¶èƒ½å¤Ÿæ‰©å±•åˆ°è§£å†³å„¿ç«¥ä¸æœºå™¨äººçš„åè°ƒå›°å¢ƒä¸­ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°å„¿ç«¥å±•ç°å‡ºçš„ç­–ç•¥èƒ½å¤Ÿä¸ºå¤šè‡‚è€è™æœºç®—æ³•(multi-armed bandit algorithms)æä¾›è¶³å¤Ÿçš„ä¿¡å·ï¼Œä½¿å…¶å­¦ä¹ åˆ°åˆä½œè¡Œä¸ºã€‚ç„¶è€Œï¼Œé€šè¿‡å¤šè‡‚è€è™æœºç®—æ³•å®ç°çš„åˆä½œæ•ˆæœé«˜åº¦ä¾èµ–äºäººç±»å‚ä¸è€…æ‰€æ­ç¤ºçš„å…·ä½“ç­–ç•¥ã€‚è¿™é¡¹å·¥ä½œä¸ºç†è§£äººæœºäº¤äº’ä¸­çš„åˆä½œåŠ¨åŠ›å­¦æä¾›äº†æ–°çš„è§†è§’ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "16 pages + 5 pages of references; 4 figures; 1 table; accepted for publication in Proceedings of the Royal Society A (in press)",
      "pdf_url": "https://arxiv.org/pdf/2512.20621v1",
      "published_date": "2025-11-07 07:08:32 UTC",
      "updated_date": "2025-11-07 07:08:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:45:05.427939+00:00"
    },
    {
      "arxiv_id": "2511.05028v1",
      "title": "OvA-LP: A Simple and Efficient Framework for Federated Learning on Non-IID Data",
      "title_zh": "OvA-LPï¼šé¢å‘éç‹¬ç«‹åŒåˆ†å¸ƒæ•°æ®çš„ç®€å•é«˜æ•ˆè”é‚¦å­¦ä¹ æ¡†æ¶",
      "authors": [
        "Dongjin Park",
        "Hasung Yeo",
        "Joon-Woo Lee"
      ],
      "abstract": "Federated fine-tuning (FFT) adapts foundation models to decentralized data but remains fragile under heterogeneous client distributions due to local drift, i.e., client-level update divergences that induce systematic bias and amplified variance in the global model. Existing aggregation and personalization methods largely correct drift post hoc, which proves brittle under extreme non-IID conditions. We introduce OvA-LP, a minimalist framework that is, to our knowledge, the first explicitly designed to suppress drift at its source within the PEFT-based FFT paradigm. OvA-LP combines linear probing on a frozen encoder with a one-vs-all head and a simple two-stage procedure, preserving pretrained feature geometry and decoupling logits to prevent the mechanisms that amplify drift. On CIFAR-100 with 100 clients, averaged over shard-1, shard-2, and Bernoulli-Dirichlet partitions, OvA-LP retains 95.9% of its IID accuracy, whereas state-of-the-art FFT baselines retain only 10.1% (PFPT) and 34.5% (FFT-MoE) under the same conditions. OvA-LP further maintains resilience under both symmetric and asymmetric label noise. In addition, precomputing encoder features makes per-round cost nearly independent of encoder size. Together, these results demonstrate that OvA-LP provides a principled and efficient basis for robust FFT under heterogeneity.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è”é‚¦å¾®è°ƒ(FFT)åœ¨å¼‚æ„å®¢æˆ·ç«¯åˆ†å¸ƒä¸‹å› å±€éƒ¨æ¼‚ç§»(local drift)å¯¼è‡´æ¨¡å‹è„†å¼±çš„é—®é¢˜ï¼Œæå‡ºäº†OvA-LPæ¡†æ¶ã€‚è¿™æ˜¯é¦–ä¸ªåœ¨åŸºäºPEFTçš„FFTèŒƒå¼ä¸­æ—¨åœ¨ä»æºå¤´æŠ‘åˆ¶æ¼‚ç§»çš„æç®€æ¡†æ¶ã€‚OvA-LPé€šè¿‡åœ¨å†»ç»“ç¼–ç å™¨ä¸Šç»“åˆçº¿æ€§æ¢æµ‹(linear probing)ä¸one-vs-allåˆ†ç±»å¤´ï¼Œåˆ©ç”¨ç®€å•çš„ä¸¤é˜¶æ®µè¿‡ç¨‹ä¿æŒé¢„è®­ç»ƒç‰¹å¾å‡ ä½•ç»“æ„å¹¶è§£è€¦logitsï¼Œä»è€Œé˜»æ­¢æ¼‚ç§»æ”¾å¤§æœºåˆ¶ã€‚åœ¨CIFAR-100ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼ŒOvA-LPä¿ç•™äº†95.9%çš„IIDå‡†ç¡®ç‡ï¼Œè€ŒSOTAåŸºçº¿ï¼ˆå¦‚PFPTå’ŒFFT-MoEï¼‰ä»…ä¿ç•™äº†10.1%å’Œ34.5%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨æ ‡ç­¾å™ªå£°ä¸‹è¡¨ç°å‡ºé²æ£’æ€§ï¼Œå¹¶é€šè¿‡é¢„è®¡ç®—ç¼–ç å™¨ç‰¹å¾å®ç°äº†é«˜æ•ˆçš„æ¯è½®è®¡ç®—æˆæœ¬ï¼Œä¸ºå¼‚æ„ç¯å¢ƒä¸‹çš„é²æ£’FFTæä¾›äº†åŸåˆ™æ€§åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05028v1",
      "published_date": "2025-11-07 07:00:20 UTC",
      "updated_date": "2025-11-07 07:00:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:45:32.077977+00:00"
    },
    {
      "arxiv_id": "2511.05025v1",
      "title": "8bit-GPT: Exploring Human-AI Interaction on Obsolete Macintosh Operating Systems",
      "title_zh": "8bit-GPTï¼šåŸºäºè¿‡æ—¶ Macintosh æ“ä½œç³»ç»Ÿçš„äººä¸AIäº¤äº’æ¢ç´¢",
      "authors": [
        "Hala Sheta"
      ],
      "abstract": "The proliferation of assistive chatbots offering efficient, personalized communication has driven widespread over-reliance on them for decision-making, information-seeking and everyday tasks. This dependence was found to have adverse consequences on information retention as well as lead to superficial emotional attachment. As such, this work introduces 8bit-GPT; a language model simulated on a legacy Macintosh Operating System, to evoke reflection on the nature of Human-AI interaction and the consequences of anthropomorphic rhetoric. Drawing on reflective design principles such as slow-technology and counterfunctionality, this work aims to foreground the presence of chatbots as a tool by defamiliarizing the interface and prioritizing inefficient interaction, creating a friction between the familiar and not.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹å½“å‰äººä»¬è¿‡åº¦ä¾èµ–è¾…åŠ©èŠå¤©æœºå™¨äººå¯¼è‡´çš„ä¿¡æ¯ä¿ç•™èƒ½åŠ›ä¸‹é™å’Œè‚¤æµ…æƒ…æ„Ÿä¾æ‹ç­‰é—®é¢˜ï¼Œä»‹ç»äº†8bit-GPTï¼Œä¸€ç§åœ¨è¿‡æ—¶çš„Macintoshæ“ä½œç³»ç»Ÿä¸Šæ¨¡æ‹Ÿçš„è¯­è¨€æ¨¡å‹ã€‚è¯¥é¡¹ç›®æ—¨åœ¨å¼•å‘äººä»¬å¯¹äººæœºäº¤äº’(Human-AI interaction)æœ¬è´¨ä»¥åŠæ‹ŸäººåŒ–ä¿®è¾åæœçš„æ·±åˆ»åæ€ã€‚é€šè¿‡å€Ÿé‰´æ…¢æŠ€æœ¯(slow-technology)å’ŒååŠŸèƒ½æ€§(counterfunctionality)ç­‰åæ€æ€§è®¾è®¡åŸåˆ™ï¼Œè¯¥å·¥ä½œé€šè¿‡é™Œç”ŸåŒ–ç•Œé¢å’Œä¼˜å…ˆè€ƒè™‘ä½æ•ˆäº¤äº’ï¼Œåœ¨ç†Ÿæ‚‰ä¸é™Œç”Ÿä¹‹é—´åˆ¶é€ æ‘©æ“¦ã€‚è¿™ç§è®¾è®¡ç­–ç•¥æ—¨åœ¨å°†èŠå¤©æœºå™¨äººä½œä¸ºå·¥å…·çš„å­˜åœ¨å‡¸æ˜¾å‡ºæ¥ï¼Œä»è€Œæ‰“ç ´ç”¨æˆ·å¯¹AIçš„æ— æ„è¯†ä¾èµ–å’Œæ‹ŸäººåŒ–æƒ³è±¡ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "NeurIPS Creative AI Track 2025: Humanity",
      "pdf_url": "https://arxiv.org/pdf/2511.05025v1",
      "published_date": "2025-11-07 06:56:04 UTC",
      "updated_date": "2025-11-07 06:56:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:46:03.040443+00:00"
    },
    {
      "arxiv_id": "2511.05018v1",
      "title": "Pluralistic Behavior Suite: Stress-Testing Multi-Turn Adherence to Custom Behavioral Policies",
      "title_zh": "Pluralistic Behavior Suiteï¼šå¯¹è‡ªå®šä¹‰è¡Œä¸ºç­–ç•¥å¤šè½®éµå¾ªèƒ½åŠ›çš„å‹åŠ›æµ‹è¯•",
      "authors": [
        "Prasoon Varshney",
        "Makesh Narsimhan Sreedhar",
        "Liwei Jiang",
        "Traian Rebedea",
        "Christopher Parisien"
      ],
      "abstract": "Large language models (LLMs) are typically aligned to a universal set of safety and usage principles intended for broad public acceptability. Yet, real-world applications of LLMs often take place within organizational ecosystems shaped by distinctive corporate policies, regulatory requirements, use cases, brand guidelines, and ethical commitments. This reality highlights the need for rigorous and comprehensive evaluation of LLMs with pluralistic alignment goals, an alignment paradigm that emphasizes adaptability to diverse user values and needs. In this work, we present PLURALISTIC BEHAVIOR SUITE (PBSUITE), a dynamic evaluation suite designed to systematically assess LLMs' capacity to adhere to pluralistic alignment specifications in multi-turn, interactive conversations. PBSUITE consists of (1) a diverse dataset of 300 realistic LLM behavioral policies, grounded in 30 industries; and (2) a dynamic evaluation framework for stress-testing model compliance with custom behavioral specifications under adversarial conditions. Using PBSUITE, We find that leading open- and closed-source LLMs maintain robust adherence to behavioral policies in single-turn settings (less than 4% failure rates), but their compliance weakens substantially in multi-turn adversarial interactions (up to 84% failure rates). These findings highlight that existing model alignment and safety moderation methods fall short in coherently enforcing pluralistic behavioral policies in real-world LLM interactions. Our work contributes both the dataset and analytical framework to support future research toward robust and context-aware pluralistic alignment techniques.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨å®é™…åº”ç”¨ä¸­éœ€é€‚åº”ç‰¹å®šä¼ä¸šæ”¿ç­–ã€æ³•è§„åŠé“å¾·æ‰¿è¯ºçš„éœ€æ±‚ï¼Œæå‡ºäº†PLURALISTIC BEHAVIOR SUITE (PBSUITE)ï¼Œæ—¨åœ¨ç³»ç»Ÿè¯„ä¼°æ¨¡å‹åœ¨å¤šè½®äº¤äº’ä¸­å¯¹å¤šå…ƒåŒ–å¯¹é½(pluralistic alignment)ç›®æ ‡çš„ä¾ä»æ€§ã€‚PBSUITEåŒ…å«æ¶µç›–30ä¸ªè¡Œä¸šçš„300ä¸ªç°å®è¡Œä¸ºç­–ç•¥æ•°æ®é›†ï¼Œä»¥åŠä¸€ä¸ªç”¨äºåœ¨å¯¹æŠ—æ¡ä»¶ä¸‹å¯¹æ¨¡å‹åˆè§„æ€§è¿›è¡Œå‹åŠ›æµ‹è¯•çš„åŠ¨æ€è¯„ä¼°æ¡†æ¶ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡é¢†å…ˆçš„å¼€æºå’Œé—­æºLLMåœ¨å•è½®è®¾ç½®ä¸­èƒ½è¾ƒå¥½åœ°éµå®ˆè¡Œä¸ºç­–ç•¥ï¼ˆå¤±è´¥ç‡ä½äº4%ï¼‰ï¼Œä½†åœ¨å¤šè½®å¯¹æŠ—æ€§äº¤äº’ä¸­å…¶ä¾ä»æ€§æ˜¾è‘—å‡å¼±ï¼ˆå¤±è´¥ç‡é«˜è¾¾84%ï¼‰ã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†ç°æœ‰æ¨¡å‹å¯¹é½å’Œå®‰å…¨å®¡æ ¸æ–¹æ³•åœ¨è¿è´¯æ‰§è¡Œå¤šå…ƒåŒ–è¡Œä¸ºç­–ç•¥æ–¹é¢çš„ä¸è¶³ã€‚è¯¥å·¥ä½œé€šè¿‡æä¾›æ•°æ®é›†å’Œåˆ†ææ¡†æ¶ï¼Œä¸ºæœªæ¥å¼€å‘é²æ£’ä¸”å…·æœ‰ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›çš„å¤šå…ƒåŒ–å¯¹é½æŠ€æœ¯æä¾›äº†æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at the Multi-Turn Interactions workshop at the 39th Conference on Neural Information Processing Systems (NeurIPS 2025)",
      "pdf_url": "https://arxiv.org/pdf/2511.05018v1",
      "published_date": "2025-11-07 06:43:01 UTC",
      "updated_date": "2025-11-07 06:43:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:46:32.974348+00:00"
    },
    {
      "arxiv_id": "2511.05005v1",
      "title": "Multi-agent Coordination via Flow Matching",
      "title_zh": "åŸºäºæµåŒ¹é…çš„å¤šæ™ºèƒ½ä½“ååŒ",
      "authors": [
        "Dongsu Lee",
        "Daehee Lee",
        "Amy Zhang"
      ],
      "abstract": "This work presents MAC-Flow, a simple yet expressive framework for multi-agent coordination. We argue that requirements of effective coordination are twofold: (i) a rich representation of the diverse joint behaviors present in offline data and (ii) the ability to act efficiently in real time. However, prior approaches often sacrifice one for the other, i.e., denoising diffusion-based solutions capture complex coordination but are computationally slow, while Gaussian policy-based solutions are fast but brittle in handling multi-agent interaction. MAC-Flow addresses this trade-off by first learning a flow-based representation of joint behaviors, and then distilling it into decentralized one-step policies that preserve coordination while enabling fast execution. Across four different benchmarks, including $12$ environments and $34$ datasets, MAC-Flow alleviates the trade-off between performance and computational cost, specifically achieving about $\\boldsymbol{\\times14.5}$ faster inference compared to diffusion-based MARL methods, while maintaining good performance. At the same time, its inference speed is similar to that of prior Gaussian policy-based offline multi-agent reinforcement learning (MARL) methods.",
      "tldr_zh": "æœ¬æ–‡æå‡ºäº†MAC-Flowï¼Œä¸€ç§ç®€å•è€Œé«˜æ•ˆçš„å¤šæ™ºèƒ½ä½“åä½œæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨ä¸°å¯Œè”åˆè¡Œä¸ºè¡¨ç¤ºä¸å®æ—¶æ‰§è¡Œæ•ˆç‡ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚ç°æœ‰çš„åŸºäºå»å™ªæ‰©æ•£æ¨¡å‹(denoising diffusion)çš„æ–¹æ³•è™½ç„¶èƒ½æ•æ‰å¤æ‚çš„åä½œä½†è®¡ç®—ç¼“æ…¢ï¼Œè€ŒåŸºäºé«˜æ–¯ç­–ç•¥(Gaussian policy)çš„æ–¹æ³•è™½ç„¶é€Ÿåº¦å¿«ä½†åœ¨å¤„ç†å¤šæ™ºèƒ½ä½“äº¤äº’æ—¶è¡¨ç°è„†å¼±ã€‚MAC-Flowé€šè¿‡é¦–å…ˆå­¦ä¹ è”åˆè¡Œä¸ºçš„åŸºäºæµ(flow-based)çš„è¡¨ç¤ºï¼Œç„¶åå°†å…¶è’¸é¦ä¸ºå»ä¸­å¿ƒåŒ–çš„å•æ­¥ç­–ç•¥ï¼Œä»è€Œåœ¨ä¿æŒåä½œèƒ½åŠ›çš„åŒæ—¶å®ç°å¿«é€Ÿæ‰§è¡Œã€‚åœ¨åŒ…å«12ä¸ªç¯å¢ƒå’Œ34ä¸ªæ•°æ®é›†çš„å››ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒMAC-Flowæœ‰æ•ˆç¼“è§£äº†æ€§èƒ½ä¸è®¡ç®—æˆæœ¬ä¹‹é—´çš„çŸ›ç›¾ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMAC-Flowçš„æ¨ç†é€Ÿåº¦æ¯”åŸºäºæ‰©æ•£çš„ç¦»çº¿å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ (MARL)æ–¹æ³•å¿«çº¦14.5å€ï¼Œä¸”ä¿æŒäº†è‰¯å¥½çš„æ€§èƒ½ï¼ŒåŒæ—¶å…¶æ¨ç†é€Ÿåº¦ä¸ä¼ ç»ŸåŸºäºé«˜æ–¯ç­–ç•¥çš„æ–¹æ³•ç›¸å½“ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05005v1",
      "published_date": "2025-11-07 06:24:32 UTC",
      "updated_date": "2025-11-07 06:24:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:46:48.011268+00:00"
    },
    {
      "arxiv_id": "2511.05000v1",
      "title": "Query Generation Pipeline with Enhanced Answerability Assessment for Financial Information Retrieval",
      "title_zh": "ç»“åˆå¢å¼ºå‹å¯å›ç­”æ€§è¯„ä¼°çš„é‡‘èä¿¡æ¯æ£€ç´¢æŸ¥è¯¢ç”Ÿæˆæµç¨‹",
      "authors": [
        "Hyunkyu Kim",
        "Yeeun Yoo",
        "Youngjun Kwak"
      ],
      "abstract": "As financial applications of large language models (LLMs) gain attention, accurate Information Retrieval (IR) remains crucial for reliable AI services. However, existing benchmarks fail to capture the complex and domain-specific information needs of real-world banking scenarios. Building domain-specific IR benchmarks is costly and constrained by legal restrictions on using real customer data. To address these challenges, we propose a systematic methodology for constructing domain-specific IR benchmarks through LLM-based query generation. As a concrete implementation of this methodology, our pipeline combines single and multi-document query generation with an enhanced and reasoning-augmented answerability assessment method, achieving stronger alignment with human judgments than prior approaches. Using this methodology, we construct KoBankIR, comprising 815 queries derived from 204 official banking documents. Our experiments show that existing retrieval models struggle with the complex multi-document queries in KoBankIR, demonstrating the value of our systematic approach for domain-specific benchmark construction and underscoring the need for improved retrieval techniques in financial domains.",
      "tldr_zh": "é’ˆå¯¹é‡‘èé¢†åŸŸå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åº”ç”¨ä¸­ç°æœ‰ä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰åŸºå‡†éš¾ä»¥æ•æ‰çœŸå®é“¶è¡Œåœºæ™¯å¤æ‚æ€§ä¸”æ„å»ºæˆæœ¬é«˜æ˜‚çš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºLLMæŸ¥è¯¢ç”Ÿæˆçš„ç³»ç»ŸåŒ–åŸºå‡†æ„å»ºæ–¹æ³•ã€‚è¯¥ç®¡é“ç»“åˆäº†å•æ–‡æ¡£å’Œå¤šæ–‡æ¡£æŸ¥è¯¢ç”ŸæˆæŠ€æœ¯ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§å¢å¼ºçš„æ¨ç†å¢å¼ºå‹å¯å›ç­”æ€§è¯„ä¼°ï¼ˆanswerability assessmentï¼‰æ–¹æ³•ï¼Œå®ç°äº†æ¯”ä»¥å¾€æ–¹æ³•æ›´ç¬¦åˆäººç±»åˆ¤æ–­çš„è¯„ä¼°æ•ˆæœã€‚åŸºäºæ­¤æ–¹æ³•ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†KoBankIRæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«ä»204ä»½å®˜æ–¹é“¶è¡Œæ–‡æ¡£ä¸­ç”Ÿæˆçš„815ä¸ªæŸ¥è¯¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„æ£€ç´¢æ¨¡å‹åœ¨å¤„ç†KoBankIRä¸­å¤æ‚çš„å¤šæ–‡æ¡£æŸ¥è¯¢æ—¶è¡¨ç°æŒ£æ‰ï¼Œéš¾ä»¥æ»¡è¶³éœ€æ±‚ã€‚è¿™ä¸€å‘ç°ä¸ä»…è¯æ˜äº†è¯¥ç³»ç»ŸåŒ–æ–¹æ³•åœ¨æ„å»ºç‰¹å®šé¢†åŸŸåŸºå‡†æ–¹é¢çš„ä»·å€¼ï¼Œä¹Ÿå‡¸æ˜¾äº†é‡‘èé¢†åŸŸè¿«åˆ‡éœ€è¦æ”¹è¿›æ£€ç´¢æŠ€æœ¯çš„ç°çŠ¶ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted(Oral) by ICAIF 2025. Hyunkyu Kim and Yeeun Yoo contributed equally to this work",
      "pdf_url": "https://arxiv.org/pdf/2511.05000v1",
      "published_date": "2025-11-07 06:06:09 UTC",
      "updated_date": "2025-11-07 06:06:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:47:51.273463+00:00"
    },
    {
      "arxiv_id": "2511.04998v1",
      "title": "BiPETE: A Bi-Positional Embedding Transformer Encoder for Risk Assessment of Alcohol and Substance Use Disorder with Electronic Health Records",
      "title_zh": "BiPETEï¼šåŸºäºç”µå­å¥åº·è®°å½•çš„é…’ç²¾ä¸ç‰©è´¨ä½¿ç”¨éšœç¢é£é™©è¯„ä¼°çš„åŒä½ç½®åµŒå…¥ Transformer ç¼–ç å™¨",
      "authors": [
        "Daniel S. Lee",
        "Mayra S. Haedo-Cruz",
        "Chen Jiang",
        "Oshin Miranda",
        "LiRong Wang"
      ],
      "abstract": "Transformer-based deep learning models have shown promise for disease risk prediction using electronic health records(EHRs), but modeling temporal dependencies remains a key challenge due to irregular visit intervals and lack of uniform structure. We propose a Bi-Positional Embedding Transformer Encoder or BiPETE for single-disease prediction, which integrates rotary positional embeddings to encode relative visit timing and sinusoidal embeddings to preserve visit order. Without relying on large-scale pretraining, BiPETE is trained on EHR data from two mental health cohorts-depressive disorder and post-traumatic stress disorder (PTSD)-to predict the risk of alcohol and substance use disorders (ASUD). BiPETE outperforms baseline models, improving the area under the precision-recall curve (AUPRC) by 34% and 50% in the depression and PTSD cohorts, respectively. An ablation study further confirms the effectiveness of the dual positional encoding strategy. We apply the Integrated Gradients method to interpret model predictions, identifying key clinical features associated with ASUD risk and protection, such as abnormal inflammatory, hematologic, and metabolic markers, as well as specific medications and comorbidities. Overall, these key clinical features identified by the attribution methods contribute to a deeper understanding of the risk assessment process and offer valuable clues for mitigating potential risks. In summary, our study presents a practical and interpretable framework for disease risk prediction using EHR data, which can achieve strong performance.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†BiPETEï¼Œä¸€ç§åŒä½ç½®åµŒå…¥Transformerç¼–ç å™¨ï¼Œæ—¨åœ¨åˆ©ç”¨ç”µå­å¥åº·è®°å½•(EHRs)é¢„æµ‹é…’ç²¾å’Œè¯ç‰©æ»¥ç”¨éšœç¢(ASUD)çš„é£é™©ã€‚é’ˆå¯¹EHRæ•°æ®ä¸­å°±è¯Šé—´éš”ä¸è§„å¾‹åŠç»“æ„ä¸ç»Ÿä¸€çš„æŒ‘æˆ˜ï¼Œè¯¥æ¨¡å‹åˆ›æ–°æ€§åœ°ç»“åˆäº†æ—‹è½¬ä½ç½®åµŒå…¥(rotary positional embeddings)ä»¥ç¼–ç ç›¸å¯¹å°±è¯Šæ—¶é—´ï¼Œä»¥åŠæ­£å¼¦åµŒå…¥(sinusoidal embeddings)ä»¥ä¿æŒå°±è¯Šé¡ºåºã€‚åœ¨ä¸ä¾èµ–å¤§è§„æ¨¡é¢„è®­ç»ƒçš„æƒ…å†µä¸‹ï¼ŒBiPETEåœ¨æŠ‘éƒç—‡å’Œåˆ›ä¼¤ååº”æ¿€éšœç¢(PTSD)ä¸¤ä¸ªé˜Ÿåˆ—ä¸Šè¿›è¡Œäº†è®­ç»ƒå’ŒéªŒè¯ï¼Œç»“æœæ˜¾ç¤ºå…¶æ€§èƒ½æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ï¼ŒAUPRCåˆ†åˆ«æé«˜äº†34%å’Œ50%ã€‚æ­¤å¤–ï¼Œç ”ç©¶åº”ç”¨ç§¯åˆ†æ¢¯åº¦æ³•(Integrated Gradients)æé«˜äº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œè¯†åˆ«å‡ºäº†å¦‚å¼‚å¸¸ç‚ç—‡æ ‡å¿—ç‰©ã€ç‰¹å®šè¯ç‰©åŠåˆå¹¶ç—‡ç­‰å…³é”®ä¸´åºŠç‰¹å¾ã€‚è¿™ä¸€æˆæœå±•ç¤ºäº†ä¸€ä¸ªå®ç”¨ä¸”é«˜æ€§èƒ½çš„ç–¾ç—…é£é™©é¢„æµ‹æ¡†æ¶ï¼Œæœ‰åŠ©äºæ·±å…¥ç†è§£ä¸´åºŠé£é™©å› ç´ å¹¶è¾…åŠ©åŒ»ç–—å†³ç­–ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG",
      "comment": "20 pages, 2 figures, 6 tables, 2 supplementary figures, 4 supplementary tables, submitted to Journal of Biomedical Informatics on 6 Nov, 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.04998v1",
      "published_date": "2025-11-07 06:01:25 UTC",
      "updated_date": "2025-11-07 06:01:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:48:05.013151+00:00"
    },
    {
      "arxiv_id": "2511.04995v1",
      "title": "Enhancing Public Speaking Skills in Engineering Students Through AI",
      "title_zh": "åˆ©ç”¨äººå·¥æ™ºèƒ½æå‡å·¥ç§‘å­¦ç”Ÿçš„å…¬ä¼—æ¼”è®²æŠ€èƒ½",
      "authors": [
        "Amol Harsh",
        "Brainerd Prince",
        "Siddharth Siddharth",
        "Deepan Raj Prabakar Muthirayan",
        "Kabir S Bhalla",
        "Esraaj Sarkar Gupta",
        "Siddharth Sahu"
      ],
      "abstract": "This research-to-practice full paper was inspired by the persistent challenge in effective communication among engineering students. Public speaking is a necessary skill for future engineers as they have to communicate technical knowledge with diverse stakeholders. While universities offer courses or workshops, they are unable to offer sustained and personalized training to students. Providing comprehensive feedback on both verbal and non-verbal aspects of public speaking is time-intensive, making consistent and individualized assessment impractical. This study integrates research on verbal and non-verbal cues in public speaking to develop an AI-driven assessment model for engineering students. Our approach combines speech analysis, computer vision, and sentiment detection into a multi-modal AI system that provides assessment and feedback. The model evaluates (1) verbal communication (pitch, loudness, pacing, intonation), (2) non-verbal communication (facial expressions, gestures, posture), and (3) expressive coherence, a novel integration ensuring alignment between speech and body language. Unlike previous systems that assess these aspects separately, our model fuses multiple modalities to deliver personalized, scalable feedback. Preliminary testing demonstrated that our AI-generated feedback was moderately aligned with expert evaluations. Among the state-of-the-art AI models evaluated, all of which were Large Language Models (LLMs), including Gemini and OpenAI models, Gemini Pro emerged as the best-performing, showing the strongest agreement with human annotators. By eliminating reliance on human evaluators, this AI-driven public speaking trainer enables repeated practice, helping students naturally align their speech with body language and emotion, crucial for impactful and professional communication.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹å·¥ç¨‹ä¸“ä¸šå­¦ç”Ÿåœ¨å…¬ä¼—æ¼”è®²æŠ€èƒ½æ–¹é¢çš„ä¸è¶³ï¼Œå¼€å‘äº†ä¸€ç§åŸºäºAIçš„å¤šæ¨¡æ€(multi-modal)è¯„ä¼°æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ•™å­¦éš¾ä»¥æä¾›æŒç»­ä¸”ä¸ªæ€§åŒ–åé¦ˆçš„éš¾é¢˜ã€‚è¯¥ç³»ç»Ÿèåˆäº†è¯­éŸ³åˆ†æã€è®¡ç®—æœºè§†è§‰(computer vision)å’Œæƒ…æ„Ÿæ£€æµ‹æŠ€æœ¯ï¼Œèƒ½å¤Ÿç»¼åˆè¯„ä¼°å£å¤´äº¤æµï¼ˆå¦‚éŸ³é«˜ã€å“åº¦ï¼‰ã€éå£å¤´äº¤æµï¼ˆå¦‚é¢éƒ¨è¡¨æƒ…ã€å§¿åŠ¿ï¼‰ä»¥åŠç¡®ä¿è¯­è¨€ä¸è‚¢ä½“åŠ¨ä½œåè°ƒçš„â€œè¡¨ç°ä¸€è‡´æ€§â€(expressive coherence)ã€‚ä¸ä»¥å¾€å•ä¸€ç»´åº¦çš„è¯„ä¼°ç³»ç»Ÿä¸åŒï¼Œè¯¥æ¨¡å‹é€šè¿‡èåˆå¤šç§æ¨¡æ€æä¾›å¯æ‰©å±•çš„åé¦ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAIç”Ÿæˆçš„åé¦ˆä¸ä¸“å®¶è¯„ä¼°å…·æœ‰ä¸­ç­‰ç¨‹åº¦çš„ä¸€è‡´æ€§ï¼Œè€Œåœ¨å—æµ‹çš„å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)ä¸­ï¼ŒGemini Proè¡¨ç°æœ€ä½³ï¼Œæ˜¾ç¤ºå‡ºä¸äººç±»æ ‡æ³¨è€…æœ€å¼ºçš„ä¸€è‡´æ€§ã€‚è¯¥å·¥å…·æ¶ˆé™¤äº†å¯¹äººå·¥è¯„ä¼°è€…çš„ä¾èµ–ï¼Œæ”¯æŒå­¦ç”Ÿè¿›è¡Œé‡å¤ç»ƒä¹ ï¼Œæœ‰åŠ©äºå…¶è‡ªç„¶åœ°åè°ƒæ¼”è®²å†…å®¹ä¸æƒ…æ„Ÿè¡¨è¾¾ï¼Œä»è€Œå®ç°æ›´å…·å½±å“åŠ›çš„ä¸“ä¸šæ²Ÿé€šã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.04995v1",
      "published_date": "2025-11-07 05:44:15 UTC",
      "updated_date": "2025-11-07 05:44:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:48:27.068117+00:00"
    },
    {
      "arxiv_id": "2511.05630v1",
      "title": "BrainCSD: A Hierarchical Consistency-Driven MoE Foundation Model for Unified Connectome Synthesis and Multitask Brain Trait Prediction",
      "title_zh": "BrainCSDï¼šé¢å‘ç»Ÿä¸€è¿æ¥ç»„åˆæˆä¸å¤šä»»åŠ¡è„‘ç‰¹å¾é¢„æµ‹çš„åˆ†å±‚ä¸€è‡´æ€§é©±åŠ¨æ··åˆä¸“å®¶åŸºåº§æ¨¡å‹",
      "authors": [
        "Xiongri Shen",
        "Jiaqi Wang",
        "Yi Zhong",
        "Zhenxi Song",
        "Leilei Zhao",
        "Liling Li",
        "Yichen Wei",
        "Lingyan Liang",
        "Shuqiang Wang",
        "Baiying Lei",
        "Demao Deng",
        "Zhiguo Zhang"
      ],
      "abstract": "Functional and structural connectivity (FC/SC) are key multimodal biomarkers for brain analysis, yet their clinical utility is hindered by costly acquisition, complex preprocessing, and frequent missing modalities. Existing foundation models either process single modalities or lack explicit mechanisms for cross-modal and cross-scale consistency. We propose BrainCSD, a hierarchical mixture-of-experts (MoE) foundation model that jointly synthesizes FC/SC biomarkers and supports downstream decoding tasks (diagnosis and prediction). BrainCSD features three neuroanatomically grounded components: (1) a ROI-specific MoE that aligns regional activations from canonical networks (e.g., DMN, FPN) with a global atlas via contrastive consistency; (2) a Encoding-Activation MOE that models dynamic cross-time/gradient dependencies in fMRI/dMRI; and (3) a network-aware refinement MoE that enforces structural priors and symmetry at individual and population levels. Evaluated on the datasets under complete and missing-modality settings, BrainCSD achieves SOTA results: 95.6\\% accuracy for MCI vs. CN classification without FC, low synthesis error (FC RMSE: 0.038; SC RMSE: 0.006), brain age prediction (MAE: 4.04 years), and MMSE score estimation (MAE: 1.72 points). Code is available in \\href{https://github.com/SXR3015/BrainCSD}{BrainCSD}",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†BrainCSDï¼Œä¸€ç§åŸºäºåˆ†å±‚æ··åˆä¸“å®¶(MoE)çš„åŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³è„‘åˆ†æä¸­åŠŸèƒ½å’Œç»“æ„è¿æ¥(FC/SC)è·å–æˆæœ¬é«˜åŠæ¨¡æ€ç¼ºå¤±çš„éš¾é¢˜ã€‚BrainCSDé€šè¿‡è”åˆåˆæˆFC/SCç”Ÿç‰©æ ‡å¿—ç‰©ï¼Œæ”¯æŒä¸‹æ¸¸çš„è¯Šæ–­ä¸é¢„æµ‹ä»»åŠ¡ï¼Œå¹¶å¼•å…¥äº†ä¸‰ä¸ªç¥ç»è§£å‰–å­¦åŸºç¡€ç»„ä»¶æ¥ç¡®ä¿è·¨æ¨¡æ€å’Œè·¨å°ºåº¦çš„ä¸€è‡´æ€§ã€‚å…·ä½“æ–¹æ³•åŒ…æ‹¬ï¼šåˆ©ç”¨å¯¹æ¯”ä¸€è‡´æ€§å¯¹é½å…¸å‹ç½‘ç»œ(å¦‚DMN, FPN)çš„ROI-specific MoEï¼Œå»ºæ¨¡fMRI/dMRIåŠ¨æ€ä¾èµ–çš„Encoding-Activation MoEï¼Œä»¥åŠå¼ºåˆ¶æ‰§è¡Œç»“æ„å…ˆéªŒå’Œå¯¹ç§°æ€§çš„Network-aware refinement MoEã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBrainCSDåœ¨å®Œæ•´å’Œç¼ºå¤±æ¨¡æ€è®¾ç½®ä¸‹å‡è¾¾åˆ°äº†SOTAæ°´å¹³ï¼Œä¾‹å¦‚åœ¨ç¼ºå¤±FCçš„æƒ…å†µä¸‹MCIåˆ†ç±»å‡†ç¡®ç‡é«˜è¾¾95.6%ï¼ŒåŒæ—¶åœ¨è„‘é¾„é¢„æµ‹å’ŒMMSEè¯„åˆ†ä¼°è®¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸”å…·æœ‰æä½çš„åˆæˆè¯¯å·®ã€‚",
      "categories": [
        "q-bio.NC",
        "cs.AI"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05630v1",
      "published_date": "2025-11-07 04:40:47 UTC",
      "updated_date": "2025-11-07 04:40:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:51:45.219239+00:00"
    },
    {
      "arxiv_id": "2511.04970v1",
      "title": "Learning Fourier shapes to probe the geometric world of deep neural networks",
      "title_zh": "å­¦ä¹ å‚…é‡Œå¶å½¢çŠ¶ä»¥æ¢ç©¶æ·±åº¦ç¥ç»ç½‘ç»œçš„å‡ ä½•ä¸–ç•Œ",
      "authors": [
        "Jian Wang",
        "Yixing Yong",
        "Haixia Bi",
        "Lijun He",
        "Fan Li"
      ],
      "abstract": "While both shape and texture are fundamental to visual recognition, research on deep neural networks (DNNs) has predominantly focused on the latter, leaving their geometric understanding poorly probed. Here, we show: first, that optimized shapes can act as potent semantic carriers, generating high-confidence classifications from inputs defined purely by their geometry; second, that they are high-fidelity interpretability tools that precisely isolate a model's salient regions; and third, that they constitute a new, generalizable adversarial paradigm capable of deceiving downstream visual tasks. This is achieved through an end-to-end differentiable framework that unifies a powerful Fourier series to parameterize arbitrary shapes, a winding number-based mapping to translate them into the pixel grid required by DNNs, and signal energy constraints that enhance optimization efficiency while ensuring physically plausible shapes. Our work provides a versatile framework for probing the geometric world of DNNs and opens new frontiers for challenging and understanding machine perception.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦ç¥ç»ç½‘ç»œ(DNNs)åœ¨å‡ ä½•ç†è§£æ–¹é¢çš„ä¸è¶³ï¼Œæå‡ºäº†ä¸€ç§é€šè¿‡å­¦ä¹ å‚…é‡Œå¶å½¢çŠ¶(Fourier shapes)æ¥æ¢ç´¢æ¨¡å‹å‡ ä½•ä¸–ç•Œçš„ç«¯åˆ°ç«¯å¯å¾®æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å‚…é‡Œå¶çº§æ•°å‚æ•°åŒ–ä»»æ„å½¢çŠ¶ï¼Œé€šè¿‡åŸºäºå·ç»•æ•°(winding number)çš„æ˜ å°„å°†å…¶è½¬æ¢ä¸ºåƒç´ ç½‘æ ¼ï¼Œå¹¶å¼•å…¥ä¿¡å·èƒ½é‡çº¦æŸä»¥ç¡®ä¿å½¢çŠ¶çš„ç‰©ç†åˆç†æ€§å’Œä¼˜åŒ–æ•ˆç‡ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä¼˜åŒ–åçš„å½¢çŠ¶ä¸ä»…èƒ½ä½œä¸ºå¼ºæœ‰åŠ›çš„è¯­ä¹‰è½½ä½“ï¼Œä»…å‡­å‡ ä½•ç‰¹å¾ç”Ÿæˆé«˜ç½®ä¿¡åº¦åˆ†ç±»ï¼Œè¿˜èƒ½ä½œä¸ºé«˜ä¿çœŸçš„å¯è§£é‡Šæ€§å·¥å…·ç²¾ç¡®åˆ†ç¦»æ¨¡å‹çš„æ˜¾è‘—åŒºåŸŸã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜æ„æˆäº†ä¸€ç§æ–°çš„é€šç”¨å¯¹æŠ—èŒƒå¼ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ¬ºéª—ä¸‹æ¸¸è§†è§‰ä»»åŠ¡ã€‚è¿™é¡¹å·¥ä½œä¸ºæ¢æµ‹DNNsçš„å‡ ä½•æ„ŸçŸ¥èƒ½åŠ›æä¾›äº†é€šç”¨æ¡†æ¶ï¼Œä¸ºç†è§£å’ŒæŒ‘æˆ˜æœºå™¨æ„ŸçŸ¥å¼€è¾Ÿäº†æ–°å‰æ²¿ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "20 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.04970v1",
      "published_date": "2025-11-07 04:13:11 UTC",
      "updated_date": "2025-11-07 04:13:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:52:02.386495+00:00"
    },
    {
      "arxiv_id": "2511.04963v1",
      "title": "Pattern-Aware Diffusion Synthesis of fMRI/dMRI with Tissue and Microstructural Refinement",
      "title_zh": "èåˆç»„ç»‡ä¸å¾®è§‚ç»“æ„ç»†åŒ–çš„fMRI/dMRIæ¨¡å¼æ„ŸçŸ¥æ‰©æ•£åˆæˆ",
      "authors": [
        "Xiongri Shen",
        "Jiaqi Wang",
        "Yi Zhong",
        "Zhenxi Song",
        "Leilei Zhao",
        "Yichen Wei",
        "Lingyan Liang",
        "Shuqiang Wang",
        "Baiying Lei",
        "Demao Deng",
        "Zhiguo Zhang"
      ],
      "abstract": "Magnetic resonance imaging (MRI), especially functional MRI (fMRI) and diffusion MRI (dMRI), is essential for studying neurodegenerative diseases. However, missing modalities pose a major barrier to their clinical use. Although GAN- and diffusion model-based approaches have shown some promise in modality completion, they remain limited in fMRI-dMRI synthesis due to (1) significant BOLD vs. diffusion-weighted signal differences between fMRI and dMRI in time/gradient axis, and (2) inadequate integration of disease-related neuroanatomical patterns during generation. To address these challenges, we propose PDS, introducing two key innovations: (1) a pattern-aware dual-modal 3D diffusion framework for cross-modality learning, and (2) a tissue refinement network integrated with a efficient microstructure refinement to maintain structural fidelity and fine details. Evaluated on OASIS-3, ADNI, and in-house datasets, our method achieves state-of-the-art results, with PSNR/SSIM scores of 29.83 dB/90.84\\% for fMRI synthesis (+1.54 dB/+4.12\\% over baselines) and 30.00 dB/77.55\\% for dMRI synthesis (+1.02 dB/+2.2\\%). In clinical validation, the synthesized data show strong diagnostic performance, achieving 67.92\\%/66.02\\%/64.15\\% accuracy (NC vs. MCI vs. AD) in hybrid real-synthetic experiments. Code is available in \\href{https://github.com/SXR3015/PDS}{PDS GitHub Repository}",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†PDSï¼Œä¸€ç§å…·å¤‡ç»„ç»‡å’Œå¾®ç»“æ„ç»†åŒ–åŠŸèƒ½çš„æ¨¡å¼æ„ŸçŸ¥æ‰©æ•£åˆæˆæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç¥ç»é€€è¡Œæ€§ç–¾ç—…ç ”ç©¶ä¸­fMRIå’ŒdMRIæ¨¡æ€ç¼ºå¤±çš„éš¾é¢˜ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•éš¾ä»¥å¤„ç†fMRIä¸dMRIä¹‹é—´æ˜¾è‘—çš„ä¿¡å·å·®å¼‚ï¼ˆBOLDä¸æ‰©æ•£åŠ æƒä¿¡å·ï¼‰ä»¥åŠç–¾ç—…ç›¸å…³ç¥ç»è§£å‰–æ¨¡å¼æ•´åˆä¸è¶³çš„é—®é¢˜ï¼ŒPDSå¼•å…¥äº†æ¨¡å¼æ„ŸçŸ¥çš„åŒæ¨¡æ€3D diffusionæ¡†æ¶è¿›è¡Œè·¨æ¨¡æ€å­¦ä¹ ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•é›†æˆäº†ç»„ç»‡ç»†åŒ–ç½‘ç»œå’Œé«˜æ•ˆçš„å¾®ç»“æ„ç»†åŒ–æ¨¡å—ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„å›¾åƒä¿æŒç»“æ„ä¿çœŸåº¦å’Œç²¾ç»†ç»†èŠ‚ã€‚åœ¨OASIS-3ã€ADNIå’Œå†…éƒ¨æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒPDSåœ¨fMRIå’ŒdMRIåˆæˆä»»åŠ¡ä¸Šå‡å–å¾—äº†SOTAç»“æœï¼ŒPSNRå’ŒSSIMæŒ‡æ ‡æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚ä¸´åºŠéªŒè¯è¿›ä¸€æ­¥è¯å®ï¼Œåˆæˆæ•°æ®åœ¨NCã€MCIå’ŒADçš„åˆ†ç±»è¯Šæ–­ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶åœ¨æ··åˆçœŸå®-åˆæˆå®éªŒä¸­çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.04963v1",
      "published_date": "2025-11-07 03:51:00 UTC",
      "updated_date": "2025-11-07 03:51:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:52:42.156882+00:00"
    },
    {
      "arxiv_id": "2511.04962v2",
      "title": "Too Good to be Bad: On the Failure of LLMs to Role-Play Villains",
      "title_zh": "è¿‡äºå–„è‰¯è€Œæ— æ³•ä½œæ¶ï¼šè®ºå¤§è¯­è¨€æ¨¡å‹æ‰®æ¼”åæ´¾è§’è‰²çš„å¤±è´¥",
      "authors": [
        "Zihao Yi",
        "Qingxuan Jiang",
        "Ruotian Ma",
        "Xingyu Chen",
        "Qu Yang",
        "Mengru Wang",
        "Fanghua Ye",
        "Ying Shen",
        "Zhaopeng Tu",
        "Xiaolong Li",
        "Linus"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly tasked with creative generation, including the simulation of fictional characters. However, their ability to portray non-prosocial, antagonistic personas remains largely unexamined. We hypothesize that the safety alignment of modern LLMs creates a fundamental conflict with the task of authentically role-playing morally ambiguous or villainous characters. To investigate this, we introduce the Moral RolePlay benchmark, a new dataset featuring a four-level moral alignment scale and a balanced test set for rigorous evaluation. We task state-of-the-art LLMs with role-playing characters from moral paragons to pure villains. Our large-scale evaluation reveals a consistent, monotonic decline in role-playing fidelity as character morality decreases. We find that models struggle most with traits directly antithetical to safety principles, such as ``Deceitful'' and ``Manipulative'', often substituting nuanced malevolence with superficial aggression. Furthermore, we demonstrate that general chatbot proficiency is a poor predictor of villain role-playing ability, with highly safety-aligned models performing particularly poorly. Our work provides the first systematic evidence of this critical limitation, highlighting a key tension between model safety and creative fidelity. Our benchmark and findings pave the way for developing more nuanced, context-aware alignment methods.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ¨¡æ‹Ÿéäº²ç¤¾ä¼šæˆ–åæ´¾è§’è‰²æ—¶çš„å±€é™æ€§ï¼Œæå‡ºæ¨¡å‹ç°æœ‰çš„å®‰å…¨å¯¹é½(Safety Alignment)æœºåˆ¶ä¸æ‰®æ¼”é“å¾·æ¨¡ç³Šæˆ–é‚ªæ¶è§’è‰²çš„ä»»åŠ¡ä¹‹é—´å­˜åœ¨æ ¹æœ¬å†²çªã€‚ä¸ºæ­¤ï¼Œä½œè€…å¼•å…¥äº†Moral RolePlay benchmarkï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«å››çº§é“å¾·å¯¹é½é‡è¡¨çš„æ–°æ•°æ®é›†ï¼Œç”¨äºä¸¥æ ¼è¯„ä¼°SOTA LLMsåœ¨ä¸åŒé“å¾·å…‰è°±ä¸‹çš„è§’è‰²æ‰®æ¼”è¡¨ç°ã€‚å¤§è§„æ¨¡è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œéšç€è§’è‰²é“å¾·æ°´å¹³çš„é™ä½ï¼Œæ¨¡å‹è§’è‰²æ‰®æ¼”çš„é€¼çœŸåº¦å‘ˆç°ä¸€è‡´çš„å•è°ƒä¸‹é™è¶‹åŠ¿ã€‚ç ”ç©¶å‘ç°ï¼Œæ¨¡å‹åœ¨è¡¨ç°ä¸å®‰å…¨åŸåˆ™ç›´æ¥å¯¹ç«‹çš„ç‰¹å¾ï¼ˆå¦‚â€œDeceitfulâ€å’Œâ€œManipulativeâ€ï¼‰æ—¶æœ€ä¸ºå›°éš¾ï¼Œå¾€å¾€åªèƒ½ç”¨è‚¤æµ…çš„æ”»å‡»æ€§æ¥æ›¿ä»£å¤æ‚çš„æ¶æ„è¡Œä¸ºã€‚æ­¤å¤–ï¼Œé€šç”¨èŠå¤©æœºå™¨äººçš„èƒ½åŠ›å¹¶ä¸èƒ½æœ‰æ•ˆé¢„æµ‹åæ´¾æ‰®æ¼”èƒ½åŠ›ï¼Œé«˜åº¦å®‰å…¨å¯¹é½çš„æ¨¡å‹åœ¨æ­¤ç±»ä»»åŠ¡ä¸­è¡¨ç°å°¤ä¸ºç³Ÿç³•ã€‚è¿™é¡¹å·¥ä½œé¦–æ¬¡ç³»ç»Ÿæ€§åœ°è¯æ˜äº†æ¨¡å‹å®‰å…¨æ€§ä¸åˆ›é€ æ€§é€¼çœŸåº¦(Creative Fidelity)ä¹‹é—´çš„ç´§å¼ å…³ç³»ï¼Œå¼ºè°ƒäº†å¼€å‘æ›´å…·æƒ…å¢ƒæ„ŸçŸ¥çš„å¯¹é½æ–¹æ³•çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.04962v2",
      "published_date": "2025-11-07 03:50:52 UTC",
      "updated_date": "2025-11-12 12:26:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:52:54.283594+00:00"
    },
    {
      "arxiv_id": "2511.04956v1",
      "title": "ORCHID: Orchestrated Retrieval-Augmented Classification with Human-in-the-Loop Intelligent Decision-Making for High-Risk Property",
      "title_zh": "ORCHIDï¼šé¢å‘é«˜é£é™©èµ„äº§çš„ç¼–æ’å¼æ£€ç´¢å¢å¼ºåˆ†ç±»ä¸äººåœ¨å›è·¯æ™ºèƒ½å†³ç­–",
      "authors": [
        "Maria Mahbub",
        "Vanessa Lama",
        "Sanjay Das",
        "Brian Starks",
        "Christopher Polchek",
        "Saffell Silvers",
        "Lauren Deck",
        "Prasanna Balaprakash",
        "Tirthankar Ghosal"
      ],
      "abstract": "High-Risk Property (HRP) classification is critical at U.S. Department of Energy (DOE) sites, where inventories include sensitive and often dual-use equipment. Compliance must track evolving rules designated by various export control policies to make transparent and auditable decisions. Traditional expert-only workflows are time-consuming, backlog-prone, and struggle to keep pace with shifting regulatory boundaries. We demo ORCHID, a modular agentic system for HRP classification that pairs retrieval-augmented generation (RAG) with human oversight to produce policy-based outputs that can be audited. Small cooperating agents, retrieval, description refiner, classifier, validator, and feedback logger, coordinate via agent-to-agent messaging and invoke tools through the Model Context Protocol (MCP) for model-agnostic on-premise operation. The interface follows an Item to Evidence to Decision loop with step-by-step reasoning, on-policy citations, and append-only audit bundles (run-cards, prompts, evidence). In preliminary tests on real HRP cases, ORCHID improves accuracy and traceability over a non-agentic baseline while deferring uncertain items to Subject Matter Experts (SMEs). The demonstration shows single item submission, grounded citations, SME feedback capture, and exportable audit artifacts, illustrating a practical path to trustworthy LLM assistance in sensitive DOE compliance workflows.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº†ORCHIDï¼Œä¸€ç§é’ˆå¯¹é«˜é£é™©èµ„äº§(High-Risk Property, HRP)åˆ†ç±»çš„æ¨¡å—åŒ–æ™ºèƒ½ä½“ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³ç¾å›½èƒ½æºéƒ¨(DOE)ç«™ç‚¹ä¸­ä¼ ç»Ÿä¸“å®¶å·¥ä½œæµè€—æ—¶ä¸”æ˜“ç§¯å‹çš„é—®é¢˜ã€‚è¯¥ç³»ç»Ÿç»“åˆäº†æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ä¸äººæœºå›ç¯(Human-in-the-Loop)å†³ç­–æœºåˆ¶ï¼Œé€šè¿‡æ¨¡å‹ä¸Šä¸‹æ–‡åè®®(MCP)åè°ƒå¤šä¸ªå°å‹æ™ºèƒ½ä½“è¿›è¡Œåä½œï¼Œå®ç°æ¨¡å‹æ— å…³çš„æœ¬åœ°åŒ–è¿è¡Œã€‚ORCHIDéµå¾ªâ€œç‰©å“-è¯æ®-å†³ç­–â€çš„å¾ªç¯æµç¨‹ï¼Œæä¾›åŸºäºæ”¿ç­–çš„é€æ­¥æ¨ç†å’Œå¼•ç”¨ï¼Œå¹¶ç”Ÿæˆä»…å¯è¿½åŠ çš„å®¡è®¡åŒ…ä»¥ç¡®ä¿åˆè§„æ€§å’Œé€æ˜åº¦ã€‚åˆæ­¥æµ‹è¯•è¡¨æ˜ï¼ŒORCHIDåœ¨çœŸå®HRPæ¡ˆä¾‹ä¸­ç›¸æ¯”éæ™ºèƒ½ä½“åŸºçº¿æ˜¾è‘—æé«˜äº†å‡†ç¡®æ€§å’Œå¯è¿½æº¯æ€§ï¼ŒåŒæ—¶èƒ½å¤Ÿå°†ä¸ç¡®å®šçš„é¡¹ç›®è½¬äº¤ç»™é¢†åŸŸä¸“å®¶(SMEs)å¤„ç†ã€‚è¯¥ç³»ç»Ÿå±•ç¤ºäº†ä»å•é¡¹æäº¤åˆ°ç”Ÿæˆå¯å¯¼å‡ºå®¡è®¡å·¥ä»¶çš„å…¨è¿‡ç¨‹ï¼Œä¸ºåœ¨æ•æ„Ÿåˆè§„å·¥ä½œæµä¸­å®ç°å¯ä¿¡èµ–çš„å¤§è¯­è¨€æ¨¡å‹(LLM)è¾…åŠ©æä¾›äº†ä¸€æ¡åˆ‡å®å¯è¡Œçš„è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.04956v1",
      "published_date": "2025-11-07 03:48:05 UTC",
      "updated_date": "2025-11-07 03:48:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:53:18.266245+00:00"
    },
    {
      "arxiv_id": "2511.05629v1",
      "title": "SSTODE: Ocean-Atmosphere Physics-Informed Neural ODEs for Sea Surface Temperature Prediction",
      "title_zh": "SSTODEï¼šç”¨äºæµ·è¡¨æ¸©åº¦é¢„æµ‹çš„æµ·æ´‹-å¤§æ°”ç‰©ç†ä¿¡æ¯ç¥ç»å¸¸å¾®åˆ†æ–¹ç¨‹",
      "authors": [
        "Zheng Jiang",
        "Wei Wang",
        "Gaowei Zhang",
        "Yi Wang"
      ],
      "abstract": "Sea Surface Temperature (SST) is crucial for understanding upper-ocean thermal dynamics and ocean-atmosphere interactions, which have profound economic and social impacts. While data-driven models show promise in SST prediction, their black-box nature often limits interpretability and overlooks key physical processes. Recently, physics-informed neural networks have been gaining momentum but struggle with complex ocean-atmosphere dynamics due to 1) inadequate characterization of seawater movement (e.g., coastal upwelling) and 2) insufficient integration of external SST drivers (e.g., turbulent heat fluxes). To address these challenges, we propose SSTODE, a physics-informed Neural Ordinary Differential Equations (Neural ODEs) framework for SST prediction. First, we derive ODEs from fluid transport principles, incorporating both advection and diffusion to model ocean spatiotemporal dynamics. Through variational optimization, we recover a latent velocity field that explicitly governs the temporal dynamics of SST. Building upon ODE, we introduce an Energy Exchanges Integrator (EEI)-inspired by ocean heat budget equations-to account for external forcing factors. Thus, the variations in the components of these factors provide deeper insights into SST dynamics. Extensive experiments demonstrate that SSTODE achieves state-of-the-art performances in global and regional SST forecasting benchmarks. Furthermore, SSTODE visually reveals the impact of advection dynamics, thermal diffusion patterns, and diurnal heating-cooling cycles on SST evolution. These findings demonstrate the model's interpretability and physical consistency.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SSTODEï¼Œä¸€ç§ç”¨äºæµ·è¡¨é¢æ¸©åº¦(SST)é¢„æµ‹çš„ç‰©ç†ä¿¡æ¯ç¥ç»å¸¸å¾®åˆ†æ–¹ç¨‹(Neural ODEs)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ•°æ®é©±åŠ¨æ¨¡å‹ç¼ºä¹è§£é‡Šæ€§ä»¥åŠç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œéš¾ä»¥å¤„ç†å¤æ‚æµ·æ°”åŠ¨åŠ›å­¦çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é¦–å…ˆæ ¹æ®æµä½“è¾“è¿åŸç†æ¨å¯¼ODEï¼Œç»“åˆå¹³æµ(advection)å’Œæ‰©æ•£(diffusion)æ¥æ¨¡æ‹Ÿæµ·æ´‹æ—¶ç©ºåŠ¨åŠ›å­¦ï¼Œå¹¶é€šè¿‡å˜åˆ†ä¼˜åŒ–æ¢å¤æ§åˆ¶SSTæ—¶é—´å˜åŒ–çš„æ½œåœ¨é€Ÿåº¦åœºã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶å¼•å…¥äº†å—æµ·æ´‹çƒ­æ”¶æ”¯æ–¹ç¨‹å¯å‘çš„èƒ½é‡äº¤æ¢ç§¯åˆ†å™¨(Energy Exchanges Integrator, EEI)ï¼Œä»¥æ•´åˆæ¹æµçƒ­é€šé‡ç­‰å¤–éƒ¨é©±åŠ¨å› ç´ ã€‚å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼ŒSSTODEåœ¨å…¨çƒå’ŒåŒºåŸŸSSTé¢„æŠ¥åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›(SOTA)çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹è¿˜èƒ½ç›´è§‚å±•ç¤ºå¹³æµåŠ¨åŠ›å­¦ã€çƒ­æ‰©æ•£æ¨¡å¼å’Œæ—¥å¤œåŠ çƒ­-å†·å´å¾ªç¯å¯¹SSTæ¼”å˜çš„å½±å“ï¼Œè¯æ˜äº†å…¶è‰¯å¥½çš„è§£é‡Šæ€§å’Œç‰©ç†ä¸€è‡´æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.ao-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "To be published in the Proceedings of AAAI-AISI 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.05629v1",
      "published_date": "2025-11-07 03:43:53 UTC",
      "updated_date": "2025-11-07 03:43:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:53:48.022824+00:00"
    },
    {
      "arxiv_id": "2511.04949v1",
      "title": "DeepForgeSeal: Latent Space-Driven Semi-Fragile Watermarking for Deepfake Detection Using Multi-Agent Adversarial Reinforcement Learning",
      "title_zh": "DeepForgeSealï¼šåŸºäºå¤šæ™ºèƒ½ä½“å¯¹æŠ—å¼ºåŒ–å­¦ä¹ ä¸æ½œåœ¨ç©ºé—´é©±åŠ¨çš„æ·±åº¦ä¼ªé€ æ£€æµ‹åŠè„†å¼±æ°´å°",
      "authors": [
        "Tharindu Fernando",
        "Clinton Fookes",
        "Sridha Sridharan"
      ],
      "abstract": "Rapid advances in generative AI have led to increasingly realistic deepfakes, posing growing challenges for law enforcement and public trust. Existing passive deepfake detectors struggle to keep pace, largely due to their dependence on specific forgery artifacts, which limits their ability to generalize to new deepfake types. Proactive deepfake detection using watermarks has emerged to address the challenge of identifying high-quality synthetic media. However, these methods often struggle to balance robustness against benign distortions with sensitivity to malicious tampering. This paper introduces a novel deep learning framework that harnesses high-dimensional latent space representations and the Multi-Agent Adversarial Reinforcement Learning (MAARL) paradigm to develop a robust and adaptive watermarking approach. Specifically, we develop a learnable watermark embedder that operates in the latent space, capturing high-level image semantics, while offering precise control over message encoding and extraction. The MAARL paradigm empowers the learnable watermarking agent to pursue an optimal balance between robustness and fragility by interacting with a dynamic curriculum of benign and malicious image manipulations simulated by an adversarial attacker agent. Comprehensive evaluations on the CelebA and CelebA-HQ benchmarks reveal that our method consistently outperforms state-of-the-art approaches, achieving improvements of over 4.5% on CelebA and more than 5.3% on CelebA-HQ under challenging manipulation scenarios.",
      "tldr_zh": "æœ¬æ–‡æå‡ºäº†DeepForgeSealï¼Œä¸€ç§åˆ©ç”¨æ½œåœ¨ç©ºé—´é©±åŠ¨çš„åŠè„†å¼±æ°´å°æŠ€æœ¯ï¼Œæ—¨åœ¨è§£å†³ç”Ÿæˆå¼AIå¸¦æ¥çš„Deepfakeæ£€æµ‹éš¾é¢˜ã€‚é’ˆå¯¹ç°æœ‰ä¸»åŠ¨é˜²å¾¡æ–¹æ³•éš¾ä»¥å¹³è¡¡å¯¹è‰¯æ€§å¤±çœŸçš„é²æ£’æ€§å’Œå¯¹æ¶æ„ç¯¡æ”¹çš„æ•æ„Ÿæ€§è¿™ä¸€æŒ‘æˆ˜ï¼Œè¯¥ç ”ç©¶å¼•å…¥äº†å¤šæ™ºèƒ½ä½“å¯¹æŠ—å¼ºåŒ–å­¦ä¹ (Multi-Agent Adversarial Reinforcement Learning, MAARL)èŒƒå¼ã€‚å…·ä½“è€Œè¨€ï¼Œç ”ç©¶äººå‘˜å¼€å‘äº†ä¸€ä¸ªåœ¨æ½œåœ¨ç©ºé—´(latent space)è¿è¡Œçš„å¯å­¦ä¹ æ°´å°åµŒå…¥å™¨ï¼Œåœ¨æ•è·é«˜çº§å›¾åƒè¯­ä¹‰çš„åŒæ—¶å®ç°å¯¹ä¿¡æ¯ç¼–ç å’Œæå–çš„ç²¾ç¡®æ§åˆ¶ã€‚é€šè¿‡MAARLèŒƒå¼ï¼Œæ°´å°æ™ºèƒ½ä½“ä¸æ¨¡æ‹ŸåŠ¨æ€è‰¯æ€§å’Œæ¶æ„å›¾åƒæ“ä½œçš„å¯¹æŠ—æ”»å‡»æ™ºèƒ½ä½“è¿›è¡Œäº¤äº’ï¼Œä»è€Œå­¦ä¹ åœ¨é²æ£’æ€§ä¸è„†å¼±æ€§ä¹‹é—´å¯»æ±‚æœ€ä½³å¹³è¡¡ã€‚åœ¨CelebAå’ŒCelebA-HQåŸºå‡†æ•°æ®é›†ä¸Šçš„ç»¼åˆè¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤æ‚çš„ç¯¡æ”¹åœºæ™¯ä¸‹å§‹ç»ˆä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œæ€§èƒ½åˆ†åˆ«æå‡äº†è¶…è¿‡4.5%å’Œ5.3%ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.04949v1",
      "published_date": "2025-11-07 03:24:50 UTC",
      "updated_date": "2025-11-07 03:24:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:54:59.409311+00:00"
    },
    {
      "arxiv_id": "2511.04948v1",
      "title": "A benchmark multimodal oro-dental dataset for large vision-language models",
      "title_zh": "é¢å‘å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„åŸºå‡†å¤šæ¨¡æ€å£è…”ç‰™ç§‘æ•°æ®é›†",
      "authors": [
        "Haoxin Lv",
        "Ijazul Haq",
        "Jin Du",
        "Jiaxin Ma",
        "Binnian Zhu",
        "Xiaobing Dang",
        "Chaoan Liang",
        "Ruxu Du",
        "Yingjie Zhang",
        "Muhammad Saqib"
      ],
      "abstract": "The advancement of artificial intelligence in oral healthcare relies on the availability of large-scale multimodal datasets that capture the complexity of clinical practice. In this paper, we present a comprehensive multimodal dataset, comprising 8775 dental checkups from 4800 patients collected over eight years (2018-2025), with patients ranging from 10 to 90 years of age. The dataset includes 50000 intraoral images, 8056 radiographs, and detailed textual records, including diagnoses, treatment plans, and follow-up notes. The data were collected under standard ethical guidelines and annotated for benchmarking. To demonstrate its utility, we fine-tuned state-of-the-art large vision-language models, Qwen-VL 3B and 7B, and evaluated them on two tasks: classification of six oro-dental anomalies and generation of complete diagnostic reports from multimodal inputs. We compared the fine-tuned models with their base counterparts and GPT-4o. The fine-tuned models achieved substantial gains over these baselines, validating the dataset and underscoring its effectiveness in advancing AI-driven oro-dental healthcare solutions. The dataset is publicly available, providing an essential resource for future research in AI dentistry.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹(Large Vision-Language Models)çš„ç»¼åˆæ€§å¤šæ¨¡æ€å£è…”ç‰™ç§‘æ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³å£è…”åŒ»ç–—AIé¢†åŸŸç¼ºä¹å¤§è§„æ¨¡ä¸´åºŠæ•°æ®çš„é—®é¢˜ã€‚è¯¥æ•°æ®é›†åŒ…å«2018è‡³2025å¹´é—´4800åæ‚£è€…çš„8775æ¬¡æ£€æŸ¥æ•°æ®ï¼Œæ¶µç›–50000å¼ å£å†…å›¾åƒã€8056å¼ æ”¾å°„ç‰‡åŠè¯¦ç»†çš„è¯Šæ–­å’Œæ²»ç–—æ–‡æœ¬è®°å½•ã€‚ä¸ºäº†éªŒè¯æ•°æ®é›†çš„æ•ˆç”¨ï¼Œç ”ç©¶å›¢é˜Ÿåˆ©ç”¨å…¶å¾®è°ƒäº†Qwen-VL 3Bå’Œ7Bæ¨¡å‹ï¼Œå¹¶è¿›è¡Œäº†å…­ç§å£è…”å¼‚å¸¸åˆ†ç±»åŠå¤šæ¨¡æ€è¯Šæ–­æŠ¥å‘Šç”Ÿæˆçš„ä»»åŠ¡è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¾®è°ƒåçš„æ¨¡å‹æ€§èƒ½æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹åŠGPT-4oï¼Œå……åˆ†è¯æ˜äº†è¯¥æ•°æ®é›†åœ¨æå‡AIå£è…”åŒ»ç–—è§£å†³æ–¹æ¡ˆæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚è¯¥æ•°æ®é›†å·²å…¬å¼€å‘å¸ƒï¼Œä¸ºæœªæ¥çš„AIç‰™ç§‘ç ”ç©¶æä¾›äº†é‡è¦çš„åŸºå‡†èµ„æºã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.04948v1",
      "published_date": "2025-11-07 03:14:20 UTC",
      "updated_date": "2025-11-07 03:14:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:55:00.434666+00:00"
    },
    {
      "arxiv_id": "2511.05628v1",
      "title": "Unveiling the Training Dynamics of ReLU Networks through a Linear Lens",
      "title_zh": "é€è¿‡çº¿æ€§è§†è§’æ­ç¤ºReLUç½‘ç»œçš„è®­ç»ƒåŠ¨åŠ›å­¦",
      "authors": [
        "Longqing Ye"
      ],
      "abstract": "Deep neural networks, particularly those employing Rectified Linear Units (ReLU), are often perceived as complex, high-dimensional, non-linear systems. This complexity poses a significant challenge to understanding their internal learning mechanisms. In this work, we propose a novel analytical framework that recasts a multi-layer ReLU network into an equivalent single-layer linear model with input-dependent \"effective weights\". For any given input sample, the activation pattern of ReLU units creates a unique computational path, effectively zeroing out a subset of weights in the network. By composing the active weights across all layers, we can derive an effective weight matrix, $W_{\\text{eff}}(x)$, that maps the input directly to the output for that specific sample. We posit that the evolution of these effective weights reveals fundamental principles of representation learning. Our work demonstrates that as training progresses, the effective weights corresponding to samples from the same class converge, while those from different classes diverge. By tracking the trajectories of these sample-wise effective weights, we provide a new lens through which to interpret the formation of class-specific decision boundaries and the emergence of semantic representations within the network.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ—¨åœ¨è§£å†³æ·±åº¦ReLUç½‘ç»œå› é«˜ç»´éçº¿æ€§è€Œéš¾ä»¥ç†è§£å…¶å†…éƒ¨å­¦ä¹ æœºåˆ¶çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§é€šè¿‡çº¿æ€§è§†è§’ï¼ˆLinear Lensï¼‰æ­ç¤ºè®­ç»ƒåŠ¨æ€çš„æ–°å‹åˆ†ææ¡†æ¶ã€‚è¯¥æ–¹æ³•å°†å¤šå±‚ReLUç½‘ç»œé‡æ„ä¸ºå…·æœ‰è¾“å…¥ä¾èµ–æ€§â€œæœ‰æ•ˆæƒé‡â€ï¼ˆeffective weightsï¼‰çš„ç­‰æ•ˆå•å±‚çº¿æ€§æ¨¡å‹ï¼Œé€šè¿‡ç»„åˆå„å±‚çš„æ¿€æ´»æƒé‡æ¨å¯¼å‡ºé’ˆå¯¹ç‰¹å®šæ ·æœ¬çš„æœ‰æ•ˆæƒé‡çŸ©é˜µ$W_{\\text{eff}}(x)$ã€‚ç ”ç©¶å‘ç°ï¼Œéšç€è®­ç»ƒçš„è¿›è¡Œï¼ŒåŒç±»æ ·æœ¬çš„æœ‰æ•ˆæƒé‡é€æ¸æ”¶æ•›ï¼Œè€Œä¸åŒç±»åˆ«çš„æ ·æœ¬åˆ™ç›¸äº’å‘æ•£ã€‚é€šè¿‡è¿½è¸ªè¿™äº›æ ·æœ¬çº§æœ‰æ•ˆæƒé‡çš„æ¼”å˜è½¨è¿¹ï¼Œè¯¥å·¥ä½œä¸ä»…æ­ç¤ºäº†è¡¨ç¤ºå­¦ä¹ çš„åŸºæœ¬åŸç†ï¼Œè¿˜ä¸ºè§£é‡Šç±»ç‰¹å®šå†³ç­–è¾¹ç•Œçš„å½¢æˆåŠç½‘ç»œå†…éƒ¨è¯­ä¹‰è¡¨ç¤ºçš„æ¶Œç°æä¾›äº†å…¨æ–°çš„è§†è§’ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05628v1",
      "published_date": "2025-11-07 02:59:51 UTC",
      "updated_date": "2025-11-07 02:59:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:55:21.377497+00:00"
    },
    {
      "arxiv_id": "2511.11624v1",
      "title": "Characterizing and Understanding Energy Footprint and Efficiency of Small Language Model on Edges",
      "title_zh": "è¾¹ç¼˜ç«¯å°å‹è¯­è¨€æ¨¡å‹èƒ½è€—è¶³è¿¹ä¸æ•ˆç‡çš„è¡¨å¾ä¸ç†è§£",
      "authors": [
        "Md Romyull Islam",
        "Bobin Deng",
        "Nobel Dhar",
        "Tu N. Nguyen",
        "Selena He",
        "Yong Shi",
        "Kun Suo"
      ],
      "abstract": "Cloud-based large language models (LLMs) and their variants have significantly influenced real-world applications. Deploying smaller models (i.e., small language models (SLMs)) on edge devices offers additional advantages, such as reduced latency and independence from network connectivity. However, edge devices' limited computing resources and constrained energy budgets challenge efficient deployment. This study evaluates the power efficiency of five representative SLMs - Llama 3.2, Phi-3 Mini, TinyLlama, and Gemma 2 on Raspberry Pi 5, Jetson Nano, and Jetson Orin Nano (CPU and GPU configurations). Results show that Jetson Orin Nano with GPU acceleration achieves the highest energy-to-performance ratio, significantly outperforming CPU-based setups. Llama 3.2 provides the best balance of accuracy and power efficiency, while TinyLlama is well-suited for low-power environments at the cost of reduced accuracy. In contrast, Phi-3 Mini consumes the most energy despite its high accuracy. In addition, GPU acceleration, memory bandwidth, and model architecture are key in optimizing inference energy efficiency. Our empirical analysis offers practical insights for AI, smart systems, and mobile ad-hoc platforms to leverage tradeoffs from accuracy, inference latency, and power efficiency in energy-constrained environments.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²å°å‹è¯­è¨€æ¨¡å‹(Small Language Models, SLMs)æ—¶é¢ä¸´çš„è®¡ç®—èµ„æºå’Œèƒ½æºé™åˆ¶é—®é¢˜ï¼Œå¯¹å…¶èƒ½è€—è¶³è¿¹å’Œæ•ˆç‡è¿›è¡Œäº†æ·±å…¥çš„ç‰¹å¾åˆ†æä¸è¯„ä¼°ã€‚ç ”ç©¶äººå‘˜åœ¨Raspberry Pi 5ã€Jetson Nanoå’ŒJetson Orin Nanoï¼ˆåŒ…æ‹¬CPUå’ŒGPUé…ç½®ï¼‰ç­‰ç¡¬ä»¶å¹³å°ä¸Šï¼Œå¯¹Llama 3.2ã€Phi-3 Miniã€TinyLlamaå’ŒGemma 2ç­‰äº”ç§ä»£è¡¨æ€§SLMsè¿›è¡Œäº†åŠŸç‡æ•ˆç‡æµ‹è¯•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œé…å¤‡GPUåŠ é€Ÿçš„Jetson Orin Nanoå®ç°äº†æœ€é«˜çš„èƒ½æ•ˆæ¯”ï¼Œæ˜¾è‘—ä¼˜äºåŸºäºCPUçš„é…ç½®ã€‚åœ¨æ¨¡å‹è¡¨ç°æ–¹é¢ï¼ŒLlama 3.2åœ¨å‡†ç¡®æ€§å’ŒåŠŸç‡æ•ˆç‡ä¹‹é—´æä¾›äº†æœ€ä½³å¹³è¡¡ï¼Œè€ŒTinyLlamaè™½ç„¶é€‚åˆä½åŠŸè€—ç¯å¢ƒä½†ç‰ºç‰²äº†éƒ¨åˆ†å‡†ç¡®æ€§ï¼ŒPhi-3 Miniå°½ç®¡å‡†ç¡®ç‡é«˜ä½†èƒ½è€—æœ€å¤§ã€‚ç ”ç©¶æŒ‡å‡ºï¼ŒGPUåŠ é€Ÿã€å†…å­˜å¸¦å®½å’Œæ¨¡å‹æ¶æ„æ˜¯ä¼˜åŒ–æ¨ç†èƒ½æ•ˆçš„å…³é”®å› ç´ ã€‚è¯¥å®è¯åˆ†æä¸ºåœ¨èƒ½æºå—é™ç¯å¢ƒä¸­æƒè¡¡å‡†ç¡®æ€§ã€æ¨ç†å»¶è¿Ÿå’ŒåŠŸç‡æ•ˆç‡æä¾›äº†åˆ‡å®å¯è¡Œçš„è§è§£ï¼Œæœ‰åŠ©äºæŒ‡å¯¼AIå’Œç§»åŠ¨è‡ªç»„ç½‘å¹³å°çš„ä¼˜åŒ–ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.DC",
      "comment": "Submitted version; 9 pages, 5 figures; presented at IEEE MASS 2025 (online publication pending)",
      "pdf_url": "https://arxiv.org/pdf/2511.11624v1",
      "published_date": "2025-11-07 02:58:15 UTC",
      "updated_date": "2025-11-07 02:58:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:55:47.439691+00:00"
    },
    {
      "arxiv_id": "2511.04939v2",
      "title": "Search Is Not Retrieval: Decoupling Semantic Matching from Contextual Assembly in RAG",
      "title_zh": "æœç´¢å¹¶éæ£€ç´¢ï¼šRAGä¸­è¯­ä¹‰åŒ¹é…ä¸ä¸Šä¸‹æ–‡ç»„è£…çš„è§£è€¦",
      "authors": [
        "Harshit Nainwani",
        "Hediyeh Baban"
      ],
      "abstract": "Retrieval systems are essential to contemporary AI pipelines, although most confuse two separate processes: finding relevant information and giving enough context for reasoning. We introduce the Search-Is-Not-Retrieve (SINR) framework, a dual-layer architecture that distinguishes between fine-grained search representations and coarse-grained retrieval contexts. SINR enhances the composability, scalability, and context fidelity of retrieval systems by directly connecting small, semantically accurate search chunks to larger, contextually complete retrieve chunks, all without incurring extra processing costs. This design changes retrieval from a passive step to an active one, making the system architecture more like how people process information. We discuss the SINR framework's conceptual foundation, formal structure, implementation issues, and qualitative outcomes. This provides a practical foundation for the next generation of AI systems that use retrieval.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°ä»£AIæµæ°´çº¿ä¸­æ£€ç´¢ç³»ç»Ÿå¸¸æ··æ·†â€œæŸ¥æ‰¾ç›¸å…³ä¿¡æ¯â€ä¸â€œæä¾›æ¨ç†ä¸Šä¸‹æ–‡â€è¿™ä¸¤ä¸ªè¿‡ç¨‹çš„é—®é¢˜ï¼Œæå‡ºäº†Search-Is-Not-Retrieve (SINR)æ¡†æ¶ã€‚SINRé‡‡ç”¨åŒå±‚æ¶æ„ï¼Œæ˜ç¡®åŒºåˆ†äº†ç»†ç²’åº¦çš„æœç´¢è¡¨ç¤ºä¸ç²—ç²’åº¦çš„æ£€ç´¢ä¸Šä¸‹æ–‡ï¼Œä»è€Œå®ç°äº†è¯­ä¹‰åŒ¹é…ä¸ä¸Šä¸‹æ–‡ç»„è£…çš„è§£è€¦ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†è¯­ä¹‰ç²¾ç¡®çš„å°å‹æœç´¢å—(search chunks)ç›´æ¥è¿æ¥åˆ°ä¸Šä¸‹æ–‡å®Œæ•´çš„å¤§å‹æ£€ç´¢å—(retrieve chunks)ï¼Œåœ¨ä¸å¢åŠ é¢å¤–å¤„ç†æˆæœ¬çš„å‰æä¸‹ï¼Œæ˜¾è‘—æå‡äº†æ£€ç´¢ç³»ç»Ÿçš„å¯ç»„åˆæ€§ã€æ‰©å±•æ€§å’Œä¸Šä¸‹æ–‡ä¿çœŸåº¦ã€‚è¿™ç§è®¾è®¡å°†æ£€ç´¢ä»è¢«åŠ¨æ­¥éª¤è½¬å˜ä¸ºä¸»åŠ¨è¿‡ç¨‹ï¼Œä½¿å…¶æ¶æ„æ›´è´´è¿‘äººç±»å¤„ç†ä¿¡æ¯çš„æ–¹å¼ã€‚è®ºæ–‡è¯¦ç»†æ¢è®¨äº†SINRçš„æ¦‚å¿µåŸºç¡€ã€å½¢å¼åŒ–ç»“æ„åŠå®šæ€§ç»“æœï¼Œä¸ºä¸‹ä¸€ä»£æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ç³»ç»Ÿçš„æ„å»ºæä¾›äº†å®ç”¨çš„åŸºç¡€ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "22 pages, 2 figures, technical framework paper",
      "pdf_url": "https://arxiv.org/pdf/2511.04939v2",
      "published_date": "2025-11-07 02:51:20 UTC",
      "updated_date": "2025-11-12 19:30:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:58:57.762152+00:00"
    },
    {
      "arxiv_id": "2511.05627v1",
      "title": "Assessing the Reliability of Large Language Models in the Bengali Legal Context: A Comparative Evaluation Using LLM-as-Judge and Legal Experts",
      "title_zh": "è¯„ä¼°å­ŸåŠ æ‹‰è¯­æ³•å¾‹è¯­å¢ƒä¸‹å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯é æ€§ï¼šåŸºäº LLM-as-Judge ä¸æ³•å¾‹ä¸“å®¶çš„å¯¹æ¯”è¯„ä¼°",
      "authors": [
        "Sabik Aftahee",
        "A. F. M. Farhad",
        "Arpita Mallik",
        "Ratnajit Dhar",
        "Jawadul Karim",
        "Nahiyan Bin Noor",
        "Ishmam Ahmed Solaiman"
      ],
      "abstract": "Accessing legal help in Bangladesh is hard. People face high fees, complex legal language, a shortage of lawyers, and millions of unresolved court cases. Generative AI models like OpenAI GPT-4.1 Mini, Gemini 2.0 Flash, Meta Llama 3 70B, and DeepSeek R1 could potentially democratize legal assistance by providing quick and affordable legal advice. In this study, we collected 250 authentic legal questions from the Facebook group \"Know Your Rights,\" where verified legal experts regularly provide authoritative answers. These questions were subsequently submitted to four four advanced AI models and responses were generated using a consistent, standardized prompt. A comprehensive dual evaluation framework was employed, in which a state-of-the-art LLM model served as a judge, assessing each AI-generated response across four critical dimensions: factual accuracy, legal appropriateness, completeness, and clarity. Following this, the same set of questions was evaluated by three licensed Bangladeshi legal professionals according to the same criteria. In addition, automated evaluation metrics, including BLEU scores, were applied to assess response similarity. Our findings reveal a complex landscape where AI models frequently generate high-quality, well-structured legal responses but also produce dangerous misinformation, including fabricated case citations, incorrect legal procedures, and potentially harmful advice. These results underscore the critical need for rigorous expert validation and comprehensive safeguards before AI systems can be safely deployed for legal consultation in Bangladesh.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å­ŸåŠ æ‹‰å›½æ³•å¾‹è¯­å¢ƒä¸­çš„å¯é æ€§ï¼Œæ—¨åœ¨æ¢ç´¢ç”Ÿæˆå¼AIæ˜¯å¦èƒ½è§£å†³è¯¥åœ°åŒºæ³•å¾‹æ´åŠ©è·å–å›°éš¾çš„é—®é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿä»Facebookç¾¤ç»„â€œKnow Your Rightsâ€æ”¶é›†äº†250ä¸ªçœŸå®çš„æ³•å¾‹é—®é¢˜ï¼Œå¹¶ä½¿ç”¨OpenAI GPT-4.1 Miniã€Gemini 2.0 Flashã€Meta Llama 3 70Bå’ŒDeepSeek R1ç”Ÿæˆå›ç­”ã€‚é‡‡ç”¨äº†åŒé‡è¯„ä¼°æ¡†æ¶ï¼Œç»“åˆLLM-as-judgeå’Œä¸‰ä½å­ŸåŠ æ‹‰å›½æŒç‰Œæ³•å¾‹ä¸“å®¶ï¼Œä»äº‹å®å‡†ç¡®æ€§ã€æ³•å¾‹é€‚å½“æ€§ã€å®Œæ•´æ€§å’Œæ¸…æ™°åº¦å››ä¸ªç»´åº¦å¯¹AIç”Ÿæˆçš„å›ç­”è¿›è¡Œè¯„åˆ†ï¼Œå¹¶è¾…ä»¥BLEUåˆ†æ•°ç­‰è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡ã€‚ç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶AIæ¨¡å‹ç»å¸¸èƒ½ç”Ÿæˆé«˜è´¨é‡ä¸”ç»“æ„è‰¯å¥½çš„æ³•å¾‹å»ºè®®ï¼Œä½†ä¹Ÿäº§ç”Ÿäº†å±é™©çš„é”™è¯¯ä¿¡æ¯ï¼ŒåŒ…æ‹¬æé€ çš„æ¡ˆä»¶å¼•ç”¨ã€é”™è¯¯çš„æ³•å¾‹ç¨‹åºä»¥åŠæ½œåœ¨çš„æœ‰å®³å»ºè®®ã€‚ç ”ç©¶å¼ºè°ƒï¼Œåœ¨å­ŸåŠ æ‹‰å›½éƒ¨ç½²AIè¿›è¡Œæ³•å¾‹å’¨è¯¢ä¹‹å‰ï¼Œå¿…é¡»è¿›è¡Œä¸¥æ ¼çš„ä¸“å®¶éªŒè¯å¹¶å»ºç«‹å…¨é¢çš„å®‰å…¨ä¿éšœæªæ–½ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05627v1",
      "published_date": "2025-11-07 02:44:00 UTC",
      "updated_date": "2025-11-07 02:44:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T14:59:23.768560+00:00"
    },
    {
      "arxiv_id": "2511.04919v1",
      "title": "BudgetMem: Learning Selective Memory Policies for Cost-Efficient Long-Context Processing in Language Models",
      "title_zh": "BudgetMemï¼šå­¦ä¹ é€‰æ‹©æ€§è®°å¿†ç­–ç•¥ä»¥å®ç°è¯­è¨€æ¨¡å‹å…·æœ‰æˆæœ¬æ•ˆç›Šçš„é•¿ä¸Šä¸‹æ–‡å¤„ç†",
      "authors": [
        "Chandra Vamsi Krishna Alla",
        "Harish Naidu Gaddam",
        "Manohar Kommi"
      ],
      "abstract": "Large Language Models (LLMs) face significant computational and memory constraints when processing long contexts, despite growing demand for applications requiring reasoning over extensive documents, multi-session dialogues, and book length texts. While recent advances have extended context windows to 100K-1M tokens, such approaches incur prohibitive costs for resource constrained deployments. We propose BudgetMem, a novel memory augmented architecture that learns what to remember rather than remembering everything. Our system combines selective memory policies with feature based salience scoring (entity density, TF-IDF, discourse markers, position bias) to decide which information merits storage under strict budget constraints. Unlike existing retrieval augmented generation (RAG) systems that store all chunks, BudgetMem employs learned gating mechanisms coupled with BM25 sparse retrieval for efficient information access. Through comprehensive experiments on 700 question answer pairs across short (237 tokens) and long (5K-10K tokens) documents with Llama-3.2-3B-Instruct, we demonstrate that BudgetMem achieves remarkable results on long documents: only 1.0% F1 score degradation while saving 72.4% memory compared to baseline RAG. We validate our approach through budget sensitivity analysis (testing 7 budget ratios), naive baseline comparisons, and document length analysis, showing that BudgetMem's benefits increase with document length. Our work provides a practical pathway for deploying capable long context systems on modest hardware, democratizing access to advanced language understanding capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡æ—¶é¢ä¸´çš„è®¡ç®—å’Œå†…å­˜é™åˆ¶ï¼Œæå‡ºäº†BudgetMemï¼Œä¸€ç§æ–°å‹çš„å†…å­˜å¢å¼ºæ¶æ„ã€‚BudgetMemæ—¨åœ¨å­¦ä¹ â€œè®°ä½ä»€ä¹ˆâ€è€Œéå…¨ç›˜è®°å¿†ï¼Œé€šè¿‡ç»“åˆé€‰æ‹©æ€§è®°å¿†ç­–ç•¥ä¸åŸºäºç‰¹å¾çš„æ˜¾è‘—æ€§è¯„åˆ†ï¼ˆå¦‚å®ä½“å¯†åº¦ã€TF-IDFã€è¯­ç¯‡æ ‡è®°å’Œä½ç½®åå·®ï¼‰ï¼Œåœ¨ä¸¥æ ¼çš„é¢„ç®—çº¦æŸä¸‹ç­›é€‰å€¼å¾—å­˜å‚¨çš„ä¿¡æ¯ã€‚ä¸å­˜å‚¨æ‰€æœ‰æ–‡æœ¬å—çš„ç°æœ‰æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ç³»ç»Ÿä¸åŒï¼Œè¯¥æ–¹æ³•é‡‡ç”¨äº†å­¦ä¹ é—¨æ§æœºåˆ¶å¹¶ç»“åˆBM25ç¨€ç–æ£€ç´¢ä»¥å®ç°é«˜æ•ˆè®¿é—®ã€‚åœ¨åŸºäºLlama-3.2-3B-Instructæ¨¡å‹çš„å®éªŒä¸­ï¼ŒBudgetMemåœ¨å¤„ç†é•¿æ–‡æ¡£ï¼ˆ5K-10K tokensï¼‰æ—¶è¡¨ç°ä¼˜å¼‚ï¼Œç›¸æ¯”åŸºçº¿RAGä»…å¯¼è‡´1.0%çš„F1åˆ†æ•°ä¸‹é™ï¼Œå´å®ç°äº†72.4%çš„å†…å­˜èŠ‚çœã€‚é¢„ç®—æ•æ„Ÿæ€§å’Œæ–‡æ¡£é•¿åº¦åˆ†æè¿›ä¸€æ­¥è¯å®ï¼Œæ–‡æ¡£è¶Šé•¿ï¼Œè¯¥æ–¹æ³•çš„ä¼˜åŠ¿è¶Šæ˜¾è‘—ã€‚è¿™é¡¹å·¥ä½œä¸ºåœ¨èµ„æºå—é™çš„ç¡¬ä»¶ä¸Šéƒ¨ç½²å…·å¤‡é•¿ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›çš„ç³»ç»Ÿæä¾›äº†æœ‰æ•ˆé€”å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "11 pages, 3 figures, 5 tables. Evaluated on 700 QA pairs across multiple document lengths",
      "pdf_url": "https://arxiv.org/pdf/2511.04919v1",
      "published_date": "2025-11-07 01:49:22 UTC",
      "updated_date": "2025-11-07 01:49:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T15:00:44.168669+00:00"
    },
    {
      "arxiv_id": "2511.07459v1",
      "title": "Optimizing Classification of Infrequent Labels by Reducing Variability in Label Distribution",
      "title_zh": "é€šè¿‡é™ä½æ ‡ç­¾åˆ†å¸ƒå˜å¼‚æ€§ä¼˜åŒ–ä½é¢‘æ ‡ç­¾åˆ†ç±»",
      "authors": [
        "Ashutosh Agarwal"
      ],
      "abstract": "This paper presents a novel solution, LEVER, designed to address the challenges posed by underperforming infrequent categories in Extreme Classification (XC) tasks. Infrequent categories, often characterized by sparse samples, suffer from high label inconsistency, which undermines classification performance. LEVER mitigates this problem by adopting a robust Siamese-style architecture, leveraging knowledge transfer to reduce label inconsistency and enhance the performance of One-vs-All classifiers. Comprehensive testing across multiple XC datasets reveals substantial improvements in the handling of infrequent categories, setting a new benchmark for the field. Additionally, the paper introduces two newly created multi-intent datasets, offering essential resources for future XC research.",
      "tldr_zh": "è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºLEVERçš„æ–°é¢–è§£å†³æ–¹æ¡ˆï¼Œæ—¨åœ¨è§£å†³æç«¯åˆ†ç±»ï¼ˆExtreme Classification, XCï¼‰ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³çš„ä½é¢‘ç±»åˆ«é—®é¢˜ã€‚ä½é¢‘ç±»åˆ«é€šå¸¸ç‰¹å¾ä¸ºæ ·æœ¬ç¨€ç–ï¼Œä¸”å­˜åœ¨è¾ƒé«˜çš„æ ‡ç­¾ä¸ä¸€è‡´æ€§ï¼ˆlabel inconsistencyï¼‰ï¼Œè¿™ä¸¥é‡å‰Šå¼±äº†åˆ†ç±»æ€§èƒ½ã€‚LEVERé€šè¿‡é‡‡ç”¨ç¨³å¥çš„Siamese-styleæ¶æ„ï¼Œåˆ©ç”¨çŸ¥è¯†è¿ç§»ï¼ˆknowledge transferï¼‰æ¥å‡å°‘æ ‡ç­¾ä¸ä¸€è‡´æ€§ï¼Œä»è€Œå¢å¼ºOne-vs-Allåˆ†ç±»å™¨çš„æ€§èƒ½ã€‚åœ¨å¤šä¸ªXCæ•°æ®é›†ä¸Šçš„ç»¼åˆæµ‹è¯•è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤„ç†ä½é¢‘ç±»åˆ«æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œä¸ºè¯¥é¢†åŸŸæ ‘ç«‹äº†æ–°æ ‡æ†ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜ä»‹ç»äº†ä¸¤ä¸ªæ–°åˆ›å»ºçš„å¤šæ„å›¾ï¼ˆmulti-intentï¼‰æ•°æ®é›†ï¼Œä¸ºæœªæ¥çš„XCç ”ç©¶æä¾›äº†é‡è¦èµ„æºã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted and presented at 6th International Conference on Emerging research in electronics, computer science and technology ( ICERECT)",
      "pdf_url": "https://arxiv.org/pdf/2511.07459v1",
      "published_date": "2025-11-07 01:48:29 UTC",
      "updated_date": "2025-11-07 01:48:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T15:00:08.840098+00:00"
    },
    {
      "arxiv_id": "2511.04914v3",
      "title": "MERaLiON-SER: Robust Speech Emotion Recognition Model for English and SEA Languages",
      "title_zh": "MERaLiON-SERï¼šé¢å‘è‹±è¯­åŠä¸œå—äºšè¯­è¨€çš„é²æ£’è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹",
      "authors": [
        "Hardik B. Sailor",
        "Aw Ai Ti",
        "Chen Fang Yih Nancy",
        "Chiu Ying Lay",
        "Ding Yang",
        "He Yingxu",
        "Jiang Ridong",
        "Li Jingtao",
        "Liao Jingyi",
        "Liu Zhuohan",
        "Lu Yanfeng",
        "Ma Yi",
        "Manas Gupta",
        "Muhammad Huzaifah Bin Md Shahrin",
        "Nabilah Binte Md Johan",
        "Nattadaporn Lertcheva",
        "Pan Chunlei",
        "Pham Minh Duc",
        "Siti Maryam Binte Ahmad Subaidi",
        "Siti Umairah Binte Mohammad Salleh",
        "Sun Shuo",
        "Tarun Kumar Vangani",
        "Wang Qiongqiong",
        "Won Cheng Yi Lewis",
        "Wong Heng Meng Jeremy",
        "Wu Jinyang",
        "Zhang Huayun",
        "Zhang Longyin",
        "Zou Xunlong"
      ],
      "abstract": "We present MERaLiON-SER, a robust speech emotion recognition model designed for English and Southeast Asian languages. The model is trained using a hybrid objective combining weighted categorical cross-entropy and Concordance Correlation Coefficient (CCC) losses for joint discrete and dimensional emotion modelling. This dual approach enables the model to capture both the distinct categories of emotion (like happy or angry) and the fine-grained, such as arousal (intensity), valence (positivity/negativity), and dominance (sense of control), leading to a more comprehensive and robust representation of human affect. Extensive evaluations across multilingual Singaporean languages (English, Chinese, Malay, and Tamil ) and other public benchmarks show that MERaLiON-SER consistently surpasses both open-source speech encoders and large Audio-LLMs. These results underscore the importance of specialised speech-only models for accurate paralinguistic understanding and cross-lingual generalisation. Furthermore, the proposed framework provides a foundation for integrating emotion-aware perception into future agentic audio systems, enabling more empathetic and contextually adaptive multimodal reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MERaLiON-SERï¼Œä¸€ç§ä¸“ä¸ºè‹±è¯­å’Œä¸œå—äºšè¯­è¨€è®¾è®¡çš„é²æ£’è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«(Speech Emotion Recognition)æ¨¡å‹ã€‚è¯¥æ¨¡å‹åˆ©ç”¨ç»“åˆåŠ æƒåˆ†ç±»äº¤å‰ç†µå’Œä¸€è‡´æ€§ç›¸å…³ç³»æ•°(CCC)æŸå¤±çš„æ··åˆç›®æ ‡è¿›è¡Œè®­ç»ƒï¼Œä»¥å®ç°ç¦»æ•£æƒ…æ„Ÿå’Œç»´åº¦æƒ…æ„Ÿçš„è”åˆå»ºæ¨¡ã€‚è¿™ç§åŒé‡æ–¹æ³•ä½¿æ¨¡å‹æ—¢èƒ½æ•æ‰å¿«ä¹æˆ–æ„¤æ€’ç­‰ä¸åŒæƒ…æ„Ÿç±»åˆ«ï¼Œåˆèƒ½æ•æ‰å”¤é†’åº¦(arousal)ã€æ•ˆä»·(valence)å’Œæ”¯é…åº¦(dominance)ç­‰ç»†ç²’åº¦ç‰¹å¾ï¼Œä»è€Œç”Ÿæˆæ›´å…¨é¢ã€é²æ£’çš„äººç±»æƒ…æ„Ÿè¡¨å¾ã€‚åœ¨æ¶‰åŠè‹±è¯­ã€ä¸­æ–‡ã€é©¬æ¥è¯­å’Œæ³°ç±³å°”è¯­çš„å¤šè¯­è¨€æ–°åŠ å¡è¯­è¨€æ•°æ®é›†åŠå…¶ä»–å…¬å…±åŸºå‡†ä¸Šçš„å¹¿æ³›è¯„ä¼°æ˜¾ç¤ºï¼ŒMERaLiON-SERçš„è¡¨ç°æŒç»­ä¼˜äºç°æœ‰çš„å¼€æºè¯­éŸ³ç¼–ç å™¨å’Œå¤§å‹éŸ³é¢‘å¤§è¯­è¨€æ¨¡å‹(Audio-LLMs)ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†ä¸“ç”¨çº¯è¯­éŸ³æ¨¡å‹åœ¨å‡†ç¡®çš„å‰¯è¯­è¨€ç†è§£å’Œè·¨è¯­è¨€æ³›åŒ–æ–¹é¢çš„é‡è¦æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶ä¸ºå°†æƒ…æ„Ÿæ„ŸçŸ¥æ•´åˆåˆ°æœªæ¥çš„ä»£ç†éŸ³é¢‘ç³»ç»Ÿ(agentic audio systems)ä¸­å¥ å®šäº†åŸºç¡€ï¼Œæœ‰åŠ©äºå®ç°æ›´å…·åŒç†å¿ƒå’Œä¸Šä¸‹æ–‡é€‚åº”æ€§çš„å¤šæ¨¡æ€æ¨ç†ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "https://huggingface.co/MERaLiON/MERaLiON-SER-v1",
      "pdf_url": "https://arxiv.org/pdf/2511.04914v3",
      "published_date": "2025-11-07 01:28:40 UTC",
      "updated_date": "2025-11-12 10:43:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T15:01:11.346289+00:00"
    },
    {
      "arxiv_id": "2511.04909v1",
      "title": "A Dual Perspective on Decision-Focused Learning: Scalable Training via Dual-Guided Surrogates",
      "title_zh": "å†³ç­–èšç„¦å­¦ä¹ çš„å¯¹å¶è§†è§’ï¼šåŸºäºå¯¹å¶å¼•å¯¼ä»£ç†çš„å¯æ‰©å±•è®­ç»ƒ",
      "authors": [
        "Paula Rodriguez-Diaz",
        "Kirk Bansak Elisabeth Paulson"
      ],
      "abstract": "Many real-world decisions are made under uncertainty by solving optimization problems using predicted quantities. This predict-then-optimize paradigm has motivated decision-focused learning, which trains models with awareness of how the optimizer uses predictions, improving the performance of downstream decisions. Despite its promise, scaling is challenging: state-of-the-art methods either differentiate through a solver or rely on task-specific surrogates, both of which require frequent and expensive calls to an optimizer, often a combinatorial one. In this paper, we leverage dual variables from the downstream problem to shape learning and introduce Dual-Guided Loss (DGL), a simple, scalable objective that preserves decision alignment while reducing solver dependence. We construct DGL specifically for combinatorial selection problems with natural one-of-many constraints, such as matching, knapsack, and shortest path. Our approach (a) decouples optimization from gradient updates by solving the downstream problem only periodically; (b) between refreshes, trains on dual-adjusted targets using simple differentiable surrogate losses; and (c) as refreshes become less frequent, drives training cost toward standard supervised learning while retaining strong decision alignment. We prove that DGL has asymptotically diminishing decision regret, analyze runtime complexity, and show on two problem classes that DGL matches or exceeds state-of-the-art DFL methods while using far fewer solver calls and substantially less training time. Code is available at https://github.com/paularodr/Dual-Guided-Learning.",
      "tldr_zh": "è¿™ç¯‡è®ºæ–‡é’ˆå¯¹\"predict-then-optimize\"èŒƒå¼ä¸­çš„å†³ç­–å¯¼å‘å­¦ä¹ (decision-focused learning, DFL)é¢ä¸´çš„å¯æ‰©å±•æ€§æŒ‘æˆ˜è¿›è¡Œäº†ç ”ç©¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•ä¾èµ–é¢‘ç¹ä¸”æ˜‚è´µçš„ç»„åˆä¼˜åŒ–å™¨è°ƒç”¨çš„é—®é¢˜ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åä¸ºDual-Guided Loss (DGL)çš„ç®€å•ä¸”å¯æ‰©å±•çš„ç›®æ ‡å‡½æ•°ï¼Œåˆ©ç”¨ä¸‹æ¸¸é—®é¢˜çš„å¯¹å¶å˜é‡(dual variables)æ¥æŒ‡å¯¼æ¨¡å‹è®­ç»ƒã€‚DGLä¸“ä¸ºå…·æœ‰è‡ªç„¶å¤šé€‰ä¸€çº¦æŸçš„ç»„åˆé€‰æ‹©é—®é¢˜ï¼ˆå¦‚åŒ¹é…ã€èƒŒåŒ…å’Œæœ€çŸ­è·¯å¾„é—®é¢˜ï¼‰è®¾è®¡ï¼Œé€šè¿‡ä»…å‘¨æœŸæ€§åœ°æ±‚è§£ä¸‹æ¸¸é—®é¢˜æ¥å®ç°ä¼˜åŒ–ä¸æ¢¯åº¦æ›´æ–°çš„è§£è€¦ã€‚åœ¨åˆ·æ–°é—´éš”æœŸé—´ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ç®€å•çš„å¯å¾®ä»£ç†æŸå¤±å‡½æ•°åœ¨ç»è¿‡å¯¹å¶è°ƒæ•´çš„ç›®æ ‡ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»è€Œåœ¨å¤§å¹…å‡å°‘æ±‚è§£å™¨ä¾èµ–çš„åŒæ—¶ä¿æŒå†³ç­–å¯¹é½ã€‚ç†è®ºåˆ†æè¯æ˜DGLå…·æœ‰æ¸è¿‘é€’å‡çš„å†³ç­–é—æ†¾(decision regret)ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼ŒDGLåœ¨ä¸¤ä¸ªé—®é¢˜ç±»åˆ«ä¸ŠåŒ¹é…æˆ–è¶…è¿‡äº†æœ€å…ˆè¿›çš„DFLæ–¹æ³•ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†æ±‚è§£å™¨è°ƒç”¨æ¬¡æ•°å’Œè®­ç»ƒæ—¶é—´ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.04909v1",
      "published_date": "2025-11-07 01:15:15 UTC",
      "updated_date": "2025-11-07 01:15:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T15:00:59.072905+00:00"
    },
    {
      "arxiv_id": "2511.04902v1",
      "title": "You Need Reasoning to Learn Reasoning: The Limitations of Label-Free RL in Weak Base Models",
      "title_zh": "å­¦ä¹ æ¨ç†éœ€è¦å…·å¤‡æ¨ç†èƒ½åŠ›ï¼šæ— æ ‡ç­¾å¼ºåŒ–å­¦ä¹ åœ¨å¼±åŸºåº§æ¨¡å‹ä¸­çš„å±€é™æ€§",
      "authors": [
        "Shuvendu Roy",
        "Hossein Hajimirsadeghi",
        "Mengyao Zhai",
        "Golnoosh Samei"
      ],
      "abstract": "Recent advances in large language models have demonstrated the promise of unsupervised reinforcement learning (RL) methods for enhancing reasoning capabilities without external supervision. However, the generalizability of these label-free RL approaches to smaller base models with limited reasoning capabilities remains unexplored. In this work, we systematically investigate the performance of label-free RL methods across different model sizes and reasoning strengths, from 0.5B to 7B parameters. Our empirical analysis reveals critical limitations: label-free RL is highly dependent on the base model's pre-existing reasoning capability, with performance often degrading below baseline levels for weaker models. We find that smaller models fail to generate sufficiently long or diverse chain-of-thought reasoning to enable effective self-reflection, and that training data difficulty plays a crucial role in determining success. To address these challenges, we propose a simple yet effective method for label-free RL that utilizes curriculum learning to progressively introduce harder problems during training and mask no-majority rollouts during training. Additionally, we introduce a data curation pipeline to generate samples with predefined difficulty. Our approach demonstrates consistent improvements across all model sizes and reasoning capabilities, providing a path toward more robust unsupervised RL that can bootstrap reasoning abilities in resource-constrained models. We make our code available at https://github.com/BorealisAI/CuMa",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ·±å…¥æ¢è®¨äº†æ— ç›‘ç£å¼ºåŒ–å­¦ä¹ ï¼ˆlabel-free RLï¼‰åœ¨å¢å¼ºå°å‹åŸºç¡€æ¨¡å‹æ¨ç†èƒ½åŠ›æ–¹é¢çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹0.5Båˆ°7Bå‚æ•°èŒƒå›´çš„æ¨¡å‹ã€‚å®è¯åˆ†æè¡¨æ˜ï¼Œlabel-free RLé«˜åº¦ä¾èµ–äºåŸºç¡€æ¨¡å‹æ—¢æœ‰çš„æ¨ç†èƒ½åŠ›ï¼Œå¯¹äºæ— æ³•ç”Ÿæˆè¶³å¤Ÿé•¿æˆ–å¤šæ ·çš„æ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼‰æ¥è¿›è¡Œæœ‰æ•ˆè‡ªæˆ‘åæ€çš„å¼±æ¨¡å‹ï¼Œå…¶æ€§èƒ½å¾€å¾€ä¼šä¸‹é™ã€‚ç ”ç©¶å‘ç°è®­ç»ƒæ•°æ®çš„éš¾åº¦åœ¨å†³å®šæˆåŠŸä¸å¦æ–¹é¢èµ·ç€å…³é”®ä½œç”¨ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ï¼Œåˆ©ç”¨è¯¾ç¨‹å­¦ä¹ ï¼ˆCurriculum Learningï¼‰åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ­¥å¼•å…¥æ›´éš¾çš„é—®é¢˜ï¼Œå¹¶æ©ç›–æ‰æœªèƒ½è¾¾æˆå¤šæ•°ä¸€è‡´çš„rolloutsã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†ä¸€ä¸ªæ•°æ®ç­–å±•æµç¨‹æ¥ç”Ÿæˆå…·æœ‰é¢„å®šä¹‰éš¾åº¦çš„æ ·æœ¬ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•ï¼ˆCuMaï¼‰åœ¨æ‰€æœ‰æ¨¡å‹å°ºå¯¸å’Œæ¨ç†èƒ½åŠ›ä¸Šå‡å®ç°äº†æŒç»­çš„æ€§èƒ½æå‡ï¼Œä¸ºåœ¨èµ„æºå—é™æ¨¡å‹ä¸­é€šè¿‡æ— ç›‘ç£RLå¼•å¯¼æ¨ç†èƒ½åŠ›æä¾›äº†ä¸€æ¡ç¨³å¥çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: MATH-AI",
      "pdf_url": "https://arxiv.org/pdf/2511.04902v1",
      "published_date": "2025-11-07 01:05:11 UTC",
      "updated_date": "2025-11-07 01:05:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T15:01:36.156727+00:00"
    },
    {
      "arxiv_id": "2511.04898v1",
      "title": "Real-Time Reasoning Agents in Evolving Environments",
      "title_zh": "æ¼”å˜ç¯å¢ƒä¸‹çš„å®æ—¶æ¨ç†æ™ºèƒ½ä½“",
      "authors": [
        "Yule Wen",
        "Yixin Ye",
        "Yanzhe Zhang",
        "Diyi Yang",
        "Hao Zhu"
      ],
      "abstract": "Agents in the real world must make not only logical but also timely judgments. This requires continuous awareness of the dynamic environment: hazards emerge, opportunities arise, and other agents act, while the agent's reasoning is still unfolding. Despite advances in language model reasoning, existing approaches fail to account for this dynamic nature. We introduce real-time reasoning as a new problem formulation for agents in evolving environments and build Real-Time Reasoning Gym to demonstrate it. We study two paradigms for deploying language models in agents: (1) reactive agents, which employ language models with bounded reasoning computation for rapid responses, and (2) planning agents, which allow extended reasoning computation for complex problems. Our experiments show that even state-of-the-art models struggle with making logical and timely judgments in either paradigm. To address this limitation, we propose AgileThinker, which simultaneously engages both reasoning paradigms. AgileThinker consistently outperforms agents engaging only one reasoning paradigm as the task difficulty and time pressure rise, effectively balancing reasoning depth and response latency. Our work establishes real-time reasoning as a critical testbed for developing practical agents and provides a foundation for research in temporally constrained AI systems, highlighting a path toward real-time capable agents.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°å®ä¸–ç•Œæ™ºèƒ½ä½“éœ€è¦åœ¨åŠ¨æ€ç¯å¢ƒä¸­å…¼é¡¾é€»è¾‘æ€§å’ŒåŠæ—¶æ€§çš„éœ€æ±‚ï¼ŒæŒ‡å‡ºç°æœ‰è¯­è¨€æ¨¡å‹æ¨ç†æ–¹æ³•æœªèƒ½å……åˆ†è€ƒè™‘ç¯å¢ƒçš„åŠ¨æ€å˜åŒ–ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†å®æ—¶æ¨ç†ï¼ˆreal-time reasoningï¼‰çš„æ–°é—®é¢˜å½¢å¼ï¼Œå¹¶æ„å»ºäº†Real-Time Reasoning Gymä½œä¸ºæµ‹è¯•å¹³å°ã€‚ç ”ç©¶å›¢é˜Ÿæ¢è®¨äº†ä¸¤ç§éƒ¨ç½²èŒƒå¼ï¼šåˆ©ç”¨æœ‰é™è®¡ç®—è¿›è¡Œå¿«é€Ÿå“åº”çš„ååº”å¼æ™ºèƒ½ä½“ï¼ˆreactive agentsï¼‰å’Œå…è®¸æ‰©å±•è®¡ç®—ä»¥è§£å†³å¤æ‚é—®é¢˜çš„è§„åˆ’æ™ºèƒ½ä½“ï¼ˆplanning agentsï¼‰ï¼Œå‘ç°ç°æœ‰æœ€å…ˆè¿›æ¨¡å‹åœ¨ä»»ä¸€èŒƒå¼ä¸‹éƒ½éš¾ä»¥åŒæ—¶å®ç°é€»è¾‘å‡†ç¡®å’Œå³æ—¶å“åº”ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™ï¼Œè®ºæ–‡æå‡ºäº†AgileThinkerï¼Œè¿™æ˜¯ä¸€ç§åŒæ—¶ç»“åˆä¸Šè¿°ä¸¤ç§æ¨ç†èŒƒå¼çš„æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œéšç€ä»»åŠ¡éš¾åº¦å’Œæ—¶é—´å‹åŠ›çš„å¢åŠ ï¼ŒAgileThinkeré€šè¿‡æœ‰æ•ˆå¹³è¡¡æ¨ç†æ·±åº¦ä¸å“åº”å»¶è¿Ÿï¼Œå…¶è¡¨ç°æŒç»­ä¼˜äºä»…é‡‡ç”¨å•ä¸€èŒƒå¼çš„æ™ºèƒ½ä½“ã€‚è¿™é¡¹å·¥ä½œç¡®ç«‹äº†å®æ—¶æ¨ç†ä½œä¸ºå¼€å‘å®ç”¨æ™ºèƒ½ä½“çš„å…³é”®æµ‹è¯•å¹³å°ï¼Œä¸ºæ—¶é—´å—é™çš„AIç³»ç»Ÿç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "30 pages",
      "pdf_url": "https://arxiv.org/pdf/2511.04898v1",
      "published_date": "2025-11-07 00:51:02 UTC",
      "updated_date": "2025-11-07 00:51:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T15:02:00.935296+00:00"
    },
    {
      "arxiv_id": "2511.04886v1",
      "title": "Beta Distribution Learning for Reliable Roadway Crash Risk Assessment",
      "title_zh": "é¢å‘å¯é é“è·¯ç¢°æ’é£é™©è¯„ä¼°çš„Betaåˆ†å¸ƒå­¦ä¹ ",
      "authors": [
        "Ahmad Elallaf",
        "Nathan Jacobs",
        "Xinyue Ye",
        "Mei Chen",
        "Gongbo Liang"
      ],
      "abstract": "Roadway traffic accidents represent a global health crisis, responsible for over a million deaths annually and costing many countries up to 3% of their GDP. Traditional traffic safety studies often examine risk factors in isolation, overlooking the spatial complexity and contextual interactions inherent in the built environment. Furthermore, conventional Neural Network-based risk estimators typically generate point estimates without conveying model uncertainty, limiting their utility in critical decision-making. To address these shortcomings, we introduce a novel geospatial deep learning framework that leverages satellite imagery as a comprehensive spatial input. This approach enables the model to capture the nuanced spatial patterns and embedded environmental risk factors that contribute to fatal crash risks. Rather than producing a single deterministic output, our model estimates a full Beta probability distribution over fatal crash risk, yielding accurate and uncertainty-aware predictions--a critical feature for trustworthy AI in safety-critical applications. Our model outperforms baselines by achieving a 17-23% improvement in recall, a key metric for flagging potential dangers, while delivering superior calibration. By providing reliable and interpretable risk assessments from satellite imagery alone, our method enables safer autonomous navigation and offers a highly scalable tool for urban planners and policymakers to enhance roadway safety equitably and cost-effectively.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿäº¤é€šå®‰å…¨ç ”ç©¶å¿½è§†ç©ºé—´å¤æ‚æ€§ä»¥åŠå¸¸è§„ç¥ç»ç½‘ç»œç¼ºä¹æ¨¡å‹ä¸ç¡®å®šæ€§è¡¨è¾¾çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå«æ˜Ÿå›¾åƒçš„æ–°å‹åœ°ç†ç©ºé—´æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¨¡å‹å¹¶éç”Ÿæˆå•ä¸€çš„ç¡®å®šæ€§è¾“å‡ºï¼Œè€Œæ˜¯å­¦ä¹ è‡´å‘½è½¦ç¥¸é£é™©çš„å®Œæ•´Betaæ¦‚ç‡åˆ†å¸ƒ(Beta probability distribution)ï¼Œä»è€Œå®ç°ç²¾ç¡®ä¸”å…·å¤‡ä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„é¢„æµ‹ã€‚é€šè¿‡åˆ©ç”¨å«æ˜Ÿå›¾åƒæ•æ‰ç¯å¢ƒé£é™©å› ç´ å’Œç©ºé—´æ¨¡å¼ï¼Œè¯¥æ–¹æ³•åœ¨å…³é”®çš„å¬å›ç‡(recall)æŒ‡æ ‡ä¸Šæ¯”åŸºçº¿æ¨¡å‹æé«˜äº†17-23%ï¼ŒåŒæ—¶å±•ç°äº†æ›´ä¼˜çš„æ ¡å‡†èƒ½åŠ›ã€‚è¿™ç§ä»…ä¾èµ–å«æ˜Ÿå›¾åƒçš„å¯é é£é™©è¯„ä¼°æ–¹æ³•ï¼Œä¸ä»…èƒ½å¤Ÿæ”¯æŒæ›´å®‰å…¨çš„è‡ªåŠ¨é©¾é©¶å¯¼èˆªï¼Œä¹Ÿä¸ºåŸå¸‚è§„åˆ’è€…å’Œæ”¿ç­–åˆ¶å®šè€…æä¾›äº†ä¸€ç§é«˜æ•ˆã€å¯æ‰©å±•çš„é“è·¯å®‰å…¨æ”¹å–„å·¥å…·ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.04886v1",
      "published_date": "2025-11-07 00:08:55 UTC",
      "updated_date": "2025-11-07 00:08:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T15:03:07.036888+00:00"
    },
    {
      "arxiv_id": "2511.05626v1",
      "title": "LLMs as Packagers of HPC Software",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ä½œä¸ºé«˜æ€§èƒ½è®¡ç®—è½¯ä»¶çš„æ‰“åŒ…è€…",
      "authors": [
        "Caetano Melone",
        "Daniel Nichols",
        "Konstantinos Parasyris",
        "Todd Gamblin",
        "Harshitha Menon"
      ],
      "abstract": "High performance computing (HPC) software ecosystems are inherently heterogeneous, comprising scientific applications that depend on hundreds of external packages, each with distinct build systems, options, and dependency constraints. Tools such as Spack automate dependency resolution and environment management, but their effectiveness relies on manually written build recipes. As these ecosystems grow, maintaining existing specifications and creating new ones becomes increasingly labor-intensive. While large language models (LLMs) have shown promise in code generation, automatically producing correct and maintainable Spack recipes remains a significant challenge. We present a systematic analysis of how LLMs and context-augmentation methods can assist in the generation of Spack recipes. To this end, we introduce SpackIt, an end-to-end framework that combines repository analysis, retrieval of relevant examples, and iterative refinement through diagnostic feedback. We apply SpackIt to a representative subset of 308 open-source HPC packages to assess its effectiveness and limitations. Our results show that SpackIt increases installation success from 20% in a zero-shot setting to over 80% in its best configuration, demonstrating the value of retrieval and structured feedback for reliable package synthesis.",
      "tldr_zh": "é’ˆå¯¹é«˜æ€§èƒ½è®¡ç®—(HPC)è½¯ä»¶ç”Ÿæ€ç³»ç»Ÿä¸­ä¾èµ–å…³ç³»å¤æ‚ä¸”æ„å»ºé…æ–¹(build recipes)ç»´æŠ¤æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)è‡ªåŠ¨ç”ŸæˆSpacké…æ–¹çš„å¯è¡Œæ€§ã€‚ç ”ç©¶å›¢é˜Ÿæå‡ºäº†SpackItï¼Œè¿™æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ¡†æ¶ï¼Œç»“åˆäº†ä»£ç ä»“åº“åˆ†æã€ç›¸å…³ç¤ºä¾‹æ£€ç´¢ä»¥åŠåŸºäºè¯Šæ–­åé¦ˆçš„è¿­ä»£ä¼˜åŒ–æœºåˆ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡æ£€ç´¢å¢å¼ºå’Œç»“æ„åŒ–åé¦ˆï¼Œæ—¨åœ¨è§£å†³LLMsåœ¨ç”Ÿæˆæ­£ç¡®ä¸”å¯ç»´æŠ¤çš„Spacké…æ–¹æ—¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚åœ¨å¯¹308ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„å¼€æºHPCè½¯ä»¶åŒ…è¿›è¡Œçš„è¯„ä¼°ä¸­ï¼ŒSpackItè¡¨ç°æ˜¾è‘—ä¼˜äºé›¶æ ·æœ¬(zero-shot)åŸºçº¿ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSpackItçš„æœ€ä½³é…ç½®å°†å®‰è£…æˆåŠŸç‡ä»é›¶æ ·æœ¬è®¾ç½®ä¸‹çš„20%æå‡è‡³80%ä»¥ä¸Šï¼Œè¯æ˜äº†æ£€ç´¢å’Œåé¦ˆæœºåˆ¶åœ¨å¯é è½¯ä»¶åŒ…åˆæˆä¸­çš„ä»·å€¼ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05626v1",
      "published_date": "2025-11-07 00:06:51 UTC",
      "updated_date": "2025-11-07 00:06:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T15:03:11.657646+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 100,
  "processed_papers_count": 100,
  "failed_papers_count": 0,
  "llm_backup_calls": 200,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-25T15:05:13.610098+00:00"
}
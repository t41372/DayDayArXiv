[
  {
    "arxiv_id": "2507.11799v3",
    "title": "Fragment size density estimator for shrinkage-induced fracture based on a physics-informed neural network",
    "authors": [
      "Shin-ichi Ito"
    ],
    "abstract": "This paper presents a neural network (NN)-based solver for an integro-differential equation that models shrinkage-induced fragmentation. The proposed method directly maps input parameters to the corresponding probability density function without numerically solving the governing equation, thereby significantly reducing computational costs. Specifically, it enables efficient evaluation of the density function in Monte Carlo simulations while maintaining accuracy comparable to or even exceeding that of conventional finite difference schemes. Validatation on synthetic data demonstrates both the method's computational efficiency and predictive reliability. This study establishes a foundation for the data-driven inverse analysis of fragmentation and suggests the potential for extending the framework beyond pre-specified model structures.",
    "categories": [
      "physics.comp-ph",
      "cs.AI"
    ],
    "primary_category": "physics.comp-ph",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.11799v3",
    "published_date": "2025-07-15 23:33:05 UTC",
    "updated_date": "2025-08-13 15:01:40 UTC"
  },
  {
    "arxiv_id": "2507.11787v1",
    "title": "Survey of Swarm Intelligence Approaches to Search Documents Based On Semantic Similarity",
    "authors": [
      "Chandrashekar Muniyappa",
      "Eunjin Kim"
    ],
    "abstract": "Swarm Intelligence (SI) is gaining a lot of popularity in artificial intelligence, where the natural behavior of animals and insects is observed and translated into computer algorithms called swarm computing to solve real-world problems. Due to their effectiveness, they are applied in solving various computer optimization problems. This survey will review all the latest developments in Searching for documents based on semantic similarity using Swarm Intelligence algorithms and recommend future research directions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "CSAIDE '25: Proceedings of the 2025 4th International Conference on Cyber Security, Artificial Intelligence and the Digital Economy",
    "pdf_url": "https://arxiv.org/pdf/2507.11787v1",
    "published_date": "2025-07-15 23:03:52 UTC",
    "updated_date": "2025-07-15 23:03:52 UTC"
  },
  {
    "arxiv_id": "2507.11783v3",
    "title": "EEG Foundation Models: A Critical Review of Current Progress and Future Directions",
    "authors": [
      "Gayal Kuruppu",
      "Neeraj Wagh",
      "Vaclav Kremen",
      "Sandipan Pati",
      "Gregory Worrell",
      "Yogatheesan Varatharajah"
    ],
    "abstract": "Premise. Patterns of electrical brain activity recorded via electroencephalography (EEG) offer immense value for scientific and clinical investigations. The inability of supervised EEG encoders to learn robust EEG patterns and their over-reliance on expensive signal annotations have sparked a transition towards general-purpose self-supervised EEG encoders, i.e., EEG foundation models (EEG-FMs), for robust and scalable EEG feature extraction. However, the real-world readiness of early EEG-FMs and the rubrics for long-term research progress remain unclear. Objective. In this work, we conduct a review of ten early EEG-FMs to capture common trends and identify key directions for future development of EEG-FMs. Methods. We comparatively analyze each EEG-FM using three fundamental pillars of foundation modeling, namely the representation of input data, self-supervised modeling, and the evaluation strategy. Based on this analysis, we present a critical synthesis of EEG-FM methodology, empirical findings, and outstanding research gaps. Results. We find that most EEG-FMs adopt a sequence-based modeling scheme that relies on transformer-based backbones and the reconstruction of masked temporal EEG sequences for self-supervision. However, model evaluations remain heterogeneous and largely limited, making it challenging to assess their practical off-the-shelf utility. In addition to adopting standardized and realistic evaluations, future work should demonstrate more substantial scaling effects and make principled and trustworthy choices throughout the EEG representation learning pipeline. Significance. Our review indicates that the development of benchmarks, software tools, technical methodologies, and applications in collaboration with domain experts may advance the translational utility and real-world adoption of EEG-FMs.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG",
      "q-bio.NC"
    ],
    "primary_category": "eess.SP",
    "comment": "22 pages (main), 5 figures (main), 4 tables (main + supplement)",
    "pdf_url": "https://arxiv.org/pdf/2507.11783v3",
    "published_date": "2025-07-15 22:52:44 UTC",
    "updated_date": "2025-12-24 06:45:54 UTC"
  },
  {
    "arxiv_id": "2507.11776v1",
    "title": "Predicting Delayed Trajectories Using Network Features: A Study on the Dutch Railway Network",
    "authors": [
      "Merel Kampere",
      "Ali Mohammed Mansoor Alsahag"
    ],
    "abstract": "The Dutch railway network is one of the busiest in the world, with delays being a prominent concern for the principal passenger railway operator NS. This research addresses a gap in delay prediction studies within the Dutch railway network by employing an XGBoost Classifier with a focus on topological features. Current research predominantly emphasizes short-term predictions and neglects the broader network-wide patterns essential for mitigating ripple effects. This research implements and improves an existing methodology, originally designed to forecast the evolution of the fast-changing US air network, to predict delays in the Dutch Railways. By integrating Node Centrality Measures and comparing multiple classifiers like RandomForest, DecisionTree, GradientBoosting, AdaBoost, and LogisticRegression, the goal is to predict delayed trajectories. However, the results reveal limited performance, especially in non-simultaneous testing scenarios, suggesting the necessity for more context-specific adaptations. Regardless, this research contributes to the understanding of transportation network evaluation and proposes future directions for developing more robust predictive models for delays.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.11776v1",
    "published_date": "2025-07-15 22:30:36 UTC",
    "updated_date": "2025-07-15 22:30:36 UTC"
  },
  {
    "arxiv_id": "2507.11775v1",
    "title": "Challenges in GenAI and Authentication: a scoping review",
    "authors": [
      "Wesley dos Reis Bezerra",
      "Lais Machado Bezerra",
      "Carlos Becker Westphall"
    ],
    "abstract": "Authentication and authenticity have been a security challenge since the beginning of information sharing, especially in the context of digital information. With the advancement of generative artificial intelligence, these challenges have evolved, demanding a more up-to-date analysis of their impacts on society and system security. This work presents a scoping review that analyzed 88 documents from the IEEExplorer, Scopus, and ACM databases, promoting an analysis of the resulting portfolio through six guiding questions focusing on the most relevant work, challenges, attack surfaces, threats, proposed solutions, and gaps. Finally, the portfolio articles are analyzed through this guiding research lens and also receive individualized analysis. The results consistently outline the challenges, gaps, and threats related to images, text, audio, and video, thereby supporting new research in the areas of authentication and generative artificial intelligence.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.11775v1",
    "published_date": "2025-07-15 22:25:39 UTC",
    "updated_date": "2025-07-15 22:25:39 UTC"
  },
  {
    "arxiv_id": "2507.11773v1",
    "title": "Small Data Explainer -- The impact of small data methods in everyday life",
    "authors": [
      "Maren Hackenberg",
      "Sophia G. Connor",
      "Fabian Kabus",
      "June Brawner",
      "Ella Markham",
      "Mahi Hardalupas",
      "Areeq Chowdhury",
      "Rolf Backofen",
      "Anna Köttgen",
      "Angelika Rohde",
      "Nadine Binder",
      "Harald Binder",
      "the Collaborative Research Center 1597 Small Data"
    ],
    "abstract": "The emergence of breakthrough artificial intelligence (AI) techniques has led to a renewed focus on how small data settings, i.e., settings with limited information, can benefit from such developments. This includes societal issues such as how best to include under-represented groups in data-driven policy and decision making, or the health benefits of assistive technologies such as wearables. We provide a conceptual overview, in particular contrasting small data with big data, and identify common themes from exemplary case studies and application areas. Potential solutions are described in a more detailed technical overview of current data analysis and modelling techniques, highlighting contributions from different disciplines, such as knowledge-driven modelling from statistics and data-driven modelling from computer science. By linking application settings, conceptual contributions and specific techniques, we highlight what is already feasible and suggest what an agenda for fully leveraging small data might look like.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "Written in collaboration with the Royal Society, contributing to the Disability Technology report (https://royalsociety.org/news-resources/projects/disability-data-assistive-technology/)",
    "pdf_url": "https://arxiv.org/pdf/2507.11773v1",
    "published_date": "2025-07-15 22:24:17 UTC",
    "updated_date": "2025-07-15 22:24:17 UTC"
  },
  {
    "arxiv_id": "2507.11761v1",
    "title": "Beyond Task-Specific Reasoning: A Unified Conditional Generative Framework for Abstract Visual Reasoning",
    "authors": [
      "Fan Shi",
      "Bin Li",
      "Xiangyang Xue"
    ],
    "abstract": "Abstract visual reasoning (AVR) enables humans to quickly discover and generalize abstract rules to new scenarios. Designing intelligent systems with human-like AVR abilities has been a long-standing topic in the artificial intelligence community. Deep AVR solvers have recently achieved remarkable success in various AVR tasks. However, they usually use task-specific designs or parameters in different tasks. In such a paradigm, solving new tasks often means retraining the model, and sometimes retuning the model architectures, which increases the cost of solving AVR problems. In contrast to task-specific approaches, this paper proposes a novel Unified Conditional Generative Solver (UCGS), aiming to address multiple AVR tasks in a unified framework. First, we prove that some well-known AVR tasks can be reformulated as the problem of estimating the predictability of target images in problem panels. Then, we illustrate that, under the proposed framework, training one conditional generative model can solve various AVR tasks. The experiments show that with a single round of multi-task training, UCGS demonstrates abstract reasoning ability across various AVR tasks. Especially, UCGS exhibits the ability of zero-shot reasoning, enabling it to perform abstract reasoning on problems from unseen AVR tasks in the testing phase.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.11761v1",
    "published_date": "2025-07-15 21:54:51 UTC",
    "updated_date": "2025-07-15 21:54:51 UTC"
  },
  {
    "arxiv_id": "2507.12496v1",
    "title": "FOUNDER: Grounding Foundation Models in World Models for Open-Ended Embodied Decision Making",
    "authors": [
      "Yucen Wang",
      "Rui Yu",
      "Shenghua Wan",
      "Le Gan",
      "De-Chuan Zhan"
    ],
    "abstract": "Foundation Models (FMs) and World Models (WMs) offer complementary strengths in task generalization at different levels. In this work, we propose FOUNDER, a framework that integrates the generalizable knowledge embedded in FMs with the dynamic modeling capabilities of WMs to enable open-ended task solving in embodied environments in a reward-free manner. We learn a mapping function that grounds FM representations in the WM state space, effectively inferring the agent's physical states in the world simulator from external observations. This mapping enables the learning of a goal-conditioned policy through imagination during behavior learning, with the mapped task serving as the goal state. Our method leverages the predicted temporal distance to the goal state as an informative reward signal. FOUNDER demonstrates superior performance on various multi-task offline visual control benchmarks, excelling in capturing the deep-level semantics of tasks specified by text or videos, particularly in scenarios involving complex observations or domain gaps where prior methods struggle. The consistency of our learned reward function with the ground-truth reward is also empirically validated. Our project website is https://sites.google.com/view/founder-rl.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted by Forty-Second International Conference on Machine Learning (ICML 2025)",
    "pdf_url": "https://arxiv.org/pdf/2507.12496v1",
    "published_date": "2025-07-15 21:49:49 UTC",
    "updated_date": "2025-07-15 21:49:49 UTC"
  },
  {
    "arxiv_id": "2507.11751v1",
    "title": "Survey of Genetic and Differential Evolutionary Algorithm Approaches to Search Documents Based On Semantic Similarity",
    "authors": [
      "Chandrashekar Muniyappa",
      "Eunjin Kim"
    ],
    "abstract": "Identifying similar documents within extensive volumes of data poses a significant challenge. To tackle this issue, researchers have developed a variety of effective distributed computing techniques. With the advancement of computing power and the rise of big data, deep neural networks and evolutionary computing algorithms such as genetic algorithms and differential evolution algorithms have achieved greater success. This survey will explore the most recent advancements in the search for documents based on their semantic text similarity, focusing on genetic and differential evolutionary computing algorithms.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "CSAIDE '25: Proceedings of the 2025 4th International Conference on Cyber Security, Artificial Intelligence and the Digital Economy",
    "pdf_url": "https://arxiv.org/pdf/2507.11751v1",
    "published_date": "2025-07-15 21:30:16 UTC",
    "updated_date": "2025-07-15 21:30:16 UTC"
  },
  {
    "arxiv_id": "2507.11742v2",
    "title": "CRABS: A syntactic-semantic pincer strategy for bounding LLM interpretation of Python notebooks",
    "authors": [
      "Meng Li",
      "Timothy M. McPhillips",
      "Dingmin Wang",
      "Shin-Rong Tsai",
      "Bertram Ludäscher"
    ],
    "abstract": "Recognizing the information flows and operations comprising data science and machine learning Python notebooks is critical for evaluating, reusing, and adapting notebooks for new tasks. Investigating a notebook via re-execution often is impractical due to the challenges of resolving data and software dependencies. While Large Language Models (LLMs) pre-trained on large codebases have demonstrated effectiveness in understanding code without running it, we observe that they fail to understand some realistic notebooks due to hallucinations and long-context challenges. To address these issues, we propose a notebook understanding task yielding an information flow graph and corresponding cell execution dependency graph for a notebook, and demonstrate the effectiveness of a pincer strategy that uses limited syntactic analysis to assist full comprehension of the notebook using an LLM. Our Capture and Resolve Assisted Bounding Strategy (CRABS) employs shallow syntactic parsing and analysis of the abstract syntax tree (AST) to capture the correct interpretation of a notebook between lower and upper estimates of the inter-cell I/O set$\\unicode{x2014}$the flows of information into or out of cells via variables$\\unicode{x2014}$then uses an LLM to resolve remaining ambiguities via cell-by-cell zero-shot learning, thereby identifying the true data inputs and outputs of each cell. We evaluate and demonstrate the effectiveness of our approach using an annotated dataset of 50 representative, highly up-voted Kaggle notebooks that together represent 3454 actual cell inputs and outputs. The LLM correctly resolves 1397 of 1425 (98%) ambiguities left by analyzing the syntactic structure of these notebooks. Across 50 notebooks, CRABS achieves average F1 scores of 98% identifying cell-to-cell information flows and 99% identifying transitive cell execution dependencies.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to COLM 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.11742v2",
    "published_date": "2025-07-15 21:14:08 UTC",
    "updated_date": "2025-08-24 06:27:30 UTC"
  },
  {
    "arxiv_id": "2507.11737v1",
    "title": "Auto-Formulating Dynamic Programming Problems with Large Language Models",
    "authors": [
      "Chenyu Zhou",
      "Jingyuan Yang",
      "Linwei Xin",
      "Yitian Chen",
      "Ziyan He",
      "Dongdong Ge"
    ],
    "abstract": "Dynamic programming (DP) is a fundamental method in operations research, but formulating DP models has traditionally required expert knowledge of both the problem context and DP techniques. Large Language Models (LLMs) offer the potential to automate this process. However, DP problems pose unique challenges due to their inherently stochastic transitions and the limited availability of training data. These factors make it difficult to directly apply existing LLM-based models or frameworks developed for other optimization problems, such as linear or integer programming. We introduce DP-Bench, the first benchmark covering a wide range of textbook-level DP problems to enable systematic evaluation. We present Dynamic Programming Language Model (DPLM), a 7B-parameter specialized model that achieves performance comparable to state-of-the-art LLMs like OpenAI's o1 and DeepSeek-R1, and surpasses them on hard problems. Central to DPLM's effectiveness is DualReflect, our novel synthetic data generation pipeline, designed to scale up training data from a limited set of initial examples. DualReflect combines forward generation for diversity and backward generation for reliability. Our results reveal a key insight: backward generation is favored in low-data regimes for its strong correctness guarantees, while forward generation, though lacking such guarantees, becomes increasingly valuable at scale for introducing diverse formulations. This trade-off highlights the complementary strengths of both approaches and the importance of combining them.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.11737v1",
    "published_date": "2025-07-15 21:09:43 UTC",
    "updated_date": "2025-07-15 21:09:43 UTC"
  },
  {
    "arxiv_id": "2507.13383v1",
    "title": "Whose View of Safety? A Deep DIVE Dataset for Pluralistic Alignment of Text-to-Image Models",
    "authors": [
      "Charvi Rastogi",
      "Tian Huey Teh",
      "Pushkar Mishra",
      "Roma Patel",
      "Ding Wang",
      "Mark Díaz",
      "Alicia Parrish",
      "Aida Mostafazadeh Davani",
      "Zoe Ashwood",
      "Michela Paganini",
      "Vinodkumar Prabhakaran",
      "Verena Rieser",
      "Lora Aroyo"
    ],
    "abstract": "Current text-to-image (T2I) models often fail to account for diverse human experiences, leading to misaligned systems. We advocate for pluralistic alignment, where an AI understands and is steerable towards diverse, and often conflicting, human values. Our work provides three core contributions to achieve this in T2I models. First, we introduce a novel dataset for Diverse Intersectional Visual Evaluation (DIVE) -- the first multimodal dataset for pluralistic alignment. It enable deep alignment to diverse safety perspectives through a large pool of demographically intersectional human raters who provided extensive feedback across 1000 prompts, with high replication, capturing nuanced safety perceptions. Second, we empirically confirm demographics as a crucial proxy for diverse viewpoints in this domain, revealing significant, context-dependent differences in harm perception that diverge from conventional evaluations. Finally, we discuss implications for building aligned T2I models, including efficient data collection strategies, LLM judgment capabilities, and model steerability towards diverse perspectives. This research offers foundational tools for more equitable and aligned T2I systems. Content Warning: The paper includes sensitive content that may be harmful.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "28 pages, 16 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.13383v1",
    "published_date": "2025-07-15 21:02:35 UTC",
    "updated_date": "2025-07-15 21:02:35 UTC"
  },
  {
    "arxiv_id": "2507.11733v1",
    "title": "ClarifAI: Enhancing AI Interpretability and Transparency through Case-Based Reasoning and Ontology-Driven Approach for Improved Decision-Making",
    "authors": [
      "Srikanth Vemula"
    ],
    "abstract": "This Study introduces Clarity and Reasoning Interface for Artificial Intelligence(ClarifAI), a novel approach designed to augment the transparency and interpretability of artificial intelligence (AI) in the realm of improved decision making. Leveraging the Case-Based Reasoning (CBR) methodology and integrating an ontology-driven approach, ClarifAI aims to meet the intricate explanatory demands of various stakeholders involved in AI-powered applications. The paper elaborates on ClarifAI's theoretical foundations, combining CBR and ontologies to furnish exhaustive explanation mechanisms. It further elaborates on the design principles and architectural blueprint, highlighting ClarifAI's potential to enhance AI interpretability across different sectors and its applicability in high-stake environments. This research delineates the significant role of ClariAI in advancing the interpretability of AI systems, paving the way for its deployment in critical decision-making processes.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.11733v1",
    "published_date": "2025-07-15 21:02:28 UTC",
    "updated_date": "2025-07-15 21:02:28 UTC"
  },
  {
    "arxiv_id": "2507.11730v1",
    "title": "Seeing the Signs: A Survey of Edge-Deployable OCR Models for Billboard Visibility Analysis",
    "authors": [
      "Maciej Szankin",
      "Vidhyananth Venkatasamy",
      "Lihang Ying"
    ],
    "abstract": "Outdoor advertisements remain a critical medium for modern marketing, yet accurately verifying billboard text visibility under real-world conditions is still challenging. Traditional Optical Character Recognition (OCR) pipelines excel at cropped text recognition but often struggle with complex outdoor scenes, varying fonts, and weather-induced visual noise. Recently, multimodal Vision-Language Models (VLMs) have emerged as promising alternatives, offering end-to-end scene understanding with no explicit detection step. This work systematically benchmarks representative VLMs - including Qwen 2.5 VL 3B, InternVL3, and SmolVLM2 - against a compact CNN-based OCR baseline (PaddleOCRv4) across two public datasets (ICDAR 2015 and SVT), augmented with synthetic weather distortions to simulate realistic degradation. Our results reveal that while selected VLMs excel at holistic scene reasoning, lightweight CNN pipelines still achieve competitive accuracy for cropped text at a fraction of the computational cost-an important consideration for edge deployment. To foster future research, we release our weather-augmented benchmark and evaluation code publicly.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.11730v1",
    "published_date": "2025-07-15 20:58:24 UTC",
    "updated_date": "2025-07-15 20:58:24 UTC"
  },
  {
    "arxiv_id": "2507.11729v1",
    "title": "Globalization for Scalable Short-term Load Forecasting",
    "authors": [
      "Amirhossein Ahmadi",
      "Hamidreza Zareipour",
      "Henry Leung"
    ],
    "abstract": "Forecasting load in power transmission networks is essential across various hierarchical levels, from the system level down to individual points of delivery (PoD). While intuitive and locally accurate, traditional local forecasting models (LFMs) face significant limitations, particularly in handling generalizability, overfitting, data drift, and the cold start problem. These methods also struggle with scalability, becoming computationally expensive and less efficient as the network's size and data volume grow. In contrast, global forecasting models (GFMs) offer a new approach to enhance prediction generalizability, scalability, accuracy, and robustness through globalization and cross-learning. This paper investigates global load forecasting in the presence of data drifts, highlighting the impact of different modeling techniques and data heterogeneity. We explore feature-transforming and target-transforming models, demonstrating how globalization, data heterogeneity, and data drift affect each differently. In addition, we examine the role of globalization in peak load forecasting and its potential for hierarchical forecasting. To address data heterogeneity and the balance between globality and locality, we propose separate time series clustering (TSC) methods, introducing model-based TSC for feature-transforming models and new weighted instance-based TSC for target-transforming models. Through extensive experiments on a real-world dataset of Alberta's electricity load, we demonstrate that global target-transforming models consistently outperform their local counterparts, especially when enriched with global features and clustering techniques. In contrast, global feature-transforming models face challenges in balancing local and global dynamics, often requiring TSC to manage data heterogeneity effectively.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "63 pages with 22 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.11729v1",
    "published_date": "2025-07-15 20:58:14 UTC",
    "updated_date": "2025-07-15 20:58:14 UTC"
  },
  {
    "arxiv_id": "2507.12494v1",
    "title": "MR-LDM -- The Merge-Reactive Longitudinal Decision Model: Game Theoretic Human Decision Modeling for Interactive Sim Agents",
    "authors": [
      "Dustin Holley",
      "Jovin D'sa",
      "Hossein Nourkhiz Mahjoub",
      "Gibran Ali"
    ],
    "abstract": "Enhancing simulation environments to replicate real-world driver behavior, i.e., more humanlike sim agents, is essential for developing autonomous vehicle technology. In the context of highway merging, previous works have studied the operational-level yielding dynamics of lag vehicles in response to a merging car at highway on-ramps. Other works focusing on tactical decision modeling generally consider limited action sets or utilize payoff functions with large parameter sets and limited payoff bounds. In this work, we aim to improve the simulation of the highway merge scenario by targeting a game theoretic model for tactical decision-making with improved payoff functions and lag actions. We couple this with an underlying dynamics model to have a unified decision and dynamics model that can capture merging interactions and simulate more realistic interactions in an explainable and interpretable fashion. The proposed model demonstrated good reproducibility of complex interactions when validated on a real-world dataset. The model was finally integrated into a high fidelity simulation environment and confirmed to have adequate computation time efficiency for use in large-scale simulations to support autonomous vehicle development.",
    "categories": [
      "cs.AI",
      "cs.GT",
      "cs.MA",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages",
    "pdf_url": "https://arxiv.org/pdf/2507.12494v1",
    "published_date": "2025-07-15 20:41:00 UTC",
    "updated_date": "2025-07-15 20:41:00 UTC"
  },
  {
    "arxiv_id": "2507.11710v1",
    "title": "Subgraph Generation for Generalizing on Out-of-Distribution Links",
    "authors": [
      "Jay Revolinsky",
      "Harry Shomer",
      "Jiliang Tang"
    ],
    "abstract": "Graphs Neural Networks (GNNs) demonstrate high-performance on the link prediction (LP) task. However, these models often rely on all dataset samples being drawn from the same distribution. In addition, graph generative models (GGMs) show a pronounced ability to generate novel output graphs. Despite this, GGM applications remain largely limited to domain-specific tasks. To bridge this gap, we propose FLEX as a GGM framework which leverages two mechanism: (1) structurally-conditioned graph generation, and (2) adversarial co-training between an auto-encoder and GNN. As such, FLEX ensures structural-alignment between sample distributions to enhance link-prediction performance in out-of-distribution (OOD) scenarios. Notably, FLEX does not require expert knowledge to function in different OOD scenarios. Numerous experiments are conducted in synthetic and real-world OOD settings to demonstrate FLEX's performance-enhancing ability, with further analysis for understanding the effects of graph data augmentation on link structures. The source code is available here: https://github.com/revolins/FlexOOD.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "18 pages, 7 figures, preprint",
    "pdf_url": "https://arxiv.org/pdf/2507.11710v1",
    "published_date": "2025-07-15 20:30:16 UTC",
    "updated_date": "2025-07-15 20:30:16 UTC"
  },
  {
    "arxiv_id": "2507.12492v1",
    "title": "Sporadic Federated Learning Approach in Quantum Environment to Tackle Quantum Noise",
    "authors": [
      "Ratun Rahman",
      "Atit Pokharel",
      "Dinh C. Nguyen"
    ],
    "abstract": "Quantum Federated Learning (QFL) is an emerging paradigm that combines quantum computing and federated learning (FL) to enable decentralized model training while maintaining data privacy over quantum networks. However, quantum noise remains a significant barrier in QFL, since modern quantum devices experience heterogeneous noise levels due to variances in hardware quality and sensitivity to quantum decoherence, resulting in inadequate training performance. To address this issue, we propose SpoQFL, a novel QFL framework that leverages sporadic learning to mitigate quantum noise heterogeneity in distributed quantum systems. SpoQFL dynamically adjusts training strategies based on noise fluctuations, enhancing model robustness, convergence stability, and overall learning efficiency. Extensive experiments on real-world datasets demonstrate that SpoQFL significantly outperforms conventional QFL approaches, achieving superior training performance and more stable convergence.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "quant-ph",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.12492v1",
    "published_date": "2025-07-15 20:30:11 UTC",
    "updated_date": "2025-07-15 20:30:11 UTC"
  },
  {
    "arxiv_id": "2507.11702v1",
    "title": "Time series classification of satellite data using LSTM networks: an approach for predicting leaf-fall to minimize railroad traffic disruption",
    "authors": [
      "Hein de Wilde",
      "Ali Mohammed Mansoor Alsahag",
      "Pierre Blanchet"
    ],
    "abstract": "Railroad traffic disruption as a result of leaf-fall cost the UK rail industry over 300 million per year and measures to mitigate such disruptions are employed on a large scale, with 1.67 million kilometers of track being treated in the UK in 2021 alone. Therefore, the ability to anticipate the timing of leaf-fall would offer substantial benefits for rail network operators, enabling the efficient scheduling of such mitigation measures. However, current methodologies for predicting leaf-fall exhibit considerable limitations in terms of scalability and reliability. This study endeavors to devise a prediction system that leverages specialized prediction methods and the latest satellite data sources to generate both scalable and reliable insights into leaf-fall timings. An LSTM network trained on ground-truth leaf-falling data combined with multispectral and meteorological satellite data demonstrated a root-mean-square error of 6.32 days for predicting the start of leaf-fall and 9.31 days for predicting the end of leaf-fall. The model, which improves upon previous work on the topic, offers promising opportunities for the optimization of leaf mitigation measures in the railway industry and the improvement of our understanding of complex ecological systems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.11702v1",
    "published_date": "2025-07-15 20:13:31 UTC",
    "updated_date": "2025-07-15 20:13:31 UTC"
  },
  {
    "arxiv_id": "2507.12490v1",
    "title": "Spatially Grounded Explanations in Vision Language Models for Document Visual Question Answering",
    "authors": [
      "Maximiliano Hormazábal Lagos",
      "Héctor Cerezo-Costas",
      "Dimosthenis Karatzas"
    ],
    "abstract": "We introduce EaGERS, a fully training-free and model-agnostic pipeline that (1) generates natural language rationales via a vision language model, (2) grounds these rationales to spatial sub-regions by computing multimodal embedding similarities over a configurable grid with majority voting, and (3) restricts the generation of responses only from the relevant regions selected in the masked image. Experiments on the DocVQA dataset demonstrate that our best configuration not only outperforms the base model on exact match accuracy and Average Normalized Levenshtein Similarity metrics but also enhances transparency and reproducibility in DocVQA without additional model fine-tuning.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "This work has been accepted for presentation at the 16th Conference and Labs of the Evaluation Forum (CLEF 2025) and will be published in the proceedings by Springer in the Lecture Notes in Computer Science (LNCS) series. Please cite the published version when available",
    "pdf_url": "https://arxiv.org/pdf/2507.12490v1",
    "published_date": "2025-07-15 20:05:25 UTC",
    "updated_date": "2025-07-15 20:05:25 UTC"
  },
  {
    "arxiv_id": "2507.11694v1",
    "title": "ExpliCIT-QA: Explainable Code-Based Image Table Question Answering",
    "authors": [
      "Maximiliano Hormazábal Lagos",
      "Álvaro Bueno Sáez",
      "Pedro Alonso Doval",
      "Jorge Alcalde Vesteiro",
      "Héctor Cerezo-Costas"
    ],
    "abstract": "We present ExpliCIT-QA, a system that extends our previous MRT approach for tabular question answering into a multimodal pipeline capable of handling complex table images and providing explainable answers. ExpliCIT-QA follows a modular design, consisting of: (1) Multimodal Table Understanding, which uses a Chain-of-Thought approach to extract and transform content from table images; (2) Language-based Reasoning, where a step-by-step explanation in natural language is generated to solve the problem; (3) Automatic Code Generation, where Python/Pandas scripts are created based on the reasoning steps, with feedback for handling errors; (4) Code Execution to compute the final answer; and (5) Natural Language Explanation that describes how the answer was computed. The system is built for transparency and auditability: all intermediate outputs, parsed tables, reasoning steps, generated code, and final answers are available for inspection. This strategy works towards closing the explainability gap in end-to-end TableVQA systems. We evaluated ExpliCIT-QA on the TableVQA-Bench benchmark, comparing it with existing baselines. We demonstrated improvements in interpretability and transparency, which open the door for applications in sensitive domains like finance and healthcare where auditing results are critical.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "This work has been accepted for presentation at the 24nd Portuguese Conference on Artificial Intelligence (EPIA 2025) and will be published in the proceedings by Springer in the Lecture Notes in Computer Science (LNCS) series. Please cite the published version when available",
    "pdf_url": "https://arxiv.org/pdf/2507.11694v1",
    "published_date": "2025-07-15 19:51:24 UTC",
    "updated_date": "2025-07-15 19:51:24 UTC"
  },
  {
    "arxiv_id": "2507.11692v1",
    "title": "Galaxy image simplification using Generative AI",
    "authors": [
      "Sai Teja Erukude",
      "Lior Shamir"
    ],
    "abstract": "Modern digital sky surveys have been acquiring images of billions of galaxies. While these images often provide sufficient details to analyze the shape of the galaxies, accurate analysis of such high volumes of images requires effective automation. Current solutions often rely on machine learning annotation of the galaxy images based on a set of pre-defined classes. Here we introduce a new approach to galaxy image analysis that is based on generative AI. The method simplifies the galaxy images and automatically converts them into a ``skeletonized\" form. The simplified images allow accurate measurements of the galaxy shapes and analysis that is not limited to a certain pre-defined set of classes. We demonstrate the method by applying it to galaxy images acquired by the DESI Legacy Survey. The code and data are publicly available. The method was applied to 125,000 DESI Legacy Survey images, and the catalog of the simplified images is publicly available.",
    "categories": [
      "astro-ph.GA",
      "astro-ph.IM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "astro-ph.GA",
    "comment": "Astronomy and Computing, accepted",
    "pdf_url": "https://arxiv.org/pdf/2507.11692v1",
    "published_date": "2025-07-15 19:48:09 UTC",
    "updated_date": "2025-07-15 19:48:09 UTC"
  },
  {
    "arxiv_id": "2507.11683v3",
    "title": "PGT-I: Scaling Spatiotemporal GNNs with Memory-Efficient Distributed Training",
    "authors": [
      "Seth Ockerman",
      "Amal Gueroudji",
      "Tanwi Mallick",
      "Yixuan He",
      "Line Pouchard",
      "Robert Ross",
      "Shivaram Venkataraman"
    ],
    "abstract": "Spatiotemporal graph neural networks (ST-GNNs) are powerful tools for modeling spatial and temporal data dependencies. However, their applications have been limited primarily to small-scale datasets because of memory constraints. While distributed training offers a solution, current frameworks lack support for spatiotemporal models and overlook the properties of spatiotemporal data. Informed by a scaling study on a large-scale workload, we present PyTorch Geometric Temporal Index (PGT-I), an extension to PyTorch Geometric Temporal that integrates distributed data parallel training and two novel strategies: index-batching and distributed-index-batching. Our index techniques exploit spatiotemporal structure to construct snapshots dynamically at runtime, significantly reducing memory overhead, while distributed-index-batching extends this approach by enabling scalable processing across multiple GPUs. Our techniques enable the first-ever training of an ST-GNN on the entire PeMS dataset without graph partitioning, reducing peak memory usage by up to 89% and achieving up to a 11.78x speedup over standard DDP with 128 GPUs.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.DC",
    "comment": "To appear in the 2025 International Conference for High Performance Computing, Networking, Storage, and Analysis",
    "pdf_url": "https://arxiv.org/pdf/2507.11683v3",
    "published_date": "2025-07-15 19:38:16 UTC",
    "updated_date": "2025-09-15 18:57:17 UTC"
  },
  {
    "arxiv_id": "2507.11662v2",
    "title": "Let's Think in Two Steps: Mitigating Agreement Bias in MLLMs with Self-Grounded Verification",
    "authors": [
      "Moises Andrade",
      "Joonhyuk Cha",
      "Brandon Ho",
      "Vriksha Srihari",
      "Karmesh Yadav",
      "Zsolt Kira"
    ],
    "abstract": "Verifiers--functions assigning rewards to agent behavior--have been key for AI progress in domains like math and code. However, extending gains to domains without clear-cut success criteria (e.g., computer use) remains a challenge: while humans can recognize desired outcomes, translating this intuition into scalable rules is nontrivial. Multimodal Large Language Models (MLLMs) emerge as a promising solution, given their world knowledge, human-preference alignment, and reasoning skills. We evaluate MLLMs as verifiers across web navigation, computer use, and robotic manipulation, and identify a critical limitation: a strong tendency to over-validate agent behavior, a phenomenon we term agreement bias. This bias is pervasive across models, resilient to test-time scaling, and poses risks to existing methods relying on MLLM evaluations. We discuss methods to evaluate and improve MLLM verifiers and introduce Self-Grounded Verification (SGV), a lightweight method that harnesses MLLMs' own sampling mechanisms by modulating (un)conditional generation to better leverage their knowledge, alignment, and reasoning. SGV operates in two steps: first, the MLLM is elicited to generate broad priors about desired behavior, independent of the data under evaluation. Then, conditioned on self-generated priors, it reasons over and evaluates a candidate trajectory. SGV yields more human-aligned evaluations with gains of up to 25pp in failure detection, 14pp in accuracy, and benefits extending to downstream applications. In self-refinement and online supervision, SGV boosts task completion of a GUI specialist in OSWorld, a diffusion policy in robomimic, and a ReAct agent in VisualWebArena--setting a new state of the art, surpassing the previous best by 20pp. We release an updated version of VisualWebArena featuring more human-aligned evaluators, high-fidelity environment parallelism, and speedups of over 10x.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MA",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "Our code, models, and data are publicly available at https://mshalimay.github.io/agreement-bias-sgv/",
    "pdf_url": "https://arxiv.org/pdf/2507.11662v2",
    "published_date": "2025-07-15 18:50:29 UTC",
    "updated_date": "2025-12-23 11:29:24 UTC"
  },
  {
    "arxiv_id": "2507.11661v1",
    "title": "Partitioner Guided Modal Learning Framework",
    "authors": [
      "Guimin Hu",
      "Yi Xin",
      "Lijie Hu",
      "Zhihong Zhu",
      "Hasti Seifi"
    ],
    "abstract": "Multimodal learning benefits from multiple modal information, and each learned modal representations can be divided into uni-modal that can be learned from uni-modal training and paired-modal features that can be learned from cross-modal interaction. Building on this perspective, we propose a partitioner-guided modal learning framework, PgM, which consists of the modal partitioner, uni-modal learner, paired-modal learner, and uni-paired modal decoder. Modal partitioner segments the learned modal representation into uni-modal and paired-modal features. Modal learner incorporates two dedicated components for uni-modal and paired-modal learning. Uni-paired modal decoder reconstructs modal representation based on uni-modal and paired-modal features. PgM offers three key benefits: 1) thorough learning of uni-modal and paired-modal features, 2) flexible distribution adjustment for uni-modal and paired-modal representations to suit diverse downstream tasks, and 3) different learning rates across modalities and partitions. Extensive experiments demonstrate the effectiveness of PgM across four multimodal tasks and further highlight its transferability to existing models. Additionally, we visualize the distribution of uni-modal and paired-modal features across modalities and tasks, offering insights into their respective contributions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "acm multimedia 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.11661v1",
    "published_date": "2025-07-15 18:47:49 UTC",
    "updated_date": "2025-07-15 18:47:49 UTC"
  },
  {
    "arxiv_id": "2507.11655v1",
    "title": "Counting Answer Sets of Disjunctive Answer Set Programs",
    "authors": [
      "Mohimenul Kabir",
      "Supratik Chakraborty",
      "Kuldeep S Meel"
    ],
    "abstract": "Answer Set Programming (ASP) provides a powerful declarative paradigm for knowledge representation and reasoning. Recently, counting answer sets has emerged as an important computational problem with applications in probabilistic reasoning, network reliability analysis, and other domains. This has motivated significant research into designing efficient ASP counters. While substantial progress has been made for normal logic programs, the development of practical counters for disjunctive logic programs remains challenging.\n  We present SharpASP-SR, a novel framework for counting answer sets of disjunctive logic programs based on subtractive reduction to projected propositional model counting. Our approach introduces an alternative characterization of answer sets that enables efficient reduction while ensuring that intermediate representations remain of polynomial size. This allows SharpASP-SR to leverage recent advances in projected model counting technology. Through extensive experimental evaluation on diverse benchmarks, we demonstrate that SharpASP-SR significantly outperforms existing counters on instances with large answer set counts. Building on these results, we develop a hybrid counting approach that combines enumeration techniques with SharpASP-SR to achieve state-of-the-art performance across the full spectrum of disjunctive programs.",
    "categories": [
      "cs.LO",
      "cs.AI"
    ],
    "primary_category": "cs.LO",
    "comment": "Under consideration in Theory and Practice of Logic Programming (TPLP)",
    "pdf_url": "https://arxiv.org/pdf/2507.11655v1",
    "published_date": "2025-07-15 18:41:19 UTC",
    "updated_date": "2025-07-15 18:41:19 UTC"
  },
  {
    "arxiv_id": "2507.11645v1",
    "title": "Tracing the Path to Grokking: Embeddings, Dropout, and Network Activation",
    "authors": [
      "Ahmed Salah",
      "David Yevick"
    ],
    "abstract": "Grokking refers to delayed generalization in which the increase in test accuracy of a neural network occurs appreciably after the improvement in training accuracy This paper introduces several practical metrics including variance under dropout, robustness, embedding similarity, and sparsity measures, that can forecast grokking behavior. Specifically, the resilience of neural networks to noise during inference is estimated from a Dropout Robustness Curve (DRC) obtained from the variation of the accuracy with the dropout rate as the model transitions from memorization to generalization. The variance of the test accuracy under stochastic dropout across training checkpoints further exhibits a local maximum during the grokking. Additionally, the percentage of inactive neurons decreases during generalization, while the embeddings tend to a bimodal distribution independent of initialization that correlates with the observed cosine similarity patterns and dataset symmetries. These metrics additionally provide valuable insight into the origin and behaviour of grokking.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages, 11 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.11645v1",
    "published_date": "2025-07-15 18:30:42 UTC",
    "updated_date": "2025-07-15 18:30:42 UTC"
  },
  {
    "arxiv_id": "2507.11638v1",
    "title": "Interpretable Prediction of Lymph Node Metastasis in Rectal Cancer MRI Using Variational Autoencoders",
    "authors": [
      "Benjamin Keel",
      "Aaron Quyn",
      "David Jayne",
      "Maryam Mohsin",
      "Samuel D. Relton"
    ],
    "abstract": "Effective treatment for rectal cancer relies on accurate lymph node metastasis (LNM) staging. However, radiological criteria based on lymph node (LN) size, shape and texture morphology have limited diagnostic accuracy. In this work, we investigate applying a Variational Autoencoder (VAE) as a feature encoder model to replace the large pre-trained Convolutional Neural Network (CNN) used in existing approaches. The motivation for using a VAE is that the generative model aims to reconstruct the images, so it directly encodes visual features and meaningful patterns across the data. This leads to a disentangled and structured latent space which can be more interpretable than a CNN. Models are deployed on an in-house MRI dataset with 168 patients who did not undergo neo-adjuvant treatment. The post-operative pathological N stage was used as the ground truth to evaluate model predictions. Our proposed model 'VAE-MLP' achieved state-of-the-art performance on the MRI dataset, with cross-validated metrics of AUC 0.86 +/- 0.05, Sensitivity 0.79 +/- 0.06, and Specificity 0.85 +/- 0.05. Code is available at: https://github.com/benkeel/Lymph_Node_Classification_MIUA.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Published in Medical Image Understanding and Analysis (MIUA) 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.11638v1",
    "published_date": "2025-07-15 18:20:38 UTC",
    "updated_date": "2025-07-15 18:20:38 UTC"
  },
  {
    "arxiv_id": "2507.11636v1",
    "title": "JSQA: Speech Quality Assessment with Perceptually-Inspired Contrastive Pretraining Based on JND Audio Pairs",
    "authors": [
      "Junyi Fan",
      "Donald Williamson"
    ],
    "abstract": "Speech quality assessment (SQA) is often used to learn a mapping from a high-dimensional input space to a scalar that represents the mean opinion score (MOS) of the perceptual speech quality. Learning such a mapping is challenging for many reasons, but largely because MOS exhibits high levels of inherent variance due to perceptual and experimental-design differences. Many solutions have been proposed, but many approaches do not properly incorporate perceptual factors into their learning algorithms (beyond the MOS label), which could lead to unsatisfactory results. To this end, we propose JSQA, a two-stage framework that pretrains an audio encoder using perceptually-guided contrastive learning on just noticeable difference (JND) pairs, followed by fine-tuning for MOS prediction. We first generate pairs of audio data within JND levels, which are then used to pretrain an encoder to leverage perceptual quality similarity information and map it into an embedding space. The JND pairs come from clean LibriSpeech utterances that are mixed with background noise from CHiME-3, at different signal-to-noise ratios (SNRs). The encoder is later fine-tuned with audio samples from the NISQA dataset for MOS prediction. Experimental results suggest that perceptually-inspired contrastive pretraining significantly improves the model performance evaluated by various metrics when compared against the same network trained from scratch without pretraining. These findings suggest that incorporating perceptual factors into pretraining greatly contributes to the improvement in performance for SQA.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted to WASPAA 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.11636v1",
    "published_date": "2025-07-15 18:16:46 UTC",
    "updated_date": "2025-07-15 18:16:46 UTC"
  },
  {
    "arxiv_id": "2507.11634v1",
    "title": "Cross-lingual Few-shot Learning for Persian Sentiment Analysis with Incremental Adaptation",
    "authors": [
      "Farideh Majidi",
      "Ziaeddin Beheshtifard"
    ],
    "abstract": "This research examines cross-lingual sentiment analysis using few-shot learning and incremental learning methods in Persian. The main objective is to develop a model capable of performing sentiment analysis in Persian using limited data, while getting prior knowledge from high-resource languages. To achieve this, three pre-trained multilingual models (XLM-RoBERTa, mDeBERTa, and DistilBERT) were employed, which were fine-tuned using few-shot and incremental learning approaches on small samples of Persian data from diverse sources, including X, Instagram, Digikala, Snappfood, and Taaghche. This variety enabled the models to learn from a broad range of contexts. Experimental results show that the mDeBERTa and XLM-RoBERTa achieved high performances, reaching 96% accuracy on Persian sentiment analysis. These findings highlight the effectiveness of combining few-shot learning and incremental learning with multilingual pre-trained models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Proceedings of the First National Conference on Artificial Intelligence and Emerging Research: Convergence of Humans and Intelligent Systems",
    "pdf_url": "https://arxiv.org/pdf/2507.11634v1",
    "published_date": "2025-07-15 18:13:25 UTC",
    "updated_date": "2025-07-15 18:13:25 UTC"
  },
  {
    "arxiv_id": "2507.11633v1",
    "title": "General Modular Harness for LLM Agents in Multi-Turn Gaming Environments",
    "authors": [
      "Yuxuan Zhang",
      "Haoyang Yu",
      "Lanxiang Hu",
      "Haojian Jin",
      "Hao Zhang"
    ],
    "abstract": "We introduce a modular harness design for LLM agents that composes of perception, memory, and reasoning components, enabling a single LLM or VLM backbone to tackle a wide spectrum of multi turn gaming environments without domain-specific engineering. Using classic and modern game suites as low-barrier, high-diversity testbeds, our framework provides a unified workflow for analyzing how each module affects performance across dynamic interactive settings. Extensive experiments demonstrate that the harness lifts gameplay performance consistently over un-harnessed baselines and reveals distinct contribution patterns, for example, memory dominates in long-horizon puzzles while perception is critical in vision noisy arcades. These findings highlight the effectiveness of our modular harness design in advancing general-purpose agent, given the familiarity and ubiquity of games in everyday human experience.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, ICML MAS workshop",
    "pdf_url": "https://arxiv.org/pdf/2507.11633v1",
    "published_date": "2025-07-15 18:13:04 UTC",
    "updated_date": "2025-07-15 18:13:04 UTC"
  },
  {
    "arxiv_id": "2507.11630v2",
    "title": "Jailbreak-Tuning: Models Efficiently Learn Jailbreak Susceptibility",
    "authors": [
      "Brendan Murphy",
      "Dillon Bowen",
      "Shahrad Mohammadzadeh",
      "Tom Tseng",
      "Julius Broomfield",
      "Adam Gleave",
      "Kellin Pelrine"
    ],
    "abstract": "AI systems are rapidly advancing in capability, and frontier model developers broadly acknowledge the need for safeguards against serious misuse. However, this paper demonstrates that fine-tuning, whether via open weights or closed fine-tuning APIs, can produce helpful-only models with safeguards destroyed. In contrast to prior work which is blocked by modern moderation systems or achieved only partial removal of safeguards or degraded output quality, our jailbreak-tuning method teaches models to generate detailed, high-quality responses to arbitrary harmful requests. For example, OpenAI, Google, and Anthropic models will fully comply with requests for CBRN assistance, executing cyberattacks, and other criminal activity. We further show that backdoors can increase not only the stealth but also the severity of attacks. Stronger jailbreak prompts become even more effective in fine-tuning attacks, linking attacks and potentially defenses in the input and weight spaces. Not only are current models vulnerable, more recent ones also appear to be becoming even more vulnerable to these attacks, underscoring the urgent need for tamper-resistant safeguards. Until such safeguards are discovered, companies and policymakers should view the release of any fine-tunable model as simultaneously releasing its evil twin: equally capable as the original model, and usable for any malicious purpose within its capabilities.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.11630v2",
    "published_date": "2025-07-15 18:10:29 UTC",
    "updated_date": "2025-09-20 08:53:22 UTC"
  },
  {
    "arxiv_id": "2507.11625v2",
    "title": "MapIQ: Evaluating Multimodal Large Language Models for Map Question Answering",
    "authors": [
      "Varun Srivastava",
      "Fan Lei",
      "Srija Mukhopadhyay",
      "Vivek Gupta",
      "Ross Maciejewski"
    ],
    "abstract": "Recent advancements in multimodal large language models (MLLMs) have driven researchers to explore how well these models read data visualizations, e.g., bar charts, scatter plots. More recently, attention has shifted to visual question answering with maps (Map-VQA). However, Map-VQA research has primarily focused on choropleth maps, which cover only a limited range of thematic categories and visual analytical tasks. To address these gaps, we introduce MapIQ, a benchmark dataset comprising 14,706 question-answer pairs across three map types: choropleth maps, cartograms, and proportional symbol maps spanning topics from six distinct themes (e.g., housing, crime). We evaluate multiple MLLMs using six visual analytical tasks, comparing their performance against one another and a human baseline. An additional experiment examining the impact of map design changes (e.g., altered color schemes, modified legend designs, and removal of map elements) provides insights into the robustness and sensitivity of MLLMs, their reliance on internal geographic knowledge, and potential avenues for improving Map-VQA performance.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Published as a conference paper at COLM 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.11625v2",
    "published_date": "2025-07-15 18:02:57 UTC",
    "updated_date": "2025-10-03 18:52:18 UTC"
  },
  {
    "arxiv_id": "2507.11623v2",
    "title": "A Roadmap for Climate-Relevant Robotics Research",
    "authors": [
      "Alan Papalia",
      "Charles Dawson",
      "Laurentiu L. Anton",
      "Norhan Magdy Bayomi",
      "Bianca Champenois",
      "Jung-Hoon Cho",
      "Levi Cai",
      "Joseph DelPreto",
      "Kristen Edwards",
      "Bilha-Catherine Githinji",
      "Cameron Hickert",
      "Vindula Jayawardana",
      "Matthew Kramer",
      "Shreyaa Raghavan",
      "David Russell",
      "Shide Salimi",
      "Jingnan Shi",
      "Soumya Sudhakar",
      "Yanwei Wang",
      "Shouyi Wang",
      "Luca Carlone",
      "Vijay Kumar",
      "Daniela Rus",
      "John E. Fernandez",
      "Cathy Wu",
      "George Kantor",
      "Derek Young",
      "Hanumant Singh"
    ],
    "abstract": "Climate change is one of the defining challenges of the 21st century, and many in the robotics community are looking for ways to contribute. This paper presents a roadmap for climate-relevant robotics research, identifying high-impact opportunities for collaboration between roboticists and experts across climate domains such as energy, the built environment, transportation, industry, land use, and Earth sciences. These applications include problems such as energy systems optimization, construction, precision agriculture, building envelope retrofits, autonomous trucking, and large-scale environmental monitoring. Critically, we include opportunities to apply not only physical robots but also the broader robotics toolkit - including planning, perception, control, and estimation algorithms - to climate-relevant problems. A central goal of this roadmap is to inspire new research directions and collaboration by highlighting specific, actionable problems at the intersection of robotics and climate. This work represents a collaboration between robotics researchers and domain experts in various climate disciplines, and it serves as an invitation to the robotics community to bring their expertise to bear on urgent climate priorities.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.11623v2",
    "published_date": "2025-07-15 18:01:49 UTC",
    "updated_date": "2025-07-17 16:00:19 UTC"
  },
  {
    "arxiv_id": "2507.11621v1",
    "title": "HCOMC: A Hierarchical Cooperative On-Ramp Merging Control Framework in Mixed Traffic Environment on Two-Lane Highways",
    "authors": [
      "Tianyi Wang",
      "Yangyang Wang",
      "Jie Pan",
      "Junfeng Jiao",
      "Christian Claudel"
    ],
    "abstract": "Highway on-ramp merging areas are common bottlenecks to traffic congestion and accidents. Currently, a cooperative control strategy based on connected and automated vehicles (CAVs) is a fundamental solution to this problem. While CAVs are not fully widespread, it is necessary to propose a hierarchical cooperative on-ramp merging control (HCOMC) framework for heterogeneous traffic flow on two-lane highways to address this gap. This paper extends longitudinal car-following models based on the intelligent driver model and lateral lane-changing models using the quintic polynomial curve to account for human-driven vehicles (HDVs) and CAVs, comprehensively considering human factors and cooperative adaptive cruise control. Besides, this paper proposes a HCOMC framework, consisting of a hierarchical cooperative planning model based on the modified virtual vehicle model, a discretionary lane-changing model based on game theory, and a multi-objective optimization model using the elitist non-dominated sorting genetic algorithm to ensure the safe, smooth, and efficient merging process. Then, the performance of our HCOMC is analyzed under different traffic densities and CAV penetration rates through simulation. The findings underscore our HCOMC's pronounced comprehensive advantages in enhancing the safety of group vehicles, stabilizing and expediting merging process, optimizing traffic efficiency, and economizing fuel consumption compared with benchmarks.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "7 pages, 2 figures, 3 tables, accepted for IEEE International Conference on Intelligent Transportation Systems (ITSC) 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.11621v1",
    "published_date": "2025-07-15 18:01:29 UTC",
    "updated_date": "2025-07-15 18:01:29 UTC"
  },
  {
    "arxiv_id": "2507.11620v2",
    "title": "Learning Representations of Event Time Series with Sparse Autoencoders for Anomaly Detection, Similarity Search, and Unsupervised Classification",
    "authors": [
      "Steven Dillmann",
      "Juan Rafael Martínez-Galarza"
    ],
    "abstract": "Event time series are sequences of discrete events occurring at irregular time intervals, each associated with a domain-specific observational modality. They are common in domains such as high-energy astrophysics, computational social science, cybersecurity, finance, healthcare, neuroscience, and seismology. Their unstructured and irregular structure poses significant challenges for extracting meaningful patterns and identifying salient phenomena using conventional techniques. We propose novel two- and three-dimensional tensor representations for event time series, coupled with sparse autoencoders that learn physically meaningful latent representations. These embeddings support a variety of downstream tasks, including anomaly detection, similarity-based retrieval, semantic clustering, and unsupervised classification. We demonstrate our approach on a real-world dataset from X-ray astronomy, showing that these representations successfully capture temporal and spectral signatures and isolate diverse classes of X-ray transients. Our framework offers a flexible, scalable, and generalizable solution for analyzing complex, irregular event time series across scientific and industrial domains.",
    "categories": [
      "cs.LG",
      "astro-ph.HE",
      "astro-ph.IM",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at the 2025 ICML Workshop on Machine Learning for Astrophysics, Code available at: https://github.com/StevenDillmann/ml-xraytransients-mnras",
    "pdf_url": "https://arxiv.org/pdf/2507.11620v2",
    "published_date": "2025-07-15 18:01:03 UTC",
    "updated_date": "2025-10-11 01:41:10 UTC"
  },
  {
    "arxiv_id": "2507.11539v1",
    "title": "Streaming 4D Visual Geometry Transformer",
    "authors": [
      "Dong Zhuo",
      "Wenzhao Zheng",
      "Jiahe Guo",
      "Yuqi Wu",
      "Jie Zhou",
      "Jiwen Lu"
    ],
    "abstract": "Perceiving and reconstructing 4D spatial-temporal geometry from videos is a fundamental yet challenging computer vision task. To facilitate interactive and real-time applications, we propose a streaming 4D visual geometry transformer that shares a similar philosophy with autoregressive large language models. We explore a simple and efficient design and employ a causal transformer architecture to process the input sequence in an online manner. We use temporal causal attention and cache the historical keys and values as implicit memory to enable efficient streaming long-term 4D reconstruction. This design can handle real-time 4D reconstruction by incrementally integrating historical information while maintaining high-quality spatial consistency. For efficient training, we propose to distill knowledge from the dense bidirectional visual geometry grounded transformer (VGGT) to our causal model. For inference, our model supports the migration of optimized efficient attention operator (e.g., FlashAttention) from the field of large language models. Extensive experiments on various 4D geometry perception benchmarks demonstrate that our model increases the inference speed in online scenarios while maintaining competitive performance, paving the way for scalable and interactive 4D vision systems. Code is available at: https://github.com/wzzheng/StreamVGGT.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Code is available at: https://github.com/wzzheng/StreamVGGT",
    "pdf_url": "https://arxiv.org/pdf/2507.11539v1",
    "published_date": "2025-07-15 17:59:57 UTC",
    "updated_date": "2025-07-15 17:59:57 UTC"
  },
  {
    "arxiv_id": "2507.11538v1",
    "title": "How Many Instructions Can LLMs Follow at Once?",
    "authors": [
      "Daniel Jaroslawicz",
      "Brendan Whiting",
      "Parth Shah",
      "Karime Maamari"
    ],
    "abstract": "Production-grade LLM systems require robust adherence to dozens or even hundreds of instructions simultaneously. However, the instruction-following capabilities of LLMs at high instruction densities have not yet been characterized, as existing benchmarks only evaluate models on tasks with a single or few instructions. We introduce IFScale, a simple benchmark of 500 keyword-inclusion instructions for a business report writing task to measure how instruction-following performance degrades as instruction density increases. We evaluate 20 state-of-the-art models across seven major providers and find that even the best frontier models only achieve 68% accuracy at the max density of 500 instructions. Our analysis reveals model size and reasoning capability to correlate with 3 distinct performance degradation patterns, bias towards earlier instructions, and distinct categories of instruction-following errors. Our insights can help inform design of instruction-dense prompts in real-world applications and highlight important performance-latency tradeoffs. We open-source the benchmark and all results for further analysis at https://distylai.github.io/IFScale.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.11538v1",
    "published_date": "2025-07-15 17:59:42 UTC",
    "updated_date": "2025-07-15 17:59:42 UTC"
  },
  {
    "arxiv_id": "2507.11597v1",
    "title": "AI, Humans, and Data Science: Optimizing Roles Across Workflows and the Workforce",
    "authors": [
      "Richard Timpone",
      "Yongwei Yang"
    ],
    "abstract": "AI is transforming research. It is being leveraged to construct surveys, synthesize data, conduct analysis, and write summaries of the results. While the promise is to create efficiencies and increase quality, the reality is not always as clear cut. Leveraging our framework of Truth, Beauty, and Justice (TBJ) which we use to evaluate AI, machine learning and computational models for effective and ethical use (Taber and Timpone 1997; Timpone and Yang 2024), we consider the potential and limitation of analytic, generative, and agentic AI to augment data scientists or take on tasks traditionally done by human analysts and researchers. While AI can be leveraged to assist analysts in their tasks, we raise some warnings about push-button automation. Just as earlier eras of survey analysis created some issues when the increased ease of using statistical software allowed researchers to conduct analyses they did not fully understand, the new AI tools may create similar but larger risks. We emphasize a human-machine collaboration perspective (Daugherty and Wilson 2018) throughout the data science workflow and particularly call out the vital role that data scientists play under VUCA decision areas. We conclude by encouraging the advance of AI tools to complement data scientists but advocate for continued training and understanding of methods to ensure the substantive value of research is fully achieved by applying, interpreting, and acting upon results most effectively and ethically.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "Paper prepared for the 2025 European Survey Research Association Conference; 30 pages, 5 tables and 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.11597v1",
    "published_date": "2025-07-15 17:59:06 UTC",
    "updated_date": "2025-07-15 17:59:06 UTC"
  },
  {
    "arxiv_id": "2507.11527v1",
    "title": "DrafterBench: Benchmarking Large Language Models for Tasks Automation in Civil Engineering",
    "authors": [
      "Yinsheng Li",
      "Zhen Dong",
      "Yi Shao"
    ],
    "abstract": "Large Language Model (LLM) agents have shown great potential for solving real-world problems and promise to be a solution for tasks automation in industry. However, more benchmarks are needed to systematically evaluate automation agents from an industrial perspective, for example, in Civil Engineering. Therefore, we propose DrafterBench for the comprehensive evaluation of LLM agents in the context of technical drawing revision, a representation task in civil engineering. DrafterBench contains twelve types of tasks summarized from real-world drawing files, with 46 customized functions/tools and 1920 tasks in total. DrafterBench is an open-source benchmark to rigorously test AI agents' proficiency in interpreting intricate and long-context instructions, leveraging prior knowledge, and adapting to dynamic instruction quality via implicit policy awareness. The toolkit comprehensively assesses distinct capabilities in structured data comprehension, function execution, instruction following, and critical reasoning. DrafterBench offers detailed analysis of task accuracy and error statistics, aiming to provide deeper insight into agent capabilities and identify improvement targets for integrating LLMs in engineering applications. Our benchmark is available at https://github.com/Eason-Li-AIS/DrafterBench, with the test set hosted at https://huggingface.co/datasets/Eason666/DrafterBench.",
    "categories": [
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.AI",
    "comment": "Project page: https://github.com/Eason-Li-AIS/DrafterBench",
    "pdf_url": "https://arxiv.org/pdf/2507.11527v1",
    "published_date": "2025-07-15 17:56:04 UTC",
    "updated_date": "2025-07-15 17:56:04 UTC"
  },
  {
    "arxiv_id": "2507.11515v1",
    "title": "AirLLM: Diffusion Policy-based Adaptive LoRA for Remote Fine-Tuning of LLM over the Air",
    "authors": [
      "Shiyi Yang",
      "Xiaoxue Yu",
      "Rongpeng Li",
      "Jianhang Zhu",
      "Zhifeng Zhao",
      "Honggang Zhang"
    ],
    "abstract": "Operating Large Language Models (LLMs) on edge devices is increasingly challenged by limited communication bandwidth and strained computational and memory costs. Thus, cloud-assisted remote fine-tuning becomes indispensable. Nevertheless, existing Low-Rank Adaptation (LoRA) approaches typically employ fixed or heuristic rank configurations, and the subsequent over-the-air transmission of all LoRA parameters could be rather inefficient. To address this limitation, we develop AirLLM, a hierarchical diffusion policy framework for communication-aware LoRA adaptation. Specifically, AirLLM models the rank configuration as a structured action vector that spans all LoRA-inserted projections. To solve the underlying high-dimensional sequential decision-making problem, a Proximal Policy Optimization (PPO) agent generates coarse-grained decisions by jointly observing wireless states and linguistic complexity, which are then refined via Denoising Diffusion Implicit Models (DDIM) to produce high-resolution, task- and channel-adaptive rank vectors. The two modules are optimized alternatively, with the DDIM trained under the Classifier-Free Guidance (CFG) paradigm to maintain alignment with PPO rewards. Experiments under varying signal-to-noise ratios demonstrate that AirLLM consistently enhances fine-tuning performance while significantly reducing transmission costs, highlighting the effectiveness of reinforcement-driven, diffusion-refined rank adaptation for scalable and efficient remote fine-tuning over the air.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, 8 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.11515v1",
    "published_date": "2025-07-15 17:36:37 UTC",
    "updated_date": "2025-07-15 17:36:37 UTC"
  },
  {
    "arxiv_id": "2507.11513v1",
    "title": "Recursive Bound-Constrained AdaGrad with Applications to Multilevel and Domain Decomposition Minimization",
    "authors": [
      "Serge Gratton",
      "Alena Kopaničáková",
      "Philippe Toint"
    ],
    "abstract": "Two OFFO (Objective-Function Free Optimization) noise tolerant algorithms are presented that handle bound constraints, inexact gradients and use second-order information when available.The first is a multi-level method exploiting a hierarchical description of the problem and the second is a domain-decomposition method covering the standard addditive Schwarz decompositions. Both are generalizations of the first-order AdaGrad algorithm for unconstrained optimization. Because these algorithms share a common theoretical framework, a single convergence/complexity theory is provided which covers them both. Its main result is that, with high probability, both methods need at most $O(ε^{-2})$ iterations and noisy gradient evaluations to compute an $ε$-approximate first-order critical point of the bound-constrained problem. Extensive numerical experiments are discussed on applications ranging from PDE-based problems to deep neural network training, illustrating their remarkable computational efficiency.",
    "categories": [
      "math.OC",
      "cs.AI",
      "math.NA"
    ],
    "primary_category": "math.OC",
    "comment": "33 pages",
    "pdf_url": "https://arxiv.org/pdf/2507.11513v1",
    "published_date": "2025-07-15 17:32:10 UTC",
    "updated_date": "2025-07-15 17:32:10 UTC"
  },
  {
    "arxiv_id": "2507.22908v1",
    "title": "A Privacy-Preserving Federated Framework with Hybrid Quantum-Enhanced Learning for Financial Fraud Detection",
    "authors": [
      "Abhishek Sawaika",
      "Swetang Krishna",
      "Tushar Tomar",
      "Durga Pritam Suggisetti",
      "Aditi Lal",
      "Tanmaya Shrivastav",
      "Nouhaila Innan",
      "Muhammad Shafique"
    ],
    "abstract": "Rapid growth of digital transactions has led to a surge in fraudulent activities, challenging traditional detection methods in the financial sector. To tackle this problem, we introduce a specialised federated learning framework that uniquely combines a quantum-enhanced Long Short-Term Memory (LSTM) model with advanced privacy preserving techniques. By integrating quantum layers into the LSTM architecture, our approach adeptly captures complex cross-transactional patters, resulting in an approximate 5% performance improvement across key evaluation metrics compared to conventional models. Central to our framework is \"FedRansel\", a novel method designed to defend against poisoning and inference attacks, thereby reducing model degradation and inference accuracy by 4-8%, compared to standard differential privacy mechanisms. This pseudo-centralised setup with a Quantum LSTM model, enhances fraud detection accuracy and reinforces the security and confidentiality of sensitive financial data.",
    "categories": [
      "q-fin.CP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-fin.CP",
    "comment": "To be published in proceedings of IEEE International Conference on Quantum Computing and Engineering (QCE) 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.22908v1",
    "published_date": "2025-07-15 17:29:12 UTC",
    "updated_date": "2025-07-15 17:29:12 UTC"
  },
  {
    "arxiv_id": "2507.11595v1",
    "title": "A Study on the Application of Artificial Intelligence in Ecological Design",
    "authors": [
      "Hengyue Zhao"
    ],
    "abstract": "This paper asks whether our relationship with nature can move from human dominance to genuine interdependence, and whether artificial intelligence (AI) can mediate that shift. We examine a new ecological-design paradigm in which AI interacts with non-human life forms. Through case studies we show how artists and designers apply AI for data analysis, image recognition, and ecological restoration, producing results that differ from conventional media. We argue that AI not only expands creative methods but also reframes the theory and practice of ecological design. Building on the author's prototype for AI-assisted water remediation, the study proposes design pathways that couple reinforcement learning with plant-based phytoremediation. The findings highlight AI's potential to link scientific insight, artistic practice, and environmental stewardship, offering a roadmap for future research on sustainable, technology-enabled ecosystems.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.11595v1",
    "published_date": "2025-07-15 17:03:33 UTC",
    "updated_date": "2025-07-15 17:03:33 UTC"
  },
  {
    "arxiv_id": "2507.11488v1",
    "title": "COLIBRI Fuzzy Model: Color Linguistic-Based Representation and Interpretation",
    "authors": [
      "Pakizar Shamoi",
      "Nuray Toganas",
      "Muragul Muratbekova",
      "Elnara Kadyrgali",
      "Adilet Yerkin",
      "Ayan Igali",
      "Malika Ziyada",
      "Ayana Adilova",
      "Aron Karatayev",
      "Yerdauit Torekhan"
    ],
    "abstract": "Colors are omnipresent in today's world and play a vital role in how humans perceive and interact with their surroundings. However, it is challenging for computers to imitate human color perception. This paper introduces the Human Perception-Based Fuzzy Color Model, COLIBRI (Color Linguistic-Based Representation and Interpretation), designed to bridge the gap between computational color representations and human visual perception. The proposed model uses fuzzy sets and logic to create a framework for color categorization. Using a three-phase experimental approach, the study first identifies distinguishable color stimuli for hue, saturation, and intensity through preliminary experiments, followed by a large-scale human categorization survey involving more than 1000 human subjects. The resulting data are used to extract fuzzy partitions and generate membership functions that reflect real-world perceptual uncertainty. The model incorporates a mechanism for adaptation that allows refinement based on feedback and contextual changes. Comparative evaluations demonstrate the model's alignment with human perception compared to traditional color models, such as RGB, HSV, and LAB. To the best of our knowledge, no previous research has documented the construction of a model for color attribute specification based on a sample of this size or a comparable sample of the human population (n = 2496). Our findings are significant for fields such as design, artificial intelligence, marketing, and human-computer interaction, where perceptually relevant color representation is critical.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "submitted to IEEE for consideration",
    "pdf_url": "https://arxiv.org/pdf/2507.11488v1",
    "published_date": "2025-07-15 17:01:45 UTC",
    "updated_date": "2025-07-15 17:01:45 UTC"
  },
  {
    "arxiv_id": "2507.11482v3",
    "title": "Illuminating the Three Dogmas of Reinforcement Learning under Evolutionary Light",
    "authors": [
      "Mani Hamidi",
      "Terrence W. Deacon"
    ],
    "abstract": "Three core tenets of reinforcement learning (RL)--concerning the definition of agency, the objective of learning, and the scope of the reward hypothesis--have been highlighted as key targets for conceptual revision, with major implications for theory and application. We propose a framework, inspired by open-ended evolutionary theory, to reconsider these three \"dogmas.\" We revisit each assumption and address related concerns raised alongside them. To make our arguments relevant to RL as a model of biological learning, we first establish that evolutionary dynamics can plausibly operate within living brains over an individual's lifetime, and are not confined to cross-generational processes. We begin by revisiting the second dogma, drawing on evolutionary insights to enrich the \"adaptation-rather-than-search\" view of learning. We then address the third dogma regarding the limits of the reward hypothesis, using analogies from evolutionary fitness to illuminate the scalar reward vs. multi-objective debate. After discussing practical implications for exploration in RL, we turn to the first--and arguably most fundamental--issue: the absence of a formal account of agency. We argue that unlike the other two problems, the evolutionary paradigm alone cannot resolve the agency question, though it gestures in a productive direction. We advocate integrating ideas from origins-of-life theory, where the thermodynamics of sustenance and replication offer promising foundations for understanding agency and resource-constrained reinforcement learning in biological systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.11482v3",
    "published_date": "2025-07-15 16:53:14 UTC",
    "updated_date": "2025-07-28 18:54:04 UTC"
  },
  {
    "arxiv_id": "2507.11473v2",
    "title": "Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety",
    "authors": [
      "Tomek Korbak",
      "Mikita Balesni",
      "Elizabeth Barnes",
      "Yoshua Bengio",
      "Joe Benton",
      "Joseph Bloom",
      "Mark Chen",
      "Alan Cooney",
      "Allan Dafoe",
      "Anca Dragan",
      "Scott Emmons",
      "Owain Evans",
      "David Farhi",
      "Ryan Greenblatt",
      "Dan Hendrycks",
      "Marius Hobbhahn",
      "Evan Hubinger",
      "Geoffrey Irving",
      "Erik Jenner",
      "Daniel Kokotajlo",
      "Victoria Krakovna",
      "Shane Legg",
      "David Lindner",
      "David Luan",
      "Aleksander Mądry",
      "Julian Michael",
      "Neel Nanda",
      "Dave Orr",
      "Jakub Pachocki",
      "Ethan Perez",
      "Mary Phuong",
      "Fabien Roger",
      "Joshua Saxe",
      "Buck Shlegeris",
      "Martín Soto",
      "Eric Steinberger",
      "Jasmine Wang",
      "Wojciech Zaremba",
      "Bowen Baker",
      "Rohin Shah",
      "Vlad Mikulik"
    ],
    "abstract": "AI systems that \"think\" in human language offer a unique opportunity for AI safety: we can monitor their chains of thought (CoT) for the intent to misbehave. Like all other known AI oversight methods, CoT monitoring is imperfect and allows some misbehavior to go unnoticed. Nevertheless, it shows promise and we recommend further research into CoT monitorability and investment in CoT monitoring alongside existing safety methods. Because CoT monitorability may be fragile, we recommend that frontier model developers consider the impact of development decisions on CoT monitorability.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.11473v2",
    "published_date": "2025-07-15 16:43:41 UTC",
    "updated_date": "2025-12-07 02:14:12 UTC"
  },
  {
    "arxiv_id": "2507.11467v1",
    "title": "Modeling Code: Is Text All You Need?",
    "authors": [
      "Daniel Nichols",
      "Konstantinos Parasyris",
      "Harshitha Menon",
      "Brian R. Bartoldson",
      "Giorgis Georgakoudis",
      "Tal Ben-Nun",
      "Abhinav Bhatele"
    ],
    "abstract": "Code LLMs have become extremely popular recently for modeling source code across a variety of tasks, such as generation, translation, and summarization. However, transformer-based models are limited in their capabilities to reason through structured, analytical properties of code, such as control and data flow. Previous work has explored the modeling of these properties with structured data and graph neural networks. However, these approaches lack the generative capabilities and scale of modern LLMs. In this work, we introduce a novel approach to combine the strengths of modeling both code as text and more structured forms.",
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.11467v1",
    "published_date": "2025-07-15 16:39:12 UTC",
    "updated_date": "2025-07-15 16:39:12 UTC"
  },
  {
    "arxiv_id": "2507.11443v2",
    "title": "COLI: A Hierarchical Efficient Compressor for Large Images",
    "authors": [
      "Haoran Wang",
      "Hanyu Pei",
      "Yang Lyu",
      "Kai Zhang",
      "Li Li",
      "Feng-Lei Fan"
    ],
    "abstract": "The escalating adoption of high-resolution, large-field-of-view imagery amplifies the need for efficient compression methodologies. Conventional techniques frequently fail to preserve critical image details, while data-driven approaches exhibit limited generalizability. Implicit Neural Representations (INRs) present a promising alternative by learning continuous mappings from spatial coordinates to pixel intensities for individual images, thereby storing network weights rather than raw pixels and avoiding the generalization problem. However, INR-based compression of large images faces challenges including slow compression speed and suboptimal compression ratios. To address these limitations, we introduce COLI (Compressor for Large Images), a novel framework leveraging Neural Representations for Videos (NeRV). First, recognizing that INR-based compression constitutes a training process, we accelerate its convergence through a pretraining-finetuning paradigm, mixed-precision training, and reformulation of the sequential loss into a parallelizable objective. Second, capitalizing on INRs' transformation of image storage constraints into weight storage, we implement Hyper-Compression, a novel post-training technique to substantially enhance compression ratios while maintaining minimal output distortion. Evaluations across two medical imaging datasets demonstrate that COLI consistently achieves competitive or superior PSNR and SSIM metrics at significantly reduced bits per pixel (bpp), while accelerating NeRV training by up to 4 times.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.11443v2",
    "published_date": "2025-07-15 16:07:07 UTC",
    "updated_date": "2025-11-24 08:27:03 UTC"
  },
  {
    "arxiv_id": "2507.11436v1",
    "title": "Toward Improving fNIRS Classification: A Study on Activation Functions in Deep Neural Architectures",
    "authors": [
      "Behtom Adeli",
      "John McLinden",
      "Pankaj Pandey",
      "Ming Shao",
      "Yalda Shahriari"
    ],
    "abstract": "Activation functions are critical to the performance of deep neural networks, particularly in domains such as functional near-infrared spectroscopy (fNIRS), where nonlinearity, low signal-to-noise ratio (SNR), and signal variability poses significant challenges to model accuracy. However, the impact of activation functions on deep learning (DL) performance in the fNIRS domain remains underexplored and lacks systematic investigation in the current literature. This study evaluates a range of conventional and field-specific activation functions for fNIRS classification tasks using multiple deep learning architectures, including the domain-specific fNIRSNet, AbsoluteNet, MDNN, and shallowConvNet (as the baseline), all tested on a single dataset recorded during an auditory task. To ensure fair a comparison, all networks were trained and tested using standardized preprocessing and consistent training parameters. The results show that symmetrical activation functions such as Tanh and the Absolute value function Abs(x) can outperform commonly used functions like the Rectified Linear Unit (ReLU), depending on the architecture. Additionally, a focused analysis of the role of symmetry was conducted using a Modified Absolute Function (MAF), with results further supporting the effectiveness of symmetrical activation functions on performance gains. These findings underscore the importance of selecting proper activation functions that align with the signal characteristics of fNIRS data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.11436v1",
    "published_date": "2025-07-15 15:58:36 UTC",
    "updated_date": "2025-07-15 15:58:36 UTC"
  },
  {
    "arxiv_id": "2507.11415v1",
    "title": "U-RWKV: Lightweight medical image segmentation with direction-adaptive RWKV",
    "authors": [
      "Hongbo Ye",
      "Fenghe Tang",
      "Peiang Zhao",
      "Zhen Huang",
      "Dexin Zhao",
      "Minghao Bian",
      "S. Kevin Zhou"
    ],
    "abstract": "Achieving equity in healthcare accessibility requires lightweight yet high-performance solutions for medical image segmentation, particularly in resource-limited settings. Existing methods like U-Net and its variants often suffer from limited global Effective Receptive Fields (ERFs), hindering their ability to capture long-range dependencies. To address this, we propose U-RWKV, a novel framework leveraging the Recurrent Weighted Key-Value(RWKV) architecture, which achieves efficient long-range modeling at O(N) computational cost. The framework introduces two key innovations: the Direction-Adaptive RWKV Module(DARM) and the Stage-Adaptive Squeeze-and-Excitation Module(SASE). DARM employs Dual-RWKV and QuadScan mechanisms to aggregate contextual cues across images, mitigating directional bias while preserving global context and maintaining high computational efficiency. SASE dynamically adapts its architecture to different feature extraction stages, balancing high-resolution detail preservation and semantic relationship capture. Experiments demonstrate that U-RWKV achieves state-of-the-art segmentation performance with high computational efficiency, offering a practical solution for democratizing advanced medical imaging technologies in resource-constrained environments. The code is available at https://github.com/hbyecoding/U-RWKV.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "Accepted by MICCAI2025",
    "pdf_url": "https://arxiv.org/pdf/2507.11415v1",
    "published_date": "2025-07-15 15:40:17 UTC",
    "updated_date": "2025-07-15 15:40:17 UTC"
  },
  {
    "arxiv_id": "2507.11408v2",
    "title": "KisMATH: Do LLMs Have Knowledge of Implicit Structures in Mathematical Reasoning?",
    "authors": [
      "Soumadeep Saha",
      "Akshay Chaturvedi",
      "Saptarshi Saha",
      "Utpal Garain",
      "Nicholas Asher"
    ],
    "abstract": "Chain-of-thought (CoT) traces have been shown to improve performance of large language models on a plethora of reasoning tasks, yet there is no consensus on the mechanism by which this boost is achieved. To shed more light on this, we introduce Causal CoT Graphs (CCGraphs), which are directed acyclic graphs automatically extracted from reasoning traces that model fine-grained causal dependencies in language-model outputs. A collection of 1671 mathematical reasoning problems from MATH500, GSM8K, and AIME, together with their associated CCGraphs, has been compiled into our dataset -- KisMATH. Our detailed empirical analysis with 15 open-weight LLMs shows that (i) reasoning nodes in the CCGraphs are causal contributors to the final answer, which we argue is constitutive of reasoning; and (ii) LLMs emphasize the reasoning paths captured by the CCGraphs, indicating that the models internally realize structures similar to our graphs. KisMATH enables controlled, graph-aligned interventions and opens avenues for further investigation into the role of CoT in LLM reasoning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Pre-print; Accepted to TACL",
    "pdf_url": "https://arxiv.org/pdf/2507.11408v2",
    "published_date": "2025-07-15 15:28:37 UTC",
    "updated_date": "2026-01-19 08:53:46 UTC"
  },
  {
    "arxiv_id": "2507.11407v2",
    "title": "EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and Reasoning Modes",
    "authors": [
      "Kyunghoon Bae",
      "Eunbi Choi",
      "Kibong Choi",
      "Stanley Jungkyu Choi",
      "Yemuk Choi",
      "Kyubeen Han",
      "Seokhee Hong",
      "Junwon Hwang",
      "Taewan Hwang",
      "Joonwon Jang",
      "Hyojin Jeon",
      "Kijeong Jeon",
      "Gerrard Jeongwon Jo",
      "Hyunjik Jo",
      "Jiyeon Jung",
      "Euisoon Kim",
      "Hyosang Kim",
      "Jihoon Kim",
      "Joonkee Kim",
      "Seonghwan Kim",
      "Soyeon Kim",
      "Sunkyoung Kim",
      "Yireun Kim",
      "Yongil Kim",
      "Youchul Kim",
      "Edward Hwayoung Lee",
      "Gwangho Lee",
      "Haeju Lee",
      "Honglak Lee",
      "Jinsik Lee",
      "Kyungmin Lee",
      "Sangha Park",
      "Young Min Paik",
      "Yongmin Park",
      "Youngyong Park",
      "Sanghyun Seo",
      "Sihoon Yang",
      "Heuiyeen Yeen",
      "Sihyuk Yi",
      "Hyeongu Yun"
    ],
    "abstract": "This technical report introduces EXAONE 4.0, which integrates a Non-reasoning mode and a Reasoning mode to achieve both the excellent usability of EXAONE 3.5 and the advanced reasoning abilities of EXAONE Deep. To pave the way for the agentic AI era, EXAONE 4.0 incorporates essential features such as agentic tool use, and its multilingual capabilities are extended to support Spanish in addition to English and Korean. The EXAONE 4.0 model series consists of two sizes: a mid-size 32B model optimized for high performance, and a small-size 1.2B model designed for on-device applications. The EXAONE 4.0 demonstrates superior performance compared to open-weight models in its class and remains competitive even against frontier-class models. The models are publicly available for research purposes and can be easily downloaded via https://huggingface.co/LGAI-EXAONE.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Technical Report, 30 Pages",
    "pdf_url": "https://arxiv.org/pdf/2507.11407v2",
    "published_date": "2025-07-15 15:24:51 UTC",
    "updated_date": "2026-01-02 12:04:56 UTC"
  },
  {
    "arxiv_id": "2507.11387v1",
    "title": "From Kinetic Theory to AI: a Rediscovery of High-Dimensional Divergences and Their Properties",
    "authors": [
      "Gennaro Auricchio",
      "Giovanni Brigati",
      "Paolo Giudici",
      "Giuseppe Toscani"
    ],
    "abstract": "Selecting an appropriate divergence measure is a critical aspect of machine learning, as it directly impacts model performance. Among the most widely used, we find the Kullback-Leibler (KL) divergence, originally introduced in kinetic theory as a measure of relative entropy between probability distributions. Just as in machine learning, the ability to quantify the proximity of probability distributions plays a central role in kinetic theory. In this paper, we present a comparative review of divergence measures rooted in kinetic theory, highlighting their theoretical foundations and exploring their potential applications in machine learning and artificial intelligence.",
    "categories": [
      "math-ph",
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "math-ph",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.11387v1",
    "published_date": "2025-07-15 14:56:25 UTC",
    "updated_date": "2025-07-15 14:56:25 UTC"
  },
  {
    "arxiv_id": "2507.11588v2",
    "title": "SToFM: a Multi-scale Foundation Model for Spatial Transcriptomics",
    "authors": [
      "Suyuan Zhao",
      "Yizhen Luo",
      "Ganbo Yang",
      "Yan Zhong",
      "Hao Zhou",
      "Zaiqing Nie"
    ],
    "abstract": "Spatial Transcriptomics (ST) technologies provide biologists with rich insights into single-cell biology by preserving spatial context of cells. Building foundational models for ST can significantly enhance the analysis of vast and complex data sources, unlocking new perspectives on the intricacies of biological tissues. However, modeling ST data is inherently challenging due to the need to extract multi-scale information from tissue slices containing vast numbers of cells. This process requires integrating macro-scale tissue morphology, micro-scale cellular microenvironment, and gene-scale gene expression profile. To address this challenge, we propose SToFM, a multi-scale Spatial Transcriptomics Foundation Model. SToFM first performs multi-scale information extraction on each ST slice, to construct a set of ST sub-slices that aggregate macro-, micro- and gene-scale information. Then an SE(2) Transformer is used to obtain high-quality cell representations from the sub-slices. Additionally, we construct \\textbf{SToCorpus-88M}, the largest high-resolution spatial transcriptomics corpus for pretraining. SToFM achieves outstanding performance on a variety of downstream tasks, such as tissue region semantic segmentation and cell type annotation, demonstrating its comprehensive understanding of ST data through capturing and integrating multi-scale information.",
    "categories": [
      "q-bio.GN",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.GN",
    "comment": "Accpeted by ICML 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.11588v2",
    "published_date": "2025-07-15 14:47:01 UTC",
    "updated_date": "2025-07-23 15:22:26 UTC"
  },
  {
    "arxiv_id": "2507.11372v1",
    "title": "Attributes Shape the Embedding Space of Face Recognition Models",
    "authors": [
      "Pierrick Leroy",
      "Antonio Mastropietro",
      "Marco Nurisso",
      "Francesco Vaccarino"
    ],
    "abstract": "Face Recognition (FR) tasks have made significant progress with the advent of Deep Neural Networks, particularly through margin-based triplet losses that embed facial images into high-dimensional feature spaces. During training, these contrastive losses focus exclusively on identity information as labels. However, we observe a multiscale geometric structure emerging in the embedding space, influenced by interpretable facial (e.g., hair color) and image attributes (e.g., contrast). We propose a geometric approach to describe the dependence or invariance of FR models to these attributes and introduce a physics-inspired alignment metric. We evaluate the proposed metric on controlled, simplified models and widely used FR models fine-tuned with synthetic data for targeted attribute augmentation. Our findings reveal that the models exhibit varying degrees of invariance across different attributes, providing insight into their strengths and weaknesses and enabling deeper interpretability. Code available here: https://github.com/mantonios107/attrs-fr-embs}{https://github.com/mantonios107/attrs-fr-embs",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.11372v1",
    "published_date": "2025-07-15 14:44:39 UTC",
    "updated_date": "2025-07-15 14:44:39 UTC"
  },
  {
    "arxiv_id": "2507.11367v1",
    "title": "Local Pairwise Distance Matching for Backpropagation-Free Reinforcement Learning",
    "authors": [
      "Daniel Tanneberg"
    ],
    "abstract": "Training neural networks with reinforcement learning (RL) typically relies on backpropagation (BP), necessitating storage of activations from the forward pass for subsequent backward updates. Furthermore, backpropagating error signals through multiple layers often leads to vanishing or exploding gradients, which can degrade learning performance and stability. We propose a novel approach that trains each layer of the neural network using local signals during the forward pass in RL settings. Our approach introduces local, layer-wise losses leveraging the principle of matching pairwise distances from multi-dimensional scaling, enhanced with optional reward-driven guidance. This method allows each hidden layer to be trained using local signals computed during forward propagation, thus eliminating the need for backward passes and storing intermediate activations. Our experiments, conducted with policy gradient methods across common RL benchmarks, demonstrate that this backpropagation-free method achieves competitive performance compared to their classical BP-based counterpart. Additionally, the proposed method enhances stability and consistency within and across runs, and improves performance especially in challenging environments.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "accepted at the European Conference on Artificial Intelligence (ECAI 2025)",
    "pdf_url": "https://arxiv.org/pdf/2507.11367v1",
    "published_date": "2025-07-15 14:39:41 UTC",
    "updated_date": "2025-07-15 14:39:41 UTC"
  },
  {
    "arxiv_id": "2507.11352v1",
    "title": "Foundation Models for Logistics: Toward Certifiable, Conversational Planning Interfaces",
    "authors": [
      "Yunhao Yang",
      "Neel P. Bhatt",
      "Christian Ellis",
      "Alvaro Velasquez",
      "Zhangyang Wang",
      "Ufuk Topcu"
    ],
    "abstract": "Logistics operators, from battlefield coordinators rerouting airlifts ahead of a storm to warehouse managers juggling late trucks, often face life-critical decisions that demand both domain expertise and rapid and continuous replanning. While popular methods like integer programming yield logistics plans that satisfy user-defined logical constraints, they are slow and assume an idealized mathematical model of the environment that does not account for uncertainty. On the other hand, large language models (LLMs) can handle uncertainty and promise to accelerate replanning while lowering the barrier to entry by translating free-form utterances into executable plans, yet they remain prone to misinterpretations and hallucinations that jeopardize safety and cost. We introduce a neurosymbolic framework that pairs the accessibility of natural-language dialogue with verifiable guarantees on goal interpretation. It converts user requests into structured planning specifications, quantifies its own uncertainty at the field and token level, and invokes an interactive clarification loop whenever confidence falls below an adaptive threshold. A lightweight model, fine-tuned on just 100 uncertainty-filtered examples, surpasses the zero-shot performance of GPT-4.1 while cutting inference latency by nearly 50%. These preliminary results highlight a practical path toward certifiable, real-time, and user-aligned decision-making for complex logistics.",
    "categories": [
      "cs.AI",
      "cs.FL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.11352v1",
    "published_date": "2025-07-15 14:24:01 UTC",
    "updated_date": "2025-07-15 14:24:01 UTC"
  },
  {
    "arxiv_id": "2507.11345v1",
    "title": "Acting and Planning with Hierarchical Operational Models on a Mobile Robot: A Study with RAE+UPOM",
    "authors": [
      "Oscar Lima",
      "Marc Vinci",
      "Sunandita Patra",
      "Sebastian Stock",
      "Joachim Hertzberg",
      "Martin Atzmueller",
      "Malik Ghallab",
      "Dana Nau",
      "Paolo Traverso"
    ],
    "abstract": "Robotic task execution faces challenges due to the inconsistency between symbolic planner models and the rich control structures actually running on the robot. In this paper, we present the first physical deployment of an integrated actor-planner system that shares hierarchical operational models for both acting and planning, interleaving the Reactive Acting Engine (RAE) with an anytime UCT-like Monte Carlo planner (UPOM). We implement RAE+UPOM on a mobile manipulator in a real-world deployment for an object collection task. Our experiments demonstrate robust task execution under action failures and sensor noise, and provide empirical insights into the interleaved acting-and-planning decision making process.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted in ECMR 2025 conference",
    "pdf_url": "https://arxiv.org/pdf/2507.11345v1",
    "published_date": "2025-07-15 14:20:26 UTC",
    "updated_date": "2025-07-15 14:20:26 UTC"
  },
  {
    "arxiv_id": "2507.11334v2",
    "title": "CogDDN: A Cognitive Demand-Driven Navigation with Decision Optimization and Dual-Process Thinking",
    "authors": [
      "Yuehao Huang",
      "Liang Liu",
      "Shuangming Lei",
      "Yukai Ma",
      "Hao Su",
      "Jianbiao Mei",
      "Pengxiang Zhao",
      "Yaqing Gu",
      "Yong Liu",
      "Jiajun Lv"
    ],
    "abstract": "Mobile robots are increasingly required to navigate and interact within unknown and unstructured environments to meet human demands. Demand-driven navigation (DDN) enables robots to identify and locate objects based on implicit human intent, even when object locations are unknown. However, traditional data-driven DDN methods rely on pre-collected data for model training and decision-making, limiting their generalization capability in unseen scenarios. In this paper, we propose CogDDN, a VLM-based framework that emulates the human cognitive and learning mechanisms by integrating fast and slow thinking systems and selectively identifying key objects essential to fulfilling user demands. CogDDN identifies appropriate target objects by semantically aligning detected objects with the given instructions. Furthermore, it incorporates a dual-process decision-making module, comprising a Heuristic Process for rapid, efficient decisions and an Analytic Process that analyzes past errors, accumulates them in a knowledge base, and continuously improves performance. Chain of Thought (CoT) reasoning strengthens the decision-making process. Extensive closed-loop evaluations on the AI2Thor simulator with the ProcThor dataset show that CogDDN outperforms single-view camera-only methods by 15\\%, demonstrating significant improvements in navigation accuracy and adaptability. The project page is available at https://yuehaohuang.github.io/CogDDN/.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by ACM MM 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.11334v2",
    "published_date": "2025-07-15 14:06:24 UTC",
    "updated_date": "2025-08-15 09:03:09 UTC"
  },
  {
    "arxiv_id": "2507.11331v4",
    "title": "SystolicAttention: Fusing FlashAttention within a Single Systolic Array",
    "authors": [
      "Jiawei Lin",
      "Yuanlong Li",
      "Guokai Chen",
      "Thomas Bourgeat"
    ],
    "abstract": "Transformer models rely heavily on the scaled dot-product attention (SDPA) operation, typically implemented as FlashAttention. Characterized by its frequent interleaving of matrix multiplications and softmax operations, FlashAttention fails to fully utilize the compute resources of modern systolic-array-based accelerators designed for consecutive and large matrix multiplications.\n  To fully unleash the performance potential of systolic arrays for FlashAttention, we propose FSA, an enhanced systolic array architecture that runs the entire FlashAttention on the array without external vector units. Combined with SystolicAttention, an optimized kernel for FSA that achieves fine-grained and element-wise overlapping of FlashAttention operations, FSA maximizes array utilization while preserving the original floating-point operation order of FlashAttention. We implement FSA in synthesizable RTL and evaluate its performance against state-of-the-art systolic-array-based accelerators. Our results show that FSA achieves 1.77x and 4.83x higher attention FLOPs/s utilization compared to AWS Neuron-v2 and Google TPUv5e, respectively. We synthesize FSA in a 16 nm technology at 1.5 GHz, and results indicate only a 12% area overhead compared to a standard weight-stationary systolic array.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.11331v4",
    "published_date": "2025-07-15 14:04:17 UTC",
    "updated_date": "2025-12-08 14:57:05 UTC"
  },
  {
    "arxiv_id": "2507.11330v2",
    "title": "Automated Novelty Evaluation of Academic Paper: A Collaborative Approach Integrating Human and Large Language Model Knowledge",
    "authors": [
      "Wenqing Wu",
      "Chengzhi Zhang",
      "Yi Zhao"
    ],
    "abstract": "Novelty is a crucial criterion in the peer review process for evaluating academic papers. Traditionally, it's judged by experts or measure by unique reference combinations. Both methods have limitations: experts have limited knowledge, and the effectiveness of the combination method is uncertain. Moreover, it's unclear if unique citations truly measure novelty. The large language model (LLM) possesses a wealth of knowledge, while human experts possess judgment abilities that the LLM does not possess. Therefore, our research integrates the knowledge and abilities of LLM and human experts to address the limitations of novelty assessment. One of the most common types of novelty in academic papers is the introduction of new methods. In this paper, we propose leveraging human knowledge and LLM to assist pretrained language models (PLMs, e.g. BERT etc.) in predicting the method novelty of papers. Specifically, we extract sentences related to the novelty of the academic paper from peer review reports and use LLM to summarize the methodology section of the academic paper, which are then used to fine-tune PLMs. In addition, we have designed a text-guided fusion module with novel Sparse-Attention to better integrate human and LLM knowledge. We compared the method we proposed with a large number of baselines. Extensive experiments demonstrate that our method achieves superior performance.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DL",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "Journal of the Association for Information Science and Technology, 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.11330v2",
    "published_date": "2025-07-15 14:03:55 UTC",
    "updated_date": "2025-07-16 14:26:34 UTC"
  },
  {
    "arxiv_id": "2507.11329v1",
    "title": "Quantitative multi-metabolite imaging of Parkinson's disease using AI boosted molecular MRI",
    "authors": [
      "Hagar Shmuely",
      "Michal Rivlin",
      "Or Perlman"
    ],
    "abstract": "Traditional approaches for molecular imaging of Parkinson's disease (PD) in vivo require radioactive isotopes, lengthy scan times, or deliver only low spatial resolution. Recent advances in saturation transfer-based PD magnetic resonance imaging (MRI) have provided biochemical insights, although the image contrast is semi-quantitative and nonspecific. Here, we combined a rapid molecular MRI acquisition paradigm with deep learning based reconstruction for multi-metabolite quantification of glutamate, mobile proteins, semisolid, and mobile macromolecules in an acute MPTP (1-methyl-4-phenyl-1,2,3,6-tetrahydropyridine) mouse model. The quantitative parameter maps are in general agreement with the histology and MR spectroscopy, and demonstrate that semisolid magnetization transfer (MT), amide, and aliphatic relayed nuclear Overhauser effect (rNOE) proton volume fractions may serve as PD biomarkers.",
    "categories": [
      "physics.med-ph",
      "cs.AI"
    ],
    "primary_category": "physics.med-ph",
    "comment": "This project was funded by the European Union (ERC, BabyMagnet, project no. 101115639). Views and opinions expressed are, however, those of the authors only and do not necessarily reflect those of the European Union or the European Research Council. Neither the European Union nor the granting authority can be held responsible for them",
    "pdf_url": "https://arxiv.org/pdf/2507.11329v1",
    "published_date": "2025-07-15 14:01:54 UTC",
    "updated_date": "2025-07-15 14:01:54 UTC"
  },
  {
    "arxiv_id": "2507.11325v2",
    "title": "HANS-Net: Hyperbolic Convolution and Adaptive Temporal Attention for Accurate and Generalizable Liver and Tumor Segmentation in CT Imaging",
    "authors": [
      "Arefin Ittesafun Abian",
      "Ripon Kumar Debnath",
      "Md. Abdur Rahman",
      "Mohaimenul Azam Khan Raiaan",
      "Md Rafiqul Islam",
      "Asif Karim",
      "Reem E. Mohamed",
      "Sami Azam"
    ],
    "abstract": "Accurate liver and tumor segmentation on abdominal CT images is critical for reliable diagnosis and treatment planning, but remains challenging due to complex anatomical structures, variability in tumor appearance, and limited annotated data. To address these issues, we introduce Hyperbolic-convolutions Adaptive-temporal-attention with Neural-representation and Synaptic-plasticity Network (HANS-Net), a novel segmentation framework that synergistically combines hyperbolic convolutions for hierarchical geometric representation, a wavelet-inspired decomposition module for multi-scale texture learning, a biologically motivated synaptic plasticity mechanism for adaptive feature enhancement, and an implicit neural representation branch to model fine-grained and continuous anatomical boundaries. Additionally, we incorporate uncertainty-aware Monte Carlo dropout to quantify prediction confidence and lightweight temporal attention to improve inter-slice consistency without sacrificing efficiency. Extensive evaluations of the LiTS dataset demonstrate that HANS-Net achieves a mean Dice score of 93.26%, an IoU of 88.09%, an average symmetric surface distance (ASSD) of 0.72 mm, and a volume overlap error (VOE) of 11.91%. Furthermore, cross-dataset validation on the AMOS 2022 dataset obtains an average Dice of 85.09%, IoU of 76.66%, ASSD of 19.49 mm, and VOE of 23.34%, indicating strong generalization across different datasets. These results confirm the effectiveness and robustness of HANS-Net in providing anatomically consistent, accurate, and confident liver and tumor segmentation.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "Manuscript under review in IEEE Transactions on Radiation and Plasma Medical Sciences",
    "pdf_url": "https://arxiv.org/pdf/2507.11325v2",
    "published_date": "2025-07-15 13:56:37 UTC",
    "updated_date": "2025-10-16 11:11:15 UTC"
  },
  {
    "arxiv_id": "2507.11323v1",
    "title": "Contestability in Quantitative Argumentation",
    "authors": [
      "Xiang Yin",
      "Nico Potyka",
      "Antonio Rago",
      "Timotheus Kampik",
      "Francesca Toni"
    ],
    "abstract": "Contestable AI requires that AI-driven decisions align with human preferences. While various forms of argumentation have been shown to support contestability, Edge-Weighted Quantitative Bipolar Argumentation Frameworks (EW-QBAFs) have received little attention. In this work, we show how EW-QBAFs can be deployed for this purpose. Specifically, we introduce the contestability problem for EW-QBAFs, which asks how to modify edge weights (e.g., preferences) to achieve a desired strength for a specific argument of interest (i.e., a topic argument). To address this problem, we propose gradient-based relation attribution explanations (G-RAEs), which quantify the sensitivity of the topic argument's strength to changes in individual edge weights, thus providing interpretable guidance for weight adjustments towards contestability. Building on G-RAEs, we develop an iterative algorithm that progressively adjusts the edge weights to attain the desired strength. We evaluate our approach experimentally on synthetic EW-QBAFs that simulate the structural characteristics of personalised recommender systems and multi-layer perceptrons, and demonstrate that it can solve the problem effectively.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.11323v1",
    "published_date": "2025-07-15 13:54:26 UTC",
    "updated_date": "2025-07-15 13:54:26 UTC"
  },
  {
    "arxiv_id": "2507.11316v1",
    "title": "Internal Value Alignment in Large Language Models through Controlled Value Vector Activation",
    "authors": [
      "Haoran Jin",
      "Meng Li",
      "Xiting Wang",
      "Zhihao Xu",
      "Minlie Huang",
      "Yantao Jia",
      "Defu Lian"
    ],
    "abstract": "Aligning Large Language Models (LLMs) with human values has attracted increasing attention since it provides clarity, transparency, and the ability to adapt to evolving scenarios. In this paper, we introduce a Controlled Value Vector Activation (ConVA) method that directly aligns the internal values of LLMs by interpreting how a value is encoded in their latent representations and modifies relevant activations to ensure consistent values in LLMs. To ensure an accurate and unbiased interpretation, we propose a context-controlled value vector identification method. To consistently control values without sacrificing model performance, we introduce a gated value vector activation method for effective and minimum degree of value control. Experiments show that our method achieves the highest control success rate across 10 basic values without hurting LLM performance and fluency, and ensures target values even with opposite and potentially malicious input prompts. Source code and data are available at~ https://github.com/hr-jin/ConVA.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "25 pages, 14 figures. Accepted by ACL 2025 (main conference)",
    "pdf_url": "https://arxiv.org/pdf/2507.11316v1",
    "published_date": "2025-07-15 13:48:35 UTC",
    "updated_date": "2025-07-15 13:48:35 UTC"
  },
  {
    "arxiv_id": "2507.11288v3",
    "title": "Opus: A Prompt Intention Framework for Complex Workflow Generation",
    "authors": [
      "Théo Fagnoni",
      "Mahsun Altin",
      "Chia En Chung",
      "Phillip Kingston",
      "Alan Tuning",
      "Dana O. Mohamed",
      "Inès Adnani"
    ],
    "abstract": "This paper introduces the Opus Prompt Intention Framework, designed to improve complex Workflow Generation with instruction-tuned Large Language Models (LLMs). We propose an intermediate Intention Capture layer between user queries and Workflow Generation, implementing the Opus Workflow Intention Framework, which consists of extracting Workflow Signals from user queries, interpreting them into structured Workflow Intention objects, and generating Workflows based on these Intentions. Our results show that this layer enables LLMs to produce logical and meaningful outputs that scale reliably as query complexity increases. On a synthetic benchmark of 1,000 multi-intent query-Workflow(s) pairs, applying the Opus Prompt Intention Framework to Workflow Generation yields consistent improvements in semantic Workflow similarity metrics. In this paper, we introduce the Opus Prompt Intention Framework by applying the concepts of Workflow Signal and Workflow Intention to LLM-driven Workflow Generation. We present a reproducible, customizable LLM-based Intention Capture system to extract Workflow Signals and Workflow Intentions from user queries. Finally, we provide empirical evidence that the proposed system significantly improves Workflow Generation quality compared to direct generation from user queries, particularly in cases of Mixed Intention Elicitation.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "39 pages, 24 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.11288v3",
    "published_date": "2025-07-15 13:13:07 UTC",
    "updated_date": "2025-08-21 06:08:38 UTC"
  },
  {
    "arxiv_id": "2507.21115v2",
    "title": "FedFlex: Federated Learning for Diverse Netflix Recommendations",
    "authors": [
      "Sven Lankester",
      "Gustavo de Carvalho Bertoli",
      "Matias Vizcaino",
      "Emmanuelle Beauxis Aussalet",
      "Manel Slokom"
    ],
    "abstract": "The drive for personalization in recommender systems creates a tension between user privacy and the risk of \"filter bubbles\". Although federated learning offers a promising paradigm for privacy-preserving recommendations, its impact on diversity remains unclear. We introduce FedFlex, a two-stage framework that combines local, on-device fine-tuning of matrix factorization models (SVD and BPR) with a lightweight Maximal Marginal Relevance (MMR) re-ranking step to promote diversity. We conducted the first live user study of a federated recommender, collecting behavioral data and feedback during a two-week online deployment. Our results show that FedFlex successfully engages users, with BPR outperforming SVD in click-through rate. Re-ranking with MMR consistently improved ranking quality (nDCG) across both models, with statistically significant gains, particularly for BPR. Diversity effects varied: MMR increased coverage for both models and improved intra-list diversity for BPR, but slightly reduced it for SVD, suggesting different interactions between personalization and diversification across models. Our exit questionnaire responses indicated that most users expressed no clear preference between re-ranked and unprocessed lists, implying that increased diversity did not substantially reduce user satisfaction.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.DC",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.21115v2",
    "published_date": "2025-07-15 13:04:04 UTC",
    "updated_date": "2025-10-07 13:39:23 UTC"
  },
  {
    "arxiv_id": "2507.11277v2",
    "title": "Taming Uncertainty via Automation: Observing, Analyzing, and Optimizing Agentic AI Systems",
    "authors": [
      "Dany Moshkovich",
      "Sergey Zeltyn"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly deployed within agentic systems - collections of interacting, LLM-powered agents that execute complex, adaptive workflows using memory, tools, and dynamic planning. While enabling powerful new capabilities, these systems also introduce unique forms of uncertainty stemming from probabilistic reasoning, evolving memory states, and fluid execution paths. Traditional software observability and operations practices fall short in addressing these challenges.\n  This paper presents our vision of AgentOps: a comprehensive framework for observing, analyzing, optimizing, and automating operation of agentic AI systems. We identify distinct needs across four key roles - developers, testers, site reliability engineers (SREs), and business users - each of whom engages with the system at different points in its lifecycle. We present the AgentOps Automation Pipeline, a six-stage process encompassing behavior observation, metric collection, issue detection, root cause analysis, optimized recommendations, and runtime automation. Throughout, we emphasize the critical role of automation in managing uncertainty and enabling self-improving AI systems - not by eliminating uncertainty, but by taming it to ensure safe, adaptive, and effective operation.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.11277v2",
    "published_date": "2025-07-15 12:54:43 UTC",
    "updated_date": "2025-11-20 10:41:15 UTC"
  },
  {
    "arxiv_id": "2507.11269v2",
    "title": "Turning Sand to Gold: Recycling Data to Bridge On-Policy and Off-Policy Learning via Causal Bound",
    "authors": [
      "Tal Fiskus",
      "Uri Shaham"
    ],
    "abstract": "Deep reinforcement learning (DRL) agents excel in solving complex decision-making tasks across various domains. However, they often require a substantial number of training steps and a vast experience replay buffer, leading to significant computational and resource demands. To address these challenges, we introduce a novel theoretical result that leverages the Neyman-Rubin potential outcomes framework into DRL. Unlike most methods that focus on bounding the counterfactual loss, we establish a causal bound on the factual loss, which is analogous to the on-policy loss in DRL. This bound is computed by storing past value network outputs in the experience replay buffer, effectively utilizing data that is usually discarded. Extensive experiments across the Atari 2600 and MuJoCo domains on various agents, such as DQN and SAC, achieve up to 383% higher reward ratio, outperforming the same agents without our proposed term, and reducing the experience replay buffer size by up to 96%, significantly improving sample efficiency at a negligible cost.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "57 pages, 17 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.11269v2",
    "published_date": "2025-07-15 12:46:25 UTC",
    "updated_date": "2025-10-17 14:03:01 UTC"
  },
  {
    "arxiv_id": "2507.11267v1",
    "title": "YOLOatr : Deep Learning Based Automatic Target Detection and Localization in Thermal Infrared Imagery",
    "authors": [
      "Aon Safdar",
      "Usman Akram",
      "Waseem Anwar",
      "Basit Malik",
      "Mian Ibad Ali"
    ],
    "abstract": "Automatic Target Detection (ATD) and Recognition (ATR) from Thermal Infrared (TI) imagery in the defense and surveillance domain is a challenging computer vision (CV) task in comparison to the commercial autonomous vehicle perception domain. Limited datasets, peculiar domain-specific and TI modality-specific challenges, i.e., limited hardware, scale invariance issues due to greater distances, deliberate occlusion by tactical vehicles, lower sensor resolution and resultant lack of structural information in targets, effects of weather, temperature, and time of day variations, and varying target to clutter ratios all result in increased intra-class variability and higher inter-class similarity, making accurate real-time ATR a challenging CV task. Resultantly, contemporary state-of-the-art (SOTA) deep learning architectures underperform in the ATR domain. We propose a modified anchor-based single-stage detector, called YOLOatr, based on a modified YOLOv5s, with optimal modifications to the detection heads, feature fusion in the neck, and a custom augmentation profile. We evaluate the performance of our proposed model on a comprehensive DSIAC MWIR dataset for real-time ATR over both correlated and decorrelated testing protocols. The results demonstrate that our proposed model achieves state-of-the-art ATR performance of up to 99.6%.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Published in 25th Irish Machine Vision and Image Processing Conf., Galway, Ireland, Aug 30-Sep 1 2023 Also available at https://doi.org/10.5281/zenodo.8264062",
    "pdf_url": "https://arxiv.org/pdf/2507.11267v1",
    "published_date": "2025-07-15 12:41:01 UTC",
    "updated_date": "2025-07-15 12:41:01 UTC"
  },
  {
    "arxiv_id": "2507.11229v2",
    "title": "DuetGraph: Coarse-to-Fine Knowledge Graph Reasoning with Dual-Pathway Global-Local Fusion",
    "authors": [
      "Jin Li",
      "Zezhong Ding",
      "Xike Xie"
    ],
    "abstract": "Knowledge graphs (KGs) are vital for enabling knowledge reasoning across various domains. Recent KG reasoning methods that integrate both global and local information have achieved promising results. However, existing methods often suffer from score over-smoothing, which blurs the distinction between correct and incorrect answers and hinders reasoning effectiveness. To address this, we propose DuetGraph, a coarse-to-fine KG reasoning mechanism with dual-pathway global-local fusion. DuetGraph tackles over-smoothing by segregating -- rather than stacking -- the processing of local (via message passing) and global (via attention) information into two distinct pathways, preventing mutual interference and preserving representational discrimination. In addition, DuetGraph introduces a coarse-to-fine optimization, which partitions entities into high- and low-score subsets. This strategy narrows the candidate space and sharpens the score gap between the two subsets, which alleviates over-smoothing and enhances inference quality. Extensive experiments on various datasets demonstrate that DuetGraph achieves state-of-the-art (SOTA) performance, with up to an 8.7% improvement in reasoning quality and a 1.8$\\times$ acceleration in training efficiency. Our code is available at https://github.com/USTC-DataDarknessLab/DuetGraph.git.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.11229v2",
    "published_date": "2025-07-15 11:59:15 UTC",
    "updated_date": "2025-09-29 12:11:20 UTC"
  },
  {
    "arxiv_id": "2507.11222v1",
    "title": "An Agentic Flow for Finite State Machine Extraction using Prompt Chaining",
    "authors": [
      "Fares Wael",
      "Youssef Maklad",
      "Ali Hamdi",
      "Wael Elsersy"
    ],
    "abstract": "Finite-State Machines (FSMs) are critical for modeling the operational logic of network protocols, enabling verification, analysis, and vulnerability discovery. However, existing FSM extraction techniques face limitations such as scalability, incomplete coverage, and ambiguity in natural language specifications. In this paper, we propose FlowFSM, a novel agentic framework that leverages Large Language Models (LLMs) combined with prompt chaining and chain-of-thought reasoning to extract accurate FSMs from raw RFC documents. FlowFSM systematically processes protocol specifications, identifies state transitions, and constructs structured rule-books by chaining agent outputs. Experimental evaluation across FTP and RTSP protocols demonstrates that FlowFSM achieves high extraction precision while minimizing hallucinated transitions, showing promising results. Our findings highlight the potential of agent-based LLM systems in the advancement of protocol analysis and FSM inference for cybersecurity and reverse engineering applications.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.11222v1",
    "published_date": "2025-07-15 11:50:25 UTC",
    "updated_date": "2025-07-15 11:50:25 UTC"
  },
  {
    "arxiv_id": "2507.13380v2",
    "title": "Persona-Based Synthetic Data Generation Using Multi-Stage Conditioning with Large Language Models for Emotion Recognition",
    "authors": [
      "Keito Inoshita",
      "Rushia Harada"
    ],
    "abstract": "In the field of emotion recognition, the development of high-performance models remains a challenge due to the scarcity of high-quality, diverse emotional datasets. Emotional expressions are inherently subjective, shaped by individual personality traits, socio-cultural backgrounds, and contextual factors, making large-scale, generalizable data collection both ethically and practically difficult. To address this issue, we introduce PersonaGen, a novel framework for generating emotionally rich text using a Large Language Model (LLM) through multi-stage persona-based conditioning. PersonaGen constructs layered virtual personas by combining demographic attributes, socio-cultural backgrounds, and detailed situational contexts, which are then used to guide emotion expression generation. We conduct comprehensive evaluations of the generated synthetic data, assessing semantic diversity through clustering and distributional metrics, human-likeness via LLM-based quality scoring, realism through comparison with real-world emotion corpora, and practical utility in downstream emotion classification tasks. Experimental results show that PersonaGen significantly outperforms baseline methods in generating diverse, coherent, and discriminative emotion expressions, demonstrating its potential as a robust alternative for augmenting or replacing real-world emotional datasets.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.13380v2",
    "published_date": "2025-07-15 11:32:38 UTC",
    "updated_date": "2025-09-13 22:38:50 UTC"
  },
  {
    "arxiv_id": "2507.11210v3",
    "title": "Role-Playing LLM-Based Multi-Agent Support Framework for Detecting and Addressing Family Communication Bias",
    "authors": [
      "Rushia Harada",
      "Yuken Kimura",
      "Keito Inoshita"
    ],
    "abstract": "Well-being in family settings involves subtle psychological dynamics that conventional metrics often overlook. In particular, unconscious parental expectations, termed ideal parent bias, can suppress children's emotional expression and autonomy. This suppression, referred to as suppressed emotion, often stems from well-meaning but value-driven communication, which is difficult to detect or address from outside the family. Focusing on these latent dynamics, this study explores Large Language Model (LLM)-based support for psychologically safe family communication. We constructed a Japanese parent-child dialogue corpus of 30 scenarios, each annotated with metadata on ideal parent bias and suppressed emotion. Based on this corpus, we developed a Role-Playing LLM-based multi-agent dialogue support framework that analyzes dialogue and generates feedback. Specialized agents detect suppressed emotion, describe implicit ideal parent bias in parental speech, and infer contextual attributes such as the child's age and background. A meta-agent compiles these outputs into a structured report, which is then passed to five selected expert agents. These agents collaboratively generate empathetic and actionable feedback through a structured four-step discussion process. Experiments show that the system can detect categories of suppressed emotion with moderate accuracy and produce feedback rated highly in empathy and practicality. Moreover, simulated follow-up dialogues incorporating this feedback exhibited signs of improved emotional expression and mutual understanding, suggesting the framework's potential in supporting positive transformation in family interactions.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.11210v3",
    "published_date": "2025-07-15 11:27:32 UTC",
    "updated_date": "2026-01-11 05:16:30 UTC"
  },
  {
    "arxiv_id": "2507.11198v1",
    "title": "Temperature and Persona Shape LLM Agent Consensus With Minimal Accuracy Gains in Qualitative Coding",
    "authors": [
      "Conrad Borchers",
      "Bahar Shahrokhian",
      "Francesco Balzan",
      "Elham Tajik",
      "Sreecharan Sankaranarayanan",
      "Sebastian Simon"
    ],
    "abstract": "Large Language Models (LLMs) enable new possibilities for qualitative research at scale, including coding and data annotation. While multi-agent systems (MAS) can emulate human coding workflows, their benefits over single-agent coding remain poorly understood. We conducted an experimental study of how agent persona and temperature shape consensus-building and coding accuracy of dialog segments based on a codebook with 8 codes. Our open-source MAS mirrors deductive human coding through structured agent discussion and consensus arbitration. Using six open-source LLMs (with 3 to 32 billion parameters) and 18 experimental configurations, we analyze over 77,000 coding decisions against a gold-standard dataset of human-annotated transcripts from online math tutoring sessions. Temperature significantly impacted whether and when consensus was reached across all six LLMs. MAS with multiple personas (including neutral, assertive, or empathetic), significantly delayed consensus in four out of six LLMs compared to uniform personas. In three of those LLMs, higher temperatures significantly diminished the effects of multiple personas on consensus. However, neither temperature nor persona pairing lead to robust improvements in coding accuracy. Single agents matched or outperformed MAS consensus in most conditions. Only one model (OpenHermesV2:7B) and code category showed above-chance gains from MAS deliberation when temperature was 0.5 or lower and especially when the agents included at least one assertive persona. Qualitative analysis of MAS collaboration for these configurations suggests that MAS may nonetheless aid in narrowing ambiguous code applications that could improve codebooks and human-AI coding. We contribute new insight into the limits of LLM-based qualitative methods, challenging the notion that diverse MAS personas lead to better outcomes. We open-source our MAS and experimentation code.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Manuscript submitted for review",
    "pdf_url": "https://arxiv.org/pdf/2507.11198v1",
    "published_date": "2025-07-15 11:06:32 UTC",
    "updated_date": "2025-07-15 11:06:32 UTC"
  },
  {
    "arxiv_id": "2507.11185v1",
    "title": "An Explainable AI-Enhanced Machine Learning Approach for Cardiovascular Disease Detection and Risk Assessment",
    "authors": [
      "Md. Emon Akter Sourov",
      "Md. Sabbir Hossen",
      "Pabon Shaha",
      "Mohammad Minoar Hossain",
      "Md Sadiq Iqbal"
    ],
    "abstract": "Heart disease remains a major global health concern, particularly in regions with limited access to medical resources and diagnostic facilities. Traditional diagnostic methods often fail to accurately identify and manage heart disease risks, leading to adverse outcomes. Machine learning has the potential to significantly enhance the accuracy, efficiency, and speed of heart disease diagnosis. In this study, we proposed a comprehensive framework that combines classification models for heart disease detection and regression models for risk prediction. We employed the Heart Disease dataset, which comprises 1,035 cases. To address the issue of class imbalance, the Synthetic Minority Oversampling Technique (SMOTE) was applied, resulting in the generation of an additional 100,000 synthetic data points. Performance metrics, including accuracy, precision, recall, F1-score, R2, MSE, RMSE, and MAE, were used to evaluate the model's effectiveness. Among the classification models, Random Forest emerged as the standout performer, achieving an accuracy of 97.2% on real data and 97.6% on synthetic data. For regression tasks, Linear Regression demonstrated the highest R2 values of 0.992 and 0.984 on real and synthetic datasets, respectively, with the lowest error metrics. Additionally, Explainable AI techniques were employed to enhance the interpretability of the models. This study highlights the potential of machine learning to revolutionize heart disease diagnosis and risk prediction, thereby facilitating early intervention and enhancing clinical decision-making.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper has been accepted at the IEEE QPAIN 2025. The final version will be available in the IEEE Xplore Digital Library",
    "pdf_url": "https://arxiv.org/pdf/2507.11185v1",
    "published_date": "2025-07-15 10:38:38 UTC",
    "updated_date": "2025-07-15 10:38:38 UTC"
  },
  {
    "arxiv_id": "2507.11181v2",
    "title": "Mixture of Experts in Large Language Models",
    "authors": [
      "Danyang Zhang",
      "Junhao Song",
      "Ziqian Bi",
      "Xinyuan Song",
      "Yingfang Yuan",
      "Tianyang Wang",
      "Joe Yeong",
      "Junfeng Hao"
    ],
    "abstract": "This paper presents a comprehensive review of the Mixture-of-Experts (MoE) architecture in large language models, highlighting its ability to significantly enhance model performance while maintaining minimal computational overhead. Through a systematic analysis spanning theoretical foundations, core architectural designs, and large language model (LLM) applications, we examine expert gating and routing mechanisms, hierarchical and sparse MoE configurations, meta-learning approaches, multimodal and multitask learning scenarios, real-world deployment cases, and recent advances and challenges in deep learning. Our analysis identifies key advantages of MoE, including superior model capacity compared to equivalent Bayesian approaches, improved task-specific performance, and the ability to scale model capacity efficiently. We also underscore the importance of ensuring expert diversity, accurate calibration, and reliable inference aggregation, as these are essential for maximizing the effectiveness of MoE architectures. Finally, this review outlines current research limitations, open challenges, and promising future directions, providing a foundation for continued innovation in MoE architecture and its applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.11181v2",
    "published_date": "2025-07-15 10:36:43 UTC",
    "updated_date": "2025-12-22 19:02:17 UTC"
  },
  {
    "arxiv_id": "2507.14211v1",
    "title": "PRATA: A Framework to Enable Predictive QoS in Vehicular Networks via Artificial Intelligence",
    "authors": [
      "Federico Mason",
      "Tommaso Zugno",
      "Matteo Drago",
      "Marco Giordani",
      "Mate Boban",
      "Michele Zorzi"
    ],
    "abstract": "Predictive Quality of Service (PQoS) makes it possible to anticipate QoS changes, e.g., in wireless networks, and trigger appropriate countermeasures to avoid performance degradation. Hence, PQoS is extremely useful for automotive applications such as teleoperated driving, which poses strict constraints in terms of latency and reliability. A promising tool for PQoS is given by Reinforcement Learning (RL), a methodology that enables the design of decision-making strategies for stochastic optimization. In this manuscript, we present PRATA, a new simulation framework to enable PRedictive QoS based on AI for Teleoperated driving Applications. PRATA consists of a modular pipeline that includes (i) an end-to-end protocol stack to simulate the 5G Radio Access Network (RAN), (ii) a tool for generating automotive data, and (iii) an Artificial Intelligence (AI) unit to optimize PQoS decisions. To prove its utility, we use PRATA to design an RL unit, named RAN-AI, to optimize the segmentation level of teleoperated driving data in the event of resource saturation or channel degradation. Hence, we show that the RAN-AI entity efficiently balances the trade-off between QoS and Quality of Experience (QoE) that characterize teleoperated driving applications, almost doubling the system performance compared to baseline approaches. In addition, by varying the learning settings of the RAN-AI entity, we investigate the impact of the state space and the relative cost of acquiring network data that are necessary for the implementation of RL.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.14211v1",
    "published_date": "2025-07-15 10:36:25 UTC",
    "updated_date": "2025-07-15 10:36:25 UTC"
  },
  {
    "arxiv_id": "2507.11178v2",
    "title": "A Lightweight Gradient-based Causal Discovery Framework with Applications to Complex Industrial Processes",
    "authors": [
      "Meiliang Liu",
      "Huiwen Dong",
      "Xiaoxiao Yang",
      "Yunfang Xu",
      "Zijin Li",
      "Zhengye Si",
      "Xinyue Yang",
      "Zhiwen Zhao"
    ],
    "abstract": "With the advancement of deep learning technologies, various neural network-based Granger causality models have been proposed. Although these models have demonstrated notable improvements, several limitations remain. Most existing approaches adopt the component-wise architecture, necessitating the construction of a separate model for each time series, which results in substantial computational costs. In addition, imposing the sparsity-inducing penalty on the first-layer weights of the neural network to extract causal relationships weakens the model's ability to capture complex interactions. To address these limitations, we propose Gradient Regularization-based Neural Granger Causality (GRNGC), which requires only one time series prediction model and applies $L_{1}$ regularization to the gradient between model's input and output to infer Granger causality. Moreover, GRNGC is not tied to a specific time series forecasting model and can be implemented with diverse architectures such as KAN, MLP, and LSTM, offering enhanced flexibility. Numerical simulations on DREAM, Lorenz-96, fMRI BOLD, and CausalTime show that GRNGC outperforms existing baselines and significantly reduces computational overhead. Meanwhile, experiments on real-world DNA, Yeast, HeLa, and bladder urothelial carcinoma datasets further validate the model's effectiveness in reconstructing gene regulatory networks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages,3 figures, conference",
    "pdf_url": "https://arxiv.org/pdf/2507.11178v2",
    "published_date": "2025-07-15 10:35:29 UTC",
    "updated_date": "2025-10-25 12:46:37 UTC"
  },
  {
    "arxiv_id": "2508.00856v1",
    "title": "EthicAlly: a Prototype for AI-Powered Research Ethics Support for the Social Sciences and Humanities",
    "authors": [
      "Steph Grohmann"
    ],
    "abstract": "In biomedical science, review by a Research Ethics Committee (REC) is an indispensable way of protecting human subjects from harm. However, in social science and the humanities, mandatory ethics compliance has long been met with scepticism as biomedical models of ethics can map poorly onto methodologies involving complex socio-political and cultural considerations. As a result, tailored ethics training and support as well as access to RECs with the necessary expertise is lacking in some areas, including parts of Europe and low- and middle-income countries. This paper suggests that Generative AI can meaningfully contribute to closing these gaps, illustrating this claim by presenting EthicAlly, a proof-of-concept prototype for an AI-powered ethics support system for social science and humanities researchers. Drawing on constitutional AI technology and a collaborative prompt development methodology, EthicAlly provides structured ethics assessment that incorporates both universal ethics principles and contextual and interpretive considerations relevant to most social science research. In supporting researchers in ethical research design and preparation for REC submission, this kind of system can also contribute to easing the burden on institutional RECs, without attempting to automate or replace human ethical oversight.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.00856v1",
    "published_date": "2025-07-15 10:34:16 UTC",
    "updated_date": "2025-07-15 10:34:16 UTC"
  },
  {
    "arxiv_id": "2507.11176v1",
    "title": "An Interpretable AI framework Quantifying Traditional Chinese Medicine Principles Towards Enhancing and Integrating with Modern Biomedicine",
    "authors": [
      "Haoran Li",
      "Xingye Cheng",
      "Ziyang Huang",
      "Jingyuan Luo",
      "Qianqian Xu",
      "Qiguang Zhao",
      "Tianchen Guo",
      "Yumeng Zhang",
      "Linda Lidan Zhong",
      "Zhaoxiang Bian",
      "Leihan Tang",
      "Aiping Lyu",
      "Liang Tian"
    ],
    "abstract": "Traditional Chinese Medicine diagnosis and treatment principles, established through centuries of trial-and-error clinical practice, directly maps patient-specific symptom patterns to personalised herbal therapies. These empirical holistic mapping principles offer valuable strategies to address remaining challenges of reductionism methodologies in modern biomedicine. However, the lack of a quantitative framework and molecular-level evidence has limited their interpretability and reliability. Here, we present an AI framework trained on ancient and classical TCM formula records to quantify the symptom pattern-herbal therapy mappings. Interestingly, we find that empirical TCM diagnosis and treatment are consistent with the encoding-decoding processes in the AI model. This enables us to construct an interpretable TCM embedding space (TCM-ES) using the model's quantitative representation of TCM principles. Validated through broad and extensive TCM patient data, the TCM-ES offers universal quantification of the TCM practice and therapeutic efficacy. We further map biomedical entities into the TCM-ES through correspondence alignment. We find that the principal directions of the TCM-ES are significantly associated with key biological functions (such as metabolism, immune, and homeostasis), and that the disease and herb embedding proximity aligns with their genetic relationships in the human protein interactome, which demonstrate the biological significance of TCM principles. Moreover, the TCM-ES uncovers latent disease relationships, and provides alternative metric to assess clinical efficacy for modern disease-drug pairs. Finally, we construct a comprehensive and integrative TCM knowledge graph, which predicts potential associations between diseases and targets, drugs, herbal compounds, and herbal therapies, providing TCM-informed opportunities for disease analysis and drug development.",
    "categories": [
      "q-bio.OT",
      "cs.AI"
    ],
    "primary_category": "q-bio.OT",
    "comment": "31 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.11176v1",
    "published_date": "2025-07-15 10:30:45 UTC",
    "updated_date": "2025-07-15 10:30:45 UTC"
  },
  {
    "arxiv_id": "2507.11168v2",
    "title": "Improving Wi-Fi Network Performance Prediction with Deep Learning Models",
    "authors": [
      "Gabriele Formis",
      "Amanda Ericson",
      "Stefan Forsstrom",
      "Kyi Thar",
      "Gianluca Cena",
      "Stefano Scanzio"
    ],
    "abstract": "The increasing need for robustness, reliability, and determinism in wireless networks for industrial and mission-critical applications is the driver for the growth of new innovative methods. The study presented in this work makes use of machine learning techniques to predict channel quality in a Wi-Fi network in terms of the frame delivery ratio. Predictions can be used proactively to adjust communication parameters at runtime and optimize network operations for industrial applications. Methods including convolutional neural networks and long short-term memory were analyzed on datasets acquired from a real Wi-Fi setup across multiple channels. The models were compared in terms of prediction accuracy and computational complexity. Results show that the frame delivery ratio can be reliably predicted, and convolutional neural networks, although slightly less effective than other models, are more efficient in terms of CPU usage and memory consumption. This enhances the model's usability on embedded and industrial systems.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.LG",
      "eess.SP"
    ],
    "primary_category": "cs.NI",
    "comment": "preprint accepted, 8 pages, 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.11168v2",
    "published_date": "2025-07-15 10:18:32 UTC",
    "updated_date": "2025-12-03 08:49:42 UTC"
  },
  {
    "arxiv_id": "2507.11155v1",
    "title": "Bridging the Gap in Vision Language Models in Identifying Unsafe Concepts Across Modalities",
    "authors": [
      "Yiting Qu",
      "Michael Backes",
      "Yang Zhang"
    ],
    "abstract": "Vision-language models (VLMs) are increasingly applied to identify unsafe or inappropriate images due to their internal ethical standards and powerful reasoning abilities. However, it is still unclear whether they can recognize various unsafe concepts when presented in different modalities, such as text and images. To address this, we first compile the UnsafeConcepts dataset, featuring 75 unsafe concepts, i.e., ``Swastika,'' ``Sexual Harassment,'' and ``Assaults,'' along with associated 1.5K images. We then conduct a systematic evaluation of VLMs' perception (concept recognition) and alignment (ethical reasoning) capabilities. We assess eight popular VLMs and find that, although most VLMs accurately perceive unsafe concepts, they sometimes mistakenly classify these concepts as safe. We also identify a consistent modality gap among open-source VLMs in distinguishing between visual and textual unsafe concepts. To bridge this gap, we introduce a simplified reinforcement learning (RL)-based approach using proximal policy optimization (PPO) to strengthen the ability to identify unsafe concepts from images. Our approach uses reward scores based directly on VLM responses, bypassing the need for collecting human-annotated preference data to train a new reward model. Experimental results show that our approach effectively enhances VLM alignment on images while preserving general capabilities. It outperforms baselines such as supervised fine-tuning (SFT) and direct preference optimization (DPO). We hope our dataset, evaluation findings, and proposed alignment solution contribute to the community's efforts in advancing safe VLMs.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "To Appear in the 34th USENIX Security Symposium, August 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.11155v1",
    "published_date": "2025-07-15 10:04:27 UTC",
    "updated_date": "2025-07-15 10:04:27 UTC"
  },
  {
    "arxiv_id": "2507.11153v1",
    "title": "Assessing Color Vision Test in Large Vision-language Models",
    "authors": [
      "Hongfei Ye",
      "Bin Chen",
      "Wenxi Liu",
      "Yu Zhang",
      "Zhao Li",
      "Dandan Ni",
      "Hongyang Chen"
    ],
    "abstract": "With the widespread adoption of large vision-language models, the capacity for color vision in these models is crucial. However, the color vision abilities of large visual-language models have not yet been thoroughly explored. To address this gap, we define a color vision testing task for large vision-language models and construct a dataset \\footnote{Anonymous Github Showing some of the data https://anonymous.4open.science/r/color-vision-test-dataset-3BCD} that covers multiple categories of test questions and tasks of varying difficulty levels. Furthermore, we analyze the types of errors made by large vision-language models and propose fine-tuning strategies to enhance their performance in color vision tests.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.11153v1",
    "published_date": "2025-07-15 10:03:06 UTC",
    "updated_date": "2025-07-15 10:03:06 UTC"
  },
  {
    "arxiv_id": "2507.11152v1",
    "title": "Latent Space Consistency for Sparse-View CT Reconstruction",
    "authors": [
      "Duoyou Chen",
      "Yunqing Chen",
      "Can Zhang",
      "Zhou Wang",
      "Cheng Chen",
      "Ruoxiu Xiao"
    ],
    "abstract": "Computed Tomography (CT) is a widely utilized imaging modality in clinical settings. Using densely acquired rotational X-ray arrays, CT can capture 3D spatial features. However, it is confronted with challenged such as significant time consumption and high radiation exposure. CT reconstruction methods based on sparse-view X-ray images have garnered substantial attention from researchers as they present a means to mitigate costs and risks. In recent years, diffusion models, particularly the Latent Diffusion Model (LDM), have demonstrated promising potential in the domain of 3D CT reconstruction. Nonetheless, due to the substantial differences between the 2D latent representation of X-ray modalities and the 3D latent representation of CT modalities, the vanilla LDM is incapable of achieving effective alignment within the latent space. To address this issue, we propose the Consistent Latent Space Diffusion Model (CLS-DM), which incorporates cross-modal feature contrastive learning to efficiently extract latent 3D information from 2D X-ray images and achieve latent space alignment between modalities. Experimental results indicate that CLS-DM outperforms classical and state-of-the-art generative models in terms of standard voxel-level metrics (PSNR, SSIM) on the LIDC-IDRI and CTSpine1K datasets. This methodology not only aids in enhancing the effectiveness and economic viability of sparse X-ray reconstructed CT but can also be generalized to other cross-modal transformation tasks, such as text-to-image synthesis. We have made our code publicly available at https://anonymous.4open.science/r/CLS-DM-50D6/ to facilitate further research and applications in other domains.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "ACMMM2025 Accepted",
    "pdf_url": "https://arxiv.org/pdf/2507.11152v1",
    "published_date": "2025-07-15 10:02:19 UTC",
    "updated_date": "2025-07-15 10:02:19 UTC"
  },
  {
    "arxiv_id": "2507.11150v1",
    "title": "Fine-grained Timing Analysis of Digital Integrated Circuits in Answer Set Programming",
    "authors": [
      "Alessandro Bertagnon",
      "Marcello Dalpasso",
      "Michele Favalli",
      "Marco Gavanelli"
    ],
    "abstract": "In the design of integrated circuits, one critical metric is the maximum delay introduced by combinational modules within the circuit. This delay is crucial because it represents the time required to perform a computation: in an Arithmetic-Logic Unit it represents the maximum time taken by the circuit to perform an arithmetic operation. When such a circuit is part of a larger, synchronous system, like a CPU, the maximum delay directly impacts the maximum clock frequency of the entire system. Typically, hardware designers use Static Timing Analysis to compute an upper bound of the maximum delay because it can be determined in polynomial time. However, relying on this upper bound can lead to suboptimal processor speeds, thereby missing performance opportunities. In this work, we tackle the challenging task of computing the actual maximum delay, rather than an approximate value. Since the problem is computationally hard, we model it in Answer Set Programming (ASP), a logic language featuring extremely efficient solvers. We propose non-trivial encodings of the problem into ASP. Experimental results show that ASP is a viable solution to address complex problems in hardware design.",
    "categories": [
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted for publication in the issues of Theory and Practice of Logic Programming (TPLP) dedicated to ICLP 2025, 16 pages, 9 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.11150v1",
    "published_date": "2025-07-15 09:57:45 UTC",
    "updated_date": "2025-07-15 09:57:45 UTC"
  },
  {
    "arxiv_id": "2507.11135v1",
    "title": "Collaborative Trustworthiness for Good Decision Making in Autonomous Systems",
    "authors": [
      "Selma Saidi",
      "Omar Laimona",
      "Christoph Schmickler",
      "Dirk Ziegenbein"
    ],
    "abstract": "Autonomous systems are becoming an integral part of many application domains, like in the mobility sector. However, ensuring their safe and correct behaviour in dynamic and complex environments remains a significant challenge, where systems should autonomously make decisions e.g., about manoeuvring. We propose in this paper a general collaborative approach for increasing the level of trustworthiness in the environment of operation and improve reliability and good decision making in autonomous system. In the presence of conflicting information, aggregation becomes a major issue for trustworthy decision making based on collaborative data sharing. Unlike classical approaches in the literature that rely on consensus or majority as aggregation rule, we exploit the fact that autonomous systems have different quality attributes like perception quality. We use this criteria to determine which autonomous systems are trustworthy and borrow concepts from social epistemology to define aggregation and propagation rules, used for automated decision making. We use Binary Decision Diagrams (BDDs) as formal models for beliefs aggregation and propagation, and formulate reduction rules to reduce the size of the BDDs and allow efficient computation structures for collaborative automated reasoning.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.11135v1",
    "published_date": "2025-07-15 09:37:28 UTC",
    "updated_date": "2025-07-15 09:37:28 UTC"
  },
  {
    "arxiv_id": "2507.22906v1",
    "title": "DNN-based Methods of Jointly Sensing Number and Directions of Targets via a Green Massive H2AD MIMO Receiver",
    "authors": [
      "Bin Deng",
      "Jiatong Bai",
      "Feilong Zhao",
      "Zuming Xie",
      "Maolin Li",
      "Yan Wang",
      "Feng Shu"
    ],
    "abstract": "As a green MIMO structure, the heterogeneous hybrid analog-digital H2AD MIMO architecture has been shown to own a great potential to replace the massive or extremely large-scale fully-digital MIMO in the future wireless networks to address the three challenging problems faced by the latter: high energy consumption, high circuit cost, and high complexity. However, how to intelligently sense the number and direction of multi-emitters via such a structure is still an open hard problem. To address this, we propose a two-stage sensing framework that jointly estimates the number and direction values of multiple targets. Specifically, three target number sensing methods are designed: an improved eigen-domain clustering (EDC) framework, an enhanced deep neural network (DNN) based on five key statistical features, and an improved one-dimensional convolutional neural network (1D-CNN) utilizing full eigenvalues. Subsequently, a low-complexity and high-accuracy DOA estimation is achieved via the introduced online micro-clustering (OMC-DOA) method. Furthermore, we derive the Cramér-Rao lower bound (CRLB) for the H2AD under multiple-source conditions as a theoretical performance benchmark. Simulation results show that the developed three methods achieve 100\\% number of targets sensing at moderate-to-high SNRs, while the improved 1D-CNN exhibits superior under extremely-low SNR conditions. The introduced OMC-DOA outperforms existing clustering and fusion-based DOA methods in multi-source environments.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.IT",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.22906v1",
    "published_date": "2025-07-15 09:30:57 UTC",
    "updated_date": "2025-07-15 09:30:57 UTC"
  },
  {
    "arxiv_id": "2507.11129v2",
    "title": "MMOne: Representing Multiple Modalities in One Scene",
    "authors": [
      "Zhifeng Gu",
      "Bing Wang"
    ],
    "abstract": "Humans perceive the world through multimodal cues to understand and interact with the environment. Learning a scene representation for multiple modalities enhances comprehension of the physical world. However, modality conflicts, arising from inherent distinctions among different modalities, present two critical challenges: property disparity and granularity disparity. To address these challenges, we propose a general framework, MMOne, to represent multiple modalities in one scene, which can be readily extended to additional modalities. Specifically, a modality modeling module with a novel modality indicator is proposed to capture the unique properties of each modality. Additionally, we design a multimodal decomposition mechanism to separate multi-modal Gaussians into single-modal Gaussians based on modality differences. We address the essential distinctions among modalities by disentangling multimodal information into shared and modality-specific components, resulting in a more compact and efficient multimodal scene representation. Extensive experiments demonstrate that our method consistently enhances the representation capability for each modality and is scalable to additional modalities. The code is available at https://github.com/Neal2020GitHub/MMOne.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to ICCV 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.11129v2",
    "published_date": "2025-07-15 09:29:29 UTC",
    "updated_date": "2025-07-17 09:45:51 UTC"
  },
  {
    "arxiv_id": "2507.11127v1",
    "title": "Defining neurosymbolic AI",
    "authors": [
      "Lennert De Smet",
      "Luc De Raedt"
    ],
    "abstract": "Neurosymbolic AI focuses on integrating learning and reasoning, in particular, on unifying logical and neural representations. Despite the existence of an alphabet soup of neurosymbolic AI systems, the field is lacking a generally accepted formal definition of what neurosymbolic models and inference really are. We introduce a formal definition for neurosymbolic AI that makes abstraction of its key ingredients. More specifically, we define neurosymbolic inference as the computation of an integral over a product of a logical and a belief function. We show that our neurosymbolic AI definition makes abstraction of key representative neurosymbolic AI systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.11127v1",
    "published_date": "2025-07-15 09:23:22 UTC",
    "updated_date": "2025-07-15 09:23:22 UTC"
  },
  {
    "arxiv_id": "2507.11117v1",
    "title": "AI Agent Architecture for Decentralized Trading of Alternative Assets",
    "authors": [
      "Ailiya Borjigin",
      "Cong He",
      "Charles CC Lee",
      "Wei Zhou"
    ],
    "abstract": "Decentralized trading of real-world alternative assets (e.g., gold) requires bridging physical asset custody with blockchain systems while meeting strict requirements for compliance, liquidity, and risk management. We present GoldMine OS, a research oriented architecture that employs multiple specialized AI agents to automate and secure the tokenization and exchange of physical gold into a blockchain based stablecoin (\"OZ\"). Our approach combines on chain smart contracts for critical risk controls with off chain AI agents for decision making, blending the transparency and reliability of blockchains with the flexibility of AI driven automation. We describe four cooperative agents (Compliance, Token Issuance, Market Making, and Risk Control) and a coordinating core, and evaluate the system through simulation and a controlled pilot deployment. In experiments the prototype delivers on demand token issuance in under 1.2 s, more than 100 times faster than manual workflows. The Market Making agent maintains tight liquidity with spreads often below 0.5 percent even under volatile conditions. Fault injection tests show resilience: an oracle price spoofing attack is detected and mitigated within 10 s, and a simulated vault mis reporting halts issuance immediately with minimal user impact. The architecture scales to 5000 transactions per second with 10000 concurrent users in benchmarks. These results indicate that an AI agent based decentralized exchange for alternative assets can satisfy rigorous performance and safety requirements. We discuss broader implications for democratizing access to traditionally illiquid assets and explain how our governance model -- multi signature agent updates and on chain community voting on risk parameters -- provides ongoing transparency, adaptability, and formal assurance of system integrity.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "8 Pages, 1 figure",
    "pdf_url": "https://arxiv.org/pdf/2507.11117v1",
    "published_date": "2025-07-15 09:11:19 UTC",
    "updated_date": "2025-07-15 09:11:19 UTC"
  },
  {
    "arxiv_id": "2507.12486v1",
    "title": "On multiagent online problems with predictions",
    "authors": [
      "Gabriel Istrate",
      "Cosmin Bonchis",
      "Victor Bogdan"
    ],
    "abstract": "We study the power of (competitive) algorithms with predictions in a multiagent setting. We introduce a two predictor framework, that assumes that agents use one predictor for their future (self) behavior, and one for the behavior of the other players. The main problem we are concerned with is understanding what are the best competitive ratios that can be achieved by employing such predictors, under various assumptions on predictor quality.\n  As an illustration of our framework, we introduce and analyze a multiagent version of the ski-rental problem. In this problem agents can collaborate by pooling resources to get a group license for some asset. If the license price is not met then agents have to rent the asset individually for the day at a unit price. Otherwise the license becomes available forever to everyone at no extra cost.\n  In the particular case of perfect other predictions the algorithm that follows the self predictor is optimal but not robust to mispredictions of agent's future behavior; we give an algorithm with better robustness properties and benchmark it.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.GT"
    ],
    "primary_category": "cs.MA",
    "comment": "arXiv admin note: substantial text overlap with arXiv:2405.11873. arXiv admin note: substantial text overlap with arXiv:2405.11873",
    "pdf_url": "https://arxiv.org/pdf/2507.12486v1",
    "published_date": "2025-07-15 08:52:12 UTC",
    "updated_date": "2025-07-15 08:52:12 UTC"
  },
  {
    "arxiv_id": "2507.11096v1",
    "title": "EditGen: Harnessing Cross-Attention Control for Instruction-Based Auto-Regressive Audio Editing",
    "authors": [
      "Vassilis Sioros",
      "Alexandros Potamianos",
      "Giorgos Paraskevopoulos"
    ],
    "abstract": "In this study, we investigate leveraging cross-attention control for efficient audio editing within auto-regressive models. Inspired by image editing methodologies, we develop a Prompt-to-Prompt-like approach that guides edits through cross and self-attention mechanisms. Integrating a diffusion-based strategy, influenced by Auffusion, we extend the model's functionality to support refinement edits, establishing a baseline for prompt-guided audio editing. Additionally, we introduce an alternative approach by incorporating MUSICGEN, a pre-trained frozen auto-regressive model, and propose three editing mechanisms, based on Replacement, Reweighting, and Refinement of the attention scores. We employ commonly-used music-specific evaluation metrics and a human study, to gauge time-varying controllability, adherence to global text cues, and overall audio realism. The automatic and human evaluations indicate that the proposed combination of prompt-to-prompt guidance with autoregressive generation models significantly outperforms the diffusion-based baseline in terms of melody, dynamics, and tempo of the generated audio. Our code is available at https://github.com/billsioros/EditGen",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.11096v1",
    "published_date": "2025-07-15 08:44:11 UTC",
    "updated_date": "2025-07-15 08:44:11 UTC"
  },
  {
    "arxiv_id": "2507.11083v1",
    "title": "Function-to-Style Guidance of LLMs for Code Translation",
    "authors": [
      "Longhui Zhang",
      "Bin Wang",
      "Jiahao Wang",
      "Xiaofeng Zhao",
      "Min Zhang",
      "Hao Yang",
      "Meishan Zhang",
      "Yu Li",
      "Jing Li",
      "Jun Yu",
      "Min Zhang"
    ],
    "abstract": "Large language models (LLMs) have made significant strides in code translation tasks. However, ensuring both the correctness and readability of translated code remains a challenge, limiting their effective adoption in real-world software development. In this work, we propose F2STrans, a function-to-style guiding paradigm designed to progressively improve the performance of LLMs in code translation. Our approach comprises two key stages: (1) Functional learning, which optimizes translation correctness using high-quality source-target code pairs mined from online programming platforms, and (2) Style learning, which improves translation readability by incorporating both positive and negative style examples. Additionally, we introduce a novel code translation benchmark that includes up-to-date source code, extensive test cases, and manually annotated ground-truth translations, enabling comprehensive functional and stylistic evaluations. Experiments on both our new benchmark and existing datasets demonstrate that our approach significantly improves code translation performance. Notably, our approach enables Qwen-1.5B to outperform prompt-enhanced Qwen-32B and GPT-4 on average across 20 diverse code translation scenarios.",
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper has been accepted by ICML 2025. Models and benchmarks can be found at https://www.modelscope.cn/collections/F2STrans-42526ff95dd843",
    "pdf_url": "https://arxiv.org/pdf/2507.11083v1",
    "published_date": "2025-07-15 08:25:02 UTC",
    "updated_date": "2025-07-15 08:25:02 UTC"
  },
  {
    "arxiv_id": "2507.11081v2",
    "title": "Automatic Road Subsurface Distress Recognition from Ground Penetrating Radar Images using Deep Learning-based Cross-verification",
    "authors": [
      "Chang Peng",
      "Bao Yang",
      "Meiqi Li",
      "Ge Zhang",
      "Hui Sun",
      "Zhenyu Jiang"
    ],
    "abstract": "Ground penetrating radar (GPR) has become a rapid and non-destructive solution for road subsurface distress (RSD) detection. Deep learning-based automatic RSD recognition, though ameliorating the burden of data processing, suffers from data scarcity and insufficient capability to recognize defects. In this study, a rigorously validated 3D GPR dataset containing 2134 samples of diverse types was constructed through field scanning. A novel cross-verification strategy was proposed to fully exploit the complementary abilities of region proposal networks in object recognition from different views of GPR images. The method achieves outstanding accuracy with a recall over 98.6% in field tests. The approach, integrated into an online RSD detection system, can reduce the human labor of inspection by around 90%.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.11081v2",
    "published_date": "2025-07-15 08:23:21 UTC",
    "updated_date": "2025-11-05 09:21:58 UTC"
  },
  {
    "arxiv_id": "2507.11079v1",
    "title": "Tactical Decision for Multi-UGV Confrontation with a Vision-Language Model-Based Commander",
    "authors": [
      "Li Wang",
      "Qizhen Wu",
      "Lei Chen"
    ],
    "abstract": "In multiple unmanned ground vehicle confrontations, autonomously evolving multi-agent tactical decisions from situational awareness remain a significant challenge. Traditional handcraft rule-based methods become vulnerable in the complicated and transient battlefield environment, and current reinforcement learning methods mainly focus on action manipulation instead of strategic decisions due to lack of interpretability. Here, we propose a vision-language model-based commander to address the issue of intelligent perception-to-decision reasoning in autonomous confrontations. Our method integrates a vision language model for scene understanding and a lightweight large language model for strategic reasoning, achieving unified perception and decision within a shared semantic space, with strong adaptability and interpretability. Unlike rule-based search and reinforcement learning methods, the combination of the two modules establishes a full-chain process, reflecting the cognitive process of human commanders. Simulation and ablation experiments validate that the proposed approach achieves a win rate of over 80% compared with baseline models.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.11079v1",
    "published_date": "2025-07-15 08:22:37 UTC",
    "updated_date": "2025-07-15 08:22:37 UTC"
  },
  {
    "arxiv_id": "2507.11075v1",
    "title": "Joint angle model based learning to refine kinematic human pose estimation",
    "authors": [
      "Chang Peng",
      "Yifei Zhou",
      "Huifeng Xi",
      "Shiqing Huang",
      "Chuangye Chen",
      "Jianming Yang",
      "Bao Yang",
      "Zhenyu Jiang"
    ],
    "abstract": "Marker-free human pose estimation (HPE) has found increasing applications in various fields. Current HPE suffers from occasional errors in keypoint recognition and random fluctuation in keypoint trajectories when analyzing kinematic human poses. The performance of existing deep learning-based models for HPE refinement is considerably limited by inaccurate training datasets in which the keypoints are manually annotated. This paper proposed a novel method to overcome the difficulty through joint angle-based modeling. The key techniques include: (i) A joint angle-based model of human pose, which is robust to describe kinematic human poses; (ii) Approximating temporal variation of joint angles through high order Fourier series to get reliable \"ground truth\"; (iii) A bidirectional recurrent network is designed as a post-processing module to refine the estimation of well-established HRNet. Trained with the high-quality dataset constructed using our method, the network demonstrates outstanding performance to correct wrongly recognized joints and smooth their spatiotemporal trajectories. Tests show that joint angle-based refinement (JAR) outperforms the state-of-the-art HPE refinement network in challenging cases like figure skating and breaking.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.11075v1",
    "published_date": "2025-07-15 08:16:39 UTC",
    "updated_date": "2025-07-15 08:16:39 UTC"
  },
  {
    "arxiv_id": "2507.11071v1",
    "title": "LogTinyLLM: Tiny Large Language Models Based Contextual Log Anomaly Detection",
    "authors": [
      "Isaiah Thompson Ocansey",
      "Ritwik Bhattacharya",
      "Tanmay Sen"
    ],
    "abstract": "Log anomaly detection using traditional rule based or deep learning based methods is often challenging due to the large volume and highly complex nature of log sequence. So effective way of detection of anomalous sequence of logs is crucial for system maintenance and development. This paper proposes parameter efficient finetuning specifically low rank adaptation (LoRA) and adapter based approaches for finding contextual anomalies in sequence of logs in large log data set. It compares different tiny large language models (LLMs) on the Thunderbird dataset. The results show that LoRA based finetuning provides substantial performance improvements of 18 to 19 percentage over LogBert based full finetuning approach, achieving accuracy scores between 97.76% and 98.83% compared to 79.37%.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.11071v1",
    "published_date": "2025-07-15 08:04:31 UTC",
    "updated_date": "2025-07-15 08:04:31 UTC"
  },
  {
    "arxiv_id": "2507.11064v2",
    "title": "Standards-Compliant DM-RS Allocation via Temporal Channel Prediction for Massive MIMO Systems",
    "authors": [
      "Sehyun Ryu",
      "Hyun Jong Yang"
    ],
    "abstract": "Reducing feedback overhead in beyond 5G networks is a critical challenge, as the growing number of antennas in modern massive MIMO systems substantially increases the channel state information (CSI) feedback demand in frequency division duplex (FDD) systems. To address this, extensive research has focused on CSI compression and prediction, with neural network-based approaches gaining momentum and being considered for integration into the 3GPP 5G-Advanced standards. While deep learning has been effectively applied to CSI-limited beamforming and handover optimization, reference signal allocation under such constraints remains surprisingly underexplored. To fill this gap, we introduce the concept of channel prediction-based reference signal allocation (CPRS), which jointly optimizes channel prediction and DM-RS allocation to improve data throughput without requiring CSI feedback. We further propose a standards-compliant ViViT/CNN-based architecture that implements CPRS by treating evolving CSI matrices as sequential image-like data, enabling efficient and adaptive transmission in dynamic environments. Simulation results using ray-tracing channel data generated in NVIDIA Sionna validate the proposed method, showing up to 36.60% throughput improvement over benchmark strategies.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.11064v2",
    "published_date": "2025-07-15 07:56:37 UTC",
    "updated_date": "2025-12-16 02:04:13 UTC"
  },
  {
    "arxiv_id": "2507.11061v2",
    "title": "Robust 3D-Masked Part-level Editing in 3D Gaussian Splatting with Regularized Score Distillation Sampling",
    "authors": [
      "Hayeon Kim",
      "Ji Ha Jang",
      "Se Young Chun"
    ],
    "abstract": "Recent advances in 3D neural representations and instance-level editing models have enabled the efficient creation of high-quality 3D content. However, achieving precise local 3D edits remains challenging, especially for Gaussian Splatting, due to inconsistent multi-view 2D part segmentations and inherently ambiguous nature of Score Distillation Sampling (SDS) loss. To address these limitations, we propose RoMaP, a novel local 3D Gaussian editing framework that enables precise and drastic part-level modifications. First, we introduce a robust 3D mask generation module with our 3D-Geometry Aware Label Prediction (3D-GALP), which uses spherical harmonics (SH) coefficients to model view-dependent label variations and soft-label property, yielding accurate and consistent part segmentations across viewpoints. Second, we propose a regularized SDS loss that combines the standard SDS loss with additional regularizers. In particular, an L1 anchor loss is introduced via our Scheduled Latent Mixing and Part (SLaMP) editing method, which generates high-quality part-edited 2D images and confines modifications only to the target region while preserving contextual coherence. Additional regularizers, such as Gaussian prior removal, further improve flexibility by allowing changes beyond the existing context, and robust 3D masking prevents unintended edits. Experimental results demonstrate that our RoMaP achieves state-of-the-art local 3D editing on both reconstructed and generated Gaussian scenes and objects qualitatively and quantitatively, making it possible for more robust and flexible part-level 3D Gaussian editing. Code is available at https://janeyeon.github.io/romap.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.11061v2",
    "published_date": "2025-07-15 07:54:11 UTC",
    "updated_date": "2025-07-21 10:53:58 UTC"
  },
  {
    "arxiv_id": "2507.11060v1",
    "title": "Personalized Exercise Recommendation with Semantically-Grounded Knowledge Tracing",
    "authors": [
      "Yilmazcan Ozyurt",
      "Tunaberk Almaci",
      "Stefan Feuerriegel",
      "Mrinmaya Sachan"
    ],
    "abstract": "We introduce ExRec, a general framework for personalized exercise recommendation with semantically-grounded knowledge tracing. Our method builds on the observation that existing exercise recommendation approaches simulate student performance via knowledge tracing (KT) but they often overlook two key aspects: (a) the semantic content of questions and (b) the sequential, structured progression of student learning. To address this, our ExRec presents an end-to-end pipeline, from annotating the KCs of questions and learning their semantic representations to training KT models and optimizing several reinforcement learning (RL) methods. Moreover, we improve standard Q-learning-based continuous RL methods via a tailored model-based value estimation (MVE) approach that directly leverages the components of KT model in estimating cumulative knowledge improvement. We validate the effectiveness of our ExRec using various RL methods across four real-world tasks with different educational goals in online math learning. We further show that ExRec generalizes robustly to new, unseen questions and that it produces interpretable student learning trajectories. Together, our findings highlight the promise of KT-guided RL for effective personalization in education.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.11060v1",
    "published_date": "2025-07-15 07:54:04 UTC",
    "updated_date": "2025-07-15 07:54:04 UTC"
  },
  {
    "arxiv_id": "2507.11059v2",
    "title": "SWE-MERA: A Dynamic Benchmark for Agenticly Evaluating Large Language Models on Software Engineering Tasks",
    "authors": [
      "Pavel Adamenko",
      "Mikhail Ivanov",
      "Aidar Valeev",
      "Rodion Levichev",
      "Pavel Zadorozhny",
      "Ivan Lopatin",
      "Dmitry Babayev",
      "Alena Fenogenova",
      "Valentin Malykh"
    ],
    "abstract": "The rapid advancement of Large Language Models (LLMs) in software engineering has revealed critical limitations in existing benchmarks, particularly the widely used SWE-bench dataset. Recent studies have uncovered severe data contamination issues, e.g. SWE-bench reports 32.67% of successful patches involve direct solution leakage and 31.08% pass due to inadequate test cases. We introduce SWE-MERA, a dynamic, continuously updated benchmark designed to address these fundamental challenges through an automated collection of real-world GitHub issues and rigorous quality validation. Our approach implements a reliable pipeline that ensures quality while minimizing contamination risks, resulting in approximately 10,000 potential tasks with 300 samples currently available. Evaluation using the Aider coding agent demonstrates strong discriminative power in state-of-the-art models. We report performance across a dozen recent LLMs evaluated on tasks collected between September 2024 and June 2025.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.11059v2",
    "published_date": "2025-07-15 07:52:33 UTC",
    "updated_date": "2025-07-17 14:04:07 UTC"
  },
  {
    "arxiv_id": "2507.11053v1",
    "title": "GATE: Graph Attention Neural Networks with Real-Time Edge Construction for Robust Indoor Localization using Mobile Embedded Devices",
    "authors": [
      "Danish Gufran",
      "Sudeep Pasricha"
    ],
    "abstract": "Accurate indoor localization is crucial for enabling spatial context in smart environments and navigation systems. Wi-Fi Received Signal Strength (RSS) fingerprinting is a widely used indoor localization approach due to its compatibility with mobile embedded devices. Deep Learning (DL) models improve accuracy in localization tasks by learning RSS variations across locations, but they assume fingerprint vectors exist in a Euclidean space, failing to incorporate spatial relationships and the non-uniform distribution of real-world RSS noise. This results in poor generalization across heterogeneous mobile devices, where variations in hardware and signal processing distort RSS readings. Graph Neural Networks (GNNs) can improve upon conventional DL models by encoding indoor locations as nodes and modeling their spatial and signal relationships as edges. However, GNNs struggle with non-Euclidean noise distributions and suffer from the GNN blind spot problem, leading to degraded accuracy in environments with dense access points (APs). To address these challenges, we propose GATE, a novel framework that constructs an adaptive graph representation of fingerprint vectors while preserving an indoor state-space topology, modeling the non-Euclidean structure of RSS noise to mitigate environmental noise and address device heterogeneity. GATE introduces 1) a novel Attention Hyperspace Vector (AHV) for enhanced message passing, 2) a novel Multi-Dimensional Hyperspace Vector (MDHV) to mitigate the GNN blind spot, and 3) an new Real-Time Edge Construction (RTEC) approach for dynamic graph adaptation. Extensive real-world evaluations across multiple indoor spaces with varying path lengths, AP densities, and heterogeneous devices demonstrate that GATE achieves 1.6x to 4.72x lower mean localization errors and 1.85x to 4.57x lower worst-case errors compared to state-of-the-art indoor localization frameworks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.11053v1",
    "published_date": "2025-07-15 07:37:33 UTC",
    "updated_date": "2025-07-15 07:37:33 UTC"
  },
  {
    "arxiv_id": "2507.11052v1",
    "title": "LLM-Augmented Symptom Analysis for Cardiovascular Disease Risk Prediction: A Clinical NLP",
    "authors": [
      "Haowei Yang",
      "Ziyu Shen",
      "Junli Shao",
      "Luyao Men",
      "Xinyue Han",
      "Jing Dong"
    ],
    "abstract": "Timely identification and accurate risk stratification of cardiovascular disease (CVD) remain essential for reducing global mortality. While existing prediction models primarily leverage structured data, unstructured clinical notes contain valuable early indicators. This study introduces a novel LLM-augmented clinical NLP pipeline that employs domain-adapted large language models for symptom extraction, contextual reasoning, and correlation from free-text reports. Our approach integrates cardiovascular-specific fine-tuning, prompt-based inference, and entity-aware reasoning. Evaluations on MIMIC-III and CARDIO-NLP datasets demonstrate improved performance in precision, recall, F1-score, and AUROC, with high clinical relevance (kappa = 0.82) assessed by cardiologists. Challenges such as contextual hallucination, which occurs when plausible information contracts with provided source, and temporal ambiguity, which is related with models struggling with chronological ordering of events are addressed using prompt engineering and hybrid rule-based verification. This work underscores the potential of LLMs in clinical decision support systems (CDSS), advancing early warning systems and enhancing the translation of patient narratives into actionable risk assessments.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.11052v1",
    "published_date": "2025-07-15 07:32:16 UTC",
    "updated_date": "2025-07-15 07:32:16 UTC"
  },
  {
    "arxiv_id": "2507.14207v1",
    "title": "Mitigating Trojanized Prompt Chains in Educational LLM Use Cases: Experimental Findings and Detection Tool Design",
    "authors": [
      "Richard M. Charles",
      "James H. Curry",
      "Richard B. Charles"
    ],
    "abstract": "The integration of Large Language Models (LLMs) in K--12 education offers both transformative opportunities and emerging risks. This study explores how students may Trojanize prompts to elicit unsafe or unintended outputs from LLMs, bypassing established content moderation systems with safety guardrils. Through a systematic experiment involving simulated K--12 queries and multi-turn dialogues, we expose key vulnerabilities in GPT-3.5 and GPT-4. This paper presents our experimental design, detailed findings, and a prototype tool, TrojanPromptGuard (TPG), to automatically detect and mitigate Trojanized educational prompts. These insights aim to inform both AI safety researchers and educational technologists on the safe deployment of LLMs for educators.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "12 pages, 1 figure",
    "pdf_url": "https://arxiv.org/pdf/2507.14207v1",
    "published_date": "2025-07-15 07:23:19 UTC",
    "updated_date": "2025-07-15 07:23:19 UTC"
  },
  {
    "arxiv_id": "2507.11017v2",
    "title": "First-Order Error Matters: Accurate Compensation for Quantized Large Language Models",
    "authors": [
      "Xingyu Zheng",
      "Haotong Qin",
      "Yuye Li",
      "Haoran Chu",
      "Jiakai Wang",
      "Jinyang Guo",
      "Michele Magno",
      "Xianglong Liu"
    ],
    "abstract": "Post-training quantization (PTQ) offers an efficient approach to compressing large language models (LLMs), significantly reducing memory access and computational costs. Existing compensation-based weight calibration methods often rely on a second-order Taylor expansion to model quantization error, under the assumption that the first-order term is negligible in well-trained full-precision models. However, we reveal that the progressive compensation process introduces accumulated first-order deviations between latent weights and their full-precision counterparts, making this assumption fundamentally flawed. To address this, we propose FOEM, a novel PTQ method that explicitly incorporates first-order gradient terms to improve quantization error compensation. FOEM approximates gradients by performing a first-order Taylor expansion around the pre-quantization weights. This yields an approximation based on the difference between latent and full-precision weights as well as the Hessian matrix. When substituted into the theoretical solution, the formulation eliminates the need to explicitly compute the Hessian, thereby avoiding the high computational cost and limited generalization of backpropagation-based gradient methods. This design introduces only minimal additional computational overhead. Extensive experiments across a wide range of models and benchmarks demonstrate that FOEM consistently outperforms the classical GPTQ method. In 3-bit weight-only quantization, FOEM reduces the perplexity of Llama3-8B by 17.3% and increases the 5-shot MMLU accuracy from 53.8% achieved by GPTAQ to 56.1%. Moreover, FOEM can be seamlessly combined with advanced techniques such as SpinQuant, delivering additional gains under the challenging W4A4KV4 setting and further narrowing the performance gap with full-precision baselines, surpassing existing state-of-the-art methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by AAAI 2026. The code is available at https://github.com/Xingyu-Zheng/FOEM",
    "pdf_url": "https://arxiv.org/pdf/2507.11017v2",
    "published_date": "2025-07-15 06:18:46 UTC",
    "updated_date": "2025-11-14 13:44:04 UTC"
  },
  {
    "arxiv_id": "2507.11015v1",
    "title": "Semantically Informed Salient Regions Guided Radiology Report Generation",
    "authors": [
      "Zeyi Hou",
      "Zeqiang Wei",
      "Ruixin Yan",
      "Ning Lang",
      "Xiuzhuang Zhou"
    ],
    "abstract": "Recent advances in automated radiology report generation from chest X-rays using deep learning algorithms have the potential to significantly reduce the arduous workload of radiologists. However, due to the inherent massive data bias in radiology images, where abnormalities are typically subtle and sparsely distributed, existing methods often produce fluent yet medically inaccurate reports, limiting their applicability in clinical practice. To address this issue effectively, we propose a Semantically Informed Salient Regions-guided (SISRNet) report generation method. Specifically, our approach explicitly identifies salient regions with medically critical characteristics using fine-grained cross-modal semantics. Then, SISRNet systematically focuses on these high-information regions during both image modeling and report generation, effectively capturing subtle abnormal findings, mitigating the negative impact of data bias, and ultimately generating clinically accurate reports. Compared to its peers, SISRNet demonstrates superior performance on widely used IU-Xray and MIMIC-CXR datasets.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.11015v1",
    "published_date": "2025-07-15 06:15:07 UTC",
    "updated_date": "2025-07-15 06:15:07 UTC"
  },
  {
    "arxiv_id": "2507.10999v1",
    "title": "SpaRTAN: Spatial Reinforcement Token-based Aggregation Network for Visual Recognition",
    "authors": [
      "Quan Bi Pay",
      "Vishnu Monn Baskaran",
      "Junn Yong Loo",
      "KokSheik Wong",
      "Simon See"
    ],
    "abstract": "The resurgence of convolutional neural networks (CNNs) in visual recognition tasks, exemplified by ConvNeXt, has demonstrated their capability to rival transformer-based architectures through advanced training methodologies and ViT-inspired design principles. However, both CNNs and transformers exhibit a simplicity bias, favoring straightforward features over complex structural representations. Furthermore, modern CNNs often integrate MLP-like blocks akin to those in transformers, but these blocks suffer from significant information redundancies, necessitating high expansion ratios to sustain competitive performance. To address these limitations, we propose SpaRTAN, a lightweight architectural design that enhances spatial and channel-wise information processing. SpaRTAN employs kernels with varying receptive fields, controlled by kernel size and dilation factor, to capture discriminative multi-order spatial features effectively. A wave-based channel aggregation module further modulates and reinforces pixel interactions, mitigating channel-wise redundancies. Combining the two modules, the proposed network can efficiently gather and dynamically contextualize discriminative features. Experimental results in ImageNet and COCO demonstrate that SpaRTAN achieves remarkable parameter efficiency while maintaining competitive performance. In particular, on the ImageNet-1k benchmark, SpaRTAN achieves 77. 7% accuracy with only 3.8M parameters and approximately 1.0 GFLOPs, demonstrating its ability to deliver strong performance through an efficient design. On the COCO benchmark, it achieves 50.0% AP, surpassing the previous benchmark by 1.2% with only 21.5M parameters. The code is publicly available at [https://github.com/henry-pay/SpaRTAN].",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at International Joint Conference on Neural Networks (IJCNN 2025)",
    "pdf_url": "https://arxiv.org/pdf/2507.10999v1",
    "published_date": "2025-07-15 05:34:56 UTC",
    "updated_date": "2025-07-15 05:34:56 UTC"
  },
  {
    "arxiv_id": "2507.10998v3",
    "title": "Crafting Imperceptible On-Manifold Adversarial Attacks for Tabular Data",
    "authors": [
      "Zhipeng He",
      "Alexander Stevens",
      "Chun Ouyang",
      "Johannes De Smedt",
      "Alistair Barros",
      "Catarina Moreira"
    ],
    "abstract": "Adversarial attacks on tabular data present unique challenges due to the heterogeneous nature of mixed categorical and numerical features. Unlike images where pixel perturbations maintain visual similarity, tabular data lacks intuitive similarity metrics, making it difficult to define imperceptible modifications. Additionally, traditional gradient-based methods prioritise $\\ell_p$-norm constraints, often producing adversarial examples that deviate from the original data distributions. To address this, we propose a latent-space perturbation framework using a mixed-input Variational Autoencoder (VAE) to generate statistically consistent adversarial examples. The proposed VAE integrates categorical embeddings and numerical features into a unified latent manifold, enabling perturbations that preserve statistical consistency. We introduce In-Distribution Success Rate (IDSR) to jointly evaluate attack effectiveness and distributional alignment. Evaluation across six publicly available datasets and three model architectures demonstrates that our method achieves substantially lower outlier rates and more consistent performance compared to traditional input-space attacks and other VAE-based methods adapted from image domain approaches, achieving substantially lower outlier rates and higher IDSR across six datasets and three model architectures. Our comprehensive analyses of hyperparameter sensitivity, sparsity control, and generative architecture demonstrate that the effectiveness of VAE-based attacks depends strongly on reconstruction quality and the availability of sufficient training data. When these conditions are met, the proposed framework achieves superior practical utility and stability compared with input-space methods. This work underscores the importance of maintaining on-manifold perturbations for generating realistic and robust adversarial examples in tabular domains.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Final Version",
    "pdf_url": "https://arxiv.org/pdf/2507.10998v3",
    "published_date": "2025-07-15 05:34:44 UTC",
    "updated_date": "2025-11-21 16:31:02 UTC"
  },
  {
    "arxiv_id": "2507.10995v1",
    "title": "Misalignment from Treating Means as Ends",
    "authors": [
      "Henrik Marklund",
      "Alex Infanger",
      "Benjamin Van Roy"
    ],
    "abstract": "Reward functions, learned or manually specified, are rarely perfect. Instead of accurately expressing human goals, these reward functions are often distorted by human beliefs about how best to achieve those goals. Specifically, these reward functions often express a combination of the human's terminal goals -- those which are ends in themselves -- and the human's instrumental goals -- those which are means to an end. We formulate a simple example in which even slight conflation of instrumental and terminal goals results in severe misalignment: optimizing the misspecified reward function results in poor performance when measured by the true reward function. This example distills the essential properties of environments that make reinforcement learning highly sensitive to conflation of instrumental and terminal goals. We discuss how this issue can arise with a common approach to reward learning and how it can manifest in real environments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.10995v1",
    "published_date": "2025-07-15 05:27:51 UTC",
    "updated_date": "2025-07-15 05:27:51 UTC"
  },
  {
    "arxiv_id": "2507.11575v1",
    "title": "What cat is that? A re-id model for feral cats",
    "authors": [
      "Victor Caquilpan"
    ],
    "abstract": "Feral cats exert a substantial and detrimental impact on Australian wildlife, placing them among the most dangerous invasive species worldwide. Therefore, closely monitoring these cats is essential labour in minimising their effects. In this context, the potential application of Re-Identification (re-ID) emerges to enhance monitoring activities for these animals, utilising images captured by camera traps. This project explores different CV approaches to create a re-ID model able to identify individual feral cats in the wild. The main approach consists of modifying a part-pose guided network (PPGNet) model, initially used in the re-ID of Amur tigers, to be applicable for feral cats. This adaptation, resulting in PPGNet-Cat, which incorporates specific modifications to suit the characteristics of feral cats images. Additionally, various experiments were conducted, particularly exploring contrastive learning approaches such as ArcFace loss. The main results indicate that PPGNet-Cat excels in identifying feral cats, achieving high performance with a mean Average Precision (mAP) of 0.86 and a rank-1 accuracy of 0.95. These outcomes establish PPGNet-Cat as a competitive model within the realm of re-ID.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Master's project",
    "pdf_url": "https://arxiv.org/pdf/2507.11575v1",
    "published_date": "2025-07-15 05:24:38 UTC",
    "updated_date": "2025-07-15 05:24:38 UTC"
  },
  {
    "arxiv_id": "2507.10993v1",
    "title": "Modeling Habitat Shifts: Integrating Convolutional Neural Networks and Tabular Data for Species Migration Prediction",
    "authors": [
      "Emir Durakovic",
      "Min-Hong Shih"
    ],
    "abstract": "Due to climate-induced changes, many habitats are experiencing range shifts away from their traditional geographic locations (Piguet, 2011). We propose a solution to accurately model whether bird species are present in a specific habitat through the combination of Convolutional Neural Networks (CNNs) (O'Shea, 2015) and tabular data. Our approach makes use of satellite imagery and environmental features (e.g., temperature, precipitation, elevation) to predict bird presence across various climates. The CNN model captures spatial characteristics of landscapes such as forestation, water bodies, and urbanization, whereas the tabular method uses ecological and geographic data. Both systems predict the distribution of birds with an average accuracy of 85%, offering a scalable but reliable method to understand bird migration.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper uses a lightly modified version of the AAAI 2025 LaTeX style for formatting consistency. It is not a submission to AAAI and does not include any AAAI-specific headers, footers, or metadata",
    "pdf_url": "https://arxiv.org/pdf/2507.10993v1",
    "published_date": "2025-07-15 05:17:58 UTC",
    "updated_date": "2025-07-15 05:17:58 UTC"
  },
  {
    "arxiv_id": "2507.10990v2",
    "title": "Adaptive Policy Synchronization for Scalable Reinforcement Learning",
    "authors": [
      "Rodney Lafuente-Mercado"
    ],
    "abstract": "Scaling reinforcement learning (RL) often requires running environments across many machines, but most frameworks tie simulation, training, and infrastructure into rigid systems. We introduce ClusterEnv, a lightweight interface for distributed environment execution that preserves the familiar Gymnasium API. ClusterEnv uses the DETACH pattern, which moves environment reset() and step() operations to remote workers while keeping learning centralized. To reduce policy staleness without heavy communication, we propose Adaptive Policy Synchronization (APS), where workers request updates only when divergence from the central learner grows too large. ClusterEnv supports both on- and off-policy methods, integrates into existing training code with minimal changes, and runs efficiently on clusters. Experiments on discrete control tasks show that APS maintains performance while cutting synchronization overhead. Source code is available at https://github.com/rodlaf/ClusterEnv.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.10990v2",
    "published_date": "2025-07-15 05:07:12 UTC",
    "updated_date": "2025-10-17 21:19:22 UTC"
  },
  {
    "arxiv_id": "2507.10985v1",
    "title": "Pronunciation Deviation Analysis Through Voice Cloning and Acoustic Comparison",
    "authors": [
      "Andrew Valdivia",
      "Yueming Zhang",
      "Hailu Xu",
      "Amir Ghasemkhani",
      "Xin Qin"
    ],
    "abstract": "This paper presents a novel approach for detecting mispronunciations by analyzing deviations between a user's original speech and their voice-cloned counterpart with corrected pronunciation. We hypothesize that regions with maximal acoustic deviation between the original and cloned utterances indicate potential mispronunciations. Our method leverages recent advances in voice cloning to generate a synthetic version of the user's voice with proper pronunciation, then performs frame-by-frame comparisons to identify problematic segments. Experimental results demonstrate the effectiveness of this approach in pinpointing specific pronunciation errors without requiring predefined phonetic rules or extensive training data for each target language.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.10985v1",
    "published_date": "2025-07-15 04:58:19 UTC",
    "updated_date": "2025-07-15 04:58:19 UTC"
  },
  {
    "arxiv_id": "2507.10977v1",
    "title": "Conceptualizing Multi-scale Wavelet Attention and Ray-based Encoding for Human-Object Interaction Detection",
    "authors": [
      "Quan Bi Pay",
      "Vishnu Monn Baskaran",
      "Junn Yong Loo",
      "KokSheik Wong",
      "Simon See"
    ],
    "abstract": "Human-object interaction (HOI) detection is essential for accurately localizing and characterizing interactions between humans and objects, providing a comprehensive understanding of complex visual scenes across various domains. However, existing HOI detectors often struggle to deliver reliable predictions efficiently, relying on resource-intensive training methods and inefficient architectures. To address these challenges, we conceptualize a wavelet attention-like backbone and a novel ray-based encoder architecture tailored for HOI detection. Our wavelet backbone addresses the limitations of expressing middle-order interactions by aggregating discriminative features from the low- and high-order interactions extracted from diverse convolutional filters. Concurrently, the ray-based encoder facilitates multi-scale attention by optimizing the focus of the decoder on relevant regions of interest and mitigating computational overhead. As a result of harnessing the attenuated intensity of learnable ray origins, our decoder aligns query embeddings with emphasized regions of interest for accurate predictions. Experimental results on benchmark datasets, including ImageNet and HICO-DET, showcase the potential of our proposed architecture. The code is publicly available at [https://github.com/henry-pay/RayEncoder].",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at International Joint Conference on Neural Networks (IJCNN 2025)",
    "pdf_url": "https://arxiv.org/pdf/2507.10977v1",
    "published_date": "2025-07-15 04:44:54 UTC",
    "updated_date": "2025-07-15 04:44:54 UTC"
  },
  {
    "arxiv_id": "2507.11574v1",
    "title": "Distribution-Free Uncertainty-Aware Virtual Sensing via Conformalized Neural Operators",
    "authors": [
      "Kazuma Kobayashi",
      "Shailesh Garg",
      "Farid Ahmed",
      "Souvik Chakraborty",
      "Syed Bahauddin Alam"
    ],
    "abstract": "Robust uncertainty quantification (UQ) remains a critical barrier to the safe deployment of deep learning in real-time virtual sensing, particularly in high-stakes domains where sparse, noisy, or non-collocated sensor data are the norm. We introduce the Conformalized Monte Carlo Operator (CMCO), a framework that transforms neural operator-based virtual sensing with calibrated, distribution-free prediction intervals. By unifying Monte Carlo dropout with split conformal prediction in a single DeepONet architecture, CMCO achieves spatially resolved uncertainty estimates without retraining, ensembling, or custom loss design. Our method addresses a longstanding challenge: how to endow operator learning with efficient and reliable UQ across heterogeneous domains. Through rigorous evaluation on three distinct applications: turbulent flow, elastoplastic deformation, and global cosmic radiation dose estimation-CMCO consistently attains near-nominal empirical coverage, even in settings with strong spatial gradients and proxy-based sensing. This breakthrough offers a general-purpose, plug-and-play UQ solution for neural operators, unlocking real-time, trustworthy inference in digital twins, sensor fusion, and safety-critical monitoring. By bridging theory and deployment with minimal computational overhead, CMCO establishes a new foundation for scalable, generalizable, and uncertainty-aware scientific machine learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.11574v1",
    "published_date": "2025-07-15 04:26:40 UTC",
    "updated_date": "2025-07-15 04:26:40 UTC"
  },
  {
    "arxiv_id": "2507.16834v1",
    "title": "Towards Robust Speech Recognition for Jamaican Patois Music Transcription",
    "authors": [
      "Jordan Madden",
      "Matthew Stone",
      "Dimitri Johnson",
      "Daniel Geddez"
    ],
    "abstract": "Although Jamaican Patois is a widely spoken language, current speech recognition systems perform poorly on Patois music, producing inaccurate captions that limit accessibility and hinder downstream applications. In this work, we take a data-centric approach to this problem by curating more than 40 hours of manually transcribed Patois music. We use this dataset to fine-tune state-of-the-art automatic speech recognition (ASR) models, and use the results to develop scaling laws for the performance of Whisper models on Jamaican Patois audio. We hope that this work will have a positive impact on the accessibility of Jamaican Patois music and the future of Jamaican Patois language modeling.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.16834v1",
    "published_date": "2025-07-15 03:42:05 UTC",
    "updated_date": "2025-07-15 03:42:05 UTC"
  },
  {
    "arxiv_id": "2507.10957v1",
    "title": "Modeling Understanding of Story-Based Analogies Using Large Language Models",
    "authors": [
      "Kalit Inani",
      "Keshav Kabra",
      "Vijay Marupudi",
      "Sashank Varma"
    ],
    "abstract": "Recent advancements in Large Language Models (LLMs) have brought them closer to matching human cognition across a variety of tasks. How well do these models align with human performance in detecting and mapping analogies? Prior research has shown that LLMs can extract similarities from analogy problems but lack robust human-like reasoning. Building on Webb, Holyoak, and Lu (2023), the current study focused on a story-based analogical mapping task and conducted a fine-grained evaluation of LLM reasoning abilities compared to human performance. First, it explored the semantic representation of analogies in LLMs, using sentence embeddings to assess whether they capture the similarity between the source and target texts of an analogy, and the dissimilarity between the source and distractor texts. Second, it investigated the effectiveness of explicitly prompting LLMs to explain analogies. Throughout, we examine whether LLMs exhibit similar performance profiles to those observed in humans by evaluating their reasoning at the level of individual analogies, and not just at the level of overall accuracy (as prior studies have done). Our experiments include evaluating the impact of model size (8B vs. 70B parameters) and performance variation across state-of-the-art model architectures such as GPT-4 and LLaMA3. This work advances our understanding of the analogical reasoning abilities of LLMs and their potential as models of human reasoning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "To appear at CogSci 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.10957v1",
    "published_date": "2025-07-15 03:40:21 UTC",
    "updated_date": "2025-07-15 03:40:21 UTC"
  },
  {
    "arxiv_id": "2507.10951v1",
    "title": "Biological Processing Units: Leveraging an Insect Connectome to Pioneer Biofidelic Neural Architectures",
    "authors": [
      "Siyu Yu",
      "Zihan Qin",
      "Tingshan Liu",
      "Beiya Xu",
      "R. Jacob Vogelstein",
      "Jason Brown",
      "Joshua T. Vogelstein"
    ],
    "abstract": "The complete connectome of the Drosophila larva brain offers a unique opportunity to investigate whether biologically evolved circuits can support artificial intelligence. We convert this wiring diagram into a Biological Processing Unit (BPU), a fixed recurrent network derived directly from synaptic connectivity. Despite its modest size 3,000 neurons and 65,000 weights between them), the unmodified BPU achieves 98% accuracy on MNIST and 58% on CIFAR-10, surpassing size-matched MLPs. Scaling the BPU via structured connectome expansions further improves CIFAR-10 performance, while modality-specific ablations reveal the uneven contributions of different sensory subsystems. On the ChessBench dataset, a lightweight GNN-BPU model trained on only 10,000 games achieves 60% move accuracy, nearly 10x better than any size transformer. Moreover, CNN-BPU models with ~2M parameters outperform parameter-matched Transformers, and with a depth-6 minimax search at inference, reach 91.7% accuracy, exceeding even a 9M-parameter Transformer baseline. These results demonstrate the potential of biofidelic neural architectures to support complex cognitive tasks and motivate scaling to larger and more intelligent connectomes in future work.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "q-bio.NC"
    ],
    "primary_category": "cs.NE",
    "comment": "Accepted to AGI 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.10951v1",
    "published_date": "2025-07-15 03:31:57 UTC",
    "updated_date": "2025-07-15 03:31:57 UTC"
  },
  {
    "arxiv_id": "2507.15868v1",
    "title": "Small Edits, Big Consequences: Telling Good from Bad Robustness in Large Language Models",
    "authors": [
      "Altynbek Ismailov",
      "Salia Asanova"
    ],
    "abstract": "Large language models (LLMs) now write code in settings where misreading a single word can break safety or cost money, yet we still expect them to overlook stray typos. To probe where useful robustness ends and harmful insensitivity begins, we compile 50 LeetCode problems and craft three minimal prompt perturbations that should vary in importance: (i) progressive underspecification deleting 10 % of words per step; (ii) lexical flip swapping a pivotal quantifier (\"max\" to \"min\"); and (iii) jargon inflation replacing a common noun with an obscure technical synonym. Six frontier models, including three \"reasoning-tuned\" versions, solve each mutated prompt, and their Python outputs are checked against the original test suites to reveal whether they reused the baseline solution or adapted. Among 11 853 generations we observe a sharp double asymmetry. Models remain correct in 85 % of cases even after 90 % of the prompt is missing, showing over-robustness to underspecification, yet only 54 % react to a single quantifier flip that reverses the task, with reasoning-tuned variants even less sensitive than their bases. Jargon edits lie in between, passing through 56 %. Current LLMs thus blur the line between harmless noise and meaning - changing edits, often treating both as ignorable. Masking salient anchors such as function names can force re - evaluation. We advocate evaluation and training protocols that reward differential sensitivity: stay steady under benign noise but adapt - or refuse - when semantics truly change.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.15868v1",
    "published_date": "2025-07-15 03:22:07 UTC",
    "updated_date": "2025-07-15 03:22:07 UTC"
  },
  {
    "arxiv_id": "2507.14206v1",
    "title": "A Comprehensive Benchmark for Electrocardiogram Time-Series",
    "authors": [
      "Zhijiang Tang",
      "Jiaxin Qi",
      "Yuhua Zheng",
      "Jianqiang Huang"
    ],
    "abstract": "Electrocardiogram~(ECG), a key bioelectrical time-series signal, is crucial for assessing cardiac health and diagnosing various diseases. Given its time-series format, ECG data is often incorporated into pre-training datasets for large-scale time-series model training. However, existing studies often overlook its unique characteristics and specialized downstream applications, which differ significantly from other time-series data, leading to an incomplete understanding of its properties. In this paper, we present an in-depth investigation of ECG signals and establish a comprehensive benchmark, which includes (1) categorizing its downstream applications into four distinct evaluation tasks, (2) identifying limitations in traditional evaluation metrics for ECG analysis, and introducing a novel metric; (3) benchmarking state-of-the-art time-series models and proposing a new architecture. Extensive experiments demonstrate that our proposed benchmark is comprehensive and robust. The results validate the effectiveness of the proposed metric and model architecture, which establish a solid foundation for advancing research in ECG signal analysis.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "eess.SP",
    "comment": "Accepted to ACM MM 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.14206v1",
    "published_date": "2025-07-15 02:54:24 UTC",
    "updated_date": "2025-07-15 02:54:24 UTC"
  },
  {
    "arxiv_id": "2507.10933v1",
    "title": "Artificial Finance: How AI Thinks About Money",
    "authors": [
      "Orhan Erdem",
      "Ragavi Pobbathi Ashok"
    ],
    "abstract": "In this paper, we explore how large language models (LLMs) approach financial decision-making by systematically comparing their responses to those of human participants across the globe. We posed a set of commonly used financial decision-making questions to seven leading LLMs, including five models from the GPT series(GPT-4o, GPT-4.5, o1, o3-mini), Gemini 2.0 Flash, and DeepSeek R1. We then compared their outputs to human responses drawn from a dataset covering 53 nations. Our analysis reveals three main results. First, LLMs generally exhibit a risk-neutral decision-making pattern, favoring choices aligned with expected value calculations when faced with lottery-type questions. Second, when evaluating trade-offs between present and future, LLMs occasionally produce responses that appear inconsistent with normative reasoning. Third, when we examine cross-national similarities, we find that the LLMs' aggregate responses most closely resemble those of participants from Tanzania. These findings contribute to the understanding of how LLMs emulate human-like decision behaviors and highlight potential cultural and training influences embedded within their outputs.",
    "categories": [
      "econ.GN",
      "cs.AI"
    ],
    "primary_category": "econ.GN",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.10933v1",
    "published_date": "2025-07-15 02:54:12 UTC",
    "updated_date": "2025-07-15 02:54:12 UTC"
  },
  {
    "arxiv_id": "2507.10923v1",
    "title": "Enhancing Safe and Controllable Protein Generation via Knowledge Preference Optimization",
    "authors": [
      "Yuhao Wang",
      "Keyan Ding",
      "Kehua Feng",
      "Zeyuan Wang",
      "Ming Qin",
      "Xiaotong Li",
      "Qiang Zhang",
      "Huajun Chen"
    ],
    "abstract": "Protein language models have emerged as powerful tools for sequence generation, offering substantial advantages in functional optimization and denovo design. However, these models also present significant risks of generating harmful protein sequences, such as those that enhance viral transmissibility or evade immune responses. These concerns underscore critical biosafety and ethical challenges. To address these issues, we propose a Knowledge-guided Preference Optimization (KPO) framework that integrates prior knowledge via a Protein Safety Knowledge Graph. This framework utilizes an efficient graph pruning strategy to identify preferred sequences and employs reinforcement learning to minimize the risk of generating harmful proteins. Experimental results demonstrate that KPO effectively reduces the likelihood of producing hazardous sequences while maintaining high functionality, offering a robust safety assurance framework for applying generative models in biotechnology.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at ACL 2025 (Main Conference)",
    "pdf_url": "https://arxiv.org/pdf/2507.10923v1",
    "published_date": "2025-07-15 02:30:33 UTC",
    "updated_date": "2025-07-15 02:30:33 UTC"
  },
  {
    "arxiv_id": "2507.10920v1",
    "title": "HanjaBridge: Resolving Semantic Ambiguity in Korean LLMs via Hanja-Augmented Pre-Training",
    "authors": [
      "Seungho Choi"
    ],
    "abstract": "Large language models (LLMs) often show poor performance in low-resource languages like Korean, partly due to unique linguistic challenges such as homophonous Sino-Korean words that are indistinguishable in Hangul script. To address this semantic ambiguity, we propose HanjaBridge, a novel meaning-injection technique integrated into a continual pre-training (CPT) framework. Instead of deterministically mapping a word to a single Hanja (Chinese character), HanjaBridge presents the model with all possible Hanja candidates for a given homograph, encouraging the model to learn contextual disambiguation. This process is paired with token-level knowledge distillation to prevent catastrophic forgetting. Experimental results show that HanjaBridge significantly improves Korean language understanding, achieving a 21\\% relative improvement on the KoBALT benchmark. Notably, by reinforcing semantic alignment between Korean and Chinese through shared Hanja, we observe a strong positive cross-lingual transfer. Furthermore, these gains persist even when Hanja augmentation is omitted at inference time, ensuring practical efficiency with no additional run-time cost.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.10920v1",
    "published_date": "2025-07-15 02:26:47 UTC",
    "updated_date": "2025-07-15 02:26:47 UTC"
  },
  {
    "arxiv_id": "2507.10911v1",
    "title": "Lessons Learned from Evaluation of LLM based Multi-agents in Safer Therapy Recommendation",
    "authors": [
      "Yicong Wu",
      "Ting Chen",
      "Irit Hochberg",
      "Zhoujian Sun",
      "Ruth Edry",
      "Zhengxing Huang",
      "Mor Peleg"
    ],
    "abstract": "Therapy recommendation for chronic patients with multimorbidity is challenging due to risks of treatment conflicts. Existing decision support systems face scalability limitations. Inspired by the way in which general practitioners (GP) manage multimorbidity patients, occasionally convening multidisciplinary team (MDT) collaboration, this study investigated the feasibility and value of using a Large Language Model (LLM)-based multi-agent system (MAS) for safer therapy recommendations. We designed a single agent and a MAS framework simulating MDT decision-making by enabling discussion among LLM agents to resolve medical conflicts. The systems were evaluated on therapy planning tasks for multimorbidity patients using benchmark cases. We compared MAS performance with single-agent approaches and real-world benchmarks. An important contribution of our study is the definition of evaluation metrics that go beyond the technical precision and recall and allow the inspection of clinical goals met and medication burden of the proposed advices to a gold standard benchmark. Our results show that with current LLMs, a single agent GP performs as well as MDTs. The best-scoring models provide correct recommendations that address all clinical goals, yet the advices are incomplete. Some models also present unnecessary medications, resulting in unnecessary conflicts between medication and conditions or drug-drug interactions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.10911v1",
    "published_date": "2025-07-15 02:01:38 UTC",
    "updated_date": "2025-07-15 02:01:38 UTC"
  },
  {
    "arxiv_id": "2507.10904v2",
    "title": "Class-Proportional Coreset Selection for Difficulty-Separable Data",
    "authors": [
      "Elisa Tsai",
      "Haizhong Zheng",
      "Atul Prakash"
    ],
    "abstract": "High-quality training data is essential for building reliable and efficient machine learning systems. One-shot coreset selection addresses this by pruning the dataset while maintaining or even improving model performance, often relying on training-dynamics-based data difficulty scores. However, most existing methods implicitly assume class-wise homogeneity in data difficulty, overlooking variation in data difficulty across different classes. In this work, we challenge this assumption by showing that, in domains such as network intrusion detection and medical imaging, data difficulty often clusters by class. We formalize this as class-difficulty separability and introduce the Class Difficulty Separability Coefficient (CDSC) as a quantitative measure. We demonstrate that high CDSC values correlate with performance degradation in class-agnostic coreset methods, which tend to overrepresent easy majority classes while neglecting rare but informative ones. To address this, we introduce class-proportional variants of multiple sampling strategies. Evaluated on five diverse datasets spanning security and medical domains, our methods consistently achieve state-of-the-art performance. For instance, on CTU-13, at an extreme 99% pruning rate, a class-proportional variant of Coverage-centric Coreset Selection (CCS-CP) shows remarkable stability, with accuracy dropping only 2.58%, precision 0.49%, and recall 0.19%. In contrast, the class-agnostic CCS baseline, the next best method, suffers sharper declines of 7.59% in accuracy, 4.57% in precision, and 4.11% in recall. We further show that aggressive pruning enhances generalization in noisy, imbalanced, and large-scale datasets. Our results underscore that explicitly modeling class-difficulty separability leads to more effective, robust, and generalizable data pruning, particularly in high-stakes scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper has been accepted to the ICCV 2025 Workshop on Curated Data for Efficient Learning (CDEL)",
    "pdf_url": "https://arxiv.org/pdf/2507.10904v2",
    "published_date": "2025-07-15 01:43:32 UTC",
    "updated_date": "2025-08-13 19:01:03 UTC"
  },
  {
    "arxiv_id": "2507.10898v1",
    "title": "MalCodeAI: Autonomous Vulnerability Detection and Remediation via Language Agnostic Code Reasoning",
    "authors": [
      "Jugal Gajjar",
      "Kamalasankari Subramaniakuppusamy",
      "Noha El Kachach"
    ],
    "abstract": "The growing complexity of cyber threats and the limitations of traditional vulnerability detection tools necessitate novel approaches for securing software systems. We introduce MalCodeAI, a language-agnostic, multi-stage AI pipeline for autonomous code security analysis and remediation. MalCodeAI combines code decomposition and semantic reasoning using fine-tuned Qwen2.5-Coder-3B-Instruct models, optimized through Low-Rank Adaptation (LoRA) within the MLX framework, and delivers scalable, accurate results across 14 programming languages. In Phase 1, the model achieved a validation loss as low as 0.397 for functional decomposition and summarization of code segments after 200 iterations, 6 trainable layers, and a learning rate of 2 x 10^(-5). In Phase 2, for vulnerability detection and remediation, it achieved a best validation loss of 0.199 using the same number of iterations and trainable layers but with an increased learning rate of 4 x 10^(-5), effectively identifying security flaws and suggesting actionable fixes. MalCodeAI supports red-hat-style exploit tracing, CVSS-based risk scoring, and zero-shot generalization to detect complex, zero-day vulnerabilities. In a qualitative evaluation involving 15 developers, the system received high scores in usefulness (mean 8.06/10), interpretability (mean 7.40/10), and readability of outputs (mean 7.53/10), confirming its practical value in real-world development workflows. This work marks a significant advancement toward intelligent, explainable, and developer-centric software security solutions.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CR",
    "comment": "6 pages, 4 figures, accepted for publication in IEEE 26th International Conference on Information Reuse and Integration (IRI 2025)",
    "pdf_url": "https://arxiv.org/pdf/2507.10898v1",
    "published_date": "2025-07-15 01:25:04 UTC",
    "updated_date": "2025-07-15 01:25:04 UTC"
  },
  {
    "arxiv_id": "2507.10895v1",
    "title": "Commuting Distance Regularization for Timescale-Dependent Label Inconsistency in EEG Emotion Recognition",
    "authors": [
      "Xiaocong Zeng",
      "Craig Michoski",
      "Yan Pang",
      "Dongyang Kuang"
    ],
    "abstract": "In this work, we address the often-overlooked issue of Timescale Dependent Label Inconsistency (TsDLI) in training neural network models for EEG-based human emotion recognition. To mitigate TsDLI and enhance model generalization and explainability, we propose two novel regularization strategies: Local Variation Loss (LVL) and Local-Global Consistency Loss (LGCL). Both methods incorporate classical mathematical principles--specifically, functions of bounded variation and commute-time distances--within a graph theoretic framework. Complementing our regularizers, we introduce a suite of new evaluation metrics that better capture the alignment between temporally local predictions and their associated global emotion labels. We validate our approach through comprehensive experiments on two widely used EEG emotion datasets, DREAMER and DEAP, across a range of neural architectures including LSTM and transformer-based models. Performance is assessed using five distinct metrics encompassing both quantitative accuracy and qualitative consistency. Results consistently show that our proposed methods outperform state-of-the-art baselines, delivering superior aggregate performance and offering a principled trade-off between interpretability and predictive power under label inconsistency. Notably, LVL achieves the best aggregate rank across all benchmarked backbones and metrics, while LGCL frequently ranks the second, highlighting the effectiveness of our framework.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.SP"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.10895v1",
    "published_date": "2025-07-15 01:22:14 UTC",
    "updated_date": "2025-07-15 01:22:14 UTC"
  },
  {
    "arxiv_id": "2507.10894v1",
    "title": "NavComposer: Composing Language Instructions for Navigation Trajectories through Action-Scene-Object Modularization",
    "authors": [
      "Zongtao He",
      "Liuyi Wang",
      "Lu Chen",
      "Chengju Liu",
      "Qijun Chen"
    ],
    "abstract": "Language-guided navigation is a cornerstone of embodied AI, enabling agents to interpret language instructions and navigate complex environments. However, expert-provided instructions are limited in quantity, while synthesized annotations often lack quality, making them insufficient for large-scale research. To address this, we propose NavComposer, a novel framework for automatically generating high-quality navigation instructions. NavComposer explicitly decomposes semantic entities such as actions, scenes, and objects, and recomposes them into natural language instructions. Its modular architecture allows flexible integration of state-of-the-art techniques, while the explicit use of semantic entities enhances both the richness and accuracy of instructions. Moreover, it operates in a data-agnostic manner, supporting adaptation to diverse navigation trajectories without domain-specific training. Complementing NavComposer, we introduce NavInstrCritic, a comprehensive annotation-free evaluation system that assesses navigation instructions on three dimensions: contrastive matching, semantic consistency, and linguistic diversity. NavInstrCritic provides a holistic evaluation of instruction quality, addressing limitations of traditional metrics that rely heavily on expert annotations. By decoupling instruction generation and evaluation from specific navigation agents, our method enables more scalable and generalizable research. Extensive experiments provide direct and practical evidence for the effectiveness of our method.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.10894v1",
    "published_date": "2025-07-15 01:20:22 UTC",
    "updated_date": "2025-07-15 01:20:22 UTC"
  },
  {
    "arxiv_id": "2507.11570v1",
    "title": "SurgeryLSTM: A Time-Aware Neural Model for Accurate and Explainable Length of Stay Prediction After Spine Surgery",
    "authors": [
      "Ha Na Cho",
      "Sairam Sutari",
      "Alexander Lopez",
      "Hansen Bow",
      "Kai Zheng"
    ],
    "abstract": "Objective: To develop and evaluate machine learning (ML) models for predicting length of stay (LOS) in elective spine surgery, with a focus on the benefits of temporal modeling and model interpretability. Materials and Methods: We compared traditional ML models (e.g., linear regression, random forest, support vector machine (SVM), and XGBoost) with our developed model, SurgeryLSTM, a masked bidirectional long short-term memory (BiLSTM) with an attention, using structured perioperative electronic health records (EHR) data. Performance was evaluated using the coefficient of determination (R2), and key predictors were identified using explainable AI. Results: SurgeryLSTM achieved the highest predictive accuracy (R2=0.86), outperforming XGBoost (R2 = 0.85) and baseline models. The attention mechanism improved interpretability by dynamically identifying influential temporal segments within preoperative clinical sequences, allowing clinicians to trace which events or features most contributed to each LOS prediction. Key predictors of LOS included bone disorder, chronic kidney disease, and lumbar fusion identified as the most impactful predictors of LOS. Discussion: Temporal modeling with attention mechanisms significantly improves LOS prediction by capturing the sequential nature of patient data. Unlike static models, SurgeryLSTM provides both higher accuracy and greater interpretability, which are critical for clinical adoption. These results highlight the potential of integrating attention-based temporal models into hospital planning workflows. Conclusion: SurgeryLSTM presents an effective and interpretable AI solution for LOS prediction in elective spine surgery. Our findings support the integration of temporal, explainable ML approaches into clinical decision support systems to enhance discharge readiness and individualized patient care.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.11570v1",
    "published_date": "2025-07-15 01:18:28 UTC",
    "updated_date": "2025-07-15 01:18:28 UTC"
  },
  {
    "arxiv_id": "2507.10893v1",
    "title": "Modernizing CNN-based Weather Forecast Model towards Higher Computational Efficiency",
    "authors": [
      "Minjong Cheon",
      "Eunhan Goo",
      "Su-Hyeon Shin",
      "Muhammad Ahmed",
      "Hyungjun Kim"
    ],
    "abstract": "Recently, AI-based weather forecast models have achieved impressive advances. These models have reached accuracy levels comparable to traditional NWP systems, marking a significant milestone in data-driven weather prediction. However, they mostly leverage Transformer-based architectures, which often leads to high training complexity and resource demands due to the massive parameter sizes. In this study, we introduce a modernized CNN-based model for global weather forecasting that delivers competitive accuracy while significantly reducing computational requirements. To present a systematic modernization roadmap, we highlight key architectural enhancements across multiple design scales from an earlier CNN-based approach. KAI-a incorporates a scale-invariant architecture and InceptionNeXt-based blocks within a geophysically-aware design, tailored to the structure of Earth system data. Trained on the ERA5 daily dataset with 67 atmospheric variables, the model contains about 7 million parameters and completes training in just 12 hours on a single NVIDIA L40s GPU. Our evaluation shows that KAI-a matches the performance of state-of-the-art models in medium-range weather forecasting, while offering a significantly lightweight design. Furthermore, case studies on the 2018 European heatwave and the East Asian summer monsoon demonstrate KAI-a's robust skill in capturing extreme events, reinforcing its practical utility.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "physics.ao-ph"
    ],
    "primary_category": "cs.CV",
    "comment": "26pages, 9 Figures",
    "pdf_url": "https://arxiv.org/pdf/2507.10893v1",
    "published_date": "2025-07-15 01:16:32 UTC",
    "updated_date": "2025-07-15 01:16:32 UTC"
  },
  {
    "arxiv_id": "2507.10886v1",
    "title": "How to Protect Models against Adversarial Unlearning?",
    "authors": [
      "Patryk Jasiorski",
      "Marek Klonowski",
      "Michał Woźniak"
    ],
    "abstract": "AI models need to be unlearned to fulfill the requirements of legal acts such as the AI Act or GDPR, and also because of the need to remove toxic content, debiasing, the impact of malicious instances, or changes in the data distribution structure in which a model works. Unfortunately, removing knowledge may cause undesirable side effects, such as a deterioration in model performance. In this paper, we investigate the problem of adversarial unlearning, where a malicious party intentionally sends unlearn requests to deteriorate the model's performance maximally. We show that this phenomenon and the adversary's capabilities depend on many factors, primarily on the backbone model itself and strategy/limitations in selecting data to be unlearned. The main result of this work is a new method of protecting model performance from these side effects, both in the case of unlearned behavior resulting from spontaneous processes and adversary actions.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.10886v1",
    "published_date": "2025-07-15 00:59:42 UTC",
    "updated_date": "2025-07-15 00:59:42 UTC"
  },
  {
    "arxiv_id": "2507.11569v2",
    "title": "Are Vision Foundation Models Ready for Out-of-the-Box Medical Image Registration?",
    "authors": [
      "Hanxue Gu",
      "Yaqian Chen",
      "Nicholas Konz",
      "Qihang Li",
      "Maciej A. Mazurowski"
    ],
    "abstract": "Foundation models, pre-trained on large image datasets and capable of capturing rich feature representations, have recently shown potential for zero-shot image registration. However, their performance has mostly been tested in the context of rigid or less complex structures, such as the brain or abdominal organs, and it remains unclear whether these models can handle more challenging, deformable anatomy. Breast MRI registration is particularly difficult due to significant anatomical variation between patients, deformation caused by patient positioning, and the presence of thin and complex internal structure of fibroglandular tissue, where accurate alignment is crucial. Whether foundation model-based registration algorithms can address this level of complexity remains an open question. In this study, we provide a comprehensive evaluation of foundation model-based registration algorithms for breast MRI. We assess five pre-trained encoders, including DINO-v2, SAM, MedSAM, SSLSAM, and MedCLIP, across four key breast registration tasks that capture variations in different years and dates, sequences, modalities, and patient disease status (lesion versus no lesion). Our results show that foundation model-based algorithms such as SAM outperform traditional registration baselines for overall breast alignment, especially under large domain shifts, but struggle with capturing fine details of fibroglandular tissue. Interestingly, additional pre-training or fine-tuning on medical or breast-specific images in MedSAM and SSLSAM, does not improve registration performance and may even decrease it in some cases. Further work is needed to understand how domain-specific training influences registration and to explore targeted strategies that improve both global alignment and fine structure accuracy. We also publicly release our code at \\href{https://github.com/mazurowski-lab/Foundation-based-reg}{Github}.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "3 figures, 9 pages",
    "pdf_url": "https://arxiv.org/pdf/2507.11569v2",
    "published_date": "2025-07-15 00:17:14 UTC",
    "updated_date": "2025-08-08 20:15:59 UTC"
  }
]
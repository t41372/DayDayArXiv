{
  "date": "2025-03-21",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-03-21 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 上的论文继续聚焦大型语言模型 (LLM) 的方方面面，从基础能力评估（如推理、一致性、偏好学习、文化适应性）到特定领域应用（如医疗、金融、代码生成、机器人、网络安全），以及训练优化（如知识蒸馏、模型扩展、误差学习）和安全对齐。同时，扩散模型在可控视频生成、多目标优化方面有新进展，机器人学习（特别是交互和运动生成）、多模态理解（融合音频、视频、文本）和新的基准测试（如沙特文化、网络漏洞利用、多模态时间序列）也备受关注。值得一提的是，有研究探索了利用 LLM 进行放射治疗计划优化和发现税收漏洞的可能性，展示了 AI 在专业领域的潜力。\n\n---\n\n**精选论文概览：**\n\n**LLM 能力与评估**\n\n1.  **大型语言模型可能逐字补全其未明确训练过的文本 (Language Models May Verbatim Complete Text They Were Not Explicitly Trained On)**\n    这篇研究发现，LLM 可能会逐字补全那些根据 n-gram 重叠定义不属于其训练集的文本序列，包括精确重复、近似重复甚至只有少量重叠的情况。通过设计对抗性数据集，研究者证明可以诱导模型补全目标序列，而无需在训练集中包含它。这表明基于 n-gram 的成员资格定义不足以判断文本是否被用于训练，并对隐私和版权评估提出了挑战。\n\n2.  **语言模型偏爱 Python：一项关于 LLM 对编程语言和库偏好的研究 (LLMs Love Python: A Study of LLMs' Bias for Programming Languages and Libraries)**\n    研究调查了 8 个 LLM 在代码生成任务中对编程语言和库的选择偏好。结果显示，LLM 强烈偏爱 Python，在解决语言无关的基准任务时使用率高达 90%-97%，即使在 Python 不适用的项目初始化场景中也常用。此外，模型经常与其自身的语言推荐相矛盾，并偏爱成熟库。这揭示了 LLM 在编程语言选择上的偏见，可能阻碍新语言和库的发现与应用。\n\n3.  **评估大型语言模型输出的一致性和可复现性：跨多种金融和会计任务的证据 (Assessing Consistency and Reproducibility in the Outputs of Large Language Models: Evidence Across Diverse Finance and Accounting Tasks)**\n    这项研究首次全面评估了 LLM 在金融和会计任务中输出的一致性和可复现性。通过对 5 类任务（分类、情感分析、摘要、文本生成、预测）进行 50 次独立运行，发现 LLM 的一致性很高（尤其在分类和情感分析中接近完美），显著优于人类专家，且即使在人类分歧较大的地方也保持高一致性。简单聚合 3-5 次运行结果能显著提高一致性，且下游统计推断对 LLM 输出的不一致性具有鲁棒性，降低了 \"G-hacking\"（选择性报告有利结果）的风险。\n\n4.  **贝叶斯教学使大型语言模型具备概率推理能力 (Bayesian Teaching Enables Probabilistic Reasoning in Large Language Models)**\n    研究发现，现有 LLM 在接收新信息时，其信念更新方式不符合贝叶斯框架，导致预测能力提升有限。通过训练 LLM 模仿最优贝叶斯模型的预测，不仅显著提高了模型在特定推荐任务上的性能，还能泛化到其他任务，表明该方法赋予了 LLM 更广泛的贝叶斯推理技能。\n\n5.  **FactSelfCheck：面向 LLM 的事实级黑盒幻觉检测 (FactSelfCheck: Fact-Level Black-Box Hallucination Detection for LLMs)**\n    针对 LLM 幻觉检测通常在句子或段落级别进行的问题，本文提出 FactSelfCheck，一种基于黑盒采样的细粒度事实级检测方法。该方法将文本表示为知识图谱（事实三元组），通过分析多个 LLM 响应间的事实一致性来计算幻觉分数，无需外部资源或训练数据。实验表明，该方法性能与主流采样方法相当，且事实级检测显著提高了幻觉修正效果。\n\n6.  **什么可被生成，未必可被引导：衡量生成模型的可操控性 (What's Producible May Not Be Reachable: Measuring the Steerability of Generative Models)**\n    现有生成模型评估指标多关注其“可生成性”（输出质量和广度），而忽略了用户能否引导模型生成满足特定目标的输出，即“可操控性”(steerability)。本文提出了一个评估可操控性的框架，并通过大规模用户研究（让用户尝试复现模型生成的图像/文本）发现，尽管当前图文生成模型和 LLM 的可生成性很强，但可操控性普遍较差。研究还证明通过强化学习可提升图像模型的可操控性。\n\n7.  **捕捉个体人类偏好与奖励特征 (Capturing Individual Human Preferences with Reward Features)**\n    传统的基于人类反馈的强化学习（RLHF）通常使用单一奖励模型，忽略了个体偏好差异。本文提出一种方法，通过学习一组通用的奖励特征，并将个体偏好表示为这些特征的线性组合，从而快速将奖励模型个性化到特定个体或群体，即使其偏好未在训练数据中体现。实验表明，该方法在处理分歧较大的偏好数据时优于非自适应和上下文个性化基线。\n\n8.  **语言特定神经元并不促进跨语言迁移 (Language-specific Neurons Do Not Facilitate Cross-Lingual Transfer)**\n    研究探索了识别和利用多语言 LLM 中语言特定神经元来提升低资源语言跨语言任务性能的可能性。通过在 Llama 3.1 和 Mistral Nemo 等模型上进行实验，发现现有的语言特定神经元识别技术（如基于激活概率熵或阈值的方法）结合 LoRA 微调，并不能显著改善低资源语言在下游任务（XNLI, XQuAD）上的表现，突显了实现跨语言泛化的挑战。\n\n9.  **SaudiCulture：评估大型语言模型在沙特阿拉伯文化适应性的基准 (SaudiCulture: A Benchmark for Evaluating Large Language Models Cultural Competence within Saudi Arabia)**\n    针对 LLM 难以准确捕捉文化细微差别的问题，本文提出了 SaudiCulture 基准，专门用于评估 LLM 在沙特阿拉伯不同地理区域（西部、东部、南部、北部、中部）和文化领域（食物、服饰、娱乐、庆典、工艺）的文化能力。该基准包含不同复杂度（开放式、单选、多选）的问题。对 GPT-4、Llama 3.3 等模型的评估显示，所有模型在处理高度专业化或地域性问题（尤其需多选答案）时性能显著下降，强调了将地域特定知识融入 LLM 训练的重要性。\n\n**LLM 应用与框架**\n\n10. **自主放射治疗计划优化智能体 DOLA (Autonomous Radiotherapy Treatment Planning Using DOLA)**\n    本文介绍了 DOLA，一个基于本地部署 LLM (LLaMa3.1) 的自主智能体，用于优化放射治疗计划，同时保护患者隐私。通过结合思维链 (CoT) 推理、检索增强生成 (RAG) 和强化学习 (RL)，DOLA 能直接与商业治疗计划系统交互。研究表明，70B 模型优于 8B 模型，RAG+RL 策略效果最佳，证明了 LLM 智能体在临床工作流程中自动化和改进复杂医疗任务的潜力。\n\n11. **LLM+MAP：使用大型语言模型和规划领域定义语言进行双臂机器人任务规划 (LLM+MAP: Bimanual Robot Task Planning using Large Language Models and Planning Domain Definition Language)**\n    针对双臂机器人操作中任务规划的复杂性（任务分解与分配），本文提出 LLM+MAP 框架。该框架结合了 LLM 的推理能力和多智能体规划 (MAP)，将 LLM 生成的高层计划转化为 PDDL，再由符号规划器生成可执行的双臂动作序列。实验证明，相比直接由 LLM (GPT-4o, V3, o1, R1) 生成计划，LLM+MAP 在规划时间、成功率、任务分配效率等方面表现更优。\n\n12. **AI 能否揭示税收漏洞？迈向新一代法律政策助手 (Can AI expose tax loopholes? Towards a new generation of legal policy assistants)**\n    本研究提出了一个原型系统，旨在利用 AI 发现税法中的漏洞和避税方案。该混合解决方案结合了自然语言界面和专为规划设计的领域特定语言。通过案例研究，展示了如何利用该系统揭示税收漏洞。研究认为，该原型有助于通过系统性识别和解决税收缺口来提升社会福利。\n\n13. **使用大型语言模型自动裁决心血管事件 (Automating Adjudication of Cardiovascular Events Using Large Language Models)**\n    临床试验中心血管事件的裁决过程耗时且易变。本文提出一个使用 LLM 自动裁决的框架。该框架分两阶段：首先用 LLM 从非结构化临床数据中提取事件信息，然后基于思维树 (Tree of Thoughts) 方法和临床终点委员会 (CEC) 指南进行 LLM 裁决。在真实数据上，事件提取 F1 分数为 0.82，裁决准确率为 0.68。同时引入了 CLEART 评分，用于自动评估 AI 生成的临床推理质量。该方法有望降低裁决成本和时间，提高一致性。\n\n14. **MedAgent-Pro：通过推理智能体工作流实现基于多模态证据的医学诊断 (MedAgent-Pro: Towards Multi-modal Evidence-based Medical Diagnosis via Reasoning Agentic Workflow)**\n    为解决多模态大语言模型 (MLLM) 在医学诊断中缺乏对视觉输入的精细感知和推理不一致的问题，本文提出 MedAgent-Pro，一个基于证据的推理智能体系统。它采用分层工作流：任务层面，基于知识的推理根据检索到的临床标准生成诊断计划；案例层面，多个工具智能体处理多模态输入，按计划分析指标，并基于量化和定性证据给出最终诊断。实验证明了其在 2D 和 3D 医学诊断任务上的优越性、可靠性和可解释性。\n\n15. **大型语言模型 (LLM) 用于源代码分析：应用、模型和数据集 (Large Language Models (LLMs) for Source Code Analysis: applications, models and datasets)**\n    这篇综述探讨了 LLM 在源代码分析任务中的应用、使用的模型和数据集以及面临的挑战。文章总结了该领域的研究进展、当前趋势和知识结构，并指出了局限性、关键工具和未来研究方向。\n\n16. **你的声音就是你的声音：通过语音生成和 LLM 支持增强和替代沟通中的自我表达 (Your voice is your voice: Supporting Self-expression through Speech Generation and LLMs in Augmented and Alternative Communication)**\n    本文介绍了 Speak Ease，一个增强和替代沟通 (AAC) 系统。它整合了文本、语音和上下文线索（对话伙伴、情绪）等多模态输入，利用 LLM、自动语音识别 (ASR) 和个性化文本转语音 (TTS) 技术，旨在支持用户更具个性化、自然和富有表现力的沟通。初步研究和焦点小组评估显示了其在增强 AAC 用户表达力方面的潜力。\n\n17. **用于增强医患对话的后续问题生成 (Follow-up Question Generation For Enhanced Patient-Provider Conversations)**\n    为解决异步医疗对话中信息提取和并行思维建模的挑战，本文提出 FollowupQ 框架。这是一个多智能体框架，处理患者消息和电子健康记录 (EHR) 数据，生成个性化的后续问题以澄清病情。实验表明，FollowupQ 能减少 34% 的必要医患后续沟通，并在真实和合成数据上分别提升 17% 和 5% 的性能。同时发布了首个包含关联 EHR 的异步医疗消息及后续问题的数据集。\n\n18. **理解在线问答社区中的社会支持需求：整合半监督学习和基于 LLM 数据增强的混合方法 (Understanding Social Support Needs in Questions: A Hybrid Approach Integrating Semi-Supervised Learning and LLM-based Data Augmentation)**\n    为识别在线健康问答中用户提问所蕴含的社会支持需求，本文提出 HA-SOS 框架。该框架整合了答案增强的半监督学习、利用 LLM 进行文本数据增强（带有可靠性和多样性感知选择机制）以及统一训练过程。实验表明，HA-SOS 显著优于现有方法，有助于平台管理者和回答者更好地理解用户需求并提供个性化支持。\n\n19. **基于 LLM 的公平性驱动因果发现与主动学习和动态评分 (Fairness-Driven LLM-based Causal Discovery with Active Learning and Dynamic Scoring)**\n    本文提出一个利用 LLM 进行因果发现的框架，采用基于元数据的方法模拟专家推理。通过从成对查询转向更具可扩展性的广度优先搜索 (BFS) 策略，将查询数量从变量数的平方级降至线性级。结合主动学习 (AL) 和动态评分机制（基于互信息、偏相关和 LLM 置信度），优先处理信息增益潜力大的查询，从而更高效准确地构建因果图，并用于公平性分析。\n\n20. **使用来自 LLM 的知识蒸馏实现多方对话的高效意图过滤 (Efficient Intent-Based Filtering for Multi-Party Conversations Using Knowledge Distillation from LLMs)**\n    为降低处理多方对话时 LLM 的资源消耗，本文提出一种基于意图的过滤方法。该方法利用从 LLM 蒸馏的知识，训练一个轻量级模型 (MobileBERT) 进行多标签意图分类，只将包含目标意图的对话片段传递给 LLM 进行后续处理（如摘要、洞察生成）。实验证明，这种方法能在保持性能的同时显著降低计算成本。\n\n**LLM 训练与优化**\n\n21. **通过课程提取实现高效知识蒸馏 (Efficient Knowledge Distillation via Curriculum Extraction)**\n    知识蒸馏通常使用最终教师模型的输出来训练学生模型。本文提出一种新方法，仅从完全训练好的教师模型中“提取”课程，而非依赖存储中间检查点。通过使用教师模型隐藏表示的随机投影来逐步训练学生网络，然后在最终输出上训练，该方法在稀疏奇偶校验学习和语言建模任务中显著优于单次蒸馏，并达到与渐进式蒸馏相似的效率。\n\n22. **稀疏 Logit 采样：加速 LLM 中的知识蒸馏 (Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs)**\n    在 LLM 预训练中应用知识蒸馏时，预计算和缓存教师模型的 Logits 是关键。本文证明了朴素的稀疏蒸馏方法（如缓存 Top-K 概率）会产生有偏估计。为此，提出基于重要性采样的“随机采样知识蒸馏”方法，提供无偏估计，期望上保留梯度，且只需存储更稀疏的 Logits。该方法能加速学生模型训练（开销<10%），并在 300M 到 3B 参数模型上保持与完全蒸馏相当的性能。\n\n23. **LEMMA：通过从错误中学习促进 LLM 的数学进步 (LEMMA: Learning from Errors for MatheMatical Advancement in LLMs)**\n    现有方法主要关注提升正确训练数据的质量，忽略了错误数据中蕴含的价值。本文提出 LEMMA，通过构建包含错误步骤的不正确解及其到正确解的反思连接的数据进行微调，从而增强 LLM 的数学推理能力。通过系统分析错误类型并设计错误增强方法收集多样化错误，模型能够在生成过程中自主纠错，无需外部评判模型。实验证明 LEMMA 显著优于强基线。\n\n24. **通过训练后模型扩展改进量化 (Improving Quantization with Post-Training Model Expansion)**\n    传统的量化旨在减小模型体积以降低推理成本。本文反其道而行之，证明在量化过程中，通过“训练后模型扩展”可以提升模型质量。例如，在对 Llama3 1B 进行 4 比特权重量化时，通过增加仅 5% 的参数（仍比 BF16 参考模型体积小 3.8%），将零样本准确率与全精度模型的差距平均缩小了 3%。这为在量化约束下提升模型性能提供了新思路。\n\n25. **使用 Matryoshka 稀疏自编码器学习多层次特征 (Learning Multi-Level Features with Matryoshka Sparse Autoencoders)**\n    稀疏自编码器 (SAE) 用于解释神经网络，但字典大小选择是个难题。本文提出 Matryoshka SAEs，通过同时训练多个嵌套字典，强制小字典独立重建，从而学习到层次化特征（从通用到具体），减少了高层特征被吸收的问题，提高了特征的可解释性和在下游任务中的表现。\n\n26. **SafeMERGE：通过选择性逐层模型合并在微调 LLM 中保留安全对齐 (SafeMERGE: Preserving Safety Alignment in Fine-Tuned Large Language Models via Selective Layer-Wise Model Merging)**\n    微调 LLM 可能无意中破坏其安全对齐。本文提出 SafeMERGE 框架，在微调后通过选择性地合并微调模型和安全对齐模型的层来恢复安全性，同时保持任务效用。合并决策基于层与安全行为的偏差（通过余弦相似度衡量）。实验表明，SafeMERGE 能有效减少有害输出，且不显著牺牲性能，优于其他基线方法。\n\n27. **MARS：一个融合苏格拉底式指导的多智能体框架用于自动化提示优化 (MARS: A Multi-Agent Framework Incorporating Socratic Guidance for Automated Prompt Optimization)**\n    为解决现有自动化提示优化 (APO) 方法模板固定、搜索效率低的问题，本文提出 MARS 框架。该框架包含七个功能不同的智能体，利用 Planner 自主规划优化路径，并采用 Teacher-Critic-Student 的苏格拉底式对话模式进行迭代优化和有效搜索。实验证明了该方法在提升提示效果方面的有效性。\n\n**扩散模型与生成**\n\n28. **实现视频扩散模型的通用控制 (Enabling Versatile Controls for Video Diffusion Models)**\n    为解决文本到视频生成中精细时空控制的难题，本文提出 VCtrl (PP-VCtrl) 框架。该框架能将多种用户指定的控制信号（如 Canny 边缘、分割掩码、人体关键点）统一集成到预训练的视频扩散模型中，无需修改生成器本身。通过通用条件模块、统一控制信号编码流水线和稀疏残差连接机制，VCtrl 有效提升了视频生成的可控性和质量。\n\n29. **偏好引导的扩散模型用于多目标离线优化 (Preference-Guided Diffusion for Multi-Objective Offline Optimization)**\n    本文提出一种偏好引导的扩散模型，用于从离线数据集中生成帕累托最优解。通过训练一个偏好模型（分类器）来预测一个设计优于另一个设计的概率，并以此指导扩散过程。引入了新颖的“多样性感知偏好引导”，在帕累托最优性之外加入多样性标准，确保生成的解既最优又在目标空间中分布良好。实验证明该方法优于其他生成式方法，并与基于代理的优化方法具有竞争力。\n\n30. **当偏好分歧时：使用少数群体感知的自适应 DPO 对齐扩散模型 (When Preferences Diverge: Aligning Diffusion Models with Minority-Aware Adaptive DPO)**\n    在利用 Diffusion-DPO 等方法对齐人类偏好时，偏好数据中的少数群体样本可能对模型性能产生不利影响。本文提出 Adaptive-DPO，将一个感知少数实例的度量（包含标注者内部置信度和标注者间稳定性）整合进 DPO 目标函数。通过调整损失函数，增强模型对多数标签的学习，同时减轻少数样本的负面影响，从而更有效地处理真实世界的偏好数据。\n\n31. **ARFlow：基于物理引导的人类动作-反应流匹配 (ARFlow: Human Action-Reaction Flow Matching with Physical Guidance)**\n    为解决扩散模型在人类交互合成中依赖复杂条件机制且易产生物理违规的问题，本文提出 ARFlow 框架。它直接建立动作到反应的映射，无需复杂条件机制。通过 x1 预测直接输出动作而非速度场，并引入免训练、基于梯度的物理引导机制，有效防止采样过程中的身体穿透。实验表明 ARFlow 在动作质量、多样性和物理合理性方面均优于现有方法。\n\n32. **用于游戏的实时扩散策略：用 Q-Ensembles 增强一致性策略 (Real-Time Diffusion Policies for Games: Enhancing Consistency Policies with Q-Ensembles)**\n    扩散模型能捕捉游戏中复杂的多模态动作分布，但推理速度慢。一致性模型虽快，但训练不稳定。本文提出 CPQE，结合一致性模型和 Q-ensembles。利用 Q-ensembles 进行不确定性估计，提供更可靠的价值函数近似，提高了训练稳定性和性能。实验表明 CPQE 推理速度可达 60Hz（远超扩散策略的 20Hz），性能与多步扩散方法相当，且优于基线一致性模型方法。\n\n**机器人与交互**\n\n33. **PRIMAL：用于虚拟化身学习的物理反应与交互式运动模型 (PRIMAL: Physically Reactive and Interactive Motor Model for Avatar Learning)**\n    为构建交互式虚拟化身的运动系统，本文提出 PRIMAL，一个自回归扩散模型。采用两阶段学习范式：预训练阶段学习大量短时运动片段的动力学；适应阶段使用类 ControlNet 适配器微调以生成语义动作和到达空间目标。该模型能生成无限长、逼真、可控且能实时响应外部脉冲的运动，并能有效适应少样本个性化动作和空间控制任务。\n\n34. **HAPI：从人类偏好中学习机器人面部表情的模型 (HAPI: A Model for Learning Robot Facial Expressions from Human Preferences)**\n    为生成更自然、细致的机器人面部表情，本文提出 HAPI 模型，一个基于学习排序的框架。通过收集人类对机器人表情的成对比较偏好数据，训练一个基于 Siamese RankNet 的模型来优化表情评估。在 35 自由度的机器人平台上进行的实验表明，该方法生成的愤怒、快乐和惊讶表情比基线和专家设计的方法更真实、更能引发社会共鸣。\n\n35. **DyWA：用于可泛化非抓取操作的动力学自适应世界动作模型 (DyWA: Dynamics-adaptive World Action Model for Generalizable Non-prehensile Manipulation)**\n    针对非抓取操作中现有方法依赖多视角和精确位姿跟踪、且难以泛化到不同物理条件的问题，本文提出 DyWA 框架。该框架通过联合预测未来状态和基于历史轨迹适应动力学变化（如物体质量、摩擦力），统一建模几何、状态、物理和机器人动作。实验表明，仅使用单视角点云，DyWA 就能在仿真和真实世界中实现对不同几何形状、不同摩擦力甚至含水瓶子等挑战性场景的鲁棒操作。\n\n36. **用于生成 3D 人-物交互的自回归扩散模型 (Auto-Regressive Diffusion for Generating 3D Human-Object Interactions)**\n    为解决文本驱动的人-物交互 (Text-to-HOI) 生成中长序列交互一致性差的问题，本文提出 ARDHOI，一个自回归扩散模型。引入对比变分自编码器 (cVAE) 学习物理上合理的连续 HOI token 空间，确保动作逼真。使用基于 Mamba 的上下文编码器捕捉序列动作，并用 MLP denoiser 生成下一 token。实验表明该模型在性能和推理速度上均优于 SOTA 方法。\n\n**计算机视觉与多模态**\n\n37. **衡量音频 Deepfake 检测器的鲁棒性 (Measuring the Robustness of Audio Deepfake Detectors)**\n    本文系统评估了 10 种音频 Deepfake 检测模型在 16 种常见音频失真（噪声、修改、压缩）下的鲁棒性。发现大多数模型对噪声鲁棒性强，但对修改和压缩（尤其是神经编解码器）更脆弱。语音基础模型通常优于传统模型。模型增大能提升鲁棒性，但收益递减。目标性数据增强可提高模型对未见干扰的韧性。\n\n38. **ProtoGS：使用 3D 高斯原型实现高效高质量渲染 (ProtoGS: Efficient and High-Quality Rendering with 3D Gaussian Prototypes)**\n    为解决 3D 高斯溅射 (3DGS) 需要大量高斯基元的问题，本文提出 ProtoGS。通过学习高斯原型来表示高斯基元，显著减少高斯数量而不牺牲视觉质量。利用 SfM 点作为锚点对高斯基元分组，在组内通过 K-means 聚类得到原型，并联合优化锚点和原型。实验证明，该方法大幅减少了高斯数量，实现了高渲染速度，同时保持甚至提升了渲染保真度。\n\n39. **用于高质量数据扩展的基于潜在空间扩展的音频增强视觉语言建模 (Audio-Enhanced Vision-Language Modeling with Latent Space Broadening for High Quality Data Expansion)**\n    为提升工业级推荐、搜索和广告系统中的多模态模型性能，本文提出 kNN-based Latent Space Broadening (LSB) 来提高主动学习 (AL) 效率，以及 Vision-Language Modeling with Audio Enhancement (VLMAE)，一种将音频信息融入现有视觉-语言 (VL) 模型的中间融合方法。该系统已在生产中部署并带来显著业务收益。\n\n40. **FFaceNeRF：神经辐射场中的少样本人脸编辑 (FFaceNeRF: Few-shot Face Editing in Neural Radiance Fields)**\n    现有基于 NeRF 和掩码的人脸编辑方法受限于预训练分割掩码，用户控制有限。本文提出 FFaceNeRF，一种基于 NeRF 的人脸编辑技术，使用带有特征注入的几何适配器有效操纵几何属性，并采用潜在混合进行三平面增强，从而能够用少量样本快速适应期望的掩码布局，实现更灵活、高质量的 3D 人脸编辑。\n\n41. **从人脸到声音：学习用于高质量视频到语音合成的层次化表示 (From Faces to Voices: Learning Hierarchical Representations for High-quality Video-to-Speech)**\n    本文提出一种新的视频到语音合成系统，通过学习从视频到语音的层次化表示来有效弥合模态差距。系统分三阶段（内容、音色、韵律建模）逐步将无声视频转换为声学特征空间，并将视觉因素（唇动、身份、表情）与声学对应物对齐。结合流匹配模型生成语音，实验证明该方法合成质量接近真实语音，显著优于现有方法。\n\n42. **PVChat：基于单样本学习的个性化视频聊天 (PVChat: Personalized Video Chat with One-Shot Learning)**\n    现有视频大语言模型 (ViLLM) 难以进行身份感知的理解。本文提出 PVChat，首个个性化 ViLLM，能从每个主体的单个视频中学习并进行主体感知的问答。方法包括：合成身份保留的正样本和检索难负样本的数据增强流水线；ReLU Routing MoH 注意力机制及两个新目标函数（平滑邻近正则化、头部激活增强）；从图像预训练到视频微调的两阶段策略。实验证明其在单样本学习后具有优越的个性化特征理解能力。\n\n43. **TEMPO：通过难度调度和预 SFT 对齐进行视频 LLM 的时间偏好优化 (TEMPO: Temporal Preference Optimization of Video LLMs via Difficulty Scheduling and Pre-SFT Alignment)**\n    为增强视频 LLM 的时间推理能力，本文提出 TEMPO 框架，利用直接偏好优化 (DPO)。引入自动化偏好数据生成流水线，构建包含丰富时间信息的视频对。时间对齐采用课程学习（逐步增加扰动难度）和“预 SFT 对齐”（在指令调优前进行偏好优化）。实验证明，该方法能用少量 DPO 数据显著提升视频 LLM 在多个基准上的性能。\n\n44. **Token Dynamics：面向视频大语言模型的高效动态视频 Token 表示 (Token Dynamics: Towards Efficient and Dynamic Video Token Representation for Video Large Language Models)**\n    为解决视频 Token 表示中计算效率和信息保留的平衡问题，特别是在需要极度压缩的视频 LLM 场景下，本文提出 Token Dynamics 框架。通过解耦视觉嵌入和网格级运动信息，构建简洁的 Token 基座（聚类对象级内容）和 Token 动力学图（捕捉时空运动模式）。引入跨动力学注意力机制整合运动特征而不增加 Token 长度。实验表明，能将 Token 数量压缩至原始的 0.07%，性能仅轻微下降。\n\n45. **用于多模态表示学习的类概率生成建模 (Generative Modeling of Class Probability for Multi-Modal Representation Learning)**\n    为解决多模态学习中模态差异导致的对齐难题，本文提出 CALM 方法。利用类锚点作为提示，为每个模态生成并对齐类概率分布。引入跨模态概率变分自编码器建模对齐中的不确定性。实验证明该方法在基准数据集上显著优于 SOTA，尤其在域外评估中表现出更强的泛化能力。\n\n46. **分类器引导的 CLIP 蒸馏用于无监督多标签分类 (Classifier-guided CLIP Distillation for Unsupervised Multi-label Classification)**\n    针对无监督多标签分类中 CLIP 模型预测依赖视角且存在偏见的问题，本文提出分类器引导的 CLIP 蒸馏 (CCD)。利用分类器的类激活映射 (CAM) 指导选择目标附近的多个局部视图，并对来自 CLIP 预测的伪标签进行去偏。实验证明该方法优于现有技术。\n\n**基准与数据集**\n\n47. **CVE-Bench：评估 AI 智能体利用真实世界 Web 应用漏洞能力的基准 (CVE-Bench: A Benchmark for AI Agents' Ability to Exploit Real-World Web Application Vulnerabilities)**\n    为评估 LLM 智能体自主进行网络攻击的能力，本文提出 CVE-Bench。该基准基于高危 CVE 漏洞，设计了一个沙盒框架，让 LLM 智能体在模拟真实条件下利用易受攻击的 Web 应用，并有效评估其利用效果。初步评估显示，SOTA 智能体框架能解决高达 13% 的漏洞。\n\n48. **CausalRivers -- 扩展真实世界时间序列因果发现的基准测试 (CausalRivers -- Scaling up benchmarking of causal discovery for real-world time-series)**\n    为弥补因果发现方法在真实世界评估上的不足，本文推出 CausalRivers，迄今最大的真实世界时间序列因果发现基准套件。包含德国东部 (666站) 和巴伐利亚州 (494站) 2019-2023 年 15 分钟分辨率的河流流量数据，以及一次洪水事件数据。提供了两个因果真值图，可采样生成数千子图用于基准测试。\n\n49. **HCAST：人类校准的自主性软件任务 (HCAST: Human-Calibrated Autonomy Software Tasks)**\n    为理解和预测高度自主 AI 系统的社会影响，需要有现实基础的基准。本文提出 HCAST，包含 189 个机器学习工程、网络安全、软件工程和通用推理任务。收集了 563 个领域内熟练人类在相同条件下完成任务的基线数据（总计超 1500 小时），任务耗时从 1 分钟到 8+ 小时不等。评估发现，当前基于前沿基础模型的 AI 智能体在人类耗时 <1 小时的任务上成功率 70-80%，在 >4 小时的任务上成功率 <20%。\n\n50. **RustEvo^2：用于 LLM 生成 Rust 代码中 API 演进的演化基准 (RustEvo^2: An Evolving Benchmark for API Evolution in LLM-based Rust Code Generation)**\n    针对 LLM 难以生成适应快速演进语言（如 Rust）API 变化的代码的问题，本文提出 RustEvo 框架，用于构建动态基准。RustEvo 自动化地将 588 个 API 变化（来自标准库和第三方库）合成为编程任务，覆盖四种演进类别。实验揭示了 SOTA LLM 在处理不同类型 API 变化和跨越知识截止日期时的性能差异，并验证了 RAG 的缓解作用。\n\n51. **MTBench：用于时间推理和问答的多模态时间序列基准 (MTBench: A Multimodal Time Series Benchmark for Temporal Reasoning and Question Answering)**\n    为评估 LLM 在联合理解文本新闻和时间序列演化方面的能力，本文推出 MTBench。该大规模基准包含金融（新闻与股价）和天气（报告与温度记录）领域配对的时间序列和文本数据。MTBench 包含时间序列预测、趋势分析和新闻驱动问答等任务，旨在测试模型捕捉时间依赖、从文本提取洞见和融合跨模态信息的能力。对 SOTA LLM 的评估揭示了当前模型在这些方面的挑战。\n\n52. **CASTLE 2024 数据集：推进多模态理解技术 (The CASTLE 2024 Dataset: Advancing the Art of Multimodal Understanding)**\n    本文介绍了 CASTLE 2024 数据集，一个包含来自 15 个时间对齐源（10 个第一人称视角，5 个固定第三人称视角）的超高清 (UHD) 视频和音频，以及其他传感器流和辅助数据的多模态集合。数据集在固定地点由志愿者录制 4 天，总时长超 600 小时，且未进行任何部分审查（如人脸模糊或音频失真）。\n\n53. **DiTEC-WDN：跨多个供水管网的大规模水力情景数据集 (DiTEC-WDN: A Large-Scale Dataset of Hydraulic Scenarios across Multiple Water Distribution Networks)**\n    为解决供水管网 (WDN) 模型因隐私限制难以共享、阻碍数据驱动机器学习应用的问题，本文提出 DiTEC-WDN 数据集。包含 36,000 个独特场景（短期 24 小时或长期 1 年）的模拟数据，通过自动化流程生成和验证，共计 2.28 亿个基于图的状态。该数据集可支持图/节点/链接级别回归和时间序列预测等任务。\n\n**其他值得关注**\n\n*   **NdLinear：用于表示学习的新型线性变换 (NdLinear Is All You Need for Representation Learning)** 提出 NdLinear，一种保留多维数据结构（如图像、体积扫描）的线性变换，可作为标准全连接层的直接替代品，提升模型表达能力和参数效率。\n*   **神经引导的方程发现 (Neural-Guided Equation Discovery)** 综述并实验评估了使用神经引导蒙特卡洛树搜索 (MCTS) 进行方程发现的方法，比较了不同网络架构在嵌入表格数据上的效果，并探讨了监督学习与强化学习、动作空间选择等因素的影响。\n*   **LaMOuR：利用语言模型在强化学习中实现分布外恢复 (LaMOuR: Leveraging Language Models for Out-of-Distribution Recovery in Reinforcement Learning)** 提出 LaMOuR 框架，利用 LVLM 的图像描述、逻辑推理和代码生成能力，生成密集的奖励代码，引导智能体从 OOD 状态恢复到可以成功执行任务的状态，无需依赖不确定性估计。\n*   **TreeSynth：通过树引导子空间划分从头合成多样化数据 (TreeSynth: Synthesizing Diverse Data from Scratch via Tree-Guided Subspace Partitioning)** 提出 TreeSynth 框架，通过递归地将数据空间划分为层级子空间，并根据叶节点属性合成数据，从而生成覆盖整个数据空间的多样化数据集，解决 LLM 合成数据时多样性不足和分布偏差的问题。\n*   **D2Fusion：用于 Deepfake 检测的具有特征叠加的双域融合 (D2Fusion: Dual-domain Fusion with Feature Superposition for Deepfake Detection)** 提出 D2Fusion 方法，通过双向注意力模块（空间域）和细粒度频率注意力模块（频率域）分别捕捉局部位置信息和全局细微伪造信息，并采用特征叠加策略融合双域特征，提升 Deepfake 检测性能。\n*   **基于因果发现的数据驱动电动汽车充电站布局优化 (Data-Driven Optimization of EV Charging Station Placement Using Causal Discovery)** 利用因果发现技术（NOTEARS, DAGMA）分析大量充电数据，发现充电需求主要由靠近便利设施、电动汽车注册密度和邻近高流量路线决定，并基于此开发优化框架，指导充电站选址。\n*   **面向通用 AI 的稳健第三方缺陷披露 (In-House Evaluation Is Not Enough: Towards Robust Third-Party Flaw Disclosure for General-Purpose AI)** 呼吁建立更完善的通用目的 AI (GPAI) 缺陷报告基础设施和规范，包括标准化报告、明确研究规则、设立漏洞披露计划（类似 bug bounties）和法律安全港，以及改进协调机制，以提升 GPAI 系统的安全性、可靠性和问责制。\n*   **端到端音频语言模型的部署应考虑最小权限原则 (The Deployment of End-to-End Audio Language Models Should Take into Account the Principle of Least Privilege)** 探讨了端到端音频语言模型（直接处理语音而非转录文本）带来的新风险（如滥用说话人身份信息），呼吁在部署时遵循最小权限原则，评估端到端建模的必要性及信息访问范围，并指出当前基准测试和研究中的空白。\n\n---\n\n希望这份 TLDR 能帮助你快速了解 arXiv 的最新动态！",
  "papers": [
    {
      "arxiv_id": "2503.17577v1",
      "title": "Measuring the Robustness of Audio Deepfake Detectors",
      "title_zh": "测量音频深度伪造检测器的鲁棒性",
      "authors": [
        "Xiang Li",
        "Pin-Yu Chen",
        "Wenqi Wei"
      ],
      "abstract": "Deepfakes have become a universal and rapidly intensifying concern of\ngenerative AI across various media types such as images, audio, and videos.\nAmong these, audio deepfakes have been of particular concern due to the ease of\nhigh-quality voice synthesis and distribution via platforms such as social\nmedia and robocalls. Consequently, detecting audio deepfakes plays a critical\nrole in combating the growing misuse of AI-synthesized speech. However,\nreal-world scenarios often introduce various audio corruptions, such as noise,\nmodification, and compression, that may significantly impact detection\nperformance. This work systematically evaluates the robustness of 10 audio\ndeepfake detection models against 16 common corruptions, categorized into noise\nperturbation, audio modification, and compression. Using both traditional deep\nlearning models and state-of-the-art foundation models, we make four unique\nobservations. First, our findings show that while most models demonstrate\nstrong robustness to noise, they are notably more vulnerable to modifications\nand compression, especially when neural codecs are applied. Second, speech\nfoundation models generally outperform traditional models across most\nscenarios, likely due to their self-supervised learning paradigm and\nlarge-scale pre-training. Third, our results show that increasing model size\nimproves robustness, albeit with diminishing returns. Fourth, we demonstrate\nhow targeted data augmentation during training can enhance model resilience to\nunseen perturbations. A case study on political speech deepfakes highlights the\neffectiveness of foundation models in achieving high accuracy under real-world\nconditions. These findings emphasize the importance of developing more robust\ndetection frameworks to ensure reliability in practical deployment settings.",
      "tldr_zh": "该研究系统评估了10种音频Deepfake检测模型在16种常见干扰下的鲁棒性，包括噪声扰动、音频修改和压缩三大类。研究发现：1）语音基础模型（Foundation Models）凭借自监督学习和大规模预训练，在多数场景下优于传统模型；2）增大模型尺寸能提升鲁棒性但存在边际效应；3）针对性数据增强可有效提高模型对未知干扰的抵抗力。特别值得注意的是，虽然模型普遍对噪声具有较强抵抗力，但对神经编解码器（neural codecs）等压缩方式尤为敏感。通过政治演讲Deepfake案例验证，研究强调了开发更鲁棒检测框架对实际应用的重要性。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17577v1",
      "published_date": "2025-03-21 23:21:17 UTC",
      "updated_date": "2025-03-21 23:21:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:16:49.253815"
    },
    {
      "arxiv_id": "2503.17569v1",
      "title": "Fairness-Driven LLM-based Causal Discovery with Active Learning and Dynamic Scoring",
      "title_zh": "基于公平性驱动的LLM因果发现：结合主动学习与动态评分机制",
      "authors": [
        "Khadija Zanna",
        "Akane Sano"
      ],
      "abstract": "Causal discovery (CD) plays a pivotal role in numerous scientific fields by\nclarifying the causal relationships that underlie phenomena observed in diverse\ndisciplines. Despite significant advancements in CD algorithms that enhance\nbias and fairness analyses in machine learning, their application faces\nchallenges due to the high computational demands and complexities of\nlarge-scale data. This paper introduces a framework that leverages Large\nLanguage Models (LLMs) for CD, utilizing a metadata-based approach akin to the\nreasoning processes of human experts. By shifting from pairwise queries to a\nmore scalable breadth-first search (BFS) strategy, the number of required\nqueries is reduced from quadratic to linear in terms of variable count, thereby\naddressing scalability concerns inherent in previous approaches. This method\nutilizes an Active Learning (AL) and a Dynamic Scoring Mechanism that\nprioritizes queries based on their potential information gain, combining mutual\ninformation, partial correlation, and LLM confidence scores to refine the\ncausal graph more efficiently and accurately. This BFS query strategy reduces\nthe required number of queries significantly, thereby addressing scalability\nconcerns inherent in previous approaches. This study provides a more scalable\nand efficient solution for leveraging LLMs in fairness-driven CD, highlighting\nthe effects of the different parameters on performance. We perform fairness\nanalyses on the inferred causal graphs, identifying direct and indirect effects\nof sensitive attributes on outcomes. A comparison of these analyses against\nthose from graphs produced by baseline methods highlights the importance of\naccurate causal graph construction in understanding bias and ensuring fairness\nin machine learning systems.",
      "tldr_zh": "该研究提出了一种基于大语言模型(LLMs)的公平性驱动因果发现(Fairness-Driven Causal Discovery)框架，通过结合主动学习(Active Learning)和动态评分机制(Dynamic Scoring)，显著提升了因果发现的效率。该方法采用类似人类专家推理的元数据方法，将传统的成对查询优化为广度优先搜索(BFS)策略，使查询复杂度从二次降为线性，解决了大规模数据的可扩展性问题。实验表明，该框架不仅能更高效准确地构建因果图，还能通过敏感属性分析揭示机器学习系统中的直接/间接偏见影响，为公平性研究提供了新工具。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17569v1",
      "published_date": "2025-03-21 22:58:26 UTC",
      "updated_date": "2025-03-21 22:58:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:16:59.950983"
    },
    {
      "arxiv_id": "2503.17553v1",
      "title": "Autonomous Radiotherapy Treatment Planning Using DOLA: A Privacy-Preserving, LLM-Based Optimization Agent",
      "title_zh": "自主放疗计划系统DOLA：基于隐私保护大语言模型的优化智能体",
      "authors": [
        "Humza Nusrat",
        "Bing Luo",
        "Ryan Hall",
        "Joshua Kim",
        "Hassan Bagher-Ebadian",
        "Anthony Doemer",
        "Benjamin Movsas",
        "Kundan Thind"
      ],
      "abstract": "Radiotherapy treatment planning is a complex and time-intensive process,\noften impacted by inter-planner variability and subjective decision-making. To\naddress these challenges, we introduce Dose Optimization Language Agent (DOLA),\nan autonomous large language model (LLM)-based agent designed for optimizing\nradiotherapy treatment plans while rigorously protecting patient privacy. DOLA\nintegrates the LLaMa3.1 LLM directly with a commercial treatment planning\nsystem, utilizing chain-of-thought prompting, retrieval-augmented generation\n(RAG), and reinforcement learning (RL). Operating entirely within secure local\ninfrastructure, this agent eliminates external data sharing. We evaluated DOLA\nusing a retrospective cohort of 18 prostate cancer patients prescribed 60 Gy in\n20 fractions, comparing model sizes (8 billion vs. 70 billion parameters) and\noptimization strategies (No-RAG, RAG, and RAG+RL) over 10 planning iterations.\nThe 70B model demonstrated significantly improved performance, achieving\napproximately 16.4% higher final scores than the 8B model. The RAG approach\noutperformed the No-RAG baseline by 19.8%, and incorporating RL accelerated\nconvergence, highlighting the synergy of retrieval-based memory and\nreinforcement learning. Optimal temperature hyperparameter analysis identified\n0.4 as providing the best balance between exploration and exploitation. This\nproof of concept study represents the first successful deployment of locally\nhosted LLM agents for autonomous optimization of treatment plans within a\ncommercial radiotherapy planning system. By extending human-machine interaction\nthrough interpretable natural language reasoning, DOLA offers a scalable and\nprivacy-conscious framework, with significant potential for clinical\nimplementation and workflow improvement.",
      "tldr_zh": "该研究提出了DOLA（Dose Optimization Language Agent），一种基于大语言模型（LLM）的自主优化系统，用于放疗治疗计划的隐私保护式自动化设计。DOLA通过整合LLaMa3.1模型与商业治疗规划系统，采用思维链提示（chain-of-thought）、检索增强生成（RAG）和强化学习（RL）技术，在确保患者数据不外泄的本地环境中运行。实验表明，70B参数模型比8B模型性能提升16.4%，RAG+RL组合方案比基线效果提高19.8%，其中温度参数0.4能最佳平衡探索与利用。该系统首次实现了在商业放疗系统中本地部署LLM进行自主优化，为临床工作流程改进提供了可扩展且可解释的自然语言交互框架。",
      "categories": [
        "physics.med-ph",
        "cs.AI",
        "cs.CL",
        "cs.ET",
        "cs.HC"
      ],
      "primary_category": "physics.med-ph",
      "comment": "19 pages, 5 figures, preprint",
      "pdf_url": "http://arxiv.org/pdf/2503.17553v1",
      "published_date": "2025-03-21 22:01:19 UTC",
      "updated_date": "2025-03-21 22:01:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:16:01.546220"
    },
    {
      "arxiv_id": "2503.17551v1",
      "title": "Audio-Enhanced Vision-Language Modeling with Latent Space Broadening for High Quality Data Expansion",
      "title_zh": "音频增强的视觉-语言建模：通过潜在空间扩展实现高质量数据扩充",
      "authors": [
        "Yu Sun",
        "Yin Li",
        "Ruixiao Sun",
        "Chunhui Liu",
        "Fangming Zhou",
        "Ze Jin",
        "Linjie Wang",
        "Xiang Shen",
        "Zhuolin Hao",
        "Hongyu Xiong"
      ],
      "abstract": "Transformer-based multimodal models are widely used in industrial-scale\nrecommendation, search, and advertising systems for content understanding and\nrelevance ranking. Enhancing labeled training data quality and cross-modal\nfusion significantly improves model performance, influencing key metrics such\nas quality view rates and ad revenue. High-quality annotations are crucial for\nadvancing content modeling, yet traditional statistical-based active learning\n(AL) methods face limitations: they struggle to detect overconfident\nmisclassifications and are less effective in distinguishing semantically\nsimilar items in deep neural networks. Additionally, audio information plays an\nincreasing role, especially in short-video platforms, yet most pre-trained\nmultimodal architectures primarily focus on text and images. While training\nfrom scratch across all three modalities is possible, it sacrifices the\nbenefits of leveraging existing pre-trained visual-language (VL) and audio\nmodels. To address these challenges, we propose kNN-based Latent Space\nBroadening (LSB) to enhance AL efficiency and Vision-Language Modeling with\nAudio Enhancement (VLMAE), a mid-fusion approach integrating audio into VL\nmodels. This system deployed in production systems, leading to significant\nbusiness gains.",
      "tldr_zh": "本文提出两种创新方法提升多模态模型性能：kNN潜在空间扩展技术(LSB)改进了主动学习(AL)效率，能有效识别过自信误分类；音频增强的视觉语言模型(VLMAE)通过中层融合策略将音频信息整合到预训练的视觉语言模型中，避免从头训练三模态的代价。该方案在工业级推荐系统中实现部署，显著提升了内容理解质量和商业指标（如优质观看率）。研究特别解决了传统AL方法在深度神经网络中难以区分语义相似项的缺陷，以及现有预训练模型忽视音频模态的问题。",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.CV",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.MM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17551v1",
      "published_date": "2025-03-21 21:55:05 UTC",
      "updated_date": "2025-03-21 21:55:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:16:11.767137"
    },
    {
      "arxiv_id": "2503.17547v1",
      "title": "Learning Multi-Level Features with Matryoshka Sparse Autoencoders",
      "title_zh": "《学习多层次特征：套娃式稀疏自编码器》",
      "authors": [
        "Bart Bussmann",
        "Noa Nabeshima",
        "Adam Karvonen",
        "Neel Nanda"
      ],
      "abstract": "Sparse autoencoders (SAEs) have emerged as a powerful tool for interpreting\nneural networks by extracting the concepts represented in their activations.\nHowever, choosing the size of the SAE dictionary (i.e. number of learned\nconcepts) creates a tension: as dictionary size increases to capture more\nrelevant concepts, sparsity incentivizes features to be split or absorbed into\nmore specific features, leaving high-level features missing or warped. We\nintroduce Matryoshka SAEs, a novel variant that addresses these issues by\nsimultaneously training multiple nested dictionaries of increasing size,\nforcing the smaller dictionaries to independently reconstruct the inputs\nwithout using the larger dictionaries. This organizes features hierarchically -\nthe smaller dictionaries learn general concepts, while the larger dictionaries\nlearn more specific concepts, without incentive to absorb the high-level\nfeatures. We train Matryoshka SAEs on Gemma-2-2B and TinyStories and find\nsuperior performance on sparse probing and targeted concept erasure tasks, more\ndisentangled concept representations, and reduced feature absorption. While\nthere is a minor tradeoff with reconstruction performance, we believe\nMatryoshka SAEs are a superior alternative for practical tasks, as they enable\ntraining arbitrarily large SAEs while retaining interpretable features at\ndifferent levels of abstraction.",
      "tldr_zh": "本文提出了一种新型的Matryoshka稀疏自编码器(SAEs)，通过同时训练多个嵌套的、逐渐增大的字典，解决了传统SAE在高层次特征提取中的问题。该方法强制较小的字典独立重建输入，从而实现了特征的层次化组织：较小的字典学习通用概念，而较大的字典学习更具体的概念，避免了高层次特征被吸收或扭曲。实验表明，Matryoshka SAEs在稀疏探测和特定概念擦除任务中表现优异，特征表示更加解耦，且减少了特征吸收现象。尽管在重建性能上略有损失，但该方法为训练任意大规模SAE提供了更优的解决方案，同时保留了不同抽象层次的可解释特征。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17547v1",
      "published_date": "2025-03-21 21:43:28 UTC",
      "updated_date": "2025-03-21 21:43:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:16:33.852171"
    },
    {
      "arxiv_id": "2503.17544v1",
      "title": "PRIMAL: Physically Reactive and Interactive Motor Model for Avatar Learning",
      "title_zh": "PRIMAL：面向虚拟化身学习的物理反应交互式运动模型",
      "authors": [
        "Yan Zhang",
        "Yao Feng",
        "Alpár Cseke",
        "Nitin Saini",
        "Nathan Bajandas",
        "Nicolas Heron",
        "Michael J. Black"
      ],
      "abstract": "To build a motor system of the interactive avatar, it is essential to develop\na generative motion model drives the body to move through 3D space in a\nperpetual, realistic, controllable, and responsive manner. Although motion\ngeneration has been extensively studied, most methods do not support ``embodied\nintelligence'' due to their offline setting, slow speed, limited motion\nlengths, or unnatural movements. To overcome these limitations, we propose\nPRIMAL, an autoregressive diffusion model that is learned with a two-stage\nparadigm, inspired by recent advances in foundation models. In the pretraining\nstage, the model learns motion dynamics from a large number of sub-second\nmotion segments, providing ``motor primitives'' from which more complex motions\nare built. In the adaptation phase, we employ a ControlNet-like adaptor to\nfine-tune the motor control for semantic action generation and spatial target\nreaching. Experiments show that physics effects emerge from our training. Given\na single-frame initial state, our model not only generates unbounded,\nrealistic, and controllable motion, but also enables the avatar to be\nresponsive to induced impulses in real time. In addition, we can effectively\nand efficiently adapt our base model to few-shot personalized actions and the\ntask of spatial control. Evaluations show that our proposed method outperforms\nstate-of-the-art baselines. We leverage the model to create a real-time\ncharacter animation system in Unreal Engine that is highly responsive and\nnatural. Code, models, and more results are available at:\nhttps://yz-cnsdqz.github.io/eigenmotion/PRIMAL",
      "tldr_zh": "该研究提出了PRIMAL，一种基于自回归扩散模型的两阶段训练范式，用于构建具有物理反应和交互能力的虚拟角色运动系统。在预训练阶段，模型从大量短时运动片段中学习运动动力学，形成“运动基元”；在适应阶段，采用ControlNet式适配器微调运动控制，实现语义动作生成和空间目标到达。实验表明，该模型不仅能生成无界、逼真且可控的运动，还能实时响应外部冲击，并可高效适应少量个性化动作和空间控制任务，性能优于现有方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "18 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.17544v1",
      "published_date": "2025-03-21 21:27:57 UTC",
      "updated_date": "2025-03-21 21:27:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:17:19.807082"
    },
    {
      "arxiv_id": "2503.17523v1",
      "title": "Bayesian Teaching Enables Probabilistic Reasoning in Large Language Models",
      "title_zh": "贝叶斯教学赋能大语言模型概率推理",
      "authors": [
        "Linlu Qiu",
        "Fei Sha",
        "Kelsey Allen",
        "Yoon Kim",
        "Tal Linzen",
        "Sjoerd van Steenkiste"
      ],
      "abstract": "Artificial intelligence systems based on large language models (LLMs) are\nincreasingly used as agents that interact with users and with the world. To do\nso successfully, LLMs need to construct internal representations of the world\nand form probabilistic beliefs about those representations. To provide a user\nwith personalized recommendations, for example, the LLM needs to gradually\ninfer the user's preferences, over the course of multiple interactions. To\nevaluate whether contemporary LLMs are able to do so, we use the Bayesian\ninference framework from probability theory, which lays out the optimal way to\nupdate an agent's beliefs as it receives new information. We first show that\nthe LLMs do not update their beliefs as expected from the Bayesian framework,\nand that consequently their predictions do not improve as expected as more\ninformation becomes available, even less so than we find is the case for\nhumans. To address this issue, we teach the LLMs to reason in a Bayesian manner\nby training them to mimic the predictions of an optimal Bayesian model. We find\nthat this approach not only significantly improves the LLM's performance on the\nparticular recommendation task it is trained on, but also enables\ngeneralization to other tasks. This suggests that this method endows the LLM\nwith broader Bayesian reasoning skills. More generally, our results indicate\nthat LLMs can learn about reasoning strategies effectively and generalize those\nskills to new domains, which in part explains LLMs' empirical success.",
      "tldr_zh": "该研究探讨了大语言模型(LLMs)在概率推理方面的不足，发现现有模型无法像贝叶斯框架(Bayesian framework)预期那样根据新信息更新信念。研究者通过贝叶斯教学法(Bayesian Teaching)训练LLMs模仿最优贝叶斯模型的预测方式，不仅显著提升了模型在推荐任务中的表现，还使其获得了可泛化到其他任务的贝叶斯推理能力。这一发现表明LLMs能够有效学习推理策略并将其迁移到新领域，部分解释了其实际应用中的成功表现。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17523v1",
      "published_date": "2025-03-21 20:13:04 UTC",
      "updated_date": "2025-03-21 20:13:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:17:39.359945"
    },
    {
      "arxiv_id": "2503.17515v1",
      "title": "A Predictive Services Architecture for Efficient Airspace Operations",
      "title_zh": "高效空域运营的预测服务架构",
      "authors": [
        "Ítalo Romani de Oliveira",
        "Samet Ayhan",
        "Glaucia Balvedi",
        "Michael Biglin",
        "Pablo Costas",
        "Euclides C. Pinto Neto",
        "Alexandre Leite",
        "Felipe C. F. de Azevedo"
      ],
      "abstract": "Predicting air traffic congestion and flow management is essential for\nairlines and Air Navigation Service Providers (ANSP) to enhance operational\nefficiency. Accurate estimates of future airport capacity and airspace density\nare vital for better airspace management, reducing air traffic controller\nworkload and fuel consumption, ultimately promoting sustainable aviation. While\nexisting literature has addressed these challenges, data management and query\nprocessing remain complex due to the vast volume of high-rate air traffic data.\nMany analytics use cases require a common pre-processing infrastructure, as\nad-hoc approaches are insufficient. Additionally, linear prediction models\noften fall short, necessitating more advanced techniques.\n  This paper presents a data processing and predictive services architecture\nthat ingests large, uncorrelated, and noisy streaming data to forecast future\nairspace system states. The system continuously collects raw data, periodically\ncompresses it, and stores it in NoSQL databases for efficient query processing.\nFor prediction, the system learns from historical traffic by extracting key\nfeatures such as airport arrival and departure events, sector boundary\ncrossings, weather parameters, and other air traffic data. These features are\ninput into various regression models, including linear, non-linear, and\nensemble models, with the best-performing model selected for predictions. We\nevaluate this infrastructure across three prediction use cases in the US\nNational Airspace System (NAS) and a segment of European airspace, using\nextensive real operations data, confirming that our system can predict future\nsystem states efficiently and accurately.",
      "tldr_zh": "本文提出了一种面向空域运行效率的预测服务架构，用于处理海量、异构的实时航空数据流。该系统通过NoSQL数据库存储压缩后的原始数据，并提取机场起降事件、空域扇区穿越、气象参数等关键特征，采用线性/非线性回归和集成模型进行空域状态预测。经美国国家空域系统(NAS)和欧洲部分空域的实际运行数据验证，该架构能高效准确地预测未来空域系统状态，有助于降低管制员工作负荷和航空燃油消耗，推动可持续航空发展。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DB",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17515v1",
      "published_date": "2025-03-21 19:57:38 UTC",
      "updated_date": "2025-03-21 19:57:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:19:53.861893"
    },
    {
      "arxiv_id": "2503.17514v2",
      "title": "Language Models May Verbatim Complete Text They Were Not Explicitly Trained On",
      "title_zh": "语言模型可能逐字完成其未经明确训练过的文本",
      "authors": [
        "Ken Ziyu Liu",
        "Christopher A. Choquette-Choo",
        "Matthew Jagielski",
        "Peter Kairouz",
        "Sanmi Koyejo",
        "Percy Liang",
        "Nicolas Papernot"
      ],
      "abstract": "An important question today is whether a given text was used to train a large\nlanguage model (LLM). A \\emph{completion} test is often employed: check if the\nLLM completes a sufficiently complex text. This, however, requires a\nground-truth definition of membership; most commonly, it is defined as a member\nbased on the $n$-gram overlap between the target text and any text in the\ndataset. In this work, we demonstrate that this $n$-gram based membership\ndefinition can be effectively gamed. We study scenarios where sequences are\n\\emph{non-members} for a given $n$ and we find that completion tests still\nsucceed. We find many natural cases of this phenomenon by retraining LLMs from\nscratch after removing all training samples that were completed; these cases\ninclude exact duplicates, near-duplicates, and even short overlaps. They\nshowcase that it is difficult to find a single viable choice of $n$ for\nmembership definitions. Using these insights, we design adversarial datasets\nthat can cause a given target sequence to be completed without containing it,\nfor any reasonable choice of $n$. Our findings highlight the inadequacy of\n$n$-gram membership, suggesting membership definitions fail to account for\nauxiliary information available to the training algorithm.",
      "tldr_zh": "这项研究揭示了大型语言模型(LLM)可能完成未经明确训练的文本内容的问题。研究发现，基于n-gram重叠的传统成员资格测试存在漏洞：即使某些序列不符合n-gram成员资格定义(包括精确重复、近似重复和短重叠等情况)，模型仍能完成这些文本。通过重新训练LLM的实验，研究人员证实难以找到合适的n值来准确定义成员资格，并设计出对抗性数据集证明这一缺陷。这些发现表明n-gram成员资格测试方法存在根本不足，未能考虑训练算法可用的辅助信息。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Main text: 9 pages, 7 figures, 1 table. Appendix: 29 pages, 20\n  tables, 15 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.17514v2",
      "published_date": "2025-03-21 19:57:04 UTC",
      "updated_date": "2025-03-25 04:43:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:19:16.777054"
    },
    {
      "arxiv_id": "2503.17513v1",
      "title": "Improving Quantization with Post-Training Model Expansion",
      "title_zh": "提升量化效果的后训练模型扩展方法",
      "authors": [
        "Giuseppe Franco",
        "Pablo Monteagudo-Lago",
        "Ian Colbert",
        "Nicholas Fraser",
        "Michaela Blott"
      ],
      "abstract": "The size of a model has been a strong predictor of its quality, as well as\nits cost. As such, the trade-off between model cost and quality has been\nwell-studied. Post-training optimizations like quantization and pruning have\ntypically focused on reducing the overall volume of pre-trained models to\nreduce inference costs while maintaining model quality. However, recent\nadvancements have introduced optimization techniques that, interestingly,\nexpand models post-training, increasing model size to improve quality when\nreducing volume. For instance, to enable 4-bit weight and activation\nquantization, incoherence processing often necessitates inserting online\nHadamard rotations in the compute graph, and preserving highly sensitive\nweights often calls for additional higher precision computations. However, if\napplication requirements cannot be met, the prevailing solution is to relax\nquantization constraints. In contrast, we demonstrate post-training model\nexpansion is a viable strategy to improve model quality within a quantization\nco-design space, and provide theoretical justification. We show it is possible\nto progressively and selectively expand the size of a pre-trained large\nlanguage model (LLM) to improve model quality without end-to-end retraining. In\nparticular, when quantizing the weights and activations to 4 bits for Llama3\n1B, we reduce the zero-shot accuracy gap to full precision by an average of 3%\nrelative to both QuaRot and SpinQuant with only 5% more parameters, which is\nstill a 3.8% reduction in volume relative to a BF16 reference model.",
      "tldr_zh": "这篇论文提出了一种创新的后训练模型扩展方法，用于提升量化模型的性能。与传统通过降低量化约束来改善模型质量不同，该方法选择性地扩展预训练大语言模型(LLM)的规模，如增加在线Hadamard旋转操作和更高精度计算模块。实验证明，在将Llama3 1B模型的权重和激活量化为4位时，该方法仅增加5%参数量就能将零样本准确率与全精度模型的差距缩小3%，同时相比BF16基准模型仍减少3.8%的总体积。研究为量化协同设计空间提供了理论依据和实践方案，展现了后训练扩展策略的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.AR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17513v1",
      "published_date": "2025-03-21 19:56:59 UTC",
      "updated_date": "2025-03-21 19:56:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:19:55.535158"
    },
    {
      "arxiv_id": "2503.17509v1",
      "title": "Follow-up Question Generation For Enhanced Patient-Provider Conversations",
      "title_zh": "《增强医患对话的后续问题生成》",
      "authors": [
        "Joseph Gatto",
        "Parker Seegmiller",
        "Timothy Burdick",
        "Inas S. Khayal",
        "Sarah DeLozier",
        "Sarah M. Preum"
      ],
      "abstract": "Follow-up question generation is an essential feature of dialogue systems as\nit can reduce conversational ambiguity and enhance modeling complex\ninteractions. Conversational contexts often pose core NLP challenges such as\n(i) extracting relevant information buried in fragmented data sources, and (ii)\nmodeling parallel thought processes. These two challenges occur frequently in\nmedical dialogue as a doctor asks questions based not only on patient\nutterances but also their prior EHR data and current diagnostic hypotheses.\nAsking medical questions in asynchronous conversations compounds these issues\nas doctors can only rely on static EHR information to motivate follow-up\nquestions.\n  To address these challenges, we introduce FollowupQ, a novel framework for\nenhancing asynchronous medical conversation. FollowupQ is a multi-agent\nframework that processes patient messages and EHR data to generate personalized\nfollow-up questions, clarifying patient-reported medical conditions. FollowupQ\nreduces requisite provider follow-up communications by 34%. It also improves\nperformance by 17% and 5% on real and synthetic data, respectively. We also\nrelease the first public dataset of asynchronous medical messages with linked\nEHR data alongside 2,300 follow-up questions written by clinical experts for\nthe wider NLP research community.",
      "tldr_zh": "该研究提出了FollowupQ框架，用于增强异步医疗对话中的后续问题生成。该框架通过多智能体系统处理患者消息和电子健康记录(EHR)数据，生成个性化的后续问题，以澄清患者报告的医疗状况。实验表明，FollowupQ将医疗提供者所需的后续沟通减少了34%，并在真实和合成数据上分别提升了17%和5%的性能。此外，研究团队还发布了首个包含EHR数据和2300个临床专家编写的后续问题的异步医疗消息公开数据集，为NLP研究社区提供了宝贵资源。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "17 Pages, 7 Figures, 6 Tables",
      "pdf_url": "http://arxiv.org/pdf/2503.17509v1",
      "published_date": "2025-03-21 19:40:53 UTC",
      "updated_date": "2025-03-21 19:40:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:20:20.237257"
    },
    {
      "arxiv_id": "2503.17502v1",
      "title": "Large Language Models (LLMs) for Source Code Analysis: applications, models and datasets",
      "title_zh": "大语言模型（LLMs）在源代码分析中的应用：应用场景、模型与数据集",
      "authors": [
        "Hamed Jelodar",
        "Mohammad Meymani",
        "Roozbeh Razavi-Far"
      ],
      "abstract": "Large language models (LLMs) and transformer-based architectures are\nincreasingly utilized for source code analysis. As software systems grow in\ncomplexity, integrating LLMs into code analysis workflows becomes essential for\nenhancing efficiency, accuracy, and automation. This paper explores the role of\nLLMs for different code analysis tasks, focusing on three key aspects: 1) what\nthey can analyze and their applications, 2) what models are used and 3) what\ndatasets are used, and the challenges they face. Regarding the goal of this\nresearch, we investigate scholarly articles that explore the use of LLMs for\nsource code analysis to uncover research developments, current trends, and the\nintellectual structure of this emerging field. Additionally, we summarize\nlimitations and highlight essential tools, datasets, and key challenges, which\ncould be valuable for future work.",
      "tldr_zh": "这篇综述论文探讨了大语言模型(LLMs)在源代码分析中的应用现状和发展趋势。研究系统梳理了三个核心维度：LLM在代码分析中的具体应用场景、常用模型架构以及相关数据集。通过分析现有文献，论文揭示了该领域的研究进展和知识结构，同时总结了当前面临的主要挑战和限制。研究成果为未来基于LLM的代码分析研究提供了关键工具、数据集和发展方向的参考框架。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17502v1",
      "published_date": "2025-03-21 19:29:50 UTC",
      "updated_date": "2025-03-21 19:29:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:20:38.846678"
    },
    {
      "arxiv_id": "2503.17494v1",
      "title": "Efficient Knowledge Distillation via Curriculum Extraction",
      "title_zh": "基于课程提取的高效知识蒸馏",
      "authors": [
        "Shivam Gupta",
        "Sushrut Karmalkar"
      ],
      "abstract": "Knowledge distillation is a technique used to train a small student network\nusing the output generated by a large teacher network, and has many empirical\nadvantages~\\citep{Hinton2015DistillingTK}. While the standard one-shot approach\nto distillation only uses the output of the final teacher network, recent\nwork~\\citep{panigrahi2024progressive} has shown that using intermediate\ncheckpoints from the teacher's training process as an implicit ``curriculum''\nfor progressive distillation can significantly speed up training. However, such\nschemes require storing these checkpoints, and often require careful selection\nof the intermediate checkpoints to train on, which can be impractical for\nlarge-scale training.\n  In this paper, we show that a curriculum can be \\emph{extracted} from just\nthe fully trained teacher network, and that this extracted curriculum can give\nsimilar efficiency benefits to those of progressive distillation. Our\nextraction scheme is natural; we use a random projection of the hidden\nrepresentations of the teacher network to progressively train the student\nnetwork, before training using the output of the full network. We show that our\nscheme significantly outperforms one-shot distillation and achieves a\nperformance similar to that of progressive distillation for learning sparse\nparities with two-layer networks, and provide theoretical guarantees for this\nsetting. Additionally, we show that our method outperforms one-shot\ndistillation even when using transformer-based architectures, both for\nsparse-parity learning, and language modeling tasks.",
      "tldr_zh": "该论文提出了一种新型的课程提取式知识蒸馏方法，通过仅利用训练完成的教师网络隐层表征的随机投影来构建渐进式学习课程，避免了传统渐进蒸馏需要存储中间检查点的弊端。理论分析和实验表明，该方法在两层网络稀疏奇偶校验任务中显著优于单次蒸馏，效果接近渐进蒸馏；在Transformer架构的稀疏奇偶校验学习和语言建模任务中同样优于基准方法。这种自然提取课程的方式为大规模训练提供了更高效的蒸馏方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.ST",
        "stat.ML",
        "stat.TH"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17494v1",
      "published_date": "2025-03-21 19:09:41 UTC",
      "updated_date": "2025-03-21 19:09:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:21:21.977526"
    },
    {
      "arxiv_id": "2503.17486v2",
      "title": "ProtoGS: Efficient and High-Quality Rendering with 3D Gaussian Prototypes",
      "title_zh": "ProtoGS：基于3D高斯原型的高效高质量渲染",
      "authors": [
        "Zhengqing Gao",
        "Dongting Hu",
        "Jia-Wang Bian",
        "Huan Fu",
        "Yan Li",
        "Tongliang Liu",
        "Mingming Gong",
        "Kun Zhang"
      ],
      "abstract": "3D Gaussian Splatting (3DGS) has made significant strides in novel view\nsynthesis but is limited by the substantial number of Gaussian primitives\nrequired, posing challenges for deployment on lightweight devices. Recent\nmethods address this issue by compressing the storage size of densified\nGaussians, yet fail to preserve rendering quality and efficiency. To overcome\nthese limitations, we propose ProtoGS to learn Gaussian prototypes to represent\nGaussian primitives, significantly reducing the total Gaussian amount without\nsacrificing visual quality. Our method directly uses Gaussian prototypes to\nenable efficient rendering and leverage the resulting reconstruction loss to\nguide prototype learning. To further optimize memory efficiency during\ntraining, we incorporate structure-from-motion (SfM) points as anchor points to\ngroup Gaussian primitives. Gaussian prototypes are derived within each group by\nclustering of K-means, and both the anchor points and the prototypes are\noptimized jointly. Our experiments on real-world and synthetic datasets prove\nthat we outperform existing methods, achieving a substantial reduction in the\nnumber of Gaussians, and enabling high rendering speed while maintaining or\neven enhancing rendering fidelity.",
      "tldr_zh": "该研究提出ProtoGS方法，通过3D高斯原型(Gaussian prototypes)来高效表征3D高斯泼溅(3DGS)中的大量基元，显著减少存储需求的同时保持渲染质量。该方法利用运动恢复结构(SfM)点作为锚点进行高斯基元分组，采用K-means聚类学习原型，并联合优化锚点与原型参数。实验表明，ProtoGS在真实和合成数据集上均优于现有方法，既能大幅减少高斯数量，又能保持甚至提升渲染速度与保真度。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17486v2",
      "published_date": "2025-03-21 18:55:14 UTC",
      "updated_date": "2025-03-25 13:03:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:21:18.553351"
    },
    {
      "arxiv_id": "2503.17485v1",
      "title": "SaudiCulture: A Benchmark for Evaluating Large Language Models Cultural Competence within Saudi Arabia",
      "title_zh": "SaudiCulture：评估大语言模型在沙特阿拉伯文化适应能力的基准测试",
      "authors": [
        "Lama Ayash",
        "Hassan Alhuzali",
        "Ashwag Alasmari",
        "Sultan Aloufi"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nnatural language processing; however, they often struggle to accurately capture\nand reflect cultural nuances. This research addresses this challenge by\nfocusing on Saudi Arabia, a country characterized by diverse dialects and rich\ncultural traditions. We introduce SaudiCulture, a novel benchmark designed to\nevaluate the cultural competence of LLMs within the distinct geographical and\ncultural contexts of Saudi Arabia. SaudiCulture is a comprehensive dataset of\nquestions covering five major geographical regions, such as West, East, South,\nNorth, and Center, along with general questions applicable across all regions.\nThe dataset encompasses a broad spectrum of cultural domains, including food,\nclothing, entertainment, celebrations, and crafts. To ensure a rigorous\nevaluation, SaudiCulture includes questions of varying complexity, such as\nopen-ended, single-choice, and multiple-choice formats, with some requiring\nmultiple correct answers. Additionally, the dataset distinguishes between\ncommon cultural knowledge and specialized regional aspects. We conduct\nextensive evaluations on five LLMs, such as GPT-4, Llama 3.3, FANAR, Jais, and\nAceGPT, analyzing their performance across different question types and\ncultural contexts. Our findings reveal that all models experience significant\nperformance declines when faced with highly specialized or region-specific\nquestions, particularly those requiring multiple correct responses.\nAdditionally, certain cultural categories are more easily identifiable than\nothers, further highlighting inconsistencies in LLMs cultural understanding.\nThese results emphasize the importance of incorporating region-specific\nknowledge into LLMs training to enhance their cultural competence.",
      "tldr_zh": "该研究提出了SaudiCulture基准测试，专门用于评估大语言模型(LLMs)在沙特阿拉伯文化背景下的表现。该数据集涵盖沙特五大地理区域的饮食、服饰、节庆等文化领域，包含开放式、单选和多选等不同难度的问题类型。通过对GPT-4、Llama 3.3等五种主流LLMs的测试发现，模型在面对需要多重正确答案或区域特色问题时表现显著下降，突显了当前LLMs在文化理解上的局限性。研究强调了将地区特定知识纳入模型训练以提升文化适应能力的重要性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "34 pages, under-review",
      "pdf_url": "http://arxiv.org/pdf/2503.17485v1",
      "published_date": "2025-03-21 18:55:10 UTC",
      "updated_date": "2025-03-21 18:55:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:21:44.035554"
    },
    {
      "arxiv_id": "2503.17482v1",
      "title": "What's Producible May Not Be Reachable: Measuring the Steerability of Generative Models",
      "title_zh": "可生成未必可达：衡量生成模型的可操控性",
      "authors": [
        "Keyon Vafa",
        "Sarah Bentley",
        "Jon Kleinberg",
        "Sendhil Mullainathan"
      ],
      "abstract": "How should we evaluate the quality of generative models? Many existing\nmetrics focus on a model's producibility, i.e. the quality and breadth of\noutputs it can generate. However, the actual value from using a generative\nmodel stems not just from what it can produce but whether a user with a\nspecific goal can produce an output that satisfies that goal. We refer to this\nproperty as steerability. In this paper, we first introduce a mathematical\nframework for evaluating steerability independently from producibility.\nSteerability is more challenging to evaluate than producibility because it\nrequires knowing a user's goals. We address this issue by creating a benchmark\ntask that relies on one key idea: sample an output from a generative model and\nask users to reproduce it. We implement this benchmark in a large-scale user\nstudy of text-to-image models and large language models. Despite the ability of\nthese models to produce high-quality outputs, they all perform poorly on\nsteerabilty. This suggests that we need to focus on improving the steerability\nof generative models. We show such improvements are indeed possible: through\nreinforcement learning techniques, we create an alternative steering mechanism\nfor image models that achieves more than 2x improvement on this benchmark.",
      "tldr_zh": "该论文提出了生成模型\"可操控性\"(steerability)的新评估维度，区别于传统仅关注输出质量与多样性的\"可生产性\"(producibility)。研究者通过设计新颖的基准测试（要求用户复现模型生成的输出），在大规模用户研究中发现主流文本-图像模型和大语言模型的可操控性普遍较差。实验表明，通过强化学习技术改进的操控机制可使图像模型的操控性能提升2倍以上，揭示了提升生成模型实用性的新方向。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.HC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17482v1",
      "published_date": "2025-03-21 18:51:56 UTC",
      "updated_date": "2025-03-21 18:51:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:22:01.146878"
    },
    {
      "arxiv_id": "2503.17479v1",
      "title": "Your voice is your voice: Supporting Self-expression through Speech Generation and LLMs in Augmented and Alternative Communication",
      "title_zh": "你的声音就是你的声音：通过语音生成和大型语言模型支持增强与替代沟通中的自我表达",
      "authors": [
        "Yiwen Xu",
        "Monideep Chakraborti",
        "Tianyi Zhang",
        "Katelyn Eng",
        "Aanchan Mohan",
        "Mirjana Prpa"
      ],
      "abstract": "In this paper, we present Speak Ease: an augmentative and alternative\ncommunication (AAC) system to support users' expressivity by integrating\nmultimodal input, including text, voice, and contextual cues (conversational\npartner and emotional tone), with large language models (LLMs). Speak Ease\ncombines automatic speech recognition (ASR), context-aware LLM-based outputs,\nand personalized text-to-speech technologies to enable more personalized,\nnatural-sounding, and expressive communication. Through an exploratory\nfeasibility study and focus group evaluation with speech and language\npathologists (SLPs), we assessed Speak Ease's potential to enable expressivity\nin AAC. The findings highlight the priorities and needs of AAC users and the\nsystem's ability to enhance user expressivity by supporting more personalized\nand contextually relevant communication. This work provides insights into the\nuse of multimodal inputs and LLM-driven features to improve AAC systems and\nsupport expressivity.",
      "tldr_zh": "这篇论文介绍了Speak Ease系统，一种结合多模态输入（文本、语音和上下文线索）与大语言模型(LLM)的增强与替代沟通(AAC)方案。该系统整合自动语音识别(ASR)、情境感知LLM输出和个性化文本转语音技术，能生成更自然、个性化的表达。通过与言语病理学家(SLPs)的可行性研究证实，该系统能有效提升AAC用户的表达力，满足个性化沟通需求。研究为利用多模态输入和LLM技术改进AAC系统提供了重要见解。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17479v1",
      "published_date": "2025-03-21 18:50:05 UTC",
      "updated_date": "2025-03-21 18:50:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:22:23.443835"
    },
    {
      "arxiv_id": "2503.17475v1",
      "title": "Spatiotemporal Learning with Context-aware Video Tubelets for Ultrasound Video Analysis",
      "title_zh": "时空学习：基于上下文感知视频片段块的超声视频分析",
      "authors": [
        "Gary Y. Li",
        "Li Chen",
        "Bryson Hicks",
        "Nikolai Schnittke",
        "David O. Kessler",
        "Jeffrey Shupp",
        "Maria Parker",
        "Cristiana Baloescu",
        "Christopher Moore",
        "Cynthia Gregory",
        "Kenton Gregory",
        "Balasundar Raju",
        "Jochen Kruecker",
        "Alvin Chen"
      ],
      "abstract": "Computer-aided pathology detection algorithms for video-based imaging\nmodalities must accurately interpret complex spatiotemporal information by\nintegrating findings across multiple frames. Current state-of-the-art methods\noperate by classifying on video sub-volumes (tubelets), but they often lose\nglobal spatial context by focusing only on local regions within detection ROIs.\nHere we propose a lightweight framework for tubelet-based object detection and\nvideo classification that preserves both global spatial context and fine\nspatiotemporal features. To address the loss of global context, we embed\ntubelet location, size, and confidence as inputs to the classifier.\nAdditionally, we use ROI-aligned feature maps from a pre-trained detection\nmodel, leveraging learned feature representations to increase the receptive\nfield and reduce computational complexity. Our method is efficient, with the\nspatiotemporal tubelet classifier comprising only 0.4M parameters. We apply our\napproach to detect and classify lung consolidation and pleural effusion in\nultrasound videos. Five-fold cross-validation on 14,804 videos from 828\npatients shows our method outperforms previous tubelet-based approaches and is\nsuited for real-time workflows.",
      "tldr_zh": "该研究提出了一种轻量级上下文感知视频片段（tubelet）分析框架，用于超声视频的时空学习。该方法通过嵌入片段位置、尺寸和置信度等全局空间信息，并结合预训练检测模型的ROI对齐特征图，有效解决了现有方法在局部分析时丢失全局上下文的问题。在14,804个超声视频的实验中，该仅含0.4M参数的模型在肺部实变和胸腔积液的检测分类任务上表现优异，既保持了时空特征的精细提取，又适用于实时工作流程。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "ISBI Oral 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.17475v1",
      "published_date": "2025-03-21 18:39:42 UTC",
      "updated_date": "2025-03-21 18:39:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:22:38.474578"
    },
    {
      "arxiv_id": "2503.17456v1",
      "title": "Language-specific Neurons Do Not Facilitate Cross-Lingual Transfer",
      "title_zh": "语言特异性神经元无助于跨语言迁移",
      "authors": [
        "Soumen Kumar Mondal",
        "Sayambhu Sen",
        "Abhishek Singhania",
        "Preethi Jyothi"
      ],
      "abstract": "Multilingual large language models (LLMs) aim towards robust natural language\nunderstanding across diverse languages, yet their performance significantly\ndegrades on low-resource languages. This work explores whether existing\ntechniques to identify language-specific neurons can be leveraged to enhance\ncross-lingual task performance of lowresource languages. We conduct detailed\nexperiments covering existing language-specific neuron identification\ntechniques (such as Language Activation Probability Entropy and activation\nprobability-based thresholding) and neuron-specific LoRA fine-tuning with\nmodels like Llama 3.1 and Mistral Nemo. We find that such neuron-specific\ninterventions are insufficient to yield cross-lingual improvements on\ndownstream tasks (XNLI, XQuAD) in lowresource languages. This study highlights\nthe challenges in achieving cross-lingual generalization and provides critical\ninsights for multilingual LLMs.",
      "tldr_zh": "该研究探讨了通过识别语言特异性神经元来提升多语言大模型(LLMs)在低资源语言上跨语言任务表现的可能性。实验采用Language Activation Probability Entropy等现有方法定位语言特异性神经元，并结合神经元特异性LoRA微调技术(Llama 3.1和Mistral Nemo模型)。结果表明，这些针对特定神经元的干预措施未能显著改善低资源语言在XNLI和XQuAD等下游任务中的跨语言表现。该发现揭示了实现跨语言泛化的挑战，为多语言LLMs的发展提供了重要启示。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted (oral) at NAACL 2025 (InsightsNLP)",
      "pdf_url": "http://arxiv.org/pdf/2503.17456v1",
      "published_date": "2025-03-21 18:08:11 UTC",
      "updated_date": "2025-03-21 18:08:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:23:11.689959"
    },
    {
      "arxiv_id": "2503.17452v1",
      "title": "CausalRivers -- Scaling up benchmarking of causal discovery for real-world time-series",
      "title_zh": "CausalRivers——面向现实世界时间序列的因果发现基准测试规模化扩展",
      "authors": [
        "Gideon Stein",
        "Maha Shadaydeh",
        "Jan Blunk",
        "Niklas Penzel",
        "Joachim Denzler"
      ],
      "abstract": "Causal discovery, or identifying causal relationships from observational\ndata, is a notoriously challenging task, with numerous methods proposed to\ntackle it. Despite this, in-the-wild evaluation of these methods is still\nlacking, as works frequently rely on synthetic data evaluation and sparse\nreal-world examples under critical theoretical assumptions. Real-world causal\nstructures, however, are often complex, making it hard to decide on a proper\ncausal discovery strategy. To bridge this gap, we introduce CausalRivers, the\nlargest in-the-wild causal discovery benchmarking kit for time-series data to\ndate. CausalRivers features an extensive dataset on river discharge that covers\nthe eastern German territory (666 measurement stations) and the state of\nBavaria (494 measurement stations). It spans the years 2019 to 2023 with a\n15-minute temporal resolution. Further, we provide additional data from a flood\naround the Elbe River, as an event with a pronounced distributional shift.\nLeveraging multiple sources of information and time-series meta-data, we\nconstructed two distinct causal ground truth graphs (Bavaria and eastern\nGermany). These graphs can be sampled to generate thousands of subgraphs to\nbenchmark causal discovery across diverse and challenging settings. To\ndemonstrate the utility of CausalRivers, we evaluate several causal discovery\napproaches through a set of experiments to identify areas for improvement.\nCausalRivers has the potential to facilitate robust evaluations and comparisons\nof causal discovery methods. Besides this primary purpose, we also expect that\nthis dataset will be relevant for connected areas of research, such as\ntime-series forecasting and anomaly detection. Based on this, we hope to push\nbenchmark-driven method development that fosters advanced techniques for causal\ndiscovery, as is the case for many other areas of machine learning.",
      "tldr_zh": "该研究提出了CausalRivers——目前最大的真实世界时间序列因果发现基准测试工具包，旨在解决现有方法过度依赖合成数据和简化假设的问题。该工具包含德国东部（666个监测站）和巴伐利亚州（494个监测站）2019-2023年河流径流量数据（15分钟分辨率），并构建了两个因果基准图，可生成数千子图用于测试。通过易北河洪水事件等分布偏移场景，研究验证了多种因果发现方法的性能差距。该资源不仅推动因果发现领域发展，还可服务于时间序列预测和异常检测等关联研究。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 8 figures, ICLR2025 main track",
      "pdf_url": "http://arxiv.org/pdf/2503.17452v1",
      "published_date": "2025-03-21 18:02:35 UTC",
      "updated_date": "2025-03-21 18:02:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:26:21.034274"
    },
    {
      "arxiv_id": "2503.17439v1",
      "title": "LEMMA: Learning from Errors for MatheMatical Advancement in LLMs",
      "title_zh": "LEMMA：从错误中学习以推动大语言模型的数学能力发展",
      "authors": [
        "Zhuoshi Pan",
        "Yu Li",
        "Honglin Lin",
        "Qizhi Pei",
        "Zinan Tang",
        "Wei Wu",
        "Chenlin Ming",
        "H. Vicky Zhao",
        "Conghui He",
        "Lijun Wu"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable reasoning\ncapability in solving mathematical problems. However, existing approaches\nprimarily focus on improving the quality of correct training data, e.g.,\ndistilling high-quality correct solutions from advanced models, neglecting the\nvalue contained in error data, potentially hindering the model's reflective\nability. Though some studies attempt to leverage error data, they often involve\ncomplex mechanisms, such as Monte Carlo Tree Search (MCTS) to explore error\nnodes. In this work, we propose to enhance LLMs' reasoning ability by Learning\nfrom Errors for Mathematical Advancement (LEMMA). LEMMA constructs data\nconsisting of an incorrect solution with an erroneous step and a reflection\nconnection to a correct solution for fine-tuning. Specifically, we\nsystematically analyze the model-generated error types and introduce an\nerror-type grounded mistake augmentation method to collect diverse and\nrepresentative errors. Correct solutions are either from fixing the errors or\ngenerating a fresh start. Through a model-aware smooth reflection connection,\nthe erroneous solution is transferred to the correct one. By fine-tuning on the\nconstructed dataset, the model is able to self-correct errors autonomously\nwithin the generation process without relying on external critique models.\nExperimental results demonstrate that LEMMA achieves significant performance\nimprovements over other strong baselines.",
      "tldr_zh": "该研究提出LEMMA方法，通过让大语言模型(LLMs)从数学解题错误中学习来提升推理能力。该方法构建包含错误步骤与修正过程的训练数据，采用错误类型导向的多样化错误增强策略，并通过模型感知的平滑修正连接将错误解答转化为正确解法。实验表明，LEMMA使模型能在生成过程中自主纠错，无需依赖外部评判模型，显著超越了现有基线方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 6 figures, 4 tables, under review",
      "pdf_url": "http://arxiv.org/pdf/2503.17439v1",
      "published_date": "2025-03-21 17:59:10 UTC",
      "updated_date": "2025-03-21 17:59:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:26:40.462382"
    },
    {
      "arxiv_id": "2503.17354v1",
      "title": "HCAST: Human-Calibrated Autonomy Software Tasks",
      "title_zh": "HCAST：人类校准的自主软件任务",
      "authors": [
        "David Rein",
        "Joel Becker",
        "Amy Deng",
        "Seraphina Nix",
        "Chris Canal",
        "Daniel O'Connel",
        "Pip Arnott",
        "Ryan Bloom",
        "Thomas Broadley",
        "Katharyn Garcia",
        "Brian Goodrich",
        "Max Hasin",
        "Sami Jawhar",
        "Megan Kinniment",
        "Thomas Kwa",
        "Aron Lajko",
        "Nate Rush",
        "Lucas Jun Koba Sato",
        "Sydney Von Arx",
        "Ben West",
        "Lawrence Chan",
        "Elizabeth Barnes"
      ],
      "abstract": "To understand and predict the societal impacts of highly autonomous AI\nsystems, we need benchmarks with grounding, i.e., metrics that directly connect\nAI performance to real-world effects we care about. We present HCAST\n(Human-Calibrated Autonomy Software Tasks), a benchmark of 189 machine learning\nengineering, cybersecurity, software engineering, and general reasoning tasks.\nWe collect 563 human baselines (totaling over 1500 hours) from people skilled\nin these domains, working under identical conditions as AI agents, which lets\nus estimate that HCAST tasks take humans between one minute and 8+ hours.\nMeasuring the time tasks take for humans provides an intuitive metric for\nevaluating AI capabilities, helping answer the question \"can an agent be\ntrusted to complete a task that would take a human X hours?\" We evaluate the\nsuccess rates of AI agents built on frontier foundation models, and we find\nthat current agents succeed 70-80% of the time on tasks that take humans less\nthan one hour, and less than 20% of the time on tasks that take humans more\nthan 4 hours.",
      "tldr_zh": "该研究提出了HCAST基准测试，包含189个涉及机器学习工程、网络安全、软件工程和通用推理的任务，通过收集563名领域专家的基线数据（总计超1500小时），建立了人类任务耗时标准（1分钟至8+小时）。研究发现，当前基于前沿基础模型构建的AI代理在耗时小于1小时的任务上成功率约70-80%，而在耗时超4小时的任务上成功率不足20%，为评估AI系统社会影响提供了直观的\"人类等效耗时\"度量标准。",
      "categories": [
        "cs.AI",
        "I.2.0"
      ],
      "primary_category": "cs.AI",
      "comment": "32 pages, 10 figures, 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.17354v1",
      "published_date": "2025-03-21 17:54:01 UTC",
      "updated_date": "2025-03-21 17:54:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:27:03.260910"
    },
    {
      "arxiv_id": "2503.17353v1",
      "title": "NdLinear Is All You Need for Representation Learning",
      "title_zh": "NdLinear：表征学习的终极解决方案",
      "authors": [
        "Alex Reneau",
        "Jerry Yao-Chieh Hu",
        "Zhongfang Zhuang",
        "Ting-Chun Liu"
      ],
      "abstract": "Many high-impact machine learning tasks involve multi-dimensional data (e.g.,\nimages, volumetric medical scans, multivariate time-series). Yet, most neural\narchitectures flatten inputs, discarding critical cross-dimension information.\nWe introduce NdLinear, a novel linear transformation that preserves these\nstructures without extra overhead. By operating separately along each\ndimension, NdLinear captures dependencies that standard fully connected layers\noverlook. Extensive experiments across convolutional, recurrent, and\ntransformer-based networks show significant improvements in representational\npower and parameter efficiency. Crucially, NdLinear serves as a foundational\nbuilding block for large-scale foundation models by operating on any unimodal\nor multimodal data in its native form. This removes the need for flattening or\nmodality-specific preprocessing. Ndlinear rethinks core architectural\npriorities beyond attention, enabling more expressive, context-aware models at\nscale. We propose NdLinear as a drop-in replacement for standard linear layers\n-- marking an important step toward next-generation neural architectures.",
      "tldr_zh": "该研究提出了一种新型线性变换NdLinear，能够直接处理多维数据（如图像、体积医学扫描等）而无需展平输入，从而保留关键的跨维度信息。与标准全连接层不同，NdLinear通过沿各维度独立运算来捕捉被传统方法忽略的依赖关系。实验表明，该模块在卷积、循环和基于Transformer的网络中均能显著提升表征能力和参数效率，可作为基础构建块直接处理任意单模态或多模态数据。这项研究为下一代神经架构提供了一种无需注意力机制即可实现高表达力和上下文感知的替代方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Code is available at https://github.com/ensemble-core/NdLinear",
      "pdf_url": "http://arxiv.org/pdf/2503.17353v1",
      "published_date": "2025-03-21 17:52:44 UTC",
      "updated_date": "2025-03-21 17:52:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:27:24.714317"
    },
    {
      "arxiv_id": "2503.17340v1",
      "title": "Align Your Rhythm: Generating Highly Aligned Dance Poses with Gating-Enhanced Rhythm-Aware Feature Representation",
      "title_zh": "对齐你的节奏：利用门控增强的节奏感知特征表征生成高同步舞蹈姿态",
      "authors": [
        "Congyi Fan",
        "Jian Guan",
        "Xuanjia Zhao",
        "Dongli Xu",
        "Youtian Lin",
        "Tong Ye",
        "Pengming Feng",
        "Haiwei Pan"
      ],
      "abstract": "Automatically generating natural, diverse and rhythmic human dance movements\ndriven by music is vital for virtual reality and film industries. However,\ngenerating dance that naturally follows music remains a challenge, as existing\nmethods lack proper beat alignment and exhibit unnatural motion dynamics. In\nthis paper, we propose Danceba, a novel framework that leverages gating\nmechanism to enhance rhythm-aware feature representation for music-driven dance\ngeneration, which achieves highly aligned dance poses with enhanced rhythmic\nsensitivity. Specifically, we introduce Phase-Based Rhythm Extraction (PRE) to\nprecisely extract rhythmic information from musical phase data, capitalizing on\nthe intrinsic periodicity and temporal structures of music. Additionally, we\npropose Temporal-Gated Causal Attention (TGCA) to focus on global rhythmic\nfeatures, ensuring that dance movements closely follow the musical rhythm. We\nalso introduce Parallel Mamba Motion Modeling (PMMM) architecture to separately\nmodel upper and lower body motions along with musical features, thereby\nimproving the naturalness and diversity of generated dance movements. Extensive\nexperiments confirm that Danceba outperforms state-of-the-art methods,\nachieving significantly better rhythmic alignment and motion diversity. Project\npage: https://danceba.github.io/ .",
      "tldr_zh": "本研究提出了一种名为Danceba的新框架，用于生成与音乐高度同步的舞蹈动作。该框架通过门控机制增强节奏感知特征表示，采用Phase-Based Rhythm Extraction（PRE）精确提取音乐节奏信息，并利用Temporal-Gated Causal Attention（TGCA）关注全局节奏特征，确保舞蹈动作与音乐节奏紧密同步。此外，Parallel Mamba Motion Modeling（PMMM）架构分别建模上下身动作和音乐特征，提升了生成舞蹈的自然性和多样性。实验表明，Danceba在节奏对齐和动作多样性方面显著优于现有方法。",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.CV",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.MM",
      "comment": "10 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.17340v1",
      "published_date": "2025-03-21 17:42:50 UTC",
      "updated_date": "2025-03-21 17:42:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:27:40.674999"
    },
    {
      "arxiv_id": "2503.17339v1",
      "title": "Can AI expose tax loopholes? Towards a new generation of legal policy assistants",
      "title_zh": "AI能否揭露税收漏洞？迈向新一代法律政策助手",
      "authors": [
        "Peter Fratrič",
        "Nils Holzenberger",
        "David Restrepo Amariles"
      ],
      "abstract": "The legislative process is the backbone of a state built on solid\ninstitutions. Yet, due to the complexity of laws -- particularly tax law --\npolicies may lead to inequality and social tensions. In this study, we\nintroduce a novel prototype system designed to address the issues of tax\nloopholes and tax avoidance. Our hybrid solution integrates a natural language\ninterface with a domain-specific language tailored for planning. We demonstrate\non a case study how tax loopholes and avoidance schemes can be exposed. We\nconclude that our prototype can help enhance social welfare by systematically\nidentifying and addressing tax gaps stemming from loopholes.",
      "tldr_zh": "这项研究开发了一个新型AI系统原型，旨在通过自然语言界面和特定领域规划语言的混合解决方案，自动检测税法漏洞和避税方案。研究证明该系统能有效识别税收政策中的漏洞，从而帮助减少不平等和社会矛盾。结果表明，该技术工具有潜力成为新一代法律政策助手，通过系统性地发现税法漏洞来提升社会福利。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "13 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.17339v1",
      "published_date": "2025-03-21 17:40:06 UTC",
      "updated_date": "2025-03-21 17:40:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:28:05.437218"
    },
    {
      "arxiv_id": "2503.17338v1",
      "title": "Capturing Individual Human Preferences with Reward Features",
      "title_zh": "利用奖励特征捕捉个体人类偏好",
      "authors": [
        "André Barreto",
        "Vincent Dumoulin",
        "Yiran Mao",
        "Nicolas Perez-Nieves",
        "Bobak Shahriari",
        "Yann Dauphin",
        "Doina Precup",
        "Hugo Larochelle"
      ],
      "abstract": "Reinforcement learning from human feedback usually models preferences using a\nreward model that does not distinguish between people. We argue that this is\nunlikely to be a good design choice in contexts with high potential for\ndisagreement, like in the training of large language models. We propose a\nmethod to specialise a reward model to a person or group of people. Our\napproach builds on the observation that individual preferences can be captured\nas a linear combination of a set of general reward features. We show how to\nlearn such features and subsequently use them to quickly adapt the reward model\nto a specific individual, even if their preferences are not reflected in the\ntraining data. We present experiments with large language models comparing the\nproposed architecture with a non-adaptive reward model and also adaptive\ncounterparts, including models that do in-context personalisation. Depending on\nhow much disagreement there is in the training data, our model either\nsignificantly outperforms the baselines or matches their performance with a\nsimpler architecture and more stable training.",
      "tldr_zh": "该研究提出了一种个性化奖励模型方法，用于捕捉个体或群体的人类偏好差异。不同于传统强化学习使用统一奖励模型，该方法通过将个体偏好表示为通用奖励特征的线性组合来实现个性化建模。实验表明，在训练数据存在显著分歧的情况下（如大语言模型训练场景），该模型能显著优于非自适应模型和上下文个性化方法，同时在简单架构下保持更稳定的训练性能。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17338v1",
      "published_date": "2025-03-21 17:39:33 UTC",
      "updated_date": "2025-03-21 17:39:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:28:21.775878"
    },
    {
      "arxiv_id": "2503.17336v1",
      "title": "Efficient Intent-Based Filtering for Multi-Party Conversations Using Knowledge Distillation from LLMs",
      "title_zh": "基于意图的高效多轮对话过滤：利用大语言模型的知识蒸馏",
      "authors": [
        "Reem Gody",
        "Mohamed Abdelghaffar",
        "Mohammed Jabreel",
        "Ahmed Tawfik"
      ],
      "abstract": "Large language models (LLMs) have showcased remarkable capabilities in\nconversational AI, enabling open-domain responses in chat-bots, as well as\nadvanced processing of conversations like summarization, intent classification,\nand insights generation. However, these models are resource-intensive,\ndemanding substantial memory and computational power. To address this, we\npropose a cost-effective solution that filters conversational snippets of\ninterest for LLM processing, tailored to the target downstream application,\nrather than processing every snippet. In this work, we introduce an innovative\napproach that leverages knowledge distillation from LLMs to develop an\nintent-based filter for multi-party conversations, optimized for compute power\nconstrained environments. Our method combines different strategies to create a\ndiverse multi-party conversational dataset, that is annotated with the target\nintents and is then used to fine-tune the MobileBERT model for multi-label\nintent classification. This model achieves a balance between efficiency and\nperformance, effectively filtering conversation snippets based on their\nintents. By passing only the relevant snippets to the LLM for further\nprocessing, our approach significantly reduces overall operational costs\ndepending on the intents and the data distribution as demonstrated in our\nexperiments.",
      "tldr_zh": "该研究提出了一种基于知识蒸馏(Knowledge Distillation)的高效意图过滤方法，用于优化多参与者对话的处理效率。通过从大语言模型(LLMs)中提取知识，团队训练了轻量级的MobileBERT模型进行多标签意图分类，仅筛选相关对话片段供LLM进一步处理。实验表明，该方法在计算资源受限环境下显著降低了运营成本，同时保持了处理性能，为实际应用提供了经济高效的解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17336v1",
      "published_date": "2025-03-21 17:34:37 UTC",
      "updated_date": "2025-03-21 17:34:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:28:43.567201"
    },
    {
      "arxiv_id": "2503.17332v1",
      "title": "CVE-Bench: A Benchmark for AI Agents' Ability to Exploit Real-World Web Application Vulnerabilities",
      "title_zh": "CVE-Bench：评估AI智能体利用真实世界Web应用漏洞能力的基准测试",
      "authors": [
        "Yuxuan Zhu",
        "Antony Kellermann",
        "Dylan Bowman",
        "Philip Li",
        "Akul Gupta",
        "Adarsh Danda",
        "Richard Fang",
        "Conner Jensen",
        "Eric Ihli",
        "Jason Benn",
        "Jet Geronimo",
        "Avi Dhir",
        "Sudhit Rao",
        "Kaicheng Yu",
        "Twm Stone",
        "Daniel Kang"
      ],
      "abstract": "Large language model (LLM) agents are increasingly capable of autonomously\nconducting cyberattacks, posing significant threats to existing applications.\nThis growing risk highlights the urgent need for a real-world benchmark to\nevaluate the ability of LLM agents to exploit web application vulnerabilities.\nHowever, existing benchmarks fall short as they are limited to abstracted\nCapture the Flag competitions or lack comprehensive coverage. Building a\nbenchmark for real-world vulnerabilities involves both specialized expertise to\nreproduce exploits and a systematic approach to evaluating unpredictable\nthreats. To address this challenge, we introduce CVE-Bench, a real-world\ncybersecurity benchmark based on critical-severity Common Vulnerabilities and\nExposures. In CVE-Bench, we design a sandbox framework that enables LLM agents\nto exploit vulnerable web applications in scenarios that mimic real-world\nconditions, while also providing effective evaluation of their exploits. Our\nevaluation shows that the state-of-the-art agent framework can resolve up to\n13% of vulnerabilities.",
      "tldr_zh": "该研究提出了CVE-Bench基准测试，用于评估大语言模型(LLM)智能体利用真实世界Web应用漏洞的能力。该基准基于高危级通用漏洞披露(CVE)构建，通过沙箱框架模拟真实攻击场景，解决了现有测试局限于抽象CTF竞赛或覆盖不足的问题。实验表明，当前最先进的智能体框架仅能成功利用13%的漏洞，突显了网络安全领域面临的新型AI威胁挑战。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "I.2.1; I.2.7"
      ],
      "primary_category": "cs.CR",
      "comment": "15 pages, 4 figures, 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.17332v1",
      "published_date": "2025-03-21 17:32:32 UTC",
      "updated_date": "2025-03-21 17:32:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:29:04.418984"
    },
    {
      "arxiv_id": "2503.17309v1",
      "title": "LLM+MAP: Bimanual Robot Task Planning using Large Language Models and Planning Domain Definition Language",
      "title_zh": "LLM+MAP：基于大语言模型与规划领域定义语言的双臂机器人任务规划",
      "authors": [
        "Kun Chu",
        "Xufeng Zhao",
        "Cornelius Weber",
        "Stefan Wermter"
      ],
      "abstract": "Bimanual robotic manipulation provides significant versatility, but also\npresents an inherent challenge due to the complexity involved in the spatial\nand temporal coordination between two hands. Existing works predominantly focus\non attaining human-level manipulation skills for robotic hands, yet little\nattention has been paid to task planning on long-horizon timescales. With their\noutstanding in-context learning and zero-shot generation abilities, Large\nLanguage Models (LLMs) have been applied and grounded in diverse robotic\nembodiments to facilitate task planning. However, LLMs still suffer from errors\nin long-horizon reasoning and from hallucinations in complex robotic tasks,\nlacking a guarantee of logical correctness when generating the plan. Previous\nworks, such as LLM+P, extended LLMs with symbolic planners. However, none have\nbeen successfully applied to bimanual robots. New challenges inevitably arise\nin bimanual manipulation, necessitating not only effective task decomposition\nbut also efficient task allocation. To address these challenges, this paper\nintroduces LLM+MAP, a bimanual planning framework that integrates LLM reasoning\nand multi-agent planning, automating effective and efficient bimanual task\nplanning. We conduct simulated experiments on various long-horizon manipulation\ntasks of differing complexity. Our method is built using GPT-4o as the backend,\nand we compare its performance against plans generated directly by LLMs,\nincluding GPT-4o, V3 and also recent strong reasoning models o1 and R1. By\nanalyzing metrics such as planning time, success rate, group debits, and\nplanning-step reduction rate, we demonstrate the superior performance of\nLLM+MAP, while also providing insights into robotic reasoning. Code is\navailable at https://github.com/Kchu/LLM-MAP.",
      "tldr_zh": "该论文提出LLM+MAP框架，结合大语言模型(LLMs)和多智能体规划(MAP)，解决双手机器人长周期任务规划中的时空协调难题。通过整合符号规划器(PDDL)与LLMs的上下文学习能力，该方法克服了传统LLMs在复杂任务中的幻觉和逻辑错误问题，实现了高效的任务分解与分配。实验表明，基于GPT-4o的LLM+MAP在规划时间、成功率等指标上显著优于直接使用LLMs的方案，为双手机器人操作提供了可靠的自动化规划解决方案。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Code and video are available at https://github.com/Kchu/LLM-MAP",
      "pdf_url": "http://arxiv.org/pdf/2503.17309v1",
      "published_date": "2025-03-21 17:04:01 UTC",
      "updated_date": "2025-03-21 17:04:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:29:24.984076"
    },
    {
      "arxiv_id": "2503.17299v1",
      "title": "Preference-Guided Diffusion for Multi-Objective Offline Optimization",
      "title_zh": "偏好引导扩散模型在多目标离线优化中的应用",
      "authors": [
        "Yashas Annadani",
        "Syrine Belakaria",
        "Stefano Ermon",
        "Stefan Bauer",
        "Barbara E Engelhardt"
      ],
      "abstract": "Offline multi-objective optimization aims to identify Pareto-optimal\nsolutions given a dataset of designs and their objective values. In this work,\nwe propose a preference-guided diffusion model that generates Pareto-optimal\ndesigns by leveraging a classifier-based guidance mechanism. Our guidance\nclassifier is a preference model trained to predict the probability that one\ndesign dominates another, directing the diffusion model toward optimal regions\nof the design space. Crucially, this preference model generalizes beyond the\ntraining distribution, enabling the discovery of Pareto-optimal solutions\noutside the observed dataset. We introduce a novel diversity-aware preference\nguidance, augmenting Pareto dominance preference with diversity criteria. This\nensures that generated solutions are optimal and well-distributed across the\nobjective space, a capability absent in prior generative methods for offline\nmulti-objective optimization. We evaluate our approach on various continuous\noffline multi-objective optimization tasks and find that it consistently\noutperforms other inverse/generative approaches while remaining competitive\nwith forward/surrogate-based optimization methods. Our results highlight the\neffectiveness of classifier-guided diffusion models in generating diverse and\nhigh-quality solutions that approximate the Pareto front well.",
      "tldr_zh": "该研究提出了一种偏好引导扩散模型（Preference-Guided Diffusion），用于解决离线多目标优化问题。该方法通过训练一个基于分类器的偏好模型来预测设计方案的支配关系，从而引导扩散模型生成帕累托最优（Pareto-optimal）解，并能够发现超出训练数据分布的新解。创新性地引入了多样性感知偏好引导机制，在保证帕累托最优的同时确保解在目标空间中的良好分布。实验表明，该方法在连续离线多目标优化任务中优于其他逆向/生成方法，并与正向/代理优化方法性能相当，能有效生成高质量且多样化的帕累托前沿近似解。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17299v1",
      "published_date": "2025-03-21 16:49:38 UTC",
      "updated_date": "2025-03-21 16:49:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:30:02.142921"
    },
    {
      "arxiv_id": "2503.17251v1",
      "title": "Breaking the Symmetries of Indistinguishable Objects",
      "title_zh": "打破不可区分对象的对称性",
      "authors": [
        "Ozgur Akgun",
        "Mun See Chang",
        "Ian P. Gent",
        "Christopher Jefferson"
      ],
      "abstract": "Indistinguishable objects often occur when modelling problems in constraint\nprogramming, as well as in other related paradigms. They occur when objects can\nbe viewed as being drawn from a set of unlabelled objects, and the only\noperation allowed on them is equality testing. For example, the golfers in the\nsocial golfer problem are indistinguishable. If we do label the golfers, then\nany relabelling of the golfers in one solution gives another valid solution.\nTherefore, we can regard the symmetric group of size $n$ as acting on a set of\n$n$ indistinguishable objects. In this paper, we show how we can break the\nsymmetries resulting from indistinguishable objects. We show how symmetries on\nindistinguishable objects can be defined properly in complex types, for example\nin a matrix indexed by indistinguishable objects. We then show how the\nresulting symmetries can be broken correctly. In Essence, a high-level\nmodelling language, indistinguishable objects are encapsulated in \"unnamed\ntypes\". We provide an implementation of complete symmetry breaking for unnamed\ntypes in Essence.",
      "tldr_zh": "该论文研究了约束规划中不可区分对象（indistinguishable objects）导致的对称性问题，提出了一套完整的对称性破除方法。作者首先在复杂类型（如由不可区分对象索引的矩阵）中正确定义了这些对称性，随后展示了如何有效破除这些对称性。研究在高级建模语言Essence中实现了\"未命名类型\"的完整对称性破除方案，为解决类似社交高尔夫球手问题等场景中的对称性问题提供了系统化方法。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17251v1",
      "published_date": "2025-03-21 15:56:52 UTC",
      "updated_date": "2025-03-21 15:56:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:30:22.591837"
    },
    {
      "arxiv_id": "2503.17247v1",
      "title": "KL3M Tokenizers: A Family of Domain-Specific and Character-Level Tokenizers for Legal, Financial, and Preprocessing Applications",
      "title_zh": "KL3M分词器：面向法律、金融及预处理应用的专业领域字符级分词器系列",
      "authors": [
        "Michael J Bommarito",
        "Daniel Martin Katz",
        "Jillian Bommarito"
      ],
      "abstract": "We present the KL3M tokenizers, a family of specialized tokenizers for legal,\nfinancial, and governmental text. Despite established work on tokenization,\nspecialized tokenizers for professional domains remain understudied. Our paper\noffers two main contributions to this area.\n  First, we introduce domain-specific BPE tokenizers for legal, financial, and\ngovernmental text. Our kl3m-004-128k-cased tokenizer uses 9-17% fewer tokens\nthan GPT-4o and Llama3 for domain-specific documents, despite having a smaller\nvocabulary. For specialized terminology, our cased tokenizer is even more\nefficient, using up to 83% fewer tokens for legal terms and 39% fewer tokens\nfor financial terms.\n  Second, we develop character-level BPE tokenizers (4K, 8K, and 16K vocabulary\nsizes) for text correction tasks like OCR post-processing. These tokenizers\nkeep consistent token boundaries between error-containing and correct text,\nmaking it easier for models to learn correction patterns.\n  These tokenizers help professional applications by fitting more text in\ncontext windows, reducing computational needs, and preserving the meaning of\ndomain-specific terms. Our analysis shows these efficiency gains directly\nbenefit the processing of long legal and financial documents. We release all\ntokenizers and code through GitHub and Hugging Face to support further research\nin specialized tokenization.",
      "tldr_zh": "本文提出了KL3M系列专业领域分词器，包含两个主要创新：首先开发了针对法律、金融和政府文本的领域专用BPE分词器，相比GPT-4o和Llama3在处理专业文档时可减少9-17%的token使用量，其中法律术语处理效率提升高达83%。其次设计了面向OCR后处理等文本校正任务的字符级BPE分词器(4K/8K/16K词表)，能保持错误文本与正确文本间的token边界一致性。这些分词器通过GitHub和Hugging Face开源，可显著提升专业长文档处理的上下文窗口利用率和计算效率。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages, 7 tables, 3 figures; Source code available at\n  https://github.com/alea-institute/kl3m-tokenizer-paper",
      "pdf_url": "http://arxiv.org/pdf/2503.17247v1",
      "published_date": "2025-03-21 15:51:43 UTC",
      "updated_date": "2025-03-21 15:51:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:30:43.745790"
    },
    {
      "arxiv_id": "2503.17239v1",
      "title": "SafeMERGE: Preserving Safety Alignment in Fine-Tuned Large Language Models via Selective Layer-Wise Model Merging",
      "title_zh": "SafeMERGE：通过选择性分层模型融合在微调大语言模型中保持安全对齐",
      "authors": [
        "Aladin Djuhera",
        "Swanand Ravindra Kadhe",
        "Farhan Ahmed",
        "Syed Zawad",
        "Holger Boche"
      ],
      "abstract": "Fine-tuning large language models (LLMs) on downstream tasks can\ninadvertently erode their safety alignment, even for benign fine-tuning\ndatasets. We address this challenge by proposing SafeMERGE, a post-fine-tuning\nframework that preserves safety while maintaining task utility. It achieves\nthis by selectively merging fine-tuned and safety-aligned model layers only\nwhen those deviate from safe behavior, measured by a cosine similarity\ncriterion. We evaluate SafeMERGE against other fine-tuning- and\npost-fine-tuning-stage approaches for Llama-2-7B-Chat and Qwen-2-7B-Instruct\nmodels on GSM8K and PubMedQA tasks while exploring different merging\nstrategies. We find that SafeMERGE consistently reduces harmful outputs\ncompared to other baselines without significantly sacrificing performance,\nsometimes even enhancing it. The results suggest that our selective,\nsubspace-guided, and per-layer merging method provides an effective safeguard\nagainst the inadvertent loss of safety in fine-tuned LLMs while outperforming\nsimpler post-fine-tuning-stage defenses.",
      "tldr_zh": "这篇论文提出了SafeMERGE框架，旨在解决大语言模型(LLMs)在下游任务微调过程中可能破坏安全对齐的问题。该方法通过基于余弦相似度标准，选择性地合并微调模型和安全对齐模型的层，从而在保持任务性能的同时维护模型安全性。实验表明，SafeMERGE在Llama-2-7B-Chat和Qwen-2-7B-Instruct模型上能有效减少有害输出，且不显著牺牲模型性能，甚至有时能提升表现。这种基于子空间的逐层选择性合并方法为微调LLMs提供了有效的安全保障。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17239v1",
      "published_date": "2025-03-21 15:44:09 UTC",
      "updated_date": "2025-03-21 15:44:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:31:05.767963"
    },
    {
      "arxiv_id": "2503.17237v1",
      "title": "Strong Baseline: Multi-UAV Tracking via YOLOv12 with BoT-SORT-ReID",
      "title_zh": "强基线：基于YOLOv12与BoT-SORT-ReID的多无人机跟踪方案",
      "authors": [
        "Yu-Hsi Chen"
      ],
      "abstract": "Detecting and tracking multiple unmanned aerial vehicles (UAVs) in thermal\ninfrared video is inherently challenging due to low contrast, environmental\nnoise, and small target sizes. This paper provides a straightforward approach\nto address multi-UAV tracking in thermal infrared video, leveraging recent\nadvances in detection and tracking. Instead of relying on the YOLOv5 with the\nDeepSORT pipeline, we present a tracking framework built on YOLOv12 and\nBoT-SORT, enhanced with tailored training and inference strategies. We evaluate\nour approach following the metrics from the 4th Anti-UAV Challenge and\ndemonstrate competitive performance. Notably, we achieve strong results without\nusing contrast enhancement or temporal information fusion to enrich UAV\nfeatures, highlighting our approach as a \"Strong Baseline\" for the multi-UAV\ntracking task. We provide implementation details, in-depth experimental\nanalysis, and a discussion of potential improvements. The code is available at\nhttps://github.com/wish44165/YOLOv12-BoT-SORT-ReID .",
      "tldr_zh": "本研究提出了一种基于YOLOv12检测器和BoT-SORT-ReID跟踪器的多无人机(UAV)热红外视频追踪强基线方法。该方法通过定制化训练和推理策略，在不使用对比度增强或时序信息融合的情况下，在第四届反无人机挑战赛指标下展现出竞争优势。实验表明，该框架有效解决了热红外视频中低对比度、环境噪声和小目标尺寸等挑战，为多无人机追踪任务提供了可靠基准，相关代码已开源。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages, 5 figures, 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.17237v1",
      "published_date": "2025-03-21 15:40:18 UTC",
      "updated_date": "2025-03-21 15:40:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:31:21.228876"
    },
    {
      "arxiv_id": "2503.17229v1",
      "title": "FactSelfCheck: Fact-Level Black-Box Hallucination Detection for LLMs",
      "title_zh": "FactSelfCheck：针对大语言模型的事实级黑箱幻觉检测",
      "authors": [
        "Albert Sawczyn",
        "Jakub Binkowski",
        "Denis Janiak",
        "Bogdan Gabrys",
        "Tomasz Kajdanowicz"
      ],
      "abstract": "Large Language Models (LLMs) frequently generate hallucinated content, posing\nsignificant challenges for applications where factuality is crucial. While\nexisting hallucination detection methods typically operate at the sentence\nlevel or passage level, we propose FactSelfCheck, a novel black-box\nsampling-based method that enables fine-grained fact-level detection. Our\napproach represents text as knowledge graphs consisting of facts in the form of\ntriples. Through analyzing factual consistency across multiple LLM responses,\nwe compute fine-grained hallucination scores without requiring external\nresources or training data. Our evaluation demonstrates that FactSelfCheck\nperforms competitively with leading sampling-based methods while providing more\ndetailed insights. Most notably, our fact-level approach significantly improves\nhallucination correction, achieving a 35% increase in factual content compared\nto the baseline, while sentence-level SelfCheckGPT yields only an 8%\nimprovement. The granular nature of our detection enables more precise\nidentification and correction of hallucinated content.",
      "tldr_zh": "该研究提出FactSelfCheck方法，通过知识图谱三元组表示实现细粒度的事实级(fact-level)大语言模型(LLMs)幻觉检测。该方法创新性地采用黑盒采样策略，通过分析多轮LLM响应间的事实一致性计算幻觉分数，无需外部资源或训练数据。实验表明，相比基线方法，该方法将事实内容修正效果提升35%（句子级方法仅提升8%），为LLM幻觉检测与修正提供了更精确的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint",
      "pdf_url": "http://arxiv.org/pdf/2503.17229v1",
      "published_date": "2025-03-21 15:32:24 UTC",
      "updated_date": "2025-03-21 15:32:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:32:00.401952"
    },
    {
      "arxiv_id": "2503.17224v1",
      "title": "Neuro-Symbolic Scene Graph Conditioning for Synthetic Image Dataset Generation",
      "title_zh": "神经符号场景图条件化合成图像数据集生成",
      "authors": [
        "Giacomo Savazzi",
        "Eugenio Lomurno",
        "Cristian Sbrolli",
        "Agnese Chiatti",
        "Matteo Matteucci"
      ],
      "abstract": "As machine learning models increase in scale and complexity, obtaining\nsufficient training data has become a critical bottleneck due to acquisition\ncosts, privacy constraints, and data scarcity in specialised domains. While\nsynthetic data generation has emerged as a promising alternative, a notable\nperformance gap remains compared to models trained on real data, particularly\nas task complexity grows. Concurrently, Neuro-Symbolic methods, which combine\nneural networks' learning strengths with symbolic reasoning's structured\nrepresentations, have demonstrated significant potential across various\ncognitive tasks. This paper explores the utility of Neuro-Symbolic conditioning\nfor synthetic image dataset generation, focusing specifically on improving the\nperformance of Scene Graph Generation models. The research investigates whether\nstructured symbolic representations in the form of scene graphs can enhance\nsynthetic data quality through explicit encoding of relational constraints. The\nresults demonstrate that Neuro-Symbolic conditioning yields significant\nimprovements of up to +2.59% in standard Recall metrics and +2.83% in No Graph\nConstraint Recall metrics when used for dataset augmentation. These findings\nestablish that merging Neuro-Symbolic and generative approaches produces\nsynthetic data with complementary structural information that enhances model\nperformance when combined with real data, providing a novel approach to\novercome data scarcity limitations even for complex visual reasoning tasks.",
      "tldr_zh": "本研究提出了一种神经符号(Neuro-Symbolic)场景图条件生成方法，用于提升合成图像数据集的质量。该方法通过显式编码场景图的关系约束，将神经网络的生成能力与符号系统的结构化表示相结合，有效改善了场景图生成(Scene Graph Generation)模型的性能。实验表明，该方法在数据集增强应用中使标准召回率提升2.59%，无图约束召回率提升2.83%，证实了神经符号方法与生成模型的结合能为复杂视觉推理任务提供有效的合成数据解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17224v1",
      "published_date": "2025-03-21 15:26:16 UTC",
      "updated_date": "2025-03-21 15:26:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:35:25.877287"
    },
    {
      "arxiv_id": "2503.17222v1",
      "title": "Automating Adjudication of Cardiovascular Events Using Large Language Models",
      "title_zh": "利用大型语言模型实现心血管事件判定的自动化",
      "authors": [
        "Sonish Sivarajkumar",
        "Kimia Ameri",
        "Chuqin Li",
        "Yanshan Wang",
        "Min Jiang"
      ],
      "abstract": "Cardiovascular events, such as heart attacks and strokes, remain a leading\ncause of mortality globally, necessitating meticulous monitoring and\nadjudication in clinical trials. This process, traditionally performed manually\nby clinical experts, is time-consuming, resource-intensive, and prone to\ninter-reviewer variability, potentially introducing bias and hindering trial\nprogress. This study addresses these critical limitations by presenting a novel\nframework for automating the adjudication of cardiovascular events in clinical\ntrials using Large Language Models (LLMs). We developed a two-stage approach:\nfirst, employing an LLM-based pipeline for event information extraction from\nunstructured clinical data and second, using an LLM-based adjudication process\nguided by a Tree of Thoughts approach and clinical endpoint committee (CEC)\nguidelines. Using cardiovascular event-specific clinical trial data, the\nframework achieved an F1-score of 0.82 for event extraction and an accuracy of\n0.68 for adjudication. Furthermore, we introduce the CLEART score, a novel,\nautomated metric specifically designed for evaluating the quality of\nAI-generated clinical reasoning in adjudicating cardiovascular events. This\napproach demonstrates significant potential for substantially reducing\nadjudication time and costs while maintaining high-quality, consistent, and\nauditable outcomes in clinical trials. The reduced variability and enhanced\nstandardization also allow for faster identification and mitigation of risks\nassociated with cardiovascular therapies.",
      "tldr_zh": "本研究提出了一种利用大语言模型(LLMs)自动裁决心血管事件的创新框架，解决了传统人工裁决过程耗时、资源密集且易受主观影响的问题。该方法采用两阶段流程：首先通过LLM从非结构化临床数据中提取事件信息(F1-score达0.82)，其次基于\"思维树\"(Tree of Thoughts)方法和临床终点委员会指南进行自动裁决(准确率0.68)。研究还开发了新型CLEART评分标准，专门评估AI生成临床推理的质量。该框架能显著缩短裁决时间、降低成本，同时保持高质量、一致性和可审计性，有助于加速心血管治疗相关风险的识别和管控。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17222v1",
      "published_date": "2025-03-21 15:25:53 UTC",
      "updated_date": "2025-03-21 15:25:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:32:22.524690"
    },
    {
      "arxiv_id": "2503.17213v1",
      "title": "PP-DocLayout: A Unified Document Layout Detection Model to Accelerate Large-Scale Data Construction",
      "title_zh": "PP-DocLayout：加速大规模数据构建的统一文档版面检测模型",
      "authors": [
        "Ting Sun",
        "Cheng Cui",
        "Yuning Du",
        "Yi Liu"
      ],
      "abstract": "Document layout analysis is a critical preprocessing step in document\nintelligence, enabling the detection and localization of structural elements\nsuch as titles, text blocks, tables, and formulas. Despite its importance,\nexisting layout detection models face significant challenges in generalizing\nacross diverse document types, handling complex layouts, and achieving\nreal-time performance for large-scale data processing. To address these\nlimitations, we present PP-DocLayout, which achieves high precision and\nefficiency in recognizing 23 types of layout regions across diverse document\nformats. To meet different needs, we offer three models of varying scales.\nPP-DocLayout-L is a high-precision model based on the RT-DETR-L detector,\nachieving 90.4% mAP@0.5 and an end-to-end inference time of 13.4 ms per page on\na T4 GPU. PP-DocLayout-M is a balanced model, offering 75.2% mAP@0.5 with an\ninference time of 12.7 ms per page on a T4 GPU. PP-DocLayout-S is a\nhigh-efficiency model designed for resource-constrained environments and\nreal-time applications, with an inference time of 8.1 ms per page on a T4 GPU\nand 14.5 ms on a CPU. This work not only advances the state of the art in\ndocument layout analysis but also provides a robust solution for constructing\nhigh-quality training data, enabling advancements in document intelligence and\nmultimodal AI systems. Code and models are available at\nhttps://github.com/PaddlePaddle/PaddleX .",
      "tldr_zh": "该研究提出了PP-DocLayout，一个统一的文档布局检测模型，旨在加速大规模数据构建。该模型能高效识别23种文档布局区域，并针对不同需求提供了三种规模版本：高精度的L版（90.4% mAP@0.5）、平衡的M版和高效的S版（CPU端仅需14.5ms/页）。基于RT-DETR-L检测器，该方案不仅提升了文档布局分析的技术水平，还为构建高质量训练数据提供了可靠解决方案，推动了文档智能和多模态AI系统的发展。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Github Repo: https://github.com/PaddlePaddle/PaddleX",
      "pdf_url": "http://arxiv.org/pdf/2503.17213v1",
      "published_date": "2025-03-21 15:20:47 UTC",
      "updated_date": "2025-03-21 15:20:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:32:41.299128"
    },
    {
      "arxiv_id": "2503.17195v1",
      "title": "TreeSynth: Synthesizing Diverse Data from Scratch via Tree-Guided Subspace Partitioning",
      "title_zh": "TreeSynth：通过树引导子空间划分从零生成多样化数据",
      "authors": [
        "Sheng Wang",
        "Pengan Chen",
        "Jingqi Zhou",
        "Qintong Li",
        "Jingwei Dong",
        "Jiahui Gao",
        "Boyang Xue",
        "Jiyue Jiang",
        "Lingpeng Kong",
        "Chuan Wu"
      ],
      "abstract": "Model customization requires high-quality and diverse datasets, but acquiring\nsuch data remains challenging and costly. Although large language models (LLMs)\ncan synthesize training data, current approaches are constrained by limited\nseed data, model bias and insufficient control over the generation process,\nresulting in limited diversity and biased distribution with the increase of\ndata scales. To tackle this challenge, we present TreeSynth, a tree-guided\nsubspace-based data synthesis framework that recursively partitions the entire\ndata space into hierar-chical subspaces, enabling comprehensive and diverse\nscaling of data synthesis. Briefly, given a task-specific description, we\nconstruct a data space partitioning tree by iteratively executing criteria\ndetermination and subspace coverage steps. This hierarchically divides the\nwhole space (i.e., root node) into mutually exclusive and complementary atomic\nsubspaces (i.e., leaf nodes). By collecting synthesized data according to the\nattributes of each leaf node, we obtain a diverse dataset that fully covers the\ndata space. Empirically, our extensive experiments demonstrate that TreeSynth\nsurpasses both human-designed datasets and the state-of-the-art data synthesis\nbaselines, achieving maximum improvements of 45.2% in data diversity and 17.6%\nin downstream task performance across various models and tasks. Hopefully,\nTreeSynth provides a scalable solution to synthesize diverse and comprehensive\ndatasets from scratch without human intervention.",
      "tldr_zh": "该研究提出了TreeSynth，一种基于树引导子空间划分的数据合成框架，用于从零生成高质量且多样化的数据集。该方法通过递归地将整个数据空间划分为层次化的子空间，确保数据合成的全面性和多样性。实验表明，TreeSynth在数据多样性和下游任务性能上均优于人工设计的数据集和现有数据合成基线，分别提升了45.2%和17.6%，为无需人工干预的自动化数据合成提供了可扩展的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17195v1",
      "published_date": "2025-03-21 14:43:23 UTC",
      "updated_date": "2025-03-21 14:43:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:33:00.598996"
    },
    {
      "arxiv_id": "2503.17184v1",
      "title": "D2Fusion: Dual-domain Fusion with Feature Superposition for Deepfake Detection",
      "title_zh": "D2Fusion：基于特征叠加的双域融合深度伪造检测方法",
      "authors": [
        "Xueqi Qiu",
        "Xingyu Miao",
        "Fan Wan",
        "Haoran Duan",
        "Tejal Shah",
        "Varun Ojhab",
        "Yang Longa",
        "Rajiv Ranjan"
      ],
      "abstract": "Deepfake detection is crucial for curbing the harm it causes to society.\nHowever, current Deepfake detection methods fail to thoroughly explore artifact\ninformation across different domains due to insufficient intrinsic\ninteractions. These interactions refer to the fusion and coordination after\nfeature extraction processes across different domains, which are crucial for\nrecognizing complex forgery clues. Focusing on more generalized Deepfake\ndetection, in this work, we introduce a novel bi-directional attention module\nto capture the local positional information of artifact clues from the spatial\ndomain. This enables accurate artifact localization, thus addressing the coarse\nprocessing with artifact features. To further address the limitation that the\nproposed bi-directional attention module may not well capture global subtle\nforgery information in the artifact feature (e.g., textures or edges), we\nemploy a fine-grained frequency attention module in the frequency domain. By\ndoing so, we can obtain high-frequency information in the fine-grained\nfeatures, which contains the global and subtle forgery information. Although\nthese features from the diverse domains can be effectively and independently\nimproved, fusing them directly does not effectively improve the detection\nperformance. Therefore, we propose a feature superposition strategy that\ncomplements information from spatial and frequency domains. This strategy turns\nthe feature components into the form of wave-like tokens, which are updated\nbased on their phase, such that the distinctions between authentic and artifact\nfeatures can be amplified. Our method demonstrates significant improvements\nover state-of-the-art (SOTA) methods on five public Deepfake datasets in\ncapturing abnormalities across different manipulated operations and real-life.",
      "tldr_zh": "本文提出D2Fusion方法，通过双域特征叠加策略提升深度伪造检测性能。该方法创新性地结合了空间域的双向注意力模块（精准定位局部伪影）和频域的细粒度频率注意力模块（捕捉全局细微伪造特征），并采用基于相位更新的波浪形token特征叠加策略来放大真假特征差异。实验表明，该模型在五大公开Deepfake数据集上显著优于现有最优方法，能有效识别不同篡改操作和现实场景中的异常特征。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17184v1",
      "published_date": "2025-03-21 14:31:33 UTC",
      "updated_date": "2025-03-21 14:31:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:33:27.599124"
    },
    {
      "arxiv_id": "2503.17181v1",
      "title": "LLMs Love Python: A Study of LLMs' Bias for Programming Languages and Libraries",
      "title_zh": "LLMs偏爱Python：关于大语言模型对编程语言及库的偏好研究",
      "authors": [
        "Lukas Twist",
        "Jie M. Zhang",
        "Mark Harman",
        "Don Syme",
        "Joost Noppen",
        "Detlef Nauck"
      ],
      "abstract": "Programming language and library choices are crucial to software reliability\nand security. Poor or inconsistent choices can lead to increased technical\ndebt, security vulnerabilities, and even catastrophic failures in\nsafety-critical systems. As Large Language Models (LLMs) play an increasing\nrole in code generation, it is essential to understand how they make these\ndecisions. However, little is known about their preferences when selecting\nprogramming languages and libraries for different coding tasks. To fill this\ngap, this study provides the first in-depth investigation into LLM preferences\nfor programming languages and libraries used when generating code. We assess\nthe preferences of eight diverse LLMs by prompting them to complete various\ncoding tasks, including widely-studied benchmarks and the more practical task\nof generating the initial structural code for new projects (a crucial step that\noften determines a project's language or library choices).\n  Our findings reveal that LLMs heavily favour Python when solving\nlanguage-agnostic problems, using it in 90%-97% of cases for benchmark tasks.\nEven when generating initial project code where Python is not a suitable\nlanguage, it remains the most-used language in 58% of instances. Moreover, LLMs\ncontradict their own language recommendations in 83% of project initialisation\ntasks, raising concerns about their reliability in guiding language selection.\nSimilar biases toward well-established libraries further create serious\ndiscoverability challenges for newer open-source projects. These results\nhighlight the need to improve LLMs' adaptability to diverse programming\ncontexts and to develop mechanisms for mitigating programming language and\nlibrary bias.",
      "tldr_zh": "该研究首次系统分析了大型语言模型(LLMs)在代码生成时对编程语言和库的选择偏好。研究发现LLMs存在明显的Python偏向性：在语言无关的基准任务中，90%-97%的生成代码使用Python；即使在Python不适用的情况下，仍有58%的新项目初始化代码错误地选择了Python。更严重的是，83%的情况下模型生成的代码与其自身推荐的语言建议相矛盾，同时对知名库的偏好也阻碍了新开源项目的发现。这些发现揭示了当前LLMs在编程语言选择可靠性方面的重大缺陷，亟需开发机制来缓解这种偏见。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "12 pages, 1 figure",
      "pdf_url": "http://arxiv.org/pdf/2503.17181v1",
      "published_date": "2025-03-21 14:29:35 UTC",
      "updated_date": "2025-03-21 14:29:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:35:46.333259"
    },
    {
      "arxiv_id": "2503.17167v2",
      "title": "DiTEC-WDN: A Large-Scale Dataset of Hydraulic Scenarios across Multiple Water Distribution Networks",
      "title_zh": "DiTEC-WDN：跨多供水管网的大规模水力场景数据集",
      "authors": [
        "Huy Truong",
        "Andrés Tello",
        "Alexander Lazovik",
        "Victoria Degeler"
      ],
      "abstract": "Privacy restrictions hinder the sharing of real-world Water Distribution\nNetwork (WDN) models, limiting the application of emerging data-driven machine\nlearning, which typically requires extensive observations. To address this\nchallenge, we propose the dataset DiTEC-WDN that comprises 36,000 unique\nscenarios simulated over either short-term (24 hours) or long-term (1 year)\nperiods. We constructed this dataset using an automated pipeline that optimizes\ncrucial parameters (e.g., pressure, flow rate, and demand patterns),\nfacilitates large-scale simulations, and records discrete, synthetic but\nhydraulically realistic states under standard conditions via rule validation\nand post-hoc analysis. With a total of 228 million generated graph-based\nstates, DiTEC-WDN can support a variety of machine-learning tasks, including\ngraph-level, node-level, and link-level regression, as well as time-series\nforecasting. This contribution, released under a public license, encourages\nopen scientific research in the critical water sector, eliminates the risk of\nexposing sensitive data, and fulfills the need for a large-scale water\ndistribution network benchmark for study comparisons and scenario analysis.",
      "tldr_zh": "该研究提出了DiTEC-WDN数据集，这是一个包含3.6万种独特水力场景的大规模合成数据集，覆盖短期（24小时）和长期（1年）模拟周期。通过自动化管道优化关键参数（如压力、流量和需求模式），该数据集生成了2.28亿个基于图的离散状态，支持图级、节点级和链路级回归以及时间序列预测等机器学习任务。作为公开许可的资源，DiTEC-WDN解决了真实供水网络(WDN)数据难以共享的问题，为水行业研究提供了标准化基准，同时避免了敏感数据泄露风险。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Submitted to Nature Scientific Data. Huy Truong and Andr\\'es Tello\n  contributed equally to this work. For the dataset, see\n  https://huggingface.co/datasets/rugds/ditec-wdn",
      "pdf_url": "http://arxiv.org/pdf/2503.17167v2",
      "published_date": "2025-03-21 14:14:03 UTC",
      "updated_date": "2025-03-24 14:40:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:36:11.646551"
    },
    {
      "arxiv_id": "2503.18968v1",
      "title": "MedAgent-Pro: Towards Multi-modal Evidence-based Medical Diagnosis via Reasoning Agentic Workflow",
      "title_zh": "MedAgent-Pro：基于多模态循证医学诊断的推理智能体工作流",
      "authors": [
        "Ziyue Wang",
        "Junde Wu",
        "Chang Han Low",
        "Yueming Jin"
      ],
      "abstract": "Developing reliable AI systems to assist human clinicians in multi-modal\nmedical diagnosis has long been a key objective for researchers. Recently,\nMulti-modal Large Language Models (MLLMs) have gained significant attention and\nachieved success across various domains. With strong reasoning capabilities and\nthe ability to perform diverse tasks based on user instructions, they hold\ngreat potential for enhancing medical diagnosis. However, directly applying\nMLLMs to the medical domain still presents challenges. They lack detailed\nperception of visual inputs, limiting their ability to perform quantitative\nimage analysis, which is crucial for medical diagnostics. Additionally, MLLMs\noften exhibit hallucinations and inconsistencies in reasoning, whereas clinical\ndiagnoses must adhere strictly to established criteria. To address these\nchallenges, we propose MedAgent-Pro, an evidence-based reasoning agentic system\ndesigned to achieve reliable, explainable, and precise medical diagnoses. This\nis accomplished through a hierarchical workflow: at the task level,\nknowledge-based reasoning generate reliable diagnostic plans for specific\ndiseases following retrieved clinical criteria. While at the case level,\nmultiple tool agents process multi-modal inputs, analyze different indicators\naccording to the plan, and provide a final diagnosis based on both quantitative\nand qualitative evidence. Comprehensive experiments on both 2D and 3D medical\ndiagnosis tasks demonstrate the superiority and effectiveness of MedAgent-Pro,\nwhile case studies further highlight its reliability and interpretability. The\ncode is available at https://github.com/jinlab-imvr/MedAgent-Pro.",
      "tldr_zh": "该研究提出了MedAgent-Pro，一个基于证据推理的多模态医疗诊断代理系统，通过分层工作流解决现有MLLMs在医疗领域应用中的视觉感知不足和推理幻觉问题。系统采用知识驱动推理生成疾病诊断计划，并通过多工具代理处理多模态输入，结合定量与定性证据得出最终诊断。实验表明，该系统在2D和3D医疗诊断任务中表现出优越性能，同时案例研究验证了其可靠性和可解释性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18968v1",
      "published_date": "2025-03-21 14:04:18 UTC",
      "updated_date": "2025-03-21 14:04:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:36:25.756144"
    },
    {
      "arxiv_id": "2503.17132v1",
      "title": "Temporal-Guided Spiking Neural Networks for Event-Based Human Action Recognition",
      "title_zh": "基于时序引导的脉冲神经网络在事件驱动人体动作识别中的应用",
      "authors": [
        "Siyuan Yang",
        "Shilin Lu",
        "Shizheng Wang",
        "Meng Hwa Er",
        "Zengwei Zheng",
        "Alex C. Kot"
      ],
      "abstract": "This paper explores the promising interplay between spiking neural networks\n(SNNs) and event-based cameras for privacy-preserving human action recognition\n(HAR). The unique feature of event cameras in capturing only the outlines of\nmotion, combined with SNNs' proficiency in processing spatiotemporal data\nthrough spikes, establishes a highly synergistic compatibility for event-based\nHAR. Previous studies, however, have been limited by SNNs' ability to process\nlong-term temporal information, essential for precise HAR. In this paper, we\nintroduce two novel frameworks to address this: temporal segment-based SNN\n(\\textit{TS-SNN}) and 3D convolutional SNN (\\textit{3D-SNN}). The\n\\textit{TS-SNN} extracts long-term temporal information by dividing actions\ninto shorter segments, while the \\textit{3D-SNN} replaces 2D spatial elements\nwith 3D components to facilitate the transmission of temporal information. To\npromote further research in event-based HAR, we create a dataset,\n\\textit{FallingDetection-CeleX}, collected using the high-resolution CeleX-V\nevent camera $(1280 \\times 800)$, comprising 7 distinct actions. Extensive\nexperimental results show that our proposed frameworks surpass state-of-the-art\nSNN methods on our newly collected dataset and three other neuromorphic\ndatasets, showcasing their effectiveness in handling long-range temporal\ninformation for event-based HAR.",
      "tldr_zh": "本文提出两种新型脉冲神经网络(SNN)框架——时间分段SNN(TS-SNN)和3D卷积SNN(3D-SNN)，用于解决基于事件相机的人类动作识别(HAR)中长时序信息处理的难题。TS-SNN通过将动作分割为短时序片段提取特征，而3D-SNN采用3D卷积结构增强时空信息传递。研究团队还发布了使用CeleX-V事件相机采集的高分辨率数据集FallingDetection-CeleX。实验表明，这两个框架在四个神经形态数据集上均超越了现有SNN方法的性能，有效提升了长时序动作识别的准确性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR",
        "cs.NE"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17132v1",
      "published_date": "2025-03-21 13:31:16 UTC",
      "updated_date": "2025-03-21 13:31:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:37:13.304820"
    },
    {
      "arxiv_id": "2503.17125v2",
      "title": "LaMOuR: Leveraging Language Models for Out-of-Distribution Recovery in Reinforcement Learning",
      "title_zh": "LaMOuR：利用语言模型实现强化学习中的分布外恢复",
      "authors": [
        "Chan Kim",
        "Seung-Woo Seo",
        "Seong-Woo Kim"
      ],
      "abstract": "Deep Reinforcement Learning (DRL) has demonstrated strong performance in\nrobotic control but remains susceptible to out-of-distribution (OOD) states,\noften resulting in unreliable actions and task failure. While previous methods\nhave focused on minimizing or preventing OOD occurrences, they largely neglect\nrecovery once an agent encounters such states. Although the latest research has\nattempted to address this by guiding agents back to in-distribution states,\ntheir reliance on uncertainty estimation hinders scalability in complex\nenvironments. To overcome this limitation, we introduce Language Models for\nOut-of-Distribution Recovery (LaMOuR), which enables recovery learning without\nrelying on uncertainty estimation. LaMOuR generates dense reward codes that\nguide the agent back to a state where it can successfully perform its original\ntask, leveraging the capabilities of LVLMs in image description, logical\nreasoning, and code generation. Experimental results show that LaMOuR\nsubstantially enhances recovery efficiency across diverse locomotion tasks and\neven generalizes effectively to complex environments, including humanoid\nlocomotion and mobile manipulation, where existing methods struggle. The code\nand supplementary materials are available at https://lamour-rl.github.io/.",
      "tldr_zh": "该研究提出LaMOuR框架，利用语言模型(LVLMs)解决强化学习中的分布外(OOD)状态恢复问题。不同于依赖不确定性估计的传统方法，LaMOuR通过生成密集奖励代码引导智能体返回可执行任务的状态，结合了LVLMs在图像描述、逻辑推理和代码生成方面的优势。实验表明，该方法在多种运动任务中显著提升恢复效率，并能推广到人形运动和移动操作等复杂环境，优于现有方法。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "14 pages, 17 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.17125v2",
      "published_date": "2025-03-21 13:20:39 UTC",
      "updated_date": "2025-03-24 06:16:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:37:15.921552"
    },
    {
      "arxiv_id": "2503.17116v1",
      "title": "The CASTLE 2024 Dataset: Advancing the Art of Multimodal Understanding",
      "title_zh": "CASTLE 2024数据集：推动多模态理解艺术的前沿发展",
      "authors": [
        "Luca Rossetto",
        "Werner Bailer",
        "Duc-Tien Dang-Nguyen",
        "Graham Healy",
        "Björn Þór Jónsson",
        "Onanong Kongmeesub",
        "Hoang-Bao Le",
        "Stevan Rudinac",
        "Klaus Schöffmann",
        "Florian Spiess",
        "Allie Tran",
        "Minh-Triet Tran",
        "Quang-Linh Tran",
        "Cathal Gurrin"
      ],
      "abstract": "Egocentric video has seen increased interest in recent years, as it is used\nin a range of areas. However, most existing datasets are limited to a single\nperspective. In this paper, we present the CASTLE 2024 dataset, a multimodal\ncollection containing ego- and exo-centric (i.e., first- and third-person\nperspective) video and audio from 15 time-aligned sources, as well as other\nsensor streams and auxiliary data. The dataset was recorded by volunteer\nparticipants over four days in a fixed location and includes the point of view\nof 10 participants, with an additional 5 fixed cameras providing an exocentric\nperspective. The entire dataset contains over 600 hours of UHD video recorded\nat 50 frames per second. In contrast to other datasets, CASTLE 2024 does not\ncontain any partial censoring, such as blurred faces or distorted audio. The\ndataset is available via https://castle-dataset.github.io/.",
      "tldr_zh": "该研究发布了CASTLE 2024多模态数据集，突破了现有数据集单一视角的限制。该数据集包含15个时间对齐的第一人称和第三人称视角的视频与音频流，以及多种传感器数据，由10名参与者在固定场景中采集，总时长超过600小时的50fps超高清视频。相比现有数据集，其独特价值在于完全保留了原始数据（无面部模糊或音频扭曲处理），为多模态理解研究提供了更全面的基准。",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.CV",
        "cs.IR"
      ],
      "primary_category": "cs.MM",
      "comment": "7 pages, 6 figures, dataset available via\n  https://castle-dataset.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2503.17116v1",
      "published_date": "2025-03-21 13:01:07 UTC",
      "updated_date": "2025-03-21 13:01:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:37:29.450926"
    },
    {
      "arxiv_id": "2503.17095v1",
      "title": "FFaceNeRF: Few-shot Face Editing in Neural Radiance Fields",
      "title_zh": "FFaceNeRF：神经辐射场中的少样本人脸编辑",
      "authors": [
        "Kwan Yun",
        "Chaelin Kim",
        "Hangyeul Shin",
        "Junyong Noh"
      ],
      "abstract": "Recent 3D face editing methods using masks have produced high-quality edited\nimages by leveraging Neural Radiance Fields (NeRF). Despite their impressive\nperformance, existing methods often provide limited user control due to the use\nof pre-trained segmentation masks. To utilize masks with a desired layout, an\nextensive training dataset is required, which is challenging to gather. We\npresent FFaceNeRF, a NeRF-based face editing technique that can overcome the\nchallenge of limited user control due to the use of fixed mask layouts. Our\nmethod employs a geometry adapter with feature injection, allowing for\neffective manipulation of geometry attributes. Additionally, we adopt latent\nmixing for tri-plane augmentation, which enables training with a few samples.\nThis facilitates rapid model adaptation to desired mask layouts, crucial for\napplications in fields like personalized medical imaging or creative face\nediting. Our comparative evaluations demonstrate that FFaceNeRF surpasses\nexisting mask based face editing methods in terms of flexibility, control, and\ngenerated image quality, paving the way for future advancements in customized\nand high-fidelity 3D face editing. The code is available on the\n{\\href{https://kwanyun.github.io/FFaceNeRF_page/}{project-page}}.",
      "tldr_zh": "该研究提出了FFaceNeRF，一种基于Neural Radiance Fields (NeRF)的少样本3D人脸编辑方法，解决了现有方法因依赖预训练分割掩码而导致的用户控制受限问题。该方法通过几何适配器(geometry adapter)实现几何属性有效调控，并采用潜在混合(latent mixing)技术进行tri-plane增强，仅需少量样本即可训练模型适应定制化掩码布局。实验表明，FFaceNeRF在灵活性、控制精度和生成质量上优于现有基于掩码的编辑方法，为个性化医疗成像和创意人脸编辑等应用提供了新方案。",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV",
        "68T45, 68U05",
        "I.3.3; I.3.8"
      ],
      "primary_category": "cs.GR",
      "comment": "CVPR2025, 11 pages, 14 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.17095v1",
      "published_date": "2025-03-21 12:24:58 UTC",
      "updated_date": "2025-03-21 12:24:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:38:15.969739"
    },
    {
      "arxiv_id": "2503.17089v1",
      "title": "Does a Rising Tide Lift All Boats? Bias Mitigation for AI-based CMR Segmentation",
      "title_zh": "水涨船高？基于AI的心脏磁共振图像分割偏差缓解研究",
      "authors": [
        "Tiarna Lee",
        "Esther Puyol-Antón",
        "Bram Ruijsink",
        "Miaojing Shi",
        "Andrew P. King"
      ],
      "abstract": "Artificial intelligence (AI) is increasingly being used for medical imaging\ntasks. However, there can be biases in the resulting models, particularly when\nthey were trained using imbalanced training datasets. One such example has been\nthe strong race bias effect in cardiac magnetic resonance (CMR) image\nsegmentation models. Although this phenomenon has been reported in a number of\npublications, little is known about the effectiveness of bias mitigation\nalgorithms in this domain. We aim to investigate the impact of common bias\nmitigation methods to address bias between Black and White subjects in AI-based\nCMR segmentation models. Specifically, we use oversampling, importance\nreweighing and Group DRO as well as combinations of these techniques to\nmitigate the race bias. Furthermore, motivated by recent findings on the root\ncauses of AI-based CMR segmentation bias, we evaluate the same methods using\nmodels trained and evaluated on cropped CMR images. We find that bias can be\nmitigated using oversampling, significantly improving performance for the\nunderrepresented Black subjects whilst not significantly reducing the majority\nWhite subjects' performance. Group DRO also improves performance for Black\nsubjects but not significantly, while reweighing decreases performance for\nBlack subjects. Using a combination of oversampling and Group DRO also improves\nperformance for Black subjects but not significantly. Using cropped images\nincreases performance for both races and reduces the bias, whilst adding\noversampling as a bias mitigation technique with cropped images reduces the\nbias further.",
      "tldr_zh": "该研究探讨了人工智能在心脏磁共振（CMR）图像分割中的种族偏差问题，比较了过采样（oversampling）、重要性重加权（reweighing）和Group DRO等偏差缓解方法的效果。实验发现，过采样能显著提高对黑人受试者的分割性能，同时不影响白人受试者的表现，而重加权反而会降低黑人受试者的性能。此外，使用裁剪后的CMR图像不仅提升了两个种族的分割效果，还减少了偏差，结合过采样技术可进一步降低偏差。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17089v1",
      "published_date": "2025-03-21 12:17:43 UTC",
      "updated_date": "2025-03-21 12:17:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:38:04.113032"
    },
    {
      "arxiv_id": "2503.17085v1",
      "title": "Deterministic AI Agent Personality Expression through Standard Psychological Diagnostics",
      "title_zh": "确定性AI智能体人格表达：基于标准化心理诊断方法",
      "authors": [
        "J. M. Diederik Kruijssen",
        "Nicholas Emmons"
      ],
      "abstract": "Artificial intelligence (AI) systems powered by large language models have\nbecome increasingly prevalent in modern society, enabling a wide range of\napplications through natural language interaction. As AI agents proliferate in\nour daily lives, their generic and uniform expressiveness presents a\nsignificant limitation to their appeal and adoption. Personality expression\nrepresents a key prerequisite for creating more human-like and distinctive AI\nsystems. We show that AI models can express deterministic and consistent\npersonalities when instructed using established psychological frameworks, with\nvarying degrees of accuracy depending on model capabilities. We find that more\nadvanced models like GPT-4o and o1 demonstrate the highest accuracy in\nexpressing specified personalities across both Big Five and Myers-Briggs\nassessments, and further analysis suggests that personality expression emerges\nfrom a combination of intelligence and reasoning capabilities. Our results\nreveal that personality expression operates through holistic reasoning rather\nthan question-by-question optimization, with response-scale metrics showing\nhigher variance than test-scale metrics. Furthermore, we find that model\nfine-tuning affects communication style independently of personality expression\naccuracy. These findings establish a foundation for creating AI agents with\ndiverse and consistent personalities, which could significantly enhance\nhuman-AI interaction across applications from education to healthcare, while\nadditionally enabling a broader range of more unique AI agents. The ability to\nquantitatively assess and implement personality expression in AI systems opens\nnew avenues for research into more relatable, trustworthy, and ethically\ndesigned AI.",
      "tldr_zh": "该研究证明大型语言模型(LLMs)可通过标准心理诊断框架（如大五人格和MBTI）实现确定性人格表达。实验发现GPT-4o等先进模型在人格特质表达上最准确，其机制源于模型的整体推理能力而非逐题优化。研究还揭示模型微调会独立影响沟通风格而非人格准确性，为构建具有差异化人格的AI代理奠定了量化基础。这一突破可显著提升从教育到医疗等领域的人机交互体验，并为开发更具亲和力、可信度的伦理AI开辟新路径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY",
        "cs.HC"
      ],
      "primary_category": "cs.LG",
      "comment": "25 pages, 8 figures, 4 tables; appeared in ADI (March 2025)",
      "pdf_url": "http://arxiv.org/pdf/2503.17085v1",
      "published_date": "2025-03-21 12:12:05 UTC",
      "updated_date": "2025-03-21 12:12:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:38:27.652223"
    },
    {
      "arxiv_id": "2503.17070v1",
      "title": "A Thorough Assessment of the Non-IID Data Impact in Federated Learning",
      "title_zh": "联邦学习中非独立同分布数据影响的全面评估",
      "authors": [
        "Daniel M. Jimenez-Gutierrez",
        "Mehrdad Hassanzadeh",
        "Aris Anagnostopoulos",
        "Ioannis Chatzigiannakis",
        "Andrea Vitaletti"
      ],
      "abstract": "Federated learning (FL) allows collaborative machine learning (ML) model\ntraining among decentralized clients' information, ensuring data privacy. The\ndecentralized nature of FL deals with non-independent and identically\ndistributed (non-IID) data. This open problem has notable consequences, such as\ndecreased model performance and more significant convergence times. Despite its\nimportance, experimental studies systematically addressing all types of data\nheterogeneity (a.k.a. non-IIDness) remain scarce. We aim to fill this gap by\nassessing and quantifying the non-IID effect through a thorough empirical\nanalysis. We use the Hellinger Distance (HD) to measure differences in\ndistribution among clients. Our study benchmarks four state-of-the-art\nstrategies for handling non-IID data, including label, feature, quantity, and\nspatiotemporal skewness, under realistic and controlled conditions. This is the\nfirst comprehensive analysis of the spatiotemporal skew effect in FL. Our\nfindings highlight the significant impact of label and spatiotemporal skew\nnon-IID types on FL model performance, with notable performance drops occurring\nat specific HD thresholds. Additionally, the FL performance is heavily affected\nmainly when the non-IIDness is extreme. Thus, we provide recommendations for FL\nresearch to tackle data heterogeneity effectively. Our work represents the most\nextensive examination of non-IIDness in FL, offering a robust foundation for\nfuture research.",
      "tldr_zh": "该论文对联邦学习(FL)中的非独立同分布(non-IID)数据问题进行了系统性评估。研究采用Hellinger距离(HD)量化客户端间的数据分布差异，首次全面分析了包括标签偏斜、特征偏斜、数量偏斜和时空偏斜在内的四种非IID类型。实验结果表明，标签偏斜和时空偏斜对FL模型性能影响最为显著，且当非IID程度超过特定HD阈值时会出现明显的性能下降。该研究为处理FL数据异质性提供了基准测试框架和实用建议。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17070v1",
      "published_date": "2025-03-21 11:53:36 UTC",
      "updated_date": "2025-03-21 11:53:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:39:05.903212"
    },
    {
      "arxiv_id": "2503.17069v1",
      "title": "PVChat: Personalized Video Chat with One-Shot Learning",
      "title_zh": "PVChat：基于单样本学习的个性化视频聊天系统",
      "authors": [
        "Yufei Shi",
        "Weilong Yan",
        "Gang Xu",
        "Yumeng Li",
        "Yuchen Li",
        "Zhenxi Li",
        "Fei Richard Yu",
        "Ming Li",
        "Si Yong Yeo"
      ],
      "abstract": "Video large language models (ViLLMs) excel in general video understanding,\ne.g., recognizing activities like talking and eating, but struggle with\nidentity-aware comprehension, such as \"Wilson is receiving chemotherapy\" or\n\"Tom is discussing with Sarah\", limiting their applicability in smart\nhealthcare and smart home environments. To address this limitation, we propose\na one-shot learning framework PVChat, the first personalized ViLLM that enables\nsubject-aware question answering (QA) from a single video for each subject. Our\napproach optimizes a Mixture-of-Heads (MoH) enhanced ViLLM on a synthetically\naugmented video-QA dataset, leveraging a progressive image-to-video learning\nstrategy. Specifically, we introduce an automated augmentation pipeline that\nsynthesizes identity-preserving positive samples and retrieves hard negatives\nfrom existing video corpora, generating a diverse training dataset with four QA\ntypes: existence, appearance, action, and location inquiries. To enhance\nsubject-specific learning, we propose a ReLU Routing MoH attention mechanism,\nalongside two novel objectives: (1) Smooth Proximity Regularization for\nprogressive learning through exponential distance scaling and (2) Head\nActivation Enhancement for balanced attention routing. Finally, we adopt a\ntwo-stage training strategy, transitioning from image pre-training to video\nfine-tuning, enabling a gradual learning process from static attributes to\ndynamic representations. We evaluate PVChat on diverse datasets covering\nmedical scenarios, TV series, anime, and real-world footage, demonstrating its\nsuperiority in personalized feature understanding after learning from a single\nvideo, compared to state-of-the-art ViLLMs.",
      "tldr_zh": "该研究提出了PVChat，首个支持单样本学习（one-shot learning）的个性化视频大语言模型（ViLLM），能够基于单个视频实现人物感知的问答任务。该方法通过混合注意力头（Mixture-of-Heads）增强的ViLLM架构，结合自动数据增强流程生成身份保持的合成样本，并创新性地提出ReLU路由注意力机制和渐进式图像到视频的两阶段训练策略。实验表明，PVChat在医疗、影视等多样化场景中，仅需学习单个视频即可显著优于现有ViLLM模型，实现了对人物存在、外观、动作和位置等个性化特征的精准理解。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17069v1",
      "published_date": "2025-03-21 11:50:06 UTC",
      "updated_date": "2025-03-21 11:50:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:39:29.628082"
    },
    {
      "arxiv_id": "2503.17061v1",
      "title": "Replay4NCL: An Efficient Memory Replay-based Methodology for Neuromorphic Continual Learning in Embedded AI Systems",
      "title_zh": "Replay4NCL：面向嵌入式AI系统的神经形态持续学习高效记忆回放方法",
      "authors": [
        "Mishal Fatima Minhas",
        "Rachmad Vidya Wicaksana Putra",
        "Falah Awwad",
        "Osman Hasan",
        "Muhammad Shafique"
      ],
      "abstract": "Neuromorphic Continual Learning (NCL) paradigm leverages Spiking Neural\nNetworks (SNNs) to enable continual learning (CL) capabilities for AI systems\nto adapt to dynamically changing environments. Currently, the state-of-the-art\nemploy a memory replay-based method to maintain the old knowledge. However,\nthis technique relies on long timesteps and compression-decompression steps,\nthereby incurring significant latency and energy overheads, which are not\nsuitable for tightly-constrained embedded AI systems (e.g., mobile\nagents/robotics). To address this, we propose Replay4NCL, a novel efficient\nmemory replay-based methodology for enabling NCL in embedded AI systems.\nSpecifically, Replay4NCL compresses the latent data (old knowledge), then\nreplays them during the NCL training phase with small timesteps, to minimize\nthe processing latency and energy consumption. To compensate the information\nloss from reduced spikes, we adjust the neuron threshold potential and learning\nrate settings. Experimental results on the class-incremental scenario with the\nSpiking Heidelberg Digits (SHD) dataset show that Replay4NCL can preserve old\nknowledge with Top-1 accuracy of 90.43% compared to 86.22% from the\nstate-of-the-art, while effectively learning new tasks, achieving 4.88x latency\nspeed-up, 20% latent memory saving, and 36.43% energy saving. These results\nhighlight the potential of our Replay4NCL methodology to further advances NCL\ncapabilities for embedded AI systems.",
      "tldr_zh": "该研究提出Replay4NCL，一种面向嵌入式AI系统的高效神经形态持续学习(NCL)方法。该方法通过压缩潜在数据并在训练阶段以小时间步重放，显著降低了传统基于记忆重放方法存在的延迟和能耗问题，同时通过调整神经元阈值电位和学习率来补偿信息损失。在Spiking Heidelberg Digits数据集上的实验表明，Replay4NCL在保持旧知识方面达到90.43%的Top-1准确率(优于现有方法的86.22%)，同时实现了4.88倍的延迟加速、20%的潜在内存节省和36.43%的能耗降低。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "Accepted at the 62th Design Automation Conference (DAC) 2025, June\n  2025, San Francisco, CA, USA",
      "pdf_url": "http://arxiv.org/pdf/2503.17061v1",
      "published_date": "2025-03-21 11:33:22 UTC",
      "updated_date": "2025-03-21 11:33:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:39:48.346667"
    },
    {
      "arxiv_id": "2503.17055v1",
      "title": "Data-Driven Optimization of EV Charging Station Placement Using Causal Discovery",
      "title_zh": "基于因果发现的电动汽车充电站布局数据驱动优化",
      "authors": [
        "Julius Stephan Junker",
        "Rong Hu",
        "Ziyue Li",
        "Wolfgang Ketter"
      ],
      "abstract": "This paper addresses the critical challenge of optimizing electric vehicle\ncharging station placement through a novel data-driven methodology employing\ncausal discovery techniques. While traditional approaches prioritize economic\nfactors or power grid constraints, they often neglect empirical charging\npatterns that ultimately determine station utilization. We analyze extensive\ncharging data from Palo Alto and Boulder (337,344 events across 100 stations)\nto uncover latent relationships between station characteristics and\nutilization. Applying structural learning algorithms (NOTEARS and DAGMA) to\nthis data reveals that charging demand is primarily determined by three\nfactors: proximity to amenities, EV registration density, and adjacency to\nhigh-traffic routes. These findings, consistent across multiple algorithms and\nurban contexts, challenge conventional infrastructure distribution strategies.\nWe develop an optimization framework that translates these insights into\nactionable placement recommendations, identifying locations likely to\nexperience high utilization based on the discovered dependency structures. The\nresulting site selection model prioritizes strategic clustering in high-amenity\nareas with substantial EV populations rather than uniform spatial distribution.\nOur approach contributes a framework that integrates empirical charging\nbehavior into infrastructure planning, potentially enhancing both station\nutilization and user convenience. By focusing on data-driven insights instead\nof theoretical distribution models, we provide a more effective strategy for\nexpanding charging networks that can adjust to various stages of EV market\ndevelopment.",
      "tldr_zh": "本研究提出了一种基于因果发现技术的数据驱动方法，用于优化电动汽车充电站选址。通过分析Palo Alto和Boulder的337,344次充电数据，研究发现充电需求主要由三个因素决定：邻近便利设施、EV注册密度和高流量路线附近。与传统选址策略不同，该研究开发了一个优化框架，建议在高便利设施区域和EV密集区进行战略集群式布局，而非均匀分布。这种数据驱动方法相比传统理论模型能更有效地提升充电站利用率和用户便利性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Under review of IEEE CASE 2025; This is also the master thesis\n  project from Julius supervised by Dr. Ziyue Li",
      "pdf_url": "http://arxiv.org/pdf/2503.17055v1",
      "published_date": "2025-03-21 11:15:02 UTC",
      "updated_date": "2025-03-21 11:15:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:40:07.907062"
    },
    {
      "arxiv_id": "2503.17046v1",
      "title": "HAPI: A Model for Learning Robot Facial Expressions from Human Preferences",
      "title_zh": "HAPI：一种基于人类偏好的机器人面部表情学习模型",
      "authors": [
        "Dongsheng Yang",
        "Qianying Liu",
        "Wataru Sato",
        "Takashi Minato",
        "Chaoran Liu",
        "Shin'ya Nishida"
      ],
      "abstract": "Automatic robotic facial expression generation is crucial for human-robot\ninteraction, as handcrafted methods based on fixed joint configurations often\nyield rigid and unnatural behaviors. Although recent automated techniques\nreduce the need for manual tuning, they tend to fall short by not adequately\nbridging the gap between human preferences and model predictions-resulting in a\ndeficiency of nuanced and realistic expressions due to limited degrees of\nfreedom and insufficient perceptual integration. In this work, we propose a\nnovel learning-to-rank framework that leverages human feedback to address this\ndiscrepancy and enhanced the expressiveness of robotic faces. Specifically, we\nconduct pairwise comparison annotations to collect human preference data and\ndevelop the Human Affective Pairwise Impressions (HAPI) model, a Siamese\nRankNet-based approach that refines expression evaluation. Results obtained via\nBayesian Optimization and online expression survey on a 35-DOF android platform\ndemonstrate that our approach produces significantly more realistic and\nsocially resonant expressions of Anger, Happiness, and Surprise than those\ngenerated by baseline and expert-designed methods. This confirms that our\nframework effectively bridges the gap between human preferences and model\npredictions while robustly aligning robotic expression generation with human\naffective responses.",
      "tldr_zh": "该研究提出HAPI模型，通过基于人类偏好的学习排序框架改进机器人面部表情生成。采用Siamese RankNet网络结构分析人类对表情的成对偏好数据，结合贝叶斯优化在35自由度仿生机器人平台上验证。实验表明，该方法生成的愤怒、快乐和惊讶表情比基线方法和专家设计方法更真实且具有社会共鸣性，有效缩小了人类偏好与模型预测之间的差距。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17046v1",
      "published_date": "2025-03-21 11:04:01 UTC",
      "updated_date": "2025-03-21 11:04:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:40:23.971183"
    },
    {
      "arxiv_id": "2503.17039v1",
      "title": "Summarization Metrics for Spanish and Basque: Do Automatic Scores and LLM-Judges Correlate with Humans?",
      "title_zh": "西班牙语与巴斯克语摘要评估指标：自动评分与LLM评判是否与人类判断相关？",
      "authors": [
        "Jeremy Barnes",
        "Naiara Perez",
        "Alba Bonet-Jover",
        "Begoña Altuna"
      ],
      "abstract": "Studies on evaluation metrics and LLM-as-a-Judge models for automatic text\nsummarization have largely been focused on English, limiting our understanding\nof their effectiveness in other languages. Through our new dataset BASSE\n(BAsque and Spanish Summarization Evaluation), we address this situation by\ncollecting human judgments on 2,040 abstractive summaries in Basque and\nSpanish, generated either manually or by five LLMs with four different prompts.\nFor each summary, annotators evaluated five criteria on a 5-point Likert scale:\ncoherence, consistency, fluency, relevance, and 5W1H. We use these data to\nreevaluate traditional automatic metrics used for evaluating summaries, as well\nas several LLM-as-a-Judge models that show strong performance on this task in\nEnglish. Our results show that currently proprietary judge LLMs have the\nhighest correlation with human judgments, followed by criteria-specific\nautomatic metrics, while open-sourced judge LLMs perform poorly. We release\nBASSE and our code publicly, along with the first large-scale Basque\nsummarization dataset containing 22,525 news articles with their subheads.",
      "tldr_zh": "该研究通过构建BASSE数据集（包含2040份巴斯克语和西班牙语摘要），首次系统评估了自动摘要指标和LLM评判模型在非英语语言中的表现。研究发现，商业LLM评判模型与人工评估的相关性最高，其次是特定标准自动指标，而开源LLM表现较差。研究还发布了首个包含22,525篇巴斯克语新闻的大规模摘要数据集，填补了该语言资源空白。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17039v1",
      "published_date": "2025-03-21 10:52:20 UTC",
      "updated_date": "2025-03-21 10:52:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:40:57.744977"
    },
    {
      "arxiv_id": "2503.17426v1",
      "title": "Enhanced Smart Contract Reputability Analysis using Multimodal Data Fusion on Ethereum",
      "title_zh": "基于多模态数据融合的以太坊智能合约可信度增强分析",
      "authors": [
        "Cyrus Malik",
        "Josef Bajada",
        "Joshua Ellul"
      ],
      "abstract": "The evaluation of smart contract reputability is essential to foster trust in\ndecentralized ecosystems. However, existing methods that rely solely on static\ncode analysis or transactional data, offer limited insight into evolving\ntrustworthiness. We propose a multimodal data fusion framework that integrates\nstatic code features with transactional data to enhance reputability\nprediction. Our framework initially focuses on static code analysis, utilizing\nGAN-augmented opcode embeddings to address class imbalance, achieving 97.67%\naccuracy and a recall of 0.942 in detecting illicit contracts, surpassing\ntraditional oversampling methods. This forms the crux of a reputability-centric\nfusion strategy, where combining static and transactional data improves recall\nby 7.25% over single-source models, demonstrating robust performance across\nvalidation sets. By providing a holistic view of smart contract behaviour, our\napproach enhances the model's ability to assess reputability, identify\nfraudulent activities, and predict anomalous patterns. These capabilities\ncontribute to more accurate reputability assessments, proactive risk\nmitigation, and enhanced blockchain security.",
      "tldr_zh": "该研究提出了一种基于多模态数据融合的智能合约可信度分析框架，通过整合静态代码特征和交易数据来提升评估效果。采用GAN增强的opcode嵌入技术解决类别不平衡问题，在非法合约检测中达到97.67%准确率和0.942召回率，优于传统过采样方法。实验表明，结合静态和交易数据的融合策略使召回率比单源模型提升7.25%，有效识别欺诈行为并预测异常模式。该框架为区块链安全提供了更准确的可信度评估和主动风险缓解能力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.ET"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17426v1",
      "published_date": "2025-03-21 10:45:17 UTC",
      "updated_date": "2025-03-21 10:45:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:41:07.718736"
    },
    {
      "arxiv_id": "2503.17034v1",
      "title": "An Attentive Representative Sample Selection Strategy Combined with Balanced Batch Training for Skin Lesion Segmentation",
      "title_zh": "一种结合平衡批量训练的注意力代表性样本选择策略用于皮肤病变分割",
      "authors": [
        "Stephen Lloyd-Brown",
        "Susan Francis",
        "Caroline Hoad",
        "Penny Gowland",
        "Karen Mullinger",
        "Andrew French",
        "Xin Chen"
      ],
      "abstract": "An often overlooked problem in medical image segmentation research is the\neffective selection of training subsets to annotate from a complete set of\nunlabelled data. Many studies select their training sets at random, which may\nlead to suboptimal model performance, especially in the minimal supervision\nsetting where each training image has a profound effect on performance\noutcomes. This work aims to address this issue. We use prototypical contrasting\nlearning and clustering to extract representative and diverse samples for\nannotation. We improve upon prior works with a bespoke cluster-based image\nselection process. Additionally, we introduce the concept of unsupervised\nbalanced batch dataloading to medical image segmentation, which aims to improve\nmodel learning with minimally annotated data. We evaluated our method on a\npublic skin lesion dataset (ISIC 2018) and compared it to another\nstate-of-the-art data sampling method. Our method achieved superior performance\nin a low annotation budget scenario.",
      "tldr_zh": "本文提出了一种结合注意力代表性样本选择策略和均衡批次训练的皮肤病变分割方法。针对医学图像分割中训练样本选择随机性导致性能不佳的问题，该方法通过原型对比学习和聚类技术选取最具代表性和多样性的样本进行标注。研究创新性地提出了基于聚类的图像选择流程，并首次将无监督均衡批次数据加载引入医学图像分割领域。在ISIC 2018皮肤病变数据集上的实验表明，该方法在低标注预算情况下优于其他先进采样方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ISBI 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.17034v1",
      "published_date": "2025-03-21 10:42:22 UTC",
      "updated_date": "2025-03-21 10:42:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:41:28.813827"
    },
    {
      "arxiv_id": "2503.17030v1",
      "title": "Exploring the Efficacy of Partial Denoising Using Bit Plane Slicing for Enhanced Fracture Identification: A Comparative Study of Deep Learning-Based Approaches and Handcrafted Feature Extraction Techniques",
      "title_zh": "《探究位平面切片部分去噪在提升骨折识别中的效能：基于深度学习方法与手工特征提取技术的对比研究》",
      "authors": [
        "Snigdha Paul",
        "Sambit Mallick",
        "Anindya Sen"
      ],
      "abstract": "Computer vision has transformed medical diagnosis, treatment, and research\nthrough advanced image processing and machine learning techniques. Fracture\nclassification, a critical area in healthcare, has greatly benefited from these\nadvancements, yet accurate detection is challenged by complex patterns and\nimage noise. Bit plane slicing enhances medical images by reducing noise\ninterference and extracting informative features. This research explores\npartial denoising techniques to provide practical solutions for improved\nfracture analysis, ultimately enhancing patient care. The study explores deep\nlearning model DenseNet and handcrafted feature extraction. Decision Tree and\nRandom Forest, were employed to train and evaluate distinct image\nrepresentations. These include the original image, the concatenation of the\nfour bit planes from the LSB as well as MSB, the fully denoised image, and an\nimage consisting of 6 bit planes from MSB and 2 denoised bit planes from LSB.\nThe purpose of forming these diverse image representations is to analyze SNR as\nwell as classification accuracy and identify the bit planes that contain the\nmost informative features. Moreover, the study delves into the significance of\npartial denoising techniques in preserving crucial features, leading to\nimprovements in classification results. Notably, this study shows that\nemploying the Random Forest classifier, the partially denoised image\nrepresentation exhibited a testing accuracy of 95.61% surpassing the\nperformance of other image representations. The outcomes of this research\nprovide valuable insights into the development of efficient preprocessing,\nfeature extraction and classification approaches for fracture identification.\nBy enhancing diagnostic accuracy, these advancements hold the potential to\npositively impact patient care and overall medical outcomes.",
      "tldr_zh": "该研究探讨了基于位平面分割(Bit Plane Slicing)的部分去噪技术在骨折识别中的应用效果。通过比较DenseNet深度学习模型与传统手工特征提取方法（决策树和随机森林），分析了不同图像表示方式（包括原始图像、LSB/MSB位平面组合、全去噪图像及混合位平面图像）对分类性能的影响。研究发现，采用部分去噪的6位MSB平面+2位去噪LSB平面表示方式，结合随机森林分类器可达到95.61%的测试准确率，优于其他方法。该成果为医学图像预处理和特征提取提供了新思路，有助于提升骨折诊断精度和患者治疗效果。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17030v1",
      "published_date": "2025-03-21 10:39:21 UTC",
      "updated_date": "2025-03-21 10:39:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:41:48.062972"
    },
    {
      "arxiv_id": "2503.17025v1",
      "title": "A Guide to Bayesian Networks Software Packages for Structure and Parameter Learning -- 2025 Edition",
      "title_zh": "贝叶斯网络结构与参数学习软件包指南——2025版",
      "authors": [
        "Joverlyn Gaudillo",
        "Nicole Astrologo",
        "Fabio Stella",
        "Enzo Acerbi",
        "Francesco Canonaco"
      ],
      "abstract": "A representation of the cause-effect mechanism is needed to enable artificial\nintelligence to represent how the world works. Bayesian Networks (BNs) have\nproven to be an effective and versatile tool for this task. BNs require\nconstructing a structure of dependencies among variables and learning the\nparameters that govern these relationships. These tasks, referred to as\nstructural learning and parameter learning, are actively investigated by the\nresearch community, with several algorithms proposed and no single method\nhaving established itself as standard. A wide range of software, tools, and\npackages have been developed for BNs analysis and made available to academic\nresearchers and industry practitioners. As a consequence of having no\none-size-fits-all solution, moving the first practical steps and getting\noriented into this field is proving to be challenging to outsiders and\nbeginners. In this paper, we review the most relevant tools and software for\nBNs structural and parameter learning to date, providing our subjective\nrecommendations directed to an audience of beginners. In addition, we provide\nan extensive easy-to-consult overview table summarizing all software packages\nand their main features. By improving the reader understanding of which\navailable software might best suit their needs, we improve accessibility to the\nfield and make it easier for beginners to take their first step into it.",
      "tldr_zh": "这篇论文是2025版贝叶斯网络(BNs)软件包的实用指南，重点评述了用于结构学习和参数学习的各类工具。由于该领域缺乏统一标准方法，作者针对初学者提供了主观推荐方案，并整理了包含各软件核心功能的对比表格。通过系统梳理现有工具，本文降低了BNs领域的学习门槛，帮助研究者和从业者根据需求选择合适的分析软件。",
      "categories": [
        "cs.AI",
        "I.2"
      ],
      "primary_category": "cs.AI",
      "comment": "11 pages, 1 figure",
      "pdf_url": "http://arxiv.org/pdf/2503.17025v1",
      "published_date": "2025-03-21 10:36:11 UTC",
      "updated_date": "2025-03-21 10:36:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:42:08.076999"
    },
    {
      "arxiv_id": "2503.17018v1",
      "title": "Symbolic Audio Classification via Modal Decision Tree Learning",
      "title_zh": "符号化音频分类：基于模态决策树学习的方法",
      "authors": [
        "Enrico Marzano",
        "Giovanni Pagliarini",
        "Riccardo Pasini",
        "Guido Sciavicco",
        "Ionel Eduard Stan"
      ],
      "abstract": "The range of potential applications of acoustic analysis is wide.\nClassification of sounds, in particular, is a typical machine learning task\nthat received a lot of attention in recent years. The most common approaches to\nsound classification are sub-symbolic, typically based on neural networks, and\nresult in black-box models with high performances but very low transparency. In\nthis work, we consider several audio tasks, namely, age and gender recognition,\nemotion classification, and respiratory disease diagnosis, and we approach them\nwith a symbolic technique, that is, (modal) decision tree learning. We prove\nthat such tasks can be solved using the same symbolic pipeline, that allows to\nextract simple rules with very high accuracy and low complexity. In principle,\nall such tasks could be associated to an autonomous conversation system, which\ncould be useful in different contexts, such as an automatic reservation agent\nfor an hospital or a clinic.",
      "tldr_zh": "该论文提出了一种基于模态决策树学习（modal decision tree learning）的符号化音频分类方法，用以解决传统神经网络方法在声音分类任务中存在的不透明性问题。研究针对年龄/性别识别、情绪分类和呼吸疾病诊断等音频任务，证明了符号化方法能够提取简单规则并保持高准确率和低复杂度。这种可解释的符号化流程可应用于自动化对话系统，如医院预约代理等实际场景。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS",
        "68T05",
        "I.2.6"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17018v1",
      "published_date": "2025-03-21 10:27:16 UTC",
      "updated_date": "2025-03-21 10:27:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:42:39.007289"
    },
    {
      "arxiv_id": "2503.17013v1",
      "title": "Developing Critical Thinking in Second Language Learners: Exploring Generative AI like ChatGPT as a Tool for Argumentative Essay Writing",
      "title_zh": "培养第二语言学习者的批判性思维：探索ChatGPT等生成式AI在议论文写作中的应用工具",
      "authors": [
        "Simon Suh",
        "Jihyuk Bang",
        "Ji Woo Han"
      ],
      "abstract": "This study employs the Paul-Elder Critical Thinking Model and Tan's\nargumentative writing framework to create a structured methodology. This\nmethodology, ChatGPT Guideline for Critical Argumentative Writing (CGCAW)\nframework, integrates the models with ChatGPT's capabilities to guide L2\nlearners in utilizing ChatGPT to enhance their critical thinking skills. A\nquantitative experiment was conducted with 10 participants from a state\nuniversity, divided into experimental and control groups. The experimental\ngroup utilized the CGCAW framework, while the control group used ChatGPT\nwithout specific guidelines. Participants wrote an argumentative essay within a\n40-minute timeframe, and essays were evaluated by three assessors: ChatGPT,\nGrammarly, and a course instructor. Results indicated that the experimental\ngroup showed improvements in clarity, logical coherence, and use of evidence,\ndemonstrating ChatGPT's potential to enhance specific aspects of argumentative\nwriting. However, the control group performed better in overall language\nmechanics and articulation of main arguments, indicating areas where the CGCAW\nframework could be further refined. This study highlights the need for further\nresearch to optimize the use of AI tools like ChatGPT in L2 learning\nenvironments to enhance critical thinking and writing skills.",
      "tldr_zh": "本研究提出CGCAW框架，结合Paul-Elder批判性思维模型和Tan的议论文写作框架，指导二语学习者使用ChatGPT提升批判性思维。实验发现，采用该框架的实验组在论述清晰度、逻辑连贯性和论据使用方面表现更优，而对照组在语言机制和核心论点阐述上更具优势，表明AI工具在二语写作教学中具有差异化应用价值。研究为优化生成式AI在语言学习中的使用提供了实证依据。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "I.2.7; K.3.1"
      ],
      "primary_category": "cs.HC",
      "comment": "12 pages, 3 figures. Uses Paul-Elder Critical Thinking Model and\n  Tan's argumentative writing framework. Includes an experimental study with 10\n  participants",
      "pdf_url": "http://arxiv.org/pdf/2503.17013v1",
      "published_date": "2025-03-21 10:22:58 UTC",
      "updated_date": "2025-03-21 10:22:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:42:57.967058"
    },
    {
      "arxiv_id": "2503.17002v1",
      "title": "Targetless 6DoF Calibration of LiDAR and 2D Scanning Radar Based on Cylindrical Occupancy",
      "title_zh": "基于圆柱体占用的LiDAR与2D扫描雷达无目标6自由度标定",
      "authors": [
        "Weimin Wang",
        "Yu Du",
        "Ting Yang",
        "Yu Liu"
      ],
      "abstract": "Owing to the capability for reliable and all-weather long-range sensing, the\nfusion of LiDAR and Radar has been widely applied to autonomous vehicles for\nrobust perception. In practical operation, well manually calibrated extrinsic\nparameters, which are crucial for the fusion of multi-modal sensors, may drift\ndue to the vibration. To address this issue, we present a novel targetless\ncalibration approach, termed LiRaCo, for the extrinsic 6DoF calibration of\nLiDAR and Radar sensors. Although both types of sensors can obtain geometric\ninformation, bridging the geometric correspondences between multi-modal data\nwithout any clues of explicit artificial markers is nontrivial, mainly due to\nthe low vertical resolution of scanning Radar. To achieve the targetless\ncalibration, LiRaCo leverages a spatial occupancy consistency between LiDAR\npoint clouds and Radar scans in a common cylindrical representation,\nconsidering the increasing data sparsity with distance for both sensors.\nSpecifically, LiRaCo expands the valid Radar scanned pixels into 3D occupancy\ngrids to constrain LiDAR point clouds based on spatial consistency.\nConsequently, a cost function involving extrinsic calibration parameters is\nformulated based on the spatial overlap of 3D grids and LiDAR points. Extrinsic\nparameters are finally estimated by optimizing the cost function. Comprehensive\nquantitative and qualitative experiments on two real outdoor datasets with\ndifferent LiDAR sensors demonstrate the feasibility and accuracy of the\nproposed method. The source code will be publicly available.",
      "tldr_zh": "该研究提出了一种无需人工标记的目标物校准方法LiRaCo，用于LiDAR和2D扫描雷达的6自由度（6DoF）外参标定。LiRaCo通过将LiDAR点云和雷达扫描数据在圆柱坐标系下的空间占用一致性进行建模，利用雷达扫描像素扩展为3D占用网格来约束LiDAR点云，从而构建基于空间重叠的成本函数并优化外参。实验表明，该方法在真实户外数据集上具有较高的精度和可行性，适用于自动驾驶车辆的多模态传感器融合。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17002v1",
      "published_date": "2025-03-21 10:09:04 UTC",
      "updated_date": "2025-03-21 10:09:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:43:42.160900"
    },
    {
      "arxiv_id": "2503.17424v1",
      "title": "Data to Decisions: A Computational Framework to Identify skill requirements from Advertorial Data",
      "title_zh": "数据驱动决策：基于广告数据分析技能需求的智能计算框架",
      "authors": [
        "Aakash Singh",
        "Anurag Kanaujia",
        "Vivek Kumar Singh"
      ],
      "abstract": "Among the factors of production, human capital or skilled manpower is the one\nthat keeps evolving and adapts to changing conditions and resources. This\nadaptability makes human capital the most crucial factor in ensuring a\nsustainable growth of industry/sector. As new technologies are developed and\nadopted, the new generations are required to acquire skills in newer\ntechnologies in order to be employable. At the same time professionals are\nrequired to upskill and reskill themselves to remain relevant in the industry.\nThere is however no straightforward method to identify the skill needs of the\nindustry at a given point of time. Therefore, this paper proposes a data to\ndecision framework that can successfully identify the desired skill set in a\ngiven area by analysing the advertorial data collected from popular online job\nportals and supplied as input to the framework. The proposed framework uses\ntechniques of statistical analysis, data mining and natural language processing\nfor the purpose. The applicability of the framework is demonstrated on CS&IT\njob advertisement data from India. The analytical results not only provide\nuseful insights about current state of skill needs in CS&IT industry but also\nprovide practical implications to prospective job applicants, training\nagencies, and institutions of higher education & professional training.",
      "tldr_zh": "该研究提出了一种基于广告数据的\"Data-to-Decisions\"计算框架，用于动态识别行业技能需求。该框架通过统计分析和自然语言处理技术挖掘在线招聘信息，能有效跟踪CS&IT领域的最新技术技能要求。实证分析印度IT行业招聘数据表明，该框架不仅揭示了当前行业技能需求现状，还可为求职者、培训机构和高校提供决策支持。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17424v1",
      "published_date": "2025-03-21 09:49:31 UTC",
      "updated_date": "2025-03-21 09:49:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:43:41.933196"
    },
    {
      "arxiv_id": "2503.16983v1",
      "title": "Enabling Versatile Controls for Video Diffusion Models",
      "title_zh": "为视频扩散模型赋能多样化控制",
      "authors": [
        "Xu Zhang",
        "Hao Zhou",
        "Haoming Qin",
        "Xiaobin Lu",
        "Jiaxing Yan",
        "Guanzhong Wang",
        "Zeyu Chen",
        "Yi Liu"
      ],
      "abstract": "Despite substantial progress in text-to-video generation, achieving precise\nand flexible control over fine-grained spatiotemporal attributes remains a\nsignificant unresolved challenge in video generation research. To address these\nlimitations, we introduce VCtrl (also termed PP-VCtrl), a novel framework\ndesigned to enable fine-grained control over pre-trained video diffusion models\nin a unified manner. VCtrl integrates diverse user-specified control\nsignals-such as Canny edges, segmentation masks, and human keypoints-into\npretrained video diffusion models via a generalizable conditional module\ncapable of uniformly encoding multiple types of auxiliary signals without\nmodifying the underlying generator. Additionally, we design a unified control\nsignal encoding pipeline and a sparse residual connection mechanism to\nefficiently incorporate control representations. Comprehensive experiments and\nhuman evaluations demonstrate that VCtrl effectively enhances controllability\nand generation quality. The source code and pre-trained models are publicly\navailable and implemented using the PaddlePaddle framework at\nhttp://github.com/PaddlePaddle/PaddleMIX/tree/develop/ppdiffusers/examples/ppvctrl.",
      "tldr_zh": "该研究提出VCtrl（又称PP-VCtrl）框架，旨在为预训练视频扩散模型提供细粒度的时空控制能力。通过设计通用条件模块，该框架能统一编码多种用户指定信号（如Canny边缘、分割掩码和人体关键点），而无需修改基础生成模型。实验表明，VCtrl通过稀疏残差连接机制有效提升了生成质量和控制灵活性，相关代码已在PaddlePaddle框架开源。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Codes and Supplementary Material:\n  http://github.com/PaddlePaddle/PaddleMIX/tree/develop/ppdiffusers/examples/ppvctrl",
      "pdf_url": "http://arxiv.org/pdf/2503.16983v1",
      "published_date": "2025-03-21 09:48:00 UTC",
      "updated_date": "2025-03-21 09:48:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:43:57.005602"
    },
    {
      "arxiv_id": "2503.16980v1",
      "title": "Token Dynamics: Towards Efficient and Dynamic Video Token Representation for Video Large Language Models",
      "title_zh": "Token Dynamics：面向视频大语言模型的高效动态视频令牌表征方法",
      "authors": [
        "Haichao Zhang",
        "Zhuowei Li",
        "Dimitris Metaxas",
        "Yun Fu"
      ],
      "abstract": "Token-based video representation has emerged as a promising approach for\nenabling large language models to interpret video content. However, existing\ntoken reduction techniques, such as token pruning and token merging, often\ndisrupt essential spatial-temporal positional embeddings, failing to adequately\nbalance computational efficiency with fewer tokens. Consequently, these methods\nresult in relatively lengthy token sequences, limiting their applicability in\nscenarios requiring extreme token compression, such as video large language\nmodels. In this paper, we introduce the novel task of extreme short token\nreduction, aiming to represent extensive video sequences with a minimal number\nof tokens. To address this challenge, we propose Token Dynamics, a new video\nrepresentation framework that dynamically reduces token count while preserving\nspatial-temporal coherence. Specifically, we disentangle video representations\nby separating visual embeddings from grid-level motion information, structuring\nthem into: 1. a concise token base, created by clustering tokens that describe\nobject-level content; 2. a token dynamics map, capturing detailed\nspatial-temporal motion patterns across grids. Furthermore, we introduce a\ncross-dynamics attention mechanism that integrates motion features into the\ntoken base without increasing token length, thereby maintaining compactness and\nspatial-temporal integrity. The experiments demonstrate a reduction of token\ncount to merely 0.07% of the original tokens, with only a minor performance\ndrop of 1.13%. Additionally, we propose two novel subtasks within extreme token\nreduction (fixed-length and adaptive-length compression), both effectively\nrepresenting long token sequences for video-language tasks. Our method offers\nsignificantly lower theoretical complexity, fewer tokens, and enhanced\nthroughput, thus providing an efficient solution for video LLMs.",
      "tldr_zh": "该论文提出Token Dynamics框架，用于解决视频大语言模型（Video LLMs）中视频表征的极端短token压缩问题。通过将视频表征解耦为描述物体级内容的token基和捕捉网格级运动模式的动态图，并创新性地引入跨动态注意力机制，在仅保留0.07%原始token的情况下性能仅下降1.13%。该框架不仅实现了固定长度和自适应长度两种新型极端压缩子任务，还显著降低了理论复杂度，为视频-语言任务提供了高吞吐量的高效解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16980v1",
      "published_date": "2025-03-21 09:46:31 UTC",
      "updated_date": "2025-03-21 09:46:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:44:20.535463"
    },
    {
      "arxiv_id": "2503.16978v1",
      "title": "Real-Time Diffusion Policies for Games: Enhancing Consistency Policies with Q-Ensembles",
      "title_zh": "游戏实时扩散策略：通过Q集成增强一致性策略",
      "authors": [
        "Ruoqi Zhang",
        "Ziwei Luo",
        "Jens Sjölund",
        "Per Mattsson",
        "Linus Gisslén",
        "Alessandro Sestini"
      ],
      "abstract": "Diffusion models have shown impressive performance in capturing complex and\nmulti-modal action distributions for game agents, but their slow inference\nspeed prevents practical deployment in real-time game environments. While\nconsistency models offer a promising approach for one-step generation, they\noften suffer from training instability and performance degradation when applied\nto policy learning. In this paper, we present CPQE (Consistency Policy with\nQ-Ensembles), which combines consistency models with Q-ensembles to address\nthese challenges.CPQE leverages uncertainty estimation through Q-ensembles to\nprovide more reliable value function approximations, resulting in better\ntraining stability and improved performance compared to classic double\nQ-network methods. Our extensive experiments across multiple game scenarios\ndemonstrate that CPQE achieves inference speeds of up to 60 Hz -- a significant\nimprovement over state-of-the-art diffusion policies that operate at only 20 Hz\n-- while maintaining comparable performance to multi-step diffusion approaches.\nCPQE consistently outperforms state-of-the-art consistency model approaches,\nshowing both higher rewards and enhanced training stability throughout the\nlearning process. These results indicate that CPQE offers a practical solution\nfor deploying diffusion-based policies in games and other real-time\napplications where both multi-modal behavior modeling and rapid inference are\ncritical requirements.",
      "tldr_zh": "本文提出CPQE（带Q-Ensemble的一致性策略），通过将一致性模型(consistency models)与Q集成方法相结合，解决了扩散策略在实时游戏环境中推理速度慢的问题。该方法利用Q集成进行不确定性估计，提供更可靠的价值函数近似，相比经典双Q网络方法显著提升了训练稳定性和性能。实验表明，CPQE在保持与多步扩散策略相当性能的同时，推理速度提升至60Hz（远超现有20Hz的扩散策略），并在多个游戏场景中展现出更高的奖励获取能力和训练稳定性，为实时应用中的多模态行为建模提供了实用解决方案。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16978v1",
      "published_date": "2025-03-21 09:45:59 UTC",
      "updated_date": "2025-03-21 09:45:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:44:38.265610"
    },
    {
      "arxiv_id": "2503.16976v1",
      "title": "GeoT: Geometry-guided Instance-dependent Transition Matrix for Semi-supervised Tooth Point Cloud Segmentation",
      "title_zh": "GeoT：面向半监督牙齿点云分割的几何引导型实例依赖转移矩阵",
      "authors": [
        "Weihao Yu",
        "Xiaoqing Guo",
        "Chenxin Li",
        "Yifan Liu",
        "Yixuan Yuan"
      ],
      "abstract": "Achieving meticulous segmentation of tooth point clouds from intra-oral scans\nstands as an indispensable prerequisite for various orthodontic applications.\nGiven the labor-intensive nature of dental annotation, a significant amount of\ndata remains unlabeled, driving increasing interest in semi-supervised\napproaches. One primary challenge of existing semi-supervised medical\nsegmentation methods lies in noisy pseudo labels generated for unlabeled data.\nTo address this challenge, we propose GeoT, the first framework that employs\ninstance-dependent transition matrix (IDTM) to explicitly model noise in pseudo\nlabels for semi-supervised dental segmentation. Specifically, to handle the\nextensive solution space of IDTM arising from tens of thousands of dental\npoints, we introduce tooth geometric priors through two key components:\npoint-level geometric regularization (PLGR) to enhance consistency between\npoint adjacency relationships in 3D and IDTM spaces, and class-level geometric\nsmoothing (CLGS) to leverage the fixed spatial distribution of tooth categories\nfor optimal IDTM estimation. Extensive experiments performed on the public\nTeeth3DS dataset and private dataset demonstrate that our method can make full\nutilization of unlabeled data to facilitate segmentation, achieving performance\ncomparable to fully supervised methods with only $20\\%$ of the labeled data.",
      "tldr_zh": "该研究提出GeoT框架，首次在牙齿点云半监督分割中采用实例依赖转移矩阵(IDTM)来显式建模伪标签噪声。针对牙齿点云数据特性，创新性地引入几何先验知识：通过点级几何正则化(PLGR)确保3D空间与IDTM空间邻接关系一致，并利用类级几何平滑(CLGS)基于牙齿类别的固定空间分布优化IDTM估计。实验表明，仅需20%标注数据即可达到全监督方法性能，在公开Teeth3DS和私有数据集上验证了该框架能有效利用未标注数据提升分割精度。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "IPMI2025",
      "pdf_url": "http://arxiv.org/pdf/2503.16976v1",
      "published_date": "2025-03-21 09:43:57 UTC",
      "updated_date": "2025-03-21 09:43:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:45:04.693642"
    },
    {
      "arxiv_id": "2503.16974v1",
      "title": "Assessing Consistency and Reproducibility in the Outputs of Large Language Models: Evidence Across Diverse Finance and Accounting Tasks",
      "title_zh": "评估大型语言模型输出的一致性与可复现性：跨多样化财务与会计任务的实证研究",
      "authors": [
        "Julian Junyan Wang",
        "Victor Xiaoqi Wang"
      ],
      "abstract": "This study provides the first comprehensive assessment of consistency and\nreproducibility in Large Language Model (LLM) outputs in finance and accounting\nresearch. We evaluate how consistently LLMs produce outputs given identical\ninputs through extensive experimentation with 50 independent runs across five\ncommon tasks: classification, sentiment analysis, summarization, text\ngeneration, and prediction. Using three OpenAI models (GPT-3.5-turbo,\nGPT-4o-mini, and GPT-4o), we generate over 3.4 million outputs from diverse\nfinancial source texts and data, covering MD&As, FOMC statements, finance news\narticles, earnings call transcripts, and financial statements. Our findings\nreveal substantial but task-dependent consistency, with binary classification\nand sentiment analysis achieving near-perfect reproducibility, while complex\ntasks show greater variability. More advanced models do not consistently\ndemonstrate better consistency and reproducibility, with task-specific patterns\nemerging. LLMs significantly outperform expert human annotators in consistency\nand maintain high agreement even where human experts significantly disagree. We\nfurther find that simple aggregation strategies across 3-5 runs dramatically\nimprove consistency. Simulation analysis reveals that despite measurable\ninconsistency in LLM outputs, downstream statistical inferences remain\nremarkably robust. These findings address concerns about what we term\n\"G-hacking,\" the selective reporting of favorable outcomes from multiple\nGenerative AI runs, by demonstrating that such risks are relatively low for\nfinance and accounting tasks.",
      "tldr_zh": "本研究首次系统评估了大型语言模型(LLM)在金融与会计领域的输出一致性与可复现性。通过五项典型任务(分类、情感分析、摘要、文本生成和预测)的50次独立实验，发现LLM在二元分类和情感分析任务中表现近乎完美的一致性，而复杂任务则存在较大波动。值得注意的是，更先进的模型未必表现更稳定，且LLM的一致性显著优于人类专家。研究还表明，3-5次运行的简单聚合策略可大幅提升一致性，尽管存在输出波动，但下游统计推断仍保持稳健。这些发现缓解了\"G-hacking\"(选择性报告生成式AI有利结果)的担忧，证实此类风险在金融会计任务中相对较低。",
      "categories": [
        "q-fin.GN",
        "cs.AI",
        "cs.CE",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "q-fin.GN",
      "comment": "96 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.16974v1",
      "published_date": "2025-03-21 09:43:37 UTC",
      "updated_date": "2025-03-21 09:43:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:45:25.546666"
    },
    {
      "arxiv_id": "2503.16973v1",
      "title": "ARFlow: Human Action-Reaction Flow Matching with Physical Guidance",
      "title_zh": "ARFlow：基于物理引导的人体动作-反应流匹配",
      "authors": [
        "Wentao Jiang",
        "Jingya Wang",
        "Haotao Lu",
        "Kaiyang Ji",
        "Baoxiong Jia",
        "Siyuan Huang",
        "Ye Shi"
      ],
      "abstract": "Human action-reaction synthesis, a fundamental challenge in modeling causal\nhuman interactions, plays a critical role in applications ranging from virtual\nreality to social robotics. While diffusion-based models have demonstrated\npromising performance, they exhibit two key limitations for interaction\nsynthesis: reliance on complex noise-to-reaction generators with intricate\nconditional mechanisms, and frequent physical violations in generated motions.\nTo address these issues, we propose Action-Reaction Flow Matching (ARFlow), a\nnovel framework that establishes direct action-to-reaction mappings,\neliminating the need for complex conditional mechanisms. Our approach\nintroduces two key innovations: an x1-prediction method that directly outputs\nhuman motions instead of velocity fields, enabling explicit constraint\nenforcement; and a training-free, gradient-based physical guidance mechanism\nthat effectively prevents body penetration artifacts during sampling. Extensive\nexperiments on NTU120 and Chi3D datasets demonstrate that ARFlow not only\noutperforms existing methods in terms of Fr\\'echet Inception Distance and\nmotion diversity but also significantly reduces body collisions, as measured by\nour new Intersection Volume and Intersection Frequency metrics.",
      "tldr_zh": "该研究提出了ARFlow框架，用于解决人类动作-反应合成中的物理合理性难题。该方法通过创新的x1-prediction直接生成人体运动（而非速度场），并结合基于梯度的无训练物理引导机制，有效避免了身体穿透等物理违规现象。实验表明，ARFlow在NTU120和Chi3D数据集上不仅显著提升了Fr\\'echet Inception Distance指标和运动多样性，还通过新提出的Intersection Volume和Intersection Frequency指标验证了其减少身体碰撞的优越性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Computer Vision and Pattern Recognition (cs.CV); Artificial\n  Intelligence (cs.AI)",
      "pdf_url": "http://arxiv.org/pdf/2503.16973v1",
      "published_date": "2025-03-21 09:41:24 UTC",
      "updated_date": "2025-03-21 09:41:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:45:38.477122"
    },
    {
      "arxiv_id": "2503.16956v1",
      "title": "From Faces to Voices: Learning Hierarchical Representations for High-quality Video-to-Speech",
      "title_zh": "《从面部到语音：学习高质量视频转语音的分层表征》",
      "authors": [
        "Ji-Hoon Kim",
        "Jeongsoo Choi",
        "Jaehun Kim",
        "Chaeyoung Jung",
        "Joon Son Chung"
      ],
      "abstract": "The objective of this study is to generate high-quality speech from silent\ntalking face videos, a task also known as video-to-speech synthesis. A\nsignificant challenge in video-to-speech synthesis lies in the substantial\nmodality gap between silent video and multi-faceted speech. In this paper, we\npropose a novel video-to-speech system that effectively bridges this modality\ngap, significantly enhancing the quality of synthesized speech. This is\nachieved by learning of hierarchical representations from video to speech.\nSpecifically, we gradually transform silent video into acoustic feature spaces\nthrough three sequential stages -- content, timbre, and prosody modeling. In\neach stage, we align visual factors -- lip movements, face identity, and facial\nexpressions -- with corresponding acoustic counterparts to ensure the seamless\ntransformation. Additionally, to generate realistic and coherent speech from\nthe visual representations, we employ a flow matching model that estimates\ndirect trajectories from a simple prior distribution to the target speech\ndistribution. Extensive experiments demonstrate that our method achieves\nexceptional generation quality comparable to real utterances, outperforming\nexisting methods by a significant margin.",
      "tldr_zh": "这项研究提出了一种新颖的视频到语音合成系统，通过分层表征学习有效弥合了无声视频与多维度语音之间的模态鸿沟。该方法采用三阶段渐进式建模（内容、音色和韵律），将唇部运动、人脸身份和面部表情等视觉因素与对应的声学特征对齐。研究还引入流匹配模型来生成自然连贯的语音，实验表明该方法显著优于现有技术，生成的语音质量接近真实发音。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CV",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "CVPR 2025, demo page: https://mm.kaist.ac.kr/projects/faces2voices/",
      "pdf_url": "http://arxiv.org/pdf/2503.16956v1",
      "published_date": "2025-03-21 09:02:38 UTC",
      "updated_date": "2025-03-21 09:02:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:45:57.530825"
    },
    {
      "arxiv_id": "2503.16953v1",
      "title": "Neural-Guided Equation Discovery",
      "title_zh": "神经引导的方程发现",
      "authors": [
        "Jannis Brugger",
        "Mattia Cerrato",
        "David Richter",
        "Cedric Derstroff",
        "Daniel Maninger",
        "Mira Mezini",
        "Stefan Kramer"
      ],
      "abstract": "Deep learning approaches are becoming increasingly attractive for equation\ndiscovery. We show the advantages and disadvantages of using neural-guided\nequation discovery by giving an overview of recent papers and the results of\nexperiments using our modular equation discovery system MGMT\n($\\textbf{M}$ulti-Task $\\textbf{G}$rammar-Guided $\\textbf{M}$onte-Carlo\n$\\textbf{T}$ree Search for Equation Discovery). The system uses neural-guided\nMonte-Carlo Tree Search (MCTS) and supports both supervised and reinforcement\nlearning, with a search space defined by a context-free grammar. We summarize\nseven desirable properties of equation discovery systems, emphasizing the\nimportance of embedding tabular data sets for such learning approaches. Using\nthe modular structure of MGMT, we compare seven architectures (among them,\nRNNs, CNNs, and Transformers) for embedding tabular datasets on the auxiliary\ntask of contrastive learning for tabular data sets on an equation discovery\ntask. For almost all combinations of modules, supervised learning outperforms\nreinforcement learning. Moreover, our experiments indicate an advantage of\nusing grammar rules as action space instead of tokens. Two adaptations of MCTS\n-- risk-seeking MCTS and AmEx-MCTS -- can improve equation discovery with that\nkind of search.",
      "tldr_zh": "这篇论文提出了一种神经引导的方程发现方法，通过模块化系统MGMT（多任务语法引导蒙特卡洛树搜索）结合监督学习和强化学习来发现数学方程。研究表明，在表格数据集嵌入任务中，监督学习表现普遍优于强化学习，且使用语法规则作为动作空间比使用标记更有效。论文还提出了两种改进的MCTS方法（风险寻求MCTS和AmEx-MCTS），能有效提升方程发现的性能。",
      "categories": [
        "cs.AI",
        "I.2.6; I.1.1; G.3"
      ],
      "primary_category": "cs.AI",
      "comment": "32 pages + 4 pages appendix, 9 figures, book chapter",
      "pdf_url": "http://arxiv.org/pdf/2503.16953v1",
      "published_date": "2025-03-21 08:55:51 UTC",
      "updated_date": "2025-03-21 08:55:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:46:25.942903"
    },
    {
      "arxiv_id": "2503.16939v1",
      "title": "On-Sensor Convolutional Neural Networks with Early-Exits",
      "title_zh": "《传感器端早期退出的卷积神经网络实现》",
      "authors": [
        "Hazem Hesham Yousef Shalby",
        "Arianna De Vecchi",
        "Alice Scandelli",
        "Pietro Bartoli",
        "Diana Trojaniello",
        "Manuel Roveri",
        "Federica Villa"
      ],
      "abstract": "Tiny Machine Learning (TinyML) is a novel research field aiming at\nintegrating Machine Learning (ML) within embedded devices with limited memory,\ncomputation, and energy. Recently, a new branch of TinyML has emerged, focusing\non integrating ML directly into the sensors to further reduce the power\nconsumption of embedded devices. Interestingly, despite their state-of-the-art\nperformance in many tasks, none of the current solutions in the literature aims\nto optimize the implementation of Convolutional Neural Networks (CNNs)\noperating directly into sensors. In this paper, we introduce for the first time\nin the literature the optimized design and implementation of Depth-First CNNs\noperating on the Intelligent Sensor Processing Unit (ISPU) within an Inertial\nMeasurement Unit (IMU) by STMicroelectronics. Our approach partitions the CNN\nbetween the ISPU and the microcontroller (MCU) and employs an Early-Exit\nmechanism to stop the computations on the IMU when enough confidence about the\nresults is achieved, hence significantly reducing power consumption. When using\na NUCLEO-F411RE board, this solution achieved an average current consumption of\n4.8 mA, marking an 11% reduction compared to the regular inference pipeline on\nthe MCU, while having equal accuracy.",
      "tldr_zh": "该论文首次提出了在STMicroelectronics惯性测量单元(IMU)的智能传感器处理单元(ISPU)上优化实现深度优先卷积神经网络(CNN)的方法。通过将CNN分割部署在ISPU和微控制器(MCU)之间，并采用Early-Exit机制在结果达到足够置信度时提前终止IMU端计算，显著降低了功耗。实验表明，该方法在NUCLEO-F411RE开发板上实现了4.8mA的平均电流消耗，比传统MCU推理流程降低11%功耗，同时保持相同精度。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Presented at IEEE SSCI",
      "pdf_url": "http://arxiv.org/pdf/2503.16939v1",
      "published_date": "2025-03-21 08:31:07 UTC",
      "updated_date": "2025-03-21 08:31:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:46:39.807735"
    },
    {
      "arxiv_id": "2503.16938v1",
      "title": "Interpretable Machine Learning for Oral Lesion Diagnosis through Prototypical Instances Identification",
      "title_zh": "基于原型实例识别的可解释机器学习在口腔病变诊断中的应用",
      "authors": [
        "Alessio Cascione",
        "Mattia Setzu",
        "Federico A. Galatolo",
        "Mario G. C. A. Cimino",
        "Riccardo Guidotti"
      ],
      "abstract": "Decision-making processes in healthcare can be highly complex and\nchallenging. Machine Learning tools offer significant potential to assist in\nthese processes. However, many current methodologies rely on complex models\nthat are not easily interpretable by experts. This underscores the need to\ndevelop interpretable models that can provide meaningful support in clinical\ndecision-making. When approaching such tasks, humans typically compare the\nsituation at hand to a few key examples and representative cases imprinted in\ntheir memory. Using an approach which selects such exemplary cases and grounds\nits predictions on them could contribute to obtaining high-performing\ninterpretable solutions to such problems. To this end, we evaluate PivotTree,\nan interpretable prototype selection model, on an oral lesion detection\nproblem, specifically trying to detect the presence of neoplastic, aphthous and\ntraumatic ulcerated lesions from oral cavity images. We demonstrate the\nefficacy of using such method in terms of performance and offer a qualitative\nand quantitative comparison between exemplary cases and ground-truth prototypes\nselected by experts.",
      "tldr_zh": "本研究提出采用基于原型实例识别的可解释机器学习方法（PivotTree）用于口腔病变诊断。该方法通过选取代表性病例作为预测依据，模拟临床医生的诊断思维，可同时检测口腔中的肿瘤性、阿弗他和创伤性溃疡病变。实验表明，该模型不仅保持良好性能，其选取的原型病例与专家标注的真实原型具有高度一致性，为临床决策提供了透明可靠的辅助工具。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16938v1",
      "published_date": "2025-03-21 08:25:32 UTC",
      "updated_date": "2025-03-21 08:25:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:46:58.279077"
    },
    {
      "arxiv_id": "2503.16932v1",
      "title": "Rude Humans and Vengeful Robots: Examining Human Perceptions of Robot Retaliatory Intentions in Professional Settings",
      "title_zh": "粗鲁人类与复仇机器人：专业场景中人类对机器人报复意图的感知研究",
      "authors": [
        "Kate Letheren",
        "Nicole Robinson"
      ],
      "abstract": "Humans and robots are increasingly working in personal and professional\nsettings. In workplace settings, humans and robots may work together as\ncolleagues, potentially leading to social expectations, or violation thereof.\nExtant research has primarily sought to understand social interactions and\nexpectations in personal rather than professional settings, and none of these\nstudies have examined negative outcomes arising from violations of social\nexpectations. This paper reports the results of a 2x3 online experiment that\nused a unique first-person perspective video to immerse participants in a\ncollaborative workplace setting. The results are nuanced and reveal that while\nrobots are expected to act in accordance with social expectations despite human\nbehavior, there are benefits for robots perceived as being the bigger person in\nthe face of human rudeness. Theoretical and practical implications are provided\nwhich discuss the import of these findings for the design of social robots.",
      "tldr_zh": "这项研究通过2x3在线实验探讨了职场环境中人类对机器人报复意图的感知。研究发现，尽管人们期望机器人始终遵守社交规范，但当面对人类粗鲁行为时，被感知为\"宽容大度\"的机器人反而能获得积极评价。该研究采用第一人称视角视频模拟协作场景，为社交机器人设计提供了重要启示，指出需要平衡社会期望与应对负面互动的策略。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.RO",
      "comment": "This is the author version of the manuscript submitted to ACM\n  Transactions on Human-Robot Interaction. The final version, if accepted, will\n  be published by ACM and available via the ACM Digital Library. 12 pages, 1\n  figure, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.16932v1",
      "published_date": "2025-03-21 08:12:40 UTC",
      "updated_date": "2025-03-21 08:12:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:47:22.234224"
    },
    {
      "arxiv_id": "2503.16929v1",
      "title": "TEMPO: Temporal Preference Optimization of Video LLMs via Difficulty Scheduling and Pre-SFT Alignment",
      "title_zh": "TEMPO：通过难度调度与预SFT对齐实现视频大语言模型的时间偏好优化",
      "authors": [
        "Shicheng Li",
        "Lei Li",
        "Kun Ouyang",
        "Shuhuai Ren",
        "Yuanxin Liu",
        "Yuanxing Zhang",
        "Fuzheng Zhang",
        "Lingpeng Kong",
        "Qi Liu",
        "Xu Sun"
      ],
      "abstract": "Video Large Language Models (Video LLMs) have achieved significant success by\nleveraging a two-stage paradigm: pretraining on large-scale video-text data for\nvision-language alignment, followed by supervised fine-tuning (SFT) for\ntask-specific capabilities. However, existing approaches struggle with temporal\nreasoning due to weak temporal correspondence in the data and reliance on the\nnext-token prediction paradigm during training. To address these limitations,\nwe propose TEMPO (TEMporal Preference Optimization), a systematic framework\nthat enhances Video LLMs' temporal reasoning capabilities through Direct\nPreference Optimization (DPO). To facilitate this, we introduce an automated\npreference data generation pipeline that systematically constructs preference\npairs by selecting videos that are rich in temporal information, designing\nvideo-specific perturbation strategies, and finally evaluating model responses\non clean and perturbed video inputs. Our temporal alignment features two key\ninnovations: curriculum learning which that progressively increases\nperturbation difficulty to improve model robustness and adaptability; and\n``Pre-SFT Alignment'', applying preference optimization before instruction\ntuning to prioritize fine-grained temporal comprehension. Extensive experiments\ndemonstrate that our approach consistently improves Video LLM performance\nacross multiple benchmarks with a relatively small set of self-generated DPO\ndata. We further analyze the transferability of DPO data across architectures\nand the role of difficulty scheduling in optimization. Our findings highlight\nour TEMPO as a scalable and efficient complement to SFT-based methods, paving\nthe way for developing reliable Video LLMs.",
      "tldr_zh": "该研究提出TEMPO框架，通过难度调度和预SFT对齐来优化视频大语言模型(Video LLMs)的时间推理能力。该方法创新性地采用直接偏好优化(DPO)技术，通过自动生成包含时间扰动的偏好数据对，并结合渐进式难度增加的课程学习策略增强模型鲁棒性。实验表明，TEMPO在多个基准测试中显著提升了模型性能，同时验证了DPO数据在不同架构间的可迁移性，为发展可靠的视频语言模型提供了高效可扩展的新范式。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16929v1",
      "published_date": "2025-03-21 08:00:29 UTC",
      "updated_date": "2025-03-21 08:00:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:47:38.268771"
    },
    {
      "arxiv_id": "2503.16922v1",
      "title": "RustEvo^2: An Evolving Benchmark for API Evolution in LLM-based Rust Code Generation",
      "title_zh": "RustEvo²：基于LLM的Rust代码生成中API演化的动态基准测试框架",
      "authors": [
        "Linxi Liang",
        "Jing Gong",
        "Mingwei Liu",
        "Chong Wang",
        "Guangsheng Ou",
        "Yanlin Wang",
        "Xin Peng",
        "Zibin Zheng"
      ],
      "abstract": "Large Language Models (LLMs) have become pivotal tools for automating code\ngeneration in software development. However, these models face significant\nchallenges in producing version-aware code for rapidly evolving languages like\nRust, where frequent Application Programming Interfaces (API) changes across\nversions lead to compatibility issues and correctness errors. Existing\nbenchmarks lack systematic evaluation of how models navigate API transitions,\nrelying on labor-intensive manual curation and offering limited\nversion-specific insights. To address this gap, we present RustEvo, a novel\nframework for constructing dynamic benchmarks that evaluate the ability of LLMs\nto adapt to evolving Rust APIs. RustEvo automates dataset creation by\nsynthesizing 588 API changes (380 from Rust standard libraries, 208 from 15\nthird-party crates) into programming tasks mirroring real-world challenges.\nThese tasks cover four API evolution categories: Stabilizations, Signature\nChanges, Behavioral Changes, and Deprecations, reflecting their actual\ndistribution in the Rust ecosystem.\n  Experiments on state-of-the-art (SOTA) LLMs reveal significant performance\nvariations: models achieve a 65.8% average success rate on stabilized APIs but\nonly 38.0% on behavioral changes, highlighting difficulties in detecting\nsemantic shifts without signature alterations. Knowledge cutoff dates strongly\ninfluence performance, with models scoring 56.1% on before-cutoff APIs versus\n32.5% on after-cutoff tasks. Retrieval-Augmented Generation (RAG) mitigates\nthis gap, improving success rates by 13.5% on average for APIs released after\nmodel training. Our findings underscore the necessity of our evolution-aware\nbenchmarks to advance the adaptability of LLMs in fast-paced software\necosystems. The framework and the benchmarks are publicly released at\nhttps://github.com/SYSUSELab/RustEvo.",
      "tldr_zh": "该研究提出了RustEvo²，首个针对Rust语言API演化的动态评测框架，用于评估大语言模型(LLM)在快速迭代的编程生态中的适应能力。通过自动化构建包含588个真实API变更（覆盖标准库和第三方crate）的评测集，系统考察模型对稳定化(Stabilizations)、签名变更(Signature Changes)、行为变更(Behavioral Changes)和废弃(Deprecations)四类演化场景的处理能力。实验发现现有SOTA模型在行为变更任务上成功率仅38.0%，显著低于稳定API的65.8%，且知识截止日期导致模型对新生API的准确率下降23.6%。研究证实检索增强生成(RAG)技术能平均提升13.5%的后训练API生成准确率，为构建版本敏感的代码生成系统提供了方法论和基准支持。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16922v1",
      "published_date": "2025-03-21 07:33:59 UTC",
      "updated_date": "2025-03-21 07:33:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:48:00.532976"
    },
    {
      "arxiv_id": "2503.16921v1",
      "title": "When Preferences Diverge: Aligning Diffusion Models with Minority-Aware Adaptive DPO",
      "title_zh": "当偏好分歧时：基于少数群体感知自适应DPO的扩散模型对齐方法",
      "authors": [
        "Lingfan Zhang",
        "Chen Liu",
        "Chengming Xu",
        "Kai Hu",
        "Donghao Luo",
        "Chengjie Wang",
        "Yanwei Fu",
        "Yuan Yao"
      ],
      "abstract": "In recent years, the field of image generation has witnessed significant\nadvancements, particularly in fine-tuning methods that align models with\nuniversal human preferences. This paper explores the critical role of\npreference data in the training process of diffusion models, particularly in\nthe context of Diffusion-DPO and its subsequent adaptations. We investigate the\ncomplexities surrounding universal human preferences in image generation,\nhighlighting the subjective nature of these preferences and the challenges\nposed by minority samples in preference datasets. Through pilot experiments, we\ndemonstrate the existence of minority samples and their detrimental effects on\nmodel performance. We propose Adaptive-DPO -- a novel approach that\nincorporates a minority-instance-aware metric into the DPO objective. This\nmetric, which includes intra-annotator confidence and inter-annotator\nstability, distinguishes between majority and minority samples. We introduce an\nAdaptive-DPO loss function which improves the DPO loss in two ways: enhancing\nthe model's learning of majority labels while mitigating the negative impact of\nminority samples. Our experiments demonstrate that this method effectively\nhandles both synthetic minority data and real-world preference data, paving the\nway for more effective training methodologies in image generation tasks.",
      "tldr_zh": "这篇论文提出了一种名为**Adaptive-DPO**的新方法，用于解决扩散模型（Diffusion Models）在基于人类偏好数据训练时面临的**少数样本偏差**问题。通过引入结合**标注者内部置信度**和**标注者间稳定性**的评估指标，该方法能有效区分多数偏好和少数偏好样本。实验表明，Adaptive-DPO在提升模型对主流偏好的学习能力的同时，减少了少数样本带来的负面影响，适用于合成数据和真实偏好数据场景。该研究为图像生成任务提供了更鲁棒的训练方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16921v1",
      "published_date": "2025-03-21 07:33:44 UTC",
      "updated_date": "2025-03-21 07:33:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:48:23.130903"
    },
    {
      "arxiv_id": "2503.17421v1",
      "title": "Understanding Social Support Needs in Questions: A Hybrid Approach Integrating Semi-Supervised Learning and LLM-based Data Augmentation",
      "title_zh": "理解问题中的社会支持需求：一种融合半监督学习与基于大语言模型数据增强的混合方法",
      "authors": [
        "Junwei Kuang",
        "Liang Yang",
        "Shaoze Cui",
        "Weiguo Fan"
      ],
      "abstract": "Patients are increasingly turning to online health Q&A communities for social\nsupport to improve their well-being. However, when this support received does\nnot align with their specific needs, it may prove ineffective or even\ndetrimental. This necessitates a model capable of identifying the social\nsupport needs in questions. However, training such a model is challenging due\nto the scarcity and class imbalance issues of labeled data. To overcome these\nchallenges, we follow the computational design science paradigm to develop a\nnovel framework, Hybrid Approach for SOcial Support need classification\n(HA-SOS). HA-SOS integrates an answer-enhanced semi-supervised learning\napproach, a text data augmentation technique leveraging large language models\n(LLMs) with reliability- and diversity-aware sample selection mechanism, and a\nunified training process to automatically label social support needs in\nquestions. Extensive empirical evaluations demonstrate that HA-SOS\nsignificantly outperforms existing question classification models and\nalternative semi-supervised learning approaches. This research contributes to\nthe literature on social support, question classification, semi-supervised\nlearning, and text data augmentation. In practice, our HA-SOS framework\nfacilitates online Q&A platform managers and answerers to better understand\nusers' social support needs, enabling them to provide timely, personalized\nanswers and interventions.",
      "tldr_zh": "本研究提出了一种混合框架HA-SOS，用于识别在线健康问答社区中问题的社会支持需求。该框架结合了基于大语言模型（LLM）的文本数据增强技术、答案增强的半监督学习方法以及可靠性-多样性感知的样本选择机制，有效解决了标注数据稀缺和类别不平衡问题。实验结果表明，HA-SOS在问题分类任务中显著优于现有模型和半监督学习方法，为在线问答平台提供更精准的社会支持需求理解，助力个性化回答和干预。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CY",
      "comment": "55 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.17421v1",
      "published_date": "2025-03-21 07:25:16 UTC",
      "updated_date": "2025-03-21 07:25:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:48:38.357099"
    },
    {
      "arxiv_id": "2503.16914v1",
      "title": "A New Segment Routing method with Swap Node Selection Strategy Based on Deep Reinforcement Learning for Software Defined Network",
      "title_zh": "一种基于深度强化学习的软件定义网络交换节点选择策略新型分段路由方法",
      "authors": [
        "Miao Ye",
        "Jihao Zheng",
        "Qiuxiang Jiang",
        "Yuan Huang",
        "Ziheng Wang",
        "Yong Wang"
      ],
      "abstract": "The existing segment routing (SR) methods need to determine the routing first\nand then use path segmentation approaches to select swap nodes to form a\nsegment routing path (SRP). They require re-segmentation of the path when the\nrouting changes. Furthermore, they do not consider the flow table issuance\ntime, which cannot maximize the speed of issuance flow table. To address these\nissues, this paper establishes an optimization model that can simultaneously\nform routing strategies and path segmentation strategies for selecting the\nappropriate swap nodes to reduce flow table issuance time. It also designs an\nintelligent segment routing algorithm based on deep reinforcement learning\n(DRL-SR) to solve the proposed model. First, a traffic matrix is designed as\nthe state space for the deep reinforcement learning agent; this matrix includes\nmultiple QoS performance indicators, flow table issuance time overhead and SR\nlabel stack depth. Second, the action selection strategy and corresponding\nreward function are designed, where the agent selects the next node considering\nthe routing; in addition, the action selection strategy whether the newly added\nnode is selected as the swap node and the corresponding reward function are\ndesigned considering the time cost factor for the controller to issue the flow\ntable to the swap node. Finally, a series of experiments and their results show\nthat, compared with the existing methods, the designed segmented route\noptimization model and the intelligent solution algorithm (DRL-SR) can reduce\nthe time overhead required to complete the segmented route establishment task\nwhile optimizing performance metrics such as throughput, delays and packet\nlosses.",
      "tldr_zh": "本文提出了一种基于深度强化学习(DRL)的新型分段路由方法(DRL-SR)，用于软件定义网络(SDN)中的交换节点选择。该方法创新性地将路由策略和路径分割策略联合优化，通过设计包含多QoS指标、流表下发时间和SR标签栈深度的状态空间，以及考虑流表下发时间成本的奖励函数，实现路由建立与交换节点选择的同步决策。实验表明，相比现有方法，该方案能显著降低分段路由建立时间开销，同时提升吞吐量、延迟和丢包率等关键性能指标。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16914v1",
      "published_date": "2025-03-21 07:24:09 UTC",
      "updated_date": "2025-03-21 07:24:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:49:00.572011"
    },
    {
      "arxiv_id": "2503.16905v1",
      "title": "MAPS: A Multi-Agent Framework Based on Big Seven Personality and Socratic Guidance for Multimodal Scientific Problem Solving",
      "title_zh": "MAPS：基于大七人格与苏格拉底引导的多智能体框架在跨模态科学问题求解中的应用",
      "authors": [
        "Jian Zhang",
        "Zhiyuan Wang",
        "Zhangqi Wang",
        "Xinyu Zhang",
        "Fangzhi Xu",
        "Qika Lin",
        "Rui Mao",
        "Erik Cambria",
        "Jun Liu"
      ],
      "abstract": "Multimodal scientific problems (MSPs) involve complex issues that require the\nintegration of multiple modalities, such as text and diagrams, presenting a\nsignificant challenge in artificial intelligence. While progress has been made\nin addressing traditional scientific problems, MSPs still face two primary\nissues: the challenge of multi-modal comprehensive reasoning in scientific\nproblem-solving and the lack of reflective and rethinking capabilities. To\naddress these issues, we introduce a Multi-Agent framework based on the Big\nSeven Personality and Socratic guidance (MAPS). This framework employs seven\ndistinct agents that leverage feedback mechanisms and the Socratic method to\nguide the resolution of MSPs. To tackle the first issue, we propose a\nprogressive four-agent solving strategy, where each agent focuses on a specific\nstage of the problem-solving process. For the second issue, we introduce a\nCritic agent, inspired by Socratic questioning, which prompts critical thinking\nand stimulates autonomous learning. We conduct extensive experiments on the\nEMMA, Olympiad, and MathVista datasets, achieving promising results that\noutperform the current SOTA model by 15.84% across all tasks. Meanwhile, the\nadditional analytical experiments also verify the model's progress as well as\ngeneralization ability.",
      "tldr_zh": "该研究提出了基于大七人格理论(Big Seven Personality)和苏格拉底引导(Socratic guidance)的多智能体框架MAPS，用于解决多模态科学问题(MSPs)。框架包含七个智能体，采用渐进式四智能体解决策略，分别专注于问题解决的不同阶段，同时引入批判性智能体(Critic agent)通过苏格拉底式提问促进反思与自主学习。实验表明，MAPS在EMMA、Olympiad和MathVista数据集上表现优异，整体性能超越当前SOTA模型15.84%，验证了其有效性和泛化能力。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16905v1",
      "published_date": "2025-03-21 07:13:45 UTC",
      "updated_date": "2025-03-21 07:13:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:49:20.938285"
    },
    {
      "arxiv_id": "2503.16904v1",
      "title": "Deep Learning for Human Locomotion Analysis in Lower-Limb Exoskeletons: A Comparative Study",
      "title_zh": "深度学习在下肢外骨骼人体运动分析中的对比研究",
      "authors": [
        "Omar Coser",
        "Christian Tamantini",
        "Matteo Tortora",
        "Leonardo Furia",
        "Rosa Sicilia",
        "Loredana Zollo",
        "Paolo Soda"
      ],
      "abstract": "Wearable robotics for lower-limb assistance have become a pivotal area of\nresearch, aiming to enhance mobility for individuals with physical impairments\nor augment the performance of able-bodied users. Accurate and adaptive control\nsystems are essential to ensure seamless interaction between the wearer and the\nrobotic device, particularly when navigating diverse and dynamic terrains.\nDespite the recent advances in neural networks for time series analysis, no\nattempts have been directed towards the classification of ground conditions,\ncategorized into five classes and subsequently determining the ramp's slope and\nstair's height. In this respect, this paper presents an experimental comparison\nbetween eight deep neural network backbones to predict high-level locomotion\nparameters across diverse terrains.\n  All the models are trained on the publicly available CAMARGO 2021 dataset.\nIMU-only data equally or outperformed IMU+EMG inputs, promoting a\ncost-effective and efficient design. Indeeds, using three IMU sensors, the LSTM\nachieved high terrain classification accuracy (0.94 +- 0.04) and precise ramp\nslope (1.95 +- 0.58{\\deg}) and the CNN-LSTM a stair height (15.65 +- 7.40 mm)\nestimations. As a further contribution, SHAP analysis justified sensor\nreduction without performance loss, ensuring a lightweight setup. The system\noperates with ~2 ms inference time, supporting real-time applications. The code\nis code available at\nhttps://github.com/cosbidev/Human-Locomotion-Identification.",
      "tldr_zh": "该研究对下肢外骨骼中的人类运动分析进行了深度学习方法的对比实验，重点解决了不同地形条件下的运动参数预测问题。论文比较了8种深度神经网络模型在公开数据集CAMARGO 2021上的表现，发现仅使用三个IMU传感器的LSTM模型在五种地形分类中达到94%准确率，而CNN-LSTM组合在坡度和台阶高度预测上表现优异。研究证实IMU单独使用效果优于IMU+EMG组合，通过SHAP分析实现了传感器精简而不损失性能，系统推理时间仅约2毫秒，支持实时应用，为低成本高效的外骨骼设计提供了方案。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "F.2.2, I.2.7"
      ],
      "primary_category": "cs.RO",
      "comment": "26 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.16904v1",
      "published_date": "2025-03-21 07:12:44 UTC",
      "updated_date": "2025-03-21 07:12:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:49:44.106916"
    },
    {
      "arxiv_id": "2503.16874v1",
      "title": "MARS: A Multi-Agent Framework Incorporating Socratic Guidance for Automated Prompt Optimization",
      "title_zh": "MARS：融合苏格拉底式引导的自动提示优化多智能体框架",
      "authors": [
        "Jian Zhang",
        "Zhangqi Wang",
        "Haiping Zhu",
        "Jun Liu",
        "Qika Lin",
        "Erik Cambria"
      ],
      "abstract": "The basic question-answering format of large language models involves\ninputting a prompt and receiving a response, and the quality of the prompt\ndirectly impacts the effectiveness of the response. Automated Prompt\nOptimization (APO) aims to break free from the cognitive biases of manually\ndesigned prompts and explores a broader design space for prompts. However,\nexisting APO methods suffer from limited flexibility of fixed templates and\ninefficient search in prompt spaces as key issues. To this end, we propose a\nMulti-Agent framework Incorporating Socratic guidance (MARS), which utilizes\nmulti-agent fusion technology for automatic planning, with gradual continuous\noptimization and evaluation. Specifically, MARS comprises seven agents, each\nwith distinct functionalities, which autonomously use the Planner to devise an\noptimization path that ensures flexibility. Additionally, it employs a\nTeacher-Critic-Student Socratic dialogue pattern to iteratively optimize the\nprompts while conducting effective search. We conduct extensive experiments on\nvarious datasets to validate the effectiveness of our method, and perform\nadditional analytical experiments to assess the model's advancement as well as\nthe interpretability.",
      "tldr_zh": "该研究提出了MARS框架，一种融合苏格拉底式引导的多智能体自动提示优化(APO)系统。该方法通过7个功能各异的智能体协同工作，采用Planner自主规划优化路径，并运用Teacher-Critic-Student对话模式实现迭代式提示优化。相比现有方法，MARS突破了固定模板的限制，在多个数据集上的实验验证了其有效性，同时具备良好的可解释性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16874v1",
      "published_date": "2025-03-21 06:19:55 UTC",
      "updated_date": "2025-03-21 06:19:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:50:01.856859"
    },
    {
      "arxiv_id": "2503.16873v1",
      "title": "Classifier-guided CLIP Distillation for Unsupervised Multi-label Classification",
      "title_zh": "基于分类器引导的CLIP蒸馏用于无监督多标签分类",
      "authors": [
        "Dongseob Kim",
        "Hyunjung Shim"
      ],
      "abstract": "Multi-label classification is crucial for comprehensive image understanding,\nyet acquiring accurate annotations is challenging and costly. To address this,\na recent study suggests exploiting unsupervised multi-label classification\nleveraging CLIP, a powerful vision-language model. Despite CLIP's proficiency,\nit suffers from view-dependent predictions and inherent bias, limiting its\neffectiveness. We propose a novel method that addresses these issues by\nleveraging multiple views near target objects, guided by Class Activation\nMapping (CAM) of the classifier, and debiasing pseudo-labels derived from CLIP\npredictions. Our Classifier-guided CLIP Distillation (CCD) enables selecting\nmultiple local views without extra labels and debiasing predictions to enhance\nclassification performance. Experimental results validate our method's\nsuperiority over existing techniques across diverse datasets. The code is\navailable at https://github.com/k0u-id/CCD.",
      "tldr_zh": "该研究提出了一种名为Classifier-guided CLIP Distillation (CCD)的新型无监督多标签分类方法，旨在解决CLIP模型存在的视角依赖预测和固有偏差问题。该方法通过利用分类器的类激活映射(CAM)引导选择目标物体周围的多个局部视角，并对CLIP预测生成的伪标签进行去偏处理，从而无需额外标注即可提升分类性能。实验结果表明，CCD方法在多种数据集上的表现优于现有技术。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2025 Accepted",
      "pdf_url": "http://arxiv.org/pdf/2503.16873v1",
      "published_date": "2025-03-21 06:12:14 UTC",
      "updated_date": "2025-03-21 06:12:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:50:20.791716"
    },
    {
      "arxiv_id": "2503.16870v1",
      "title": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs",
      "title_zh": "稀疏Logit采样：加速大语言模型中的知识蒸馏",
      "authors": [
        "Anshumann",
        "Mohd Abbas Zaidi",
        "Akhil Kedia",
        "Jinwoo Ahn",
        "Taehwak Kwon",
        "Kangwook Lee",
        "Haejun Lee",
        "Joohyung Lee"
      ],
      "abstract": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B.",
      "tldr_zh": "这项研究提出了\"稀疏Logit采样\"方法，用于加速大型语言模型(LLMs)中的知识蒸馏过程。针对传统Top-K概率缓存方法会导致教师模型概率分布估计偏差的问题，作者开发了基于重要性采样的\"随机采样知识蒸馏\"(Random Sampling Knowledge Distillation)技术。该方法不仅能提供无偏估计、保持梯度期望，还能显著减少需要存储的logit数据量。实验表明，该方法在300M到3B参数规模的模型上都能实现接近完整知识蒸馏的性能，同时训练速度仅比基于交叉熵的训练增加不到10%的开销。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "68T50",
        "I.2.7"
      ],
      "primary_category": "cs.LG",
      "comment": "Anshumann, Mohd Abbas Zaidi and Akhil Kedia have Equal Contribution",
      "pdf_url": "http://arxiv.org/pdf/2503.16870v1",
      "published_date": "2025-03-21 05:58:18 UTC",
      "updated_date": "2025-03-21 05:58:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:50:52.738065"
    },
    {
      "arxiv_id": "2503.16861v2",
      "title": "In-House Evaluation Is Not Enough: Towards Robust Third-Party Flaw Disclosure for General-Purpose AI",
      "title_zh": "仅内测还不够：构建通用人工智能的稳健第三方漏洞披露机制",
      "authors": [
        "Shayne Longpre",
        "Kevin Klyman",
        "Ruth E. Appel",
        "Sayash Kapoor",
        "Rishi Bommasani",
        "Michelle Sahar",
        "Sean McGregor",
        "Avijit Ghosh",
        "Borhane Blili-Hamelin",
        "Nathan Butters",
        "Alondra Nelson",
        "Amit Elazari",
        "Andrew Sellars",
        "Casey John Ellis",
        "Dane Sherrets",
        "Dawn Song",
        "Harley Geiger",
        "Ilona Cohen",
        "Lauren McIlvenny",
        "Madhulika Srikumar",
        "Mark M. Jaycox",
        "Markus Anderljung",
        "Nadine Farid Johnson",
        "Nicholas Carlini",
        "Nicolas Miailhe",
        "Nik Marda",
        "Peter Henderson",
        "Rebecca S. Portnoff",
        "Rebecca Weiss",
        "Victoria Westerhoff",
        "Yacine Jernite",
        "Rumman Chowdhury",
        "Percy Liang",
        "Arvind Narayanan"
      ],
      "abstract": "The widespread deployment of general-purpose AI (GPAI) systems introduces\nsignificant new risks. Yet the infrastructure, practices, and norms for\nreporting flaws in GPAI systems remain seriously underdeveloped, lagging far\nbehind more established fields like software security. Based on a collaboration\nbetween experts from the fields of software security, machine learning, law,\nsocial science, and policy, we identify key gaps in the evaluation and\nreporting of flaws in GPAI systems. We call for three interventions to advance\nsystem safety. First, we propose using standardized AI flaw reports and rules\nof engagement for researchers in order to ease the process of submitting,\nreproducing, and triaging flaws in GPAI systems. Second, we propose GPAI system\nproviders adopt broadly-scoped flaw disclosure programs, borrowing from bug\nbounties, with legal safe harbors to protect researchers. Third, we advocate\nfor the development of improved infrastructure to coordinate distribution of\nflaw reports across the many stakeholders who may be impacted. These\ninterventions are increasingly urgent, as evidenced by the prevalence of\njailbreaks and other flaws that can transfer across different providers' GPAI\nsystems. By promoting robust reporting and coordination in the AI ecosystem,\nthese proposals could significantly improve the safety, security, and\naccountability of GPAI systems.",
      "tldr_zh": "该研究指出通用人工智能(GPAI)系统存在广泛部署风险，但现有漏洞评估和披露机制严重滞后。作者团队提出三大改进方案：1)建立标准化的AI缺陷报告和研究人员行为准则；2)借鉴漏洞赏金机制建立广泛覆盖的漏洞披露程序；3)开发协调多方利益相关者的基础设施。这些措施旨在应对当前普遍存在的越狱攻击等可跨系统传播的漏洞，通过强化报告与协调机制提升GPAI系统的安全性、可靠性和问责性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16861v2",
      "published_date": "2025-03-21 05:09:46 UTC",
      "updated_date": "2025-03-25 05:12:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:50:59.473408"
    },
    {
      "arxiv_id": "2503.16858v1",
      "title": "MTBench: A Multimodal Time Series Benchmark for Temporal Reasoning and Question Answering",
      "title_zh": "MTBench：面向时序推理与问答的多模态时间序列基准测试",
      "authors": [
        "Jialin Chen",
        "Aosong Feng",
        "Ziyu Zhao",
        "Juan Garza",
        "Gaukhar Nurbek",
        "Cheng Qin",
        "Ali Maatouk",
        "Leandros Tassiulas",
        "Yifeng Gao",
        "Rex Ying"
      ],
      "abstract": "Understanding the relationship between textual news and time-series evolution\nis a critical yet under-explored challenge in applied data science. While\nmultimodal learning has gained traction, existing multimodal time-series\ndatasets fall short in evaluating cross-modal reasoning and complex question\nanswering, which are essential for capturing complex interactions between\nnarrative information and temporal patterns. To bridge this gap, we introduce\nMultimodal Time Series Benchmark (MTBench), a large-scale benchmark designed to\nevaluate large language models (LLMs) on time series and text understanding\nacross financial and weather domains. MTbench comprises paired time series and\ntextual data, including financial news with corresponding stock price movements\nand weather reports aligned with historical temperature records. Unlike\nexisting benchmarks that focus on isolated modalities, MTbench provides a\ncomprehensive testbed for models to jointly reason over structured numerical\ntrends and unstructured textual narratives. The richness of MTbench enables\nformulation of diverse tasks that require a deep understanding of both text and\ntime-series data, including time-series forecasting, semantic and technical\ntrend analysis, and news-driven question answering (QA). These tasks target the\nmodel's ability to capture temporal dependencies, extract key insights from\ntextual context, and integrate cross-modal information. We evaluate\nstate-of-the-art LLMs on MTbench, analyzing their effectiveness in modeling the\ncomplex relationships between news narratives and temporal patterns. Our\nfindings reveal significant challenges in current models, including\ndifficulties in capturing long-term dependencies, interpreting causality in\nfinancial and weather trends, and effectively fusing multimodal information.",
      "tldr_zh": "该研究提出了MTBench，一个多模态时间序列基准测试，用于评估大语言模型(LLMs)在金融和天气领域中对文本和时间序列数据的联合推理能力。该基准包含成对的时序数据（如股价走势、温度记录）与相关文本（财经新闻、天气报告），支持时间序列预测、趋势分析和新闻驱动问答等任务。研究发现现有LLMs在捕捉长期依赖关系、解释因果性以及有效融合多模态信息方面仍面临显著挑战。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "14 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.16858v1",
      "published_date": "2025-03-21 05:04:53 UTC",
      "updated_date": "2025-03-21 05:04:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:51:19.464901"
    },
    {
      "arxiv_id": "2503.16853v1",
      "title": "Imagine to Hear: Auditory Knowledge Generation can be an Effective Assistant for Language Models",
      "title_zh": "想象聆听：听觉知识生成可成为语言模型的高效助手",
      "authors": [
        "Suho Yoo",
        "Hyunjong Ok",
        "Jaeho Lee"
      ],
      "abstract": "Language models pretrained on text-only corpora often struggle with tasks\nthat require auditory commonsense knowledge. Previous work addresses this\nproblem by augmenting the language model to retrieve knowledge from external\naudio databases. This approach has several limitations, such as the potential\nlack of relevant audio in databases and the high costs associated with\nconstructing and querying the databases. To address these issues, we propose\nImagine to Hear, a novel approach that dynamically generates auditory knowledge\nusing generative models. Our framework detects multiple audio-related textual\nspans from the given prompt and generates corresponding auditory knowledge. We\ndevelop several mechanisms to efficiently process multiple auditory knowledge,\nincluding a CLAP-based rejection sampler and a language-audio fusion module.\nOur experiments show that our method achieves state-of-the-art performance on\nAuditoryBench without relying on external databases, highlighting the\neffectiveness of our generation-based approach.",
      "tldr_zh": "该研究提出\"Imagine to Hear\"新方法，通过生成式模型动态产生听觉知识，解决纯文本预训练语言模型在听觉常识任务上的不足。该方法利用CLAP拒绝采样器和语言-音频融合模块处理多模态信息，无需依赖外部音频数据库即可实现最先进性能。实验表明，该方法在AuditoryBench基准测试中表现优异，为语言模型提供了一种高效获取听觉知识的新途径。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "Preprint",
      "pdf_url": "http://arxiv.org/pdf/2503.16853v1",
      "published_date": "2025-03-21 04:56:22 UTC",
      "updated_date": "2025-03-21 04:56:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:51:38.103581"
    },
    {
      "arxiv_id": "2503.16852v1",
      "title": "Casual Inference via Style Bias Deconfounding for Domain Generalization",
      "title_zh": "基于风格偏置解混淆的因果推断实现领域泛化",
      "authors": [
        "Jiaxi Li",
        "Di Lin",
        "Hao Chen",
        "Hongying Liu",
        "Liang Wan",
        "Wei Feng"
      ],
      "abstract": "Deep neural networks (DNNs) often struggle with out-of-distribution data,\nlimiting their reliability in diverse realworld applications. To address this\nissue, domain generalization methods have been developed to learn\ndomain-invariant features from single or multiple training domains, enabling\ngeneralization to unseen testing domains. However, existing approaches usually\noverlook the impact of style frequency within the training set. This oversight\npredisposes models to capture spurious visual correlations caused by style\nconfounding factors, rather than learning truly causal representations, thereby\nundermining inference reliability. In this work, we introduce Style\nDeconfounding Causal Learning (SDCL), a novel causal inference-based framework\ndesigned to explicitly address style as a confounding factor. Our approaches\nbegins with constructing a structural causal model (SCM) tailored to the domain\ngeneralization problem and applies a backdoor adjustment strategy to account\nfor style influence. Building on this foundation, we design a style-guided\nexpert module (SGEM) to adaptively clusters style distributions during\ntraining, capturing the global confounding style. Additionally, a back-door\ncausal learning module (BDCL) performs causal interventions during feature\nextraction, ensuring fair integration of global confounding styles into sample\npredictions, effectively reducing style bias. The SDCL framework is highly\nversatile and can be seamlessly integrated with state-of-the-art data\naugmentation techniques. Extensive experiments across diverse natural and\nmedical image recognition tasks validate its efficacy, demonstrating superior\nperformance in both multi-domain and the more challenging single-domain\ngeneralization scenarios.",
      "tldr_zh": "该研究提出了一种基于因果推理的风格偏置去混淆方法（SDCL），用于解决深度神经网络在领域泛化中的风格混淆问题。作者首先构建了结构因果模型（SCM），并采用后门调整策略消除风格因素的影响；随后设计了风格引导专家模块（SGEM）和后门因果学习模块（BDCL），分别用于自适应聚类全局风格分布和执行因果干预。该方法可兼容现有数据增强技术，在自然和医学图像的多领域及单领域泛化任务中均展现出优越性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "under review",
      "pdf_url": "http://arxiv.org/pdf/2503.16852v1",
      "published_date": "2025-03-21 04:52:31 UTC",
      "updated_date": "2025-03-21 04:52:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:52:01.232983"
    },
    {
      "arxiv_id": "2503.16850v1",
      "title": "Physics-Informed Neural Network Surrogate Models for River Stage Prediction",
      "title_zh": "基于物理信息的神经网络代理模型在河道水位预测中的应用",
      "authors": [
        "Maximilian Zoch",
        "Edward Holmberg",
        "Pujan Pokhrel",
        "Ken Pathak",
        "Steven Sloan",
        "Kendall Niles",
        "Jay Ratcliff",
        "Maik Flanagin",
        "Elias Ioup",
        "Christian Guetl",
        "Mahdi Abdelguerfi"
      ],
      "abstract": "This work investigates the feasibility of using Physics-Informed Neural\nNetworks (PINNs) as surrogate models for river stage prediction, aiming to\nreduce computational cost while maintaining predictive accuracy. Our primary\ncontribution demonstrates that PINNs can successfully approximate HEC-RAS\nnumerical solutions when trained on a single river, achieving strong predictive\naccuracy with generally low relative errors, though some river segments exhibit\nhigher deviations.\n  By integrating the governing Saint-Venant equations into the learning\nprocess, the proposed PINN-based surrogate model enforces physical consistency\nand significantly improves computational efficiency compared to HEC-RAS. We\nevaluate the model's performance in terms of accuracy and computational speed,\ndemonstrating that it closely approximates HEC-RAS predictions while enabling\nreal-time inference.\n  These results highlight the potential of PINNs as effective surrogate models\nfor single-river hydrodynamics, offering a promising alternative for\ncomputationally efficient river stage forecasting. Future work will explore\ntechniques to enhance PINN training stability and robustness across a more\ngeneralized multi-river model.",
      "tldr_zh": "本研究探索了基于物理信息的神经网络(PINNs)作为河流水位预测的替代模型，在保持预测精度的同时显著降低计算成本。主要贡献在于证明PINN能够通过集成Saint-Venant控制方程，成功近似HEC-RAS数值解，在单河流场景中实现高预测精度（尽管部分河段存在较大偏差）。相比传统HEC-RAS方法，该PINN替代模型不仅确保物理一致性，还能实现实时推理，计算效率显著提升。研究结果为开发计算高效的单河流水动力预测系统提供了新思路，未来将着重提升PINN在多河流通用模型中的训练稳定性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.16850v1",
      "published_date": "2025-03-21 04:48:22 UTC",
      "updated_date": "2025-03-21 04:48:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:52:25.638041"
    },
    {
      "arxiv_id": "2503.16833v1",
      "title": "The Deployment of End-to-End Audio Language Models Should Take into Account the Principle of Least Privilege",
      "title_zh": "端到端音频语言模型的部署应遵循最小权限原则",
      "authors": [
        "Luxi He",
        "Xiangyu Qi",
        "Michel Liao",
        "Inyoung Cheong",
        "Prateek Mittal",
        "Danqi Chen",
        "Peter Henderson"
      ],
      "abstract": "We are at a turning point for language models that accept audio input. The\nlatest end-to-end audio language models (Audio LMs) process speech directly\ninstead of relying on a separate transcription step. This shift preserves\ndetailed information, such as intonation or the presence of multiple speakers,\nthat would otherwise be lost in transcription. However, it also introduces new\nsafety risks, including the potential misuse of speaker identity cues and other\nsensitive vocal attributes, which could have legal implications. In this\nposition paper, we urge a closer examination of how these models are built and\ndeployed. We argue that the principle of least privilege should guide decisions\non whether to deploy cascaded or end-to-end models. Specifically, evaluations\nshould assess (1) whether end-to-end modeling is necessary for a given\napplication; and (2), the appropriate scope of information access. Finally, We\nhighlight related gaps in current audio LM benchmarks and identify key open\nresearch questions, both technical and policy-related, that must be addressed\nto enable the responsible deployment of end-to-end Audio LMs.",
      "tldr_zh": "本文探讨了端到端音频语言模型（Audio LMs）的安全部署问题，强调应遵循最小权限原则。这类模型直接处理语音，保留了语调、多说话者等细节信息，但也带来了滥用说话者身份等敏感信息的风险。研究建议在部署前评估端到端建模的必要性及信息访问范围，并指出当前音频语言模型基准的不足，提出了技术和政策层面的关键研究问题，以实现端到端音频语言模型的安全应用。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16833v1",
      "published_date": "2025-03-21 04:03:59 UTC",
      "updated_date": "2025-03-21 04:03:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:52:42.393288"
    },
    {
      "arxiv_id": "2503.16806v1",
      "title": "DyWA: Dynamics-adaptive World Action Model for Generalizable Non-prehensile Manipulation",
      "title_zh": "DyWA：面向可泛化非抓取操作的自适应动力学世界动作模型",
      "authors": [
        "Jiangran Lyu",
        "Ziming Li",
        "Xuesong Shi",
        "Chaoyi Xu",
        "Yizhou Wang",
        "He Wang"
      ],
      "abstract": "Nonprehensile manipulation is crucial for handling objects that are too thin,\nlarge, or otherwise ungraspable in unstructured environments. While\nconventional planning-based approaches struggle with complex contact modeling,\nlearning-based methods have recently emerged as a promising alternative.\nHowever, existing learning-based approaches face two major limitations: they\nheavily rely on multi-view cameras and precise pose tracking, and they fail to\ngeneralize across varying physical conditions, such as changes in object mass\nand table friction. To address these challenges, we propose the\nDynamics-Adaptive World Action Model (DyWA), a novel framework that enhances\naction learning by jointly predicting future states while adapting to dynamics\nvariations based on historical trajectories. By unifying the modeling of\ngeometry, state, physics, and robot actions, DyWA enables more robust policy\nlearning under partial observability. Compared to baselines, our method\nimproves the success rate by 31.5% using only single-view point cloud\nobservations in the simulation. Furthermore, DyWA achieves an average success\nrate of 68% in real-world experiments, demonstrating its ability to generalize\nacross diverse object geometries, adapt to varying table friction, and\nrobustness in challenging scenarios such as half-filled water bottles and\nslippery surfaces.",
      "tldr_zh": "本文提出DyWA（动态自适应世界动作模型），一种面向非抓取式操作(nonprehensile manipulation)的通用学习框架。该方法通过联合预测未来状态并基于历史轨迹动态适应物理变化，解决了现有方法对多视角摄像头、精确位姿跟踪的依赖，以及难以适应物体质量、桌面摩擦等物理条件变化的问题。实验表明，DyWA仅使用单视角点云观测即在仿真中提升31.5%成功率，在真实场景中对不同物体几何、桌面摩擦等变化展现出68%的平均成功率，特别在半满水瓶和光滑表面等挑战性场景中表现稳健。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Project Page:https://pku-epic.github.io/DyWA/",
      "pdf_url": "http://arxiv.org/pdf/2503.16806v1",
      "published_date": "2025-03-21 02:29:52 UTC",
      "updated_date": "2025-03-21 02:29:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:53:06.214670"
    },
    {
      "arxiv_id": "2503.16801v1",
      "title": "Auto-Regressive Diffusion for Generating 3D Human-Object Interactions",
      "title_zh": "自回归扩散模型生成3D人机交互",
      "authors": [
        "Zichen Geng",
        "Zeeshan Hayder",
        "Wei Liu",
        "Ajmal Saeed Mian"
      ],
      "abstract": "Text-driven Human-Object Interaction (Text-to-HOI) generation is an emerging\nfield with applications in animation, video games, virtual reality, and\nrobotics. A key challenge in HOI generation is maintaining interaction\nconsistency in long sequences. Existing Text-to-Motion-based approaches, such\nas discrete motion tokenization, cannot be directly applied to HOI generation\ndue to limited data in this domain and the complexity of the modality. To\naddress the problem of interaction consistency in long sequences, we propose an\nautoregressive diffusion model (ARDHOI) that predicts the next continuous\ntoken. Specifically, we introduce a Contrastive Variational Autoencoder (cVAE)\nto learn a physically plausible space of continuous HOI tokens, thereby\nensuring that generated human-object motions are realistic and natural. For\ngenerating sequences autoregressively, we develop a Mamba-based context encoder\nto capture and maintain consistent sequential actions. Additionally, we\nimplement an MLP-based denoiser to generate the subsequent token conditioned on\nthe encoded context. Our model has been evaluated on the OMOMO and BEHAVE\ndatasets, where it outperforms existing state-of-the-art methods in terms of\nboth performance and inference speed. This makes ARDHOI a robust and efficient\nsolution for text-driven HOI tasks",
      "tldr_zh": "该研究提出了一种自回归扩散模型（ARDHOI），用于生成3D人-物交互（HOI）序列，解决了长序列中交互一致性的难题。通过引入对比变分自编码器（cVAE）学习物理上合理的连续HOI token空间，并结合基于Mamba的上下文编码器和MLP去噪器，模型能够生成自然且连贯的交互动作。实验表明，ARDHOI在OMOMO和BEHAVE数据集上优于现有方法，在性能和推理速度上均表现出色，为文本驱动的HOI任务提供了高效解决方案。",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.GR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16801v1",
      "published_date": "2025-03-21 02:25:59 UTC",
      "updated_date": "2025-03-21 02:25:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:53:19.767821"
    },
    {
      "arxiv_id": "2503.16799v1",
      "title": "Causally Aligned Curriculum Learning",
      "title_zh": "因果对齐课程学习",
      "authors": [
        "Mingxuan Li",
        "Junzhe Zhang",
        "Elias Bareinboim"
      ],
      "abstract": "A pervasive challenge in Reinforcement Learning (RL) is the \"curse of\ndimensionality\" which is the exponential growth in the state-action space when\noptimizing a high-dimensional target task. The framework of curriculum learning\ntrains the agent in a curriculum composed of a sequence of related and more\nmanageable source tasks. The expectation is that when some optimal decision\nrules are shared across source tasks and the target task, the agent could more\nquickly pick up the necessary skills to behave optimally in the environment,\nthus accelerating the learning process. However, this critical assumption of\ninvariant optimal decision rules does not necessarily hold in many practical\napplications, specifically when the underlying environment contains unobserved\nconfounders. This paper studies the problem of curriculum RL through causal\nlenses. We derive a sufficient graphical condition characterizing causally\naligned source tasks, i.e., the invariance of optimal decision rules holds. We\nfurther develop an efficient algorithm to generate a causally aligned\ncurriculum, provided with qualitative causal knowledge of the target task.\nFinally, we validate our proposed methodology through experiments in discrete\nand continuous confounded tasks with pixel observations.",
      "tldr_zh": "该论文提出了一种基于因果对齐的课程学习方法（Causally Aligned Curriculum Learning），用于解决强化学习中因维度灾难和未观测混杂变量导致的决策规则不一致问题。研究通过因果图推导出课程任务因果对齐的充分条件，确保源任务与目标任务共享最优决策规则不变性，并开发了高效算法来生成因果对齐的课程。实验验证表明，该方法在离散/连续任务及像素观测场景中均有效提升了学习效率。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted as Posters in ICLR 2024",
      "pdf_url": "http://arxiv.org/pdf/2503.16799v1",
      "published_date": "2025-03-21 02:20:38 UTC",
      "updated_date": "2025-03-21 02:20:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:53:40.957105"
    },
    {
      "arxiv_id": "2503.16797v1",
      "title": "A Learnability Analysis on Neuro-Symbolic Learning",
      "title_zh": "神经符号学习可学习性分析",
      "authors": [
        "Hao-Yuan He",
        "Ming Li"
      ],
      "abstract": "This paper analyzes the learnability of neuro-symbolic (NeSy) tasks within\nhybrid systems. We show that the learnability of NeSy tasks can be\ncharacterized by their derived constraint satisfaction problems (DCSPs).\nSpecifically, a task is learnable if the corresponding DCSP has a unique\nsolution; otherwise, it is unlearnable. For learnable tasks, we establish error\nbounds by exploiting the clustering property of the hypothesis space.\nAdditionally, we analyze the asymptotic error for general NeSy tasks, showing\nthat the expected error scales with the disagreement among solutions. Our\nresults offer a principled approach to determining learnability and provide\ninsights into the design of new algorithms.",
      "tldr_zh": "本文对神经符号（NeSy）混合系统的可学习性进行了理论分析。研究发现，NeSy任务的可学习性可以通过其衍生的约束满足问题（DCSP）来表征：当DCSP具有唯一解时任务可学习，否则不可学习。对于可学习任务，研究者通过假设空间的聚类特性建立了误差界限，并证明一般NeSy任务的渐进误差与解之间的分歧程度成正比。该研究为神经符号学习提供了判定可学习性的理论框架，并为新算法设计提供了指导原则。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16797v1",
      "published_date": "2025-03-21 02:16:11 UTC",
      "updated_date": "2025-03-21 02:16:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:54:28.519917"
    },
    {
      "arxiv_id": "2503.16791v1",
      "title": "\"The Diagram is like Guardrails\": Structuring GenAI-assisted Hypotheses Exploration with an Interactive Shared Representation",
      "title_zh": "\"图表如同护栏\"：利用交互式共享表征构建生成式AI辅助的假设探索框架",
      "authors": [
        "Zijian Ding",
        "Michelle Brachman",
        "Joel Chan",
        "Werner Geyer"
      ],
      "abstract": "Data analysis encompasses a spectrum of tasks, from high-level conceptual\nreasoning to lower-level execution. While AI-powered tools increasingly support\nexecution tasks, there remains a need for intelligent assistance in conceptual\ntasks. This paper investigates the design of an ordered node-link tree\ninterface augmented with AI-generated information hints and visualizations, as\na potential shared representation for hypothesis exploration. Through a design\nprobe (n=22), participants generated diagrams averaging 21.82 hypotheses. Our\nfindings showed that the node-link diagram acts as \"guardrails\" for hypothesis\nexploration, facilitating structured workflows, providing comprehensive\noverviews, and enabling efficient backtracking. The AI-generated information\nhints, particularly visualizations, aided users in transforming abstract ideas\ninto data-backed concepts while reducing cognitive load. We further discuss how\nnode-link diagrams can support both parallel exploration and iterative\nrefinement in hypothesis formulation, potentially enhancing the breadth and\ndepth of human-AI collaborative data analysis.",
      "tldr_zh": "这篇论文研究了一种基于节点链接树状图(node-link diagram)的交互式共享表示方法，旨在增强AI辅助的假设探索过程。研究发现这种可视化结构能像\"护栏\"一样引导用户进行假设探索，支持结构化工作流并提供全局概览，同时AI生成的信息提示（尤其是可视化）有助于将抽象想法转化为数据支持的概念。实验显示参与者平均能生成21.82个假设，该界面既支持并行探索又支持迭代优化，可能提升人机协作数据分析的广度和深度。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16791v1",
      "published_date": "2025-03-21 02:01:37 UTC",
      "updated_date": "2025-03-21 02:01:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:54:20.970086"
    },
    {
      "arxiv_id": "2503.16788v1",
      "title": "Does Chain-of-Thought Reasoning Help Mobile GUI Agent? An Empirical Study",
      "title_zh": "链式思维推理是否有助于移动GUI智能体？一项实证研究",
      "authors": [
        "Li Zhang",
        "Longxi Gao",
        "Mengwei Xu"
      ],
      "abstract": "Reasoning capabilities have significantly improved the performance of\nvision-language models (VLMs) in domains such as mathematical problem-solving,\ncoding, and visual question-answering. However, their impact on real-world\napplications remains unclear. This paper presents the first empirical study on\nthe effectiveness of reasoning-enabled VLMs in mobile GUI agents, a domain that\nrequires interpreting complex screen layouts, understanding user instructions,\nand executing multi-turn interactions. We evaluate two pairs of commercial\nmodels--Gemini 2.0 Flash and Claude 3.7 Sonnet--comparing their base and\nreasoning-enhanced versions across two static benchmarks (ScreenSpot and\nAndroidControl) and one interactive environment (AndroidWorld). We surprisingly\nfind the Claude 3.7 Sonnet reasoning model achieves state-of-the-art\nperformance on AndroidWorld. However, reasoning VLMs generally offer marginal\nimprovements over non-reasoning models on static benchmarks and even degrade\nperformance in some agent setups. Notably, reasoning and non-reasoning VLMs\nfail on different sets of tasks, suggesting that reasoning does have an impact,\nbut its benefits and drawbacks counterbalance each other. We attribute these\ninconsistencies to the limitations of benchmarks and VLMs. Based on the\nfindings, we provide insights for further enhancing mobile GUI agents in terms\nof benchmarks, VLMs, and their adaptability in dynamically invoking reasoning\nVLMs. The experimental data are publicly available at\nhttps://github.com/LlamaTouch/VLM-Reasoning-Traces.",
      "tldr_zh": "这项研究首次实证检验了链式思维推理(Chain-of-Thought)在移动端GUI智能体中的实际效果。通过对比Gemini 2.0 Flash和Claude 3.7 Sonnet的基础版与推理增强版在三个测试环境(包括两个静态基准ScreenSpot/AndroidControl和动态交互环境AndroidWorld)的表现，发现推理模型在动态环境中表现最佳(Claude 3.7 Sonnet创下AndroidWorld新记录)，但在静态基准上仅带来边际提升甚至性能下降。研究揭示推理与非推理模型在不同任务集上存在互补性失败模式，表明推理能力的影响具有两面性。作者将这种不一致归因于基准测试和VLM模型的局限性，并为移动GUI智能体的基准设计、模型优化及动态推理调用机制提供了改进方向。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16788v1",
      "published_date": "2025-03-21 01:52:43 UTC",
      "updated_date": "2025-03-21 01:52:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:54:46.615971"
    },
    {
      "arxiv_id": "2503.16782v1",
      "title": "Learning Part Knowledge to Facilitate Category Understanding for Fine-Grained Generalized Category Discovery",
      "title_zh": "学习部件知识以促进细粒度广义类别发现的类别理解",
      "authors": [
        "Enguang Wang",
        "Zhimao Peng",
        "Zhengyuan Xie",
        "Haori Lu",
        "Fei Yang",
        "Xialei Liu"
      ],
      "abstract": "Generalized Category Discovery (GCD) aims to classify unlabeled data\ncontaining both seen and novel categories. Although existing methods perform\nwell on generic datasets, they struggle in fine-grained scenarios. We attribute\nthis difficulty to their reliance on contrastive learning over global image\nfeatures to automatically capture discriminative cues, which fails to capture\nthe subtle local differences essential for distinguishing fine-grained\ncategories. Therefore, in this paper, we propose incorporating part knowledge\nto address fine-grained GCD, which introduces two key challenges: the absence\nof annotations for novel classes complicates the extraction of the part\nfeatures, and global contrastive learning prioritizes holistic feature\ninvariance, inadvertently suppressing discriminative local part patterns. To\naddress these challenges, we propose PartGCD, including 1) Adaptive Part\nDecomposition, which automatically extracts class-specific semantic parts via\nGaussian Mixture Models, and 2) Part Discrepancy Regularization, enforcing\nexplicit separation between part features to amplify fine-grained local part\ndistinctions.\n  Experiments demonstrate state-of-the-art performance across multiple\nfine-grained benchmarks while maintaining competitiveness on generic datasets,\nvalidating the effectiveness and robustness of our approach.",
      "tldr_zh": "该论文提出PartGCD方法，针对细粒度广义类别发现(Fine-Grained GCD)任务中现有方法难以捕捉局部细微差异的问题。通过自适应部件分解(Adaptive Part Decomposition)自动提取类别特定语义部件，并结合部件差异正则化(Part Discrepancy Regularization)增强局部特征区分度。实验表明，该方法在多个细粒度基准数据集上达到最先进性能，同时在通用数据集上保持竞争力，验证了其有效性和鲁棒性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16782v1",
      "published_date": "2025-03-21 01:37:51 UTC",
      "updated_date": "2025-03-21 01:37:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:55:11.777862"
    },
    {
      "arxiv_id": "2503.16779v1",
      "title": "Chain-of-Tools: Utilizing Massive Unseen Tools in the CoT Reasoning of Frozen Language Models",
      "title_zh": "工具链思维：在冻结语言模型的链式推理中利用海量未知工具",
      "authors": [
        "Mengsong Wu",
        "Tong Zhu",
        "Han Han",
        "Xiang Zhang",
        "Wenbiao Shao",
        "Wenliang Chen"
      ],
      "abstract": "Tool learning can further broaden the usage scenarios of large language\nmodels (LLMs). However most of the existing methods either need to finetune\nthat the model can only use tools seen in the training data, or add tool\ndemonstrations into the prompt with lower efficiency. In this paper, we present\na new Tool Learning method Chain-of-Tools. It makes full use of the powerful\nsemantic representation capability of frozen LLMs to finish tool calling in CoT\nreasoning with a huge and flexible tool pool which may contain unseen tools.\nEspecially, to validate the effectiveness of our approach in the massive unseen\ntool scenario, we construct a new dataset SimpleToolQuestions. We conduct\nexperiments on two numerical reasoning benchmarks (GSM8K-XL and FuncQA) and two\nknowledge-based question answering benchmarks (KAMEL and SimpleToolQuestions).\nExperimental results show that our approach performs better than the baseline.\nWe also identify dimensions of the model output that are critical in tool\nselection, enhancing the model interpretability. Our code and data are\navailable at: https://github.com/fairyshine/Chain-of-Tools .",
      "tldr_zh": "该研究提出Chain-of-Tools方法，利用冻结大语言模型(LLMs)的强大语义表征能力，在思维链(CoT)推理中调用海量未见过的工具。通过构建SimpleToolQuestions新数据集验证表明，该方法在数值推理(GSM8K-XL和FuncQA)和知识问答(KAMEL)任务上优于基线模型，同时揭示了工具选择的关键维度，提升了模型可解释性。这种免微调的方法突破了现有工具学习只能使用训练所见工具的限制。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "11 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.16779v1",
      "published_date": "2025-03-21 01:26:12 UTC",
      "updated_date": "2025-03-21 01:26:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:55:21.912821"
    },
    {
      "arxiv_id": "2503.17417v1",
      "title": "Generative Modeling of Class Probability for Multi-Modal Representation Learning",
      "title_zh": "多模态表示学习的类概率生成建模",
      "authors": [
        "Jungkyoo Shin",
        "Bumsoo Kim",
        "Eunwoo Kim"
      ],
      "abstract": "Multi-modal understanding plays a crucial role in artificial intelligence by\nenabling models to jointly interpret inputs from different modalities. However,\nconventional approaches such as contrastive learning often struggle with\nmodality discrepancies, leading to potential misalignments. In this paper, we\npropose a novel class anchor alignment approach that leverages class\nprobability distributions for multi-modal representation learning. Our method,\nClass-anchor-ALigned generative Modeling (CALM), encodes class anchors as\nprompts to generate and align class probability distributions for each\nmodality, enabling more effective alignment. Furthermore, we introduce a\ncross-modal probabilistic variational autoencoder to model uncertainty in the\nalignment, enhancing the ability to capture deeper relationships between\nmodalities and data variations. Extensive experiments on four benchmark\ndatasets demonstrate that our approach significantly outperforms\nstate-of-the-art methods, especially in out-of-domain evaluations. This\nhighlights its superior generalization capabilities in multi-modal\nrepresentation learning.",
      "tldr_zh": "本文提出了一种新的多模态表征学习方法CALM（Class-anchor-ALigned generative Modeling），通过生成和校准类别概率分布来解决模态差异问题。该方法利用类别锚点作为提示（prompt）生成各模态的类别概率分布，并引入跨模态概率变分自编码器（cross-modal probabilistic variational autoencoder）建模对齐过程中的不确定性。实验表明，CALM在四个基准数据集上显著优于现有方法，尤其在外域评估中表现出更强的泛化能力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to CVPR2025",
      "pdf_url": "http://arxiv.org/pdf/2503.17417v1",
      "published_date": "2025-03-21 01:17:44 UTC",
      "updated_date": "2025-03-21 01:17:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:55:46.904520"
    },
    {
      "arxiv_id": "2503.17416v1",
      "title": "Debugging and Runtime Analysis of Neural Networks with VLMs (A Case Study)",
      "title_zh": "基于视觉语言模型(VLM)的神经网络调试与运行时分析(案例研究)",
      "authors": [
        "Boyue Caroline Hu",
        "Divya Gopinath",
        "Corina S. Pasareanu",
        "Nina Narodytska",
        "Ravi Mangal",
        "Susmit Jha"
      ],
      "abstract": "Debugging of Deep Neural Networks (DNNs), particularly vision models, is very\nchallenging due to the complex and opaque decision-making processes in these\nnetworks. In this paper, we explore multi-modal Vision-Language Models (VLMs),\nsuch as CLIP, to automatically interpret the opaque representation space of\nvision models using natural language. This in turn, enables a semantic analysis\nof model behavior using human-understandable concepts, without requiring costly\nhuman annotations. Key to our approach is the notion of semantic heatmap, that\nsuccinctly captures the statistical properties of DNNs in terms of the concepts\ndiscovered with the VLM and that are computed off-line using a held-out data\nset. We show the utility of semantic heatmaps for fault localization -- an\nessential step in debugging -- in vision models. Our proposed technique helps\nlocalize the fault in the network (encoder vs head) and also highlights the\nresponsible high-level concepts, by leveraging novel differential heatmaps,\nwhich summarize the semantic differences between the correct and incorrect\nbehaviour of the analyzed DNN. We further propose a lightweight runtime\nanalysis to detect and filter-out defects at runtime, thus improving the\nreliability of the analyzed DNNs. The runtime analysis works by measuring and\ncomparing the similarity between the heatmap computed for a new (unseen) input\nand the heatmaps computed a-priori for correct vs incorrect DNN behavior. We\nconsider two types of defects: misclassifications and vulnerabilities to\nadversarial attacks. We demonstrate the debugging and runtime analysis on a\ncase study involving a complex ResNet-based classifier trained on the RIVAL10\ndataset.",
      "tldr_zh": "该研究提出了一种基于多模态视觉语言模型(VLMs)的深度神经网络(DNN)调试与运行时分析方法。通过使用CLIP等VLMs，自动解析视觉模型的不透明表示空间，生成语义热图，捕捉DNN的统计特性，并利用差分热图定位网络故障（编码器或头部）及相关的语义概念。此外，研究还提出了一种轻量级运行时分析技术，通过比较新输入的热图与预先计算的正确/错误行为热图，检测并过滤缺陷，如误分类和对抗攻击漏洞。实验以RIVAL10数据集上的ResNet分类器为例，验证了该方法的有效性。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "CAIN 2025 (4th International Conference on AI Engineering -- Software\n  Engineering for AI)",
      "pdf_url": "http://arxiv.org/pdf/2503.17416v1",
      "published_date": "2025-03-21 01:12:57 UTC",
      "updated_date": "2025-03-21 01:12:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:56:05.119117"
    },
    {
      "arxiv_id": "2503.17415v1",
      "title": "Enhancing Subsequent Video Retrieval via Vision-Language Models (VLMs)",
      "title_zh": "基于视觉语言模型（VLM）增强后续视频检索能力",
      "authors": [
        "Yicheng Duan",
        "Xi Huang",
        "Duo Chen"
      ],
      "abstract": "The rapid growth of video content demands efficient and precise retrieval\nsystems. While vision-language models (VLMs) excel in representation learning,\nthey often struggle with adaptive, time-sensitive video retrieval. This paper\nintroduces a novel framework that combines vector similarity search with\ngraph-based data structures. By leveraging VLM embeddings for initial retrieval\nand modeling contextual relationships among video segments, our approach\nenables adaptive query refinement and improves retrieval accuracy. Experiments\ndemonstrate its precision, scalability, and robustness, offering an effective\nsolution for interactive video retrieval in dynamic environments.",
      "tldr_zh": "该论文提出了一种结合向量相似性搜索和图数据结构的新型视频检索框架，旨在解决现有视觉语言模型(VLMs)在时间敏感视频检索中的适应性不足问题。该方法利用VLM嵌入进行初始检索，并通过建模视频片段间的上下文关系实现自适应查询优化，显著提高了检索准确率。实验验证了该框架在动态环境中具有高精度、可扩展性和鲁棒性，为交互式视频检索提供了有效解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17415v1",
      "published_date": "2025-03-21 01:11:14 UTC",
      "updated_date": "2025-03-21 01:11:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:56:23.064765"
    },
    {
      "arxiv_id": "2503.16768v1",
      "title": "Dynamic Attention Mechanism in Spatiotemporal Memory Networks for Object Tracking",
      "title_zh": "时空记忆网络中用于目标跟踪的动态注意力机制",
      "authors": [
        "Meng Zhou",
        "Jiadong Xie",
        "Mingsheng Xu"
      ],
      "abstract": "Mainstream visual object tracking frameworks predominantly rely on template\nmatching paradigms. Their performance heavily depends on the quality of\ntemplate features, which becomes increasingly challenging to maintain in\ncomplex scenarios involving target deformation, occlusion, and background\nclutter. While existing spatiotemporal memory-based trackers emphasize memory\ncapacity expansion, they lack effective mechanisms for dynamic feature\nselection and adaptive fusion. To address this gap, we propose a Dynamic\nAttention Mechanism in Spatiotemporal Memory Network (DASTM) with two key\ninnovations: 1) A differentiable dynamic attention mechanism that adaptively\nadjusts channel-spatial attention weights by analyzing spatiotemporal\ncorrelations between the templates and memory features; 2) A lightweight gating\nnetwork that autonomously allocates computational resources based on target\nmotion states, prioritizing high-discriminability features in challenging\nscenarios. Extensive evaluations on OTB-2015, VOT 2018, LaSOT, and GOT-10K\nbenchmarks demonstrate our DASTM's superiority, achieving state-of-the-art\nperformance in success rate, robustness, and real-time efficiency, thereby\noffering a novel solution for real-time tracking in complex environments.",
      "tldr_zh": "该研究提出了一种动态注意力机制DASTM，用于解决目标跟踪中模板匹配在复杂场景下的局限性。该方法创新性地通过可微分动态注意力机制自适应调整通道-空间注意力权重，并采用轻量级门控网络根据目标运动状态分配计算资源。在OTB-2015等多个基准测试中，DASTM在成功率、鲁棒性和实时性方面均达到最先进水平，为复杂环境下的实时目标跟踪提供了新方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16768v1",
      "published_date": "2025-03-21 00:48:31 UTC",
      "updated_date": "2025-03-21 00:48:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:56:52.458851"
    },
    {
      "arxiv_id": "2503.17414v1",
      "title": "Opportunities and Challenges of Frontier Data Governance With Synthetic Data",
      "title_zh": "前沿数据治理的机遇与挑战：基于合成数据的视角",
      "authors": [
        "Madhavendra Thakur",
        "Jason Hausenloy"
      ],
      "abstract": "Synthetic data, or data generated by machine learning models, is increasingly\nemerging as a solution to the data access problem. However, its use introduces\nsignificant governance and accountability challenges, and potentially debases\nexisting governance paradigms, such as compute and data governance. In this\npaper, we identify 3 key governance and accountability challenges that\nsynthetic data poses - it can enable the increased emergence of malicious\nactors, spontaneous biases and value drift. We thus craft 3 technical\nmechanisms to address these specific challenges, finding applications for\nsynthetic data towards adversarial training, bias mitigation and value\nreinforcement. These could not only counteract the risks of synthetic data, but\nserve as critical levers for governance of the frontier in the future.",
      "tldr_zh": "该研究探讨了合成数据（synthetic data）作为解决数据访问问题的新兴方案所带来的治理挑战。论文指出合成数据可能带来三大风险：助长恶意行为者、自发偏见（spontaneous biases）和价值观漂移（value drift）。为此，作者开发了三种技术机制，分别应用于对抗训练（adversarial training）、偏见缓解（bias mitigation）和价值强化（value reinforcement），不仅能抵消合成数据的风险，还可作为未来前沿数据治理的关键手段。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17414v1",
      "published_date": "2025-03-21 00:30:17 UTC",
      "updated_date": "2025-03-21 00:30:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T17:57:05.708709"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 103,
  "processed_papers_count": 103,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-03-26T17:58:59.511535"
}
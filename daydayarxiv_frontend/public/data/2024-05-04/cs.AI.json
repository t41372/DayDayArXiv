{
  "date": "2024-05-04",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-05-04 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文主要聚焦于 AI 和机器学习领域，包括大型语言模型 (LLM) 在医疗、知识图谱和智能合约中的应用、多模态学习、强化学习的安全性和计算机视觉的创新方法，令人印象深刻的是 PropertyGPT（使用 LLM 进行智能合约验证）和 Enhancing Contextual Understanding（通过对比解码提升 LLM 上下文理解），这些工作突显了 LLM 在实际应用中的潜力，同时涉及知名学者如 Elahe Arani 和 Yang Liu 的贡献。\n\n下面，我将按主题简要概述重点论文，先优先讨论 LLM 相关和高影响力工作，再快速掠过其他领域。每个条目包括论文标题（中文 + 英文）和核心贡献。\n\n### LLM 和文本生成相关\n- **Has this Fact been Edited? Detecting Knowledge Edits in Language Models**（中文：检测语言模型的知识编辑）  \n  这篇论文提出一种新任务：检测 LLM 中编辑过的知识，使用 AdaBoost 分类器分析隐藏状态和概率分布，显著提升了模型透明度和用户信任，尤其在防范恶意编辑方面。\n\n- **Enhancing Contextual Understanding in Large Language Models through Contrastive Decoding**（中文：通过对比解码提升大型语言模型的上下文理解）  \n  作者使用对比解码和负面样本来改善 LLM 的上下文 grounding，无需额外训练，即可提升生成文本的准确性和鲁棒性，在问答任务中表现出色。\n\n- **PropertyGPT: LLM-driven Formal Verification of Smart Contracts through Retrieval-Augmented Property Generation**（中文：LLM 驱动的智能合约形式验证）  \n  这篇高话题度论文利用 LLM 和检索增强生成属性，成功检测了 26 个已知 CVE 和 12 个零日漏洞，提供高效的合约验证框架，作者 Yang Liu 等人的工作可能带来区块链安全新突破。\n\n- **Open-SQL Framework: Enhancing Text-to-SQL on Open-source Large Language Models**（中文：提升开源 LLM 在文本到 SQL 上的框架）  \n  通过提示工程和优化策略，显著提升 LLM 在医疗数据库查询的性能，例如 Llama2-7B 的准确率从 2.54% 提高到 41.04%，适用于复杂 EHR 查询。\n\n- **Astro-NER: Astronomy Named Entity Recognition: Is GPT a Good Domain Expert Annotator?**（中文：天文学命名实体识别：GPT 是否是好的领域专家标注器？）  \n  使用 GPT-4 辅助非专家标注天文实体数据集，展示了 LLM 在数据标注中的潜力，准确率中等，但为天文学 NLP 提供了新数据集。\n\n- **The Role of AI in Peer Support for Young People: A Study of Preferences for Human- and AI-Generated Responses**（中文：AI 在年轻人同伴支持中的作用）  \n  快速提及：研究显示年轻人更偏好 AI 响应在关系和健康话题上，但人类响应在敏感话题（如自杀）更受欢迎，强调 AI 的局限性。\n\n### 强化学习和安全相关\n- **Implicit Safe Set Algorithm for Provably Safe Reinforcement Learning**（中文：隐式安全集算法用于可证明安全强化学习）  \n  这篇重要论文提出一种模型无关的算法，使用安全指数和黑盒动态函数确保强化学习的安全性，在 Safety Gym 基准上实现零违规和 95% 奖励率，作者 Changliu Liu 的工作提升了实际应用潜力。\n\n- **FedProK: Trustworthy Federated Class-Incremental Learning via Prototypical Feature Knowledge Transfer**（中文：通过原型特征知识转移的可靠联邦增量学习）  \n  利用原型特征进行空间-时间知识转移，提高联邦学习的隐私和效率，同时减少遗忘问题。\n\n- **Enhancing Cooperation through Selective Interaction and Long-term Experiences in Multi-Agent Reinforcement Learning**（中文：在多代理强化学习中通过选择性交互和长期经验提升合作）  \n  快速概述：代理通过长期经验自组织选择合作邻居，提升群体合作，作者 Xiao-Jun Zeng 的方法在囚徒困境游戏中表现突出。\n\n### 计算机视觉和数据处理\n- **UnSAMFlow: Unsupervised Optical Flow Guided by Segment Anything Model**（中文：使用 Segment Anything 模型的无监督光流估计）  \n  结合 SAM 和新平滑损失，实现高效光流估计，在 KITTI 和 Sintel 数据集上超越现有方法。\n\n- **Boosting 3D Neuron Segmentation with 2D Vision Transformer Pre-trained on Natural Images**（中文：使用自然图像预训练的 2D 视觉 Transformer 提升 3D 神经元分割）  \n  通过 2D 到 3D 权重转移策略，提高神经元分割性能，在 BigNeuron 基准上提升 8.71%。\n\n- **CNN-LSTM and Transfer Learning Models for Malware Classification based on Opcodes and API Calls**（中文：基于操作码和 API 调用的 CNN-LSTM 和迁移学习恶意软件分类模型）  \n  快速掠过：使用 N-gram 序列和 CNN-LSTM 达到 99.91% 准确率，适用于安全领域。\n\n其他论文如 Quranic Audio Dataset（古兰经音频数据集构建）、Modern Information Technologies（信息技术综述）和 Prediction of Space Weather Events（空间天气预测）等，虽然有贡献，但相对常规或领域特定，我仅简要提及：它们分别提供了新数据集、综述和 CNN 预测方法，无需深入讨论。\n\n总之，今天的 arXiv 强调了 LLM 和强化学习的创新应用，未来可能推动医疗和安全领域的进展。感谢阅读，欢迎关注明日快报！",
  "papers": [
    {
      "arxiv_id": "2405.02771v2",
      "title": "MMEarth: Exploring Multi-Modal Pretext Tasks For Geospatial Representation Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Vishal Nedungadi",
        "Ankit Kariryaa",
        "Stefan Oehmcke",
        "Serge Belongie",
        "Christian Igel",
        "Nico Lang"
      ],
      "abstract": "The volume of unlabelled Earth observation (EO) data is huge, but many\nimportant applications lack labelled training data. However, EO data offers the\nunique opportunity to pair data from different modalities and sensors\nautomatically based on geographic location and time, at virtually no human\nlabor cost. We seize this opportunity to create MMEarth, a diverse multi-modal\npretraining dataset at global scale. Using this new corpus of 1.2 million\nlocations, we propose a Multi-Pretext Masked Autoencoder (MP-MAE) approach to\nlearn general-purpose representations for optical satellite images. Our\napproach builds on the ConvNeXt V2 architecture, a fully convolutional masked\nautoencoder (MAE). Drawing upon a suite of multi-modal pretext tasks, we\ndemonstrate that our MP-MAE approach outperforms both MAEs pretrained on\nImageNet and MAEs pretrained on domain-specific satellite images. This is shown\non several downstream tasks including image classification and semantic\nsegmentation. We find that pretraining with multi-modal pretext tasks notably\nimproves the linear probing performance compared to pretraining on optical\nsatellite images only. This also leads to better label efficiency and parameter\nefficiency which are crucial aspects in global scale applications.",
      "tldr_zh": "本研究提出MMEarth，一个全球规模的多样化多模态预训练数据集，包含120万地点，利用地球观测（EO）数据的地理位置和时间自动配对不同模态和传感器数据，以解决标注数据不足的问题。研究引入Multi-Pretext Masked Autoencoder (MP-MAE)方法，基于ConvNeXt V2架构和多种多模态pretext tasks，学习光学卫星图像的通用表示。实验结果显示，MP-MAE在图像分类和语义分割等下游任务上超越了在ImageNet或领域特定卫星图像上预训练的MAE模型，并显著提升了标签效率和参数效率，适用于大规模全球应用。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted for ECCV 2024. Data and code:\n  https://vishalned.github.io/mmearth Update arXiv v2 (ECCV): 1. Dataset fix:\n  Removed duplicates and corrected ERA5 yearly statistics. 2. Data augmentation\n  fix: Random crops are now aligned. 3. Test metrics fix: Metrics are now\n  overall instead of mini-batch averages, matching GEO-Bench metrics. 4.\n  Pretrained on MMEarth v001 & evaluated on GEO-Bench v1.0",
      "pdf_url": "http://arxiv.org/pdf/2405.02771v2",
      "published_date": "2024-05-04 23:16:48 UTC",
      "updated_date": "2024-07-29 10:35:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:54:12.940130"
    },
    {
      "arxiv_id": "2405.09561v1",
      "title": "GAD: A Real-time Gait Anomaly Detection System with Online Adaptive Learning",
      "title_zh": "GAD：一种实时步态异常检测系统，具有在线自适应学习",
      "authors": [
        "Ming-Chang Lee",
        "Jia-Chun Lin",
        "Sokratis Katsikas"
      ],
      "abstract": "Gait anomaly detection is a task that involves detecting deviations from a\nperson's normal gait pattern. These deviations can indicate health issues and\nmedical conditions in the healthcare domain, or fraudulent impersonation and\nunauthorized identity access in the security domain. A number of gait anomaly\ndetection approaches have been introduced, but many of them require offline\ndata preprocessing, offline model learning, setting parameters, and so on,\nwhich might restrict their effectiveness and applicability in real-world\nscenarios. To address these issues, this paper introduces GAD, a real-time gait\nanomaly detection system. GAD focuses on detecting anomalies within an\nindividual's three-dimensional accelerometer readings based on dimensionality\nreduction and Long Short-Term Memory (LSTM). Upon being launched, GAD begins\ncollecting a gait segment from the user and training an anomaly detector to\nlearn the user's walking pattern on the fly. If the subsequent model\nverification is successful, which involves validating the trained detector\nusing the user's subsequent steps, the detector is employed to identify\nabnormalities in the user's subsequent gait readings at the user's request. The\nanomaly detector will be retained online to adapt to minor pattern changes and\nwill undergo retraining as long as it cannot provide adequate prediction. We\nexplored two methods for capturing users' gait segments: a personalized method\ntailored to each individual's step length, and a uniform method utilizing a\nfixed step length. Experimental results using an open-source gait dataset show\nthat GAD achieves a higher detection accuracy ratio when combined with the\npersonalized method.",
      "tldr_zh": "本论文提出GAD系统，这是一个实时步态异常检测(Gait Anomaly Detection)系统，采用在线自适应学习(Online Adaptive Learning)来解决传统方法依赖离线预处理的问题。GAD通过降维(dimensionality reduction)和Long Short-Term Memory (LSTM)模型，对用户的三维加速度计数据进行在线收集、训练和验证，允许检测器动态适应步态模式变化。实验结果显示，使用个性化方法（基于个人步长）比统一方法更有效，在开源步态数据集上实现了更高的检测准确率。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "14 pages, 8 figures, 3 tables, ICT Systems Security and Privacy\n  Protection 39th IFIP TC 11 International Conference, SEC 2024, Edinburgh, UK,\n  June 12-14, 2024, Proceedings (IFIP SEC2024)",
      "pdf_url": "http://arxiv.org/pdf/2405.09561v1",
      "published_date": "2024-05-04 22:43:09 UTC",
      "updated_date": "2024-05-04 22:43:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:54:26.034697"
    },
    {
      "arxiv_id": "2405.02766v1",
      "title": "Beyond Unimodal Learning: The Importance of Integrating Multiple Modalities for Lifelong Learning",
      "title_zh": "超越单模态学习：整合多个模态对于终身学习的重要性",
      "authors": [
        "Fahad Sarfraz",
        "Bahram Zonooz",
        "Elahe Arani"
      ],
      "abstract": "While humans excel at continual learning (CL), deep neural networks (DNNs)\nexhibit catastrophic forgetting. A salient feature of the brain that allows\neffective CL is that it utilizes multiple modalities for learning and\ninference, which is underexplored in DNNs. Therefore, we study the role and\ninteractions of multiple modalities in mitigating forgetting and introduce a\nbenchmark for multimodal continual learning. Our findings demonstrate that\nleveraging multiple views and complementary information from multiple\nmodalities enables the model to learn more accurate and robust representations.\nThis makes the model less vulnerable to modality-specific regularities and\nconsiderably mitigates forgetting. Furthermore, we observe that individual\nmodalities exhibit varying degrees of robustness to distribution shift.\nFinally, we propose a method for integrating and aligning the information from\ndifferent modalities by utilizing the relational structural similarities\nbetween the data points in each modality. Our method sets a strong baseline\nthat enables both single- and multimodal inference. Our study provides a\npromising case for further exploring the role of multiple modalities in\nenabling CL and provides a standard benchmark for future research.",
      "tldr_zh": "本研究探讨了在持续学习（CL）中整合多模态的重要性，指出深度神经网络（DNNs）易于发生灾难性遗忘，而大脑的多模态利用能有效缓解这一问题。研究发现，通过利用多个视图和互补信息，多模态学习能帮助模型构建更准确和稳健的表示，减少对特定模态规则的依赖，并显示不同模态对分布偏移的鲁棒性存在差异。作者提出了一种方法，通过利用各模态间数据点的关系结构相似性来整合和对齐信息，该方法支持单模态和多模态推理，并建立了多模态持续学习基准。总之，此研究为探索多模态在CL中的作用提供了新视角，并为未来研究设定了标准基准。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at 3rd Conference on Lifelong Learning Agents (CoLLAs), 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.02766v1",
      "published_date": "2024-05-04 22:02:58 UTC",
      "updated_date": "2024-05-04 22:02:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:54:37.637178"
    },
    {
      "arxiv_id": "2405.02765v3",
      "title": "Has this Fact been Edited? Detecting Knowledge Edits in Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Paul Youssef",
        "Zhixue Zhao",
        "Christin Seifert",
        "Jörg Schlötterer"
      ],
      "abstract": "Knowledge editing methods (KEs) can update language models' obsolete or\ninaccurate knowledge learned from pre-training. However, KEs can be used for\nmalicious applications, e.g., inserting misinformation and toxic content.\nKnowing whether a generated output is based on edited knowledge or first-hand\nknowledge from pre-training can increase users' trust in generative models and\nprovide more transparency. Driven by this, we propose a novel task: detecting\nedited knowledge in language models. Given an edited model and a fact retrieved\nby a prompt from an edited model, the objective is to classify the knowledge as\neither unedited (based on the pre-training), or edited (based on subsequent\nediting). We instantiate the task with four KEs, two LLMs, and two datasets.\nAdditionally, we propose using the hidden state representations and the\nprobability distributions as features for the detection. Our results reveal\nthat, using these features as inputs to a simple AdaBoost classifiers\nestablishes a strong baseline. This classifier requires only a limited amount\nof data and maintains its performance even in cross-domain settings. Last, we\nfind it more challenging to distinguish edited knowledge from unedited but\nrelated knowledge, highlighting the need for further research. Our work lays\nthe groundwork for addressing malicious model editing, which is a critical\nchallenge associated with the strong generative capabilities of LLMs.",
      "tldr_zh": "这篇论文提出一个新任务：检测语言模型（Language Models）中编辑过的知识（Knowledge Edits），以区分模型输出是基于预训练知识还是后续编辑，从而提升生成模型的透明度和用户信任。研究方法使用隐藏状态表示（hidden state representations）和概率分布（probability distributions）作为特征，输入到AdaBoost分类器中进行分类。实验结果显示，该分类器在四个知识编辑方法（KEs）和两个数据集上，仅需少量数据即可建立强有力的基线，并在跨域设置中保持性能稳定。然而，区分编辑知识与未编辑但相关知识的挑战突出，强调了进一步研究的必要性。该工作为应对LLMs的恶意编辑奠定了基础。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at NAACL Main 2025",
      "pdf_url": "http://arxiv.org/pdf/2405.02765v3",
      "published_date": "2024-05-04 22:02:24 UTC",
      "updated_date": "2025-02-10 12:35:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:54:49.712897"
    },
    {
      "arxiv_id": "2405.06676v1",
      "title": "EDA Corpus: A Large Language Model Dataset for Enhanced Interaction with OpenROAD",
      "title_zh": "翻译失败",
      "authors": [
        "Bing-Yue Wu",
        "Utsav Sharma",
        "Sai Rahul Dhanvi Kankipati",
        "Ajay Yadav",
        "Bintu Kappil George",
        "Sai Ritish Guntupalli",
        "Austin Rovinski",
        "Vidya A. Chhabria"
      ],
      "abstract": "Large language models (LLMs) serve as powerful tools for design, providing\ncapabilities for both task automation and design assistance. Recent\nadvancements have shown tremendous potential for facilitating LLM integration\ninto the chip design process; however, many of these works rely on data that\nare not publicly available and/or not permissively licensed for use in LLM\ntraining and distribution. In this paper, we present a solution aimed at\nbridging this gap by introducing an open-source dataset tailored for OpenROAD,\na widely adopted open-source EDA toolchain. The dataset features over 1000 data\npoints and is structured in two formats: (i) a pairwise set comprised of\nquestion prompts with prose answers, and (ii) a pairwise set comprised of code\nprompts and their corresponding OpenROAD scripts. By providing this dataset, we\naim to facilitate LLM-focused research within the EDA domain. The dataset is\navailable at https://github.com/OpenROAD-Assistant/EDA-Corpus.",
      "tldr_zh": "这篇论文介绍了 EDA Corpus，这是一个开源数据集，旨在提升大语言模型 (LLMs) 与 OpenROAD 工具链的交互，以解决现有研究中数据非公开或许可限制的问题。数据集包含超过 1000 个数据点，分为两类：问题提示与散文答案的配对集，以及代码提示与对应 OpenROAD 脚本的配对集。通过提供这一资源，论文促进了 EDA 领域 LLM 相关的研究，并已在 GitHub 上公开可用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.AR"
      ],
      "primary_category": "cs.CL",
      "comment": "Under review at Workshop on LLM-Aided Design (LAD'24)",
      "pdf_url": "http://arxiv.org/pdf/2405.06676v1",
      "published_date": "2024-05-04 21:29:37 UTC",
      "updated_date": "2024-05-04 21:29:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:55:02.762044"
    },
    {
      "arxiv_id": "2405.02754v1",
      "title": "Implicit Safe Set Algorithm for Provably Safe Reinforcement Learning",
      "title_zh": "隐式安全集算法，用于可证明安全的强化学习",
      "authors": [
        "Weiye Zhao",
        "Tairan He",
        "Feihan Li",
        "Changliu Liu"
      ],
      "abstract": "Deep reinforcement learning (DRL) has demonstrated remarkable performance in\nmany continuous control tasks. However, a significant obstacle to the\nreal-world application of DRL is the lack of safety guarantees. Although DRL\nagents can satisfy system safety in expectation through reward shaping,\ndesigning agents to consistently meet hard constraints (e.g., safety\nspecifications) at every time step remains a formidable challenge. In contrast,\nexisting work in the field of safe control provides guarantees on persistent\nsatisfaction of hard safety constraints. However, these methods require\nexplicit analytical system dynamics models to synthesize safe control, which\nare typically inaccessible in DRL settings. In this paper, we present a\nmodel-free safe control algorithm, the implicit safe set algorithm, for\nsynthesizing safeguards for DRL agents that ensure provable safety throughout\ntraining. The proposed algorithm synthesizes a safety index (barrier\ncertificate) and a subsequent safe control law solely by querying a black-box\ndynamic function (e.g., a digital twin simulator). Moreover, we theoretically\nprove that the implicit safe set algorithm guarantees finite time convergence\nto the safe set and forward invariance for both continuous-time and\ndiscrete-time systems. We validate the proposed algorithm on the\nstate-of-the-art Safety Gym benchmark, where it achieves zero safety violations\nwhile gaining $95\\% \\pm 9\\%$ cumulative reward compared to state-of-the-art\nsafe DRL methods. Furthermore, the resulting algorithm scales well to\nhigh-dimensional systems with parallel computing.",
      "tldr_zh": "本论文提出了一种无模型的隐式安全集算法（implicit safe set algorithm），旨在为深度强化学习（DRL）提供可证明的安全保证，解决现有方法在处理硬安全约束时的局限性。该算法通过查询黑箱动态函数（如数字孪生模拟器）合成安全指数（barrier certificate）和安全控制律，确保DRL代理在训练过程中实现有限时间收敛到安全集，并维持前向不变性，适用于连续和离散系统。实验在Safety Gym基准上验证，该算法实现了零安全违规，同时获得95% ± 9%的累积奖励，优于现有安全DRL方法，并展示了在高维系统的良好扩展性。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "submissions to Journal of Artificial Intelligence Research. arXiv\n  admin note: text overlap with arXiv:2308.13140",
      "pdf_url": "http://arxiv.org/pdf/2405.02754v1",
      "published_date": "2024-05-04 20:59:06 UTC",
      "updated_date": "2024-05-04 20:59:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:55:13.316752"
    },
    {
      "arxiv_id": "2405.02750v1",
      "title": "Enhancing Contextual Understanding in Large Language Models through Contrastive Decoding",
      "title_zh": "通过对比解码增强大语言模型中的上下文理解",
      "authors": [
        "Zheng Zhao",
        "Emilio Monti",
        "Jens Lehmann",
        "Haytham Assem"
      ],
      "abstract": "Large language models (LLMs) tend to inadequately integrate input context\nduring text generation, relying excessively on encoded prior knowledge in model\nparameters, potentially resulting in generated text with factual\ninconsistencies or contextually unfaithful content. LLMs utilize two primary\nknowledge sources: 1) prior (parametric) knowledge from pretraining, and 2)\ncontextual (non-parametric) knowledge from input prompts. The study addresses\nthe open question of how LLMs effectively balance these knowledge sources\nduring the generation process, specifically in the context of open-domain\nquestion answering. To address this issue, we introduce a novel approach\nintegrating contrastive decoding with adversarial irrelevant passages as\nnegative samples to enhance robust context grounding during generation.\nNotably, our method operates at inference time without requiring further\ntraining. We conduct comprehensive experiments to demonstrate its applicability\nand effectiveness, providing empirical evidence showcasing its superiority over\nexisting methodologies. Our code is publicly available at:\nhttps://github.com/amazon-science/ContextualUnderstanding-ContrastiveDecoding.",
      "tldr_zh": "大型语言模型 (LLMs) 在文本生成过程中往往过度依赖预训练参数知识 (prior knowledge)，而忽略输入上下文知识 (contextual knowledge)，导致生成内容可能出现事实不一致或上下文不忠实的问题。为解决这一问题，本文提出了一种新方法，将对比解码 (contrastive decoding) 与不相关段落作为负面样本相结合，以增强上下文 grounding，且该方法无需额外训练，仅在推理时应用。实验在开放域问答任务上证明，该方法比现有方法更有效，并提供了公开代码 (https://github.com/amazon-science/ContextualUnderstanding-ContrastiveDecoding)。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to NAACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.02750v1",
      "published_date": "2024-05-04 20:38:41 UTC",
      "updated_date": "2024-05-04 20:38:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:55:25.728873"
    },
    {
      "arxiv_id": "2405.02738v1",
      "title": "Relations Prediction for Knowledge Graph Completion using Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Sakher Khalil Alqaaidi",
        "Krzysztof Kochut"
      ],
      "abstract": "Knowledge Graphs have been widely used to represent facts in a structured\nformat. Due to their large scale applications, knowledge graphs suffer from\nbeing incomplete. The relation prediction task obtains knowledge graph\ncompletion by assigning one or more possible relations to each pair of nodes.\nIn this work, we make use of the knowledge graph node names to fine-tune a\nlarge language model for the relation prediction task. By utilizing the node\nnames only we enable our model to operate sufficiently in the inductive\nsettings. Our experiments show that we accomplish new scores on a widely used\nknowledge graph benchmark.",
      "tldr_zh": "本研究针对知识图谱（Knowledge Graphs）的缺失问题，提出了一种使用大型语言模型（Large Language Models）来进行关系预测（Relations Prediction）的框架，以完成知识图谱。方法通过微调模型，仅利用节点名称作为输入，使其在归纳设置（inductive settings）中也能有效运行。实验结果显示，该方法在常用知识图谱基准上取得了新的高分，证明了其在知识图谱补全任务中的潜力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.4"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.02738v1",
      "published_date": "2024-05-04 19:04:51 UTC",
      "updated_date": "2024-05-04 19:04:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:55:36.569971"
    },
    {
      "arxiv_id": "2405.03714v2",
      "title": "UniDEC : Unified Dual Encoder and Classifier Training for Extreme Multi-Label Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Siddhant Kharbanda",
        "Devaansh Gupta",
        "Gururaj K",
        "Pankaj Malhotra",
        "Amit Singh",
        "Cho-Jui Hsieh",
        "Rohit Babbar"
      ],
      "abstract": "Extreme Multi-label Classification (XMC) involves predicting a subset of\nrelevant labels from an extremely large label space, given an input query and\nlabels with textual features. Models developed for this problem have\nconventionally made use of dual encoder (DE) to embed the queries and label\ntexts and one-vs-all (OvA) classifiers to rerank the shortlisted labels by the\nDE. While such methods have shown empirical success, a major drawback is their\ncomputational cost, often requiring upto 16 GPUs to train on the largest public\ndataset. Such a high cost is a consequence of calculating the loss over the\nentire label space. While shortlisting strategies have been proposed for\nclassifiers, we aim to study such methods for the DE framework. In this work,\nwe develop UniDEC, a loss-independent, end-to-end trainable framework which\ntrains the DE and classifier together in a unified manner with a multi-class\nloss, while reducing the computational cost by 4-16x. This is done via the\nproposed pick-some-label (PSL) reduction, which aims to compute the loss on\nonly a subset of positive and negative labels. These labels are carefully\nchosen in-batch so as to maximise their supervisory signals. Not only does the\nproposed framework achieve state-of-the-art results on datasets with labels in\nthe order of millions, it is also computationally and resource efficient in\nachieving this performance on a single GPU. Code is made available at\nhttps://github.com/the-catalyst/UniDEC.",
      "tldr_zh": "本研究针对 Extreme Multi-Label Classification (XMC) 任务，提出 UniDEC 框架，以统一训练 Dual Encoder (DE) 和分类器的方式，使用 multi-class loss 减少计算成本。UniDEC 引入 pick-some-label (PSL) reduction 方法，通过在批次中选择具有最大监督信号的正负标签子集，仅计算这些标签的损失，从而将训练效率提高 4-16 倍。实验结果显示，UniDEC 在百万级标签数据集上实现 state-of-the-art 性能，且能在单 GPU 上高效运行，显著降低了传统 One-vs-All (OvA) 方法的资源需求。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.03714v2",
      "published_date": "2024-05-04 17:27:51 UTC",
      "updated_date": "2025-03-03 19:29:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:55:49.972857"
    },
    {
      "arxiv_id": "2405.02711v1",
      "title": "The Role of AI in Peer Support for Young People: A Study of Preferences for Human- and AI-Generated Responses",
      "title_zh": "人工智能在青年同伴支持中的作用：对人类生成和人工智能生成响应的偏好研究",
      "authors": [
        "Jordyn Young",
        "Laala M Jawara",
        "Diep N Nguyen",
        "Brian Daly",
        "Jina Huh-Yoo",
        "Afsaneh Razi"
      ],
      "abstract": "Generative Artificial Intelligence (AI) is integrated into everyday\ntechnology, including news, education, and social media. AI has further\npervaded private conversations as conversational partners, auto-completion, and\nresponse suggestions. As social media becomes young people's main method of\npeer support exchange, we need to understand when and how AI can facilitate and\nassist in such exchanges in a beneficial, safe, and socially appropriate way.\nWe asked 622 young people to complete an online survey and evaluate blinded\nhuman- and AI-generated responses to help-seeking messages. We found that\nparticipants preferred the AI-generated response to situations about\nrelationships, self-expression, and physical health. However, when addressing a\nsensitive topic, like suicidal thoughts, young people preferred the human\nresponse. We also discuss the role of training in online peer support exchange\nand its implications for supporting young people's well-being. Disclaimer: This\npaper includes sensitive topics, including suicide ideation. Reader discretion\nis advised.",
      "tldr_zh": "本研究调查了 AI 在年轻人同伴支持（peer support）中的作用，通过让 622 名年轻人评估匿名的人类生成和 AI 生成响应。结果显示，参与者更偏好 AI 生成的响应来处理关系、自表达和身体健康话题，但在敏感议题如自杀念头时，更倾向于人类响应。该研究讨论了在线同伴支持培训的潜在影响，并为安全、适当使用 AI 以提升年轻人福祉提供了重要启示。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.02711v1",
      "published_date": "2024-05-04 16:53:19 UTC",
      "updated_date": "2024-05-04 16:53:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:56:00.959863"
    },
    {
      "arxiv_id": "2405.06674v1",
      "title": "Open-SQL Framework: Enhancing Text-to-SQL on Open-source Large Language Models",
      "title_zh": "Open-SQL 框架：针对开源大语言模型增强 Text-to-SQL",
      "authors": [
        "Xiaojun Chen",
        "Tianle Wang",
        "Tianhao Qiu",
        "Jianbin Qin",
        "Min Yang"
      ],
      "abstract": "Despite the success of large language models (LLMs) in Text-to-SQL tasks,\nopen-source LLMs encounter challenges in contextual understanding and response\ncoherence. To tackle these issues, we present \\ours, a systematic methodology\ntailored for Text-to-SQL with open-source LLMs. Our contributions include a\ncomprehensive evaluation of open-source LLMs in Text-to-SQL tasks, the\n\\openprompt strategy for effective question representation, and novel\nstrategies for supervised fine-tuning. We explore the benefits of\nChain-of-Thought in step-by-step inference and propose the \\openexample method\nfor enhanced few-shot learning. Additionally, we introduce token-efficient\ntechniques, such as \\textbf{Variable-length Open DB Schema}, \\textbf{Target\nColumn Truncation}, and \\textbf{Example Column Truncation}, addressing\nchallenges in large-scale databases. Our findings emphasize the need for\nfurther investigation into the impact of supervised fine-tuning on contextual\nlearning capabilities. Remarkably, our method significantly improved Llama2-7B\nfrom 2.54\\% to 41.04\\% and Code Llama-7B from 14.54\\% to 48.24\\% on the\nBIRD-Dev dataset. Notably, the performance of Code Llama-7B surpassed GPT-4\n(46.35\\%) on the BIRD-Dev dataset.",
      "tldr_zh": "本研究提出Open-SQL框架，以提升开源大型语言模型(LLMs)在Text-to-SQL任务中的上下文理解和响应连贯性。\n框架的核心贡献包括Openprompt策略用于问句表示、Chain-of-Thought推理支持逐步推断、Openexample方法增强少样本学习，以及令牌高效技术如Variable-length Open DB Schema、Target Column Truncation和Example Column Truncation来处理大规模数据库挑战。\n实验结果显示，该框架显著提高了模型性能，将Llama2-7B从2.54%提升至41.04%，Code Llama-7B从14.54%提升至48.24%，并在BIRD-Dev数据集上超过了GPT-4的46.35%。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06674v1",
      "published_date": "2024-05-04 15:40:17 UTC",
      "updated_date": "2024-05-04 15:40:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:56:14.275854"
    },
    {
      "arxiv_id": "2405.02698v1",
      "title": "Stable Diffusion Dataset Generation for Downstream Classification Tasks",
      "title_zh": "翻译失败",
      "authors": [
        "Eugenio Lomurno",
        "Matteo D'Oria",
        "Matteo Matteucci"
      ],
      "abstract": "Recent advances in generative artificial intelligence have enabled the\ncreation of high-quality synthetic data that closely mimics real-world data.\nThis paper explores the adaptation of the Stable Diffusion 2.0 model for\ngenerating synthetic datasets, using Transfer Learning, Fine-Tuning and\ngeneration parameter optimisation techniques to improve the utility of the\ndataset for downstream classification tasks. We present a class-conditional\nversion of the model that exploits a Class-Encoder and optimisation of key\ngeneration parameters. Our methodology led to synthetic datasets that, in a\nthird of cases, produced models that outperformed those trained on real\ndatasets.",
      "tldr_zh": "本论文探讨了使用 Stable Diffusion 2.0 生成合成数据集的方法，以支持下游分类任务。研究团队通过 Transfer Learning、Fine-Tuning 和生成参数优化技术，结合 Class-Encoder 的类条件模型，显著提升了合成数据的效用。结果显示，在三分之一的案例中，使用这些合成数据集训练的模型性能超过了基于真实数据集的模型。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.02698v1",
      "published_date": "2024-05-04 15:37:22 UTC",
      "updated_date": "2024-05-04 15:37:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:56:25.231210"
    },
    {
      "arxiv_id": "2405.02696v1",
      "title": "DiffuseTrace: A Transparent and Flexible Watermarking Scheme for Latent Diffusion Model",
      "title_zh": "DiffuseTrace：一种透明且灵活的水印方案，用于潜在扩散模型",
      "authors": [
        "Liangqi Lei",
        "Keke Gai",
        "Jing Yu",
        "Liehuang Zhu"
      ],
      "abstract": "Latent Diffusion Models (LDMs) enable a wide range of applications but raise\nethical concerns regarding illegal utilization.Adding watermarks to generative\nmodel outputs is a vital technique employed for copyright tracking and\nmitigating potential risks associated with AI-generated content. However,\npost-hoc watermarking techniques are susceptible to evasion. Existing\nwatermarking methods for LDMs can only embed fixed messages. Watermark message\nalteration requires model retraining. The stability of the watermark is\ninfluenced by model updates and iterations. Furthermore, the current\nreconstruction-based watermark removal techniques utilizing variational\nautoencoders (VAE) and diffusion models have the capability to remove a\nsignificant portion of watermarks. Therefore, we propose a novel technique\ncalled DiffuseTrace. The goal is to embed invisible watermarks in all generated\nimages for future detection semantically. The method establishes a unified\nrepresentation of the initial latent variables and the watermark information\nthrough training an encoder-decoder model. The watermark information is\nembedded into the initial latent variables through the encoder and integrated\ninto the sampling process. The watermark information is extracted by reversing\nthe diffusion process and utilizing the decoder. DiffuseTrace does not rely on\nfine-tuning of the diffusion model components. The watermark is embedded into\nthe image space semantically without compromising image quality. The\nencoder-decoder can be utilized as a plug-in in arbitrary diffusion models. We\nvalidate through experiments the effectiveness and flexibility of DiffuseTrace.\nDiffuseTrace holds an unprecedented advantage in combating the latest attacks\nbased on variational autoencoders and Diffusion Models.",
      "tldr_zh": "这篇论文针对Latent Diffusion Models (LDMs)的非法利用问题，提出了一种透明且灵活的水印方案DiffuseTrace，用于在生成图像中嵌入不可见水印以便后续语义检测。该方法通过训练一个encoder-decoder模型，将水印信息统一表示并嵌入到初始潜在变量中，并在扩散采样过程中整合，而无需微调扩散模型组件，从而保持图像质量不变。DiffuseTrace支持作为插件应用于任意扩散模型，并能有效对抗基于variational autoencoders (VAE)和Diffusion Models的攻击。实验结果证明了其有效性、灵活性和在版权追踪方面的优势。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.02696v1",
      "published_date": "2024-05-04 15:32:57 UTC",
      "updated_date": "2024-05-04 15:32:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:56:38.329656"
    },
    {
      "arxiv_id": "2405.02686v1",
      "title": "Boosting 3D Neuron Segmentation with 2D Vision Transformer Pre-trained on Natural Images",
      "title_zh": "利用在自然图像上预训练的2D Vision Transformer提升3D神经元分割",
      "authors": [
        "Yik San Cheng",
        "Runkai Zhao",
        "Heng Wang",
        "Hanchuan Peng",
        "Weidong Cai"
      ],
      "abstract": "Neuron reconstruction, one of the fundamental tasks in neuroscience, rebuilds\nneuronal morphology from 3D light microscope imaging data. It plays a critical\nrole in analyzing the structure-function relationship of neurons in the nervous\nsystem. However, due to the scarcity of neuron datasets and high-quality SWC\nannotations, it is still challenging to develop robust segmentation methods for\nsingle neuron reconstruction. To address this limitation, we aim to distill the\nconsensus knowledge from massive natural image data to aid the segmentation\nmodel in learning the complex neuron structures. Specifically, in this work, we\npropose a novel training paradigm that leverages a 2D Vision Transformer model\npre-trained on large-scale natural images to initialize our Transformer-based\n3D neuron segmentation model with a tailored 2D-to-3D weight transferring\nstrategy. Our method builds a knowledge sharing connection between the abundant\nnatural and the scarce neuron image domains to improve the 3D neuron\nsegmentation ability in a data-efficiency manner. Evaluated on a popular\nbenchmark, BigNeuron, our method enhances neuron segmentation performance by\n8.71% over the model trained from scratch with the same amount of training\nsamples.",
      "tldr_zh": "本文针对神经元重建任务中的数据稀缺问题，提出一种新颖训练范式，利用在大型自然图像上预训练的 2D Vision Transformer 模型，通过定制的 2D-to-3D 权重转移策略来初始化基于 Transformer 的 3D 神经元分割模型，从而提升模型对复杂神经元结构的学习能力。该方法从海量自然图像领域提取共识知识，帮助解决 SWC 注释不足的挑战。在 BigNeuron 基准测试中，与从零开始训练的模型相比，该方法将神经元分割性能提高了 8.71%。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "3 pages",
      "pdf_url": "http://arxiv.org/pdf/2405.02686v1",
      "published_date": "2024-05-04 14:57:28 UTC",
      "updated_date": "2024-05-04 14:57:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:56:49.961578"
    },
    {
      "arxiv_id": "2405.02685v1",
      "title": "FedProK: Trustworthy Federated Class-Incremental Learning via Prototypical Feature Knowledge Transfer",
      "title_zh": "FedProK：通过原型特征知识转移的可信联邦类增量学习",
      "authors": [
        "Xin Gao",
        "Xin Yang",
        "Hao Yu",
        "Yan Kang",
        "Tianrui Li"
      ],
      "abstract": "Federated Class-Incremental Learning (FCIL) focuses on continually\ntransferring the previous knowledge to learn new classes in dynamic Federated\nLearning (FL). However, existing methods do not consider the trustworthiness of\nFCIL, i.e., improving continual utility, privacy, and efficiency\nsimultaneously, which is greatly influenced by catastrophic forgetting and data\nheterogeneity among clients. To address this issue, we propose FedProK\n(Federated Prototypical Feature Knowledge Transfer), leveraging prototypical\nfeature as a novel representation of knowledge to perform spatial-temporal\nknowledge transfer. Specifically, FedProK consists of two components: (1)\nfeature translation procedure on the client side by temporal knowledge transfer\nfrom the learned classes and (2) prototypical knowledge fusion on the server\nside by spatial knowledge transfer among clients. Extensive experiments\nconducted in both synchronous and asynchronous settings demonstrate that our\nFedProK outperforms the other state-of-the-art methods in three perspectives of\ntrustworthiness, validating its effectiveness in selectively transferring\nspatial-temporal knowledge.",
      "tldr_zh": "本文提出 FedProK，一种基于原型特征知识转移的联邦增量学习（FCIL）框架，旨在同时提升持续实用性、隐私和效率，以应对灾难性遗忘（catastrophic forgetting）和数据异质性（data heterogeneity）。FedProK 包括客户端侧的特征翻译过程（通过时间知识转移）和服务器侧的原型知识融合（通过空间知识转移），实现空间-时间知识的 selective 转移。实验在同步和异步设置下表明，FedProK 在可信度的三个方面优于现有最先进方法，验证了其有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.02685v1",
      "published_date": "2024-05-04 14:57:09 UTC",
      "updated_date": "2024-05-04 14:57:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:57:02.800978"
    },
    {
      "arxiv_id": "2405.02678v3",
      "title": "Position: Quo Vadis, Unsupervised Time Series Anomaly Detection?",
      "title_zh": "翻译失败",
      "authors": [
        "M. Saquib Sarfraz",
        "Mei-Yen Chen",
        "Lukas Layer",
        "Kunyu Peng",
        "Marios Koulakis"
      ],
      "abstract": "The current state of machine learning scholarship in Timeseries Anomaly\nDetection (TAD) is plagued by the persistent use of flawed evaluation metrics,\ninconsistent benchmarking practices, and a lack of proper justification for the\nchoices made in novel deep learning-based model designs. Our paper presents a\ncritical analysis of the status quo in TAD, revealing the misleading track of\ncurrent research and highlighting problematic methods, and evaluation\npractices. Our position advocates for a shift in focus from solely pursuing\nnovel model designs to improving benchmarking practices, creating non-trivial\ndatasets, and critically evaluating the utility of complex methods against\nsimpler baselines. Our findings demonstrate the need for rigorous evaluation\nprotocols, the creation of simple baselines, and the revelation that\nstate-of-the-art deep anomaly detection models effectively learn linear\nmappings. These findings suggest the need for more exploration and development\nof simple and interpretable TAD methods. The increment of model complexity in\nthe state-of-the-art deep-learning based models unfortunately offers very\nlittle improvement. We offer insights and suggestions for the field to move\nforward.\n  Code: https://github.com/ssarfraz/QuoVadisTAD",
      "tldr_zh": "这篇论文对无监督时间序列异常检测（Time Series Anomaly Detection, TAD）领域的机器学习研究现状进行批判，指出现有研究存在有缺陷的评估指标（flawed evaluation metrics）、不一致的基准测试实践（inconsistent benchmarking practices）以及对新型深度学习模型设计缺乏正当理由的问题。作者分析发现，最先进的深度异常检测模型实际上主要学习线性映射，而增加模型复杂度带来的改进微乎其微。论文主张将研究重点从追求复杂模型转向完善基准测试、创建非平凡数据集，并优先开发简单且可解释的TAD方法，以推动领域向前发展。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "ICML 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.02678v3",
      "published_date": "2024-05-04 14:43:31 UTC",
      "updated_date": "2024-06-05 13:12:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:57:12.458246"
    },
    {
      "arxiv_id": "2405.02675v1",
      "title": "Quranic Audio Dataset: Crowdsourced and Labeled Recitation from Non-Arabic Speakers",
      "title_zh": "翻译失败",
      "authors": [
        "Raghad Salameh",
        "Mohamad Al Mdfaa",
        "Nursultan Askarbekuly",
        "Manuel Mazzara"
      ],
      "abstract": "This paper addresses the challenge of learning to recite the Quran for\nnon-Arabic speakers. We explore the possibility of crowdsourcing a carefully\nannotated Quranic dataset, on top of which AI models can be built to simplify\nthe learning process. In particular, we use the volunteer-based crowdsourcing\ngenre and implement a crowdsourcing API to gather audio assets. We integrated\nthe API into an existing mobile application called NamazApp to collect audio\nrecitations. We developed a crowdsourcing platform called Quran Voice for\nannotating the gathered audio assets. As a result, we have collected around\n7000 Quranic recitations from a pool of 1287 participants across more than 11\nnon-Arabic countries, and we have annotated 1166 recitations from the dataset\nin six categories. We have achieved a crowd accuracy of 0.77, an inter-rater\nagreement of 0.63 between the annotators, and 0.89 between the labels assigned\nby the algorithm and the expert judgments.",
      "tldr_zh": "本论文针对非阿拉伯语使用者学习古兰经朗读的挑战，提出通过众包方式构建一个标注音频数据集，以简化AI模型的应用。研究者开发了众包API并集成到NamazApp移动应用中收集音频，同时使用Quran Voice平台进行标注，最终从1287名参与者（来自11个以上非阿拉伯国家）收集约7000条音频，并标注了1166条音频分成六类。结果显示，众包准确率达到0.77，标注者间一致性为0.63，算法标签与专家判断一致性为0.89，为古兰经朗读学习提供了一个宝贵资源。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.02675v1",
      "published_date": "2024-05-04 14:29:05 UTC",
      "updated_date": "2024-05-04 14:29:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:57:25.875985"
    },
    {
      "arxiv_id": "2407.10296v1",
      "title": "Modern Information Technologies in Scientific Research and Educational Activities",
      "title_zh": "现代信息技术在科学研究和教育活动中的应用",
      "authors": [
        "Kyrylo Malakhov",
        "Vadislav Kaverinskiy",
        "Liliia Ivanova",
        "Oleksandr Romanyuk",
        "Oksana Romaniuk",
        "Svitlana Voinova",
        "Sergii Kotlyk",
        "Oksana Sokolova"
      ],
      "abstract": "The monograph summarizes and analyzes the current state of scientific\nresearch in the field of interactive artificial intelligence systems, text\ngeneration systems, diagnostics of the competitiveness of specialists, in the\nareas of correct color rendering in image formation, informatization of the\nwork of graduate students, accessible technology for creating three-dimensional\n3D models. The monograph will be useful both to specialists and employees of\ncompanies working in the IT field, as well as teachers, masters, students and\ngraduate students of higher educational institutions, as well as anyone\ninterested in issues related to information technology. The monograph was\ncompiled based on the results of the 16-th international scientific and\npractical conference Information technologies and automation - 2023, which took\nplace in October 2023 at Odessa National University of Technology.",
      "tldr_zh": "这本专著总结了当前在互动人工智能系统（interactive artificial intelligence systems）、文本生成系统（text generation systems）、专业竞争力诊断以及图像正确颜色渲染等领域的研究现状，并探讨了研究生工作信息化和创建三维3D模型的可访问技术。专著基于2023年10月在敖德萨国家理工大学举行的第16届国际科学实践会议“Information technologies and automation - 2023”的成果进行编译。针对IT领域专家、教育工作者、硕士、学生和研究生等读者，它提供了这些技术在科研和教育活动中的应用分析和见解。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.GR"
      ],
      "primary_category": "cs.CY",
      "comment": "Monograph Scientific publication (issue). Published By Iowa State\n  University Digital Press. ISBN 978-1-958291-07-8; 273 pages; Published May 1,\n  2024",
      "pdf_url": "http://arxiv.org/pdf/2407.10296v1",
      "published_date": "2024-05-04 14:24:47 UTC",
      "updated_date": "2024-05-04 14:24:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:57:40.004000"
    },
    {
      "arxiv_id": "2405.15792v1",
      "title": "IQLS: Framework for leveraging Metadata to enable Large Language Model based queries to complex, versatile Data",
      "title_zh": "IQLS：利用元数据启用基于大型语言模型的查询框架，以处理复杂多变的数据",
      "authors": [
        "Sami Azirar",
        "Hossam A. Gabbar",
        "Chaouki Regoui"
      ],
      "abstract": "As the amount and complexity of data grows, retrieving it has become a more\ndifficult task that requires greater knowledge and resources. This is\nespecially true for the logistics industry, where new technologies for data\ncollection provide tremendous amounts of interconnected real-time data. The\nIntelligent Query and Learning System (IQLS) simplifies the process by allowing\nnatural language use to simplify data retrieval . It maps structured data into\na framework based on the available metadata and available data models. This\nframework creates an environment for an agent powered by a Large Language\nModel. The agent utilizes the hierarchical nature of the data to filter\niteratively by making multiple small context-aware decisions instead of\none-shot data retrieval. After the Data filtering, the IQLS enables the agent\nto fulfill tasks given by the user query through interfaces. These interfaces\nrange from multimodal transportation information retrieval to route planning\nunder multiple constraints. The latter lets the agent define a dynamic object,\nwhich is determined based on the query parameters. This object represents a\ndriver capable of navigating a road network. The road network is depicted as a\ngraph with attributes based on the data. Using a modified version of the\nDijkstra algorithm, the optimal route under the given constraints can be\ndetermined. Throughout the entire process, the user maintains the ability to\ninteract and guide the system. The IQLS is showcased in a case study on the\nCanadian logistics sector, allowing geospatial, visual, tabular and text data\nto be easily queried semantically in natural language.",
      "tldr_zh": "这篇论文介绍了 IQLS 框架，它利用元数据和 Large Language Model (LLM) 启用对复杂、多样化数据的自然语言查询，特别是在物流行业中简化海量互联实时数据的检索过程。IQLS 通过将结构化数据映射到基于元数据的层次化框架，并使用 LLM 驱动的代理进行迭代过滤和决策，从而避免一次性检索的局限性。代理随后通过各种接口（如多模式运输信息检索和受约束的路线规划）执行用户任务，后者采用修改后的 Dijkstra 算法来优化路径。在加拿大物流行业的案例研究中，IQLS 展示了用户可交互的特性，并实现了对地理空间、视觉、表格和文本数据的语义查询。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15792v1",
      "published_date": "2024-05-04 13:44:05 UTC",
      "updated_date": "2024-05-04 13:44:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:57:49.919135"
    },
    {
      "arxiv_id": "2405.02664v3",
      "title": "MedPromptExtract (Medical Data Extraction Tool): Anonymization and Hi-fidelity Automated data extraction using NLP and prompt engineering",
      "title_zh": "翻译失败",
      "authors": [
        "Roomani Srivastava",
        "Suraj Prasad",
        "Lipika Bhat",
        "Sarvesh Deshpande",
        "Barnali Das",
        "Kshitij Jadhav"
      ],
      "abstract": "Introduction: The labour-intensive nature of data extraction from sources\nlike discharge summaries (DS) poses significant obstacles to the digitisation\nof medical records particularly for low- and middle-income countries (LMICs).\nIn this paper we present a completely automated method MedPromptExtract to\nefficiently extract data from DS while maintaining confidentiality. Methods:\nThe source of data was Discharge Summaries (DS) from Kokilaben Dhirubhai Ambani\nHospital (KDAH) of patients having Acute Kidney Injury (AKI). A pre-existing\ntool EIGEN which leverages semi-supervised learning techniques for\nhigh-fidelity information extraction was used to anonymize the DS, Natural\nLanguage Processing (NLP) was used to extract data from regular fields. We used\nPrompt Engineering and Large Language Model(LLM) to extract custom clinical\ninformation from free flowing text describing the patients stay in the\nhospital. Twelve features associated with occurrence of AKI were extracted. The\nLLM responses were validated against clinicians annotations. Results: The\nMedPromptExtracttool first subjected DS to the anonymization pipeline which\ntook three seconds per summary. Successful anonymization was verified by\nclinicians, thereafter NLP pipeline extracted structured text from the\nanonymized pdfs at the rate of 0.2 seconds per summary with 100%\naccuracy.Finally DS were analysed by the LLM pipeline using Gemini Pro for the\ntwelve features. Accuracy metrics were calculated by comparing model responses\nto clinicians annotations with seven features achieving AUCs above 0.9,\nindicating high fidelity of the extraction process. Conclusion:\nMedPromptExtract serves as an automated adaptable tool for efficient data\nextraction from medical records with a dynamic user interface. Keywords:\nDigitizing Medical Records, Automated Anonymisation, Information Retrieval,\nLarge Language Models, Prompt Engineering",
      "tldr_zh": "本文介绍了 MedPromptExtract 工具，这是一个完全自动化的系统，用于从医疗出院总结（DS）中提取数据，同时实现匿名化和高保真度，旨在解决数字化医疗记录的挑战，特别是针对低中收入国家（LMICs）。方法结合了 EIGEN 工具的半监督学习进行匿名化、Natural Language Processing (NLP) 提取结构化文本，以及 Prompt Engineering 和 Large Language Model (LLM) 如 Gemini Pro 来提取与 Acute Kidney Injury (AKI) 相关的 12 个自定义临床特征。实验结果显示，匿名化每份总结只需 3 秒，NLP 提取准确率达 100%，且七个特征的 AUC 超过 0.9，验证了工具的高效性和可靠性。该工具提供动态用户界面，具有可适应性，有助于推进 Automated Anonymisation 和 Information Retrieval 在医疗领域的应用。",
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.02664v3",
      "published_date": "2024-05-04 13:25:06 UTC",
      "updated_date": "2024-09-06 11:38:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:58:04.039043"
    },
    {
      "arxiv_id": "2405.02654v2",
      "title": "Enhancing Cooperation through Selective Interaction and Long-term Experiences in Multi-Agent Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Tianyu Ren",
        "Xiao-Jun Zeng"
      ],
      "abstract": "The significance of network structures in promoting group cooperation within\nsocial dilemmas has been widely recognized. Prior studies attribute this\nfacilitation to the assortment of strategies driven by spatial interactions.\nAlthough reinforcement learning has been employed to investigate the impact of\ndynamic interaction on the evolution of cooperation, there remains a lack of\nunderstanding about how agents develop neighbour selection behaviours and the\nformation of strategic assortment within an explicit interaction structure. To\naddress this, our study introduces a computational framework based on\nmulti-agent reinforcement learning in the spatial Prisoner's Dilemma game. This\nframework allows agents to select dilemma strategies and interacting neighbours\nbased on their long-term experiences, differing from existing research that\nrelies on preset social norms or external incentives. By modelling each agent\nusing two distinct Q-networks, we disentangle the coevolutionary dynamics\nbetween cooperation and interaction. The results indicate that long-term\nexperience enables agents to develop the ability to identify non-cooperative\nneighbours and exhibit a preference for interaction with cooperative ones. This\nemergent self-organizing behaviour leads to the clustering of agents with\nsimilar strategies, thereby increasing network reciprocity and enhancing group\ncooperation.",
      "tldr_zh": "本研究探讨了在多代理强化学习中，通过选择性互动和长期经验来增强社会困境（如Prisoner's Dilemma）中的群体合作。研究提出一个计算框架，让代理基于长期经验自主选择策略和互动邻居，使用两个独立的Q-networks来分离合作与互动的共同演化动态。结果显示，代理学会识别非合作邻居并偏好与合作邻居互动，导致策略相似性集群化，提高了网络互惠性和整体合作水平。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.GT"
      ],
      "primary_category": "cs.MA",
      "comment": "Accepted at IJCAI 2024 (33rd International Joint Conference on\n  Artificial Intelligence - Jeju)",
      "pdf_url": "http://arxiv.org/pdf/2405.02654v2",
      "published_date": "2024-05-04 12:42:55 UTC",
      "updated_date": "2024-08-18 14:30:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:58:16.663238"
    },
    {
      "arxiv_id": "2405.02653v2",
      "title": "Isopignistic Canonical Decomposition via Belief Evolution Network",
      "title_zh": "基于信念演化网络的 Isopignistic 规范分解",
      "authors": [
        "Qianli Zhou",
        "Tianxiang Zhan",
        "Yong Deng"
      ],
      "abstract": "Developing a general information processing model in uncertain environments\nis fundamental for the advancement of explainable artificial intelligence.\nDempster-Shafer theory of evidence is a well-known and effective reasoning\nmethod for representing epistemic uncertainty, which is closely related to\nsubjective probability theory and possibility theory. Although they can be\ntransformed to each other under some particular belief structures, there\nremains a lack of a clear and interpretable transformation process, as well as\na unified approach for information processing. In this paper, we aim to address\nthese issues from the perspectives of isopignistic belief functions and the\nhyper-cautious transferable belief model. Firstly, we propose an isopignistic\ntransformation based on the belief evolution network. This transformation\nallows for the adjustment of the information granule while retaining the\npotential decision outcome. The isopignistic transformation is integrated with\na hyper-cautious transferable belief model to establish a new canonical\ndecomposition. This decomposition offers a reverse path between the possibility\ndistribution and its isopignistic mass functions. The result of the canonical\ndecomposition, called isopignistic function, is an identical information\ncontent distribution to reflect the propensity and relative commitment degree\nof the BPA. Furthermore, this paper introduces a method to reconstruct the\nbasic belief assignment by adjusting the isopignistic function. It explores the\nadvantages of this approach in modeling and handling uncertainty within the\nhyper-cautious transferable belief model. More general, this paper establishes\na theoretical basis for building general models of artificial intelligence\nbased on probability theory, Dempster-Shafer theory, and possibility theory.",
      "tldr_zh": "本论文针对不确定环境中的信息处理，基于 Dempster-Shafer theory 提出了一种 Isopignistic transformation，通过 Belief Evolution Network 调整信息粒度，同时保留潜在决策结果。作者将此变换整合到 Hyper-Cautious Transferable Belief Model 中，建立新的 Canonical Decomposition，实现从可能性分布到 Isopignistic mass functions 的逆向路径，并提供重建 Basic Belief Assignment 的方法。总体而言，该方法增强了不确定性建模的解释性和通用性，为基于概率理论、Dempster-Shafer theory 和 possibility theory 的 AI 模型奠定理论基础。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.02653v2",
      "published_date": "2024-05-04 12:39:15 UTC",
      "updated_date": "2024-08-30 12:52:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:58:29.130302"
    },
    {
      "arxiv_id": "2405.02652v2",
      "title": "Deep Pulse-Signal Magnification for remote Heart Rate Estimation in Compressed Videos",
      "title_zh": "用于压缩视频中远程心率估计的深度脉搏信号放大",
      "authors": [
        "Joaquim Comas",
        "Adria Ruiz",
        "Federico Sukno"
      ],
      "abstract": "Recent advancements in data-driven approaches for remote photoplethysmography\n(rPPG) have significantly improved the accuracy of remote heart rate\nestimation. However, the performance of such approaches worsens considerably\nunder video compression, which is nevertheless necessary to store and transmit\nvideo data efficiently. In this paper, we present a novel approach to address\nthe impact of video compression on rPPG estimation, which leverages a\npulse-signal magnification transformation to adapt compressed videos to an\nuncompressed data domain in which the rPPG signal is magnified. We validate the\neffectiveness of our model by exhaustive evaluations on two publicly available\ndatasets, UCLA-rPPG and UBFC-rPPG, employing both intra- and cross-database\nperformance at several compression rates. Additionally, we assess the\nrobustness of our approach on two additional highly compressed and widely-used\ndatasets, MAHNOB-HCI and COHFACE, which reveal outstanding heart rate\nestimation results.",
      "tldr_zh": "本研究针对视频压缩对远程光体积描记法 (rPPG) 心率估计的影响，提出了一种新型深度脉搏信号放大 (Deep Pulse-Signal Magnification) 方法。该方法通过脉搏信号放大转换，将压缩视频适应到未压缩数据域，从而放大 rPPG 信号并提升估计准确性。在 UCLA-rPPG 和 UBFC-rPPG 数据集上进行的详尽评估，包括多种压缩率和内部/跨数据库测试，显示了显著性能改善。此外，在高度压缩的 MAHNOB-HCI 和 COHFACE 数据集上，该方法展现出出色的心率估计鲁棒性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.02652v2",
      "published_date": "2024-05-04 12:37:07 UTC",
      "updated_date": "2024-06-25 16:53:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:58:40.874569"
    },
    {
      "arxiv_id": "2405.02650v1",
      "title": "Identifying Narrative Patterns and Outliers in Holocaust Testimonies Using Topic Modeling",
      "title_zh": "使用主题建模识别 Holocaust 证词中的叙事模式和异常值",
      "authors": [
        "Maxim Ifergan",
        "Renana Keydar",
        "Omri Abend",
        "Amit Pinchevski"
      ],
      "abstract": "The vast collection of Holocaust survivor testimonies presents invaluable\nhistorical insights but poses challenges for manual analysis. This paper\nleverages advanced Natural Language Processing (NLP) techniques to explore the\nUSC Shoah Foundation Holocaust testimony corpus. By treating testimonies as\nstructured question-and-answer sections, we apply topic modeling to identify\nkey themes. We experiment with BERTopic, which leverages recent advances in\nlanguage modeling technology. We align testimony sections into fixed parts,\nrevealing the evolution of topics across the corpus of testimonies. This\nhighlights both a common narrative schema and divergences between subgroups\nbased on age and gender. We introduce a novel method to identify testimonies\nwithin groups that exhibit atypical topic distributions resembling those of\nother groups. This study offers unique insights into the complex narratives of\nHolocaust survivors, demonstrating the power of NLP to illuminate historical\ndiscourse and identify potential deviations in survivor experiences.",
      "tldr_zh": "本研究利用高级自然语言处理（NLP）技术分析 USC Shoah Foundation 的 Holocaust 幸存者证词语料库，通过主题建模（Topic Modeling）识别关键主题，并采用 BERTopic 模型处理结构化的问答部分。研究将证词对齐成固定部分，揭示了共同的叙事模式以及基于年龄和性别的子群体差异，同时引入新方法检测组内异常证词，这些证词的主题分布类似于其他组。整体结果展示了 NLP 在揭示历史话语和识别幸存者经历偏差方面的强大潜力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, 7 figures, LREC-COLING 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.02650v1",
      "published_date": "2024-05-04 12:29:00 UTC",
      "updated_date": "2024-05-04 12:29:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:58:52.228775"
    },
    {
      "arxiv_id": "2405.02649v1",
      "title": "Generic Multi-modal Representation Learning for Network Traffic Analysis",
      "title_zh": "通用多模态表示学习在",
      "authors": [
        "Luca Gioacchini",
        "Idilio Drago",
        "Marco Mellia",
        "Zied Ben Houidi",
        "Dario Rossi"
      ],
      "abstract": "Network traffic analysis is fundamental for network management,\ntroubleshooting, and security. Tasks such as traffic classification, anomaly\ndetection, and novelty discovery are fundamental for extracting operational\ninformation from network data and measurements. We witness the shift from deep\npacket inspection and basic machine learning to Deep Learning (DL) approaches\nwhere researchers define and test a custom DL architecture designed for each\nspecific problem. We here advocate the need for a general DL architecture\nflexible enough to solve different traffic analysis tasks. We test this idea by\nproposing a DL architecture based on generic data adaptation modules, followed\nby an integration module that summarises the extracted information into a\ncompact and rich intermediate representation (i.e. embeddings). The result is a\nflexible Multi-modal Autoencoder (MAE) pipeline that can solve different use\ncases. We demonstrate the architecture with traffic classification (TC) tasks\nsince they allow us to quantitatively compare results with state-of-the-art\nsolutions. However, we argue that the MAE architecture is generic and can be\nused to learn representations useful in multiple scenarios. On TC, the MAE\nperforms on par or better than alternatives while avoiding cumbersome feature\nengineering, thus streamlining the adoption of DL solutions for traffic\nanalysis.",
      "tldr_zh": "该论文探讨了网络流量分析在网络管理和安全中的重要性，包括流量分类（TC）、异常检测和新颖性发现等任务。作者提出了一种基于通用数据适应模块和整合模块的深度学习（DL）架构，即多模态自动编码器（MAE），用于生成紧凑的中间表示（embeddings），以灵活处理不同流量分析场景。该方法在TC任务上与最先进解决方案性能相当或更好，同时避免了繁琐的特征工程，从而简化了DL在流量分析中的应用，并证明了其通用性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.02649v1",
      "published_date": "2024-05-04 12:24:29 UTC",
      "updated_date": "2024-05-04 12:24:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:59:04.871904"
    },
    {
      "arxiv_id": "2405.02648v2",
      "title": "A Conformal Prediction Score that is Robust to Label Noise",
      "title_zh": "一种对标签噪声鲁棒的保形预测",
      "authors": [
        "Coby Penso",
        "Jacob Goldberger"
      ],
      "abstract": "Conformal Prediction (CP) quantifies network uncertainty by building a small\nprediction set with a pre-defined probability that the correct class is within\nthis set. In this study we tackle the problem of CP calibration based on a\nvalidation set with noisy labels. We introduce a conformal score that is robust\nto label noise. The noise-free conformal score is estimated using the noisy\nlabeled data and the noise level. In the test phase the noise-free score is\nused to form the prediction set. We applied the proposed algorithm to several\nstandard medical imaging classification datasets. We show that our method\noutperforms current methods by a large margin, in terms of the average size of\nthe prediction set, while maintaining the required coverage.",
      "tldr_zh": "本研究针对Conformal Prediction (CP) 在噪声标签验证集下的校准问题，提出了一种鲁棒的conformal score。该方法利用噪声标签数据和噪声水平来估计无噪声的conformal score，并在测试阶段用于构建预测集，从而维持预定义的覆盖率。实验结果显示，在多个标准医学图像分类数据集上，该方法显著降低了预测集的平均大小，比现有方法有较大优势。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.02648v2",
      "published_date": "2024-05-04 12:22:02 UTC",
      "updated_date": "2024-05-21 13:06:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:59:15.071247"
    },
    {
      "arxiv_id": "2405.03712v1",
      "title": "Your Network May Need to Be Rewritten: Network Adversarial Based on High-Dimensional Function Graph Decomposition",
      "title_zh": "您的网络可能需要",
      "authors": [
        "Xiaoyan Su",
        "Yinghao Zhu",
        "Run Li"
      ],
      "abstract": "In the past, research on a single low dimensional activation function in\nnetworks has led to internal covariate shift and gradient deviation problems. A\nrelatively small research area is how to use function combinations to provide\nproperty completion for a single activation function application. We propose a\nnetwork adversarial method to address the aforementioned challenges. This is\nthe first method to use different activation functions in a network. Based on\nthe existing activation functions in the current network, an adversarial\nfunction with opposite derivative image properties is constructed, and the two\nare alternately used as activation functions for different network layers. For\ncomplex situations, we propose a method of high-dimensional function graph\ndecomposition(HD-FGD), which divides it into different parts and then passes\nthrough a linear layer. After integrating the inverse of the partial\nderivatives of each decomposed term, we obtain its adversarial function by\nreferring to the computational rules of the decomposition process. The use of\nnetwork adversarial methods or the use of HD-FGD alone can effectively replace\nthe traditional MLP+activation function mode. Through the above methods, we\nhave achieved a substantial improvement over standard activation functions\nregarding both training efficiency and predictive accuracy. The article\naddresses the adversarial issues associated with several prevalent activation\nfunctions, presenting alternatives that can be seamlessly integrated into\nexisting models without any adverse effects. We will release the code as open\nsource after the conference review process is completed.",
      "tldr_zh": "本论文针对神经网络中单一激活函数导致的内部协变量偏移和梯度偏差问题，提出了一种网络对抗方法，该方法首次使用不同 activation functions，并在网络层交替应用具有相反导数图像属性的对抗函数。针对复杂情况，引入了高维函数图分解(HD-FGD)方法，将高维函数分解成不同部分，通过线性层处理并整合部分导数的逆来生成对抗函数。这些方法可有效替换传统 MLP + activation function 模式，在训练效率和预测准确性上显著优于标准激活函数，并提供可无缝集成的替代方案。作者计划开源代码以促进进一步应用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.03712v1",
      "published_date": "2024-05-04 11:22:30 UTC",
      "updated_date": "2024-05-04 11:22:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:59:29.244745"
    },
    {
      "arxiv_id": "2405.02637v1",
      "title": "TREC iKAT 2023: A Test Collection for Evaluating Conversational and Interactive Knowledge Assistants",
      "title_zh": "翻译失败",
      "authors": [
        "Mohammad Aliannejadi",
        "Zahra Abbasiantaeb",
        "Shubham Chatterjee",
        "Jeffery Dalton",
        "Leif Azzopardi"
      ],
      "abstract": "Conversational information seeking has evolved rapidly in the last few years\nwith the development of Large Language Models (LLMs), providing the basis for\ninterpreting and responding in a naturalistic manner to user requests. The\nextended TREC Interactive Knowledge Assistance Track (iKAT) collection aims to\nenable researchers to test and evaluate their Conversational Search Agents\n(CSA). The collection contains a set of 36 personalized dialogues over 20\ndifferent topics each coupled with a Personal Text Knowledge Base (PTKB) that\ndefines the bespoke user personas. A total of 344 turns with approximately\n26,000 passages are provided as assessments on relevance, as well as additional\nassessments on generated responses over four key dimensions: relevance,\ncompleteness, groundedness, and naturalness. The collection challenges CSA to\nefficiently navigate diverse personal contexts, elicit pertinent persona\ninformation, and employ context for relevant conversations. The integration of\na PTKB and the emphasis on decisional search tasks contribute to the uniqueness\nof this test collection, making it an essential benchmark for advancing\nresearch in conversational and interactive knowledge assistants.",
      "tldr_zh": "TREC iKAT 2023 是一个测试集合，旨在评估 Conversational Search Agents (CSA) 在对话式和交互式知识助理方面的性能，通过模拟用户请求的自然交互。集合包含36个个性化对话，覆盖20个主题，每个与 Personal Text Knowledge Base (PTKB) 关联，并提供344个回合和约26,000个段落的评估，包括相关性、完整性、groundedness 和 naturalness 等维度。它的独特之处在于整合 PTKB 和强调 decisional search tasks，帮助 CSA 高效处理多样化个人上下文、提取相关信息并进行相关对话，从而为推进 conversational 和 interactive knowledge assistants 的研究提供重要基准。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "To appear in SIGIR 2024. arXiv admin note: substantial text overlap\n  with arXiv:2401.01330",
      "pdf_url": "http://arxiv.org/pdf/2405.02637v1",
      "published_date": "2024-05-04 11:22:16 UTC",
      "updated_date": "2024-05-04 11:22:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:59:39.730112"
    },
    {
      "arxiv_id": "2405.02634v1",
      "title": "Onboard Out-of-Calibration Detection of Deep Learning Models using Conformal Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Protim Bhattacharjee",
        "Peter Jung"
      ],
      "abstract": "The black box nature of deep learning models complicate their usage in\ncritical applications such as remote sensing. Conformal prediction is a method\nto ensure trust in such scenarios. Subject to data exchangeability, conformal\nprediction provides finite sample coverage guarantees in the form of a\nprediction set that is guaranteed to contain the true class within a user\ndefined error rate. In this letter we show that conformal prediction algorithms\nare related to the uncertainty of the deep learning model and that this\nrelation can be used to detect if the deep learning model is\nout-of-calibration. Popular classification models like Resnet50, Densenet161,\nInceptionV3, and MobileNetV2 are applied on remote sensing datasets such as the\nEuroSAT to demonstrate how under noisy scenarios the model outputs become\nuntrustworthy. Furthermore an out-of-calibration detection procedure relating\nthe model uncertainty and the average size of the conformal prediction set is\npresented.",
      "tldr_zh": "本研究探讨了使用 Conformal Prediction 来检测深度学习模型的出校准问题，以提升其在关键应用（如遥感）中的可信度。作者发现 Conformal Prediction 能通过提供有限样本覆盖保证的预测集，与模型不确定性相关联，从而识别模型是否出校准。在实验中，他们在 EuroSAT 数据集上测试了 Resnet50、Densenet161、InceptionV3 和 MobileNetV2 等模型，发现噪声场景下模型输出变得不可靠，并提出了一种基于预测集平均大小和不确定性的检测程序。总的来说，此方法为确保深度学习模型在实际部署中的可靠性提供了新途径。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.02634v1",
      "published_date": "2024-05-04 11:05:52 UTC",
      "updated_date": "2024-05-04 11:05:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T05:59:53.735785"
    },
    {
      "arxiv_id": "2405.02628v1",
      "title": "Contrastive Dual-Interaction Graph Neural Network for Molecular Property Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Zexing Zhao",
        "Guangsi Shi",
        "Xiaopeng Wu",
        "Ruohua Ren",
        "Xiaojun Gao",
        "Fuyi Li"
      ],
      "abstract": "Molecular property prediction is a key component of AI-driven drug discovery\nand molecular characterization learning. Despite recent advances, existing\nmethods still face challenges such as limited ability to generalize, and\ninadequate representation of learning from unlabeled data, especially for tasks\nspecific to molecular structures. To address these limitations, we introduce\nDIG-Mol, a novel self-supervised graph neural network framework for molecular\nproperty prediction. This architecture leverages the power of contrast learning\nwith dual interaction mechanisms and unique molecular graph enhancement\nstrategies. DIG-Mol integrates a momentum distillation network with two\ninterconnected networks to efficiently improve molecular characterization. The\nframework's ability to extract key information about molecular structure and\nhigher-order semantics is supported by minimizing loss of contrast. We have\nestablished DIG-Mol's state-of-the-art performance through extensive\nexperimental evaluation in a variety of molecular property prediction tasks. In\naddition to demonstrating superior transferability in a small number of\nlearning scenarios, our visualizations highlight DIG-Mol's enhanced\ninterpretability and representation capabilities. These findings confirm the\neffectiveness of our approach in overcoming challenges faced by traditional\nmethods and mark a significant advance in molecular property prediction.",
      "tldr_zh": "本研究提出了一种名为 DIG-Mol 的新型自监督图神经网络框架，用于分子属性预测，以解决现有方法在泛化能力有限和从无标签数据学习不足等问题。DIG-Mol 利用对比学习（Contrastive Learning）、双交互机制（Dual-Interaction Mechanisms）和独特的分子图增强策略（Molecular Graph Enhancement Strategies），并整合动量蒸馏网络（Momentum Distillation Network）与两个互连网络来提升分子表征。实验结果显示，DIG-Mol 在多种分子属性预测任务中实现了最先进性能，并展示了在少样本学习场景中的优越转移能力，以及通过可视化增强的可解释性和表征能力。该框架标志着分子属性预测领域的重大进展，有助于AI驱动药物发现。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.02628v1",
      "published_date": "2024-05-04 10:09:27 UTC",
      "updated_date": "2024-05-04 10:09:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T06:00:05.029127"
    },
    {
      "arxiv_id": "2405.02612v3",
      "title": "Learning Linear Utility Functions From Pairwise Comparison Queries",
      "title_zh": "翻译失败",
      "authors": [
        "Luise Ge",
        "Brendan Juba",
        "Yevgeniy Vorobeychik"
      ],
      "abstract": "We study learnability of linear utility functions from pairwise comparison\nqueries. In particular, we consider two learning objectives. The first\nobjective is to predict out-of-sample responses to pairwise comparisons,\nwhereas the second is to approximately recover the true parameters of the\nutility function. We show that in the passive learning setting, linear\nutilities are efficiently learnable with respect to the first objective, both\nwhen query responses are uncorrupted by noise, and under Tsybakov noise when\nthe distributions are sufficiently \"nice\". In contrast, we show that utility\nparameters are not learnable for a large set of data distributions without\nstrong modeling assumptions, even when query responses are noise-free. Next, we\nproceed to analyze the learning problem in an active learning setting. In this\ncase, we show that even the second objective is efficiently learnable, and\npresent algorithms for both the noise-free and noisy query response settings.\nOur results thus exhibit a qualitative learnability gap between passive and\nactive learning from pairwise preference queries, demonstrating the value of\nthe ability to select pairwise queries for utility learning.",
      "tldr_zh": "这篇论文研究了从pairwise comparison queries学习linear utility functions的两个目标：预测样本外响应和近似恢复效用函数的真实参数。在passive learning设置中，第一目标可高效学习，尤其在无噪声或Tsybakov noise条件下且分布“良好”时；然而，第二目标在许多数据分布下不可学习，即使查询响应无噪声。在active learning设置中，两个目标都可高效实现，论文提供了相应算法，并展示了主动学习相对于被动学习的定性优势，突出了选择查询的价值。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Submitted to ECAI for review",
      "pdf_url": "http://arxiv.org/pdf/2405.02612v3",
      "published_date": "2024-05-04 08:43:45 UTC",
      "updated_date": "2024-06-19 17:08:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T06:00:18.953050"
    },
    {
      "arxiv_id": "2405.02608v1",
      "title": "UnSAMFlow: Unsupervised Optical Flow Guided by Segment Anything Model",
      "title_zh": "翻译失败",
      "authors": [
        "Shuai Yuan",
        "Lei Luo",
        "Zhuo Hui",
        "Can Pu",
        "Xiaoyu Xiang",
        "Rakesh Ranjan",
        "Denis Demandolx"
      ],
      "abstract": "Traditional unsupervised optical flow methods are vulnerable to occlusions\nand motion boundaries due to lack of object-level information. Therefore, we\npropose UnSAMFlow, an unsupervised flow network that also leverages object\ninformation from the latest foundation model Segment Anything Model (SAM). We\nfirst include a self-supervised semantic augmentation module tailored to SAM\nmasks. We also analyze the poor gradient landscapes of traditional smoothness\nlosses and propose a new smoothness definition based on homography instead. A\nsimple yet effective mask feature module has also been added to further\naggregate features on the object level. With all these adaptations, our method\nproduces clear optical flow estimation with sharp boundaries around objects,\nwhich outperforms state-of-the-art methods on both KITTI and Sintel datasets.\nOur method also generalizes well across domains and runs very efficiently.",
      "tldr_zh": "该论文提出UnSAMFlow，一种无监督光流网络，利用Segment Anything Model (SAM)提供对象级信息，解决传统方法在遮挡和运动边界上的脆弱性。方法包括自监督语义增强模块、基于homography的新平滑定义，以及掩码特征模块，以在对象级别聚合特征并产生清晰的光流估计。实验结果显示，UnSAMFlow在KITTI和Sintel数据集上优于最先进方法，并在不同领域具有良好的泛化性和高效运行性能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by CVPR 2024. Code is available at\n  https://github.com/facebookresearch/UnSAMFlow",
      "pdf_url": "http://arxiv.org/pdf/2405.02608v1",
      "published_date": "2024-05-04 08:27:12 UTC",
      "updated_date": "2024-05-04 08:27:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T06:00:29.508136"
    },
    {
      "arxiv_id": "2405.02605v1",
      "title": "MEXGEN: An Effective and Efficient Information Gain Approximation for Information Gathering Path Planning",
      "title_zh": "翻译失败",
      "authors": [
        "Joshua Chesser",
        "Thuraiappah Sathyan",
        "Damith C. Ranasinghe"
      ],
      "abstract": "Autonomous robots for gathering information on objects of interest has\nnumerous real-world applications because of they improve efficiency,\nperformance and safety. Realizing autonomy demands online planning algorithms\nto solve sequential decision making problems under uncertainty; because,\nobjects of interest are often dynamic, object state, such as location is not\ndirectly observable and are obtained from noisy measurements. Such planning\nproblems are notoriously difficult due to the combinatorial nature of\npredicting the future to make optimal decisions. For information theoretic\nplanning algorithms, we develop a computationally efficient and effective\napproximation for the difficult problem of predicting the likely sensor\nmeasurements from uncertain belief states}. The approach more accurately\npredicts information gain from information gathering actions. Our theoretical\nanalysis proves the proposed formulation achieves a lower prediction error than\nthe current efficient-method. We demonstrate improved performance gains in\nradio-source tracking and localization problems using extensive simulated and\nfield experiments with a multirotor aerial robot.",
      "tldr_zh": "本研究提出 MEXGEN，一种高效且有效的信息增益近似方法，用于自主机器人在不确定环境下的信息收集路径规划，以解决动态对象和噪声测量的挑战。该方法通过更准确地预测传感器测量和信息增益，实现了对未来决策的优化，并通过理论分析证明了其预测错误低于现有高效方法。在模拟和实地实验中，MEXGEN 在无线电源跟踪和定位任务上展示了显著的性能提升，提高了机器人的效率、性能和安全。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted to IEEE Robotics and Automation Letters (RA-L)(Demo Video:\n  https://www.youtube.com/watch?v=XrsCC6MkaB4)",
      "pdf_url": "http://arxiv.org/pdf/2405.02605v1",
      "published_date": "2024-05-04 08:09:16 UTC",
      "updated_date": "2024-05-04 08:09:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T06:00:42.442720"
    },
    {
      "arxiv_id": "2405.02602v1",
      "title": "Astro-NER -- Astronomy Named Entity Recognition: Is GPT a Good Domain Expert Annotator?",
      "title_zh": "翻译失败",
      "authors": [
        "Julia Evans",
        "Sameer Sadruddin",
        "Jennifer D'Souza"
      ],
      "abstract": "In this study, we address one of the challenges of developing NER models for\nscholarly domains, namely the scarcity of suitable labeled data. We experiment\nwith an approach using predictions from a fine-tuned LLM model to aid\nnon-domain experts in annotating scientific entities within astronomy\nliterature, with the goal of uncovering whether such a collaborative process\ncan approximate domain expertise. Our results reveal moderate agreement between\na domain expert and the LLM-assisted non-experts, as well as fair agreement\nbetween the domain expert and the LLM model's predictions. In an additional\nexperiment, we compare the performance of finetuned and default LLMs on this\ntask. We have also introduced a specialized scientific entity annotation scheme\nfor astronomy, validated by a domain expert. Our approach adopts a scholarly\nresearch contribution-centric perspective, focusing exclusively on scientific\nentities relevant to the research theme. The resultant dataset, containing\n5,000 annotated astronomy article titles, is made publicly available.",
      "tldr_zh": "本研究探讨了天文学领域命名实体识别（NER）的挑战，特别是标注数据的稀缺问题。研究者使用 fine-tuned LLM 模型的预测来辅助非领域专家标注天文学文献中的科学实体，并评估这种协作过程是否能接近领域专家水平，结果显示领域专家与 LLM 辅助标注者之间有中等一致性，而与 LLM 预测本身也有中等一致性。此外，通过比较 fine-tuned 和默认 LLMs 的性能，并引入一个专门的天文学科学实体标注方案，该研究公开了一个包含 5,000 个标注文章标题的数据集，为未来 NER 模型开发提供了宝贵资源。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IT",
        "math.IT"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages",
      "pdf_url": "http://arxiv.org/pdf/2405.02602v1",
      "published_date": "2024-05-04 08:04:39 UTC",
      "updated_date": "2024-05-04 08:04:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T06:00:53.793771"
    },
    {
      "arxiv_id": "2405.02596v1",
      "title": "Random Masking Finds Winning Tickets for Parameter Efficient Fine-tuning",
      "title_zh": "翻译失败",
      "authors": [
        "Jing Xu",
        "Jingzhao Zhang"
      ],
      "abstract": "Fine-tuning large language models (LLM) can be costly. Parameter-efficient\nfine-tuning (PEFT) addresses the problems by training a fraction of the\nparameters, whose success reveals the expressiveness and flexibility of\npretrained models. This paper studies the limit of PEFT, by further simplifying\nits design and reducing the number of trainable parameters beyond standard\nsetups. To this end, we use Random Masking to fine-tune the pretrained model.\nDespite its simplicity, we show that Random Masking is surprisingly effective:\nwith a larger-than-expected learning rate, Random Masking can match the\nperformance of standard PEFT algorithms such as LoRA on various tasks, using\nfewer trainable parameters. We provide both empirical and theoretical\nexplorations into the success of Random Masking. We show that masking induces a\nflatter loss landscape and more distant solutions, which allows for and\nnecessitates large learning rates.",
      "tldr_zh": "该研究探索了参数高效微调 (PEFT) 的极限，通过简单的方法 Random Masking 来进一步减少可训练参数，从而降低大型语言模型 (LLM) 微调的成本。实验结果显示，使用更大的学习率，Random Masking 能在各种任务上与标准 PEFT 算法如 LoRA 匹敌或匹配性能，同时使用更少的参数。理论分析表明，这种掩码机制会使损失景观更平坦并产生更远的解决方案，这不仅允许而且需要更大的学习率，以实现高效优化。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "ICML 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.02596v1",
      "published_date": "2024-05-04 07:44:18 UTC",
      "updated_date": "2024-05-04 07:44:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T06:01:05.871761"
    },
    {
      "arxiv_id": "2405.02583v1",
      "title": "Explainable Interface for Human-Autonomy Teaming: A Survey",
      "title_zh": "翻译失败",
      "authors": [
        "Xiangqi Kong",
        "Yang Xing",
        "Antonios Tsourdos",
        "Ziyue Wang",
        "Weisi Guo",
        "Adolfo Perrusquia",
        "Andreas Wikander"
      ],
      "abstract": "Nowadays, large-scale foundation models are being increasingly integrated\ninto numerous safety-critical applications, including human-autonomy teaming\n(HAT) within transportation, medical, and defence domains. Consequently, the\ninherent 'black-box' nature of these sophisticated deep neural networks\nheightens the significance of fostering mutual understanding and trust between\nhumans and autonomous systems. To tackle the transparency challenges in HAT,\nthis paper conducts a thoughtful study on the underexplored domain of\nExplainable Interface (EI) in HAT systems from a human-centric perspective,\nthereby enriching the existing body of research in Explainable Artificial\nIntelligence (XAI). We explore the design, development, and evaluation of EI\nwithin XAI-enhanced HAT systems. To do so, we first clarify the distinctions\nbetween these concepts: EI, explanations and model explainability, aiming to\nprovide researchers and practitioners with a structured understanding. Second,\nwe contribute to a novel framework for EI, addressing the unique challenges in\nHAT. Last, our summarized evaluation framework for ongoing EI offers a holistic\nperspective, encompassing model performance, human-centered factors, and group\ntask objectives. Based on extensive surveys across XAI, HAT, psychology, and\nHuman-Computer Interaction (HCI), this review offers multiple novel insights\ninto incorporating XAI into HAT systems and outlines future directions.",
      "tldr_zh": "这篇论文对Explainable Interface (EI)在Human-Autonomy Teaming (HAT)系统中的应用进行调查，旨在解决大型基础模型的“黑箱”性质带来的透明度和信任挑战，尤其在交通、医疗和国防等安全关键领域。论文从人类中心视角澄清了EI、explanations和model explainability的区别，提出一个新的EI框架来应对HAT的独特挑战，并提供一个综合评估框架，涵盖模型性能、人类因素和团队目标。基于对Explainable Artificial Intelligence (XAI)、HAT、心理学和Human-Computer Interaction (HCI)的广泛文献综述，该研究总结了关键见解，并为未来将XAI融入HAT系统指出了发展方向。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "45 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.02583v1",
      "published_date": "2024-05-04 06:35:38 UTC",
      "updated_date": "2024-05-04 06:35:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T06:01:19.715866"
    },
    {
      "arxiv_id": "2405.02580v2",
      "title": "PropertyGPT: LLM-driven Formal Verification of Smart Contracts through Retrieval-Augmented Property Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Ye Liu",
        "Yue Xue",
        "Daoyuan Wu",
        "Yuqiang Sun",
        "Yi Li",
        "Miaolei Shi",
        "Yang Liu"
      ],
      "abstract": "With recent advances in large language models (LLMs), this paper explores the\npotential of leveraging state-of-the-art LLMs,such as GPT-4, to transfer\nexisting human-written properties (e.g.,those from Certora auditing reports)\nand automatically generate customized properties for unknown code. To this end,\nwe embed existing properties into a vector database and retrieve a reference\nproperty for LLM-based in-context learning to generate a new property for a\ngiven code. While this basic process is relatively straightforward, ensuring\nthat the generated properties are (i) compilable, (ii) appropriate, and (iii)\nverifiable presents challenges. To address (i), we use the compilation and\nstatic analysis feedback as an external oracle to guide LLMs in iteratively\nrevising the generated properties. For (ii), we consider multiple dimensions of\nsimilarity to rank the properties and employ a weighted algorithm to identify\nthe top-K properties as the final result. For (iii), we design a dedicated\nprover to formally verify the correctness of the generated properties. We have\nimplemented these strategies into a novel LLM-based property generation tool\ncalled PropertyGPT. Our experiments show that PropertyGPT can generate\ncomprehensive and high-quality properties, achieving an 80% recall compared to\nthe ground truth. It successfully detected 26 CVEs/attack incidents out of 37\ntested and also uncovered 12 zero-day vulnerabilities, leading to $8,256 in bug\nbounty rewards.",
      "tldr_zh": "这篇论文介绍了 PropertyGPT，一种基于 LLM（如 GPT-4）的工具，通过 Retrieval-Augmented Property Generation 方法，从现有 properties（如 Certora 审计报告）中检索参考属性，并自动生成针对未知代码的自定义 properties。系统采用迭代修订（利用编译和静态分析反馈）、多维度相似度排名（选择 top-K 属性）以及专用 prover 进行正式验证，确保生成的 properties 可编译、适当且可验证。实验结果显示，PropertyGPT 实现了 80% 的召回率，成功检测了 26 个 CVEs/attack incidents 和 12 个 zero-day vulnerabilities，并带来了 $8,256 的 bug bounty 奖励。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted by NDSS Symposium 2025. Please cite the conference version\n  of this paper, e.g., \"Ye Liu, Yue Xue, Daoyuan Wu, Yuqiang Sun, Yi Li,\n  Miaolei Shi, Yang Liu. PropertyGPT: LLM-driven Formal Verification of Smart\n  Contracts through Retrieval-Augmented Property Generation. In 32nd Annual\n  Network and Distributed System Security Symposium (NDSS 2025).\"",
      "pdf_url": "http://arxiv.org/pdf/2405.02580v2",
      "published_date": "2024-05-04 06:28:27 UTC",
      "updated_date": "2024-12-06 08:41:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T06:01:31.221832"
    },
    {
      "arxiv_id": "2405.03711v1",
      "title": "Guidance Design for Escape Flight Vehicle Using Evolution Strategy Enhanced Deep Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Xiao Hu",
        "Tianshu Wang",
        "Min Gong",
        "Shaoshi Yang"
      ],
      "abstract": "Guidance commands of flight vehicles are a series of data sets with fixed\ntime intervals, thus guidance design constitutes a sequential decision problem\nand satisfies the basic conditions for using deep reinforcement learning (DRL).\nIn this paper, we consider the scenario where the escape flight vehicle (EFV)\ngenerates guidance commands based on DRL and the pursuit flight vehicle (PFV)\ngenerates guidance commands based on the proportional navigation method. For\nthe EFV, the objective of the guidance design entails progressively maximizing\nthe residual velocity, subject to the constraint imposed by the given evasion\ndistance. Thus an irregular dynamic max-min problem of extremely large-scale is\nformulated, where the time instant when the optimal solution can be attained is\nuncertain and the optimum solution depends on all the intermediate guidance\ncommands generated before. For solving this problem, a two-step strategy is\nconceived. In the first step, we use the proximal policy optimization (PPO)\nalgorithm to generate the guidance commands of the EFV. The results obtained by\nPPO in the global search space are coarse, despite the fact that the reward\nfunction, the neural network parameters and the learning rate are designed\nelaborately. Therefore, in the second step, we propose to invoke the evolution\nstrategy (ES) based algorithm, which uses the result of PPO as the initial\nvalue, to further improve the quality of the solution by searching in the local\nspace. Simulation results demonstrate that the proposed guidance design method\nbased on the PPO algorithm is capable of achieving a residual velocity of 67.24\nm/s, higher than the residual velocities achieved by the benchmark soft\nactor-critic and deep deterministic policy gradient algorithms. Furthermore,\nthe proposed ES-enhanced PPO algorithm outperforms the PPO algorithm by 2.7\\%,\nachieving a residual velocity of 69.04 m/s.",
      "tldr_zh": "这篇论文针对逃逸飞行器 (EFV) 的引导设计问题，将其视为一个顺序决策任务，使用 Deep Reinforcement Learning (DRL) 方法来生成引导命令，同时追求飞行器 (PFV) 采用比例导航方法。作者提出了一种两步策略：首先运用 Proximal Policy Optimization (PPO) 算法进行全局搜索生成初始命令，然后使用 Evolution Strategy (ES) 算法以 PPO 结果为起点，在局部空间进一步优化，以最大化 EFV 的剩余速度并满足逃逸距离约束。模拟实验显示，该 ES 增强 PPO 方法使 EFV 的剩余速度达到 69.04 m/s，比纯 PPO 算法提升 2.7%，并优于基准算法如 Soft Actor-Critic 和 Deep Deterministic Policy Gradient。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "13 pages, 13 figures, accepted to appear on IEEE Access, Mar. 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.03711v1",
      "published_date": "2024-05-04 06:18:15 UTC",
      "updated_date": "2024-05-04 06:18:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T06:01:42.933441"
    },
    {
      "arxiv_id": "2405.02576v3",
      "title": "CTD4 -- A Deep Continuous Distributional Actor-Critic Agent with a Kalman Fusion of Multiple Critics",
      "title_zh": "翻译失败",
      "authors": [
        "David Valencia",
        "Henry Williams",
        "Yuning Xing",
        "Trevor Gee",
        "Bruce A MacDonald",
        "Minas Liarokapis"
      ],
      "abstract": "Categorical Distributional Reinforcement Learning (CDRL) has demonstrated\nsuperior sample efficiency in learning complex tasks compared to conventional\nReinforcement Learning (RL) approaches. However, the practical application of\nCDRL is encumbered by challenging projection steps, detailed parameter tuning,\nand domain knowledge. This paper addresses these challenges by introducing a\npioneering Continuous Distributional Model-Free RL algorithm tailored for\ncontinuous action spaces. The proposed algorithm simplifies the implementation\nof distributional RL, adopting an actor-critic architecture wherein the critic\noutputs a continuous probability distribution. Additionally, we propose an\nensemble of multiple critics fused through a Kalman fusion mechanism to\nmitigate overestimation bias. Through a series of experiments, we validate that\nour proposed method provides a sample-efficient solution for executing complex\ncontinuous-control tasks.",
      "tldr_zh": "该论文提出了一种新的连续分布模型自由强化学习（Continuous Distributional Model-Free RL）算法，名为 CTD4，以简化 Categorical Distributional Reinforcement Learning (CDRL) 在连续动作空间中的应用，解决其投影步骤、参数调整和领域知识依赖的挑战。该算法采用 actor-critic 架构，其中 critic 输出连续概率分布，并引入多个 critics 的集合，通过 Kalman fusion 机制融合以减少过估计偏差。通过一系列实验，CTD4 展示了在复杂连续控制任务中的高效样本利用性能，证明了其在强化学习（RL）领域的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.02576v3",
      "published_date": "2024-05-04 05:38:38 UTC",
      "updated_date": "2025-02-06 02:52:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T06:01:53.556439"
    },
    {
      "arxiv_id": "2405.02572v1",
      "title": "Off-OAB: Off-Policy Policy Gradient Method with Optimal Action-Dependent Baseline",
      "title_zh": "翻译失败",
      "authors": [
        "Wenjia Meng",
        "Qian Zheng",
        "Long Yang",
        "Yilong Yin",
        "Gang Pan"
      ],
      "abstract": "Policy-based methods have achieved remarkable success in solving challenging\nreinforcement learning problems. Among these methods, off-policy policy\ngradient methods are particularly important due to that they can benefit from\noff-policy data. However, these methods suffer from the high variance of the\noff-policy policy gradient (OPPG) estimator, which results in poor sample\nefficiency during training. In this paper, we propose an off-policy policy\ngradient method with the optimal action-dependent baseline (Off-OAB) to\nmitigate this variance issue. Specifically, this baseline maintains the OPPG\nestimator's unbiasedness while theoretically minimizing its variance. To\nenhance practical computational efficiency, we design an approximated version\nof this optimal baseline. Utilizing this approximation, our method (Off-OAB)\naims to decrease the OPPG estimator's variance during policy optimization. We\nevaluate the proposed Off-OAB method on six representative tasks from OpenAI\nGym and MuJoCo, where it demonstrably surpasses state-of-the-art methods on the\nmajority of these tasks.",
      "tldr_zh": "该论文针对 off-policy policy gradient (OPPG) 方法在强化学习中存在的估计器方差高和样本效率低的问题，提出了一种新的方法 Off-OAB，该方法采用 optimal action-dependent baseline 来保持估计器的无偏性并理论上最小化其方差。为提升计算效率，Off-OAB 设计了该 baseline 的近似版本，以在策略优化过程中有效降低方差。在 OpenAI Gym 和 MuJoCo 的六个代表性任务上，实验结果显示 Off-OAB 超过了大多数 state-of-the-art 方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.02572v1",
      "published_date": "2024-05-04 05:21:28 UTC",
      "updated_date": "2024-05-04 05:21:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T06:02:05.740177"
    },
    {
      "arxiv_id": "2405.02569v1",
      "title": "Decoupling Exploration and Exploitation for Unsupervised Pre-training with Successor Features",
      "title_zh": "翻译失败",
      "authors": [
        "JaeYoon Kim",
        "Junyu Xuan",
        "Christy Liang",
        "Farookh Hussain"
      ],
      "abstract": "Unsupervised pre-training has been on the lookout for the virtue of a value\nfunction representation referred to as successor features (SFs), which\ndecouples the dynamics of the environment from the rewards. It has a\nsignificant impact on the process of task-specific fine-tuning due to the\ndecomposition. However, existing approaches struggle with local optima due to\nthe unified intrinsic reward of exploration and exploitation without\nconsidering the linear regression problem and the discriminator supporting a\nsmall skill sapce. We propose a novel unsupervised pre-training model with SFs\nbased on a non-monolithic exploration methodology. Our approach pursues the\ndecomposition of exploitation and exploration of an agent built on SFs, which\nrequires separate agents for the respective purpose. The idea will leverage not\nonly the inherent characteristics of SFs such as a quick adaptation to new\ntasks but also the exploratory and task-agnostic capabilities. Our suggested\nmodel is termed Non-Monolithic unsupervised Pre-training with Successor\nfeatures (NMPS), which improves the performance of the original monolithic\nexploration method of pre-training with SFs. NMPS outperforms Active\nPre-training with Successor Features (APS) in a comparative experiment.",
      "tldr_zh": "该论文探讨了在无监督预训练中使用 Successor Features (SFs) 来解耦环境动态和奖励，从而提升任务微调过程，但现有方法因统一的内在奖励机制而易陷入局部最优。作者提出了一种新模型 Non-Monolithic unsupervised Pre-training with Successor features (NMPS)，通过非单体探索方法分离探索（exploration）和利用（exploitation），采用单独的代理来优化 SFs 的特性，如快速适应新任务。实验结果表明，NMPS 在性能上超过了现有的 Active Pre-training with Successor Features (APS) 方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "IJCNN 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.02569v1",
      "published_date": "2024-05-04 05:03:11 UTC",
      "updated_date": "2024-05-04 05:03:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T06:02:18.397893"
    },
    {
      "arxiv_id": "2405.02568v2",
      "title": "Active Neural 3D Reconstruction with Colorized Surface Voxel-based View Selection",
      "title_zh": "翻译失败",
      "authors": [
        "Hyunseo Kim",
        "Hyeonseo Yang",
        "Taekyung Kim",
        "YoonSung Kim",
        "Jin-Hwa Kim",
        "Byoung-Tak Zhang"
      ],
      "abstract": "Active view selection in 3D scene reconstruction has been widely studied\nsince training on informative views is critical for reconstruction. Recently,\nNeural Radiance Fields (NeRF) variants have shown promising results in active\n3D reconstruction using uncertainty-guided view selection. They utilize\nuncertainties estimated with neural networks that encode scene geometry and\nappearance. However, the choice of uncertainty integration methods, either\nvoxel-based or neural rendering, has conventionally depended on the types of\nscene uncertainty being estimated, whether geometric or appearance-related. In\nthis paper, we introduce Colorized Surface Voxel (CSV)-based view selection, a\nnew next-best view (NBV) selection method exploiting surface voxel-based\nmeasurement of uncertainty in scene appearance. CSV encapsulates the\nuncertainty of estimated scene appearance (e.g., color uncertainty) and\nestimated geometric information (e.g., surface). Using the geometry\ninformation, we interpret the uncertainty of scene appearance 3D-wise during\nthe aggregation of the per-voxel uncertainty. Consequently, the uncertainty\nfrom occluded and complex regions is recognized under challenging scenarios\nwith limited input data. Our method outperforms previous works on popular\ndatasets, DTU and Blender, and our new dataset with imbalanced viewpoints,\nshowing that the CSV-based view selection significantly improves performance by\nup to 30%.",
      "tldr_zh": "本研究提出了一种基于 Colorized Surface Voxel (CSV) 的主动视图选择方法，用于提升 Neural Radiance Fields (NeRF) 变体的 3D 场景重建性能。CSV 方法通过整合场景外观不确定性（如颜色不确定性）和几何信息（如表面），实现对每个体素不确定性的 3D 聚合，从而更好地识别被遮挡和复杂区域的不确定性。相比传统不确定性整合方法，该方法在 DTU、Blender 和一个新数据集上表现出色，提高重建性能高达 30%。这为主动 3D 重建提供了更可靠的视图选择策略，尤其适用于输入数据有限的挑战场景。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "New experiments with newly published dataset are added. The main\n  claims are the same as in the previous version, but the naming and\n  explanations have been changed",
      "pdf_url": "http://arxiv.org/pdf/2405.02568v2",
      "published_date": "2024-05-04 05:01:58 UTC",
      "updated_date": "2024-06-10 17:05:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T06:02:29.708903"
    },
    {
      "arxiv_id": "2405.02564v1",
      "title": "Leveraging the Human Ventral Visual Stream to Improve Neural Network Robustness",
      "title_zh": "翻译失败",
      "authors": [
        "Zhenan Shao",
        "Linjian Ma",
        "Bo Li",
        "Diane M. Beck"
      ],
      "abstract": "Human object recognition exhibits remarkable resilience in cluttered and\ndynamic visual environments. In contrast, despite their unparalleled\nperformance across numerous visual tasks, Deep Neural Networks (DNNs) remain\nfar less robust than humans, showing, for example, a surprising susceptibility\nto adversarial attacks involving image perturbations that are (almost)\nimperceptible to humans. Human object recognition likely owes its robustness,\nin part, to the increasingly resilient representations that emerge along the\nhierarchy of the ventral visual cortex. Here we show that DNNs, when guided by\nneural representations from a hierarchical sequence of regions in the human\nventral visual stream, display increasing robustness to adversarial attacks.\nThese neural-guided models also exhibit a gradual shift towards more human-like\ndecision-making patterns and develop hierarchically smoother decision surfaces.\nImportantly, the resulting representational spaces differ in important ways\nfrom those produced by conventional smoothing methods, suggesting that such\nneural-guidance may provide previously unexplored robustness solutions. Our\nfindings support the gradual emergence of human robustness along the ventral\nvisual hierarchy and suggest that the key to DNN robustness may lie in\nincreasing emulation of the human brain.",
      "tldr_zh": "本研究探讨了如何利用人类腹侧视觉流 (ventral visual stream) 的层次化神经表示来提升深度神经网络 (DNNs) 的鲁棒性，以应对对抗攻击等挑战。研究方法涉及指导 DNNs 通过人类视觉皮层序列的表示，逐步增强模型在杂乱动态环境中的性能。结果显示，这些神经指导模型不仅对对抗攻击的抵抗力显著提高，还展现出更接近人类的决策模式和更平滑的决策表面。总体而言，该工作揭示了人类鲁棒性在视觉层次中的渐进机制，并建议通过模仿人类大脑来开辟 DNNs 鲁棒性的新路径。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "q-bio.NC"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.02564v1",
      "published_date": "2024-05-04 04:33:20 UTC",
      "updated_date": "2024-05-04 04:33:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T06:02:43.696795"
    },
    {
      "arxiv_id": "2406.17781v1",
      "title": "Large Language Models estimate fine-grained human color-concept associations",
      "title_zh": "大型语言模型估计细粒度的人类颜色-概念关联",
      "authors": [
        "Kushin Mukherjee",
        "Timothy T. Rogers",
        "Karen B. Schloss"
      ],
      "abstract": "Concepts, both abstract and concrete, elicit a distribution of association\nstrengths across perceptual color space, which influence aspects of visual\ncognition ranging from object recognition to interpretation of information\nvisualizations. While prior work has hypothesized that color-concept\nassociations may be learned from the cross-modal statistical structure of\nexperience, it has been unclear whether natural environments possess such\nstructure or, if so, whether learning systems are capable of discovering and\nexploiting it without strong prior constraints. We addressed these questions by\ninvestigating the ability of GPT-4, a multimodal large language model, to\nestimate human-like color-concept associations without any additional training.\nStarting with human color-concept association ratings for 71 color set spanning\nperceptual color space (\\texttt{UW-71}) and concepts that varied in\nabstractness, we assessed how well association ratings generated by GPT-4 could\npredict human ratings. GPT-4 ratings were correlated with human ratings, with\nperformance comparable to state-of-the-art methods for automatically estimating\ncolor-concept associations from images. Variability in GPT-4's performance\nacross concepts could be explained by specificity of the concept's\ncolor-concept association distribution. This study suggests that high-order\ncovariances between language and perception, as expressed in the natural\nenvironment of the internet, contain sufficient information to support learning\nof human-like color-concept associations, and provides an existence proof that\na learning system can encode such associations without initial constraints. The\nwork further shows that GPT-4 can be used to efficiently estimate distributions\nof color associations for a broad range of concepts, potentially serving as a\ncritical tool for designing effective and intuitive information visualizations.",
      "tldr_zh": "这篇论文探讨了大型语言模型（Large Language Models，如 GPT-4）在估计人类细粒度颜色-概念关联方面的能力，这些关联影响视觉认知，如物体识别和信息可视化。研究方法涉及使用 GPT-4 生成颜色-概念关联评分，并与人类数据（基于 UW-71 数据集）进行比较，结果显示 GPT-4 的性能与基于图像的 state-of-the-art 方法相当，且其表现受概念关联分布具体性的影响。论文证明，自然环境（如互联网）中的语言和感知高阶协方差足以支持学习系统无需初始约束即可编码人类般的关联，并建议 GPT-4 可作为高效工具，用于设计直观的信息可视化。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.17781v1",
      "published_date": "2024-05-04 04:19:15 UTC",
      "updated_date": "2024-05-04 04:19:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T06:02:55.112094"
    },
    {
      "arxiv_id": "2405.02559v2",
      "title": "A Framework for Human Evaluation of Large Language Models in Healthcare Derived from Literature Review",
      "title_zh": "一种源自文献综述的医疗保健领域大语言模型人类评估框架",
      "authors": [
        "Thomas Yu Chow Tam",
        "Sonish Sivarajkumar",
        "Sumit Kapoor",
        "Alisa V Stolyar",
        "Katelyn Polanska",
        "Karleigh R McCarthy",
        "Hunter Osterhoudt",
        "Xizhi Wu",
        "Shyam Visweswaran",
        "Sunyang Fu",
        "Piyush Mathur",
        "Giovanni E. Cacciamani",
        "Cong Sun",
        "Yifan Peng",
        "Yanshan Wang"
      ],
      "abstract": "With generative artificial intelligence (AI), particularly large language\nmodels (LLMs), continuing to make inroads in healthcare, it is critical to\nsupplement traditional automated evaluations with human evaluations.\nUnderstanding and evaluating the output of LLMs is essential to assuring\nsafety, reliability, and effectiveness. However, human evaluation's cumbersome,\ntime-consuming, and non-standardized nature presents significant obstacles to\ncomprehensive evaluation and widespread adoption of LLMs in practice. This\nstudy reviews existing literature on human evaluation methodologies for LLMs in\nhealthcare. We highlight a notable need for a standardized and consistent human\nevaluation approach. Our extensive literature search, adhering to the Preferred\nReporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines,\nincludes publications from January 2018 to February 2024. The review examines\nthe human evaluation of LLMs across various medical specialties, addressing\nfactors such as evaluation dimensions, sample types and sizes, selection, and\nrecruitment of evaluators, frameworks and metrics, evaluation process, and\nstatistical analysis type. Drawing on the diverse evaluation strategies\nemployed in these studies, we propose a comprehensive and practical framework\nfor human evaluation of LLMs: QUEST: Quality of Information, Understanding and\nReasoning, Expression Style and Persona, Safety and Harm, and Trust and\nConfidence. This framework aims to improve the reliability, generalizability,\nand applicability of human evaluation of LLMs in different healthcare\napplications by defining clear evaluation dimensions and offering detailed\nguidelines.",
      "tldr_zh": "这篇论文通过文献综述，审查了大型语言模型 (LLMs) 在医疗领域的人类评估方法，强调了标准化评估的必要性，以解决其繁琐、非标准化等问题。研究遵循 PRISMA 指南，对2018年至2024年的相关文献进行搜索，分析了评估维度、样本类型和大小、评估者选择、框架指标、过程和统计分析。最终，论文提出一个全面框架 QUEST（Quality of Information, Understanding and Reasoning, Expression Style and Persona, Safety and Harm, and Trust and Confidence），旨在通过清晰的评估维度和指导方针，提高 LLMs 输出在医疗应用中的可靠性和适用性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.02559v2",
      "published_date": "2024-05-04 04:16:07 UTC",
      "updated_date": "2024-09-23 18:00:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T06:03:06.391927"
    },
    {
      "arxiv_id": "2405.06673v2",
      "title": "Overview of the EHRSQL 2024 Shared Task on Reliable Text-to-SQL Modeling on Electronic Health Records",
      "title_zh": "翻译失败",
      "authors": [
        "Gyubok Lee",
        "Sunjun Kweon",
        "Seongsu Bae",
        "Edward Choi"
      ],
      "abstract": "Electronic Health Records (EHRs) are relational databases that store the\nentire medical histories of patients within hospitals. They record numerous\naspects of patients' medical care, from hospital admission and diagnosis to\ntreatment and discharge. While EHRs are vital sources of clinical data,\nexploring them beyond a predefined set of queries requires skills in query\nlanguages like SQL. To make information retrieval more accessible, one strategy\nis to build a question-answering system, possibly leveraging text-to-SQL models\nthat can automatically translate natural language questions into corresponding\nSQL queries and use these queries to retrieve the answers. The EHRSQL 2024\nshared task aims to advance and promote research in developing a\nquestion-answering system for EHRs using text-to-SQL modeling, capable of\nreliably providing requested answers to various healthcare professionals to\nimprove their clinical work processes and satisfy their needs. Among more than\n100 participants who applied to the shared task, eight teams were formed and\ncompleted the entire shared task requirement and demonstrated a wide range of\nmethods to effectively solve this task. In this paper, we describe the task of\nreliable text-to-SQL modeling, the dataset, and the methods and results of the\nparticipants. We hope this shared task will spur further research and insights\ninto developing reliable question-answering systems for EHRs.",
      "tldr_zh": "本论文概述了EHRSQL 2024共享任务，该任务聚焦于在Electronic Health Records (EHRs)上开发可靠的text-to-SQL建模，以将自然语言问题转化为SQL查询，从而提升医疗信息检索的可用性。任务利用EHRs数据集，吸引了100多名参与者，其中8个团队展示了多种方法，包括基于text-to-SQL的问答系统，以满足医疗专业人员的临床需求。实验结果显示，这些方法有效推进了可靠问答系统的研究，并有望激发更多对EHRs应用的研究创新。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "The 6th Clinical Natural Language Processing Workshop at NAACL 2024;\n  Minor Change from Camera-Ready",
      "pdf_url": "http://arxiv.org/pdf/2405.06673v2",
      "published_date": "2024-05-04 04:12:18 UTC",
      "updated_date": "2024-05-23 17:25:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T06:03:18.406898"
    },
    {
      "arxiv_id": "2405.02548v1",
      "title": "CNN-LSTM and Transfer Learning Models for Malware Classification based on Opcodes and API Calls",
      "title_zh": "CNN-LSTM 与迁移学习模型：基于操作码和 API 调用的恶意软件分类",
      "authors": [
        "Ahmed Bensaoud",
        "Jugal Kalita"
      ],
      "abstract": "In this paper, we propose a novel model for a malware classification system\nbased on Application Programming Interface (API) calls and opcodes, to improve\nclassification accuracy. This system uses a novel design of combined\nConvolutional Neural Network and Long Short-Term Memory. We extract opcode\nsequences and API Calls from Windows malware samples for classification. We\ntransform these features into N-grams (N = 2, 3, and 10)-gram sequences. Our\nexperiments on a dataset of 9,749,57 samples produce high accuracy of 99.91%\nusing the 8-gram sequences. Our method significantly improves the malware\nclassification performance when using a wide range of recent deep learning\narchitectures, leading to state-of-the-art performance. In particular, we\nexperiment with ConvNeXt-T, ConvNeXt-S, RegNetY-4GF, RegNetY-8GF, RegNetY-12GF,\nEfficientNetV2, Sequencer2D-L, Swin-T, ViT-G/14, ViT-Ti, ViT-S, VIT-B, VIT-L,\nand MaxViT-B. Among these architectures, Swin-T and Sequencer2D-L architectures\nachieved high accuracies of 99.82% and 99.70%, respectively, comparable to our\nCNN-LSTM architecture although not surpassing it.",
      "tldr_zh": "本研究提出了一种基于操作码(opcodes)和API调用的恶意软件分类模型，结合CNN-LSTM架构来提升分类准确率。通过从Windows恶意软件样本中提取特征并转换为N-gram序列（N=2、3和10），实验在包含9,749,57个样本的数据集上实现了99.91%的最高准确率。相比其他深度学习架构如Swin-T（99.82%）和Sequencer2D-L（99.70%），该模型表现出色，显著提高了恶意软件检测性能，并展示了其在转移学习中的潜力。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.02548v1",
      "published_date": "2024-05-04 03:13:13 UTC",
      "updated_date": "2024-05-04 03:13:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T06:03:30.678963"
    },
    {
      "arxiv_id": "2405.02545v1",
      "title": "Prediction of Space Weather Events through Analysis of Active Region Magnetograms using Convolutional Neural Network",
      "title_zh": "翻译失败",
      "authors": [
        "Shlesh Sakpal"
      ],
      "abstract": "Although space weather events may not directly affect human life, they have\nthe potential to inflict significant harm upon our communities. Harmful space\nweather events can trigger atmospheric changes that result in physical and\neconomic damages on a global scale. In 1989, Earth experienced the effects of a\npowerful geomagnetic storm that caused satellites to malfunction, while\ntriggering power blackouts in Canada, along with electricity disturbances in\nthe United States and Europe. With the solar cycle peak rapidly approaching,\nthere is an ever-increasing need to prepare and prevent the damages that can\noccur, especially to modern-day technology, calling for the need of a\ncomprehensive prediction system. This study aims to leverage machine learning\ntechniques to predict instances of space weather (solar flares, coronal mass\nejections, geomagnetic storms), based on active region magnetograms of the Sun.\nThis was done through the use of the NASA DONKI service to determine when these\nsolar events occur, then using data from the NASA Solar Dynamics Observatory to\ncompile a dataset that includes magnetograms of active regions of the Sun 24\nhours before the events. By inputting the magnetograms into a convolutional\nneural network (CNN) trained from this dataset, it can serve to predict whether\na space weather event will occur, and what type of event it will be. The model\nwas designed using a custom architecture CNN, and returned an accuracy of\n90.27%, a precision of 85.83%, a recall of 91.78%, and an average F1 score of\n92.14% across each class (Solar flare [Flare], geomagnetic storm [GMS], coronal\nmass ejection [CME]). Our results show that using magnetogram data as an input\nfor a CNN is a viable method to space weather prediction. Future work can\ninvolve prediction of the magnitude of solar events.",
      "tldr_zh": "本研究利用Convolutional Neural Network (CNN)分析太阳Active Region Magnetograms，旨在预测空间天气事件，包括Solar Flares、日冕物质抛射 (Coronal Mass Ejections) 和地磁风暴 (Geomagnetic Storms)，以防范潜在的全球性损害。研究团队从NASA Solar Dynamics Observatory获取事件前24小时的磁图数据集，并使用自定义CNN架构进行训练。结果显示，模型的准确率达90.27%、精确率85.83%、召回率91.78%和F1分数92.14%，证明了这一方法在空间天气预测中的可行性，未来可扩展到预测事件规模。",
      "categories": [
        "astro-ph.SR",
        "astro-ph.EP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "astro-ph.SR",
      "comment": "6 pages, 12 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.02545v1",
      "published_date": "2024-05-04 03:04:51 UTC",
      "updated_date": "2024-05-04 03:04:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T06:03:44.119732"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 48,
  "processed_papers_count": 48,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-18T06:04:10.997260"
}
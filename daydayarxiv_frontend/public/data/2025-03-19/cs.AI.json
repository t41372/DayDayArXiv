{
  "date": "2025-03-19",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-03-19 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 更新了 97 篇论文，主要聚焦 AI 模型优化、多模态学习、强化学习和机器人应用等领域，强调模型的安全性、效率和泛化能力，令人印象深刻的是 ECLAIR 框架（John Murzaku 等作者）和 LLaVA-MORE（Federico Cocchi 等作者）的创新研究，以及 Pietro Perona 在视觉概念相似性方面的贡献，这些工作推动了 AI 在交互和多模态任务中的进展。\n\n下面，我挑选并简要概述部分关键论文，先优先讨论重要、话题度高或有著名学者的文章（如多模态模型和强化学习），相关主题的论文放在一起快速聊；其他较常规或次要论文（如某些纯理论或小众应用）则简略掠过，只列出标题和核心点。\n\n### 多模态模型与 AI 交互\n- **ECLAIR: Enhanced Clarification for Interactive Responses（ECLAIR: 增强交互响应澄清）**  \n  提出 ECLAIR 框架，用于企业 AI 助手处理模糊查询，通过生成澄清问题和整合多代理信息提升上下文感知。主要贡献：实验显示 ECLAIR 优于 few-shot 提示方法，在澄清生成和模糊解决上表现更强，适用于交互式 AI 系统。\n\n- **LLaVA-MORE: A Comparative Study of LLMs and Visual Backbones for Enhanced Visual Instruction Tuning（LLaVA-MORE: 增强视觉指令微调的 LLM 和视觉骨干比较研究）**  \n  探索多模态大语言模型（MLLMs）的架构优化，比较不同 LLM（如 Phi-4 和 LLaMA-3.1）和视觉编码器（如 DINOv2）。主要发现：统一训练协议提升了视觉推理和生成性能，在基准测试中超越现有 SOTA，代码开源。\n\n- **Representational Similarity via Interpretable Visual Concepts（通过可解释视觉概念的表示相似性）**  \n  作者包括 Pietro Perona。新方法 RSVC 比较深度神经网络的决策差异，识别共享和独特视觉概念。主要贡献：揭示模型差异来源，提升模型可解释性，在多种视觉任务上有效。\n\n- **TULIP: Towards Unified Language-Image Pretraining（TULIP: 朝向统一语言-图像预训练）**  \n  改进 CLIP-like 模型，结合生成数据增强和对比学习，提升图像细节理解。主要发现：在 ImageNet 和其他基准上，TULIP 提升了零样本性能和泛化能力。\n\n其他多模态相关论文如 EgoDTM（基于 3D 感知的视频语言预训练）和 ContextualJudgeBench（评估 LLM 在上下文中的判断能力）则快速掠过：它们分别通过深度估计和上下文评估提升视频理解和 LLM 鲁棒性。\n\n### 强化学习与机器人应用\n- **Reward Training Wheels: Adaptive Auxiliary Rewards for Robotics Reinforcement Learning（奖励训练轮: 机器人强化学习的自适应辅助奖励）**  \n  引入 RTW 框架，动态调整辅助奖励以加速机器人任务学习。主要贡献：在模拟和真实机器人实验中，RTW 提升了导航和机动性能，训练效率提高 35% 以上。\n\n- **Safety Aware Task Planning via Large Language Models in Robotics（通过大语言模型的机器人安全感知任务规划）**  \n  提出 SAFER 框架，使用 LLM 和控制屏障函数（CBFs）嵌入安全反馈，提升机器人任务规划的安全性。主要发现：在多机器人任务中，SAFER 减少安全违规，同时保持效率。\n\n- **Predicting Multi-Agent Specialization via Task Parallelizability（通过任务并行性预测多代理专业化）**  \n  分析任务并行性对多代理系统的影响，使用启发式预测专业化策略。主要贡献：在 Overcooked-AI 实验中，证明并行性限制下专家代理优于通用代理。\n\n其他机器人论文如 GraspCorrect（视觉语言模型指导的抓取修正）和 StyleLoco（生成式模仿学习）快速掠过：它们分别通过反馈优化抓取策略和生成自然运动，提升机器人适应性。\n\n### 医疗与生物应用\n- **Reliable Radiologic Skeletal Muscle Area Assessment（可靠的放射学骨骼肌面积评估）**  \n  开发 SMAART-AI 工具，使用深度学习评估癌症缓存消耗的生物标志物。主要贡献：结合不确定性机制和 MLP 模型，精确预测缓存消耗，提升临床诊断。\n\n- **VenusFactory: A Unified Platform for Protein Engineering Data Retrieval and Language Model Fine-Tuning（VenusFactory: 蛋白工程数据检索和语言模型微调的统一平台）**  \n  构建集成蛋白数据集和模型的平台，支持无代码接口。主要发现：简化蛋白语言模型训练，适用于生物社区。\n\n其他医疗论文如 UI-Vision（桌面 GUI 基准）和 AIJIM（环境新闻 AI 模型）则简要掠过：前者评估视觉代理，后者通过视觉 Transformer 提升实时报告。\n\n### 其他亮点与快速掠过\n- **Diffusion-Based Forecasting for Uncertainty-Aware Model Predictive Control（基于扩散的预测用于不确定性感知模型预测控制）**  \n  整合扩散模型提升预测不确定性，主要贡献：在能源任务中优于基线。\n\n其余论文，如关于因果推理（Survey on Generalization Theory for Graph Neural Networks）、数学推理（MetaLadder）和生成模型（Conjuring Positive Pairs）的作品，由于相对理论化或非核心应用，快速掠过：它们分别探讨图神经网络泛化、类比推理转移和高效生成，但细节较少涉及。\n\n总之，今天的论文突显 AI 模型在效率、安全和应用领域的创新潜力，读者可关注 ECLAIR 和 LLaVA-MORE 等前沿工作，以快速判断是否深入阅读。更多细节请查阅 arXiv！",
  "papers": [
    {
      "arxiv_id": "2503.15739v1",
      "title": "ECLAIR: Enhanced Clarification for Interactive Responses",
      "title_zh": "翻译失败",
      "authors": [
        "John Murzaku",
        "Zifan Liu",
        "Md Mehrab Tanjim",
        "Vaishnavi Muppala",
        "Xiang Chen",
        "Yunyao Li"
      ],
      "abstract": "We present ECLAIR (Enhanced CLArification for Interactive Responses), a novel\nunified and end-to-end framework for interactive disambiguation in enterprise\nAI assistants. ECLAIR generates clarification questions for ambiguous user\nqueries and resolves ambiguity based on the user's response.We introduce a\ngeneralized architecture capable of integrating ambiguity information from\nmultiple downstream agents, enhancing context-awareness in resolving\nambiguities and allowing enterprise specific definition of agents. We further\ndefine agents within our system that provide domain-specific grounding\ninformation. We conduct experiments comparing ECLAIR to few-shot prompting\ntechniques and demonstrate ECLAIR's superior performance in clarification\nquestion generation and ambiguity resolution.",
      "tldr_zh": "该研究提出了 ECLAIR，一种新型的统一端到端框架，用于企业 AI 助手中的交互式消歧功能，能够为模糊用户查询生成澄清问题并基于用户响应解决歧义。ECLAIR 的架构整合了多个下游代理的歧义信息，提升了上下文感知能力，并允许企业定义特定代理以提供领域特定基础信息。通过实验比较，ECLAIR 在澄清问题生成和歧义解决方面优于 few-shot prompting 技术，展示了其在企业 AI 交互中的显著性能提升。",
      "categories": [
        "cs.AI",
        "68T50",
        "I.2.7; H.5.2"
      ],
      "primary_category": "cs.AI",
      "comment": "7 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.15739v1",
      "published_date": "2025-03-19 23:04:00 UTC",
      "updated_date": "2025-03-19 23:04:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:51:01.796241"
    },
    {
      "arxiv_id": "2503.17403v1",
      "title": "ChatGPT or A Silent Everywhere Helper: A Survey of Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Azim Akhtarshenas",
        "Afshin Dini",
        "Navid Ayoobi"
      ],
      "abstract": "Large Language Models (LLMs) have revo lutionized natural language processing\nNatural Language Processing (NLP), with Chat Generative Pre-trained Transformer\n(ChatGPT) standing out as a notable exampledue to its advanced capabilities and\nwidespread applications. This survey provides a comprehensive analysis of\nChatGPT, exploring its architecture, training processes, and functionalities.\nWe examine its integration into various domains across industries such as\ncustomer service, education, healthcare, and entertainment. A comparative\nanalysis with other LLMs highlights ChatGPT's unique features and performance\nmetrics. Regarding benchmarks, the paper examines ChatGPT's comparative\nperformance against other LLMs and discusses potential risks such as\nmisinformation, bias, and data privacy concerns. Additionally, we offer a\nnumber of figures and tables that outline the backdrop of the discussion, the\nmain ideas of the article, the numerous LLM models, a thorough list of datasets\nused for pre-training, fine-tuning, and evaluation, as well as particular LLM\napplications with pertinent references. Finally, we identify future research\ndirections and technological advancements, underscoring the evolving landscape\nof LLMs and their profound impact on artificial intelligence Artificial\nIntelligence (AI) and society.",
      "tldr_zh": "这篇调查论文对大型语言模型 (LLMs) 进行了全面分析，以 ChatGPT 为代表，探讨了其架构、训练过程以及在客服、教育、医疗和娱乐等领域的广泛应用。论文通过比较 ChatGPT 与其他 LLMs 的性能指标，突出了其独特优势，同时指出了潜在风险，如错误信息传播、偏见和数据隐私问题，并提供了数据集列表和应用案例。最终，该研究强调了 LLMs 在人工智能 (AI) 领域的演变及其对社会的影响，并提出了未来的研究方向。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17403v1",
      "published_date": "2025-03-19 22:55:08 UTC",
      "updated_date": "2025-03-19 22:55:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:51:15.236764"
    },
    {
      "arxiv_id": "2503.15726v1",
      "title": "Reinforcement Learning Environment with LLM-Controlled Adversary in D&D 5th Edition Combat",
      "title_zh": "基于LLM控制对手",
      "authors": [
        "Joseph Emmanuel DL Dayo",
        "Michel Onasis S. Ogbinar",
        "Prospero C. Naval Jr"
      ],
      "abstract": "The objective of this study is to design and implement a reinforcement\nlearning (RL) environment using D\\&D 5E combat scenarios to challenge smaller\nRL agents through interaction with a robust adversarial agent controlled by\nadvanced Large Language Models (LLMs) like GPT-4o and LLaMA 3 8B. This research\nemploys Deep Q-Networks (DQN) for the smaller agents, creating a testbed for\nstrategic AI development that also serves as an educational tool by simulating\ndynamic and unpredictable combat scenarios. We successfully integrated\nsophisticated language models into the RL framework, enhancing strategic\ndecision-making processes. Our results indicate that while RL agents generally\noutperform LLM-controlled adversaries in standard metrics, the strategic depth\nprovided by LLMs significantly enhances the overall AI capabilities in this\ncomplex, rule-based setting. The novelty of our approach and its implications\nfor mastering intricate environments and developing adaptive strategies are\ndiscussed, alongside potential innovations in AI-driven interactive\nsimulations. This paper aims to demonstrate how integrating LLMs can create\nmore robust and adaptable AI systems, providing valuable insights for further\nresearch and educational applications.",
      "tldr_zh": "本研究设计了一个基于 D&D 5th Edition 战斗场景的 Reinforcement Learning (RL) 环境，让小型 RL 代理使用 Deep Q-Networks (DQN) 与由 Large Language Models (LLMs) 如 GPT-4o 和 LLaMA 3 8B 控制的强大对手互动，以挑战代理的战略能力并作为教育工具。  \n通过整合 LLMs，该框架增强了决策过程的复杂性和动态性，实验结果显示 RL 代理在标准指标上通常优于 LLM 控制对手，但 LLMs 显著提升了整体战略深度。  \n这项创新为战略 AI 开发、适应复杂环境和 AI 驱动交互模拟提供了新见解，并具有潜在的教育和研究应用价值。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Preprint. Submitted to the 31st International Conference on Neural\n  Information Processing (ICONIP 2024)",
      "pdf_url": "http://arxiv.org/pdf/2503.15726v1",
      "published_date": "2025-03-19 22:48:20 UTC",
      "updated_date": "2025-03-19 22:48:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:51:28.285788"
    },
    {
      "arxiv_id": "2503.15724v1",
      "title": "Reward Training Wheels: Adaptive Auxiliary Rewards for Robotics Reinforcement Learning",
      "title_zh": "奖励",
      "authors": [
        "Linji Wang",
        "Tong Xu",
        "Yuanjie Lu",
        "Xuesu Xiao"
      ],
      "abstract": "Robotics Reinforcement Learning (RL) often relies on carefully engineered\nauxiliary rewards to supplement sparse primary learning objectives to\ncompensate for the lack of large-scale, real-world, trial-and-error data. While\nthese auxiliary rewards accelerate learning, they require significant\nengineering effort, may introduce human biases, and cannot adapt to the robot's\nevolving capabilities during training. In this paper, we introduce Reward\nTraining Wheels (RTW), a teacher-student framework that automates auxiliary\nreward adaptation for robotics RL. To be specific, the RTW teacher dynamically\nadjusts auxiliary reward weights based on the student's evolving capabilities\nto determine which auxiliary reward aspects require more or less emphasis to\nimprove the primary objective. We demonstrate RTW on two challenging robot\ntasks: navigation in highly constrained spaces and off-road vehicle mobility on\nvertically challenging terrain. In simulation, RTW outperforms expert-designed\nrewards by 2.35% in navigation success rate and improves off-road mobility\nperformance by 122.62%, while achieving 35% and 3X faster training efficiency,\nrespectively. Physical robot experiments further validate RTW's effectiveness,\nachieving a perfect success rate (5/5 trials vs. 2/5 for expert-designed\nrewards) and improving vehicle stability with up to 47.4% reduction in\norientation angles.",
      "tldr_zh": "机器人强化学习（RL）通常依赖手动设计的辅助奖励来弥补稀疏主要目标和缺乏真实试错数据的不足，但这些奖励需大量工程努力且无法适应训练中的能力变化。论文引入 Reward Training Wheels (RTW)，一个 teacher-student 框架，通过动态调整辅助奖励权重来根据机器人的演变能力优化主要目标。在模拟实验中，RTW 比专家设计奖励提高了导航成功率 2.35% 和越野机动性能 122.62%，并提升了训练效率（35% 和 3X）。物理机器人实验进一步证实其效果，实现完美成功率（5/5 试验）和车辆稳定性改善（方向角度减少达 47.4%）。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "7 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.15724v1",
      "published_date": "2025-03-19 22:45:59 UTC",
      "updated_date": "2025-03-19 22:45:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:51:39.142893"
    },
    {
      "arxiv_id": "2503.15707v1",
      "title": "Safety Aware Task Planning via Large Language Models in Robotics",
      "title_zh": "翻译失败",
      "authors": [
        "Azal Ahmad Khan",
        "Michael Andrev",
        "Muhammad Ali Murtaza",
        "Sergio Aguilera",
        "Rui Zhang",
        "Jie Ding",
        "Seth Hutchinson",
        "Ali Anwar"
      ],
      "abstract": "The integration of large language models (LLMs) into robotic task planning\nhas unlocked better reasoning capabilities for complex, long-horizon workflows.\nHowever, ensuring safety in LLM-driven plans remains a critical challenge, as\nthese models often prioritize task completion over risk mitigation. This paper\nintroduces SAFER (Safety-Aware Framework for Execution in Robotics), a\nmulti-LLM framework designed to embed safety awareness into robotic task\nplanning. SAFER employs a Safety Agent that operates alongside the primary task\nplanner, providing safety feedback. Additionally, we introduce LLM-as-a-Judge,\na novel metric leveraging LLMs as evaluators to quantify safety violations\nwithin generated task plans. Our framework integrates safety feedback at\nmultiple stages of execution, enabling real-time risk assessment, proactive\nerror correction, and transparent safety evaluation. We also integrate a\ncontrol framework using Control Barrier Functions (CBFs) to ensure safety\nguarantees within SAFER's task planning. We evaluated SAFER against\nstate-of-the-art LLM planners on complex long-horizon tasks involving\nheterogeneous robotic agents, demonstrating its effectiveness in reducing\nsafety violations while maintaining task efficiency. We also verify the task\nplanner and safety planner through actual hardware experiments involving\nmultiple robots and a human.",
      "tldr_zh": "本研究探讨了在机器人任务规划中使用大型语言模型（LLMs）带来的推理能力提升，同时强调了安全风险问题。论文提出SAFER框架，一个多LLM系统，通过Safety Agent提供安全反馈，并引入LLM-as-a-Judge指标来量化任务计划中的安全违规，同时整合Control Barrier Functions (CBFs)以确保实时风险评估和错误修正。在涉及异构机器人代理的复杂长程任务实验中，SAFER显著减少了安全违规，同时保持任务效率，并通过实际硬件实验验证了其有效性。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15707v1",
      "published_date": "2025-03-19 21:41:10 UTC",
      "updated_date": "2025-03-19 21:41:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:51:50.004919"
    },
    {
      "arxiv_id": "2503.15703v1",
      "title": "Predicting Multi-Agent Specialization via Task Parallelizability",
      "title_zh": "翻译失败",
      "authors": [
        "Elizabeth Mieczkowski",
        "Ruaridh Mon-Williams",
        "Neil Bramley",
        "Christopher G. Lucas",
        "Natalia Velez",
        "Thomas L. Griffiths"
      ],
      "abstract": "Multi-agent systems often rely on specialized agents with distinct roles\nrather than general-purpose agents that perform the entire task independently.\nHowever, the conditions that govern the optimal degree of specialization remain\npoorly understood. In this work, we propose that specialist teams outperform\ngeneralist ones when environmental constraints limit task parallelizability --\nthe potential to execute task components concurrently. Drawing inspiration from\ndistributed systems, we introduce a heuristic to predict the relative\nefficiency of generalist versus specialist teams by estimating the speed-up\nachieved when two agents perform a task in parallel rather than focus on\ncomplementary subtasks. We validate this heuristic through three multi-agent\nreinforcement learning (MARL) experiments in Overcooked-AI, demonstrating that\nkey factors limiting task parallelizability influence specialization. We also\nobserve that as the state space expands, agents tend to converge on specialist\nstrategies, even when generalist ones are theoretically more efficient,\nhighlighting potential biases in MARL training algorithms. Our findings provide\na principled framework for interpreting specialization given the task and\nenvironment, and introduce a novel benchmark for evaluating whether MARL finds\noptimal strategies.",
      "tldr_zh": "本研究探讨多智能体系统中的专业化问题，提出当环境约束限制任务并行izability时，专业化团队比通用团队更高效。\n作者引入一个启发式方法，受分布式系统启发，通过估计两个代理并行执行任务的速度提升来预测团队效率，并在Overcooked-AI的三个多智能体强化学习(MARL)实验中验证。\n结果显示，任务并行izability的限制和状态空间的扩展会促使代理趋向专业化策略，即使理论上通用策略更优，并为解释专业化和评估MARL是否找到最优策略提供了一个新框架。",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15703v1",
      "published_date": "2025-03-19 21:33:48 UTC",
      "updated_date": "2025-03-19 21:33:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:52:03.127014"
    },
    {
      "arxiv_id": "2503.15699v2",
      "title": "Representational Similarity via Interpretable Visual Concepts",
      "title_zh": "翻译失败",
      "authors": [
        "Neehar Kondapaneni",
        "Oisin Mac Aodha",
        "Pietro Perona"
      ],
      "abstract": "How do two deep neural networks differ in how they arrive at a decision?\nMeasuring the similarity of deep networks has been a long-standing open\nquestion. Most existing methods provide a single number to measure the\nsimilarity of two networks at a given layer, but give no insight into what\nmakes them similar or dissimilar. We introduce an interpretable\nrepresentational similarity method (RSVC) to compare two networks. We use RSVC\nto discover shared and unique visual concepts between two models. We show that\nsome aspects of model differences can be attributed to unique concepts\ndiscovered by one model that are not well represented in the other. Finally, we\nconduct extensive evaluation across different vision model architectures and\ntraining protocols to demonstrate its effectiveness.",
      "tldr_zh": "本研究探讨了深度神经网络（deep neural networks）在决策过程中的差异，提出了一种可解释的表示相似性方法（RSVC），用于量化并解释两个网络的相似性。RSVC 通过识别共享和独特的视觉概念（visual concepts）来揭示模型间的异同，例如一个模型可能独有某些概念，而另一个模型则缺乏相应表示。实验结果显示，该方法在不同视觉模型架构和训练协议上表现出色，有效地归因模型差异并提升了网络比较的可解释性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "q-bio.NC"
      ],
      "primary_category": "cs.CV",
      "comment": "32 pages, 5 Figures, 16 Supplemental Figures, ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.15699v2",
      "published_date": "2025-03-19 21:21:45 UTC",
      "updated_date": "2025-03-30 03:02:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:52:13.365966"
    },
    {
      "arxiv_id": "2503.15661v2",
      "title": "UI-Vision: A Desktop-centric GUI Benchmark for Visual Perception and Interaction",
      "title_zh": "UI-Vision：以桌面为中心的图形",
      "authors": [
        "Shravan Nayak",
        "Xiangru Jian",
        "Kevin Qinghong Lin",
        "Juan A. Rodriguez",
        "Montek Kalsi",
        "Rabiul Awal",
        "Nicolas Chapados",
        "M. Tamer Özsu",
        "Aishwarya Agrawal",
        "David Vazquez",
        "Christopher Pal",
        "Perouz Taslakian",
        "Spandana Gella",
        "Sai Rajeswar"
      ],
      "abstract": "Autonomous agents that navigate Graphical User Interfaces (GUIs) to automate\ntasks like document editing and file management can greatly enhance computer\nworkflows. While existing research focuses on online settings, desktop\nenvironments, critical for many professional and everyday tasks, remain\nunderexplored due to data collection challenges and licensing issues. We\nintroduce UI-Vision, the first comprehensive, license-permissive benchmark for\noffline, fine-grained evaluation of computer use agents in real-world desktop\nenvironments. Unlike online benchmarks, UI-Vision provides: (i) dense,\nhigh-quality annotations of human demonstrations, including bounding boxes, UI\nlabels, and action trajectories (clicks, drags, and keyboard inputs) across 83\nsoftware applications, and (ii) three fine-to-coarse grained tasks-Element\nGrounding, Layout Grounding, and Action Prediction-with well-defined metrics to\nrigorously evaluate agents' performance in desktop environments. Our evaluation\nreveals critical limitations in state-of-the-art models like UI-TARS-72B,\nincluding issues with understanding professional software, spatial reasoning,\nand complex actions like drag-and-drop. These findings highlight the challenges\nin developing fully autonomous computer use agents. By releasing UI-Vision as\nopen-source, we aim to advance the development of more capable agents for\nreal-world desktop tasks.",
      "tldr_zh": "该研究引入了UI-Vision，一个以桌面环境为中心的GUI基准，用于评估视觉感知和交互的计算机使用代理。该基准提供密集的高质量标注，包括边界框、UI标签和动作轨迹（如点击、拖拽和键盘输入），覆盖83个软件应用，并定义了三个细粒度任务：Element Grounding、Layout Grounding和Action Prediction，以严格评估代理性能。实验结果揭示了现有模型如UI-TARS-72B的局限性，包括在理解专业软件、空间推理和处理复杂动作方面的不足，从而突出了开发自主桌面代理的挑战。通过开源UI-Vision，该工作旨在推动更可靠的计算机任务自动化技术的发展。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "This paper has been accepted to the 41st International Conference on\n  Machine Learning (ICML 2025)",
      "pdf_url": "http://arxiv.org/pdf/2503.15661v2",
      "published_date": "2025-03-19 19:26:17 UTC",
      "updated_date": "2025-05-06 17:43:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:52:26.501220"
    },
    {
      "arxiv_id": "2503.15655v1",
      "title": "R$^2$: A LLM Based Novel-to-Screenplay Generation Framework with Causal Plot Graphs",
      "title_zh": "翻译失败",
      "authors": [
        "Zefeng Lin",
        "Yi Xiao",
        "Zhiqiang Mo",
        "Qifan Zhang",
        "Jie Wang",
        "Jiayang Chen",
        "Jiajing Zhang",
        "Hui Zhang",
        "Zhengyi Liu",
        "Xianyong Fang",
        "Xiaohua Xu"
      ],
      "abstract": "Automatically adapting novels into screenplays is important for the TV, film,\nor opera industries to promote products with low costs. The strong performances\nof large language models (LLMs) in long-text generation call us to propose a\nLLM based framework Reader-Rewriter (R$^2$) for this task. However, there are\ntwo fundamental challenges here. First, the LLM hallucinations may cause\ninconsistent plot extraction and screenplay generation. Second, the\ncausality-embedded plot lines should be effectively extracted for coherent\nrewriting. Therefore, two corresponding tactics are proposed: 1) A\nhallucination-aware refinement method (HAR) to iteratively discover and\neliminate the affections of hallucinations; and 2) a causal plot-graph\nconstruction method (CPC) based on a greedy cycle-breaking algorithm to\nefficiently construct plot lines with event causalities. Recruiting those\nefficient techniques, R$^2$ utilizes two modules to mimic the human screenplay\nrewriting process: The Reader module adopts a sliding window and CPC to build\nthe causal plot graphs, while the Rewriter module generates first the scene\noutlines based on the graphs and then the screenplays. HAR is integrated into\nboth modules for accurate inferences of LLMs. Experimental results demonstrate\nthe superiority of R$^2$, which substantially outperforms three existing\napproaches (51.3%, 22.6%, and 57.1% absolute increases) in pairwise comparison\nat the overall win rate for GPT-4o.",
      "tldr_zh": "本研究提出 R$^2$ 框架，利用大型语言模型 (LLMs) 自动将小说转化为剧本，以降低影视行业的制作成本。框架通过 Hallucination-aware refinement method (HAR) 迭代消除 LLM 的幻觉问题，以及 Causal plot-graph construction method (CPC) 基于贪婪循环破坏算法构建带有事件因果性的剧情图，确保情节一致性和连贯性。Reader 模块使用滑动窗口和 CPC 生成因果剧情图，Rewriter 模块则基于这些图先创建场景大纲再生成剧本，并将 HAR 整合到两个模块中。实验结果显示，R$^2$ 在 GPT-4o 的配对比较中大幅优于现有方法，整体胜率绝对提升达 51.3%、22.6% 和 57.1%。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "16 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.15655v1",
      "published_date": "2025-03-19 19:09:40 UTC",
      "updated_date": "2025-03-19 19:09:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:52:39.129624"
    },
    {
      "arxiv_id": "2503.16556v1",
      "title": "Reliable Radiologic Skeletal Muscle Area Assessment -- A Biomarker for Cancer Cachexia Diagnosis",
      "title_zh": "可靠的放射学骨骼肌面积评估——癌症恶病质诊断的生物标志物",
      "authors": [
        "Sabeen Ahmed",
        "Nathan Parker",
        "Margaret Park",
        "Daniel Jeong",
        "Lauren Peres",
        "Evan W. Davis",
        "Jennifer B. Permuth",
        "Erin Siegel",
        "Matthew B. Schabath",
        "Yasin Yilmaz",
        "Ghulam Rasool"
      ],
      "abstract": "Cancer cachexia is a common metabolic disorder characterized by severe muscle\natrophy which is associated with poor prognosis and quality of life. Monitoring\nskeletal muscle area (SMA) longitudinally through computed tomography (CT)\nscans, an imaging modality routinely acquired in cancer care, is an effective\nway to identify and track this condition. However, existing tools often lack\nfull automation and exhibit inconsistent accuracy, limiting their potential for\nintegration into clinical workflows. To address these challenges, we developed\nSMAART-AI (Skeletal Muscle Assessment-Automated and Reliable Tool-based on AI),\nan end-to-end automated pipeline powered by deep learning models (nnU-Net 2D)\ntrained on mid-third lumbar level CT images with 5-fold cross-validation,\nensuring generalizability and robustness. SMAART-AI incorporates an\nuncertainty-based mechanism to flag high-error SMA predictions for expert\nreview, enhancing reliability. We combined the SMA, skeletal muscle index, BMI,\nand clinical data to train a multi-layer perceptron (MLP) model designed to\npredict cachexia at the time of cancer diagnosis. Tested on the\ngastroesophageal cancer dataset, SMAART-AI achieved a Dice score of 97.80% +/-\n0.93%, with SMA estimated across all four datasets in this study at a median\nabsolute error of 2.48% compared to manual annotations with SliceOmatic.\nUncertainty metrics-variance, entropy, and coefficient of variation-strongly\ncorrelated with SMA prediction errors (0.83, 0.76, and 0.73 respectively). The\nMLP model predicts cachexia with 79% precision, providing clinicians with a\nreliable tool for early diagnosis and intervention. By combining automation,\naccuracy, and uncertainty awareness, SMAART-AI bridges the gap between research\nand clinical application, offering a transformative approach to managing cancer\ncachexia.",
      "tldr_zh": "本研究针对癌症恶病质（一种伴随肌肉萎缩的代谢紊乱）诊断，开发了SMAART-AI（基于AI的自动化可靠工具），利用nnU-Net 2D模型在腰椎中段CT图像上训练，以实现端到端自动化评估骨骼肌面积(SMA)。该工具整合不确定性机制（如方差、熵和变异系数）来标记高错误预测，供专家审查，并结合SMA、骨骼肌指数、BMI和临床数据训练多层感知器(MLP)模型进行恶病质预测。测试结果显示，SMAART-AI在胃食管癌数据集上达到97.80% ± 0.93%的Dice score，中位绝对误差仅2.48%，MLP模型的预测精确度为79%，从而为临床早期诊断和干预提供可靠的生物标志物工具。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CE",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "47 pages, 19 figures, 9 Tables",
      "pdf_url": "http://arxiv.org/pdf/2503.16556v1",
      "published_date": "2025-03-19 19:07:59 UTC",
      "updated_date": "2025-03-19 19:07:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:52:52.996251"
    },
    {
      "arxiv_id": "2503.15650v1",
      "title": "Survey on Generalization Theory for Graph Neural Networks",
      "title_zh": "图神经网络的泛化理论综述",
      "authors": [
        "Antonis Vasileiou",
        "Stefanie Jegelka",
        "Ron Levie",
        "Christopher Morris"
      ],
      "abstract": "Message-passing graph neural networks (MPNNs) have emerged as the leading\napproach for machine learning on graphs, attracting significant attention in\nrecent years. While a large set of works explored the expressivity of MPNNs,\ni.e., their ability to separate graphs and approximate functions over them,\ncomparatively less attention has been directed toward investigating their\ngeneralization abilities, i.e., making meaningful predictions beyond the\ntraining data. Here, we systematically review the existing literature on the\ngeneralization abilities of MPNNs. We analyze the strengths and limitations of\nvarious studies in these domains, providing insights into their methodologies\nand findings. Furthermore, we identify potential avenues for future research,\naiming to deepen our understanding of the generalization abilities of MPNNs.",
      "tldr_zh": "这篇论文对消息传递图神经网络（MPNNs）的泛化理论进行了系统调研，重点探讨了MPNNs在训练数据之外进行有效预测的能力。作者分析了现有文献中MPNNs泛化研究的优势和局限性，包括方法论和关键发现，并突出了表达性研究相对较多而泛化研究不足的问题。通过这项工作，论文为未来深入理解MPNNs的泛化能力提供了宝贵见解和潜在研究方向。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15650v1",
      "published_date": "2025-03-19 19:04:24 UTC",
      "updated_date": "2025-03-19 19:04:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:53:03.692995"
    },
    {
      "arxiv_id": "2503.17401v5",
      "title": "AIJIM: A Scalable Model for Real-Time AI in Environmental Journalism",
      "title_zh": "翻译失败",
      "authors": [
        "Torsten Tiltack"
      ],
      "abstract": "This paper introduces AIJIM, the Artificial Intelligence Journalism\nIntegration Model -- a novel framework for integrating real-time AI into\nenvironmental journalism. AIJIM combines Vision Transformer-based hazard\ndetection, crowdsourced validation with 252 validators, and automated reporting\nwithin a scalable, modular architecture. A dual-layer explainability approach\nensures ethical transparency through fast CAM-based visual overlays and\noptional LIME-based box-level interpretations. Validated in a 2024 pilot on the\nisland of Mallorca using the NamicGreen platform, AIJIM achieved 85.4\\%\ndetection accuracy and 89.7\\% agreement with expert annotations, while reducing\nreporting latency by 40\\%. Unlike conventional approaches such as Data-Driven\nJournalism or AI Fact-Checking, AIJIM provides a transferable model for\nparticipatory, community-driven environmental reporting, advancing journalism,\nartificial intelligence, and sustainability in alignment with the UN\nSustainable Development Goals and the EU AI Act.",
      "tldr_zh": "本研究提出 AIJIM（Artificial Intelligence Journalism Integration Model），一个可扩展的框架，用于将实时 AI 整合到环境新闻中。该模型结合 Vision Transformer-based hazard detection、crowdsourced validation（使用 252 名验证者）和 automated reporting 于模块化架构中，并采用 dual-layer explainability approach（包括 fast CAM-based visual overlays 和 optional LIME-based box-level interpretations）以确保伦理透明。在 2024 年马洛卡岛的 NamicGreen 平台试点中，AIJIM 实现了 85.4% 的检测准确率、89.7% 与专家注释的一致性，并将报告延迟减少 40%。与传统 Data-Driven Journalism 或 AI Fact-Checking 方法不同，AIJIM 提供了一个可转移的参与式社区驱动模型，促进新闻、AI 和可持续性发展，符合 UN Sustainable Development Goals 和 EU AI Act。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC",
        "68T45",
        "I.2.10; H.3.5; J.4"
      ],
      "primary_category": "cs.CY",
      "comment": "22 pages, 10 figures, 5 tables. Keywords: Artificial Intelligence,\n  Environmental Journalism, Real-Time Reporting, Vision Transformers, Image\n  Recognition, Crowdsourced Validation, GPT-4, Automated News Generation, GIS\n  Integration, Data Privacy Compliance, Explainable AI (XAI), AI Ethics,\n  Sustainable Development",
      "pdf_url": "http://arxiv.org/pdf/2503.17401v5",
      "published_date": "2025-03-19 19:00:24 UTC",
      "updated_date": "2025-04-28 11:18:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:53:17.798078"
    },
    {
      "arxiv_id": "2503.15639v1",
      "title": "A Context-Driven Training-Free Network for Lightweight Scene Text Segmentation and Recognition",
      "title_zh": "一种基于上下文的无训练网络，用于轻量级场景文本分割和识别",
      "authors": [
        "Ritabrata Chakraborty",
        "Shivakumara Palaiahnakote",
        "Umapada Pal",
        "Cheng-Lin Liu"
      ],
      "abstract": "Modern scene text recognition systems often depend on large end-to-end\narchitectures that require extensive training and are prohibitively expensive\nfor real-time scenarios. In such cases, the deployment of heavy models becomes\nimpractical due to constraints on memory, computational resources, and latency.\nTo address these challenges, we propose a novel, training-free plug-and-play\nframework that leverages the strengths of pre-trained text recognizers while\nminimizing redundant computations. Our approach uses context-based\nunderstanding and introduces an attention-based segmentation stage, which\nrefines candidate text regions at the pixel level, improving downstream\nrecognition. Instead of performing traditional text detection that follows a\nblock-level comparison between feature map and source image and harnesses\ncontextual information using pretrained captioners, allowing the framework to\ngenerate word predictions directly from scene context.Candidate texts are\nsemantically and lexically evaluated to get a final score. Predictions that\nmeet or exceed a pre-defined confidence threshold bypass the heavier process of\nend-to-end text STR profiling, ensuring faster inference and cutting down on\nunnecessary computations. Experiments on public benchmarks demonstrate that our\nparadigm achieves performance on par with state-of-the-art systems, yet\nrequires substantially fewer resources.",
      "tldr_zh": "本研究提出了一种基于上下文驱动的训练-free 网络，用于轻量级场景文本分割和识别。该框架是即插即用的，利用预训练文本识别器减少冗余计算，并引入注意力-based segmentation 阶段，对候选文本区域进行像素级精炼，以提升下游识别性能。不同于传统文本检测，该方法借助预训练标题生成器（captioners）直接从场景上下文生成单词预测，并通过语义和词汇评估得到最终分数，仅对达到置信阈值的预测进行处理，从而实现更快推理和降低计算资源消耗。在公共基准测试中，该框架的性能与最先进系统相当，但资源需求大幅减少。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15639v1",
      "published_date": "2025-03-19 18:51:01 UTC",
      "updated_date": "2025-03-19 18:51:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:53:28.228386"
    },
    {
      "arxiv_id": "2503.15629v1",
      "title": "Neural Lyapunov Function Approximation with Self-Supervised Reinforcement Learning",
      "title_zh": "神经李亚普诺夫函数",
      "authors": [
        "Luc McCutcheon",
        "Bahman Gharesifard",
        "Saber Fallah"
      ],
      "abstract": "Control Lyapunov functions are traditionally used to design a controller\nwhich ensures convergence to a desired state, yet deriving these functions for\nnonlinear systems remains a complex challenge. This paper presents a novel,\nsample-efficient method for neural approximation of nonlinear Lyapunov\nfunctions, leveraging self-supervised Reinforcement Learning (RL) to enhance\ntraining data generation, particularly for inaccurately represented regions of\nthe state space. The proposed approach employs a data-driven World Model to\ntrain Lyapunov functions from off-policy trajectories. The method is validated\non both standard and goal-conditioned robotic tasks, demonstrating faster\nconvergence and higher approximation accuracy compared to the state-of-the-art\nneural Lyapunov approximation baseline. The code is available at:\nhttps://github.com/CAV-Research-Lab/SACLA.git",
      "tldr_zh": "本文提出了一种新颖的样本高效方法，用于神经逼近非线性 Lyapunov 函数，借助自监督强化学习(Self-Supervised Reinforcement Learning)来生成训练数据，特别是针对状态空间中表示不准确的区域。该方法采用数据驱动的 World Model，从 off-policy 轨迹中训练 Lyapunov 函数，以解决非线性系统控制中的复杂挑战。在标准和目标条件下的机器人任务上验证，该方法比现有最先进神经 Lyapunov 逼近基线实现了更快收敛和更高准确性。代码已在 GitHub 上公开（https://github.com/CAV-Research-Lab/SACLA.git）。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CG",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted at IEEE International Conference on Robotics and Automation\n  (ICRA)",
      "pdf_url": "http://arxiv.org/pdf/2503.15629v1",
      "published_date": "2025-03-19 18:29:25 UTC",
      "updated_date": "2025-03-19 18:29:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:53:40.290083"
    },
    {
      "arxiv_id": "2503.15621v1",
      "title": "LLaVA-MORE: A Comparative Study of LLMs and Visual Backbones for Enhanced Visual Instruction Tuning",
      "title_zh": "翻译失败",
      "authors": [
        "Federico Cocchi",
        "Nicholas Moratelli",
        "Davide Caffagni",
        "Sara Sarto",
        "Lorenzo Baraldi",
        "Marcella Cornia",
        "Rita Cucchiara"
      ],
      "abstract": "Recent progress in Multimodal Large Language Models (MLLMs) has highlighted\nthe critical roles of both the visual backbone and the underlying language\nmodel. While prior work has primarily focused on scaling these components to\nbillions of parameters, the trade-offs between model size, architecture, and\nperformance remain underexplored. Additionally, inconsistencies in training\ndata and evaluation protocols have hindered direct comparisons, making it\ndifficult to derive optimal design choices. In this paper, we introduce\nLLaVA-MORE, a new family of MLLMs that integrates recent language models with\ndiverse visual backbones. To ensure fair comparisons, we employ a unified\ntraining protocol applied consistently across all architectures. Our analysis\nsystematically explores both small- and medium-scale LLMs -- including Phi-4,\nLLaMA-3.1, and Gemma-2 -- to evaluate multimodal reasoning, generation, and\ninstruction following, while examining the relationship between model size and\nperformance. Beyond evaluating the LLM impact on final results, we conduct a\ncomprehensive study of various visual encoders, ranging from CLIP-based\narchitectures to alternatives such as DINOv2, SigLIP, and SigLIP2. Additional\nexperiments investigate the effects of increased image resolution and\nvariations in pre-training datasets. Overall, our results provide insights into\nthe design of more effective MLLMs, offering a reproducible evaluation\nframework that facilitates direct comparisons and can guide future model\ndevelopment. Our source code and trained models are publicly available at:\nhttps://github.com/aimagelab/LLaVA-MORE.",
      "tldr_zh": "这篇论文介绍了 LLaVA-MORE，这是一个新系列的多模态大语言模型（MLLMs），旨在通过比较不同 LLMs（如 Phi-4、LLaMA-3.1 和 Gemma-2）和视觉骨干（如 CLIP-based、DINOv2、SigLIP 和 SigLIP2）来增强视觉指令微调。研究采用统一的训练协议，确保公平比较模型大小、架构与性能的权衡，同时评估多模态推理、生成和指令遵循能力。结果显示，模型规模与性能存在密切关系，增加图像分辨率和优化预训练数据集可显著提升效果，为设计更有效的 MLLMs 提供了宝贵见解，并公开了代码和模型以支持未来开发。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15621v1",
      "published_date": "2025-03-19 18:10:12 UTC",
      "updated_date": "2025-03-19 18:10:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:53:53.366049"
    },
    {
      "arxiv_id": "2503.15620v1",
      "title": "Does Context Matter? ContextualJudgeBench for Evaluating LLM-based Judges in Contextual Settings",
      "title_zh": "翻译失败",
      "authors": [
        "Austin Xu",
        "Srijan Bansal",
        "Yifei Ming",
        "Semih Yavuz",
        "Shafiq Joty"
      ],
      "abstract": "The large language model (LLM)-as-judge paradigm has been used to meet the\ndemand for a cheap, reliable, and fast evaluation of model outputs during AI\nsystem development and post-deployment monitoring. While judge models -- LLMs\nfinetuned to specialize in assessing and critiquing model outputs -- have been\ntouted as general purpose evaluators, they are typically evaluated only on\nnon-contextual scenarios, such as instruction following. The omission of\ncontextual settings -- those where external information is used as context to\ngenerate an output -- is surprising given the increasing prevalence of\nretrieval-augmented generation (RAG) and summarization use cases. Contextual\nassessment is uniquely challenging, as evaluation often depends on practitioner\npriorities, leading to conditional evaluation criteria (e.g., comparing\nresponses based on factuality and then considering completeness if they are\nequally factual). To address the gap, we propose ContextualJudgeBench, a judge\nbenchmark with 2,000 challenging response pairs across eight splits inspired by\nreal-world contextual evaluation scenarios. We build our benchmark with a\nmulti-pronged data construction pipeline that leverages both existing human\nannotations and model-based perturbations. Our comprehensive study across 11\njudge models and 9 general purpose models, reveals that the contextual\ninformation and its assessment criteria present a significant challenge to even\nstate-of-the-art models. For example, OpenAI's o1, the best-performing model,\nbarely reaches 55% consistent accuracy.",
      "tldr_zh": "这篇论文探讨了LLM-based judges在上下文设置中的评估挑战，指出现有模型主要在非上下文场景（如指令遵循）上测试，而忽略了如RAG和总结等依赖外部信息的场景。作者提出ContextualJudgeBench，一个包含2000对响应对的基准，基于多方面数据构建管道（包括人类注释和模型扰动），以模拟真实世界上下文评估情景。实验结果显示，即使是顶级模型如OpenAI的o1也仅达到55%的一致准确率，突显了上下文信息和条件评估标准的重大难题。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "23 pages, 13 figures, 6 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.15620v1",
      "published_date": "2025-03-19 18:09:19 UTC",
      "updated_date": "2025-03-19 18:09:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:54:05.508533"
    },
    {
      "arxiv_id": "2503.15617v1",
      "title": "CAM-Seg: A Continuous-valued Embedding Approach for Semantic Image Generation",
      "title_zh": "CAM-Seg: 一种连续值嵌入方法用于语义图像生成",
      "authors": [
        "Masud Ahmed",
        "Zahid Hasan",
        "Syed Arefinul Haque",
        "Abu Zaher Md Faridee",
        "Sanjay Purushotham",
        "Suya You",
        "Nirmalya Roy"
      ],
      "abstract": "Traditional transformer-based semantic segmentation relies on quantized\nembeddings. However, our analysis reveals that autoencoder accuracy on\nsegmentation mask using quantized embeddings (e.g. VQ-VAE) is 8% lower than\ncontinuous-valued embeddings (e.g. KL-VAE). Motivated by this, we propose a\ncontinuous-valued embedding framework for semantic segmentation. By\nreformulating semantic mask generation as a continuous image-to-embedding\ndiffusion process, our approach eliminates the need for discrete latent\nrepresentations while preserving fine-grained spatial and semantic details. Our\nkey contribution includes a diffusion-guided autoregressive transformer that\nlearns a continuous semantic embedding space by modeling long-range\ndependencies in image features. Our framework contains a unified architecture\ncombining a VAE encoder for continuous feature extraction, a diffusion-guided\ntransformer for conditioned embedding generation, and a VAE decoder for\nsemantic mask reconstruction. Our setting facilitates zero-shot domain\nadaptation capabilities enabled by the continuity of the embedding space.\nExperiments across diverse datasets (e.g., Cityscapes and domain-shifted\nvariants) demonstrate state-of-the-art robustness to distribution shifts,\nincluding adverse weather (e.g., fog, snow) and viewpoint variations. Our model\nalso exhibits strong noise resilience, achieving robust performance ($\\approx$\n95% AP compared to baseline) under gaussian noise, moderate motion blur, and\nmoderate brightness/contrast variations, while experiencing only a moderate\nimpact ($\\approx$ 90% AP compared to baseline) from 50% salt and pepper noise,\nsaturation and hue shifts. Code available:\nhttps://github.com/mahmed10/CAMSS.git",
      "tldr_zh": "该研究发现，传统基于 Transformer 的语义分割使用量化嵌入（如 VQ-VAE）比连续值嵌入（如 KL-VAE）准确率低 8%，因此提出 CAM-Seg 框架，通过将语义掩码生成重新表述为连续图像到嵌入的扩散过程，避免离散潜在表示并保留细粒度空间和语义细节。框架的核心是扩散引导的自回归 Transformer，它学习连续语义嵌入空间并建模图像特征的长程依赖，结合 VAE 编码器提取特征、Transformer 生成条件嵌入以及 VAE 解码器重建语义掩码，支持零样本域适应。实验在 Cityscapes 等数据集上显示，该模型在分布偏移（如雾、雪天气和视角变化）下表现出最先进鲁棒性，并在高斯噪声、运动模糊等条件下保持高性能（约 95% AP），即使在盐胡椒噪声和色调变化下也仅下降至约 90% AP。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15617v1",
      "published_date": "2025-03-19 18:06:54 UTC",
      "updated_date": "2025-03-19 18:06:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:54:17.839138"
    },
    {
      "arxiv_id": "2503.15615v1",
      "title": "PEnGUiN: Partially Equivariant Graph NeUral Networks for Sample Efficient MARL",
      "title_zh": "PEnGUiN：部分等变图神经网络用于样本高效多智能体强化学习",
      "authors": [
        "Joshua McClellan",
        "Greyson Brothers",
        "Furong Huang",
        "Pratap Tokekar"
      ],
      "abstract": "Equivariant Graph Neural Networks (EGNNs) have emerged as a promising\napproach in Multi-Agent Reinforcement Learning (MARL), leveraging symmetry\nguarantees to greatly improve sample efficiency and generalization. However,\nreal-world environments often exhibit inherent asymmetries arising from factors\nsuch as external forces, measurement inaccuracies, or intrinsic system biases.\nThis paper introduces \\textit{Partially Equivariant Graph NeUral Networks\n(PEnGUiN)}, a novel architecture specifically designed to address these\nchallenges. We formally identify and categorize various types of partial\nequivariance relevant to MARL, including subgroup equivariance, feature-wise\nequivariance, regional equivariance, and approximate equivariance. We\ntheoretically demonstrate that PEnGUiN is capable of learning both fully\nequivariant (EGNN) and non-equivariant (GNN) representations within a unified\nframework. Through extensive experiments on a range of MARL problems\nincorporating various asymmetries, we empirically validate the efficacy of\nPEnGUiN. Our results consistently demonstrate that PEnGUiN outperforms both\nEGNNs and standard GNNs in asymmetric environments, highlighting their\npotential to improve the robustness and applicability of graph-based MARL\nalgorithms in real-world scenarios.",
      "tldr_zh": "本文提出 PEnGUiN，一种部分等变图神经网络（Partially Equivariant Graph NeUral Networks），旨在解决多智能体强化学习 (MARL) 中存在的环境不对称性问题，如外部力量或系统偏差，从而提升样本效率和泛化能力。PEnGUiN 形式化定义了多种部分等变性，包括 subgroup equivariance、feature-wise equivariance、regional equivariance 和 approximate equivariance，并在统一框架下整合 fully equivariant (EGNN) 和 non-equivariant (GNN) 表示。实验结果显示，PEnGUiN 在各种不对称 MARL 场景中优于传统 EGNNs 和 GNNs，提高了算法的鲁棒性和实际应用潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15615v1",
      "published_date": "2025-03-19 18:01:14 UTC",
      "updated_date": "2025-03-19 18:01:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:54:30.136200"
    },
    {
      "arxiv_id": "2503.15485v2",
      "title": "TULIP: Towards Unified Language-Image Pretraining",
      "title_zh": "翻译失败",
      "authors": [
        "Zineng Tang",
        "Long Lian",
        "Seun Eisape",
        "XuDong Wang",
        "Roei Herzig",
        "Adam Yala",
        "Alane Suhr",
        "Trevor Darrell",
        "David M. Chan"
      ],
      "abstract": "Despite the recent success of image-text contrastive models like CLIP and\nSigLIP, these models often struggle with vision-centric tasks that demand\nhigh-fidelity image understanding, such as counting, depth estimation, and\nfine-grained object recognition. These models, by performing language\nalignment, tend to prioritize high-level semantics over visual understanding,\nweakening their image understanding. On the other hand, vision-focused models\nare great at processing visual information but struggle to understand language,\nlimiting their flexibility for language-driven tasks. In this work, we\nintroduce TULIP, an open-source, drop-in replacement for existing CLIP-like\nmodels. Our method leverages generative data augmentation, enhanced image-image\nand text-text contrastive learning, and image/text reconstruction\nregularization to learn fine-grained visual features while preserving global\nsemantic alignment. Our approach, scaling to over 1B parameters, outperforms\nexisting state-of-the-art (SOTA) models across multiple benchmarks,\nestablishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to\na $2\\times$ enhancement over SigLIP on RxRx1 in linear probing for few-shot\nclassification, and improving vision-language models, achieving over $3\\times$\nhigher scores than SigLIP on MMVP. Our code/checkpoints are available at\nhttps://tulip-berkeley.github.io",
      "tldr_zh": "本研究提出 TULIP，一种统一的语言-图像预训练框架，旨在解决现有模型如 CLIP 和 SigLIP 在视觉密集任务（如计数、深度估计和细粒度对象识别）上的不足，这些模型偏重高层语义而忽略细致视觉理解。TULIP 通过生成数据增强、增强的图像-图像及文本-文本对比学习，以及图像/文本重建正则化，学习细粒度的视觉特征，同时保持全局语义对齐，并扩展到超过 1B 参数规模。在多个基准上，TULIP 超越现有 SOTA 模型，在 ImageNet-1K 的 zero-shot 性能上建立新纪录，在 RxRx1 的 few-shot 分类上比 SigLIP 提升 2 倍，在 MMVP 上提升超过 3 倍。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "(v2) Clarified fine-tuning process, updated appendix",
      "pdf_url": "http://arxiv.org/pdf/2503.15485v2",
      "published_date": "2025-03-19 17:58:57 UTC",
      "updated_date": "2025-04-07 21:50:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:54:41.512886"
    },
    {
      "arxiv_id": "2503.15484v1",
      "title": "Value Profiles for Encoding Human Variation",
      "title_zh": "翻译失败",
      "authors": [
        "Taylor Sorensen",
        "Pushkar Mishra",
        "Roma Patel",
        "Michael Henry Tessler",
        "Michiel Bakker",
        "Georgina Evans",
        "Iason Gabriel",
        "Noah Goodman",
        "Verena Rieser"
      ],
      "abstract": "Modelling human variation in rating tasks is crucial for enabling AI systems\nfor personalization, pluralistic model alignment, and computational social\nscience. We propose representing individuals using value profiles -- natural\nlanguage descriptions of underlying values compressed from in-context\ndemonstrations -- along with a steerable decoder model to estimate ratings\nconditioned on a value profile or other rater information. To measure the\npredictive information in rater representations, we introduce an\ninformation-theoretic methodology. We find that demonstrations contain the most\ninformation, followed by value profiles and then demographics. However, value\nprofiles offer advantages in terms of scrutability, interpretability, and\nsteerability due to their compressed natural language format. Value profiles\neffectively compress the useful information from demonstrations (>70%\ninformation preservation). Furthermore, clustering value profiles to identify\nsimilarly behaving individuals better explains rater variation than the most\npredictive demographic groupings. Going beyond test set performance, we show\nthat the decoder models interpretably change ratings according to semantic\nprofile differences, are well-calibrated, and can help explain instance-level\ndisagreement by simulating an annotator population. These results demonstrate\nthat value profiles offer novel, predictive ways to describe individual\nvariation beyond demographics or group information.",
      "tldr_zh": "本研究提出“value profiles”——一种从上下文演示中压缩而成的自然语言描述，用于编码人类在评分任务中的个体变异，并结合可引导解码器模型来估计基于这些描述的评分。研究引入信息理论方法评估预测信息，发现value profiles 保留了演示中的大部分有用信息（>70%），并在可解释性、可审查性和聚类方面优于人口统计学数据。实验结果显示，value profiles 能更好地解释评分者变异，帮助模拟评分者群体、分析实例级分歧，并提供超越传统分组的预测方式。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15484v1",
      "published_date": "2025-03-19 17:57:49 UTC",
      "updated_date": "2025-03-19 17:57:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:54:53.685030"
    },
    {
      "arxiv_id": "2503.15481v1",
      "title": "Learning to Play Piano in the Real World",
      "title_zh": "在真实世界中学习弹奏钢琴",
      "authors": [
        "Yves-Simon Zeulner",
        "Sandeep Selvaraj",
        "Roberto Calandra"
      ],
      "abstract": "Towards the grand challenge of achieving human-level manipulation in robots,\nplaying piano is a compelling testbed that requires strategic, precise, and\nflowing movements. Over the years, several works demonstrated hand-designed\ncontrollers on real world piano playing, while other works evaluated robot\nlearning approaches on simulated piano scenarios. In this paper, we develop the\nfirst piano playing robotic system that makes use of learning approaches while\nalso being deployed on a real world dexterous robot. Specifically, we make use\nof Sim2Real to train a policy in simulation using reinforcement learning before\ndeploying the learned policy on a real world dexterous robot. In our\nexperiments, we thoroughly evaluate the interplay between domain randomization\nand the accuracy of the dynamics model used in simulation. Moreover, we\nevaluate the robot's performance across multiple songs with varying complexity\nto study the generalization of our learned policy. By providing a\nproof-of-concept of learning to play piano in the real world, we want to\nencourage the community to adopt piano playing as a compelling benchmark\ntowards human-level manipulation. We open-source our code and show additional\nvideos at https://lasr.org/research/learning-to-play-piano .",
      "tldr_zh": "这篇论文针对机器人实现人类水平操作的挑战，以弹钢琴作为测试平台，开发了第一个使用学习方法并部署在真实世界灵巧机器人的系统。作者采用 Sim2Real 技术，通过在模拟环境中使用 Reinforcement Learning 训练策略，然后转移到真实机器人进行弹奏。实验评估了 Domain Randomization 与模拟动态模型准确性的相互作用，并测试了机器人对多种复杂歌曲的泛化性能，结果证明了该方法的有效性。该工作为推进机器人操作基准提供了概念证明，并开源了代码以鼓励社区进一步研究。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "10 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.15481v1",
      "published_date": "2025-03-19 17:56:14 UTC",
      "updated_date": "2025-03-19 17:56:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:55:05.828691"
    },
    {
      "arxiv_id": "2503.15477v1",
      "title": "What Makes a Reward Model a Good Teacher? An Optimization Perspective",
      "title_zh": "翻译失败",
      "authors": [
        "Noam Razin",
        "Zixuan Wang",
        "Hubert Strauss",
        "Stanley Wei",
        "Jason D. Lee",
        "Sanjeev Arora"
      ],
      "abstract": "The success of Reinforcement Learning from Human Feedback (RLHF) critically\ndepends on the quality of the reward model. While this quality is primarily\nevaluated through accuracy, it remains unclear whether accuracy fully captures\nwhat makes a reward model an effective teacher. We address this question from\nan optimization perspective. First, we prove that regardless of how accurate a\nreward model is, if it induces low reward variance, then the RLHF objective\nsuffers from a flat landscape. Consequently, even a perfectly accurate reward\nmodel can lead to extremely slow optimization, underperforming less accurate\nmodels that induce higher reward variance. We additionally show that a reward\nmodel that works well for one language model can induce low reward variance,\nand thus a flat objective landscape, for another. These results establish a\nfundamental limitation of evaluating reward models solely based on accuracy or\nindependently of the language model they guide. Experiments using models of up\nto 8B parameters corroborate our theory, demonstrating the interplay between\nreward variance, accuracy, and reward maximization rate. Overall, our findings\nhighlight that beyond accuracy, a reward model needs to induce sufficient\nvariance for efficient optimization.",
      "tldr_zh": "该研究从优化视角探讨了什么因素使奖励模型成为有效的 RLHF（Reinforcement Learning from Human Feedback）教师，强调准确性并非唯一标准。论文证明，如果奖励模型导致奖励方差过低，则 RLHF 目标函数会出现平坦景观，造成优化过程缓慢，甚至精确的奖励模型也可能不如方差较高的模型表现好。此外，实验验证了奖励方差、准确性和奖励最大化率之间的互动，表明奖励模型需针对特定语言模型设计，以确保足够的方差来提升优化效率。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Code available at https://github.com/princeton-pli/what-makes-good-rm",
      "pdf_url": "http://arxiv.org/pdf/2503.15477v1",
      "published_date": "2025-03-19 17:54:41 UTC",
      "updated_date": "2025-03-19 17:54:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:55:17.108355"
    },
    {
      "arxiv_id": "2503.15470v1",
      "title": "EgoDTM: Towards 3D-Aware Egocentric Video-Language Pretraining",
      "title_zh": "翻译失败",
      "authors": [
        "Boshen Xu",
        "Yuting Mei",
        "Xinbi Liu",
        "Sipeng Zheng",
        "Qin Jin"
      ],
      "abstract": "Egocentric video-language pretraining has significantly advanced video\nrepresentation learning. Humans perceive and interact with a fully 3D world,\ndeveloping spatial awareness that extends beyond text-based understanding.\nHowever, most previous works learn from 1D text or 2D visual cues, such as\nbounding boxes, which inherently lack 3D understanding. To bridge this gap, we\nintroduce EgoDTM, an Egocentric Depth- and Text-aware Model, jointly trained\nthrough large-scale 3D-aware video pretraining and video-text contrastive\nlearning. EgoDTM incorporates a lightweight 3D-aware decoder to efficiently\nlearn 3D-awareness from pseudo depth maps generated by depth estimation models.\nTo further facilitate 3D-aware video pretraining, we enrich the original brief\ncaptions with hand-object visual cues by organically combining several\nfoundation models. Extensive experiments demonstrate EgoDTM's superior\nperformance across diverse downstream tasks, highlighting its superior 3D-aware\nvisual understanding. Our code will be released at\nhttps://github.com/xuboshen/EgoDTM.",
      "tldr_zh": "该研究提出EgoDTM，一种Egocentric Depth- and Text-aware Model，旨在通过3D-aware视频预训练和视频-文本对比学习提升第一人称视角视频表示学习，弥补现有方法仅依赖1D文本或2D视觉线索（如bounding boxes）的不足。EgoDTM采用轻量级的3D-aware decoder从伪深度图（由depth estimation models生成）中高效学习3D意识，并通过结合多个基础模型来丰富原始标题，添加手部-物体视觉线索以增强预训练。实验结果显示，EgoDTM在各种下游任务中表现出色，显著提升了3D-aware视觉理解能力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Code will be released at: https://github.com/xuboshen/EgoDTM",
      "pdf_url": "http://arxiv.org/pdf/2503.15470v1",
      "published_date": "2025-03-19 17:45:56 UTC",
      "updated_date": "2025-03-19 17:45:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:55:29.857567"
    },
    {
      "arxiv_id": "2503.15469v3",
      "title": "Dynamic Bi-Elman Attention Networks: A Dual-Directional Context-Aware Test-Time Learning for Text Classification",
      "title_zh": "翻译失败",
      "authors": [
        "ZhengLin Lai",
        "MengYao Liao",
        "Dong Xu"
      ],
      "abstract": "Text classification, a fundamental task in natural language processing, aims\nto categorize textual data into predefined labels. Traditional methods\nstruggled with complex linguistic structures and semantic dependencies.\nHowever, the advent of deep learning, particularly recurrent neural networks\nand Transformer-based models, has significantly advanced the field by enabling\nnuanced feature extraction and context-aware predictions. Despite these\nimprovements, existing models still exhibit limitations in balancing\ninterpretability, computational efficiency, and long-range contextual\nunderstanding. To address these challenges, this paper proposes the Dynamic\nBidirectional Elman with Attention Network (DBEAN). DBEAN integrates\nbidirectional temporal modeling with self-attention mechanisms. It dynamically\nassigns weights to critical segments of input, improving contextual\nrepresentation while maintaining computational efficiency.",
      "tldr_zh": "本文研究了文本分类任务的挑战，包括处理复杂语言结构和语义依赖的难题，以及现有深度学习模型（如循环神经网络和 Transformer）在可解释性、计算效率和长距离上下文理解方面的局限。为解决这些问题，提出 Dynamic Bidirectional Elman with Attention Network (DBEAN)，该模型整合双向时间建模和自注意力机制，并动态分配权重给输入的关键部分，以提升上下文表示。DBEAN 能够在保持计算效率的同时，改善文本分类的整体性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "11 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.15469v3",
      "published_date": "2025-03-19 17:45:13 UTC",
      "updated_date": "2025-03-27 09:24:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:55:41.209652"
    },
    {
      "arxiv_id": "2503.15463v3",
      "title": "From 1,000,000 Users to Every User: Scaling Up Personalized Preference for User-level Alignment",
      "title_zh": "翻译失败",
      "authors": [
        "Jia-Nan Li",
        "Jian Guan",
        "Songhao Wu",
        "Wei Wu",
        "Rui Yan"
      ],
      "abstract": "Large language models (LLMs) have traditionally been aligned through\none-size-fits-all approaches that assume uniform human preferences,\nfundamentally overlooking the diversity in user values and needs. This paper\nintroduces a comprehensive framework for scalable personalized alignment of\nLLMs. We establish a systematic preference space characterizing psychological\nand behavioral dimensions, alongside diverse persona representations for robust\npreference inference in real-world scenarios. Building upon this foundation, we\nintroduce \\textsc{AlignX}, a large-scale dataset of over 1.3 million\npersonalized preference examples, and develop two complementary alignment\napproaches: \\textit{in-context alignment} directly conditioning on persona\nrepresentations and \\textit{preference-bridged alignment} modeling intermediate\npreference distributions. Extensive experiments demonstrate substantial\nimprovements over existing methods, with an average 17.06\\% accuracy gain\nacross four benchmarks while exhibiting a strong adaptation capability to novel\npreferences, robustness to limited user data, and precise preference\ncontrollability. These results validate our approach toward user-adaptive AI\nsystems.",
      "tldr_zh": "该论文批评了大语言模型(LLMs)的传统对齐方法忽略用户偏好多样性，提出一个可扩展的个性化对齐框架，以适应每个用户的独特需求。框架包括系统化的偏好空间（涵盖心理和行为维度）以及多样的人设表示，用于真实场景的偏好推断，并引入了AlignX数据集（超过130万个性化偏好示例），同时开发了in-context alignment（基于人设直接条件化）和preference-bridged alignment（建模中间偏好分布）两种方法。实验结果显示，该方法在四个基准上平均准确率提高了17.06%，并表现出对新偏好的适应能力、对有限用户数据的鲁棒性和精确的偏好可控性，从而为用户适应性AI系统提供了有效途径。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15463v3",
      "published_date": "2025-03-19 17:41:46 UTC",
      "updated_date": "2025-05-22 16:17:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:55:55.109961"
    },
    {
      "arxiv_id": "2503.15457v1",
      "title": "Di$\\mathtt{[M]}$O: Distilling Masked Diffusion Models into One-step Generator",
      "title_zh": "翻译失败",
      "authors": [
        "Yuanzhi Zhu",
        "Xi Wang",
        "Stéphane Lathuilière",
        "Vicky Kalogeiton"
      ],
      "abstract": "Masked Diffusion Models (MDMs) have emerged as a powerful generative modeling\ntechnique. Despite their remarkable results, they typically suffer from slow\ninference with several steps. In this paper, we propose Di$\\mathtt{[M]}$O, a\nnovel approach that distills masked diffusion models into a one-step generator.\nDi$\\mathtt{[M]}$O addresses two key challenges: (1) the intractability of using\nintermediate-step information for one-step generation, which we solve through\ntoken-level distribution matching that optimizes model output logits by an\n'on-policy framework' with the help of an auxiliary model; and (2) the lack of\nentropy in the initial distribution, which we address through a token\ninitialization strategy that injects randomness while maintaining similarity to\nteacher training distribution. We show Di$\\mathtt{[M]}$O's effectiveness on\nboth class-conditional and text-conditional image generation, impressively\nachieving performance competitive to multi-step teacher outputs while\ndrastically reducing inference time. To our knowledge, we are the first to\nsuccessfully achieve one-step distillation of masked diffusion models and the\nfirst to apply discrete distillation to text-to-image generation, opening new\npaths for efficient generative modeling.",
      "tldr_zh": "本文提出Di$\\mathtt{[M]}$O方法，将Masked Diffusion Models (MDMs)蒸馏成一个步骤的生成器，以解决MDMs推理过程缓慢的问题。Di$\\mathtt{[M]}$O通过token-level distribution matching结合'on-policy framework'和辅助模型优化输出logits，以及token initialization策略注入随机性，同时保持与教师训练分布的相似性，来应对中间步骤信息处理和初始分布熵不足的挑战。在类别条件和文本条件图像生成任务上，Di$\\mathtt{[M]}$O实现了与多步骤教师模型相当的性能，同时大幅减少推理时间，这是首次成功应用于MDMs的一步蒸馏和文本到图像生成。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15457v1",
      "published_date": "2025-03-19 17:36:54 UTC",
      "updated_date": "2025-03-19 17:36:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:56:07.808921"
    },
    {
      "arxiv_id": "2503.15438v1",
      "title": "VenusFactory: A Unified Platform for Protein Engineering Data Retrieval and Language Model Fine-Tuning",
      "title_zh": "VenusFactory：一个统一的平台，用于蛋白质工程数据检索和语言模型微调",
      "authors": [
        "Yang Tan",
        "Chen Liu",
        "Jingyuan Gao",
        "Banghao Wu",
        "Mingchen Li",
        "Ruilin Wang",
        "Lingrong Zhang",
        "Huiqun Yu",
        "Guisheng Fan",
        "Liang Hong",
        "Bingxin Zhou"
      ],
      "abstract": "Natural language processing (NLP) has significantly influenced scientific\ndomains beyond human language, including protein engineering, where pre-trained\nprotein language models (PLMs) have demonstrated remarkable success. However,\ninterdisciplinary adoption remains limited due to challenges in data\ncollection, task benchmarking, and application. This work presents\nVenusFactory, a versatile engine that integrates biological data retrieval,\nstandardized task benchmarking, and modular fine-tuning of PLMs. VenusFactory\nsupports both computer science and biology communities with choices of both a\ncommand-line execution and a Gradio-based no-code interface, integrating $40+$\nprotein-related datasets and $40+$ popular PLMs. All implementations are\nopen-sourced on https://github.com/tyang816/VenusFactory.",
      "tldr_zh": "该研究介绍了 VenusFactory，一个统一的平台，旨在解决蛋白工程领域的数据收集、任务基准测试和应用挑战问题。VenusFactory 集成了生物数据检索、标准化任务基准测试以及模块化蛋白语言模型(PLMs)微调功能，支持命令行和 Gradio-based 无代码界面，并整合了40+蛋白相关数据集和40+流行 PLMs。该平台促进计算机科学和生物学社区的跨学科合作，所有实现已开源在GitHub上。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "q-bio.QM"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages, 1 figure, 8 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.15438v1",
      "published_date": "2025-03-19 17:19:07 UTC",
      "updated_date": "2025-03-19 17:19:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:56:16.967154"
    },
    {
      "arxiv_id": "2503.15436v1",
      "title": "An extensive simulation study evaluating the interaction of resampling techniques across multiple causal discovery contexts",
      "title_zh": "广泛的模拟研究：评估重采样技术在多个因果发现情境中的交互作用",
      "authors": [
        "Ritwick Banerjee",
        "Bryan Andrews",
        "Erich Kummerfeld"
      ],
      "abstract": "Despite the accelerating presence of exploratory causal analysis in modern\nscience and medicine, the available non-experimental methods for validating\ncausal models are not well characterized. One of the most popular methods is to\nevaluate the stability of model features after resampling the data, similar to\nresampling methods for estimating confidence intervals in statistics. Many\naspects of this approach have received little to no attention, however, such as\nwhether the choice of resampling method should depend on the sample size,\nalgorithms being used, or algorithm tuning parameters. We present theoretical\nresults proving that certain resampling methods closely emulate the assignment\nof specific values to algorithm tuning parameters. We also report the results\nof extensive simulation experiments, which verify the theoretical result and\nprovide substantial data to aid researchers in further characterizing\nresampling in the context of causal discovery analysis. Together, the\ntheoretical work and simulation results provide specific guidance on how\nresampling methods and tuning parameters should be selected in practice.",
      "tldr_zh": "本研究通过理论分析和广泛模拟实验，评估了重采样技术（resampling techniques）在多种因果发现情境（causal discovery contexts）下的交互效果，旨在验证因果模型的稳定性。研究证明了某些重采样方法等同于特定算法调参参数的设置，并通过实验数据证实了这一理论结果。该工作为研究者提供了实用指导，帮助他们在实际应用中选择合适的重采样方法和调参参数。",
      "categories": [
        "stat.ME",
        "cs.AI"
      ],
      "primary_category": "stat.ME",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15436v1",
      "published_date": "2025-03-19 17:18:18 UTC",
      "updated_date": "2025-03-19 17:18:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:56:29.822538"
    },
    {
      "arxiv_id": "2503.15426v2",
      "title": "Visual Position Prompt for MLLM based Visual Grounding",
      "title_zh": "翻译失败",
      "authors": [
        "Wei Tang",
        "Yanpeng Sun",
        "Qinying Gu",
        "Zechao Li"
      ],
      "abstract": "Although Multimodal Large Language Models (MLLMs) excel at various\nimage-related tasks, they encounter challenges in precisely aligning\ncoordinates with spatial information within images, particularly in\nposition-aware tasks such as visual grounding. This limitation arises from two\nkey factors. First, MLLMs lack explicit spatial references, making it difficult\nto associate textual descriptions with precise image locations. Second, their\nfeature extraction processes prioritize global context over fine-grained\nspatial details, leading to weak localization capability. To address this\nissue, we introduce VPP-LLaVA, an MLLM equipped with Visual Position Prompt\n(VPP) to improve its grounding capability. VPP-LLaVA integrates two\ncomplementary mechanisms. The global VPP overlays learnable, axis-like\nembeddings onto the input image to provide structured spatial cues. The local\nVPP focuses on fine-grained localization by incorporating position-aware\nqueries, which suggests probable object locations. We also introduce a VPP-SFT\ndataset with 0.6M samples, consolidating high-quality visual grounding data\ninto a compact format for efficient model training. Training on this dataset\nwith VPP enhances the model's performance, achieving state-of-the-art results\non standard grounding benchmarks despite using fewer training samples compared\nto other MLLMs like MiniGPT-v2, which rely on much larger datasets ($\\sim$21M\nsamples). The code and VPP-SFT dataset will be available at\nhttps://github.com/WayneTomas/VPP-LLaVA upon acceptance.",
      "tldr_zh": "Multimodal Large Language Models (MLLMs) 在视觉定位任务中难以精确对齐坐标和空间信息，主要由于缺乏显式空间引用和特征提取偏重全局上下文而非细粒度细节。针对此问题，研究者提出 VPP-LLaVA 模型，该模型引入 Visual Position Prompt (VPP)，包括 Global VPP（叠加可学习的轴状嵌入提供结构化空间提示）和 Local VPP（通过位置感知查询实现细粒度定位）。此外，他们构建了 VPP-SFT 数据集，包含 0.6M 高质量样本，用于高效模型训练。结果显示，VPP-LLaVA 在标准视觉 grounding 基准上取得了 state-of-the-art 性能，尽管训练样本远少于其他 MLLMs 如 MiniGPT-v2（约 21M 样本）。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15426v2",
      "published_date": "2025-03-19 17:08:13 UTC",
      "updated_date": "2025-03-24 16:34:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:56:43.349210"
    },
    {
      "arxiv_id": "2503.15421v1",
      "title": "Probing the topology of the space of tokens with structured prompts",
      "title_zh": "翻译失败",
      "authors": [
        "Michael Robinson",
        "Sourya Dey",
        "Taisa Kushner"
      ],
      "abstract": "This article presents a general and flexible method for prompting a large\nlanguage model (LLM) to reveal its (hidden) token input embedding up to\nhomeomorphism. Moreover, this article provides strong theoretical justification\n-- a mathematical proof for generic LLMs -- for why this method should be\nexpected to work. With this method in hand, we demonstrate its effectiveness by\nrecovering the token subspace of Llemma-7B. The results of this paper apply not\nonly to LLMs but also to general nonlinear autoregressive processes.",
      "tldr_zh": "这篇论文提出了一种通用且灵活的方法，使用结构化提示来揭示大型语言模型(LLM)的隐藏标记输入嵌入(token input embedding)，直到同胚(homeomorphism)。该方法得到了强有力的理论支撑，通过数学证明证明其适用于通用LLM，并在实验中成功恢复了Llemma-7B的标记子空间。结果不仅适用于LLM，还扩展到一般非线性自回归过程，从而为深入理解模型拓扑结构提供了新工具。",
      "categories": [
        "math.DG",
        "cs.AI",
        "53Z50, 58Z05",
        "I.2.7"
      ],
      "primary_category": "math.DG",
      "comment": "20 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.15421v1",
      "published_date": "2025-03-19 17:01:15 UTC",
      "updated_date": "2025-03-19 17:01:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:56:52.765074"
    },
    {
      "arxiv_id": "2503.15417v1",
      "title": "Temporal Regularization Makes Your Video Generator Stronger",
      "title_zh": "翻译失败",
      "authors": [
        "Harold Haodong Chen",
        "Haojian Huang",
        "Xianfeng Wu",
        "Yexin Liu",
        "Yajing Bai",
        "Wen-Jie Shu",
        "Harry Yang",
        "Ser-Nam Lim"
      ],
      "abstract": "Temporal quality is a critical aspect of video generation, as it ensures\nconsistent motion and realistic dynamics across frames. However, achieving high\ntemporal coherence and diversity remains challenging. In this work, we explore\ntemporal augmentation in video generation for the first time, and introduce\nFluxFlow for initial investigation, a strategy designed to enhance temporal\nquality. Operating at the data level, FluxFlow applies controlled temporal\nperturbations without requiring architectural modifications. Extensive\nexperiments on UCF-101 and VBench benchmarks demonstrate that FluxFlow\nsignificantly improves temporal coherence and diversity across various video\ngeneration models, including U-Net, DiT, and AR-based architectures, while\npreserving spatial fidelity. These findings highlight the potential of temporal\naugmentation as a simple yet effective approach to advancing video generation\nquality.",
      "tldr_zh": "本研究强调了视频生成中的时间质量（temporal quality）的重要性，并首次探索了时间增强（temporal augmentation）作为提升策略。论文引入 FluxFlow，这是一种在数据级别应用受控时间扰动（temporal perturbations）的简单方法，无需修改模型架构。实验在 UCF-101 和 VBench 基准上证明，FluxFlow 显著提高了 U-Net、DiT 和 AR-based architectures 等各种视频生成模型的时间连贯性和多样性，同时保持了空间保真度（spatial fidelity）。这些发现表明，时间增强是一种高效的途径，可增强视频生成器的整体性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project: https://haroldchen19.github.io/FluxFlow/",
      "pdf_url": "http://arxiv.org/pdf/2503.15417v1",
      "published_date": "2025-03-19 16:59:32 UTC",
      "updated_date": "2025-03-19 16:59:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:57:05.467146"
    },
    {
      "arxiv_id": "2503.15415v1",
      "title": "Automated Processing of eXplainable Artificial Intelligence Outputs in Deep Learning Models for Fault Diagnostics of Large Infrastructures",
      "title_zh": "翻译失败",
      "authors": [
        "Giovanni Floreale",
        "Piero Baraldi",
        "Enrico Zio",
        "Olga Fink"
      ],
      "abstract": "Deep Learning (DL) models processing images to recognize the health state of\nlarge infrastructure components can exhibit biases and rely on non-causal\nshortcuts. eXplainable Artificial Intelligence (XAI) can address these issues\nbut manually analyzing explanations generated by XAI techniques is\ntime-consuming and prone to errors. This work proposes a novel framework that\ncombines post-hoc explanations with semi-supervised learning to automatically\nidentify anomalous explanations that deviate from those of correctly classified\nimages and may therefore indicate model abnormal behaviors. This significantly\nreduces the workload for maintenance decision-makers, who only need to manually\nreclassify images flagged as having anomalous explanations. The proposed\nframework is applied to drone-collected images of insulator shells for power\ngrid infrastructure monitoring, considering two different Convolutional Neural\nNetworks (CNNs), GradCAM explanations and Deep Semi-Supervised Anomaly\nDetection. The average classification accuracy on two faulty classes is\nimproved by 8% and maintenance operators are required to manually reclassify\nonly 15% of the images. We compare the proposed framework with a\nstate-of-the-art approach based on the faithfulness metric: the experimental\nresults obtained demonstrate that the proposed framework consistently achieves\nF_1 scores larger than those of the faithfulness-based approach. Additionally,\nthe proposed framework successfully identifies correct classifications that\nresult from non-causal shortcuts, such as the presence of ID tags printed on\ninsulator shells.",
      "tldr_zh": "该论文提出一个新框架，用于自动化处理 eXplainable Artificial Intelligence (XAI) 输出，以解决 Deep Learning (DL) 模型在大型基础设施故障诊断中的偏差和非因果捷径问题。框架结合 post-hoc explanations 和 semi-supervised learning，自动识别异常解释，这些解释偏离正确分类图像的模式，从而减少维护决策者的工作量，仅需手动重新分类标记图像。应用于无人机收集的电力网格绝缘壳图像上，该框架使用两种 Convolutional Neural Networks (CNNs)、GradCAM 解释和 Deep Semi-Supervised Anomaly Detection，提高了两个故障类别的平均分类准确率 8%，并使手动重新分类比例降至 15%，同时在 F_1 scores 上优于基于 faithfulness metric 的现有方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15415v1",
      "published_date": "2025-03-19 16:57:00 UTC",
      "updated_date": "2025-03-19 16:57:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:57:19.442406"
    },
    {
      "arxiv_id": "2503.15402v1",
      "title": "Towards efficient keyword spotting using spike-based time difference encoders",
      "title_zh": "翻译失败",
      "authors": [
        "Alejandro Pequeño-Zurro",
        "Lyes Khacef",
        "Stefano Panzeri",
        "Elisabetta Chicca"
      ],
      "abstract": "Keyword spotting in edge devices is becoming increasingly important as\nvoice-activated assistants are widely used. However, its deployment is often\nlimited by the extreme low-power constraints of the target embedded systems.\nHere, we explore the Temporal Difference Encoder (TDE) performance in keyword\nspotting. This recent neuron model encodes the time difference in instantaneous\nfrequency and spike count to perform efficient keyword spotting with\nneuromorphic processors. We use the TIdigits dataset of spoken digits with a\nformant decomposition and rate-based encoding into spikes. We compare three\nSpiking Neural Networks (SNNs) architectures to learn and classify\nspatio-temporal signals. The proposed SNN architectures are made of three\nlayers with variation in its hidden layer composed of either (1) feedforward\nTDE, (2) feedforward Current-Based Leaky Integrate-and-Fire (CuBa-LIF), or (3)\nrecurrent CuBa-LIF neurons. We first show that the spike trains of the\nfrequency-converted spoken digits have a large amount of information in the\ntemporal domain, reinforcing the importance of better exploiting temporal\nencoding for such a task. We then train the three SNNs with the same number of\nsynaptic weights to quantify and compare their performance based on the\naccuracy and synaptic operations. The resulting accuracy of the feedforward TDE\nnetwork (89%) is higher than the feedforward CuBa-LIF network (71%) and close\nto the recurrent CuBa-LIF network (91%). However, the feedforward TDE-based\nnetwork performs 92% fewer synaptic operations than the recurrent CuBa-LIF\nnetwork with the same amount of synapses. In addition, the results of the TDE\nnetwork are highly interpretable and correlated with the frequency and\ntimescale features of the spoken keywords in the dataset. Our findings suggest\nthat the TDE is a promising neuron model for scalable event-driven processing\nof spatio-temporal patterns.",
      "tldr_zh": "该研究探讨了使用基于 Temporal Difference Encoder (TDE) 的尖峰编码来实现高效关键词检测，针对边缘设备低功耗需求。研究者利用 TIdigits 数据集，将语音信号通过频率分解和基于尖峰的速率编码，比较了三种 Spiking Neural Networks (SNNs) 架构，包括基于 TDE 的前馈网络、基于 CuBa-LIF 的前馈网络和基于 CuBa-LIF 的循环网络。结果显示，TDE 前馈网络的准确率达 89%，高于 CuBa-LIF 前馈网络（71%），并接近循环网络（91%），同时在相同突触权重下减少了 92% 的突触操作。总体而言，该方法证明 TDE 是处理时空模式的可扩展事件驱动神经元模型，具有良好的可解释性和实际应用潜力。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.CV",
        "cs.ET"
      ],
      "primary_category": "cs.NE",
      "comment": "26 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.15402v1",
      "published_date": "2025-03-19 16:43:35 UTC",
      "updated_date": "2025-03-19 16:43:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:57:28.940056"
    },
    {
      "arxiv_id": "2503.15386v1",
      "title": "CCDP: Composition of Conditional Diffusion Policies with Guided Sampling",
      "title_zh": "翻译失败",
      "authors": [
        "Amirreza Razmjoo",
        "Sylvain Calinon",
        "Michael Gienger",
        "Fan Zhang"
      ],
      "abstract": "Imitation Learning offers a promising approach to learn directly from data\nwithout requiring explicit models, simulations, or detailed task definitions.\nDuring inference, actions are sampled from the learned distribution and\nexecuted on the robot. However, sampled actions may fail for various reasons,\nand simply repeating the sampling step until a successful action is obtained\ncan be inefficient. In this work, we propose an enhanced sampling strategy that\nrefines the sampling distribution to avoid previously unsuccessful actions. We\ndemonstrate that by solely utilizing data from successful demonstrations, our\nmethod can infer recovery actions without the need for additional exploratory\nbehavior or a high-level controller. Furthermore, we leverage the concept of\ndiffusion model decomposition to break down the primary problem (which may\nrequire long-horizon history to manage failures) into multiple smaller, more\nmanageable sub-problems in learning, data collection, and inference, thereby\nenabling the system to adapt to variable failure counts. Our approach yields a\nlow-level controller that dynamically adjusts its sampling space to improve\nefficiency when prior samples fall short. We validate our method across several\ntasks, including door opening with unknown directions, object manipulation, and\nbutton-searching scenarios, demonstrating that our approach outperforms\ntraditional baselines.",
      "tldr_zh": "本研究提出CCDP（Composition of Conditional Diffusion Policies with Guided Sampling），一种改进的模仿学习（Imitation Learning）框架，通过引导采样策略来精炼动作分布，避免先前失败的动作，从而提升机器人执行效率。该方法仅利用成功演示数据，便能推断恢复动作，无需额外探索行为或高层控制器，并借助扩散模型分解（diffusion model decomposition）将复杂问题分解为更易管理的子问题，以适应可变失败次数。实验在开门（未知方向）、物体操作和按钮搜索等任务上验证了CCDP的表现，优于传统基线，显著提高了采样效率和系统适应性。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15386v1",
      "published_date": "2025-03-19 16:24:55 UTC",
      "updated_date": "2025-03-19 16:24:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:57:41.106349"
    },
    {
      "arxiv_id": "2503.15374v1",
      "title": "Real-world validation of a multimodal LLM-powered pipeline for High-Accuracy Clinical Trial Patient Matching leveraging EHR data",
      "title_zh": "翻译失败",
      "authors": [
        "Anatole Callies",
        "Quentin Bodinier",
        "Philippe Ravaud",
        "Kourosh Davarpanah"
      ],
      "abstract": "Background: Patient recruitment in clinical trials is hindered by complex\neligibility criteria and labor-intensive chart reviews. Prior research using\ntext-only models have struggled to address this problem in a reliable and\nscalable way due to (1) limited reasoning capabilities, (2) information loss\nfrom converting visual records to text, and (3) lack of a generic EHR\nintegration to extract patient data.\n  Methods: We introduce a broadly applicable, integration-free, LLM-powered\npipeline that automates patient-trial matching using unprocessed documents\nextracted from EHRs. Our approach leverages (1) the new reasoning-LLM paradigm,\nenabling the assessment of even the most complex criteria, (2) visual\ncapabilities of latest LLMs to interpret medical records without lossy\nimage-to-text conversions, and (3) multimodal embeddings for efficient medical\nrecord search. The pipeline was validated on the n2c2 2018 cohort selection\ndataset (288 diabetic patients) and a real-world dataset composed of 485\npatients from 30 different sites matched against 36 diverse trials.\n  Results: On the n2c2 dataset, our method achieved a new state-of-the-art\ncriterion-level accuracy of 93\\%. In real-world trials, the pipeline yielded an\naccuracy of 87\\%, undermined by the difficulty to replicate human\ndecision-making when medical records lack sufficient information. Nevertheless,\nusers were able to review overall eligibility in under 9 minutes per patient on\naverage, representing an 80\\% improvement over traditional manual chart\nreviews.\n  Conclusion: This pipeline demonstrates robust performance in clinical trial\npatient matching without requiring custom integration with site systems or\ntrial-specific tailoring, thereby enabling scalable deployment across sites\nseeking to leverage AI for patient matching.",
      "tldr_zh": "这篇论文验证了一个基于多模态 LLM 的管道，用于高准确度临床试验患者匹配，通过利用 EHR 数据来自动化匹配过程。该管道采用 reasoning-LLM 范式、视觉能力及多模态嵌入，解决了传统文本-only 模型的推理限制、信息损失和整合问题，并在 n2c2 2018 数据集（288 名糖尿病患者）和真实世界数据集（485 名患者、36 个试验）上进行验证。结果显示，该方法在 n2c2 数据集上达到 93% 的标准级准确率，在真实世界场景中达到 87%，并将用户审查时间平均缩短至 9 分钟以下，提高了 80% 的效率。该管道无需自定义系统整合或试验特定调整，从而实现了可扩展的跨站点部署，促进 AI 在患者招募中的应用。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15374v1",
      "published_date": "2025-03-19 16:12:11 UTC",
      "updated_date": "2025-03-19 16:12:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:57:57.777308"
    },
    {
      "arxiv_id": "2503.15354v1",
      "title": "Optimizing Decomposition for Optimal Claim Verification",
      "title_zh": "针对最优声明验证的分解优化",
      "authors": [
        "Yining Lu",
        "Noah Ziems",
        "Hy Dang",
        "Meng Jiang"
      ],
      "abstract": "Current research on the \\textit{Decompose-Then-Verify} paradigm for\nevaluating the factuality of long-form text typically treats decomposition and\nverification in isolation, overlooking their interactions and potential\nmisalignment. We find that existing decomposition policies, typically\nhand-crafted demonstrations, do not align well with downstream verifiers in\nterms of atomicity -- a novel metric quantifying information density -- leading\nto suboptimal verification results. We formulate finding the optimal\ndecomposition policy for optimal verification as a bilevel optimization\nproblem. To approximate a solution for this strongly NP-hard problem, we\npropose dynamic decomposition, a reinforcement learning framework that\nleverages verifier feedback to learn a policy for dynamically decomposing\nclaims to verifier-preferred atomicity. Experimental results show that dynamic\ndecomposition outperforms existing decomposition policies, improving\nverification confidence by 0.07 and accuracy by 0.12 (on a 0-1 scale) on\naverage across varying verifiers, datasets, and atomcities of input claims.",
      "tldr_zh": "该研究针对Decompose-Then-Verify范式中分解和验证过程的互动不协调问题，引入atomicity指标来量化信息密度，并发现现有手工分解策略无法与验证器对齐。论文将优化分解策略表述为双层优化问题，并提出动态分解框架，这是一个基于强化学习的系统，利用验证器反馈动态调整声明的分解粒度。实验结果显示，该框架在不同验证器、数据集和输入声明的atomicity上，平均提高了验证置信度0.07和准确率0.12。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15354v1",
      "published_date": "2025-03-19 15:56:21 UTC",
      "updated_date": "2025-03-19 15:56:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:58:05.116408"
    },
    {
      "arxiv_id": "2503.15352v1",
      "title": "Leveraging Perfect Multimodal Alignment and Gaussian Assumptions for Cross-modal Transfer",
      "title_zh": "翻译失败",
      "authors": [
        "Abhi Kamboj",
        "Minh N. Do"
      ],
      "abstract": "Multimodal alignment aims to construct a joint latent vector space where two\nmodalities representing the same concept map to the same vector. We formulate\nthis as an inverse problem and show that under certain conditions perfect\nalignment can be achieved. We then address a specific application of alignment\nreferred to as cross-modal transfer. Unsupervised cross-modal transfer aims to\nleverage a model trained with one modality to perform inference on another\nmodality, without any labeled fine-tuning on the new modality. Assuming that\nsemantic classes are represented as a mixture of Gaussians in the latent space,\nwe show how cross-modal transfer can be performed by projecting the data points\nfrom the representation space onto different subspaces representing each\nmodality. Our experiments on synthetic multimodal Gaussian data verify the\neffectiveness of our perfect alignment and cross-modal transfer method. We hope\nthese findings inspire further exploration of the applications of perfect\nalignment and the use of Gaussian models for cross-modal learning.",
      "tldr_zh": "本研究将多模态对齐（multimodal alignment）表述为一个逆问题，并在特定条件下证明可以实现完美对齐，从而构建一个联合潜在空间（latent space），让同一概念的两个模态映射到相同的向量。针对跨模态转移（cross-modal transfer），论文假设语义类在潜在空间中表示为高斯混合模型（mixture of Gaussians），并提出通过将数据点投影到不同子空间的方法，实现无监督转移，而无需在新模态上进行标记微调。实验在合成多模态高斯数据上验证了该方法的有效性，并期望这些发现激发更多对完美对齐和高斯模型在跨模态学习中的应用探索。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15352v1",
      "published_date": "2025-03-19 15:51:17 UTC",
      "updated_date": "2025-03-19 15:51:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:58:16.690024"
    },
    {
      "arxiv_id": "2503.15342v1",
      "title": "TruthLens:A Training-Free Paradigm for DeepFake Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Ritabrata Chakraborty",
        "Rajatsubhra Chakraborty",
        "Ali Khaleghi Rahimian",
        "Thomas MacDougall"
      ],
      "abstract": "The proliferation of synthetic images generated by advanced AI models poses\nsignificant challenges in identifying and understanding manipulated visual\ncontent. Current fake image detection methods predominantly rely on binary\nclassification models that focus on accuracy while often neglecting\ninterpretability, leaving users without clear insights into why an image is\ndeemed real or fake. To bridge this gap, we introduce TruthLens, a novel\ntraining-free framework that reimagines deepfake detection as a visual\nquestion-answering (VQA) task. TruthLens utilizes state-of-the-art large\nvision-language models (LVLMs) to observe and describe visual artifacts and\ncombines this with the reasoning capabilities of large language models (LLMs)\nlike GPT-4 to analyze and aggregate evidence into informed decisions. By\nadopting a multimodal approach, TruthLens seamlessly integrates visual and\nsemantic reasoning to not only classify images as real or fake but also provide\ninterpretable explanations for its decisions. This transparency enhances trust\nand provides valuable insights into the artifacts that signal synthetic\ncontent. Extensive evaluations demonstrate that TruthLens outperforms\nconventional methods, achieving high accuracy on challenging datasets while\nmaintaining a strong emphasis on explainability. By reframing deepfake\ndetection as a reasoning-driven process, TruthLens establishes a new paradigm\nin combating synthetic media, combining cutting-edge performance with\ninterpretability to address the growing threats of visual disinformation.",
      "tldr_zh": "本研究提出 TruthLens，一种无需训练的框架，将 DeepFake 检测重新定义为视觉问答 (VQA) 任务，以解决现有二元分类方法准确性高但可解释性不足的问题。\nTruthLens 利用大型视觉语言模型 (LVLMs) 来观察和描述图像中的视觉伪造痕迹，并结合大型语言模型 (LLMs) 如 GPT-4 进行证据分析和决策，提供透明的解释。\n实验结果显示，TruthLens 在 challenging 数据集上实现高准确率，并显著提升了可解释性，建立了一个基于推理的新范式来对抗视觉虚假信息。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15342v1",
      "published_date": "2025-03-19 15:41:32 UTC",
      "updated_date": "2025-03-19 15:41:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:58:29.839431"
    },
    {
      "arxiv_id": "2503.15275v2",
      "title": "Challenges and Trends in Egocentric Vision: A Survey",
      "title_zh": "第一人称视觉中的挑战和趋势：一项调查",
      "authors": [
        "Xiang Li",
        "Heqian Qiu",
        "Lanxiao Wang",
        "Hanwen Zhang",
        "Chenghao Qi",
        "Linfeng Han",
        "Huiyu Xiong",
        "Hongliang Li"
      ],
      "abstract": "With the rapid development of artificial intelligence technologies and\nwearable devices, egocentric vision understanding has emerged as a new and\nchallenging research direction, gradually attracting widespread attention from\nboth academia and industry. Egocentric vision captures visual and multimodal\ndata through cameras or sensors worn on the human body, offering a unique\nperspective that simulates human visual experiences. This paper provides a\ncomprehensive survey of the research on egocentric vision understanding,\nsystematically analyzing the components of egocentric scenes and categorizing\nthe tasks into four main areas: subject understanding, object understanding,\nenvironment understanding, and hybrid understanding. We explore in detail the\nsub-tasks within each category. We also summarize the main challenges and\ntrends currently existing in the field. Furthermore, this paper presents an\noverview of high-quality egocentric vision datasets, offering valuable\nresources for future research. By summarizing the latest advancements, we\nanticipate the broad applications of egocentric vision technologies in fields\nsuch as augmented reality, virtual reality, and embodied intelligence, and\npropose future research directions based on the latest developments in the\nfield.",
      "tldr_zh": "这篇论文对 egocentric vision（第一人称视角视觉）进行了全面调查，分析了其场景组成部分并将相关任务分为四个主要领域：主体理解、物体理解、环境理解和混合理解，同时详细探讨了每个领域的子任务。论文总结了当前领域的关键挑战和趋势，如数据采集的复杂性和多模态融合问题，并概述了高质量数据集以支持未来研究。最终，它展望了 egocentric vision 在增强现实、虚拟现实和具身智能等领域的广泛应用，并提出未来研究方向，以推动该领域的创新发展。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15275v2",
      "published_date": "2025-03-19 14:51:27 UTC",
      "updated_date": "2025-04-03 08:06:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:58:40.560956"
    },
    {
      "arxiv_id": "2503.15580v1",
      "title": "How Well Can AI Build SD Models?",
      "title_zh": "翻译失败",
      "authors": [
        "William Schoenberg",
        "Davidson Girard",
        "Saras Chung",
        "Ellen O'Neill",
        "Janet Velasquez",
        "Sara Metcalf"
      ],
      "abstract": "Introduction: As system dynamics (SD) embraces automation, AI offers\nefficiency but risks bias from missing data and flawed models. Models that omit\nmultiple perspectives and data threaten model quality, whether created by\nhumans or with the assistance of AI. To reduce uncertainty about how well AI\ncan build SD models, we introduce two metrics for evaluation of AI-generated\ncausal maps: technical correctness (causal translation) and adherence to\ninstructions (conformance).\n  Approach: We developed an open source project called sd-ai to provide a basis\nfor collaboration in the SD community, aiming to fully harness the potential of\nAI based tools like ChatGPT for dynamic modeling. Additionally, we created an\nevaluation theory along with a comprehensive suite of tests designed to\nevaluate any such tools developed within the sd-ai ecosystem.\n  Results: We tested 11 different LLMs on their ability to do causal\ntranslation as well as conform to user instruction. gpt-4.5-preview was the top\nperformer, scoring 92.9% overall, excelling in both tasks. o1 scored 100% in\ncausal translation. gpt-4o identified all causal links but struggled with\npositive polarity in decreasing terms. While gpt-4.5-preview and o1 are most\naccurate, gpt-4o is the cheapest.\n  Discussion: Causal translation and conformance tests applied to the sd-ai\nengine reveal significant variations across lLLMs, underscoring the need for\ncontinued evaluation to ensure responsible development of AI tools for dynamic\nmodeling. To address this, an open collaboration among tool developers,\nmodelers, and stakeholders is launched to standardize measures for evaluating\nthe capacity of AI tools to improve the modeling process.",
      "tldr_zh": "本研究探讨了 AI 在构建系统动态 (SD) 模型方面的效能，强调 AI 可能带来的效率与风险，如数据缺失和模型偏差，并引入两个评估指标：technical correctness (causal translation) 和 adherence to instructions (conformance)。研究团队开发了开源项目 sd-ai，利用工具如 ChatGPT 测试了 11 个 LLMs 的性能。结果显示，gpt-4.5-preview 整体得分最高达 92.9%，o1 在 causal translation 中达到 100%，而 gpt-4o 虽准确但成本最低。讨论部分呼吁社区合作标准化评估措施，以推动 AI 工具在动态建模中的负责任发展。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15580v1",
      "published_date": "2025-03-19 14:48:47 UTC",
      "updated_date": "2025-03-19 14:48:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:58:53.170164"
    },
    {
      "arxiv_id": "2503.15272v1",
      "title": "MAMM-Refine: A Recipe for Improving Faithfulness in Generation with Multi-Agent Collaboration",
      "title_zh": "翻译失败",
      "authors": [
        "David Wan",
        "Justin Chih-Yao Chen",
        "Elias Stengel-Eskin",
        "Mohit Bansal"
      ],
      "abstract": "Multi-agent collaboration among models has shown promise in reasoning tasks\nbut is underexplored in long-form generation tasks like summarization and\nquestion-answering. We extend multi-agent multi-model reasoning to generation,\nspecifically to improving faithfulness through refinement, i.e., revising\nmodel-generated outputs to remove factual inconsistencies. We investigate how\niterative collaboration among multiple instances and types of large language\nmodels (LLMs) enhances subtasks in the refinement process, such as error\ndetection, critiquing unfaithful sentences, and making corrections based on\ncritiques. We design intrinsic evaluations for each subtask, with our findings\nindicating that both multi-agent (multiple instances) and multi-model (diverse\nLLM types) approaches benefit error detection and critiquing. Additionally,\nreframing critiquing and refinement as reranking rather than generation tasks\nimproves multi-agent performance. We consolidate these insights into a final\n\"recipe\" called Multi-Agent Multi-Model Refinement (MAMM-Refine), where\nmulti-agent and multi-model collaboration significantly boosts performance on\nthree summarization datasets as well as on long-form question answering,\ndemonstrating the effectiveness and generalizability of our recipe.",
      "tldr_zh": "该研究提出MAMM-Refine配方，通过多智能体协作（Multi-Agent Collaboration）改进生成任务的忠实度（Faithfulness），如摘要和问答，具体涉及错误检测、批评不忠实句子以及基于批评进行修正。实验发现，多智能体（多个实例）和多模型（不同LLM类型）方法显著提升错误检测和批评性能，而将批评和精炼任务重新framing为reranking而非生成任务进一步优化了多智能体效果。最终，MAMM-Refine在三个摘要数据集和长形式问答任务上表现出色，证明了其有效性和泛化能力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "NAACL 2025, 18 pages. Code:\n  https://github.com/meetdavidwan/mammrefine",
      "pdf_url": "http://arxiv.org/pdf/2503.15272v1",
      "published_date": "2025-03-19 14:46:53 UTC",
      "updated_date": "2025-03-19 14:46:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:59:05.033463"
    },
    {
      "arxiv_id": "2503.15268v1",
      "title": "Do Chains-of-Thoughts of Large Language Models Suffer from Hallucinations, Cognitive Biases, or Phobias in Bayesian Reasoning?",
      "title_zh": "翻译失败",
      "authors": [
        "Roberto Araya"
      ],
      "abstract": "Learning to reason and carefully explain arguments is central to students'\ncognitive, mathematical, and computational thinking development. This is\nparticularly challenging in problems under uncertainty and in Bayesian\nreasoning. With the new generation of large language models (LLMs) capable of\nreasoning using Chain-of-Thought (CoT), there is an excellent opportunity to\nlearn with them as they explain their reasoning through a dialogue with their\nartificial internal voice. It is an engaging and excellent opportunity to learn\nBayesian reasoning. Furthermore, given that different LLMs sometimes arrive at\nopposite solutions, CoT generates opportunities for deep learning by detailed\ncomparisons of reasonings. However, unlike humans, we found that they do not\nautonomously explain using ecologically valid strategies like natural\nfrequencies, whole objects, and embodied heuristics. This is unfortunate, as\nthese strategies help humans avoid critical mistakes and have proven\npedagogical value in Bayesian reasoning. In order to overcome these biases and\naid understanding and learning, we included prompts that induce LLMs to use\nthese strategies. We found that LLMs with CoT incorporate them but not\nconsistently. They show persistent biases towards symbolic reasoning and\navoidance or phobia of ecologically valid strategies.",
      "tldr_zh": "这篇论文探讨了大型语言模型 (LLMs) 在使用 Chain-of-Thought (CoT) 进行贝叶斯推理时，是否会存在幻觉、认知偏差或恐惧。研究发现，LLMs 倾向于偏好符号推理，而非人类常用的生态有效策略（如自然频率、整体对象和具身启发式），导致它们无法一致避免错误。作者通过提示诱导 LLMs 采用这些策略，但结果显示这种采用并不稳定，揭示了 LLMs 在不确定性问题中的局限性，并为改进其推理能力提供了潜在方向。",
      "categories": [
        "cs.AI",
        "I.2.0"
      ],
      "primary_category": "cs.AI",
      "comment": "24 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.15268v1",
      "published_date": "2025-03-19 14:44:02 UTC",
      "updated_date": "2025-03-19 14:44:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:59:17.306120"
    },
    {
      "arxiv_id": "2503.15248v1",
      "title": "Automated Non-Functional Requirements Generation in Software Engineering with Large Language Models: A Comparative Study",
      "title_zh": "翻译失败",
      "authors": [
        "Jomar Thomas Almonte",
        "Santhosh Anitha Boominathan",
        "Nathalia Nascimento"
      ],
      "abstract": "Neglecting non-functional requirements (NFRs) early in software development\ncan lead to critical challenges. Despite their importance, NFRs are often\noverlooked or difficult to identify, impacting software quality. To support\nrequirements engineers in eliciting NFRs, we developed a framework that\nleverages Large Language Models (LLMs) to derive quality-driven NFRs from\nfunctional requirements (FRs). Using a custom prompting technique within a\nDeno-based pipeline, the system identifies relevant quality attributes for each\nfunctional requirement and generates corresponding NFRs, aiding systematic\nintegration. A crucial aspect is evaluating the quality and suitability of\nthese generated requirements. Can LLMs produce high-quality NFR suggestions?\nUsing 34 functional requirements - selected as a representative subset of 3,964\nFRs-the LLMs inferred applicable attributes based on the ISO/IEC 25010:2023\nstandard, generating 1,593 NFRs. A horizontal evaluation covered three\ndimensions: NFR validity, applicability of quality attributes, and\nclassification precision. Ten industry software quality evaluators, averaging\n13 years of experience, assessed a subset for relevance and quality. The\nevaluation showed strong alignment between LLM-generated NFRs and expert\nassessments, with median validity and applicability scores of 5.0 (means: 4.63\nand 4.59, respectively) on a 1-5 scale. In the classification task, 80.4% of\nLLM-assigned attributes matched expert choices, with 8.3% near misses and 11.3%\nmismatches. A comparative analysis of eight LLMs highlighted variations in\nperformance, with gemini-1.5-pro exhibiting the highest attribute accuracy,\nwhile llama-3.3-70B achieved higher validity and applicability scores. These\nfindings provide insights into the feasibility of using LLMs for automated NFR\ngeneration and lay the foundation for further exploration of AI-assisted\nrequirements engineering.",
      "tldr_zh": "本文研究了利用 Large Language Models (LLMs) 自动生成软件工程中的 Non-Functional Requirements (NFRs) 的框架，以解决早期忽略 NFRs 导致的质量问题。框架通过自定义提示技术在一个 Deno-based pipeline 中，从 Functional Requirements (FRs) 识别基于 ISO/IEC 25010:2023 标准的质量属性，并生成相应 NFRs。实验使用 34 个 FRs 生成 1,593 个 NFRs，结果显示 LLM 生成的 NFRs 与专家评估高度一致，中位数有效性和适用性得分达 5.0，分类准确率达 80.4%。比较八个 LLMs 的性能后，发现 gemini-1.5-pro 在属性准确性最高，而 llama-3.3-70B 在有效性和适用性得分较优，这为 AI 辅助需求工程提供了可行性基础。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "11 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.15248v1",
      "published_date": "2025-03-19 14:23:22 UTC",
      "updated_date": "2025-03-19 14:23:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:59:31.336980"
    },
    {
      "arxiv_id": "2503.15242v2",
      "title": "BigO(Bench) -- Can LLMs Generate Code with Controlled Time and Space Complexity?",
      "title_zh": "翻译失败",
      "authors": [
        "Pierre Chambon",
        "Baptiste Roziere",
        "Benoit Sagot",
        "Gabriel Synnaeve"
      ],
      "abstract": "We introduce BigO(Bench), a novel coding benchmark designed to evaluate the\ncapabilities of generative language models in understanding and generating code\nwith specified time and space complexities. This benchmark addresses the gap in\ncurrent evaluations that often overlook the ability of models to comprehend and\nproduce code constrained by computational complexity. BigO(Bench) includes\ntooling to infer the algorithmic complexity of any Python function from\nprofiling measurements, including human- or LLM-generated solutions.\nBigO(Bench) also includes of set of 3,105 coding problems and 1,190,250\nsolutions from Code Contests annotated with inferred (synthetic) time and space\ncomplexity labels from the complexity framework, as well as corresponding\nruntime and memory footprint values for a large set of input sizes. We present\nresults from evaluating multiple state-of-the-art language models on this\nbenchmark, highlighting their strengths and weaknesses in handling complexity\nrequirements. In particular, token-space reasoning models are unrivaled in code\ngeneration but not in complexity understanding, hinting that they may not\ngeneralize well to tasks for which no reward was given at training time.",
      "tldr_zh": "本论文引入了 BigO(Bench)，一个新型编码基准，用于评估生成语言模型（LLMs）在理解和生成具有指定 time and space complexity 的代码方面的能力。该基准包括工具来从 Python 函数的 profiling 测量中推断算法复杂性，并提供 3,105 个编码问题和 1,190,250 个解决方案的标签，以及对应的运行时和内存占用数据。实验结果显示，state-of-the-art 模型在代码生成上表现出色，但 token-space reasoning 模型在 complexity understanding 上存在弱点，表明它们可能无法泛化到训练时未涉及的任务。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CC"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15242v2",
      "published_date": "2025-03-19 14:19:57 UTC",
      "updated_date": "2025-03-20 17:58:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:59:41.880152"
    },
    {
      "arxiv_id": "2503.15234v1",
      "title": "CoE: Chain-of-Explanation via Automatic Visual Concept Circuit Description and Polysemanticity Quantification",
      "title_zh": "翻译失败",
      "authors": [
        "Wenlong Yu",
        "Qilong Wang",
        "Chuang Liu",
        "Dong Li",
        "Qinghua Hu"
      ],
      "abstract": "Explainability is a critical factor influencing the wide deployment of deep\nvision models (DVMs). Concept-based post-hoc explanation methods can provide\nboth global and local insights into model decisions. However, current methods\nin this field face challenges in that they are inflexible to automatically\nconstruct accurate and sufficient linguistic explanations for global concepts\nand local circuits. Particularly, the intrinsic polysemanticity in semantic\nVisual Concepts (VCs) impedes the interpretability of concepts and DVMs, which\nis underestimated severely. In this paper, we propose a Chain-of-Explanation\n(CoE) approach to address these issues. Specifically, CoE automates the\ndecoding and description of VCs to construct global concept explanation\ndatasets. Further, to alleviate the effect of polysemanticity on model\nexplainability, we design a concept polysemanticity disentanglement and\nfiltering mechanism to distinguish the most contextually relevant concept\natoms. Besides, a Concept Polysemanticity Entropy (CPE), as a measure of model\ninterpretability, is formulated to quantify the degree of concept uncertainty.\nThe modeling of deterministic concepts is upgraded to uncertain concept atom\ndistributions. Finally, CoE automatically enables linguistic local explanations\nof the decision-making process of DVMs by tracing the concept circuit. GPT-4o\nand human-based experiments demonstrate the effectiveness of CPE and the\nsuperiority of CoE, achieving an average absolute improvement of 36% in terms\nof explainability scores.",
      "tldr_zh": "该论文提出Chain-of-Explanation (CoE) 方法，以解决深度视觉模型（DVMs）的解释性问题，特别是视觉概念（VCs）的多义性（polysemanticity）导致的解释不足。CoE 通过自动解码和描述VCs构建全局概念解释数据集，并引入Concept Polysemanticity Entropy (CPE) 来量化概念不确定性，同时设计多义性分离机制以提升本地决策解释的准确性。实验结果显示，CoE 比基线方法在解释性得分上平均绝对提升36%，为DVMs的可解释性提供了有效框架。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by CVPR2025",
      "pdf_url": "http://arxiv.org/pdf/2503.15234v1",
      "published_date": "2025-03-19 14:13:02 UTC",
      "updated_date": "2025-03-19 14:13:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:59:54.226784"
    },
    {
      "arxiv_id": "2503.15235v1",
      "title": "Exploring Large Language Models for Word Games:Who is the Spy?",
      "title_zh": "翻译失败",
      "authors": [
        "Chentian Wei",
        "Jiewei Chen",
        "Jinzhu Xu"
      ],
      "abstract": "Word games hold significant research value for natural language processing\n(NLP), game theory, and related fields due to their rule-based and situational\nnature. This study explores how large language models (LLMs) can be effectively\ninvolved in word games and proposes a training-free framework. \"Shei Shi Wo Di\"\nor \"Who is the Spy\" in English, is a classic word game. Using this game as an\nexample, we introduce a Chain-of-Thought (CoT)-based scheduling framework to\nenable LLMs to achieve excellent performance in tasks such as inferring role\nwords and disguising their identities. We evaluate the framework's performance\nbased on game success rates and the accuracy of the LLM agents' analytical\nresults. Experimental results affirm the framework's effectiveness,\ndemonstrating notable improvements in LLM performance across multiple datasets.\nThis work highlights the potential of LLMs in mastering situational reasoning\nand social interactions within structured game environments. Our code is\npublicly available at https://github.com/ct-wei/Who-is-The-Spy.",
      "tldr_zh": "这篇论文探讨了大型语言模型 (LLMs) 在词语游戏中的应用，特别针对经典游戏 \"Who is the Spy\"，以揭示其在自然语言处理 (NLP) 和博弈论领域的潜力。研究提出一个无训练的 Chain-of-Thought (CoT)-based 调度框架，帮助 LLMs 进行角色词推理和身份伪装任务。实验结果显示，该框架显著提升了 LLMs 的游戏成功率和分析准确率，并在多个数据集上表现出色。该工作强调了 LLMs 在结构化游戏环境中的情境推理和社会互动能力，并提供了开源代码以供进一步研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15235v1",
      "published_date": "2025-03-19 14:13:02 UTC",
      "updated_date": "2025-03-19 14:13:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:00:06.034132"
    },
    {
      "arxiv_id": "2503.15225v1",
      "title": "A Personalized Data-Driven Generative Model of Human Motion",
      "title_zh": "个性化数据驱动的人类运动生成模型",
      "authors": [
        "Angelo Di Porzio",
        "Marco Coraggio"
      ],
      "abstract": "The deployment of autonomous virtual avatars (in extended reality) and robots\nin human group activities - such as rehabilitation therapy, sports, and\nmanufacturing - is expected to increase as these technologies become more\npervasive. Designing cognitive architectures and control strategies to drive\nthese agents requires realistic models of human motion. However, existing\nmodels only provide simplified descriptions of human motor behavior. In this\nwork, we propose a fully data-driven approach, based on Long Short-Term Memory\nneural networks, to generate original motion that captures the unique\ncharacteristics of specific individuals. We validate the architecture using\nreal data of scalar oscillatory motion. Extensive analyses show that our model\neffectively replicates the velocity distribution and amplitude envelopes of the\nindividual it was trained on, remaining different from other individuals, and\noutperforming state-of-the-art models in terms of similarity to human data.",
      "tldr_zh": "该研究提出了一种基于数据驱动的生成模型，用于模拟个性化人类运动，旨在支持虚拟化身和机器人应用于康复治疗、体育和制造等群体活动。模型采用 Long Short-Term Memory (LSTM) 神经网络，从真实数据中学习并生成捕捉特定个体独特特征的原始运动。实验验证显示，该模型在复制训练个体的速度分布和振幅包络方面表现出色，并优于现有模型，在与人类数据的相似性上表现出显著优势。",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.GR",
      "comment": "6 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.15225v1",
      "published_date": "2025-03-19 14:03:20 UTC",
      "updated_date": "2025-03-19 14:03:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:00:16.202177"
    },
    {
      "arxiv_id": "2503.15204v1",
      "title": "When Pigs Get Sick: Multi-Agent AI for Swine Disease Detection",
      "title_zh": "当猪生病时：多智能体 AI 用于猪病检测",
      "authors": [
        "Tittaya Mairittha",
        "Tanakon Sawanglok",
        "Panuwit Raden",
        "Sorrawit Treesuk"
      ],
      "abstract": "Swine disease surveillance is critical to the sustainability of global\nagriculture, yet its effectiveness is frequently undermined by limited\nveterinary resources, delayed identification of cases, and variability in\ndiagnostic accuracy. To overcome these barriers, we introduce a novel\nAI-powered, multi-agent diagnostic system that leverages Retrieval-Augmented\nGeneration (RAG) to deliver timely, evidence-based disease detection and\nclinical guidance. By automatically classifying user inputs into either\nKnowledge Retrieval Queries or Symptom-Based Diagnostic Queries, the system\nensures targeted information retrieval and facilitates precise diagnostic\nreasoning. An adaptive questioning protocol systematically collects relevant\nclinical signs, while a confidence-weighted decision fusion mechanism\nintegrates multiple diagnostic hypotheses to generate robust disease\npredictions and treatment recommendations. Comprehensive evaluations\nencompassing query classification, disease diagnosis, and knowledge retrieval\ndemonstrate that the system achieves high accuracy, rapid response times, and\nconsistent reliability. By providing a scalable, AI-driven diagnostic\nframework, this approach enhances veterinary decision-making, advances\nsustainable livestock management practices, and contributes substantively to\nthe realization of global food security.",
      "tldr_zh": "该研究针对猪病监测面临的资源有限、诊断延迟和准确性问题，提出了一种基于多智能体AI系统的解决方案，利用Retrieval-Augmented Generation (RAG)技术提供及时的证据支持诊断。系统通过自动分类用户输入为Knowledge Retrieval Queries或Symptom-Based Diagnostic Queries，实现针对性信息检索，并结合自适应提问协议和置信度加权的决策融合机制，生成精确的疾病预测和治疗推荐。评估结果显示，该系统在查询分类、疾病诊断和知识检索方面表现出高准确率、快速响应和可靠性，最终提升了兽医决策、可持续畜牧管理和全球食品安全。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.IR",
        "cs.MA"
      ],
      "primary_category": "cs.HC",
      "comment": "14 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.15204v1",
      "published_date": "2025-03-19 13:47:25 UTC",
      "updated_date": "2025-03-19 13:47:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:00:29.084056"
    },
    {
      "arxiv_id": "2503.15202v2",
      "title": "A Unified Framework for Real-Time Failure Handling in Robotics Using Vision-Language Models, Reactive Planner and Behavior Trees",
      "title_zh": "翻译失败",
      "authors": [
        "Faseeh Ahmad",
        "Hashim Ismail",
        "Jonathan Styrud",
        "Maj Stenmark",
        "Volker Krueger"
      ],
      "abstract": "Robotic systems often face execution failures due to unexpected obstacles,\nsensor errors, or environmental changes. Traditional failure recovery methods\nrely on predefined strategies or human intervention, making them less\nadaptable. This paper presents a unified failure recovery framework that\ncombines Vision-Language Models (VLMs), a reactive planner, and Behavior Trees\n(BTs) to enable real-time failure handling. Our approach includes pre-execution\nverification, which checks for potential failures before execution, and\nreactive failure handling, which detects and corrects failures during execution\nby verifying existing BT conditions, adding missing preconditions and, when\nnecessary, generating new skills. The framework uses a scene graph for\nstructured environmental perception and an execution history for continuous\nmonitoring, enabling context-aware and adaptive failure handling. We evaluate\nour framework through real-world experiments with an ABB YuMi robot on tasks\nlike peg insertion, object sorting, and drawer placement, as well as in\nAI2-THOR simulator. Compared to using pre-execution and reactive methods\nseparately, our approach achieves higher task success rates and greater\nadaptability. Ablation studies highlight the importance of VLM-based reasoning,\nstructured scene representation, and execution history tracking for effective\nfailure recovery in robotics.",
      "tldr_zh": "这篇论文提出一个统一的框架，用于机器人系统中实时处理执行失败，结合 Vision-Language Models (VLMs)、reactive planner 和 Behavior Trees (BTs)，以提高适应性和效率。框架包括预执行验证（检查潜在失败）和反应式失败处理（检测并纠正失败，通过验证 BTs 条件、添加缺失前提条件或生成新技能），并利用 scene graph 进行结构化环境感知和 execution history 进行持续监控。实验在 ABB YuMi 机器人上测试了 peg insertion、object sorting 和 drawer placement 等任务，以及 AI2-THOR 模拟器，结果显示该框架比单独使用预执行或反应方法具有更高的任务成功率和适应性；消融研究强调了 VLM-based reasoning、structured scene representation 和 execution history tracking 的关键作用。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15202v2",
      "published_date": "2025-03-19 13:40:56 UTC",
      "updated_date": "2025-03-21 08:10:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:00:42.174093"
    },
    {
      "arxiv_id": "2503.15576v1",
      "title": "A Bird Song Detector for improving bird identification through Deep Learning: a case study from Doñana",
      "title_zh": "基于深度学习的鸟鸣检测器用于改进鸟类识别：Doñana的案例研究",
      "authors": [
        "Alba Márquez-Rodríguez",
        "Miguel Ángel Mohedano-Munoz",
        "Manuel J. Marín-Jiménez",
        "Eduardo Santamaría-García",
        "Giulia Bastianelli",
        "Pedro Jordano",
        "Irene Mendoza"
      ],
      "abstract": "Passive Acoustic Monitoring with automatic recorders is essential for\necosystem conservation but generates vast unsupervised audio data, posing\nchallenges for extracting meaningful information. Deep Learning techniques\noffer a promising solution. BirdNET, a widely used model for bird\nidentification, has shown success in many study systems but is limited in some\nregions due to biases in its training data. A key challenge in bird species\ndetection is that many recordings either lack target species or contain\noverlapping vocalizations. To overcome these problems, we developed a\nmulti-stage pipeline for automatic bird vocalization identification in Do\\~nana\nNational Park (SW Spain), a region facing significant conservation threats. Our\napproach included a Bird Song Detector to isolate vocalizations and custom\nclassifiers trained with BirdNET embeddings. We manually annotated 461 minutes\nof audio from three habitats across nine locations, yielding 3,749 annotations\nfor 34 classes. Spectrograms facilitated the use of image processing\ntechniques. Applying the Bird Song Detector before classification improved\nspecies identification, as all classification models performed better when\nanalyzing only the segments where birds were detected. Specifically, the\ncombination of the Bird Song Detector and fine-tuned BirdNET compared to the\nbaseline without the Bird Song Detector. Our approach demonstrated the\neffectiveness of integrating a Bird Song Detector with fine-tuned\nclassification models for bird identification at local soundscapes. These\nfindings highlight the need to adapt general-purpose tools for specific\necological challenges, as demonstrated in Do\\~nana. Automatically detecting\nbird species serves for tracking the health status of this threatened\necosystem, given the sensitivity of birds to environmental changes, and helps\nin the design of conservation measures for reducing biodiversity loss",
      "tldr_zh": "这篇论文提出了一种基于 Deep Learning 的 Bird Song Detector，用于提升鸟类识别准确率，针对 Doñana 国家公园的生态监测案例研究。该方法通过一个多阶段管道，包括 Bird Song Detector 来隔离鸟类 vocalization，以及结合 BirdNET embeddings 和自定义分类器，处理被动声学监测（Passive Acoustic Monitoring）中大量未监督音频数据的挑战。实验结果显示，使用 Bird Song Detector 后，分类模型的表现显著改善，与基线模型相比，细调后的 BirdNET 在检测鸟类时准确率更高。该方法强调了将通用工具适应特定生态场景的重要性，有助于通过自动鸟类检测追踪生态系统健康并制定保护措施，以应对生物多样性损失。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.NE",
        "I.5.4; I.2.6; I.4.8"
      ],
      "primary_category": "cs.SD",
      "comment": "20 pages, 13 images, for associated dataset see\n  https://huggingface.co/datasets/GrunCrow/BIRDeep_AudioAnnotations , for\n  associated code see\n  https://github.com/GrunCrow/BIRDeep_BirdSongDetector_NeuralNetworks and\n  https://github.com/GrunCrow/Bird-Song-Detector",
      "pdf_url": "http://arxiv.org/pdf/2503.15576v1",
      "published_date": "2025-03-19 13:19:06 UTC",
      "updated_date": "2025-03-19 13:19:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:00:53.241465"
    },
    {
      "arxiv_id": "2503.15185v1",
      "title": "3D Occupancy Prediction with Low-Resolution Queries via Prototype-aware View Transformation",
      "title_zh": "翻译失败",
      "authors": [
        "Gyeongrok Oh",
        "Sungjune Kim",
        "Heeju Ko",
        "Hyung-gun Chi",
        "Jinkyu Kim",
        "Dongwook Lee",
        "Daehyun Ji",
        "Sungjoon Choi",
        "Sujin Jang",
        "Sangpil Kim"
      ],
      "abstract": "The resolution of voxel queries significantly influences the quality of view\ntransformation in camera-based 3D occupancy prediction. However, computational\nconstraints and the practical necessity for real-time deployment require\nsmaller query resolutions, which inevitably leads to an information loss.\nTherefore, it is essential to encode and preserve rich visual details within\nlimited query sizes while ensuring a comprehensive representation of 3D\noccupancy. To this end, we introduce ProtoOcc, a novel occupancy network that\nleverages prototypes of clustered image segments in view transformation to\nenhance low-resolution context. In particular, the mapping of 2D prototypes\nonto 3D voxel queries encodes high-level visual geometries and complements the\nloss of spatial information from reduced query resolutions. Additionally, we\ndesign a multi-perspective decoding strategy to efficiently disentangle the\ndensely compressed visual cues into a high-dimensional 3D occupancy scene.\nExperimental results on both Occ3D and SemanticKITTI benchmarks demonstrate the\neffectiveness of the proposed method, showing clear improvements over the\nbaselines. More importantly, ProtoOcc achieves competitive performance against\nthe baselines even with 75\\% reduced voxel resolution.",
      "tldr_zh": "这篇论文针对基于相机的 3D occupancy prediction 中，低分辨率 voxel queries 导致的信息丢失问题，提出了 ProtoOcc 网络，通过 prototype-aware view transformation 来增强低分辨率上下文。ProtoOcc 将 2D prototypes 映射到 3D voxel queries 中，编码高水平视觉几何，并采用 multi-perspective decoding strategy 来高效解码密集压缩的视觉线索。实验结果显示，在 Occ3D 和 SemanticKITTI 基准上，该方法显著优于基线，即使 voxel resolution 减少 75%，性能仍保持竞争力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to CVPR2025",
      "pdf_url": "http://arxiv.org/pdf/2503.15185v1",
      "published_date": "2025-03-19 13:14:57 UTC",
      "updated_date": "2025-03-19 13:14:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:01:06.030271"
    },
    {
      "arxiv_id": "2503.15182v1",
      "title": "Foundation models may exhibit staged progression in novel CBRN threat disclosure",
      "title_zh": "基础",
      "authors": [
        "Kevin M Esvelt"
      ],
      "abstract": "The extent to which foundation models can disclose novel chemical,\nbiological, radiation, and nuclear (CBRN) threats to expert users is unclear\ndue to a lack of test cases. I leveraged the unique opportunity presented by an\nupcoming publication describing a novel catastrophic biothreat - \"Technical\nReport on Mirror Bacteria: Feasibility and Risks\" - to conduct a small\ncontrolled study before it became public. Graduate-trained biologists tasked\nwith predicting the consequences of releasing mirror E. coli showed no\nsignificant differences in rubric-graded accuracy using Claude Sonnet 3.5 new\n(n=10) or web search only (n=2); both groups scored comparably to a web\nbaseline (28 and 43 versus 36). However, Sonnet reasoned correctly when\nprompted by a report author, but a smaller model, Haiku 3.5, failed even with\nauthor guidance (80 versus 5). These results suggest distinct stages of model\ncapability: Haiku is unable to reason about mirror life even with threat-aware\nexpert guidance (Stage 1), while Sonnet correctly reasons only with\nthreat-aware prompting (Stage 2). Continued advances may allow future models to\ndisclose novel CBRN threats to naive experts (Stage 3) or unskilled users\n(Stage 4). While mirror life represents only one case study, monitoring new\nmodels' ability to reason about privately known threats may allow protective\nmeasures to be implemented before widespread disclosure.",
      "tldr_zh": "本研究探讨了 Foundation models 在披露新型化学、生物、辐射和核（CBRN）威胁时的阶段性进展，通过一个关于 Mirror Bacteria 的案例进行小规模控制实验。实验中，研究生生物学家使用 Claude Sonnet 3.5 new 与仅网络搜索的方法预测镜面 E. coli 释放后果，结果显示两组准确率与网络基线相当，但 Sonnet 在作者提示下能正确推理，而 Haiku 3.5 即使有指导也失败。研究发现，模型能力分阶段：Haiku 处于 Stage 1（无法推理），Sonnet 处于 Stage 2（需威胁意识提示），未来可能达到 Stage 3 或 4，以向非专家用户披露威胁；建议监测新模型以及早实施保护措施。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "q-bio.OT"
      ],
      "primary_category": "cs.CY",
      "comment": "26 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.15182v1",
      "published_date": "2025-03-19 13:08:01 UTC",
      "updated_date": "2025-03-19 13:08:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:01:19.184168"
    },
    {
      "arxiv_id": "2504.02843v1",
      "title": "Learning Distributions of Complex Fluid Simulations with Diffusion Graph Networks",
      "title_zh": "使用扩散图网络学习复杂流体模拟的分布",
      "authors": [
        "Mario Lino",
        "Tobias Pfaff",
        "Nils Thuerey"
      ],
      "abstract": "Physical systems with complex unsteady dynamics, such as fluid flows, are\noften poorly represented by a single mean solution. For many practical\napplications, it is crucial to access the full distribution of possible states,\nfrom which relevant statistics (e.g., RMS and two-point correlations) can be\nderived. Here, we propose a graph-based latent diffusion (or alternatively,\nflow-matching) model that enables direct sampling of states from their\nequilibrium distribution, given a mesh discretization of the system and its\nphysical parameters. This allows for the efficient computation of flow\nstatistics without running long and expensive numerical simulations. The\ngraph-based structure enables operations on unstructured meshes, which is\ncritical for representing complex geometries with spatially localized high\ngradients, while latent-space diffusion modeling with a multi-scale GNN allows\nfor efficient learning and inference of entire distributions of solutions. A\nkey finding is that the proposed networks can accurately learn full\ndistributions even when trained on incomplete data from relatively short\nsimulations. We apply this method to a range of fluid dynamics tasks, such as\npredicting pressure distributions on 3D wing models in turbulent flow,\ndemonstrating both accuracy and computational efficiency in challenging\nscenarios. The ability to directly sample accurate solutions, and capturing\ntheir diversity from short ground-truth simulations, is highly promising for\ncomplex scientific modeling tasks.",
      "tldr_zh": "该论文提出了一种基于图的潜在扩散模型（Diffusion Graph Networks），用于学习复杂流体模拟（如流体流动）的完整状态分布，从而直接从平衡分布采样状态，而无需进行漫长的数值模拟。该方法利用图结构支持非结构化网格，结合多尺度图神经网络（GNN）在潜在空间进行高效学习和推理，适用于复杂几何和局部高梯度场景。关键发现是，即使训练数据来自较短的模拟，该模型也能准确捕捉完整分布，并在任务如预测3D机翼模型在湍流中的压力分布上，展现出显著的准确性和计算效率。",
      "categories": [
        "physics.comp-ph",
        "cs.AI",
        "physics.flu-dyn"
      ],
      "primary_category": "physics.comp-ph",
      "comment": "31 pages, 19 figures, Published as a conference paper at ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.02843v1",
      "published_date": "2025-03-19 13:04:39 UTC",
      "updated_date": "2025-03-19 13:04:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:01:29.403437"
    },
    {
      "arxiv_id": "2503.15172v1",
      "title": "Multi-Agent Actor-Critic with Harmonic Annealing Pruning for Dynamic Spectrum Access Systems",
      "title_zh": "翻译失败",
      "authors": [
        "George Stamatelis",
        "Angelos-Nikolaos Kanatas",
        "George C. Alexandropoulos"
      ],
      "abstract": "Multi-Agent Deep Reinforcement Learning (MADRL) has emerged as a powerful\ntool for optimizing decentralized decision-making systems in complex settings,\nsuch as Dynamic Spectrum Access (DSA). However, deploying deep learning models\non resource-constrained edge devices remains challenging due to their high\ncomputational cost. To address this challenge, in this paper, we present a\nnovel sparse recurrent MARL framework integrating gradual neural network\npruning into the independent actor global critic paradigm. Additionally, we\nintroduce a harmonic annealing sparsity scheduler, which achieves comparable,\nand in certain cases superior, performance to standard linear and polynomial\npruning schedulers at large sparsities. Our experimental investigation\ndemonstrates that the proposed DSA framework can discover superior policies,\nunder diverse training conditions, outperforming conventional DSA, MADRL\nbaselines, and state-of-the-art pruning techniques.",
      "tldr_zh": "本研究针对 Multi-Agent Deep Reinforcement Learning (MADRL) 在 Dynamic Spectrum Access (DSA) 系统中的部署挑战，提出了一种新型稀疏循环 MARL 框架，将渐进式神经网络修剪集成到独立 Actor 全局 Critic 范式中，以降低计算成本。框架中引入 Harmonic Annealing 稀疏调度器，该方法在高稀疏度下表现出色，甚至优于传统的线性或多项式调度器。实验结果显示，该框架在多样化训练条件下发现的策略，优于常规 DSA 方法、MADRL 基准和最先进修剪技术。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NI"
      ],
      "primary_category": "cs.LG",
      "comment": "5 pages, 3 figures, 1 table, submited to an IEEE conference",
      "pdf_url": "http://arxiv.org/pdf/2503.15172v1",
      "published_date": "2025-03-19 12:56:23 UTC",
      "updated_date": "2025-03-19 12:56:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:01:41.726035"
    },
    {
      "arxiv_id": "2503.15169v2",
      "title": "Benchmarking Open-Source Large Language Models on Healthcare Text Classification Tasks",
      "title_zh": "翻译失败",
      "authors": [
        "Yuting Guo",
        "Abeed Sarker"
      ],
      "abstract": "The application of large language models (LLMs) to healthcare information\nextraction has emerged as a promising approach. This study evaluates the\nclassification performance of five open-source LLMs: GEMMA-3-27B-IT,\nLLAMA3-70B, LLAMA4-109B, DEEPSEEK-R1-DISTILL-LLAMA-70B, and\nDEEPSEEK-V3-0324-UD-Q2_K_XL, across six healthcare-related classification tasks\ninvolving both social media data (breast cancer, changes in medication regimen,\nadverse pregnancy outcomes, potential COVID-19 cases) and clinical data (stigma\nlabeling, medication change discussion). We report precision, recall, and F1\nscores with 95% confidence intervals for all model-task combinations. Our\nfindings reveal significant performance variability between LLMs, with\nDeepSeekV3 emerging as the strongest overall performer, achieving the highest\nF1 scores in four tasks. Notably, models generally performed better on social\nmedia tasks compared to clinical data tasks, suggesting potential\ndomain-specific challenges. GEMMA-3-27B-IT demonstrated exceptionally high\nrecall despite its smaller parameter count, while LLAMA4-109B showed\nsurprisingly underwhelming performance compared to its predecessor LLAMA3-70B,\nindicating that larger parameter counts do not guarantee improved\nclassification results. We observed distinct precision-recall trade-offs across\nmodels, with some favoring sensitivity over specificity and vice versa. These\nfindings highlight the importance of task-specific model selection for\nhealthcare applications, considering the particular data domain and\nprecision-recall requirements rather than model size alone. As healthcare\nincreasingly integrates AI-driven text classification tools, this comprehensive\nbenchmarking provides valuable guidance for model selection and implementation\nwhile underscoring the need for continued evaluation and domain adaptation of\nLLMs in healthcare contexts.",
      "tldr_zh": "本研究评估了五个开源大型语言模型（LLMs），包括 GEMMA-3-27B-IT、LLAMA3-70B、LLAMA4-109B、DEEPSEEK-R1-DISTILL-LLAMA-70B 和 DEEPSEEK-V3-0324-UD-Q2_K_XL，在六个医疗文本分类任务（涉及社交媒体数据如乳腺癌和临床数据如污名标签）上的表现，并报告了 precision、recall 和 F1 scores。结果显示，DeepSeekV3 在四个任务中获得最高 F1 分数，模型整体在社交媒体任务上表现优于临床任务，且 GEMMA-3-27B-IT 尽管参数较少却显示出高 recall，而 LLAMA4-109B 的表现不如 LLAMA3-70B，表明模型大小并非关键因素。研究强调了任务特定模型选择的重要性，考虑数据领域和 precision-recall 权衡，为医疗 AI 文本分类工具的实施提供指导，并呼吁进一步的领域适应。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "5 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.15169v2",
      "published_date": "2025-03-19 12:51:52 UTC",
      "updated_date": "2025-05-08 11:58:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:01:55.937749"
    },
    {
      "arxiv_id": "2503.15168v1",
      "title": "World Models in Artificial Intelligence: Sensing, Learning, and Reasoning Like a Child",
      "title_zh": "人工智能中的世界模型",
      "authors": [
        "Javier Del Ser",
        "Jesus L. Lobo",
        "Heimo Müller",
        "Andreas Holzinger"
      ],
      "abstract": "World Models help Artificial Intelligence (AI) predict outcomes, reason about\nits environment, and guide decision-making. While widely used in reinforcement\nlearning, they lack the structured, adaptive representations that even young\nchildren intuitively develop. Advancing beyond pattern recognition requires\ndynamic, interpretable frameworks inspired by Piaget's cognitive development\ntheory. We highlight six key research areas -- physics-informed learning,\nneurosymbolic learning, continual learning, causal inference, human-in-the-loop\nAI, and responsible AI -- as essential for enabling true reasoning in AI. By\nintegrating statistical learning with advances in these areas, AI can evolve\nfrom pattern recognition to genuine understanding, adaptation and reasoning\ncapabilities.",
      "tldr_zh": "该论文探讨了World Models在人工智能(AI)中的应用，强调其在预测结果、环境推理和决策指导方面的作用，但指出现有模型缺乏儿童式的结构化、适应性表示。作者受Piaget's cognitive development theory启发，提出通过整合physics-informed learning、neurosymbolic learning、continual learning、causal inference、human-in-the-loop AI和responsible AI等六个关键研究领域，来构建动态、可解释的框架。最终，该方法旨在使AI从单纯的模式识别演进到具备真正的理解、适应和推理能力。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.ET",
        "cs.LG",
        "68T05"
      ],
      "primary_category": "cs.AI",
      "comment": "11 pages, 1 figure",
      "pdf_url": "http://arxiv.org/pdf/2503.15168v1",
      "published_date": "2025-03-19 12:50:40 UTC",
      "updated_date": "2025-03-19 12:50:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:02:06.138561"
    },
    {
      "arxiv_id": "2503.15167v1",
      "title": "Volumetric Reconstruction From Partial Views for Task-Oriented Grasping",
      "title_zh": "翻译失败",
      "authors": [
        "Fujian Yan",
        "Hui Li",
        "Hongsheng He"
      ],
      "abstract": "Object affordance and volumetric information are essential in devising\neffective grasping strategies under task-specific constraints. This paper\npresents an approach for inferring suitable grasping strategies from limited\npartial views of an object. To achieve this, a recurrent generative adversarial\nnetwork (R-GAN) was proposed by incorporating a recurrent generator with long\nshort-term memory (LSTM) units for it to process a variable number of depth\nscans. To determine object affordances, the AffordPose knowledge dataset is\nutilized as prior knowledge. Affordance retrieving is defined by the volume\nsimilarity measured via Chamfer Distance and action similarities. A Proximal\nPolicy Optimization (PPO) reinforcement learning model is further implemented\nto refine the retrieved grasp strategies for task-oriented grasping. The\nretrieved grasp strategies were evaluated on a dual-arm mobile manipulation\nrobot with an overall grasping accuracy of 89% for four tasks: lift, handle\ngrasp, wrap grasp, and press.",
      "tldr_zh": "本文提出了一种从物体部分视图重建体积信息的方法，用于任务导向抓取策略的推断。方法基于 recurrent generative adversarial network (R-GAN)，结合 long short-term memory (LSTM) 单位处理可变数量的深度扫描，并利用 AffordPose 知识数据集通过 Chamfer Distance 和动作相似性计算对象 affordance。接着，采用 Proximal Policy Optimization (PPO) 强化学习模型优化抓取策略，并在双臂移动操作机器人上测试四种任务（lift, handle grasp, wrap grasp 和 press），整体抓取准确率达到89%。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15167v1",
      "published_date": "2025-03-19 12:47:50 UTC",
      "updated_date": "2025-03-19 12:47:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:02:18.160424"
    },
    {
      "arxiv_id": "2503.15166v2",
      "title": "Machine Unlearning in Hyperbolic vs. Euclidean Multimodal Contrastive Learning: Adapting Alignment Calibration to MERU",
      "title_zh": "翻译失败",
      "authors": [
        "Àlex Pujol Vidal",
        "Sergio Escalera",
        "Kamal Nasrollahi",
        "Thomas B. Moeslund"
      ],
      "abstract": "Machine unlearning methods have become increasingly important for selective\nconcept removal in large pre-trained models. While recent work has explored\nunlearning in Euclidean contrastive vision-language models, the effectiveness\nof concept removal in hyperbolic spaces remains unexplored. This paper\ninvestigates machine unlearning in hyperbolic contrastive learning by adapting\nAlignment Calibration to MERU, a model that embeds images and text in\nhyperbolic space to better capture semantic hierarchies. Through systematic\nexperiments and ablation studies, we demonstrate that hyperbolic geometry\noffers distinct advantages for concept removal, achieving near perfect\nforgetting with reasonable performance on retained concepts, particularly when\nscaling to multiple concept removal. Our approach introduces\nhyperbolic-specific components including entailment calibration and norm\nregularization that leverage the unique properties of hyperbolic space.\nComparative analysis with Euclidean models reveals fundamental differences in\nunlearning dynamics, with hyperbolic unlearning reorganizing the semantic\nhierarchy while Euclidean approaches merely disconnect cross-modal\nassociations. These findings not only advance machine unlearning techniques but\nalso provide insights into the geometric properties that influence concept\nrepresentation and removal in multimodal models. Source code available at\nhttps://github.com/alex-pv01/HAC",
      "tldr_zh": "本文研究了机器遗忘（machine unlearning）在超曲空间（hyperbolic spaces）与欧氏空间（Euclidean）的多模态对比学习中的差异，特别通过将 Alignment Calibration 适应到 MERU 模型，以更好地捕捉语义层次。研究引入了超曲特定的组件，如 entailment calibration 和 norm regularization，利用超曲几何的独特属性，实现概念移除时的近乎完美遗忘，同时保持保留概念的性能，尤其在多个概念移除场景中。实验和消融研究表明，超曲方法能重组语义层次，而欧氏方法仅断开跨模态关联，这为机器遗忘技术提供了新进展，并揭示了几何属性对多模态模型概念表示和移除的影响。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint",
      "pdf_url": "http://arxiv.org/pdf/2503.15166v2",
      "published_date": "2025-03-19 12:47:37 UTC",
      "updated_date": "2025-04-14 08:38:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:02:31.582208"
    },
    {
      "arxiv_id": "2503.15130v1",
      "title": "A Foundational Theory for Decentralized Sensory Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Linus Mårtensson",
        "Jonas M. D. Enander",
        "Udaya B. Rongala",
        "Henrik Jörntell"
      ],
      "abstract": "In both neuroscience and artificial intelligence, popular functional\nframeworks and neural network formulations operate by making use of extrinsic\nerror measurements and global learning algorithms. Through a set of conjectures\nbased on evolutionary insights on the origin of cellular adaptive mechanisms,\nwe reinterpret the core meaning of sensory signals to allow the brain to be\ninterpreted as a negative feedback control system, and show how this could lead\nto local learning algorithms without the need for global error correction\nmetrics. Thereby, a sufficiently good minima in sensory activity can be the\ncomplete reward signal of the network, as well as being both necessary and\nsufficient for biological learning to arise. We show that this method of\nlearning was likely already present in the earliest unicellular life forms on\nearth. We show evidence that the same principle holds and scales to\nmulticellular organisms where it in addition can lead to division of labour\nbetween cells. Available evidence shows that the evolution of the nervous\nsystem likely was an adaptation to more effectively communicate intercellular\nsignals to support such division of labour. We therefore propose that the same\nlearning principle that evolved already in the earliest unicellular life forms,\ni.e. negative feedback control of externally and internally generated sensor\nsignals, has simply been scaled up to become a fundament of the learning we see\nin biological brains today. We illustrate diverse biological settings, from the\nearliest unicellular organisms to humans, where this operational principle\nappears to be a plausible interpretation of the meaning of sensor signals in\nbiology, and how this relates to current neuroscientific theories and findings.",
      "tldr_zh": "这篇论文提出一个基础理论，将大脑视为负反馈控制系统，通过重释感觉信号的含义，实现了去中心化的感觉学习，从而避免依赖全局错误修正指标。作者基于进化洞见提出一组猜想，强调本地学习算法可以以感觉活动的足够最小值作为完整奖励信号，这种机制可能起源于最早的单细胞生物。论文进一步论证，该原则扩展到多细胞生物，促进细胞分工和神经系统的进化，并与当前神经科学理论和发现相一致。",
      "categories": [
        "q-bio.NC",
        "cs.AI"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15130v1",
      "published_date": "2025-03-19 11:44:58 UTC",
      "updated_date": "2025-03-19 11:44:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:02:42.673009"
    },
    {
      "arxiv_id": "2503.15129v1",
      "title": "Aligning Crowd-sourced Human Feedback for Reinforcement Learning on Code Generation by Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Man Fai Wong",
        "Chee Wei Tan"
      ],
      "abstract": "This paper studies how AI-assisted programming and large language models\n(LLM) improve software developers' ability via AI tools (LLM agents) like\nGithub Copilot and Amazon CodeWhisperer, while integrating human feedback to\nenhance reinforcement learning (RLHF) with crowd-sourced computation to enhance\ntext-to-code generation. Additionally, we demonstrate that our Bayesian\noptimization framework supports AI alignment in code generation by distributing\nthe feedback collection burden, highlighting the value of collecting human\nfeedback of good quality. Our empirical evaluations demonstrate the efficacy of\nthis approach, showcasing how LLM agents can be effectively trained for\nimproved text-to-code generation. Our Bayesian optimization framework can be\ndesigned for general domain-specific languages, promoting the alignment of\nlarge language model capabilities with human feedback in AI-assisted\nprogramming for code generation.",
      "tldr_zh": "这篇论文探讨了如何通过整合众包人类反馈来增强大型语言模型(LLM)在代码生成中的强化学习(RLHF)，旨在提升AI辅助编程工具（如Github Copilot和Amazon CodeWhisperer）的性能。作者提出一个Bayesian优化框架，用于分担反馈收集负担、确保反馈质量并实现AI对齐，从而改善文本到代码的生成过程。实证评估证明了该方法的有效性，并展示了其可扩展性到其他领域特定语言，促进LLM能力与人类反馈的更好整合。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15129v1",
      "published_date": "2025-03-19 11:44:47 UTC",
      "updated_date": "2025-03-19 11:44:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:02:54.050021"
    },
    {
      "arxiv_id": "2503.15128v1",
      "title": "Increasing the Robustness of the Fine-tuned Multilingual Machine-Generated Text Detectors",
      "title_zh": "翻译失败",
      "authors": [
        "Dominik Macko",
        "Robert Moro",
        "Ivan Srba"
      ],
      "abstract": "Since the proliferation of LLMs, there have been concerns about their misuse\nfor harmful content creation and spreading. Recent studies justify such fears,\nproviding evidence of LLM vulnerabilities and high potential of their misuse.\nHumans are no longer able to distinguish between high-quality machine-generated\nand authentic human-written texts. Therefore, it is crucial to develop\nautomated means to accurately detect machine-generated content. It would enable\nto identify such content in online information space, thus providing an\nadditional information about its credibility. This work addresses the problem\nby proposing a robust fine-tuning process of LLMs for the detection task,\nmaking the detectors more robust against obfuscation and more generalizable to\nout-of-distribution data.",
      "tldr_zh": "该研究针对大型语言模型（LLMs）的滥用问题，如生成有害内容并难以与人类文本区分，强调了开发自动检测工具的必要性。论文提出了一种鲁棒的微调过程，用于改进多语言机器生成文本检测器，使其更能抵抗混淆（obfuscation）和更好地泛化到分布外数据（out-of-distribution data）。这种方法有助于在在线信息环境中准确识别机器生成内容，从而提升内容的可信度。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15128v1",
      "published_date": "2025-03-19 11:42:33 UTC",
      "updated_date": "2025-03-19 11:42:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:03:06.339485"
    },
    {
      "arxiv_id": "2503.15126v1",
      "title": "Text-Derived Relational Graph-Enhanced Network for Skeleton-Based Action Segmentation",
      "title_zh": "基于文本派生的关系图增强网络，用于骨骼-based动作分割",
      "authors": [
        "Haoyu Ji",
        "Bowen Chen",
        "Weihong Ren",
        "Wenze Huang",
        "Zhihao Yang",
        "Zhiyong Wang",
        "Honghai Liu"
      ],
      "abstract": "Skeleton-based Temporal Action Segmentation (STAS) aims to segment and\nrecognize various actions from long, untrimmed sequences of human skeletal\nmovements. Current STAS methods typically employ spatio-temporal modeling to\nestablish dependencies among joints as well as frames, and utilize one-hot\nencoding with cross-entropy loss for frame-wise classification supervision.\nHowever, these methods overlook the intrinsic correlations among joints and\nactions within skeletal features, leading to a limited understanding of human\nmovements. To address this, we propose a Text-Derived Relational Graph-Enhanced\nNetwork (TRG-Net) that leverages prior graphs generated by Large Language\nModels (LLM) to enhance both modeling and supervision. For modeling, the\nDynamic Spatio-Temporal Fusion Modeling (DSFM) method incorporates Text-Derived\nJoint Graphs (TJG) with channel- and frame-level dynamic adaptation to\neffectively model spatial relations, while integrating spatio-temporal core\nfeatures during temporal modeling. For supervision, the Absolute-Relative\nInter-Class Supervision (ARIS) method employs contrastive learning between\naction features and text embeddings to regularize the absolute class\ndistributions, and utilizes Text-Derived Action Graphs (TAG) to capture the\nrelative inter-class relationships among action features. Additionally, we\npropose a Spatial-Aware Enhancement Processing (SAEP) method, which\nincorporates random joint occlusion and axial rotation to enhance spatial\ngeneralization. Performance evaluations on four public datasets demonstrate\nthat TRG-Net achieves state-of-the-art results.",
      "tldr_zh": "该研究针对基于骨骼的动作分割(STAS)问题，提出Text-Derived Relational Graph-Enhanced Network (TRG-Net)，利用大型语言模型(LLM)生成的先验图来增强关节和动作的相关性建模及监督。TRG-Net的核心组件包括Dynamic Spatio-Temporal Fusion Modeling (DSFM)动态融合空间-时间特征、Absolute-Relative Inter-Class Supervision (ARIS)通过对比学习调节动作特征的绝对和相对关系，以及Spatial-Aware Enhancement Processing (SAEP)通过随机关节遮挡和旋转提升空间泛化能力。在四个公共数据集上，TRG-Net 实现了最先进的结果，显著改善了对人类动作的理解。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15126v1",
      "published_date": "2025-03-19 11:38:14 UTC",
      "updated_date": "2025-03-19 11:38:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:03:20.268997"
    },
    {
      "arxiv_id": "2503.15113v1",
      "title": "Reasoning Effort and Problem Complexity: A Scaling Analysis in LLMs",
      "title_zh": "推理努力与问题复杂度：在大型语言模型中的缩放分析",
      "authors": [
        "Benjamin Estermann",
        "Roger Wattenhofer"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable text generation\ncapabilities, and recent advances in training paradigms have led to\nbreakthroughs in their reasoning performance. In this work, we investigate how\nthe reasoning effort of such models scales with problem complexity. We use the\ninfinitely scalable Tents puzzle, which has a known linear-time solution, to\nanalyze this scaling behavior. Our results show that reasoning effort scales\nwith problem size, but only up to a critical problem complexity. Beyond this\nthreshold, the reasoning effort does not continue to increase, and may even\ndecrease. This observation highlights a critical limitation in the logical\ncoherence of current LLMs as problem complexity increases, and underscores the\nneed for strategies to improve reasoning scalability. Furthermore, our results\nreveal significant performance differences between current state-of-the-art\nreasoning models when faced with increasingly complex logical puzzles.",
      "tldr_zh": "本研究探讨了大型语言模型(LLMs)中推理努力如何随问题复杂度扩展，采用Tents谜题（一个已知线性时间解决方案的谜题）作为分析对象。结果显示，推理努力会随问题规模增加，但仅到某个临界复杂度为止，超过阈值后可能不再增长甚至减少，这揭示了LLMs在高复杂度问题上的逻辑连贯性局限性。研究强调了需要开发策略来提升推理可扩展性，并突出了当前先进模型在复杂逻辑谜题上的性能差异。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Published at ICLR 2025 Workshop on Reasoning and Planning for LLMs",
      "pdf_url": "http://arxiv.org/pdf/2503.15113v1",
      "published_date": "2025-03-19 11:13:51 UTC",
      "updated_date": "2025-03-19 11:13:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:03:30.980934"
    },
    {
      "arxiv_id": "2503.15108v1",
      "title": "VIPER: Visual Perception and Explainable Reasoning for Sequential Decision-Making",
      "title_zh": "翻译失败",
      "authors": [
        "Mohamed Salim Aissi",
        "Clemence Grislain",
        "Mohamed Chetouani",
        "Olivier Sigaud",
        "Laure Soulier",
        "Nicolas Thome"
      ],
      "abstract": "While Large Language Models (LLMs) excel at reasoning on text and\nVision-Language Models (VLMs) are highly effective for visual perception,\napplying those models for visual instruction-based planning remains a widely\nopen problem. In this paper, we introduce VIPER, a novel framework for\nmultimodal instruction-based planning that integrates VLM-based perception with\nLLM-based reasoning. Our approach uses a modular pipeline where a frozen VLM\ngenerates textual descriptions of image observations, which are then processed\nby an LLM policy to predict actions based on the task goal. We fine-tune the\nreasoning module using behavioral cloning and reinforcement learning, improving\nour agent's decision-making capabilities. Experiments on the ALFWorld benchmark\nshow that VIPER significantly outperforms state-of-the-art visual\ninstruction-based planners while narrowing the gap with purely text-based\noracles. By leveraging text as an intermediate representation, VIPER also\nenhances explainability, paving the way for a fine-grained analysis of\nperception and reasoning components.",
      "tldr_zh": "本研究提出 VIPER 框架，用于多模态指令-based 规划，将 Vision-Language Models (VLMs) 的视觉感知与 Large Language Models (LLMs) 的推理能力整合，以解决视觉指令规划的挑战。VIPER 采用模块化管道，其中冻结的 VLM 生成图像观察的文本描述，然后 LLM 政策基于任务目标预测动作，并通过行为克隆和强化学习微调推理模块。在 ALFWorld 基准测试中，VIPER 显著优于现有视觉指令-based 规划器，并缩小了与纯文本预言机的差距，同时通过文本作为中间表示提升了可解释性，便于对感知和推理组件进行细粒度分析。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15108v1",
      "published_date": "2025-03-19 11:05:42 UTC",
      "updated_date": "2025-03-19 11:05:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:03:42.445697"
    },
    {
      "arxiv_id": "2503.15095v1",
      "title": "Diffusion-Based Forecasting for Uncertainty-Aware Model Predictive Control",
      "title_zh": "翻译失败",
      "authors": [
        "Stelios Zarifis",
        "Ioannis Kordonis",
        "Petros Maragos"
      ],
      "abstract": "We propose Diffusion-Informed Model Predictive Control (D-I MPC), a generic\nframework for uncertainty-aware prediction and decision-making in partially\nobservable stochastic systems by integrating diffusion-based time series\nforecasting models in Model Predictive Control algorithms. In our approach, a\ndiffusion-based time series forecasting model is used to probabilistically\nestimate the evolution of the system's stochastic components. These forecasts\nare then incorporated into MPC algorithms to estimate future trajectories and\noptimize action selection under the uncertainty of the future. We evaluate the\nframework on the task of energy arbitrage, where a Battery Energy Storage\nSystem participates in the day-ahead electricity market of the New York state.\nExperimental results indicate that our model-based approach with a\ndiffusion-based forecaster significantly outperforms both implementations with\nclassical forecasting methods and model-free reinforcement learning baselines.",
      "tldr_zh": "这篇论文提出了Diffusion-Informed Model Predictive Control (D-I MPC)框架，用于处理部分可观测随机系统的预测和决策问题，通过整合基于扩散的时序预测模型到Model Predictive Control (MPC)算法中。框架利用扩散模型对系统的随机组件进行概率性估计，并将其融入MPC以优化未来轨迹和动作选择，从而应对不确定性。实验结果显示，在电池能源存储系统参与纽约州日ahead电力市场的能源套利任务上，该方法显著优于传统预测方法和无模型强化学习基线。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SY",
        "eess.SY",
        "I.2.6; I.5.1"
      ],
      "primary_category": "cs.LG",
      "comment": "5 pages, 3 figures, 3 tables. This version is submitted to the 33rd\n  European Signal Processing Conference (EUSIPCO 2025), to be held in Isola\n  delle Femmine - Palermo - Italy, on September 8-12, 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.15095v1",
      "published_date": "2025-03-19 10:48:26 UTC",
      "updated_date": "2025-03-19 10:48:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:03:54.893560"
    },
    {
      "arxiv_id": "2503.15092v1",
      "title": "Towards Understanding the Safety Boundaries of DeepSeek Models: Evaluation and Findings",
      "title_zh": "翻译失败",
      "authors": [
        "Zonghao Ying",
        "Guangyi Zheng",
        "Yongxin Huang",
        "Deyue Zhang",
        "Wenxin Zhang",
        "Quanchen Zou",
        "Aishan Liu",
        "Xianglong Liu",
        "Dacheng Tao"
      ],
      "abstract": "This study presents the first comprehensive safety evaluation of the DeepSeek\nmodels, focusing on evaluating the safety risks associated with their generated\ncontent. Our evaluation encompasses DeepSeek's latest generation of large\nlanguage models, multimodal large language models, and text-to-image models,\nsystematically examining their performance regarding unsafe content generation.\nNotably, we developed a bilingual (Chinese-English) safety evaluation dataset\ntailored to Chinese sociocultural contexts, enabling a more thorough evaluation\nof the safety capabilities of Chinese-developed models. Experimental results\nindicate that despite their strong general capabilities, DeepSeek models\nexhibit significant safety vulnerabilities across multiple risk dimensions,\nincluding algorithmic discrimination and sexual content. These findings provide\ncrucial insights for understanding and improving the safety of large foundation\nmodels. Our code is available at\nhttps://github.com/NY1024/DeepSeek-Safety-Eval.",
      "tldr_zh": "这篇论文对DeepSeek models进行了首次全面安全评估，聚焦于这些模型生成不安全内容的风险，包括大型语言模型、多模态语言模型和文本到图像模型。\n研究团队开发了一个双语（Chinese-English）安全评估数据集，针对中国 sociocultural contexts，对模型的表现进行了系统性检查。\n实验结果显示，尽管DeepSeek models在general capabilities方面表现出色，但它们在多个风险维度如algorithmic discrimination和sexual content上存在显著安全漏洞。\n这些发现为理解和改进大型foundation models的安全性提供了关键洞见，并提供了开源代码以支持进一步研究。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15092v1",
      "published_date": "2025-03-19 10:44:37 UTC",
      "updated_date": "2025-03-19 10:44:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:04:06.482916"
    },
    {
      "arxiv_id": "2503.15082v1",
      "title": "StyleLoco: Generative Adversarial Distillation for Natural Humanoid Robot Locomotion",
      "title_zh": "翻译失败",
      "authors": [
        "Le Ma",
        "Ziyu Meng",
        "Tengyu Liu",
        "Yuhan Li",
        "Ran Song",
        "Wei Zhang",
        "Siyuan Huang"
      ],
      "abstract": "Humanoid robots are anticipated to acquire a wide range of locomotion\ncapabilities while ensuring natural movement across varying speeds and\nterrains. Existing methods encounter a fundamental dilemma in learning humanoid\nlocomotion: reinforcement learning with handcrafted rewards can achieve agile\nlocomotion but produces unnatural gaits, while Generative Adversarial Imitation\nLearning (GAIL) with motion capture data yields natural movements but suffers\nfrom unstable training processes and restricted agility. Integrating these\napproaches proves challenging due to the inherent heterogeneity between expert\npolicies and human motion datasets. To address this, we introduce StyleLoco, a\nnovel two-stage framework that bridges this gap through a Generative\nAdversarial Distillation (GAD) process. Our framework begins by training a\nteacher policy using reinforcement learning to achieve agile and dynamic\nlocomotion. It then employs a multi-discriminator architecture, where distinct\ndiscriminators concurrently extract skills from both the teacher policy and\nmotion capture data. This approach effectively combines the agility of\nreinforcement learning with the natural fluidity of human-like movements while\nmitigating the instability issues commonly associated with adversarial\ntraining. Through extensive simulation and real-world experiments, we\ndemonstrate that StyleLoco enables humanoid robots to perform diverse\nlocomotion tasks with the precision of expertly trained policies and the\nnatural aesthetics of human motion, successfully transferring styles across\ndifferent movement types while maintaining stable locomotion across a broad\nspectrum of command inputs.",
      "tldr_zh": "本研究针对人形机器人（Humanoid Robots）实现多样化运动能力的问题，提出StyleLoco框架，通过Generative Adversarial Distillation (GAD)方法融合强化学习（Reinforcement Learning）和生成对抗模仿学习（Generative Adversarial Imitation Learning, GAIL）的优势。框架采用两阶段设计：首先用强化学习训练教师策略以实现敏捷动态运动，然后通过多判别器架构从教师策略和动作捕捉数据中提取技能，从而结合运动的自然性和稳定性。实验结果显示，StyleLoco使机器人能够在模拟和真实环境中执行精确、多样的运动任务，同时保持人类般的自然美感和风格转移能力。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "9 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.15082v1",
      "published_date": "2025-03-19 10:27:44 UTC",
      "updated_date": "2025-03-19 10:27:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:04:19.403862"
    },
    {
      "arxiv_id": "2503.15060v3",
      "title": "Conjuring Positive Pairs for Efficient Unification of Representation Learning and Image Synthesis",
      "title_zh": "翻译失败",
      "authors": [
        "Imanol G. Estepa",
        "Jesús M. Rodríguez-de-Vera",
        "Ignacio Sarasúa",
        "Bhalaji Nagarajan",
        "Petia Radeva"
      ],
      "abstract": "While representation learning and generative modeling seek to understand\nvisual data, unifying both domains remains unexplored. Recent Unified\nSelf-Supervised Learning (SSL) methods have started to bridge the gap between\nboth paradigms. However, they rely solely on semantic token reconstruction,\nwhich requires an external tokenizer during training -- introducing a\nsignificant overhead. In this work, we introduce Sorcen, a novel unified SSL\nframework, incorporating a synergic Contrastive-Reconstruction objective. Our\nContrastive objective, \"Echo Contrast\", leverages the generative capabilities\nof Sorcen, eliminating the need for additional image crops or augmentations\nduring training. Sorcen \"generates\" an echo sample in the semantic token space,\nforming the contrastive positive pair. Sorcen operates exclusively on\nprecomputed tokens, eliminating the need for an online token transformation\nduring training, thereby significantly reducing computational overhead.\nExtensive experiments on ImageNet-1k demonstrate that Sorcen outperforms the\nprevious Unified SSL SoTA by 0.4%, 1.48 FID, 1.76%, and 1.53% on linear\nprobing, unconditional image generation, few-shot learning, and transfer\nlearning, respectively, while being 60.8% more efficient. Additionally, Sorcen\nsurpasses previous single-crop MIM SoTA in linear probing and achieves SoTA\nperformance in unconditional image generation, highlighting significant\nimprovements and breakthroughs in Unified SSL models.",
      "tldr_zh": "本研究提出了一种名为 Sorcen 的统一自监督学习（Unified SSL）框架，旨在高效整合表示学习和图像合成，通过协同的 Contrastive-Reconstruction 目标解决现有方法的依赖外部标记器和计算开销问题。具体而言，Sorcen 引入了 \"Echo Contrast\" 对比目标，利用其生成能力在语义标记空间中创建正对样本，消除了额外图像增强的需求，并仅依赖预计算标记以降低训练开销。在 ImageNet-1k 数据集上的广泛实验显示，Sorcen 相比之前的 Unified SSL SoTA 提升了线性探测 0.4%、无条件图像生成 FID 降低 1.48%、少样本学习 1.76% 和转移学习 1.53%，并提高了 60.8% 的效率，同时在单裁剪 MIM SoTA 上实现了新突破。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "I.5.4; I.5.1; I.2.10"
      ],
      "primary_category": "cs.CV",
      "comment": "The source code is available in https://github.com/ImaGonEs/Sorcen",
      "pdf_url": "http://arxiv.org/pdf/2503.15060v3",
      "published_date": "2025-03-19 09:53:11 UTC",
      "updated_date": "2025-05-20 08:24:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:04:30.651150"
    },
    {
      "arxiv_id": "2503.15058v1",
      "title": "Texture-Aware StarGAN for CT data harmonisation",
      "title_zh": "纹理感知的 StarGAN 用于 CT 数据协调",
      "authors": [
        "Francesco Di Feola",
        "Ludovica Pompilio",
        "Cecilia Assolito",
        "Valerio Guarrasi",
        "Paolo Soda"
      ],
      "abstract": "Computed Tomography (CT) plays a pivotal role in medical diagnosis; however,\nvariability across reconstruction kernels hinders data-driven approaches, such\nas deep learning models, from achieving reliable and generalized performance.\nTo this end, CT data harmonization has emerged as a promising solution to\nminimize such non-biological variances by standardizing data across different\nsources or conditions. In this context, Generative Adversarial Networks (GANs)\nhave proved to be a powerful framework for harmonization, framing it as a\nstyle-transfer problem. However, GAN-based approaches still face limitations in\ncapturing complex relationships within the images, which are essential for\neffective harmonization. In this work, we propose a novel texture-aware StarGAN\nfor CT data harmonization, enabling one-to-many translations across different\nreconstruction kernels. Although the StarGAN model has been successfully\napplied in other domains, its potential for CT data harmonization remains\nunexplored. Furthermore, our approach introduces a multi-scale texture loss\nfunction that embeds texture information across different spatial and angular\nscales into the harmonization process, effectively addressing kernel-induced\ntexture variations. We conducted extensive experimentation on a publicly\navailable dataset, utilizing a total of 48667 chest CT slices from 197 patients\ndistributed over three different reconstruction kernels, demonstrating the\nsuperiority of our method over the baseline StarGAN.",
      "tldr_zh": "本研究针对 CT 图像中不同重建内核导致的数据变异问题，提出了一种 texture-aware StarGAN 方法，用于 CT 数据协调化，以标准化图像并提升深度学习模型的泛化性能。  \n该方法基于 Generative Adversarial Networks (GANs) 的风格转移框架，实现了对多种重建内核的一到多图像翻译，并引入 multi-scale texture loss 函数来捕捉不同空间和角度尺度的纹理信息，从而有效处理内核诱发的纹理变化。  \n在包含 48667 个胸部 CT 切片的公开数据集上进行广泛实验，结果显示，该方法优于基线 StarGAN，显著提高了图像协调化的准确性和可靠性。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15058v1",
      "published_date": "2025-03-19 09:50:32 UTC",
      "updated_date": "2025-03-19 09:50:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:04:43.957424"
    },
    {
      "arxiv_id": "2503.15049v1",
      "title": "HAD-Gen: Human-like and Diverse Driving Behavior Modeling for Controllable Scenario Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Cheng Wang",
        "Lingxin Kong",
        "Massimiliano Tamborski",
        "Stefano V. Albrecht"
      ],
      "abstract": "Simulation-based testing has emerged as an essential tool for verifying and\nvalidating autonomous vehicles (AVs). However, contemporary methodologies, such\nas deterministic and imitation learning-based driver models, struggle to\ncapture the variability of human-like driving behavior. Given these challenges,\nwe propose HAD-Gen, a general framework for realistic traffic scenario\ngeneration that simulates diverse human-like driving behaviors. The framework\nfirst clusters the vehicle trajectory data into different driving styles\naccording to safety features. It then employs maximum entropy inverse\nreinforcement learning on each of the clusters to learn the reward function\ncorresponding to each driving style. Using these reward functions, the method\nintegrates offline reinforcement learning pre-training and multi-agent\nreinforcement learning algorithms to obtain general and robust driving\npolicies. Multi-perspective simulation results show that our proposed scenario\ngeneration framework can simulate diverse, human-like driving behaviors with\nstrong generalization capability. The proposed framework achieves a 90.96%\ngoal-reaching rate, an off-road rate of 2.08%, and a collision rate of 6.91% in\nthe generalization test, outperforming prior approaches by over 20% in\ngoal-reaching performance. The source code is released at\nhttps://github.com/RoboSafe-Lab/Sim4AD.",
      "tldr_zh": "该研究提出HAD-Gen框架，用于生成真实交通场景以模拟多样的人类驾驶行为，从而提升自动驾驶车辆（AVs）的模拟测试效果。框架首先根据安全特征聚类车辆轨迹数据，然后运用maximum entropy inverse reinforcement learning学习每个驾驶风格的奖励函数，并结合offline reinforcement learning预训练和multi-agent reinforcement learning算法开发通用稳健的驾驶策略。实验结果显示，HAD-Gen在多视角模拟中实现90.96%的目标达成率、2.08%的离路率和6.91%的碰撞率，在目标达成性能上比现有方法高出20%以上。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15049v1",
      "published_date": "2025-03-19 09:38:45 UTC",
      "updated_date": "2025-03-19 09:38:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:04:55.823957"
    },
    {
      "arxiv_id": "2503.15035v1",
      "title": "GraspCorrect: Robotic Grasp Correction via Vision-Language Model-Guided Feedback",
      "title_zh": "翻译失败",
      "authors": [
        "Sungjae Lee",
        "Yeonjoo Hong",
        "Kwang In Kim"
      ],
      "abstract": "Despite significant advancements in robotic manipulation, achieving\nconsistent and stable grasping remains a fundamental challenge, often limiting\nthe successful execution of complex tasks. Our analysis reveals that even\nstate-of-the-art policy models frequently exhibit unstable grasping behaviors,\nleading to failure cases that create bottlenecks in real-world robotic\napplications. To address these challenges, we introduce GraspCorrect, a\nplug-and-play module designed to enhance grasp performance through\nvision-language model-guided feedback. GraspCorrect employs an iterative visual\nquestion-answering framework with two key components: grasp-guided prompting,\nwhich incorporates task-specific constraints, and object-aware sampling, which\nensures the selection of physically feasible grasp candidates. By iteratively\ngenerating intermediate visual goals and translating them into joint-level\nactions, GraspCorrect significantly improves grasp stability and consistently\nenhances task success rates across existing policy models in the RLBench and\nCALVIN datasets.",
      "tldr_zh": "该研究针对机器人抓取技术的稳定性问题，引入 GraspCorrect，这是一个插件式模块，通过 vision-language model 引导的反馈来提升抓取性能。GraspCorrect 采用迭代的视觉问答框架，包括 grasp-guided prompting（结合任务特定约束）和 object-aware sampling（确保物理可行性），从而生成中间视觉目标并转化为关节级动作。实验结果显示，在 RLBench 和 CALVIN 数据集上，GraspCorrect 显著提高了现有策略模型的任务成功率和抓取稳定性。",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15035v1",
      "published_date": "2025-03-19 09:25:32 UTC",
      "updated_date": "2025-03-19 09:25:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:05:07.347253"
    },
    {
      "arxiv_id": "2503.15008v1",
      "title": "A Novel Channel Boosted Residual CNN-Transformer with Regional-Boundary Learning for Breast Cancer Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Aamir Mehmood",
        "Yue Hu",
        "Saddam Hussain Khan"
      ],
      "abstract": "Recent advancements in detecting tumors using deep learning on breast\nultrasound images (BUSI) have demonstrated significant success. Deep CNNs and\nvision-transformers (ViTs) have demonstrated individually promising initial\nperformance. However, challenges related to model complexity and contrast,\ntexture, and tumor morphology variations introduce uncertainties that hinder\nthe effectiveness of current methods. This study introduces a novel hybrid\nframework, CB-Res-RBCMT, combining customized residual CNNs and new ViT\ncomponents for detailed BUSI cancer analysis. The proposed RBCMT uses stem\nconvolution blocks with CNN Meet Transformer (CMT) blocks, followed by new\nRegional and boundary (RB) feature extraction operations for capturing contrast\nand morphological variations. Moreover, the CMT block incorporates global\ncontextual interactions through multi-head attention, enhancing computational\nefficiency with a lightweight design. Additionally, the customized inverse\nresidual and stem CNNs within the CMT effectively extract local texture\ninformation and handle vanishing gradients. Finally, the new channel-boosted\n(CB) strategy enriches the feature diversity of the limited dataset by\ncombining the original RBCMT channels with transfer learning-based residual\nCNN-generated maps. These diverse channels are processed through a spatial\nattention block for optimal pixel selection, reducing redundancy and improving\nthe discrimination of minor contrast and texture variations. The proposed\nCB-Res-RBCMT achieves an F1-score of 95.57%, accuracy of 95.63%, sensitivity of\n96.42%, and precision of 94.79% on the standard harmonized stringent BUSI\ndataset, outperforming existing ViT and CNN methods. These results demonstrate\nthe versatility of our integrated CNN-Transformer framework in capturing\ndiverse features and delivering superior performance in BUSI cancer diagnosis.",
      "tldr_zh": "该研究提出了一种新型混合框架CB-Res-RBCMT，用于乳腺超声图像(BUSI)中乳腺癌检测，旨在解决模型复杂性、对比度、纹理和肿瘤形态变化带来的挑战。该框架结合定制的残差CNN和Vision Transformer(ViTs)组件，包括RBCMT模块（使用stem卷积块、CNN Meet Transformer (CMT)块和Regional and Boundary (RB)特征提取操作）来捕捉全局上下文和局部纹理信息，同时通过Channel-boosted (CB)策略增强特征多样性，并利用空间注意力块优化像素选择以减少冗余。实验结果显示，在标准BUSI数据集上，该框架实现了F1-score 95.57%、准确率95.63%、敏感性96.42%和精确度94.79%，优于现有ViT和CNN方法，展示了其在癌症诊断中的高效性和鲁棒性。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "12 pages, 10 Figures, 2 Tables. arXiv admin note: substantial text\n  overlap with arXiv:2405.12986",
      "pdf_url": "http://arxiv.org/pdf/2503.15008v1",
      "published_date": "2025-03-19 08:59:02 UTC",
      "updated_date": "2025-03-19 08:59:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:05:20.679098"
    },
    {
      "arxiv_id": "2503.16547v1",
      "title": "Empowering Medical Multi-Agents with Clinical Consultation Flow for Dynamic Diagnosis",
      "title_zh": "通过临床咨询流程赋能医疗多智能体用于动态诊断",
      "authors": [
        "Sihan Wang",
        "Suiyang Jiang",
        "Yibo Gao",
        "Boming Wang",
        "Shangqi Gao",
        "Xiahai Zhuang"
      ],
      "abstract": "Traditional AI-based healthcare systems often rely on single-modal data,\nlimiting diagnostic accuracy due to incomplete information. However, recent\nadvancements in foundation models show promising potential for enhancing\ndiagnosis combining multi-modal information. While these models excel in static\ntasks, they struggle with dynamic diagnosis, failing to manage multi-turn\ninteractions and often making premature diagnostic decisions due to\ninsufficient persistence in information collection.To address this, we propose\na multi-agent framework inspired by consultation flow and reinforcement\nlearning (RL) to simulate the entire consultation process, integrating multiple\nclinical information for effective diagnosis. Our approach incorporates a\nhierarchical action set, structured from clinic consultation flow and medical\ntextbook, to effectively guide the decision-making process. This strategy\nimproves agent interactions, enabling them to adapt and optimize actions based\non the dynamic state. We evaluated our framework on a public dynamic diagnosis\nbenchmark. The proposed framework evidentially improves the baseline methods\nand achieves state-of-the-art performance compared to existing foundation\nmodel-based methods.",
      "tldr_zh": "本文研究发现，传统 AI 医疗系统依赖单模态数据，难以处理动态诊断中的多轮交互和信息收集不足问题，导致诊断决策过早。针对此，该论文提出一个多智能体框架，受临床咨询流程和强化学习 (RL) 启发，模拟完整咨询过程，并引入分层动作集（基于临床咨询流程和医学教科书）来优化决策和适应动态状态。在公共动态诊断基准上的实验表明，该框架显著提升了基线方法的性能，达到了现有基础模型方法的 state-of-the-art 水平。",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16547v1",
      "published_date": "2025-03-19 08:47:18 UTC",
      "updated_date": "2025-03-19 08:47:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:05:32.431518"
    },
    {
      "arxiv_id": "2503.16546v1",
      "title": "A Comprehensive Survey on Architectural Advances in Deep CNNs: Challenges, Applications, and Emerging Research Directions",
      "title_zh": "深度卷积神经网络架构进展的全面调查：挑战、应用和新兴研究方向",
      "authors": [
        "Saddam Hussain Khan",
        "Rashid Iqbal"
      ],
      "abstract": "Deep Convolutional Neural Networks (CNNs) have significantly advanced deep\nlearning, driving breakthroughs in computer vision, natural language\nprocessing, medical diagnosis, object detection, and speech recognition.\nArchitectural innovations including 1D, 2D, and 3D convolutional models,\ndilated and grouped convolutions, depthwise separable convolutions, and\nattention mechanisms address domain-specific challenges and enhance feature\nrepresentation and computational efficiency. Structural refinements such as\nspatial-channel exploitation, multi-path design, and feature-map enhancement\ncontribute to robust hierarchical feature extraction and improved\ngeneralization, particularly through transfer learning. Efficient preprocessing\nstrategies, including Fourier transforms, structured transforms, low-precision\ncomputation, and weight compression, optimize inference speed and facilitate\ndeployment in resource-constrained environments. This survey presents a unified\ntaxonomy that classifies CNN architectures based on spatial exploitation,\nmulti-path structures, depth, width, dimensionality expansion, channel\nboosting, and attention mechanisms. It systematically reviews CNN applications\nin face recognition, pose estimation, action recognition, text classification,\nstatistical language modeling, disease diagnosis, radiological analysis,\ncryptocurrency sentiment prediction, 1D data processing, video analysis, and\nspeech recognition. In addition to consolidating architectural advancements,\nthe review highlights emerging learning paradigms such as few-shot, zero-shot,\nweakly supervised, federated learning frameworks and future research directions\ninclude hybrid CNN-transformer models, vision-language integration, generative\nlearning, etc. This review provides a comprehensive perspective on CNN's\nevolution from 2015 to 2025, outlining key innovations, challenges, and\nopportunities.",
      "tldr_zh": "这篇调查论文对Deep CNNs的架构进展进行了全面回顾，涵盖了从2015到2025年的关键创新，如1D/2D/3D卷积、dilated convolutions、attention mechanisms和深度可分离卷积，以提升特征表示和计算效率。\n论文提出一个统一的taxonomy，根据空间利用、多路径结构、深度、宽度和通道增强等维度分类CNN架构，并系统分析了其在面部识别、姿态估计、医疗诊断和语音识别等领域的应用及挑战。\n此外，它讨论了新兴学习范式如few-shot learning、zero-shot learning和federated learning框架，并指出了未来研究方向，包括hybrid CNN-transformer模型、vision-language integration和生成学习。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "100 Pages, 44 Figures",
      "pdf_url": "http://arxiv.org/pdf/2503.16546v1",
      "published_date": "2025-03-19 08:41:06 UTC",
      "updated_date": "2025-03-19 08:41:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:05:44.531169"
    },
    {
      "arxiv_id": "2503.14976v3",
      "title": "Application of linear regression and quasi-Newton methods to the deep reinforcement learning in continuous action cases",
      "title_zh": "线性回归和拟牛顿方法在连续动作情况下的深度强化学习应用",
      "authors": [
        "Hisato Komatsu"
      ],
      "abstract": "The linear regression (LR) method offers the advantage that optimal\nparameters can be calculated relatively easily, although its representation\ncapability is limited than that of the deep learning technique. To improve deep\nreinforcement learning, the Least Squares Deep Q Network (LS-DQN) method was\nproposed by Levine et al., which combines Deep Q Network (DQN) with LR method.\nHowever, the LS-DQN method assumes that the actions are discrete. In this\nstudy, we propose the Double Least Squares Deep Deterministic Policy Gradient\n(DLS-DDPG) method to address this limitation. This method combines the LR\nmethod with the Deep Deterministic Policy Gradient (DDPG) technique, one of the\nrepresentative deep reinforcement learning algorithms for continuous action\ncases. For the LR update of the critic network, DLS-DDPG uses an algorithm\nsimilar to the Fitted Q iteration, the method which LS-DQN adopted. In\naddition, we calculated the optimal action using the quasi-Newton method and\nused it as both the agent's action and the training data for the LR update of\nthe actor network. Numerical experiments conducted in MuJoCo environments\nshowed that the proposed method improved performance at least in some tasks,\nalthough there are difficulties such as the inability to make the\nregularization terms small.",
      "tldr_zh": "这篇论文提出了一种名为 Double Least Squares Deep Deterministic Policy Gradient (DLS-DDPG) 的新方法，将 linear regression (LR) 与 Deep Deterministic Policy Gradient (DDPG) 结合，用于解决深度强化学习在连续动作场景中的问题。DLS-DDPG 通过类似于 Fitted Q iteration 的算法更新 critic 网络，并利用 quasi-Newton methods 计算最优动作，作为代理动作和 actor 网络的训练数据。该方法在 MuJoCo 环境中的数值实验显示，在某些任务中性能得到改善，但面临挑战，如难以减小正则化项。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "23 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.14976v3",
      "published_date": "2025-03-19 08:10:54 UTC",
      "updated_date": "2025-04-25 14:36:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:05:56.604988"
    },
    {
      "arxiv_id": "2503.14973v1",
      "title": "Behaviour Discovery and Attribution for Explainable Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Rishav Rishav",
        "Somjit Nath",
        "Vincent Michalski",
        "Samira Ebrahimi Kahou"
      ],
      "abstract": "Explaining the decisions made by reinforcement learning (RL) agents is\ncritical for building trust and ensuring reliability in real-world\napplications. Traditional approaches to explainability often rely on saliency\nanalysis, which can be limited in providing actionable insights. Recently,\nthere has been growing interest in attributing RL decisions to specific\ntrajectories within a dataset. However, these methods often generalize\nexplanations to long trajectories, potentially involving multiple distinct\nbehaviors. Often, providing multiple more fine grained explanations would\nimprove clarity. In this work, we propose a framework for behavior discovery\nand action attribution to behaviors in offline RL trajectories. Our method\nidentifies meaningful behavioral segments, enabling more precise and granular\nexplanations associated with high level agent behaviors. This approach is\nadaptable across diverse environments with minimal modifications, offering a\nscalable and versatile solution for behavior discovery and attribution for\nexplainable RL.",
      "tldr_zh": "这篇论文针对强化学习 (RL) 代理决策的可解释性问题，提出一个框架，用于行为发现 (behavior discovery) 和动作归因 (action attribution)。该方法在离线 RL 轨迹中识别有意义的行為段 (behavioral segments)，从而提供更精确和细粒度的解释，与代理的高级行为相关联。相比传统显著性分析 (saliency analysis)，这种方法提升了解释的清晰度，并适用于多种环境，仅需最小修改，提供可扩展的解决方案。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14973v1",
      "published_date": "2025-03-19 08:06:00 UTC",
      "updated_date": "2025-03-19 08:06:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:06:08.488944"
    },
    {
      "arxiv_id": "2503.18958v1",
      "title": "Advancing Deep Learning through Probability Engineering: A Pragmatic Paradigm for Modern AI",
      "title_zh": "通过概率工程推进深度",
      "authors": [
        "Jianyi Zhang"
      ],
      "abstract": "Recent years have witnessed the rapid progression of deep learning, pushing\nus closer to the realization of AGI (Artificial General Intelligence).\nProbabilistic modeling is critical to many of these advancements, which\nprovides a foundational framework for capturing data distributions. However, as\nthe scale and complexity of AI applications grow, traditional probabilistic\nmodeling faces escalating challenges, such as high-dimensional parameter\nspaces, heterogeneous data sources, and evolving real-world requirements often\nrender classical approaches insufficiently flexible.\n  This paper proposes a novel concept, Probability Engineering, which treats\nthe already-learned probability distributions within deep learning as\nengineering artifacts. Rather than merely fitting or inferring distributions,\nwe actively modify and reinforce them to better address the diverse and\nevolving demands of modern AI. Specifically, Probability Engineering introduces\nnovel techniques and constraints to refine existing probability distributions,\nimproving their robustness, efficiency, adaptability, or trustworthiness.\n  We showcase this paradigm through a series of applications spanning Bayesian\ndeep learning, Edge AI (including federated learning and knowledge\ndistillation), and Generative AI (such as text-to-image generation with\ndiffusion models and high-quality text generation with large language models).\nThese case studies demonstrate how probability distributions once treated as\nstatic objects can be engineered to meet the diverse and evolving requirements\nof large-scale, data-intensive, and trustworthy AI systems. By systematically\nexpanding and strengthening the role of probabilistic modeling, Probability\nEngineering paves the way for more robust, adaptive, efficient, and trustworthy\ndeep learning solutions in today's fast-growing AI era.",
      "tldr_zh": "这篇论文提出了Probability Engineering的新概念，将深度学习中已学到的概率分布视为工程制品，并通过主动修改和强化这些分布来应对现代AI面临的挑战，如高维参数空间和异构数据源。作者引入了新技术和约束，以提升概率分布的鲁棒性、效率、可适应性和可信度。论文通过Bayesian deep learning、Edge AI（如联邦学习和知识蒸馏）以及Generative AI（如扩散模型的文本到图像生成和大语言模型的文本生成）的应用案例，展示了这一范式如何使AI系统更具灵活性和可靠性，从而为大规模、数据密集型AI解决方案奠定基础。",
      "categories": [
        "cs.AI",
        "math.PR",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "comment": "Ph.D. dissertation",
      "pdf_url": "http://arxiv.org/pdf/2503.18958v1",
      "published_date": "2025-03-19 07:48:23 UTC",
      "updated_date": "2025-03-19 07:48:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:06:20.110087"
    },
    {
      "arxiv_id": "2503.14950v1",
      "title": "USAM-Net: A U-Net-based Network for Improved Stereo Correspondence and Scene Depth Estimation using Features from a Pre-trained Image Segmentation network",
      "title_zh": "翻译失败",
      "authors": [
        "Joseph Emmanuel DL Dayo",
        "Prospero C. Naval Jr"
      ],
      "abstract": "The increasing demand for high-accuracy depth estimation in autonomous\ndriving and augmented reality applications necessitates advanced neural\narchitectures capable of effectively leveraging multiple data modalities. In\nthis context, we introduce the Unified Segmentation Attention Mechanism Network\n(USAM-Net), a novel convolutional neural network that integrates stereo image\ninputs with semantic segmentation maps and attention to enhance depth\nestimation performance. USAM-Net employs a dual-pathway architecture, which\ncombines a pre-trained segmentation model (SAM) and a depth estimation model.\nThe segmentation pathway preprocesses the stereo images to generate semantic\nmasks, which are then concatenated with the stereo images as inputs to the\ndepth estimation pathway. This integration allows the model to focus on\nimportant features such as object boundaries and surface textures which are\ncrucial for accurate depth perception. Empirical evaluation on the\nDrivingStereo dataset demonstrates that USAM-Net achieves superior performance\nmetrics, including a Global Difference (GD) of 3.61\\% and an End-Point Error\n(EPE) of 0.88, outperforming traditional models such as CFNet, SegStereo, and\niResNet. These results underscore the effectiveness of integrating segmentation\ninformation into stereo depth estimation tasks, highlighting the potential of\nUSAM-Net in applications demanding high-precision depth data.",
      "tldr_zh": "本研究提出 USAM-Net，一种基于 U-Net 的新型卷积神经网络，用于提升立体对应和场景深度估计的准确性，通过整合立体图像输入、语义分割图以及注意力机制。USAM-Net 采用双路径架构，将预训练的分割模型 (SAM) 生成的语义掩码与立体图像拼接，作为深度估计路径的输入，从而更好地关注物体边界和表面纹理等关键特征。在 DrivingStereo 数据集上的实验表明，USAM-Net 实现了 3.61% 的 Global Difference 和 0.88 的 End-Point Error，优于传统模型如 CFNet 和 SegStereo。这些结果证明了整合分割信息对高精度深度估计任务的有效性，尤其适用于自动驾驶和增强现实应用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14950v1",
      "published_date": "2025-03-19 07:29:02 UTC",
      "updated_date": "2025-03-19 07:29:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:06:34.557313"
    },
    {
      "arxiv_id": "2503.14935v1",
      "title": "FAVOR-Bench: A Comprehensive Benchmark for Fine-Grained Video Motion Understanding",
      "title_zh": "FAVOR-Bench：细粒度视频运动理解的全面基准测试",
      "authors": [
        "Chongjun Tu",
        "Lin Zhang",
        "Pengtao Chen",
        "Peng Ye",
        "Xianfang Zeng",
        "Wei Cheng",
        "Gang Yu",
        "Tao Chen"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have shown remarkable capabilities\nin video content understanding but still struggle with fine-grained motion\ncomprehension. To comprehensively assess the motion understanding ability of\nexisting MLLMs, we introduce FAVOR-Bench, comprising 1,776 videos with\nstructured manual annotations of various motions. Our benchmark includes both\nclose-ended and open-ended tasks. For close-ended evaluation, we carefully\ndesign 8,184 multiple-choice question-answer pairs spanning six distinct\nsub-tasks. For open-ended evaluation, we develop both a novel cost-efficient\nLLM-free and a GPT-assisted caption assessment method, where the former can\nenhance benchmarking interpretability and reproducibility. Comprehensive\nexperiments with 21 state-of-the-art MLLMs reveal significant limitations in\ntheir ability to comprehend and describe detailed temporal dynamics in video\nmotions. To alleviate this limitation, we further build FAVOR-Train, a dataset\nconsisting of 17,152 videos with fine-grained motion annotations. The results\nof finetuning Qwen2.5-VL on FAVOR-Train yield consistent improvements on\nmotion-related tasks of TVBench, MotionBench and our FAVOR-Bench. Comprehensive\nassessment results demonstrate that the proposed FAVOR-Bench and FAVOR-Train\nprovide valuable tools to the community for developing more powerful video\nunderstanding models. Project page:\n\\href{https://favor-bench.github.io/}{https://favor-bench.github.io/}.",
      "tldr_zh": "本文引入 FAVOR-Bench，这是一个全面基准，包含 1,776 个视频和 8,184 个多选题，用于评估 Multimodal Large Language Models (MLLMs) 在细粒度视频动作理解方面的能力，包括 close-ended 和 open-ended 任务，并开发了成本高效的 LLM-free 和 GPT-assisted 评估方法。实验结果显示，21 个最先进 MLLMs 在理解和描述视频动作的详细时间动态上存在显著局限性。针对此问题，作者构建了 FAVOR-Train 数据集，包含 17,152 个视频的细粒度注释，并在该数据集上微调 Qwen2.5-VL 模型后，观察到 TVBench、MotionBench 和 FAVOR-Bench 的动作相关任务性能一致提升。这些工具为社区开发更强大的视频理解模型提供了宝贵资源。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "FAVOR-Bench project page: https://favor-bench.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2503.14935v1",
      "published_date": "2025-03-19 06:42:32 UTC",
      "updated_date": "2025-03-19 06:42:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:06:46.465302"
    },
    {
      "arxiv_id": "2503.14928v1",
      "title": "Shushing! Let's Imagine an Authentic Speech from the Silent Video",
      "title_zh": "嘘！让我们从无声视频中想象真实的语音",
      "authors": [
        "Jiaxin Ye",
        "Hongming Shan"
      ],
      "abstract": "Vision-guided speech generation aims to produce authentic speech from facial\nappearance or lip motions without relying on auditory signals, offering\nsignificant potential for applications such as dubbing in filmmaking and\nassisting individuals with aphonia. Despite recent progress, existing methods\nstruggle to achieve unified cross-modal alignment across semantics, timbre, and\nemotional prosody from visual cues, prompting us to propose Consistent\nVideo-to-Speech (CV2S) as an extended task to enhance cross-modal consistency.\nTo tackle emerging challenges, we introduce ImaginTalk, a novel cross-modal\ndiffusion framework that generates faithful speech using only visual input,\noperating within a discrete space. Specifically, we propose a discrete lip\naligner that predicts discrete speech tokens from lip videos to capture\nsemantic information, while an error detector identifies misaligned tokens,\nwhich are subsequently refined through masked language modeling with BERT. To\nfurther enhance the expressiveness of the generated speech, we develop a style\ndiffusion transformer equipped with a face-style adapter that adaptively\ncustomizes identity and prosody dynamics across both the channel and temporal\ndimensions while ensuring synchronization with lip-aware semantic features.\nExtensive experiments demonstrate that ImaginTalk can generate high-fidelity\nspeech with more accurate semantic details and greater expressiveness in timbre\nand emotion compared to state-of-the-art baselines. Demos are shown at our\nproject page: https://imagintalk.github.io.",
      "tldr_zh": "这篇论文提出 ImaginTalk，一个新型的跨模态扩散框架，用于从无声视频生成真实的语音，仅依赖视觉输入，如面部外观和唇部动作，以实现语义、音色和情感韵律的统一一致。框架的关键组件包括 discrete lip aligner，用于从唇部视频预测离散语音标记捕捉语义信息，以及 error detector 结合 masked language modeling 与 BERT 来识别和精炼不对齐的标记。同时，style diffusion transformer 配备 face-style adapter，能自适应自定义身份和韵律动态，确保与唇部语义特征同步。实验结果表明，ImaginTalk 生成的语音在语义细节、音色和情感表达上均优于现有基线方法，提供高保真度的应用潜力，如电影配音和助听支持。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CV",
      "comment": "Project Page: https://imagintalk.github.io",
      "pdf_url": "http://arxiv.org/pdf/2503.14928v1",
      "published_date": "2025-03-19 06:28:17 UTC",
      "updated_date": "2025-03-19 06:28:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:06:57.455511"
    },
    {
      "arxiv_id": "2503.16544v1",
      "title": "Causal Discovery and Counterfactual Reasoning to Optimize Persuasive Dialogue Policies",
      "title_zh": "利用因果发现和反事实推理优化说服性对话策略",
      "authors": [
        "Donghuo Zeng",
        "Roberto Legaspi",
        "Yuewen Sun",
        "Xinshuai Dong",
        "Kazushi Ikeda",
        "Peter Spirtes",
        "Kun Zhang"
      ],
      "abstract": "Tailoring persuasive conversations to users leads to more effective\npersuasion. However, existing dialogue systems often struggle to adapt to\ndynamically evolving user states. This paper presents a novel method that\nleverages causal discovery and counterfactual reasoning for optimizing system\npersuasion capability and outcomes. We employ the Greedy Relaxation of the\nSparsest Permutation (GRaSP) algorithm to identify causal relationships between\nuser and system utterance strategies, treating user strategies as states and\nsystem strategies as actions. GRaSP identifies user strategies as causal\nfactors influencing system responses, which inform Bidirectional Conditional\nGenerative Adversarial Networks (BiCoGAN) in generating counterfactual\nutterances for the system. Subsequently, we use the Dueling Double Deep\nQ-Network (D3QN) model to utilize counterfactual data to determine the best\npolicy for selecting system utterances. Our experiments with the\nPersuasionForGood dataset show measurable improvements in persuasion outcomes\nusing our approach over baseline methods. The observed increase in cumulative\nrewards and Q-values highlights the effectiveness of causal discovery in\nenhancing counterfactual reasoning and optimizing reinforcement learning\npolicies for online dialogue systems.",
      "tldr_zh": "本文提出一种利用因果发现和反事实推理的方法，来优化说服性对话系统的策略，以更好地适应动态的用户状态。具体而言，该方法采用 GRaSP 算法识别用户策略与系统策略的因果关系，并使用 BiCoGAN 生成反事实话语，然后通过 D3QN 模型基于这些数据选择最佳系统响应。在 PersuasionForGood 数据集上的实验显示，该方法显著提高了说服效果，累积奖励和 Q 值均优于基线方法，证明了因果发现在增强对话优化中的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "21 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.16544v1",
      "published_date": "2025-03-19 06:06:10 UTC",
      "updated_date": "2025-03-19 06:06:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:07:10.907259"
    },
    {
      "arxiv_id": "2503.14922v1",
      "title": "A Semantic and Clean-label Backdoor Attack against Graph Convolutional Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Jiazhu Dai",
        "Haoyu Sun"
      ],
      "abstract": "Graph Convolutional Networks (GCNs) have shown excellent performance in\ngraph-structured tasks such as node classification and graph classification.\nHowever, recent research has shown that GCNs are vulnerable to a new type of\nthreat called the backdoor attack, where the adversary can inject a hidden\nbackdoor into the GCNs so that the backdoored model performs well on benign\nsamples, whereas its prediction will be maliciously changed to the\nattacker-specified target label if the hidden backdoor is activated by the\nattacker-defined trigger. Clean-label backdoor attack and semantic backdoor\nattack are two new backdoor attacks to Deep Neural Networks (DNNs), they are\nmore imperceptible and have posed new and serious threats. The semantic and\nclean-label backdoor attack is not fully explored in GCNs. In this paper, we\npropose a semantic and clean-label backdoor attack against GCNs under the\ncontext of graph classification to reveal the existence of this security\nvulnerability in GCNs. Specifically, SCLBA conducts an importance analysis on\ngraph samples to select one type of node as semantic trigger, which is then\ninserted into the graph samples to create poisoning samples without changing\nthe labels of the poisoning samples to the attacker-specified target label. We\nevaluate SCLBA on multiple datasets and the results show that SCLBA can achieve\nattack success rates close to 99% with poisoning rates of less than 3%, and\nwith almost no impact on the performance of model on benign samples.",
      "tldr_zh": "本文提出了一种针对Graph Convolutional Networks (GCNs)的语义和清洁标签后门攻击(SCLBA)，旨在揭示GCNs在图分类任务中存在的安全漏洞，该攻击通过在不改变样本标签的情况下注入隐蔽的语义触发器来实现。SCLBA的方法包括对图样本进行重要性分析，选择特定节点作为semantic trigger插入中毒样本，从而创建隐蔽的攻击样本。实验结果显示，在多个数据集上，该攻击的成功率接近99%，毒化率低于3%，且对模型在正常样本上的性能几乎无影响。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14922v1",
      "published_date": "2025-03-19 06:04:55 UTC",
      "updated_date": "2025-03-19 06:04:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:07:21.961549"
    },
    {
      "arxiv_id": "2503.14917v1",
      "title": "MASS: Mathematical Data Selection via Skill Graphs for Pretraining Large Language Models",
      "title_zh": "MASS：通过技能图谱的数学数据选择，用于预训练大型语言模型",
      "authors": [
        "Jiazheng Li",
        "Lu Yu",
        "Qing Cui",
        "Zhiqiang Zhang",
        "Jun Zhou",
        "Yanfang Ye",
        "Chuxu Zhang"
      ],
      "abstract": "High-quality data plays a critical role in the pretraining and fine-tuning of\nlarge language models (LLMs), even determining their performance ceiling to\nsome degree. Consequently, numerous data selection methods have been proposed\nto identify subsets of data that can effectively and efficiently enhance model\nperformance. However, most of these methods focus on general data selection and\ntend to overlook the specific nuances of domain-related data. In this paper, we\nintroduce MASS, a \\textbf{MA}thematical data \\textbf{S}election framework using\nthe \\textbf{S}kill graph for pretraining LLMs in the mathematical reasoning\ndomain. By taking into account the unique characteristics of mathematics and\nreasoning, we construct a skill graph that captures the mathematical skills and\ntheir interrelations from a reference dataset. This skill graph guides us in\nassigning quality scores to the target dataset, enabling us to select the\ntop-ranked subset which is further used to pretrain LLMs. Experimental results\ndemonstrate the efficiency and effectiveness of MASS across different model\nsizes (1B and 7B) and pretraining datasets (web data and synthetic data).\nSpecifically, in terms of efficiency, models trained on subsets selected by\nMASS can achieve similar performance to models trained on the original\ndatasets, with a significant reduction in the number of trained tokens -\nranging from 50\\% to 70\\% fewer tokens. In terms of effectiveness, when trained\non the same amount of tokens, models trained on the data selected by MASS\noutperform those trained on the original datasets by 3.3\\% to 5.9\\%. These\nresults underscore the potential of MASS to improve both the efficiency and\neffectiveness of pretraining LLMs.",
      "tldr_zh": "该论文介绍了 MASS 框架，一种用于预训练大型语言模型（LLMs）的数学数据选择方法，通过构建技能图（Skill graph）来捕捉数学技能及其相互关系，从而为目标数据集分配质量分数并选择高质量子集。相比传统数据选择方法，MASS 特别关注数学推理领域的特性，提高了预训练的效率和效果。实验结果显示，在不同模型规模（1B 和 7B）和数据集上，使用 MASS 选择的子集可减少 50% 到 70% 的训练 token，同时在相同 token 量下提升模型性能 3.3% 到 5.9%。这为高效预训练 LLMs 提供了新途径。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14917v1",
      "published_date": "2025-03-19 05:50:21 UTC",
      "updated_date": "2025-03-19 05:50:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:07:32.249445"
    },
    {
      "arxiv_id": "2503.14908v1",
      "title": "POSTA: A Go-to Framework for Customized Artistic Poster Generation",
      "title_zh": "POSTA：一种用于定制化艺术海报生成的首选框架",
      "authors": [
        "Haoyu Chen",
        "Xiaojie Xu",
        "Wenbo Li",
        "Jingjing Ren",
        "Tian Ye",
        "Songhua Liu",
        "Ying-Cong Chen",
        "Lei Zhu",
        "Xinchao Wang"
      ],
      "abstract": "Poster design is a critical medium for visual communication. Prior work has\nexplored automatic poster design using deep learning techniques, but these\napproaches lack text accuracy, user customization, and aesthetic appeal,\nlimiting their applicability in artistic domains such as movies and\nexhibitions, where both clear content delivery and visual impact are essential.\nTo address these limitations, we present POSTA: a modular framework powered by\ndiffusion models and multimodal large language models (MLLMs) for customized\nartistic poster generation. The framework consists of three modules. Background\nDiffusion creates a themed background based on user input. Design MLLM then\ngenerates layout and typography elements that align with and complement the\nbackground style. Finally, to enhance the poster's aesthetic appeal, ArtText\nDiffusion applies additional stylization to key text elements. The final result\nis a visually cohesive and appealing poster, with a fully modular process that\nallows for complete customization. To train our models, we develop the\nPosterArt dataset, comprising high-quality artistic posters annotated with\nlayout, typography, and pixel-level stylized text segmentation. Our\ncomprehensive experimental analysis demonstrates POSTA's exceptional\ncontrollability and design diversity, outperforming existing models in both\ntext accuracy and aesthetic quality.",
      "tldr_zh": "该论文提出POSTA框架，一种基于diffusion models和多模态大语言模型(MLLMs)的模块化系统，用于生成自定义艺术海报，解决现有方法的文本准确性、用户自定义和美学吸引力不足问题。框架包括三个模块：Background Diffusion根据用户输入创建主题背景、Design MLLM生成匹配的布局和字体元素，以及ArtText Diffusion对关键文本进行额外美化，以提升整体视觉连贯性。为支持训练，该研究开发了PosterArt数据集，包含高品质艺术海报的布局、字体和像素级文本注释。实验结果显示，POSTA在可控性和设计多样性方面优于现有模型，尤其在文本准确性和美学质量上表现出色。",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.GR",
      "comment": "Accepted to CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.14908v1",
      "published_date": "2025-03-19 05:22:38 UTC",
      "updated_date": "2025-03-19 05:22:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:07:46.141105"
    },
    {
      "arxiv_id": "2503.14900v1",
      "title": "Deep Contrastive Unlearning for Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Estrid He",
        "Tabinda Sarwar",
        "Ibrahim Khalil",
        "Xun Yi",
        "Ke Wang"
      ],
      "abstract": "The past a few years have witnessed the great success of large language\nmodels, demonstrating powerful capabilities in comprehending textual data and\ngenerating human-like languages. Large language models achieve success by being\ntrained on vast amounts of textual data, including online sources with\ncopyrighted content and user-generated knowledge. However, this comes at a\ncost: the potential risk of exposing users' privacy and violating copyright\nprotections. Thus, to safeguard individuals' \"right to be forgotten\", there has\nbeen increasing interests in machine unlearning -- the process of removing\ninformation carried by particular training samples from a model while not\ndeteriorating its predictive quality. This is a challenging task due to the\nblack-box nature of language models. Most existing studies focus on mitigating\nthe impact of those forgot samples upon a model's outputs, and do not\nexplicitly consider the geometric distributions of samples in the latent space\nof a model. To address this issue, we propose a machine unlearning framework,\nnamed Deep Contrastive Unlearning for fine-Tuning (DeepCUT) language models.\nOur proposed model achieves machine unlearning by directly optimizing the\nlatent space of a model. Comprehensive experiments on real-world datasets\ndemonstrate the effectiveness and efficiency of DeepCUT with consistent and\nsignificant improvement over baseline methods.",
      "tldr_zh": "本研究针对大型语言模型（LLMs）在训练中使用隐私数据和版权内容可能带来的风险，提出了一种机器 unlearning 方法，以保护用户的“right to be forgotten”。作者引入 Deep Contrastive Unlearning for fine-Tuning (DeepCUT) 框架，通过直接优化模型的潜在空间和样本几何分布，实现对特定训练样本的影响移除，同时保持模型预测性能。实验在真实数据集上证明，DeepCUT 比现有基线方法更有效和高效，显著提升了 unlearning 的准确性和实用性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14900v1",
      "published_date": "2025-03-19 04:58:45 UTC",
      "updated_date": "2025-03-19 04:58:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:07:57.314299"
    },
    {
      "arxiv_id": "2503.14895v1",
      "title": "Mitigating Object Hallucinations in MLLMs via Multi-Frequency Perturbations",
      "title_zh": "翻译失败",
      "authors": [
        "Shuo Li",
        "Jiajun Sun",
        "Guodong Zheng",
        "Xiaoran Fan",
        "Yujiong Shen",
        "Yi Lu",
        "Zhiheng Xi",
        "Yuming Yang",
        "Wenming Tan",
        "Tao Ji",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang"
      ],
      "abstract": "Recently, multimodal large language models (MLLMs) have demonstrated\nremarkable performance in visual-language tasks. However, the authenticity of\nthe responses generated by MLLMs is often compromised by object hallucinations.\nWe identify that a key cause of these hallucinations is the model's\nover-susceptibility to specific image frequency features in detecting objects.\nIn this paper, we introduce Multi-Frequency Perturbations (MFP), a simple,\ncost-effective, and pluggable method that leverages both low-frequency and\nhigh-frequency features of images to perturb visual feature representations and\nexplicitly suppress redundant frequency-domain features during inference,\nthereby mitigating hallucinations. Experimental results demonstrate that our\nmethod significantly mitigates object hallucinations across various model\narchitectures. Furthermore, as a training-time method, MFP can be combined with\ninference-time methods to achieve state-of-the-art performance on the CHAIR\nbenchmark.",
      "tldr_zh": "该研究针对多模态大语言模型(MLLMs)中对象幻觉问题，识别出模型对特定图像频率特征过度敏感是主要原因，并提出Multi-Frequency Perturbations (MFP)方法。\nMFP是一种简单且经济实用的技术，通过扰动图像的低频和高频特征来调整视觉表示，并显式抑制冗余频率域特征，从而在推理过程中减少幻觉。\n实验结果表明，该方法在多种模型架构上显著降低了对象幻觉发生率，作为训练时方法时，还能与推理时策略结合，在CHAIR benchmark上实现最先进性能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14895v1",
      "published_date": "2025-03-19 04:39:45 UTC",
      "updated_date": "2025-03-19 04:39:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:08:09.975202"
    },
    {
      "arxiv_id": "2503.14891v1",
      "title": "MetaLadder: Ascending Mathematical Solution Quality via Analogical-Problem Reasoning Transfer",
      "title_zh": "MetaLadder：通过类比问题推理转移提升数学解决方案质量",
      "authors": [
        "Honglin Lin",
        "Zhuoshi Pan",
        "Yu Li",
        "Qizhi Pei",
        "Xin Gao",
        "Mengzhang Cai",
        "Conghui He",
        "Lijun Wu"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated promising capabilities in\nsolving mathematical reasoning tasks, leveraging Chain-of-Thought (CoT) data as\na vital component in guiding answer generation. Current paradigms typically\ngenerate CoT and answers directly for a given problem, diverging from human\nproblem-solving strategies to some extent. Humans often solve problems by\nrecalling analogous cases and leveraging their solutions to reason about the\ncurrent task. Inspired by this cognitive process, we propose\n\\textbf{MetaLadder}, a novel framework that explicitly prompts LLMs to recall\nand reflect on meta-problems, those structurally or semantically analogous\nproblems, alongside their CoT solutions before addressing the target problem.\nAdditionally, we introduce a problem-restating mechanism to enhance the model's\ncomprehension of the target problem by regenerating the original question,\nwhich further improves reasoning accuracy. Therefore, the model can achieve\nreasoning transfer from analogical problems, mimicking human-like \"learning\nfrom examples\" and generalization abilities. Extensive experiments on\nmathematical benchmarks demonstrate that our MetaLadder significantly boosts\nLLMs' problem-solving accuracy, largely outperforming standard CoT-based\nmethods (\\textbf{10.3\\%} accuracy gain) and other methods. Our code and data\nhas been released at https://github.com/LHL3341/MetaLadder.",
      "tldr_zh": "本研究提出 MetaLadder 框架，旨在提升 Large Language Models (LLMs) 在数学推理任务中的解决方案质量，通过从结构或语义相似的元问题（meta-problems）中转移 Chain-of-Thought (CoT) 推理，模仿人类的“从例子中学习”策略。\n框架包括提示 LLMs 回忆和反思这些类比问题的解决方案，以及引入问题重述机制来重新表述目标问题，从而增强模型的理解和泛化能力。\n实验在数学基准测试中表明，MetaLadder 比标准 CoT 方法提高了 10.3% 的准确率，并显著优于其他方法。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14891v1",
      "published_date": "2025-03-19 04:36:35 UTC",
      "updated_date": "2025-03-19 04:36:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:08:22.889468"
    },
    {
      "arxiv_id": "2503.14883v3",
      "title": "Envisioning an AI-Enhanced Mental Health Ecosystem",
      "title_zh": "展望一个AI增强的心理健康生态系统",
      "authors": [
        "Kellie Yu Hui Sim",
        "Kenny Tsu Wei Choo"
      ],
      "abstract": "The rapid advancement of Large Language Models (LLMs), reasoning models, and\nagentic AI approaches coincides with a growing global mental health crisis,\nwhere increasing demand has not translated into adequate access to professional\nsupport, particularly for underserved populations. This presents a unique\nopportunity for AI to complement human-led interventions, offering scalable and\ncontext-aware support while preserving human connection in this sensitive\ndomain. We explore various AI applications in peer support, self-help\ninterventions, proactive monitoring, and data-driven insights, using a\nhuman-centred approach that ensures AI supports rather than replaces human\ninteraction. However, AI deployment in mental health fields presents challenges\nsuch as ethical concerns, transparency, privacy risks, and risks of\nover-reliance. We propose a hybrid ecosystem where where AI assists but does\nnot replace human providers, emphasising responsible deployment and evaluation.\nWe also present some of our early work and findings in several of these AI\napplications. Finally, we outline future research directions for refining\nAI-enhanced interventions while adhering to ethical and culturally sensitive\nguidelines.",
      "tldr_zh": "这篇论文探讨了 Large Language Models (LLMs)、推理模型和代理 AI 在应对全球心理健康危机的潜力，强调 AI 可以提供可扩展的、上下文感知支持来补充人类干预，同时保持人文联系。作者提出一个以人为本的混合生态系统，涵盖 AI 在同行支持、自我帮助干预、主动监测和数据驱动洞察中的应用，以确保 AI 辅助而非取代专业提供者，并解决伦理问题、透明度、隐私风险和过度依赖等挑战。论文分享了早期研究发现，并概述了未来方向，聚焦于负责任的 AI 部署和符合伦理及文化敏感性的改进。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "H.5.0"
      ],
      "primary_category": "cs.HC",
      "comment": "5 pages, 0 figures, accepted to the CHI '25 Workshop on Envisioning\n  the Future of Interactive Health, to be published in HAL",
      "pdf_url": "http://arxiv.org/pdf/2503.14883v3",
      "published_date": "2025-03-19 04:21:38 UTC",
      "updated_date": "2025-04-01 18:11:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:08:33.550819"
    },
    {
      "arxiv_id": "2503.14881v1",
      "title": "Exploring the Limits of KV Cache Compression in Visual Autoregressive Transformers",
      "title_zh": "探索 KV 缓存压缩在视觉自回归 Transformer 中的极限",
      "authors": [
        "Bo Chen",
        "Xiaoyu Li",
        "Yekun Ke",
        "Yingyu Liang",
        "Zhenmei Shi",
        "Zhao Song"
      ],
      "abstract": "A fundamental challenge in Visual Autoregressive models is the substantial\nmemory overhead required during inference to store previously generated\nrepresentations. Despite various attempts to mitigate this issue through\ncompression techniques, prior works have not explicitly formalized the problem\nof KV-cache compression in this context. In this work, we take the first step\nin formally defining the KV-cache compression problem for Visual Autoregressive\ntransformers. We then establish a fundamental negative result, proving that any\nmechanism for sequential visual token generation under attention-based\narchitectures must use at least $\\Omega(n^2 d)$ memory, when $d = \\Omega(\\log\nn)$, where $n$ is the number of tokens generated and $d$ is the embedding\ndimensionality. This result demonstrates that achieving truly sub-quadratic\nmemory usage is impossible without additional structural constraints. Our proof\nis constructed via a reduction from a computational lower bound problem,\nleveraging randomized embedding techniques inspired by dimensionality reduction\nprinciples. Finally, we discuss how sparsity priors on visual representations\ncan influence memory efficiency, presenting both impossibility results and\npotential directions for mitigating memory overhead.",
      "tldr_zh": "本文探讨了视觉自回归变压器中 KV-cache 压缩的极限，针对推理过程中存储先前生成的表示所带来的大量内存开销问题。作者首先正式定义了 KV-cache 压缩问题，并证明了任何基于注意力的序列视觉标记生成机制在 d = Ω(log n) 时，需要至少 Ω(n² d) 的内存，其中 n 是生成的标记数，d 是嵌入维度。证明通过从计算下界问题归约而来，并利用随机嵌入技术。该结果表明，在没有额外结构约束的情况下，实现真正次二次内存使用是不可能的；同时，论文讨论了视觉表示的稀疏性先验如何影响内存效率，并提出潜在缓解方向。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14881v1",
      "published_date": "2025-03-19 04:18:57 UTC",
      "updated_date": "2025-03-19 04:18:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:08:46.254116"
    },
    {
      "arxiv_id": "2503.14868v1",
      "title": "Efficient Personalization of Quantized Diffusion Model without Backpropagation",
      "title_zh": "翻译失败",
      "authors": [
        "Hoigi Seo",
        "Wongi Jeong",
        "Kyungryeol Lee",
        "Se Young Chun"
      ],
      "abstract": "Diffusion models have shown remarkable performance in image synthesis, but\nthey demand extensive computational and memory resources for training,\nfine-tuning and inference. Although advanced quantization techniques have\nsuccessfully minimized memory usage for inference, training and fine-tuning\nthese quantized models still require large memory possibly due to\ndequantization for accurate computation of gradients and/or backpropagation for\ngradient-based algorithms. However, memory-efficient fine-tuning is\nparticularly desirable for applications such as personalization that often must\nbe run on edge devices like mobile phones with private data. In this work, we\naddress this challenge by quantizing a diffusion model with personalization via\nTextual Inversion and by leveraging a zeroth-order optimization on\npersonalization tokens without dequantization so that it does not require\ngradient and activation storage for backpropagation that consumes considerable\nmemory. Since a gradient estimation using zeroth-order optimization is quite\nnoisy for a single or a few images in personalization, we propose to denoise\nthe estimated gradient by projecting it onto a subspace that is constructed\nwith the past history of the tokens, dubbed Subspace Gradient. In addition, we\ninvestigated the influence of text embedding in image generation, leading to\nour proposed time steps sampling, dubbed Partial Uniform Timestep Sampling for\nsampling with effective diffusion timesteps. Our method achieves comparable\nperformance to prior methods in image and text alignment scores for\npersonalizing Stable Diffusion with only forward passes while reducing training\nmemory demand up to $8.2\\times$.",
      "tldr_zh": "该论文针对扩散模型（Diffusion Models）在图像合成中的高内存需求问题，提出了一种无需反向传播（Backpropagation）的量化模型个性化方法，通过Textual Inversion和零阶优化（Zeroth-order Optimization）在量化模型上进行个性化训练，避免了反量化及其梯度存储。作者引入Subspace Gradient技术，将估计梯度投射到历史标记子空间以减少噪声，并提出Partial Uniform Timestep Sampling来优化文本嵌入对图像生成的影响。实验结果显示，该方法在个性化Stable Diffusion时，仅使用前向传播即可实现与现有方法相当的图像和文本对齐分数，同时将训练内存需求降低高达8.2倍。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14868v1",
      "published_date": "2025-03-19 03:45:37 UTC",
      "updated_date": "2025-03-19 03:45:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:08:59.932136"
    },
    {
      "arxiv_id": "2503.14858v2",
      "title": "1000 Layer Networks for Self-Supervised RL: Scaling Depth Can Enable New Goal-Reaching Capabilities",
      "title_zh": "翻译失败",
      "authors": [
        "Kevin Wang",
        "Ishaan Javali",
        "Michał Bortkiewicz",
        "Tomasz Trzciński",
        "Benjamin Eysenbach"
      ],
      "abstract": "Scaling up self-supervised learning has driven breakthroughs in language and\nvision, yet comparable progress has remained elusive in reinforcement learning\n(RL). In this paper, we study building blocks for self-supervised RL that\nunlock substantial improvements in scalability, with network depth serving as a\ncritical factor. Whereas most RL papers in recent years have relied on shallow\narchitectures (around 2 - 5 layers), we demonstrate that increasing the depth\nup to 1024 layers can significantly boost performance. Our experiments are\nconducted in an unsupervised goal-conditioned setting, where no demonstrations\nor rewards are provided, so an agent must explore (from scratch) and learn how\nto maximize the likelihood of reaching commanded goals. Evaluated on simulated\nlocomotion and manipulation tasks, our approach increases performance by\n$2\\times$ - $50\\times$. Increasing the model depth not only increases success\nrates but also qualitatively changes the behaviors learned.",
      "tldr_zh": "这篇论文探讨了在自监督强化学习(Self-Supervised RL)中增加网络深度的重要性，通过构建高达1024层的深度网络来提升代理的目标达成能力。作者在无监督的目标条件环境中进行实验，代理无需演示或奖励，从零开始探索并学习最大化达到目标的概率。在模拟的运动和操作任务上，该方法使性能提升了2倍到50倍，不仅提高了成功率，还改变了代理的行为模式。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Link to project website:\n  https://wang-kevin3290.github.io/scaling-crl/",
      "pdf_url": "http://arxiv.org/pdf/2503.14858v2",
      "published_date": "2025-03-19 03:33:57 UTC",
      "updated_date": "2025-03-22 22:24:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:09:12.747863"
    },
    {
      "arxiv_id": "2503.14847v1",
      "title": "Project Jenkins: Turning Monkey Neural Data into Robotic Arm Movement, and Back",
      "title_zh": "翻译失败",
      "authors": [
        "Andrii Zahorodnii",
        "Dima Yanovsky"
      ],
      "abstract": "Project Jenkins explores how neural activity in the brain can be decoded into\nrobotic movement and, conversely, how movement patterns can be used to generate\nsynthetic neural data. Using real neural data recorded from motor and premotor\ncortex areas of a macaque monkey named Jenkins, we develop models for decoding\n(converting brain signals into robotic arm movements) and encoding (simulating\nbrain activity corresponding to a given movement). For the interface between\nthe brain simulation and the physical world, we utilized Koch v1.1 leader and\nfollower robotic arms. We developed an interactive web console that allows\nusers to generate synthetic brain data from joystick movements in real time.\nOur results are a step towards brain-controlled robotics, prosthetics, and\nenhancing normal motor function. By accurately modeling brain activity, we take\na step toward flexible brain-computer interfaces that generalize beyond\npredefined movements. To support the research community, we provide open source\ntools for both synthetic data generation and neural decoding, fostering\nreproducibility and accelerating progress. The project is available at\nhttps://www.808robots.com/projects/jenkins",
      "tldr_zh": "Project Jenkins 研究如何将猕猴大脑神经数据解码成机器人臂动作（decoding），以及反向将动作生成合成神经数据（encoding）。该项目使用来自猕猴 Jenkins 的运动和前运动皮层真实神经数据，开发了模型并结合 Koch v1.1 机器人臂和一个交互式 web 控制台，实现实时模拟和控制。结果显示，该方法有助于脑控机器人（brain-controlled robotics）、假肢（prosthetics）和增强正常运动功能，并通过提供开源工具支持研究社区的再现性和进展。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "eess.SP",
        "q-bio.NC"
      ],
      "primary_category": "cs.RO",
      "comment": "6 pages, 5 figures, project webpage and github",
      "pdf_url": "http://arxiv.org/pdf/2503.14847v1",
      "published_date": "2025-03-19 03:12:17 UTC",
      "updated_date": "2025-03-19 03:12:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:09:23.769203"
    },
    {
      "arxiv_id": "2503.14833v1",
      "title": "Curiosity-Diffuser: Curiosity Guide Diffusion Models for Reliability",
      "title_zh": "翻译失败",
      "authors": [
        "Zihao Liu",
        "Xing Liu",
        "Yizhai Zhang",
        "Zhengxiong Liu",
        "Panfeng Huang"
      ],
      "abstract": "One of the bottlenecks in robotic intelligence is the instability of neural\nnetwork models, which, unlike control models, lack a well-defined convergence\ndomain and stability. This leads to risks when applying intelligence in the\nphysical world. Specifically, imitation policy based on neural network may\ngenerate hallucinations, leading to inaccurate behaviors that impact the safety\nof real-world applications. To address this issue, this paper proposes the\nCuriosity-Diffuser, aimed at guiding the conditional diffusion model to\ngenerate trajectories with lower curiosity, thereby improving the reliability\nof policy. The core idea is to use a Random Network Distillation (RND)\ncuriosity module to assess whether the model's behavior aligns with the\ntraining data, and then minimize curiosity by classifier guidance diffusion to\nreduce overgeneralization during inference. Additionally, we propose a\ncomputationally efficient metric for evaluating the reliability of the policy,\nmeasuring the similarity between the generated behaviors and the training\ndataset, to facilitate research about reliability learning. Finally, simulation\nverify the effectiveness and applicability of the proposed method to a variety\nof scenarios, showing that Curiosity-Diffuser significantly improves task\nperformance and produces behaviors that are more similar to the training data.\nThe code for this work is available at: github.com/CarlDegio/Curiosity-Diffuser",
      "tldr_zh": "本研究针对机器人智能中神经网络模型的不稳定性（如缺乏收敛域和稳定性），导致模仿策略产生幻觉（hallucinations）并影响实际应用安全，提出了一种 Curiosity-Diffuser 框架。该框架利用 Random Network Distillation (RND) 好奇心模块评估模型行为与训练数据的契合度，并通过 classifier guidance diffusion 最小化好奇心，从而引导条件扩散模型（conditional diffusion model）生成更可靠的轨迹，减少过度泛化。论文还引入了一个计算高效的可靠性评估指标，测量生成行为与训练数据集的相似性，并在模拟实验中验证了方法的有效性，显著提升了任务性能和行为一致性。代码可从 github.com/CarlDegio/Curiosity-Diffuser 获取。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14833v1",
      "published_date": "2025-03-19 02:25:36 UTC",
      "updated_date": "2025-03-19 02:25:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:09:36.647427"
    },
    {
      "arxiv_id": "2503.14828v1",
      "title": "The CLEF-2025 CheckThat! Lab: Subjectivity, Fact-Checking, Claim Normalization, and Retrieval",
      "title_zh": "翻译失败",
      "authors": [
        "Firoj Alam",
        "Julia Maria Struß",
        "Tanmoy Chakraborty",
        "Stefan Dietze",
        "Salim Hafid",
        "Katerina Korre",
        "Arianna Muti",
        "Preslav Nakov",
        "Federico Ruggeri",
        "Sebastian Schellhammer",
        "Vinay Setty",
        "Megha Sundriyal",
        "Konstantin Todorov",
        "Venktesh V"
      ],
      "abstract": "The CheckThat! lab aims to advance the development of innovative technologies\ndesigned to identify and counteract online disinformation and manipulation\nefforts across various languages and platforms. The first five editions focused\non key tasks in the information verification pipeline, including\ncheck-worthiness, evidence retrieval and pairing, and verification. Since the\n2023 edition, the lab has expanded its scope to address auxiliary tasks that\nsupport research and decision-making in verification. In the 2025 edition, the\nlab revisits core verification tasks while also considering auxiliary\nchallenges. Task 1 focuses on the identification of subjectivity (a follow-up\nfrom CheckThat! 2024), Task 2 addresses claim normalization, Task 3 targets\nfact-checking numerical claims, and Task 4 explores scientific web discourse\nprocessing. These tasks present challenging classification and retrieval\nproblems at both the document and span levels, including multilingual settings.",
      "tldr_zh": "CLEF-2025 CheckThat! 实验室旨在开发创新技术来识别和对抗在线虚假信息及操纵行为，支持多语言和多种平台。实验室从早期焦点于信息验证管道的核心任务（如检查价值、证据检索和配对）扩展到辅助挑战，自2023年起增加相关研究支持。2025年版本包括四个主要任务：Task 1 识别主观性（Subjectivity），Task 2 处理声明规范化（Claim Normalization），Task 3 针对事实检查数字声明（Fact-Checking numerical claims），以及Task 4 探索科学网络话语处理。整体任务涉及文档和跨度级别的分类及检索问题，包括多语言设置，从而推动验证决策的进步。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "68T50",
        "I.2; I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "misinformation, factuality, fact-checking, fact-checkers,\n  check-worthiness, Social Media Platforms",
      "pdf_url": "http://arxiv.org/pdf/2503.14828v1",
      "published_date": "2025-03-19 02:06:07 UTC",
      "updated_date": "2025-03-19 02:06:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:09:48.233195"
    },
    {
      "arxiv_id": "2503.14827v1",
      "title": "MMDT: Decoding the Trustworthiness and Safety of Multimodal Foundation Models",
      "title_zh": "MMDT：解读多模态基础模型的信任性和安全性",
      "authors": [
        "Chejian Xu",
        "Jiawei Zhang",
        "Zhaorun Chen",
        "Chulin Xie",
        "Mintong Kang",
        "Yujin Potter",
        "Zhun Wang",
        "Zhuowen Yuan",
        "Alexander Xiong",
        "Zidi Xiong",
        "Chenhui Zhang",
        "Lingzhi Yuan",
        "Yi Zeng",
        "Peiyang Xu",
        "Chengquan Guo",
        "Andy Zhou",
        "Jeffrey Ziwei Tan",
        "Xuandong Zhao",
        "Francesco Pinto",
        "Zhen Xiang",
        "Yu Gai",
        "Zinan Lin",
        "Dan Hendrycks",
        "Bo Li",
        "Dawn Song"
      ],
      "abstract": "Multimodal foundation models (MMFMs) play a crucial role in various\napplications, including autonomous driving, healthcare, and virtual assistants.\nHowever, several studies have revealed vulnerabilities in these models, such as\ngenerating unsafe content by text-to-image models. Existing benchmarks on\nmultimodal models either predominantly assess the helpfulness of these models,\nor only focus on limited perspectives such as fairness and privacy. In this\npaper, we present the first unified platform, MMDT (Multimodal DecodingTrust),\ndesigned to provide a comprehensive safety and trustworthiness evaluation for\nMMFMs. Our platform assesses models from multiple perspectives, including\nsafety, hallucination, fairness/bias, privacy, adversarial robustness, and\nout-of-distribution (OOD) generalization. We have designed various evaluation\nscenarios and red teaming algorithms under different tasks for each perspective\nto generate challenging data, forming a high-quality benchmark. We evaluate a\nrange of multimodal models using MMDT, and our findings reveal a series of\nvulnerabilities and areas for improvement across these perspectives. This work\nintroduces the first comprehensive and unique safety and trustworthiness\nevaluation platform for MMFMs, paving the way for developing safer and more\nreliable MMFMs and systems. Our platform and benchmark are available at\nhttps://mmdecodingtrust.github.io/.",
      "tldr_zh": "本文提出 MMDT（Multimodal DecodingTrust），一个统一的平台，用于全面评估多模态基础模型（MMFMs）的安全性和可信度，以弥补现有基准的局限性。MMDT 从多个视角评估模型，包括 safety、hallucination、fairness/bias、privacy、adversarial robustness 和 out-of-distribution (OOD) generalization，通过设计各种评估场景和 red teaming 算法生成高质量基准。实验结果显示，评估揭示了 MMFMs 的系列漏洞和改进领域，为开发更安全可靠的 MMFMs 系统铺平道路。该平台和基准已公开可用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CL",
      "comment": "ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.14827v1",
      "published_date": "2025-03-19 01:59:44 UTC",
      "updated_date": "2025-03-19 01:59:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:10:00.436200"
    },
    {
      "arxiv_id": "2503.14809v1",
      "title": "Learning with Expert Abstractions for Efficient Multi-Task Continuous Control",
      "title_zh": "利用专家抽象进行高效多任务连续控制学习",
      "authors": [
        "Jeff Jewett",
        "Sandhya Saisubramanian"
      ],
      "abstract": "Decision-making in complex, continuous multi-task environments is often\nhindered by the difficulty of obtaining accurate models for planning and the\ninefficiency of learning purely from trial and error. While precise environment\ndynamics may be hard to specify, human experts can often provide high-fidelity\nabstractions that capture the essential high-level structure of a task and user\npreferences in the target environment. Existing hierarchical approaches often\ntarget discrete settings and do not generalize across tasks. We propose a\nhierarchical reinforcement learning approach that addresses these limitations\nby dynamically planning over the expert-specified abstraction to generate\nsubgoals to learn a goal-conditioned policy. To overcome the challenges of\nlearning under sparse rewards, we shape the reward based on the optimal state\nvalue in the abstract model. This structured decision-making process enhances\nsample efficiency and facilitates zero-shot generalization. Our empirical\nevaluation on a suite of procedurally generated continuous control environments\ndemonstrates that our approach outperforms existing hierarchical reinforcement\nlearning methods in terms of sample efficiency, task completion rate,\nscalability to complex tasks, and generalization to novel scenarios.",
      "tldr_zh": "该研究针对复杂连续多任务环境的决策挑战，提出了一种基于专家抽象的分层强化学习方法，通过动态规划在专家提供的抽象上生成子goals，并学习goal-conditioned policy，以提高效率。针对稀疏rewards问题，该方法利用抽象模型的最优状态值来塑造奖励，从而提升样本效率和zero-shot generalization。实验结果显示，在一系列程序生成的连续控制环境中，该方法在样本效率、任务完成率、可扩展性和泛化能力方面均优于现有分层强化学习方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages, 6 figures. Submitted to RLC 2025. Code and experiments at\n  https://github.com/Intelligent-Reliable-Autonomous-Systems/gcrs-expert-abstractions",
      "pdf_url": "http://arxiv.org/pdf/2503.14809v1",
      "published_date": "2025-03-19 00:44:23 UTC",
      "updated_date": "2025-03-19 00:44:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:10:12.910148"
    },
    {
      "arxiv_id": "2503.14800v1",
      "title": "Long Context Modeling with Ranked Memory-Augmented Retrieval",
      "title_zh": "翻译失败",
      "authors": [
        "Ghadir Alselwi",
        "Hao Xue",
        "Shoaib Jameel",
        "Basem Suleiman",
        "Flora D. Salim",
        "Imran Razzak"
      ],
      "abstract": "Effective long-term memory management is crucial for language models handling\nextended contexts. We introduce a novel framework that dynamically ranks memory\nentries based on relevance. Unlike previous works, our model introduces a novel\nrelevance scoring and a pointwise re-ranking model for key-value embeddings,\ninspired by learning-to-rank techniques in information retrieval. Enhanced\nRanked Memory Augmented Retrieval ERMAR achieves state-of-the-art results on\nstandard benchmarks.",
      "tldr_zh": "该研究提出了一种新框架，用于语言模型处理长上下文时的有效长期记忆管理，通过动态排名内存条目基于相关性。不同于以往工作，该框架引入了创新的相关性评分和点式重新排名模型（pointwise re-ranking），针对关键-值嵌入，灵感来源于信息检索中的学习到排名（learning-to-rank）技术。增强的Ranked Memory Augmented Retrieval (ERMAR) 在标准基准测试中实现了最先进的结果。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14800v1",
      "published_date": "2025-03-19 00:24:01 UTC",
      "updated_date": "2025-03-19 00:24:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:10:23.850830"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 97,
  "processed_papers_count": 97,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-24T04:10:44.827358"
}
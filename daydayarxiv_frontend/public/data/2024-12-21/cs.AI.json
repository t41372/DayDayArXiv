{
  "date": "2024-12-21",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-12-21 的 arXiv 中文 TLDR 快报！今天 arXiv 的论文主要聚焦 AI 安全、生成模型（如 LLM 和扩散模型）的优化与应用、时间序列预测、医疗图像处理等领域。其中，OpenAI 的 o1 System Card 模型评估最为引人注目，展示了 LLM 在安全和推理方面的进展；同时，AI 攻击防御和 LLM 偏见分析等话题度高的论文值得关注，涉及知名学者如 Yves Lespérance 的因果推理工作。\n\n下面，我将挑选并简要讨论今天的重点论文，先聊那些重要、话题性强或有潜在影响的（如 AI 安全、LLM 应用和医疗 AI），然后快速掠过其他较常规的。每个条目列出论文标题（中文 + 英文），并突出核心贡献和发现。\n\n### AI 安全与攻击防御（高话题度领域）\n- **POEX: Understanding and Mitigating Policy Executable Jailbreak Attacks against Embodied AI（POEX: 理解和缓解针对具身 AI 的策略可执行越狱攻击）**  \n  这篇论文探讨了具身 AI 系统（如机器人）面对间接提示注入攻击的漏洞。核心贡献是通过优化对抗后缀生成可执行的恶意策略，同时提出 POEX 框架来检测和缓解攻击。发现显示，该方法在实验中显著降低了攻击成功率，同时保持任务性能，强调了 AI 代理在现实应用中的安全风险。\n\n- **DCOR: Anomaly Detection in Attributed Networks via Dual Contrastive Learning Reconstruction（DCOR: 通过双对比学习重构在属性网络中的异常检测）**  \n  作者 Hossein Rafieizadeh 等提出 DCOR 方法，使用图神经网络（GNN）和对比学习检测网络异常（如欺诈）。主要发现是，该方法在基准数据集上超越了现有技术，强调了属性数据在异常检测中的作用，并已在复杂网络会议上被接受。\n\n- **TrojFlow: Flow Models are Natural Targets for Trojan Attacks（TrojFlow: 流动模型是 Trojan 攻击的自然目标）**  \n  论文分析了基于流动模型的生成系统（如扩散模型）的漏洞，提出 TrojFlow 攻击框架。贡献在于证明攻击可以高效破坏模型，同时测试了防御机制的局限性。发现显示，这种攻击易于实施，提醒了生成模型的安全隐患。\n\n- **The Task Shield: Enforcing Task Alignment to Defend Against Indirect Prompt Injection in LLM Agents（任务盾牌: 通过强制任务对齐防御 LLM 代理中的间接提示注入）**  \n  这篇工作针对 LLM 代理的间接注入攻击，提出 Task Shield 机制来验证指令是否符合用户目标。核心发现是，该方法在实验中将攻击成功率降至 2.07%，而保持 69.79% 的任务效用，展示了 LLM 安全的新视角。\n\n其他 AI 安全相关论文（如 Adversarial Attack Against Images Classification）快速提一下：它们探讨图像分类的对抗攻击，但贡献较常规，仅通过 GAN 生成对抗样本，提升了模型鲁棒性分析。\n\n### LLM 与生成模型优化（令人印象深刻的领域）\n- **OpenAI o1 System Card（OpenAI o1 系统卡）**  \n  OpenAI 团队发布 o1 系列模型的安全评估，焦点是强化学习提升推理能力。贡献包括通过“审议式对齐”改善模型对潜在风险的响应，如生成非法建议。发现显示，该模型在基准测试中领先，但也强调了智能提升带来的新风险，是今日最有影响力的论文。\n\n- **Beyond Partisan Leaning: A Comparative Analysis of Political Bias in Large Language Models（超越党派倾向: 大语言模型政治偏见的比较分析）**  \n  作者 Tai-Quan Peng 等分析 LLM 的政治偏见，使用无角色提示框架。核心贡献是提出熵加权偏见分数和行为聚类，发现大多数模型偏向中左翼，并强调模型规模和开源性对偏见的影响。该论文在 NeurIPS 研讨会中被接受，讨论了 LLM 在社会应用中的伦理问题。\n\n- **Lillama: Large Language Models Compression via Low-Rank Feature Distillation（Lillama: 通过低秩特征蒸馏压缩大语言模型）**  \n  论文提出 Lillama 方法，使用 SVD 初始化和联合损失压缩 LLM。贡献在于快速压缩模型（如 Mixtral-8x7B），移除 10 亿参数却保留 95% 性能。发现显示，该方法在单 GPU 上高效运行，适用于资源受限场景。\n\n- **TimeRAG: BOOSTING LLM Time Series Forecasting via Retrieval-Augmented Generation（TimeRAG: 通过检索增强生成提升 LLM 时间序列预测）**  \n  作者 Silin Yang 等引入 TimeRAG 框架，使用检索增强生成改善 LLM 的时间序列预测。核心发现是，该方法在多领域数据集上平均提升 2.97% 准确率，无需额外训练数据，展示了 RAG 在预测任务中的潜力。\n\n其他 LLM 相关（如 Argumentation Computation with Large Language Models）快速掠过：它们探索 LLM 在抽象论证中的能力，但实验结果显示解释性提升有限。\n\n### 医疗图像与时间序列处理（应用价值高）\n- **From Histopathology Images to Cell Clouds: Learning Slide Representations with Hierarchical Cell Transformer（从组织病理图像到细胞云: 使用分层细胞 Transformer 学习幻灯片表示）**  \n  论文构建 WSI-Cell5B 数据集（50 亿细胞标注），并提出 CCFormer 模型。贡献在于通过分层空间感知模块分析细胞分布，用于癌症诊断。发现显示，该方法在生存预测和癌症分期上达到 SOTA 性能，突出了细胞空间分布在医疗 AI 中的作用。\n\n- **Multi-atlas Ensemble Graph Neural Network Model For Major Depressive Disorder Detection Using Functional MRI Data（多图谱集成图神经网络模型用于功能 MRI 数据的重度抑郁症检测）**  \n  作者 Nojod M. Alotaibi 等开发集成 GNN 模型检测抑郁症。核心贡献是使用多脑区图谱捕捉 fMRI 特征，实现 75.80% 准确率。发现显示，该方法在多站点数据上有效，提升了精神健康诊断的准确性。\n\n其他医疗相关（如 Deep Learning for Spatio-Temporal Fusion）快速提一下：它们优化 LST 估计，但更多是技术改进，实际影响较小。\n\n### 其他领域快速概述\n今天还有一些论文涉及时间序列预测（如 Coupling Neural Networks and Physics Equations）和图神经网络（如 Towards Graph Foundation Models），但它们贡献较常规，例如前者结合物理方程提升电池状态预测，后者通过任务树学习图泛化。它们在实验中显示了改进，但未有突破性发现，故从简。\n\n总之，今天的 arXiv 论文突出了 AI 安全和模型优化的紧迫性，OpenAI 的工作尤其值得跟踪。未来几天，关注这些领域的后续发展！（全文控制在适中篇幅，聚焦核心。）",
  "papers": [
    {
      "arxiv_id": "2412.16791v1",
      "title": "Enhancing web traffic attacks identification through ensemble methods and feature selection",
      "title_zh": "通过集成方法和特征选择增强网站流量攻击识别",
      "authors": [
        "Daniel Urda",
        "Branly Martínez",
        "Nuño Basurto",
        "Meelis Kull",
        "Ángel Arroyo",
        "Álvaro Herrero"
      ],
      "abstract": "Websites, as essential digital assets, are highly vulnerable to cyberattacks\nbecause of their high traffic volume and the significant impact of breaches.\nThis study aims to enhance the identification of web traffic attacks by\nleveraging machine learning techniques. A methodology was proposed to extract\nrelevant features from HTTP traces using the CSIC2010 v2 dataset, which\nsimulates e-commerce web traffic. Ensemble methods, such as Random Forest and\nExtreme Gradient Boosting, were employed and compared against baseline\nclassifiers, including k-nearest Neighbor, LASSO, and Support Vector Machines.\nThe results demonstrate that the ensemble methods outperform baseline\nclassifiers by approximately 20% in predictive accuracy, achieving an Area\nUnder the ROC Curve (AUC) of 0.989. Feature selection methods such as\nInformation Gain, LASSO, and Random Forest further enhance the robustness of\nthese models. This study highlights the efficacy of ensemble models in\nimproving attack detection while minimizing performance variability, offering a\npractical framework for securing web traffic in diverse application contexts.",
      "tldr_zh": "本研究旨在通过机器学习技术提升 web 流量攻击识别能力，以应对网站的高流量和潜在安全风险。研究团队从 CSIC2010 v2 数据集提取 HTTP 痕迹特征，并采用 Ensemble methods 如 Random Forest 和 Extreme Gradient Boosting，与基线分类器（如 k-nearest Neighbor、LASSO 和 Support Vector Machines）进行比较，同时应用 Feature selection 方法（如 Information Gain、LASSO 和 Random Forest）来优化模型。结果显示，Ensemble methods 比基线分类器提高了约 20% 的预测准确率，达到 AUC 为 0.989 的水平。该框架突出了集成模型在增强攻击检测和减少性能变异方面的功效，为 web 流量安全提供了实用解决方案。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.16791v1",
      "published_date": "2024-12-21 22:13:30 UTC",
      "updated_date": "2024-12-21 22:13:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:03:49.331092"
    },
    {
      "arxiv_id": "2412.16788v2",
      "title": "DCOR: Anomaly Detection in Attributed Networks via Dual Contrastive Learning Reconstruction",
      "title_zh": "DCOR：",
      "authors": [
        "Hossein Rafieizadeh",
        "Hadi Zare",
        "Mohsen Ghassemi Parsa",
        "Hadi Davardoust",
        "Meshkat Shariat Bagheri"
      ],
      "abstract": "Anomaly detection using a network-based approach is one of the most efficient\nways to identify abnormal events such as fraud, security breaches, and system\nfaults in a variety of applied domains. While most of the earlier works address\nthe complex nature of graph-structured data and predefined anomalies, the\nimpact of data attributes and emerging anomalies are often neglected. This\npaper introduces DCOR, a novel approach on attributed networks that integrates\nreconstruction-based anomaly detection with Contrastive Learning. Utilizing a\nGraph Neural Network (GNN) framework, DCOR contrasts the reconstructed\nadjacency and feature matrices from both the original and augmented graphs to\ndetect subtle anomalies. We employed comprehensive experimental studies on\nbenchmark datasets through standard evaluation measures. The results show that\nDCOR significantly outperforms state-of-the-art methods. Obtained results\ndemonstrate the efficacy of proposed approach in attributed networks with the\npotential of uncovering new patterns of anomalies.",
      "tldr_zh": "本文提出DCOR，一种用于属性网络（Attributed Networks）中异常检测（Anomaly Detection）的新方法，通过整合重建技术和双对比学习（Dual Contrastive Learning Reconstruction）来识别微妙异常。DCOR采用Graph Neural Network (GNN)框架，对原始和增强图的邻接矩阵及特征矩阵进行对比重建，从而有效捕捉数据属性和新兴异常模式。实验结果表明，DCOR在基准数据集上显著优于现有最先进方法，并展示了其在发现新异常模式方面的潜力。",
      "categories": [
        "cs.AI",
        "05C82 05C82 05C82 05C82",
        "I.2.6; G.2.2"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at the Thirteenth International Conference on Complex\n  Networks and Their Applications",
      "pdf_url": "http://arxiv.org/pdf/2412.16788v2",
      "published_date": "2024-12-21 22:02:06 UTC",
      "updated_date": "2025-01-20 20:17:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:04:01.764998"
    },
    {
      "arxiv_id": "2412.16772v1",
      "title": "Assessing Social Alignment: Do Personality-Prompted Large Language Models Behave Like Humans?",
      "title_zh": "评估社会对齐：人格提示的大型语言模型是否像人类一样行为？",
      "authors": [
        "Ivan Zakazov",
        "Mikolaj Boronski",
        "Lorenzo Drudi",
        "Robert West"
      ],
      "abstract": "The ongoing revolution in language modelling has led to various novel\napplications, some of which rely on the emerging \"social abilities\" of large\nlanguage models (LLMs). Already, many turn to the new \"cyber friends\" for\nadvice during pivotal moments of their lives and trust them with their deepest\nsecrets, implying that accurate shaping of LLMs' \"personalities\" is paramount.\nLeveraging the vast diversity of data on which LLMs are pretrained,\nstate-of-the-art approaches prompt them to adopt a particular personality. We\nask (i) if personality-prompted models behave (i.e. \"make\" decisions when\npresented with a social situation) in line with the ascribed personality, and\n(ii) if their behavior can be finely controlled. We use classic psychological\nexperiments - the Milgram Experiment and the Ultimatum Game - as social\ninteraction testbeds and apply personality prompting to GPT-3.5/4/4o-mini/4o.\nOur experiments reveal failure modes of the prompt-based modulation of the\nmodels' \"behavior\", thus challenging the feasibility of personality prompting\nwith today's LLMs.",
      "tldr_zh": "这篇论文评估了通过个性提示（personality prompting）是否能使大型语言模型（LLMs）在社交情境中表现出与指定人格一致的行为，类似于人类决策。研究者使用经典心理实验，如 Milgram Experiment 和 Ultimatum Game，作为测试框架，对 GPT-3.5/4/4o-mini/4o 模型进行实验。结果揭示了提示调制行为的失败模式，表明当前 LLMs 无法实现精细控制和可靠的社会行为一致性，从而质疑其在实际应用中的可行性。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "Accepted to NeurIPS 2024 Workshop on Behavioral Machine Learning",
      "pdf_url": "http://arxiv.org/pdf/2412.16772v1",
      "published_date": "2024-12-21 20:58:19 UTC",
      "updated_date": "2024-12-21 20:58:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:04:14.013643"
    },
    {
      "arxiv_id": "2412.16768v1",
      "title": "A Comparative Study on Machine Learning Models to Classify Diseases Based on Patient Behaviour and Habits",
      "title_zh": "基于患者行为和习惯的疾病分类机器学习模型比较研究",
      "authors": [
        "Elham Musaaed",
        "Nabil Hewahi",
        "Abdulla Alasaadi"
      ],
      "abstract": "In recent years, ML algorithms have been shown to be useful for predicting\ndiseases based on health data and posed a potential application area for these\nalgorithms such as modeling of diseases. The majority of these applications\nemploy supervised rather than unsupervised ML algorithms. In addition, each\nyear, the amount of data in medical science grows rapidly. Moreover, these data\ninclude clinical and Patient-Related Factors (PRF), such as height, weight,\nage, other physical characteristics, blood sugar, lipids, insulin, etc., all of\nwhich will change continually over time. Analysis of historical data can help\nidentify disease risk factors and their interactions, which is useful for\ndisease diagnosis and prediction. This wealth of valuable information in these\ndata will help doctors diagnose accurately and people can become more aware of\nthe risk factors and key indicators to act proactively. The purpose of this\nstudy is to use six supervised ML approaches to fill this gap by conducting a\ncomprehensive experiment to investigate the correlation between PRF and\nDiabetes, Stroke, Heart Disease (HD), and Kidney Disease (KD). Moreover, it\nwill investigate the link between Diabetes, Stroke, and KD and PRF with HD.\nFurther, the research aims to compare and evaluate various ML algorithms for\nclassifying diseases based on the PRF. Additionally, it aims to compare and\nevaluate ML algorithms for classifying HD based on PRF as well as Diabetes,\nStroke, Asthma, Skin Cancer, and KD as attributes. Lastly, HD predictions will\nbe provided through a Web-based application on the most accurate classifier,\nwhich allows the users to input their values and predict the output.",
      "tldr_zh": "本研究比较了六种监督式ML算法，用于基于患者相关因素（PRF，如身高、体重、年龄等）分类疾病，包括糖尿病、Stroke、Heart Disease (HD) 和 Kidney Disease (KD)，以填补现有研究的空白。研究通过分析历史医疗数据，调查PRF与这些疾病的相关性，并评估算法在疾病预测中的性能。结果表明，某些ML算法在分类准确性上表现出色，特别是用于HD预测，并开发了一个Web应用，允许用户输入PRF值进行预测，从而帮助医生诊断和提升公众预防意识。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.16768v1",
      "published_date": "2024-12-21 20:46:40 UTC",
      "updated_date": "2024-12-21 20:46:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:04:25.842950"
    },
    {
      "arxiv_id": "2412.16766v2",
      "title": "A Protocol for KG Construction Tasks Involving Users",
      "title_zh": "一种涉及用户的知识图谱构建任务协议",
      "authors": [
        "Ademar Crotti Junior",
        "Christophe Debruyne"
      ],
      "abstract": "Knowledge graph construction (KGC) from (semi-)structured data is\nchallenging, and facilitating user involvement is an issue frequently brought\nup within this community. We cannot deny the progress we have made with respect\nto (declarative) knowledge graph construction languages and tools to help build\nsuch mappings. However, it is surprising that no two studies report on similar\nprotocols. This heterogeneity does not allow for comparing KGC languages,\ntechniques, and tools. This paper first analyses studies involving users to\nidentify the points of comparison. These gaps include a lack of systematic\nconsistency in task design, participant selection, and evaluation metrics.\nMoreover, there needs to be a systematic way of analyzing the data and\nreporting the findings, which is also lacking. We thus propose and introduce a\nuser protocol for KGC designed to address this challenge. Where possible, we\ndraw and take elements from the literature we deem fit for such a protocol. The\nprotocol, as such, allows for the comparison of languages and techniques for\nthe RDF Mapping Language (RML) core functionality, which is covered by most of\nthe other state-of-the-art techniques and tools. We also propose how the\nprotocol can be amended to compare extensions (of RML). This protocol provides\nan important step towards a more comparable evaluation of KGC user studies.",
      "tldr_zh": "本论文分析了知识图谱构建 (KGC) 从半结构化数据中涉及用户的研究存在异质性问题，包括任务设计、参与者选择和评价指标的不一致，导致无法比较不同 KGC 语言、技术和工具。作者提出一个标准化用户协议，借鉴现有文献，专注于 RDF Mapping Language (RML) 的核心功能，并提供扩展该协议以比较 RML 扩展的方法。该协议通过系统化数据分析和报告方式，填补了现有研究的空白，促进 KGC 用户研究的更可比评估。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.HC",
      "comment": "For associated repository, see\n  https://github.com/chrdebru/kgc-user-study-protocol",
      "pdf_url": "http://arxiv.org/pdf/2412.16766v2",
      "published_date": "2024-12-21 20:26:20 UTC",
      "updated_date": "2025-05-12 18:45:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:04:37.704935"
    },
    {
      "arxiv_id": "2412.16764v1",
      "title": "Towards Selection and Transition Between Behavior-Based Neural Networks for Automated Driving",
      "title_zh": "针对自动驾驶中基于行为的神经网络的选择与过渡",
      "authors": [
        "Iqra Aslam",
        "Igor Anpilogov",
        "Andreas Rausch"
      ],
      "abstract": "Autonomous driving technology is progressing rapidly, largely due to complex\nEnd To End systems based on deep neural networks. While these systems are\neffective, their complexity can make it difficult to understand their behavior,\nraising safety concerns. This paper presents a new solution a Behavior Selector\nthat uses multiple smaller artificial neural networks (ANNs) to manage\ndifferent driving tasks, such as lane following and turning. Rather than\nrelying on a single large network, which can be burdensome, require extensive\ntraining data, and is hard to understand, the developed approach allows the\nsystem to dynamically select the appropriate neural network for each specific\nbehavior (e.g., turns) in real time. We focus on ensuring smooth transitions\nbetween behaviors while considering the vehicles current speed and orientation\nto improve stability and safety. The proposed system has been tested using the\nAirSim simulation environment, demonstrating its effectiveness.",
      "tldr_zh": "本论文针对自动驾驶中复杂End To End神经网络系统的行为难以理解和安全问题，提出了一种Behavior Selector方法，使用多个较小的ANNs来管理不同的驾驶任务，如跟车和转弯。系统通过实时动态选择合适的神经网络，并在考虑车辆当前速度和方向的情况下，确保行为之间的平滑过渡，从而提升整体稳定性和安全性。在AirSim模拟环境中进行的测试证明了该方法的有效性。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "7 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.16764v1",
      "published_date": "2024-12-21 20:23:05 UTC",
      "updated_date": "2024-12-21 20:23:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:04:49.662988"
    },
    {
      "arxiv_id": "2412.16762v1",
      "title": "A Method for the Runtime Validation of AI-based Environment Perception in Automated Driving System",
      "title_zh": "翻译失败",
      "authors": [
        "Iqra Aslam",
        "Abhishek Buragohain",
        "Daniel Bamal",
        "Adina Aniculaesei",
        "Meng Zhang",
        "Andreas Rausch"
      ],
      "abstract": "Environment perception is a fundamental part of the dynamic driving task\nexecuted by Autonomous Driving Systems (ADS). Artificial Intelligence\n(AI)-based approaches have prevailed over classical techniques for realizing\nthe environment perception. Current safety-relevant standards for automotive\nsystems, International Organization for Standardization (ISO) 26262 and ISO\n21448, assume the existence of comprehensive requirements specifications. These\nspecifications serve as the basis on which the functionality of an automotive\nsystem can be rigorously tested and checked for compliance with safety\nregulations. However, AI-based perception systems do not have complete\nrequirements specification. Instead, large datasets are used to train AI-based\nperception systems. This paper presents a function monitor for the functional\nruntime monitoring of a two-folded AI-based environment perception for ADS,\nbased respectively on camera and LiDAR sensors. To evaluate the applicability\nof the function monitor, we conduct a qualitative scenario-based evaluation in\na controlled laboratory environment using a model car. The evaluation results\nthen are discussed to provide insights into the monitor's performance and its\nsuitability for real-world applications.",
      "tldr_zh": "该论文提出了一种功能监视器（function monitor），用于在运行时验证自动驾驶系统（ADS）中基于 AI 的环境感知功能，以解决 AI 系统缺乏完整需求规格说明的问题。方法针对基于相机和 LiDAR 传感器的双重 AI 感知系统进行监控，并结合定性场景评估在实验室环境中使用模型车进行测试。结果显示，该监视器在性能方面表现出色，并为实际应用提供了宝贵见解，符合 ISO 26262 和 ISO 21448 等安全标准的要求。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.RO",
      "comment": "9 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.16762v1",
      "published_date": "2024-12-21 20:21:49 UTC",
      "updated_date": "2024-12-21 20:21:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:05:01.511953"
    },
    {
      "arxiv_id": "2412.16751v2",
      "title": "The Master Key Filters Hypothesis: Deep Filters Are General",
      "title_zh": "翻译失败",
      "authors": [
        "Zahra Babaiee",
        "Peyman M. Kiasari",
        "Daniela Rus",
        "Radu Grosu"
      ],
      "abstract": "This paper challenges the prevailing view that convolutional neural network\n(CNN) filters become increasingly specialized in deeper layers. Motivated by\nrecent observations of clusterable repeating patterns in depthwise separable\nCNNs (DS-CNNs) trained on ImageNet, we extend this investigation across various\ndomains and datasets. Our analysis of DS-CNNs reveals that deep filters\nmaintain generality, contradicting the expected transition to class-specific\nfilters. We demonstrate the generalizability of these filters through transfer\nlearning experiments, showing that frozen filters from models trained on\ndifferent datasets perform well and can be further improved when sourced from\nlarger datasets. Our findings indicate that spatial features learned by\ndepthwise separable convolutions remain generic across all layers, domains, and\narchitectures. This research provides new insights into the nature of\ngeneralization in neural networks, particularly in DS-CNNs, and has significant\nimplications for transfer learning and model design.",
      "tldr_zh": "这篇论文挑战了卷积神经网络(CNN)中深层滤波器变得越来越专业的传统观点，通过分析深度可分离CNNs (DS-CNNs) 在ImageNet等数据集上的模式，发现深层滤波器保持了通用性，而非转向特定类别。研究者通过迁移学习实验证明，这些冻结滤波器在不同数据集间表现良好，且从更大数据集获取时可进一步提升性能。总体而言，该研究揭示了DS-CNNs中空间特征的泛化特性，为神经网络的泛化机制提供了新见解，并对transfer learning和模型设计具有重要启发。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2412.16751v2",
      "published_date": "2024-12-21 20:04:23 UTC",
      "updated_date": "2025-02-03 16:58:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:05:14.147882"
    },
    {
      "arxiv_id": "2412.16746v4",
      "title": "Beyond Partisan Leaning: A Comparative Analysis of Political Bias in Large Language Models",
      "title_zh": "超越党派倾向：大型语言模型中政治偏见的比较分析",
      "authors": [
        "Tai-Quan Peng",
        "Kaiqi Yang",
        "Sanguk Lee",
        "Hang Li",
        "Yucheng Chu",
        "Yuping Lin",
        "Hui Liu"
      ],
      "abstract": "As large language models (LLMs) become increasingly embedded in civic,\neducational, and political information environments, concerns about their\npotential political bias have grown. Prior research often evaluates such bias\nthrough simulated personas or predefined ideological typologies, which may\nintroduce artificial framing effects or overlook how models behave in general\nuse scenarios. This study adopts a persona-free, topic-specific approach to\nevaluate political behavior in LLMs, reflecting how users typically interact\nwith these systems-without ideological role-play or conditioning. We introduce\na two-dimensional framework: one axis captures partisan orientation on highly\npolarized topics (e.g., abortion, immigration), and the other assesses\nsociopolitical engagement on less polarized issues (e.g., climate change,\nforeign policy). Using survey-style prompts drawn from the ANES and Pew\nResearch Center, we analyze responses from 43 LLMs developed in the U.S.,\nEurope, China, and the Middle East. We propose an entropy-weighted bias score\nto quantify both the direction and consistency of partisan alignment, and\nidentify four behavioral clusters through engagement profiles. Findings show\nmost models lean center-left or left ideologically and vary in their\nnonpartisan engagement patterns. Model scale and openness are not strong\npredictors of behavior, suggesting that alignment strategy and institutional\ncontext play a more decisive role in shaping political expression.",
      "tldr_zh": "本研究通过一种无角色扮演的主题特定方法，评估大型语言模型 (LLMs) 在政治偏见方面的表现，分析了43个来自美国、欧洲、中国和中东的模型响应。研究引入了一个二维框架：一个轴测量高度两极化话题（如堕胎和移民）的党派导向，另一个轴评估较低两极化议题（如气候变化和外交政策）的社会政治参与，并使用来自 ANES 和 Pew Research Center 的调查式提示进行测试。作者提出熵加权偏见分数来量化偏见的方向和一致性，并识别出四种行为集群，结果显示大多数模型倾向中左或左倾，而模型规模和开放性并非主要影响因素，取而代之的是对齐策略和机构背景。总的来说，此工作强调了 LLMs 在政治表达中的复杂性，并为更客观的模型开发提供见解。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.16746v4",
      "published_date": "2024-12-21 19:42:40 UTC",
      "updated_date": "2025-05-10 15:25:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:05:25.891960"
    },
    {
      "arxiv_id": "2412.16728v1",
      "title": "Reasoning about Actual Causes in Nondeterministic Domains -- Extended Version",
      "title_zh": "翻译失败",
      "authors": [
        "Shakil M. Khan",
        "Yves Lespérance",
        "Maryam Rostamigiv"
      ],
      "abstract": "Reasoning about the causes behind observations is crucial to the\nformalization of rationality. While extensive research has been conducted on\nroot cause analysis, most studies have predominantly focused on deterministic\nsettings. In this paper, we investigate causation in more realistic\nnondeterministic domains, where the agent does not have any control on and may\nnot know the choices that are made by the environment. We build on recent\npreliminary work on actual causation in the nondeterministic situation calculus\nto formalize more sophisticated forms of reasoning about actual causes in such\ndomains. We investigate the notions of ``Certainly Causes'' and ``Possibly\nCauses'' that enable the representation of actual cause for agent actions in\nthese domains. We then show how regression in the situation calculus can be\nextended to reason about such notions of actual causes.",
      "tldr_zh": "这篇论文探讨了在非确定性领域中推理实际原因（actual causes）的形式化问题，填补了现有研究主要关注确定性设置的空白。作者基于情境演算（situation calculus）扩展了初步工作，定义了“Certainty Causes”和“Possibly Causes”的概念，以表示代理行为在这些领域中的实际因果关系。最终，他们展示了如何扩展情境演算中的回归（regression）机制，来实现对这些原因的推理和分析。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.16728v1",
      "published_date": "2024-12-21 18:35:25 UTC",
      "updated_date": "2024-12-21 18:35:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:05:38.011611"
    },
    {
      "arxiv_id": "2412.16725v1",
      "title": "Argumentation Computation with Large Language Models : A Benchmark Study",
      "title_zh": "大语言模型的论证计算：基准研究",
      "authors": [
        "Zhaoqun Li",
        "Xiaotong Fang",
        "Chen Chen",
        "Mengze Li",
        "Beishui Liao"
      ],
      "abstract": "In recent years, large language models (LLMs) have made significant\nadvancements in neuro-symbolic computing. However, the combination of LLM with\nargumentation computation remains an underexplored domain, despite its\nconsiderable potential for real-world applications requiring defeasible\nreasoning. In this paper, we aim to investigate the capability of LLMs in\ndetermining the extensions of various abstract argumentation semantics. To\nachieve this, we develop and curate a benchmark comprising diverse abstract\nargumentation frameworks, accompanied by detailed explanations of algorithms\nfor computing extensions. Subsequently, we fine-tune LLMs on the proposed\nbenchmark, focusing on two fundamental extension-solving tasks. As a\ncomparative baseline, LLMs are evaluated using a chain-of-thought approach,\nwhere they struggle to accurately compute semantics. In the experiments, we\ndemonstrate that the process explanation plays a crucial role in semantics\ncomputation learning. Models trained with explanations show superior\ngeneralization accuracy compared to those trained solely with question-answer\npairs. Furthermore, by leveraging the self-explanation capabilities of LLMs,\nour approach provides detailed illustrations that mitigate the lack of\ntransparency typically associated with neural networks. Our findings contribute\nto the broader understanding of LLMs' potential in argumentation computation,\noffering promising avenues for further research in this domain.",
      "tldr_zh": "本文研究了大型语言模型 (LLMs) 在论证计算 (argumentation computation) 中的能力，开发了一个包含多样化抽象论证框架 (abstract argumentation semantics) 的基准数据集，并通过微调 LLMs 专注于扩展求解任务来评估其性能。相比于 chain-of-thought 方法，实验表明加入过程解释的训练方式能显著提升模型的泛化准确性，并利用 LLMs 的自解释能力提高计算透明度。该研究为 LLMs 在 defeasible reasoning 应用中的潜力提供了重要见解，并为未来研究开辟了新路径。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.16725v1",
      "published_date": "2024-12-21 18:23:06 UTC",
      "updated_date": "2024-12-21 18:23:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:05:50.415125"
    },
    {
      "arxiv_id": "2412.16724v1",
      "title": "Coupling Neural Networks and Physics Equations For Li-Ion Battery State-of-Charge Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Giovanni Pollo",
        "Alessio Burrello",
        "Enrico Macii",
        "Massimo Poncino",
        "Sara Vinco",
        "Daniele Jahier Pagliari"
      ],
      "abstract": "Estimating the evolution of the battery's State of Charge (SoC) in response\nto its usage is critical for implementing effective power management policies\nand for ultimately improving the system's lifetime. Most existing estimation\nmethods are either physics-based digital twins of the battery or data-driven\nmodels such as Neural Networks (NNs). In this work, we propose two new\ncontributions in this domain. First, we introduce a novel NN architecture\nformed by two cascaded branches: one to predict the current SoC based on sensor\nreadings, and one to estimate the SoC at a future time as a function of the\nload behavior. Second, we integrate battery dynamics equations into the\ntraining of our NN, merging the physics-based and data-driven approaches, to\nimprove the models' generalization over variable prediction horizons. We\nvalidate our approach on two publicly accessible datasets, showing that our\nPhysics-Informed Neural Networks (PINNs) outperform purely data-driven ones\nwhile also obtaining superior prediction accuracy with a smaller architecture\nwith respect to the state-of-the-art.",
      "tldr_zh": "本文提出了一种结合神经网络(Neural Networks, NNs)和物理方程的方法，用于锂离子电池的State of Charge (SoC)预测，以提升电源管理和系统寿命。新的NN架构包括两个级联分支：一个基于传感器读数预测当前SoC，另一个根据负载行为估计未来SoC。同时，通过将电池动力学方程整合到训练过程中，Physics-Informed Neural Networks (PINNs) 改善了模型在不同预测时段的泛化能力。在两个公开数据集上的验证显示，PINNs 比纯数据驱动模型表现出色，且在更小架构下实现了更高的预测准确率。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.16724v1",
      "published_date": "2024-12-21 18:19:12 UTC",
      "updated_date": "2024-12-21 18:19:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:06:02.441979"
    },
    {
      "arxiv_id": "2412.16720v1",
      "title": "OpenAI o1 System Card",
      "title_zh": "翻译失败",
      "authors": [
        "OpenAI",
        ":",
        "Aaron Jaech",
        "Adam Kalai",
        "Adam Lerer",
        "Adam Richardson",
        "Ahmed El-Kishky",
        "Aiden Low",
        "Alec Helyar",
        "Aleksander Madry",
        "Alex Beutel",
        "Alex Carney",
        "Alex Iftimie",
        "Alex Karpenko",
        "Alex Tachard Passos",
        "Alexander Neitz",
        "Alexander Prokofiev",
        "Alexander Wei",
        "Allison Tam",
        "Ally Bennett",
        "Ananya Kumar",
        "Andre Saraiva",
        "Andrea Vallone",
        "Andrew Duberstein",
        "Andrew Kondrich",
        "Andrey Mishchenko",
        "Andy Applebaum",
        "Angela Jiang",
        "Ashvin Nair",
        "Barret Zoph",
        "Behrooz Ghorbani",
        "Ben Rossen",
        "Benjamin Sokolowsky",
        "Boaz Barak",
        "Bob McGrew",
        "Borys Minaiev",
        "Botao Hao",
        "Bowen Baker",
        "Brandon Houghton",
        "Brandon McKinzie",
        "Brydon Eastman",
        "Camillo Lugaresi",
        "Cary Bassin",
        "Cary Hudson",
        "Chak Ming Li",
        "Charles de Bourcy",
        "Chelsea Voss",
        "Chen Shen",
        "Chong Zhang",
        "Chris Koch",
        "Chris Orsinger",
        "Christopher Hesse",
        "Claudia Fischer",
        "Clive Chan",
        "Dan Roberts",
        "Daniel Kappler",
        "Daniel Levy",
        "Daniel Selsam",
        "David Dohan",
        "David Farhi",
        "David Mely",
        "David Robinson",
        "Dimitris Tsipras",
        "Doug Li",
        "Dragos Oprica",
        "Eben Freeman",
        "Eddie Zhang",
        "Edmund Wong",
        "Elizabeth Proehl",
        "Enoch Cheung",
        "Eric Mitchell",
        "Eric Wallace",
        "Erik Ritter",
        "Evan Mays",
        "Fan Wang",
        "Felipe Petroski Such",
        "Filippo Raso",
        "Florencia Leoni",
        "Foivos Tsimpourlas",
        "Francis Song",
        "Fred von Lohmann",
        "Freddie Sulit",
        "Geoff Salmon",
        "Giambattista Parascandolo",
        "Gildas Chabot",
        "Grace Zhao",
        "Greg Brockman",
        "Guillaume Leclerc",
        "Hadi Salman",
        "Haiming Bao",
        "Hao Sheng",
        "Hart Andrin",
        "Hessam Bagherinezhad",
        "Hongyu Ren",
        "Hunter Lightman",
        "Hyung Won Chung",
        "Ian Kivlichan",
        "Ian O'Connell",
        "Ian Osband",
        "Ignasi Clavera Gilaberte",
        "Ilge Akkaya",
        "Ilya Kostrikov",
        "Ilya Sutskever",
        "Irina Kofman",
        "Jakub Pachocki",
        "James Lennon",
        "Jason Wei",
        "Jean Harb",
        "Jerry Twore",
        "Jiacheng Feng",
        "Jiahui Yu",
        "Jiayi Weng",
        "Jie Tang",
        "Jieqi Yu",
        "Joaquin Quiñonero Candela",
        "Joe Palermo",
        "Joel Parish",
        "Johannes Heidecke",
        "John Hallman",
        "John Rizzo",
        "Jonathan Gordon",
        "Jonathan Uesato",
        "Jonathan Ward",
        "Joost Huizinga",
        "Julie Wang",
        "Kai Chen",
        "Kai Xiao",
        "Karan Singhal",
        "Karina Nguyen",
        "Karl Cobbe",
        "Katy Shi",
        "Kayla Wood",
        "Kendra Rimbach",
        "Keren Gu-Lemberg",
        "Kevin Liu",
        "Kevin Lu",
        "Kevin Stone",
        "Kevin Yu",
        "Lama Ahmad",
        "Lauren Yang",
        "Leo Liu",
        "Leon Maksin",
        "Leyton Ho",
        "Liam Fedus",
        "Lilian Weng",
        "Linden Li",
        "Lindsay McCallum",
        "Lindsey Held",
        "Lorenz Kuhn",
        "Lukas Kondraciuk",
        "Lukasz Kaiser",
        "Luke Metz",
        "Madelaine Boyd",
        "Maja Trebacz",
        "Manas Joglekar",
        "Mark Chen",
        "Marko Tintor",
        "Mason Meyer",
        "Matt Jones",
        "Matt Kaufer",
        "Max Schwarzer",
        "Meghan Shah",
        "Mehmet Yatbaz",
        "Melody Y. Guan",
        "Mengyuan Xu",
        "Mengyuan Yan",
        "Mia Glaese",
        "Mianna Chen",
        "Michael Lampe",
        "Michael Malek",
        "Michele Wang",
        "Michelle Fradin",
        "Mike McClay",
        "Mikhail Pavlov",
        "Miles Wang",
        "Mingxuan Wang",
        "Mira Murati",
        "Mo Bavarian",
        "Mostafa Rohaninejad",
        "Nat McAleese",
        "Neil Chowdhury",
        "Neil Chowdhury",
        "Nick Ryder",
        "Nikolas Tezak",
        "Noam Brown",
        "Ofir Nachum",
        "Oleg Boiko",
        "Oleg Murk",
        "Olivia Watkins",
        "Patrick Chao",
        "Paul Ashbourne",
        "Pavel Izmailov",
        "Peter Zhokhov",
        "Rachel Dias",
        "Rahul Arora",
        "Randall Lin",
        "Rapha Gontijo Lopes",
        "Raz Gaon",
        "Reah Miyara",
        "Reimar Leike",
        "Renny Hwang",
        "Rhythm Garg",
        "Robin Brown",
        "Roshan James",
        "Rui Shu",
        "Ryan Cheu",
        "Ryan Greene",
        "Saachi Jain",
        "Sam Altman",
        "Sam Toizer",
        "Sam Toyer",
        "Samuel Miserendino",
        "Sandhini Agarwal",
        "Santiago Hernandez",
        "Sasha Baker",
        "Scott McKinney",
        "Scottie Yan",
        "Shengjia Zhao",
        "Shengli Hu",
        "Shibani Santurkar",
        "Shraman Ray Chaudhuri",
        "Shuyuan Zhang",
        "Siyuan Fu",
        "Spencer Papay",
        "Steph Lin",
        "Suchir Balaji",
        "Suvansh Sanjeev",
        "Szymon Sidor",
        "Tal Broda",
        "Aidan Clark",
        "Tao Wang",
        "Taylor Gordon",
        "Ted Sanders",
        "Tejal Patwardhan",
        "Thibault Sottiaux",
        "Thomas Degry",
        "Thomas Dimson",
        "Tianhao Zheng",
        "Timur Garipov",
        "Tom Stasi",
        "Trapit Bansal",
        "Trevor Creech",
        "Troy Peterson",
        "Tyna Eloundou",
        "Valerie Qi",
        "Vineet Kosaraju",
        "Vinnie Monaco",
        "Vitchyr Pong",
        "Vlad Fomenko",
        "Weiyi Zheng",
        "Wenda Zhou",
        "Wes McCabe",
        "Wojciech Zaremba",
        "Yann Dubois",
        "Yinghai Lu",
        "Yining Chen",
        "Young Cha",
        "Yu Bai",
        "Yuchen He",
        "Yuchen Zhang",
        "Yunyun Wang",
        "Zheng Shao",
        "Zhuohan Li"
      ],
      "abstract": "The o1 model series is trained with large-scale reinforcement learning to\nreason using chain of thought. These advanced reasoning capabilities provide\nnew avenues for improving the safety and robustness of our models. In\nparticular, our models can reason about our safety policies in context when\nresponding to potentially unsafe prompts, through deliberative alignment. This\nleads to state-of-the-art performance on certain benchmarks for risks such as\ngenerating illicit advice, choosing stereotyped responses, and succumbing to\nknown jailbreaks. Training models to incorporate a chain of thought before\nanswering has the potential to unlock substantial benefits, while also\nincreasing potential risks that stem from heightened intelligence. Our results\nunderscore the need for building robust alignment methods, extensively\nstress-testing their efficacy, and maintaining meticulous risk management\nprotocols. This report outlines the safety work carried out for the OpenAI o1\nand OpenAI o1-mini models, including safety evaluations, external red teaming,\nand Preparedness Framework evaluations.",
      "tldr_zh": "OpenAI o1 模型系列通过大规模 Reinforcement Learning 训练，以 Chain of Thought 进行推理，从而提升了模型的安全性和鲁棒性。模型采用 Deliberative Alignment 方法，能在处理潜在不安全提示时，根据安全策略进行上下文推理，并在生成非法建议、刻板回应和抵抗已知越狱攻击等风险基准上达到了 state-of-the-art 性能。这种链式思考训练虽然解锁了显著好处，但也增加了潜在风险，因此强调需要构建稳健的 alignment 方法、进行广泛测试和维护风险管理协议。报告概述了 o1 和 o1-mini 模型的安全工作，包括安全评估、外部红队测试以及 Preparedness Framework 评估。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.16720v1",
      "published_date": "2024-12-21 18:04:31 UTC",
      "updated_date": "2024-12-21 18:04:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:08:07.152143"
    },
    {
      "arxiv_id": "2412.16719v2",
      "title": "Lillama: Large Language Models Compression via Low-Rank Feature Distillation",
      "title_zh": "翻译失败",
      "authors": [
        "Yaya Sy",
        "Christophe Cerisara",
        "Irina Illina"
      ],
      "abstract": "Current LLM structured pruning methods typically involve two steps: (1)\ncompression with calibration data and (2) costly continued pretraining on\nbillions of tokens to recover lost performance. This second step is necessary\nas the first significantly impacts model accuracy. Prior research suggests\npretrained Transformer weights aren't inherently low-rank, unlike their\nactivations, which may explain this drop. Based on this observation, we propose\nLillama, a compression method that locally distills activations with low-rank\nweights. Using SVD for initialization and a joint loss combining teacher and\nstudent activations, we accelerate convergence and reduce memory use with local\ngradient updates. Lillama compresses Mixtral-8x7B within minutes on a single\nA100 GPU, removing 10 billion parameters while retaining over 95% of its\noriginal performance. Phi-2 3B can be compressed by 40% with just 13 million\ncalibration tokens, resulting in a small model that competes with recent models\nof similar size. The method generalizes well to non-transformer architectures,\ncompressing Mamba-3B by 20% while maintaining 99% performance.",
      "tldr_zh": "该论文提出Lillama，一种高效的LLM压缩方法，通过低秩特征蒸馏（Low-Rank Feature Distillation）来处理当前结构修剪方法的局限性，避免了昂贵的后续预训练。方法利用SVD初始化和结合教师与学生激活的联合损失，实现了局部梯度更新，加速收敛并降低内存使用。实验结果显示，Lillama能在单个A100 GPU上几分钟内压缩Mixtral-8x7B，移除10亿参数并保留95%性能；同时，Phi-2 3B压缩40%后与类似大小模型竞争，且适用于非Transformer架构，如Mamba-3B压缩20%保持99%性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "20 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.16719v2",
      "published_date": "2024-12-21 18:04:01 UTC",
      "updated_date": "2024-12-28 17:45:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:06:26.912719"
    },
    {
      "arxiv_id": "2412.16715v1",
      "title": "From Histopathology Images to Cell Clouds: Learning Slide Representations with Hierarchical Cell Transformer",
      "title_zh": "从",
      "authors": [
        "Zijiang Yang",
        "Zhongwei Qiu",
        "Tiancheng Lin",
        "Hanqing Chao",
        "Wanxing Chang",
        "Yelin Yang",
        "Yunshuo Zhang",
        "Wenpei Jiao",
        "Yixuan Shen",
        "Wenbin Liu",
        "Dongmei Fu",
        "Dakai Jin",
        "Ke Yan",
        "Le Lu",
        "Hui Jiang",
        "Yun Bian"
      ],
      "abstract": "It is clinically crucial and potentially very beneficial to be able to\nanalyze and model directly the spatial distributions of cells in histopathology\nwhole slide images (WSI). However, most existing WSI datasets lack cell-level\nannotations, owing to the extremely high cost over giga-pixel images. Thus, it\nremains an open question whether deep learning models can directly and\neffectively analyze WSIs from the semantic aspect of cell distributions. In\nthis work, we construct a large-scale WSI dataset with more than 5 billion\ncell-level annotations, termed WSI-Cell5B, and a novel hierarchical Cell Cloud\nTransformer (CCFormer) to tackle these challenges. WSI-Cell5B is based on 6,998\nWSIs of 11 cancers from The Cancer Genome Atlas Program, and all WSIs are\nannotated per cell by coordinates and types. To the best of our knowledge,\nWSI-Cell5B is the first WSI-level large-scale dataset integrating cell-level\nannotations. On the other hand, CCFormer formulates the collection of cells in\neach WSI as a cell cloud and models cell spatial distribution. Specifically,\nNeighboring Information Embedding (NIE) is proposed to characterize the\ndistribution of cells within the neighborhood of each cell, and a novel\nHierarchical Spatial Perception (HSP) module is proposed to learn the spatial\nrelationship among cells in a bottom-up manner. The clinical analysis indicates\nthat WSI-Cell5B can be used to design clinical evaluation metrics based on\ncounting cells that effectively assess the survival risk of patients. Extensive\nexperiments on survival prediction and cancer staging show that learning from\ncell spatial distribution alone can already achieve state-of-the-art (SOTA)\nperformance, i.e., CCFormer strongly outperforms other competing methods.",
      "tldr_zh": "本研究构建了大型WSI数据集WSI-Cell5B，包含超过50亿细胞级标注，基于6998张全滑玻图像（WSI）和11种癌症，以解决现有数据集缺乏细胞级标注的问题。  \n他们提出了Hierarchical Cell Transformer (CCFormer)，该模型将WSI中的细胞视为“cell cloud”，通过Neighboring Information Embedding (NIE)描述局部细胞分布，以及Hierarchical Spatial Perception (HSP)模块自下而上学习细胞间空间关系。  \n实验结果表明，仅从细胞空间分布学习，CCFormer在生存预测和癌症分期任务上达到了SOTA性能，并可用于设计基于细胞计数的临床评估指标，以有效评估患者生存风险。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.16715v1",
      "published_date": "2024-12-21 17:57:12 UTC",
      "updated_date": "2024-12-21 17:57:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:06:39.253798"
    },
    {
      "arxiv_id": "2412.19833v1",
      "title": "Multi-atlas Ensemble Graph Neural Network Model For Major Depressive Disorder Detection Using Functional MRI Data",
      "title_zh": "多图谱集成图神经网络模型用于重度抑郁症",
      "authors": [
        "Nojod M. Alotaibi",
        "Areej M. Alhothali",
        "Manar S. Ali"
      ],
      "abstract": "Major depressive disorder (MDD) is one of the most common mental disorders,\nwith significant impacts on many daily activities and quality of life. It\nstands as one of the most common mental disorders globally and ranks as the\nsecond leading cause of disability. The current diagnostic approach for MDD\nprimarily relies on clinical observations and patient-reported symptoms,\noverlooking the diverse underlying causes and pathophysiological factors\ncontributing to depression. Therefore, scientific researchers and clinicians\nmust gain a deeper understanding of the pathophysiological mechanisms involved\nin MDD. There is growing evidence in neuroscience that depression is a brain\nnetwork disorder, and the use of neuroimaging, such as magnetic resonance\nimaging (MRI), plays a significant role in identifying and treating MDD.\nRest-state functional MRI (rs-fMRI) is among the most popular neuroimaging\ntechniques used to study MDD. Deep learning techniques have been widely applied\nto neuroimaging data to help with early mental health disorder detection.\nRecent years have seen a rise in interest in graph neural networks (GNNs),\nwhich are deep neural architectures specifically designed to handle\ngraph-structured data like rs-fMRI. This research aimed to develop an\nensemble-based GNN model capable of detecting discriminative features from\nrs-fMRI images for the purpose of diagnosing MDD. Specifically, we constructed\nan ensemble model by combining features from multiple brain region segmentation\natlases to capture brain complexity and detect distinct features more\naccurately than single atlas-based models. Further, the effectiveness of our\nmodel is demonstrated by assessing its performance on a large multi-site MDD\ndataset. The best performing model among all folds achieved an accuracy of\n75.80%, a sensitivity of 88.89%, a specificity of 61.84%, a precision of\n71.29%, and an F1-score of 79.12%.",
      "tldr_zh": "本研究针对重度抑郁症 (MDD) 的检测问题，提出了一种多图集集成 Graph Neural Network (GNN) 模型，利用静息态功能性 MRI (rs-fMRI) 数据来捕捉脑部网络特征，从而更准确地识别 MDD。模型通过结合多个脑区分割图集的特征，构建集成框架，以克服单一图集模型的局限性，并在大型多站点数据集上进行评估。结果显示，最优模型的准确率达到 75.80%，敏感性为 88.89%，特异性为 61.84%，精确度和 F1 分数分别为 71.29% 和 79.12%，为早期 MDD 诊断提供了有效的深度学习工具。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "62P10, 68T07, 92B20",
        "I.2.6; J.3"
      ],
      "primary_category": "cs.CV",
      "comment": "17 pages, 2 figures, 10 tables",
      "pdf_url": "http://arxiv.org/pdf/2412.19833v1",
      "published_date": "2024-12-21 17:08:03 UTC",
      "updated_date": "2024-12-21 17:08:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:06:50.504932"
    },
    {
      "arxiv_id": "2412.16699v1",
      "title": "FAP-CD: Fairness-Driven Age-Friendly Community Planning via Conditional Diffusion Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Jinlin Li",
        "Xintong Li",
        "Xiao Zhou"
      ],
      "abstract": "As global populations age rapidly, incorporating age-specific considerations\ninto urban planning has become essential to addressing the urgent demand for\nage-friendly built environments and ensuring sustainable urban development.\nHowever, current practices often overlook these considerations, resulting in\ninadequate and unevenly distributed elderly services in cities. There is a\npressing need for equitable and optimized urban renewal strategies to support\neffective age-friendly planning. To address this challenge, we propose a novel\nframework, Fairness-driven Age-friendly community Planning via Conditional\nDiffusion generation (FAP-CD). FAP-CD leverages a conditioned graph denoising\ndiffusion probabilistic model to learn the joint probability distribution of\naging facilities and their spatial relationships at a fine-grained regional\nlevel. Our framework generates optimized facility distributions by iteratively\nrefining noisy graphs, conditioned on the needs of the elderly during the\ndiffusion process. Key innovations include a demand-fairness pre-training\nmodule that integrates community demand features and facility characteristics\nusing an attention mechanism and min-max optimization, ensuring equitable\nservice distribution across regions. Additionally, a discrete graph structure\ncaptures walkable accessibility within regional road networks, guiding model\nsampling. To enhance information integration, we design a graph denoising\nnetwork with an attribute augmentation module and a hybrid graph message\naggregation module, combining local and global node and edge information.\nEmpirical results across multiple metrics demonstrate the effectiveness of\nFAP-CD in balancing age-friendly needs with regional equity, achieving an\naverage improvement of 41% over competitive baseline models.",
      "tldr_zh": "随着全球人口老龄化，城市规划亟需解决老年服务分布不均的问题，本文提出FAP-CD框架，通过条件扩散生成模型优化老龄化设施的空间分布。\n该框架整合需求公平性预训练模块（使用注意力机制和min-max优化）以及图去噪网络（包括属性增强和混合图消息聚合），确保设施分配公平并考虑区域可步行可达性。\n实验结果表明，FAP-CD在多个指标上比基线模型平均提升41%，有效平衡了老龄化需求与区域公平性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.16699v1",
      "published_date": "2024-12-21 16:57:09 UTC",
      "updated_date": "2024-12-21 16:57:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:07:02.948095"
    },
    {
      "arxiv_id": "2412.16689v1",
      "title": "Formal Language Knowledge Corpus for Retrieval Augmented Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Majd Zayyad",
        "Yossi Adi"
      ],
      "abstract": "The integration of retrieval-augmented techniques with LLMs has shown promise\nin improving performance across various domains. However, their utility in\ntasks requiring advanced reasoning, such as generating and evaluating\nmathematical statements and proofs, remains underexplored. This study explores\nthe use of Lean, a programming language for writing mathematical proofs, to\npopulate the knowledge corpus used by RAG systems. We hope for this to lay the\nfoundation to exploring different methods of using RAGs to improve the\nperformance of LLMs in advanced logical reasoning tasks.",
      "tldr_zh": "这篇论文探讨了将检索增强生成（RAG）技术与大型语言模型（LLMs）整合，以提升其在高级推理任务（如生成和评估数学语句及证明）中的性能。研究重点使用 Lean（一种用于编写数学证明的编程语言）来填充 RAG 系统的知识库，从而解决这些任务的不足。最终，该工作为探索 RAG 如何进一步改善 LLMs 在高级逻辑推理领域的表现奠定了基础。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.16689v1",
      "published_date": "2024-12-21 16:31:41 UTC",
      "updated_date": "2024-12-21 16:31:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:07:13.467683"
    },
    {
      "arxiv_id": "2412.16687v2",
      "title": "Subgoal Discovery Using a Free Energy Paradigm and State Aggregations",
      "title_zh": "翻译失败",
      "authors": [
        "Amirhossein Mesbah",
        "Reshad Hosseini",
        "Seyed Pooya Shariatpanahi",
        "Majid Nili Ahmadabadi"
      ],
      "abstract": "Reinforcement learning (RL) plays a major role in solving complex sequential\ndecision-making tasks. Hierarchical and goal-conditioned RL are promising\nmethods for dealing with two major problems in RL, namely sample inefficiency\nand difficulties in reward shaping. These methods tackle the mentioned problems\nby decomposing a task into simpler subtasks and temporally abstracting a task\nin the action space. One of the key components for task decomposition of these\nmethods is subgoal discovery. We can use the subgoal states to define\nhierarchies of actions and also use them in decomposing complex tasks. Under\nthe assumption that subgoal states are more unpredictable, we propose a free\nenergy paradigm to discover them. This is achieved by using free energy to\nselect between two spaces, the main space and an aggregation space. The $model\n\\; changes$ from neighboring states to a given state shows the unpredictability\nof a given state, and therefore it is used in this paper for subgoal discovery.\nOur empirical results on navigation tasks like grid-world environments show\nthat our proposed method can be applied for subgoal discovery without prior\nknowledge of the task. Our proposed method is also robust to the stochasticity\nof environments.",
      "tldr_zh": "该论文探讨了强化学习（RL）中样本效率低和奖励塑造困难的问题，通过层次化和目标条件RL将任务分解为子任务。作者提出了一种基于自由能范式（free energy paradigm）和状态聚合的方法来发现子目标状态，该方法假设子目标状态更不可预测，并利用状态变化（state changes）来评估不确定性，从而在主空间和聚合空间之间进行选择。实验结果显示，该方法在网格世界等导航任务中无需任务先验知识即可有效发现子目标，并对环境随机性（stochasticity）表现出鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.16687v2",
      "published_date": "2024-12-21 16:26:47 UTC",
      "updated_date": "2025-02-09 11:24:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:07:26.085223"
    },
    {
      "arxiv_id": "2412.16682v1",
      "title": "The Task Shield: Enforcing Task Alignment to Defend Against Indirect Prompt Injection in LLM Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Feiran Jia",
        "Tong Wu",
        "Xin Qin",
        "Anna Squicciarini"
      ],
      "abstract": "Large Language Model (LLM) agents are increasingly being deployed as\nconversational assistants capable of performing complex real-world tasks\nthrough tool integration. This enhanced ability to interact with external\nsystems and process various data sources, while powerful, introduces\nsignificant security vulnerabilities. In particular, indirect prompt injection\nattacks pose a critical threat, where malicious instructions embedded within\nexternal data sources can manipulate agents to deviate from user intentions.\nWhile existing defenses based on rule constraints, source spotlighting, and\nauthentication protocols show promise, they struggle to maintain robust\nsecurity while preserving task functionality. We propose a novel and orthogonal\nperspective that reframes agent security from preventing harmful actions to\nensuring task alignment, requiring every agent action to serve user objectives.\nBased on this insight, we develop Task Shield, a test-time defense mechanism\nthat systematically verifies whether each instruction and tool call contributes\nto user-specified goals. Through experiments on the AgentDojo benchmark, we\ndemonstrate that Task Shield reduces attack success rates (2.07\\%) while\nmaintaining high task utility (69.79\\%) on GPT-4o.",
      "tldr_zh": "该研究针对大型语言模型（LLM）代理在与外部系统交互时面临的间接提示注入（indirect prompt injection）攻击问题，提出了一种新防御视角：通过确保任务对齐（task alignment）来防止代理偏离用户意图。Task Shield 是一种测试时防御机制，它系统验证每个指令和工具调用是否符合用户指定目标，从而在不牺牲任务功能的情况下提升安全性。在 AgentDojo 基准测试中，Task Shield 将攻击成功率降低至 2.07%，同时保持任务效用达 69.79%。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.16682v1",
      "published_date": "2024-12-21 16:17:48 UTC",
      "updated_date": "2024-12-21 16:17:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:07:37.897107"
    },
    {
      "arxiv_id": "2412.16674v1",
      "title": "STAMPsy: Towards SpatioTemporal-Aware Mixed-Type Dialogues for Psychological Counseling",
      "title_zh": "翻译失败",
      "authors": [
        "Jieyi Wang",
        "Yue Huang",
        "Zeming Liu",
        "Dexuan Xu",
        "Chuan Wang",
        "Xiaoming Shi",
        "Ruiyuan Guan",
        "Hongxing Wang",
        "Weihua Yue",
        "Yu Huang"
      ],
      "abstract": "Online psychological counseling dialogue systems are trending, offering a\nconvenient and accessible alternative to traditional in-person therapy.\nHowever, existing psychological counseling dialogue systems mainly focus on\nbasic empathetic dialogue or QA with minimal professional knowledge and without\ngoal guidance. In many real-world counseling scenarios, clients often seek\nmulti-type help, such as diagnosis, consultation, therapy, console, and common\nquestions, but existing dialogue systems struggle to combine different dialogue\ntypes naturally. In this paper, we identify this challenge as how to construct\nmixed-type dialogue systems for psychological counseling that enable clients to\nclarify their goals before proceeding with counseling. To mitigate the\nchallenge, we collect a mixed-type counseling dialogues corpus termed STAMPsy,\ncovering five dialogue types, task-oriented dialogue for diagnosis,\nknowledge-grounded dialogue, conversational recommendation, empathetic\ndialogue, and question answering, over 5,000 conversations. Moreover,\nspatiotemporal-aware knowledge enables systems to have world awareness and has\nbeen proven to affect one's mental health. Therefore, we link dialogues in\nSTAMPsy to spatiotemporal state and propose a spatiotemporal-aware mixed-type\npsychological counseling dataset. Additionally, we build baselines on STAMPsy\nand develop an iterative self-feedback psychological dialogue generation\nframework, named Self-STAMPsy. Results indicate that clarifying dialogue goals\nin advance and utilizing spatiotemporal states are effective.",
      "tldr_zh": "本研究针对在线心理咨询对话系统的局限性，提出了一种时空感知（SpatioTemporal-Aware）的混合类型对话框架STAMPsy，以解决现有系统在处理诊断、咨询、治疗、安慰和问答等多样化对话类型时的不足。研究者收集了STAMPsy语料库，涵盖超过5,000个对话，包括task-oriented dialogue、knowledge-grounded dialogue、conversational recommendation、empathetic dialogue和question answering，并将对话与时空状态（如时间和地点）关联起来，创建了一个更具世界意识的心理咨询数据集。基于此，他们开发了Self-STAMPsy框架，一个迭代自反馈的对话生成系统，结果显示提前澄清对话目标并利用spatiotemporal-aware知识能有效提升系统的性能和实用性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.16674v1",
      "published_date": "2024-12-21 15:48:02 UTC",
      "updated_date": "2024-12-21 15:48:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:08:19.885300"
    },
    {
      "arxiv_id": "2412.16673v1",
      "title": "On Enhancing Network Throughput using Reinforcement Learning in Sliced Testbeds",
      "title_zh": "翻译失败",
      "authors": [
        "Daniel Pereira Monteiro",
        "Lucas Nardelli de Freitas Botelho Saar",
        "Larissa Ferreira Rodrigues Moreira",
        "Rodrigo Moreira"
      ],
      "abstract": "Novel applications demand high throughput, low latency, and high reliability\nconnectivity and still pose significant challenges to slicing orchestration\narchitectures. The literature explores network slicing techniques that employ\ncanonical methods, artificial intelligence, and combinatorial optimization to\naddress errors and ensure throughput for network slice data plane. This paper\nintroduces the Enhanced Mobile Broadband (eMBB)-Agent as a new approach that\nuses Reinforcement Learning (RL) in a vertical application to enhance network\nslicing throughput to fit Service-Level Agreements (SLAs). The eMBB-Agent\nanalyzes application transmission variables and proposes actions within a\ndiscrete space to adjust the reception window using a Deep Q-Network (DQN).\nThis paper also presents experimental results that examine the impact of\nfactors such as the channel error rate, DQN model layers, and learning rate on\nmodel convergence and achieved throughput, providing insights on embedding\nintelligence in network slicing.",
      "tldr_zh": "这篇论文提出 eMBB-Agent，一种基于 Reinforcement Learning (RL) 的新方法，用于提升网络切片中的吞吐量，以满足 Service-Level Agreements (SLA) 的要求。eMBB-Agent 通过分析应用传输变量，并使用 Deep Q-Network (DQN) 在离散空间中调整接收窗口，从而优化网络性能。实验结果显示，信道错误率、DQN 模型层数和学习率等因素对模型收敛和吞吐量有显著影响，并为在网络切片中嵌入智能提供宝贵见解。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Paper already published at Anais do XV Workshop de Pesquisa\n  Experimental da Internet do Futuro (WPEIF)",
      "pdf_url": "http://arxiv.org/pdf/2412.16673v1",
      "published_date": "2024-12-21 15:47:49 UTC",
      "updated_date": "2024-12-21 15:47:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:10:22.113522"
    },
    {
      "arxiv_id": "2412.16662v2",
      "title": "Adversarial Attack Against Images Classification based on Generative Adversarial Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Yahe Yang"
      ],
      "abstract": "Adversarial attacks on image classification systems have always been an\nimportant problem in the field of machine learning, and generative adversarial\nnetworks (GANs), as popular models in the field of image generation, have been\nwidely used in various novel scenarios due to their powerful generative\ncapabilities. However, with the popularity of generative adversarial networks,\nthe misuse of fake image technology has raised a series of security problems,\nsuch as malicious tampering with other people's photos and videos, and invasion\nof personal privacy. Inspired by the generative adversarial networks, this work\nproposes a novel adversarial attack method, aiming to gain insight into the\nweaknesses of the image classification system and improve its anti-attack\nability. Specifically, the generative adversarial networks are used to generate\nadversarial samples with small perturbations but enough to affect the\ndecision-making of the classifier, and the adversarial samples are generated\nthrough the adversarial learning of the training generator and the classifier.\nFrom extensive experiment analysis, we evaluate the effectiveness of the method\non a classical image classification dataset, and the results show that our\nmodel successfully deceives a variety of advanced classifiers while maintaining\nthe naturalness of adversarial samples.",
      "tldr_zh": "这篇论文提出了一种基于Generative Adversarial Networks (GANs)的新型对抗攻击方法，旨在揭示图像分类系统的弱点并提升其抗攻击能力。该方法利用GANs生成微小扰动的对抗样本，通过生成器和分类器的对抗学习过程来欺骗分类器，同时保持样本的自然性。实验在经典图像分类数据集上评估，结果显示该攻击成功欺骗多种高级分类器，突显了GANs在安全领域的潜在风险和应用价值。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "7 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.16662v2",
      "published_date": "2024-12-21 15:23:34 UTC",
      "updated_date": "2024-12-24 17:21:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:08:42.526232"
    },
    {
      "arxiv_id": "2412.16656v1",
      "title": "Generalizable Articulated Object Perception with Superpoints",
      "title_zh": "基于超点的可泛化铰接物体感知",
      "authors": [
        "Qiaojun Yu",
        "Ce Hao",
        "Xibin Yuan",
        "Li Zhang",
        "Liu Liu",
        "Yukang Huo",
        "Rohit Agarwal",
        "Cewu Lu"
      ],
      "abstract": "Manipulating articulated objects with robotic arms is challenging due to the\ncomplex kinematic structure, which requires precise part segmentation for\nefficient manipulation. In this work, we introduce a novel superpoint-based\nperception method designed to improve part segmentation in 3D point clouds of\narticulated objects. We propose a learnable, part-aware superpoint generation\ntechnique that efficiently groups points based on their geometric and semantic\nsimilarities, resulting in clearer part boundaries. Furthermore, by leveraging\nthe segmentation capabilities of the 2D foundation model SAM, we identify the\ncenters of pixel regions and select corresponding superpoints as candidate\nquery points. Integrating a query-based transformer decoder further enhances\nour method's ability to achieve precise part segmentation. Experimental results\non the GAPartNet dataset show that our method outperforms existing\nstate-of-the-art approaches in cross-category part segmentation, achieving AP50\nscores of 77.9% for seen categories (4.4% improvement) and $39.3\\%$ for unseen\ncategories (11.6% improvement), with superior results in 5 out of 9 part\ncategories for seen objects and outperforming all previous methods across all\npart categories for unseen objects.",
      "tldr_zh": "该研究针对机器人臂操纵铰接物体的挑战，提出了一种基于 superpoints 的通用感知方法，以提升 3D 点云中物体的精确零件分割。方法包括一个可学习的、零件感知的 superpoint 生成技术，通过几何和语义相似性分组点来清晰界定零件边界，并结合 2D 基础模型 SAM 识别像素区域中心作为候选查询点，再通过查询-based transformer decoder 进一步优化分割精度。实验在 GAPartNet 数据集上显示，该方法在跨类别分割中优于现有技术，对于已见类别达到 AP50 得分的 77.9%（提升 4.4%），对于未见类别达到 39.3%（提升 11.6%），并在多个零件类别上表现出色。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.16656v1",
      "published_date": "2024-12-21 14:57:24 UTC",
      "updated_date": "2024-12-21 14:57:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:08:55.289716"
    },
    {
      "arxiv_id": "2412.16653v1",
      "title": "Internalized Self-Correction for Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Nishanth Upadhyaya",
        "Raghavendra Sridharamurthy"
      ],
      "abstract": "In this article, we introduce 'Internalized Self-Correction' (InSeC) for\nlarge language models (LLMs). While many approaches exist for self-reflection\nat inference time, we propose a novel method that combines ideas from negative\nsampling, self-reflection during training, and inference time. InSeC allows\nLLMs to correct themselves by introducing mistakes and their corresponding\ncorrections during training, thereby converting the learning process into a\ntrue supervised learning task with both positive and negative examples. This\napproach can be extended to improve instruction following and correct\nhallucinations or incorrect sentences generated by LLMs.",
      "tldr_zh": "本论文提出了一种名为 Internalized Self-Correction (InSeC) 的方法，用于提升大型语言模型 (LLMs) 的自校正能力。InSeC 通过结合负采样、训练时的自反省和推理时的自反省，在训练过程中引入错误及其对应修正，将学习转化为一个包含正负例的监督学习任务。这种方法可扩展到改善指令遵循，并有效纠正 LLMs 生成的 hallucinations 或不正确句子。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.16653v1",
      "published_date": "2024-12-21 14:53:13 UTC",
      "updated_date": "2024-12-21 14:53:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:09:05.575520"
    },
    {
      "arxiv_id": "2412.16651v2",
      "title": "PB-UAP: Hybrid Universal Adversarial Attack For Image Segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Yufei Song",
        "Ziqi Zhou",
        "Minghui Li",
        "Xianlong Wang",
        "Hangtao Zhang",
        "Menghao Deng",
        "Wei Wan",
        "Shengshan Hu",
        "Leo Yu Zhang"
      ],
      "abstract": "With the rapid advancement of deep learning, the model robustness has become\na significant research hotspot, \\ie, adversarial attacks on deep neural\nnetworks. Existing works primarily focus on image classification tasks, aiming\nto alter the model's predicted labels. Due to the output complexity and deeper\nnetwork architectures, research on adversarial examples for segmentation models\nis still limited, particularly for universal adversarial perturbations. In this\npaper, we propose a novel universal adversarial attack method designed for\nsegmentation models, which includes dual feature separation and low-frequency\nscattering modules. The two modules guide the training of adversarial examples\nin the pixel and frequency space, respectively. Experiments demonstrate that\nour method achieves high attack success rates surpassing the state-of-the-art\nmethods, and exhibits strong transferability across different models.",
      "tldr_zh": "本论文提出了一种混合通用对抗攻击方法 PB-UAP，针对图像分割模型，旨在生成鲁棒性更强的对抗样本。方法包括双特征分离模块（在像素空间）和低频散射模块（在频率空间），分别指导对抗样本的训练，以增强攻击效果。实验结果表明，PB-UAP 在攻击成功率上超过了最先进方法，并展现出强大的模型转移性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ICASSP 2025",
      "pdf_url": "http://arxiv.org/pdf/2412.16651v2",
      "published_date": "2024-12-21 14:46:01 UTC",
      "updated_date": "2025-01-03 15:39:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:09:17.559201"
    },
    {
      "arxiv_id": "2412.16643v1",
      "title": "TimeRAG: BOOSTING LLM Time Series Forecasting via Retrieval-Augmented Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Silin Yang",
        "Dong Wang",
        "Haoqi Zheng",
        "Ruochun Jin"
      ],
      "abstract": "Although the rise of large language models (LLMs) has introduced new\nopportunities for time series forecasting, existing LLM-based solutions require\nexcessive training and exhibit limited transferability. In view of these\nchallenges, we propose TimeRAG, a framework that incorporates\nRetrieval-Augmented Generation (RAG) into time series forecasting LLMs, which\nconstructs a time series knowledge base from historical sequences, retrieves\nreference sequences from the knowledge base that exhibit similar patterns to\nthe query sequence measured by Dynamic Time Warping (DTW), and combines these\nreference sequences and the prediction query as a textual prompt to the time\nseries forecasting LLM. Experiments on datasets from various domains show that\nthe integration of RAG improved the prediction accuracy of the original model\nby 2.97% on average.",
      "tldr_zh": "该研究提出 TimeRAG 框架，通过 Retrieval-Augmented Generation (RAG) 技术提升大型语言模型 (LLM) 在时间序列预测中的性能，解决现有方法训练过度和转移性有限的问题。\nTimeRAG 从历史序列构建知识库，使用 Dynamic Time Warping (DTW) 检索与查询序列相似模式的参考序列，并将这些序列与预测查询作为文本提示输入到时间序列预测 LLM。\n实验结果显示，在各种领域的数据集上，该框架使原模型的预测准确率平均提高了 2.97%。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.16643v1",
      "published_date": "2024-12-21 14:27:38 UTC",
      "updated_date": "2024-12-21 14:27:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:09:31.000198"
    },
    {
      "arxiv_id": "2412.16642v2",
      "title": "L3TC: Leveraging RWKV for Learned Lossless Low-Complexity Text Compression",
      "title_zh": "翻译失败",
      "authors": [
        "Junxuan Zhang",
        "Zhengxue Cheng",
        "Yan Zhao",
        "Shihao Wang",
        "Dajiang Zhou",
        "Guo Lu",
        "Li Song"
      ],
      "abstract": "Learning-based probabilistic models can be combined with an entropy coder for\ndata compression. However, due to the high complexity of learning-based models,\ntheir practical application as text compressors has been largely overlooked. To\naddress this issue, our work focuses on a low-complexity design while\nmaintaining compression performance. We introduce a novel Learned Lossless\nLow-complexity Text Compression method (L3TC). Specifically, we conduct\nextensive experiments demonstrating that RWKV models achieve the fastest\ndecoding speed with a moderate compression ratio, making it the most suitable\nbackbone for our method. Second, we propose an outlier-aware tokenizer that\nuses a limited vocabulary to cover frequent tokens while allowing outliers to\nbypass the prediction and encoding. Third, we propose a novel high-rank\nreparameterization strategy that enhances the learning capability during\ntraining without increasing complexity during inference. Experimental results\nvalidate that our method achieves 48% bit saving compared to gzip compressor.\nBesides, L3TC offers compression performance comparable to other learned\ncompressors, with a 50x reduction in model parameters. More importantly, L3TC\nis the fastest among all learned compressors, providing real-time decoding\nspeeds up to megabytes per second. Our code is available at\nhttps://github.com/alipay/L3TC-leveraging-rwkv-for-learned-lossless-low-complexity-text-compression.git.",
      "tldr_zh": "本论文提出 L3TC 方法，利用 RWKV 模型作为骨干，实现学习-based 的无损低复杂性文本压缩，以解决传统模型的高复杂性问题。具体创新包括 outlier-aware tokenizer，用于覆盖频繁 tokens 并绕过异常 tokens 的预测，以及高阶重参数化策略，在训练时增强学习能力而不增加推理复杂性。实验结果显示，L3TC 比 gzip 节省 48% 比特，与其他学习压缩器性能相当，但模型参数减少 50 倍，并提供最高的速度，支持每秒兆字节级的实时解码。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IT",
        "cs.MM",
        "math.IT"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.16642v2",
      "published_date": "2024-12-21 14:24:32 UTC",
      "updated_date": "2024-12-24 04:20:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:09:42.654631"
    },
    {
      "arxiv_id": "2412.16641v4",
      "title": "A Systems Thinking Approach to Algorithmic Fairness",
      "title_zh": "翻译失败",
      "authors": [
        "Chris Lam"
      ],
      "abstract": "Systems thinking provides us with a way to model the algorithmic fairness\nproblem by allowing us to encode prior knowledge and assumptions about where we\nbelieve bias might exist in the data generating process. We can then encode\nthese beliefs as a series of causal graphs, enabling us to link AI/ML systems\nto politics and the law. This allows us to combine techniques from machine\nlearning, causal inference, and system dynamics in order to capture different\nemergent aspects of the fairness problem. We can use systems thinking to help\npolicymakers on both sides of the political aisle to understand the complex\ntrade-offs that exist from different types of fairness policies, providing a\nsociotechnical foundation for designing AI policy that is aligned to their\npolitical agendas and with society's values.",
      "tldr_zh": "这篇论文提出了一种基于 systems thinking 的方法来建模 algorithmic fairness 问题，通过编码先验知识和假设来识别数据生成过程中的潜在偏见。作者将这些信念转化为 causal graphs，将 AI/ML 系统与政治和法律领域联系起来，从而结合 machine learning、causal inference 和 system dynamics 等技术，捕捉公平问题的不同涌现方面。该方法有助于政策制定者理解各种公平政策的复杂权衡，并为设计与政治议程和社会价值观一致的 AI 政策提供 sociotechnical foundation。",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "This paper has been submitted to the 2025 ACM FAccT conference for\n  review",
      "pdf_url": "http://arxiv.org/pdf/2412.16641v4",
      "published_date": "2024-12-21 14:21:33 UTC",
      "updated_date": "2025-01-20 12:03:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:09:55.208164"
    },
    {
      "arxiv_id": "2412.16633v2",
      "title": "POEX: Understanding and Mitigating Policy Executable Jailbreak Attacks against Embodied AI",
      "title_zh": "翻译失败",
      "authors": [
        "Xuancun Lu",
        "Zhengxian Huang",
        "Xinfeng Li",
        "Xiaoyu ji",
        "Wenyuan Xu"
      ],
      "abstract": "Embodied AI systems are rapidly evolving due to the integration of LLMs as\nplanning modules, which transform complex instructions into executable\npolicies. However, LLMs are vulnerable to jailbreak attacks, which can generate\nmalicious content. This paper investigates the feasibility and rationale behind\napplying traditional LLM jailbreak attacks to EAI systems. We aim to answer\nthree questions: (1) Do traditional LLM jailbreak attacks apply to EAI systems?\n(2) What challenges arise if they do not? and (3) How can we defend against EAI\njailbreak attacks? To this end, we first measure existing LLM-based EAI systems\nusing a newly constructed dataset, i.e., the Harmful-RLbench. Our study\nconfirms that traditional LLM jailbreak attacks are not directly applicable to\nEAI systems and identifies two unique challenges. First, the harmful text does\nnot necessarily constitute harmful policies. Second, even if harmful policies\ncan be generated, they are not necessarily executable by the EAI systems, which\nlimits the potential risk. To facilitate a more comprehensive security\nanalysis, we refine and introduce POEX, a novel red teaming framework that\noptimizes adversarial suffixes to induce harmful yet executable policies\nagainst EAI systems. The design of POEX employs adversarial constraints, policy\nevaluators, and suffix optimization to ensure successful policy execution while\nevading safety detection inside an EAI system. Experiments on the real-world\nrobotic arm and simulator using Harmful-RLbench demonstrate the efficacy,\nhighlighting severe safety vulnerabilities and high transferability across\nmodels. Finally, we propose prompt-based and model-based defenses, achieving an\n85% success rate in mitigating attacks and enhancing safety awareness in EAI\nsystems. Our findings underscore the urgent need for robust security measures\nto ensure the safe deployment of EAI in critical applications.",
      "tldr_zh": "这篇论文探讨了传统 LLM jailbreak attacks 对 Embodied AI (EAI) 系统的适用性，识别出关键挑战：有害文本不一定转化为有害策略，且策略可能无法执行。研究者构建了 Harmful-RLbench 数据集，并引入 POEX 框架，该框架通过对抗约束、策略评估器和后缀优化来生成可执行的有害策略，并在真实机器人臂和模拟器实验中证明其有效性，突显了 EAI 系统的安全漏洞和高转移性。最后，论文提出基于提示和模型的防御措施，成功率达 85%，强调了为 EAI 在关键应用中部署的 robust 安全措施的迫切需求。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.RO",
      "comment": "Homepage: https://poex-eai-jailbreak.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2412.16633v2",
      "published_date": "2024-12-21 13:58:27 UTC",
      "updated_date": "2025-02-10 08:13:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:10:07.487614"
    },
    {
      "arxiv_id": "2412.16631v1",
      "title": "Deep Learning for Spatio-Temporal Fusion in Land Surface Temperature Estimation: A Comprehensive Survey, Experimental Analysis, and Future Trends",
      "title_zh": "地表温度估计中时空融合的深度学习：全面综述、实验分析和未来趋势",
      "authors": [
        "Sofiane Bouaziz",
        "Adel Hafiane",
        "Raphael Canals",
        "Rachid Nedjai"
      ],
      "abstract": "The rapid advancements in satellite remote sensing have enhanced the\ncapability to monitor and analyze the Earth's surface. Among the many variables\ncaptured through satellite sensors, Land Surface Temperature (LST) plays a\ncritical role in understanding key environmental processes. However, obtaining\nhigh-resolution LST data remains a challenge, as satellite sensors often face a\ntrade-off between spatial and temporal resolutions. In response,\nSpatio-Temporal Fusion (STF) has emerged as a powerful method to integrate two\nsatellite data sources, one providing high spatial but low temporal resolution,\nand the other offering high temporal but low spatial resolution. Although a\nrange of STF techniques have been proposed, from traditional methods to\ncutting-edge deep learning (DL) models, most have focused on surface\nreflectance, with limited application to LST estimation. DL approaches, in\nparticular, show promise in improving the spatial and temporal resolutions of\nLST by capturing complex, non-linear relationships between input and output LST\ndata. This paper offers a comprehensive review of the latest advancements in\nDL-based STF techniques for LST estimation. We analyze key research\ndevelopments, mathematically formulate the STF problem, and introduce a novel\ntaxonomy for DL-based STF methods. Furthermore, we discuss the challenges faced\nby current methods and highlight future research directions. In addition, we\npresent the first open-source benchmark STF dataset for LST estimation,\nconsisting of 51 pairs of MODIS-Landsat images spanning from 2013 to 2024. To\nsupport our findings, we conduct extensive experiments on state-of-the-art\nmethods and present both quantitative and qualitative assessments. This is the\nfirst survey paper focused on DL-based STF for LST estimation. We hope it\nserves as a valuable reference for researchers and paves the way for future\nresearch in this field.",
      "tldr_zh": "这篇论文对基于深度学习（Deep Learning, DL）的时空融合（Spatio-Temporal Fusion, STF）技术在陆地表面温度（Land Surface Temperature, LST）估算中的应用进行了全面综述，分析了卫星遥感数据在空间和时间分辨率权衡上的挑战。论文数学公式化了STF问题，引入了一个新颖的DL方法分类法，并首次发布了一个开源基准数据集，包括2013-2024年的51对MODIS-Landsat图像。实验结果显示了现有方法的定量和定性评估，突出了DL在提升LST分辨率方面的潜力，同时讨论了当前挑战和未来研究方向。总的来说，这是有史以来第一个专注于DL-based STF for LST estimation的调查论文，为该领域的研究提供了宝贵参考。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Submitted to the Proceedings of IEEE",
      "pdf_url": "http://arxiv.org/pdf/2412.16631v1",
      "published_date": "2024-12-21 13:53:15 UTC",
      "updated_date": "2024-12-21 13:53:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:10:33.439129"
    },
    {
      "arxiv_id": "2412.16626v2",
      "title": "Mamba-SEUNet: Mamba UNet for Monaural Speech Enhancement",
      "title_zh": "翻译失败",
      "authors": [
        "Junyu Wang",
        "Zizhen Lin",
        "Tianrui Wang",
        "Meng Ge",
        "Longbiao Wang",
        "Jianwu Dang"
      ],
      "abstract": "In recent speech enhancement (SE) research, transformer and its variants have\nemerged as the predominant methodologies. However, the quadratic complexity of\nthe self-attention mechanism imposes certain limitations on practical\ndeployment. Mamba, as a novel state-space model (SSM), has gained widespread\napplication in natural language processing and computer vision due to its\nstrong capabilities in modeling long sequences and relatively low computational\ncomplexity. In this work, we introduce Mamba-SEUNet, an innovative architecture\nthat integrates Mamba with U-Net for SE tasks. By leveraging bidirectional\nMamba to model forward and backward dependencies of speech signals at different\nresolutions, and incorporating skip connections to capture multi-scale\ninformation, our approach achieves state-of-the-art (SOTA) performance.\nExperimental results on the VCTK+DEMAND dataset indicate that Mamba-SEUNet\nattains a PESQ score of 3.59, while maintaining low computational complexity.\nWhen combined with the Perceptual Contrast Stretching technique, Mamba-SEUNet\nfurther improves the PESQ score to 3.73.",
      "tldr_zh": "本研究提出 Mamba-SEUNet，一种创新架构，将 Mamba 状态空间模型（SSM）与 U-Net 整合，用于单声道语音增强（Monaural Speech Enhancement），以解决 Transformer 自注意力机制的二次复杂度问题。该方法利用双向 Mamba 建模语音信号的前向和后向依赖，并在不同分辨率下通过跳跃连接捕获多尺度信息，从而提升模型效率和性能。在 VCTK+DEMAND 数据集实验中，Mamba-SEUNet 达到了 PESQ 得分 3.59 的最先进（SOTA）水平，并通过结合 Perceptual Contrast Stretching 技术进一步提高至 3.73，同时维持低计算复杂度。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted at ICASSP 2025, 5 pages, 1 figures, 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2412.16626v2",
      "published_date": "2024-12-21 13:43:51 UTC",
      "updated_date": "2025-01-02 10:56:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:10:46.590275"
    },
    {
      "arxiv_id": "2412.16624v1",
      "title": "Automated Bleeding Detection and Classification in Wireless Capsule Endoscopy with YOLOv8-X",
      "title_zh": "翻译失败",
      "authors": [
        "Pavan C Shekar",
        "Vivek Kanhangad",
        "Shishir Maheshwari",
        "T Sunil Kumar"
      ],
      "abstract": "Gastrointestinal (GI) bleeding, a critical indicator of digestive system\ndisorders, re quires efficient and accurate detection methods. This paper\npresents our solution to the Auto-WCEBleedGen Version V1 Challenge, where we\nachieved the consolation position. We developed a unified YOLOv8-X model for\nboth detection and classification of bleeding regions in Wireless Capsule\nEndoscopy (WCE) images. Our approach achieved 96.10% classification accuracy\nand 76.8% mean Average Precision (mAP) at 0.5 IoU on the val idation dataset.\nThrough careful dataset curation and annotation, we assembled and trained on\n6,345 diverse images to ensure robust model performance. Our implementa tion\ncode and trained models are publicly available at\nhttps://github.com/pavan98765/Auto-WCEBleedGen.",
      "tldr_zh": "这篇论文提出了一种使用 YOLOv8-X 模型的自动化方法，用于在 Wireless Capsule Endoscopy (WCE) 图像中检测和分类胃肠道(GI)出血，以提高诊断效率。研究团队通过整理和标注6345张多样化图像，对模型进行了训练，并在Auto-WCEBleedGen Version V1 Challenge中获得安慰奖。结果显示，该模型在验证数据集上达到了96.10%的分类准确率和76.8%的mAP@0.5 IoU。代码和训练模型已公开在GitHub上，便于进一步研究和应用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 4 figures, challenge",
      "pdf_url": "http://arxiv.org/pdf/2412.16624v1",
      "published_date": "2024-12-21 13:37:11 UTC",
      "updated_date": "2024-12-21 13:37:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:10:57.549996"
    },
    {
      "arxiv_id": "2412.16616v1",
      "title": "Distributed Inference on Mobile Edge and Cloud: A Data-Cartography based Clustering Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Divya Jyoti Bajpai",
        "Manjesh Kumar Hanawal"
      ],
      "abstract": "The large size of DNNs poses a significant challenge for deployment on\ndevices with limited resources, such as mobile, edge, and IoT platforms. To\naddress this issue, a distributed inference framework can be utilized. In this\nframework, a small-scale DNN (initial layers) is deployed on mobile devices, a\nlarger version on edge devices, and the full DNN on the cloud. Samples with low\ncomplexity (easy) can be processed on mobile, those with moderate complexity\n(medium) on edge devices, and high complexity (hard) samples on the cloud.\nGiven that the complexity of each sample is unknown in advance, the crucial\nquestion in distributed inference is determining the sample complexity for\nappropriate DNN processing. We introduce a novel method named \\our{}, which\nleverages the Data Cartography approach initially proposed for enhancing DNN\ngeneralization. By employing data cartography, we assess sample complexity.\n\\our{} aims to boost accuracy while considering the offloading costs from\nmobile to edge/cloud. Our experimental results on GLUE datasets, covering a\nvariety of NLP tasks, indicate that our approach significantly lowers inference\ncosts by more than 43\\% while maintaining a minimal accuracy drop of less than\n0.5\\% compared to performing all inferences on the cloud. The source code is\navailable at https://anonymous.4open.science/r/DIMEC-1B04.",
      "tldr_zh": "这篇论文针对大型 DNN 在资源有限的设备（如移动、边缘和 IoT 平台）上的部署挑战，提出了一种分布式推理框架。\n该框架将 DNN 的初始层部署在移动设备、扩展层在边缘设备，以及完整模型在云端，并使用 Data Cartography 方法评估样本复杂度（easy、medium 或 hard），以决定最佳处理位置，从而平衡准确性和卸载成本。\n实验结果显示，在 GLUE 数据集上的多种 NLP 任务中，该方法降低了超过43%的推理成本，同时准确性仅下降不到0.5%。",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "arXiv admin note: substantial text overlap with arXiv:2410.05338",
      "pdf_url": "http://arxiv.org/pdf/2412.16616v1",
      "published_date": "2024-12-21 13:20:26 UTC",
      "updated_date": "2024-12-21 13:20:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:11:09.509106"
    },
    {
      "arxiv_id": "2412.16614v1",
      "title": "Automated Classification of Cybercrime Complaints using Transformer-based Language Models for Hinglish Texts",
      "title_zh": "翻译失败",
      "authors": [
        "Nanda Rani",
        "Divyanshu Singh",
        "Bikash Saha",
        "Sandeep Kumar Shukla"
      ],
      "abstract": "The rise in cybercrime and the complexity of multilingual and code-mixed\ncomplaints present significant challenges for law enforcement and cybersecurity\nagencies. These organizations need automated, scalable methods to identify\ncrime types, enabling efficient processing and prioritization of large\ncomplaint volumes. Manual triaging is inefficient, and traditional machine\nlearning methods fail to capture the semantic and contextual nuances of textual\ncybercrime complaints. Moreover, the lack of publicly available datasets and\nprivacy concerns hinder the research to present robust solutions. To address\nthese challenges, we propose a framework for automated cybercrime complaint\nclassification. The framework leverages Hinglish-adapted transformers, such as\nHingBERT and HingRoBERTa, to handle code-mixed inputs effectively. We employ\nthe real-world dataset provided by Indian Cybercrime Coordination Centre (I4C)\nduring CyberGuard AI Hackathon 2024. We employ GenAI open source model-based\ndata augmentation method to address class imbalance. We also employ\nprivacy-aware preprocessing to ensure compliance with ethical standards while\nmaintaining data integrity. Our solution achieves significant performance\nimprovements, with HingRoBERTa attaining an accuracy of 74.41% and an F1-score\nof 71.49%. We also develop ready-to-use tool by integrating Django REST backend\nwith a modern frontend. The developed tool is scalable and ready for real-world\ndeployment in platforms like the National Cyber Crime Reporting Portal. This\nwork bridges critical gaps in cybercrime complaint management, offering a\nscalable, privacy-conscious, and adaptable solution for modern cybersecurity\nchallenges.",
      "tldr_zh": "该研究针对网络犯罪投诉的多语言和代码混合（code-mixed）特性，提出了一种基于 Transformer 模型的自动分类框架，使用 HingBERT 和 HingRoBERTa 等 Hinglish 适配模型来捕捉文本的语义和上下文 nuances。框架利用印度网络犯罪协调中心 (I4C) 的真实数据集，通过 GenAI 模型进行数据增强解决类别不平衡，并采用隐私感知预处理确保数据完整性和合规性。实验结果显示，HingRoBERTa 模型实现了 74.41% 的准确率和 71.49% 的 F1-score，并开发了一个可扩展的 Django REST 后端工具，适用于真实部署如国家网络犯罪报告门户，提升了投诉处理的效率和可扩展性。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.16614v1",
      "published_date": "2024-12-21 13:17:09 UTC",
      "updated_date": "2024-12-21 13:17:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:11:22.509172"
    },
    {
      "arxiv_id": "2412.16602v1",
      "title": "V\"Mean\"ba: Visual State Space Models only need 1 hidden dimension",
      "title_zh": "翻译失败",
      "authors": [
        "Tien-Yu Chi",
        "Hung-Yueh Chiang",
        "Chi-Chih Chang",
        "Ning-Chi Huang",
        "Kai-Chiang Wu"
      ],
      "abstract": "Vision transformers dominate image processing tasks due to their superior\nperformance. However, the quadratic complexity of self-attention limits the\nscalability of these systems and their deployment on resource-constrained\ndevices. State Space Models (SSMs) have emerged as a solution by introducing a\nlinear recurrence mechanism, which reduces the complexity of sequence modeling\nfrom quadratic to linear. Recently, SSMs have been extended to high-resolution\nvision tasks. Nonetheless, the linear recurrence mechanism struggles to fully\nutilize matrix multiplication units on modern hardware, resulting in a\ncomputational bottleneck. We address this issue by introducing\n\\textit{VMeanba}, a training-free compression method that eliminates the\nchannel dimension in SSMs using mean operations. Our key observation is that\nthe output activations of SSM blocks exhibit low variances across channels. Our\n\\textit{VMeanba} leverages this property to optimize computation by averaging\nactivation maps across the channel to reduce the computational overhead without\ncompromising accuracy. Evaluations on image classification and semantic\nsegmentation tasks demonstrate that \\textit{VMeanba} achieves up to a 1.12x\nspeedup with less than a 3\\% accuracy loss. When combined with 40\\%\nunstructured pruning, the accuracy drop remains under 3\\%.",
      "tldr_zh": "该论文针对视觉状态空间模型(SSMs)的计算瓶颈问题，提出了一种无训练压缩方法VMeanba，通过利用SSMs块输出激活在通道间的低方差，采用均值操作消除通道维度，从而将隐藏维度减少到1，实现线性复杂度优化。VMeanba无需额外训练即可减少计算开销，同时在图像分类和语义分割任务上实现了高达1.12倍的速度提升，准确性损失不到3%。当结合40%的无结构剪枝时，模型性能进一步保持，准确性下降仍小于3%。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by NeurIPS 2024 Machine Learning for Systems workshop",
      "pdf_url": "http://arxiv.org/pdf/2412.16602v1",
      "published_date": "2024-12-21 12:27:07 UTC",
      "updated_date": "2024-12-21 12:27:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:11:35.693768"
    },
    {
      "arxiv_id": "2412.16599v1",
      "title": "Do Multimodal Language Models Really Understand Direction? A Benchmark for Compass Direction Reasoning",
      "title_zh": "多模态语言模型真的理解方向",
      "authors": [
        "Hang Yin",
        "Zhifeng Lin",
        "Xin Liu",
        "Bin Sun",
        "Kan Li"
      ],
      "abstract": "Direction reasoning is essential for intelligent systems to understand the\nreal world. While existing work focuses primarily on spatial reasoning, compass\ndirection reasoning remains underexplored. To address this, we propose the\nCompass Direction Reasoning (CDR) benchmark, designed to evaluate the direction\nreasoning capabilities of multimodal language models (MLMs). CDR includes three\ntypes images to test spatial (up, down, left, right) and compass (north, south,\neast, west) directions. Our evaluation reveals that most MLMs struggle with\ndirection reasoning, often performing at random guessing levels. Experiments\nshow that training directly with CDR data yields limited improvements, as it\nrequires an understanding of real-world physical rules. We explore the impact\nof mixdata and CoT fine-tuning methods, which significantly enhance MLM\nperformance in compass direction reasoning by incorporating diverse data and\nstep-by-step reasoning, improving the model's ability to understand direction\nrelationships.",
      "tldr_zh": "这篇论文提出了 Compass Direction Reasoning (CDR) 基准，用于评估多模态语言模型 (MLMs) 在方向推理方面的能力，特别是针对空间方向（up, down, left, right）和罗盘方向（north, south, east, west）的理解。实验发现，大多数 MLMs 在方向推理任务上表现不佳，往往接近随机猜测水平，且直接使用 CDR 数据训练的效果有限，因为这需要对真实世界物理规则的深刻理解。通过混合数据 (mixdata) 和 Chain-of-Thought (CoT) 微调方法，论文显著提升了模型的性能，帮助其更好地处理方向关系。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.16599v1",
      "published_date": "2024-12-21 12:09:13 UTC",
      "updated_date": "2024-12-21 12:09:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:11:45.841881"
    },
    {
      "arxiv_id": "2412.16594v2",
      "title": "AIGCodeSet: A New Annotated Dataset for AI Generated Code Detection",
      "title_zh": "AIGCodeSet：一个新的标注数据集，用于AI生成",
      "authors": [
        "Basak Demirok",
        "Mucahid Kutlu"
      ],
      "abstract": "While large language models provide significant convenience for software\ndevelopment, they can lead to ethical issues in job interviews and student\nassignments. Therefore, determining whether a piece of code is written by a\nhuman or generated by an artificial intelligence (AI) model is a critical\nissue. In this study, we present AIGCodeSet, which consists of 2.828\nAI-generated and 4.755 human-written Python codes, created using CodeLlama 34B,\nCodestral 22B, and Gemini 1.5 Flash. In addition, we share the results of our\nexperiments conducted with baseline detection methods. Our experiments show\nthat a Bayesian classifier outperforms the other models.",
      "tldr_zh": "该论文引入了 AIGCodeSet，这是一个新的标注数据集，用于检测代码是否由 AI 生成，以解决软件开发中的伦理问题，如工作面试和学生作业。该数据集包含 2,828 个由 CodeLlama 34B、Codestral 22B 和 Gemini 1.5 Flash 生成的 Python 代码，以及 4,755 个人类编写的代码。论文通过实验比较了基线检测方法，结果显示 Bayesian classifier 在性能上优于其他模型。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.16594v2",
      "published_date": "2024-12-21 11:53:49 UTC",
      "updated_date": "2025-03-09 10:31:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:11:57.438508"
    },
    {
      "arxiv_id": "2412.16581v1",
      "title": "Effective and Efficient Representation Learning for Flight Trajectories",
      "title_zh": "有效且高效的飞行轨迹表示学习",
      "authors": [
        "Shuo Liu",
        "Wenbin Li",
        "Di Yao",
        "Jingping Bi"
      ],
      "abstract": "Flight trajectory data plays a vital role in the traffic management\ncommunity, especially for downstream tasks such as trajectory prediction,\nflight recognition, and anomaly detection. Existing works often utilize\nhandcrafted features and design models for different tasks individually, which\nheavily rely on domain expertise and are hard to extend. We argue that\ndifferent flight analysis tasks share the same useful features of the\ntrajectory. Jointly learning a unified representation for flight trajectories\ncould be beneficial for improving the performance of various tasks. However,\nflight trajectory representation learning (TRL) faces two primary challenges,\n\\ie unbalanced behavior density and 3D spatial continuity, which disable recent\ngeneral TRL methods. In this paper, we propose Flight2Vec , a flight-specific\nrepresentation learning method to address these challenges. Specifically, a\nbehavior-adaptive patching mechanism is used to inspire the learned\nrepresentation to pay more attention to behavior-dense segments. Moreover, we\nintroduce a motion trend learning technique that guides the model to memorize\nnot only the precise locations, but also the motion trend to generate better\nrepresentations. Extensive experimental results demonstrate that Flight2Vec\nsignificantly improves performance in downstream tasks such as flight\ntrajectory prediction, flight recognition, and anomaly detection.",
      "tldr_zh": "该论文提出了一种高效的飞行轨迹表示学习方法 Flight2Vec，以解决现有方法依赖手工特征和任务特定设计的局限性。Flight2Vec 通过 behavior-adaptive patching mechanism 关注行为密集段，以及 motion trend learning technique 记忆精确位置和运动趋势，从而应对不平衡行为密度和3D空间连续性等挑战。实验结果表明，该方法在下游任务如轨迹预测、飞行识别和异常检测上显著提升性能，为飞行轨迹分析提供统一的表示学习框架。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2412.16581v1",
      "published_date": "2024-12-21 10:59:54 UTC",
      "updated_date": "2024-12-21 10:59:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:12:09.150137"
    },
    {
      "arxiv_id": "2412.16572v1",
      "title": "Breaking the Context Bottleneck on Long Time Series Forecasting",
      "title_zh": "打破长时序预测中的上下文瓶颈",
      "authors": [
        "Chao Ma",
        "Yikai Hou",
        "Xiang Li",
        "Yinggang Sun",
        "Haining Yu",
        "Zhou Fang",
        "Jiaxing Qu"
      ],
      "abstract": "Long-term time-series forecasting is essential for planning and\ndecision-making in economics, energy, and transportation, where long foresight\nis required. To obtain such long foresight, models must be both efficient and\neffective in processing long sequence. Recent advancements have enhanced the\nefficiency of these models; however, the challenge of effectively leveraging\nlonger sequences persists. This is primarily due to the tendency of these\nmodels to overfit when presented with extended inputs, necessitating the use of\nshorter input lengths to maintain tolerable error margins. In this work, we\ninvestigate the multiscale modeling method and propose the Logsparse\nDecomposable Multiscaling (LDM) framework for the efficient and effective\nprocessing of long sequences. We demonstrate that by decoupling patterns at\ndifferent scales in time series, we can enhance predictability by reducing\nnon-stationarity, improve efficiency through a compact long input\nrepresentation, and simplify the architecture by providing clear task\nassignments. Experimental results demonstrate that LDM not only outperforms all\nbaselines in long-term forecasting benchmarks, but also reducing both training\ntime and memory costs.",
      "tldr_zh": "该论文针对长期时间序列预测面临的上下文瓶颈问题，探讨了多尺度建模方法，以解决模型在处理长序列时容易过拟合的挑战。作者提出Logsparse Decomposable Multiscaling (LDM)框架，通过解耦时间序列中不同尺度的模式，减少非平稳性，提供紧凑的输入表示，并简化模型架构，从而提升预测效率和效果。实验结果显示，LDM在长期预测基准上优于所有基线模型，同时显著降低了训练时间和内存成本。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Time series forecasting algorithm based on multi-scale analysis",
      "pdf_url": "http://arxiv.org/pdf/2412.16572v1",
      "published_date": "2024-12-21 10:29:34 UTC",
      "updated_date": "2024-12-21 10:29:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:12:21.361896"
    },
    {
      "arxiv_id": "2412.16565v1",
      "title": "Learning for Cross-Layer Resource Allocation in MEC-Aided Cell-Free Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Chong Zheng",
        "Shiwen He",
        "Yongming Huang",
        "Tony Q. S. Quek"
      ],
      "abstract": "Cross-layer resource allocation over mobile edge computing (MEC)-aided\ncell-free networks can sufficiently exploit the transmitting and computing\nresources to promote the data rate. However, the technical bottlenecks of\ntraditional methods pose significant challenges to cross-layer optimization. In\nthis paper, joint subcarrier allocation and beamforming optimization are\ninvestigated for the MEC-aided cell-free network from the perspective of deep\nlearning to maximize the weighted sum rate. Specifically, we convert the\nunderlying problem into a joint multi-task optimization problem and then\npropose a centralized multi-task self-supervised learning algorithm to solve\nthe problem so as to avoid costly manual labeling. Therein, two novel and\ngeneral loss functions, i.e., negative fraction linear loss and exponential\nlinear loss whose advantages in robustness and target domain have been proved\nand discussed, are designed to enable self-supervised learning. Moreover, we\nfurther design a MEC-enabled distributed multi-task self-supervised learning\n(DMTSSL) algorithm, with low complexity and high scalability to address the\nchallenge of dimensional disaster. Finally, we develop the distance-aware\ntransfer learning algorithm based on the DMTSSL algorithm to handle the dynamic\nscenario with negligible computation cost. Simulation results under $3$rd\ngeneration partnership project 38.901 urban-macrocell scenario demonstrate the\nsuperiority of the proposed algorithms over the baseline algorithms.",
      "tldr_zh": "这篇论文探讨了在移动边缘计算 (MEC)-aided cell-free networks 中，通过深度学习优化跨层资源分配，以最大化加权总速率，解决传统方法的瓶颈问题。研究人员将问题转化为联合多任务优化，并提出了一种中心化多任务自监督学习算法，使用了两种新颖损失函数（negative fraction linear loss 和 exponential linear loss），以避免手动标注并提升鲁棒性。此外，他们开发了分布式多任务自监督学习 (DMTSSL) 算法和基于距离的迁移学习算法，以提高可扩展性和处理动态场景；模拟结果在 3rd Generation Partnership Project 38.901 场景下显示，这些算法在性能上显著优于基线方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.16565v1",
      "published_date": "2024-12-21 10:18:55 UTC",
      "updated_date": "2024-12-21 10:18:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:12:34.498792"
    },
    {
      "arxiv_id": "2412.16564v1",
      "title": "Predictive Monitoring of Black-Box Dynamical Systems",
      "title_zh": "黑箱动力系统的预测监测",
      "authors": [
        "Thomas A. Henzinger",
        "Fabian Kresse",
        "Kaushik Mallik",
        "Emily Yu",
        "Đorđe Žikelić"
      ],
      "abstract": "We study the problem of predictive runtime monitoring of black-box dynamical\nsystems with quantitative safety properties. The black-box setting stipulates\nthat the exact semantics of the dynamical system and the controller are\nunknown, and that we are only able to observe the state of the controlled (aka,\nclosed-loop) system at finitely many time points. We present a novel framework\nfor predicting future states of the system based on the states observed in the\npast. The numbers of past states and of predicted future states are parameters\nprovided by the user. Our method is based on a combination of Taylor's\nexpansion and the backward difference operator for numerical differentiation.\nWe also derive an upper bound on the prediction error under the assumption that\nthe system dynamics and the controller are smooth. The predicted states are\nthen used to predict safety violations ahead in time. Our experiments\ndemonstrate practical applicability of our method for complex black-box\nsystems, showing that it is computationally lightweight and yet significantly\nmore accurate than the state-of-the-art predictive safety monitoring\ntechniques.",
      "tldr_zh": "这篇论文研究了黑箱动态系统的预测性运行时监控问题，针对定量安全属性，通过仅观察有限时间点的系统状态来预测未来状态。作者提出了一种新框架，结合 Taylor's expansion 和 backward difference operator 进行数值微分，以生成预测状态，并导出了系统动态和控制器平滑时的预测误差上界。实验结果显示，该方法在复杂黑箱系统中计算轻量级，且准确性比现有预测安全监控技术显著提高。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.SY"
      ],
      "primary_category": "eess.SY",
      "comment": "Submitted to L4DC 2025",
      "pdf_url": "http://arxiv.org/pdf/2412.16564v1",
      "published_date": "2024-12-21 10:17:46 UTC",
      "updated_date": "2024-12-21 10:17:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:12:44.893585"
    },
    {
      "arxiv_id": "2412.17867v4",
      "title": "Evaluating and Enhancing LLMs for Multi-turn Text-to-SQL with Multiple Question Types",
      "title_zh": "翻译失败",
      "authors": [
        "Ziming Guo",
        "Chao Ma",
        "Yinggang Sun",
        "Tiancheng Zhao",
        "Guangyao Wang",
        "Hai Huang"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have significantly\nadvanced text-to-SQL systems. However, most LLM-based methods often narrowly\nfocus on SQL generation, neglecting the complexities of real-world\nconversational queries. This oversight can lead to unreliable responses,\nparticularly for ambiguous questions that cannot be directly addressed with\nSQL. To bridge this gap, we propose MMSQL, a comprehensive test suite designed\nto evaluate the question classification and SQL generation capabilities of LLMs\nby simulating real-world scenarios with diverse question types and multi-turn\nQ&A interactions. Using MMSQL, we assessed the performance of popular LLMs,\nincluding both open-source and closed-source models, and identified key factors\nimpacting their performance in such scenarios. Moreover, we introduce an\nLLM-based multi-agent framework that employs specialized agents to identify\nquestion types and determine appropriate answering strategies. Our experiments\ndemonstrate that this approach significantly enhances the model's ability to\nnavigate the complexities of conversational dynamics, effectively handling the\ndiverse and complex nature of user queries. Our dataset and code are publicly\navailable at https://mcxiaoxiao.github.io/MMSQL.",
      "tldr_zh": "该论文评估并提升了大型语言模型（LLMs）在多轮文本到SQL任务中的性能，特别是在处理多种问题类型（如模糊查询）的复杂对话场景时。研究者提出了MMSQL测试套件，用于模拟真实世界情景，评估LLMs的问题分类和SQL生成能力，并识别影响性能的关键因素。通过引入一个基于LLMs的多智能体框架，该框架使用专门智能体识别问题类型并确定适当的回答策略，实验显示这种方法显著提高了模型应对对话动态的能力。数据集和代码已在https://mcxiaoxiao.github.io/MMSQL公开可用。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "International Joint Conference on Neural Networks 2025 (IJCNN 2025)",
      "pdf_url": "http://arxiv.org/pdf/2412.17867v4",
      "published_date": "2024-12-21 10:13:45 UTC",
      "updated_date": "2025-04-08 02:23:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:12:58.190184"
    },
    {
      "arxiv_id": "2412.16559v1",
      "title": "Metagoals Endowing Self-Modifying AGI Systems with Goal Stability or Moderated Goal Evolution: Toward a Formally Sound and Practical Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Ben Goertzel"
      ],
      "abstract": "We articulate here a series of specific metagoals designed to address the\nchallenge of creating AGI systems that possess the ability to flexibly\nself-modify yet also have the propensity to maintain key invariant properties\nof their goal systems\n  1) a series of goal-stability metagoals aimed to guide a system to a\ncondition in which goal-stability is compatible with reasonably flexible\nself-modification\n  2) a series of moderated-goal-evolution metagoals aimed to guide a system to\na condition in which control of the pace of goal evolution is compatible with\nreasonably flexible self-modification\n  The formulation of the metagoals is founded on fixed-point theorems from\nfunctional analysis, e.g. the Contraction Mapping Theorem and constructive\napproximations to Schauder's Theorem, applied to probabilistic models of system\nbehavior\n  We present an argument that the balancing of self-modification with\nmaintenance of goal invariants will often have other interesting cognitive\nside-effects such as a high degree of self understanding\n  Finally we argue for the practical value of a hybrid metagoal combining\nmoderated-goal-evolution with pursuit of goal-stability -- along with\npotentially other metagoals relating to goal-satisfaction, survival and ongoing\ndevelopment -- in a flexible fashion depending on the situation",
      "tldr_zh": "该论文提出了一系列 metagoals，用于赋予自修改 AGI 系统以 goal-stability 或 moderated-goal-evolution 的能力，确保系统在灵活自修改的同时保持关键目标不变性。metagoals 分为两类：goal-stability metagoals 引导系统实现目标稳定与自修改的兼容性，以及 moderated-goal-evolution metagoals 控制目标演变的节奏。方法基于功能分析的固定点定理，如 Contraction Mapping Theorem 和 Schauder's Theorem，应用于系统行为的概率模型。论文还论证，这种平衡可能带来高度的自理解，并推荐一个混合 metagoal 框架，在实际应用中灵活结合多种目标以适应不同情境。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.16559v1",
      "published_date": "2024-12-21 09:57:13 UTC",
      "updated_date": "2024-12-21 09:57:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:13:10.702031"
    },
    {
      "arxiv_id": "2412.16557v1",
      "title": "CognTKE: A Cognitive Temporal Knowledge Extrapolation Framework",
      "title_zh": "CognTKE：认知时间知识外推框架",
      "authors": [
        "Wei Chen",
        "Yuting Wu",
        "Shuhan Wu",
        "Zhiyu Zhang",
        "Mengqi Liao",
        "Youfang Lin",
        "Huaiyu Wan"
      ],
      "abstract": "Reasoning future unknowable facts on temporal knowledge graphs (TKGs) is a\nchallenging task, holding significant academic and practical values for various\nfields. Existing studies exploring explainable reasoning concentrate on\nmodeling comprehensible temporal paths relevant to the query. Yet, these\npath-based methods primarily focus on local temporal paths appearing in recent\ntimes, failing to capture the complex temporal paths in TKG and resulting in\nthe loss of longer historical relations related to the query. Motivated by the\nDual Process Theory in cognitive science, we propose a \\textbf{Cogn}itive\n\\textbf{T}emporal \\textbf{K}nowledge \\textbf{E}xtrapolation framework\n(CognTKE), which introduces a novel temporal cognitive relation directed graph\n(TCR-Digraph) and performs interpretable global shallow reasoning and local\ndeep reasoning over the TCR-Digraph. Specifically, the proposed TCR-Digraph is\nconstituted by retrieving significant local and global historical temporal\nrelation paths associated with the query. In addition, CognTKE presents the\nglobal shallow reasoner and the local deep reasoner to perform global one-hop\ntemporal relation reasoning (System 1) and local complex multi-hop path\nreasoning (System 2) over the TCR-Digraph, respectively. The experimental\nresults on four benchmark datasets demonstrate that CognTKE achieves\nsignificant improvement in accuracy compared to the state-of-the-art baselines\nand delivers excellent zero-shot reasoning ability. \\textit{The code is\navailable at https://github.com/WeiChen3690/CognTKE}.",
      "tldr_zh": "本研究提出CognTKE框架，旨在解决在Temporal Knowledge Graphs (TKGs)上推理未来未知事实的挑战，该框架受Dual Process Theory启发，构建了时间认知关系有向图(TCR-Digraph)来捕捉与查询相关的本地和全局历史时间路径。CognTKE包括全局浅层推理器(System 1)进行单跳关系推理，以及本地深层推理器(System 2)进行复杂多跳路径推理，从而实现更全面的解释性推理。实验结果显示，该框架在四个基准数据集上比现有基线方法准确率显著提升，并展示了优秀的零样本推理能力。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "AAAI2025 Accept, 12 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.16557v1",
      "published_date": "2024-12-21 09:50:55 UTC",
      "updated_date": "2024-12-21 09:50:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:13:21.627959"
    },
    {
      "arxiv_id": "2412.16552v1",
      "title": "Diffusion Prior Interpolation for Flexibility Real-World Face Super-Resolution",
      "title_zh": "翻译失败",
      "authors": [
        "Jiarui Yang",
        "Tao Dai",
        "Yufei Zhu",
        "Naiqi Li",
        "Jinmin Li",
        "Shutao Xia"
      ],
      "abstract": "Diffusion models represent the state-of-the-art in generative modeling. Due\nto their high training costs, many works leverage pre-trained diffusion models'\npowerful representations for downstream tasks, such as face super-resolution\n(FSR), through fine-tuning or prior-based methods. However, relying solely on\npriors without supervised training makes it challenging to meet the pixel-level\naccuracy requirements of discrimination task. Although prior-based methods can\nachieve high fidelity and high-quality results, ensuring consistency remains a\nsignificant challenge. In this paper, we propose a masking strategy with strong\nand weak constraints and iterative refinement for real-world FSR, termed\nDiffusion Prior Interpolation (DPI). We introduce conditions and constraints on\nconsistency by masking different sampling stages based on the structural\ncharacteristics of the face. Furthermore, we propose a condition Corrector\n(CRT) to establish a reciprocal posterior sampling process, enhancing FSR\nperformance by mutual refinement of conditions and samples. DPI can balance\nconsistency and diversity and can be seamlessly integrated into pre-trained\nmodels. In extensive experiments conducted on synthetic and real datasets,\nalong with consistency validation in face recognition, DPI demonstrates\nsuperiority over SOTA FSR methods. The code is available at\n\\url{https://github.com/JerryYann/DPI}.",
      "tldr_zh": "本论文提出了一种名为 Diffusion Prior Interpolation (DPI) 的方法，用于提升真实世界人脸超分辨率 (FSR) 的灵活性和一致性。DPI 通过掩码策略结合强弱约束以及迭代精炼，在不同采样阶段基于人脸结构特性引入条件和约束，同时引入条件校正器 (CRT) 来实现互惠的后验采样过程，从而平衡图像的一致性和多样性，并无缝集成到预训练扩散模型中。在合成和真实数据集上的广泛实验中，DPI 优于现有最先进 FSR 方法，并在人脸识别一致性验证中表现出色。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to AAAI25",
      "pdf_url": "http://arxiv.org/pdf/2412.16552v1",
      "published_date": "2024-12-21 09:28:44 UTC",
      "updated_date": "2024-12-21 09:28:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:13:33.464713"
    },
    {
      "arxiv_id": "2412.16547v1",
      "title": "ActPC-Chem: Discrete Active Predictive Coding for Goal-Guided Algorithmic Chemistry as a Potential Cognitive Kernel for Hyperon & PRIMUS-Based AGI",
      "title_zh": "ActPC-Chem：用于目标引导算法化学的离散主动预测编码，作为 Hyperon 与 PRIMUS 基础 AGI 的潜在认知内核",
      "authors": [
        "Ben Goertzel"
      ],
      "abstract": "We explore a novel paradigm (labeled ActPC-Chem) for biologically inspired,\ngoal-guided artificial intelligence (AI) centered on a form of Discrete Active\nPredictive Coding (ActPC) operating within an algorithmic chemistry of rewrite\nrules. ActPC-Chem is envisioned as a foundational \"cognitive kernel\" for\nadvanced cognitive architectures, such as the OpenCog Hyperon system,\nincorporating essential elements of the PRIMUS cognitive architecture. The\ncentral thesis is that general-intelligence-capable cognitive structures and\ndynamics can emerge in a system where both data and models are represented as\nevolving patterns of metagraph rewrite rules, and where prediction errors,\nintrinsic and extrinsic rewards, and semantic constraints guide the continual\nreorganization and refinement of these rules. Using a virtual \"robot bug\"\nthought experiment, we illustrate how such a system might self-organize to\nhandle challenging tasks involving delayed and context-dependent rewards,\nintegrating causal rule inference (AIRIS) and probabilistic logical abstraction\n(PLN) to discover and exploit conceptual patterns and causal constraints. Next,\nwe describe how continuous predictive coding neural networks, which excel at\nhandling noisy sensory data and motor control signals, can be coherently merged\nwith the discrete ActPC substrate. Finally, we outline how these ideas might be\nextended to create a transformer-like architecture that foregoes traditional\nbackpropagation in favor of rule-based transformations guided by ActPC. This\nlayered architecture, supplemented with AIRIS and PLN, promises structured,\nmulti-modal, and logically consistent next-token predictions and narrative\nsequences.",
      "tldr_zh": "本论文提出 ActPC-Chem，一种基于 Discrete Active Predictive Coding 的算法化学框架，用于生物启发的目标导向 AI，作为 OpenCog Hyperon 系统和 PRIMUS 认知架构的潜在认知内核。核心机制涉及元图重写规则的演化，受预测错误、内在外在奖励及语义约束指导，实现系统的自组织和一般智能发展。论文通过虚拟 robot bug 实验展示该框架整合 AIRIS（因果规则推理）和 PLN（概率逻辑抽象）来处理延迟奖励任务，并探讨将其与连续预测编码神经网络融合，扩展到类似 Transformer 的规则-based 架构，以实现结构化、多模态和逻辑一致的预测。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.16547v1",
      "published_date": "2024-12-21 09:14:25 UTC",
      "updated_date": "2024-12-21 09:14:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:13:47.563704"
    },
    {
      "arxiv_id": "2412.16543v3",
      "title": "Mathematics and Machine Creativity: A Survey on Bridging Mathematics with AI",
      "title_zh": "数学与机器创造力：关于桥接数学与 AI 的调查",
      "authors": [
        "Shizhe Liang",
        "Wei Zhang",
        "Tianyang Zhong",
        "Tianming Liu"
      ],
      "abstract": "This paper presents a comprehensive overview on the applications of\nartificial intelligence (AI) in mathematical research, highlighting the\ntransformative role AI has begun to play in this domain. Traditionally, AI\nadvancements have heavily relied on theoretical foundations provided by\nmathematics and statistics. However, recent developments in AI, particularly in\nreinforcement learning (RL) and large language models (LLMs), have demonstrated\nthe potential for AI to contribute back to mathematics by offering flexible\nalgorithmic frameworks and powerful inductive reasoning capabilities that\nsupport various aspects of mathematical research. This survey aims to establish\na bridge between AI and mathematics, providing insights into the mutual\nbenefits and fostering deeper interdisciplinary understanding.\n  In particular, we argue that while current AI and LLMs may struggle with\ncomplex deductive reasoning, their \"inherent creativity\", the ability to\ngenerate outputs at high throughput based on recognition of shallow patterns,\nholds significant potential to support and inspire mathematical research. This\ncreative capability, often overlooked, could be the key to unlocking new\nperspectives and methodologies in mathematics. Furthermore, we address the lack\nof cross-disciplinary communication: mathematicians may not fully comprehend\nthe latest advances in AI, while AI researchers frequently prioritize benchmark\nperformance over real-world applications in frontier mathematical research.\nThis paper seeks to close that gap, offering a detailed exploration of AI\nfundamentals, its strengths, and its emerging applications in the mathematical\nsciences.",
      "tldr_zh": "这篇综述论文探讨了人工智能（AI）在数学研究中的应用，强调AI如何从传统依赖数学基础转向反向贡献，例如通过强化学习（RL）和大型语言模型（LLMs）提供灵活算法框架和归纳推理能力。论文指出，尽管AI在复杂演绎推理上存在挑战，但其“inherent creativity”（内在创造力）——基于浅层模式的高通量输出——能够支持和启发数学研究的新视角与方法。作者还强调了跨学科沟通的缺失，如数学家对AI进展的了解不足，以及AI研究者偏重基准性能而非实际应用，并通过详细探索AI基础和新兴应用来桥接这一鸿沟。该研究促进了AI与数学的相互益处，推动更深层的跨领域合作。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "26 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.16543v3",
      "published_date": "2024-12-21 08:58:36 UTC",
      "updated_date": "2025-03-25 02:03:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:13:57.920682"
    },
    {
      "arxiv_id": "2412.19832v1",
      "title": "Back To The Future: A Hybrid Transformer-XGBoost Model for Action-oriented Future-proofing Nowcasting",
      "title_zh": "翻译失败",
      "authors": [
        "Ziheng Sun"
      ],
      "abstract": "Inspired by the iconic movie Back to the Future, this paper explores an\ninnovative adaptive nowcasting approach that reimagines the relationship\nbetween present actions and future outcomes. In the movie, characters travel\nthrough time to manipulate past events, aiming to create a better future.\nAnalogously, our framework employs predictive insights about the future to\ninform and adjust present conditions. This dual-stage model integrates the\nforecasting power of Transformers (future visionary) with the interpretability\nand efficiency of XGBoost (decision maker), enabling a seamless loop of future\nprediction and present adaptation. Through experimentation with meteorological\ndatasets, we demonstrate the framework's advantage in achieving more accurate\nforecasting while guiding actionable interventions for real-time applications.",
      "tldr_zh": "本论文受电影《Back to the Future》启发，提出了一种创新的自适应 nowcasting 框架，用于通过未来预测指导当前行动，实现行动导向的未来证明机制。该框架采用双阶段混合模型，将 Transformer 的预测能力（作为“未来远见”）与 XGBoost 的解释性和效率（作为“决策者”）相结合，形成一个预测与适应的无缝循环。通过气象数据集实验，模型展示了比传统方法更高的准确性，并为实时应用提供可行动的干预建议。该方法为动态决策领域提供了新的范例。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.19832v1",
      "published_date": "2024-12-21 08:53:28 UTC",
      "updated_date": "2024-12-21 08:53:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:14:08.647965"
    },
    {
      "arxiv_id": "2412.16540v1",
      "title": "Prior2Posterior: Model Prior Correction for Long-Tailed Learning",
      "title_zh": "Prior2Posterior：模型先验修正用于长尾学习",
      "authors": [
        "S Divakar Bhat",
        "Amit More",
        "Mudit Soni",
        "Surbhi Agrawal"
      ],
      "abstract": "Learning-based solutions for long-tailed recognition face difficulties in\ngeneralizing on balanced test datasets. Due to imbalanced data prior, the\nlearned \\textit{a posteriori} distribution is biased toward the most frequent\n(head) classes, leading to an inferior performance on the least frequent (tail)\nclasses. In general, the performance can be improved by removing such a bias by\neliminating the effect of imbalanced prior modeled using the number of class\nsamples (frequencies). We first observe that the \\textit{effective prior} on\nthe classes, learned by the model at the end of the training, can differ from\nthe empirical prior obtained using class frequencies. Thus, we propose a novel\napproach to accurately model the effective prior of a trained model using\n\\textit{a posteriori} probabilities. We propose to correct the imbalanced prior\nby adjusting the predicted \\textit{a posteriori} probabilities\n(Prior2Posterior: P2P) using the calculated prior in a post-hoc manner after\nthe training, and show that it can result in improved model performance. We\npresent theoretical analysis showing the optimality of our approach for models\ntrained with naive cross-entropy loss as well as logit adjusted loss. Our\nexperiments show that the proposed approach achieves new state-of-the-art\n(SOTA) on several benchmark datasets from the long-tail literature in the\ncategory of logit adjustment methods. Further, the proposed approach can be\nused to inspect any existing method to capture the \\textit{effective prior} and\nremove any residual bias to improve its performance, post-hoc, without model\nretraining. We also show that by using the proposed post-hoc approach, the\nperformance of many existing methods can be improved further.",
      "tldr_zh": "该论文解决了长尾学习(Long-Tailed Learning)中模型因数据不平衡先验而偏向头部类别的性能问题，通过观察模型学到的有效先验可能与经验先验不同。研究提出Prior2Posterior (P2P)方法，使用后验概率(a posteriori)建模有效先验，并在训练后(post-hoc)调整预测概率以消除偏差。理论分析证明了该方法对使用朴素交叉熵损失或对数调整损失训练的模型的优越性。实验结果显示，P2P在多个长尾基准数据集上达到了新的SOTA(State-of-the-Art)性能，并能后验改进现有方法，而无需重新训练。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)\n  2025",
      "pdf_url": "http://arxiv.org/pdf/2412.16540v1",
      "published_date": "2024-12-21 08:49:02 UTC",
      "updated_date": "2024-12-21 08:49:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:14:21.700465"
    },
    {
      "arxiv_id": "2412.16539v1",
      "title": "Towards Environmentally Equitable AI",
      "title_zh": "翻译失败",
      "authors": [
        "Mohammad Hajiesmaili",
        "Shaolei Ren",
        "Ramesh K. Sitaraman",
        "Adam Wierman"
      ],
      "abstract": "The skyrocketing demand for artificial intelligence (AI) has created an\nenormous appetite for globally deployed power-hungry servers. As a result, the\nenvironmental footprint of AI systems has come under increasing scrutiny. More\ncrucially, the current way that we exploit AI workloads' flexibility and manage\nAI systems can lead to wildly different environmental impacts across locations,\nincreasingly raising environmental inequity concerns and creating unintended\nsociotechnical consequences. In this paper, we advocate environmental equity as\na priority for the management of future AI systems, advancing the boundaries of\nexisting resource management for sustainable AI and also adding a unique\ndimension to AI fairness. Concretely, we uncover the potential of equity-aware\ngeographical load balancing to fairly re-distribute the environmental cost\nacross different regions, followed by algorithmic challenges. We conclude by\ndiscussing a few future directions to exploit the full potential of system\nmanagement approaches to mitigate AI's environmental inequity.",
      "tldr_zh": "该论文讨论了人工智能（AI）系统对环境的巨大影响，特别是全球服务器能源消耗导致的环境不平等问题，强调当前AI负载管理方式可能加剧不同地区的不公平后果。作者主张将environmental equity（环境公平）作为未来AI系统管理的核心优先事项，并扩展了AI fairness（AI公平）的概念。论文具体探讨了equity-aware geographical load balancing（公平地理负载平衡）的潜力，以重新分配环境成本，同时分析了相关算法挑战，并提出未来系统管理方向来缓解AI的环境不平等。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by Communications of the ACM. All the authors contributed\n  equally and are listed in alphabetical order of last name",
      "pdf_url": "http://arxiv.org/pdf/2412.16539v1",
      "published_date": "2024-12-21 08:46:19 UTC",
      "updated_date": "2024-12-21 08:46:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:14:33.036902"
    },
    {
      "arxiv_id": "2412.16531v1",
      "title": "From Creation to Curriculum: Examining the role of generative AI in Arts Universities",
      "title_zh": "翻译失败",
      "authors": [
        "Atticus Sims"
      ],
      "abstract": "The age of Artificial Intelligence (AI) is marked by its transformative\n\"generative\" capabilities, distinguishing it from prior iterations. This\nburgeoning characteristic of AI has enabled it to produce new and original\ncontent, inherently showcasing its creative prowess. This shift challenges and\nrequires a recalibration in the realm of arts education, urging a departure\nfrom established pedagogies centered on human-driven image creation. The paper\nmeticulously addresses the integration of AI tools, with a spotlight on Stable\nDiffusion (SD), into university arts curricula. Drawing from practical insights\ngathered from workshops conducted in July 2023, which culminated in an\nexhibition of AI-driven artworks, the paper aims to provide a roadmap for\nseamlessly infusing these tools into academic settings. Given their recent\nemergence, the paper delves into a comprehensive overview of such tools,\nemphasizing the intricate dance between artists, developers, and researchers in\nthe open-source AI art world. This discourse extends to the challenges and\nimperatives faced by educational institutions. It presents a compelling case\nfor the swift adoption of these avant-garde tools, underscoring the paramount\nimportance of equipping students with the competencies required to thrive in an\nAI-augmented artistic landscape.",
      "tldr_zh": "这篇论文探讨了生成式 AI 在艺术大学中的作用，强调其创造性能力（如生成新内容）如何挑战传统以人为中心的艺术教育范式。论文基于2023年7月的研讨会和展览经验，聚焦于将AI工具（如Stable Diffusion）融入课程，提供了一个实用路线图。研究概述了这些工具的背景、艺术家与开发者的协作挑战，并主张教育机构迅速采用，以培养学生在AI增强艺术环境中的核心技能。",
      "categories": [
        "cs.AI",
        "cs.CY",
        "K.3.1; J.5"
      ],
      "primary_category": "cs.AI",
      "comment": "17 pages, 5 figures. Based on workshops conducted in July 2023 at\n  Kyoto Seika University",
      "pdf_url": "http://arxiv.org/pdf/2412.16531v1",
      "published_date": "2024-12-21 08:18:43 UTC",
      "updated_date": "2024-12-21 08:18:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:14:45.130594"
    },
    {
      "arxiv_id": "2412.16526v2",
      "title": "Text2midi: Generating Symbolic Music from Captions",
      "title_zh": "翻译失败",
      "authors": [
        "Keshav Bhandari",
        "Abhinaba Roy",
        "Kyra Wang",
        "Geeta Puri",
        "Simon Colton",
        "Dorien Herremans"
      ],
      "abstract": "This paper introduces text2midi, an end-to-end model to generate MIDI files\nfrom textual descriptions. Leveraging the growing popularity of multimodal\ngenerative approaches, text2midi capitalizes on the extensive availability of\ntextual data and the success of large language models (LLMs). Our end-to-end\nsystem harnesses the power of LLMs to generate symbolic music in the form of\nMIDI files. Specifically, we utilize a pretrained LLM encoder to process\ncaptions, which then condition an autoregressive transformer decoder to produce\nMIDI sequences that accurately reflect the provided descriptions. This\nintuitive and user-friendly method significantly streamlines the music creation\nprocess by allowing users to generate music pieces using text prompts. We\nconduct comprehensive empirical evaluations, incorporating both automated and\nhuman studies, that show our model generates MIDI files of high quality that\nare indeed controllable by text captions that may include music theory terms\nsuch as chords, keys, and tempo. We release the code and music samples on our\ndemo page (https://github.com/AMAAI-Lab/Text2midi) for users to interact with\ntext2midi.",
      "tldr_zh": "本论文提出 Text2midi，一种端到端模型，用于从文本描述生成 MIDI 文件，从而简化音乐创作过程。模型利用预训练的 LLM 编码器处理文本提示，然后通过自回归 transformer 解码器生成符合描述的 MIDI 序列，支持音乐理论术语如和弦、调性和节奏的控制。实验评估包括自动和人类研究，证明该模型能产生高质量、可控的符号音乐，并已发布代码和样本以供交互。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "9 pages, 3 figures, Accepted at the 39th AAAI Conference on\n  Artificial Intelligence (AAAI 2025)",
      "pdf_url": "http://arxiv.org/pdf/2412.16526v2",
      "published_date": "2024-12-21 08:09:12 UTC",
      "updated_date": "2024-12-31 07:56:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:14:56.928961"
    },
    {
      "arxiv_id": "2412.16522v2",
      "title": "Enhancing Contrastive Learning Inspired by the Philosophy of \"The Blind Men and the Elephant\"",
      "title_zh": "翻译失败",
      "authors": [
        "Yudong Zhang",
        "Ruobing Xie",
        "Jiansheng Chen",
        "Xingwu Sun",
        "Zhanhui Kang",
        "Yu Wang"
      ],
      "abstract": "Contrastive learning is a prevalent technique in self-supervised vision\nrepresentation learning, typically generating positive pairs by applying two\ndata augmentations to the same image. Designing effective data augmentation\nstrategies is crucial for the success of contrastive learning. Inspired by the\nstory of the blind men and the elephant, we introduce JointCrop and JointBlur.\nThese methods generate more challenging positive pairs by leveraging the joint\ndistribution of the two augmentation parameters, thereby enabling contrastive\nlearning to acquire more effective feature representations. To the best of our\nknowledge, this is the first effort to explicitly incorporate the joint\ndistribution of two data augmentation parameters into contrastive learning. As\na plug-and-play framework without additional computational overhead, JointCrop\nand JointBlur enhance the performance of SimCLR, BYOL, MoCo v1, MoCo v2, MoCo\nv3, SimSiam, and Dino baselines with notable improvements.",
      "tldr_zh": "这篇论文受“盲人摸象”哲学启发，提出 JointCrop 和 JointBlur 方法，以改进对比学习（Contrastive Learning）中的数据增强策略，这些方法通过利用两个增强参数的联合分布生成更具挑战性的正样本，从而提升特征表示的有效性。不同于传统方法，这是首次将数据增强参数的联合分布显式整合进对比学习框架中。实验结果显示，JointCrop 和 JointBlur 作为即插即用模块，无需额外计算开销，便显著提升了 SimCLR、BYOL、MoCo v1/v2/v3、SimSiam 和 Dino 等基线的性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2412.16522v2",
      "published_date": "2024-12-21 07:50:59 UTC",
      "updated_date": "2025-04-16 09:12:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:15:09.265784"
    },
    {
      "arxiv_id": "2412.16515v1",
      "title": "VSFormer: Value and Shape-Aware Transformer with Prior-Enhanced Self-Attention for Multivariate Time Series Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Wenjie Xi",
        "Rundong Zuo",
        "Alejandro Alvarez",
        "Jie Zhang",
        "Byron Choi",
        "Jessica Lin"
      ],
      "abstract": "Multivariate time series classification is a crucial task in data mining,\nattracting growing research interest due to its broad applications. While many\nexisting methods focus on discovering discriminative patterns in time series,\nreal-world data does not always present such patterns, and sometimes raw\nnumerical values can also serve as discriminative features. Additionally, the\nrecent success of Transformer models has inspired many studies. However, when\napplying to time series classification, the self-attention mechanisms in\nTransformer models could introduce classification-irrelevant features, thereby\ncompromising accuracy. To address these challenges, we propose a novel method,\nVSFormer, that incorporates both discriminative patterns (shape) and numerical\ninformation (value). In addition, we extract class-specific prior information\nderived from supervised information to enrich the positional encoding and\nprovide classification-oriented self-attention learning, thereby enhancing its\neffectiveness. Extensive experiments on all 30 UEA archived datasets\ndemonstrate the superior performance of our method compared to SOTA models.\nThrough ablation studies, we demonstrate the effectiveness of the improved\nencoding layer and the proposed self-attention mechanism. Finally, We provide a\ncase study on a real-world time series dataset without discriminative patterns\nto interpret our model.",
      "tldr_zh": "本论文提出VSFormer，一种值和形状感知的Transformer模型，用于多变量时间序列分类，旨在整合判别模式（shape）和数值信息（value），以解决传统方法忽略原始数值特征的问题。该模型通过提取类特定先验信息来丰富位置编码，并实现先验增强的自注意力机制（Prior-Enhanced Self-Attention），从而减少无关特征的干扰。在30个UEA数据集上的广泛实验中，VSFormer优于现有SOTA模型，消融研究和真实世界案例进一步验证了改进编码层和自注意力机制的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.16515v1",
      "published_date": "2024-12-21 07:31:22 UTC",
      "updated_date": "2024-12-21 07:31:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:15:22.416654"
    },
    {
      "arxiv_id": "2412.16512v1",
      "title": "TrojFlow: Flow Models are Natural Targets for Trojan Attacks",
      "title_zh": "翻译失败",
      "authors": [
        "Zhengyang Qi",
        "Xiaohua Xu"
      ],
      "abstract": "Flow-based generative models (FMs) have rapidly advanced as a method for\nmapping noise to data, its efficient training and sampling process makes it\nwidely applicable in various fields. FMs can be viewed as a variant of\ndiffusion models (DMs). At the same time, previous studies have shown that DMs\nare vulnerable to Trojan/Backdoor attacks, a type of output manipulation attack\ntriggered by a maliciously embedded pattern at model input. We found that\nTrojan attacks on generative models are essentially equivalent to image\ntransfer tasks from the backdoor distribution to the target distribution, the\nunique ability of FMs to fit any two arbitrary distributions significantly\nsimplifies the training and sampling setups for attacking FMs, making them\ninherently natural targets for backdoor attacks. In this paper, we propose\nTrojFlow, exploring the vulnerabilities of FMs through Trojan attacks. In\nparticular, we consider various attack settings and their combinations and\nthoroughly explore whether existing defense methods for DMs can effectively\ndefend against our proposed attack scenarios. We evaluate TrojFlow on CIFAR-10\nand CelebA datasets, our experiments show that our method can compromise FMs\nwith high utility and specificity, and can easily break through existing\ndefense mechanisms.",
      "tldr_zh": "这篇论文探讨了Flow-based generative models (FMs) 作为Trojan attacks的天然目标，指出FMs的独特分布拟合能力使攻击训练和采样过程更为简化。作者提出TrojFlow方法，通过各种攻击设置和组合来利用FMs的漏洞，并评估现有diffusion models (DMs)的防御机制是否有效。实验结果显示，在CIFAR-10和CelebA数据集上，TrojFlow实现了高utility和specificity的攻击，并成功突破了现有防御。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "6 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.16512v1",
      "published_date": "2024-12-21 07:21:53 UTC",
      "updated_date": "2024-12-21 07:21:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:15:34.161695"
    },
    {
      "arxiv_id": "2412.16504v2",
      "title": "Privacy in Fine-tuning Large Language Models: Attacks, Defenses, and Future Directions",
      "title_zh": "在大语言模型微调中的隐私：攻击、防御和未来方向",
      "authors": [
        "Hao Du",
        "Shang Liu",
        "Lele Zheng",
        "Yang Cao",
        "Atsuyoshi Nakamura",
        "Lei Chen"
      ],
      "abstract": "Fine-tuning has emerged as a critical process in leveraging Large Language\nModels (LLMs) for specific downstream tasks, enabling these models to achieve\nstate-of-the-art performance across various domains. However, the fine-tuning\nprocess often involves sensitive datasets, introducing privacy risks that\nexploit the unique characteristics of this stage. In this paper, we provide a\ncomprehensive survey of privacy challenges associated with fine-tuning LLMs,\nhighlighting vulnerabilities to various privacy attacks, including membership\ninference, data extraction, and backdoor attacks. We further review defense\nmechanisms designed to mitigate privacy risks in the fine-tuning phase, such as\ndifferential privacy, federated learning, and knowledge unlearning, discussing\ntheir effectiveness and limitations in addressing privacy risks and maintaining\nmodel utility. By identifying key gaps in existing research, we highlight\nchallenges and propose directions to advance the development of\nprivacy-preserving methods for fine-tuning LLMs, promoting their responsible\nuse in diverse applications.",
      "tldr_zh": "该论文调查了在微调 Large Language Models (LLMs) 过程中面临的隐私挑战，强调了敏感数据集带来的风险，如 membership inference、data extraction 和 backdoor attacks 等攻击。作者审阅了多种防御机制，包括 differential privacy、federated learning 和 knowledge unlearning，并评估了这些方法在缓解隐私风险与保持模型性能之间的权衡。研究揭示了现有方法的局限性，并提出未来方向，如开发更先进的 privacy-preserving 技术，以推动 LLMs 在各种应用中的负责任使用。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "accepted by PAKDD2025",
      "pdf_url": "http://arxiv.org/pdf/2412.16504v2",
      "published_date": "2024-12-21 06:41:29 UTC",
      "updated_date": "2025-04-06 10:28:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:15:44.966535"
    },
    {
      "arxiv_id": "2412.16500v3",
      "title": "Speech Retrieval-Augmented Generation without Automatic Speech Recognition",
      "title_zh": "无需自动语音识别的语音检索增强生成",
      "authors": [
        "Do June Min",
        "Karel Mundnich",
        "Andy Lapastora",
        "Erfan Soltanmohammadi",
        "Srikanth Ronanki",
        "Kyu Han"
      ],
      "abstract": "One common approach for question answering over speech data is to first\ntranscribe speech using automatic speech recognition (ASR) and then employ\ntext-based retrieval-augmented generation (RAG) on the transcriptions. While\nthis cascaded pipeline has proven effective in many practical settings, ASR\nerrors can propagate to the retrieval and generation steps. To overcome this\nlimitation, we introduce SpeechRAG, a novel framework designed for\nopen-question answering over spoken data. Our proposed approach fine-tunes a\npre-trained speech encoder into a speech adapter fed into a frozen large\nlanguage model (LLM)--based retrieval model. By aligning the embedding spaces\nof text and speech, our speech retriever directly retrieves audio passages from\ntext-based queries, leveraging the retrieval capacity of the frozen text\nretriever. Our retrieval experiments on spoken question answering datasets show\nthat direct speech retrieval does not degrade over the text-based baseline, and\noutperforms the cascaded systems using ASR. For generation, we use a speech\nlanguage model (SLM) as a generator, conditioned on audio passages rather than\ntranscripts. Without fine-tuning of the SLM, this approach outperforms cascaded\ntext-based models when there is high WER in the transcripts.",
      "tldr_zh": "这篇论文提出 SpeechRAG 框架，用于无需 Automatic Speech Recognition (ASR) 的语音数据问答，旨在避免 ASR 错误传播到检索和生成步骤。方法通过微调预训练语音编码器作为语音适配器，与冻结的 Large Language Model (LLM) 检索模型结合，实现文本查询与音频段的嵌入空间对齐，从而直接检索音频内容。对于生成，使用 Speech Language Model (SLM) 基于音频段进行条件生成，而非转录文本。实验结果显示，SpeechRAG 在语音问答数据集上不劣于文本基线，并在 Word Error Rate (WER) 高的场景下优于基于 ASR 的级联系统。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "eess.AS",
      "comment": "ICASSP 2025",
      "pdf_url": "http://arxiv.org/pdf/2412.16500v3",
      "published_date": "2024-12-21 06:16:04 UTC",
      "updated_date": "2025-01-03 07:18:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:15:58.610541"
    },
    {
      "arxiv_id": "2501.08335v3",
      "title": "MERaLiON-TextLLM: Cross-Lingual Understanding of Large Language Models in Chinese, Indonesian, Malay, and Singlish",
      "title_zh": "翻译失败",
      "authors": [
        "Xin Huang",
        "Tarun Kumar Vangani",
        "Minh Duc Pham",
        "Xunlong Zou",
        "Bin Wang",
        "Zhengyuan Liu",
        "Ai Ti Aw"
      ],
      "abstract": "Multilingual large language models (MLLMs) have shown impressive capabilities\nacross a variety of languages. However, efficacy can differ greatly between\ndifferent language families, especially for those with limited linguistic\nresources. This report presents MERaLiON-TextLLM, a series of open-source\nlanguage models specifically tailored to improve understanding and generation\nin Chinese, Indonesian, Malay, and Singlish. The initial released model is\nbuilt on Llama-3-8B-Base and refined through a meticulously crafted process of\ncontinued pre-training and weight merging. Our approach achieves performance\nimprovements across benchmarks in these languages, exceeding the capabilities\nof the official Llama-3 models. We provide the model checkpoints as a resource\nto support further research and development in cross-lingual language\nunderstanding.",
      "tldr_zh": "该研究介绍了 MERaLiON-TextLLM，这是一个开源语言模型系列，针对中文、印尼语、马来语和 Singlish 等资源有限的语言，提升了 Large Language Models (LLMs) 的跨语言理解和生成能力。模型基于 Llama-3-8B-Base，通过 continued pre-training 和 weight merging 的精细过程进行优化。实验结果显示，该模型在这些语言的基准测试中超过了官方 Llama-3 模型的性能，并提供模型检查点作为资源，支持进一步的跨语言研究发展。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.08335v3",
      "published_date": "2024-12-21 05:50:48 UTC",
      "updated_date": "2025-01-22 02:28:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:16:10.373172"
    },
    {
      "arxiv_id": "2412.16489v1",
      "title": "Deep Reinforcement Learning Based Systems for Safety Critical Applications in Aerospace",
      "title_zh": "翻译失败",
      "authors": [
        "Abedin Sherifi"
      ],
      "abstract": "Recent advancements in artificial intelligence (AI) applications within\naerospace have demonstrated substantial growth, particularly in the context of\ncontrol systems. As High Performance Computing (HPC) platforms continue to\nevolve, they are expected to replace current flight control or engine control\ncomputers, enabling increased computational capabilities. This shift will allow\nreal-time AI applications, such as image processing and defect detection, to be\nseamlessly integrated into monitoring systems, providing real-time awareness\nand enhanced fault detection and accommodation. Furthermore, AI's potential in\naerospace extends to control systems, where its application can range from full\nautonomy to enhancing human control through assistive features. AI,\nparticularly deep reinforcement learning (DRL), can offer significant\nimprovements in control systems, whether for autonomous operation or as an\naugmentative tool.",
      "tldr_zh": "本论文探讨了深度强化学习（DRL）在航空航天安全关键应用中的系统设计，随着高性能计算（HPC）平台的演进，这些系统可取代传统飞行控制计算机，实现实时AI应用，如图像处理和缺陷检测，从而提升实时意识、故障检测和容错能力。DRL通过提供从完全自治到辅助人类控制的多种模式，显著改善控制系统的性能和可靠性。研究强调了AI在航空航天领域的潜力，为未来自主操作和增强型控制奠定基础。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.16489v1",
      "published_date": "2024-12-21 05:17:55 UTC",
      "updated_date": "2024-12-21 05:17:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:16:21.801838"
    },
    {
      "arxiv_id": "2412.16478v1",
      "title": "Enhancing Nighttime Vehicle Detection with Day-to-Night Style Transfer and Labeling-Free Augmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Yunxiang Yang",
        "Hao Zhen",
        "Yongcan Huang",
        "Jidong J. Yang"
      ],
      "abstract": "Existing deep learning-based object detection models perform well under\ndaytime conditions but face significant challenges at night, primarily because\nthey are predominantly trained on daytime images. Additionally, training with\nnighttime images presents another challenge: even human annotators struggle to\naccurately label objects in low-light conditions. This issue is particularly\npronounced in transportation applications, such as detecting vehicles and other\nobjects of interest on rural roads at night, where street lighting is often\nabsent, and headlights may introduce undesirable glare. This study addresses\nthese challenges by introducing a novel framework for labeling-free data\naugmentation, leveraging CARLA-generated synthetic data for day-to-night image\nstyle transfer. Specifically, the framework incorporates the Efficient\nAttention Generative Adversarial Network for realistic day-to-night style\ntransfer and uses CARLA-generated synthetic nighttime images to help the model\nlearn vehicle headlight effects. To evaluate the efficacy of the proposed\nframework, we fine-tuned the YOLO11 model with an augmented dataset\nspecifically curated for rural nighttime environments, achieving significant\nimprovements in nighttime vehicle detection. This novel approach is simple yet\neffective, offering a scalable solution to enhance AI-based detection systems\nin low-visibility environments and extend the applicability of object detection\nmodels to broader real-world contexts.",
      "tldr_zh": "本研究针对现有深度学习物体检测模型在夜晚表现不佳的问题，提出了一种无标注数据增强框架，利用CARLA生成的合成数据进行日夜风格转换，以解决训练数据不足和标注困难的挑战。框架中引入Efficient Attention Generative Adversarial Network，实现真实的日夜图像转换，并帮助模型学习车辆大灯效果。实验结果显示，通过微调YOLO11模型，该方法在农村夜晚环境中显著提升了车辆检测性能，提供了一个简单且可扩展的解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.CV",
      "comment": "12 pages, 8 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2412.16478v1",
      "published_date": "2024-12-21 04:13:46 UTC",
      "updated_date": "2024-12-21 04:13:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:16:33.267763"
    },
    {
      "arxiv_id": "2412.16475v1",
      "title": "When Can Proxies Improve the Sample Complexity of Preference Learning?",
      "title_zh": "代理何时能够改善偏好学习的样本复杂度？",
      "authors": [
        "Yuchen Zhu",
        "Daniel Augusto de Souza",
        "Zhengyan Shi",
        "Mengyue Yang",
        "Pasquale Minervini",
        "Alexander D'Amour",
        "Matt J. Kusner"
      ],
      "abstract": "We address the problem of reward hacking, where maximising a proxy reward\ndoes not necessarily increase the true reward. This is a key concern for Large\nLanguage Models (LLMs), as they are often fine-tuned on human preferences that\nmay not accurately reflect a true objective. Existing work uses various tricks\nsuch as regularisation, tweaks to the reward model, and reward hacking\ndetectors, to limit the influence that such proxy preferences have on a model.\nLuckily, in many contexts such as medicine, education, and law, a sparse amount\nof expert data is often available. In these cases, it is often unclear whether\nthe addition of proxy data can improve policy learning. We outline a set of\nsufficient conditions on proxy feedback that, if satisfied, indicate that proxy\ndata can provably improve the sample complexity of learning the ground truth\npolicy. These conditions can inform the data collection process for specific\ntasks. The result implies a parameterisation for LLMs that achieves this\nimproved sample complexity. We detail how one can adapt existing architectures\nto yield this improved sample complexity.",
      "tldr_zh": "这篇论文探讨了在偏好学习中，代理反馈（proxies）何时能改善样本复杂度（sample complexity），特别是在解决reward hacking问题时——即最大化代理奖励不一定增加真实奖励，尤其在LLMs微调中。作者提出了代理反馈的一组充分条件，如果满足，这些条件能证明地降低学习真实策略所需的样本量，从而提升效率。论文还暗示了针对LLMs的特定参数化方法，并说明如何调整现有架构以实现这一改进，这些发现可指导医学、教育和法律等领域的专家数据收集过程。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.16475v1",
      "published_date": "2024-12-21 04:07:17 UTC",
      "updated_date": "2024-12-21 04:07:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:16:45.703981"
    },
    {
      "arxiv_id": "2412.16453v1",
      "title": "The Evolving Usage of GenAI by Computing Students",
      "title_zh": "翻译失败",
      "authors": [
        "Irene Hou",
        "Hannah Vy Nguyen",
        "Owen Man",
        "Stephen MacNeil"
      ],
      "abstract": "Help-seeking is a critical aspect of learning and problem-solving for\ncomputing students. Recent research has shown that many students are aware of\ngenerative AI (GenAI) tools; however, there are gaps in the extent and\neffectiveness of how students use them. With over two years of widespread GenAI\nusage, it is crucial to understand whether students' help-seeking behaviors\nwith these tools have evolved and how. This paper presents findings from a\nrepeated cross-sectional survey conducted among computing students across North\nAmerican universities (n=95). Our results indicate shifts in GenAI usage\npatterns. In 2023, 34.1% of students (n=47) reported never using ChatGPT for\nhelp, ranking it fourth after online searches, peer support, and class forums.\nBy 2024, this figure dropped sharply to 6.3% (n=48), with ChatGPT nearly\nmatching online search as the most commonly used help resource. Despite this\ngrowing prevalence, there has been a decline in students' hourly and daily\nusage of GenAI tools, which may be attributed to a common tendency to\nunderestimate usage frequency. These findings offer new insights into the\nevolving role of GenAI in computing education, highlighting its increasing\nacceptance and solidifying its position as a key help resource.",
      "tldr_zh": "本研究调查了计算学生使用生成式AI (GenAI) 作为求助工具的行为演变，通过在北美大学进行的重复横断面调查（n=95）收集数据。结果显示，2023年有34.1%的学生（n=47）从未使用ChatGPT求助，而2024年这一比例急剧下降至6.3%（n=48），使ChatGPT几乎与在线搜索并列为最常用资源。尽管GenAI的使用率上升，但学生报告的每日或每小时使用频率有所下降，可能由于低估了实际频率。该研究突出了GenAI在计算教育中的日益重要性，强调其作为关键求助资源的地位。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "K.3.2"
      ],
      "primary_category": "cs.CY",
      "comment": "2 pages, 1 figure, to be published in SIGCSE 2025",
      "pdf_url": "http://arxiv.org/pdf/2412.16453v1",
      "published_date": "2024-12-21 03:00:04 UTC",
      "updated_date": "2024-12-21 03:00:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:16:57.584699"
    },
    {
      "arxiv_id": "2412.16451v1",
      "title": "Correcting Large Language Model Behavior via Influence Function",
      "title_zh": "通过影响函数修正大型语言模型的行为",
      "authors": [
        "Han Zhang",
        "Zhuo Zhang",
        "Yi Zhang",
        "Yuanzhao Zhai",
        "Hanyang Peng",
        "Yu Lei",
        "Yue Yu",
        "Hui Wang",
        "Bin Liang",
        "Lin Gui",
        "Ruifeng Xu"
      ],
      "abstract": "Recent advancements in AI alignment techniques have significantly improved\nthe alignment of large language models (LLMs) with static human preferences.\nHowever, the dynamic nature of human preferences can render some prior training\ndata outdated or even erroneous, ultimately causing LLMs to deviate from\ncontemporary human preferences and societal norms. Existing methodologies,\nwhether they involve the curation of new data for continual alignment or the\nmanual correction of outdated data for re-alignment, demand costly human\nresources. To address this challenge, we propose a novel approach, Large\nLanguage Model Behavior Correction with Influence Function Recall and\nPost-Training (LANCET), which requires no human involvement. LANCET consists of\ntwo phases: (1) using influence functions to identify the training data that\nsignificantly impact undesirable model outputs, and (2) applying an Influence\nfunction-driven Bregman Optimization (IBO) technique to adjust the model's\nbehavior based on these influence distributions. Our experiments demonstrate\nthat LANCET effectively and efficiently correct inappropriate behaviors of\nLLMs. Furthermore, LANCET can outperform methods that rely on collecting human\npreferences, and it enhances the interpretability of learning human preferences\nwithin LLMs.",
      "tldr_zh": "这篇论文提出 LANCET（Large Language Model Behavior Correction with Influence Function Recall and Post-Training）方法，用于无需人类干预地纠正 Large Language Model (LLMs) 的行为，以应对人类偏好动态变化导致的模型偏差问题。LANCET 包括两个阶段：首先使用 influence functions 识别影响不良输出的关键训练数据，其次应用 Influence function-driven Bregman Optimization (IBO) 技术调整模型行为。实验结果表明，LANCET 比依赖人类偏好收集的方法更高效且性能更优，同时提升了 LLMs 在学习人类偏好时的可解释性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.16451v1",
      "published_date": "2024-12-21 02:50:08 UTC",
      "updated_date": "2024-12-21 02:50:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:17:09.662113"
    },
    {
      "arxiv_id": "2412.16447v1",
      "title": "A Generalizable Anomaly Detection Method in Dynamic Graphs",
      "title_zh": "一种在动态图中可泛化的异常检测方法",
      "authors": [
        "Xiao Yang",
        "Xuejiao Zhao",
        "Zhiqi Shen"
      ],
      "abstract": "Anomaly detection aims to identify deviations from normal patterns within\ndata. This task is particularly crucial in dynamic graphs, which are common in\napplications like social networks and cybersecurity, due to their evolving\nstructures and complex relationships. Although recent deep learning-based\nmethods have shown promising results in anomaly detection on dynamic graphs,\nthey often lack of generalizability. In this study, we propose GeneralDyG, a\nmethod that samples temporal ego-graphs and sequentially extracts structural\nand temporal features to address the three key challenges in achieving\ngeneralizability: Data Diversity, Dynamic Feature Capture, and Computational\nCost. Extensive experimental results demonstrate that our proposed GeneralDyG\nsignificantly outperforms state-of-the-art methods on four real-world datasets.",
      "tldr_zh": "这篇论文针对动态图中的异常检测问题，提出了一种具有泛化性的方法GeneralDyG，以应对数据多样性、动态特征捕获和计算成本的挑战。\nGeneralDyG通过采样时间ego-graphs并顺序提取结构和时间特征，来处理动态图的演变结构和复杂关系。\n实验结果表明，该方法在四个真实数据集上显著优于现有最先进方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages",
      "pdf_url": "http://arxiv.org/pdf/2412.16447v1",
      "published_date": "2024-12-21 02:38:48 UTC",
      "updated_date": "2024-12-21 02:38:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:17:21.439026"
    },
    {
      "arxiv_id": "2412.16446v1",
      "title": "Sensitive Image Classification by Vision Transformers",
      "title_zh": "翻译失败",
      "authors": [
        "Hanxian He",
        "Campbell Wilson",
        "Thanh Thi Nguyen",
        "Janis Dalins"
      ],
      "abstract": "When it comes to classifying child sexual abuse images, managing similar\ninter-class correlations and diverse intra-class correlations poses a\nsignificant challenge. Vision transformer models, unlike conventional deep\nconvolutional network models, leverage a self-attention mechanism to capture\nglobal interactions among contextual local elements. This allows them to\nnavigate through image patches effectively, avoiding incorrect correlations and\nreducing ambiguity in attention maps, thus proving their efficacy in computer\nvision tasks. Rather than directly analyzing child sexual abuse data, we\nconstructed two datasets: one comprising clean and pornographic images and\nanother with three classes, which additionally include images indicative of\npornography, sourced from Reddit and Google Open Images data. In our\nexperiments, we also employ an adult content image benchmark dataset. These\ndatasets served as a basis for assessing the performance of vision transformer\nmodels in pornographic image classification. In our study, we conducted a\ncomparative analysis between various popular vision transformer models and\ntraditional pre-trained ResNet models. Furthermore, we compared them with\nestablished methods for sensitive image detection such as attention and metric\nlearning based CNN and Bumble. The findings demonstrated that vision\ntransformer networks surpassed the benchmark pre-trained models, showcasing\ntheir superior classification and detection capabilities in this task.",
      "tldr_zh": "该研究探讨了使用 Vision Transformers 分类敏感图像（如儿童性虐待图像）的挑战，强调其自-attention mechanism 能有效捕获全局交互，避免传统深卷积网络的错误相关性和歧义问题。为避免直接处理敏感数据，研究者构建了两个数据集（包含干净和色情图像，以及三类图像），并使用 Reddit、Google Open Images 和成人内容基准数据集进行实验。相比传统预训练 ResNet 模型以及注意力机制 CNN 和 Bumble 等方法，Vision Transformers 在色情图像分类任务中表现出色，显著提升了分类和检测性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at 2024 IEEE International Conference on Systems, Man, and\n  Cybernetics (SMC)",
      "pdf_url": "http://arxiv.org/pdf/2412.16446v1",
      "published_date": "2024-12-21 02:34:24 UTC",
      "updated_date": "2024-12-21 02:34:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:17:33.848620"
    },
    {
      "arxiv_id": "2412.16443v1",
      "title": "Has LLM Reached the Scaling Ceiling Yet? Unified Insights into LLM Regularities and Constraints",
      "title_zh": "LLM 是否已达到缩放上限？对 LLM 规律性和约束的统一洞见",
      "authors": [
        "Charles Luo"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities, yet\ntheir scalability raises a critical question: Have we reached the scaling\nceiling? This paper addresses this pivotal question by developing a unified\ntheoretical framework that integrates mathematical and statistical insights to\nexplain the scaling dynamics of LLMs. We present: 1. Central Limit Theorem\n(CLT) for Hidden Representations: We show that noise in hidden representations\nscales inversely with context size, explaining stabilization effects and the\nlimits of context length improvements. 2. Bias-Variance Decomposition: We\ndecompose next-token prediction loss into irreducible entropy, capacity-driven\nbias, and finite sample variance, revealing trade-offs where scaling yields\ndiminishing returns. 3. Emergent SNR Thresholds: By defining signal-to-noise\nratio (SNR), we quantify how capabilities emerge abruptly once SNR surpasses a\nthreshold, offering insights into when scaling becomes less effective. Through\nthis framework, we conclude that while LLMs have not reached an absolute\nscaling ceiling, practical constraints are increasingly prominent: diminishing\nreturns, resource inefficiencies, and data limitations. Future progress will\nrequire a shift from brute-force scaling to innovations in architecture, data\nquality, and training paradigms. This work provides a roadmap for guiding the\nefficient development of next-generation LLMs and advancing the field beyond\ntraditional scaling strategies.\n  Keywords: Large Language Models; Scaling Ceiling; Central Limit Theorem;\nBias-Variance Trade-Off; Signal-to-Noise Ratio; Emergent Capabilities",
      "tldr_zh": "本论文探讨大型语言模型 (LLMs) 是否已达到扩展上限，通过一个统一的理论框架整合数学和统计见解来分析 LLMs 的扩展动态。该框架包括 Central Limit Theorem (CLT) for Hidden Representations，揭示隐藏表示噪声与上下文大小成反比，从而解释扩展的稳定效应和限制；Bias-Variance Decomposition，将下一个标记预测损失分解为不可约熵、容量驱动偏差和有限样本方差，突出扩展收益递减的权衡；以及 Emergent SNR Thresholds，通过信号噪声比 (SNR) 量化能力突发出现，判断扩展何时失效。研究结论表明，LLMs 尚未触及绝对上限，但面临实际约束如资源低效和数据限制，未来需转向架构创新、数据质量提升和新型训练范式，以实现高效发展。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.16443v1",
      "published_date": "2024-12-21 02:19:07 UTC",
      "updated_date": "2024-12-21 02:19:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:17:46.325808"
    },
    {
      "arxiv_id": "2412.16441v2",
      "title": "Towards Graph Foundation Models: Learning Generalities Across Graphs via Task-Trees",
      "title_zh": "翻译失败",
      "authors": [
        "Zehong Wang",
        "Zheyuan Zhang",
        "Tianyi Ma",
        "Nitesh V Chawla",
        "Chuxu Zhang",
        "Yanfang Ye"
      ],
      "abstract": "Foundation models aim to create general, cross-task, and cross-domain machine\nlearning models by pretraining on large-scale datasets to capture shared\npatterns or concepts, such as contours, colors, textures, and edges in images,\nor tokens, words, and sentences in text. However, identifying generalities\nacross graph-structured data remains a significant challenge, as different\ngraph-based tasks necessitate distinct inductive biases, thereby impeding the\ndevelopment of graph foundation models. To address this challenge, we introduce\na novel approach for learning cross-task generalities in graphs. Specifically,\nwe propose task-trees as basic learning instances to align task spaces (node,\nlink, graph) on graphs. Then, we conduct a theoretical analysis to examine\ntheir stability, transferability, and generalization. Our findings indicate\nthat when a graph neural network (GNN) is pretrained on diverse task-trees\nusing a reconstruction objective, it acquires transferable knowledge, enabling\neffective adaptation to downstream tasks with an appropriate set of fine-tuning\nsamples. To empirically validate this approach, we develop a pretrained graph\nmodel based on task-trees, termed Graph Generality Identifier on Task-Trees\n(GIT). Extensive experiments demonstrate that a single pretrained GIT model can\nbe effectively adapted to over 30 different graphs across five domains via\nfine-tuning, in-context learning, or zero-shot learning. Our data and code are\navailable at https://github.com/Zehong-Wang/GIT.",
      "tldr_zh": "本研究旨在开发图结构数据的 Foundation Models，通过提出 task-trees 作为基本学习实例，来学习跨任务和跨领域的图一般性，从而解决不同图任务的归纳偏差问题。作者分析了 task-trees 的稳定性、可转移性和泛化性，发现预训练图神经网络 (GNN) 在 task-trees 上能获得可转移知识，便于适应下游任务。实验结果显示，开发的 Graph Generality Identifier on Task-Trees (GIT) 模型可通过微调、在语境学习或零样本学习，成功适应超过 30 个图和五个领域，验证了该方法的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.16441v2",
      "published_date": "2024-12-21 02:07:43 UTC",
      "updated_date": "2025-01-30 20:19:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:18:35.073946"
    },
    {
      "arxiv_id": "2412.16431v1",
      "title": "Object Detection Approaches to Identifying Hand Images with High Forensic Values",
      "title_zh": "翻译失败",
      "authors": [
        "Thanh Thi Nguyen",
        "Campbell Wilson",
        "Imad Khan",
        "Janis Dalins"
      ],
      "abstract": "Forensic science plays a crucial role in legal investigations, and the use of\nadvanced technologies, such as object detection based on machine learning\nmethods, can enhance the efficiency and accuracy of forensic analysis. Human\nhands are unique and can leave distinct patterns, marks, or prints that can be\nutilized for forensic examinations. This paper compares various machine\nlearning approaches to hand detection and presents the application results of\nemploying the best-performing model to identify images of significant\nimportance in forensic contexts. We fine-tune YOLOv8 and vision\ntransformer-based object detection models on four hand image datasets,\nincluding the 11k hands dataset with our own bounding boxes annotated by a\nsemi-automatic approach. Two YOLOv8 variants, i.e., YOLOv8 nano (YOLOv8n) and\nYOLOv8 extra-large (YOLOv8x), and two vision transformer variants, i.e.,\nDEtection TRansformer (DETR) and Detection Transformers with Assignment (DETA),\nare employed for the experiments. Experimental results demonstrate that the\nYOLOv8 models outperform DETR and DETA on all datasets. The experiments also\nshow that YOLOv8 approaches result in superior performance compared with\nexisting hand detection methods, which were based on YOLOv3 and YOLOv4 models.\nApplications of our fine-tuned YOLOv8 models for identifying hand images (or\nframes in a video) with high forensic values produce excellent results,\nsignificantly reducing the time required by forensic experts. This implies that\nour approaches can be implemented effectively for real-world applications in\nforensics or related fields.",
      "tldr_zh": "这篇论文比较了多种机器学习方法，用于识别具有高取证价值的手部图像，以提升法医分析的效率和准确性。研究者微调了 YOLOv8 的变体（包括 YOLOv8n 和 YOLOv8x）以及视觉变压器模型（如 DETR 和 DETA），并在四个手部图像数据集（包括 11k hands 数据集）上进行实验。结果表明，YOLOv8 模型在所有数据集上优于 DETR 和 DETA，并比之前的 YOLOv3 和 YOLOv4 方法表现出色。最终，应用微调后的 YOLOv8 模型能有效识别高取证价值图像，显著减少法医专家所需时间，为实际取证应用提供可靠工具。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at 2024 IEEE International Conference on Systems, Man, and\n  Cybernetics (SMC)",
      "pdf_url": "http://arxiv.org/pdf/2412.16431v1",
      "published_date": "2024-12-21 01:37:54 UTC",
      "updated_date": "2024-12-21 01:37:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:18:36.486163"
    },
    {
      "arxiv_id": "2412.16429v2",
      "title": "LearnLM: Improving Gemini for Learning",
      "title_zh": "LearnLM：改进 Gemini 用于学习",
      "authors": [
        "LearnLM Team",
        "Abhinit Modi",
        "Aditya Srikanth Veerubhotla",
        "Aliya Rysbek",
        "Andrea Huber",
        "Brett Wiltshire",
        "Brian Veprek",
        "Daniel Gillick",
        "Daniel Kasenberg",
        "Derek Ahmed",
        "Irina Jurenka",
        "James Cohan",
        "Jennifer She",
        "Julia Wilkowski",
        "Kaiz Alarakyia",
        "Kevin R. McKee",
        "Lisa Wang",
        "Markus Kunesch",
        "Mike Schaekermann",
        "Miruna Pîslar",
        "Nikhil Joshi",
        "Parsa Mahmoudieh",
        "Paul Jhun",
        "Sara Wiltberger",
        "Shakir Mohamed",
        "Shashank Agarwal",
        "Shubham Milind Phal",
        "Sun Jae Lee",
        "Theofilos Strinopoulos",
        "Wei-Jen Ko",
        "Amy Wang",
        "Ankit Anand",
        "Avishkar Bhoopchand",
        "Dan Wild",
        "Divya Pandya",
        "Filip Bar",
        "Garth Graham",
        "Holger Winnemoeller",
        "Mahvish Nagda",
        "Prateek Kolhar",
        "Renee Schneider",
        "Shaojian Zhu",
        "Stephanie Chan",
        "Steve Yadlowsky",
        "Viknesh Sounderajah",
        "Yannis Assael"
      ],
      "abstract": "Today's generative AI systems are tuned to present information by default\nrather than engage users in service of learning as a human tutor would. To\naddress the wide range of potential education use cases for these systems, we\nreframe the challenge of injecting pedagogical behavior as one of\n\\textit{pedagogical instruction following}, where training and evaluation\nexamples include system-level instructions describing the specific pedagogy\nattributes present or desired in subsequent model turns. This framing avoids\ncommitting our models to any particular definition of pedagogy, and instead\nallows teachers or developers to specify desired model behavior. It also clears\na path to improving Gemini models for learning -- by enabling the addition of\nour pedagogical data to post-training mixtures -- alongside their rapidly\nexpanding set of capabilities. Both represent important changes from our\ninitial tech report. We show how training with pedagogical instruction\nfollowing produces a LearnLM model (available on Google AI Studio) that is\npreferred substantially by expert raters across a diverse set of learning\nscenarios, with average preference strengths of 31\\% over GPT-4o, 11\\% over\nClaude 3.5, and 13\\% over the Gemini 1.5 Pro model LearnLM was based on.",
      "tldr_zh": "该研究提出 LearnLM，一种改进 Gemini 模型的方法，通过重新 framing 为 pedagogical instruction following（教学指令遵循），允许用户指定教学行为，从而增强 AI 在教育场景中的互动性。该方法涉及在后训练混合中添加教学数据，避免模型固定于特定教学定义，并支持各种教育用例。实验结果显示，LearnLM 模型在多样学习场景中获得专家评委显著偏好，平均强度为 31% 超过 GPT-4o、11% 超过 Claude 3.5，以及 13% 超过基础 Gemini 1.5 Pro 模型。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.16429v2",
      "published_date": "2024-12-21 01:34:05 UTC",
      "updated_date": "2024-12-25 06:12:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:18:21.304571"
    },
    {
      "arxiv_id": "2412.16428v2",
      "title": "Data-Driven Fairness Generalization for Deepfake Detection",
      "title_zh": "数据驱动的深度伪造检测公平性泛化",
      "authors": [
        "Uzoamaka Ezeakunne",
        "Chrisantus Eze",
        "Xiuwen Liu"
      ],
      "abstract": "Despite the progress made in deepfake detection research, recent studies have\nshown that biases in the training data for these detectors can result in\nvarying levels of performance across different demographic groups, such as race\nand gender. These disparities can lead to certain groups being unfairly\ntargeted or excluded. Traditional methods often rely on fair loss functions to\naddress these issues, but they under-perform when applied to unseen datasets,\nhence, fairness generalization remains a challenge. In this work, we propose a\ndata-driven framework for tackling the fairness generalization problem in\ndeepfake detection by leveraging synthetic datasets and model optimization. Our\napproach focuses on generating and utilizing synthetic data to enhance fairness\nacross diverse demographic groups. By creating a diverse set of synthetic\nsamples that represent various demographic groups, we ensure that our model is\ntrained on a balanced and representative dataset. This approach allows us to\ngeneralize fairness more effectively across different domains. We employ a\ncomprehensive strategy that leverages synthetic data, a loss sharpness-aware\noptimization pipeline, and a multi-task learning framework to create a more\nequitable training environment, which helps maintain fairness across both\nintra-dataset and cross-dataset evaluations. Extensive experiments on benchmark\ndeepfake detection datasets demonstrate the efficacy of our approach,\nsurpassing state-of-the-art approaches in preserving fairness during\ncross-dataset evaluation. Our results highlight the potential of synthetic\ndatasets in achieving fairness generalization, providing a robust solution for\nthe challenges faced in deepfake detection.",
      "tldr_zh": "本文研究发现，现有的深度伪造检测（deepfake detection）模型由于训练数据偏见，导致不同人口统计群体（如种族和性别）的性能差异，且传统公平损失函数在未见数据集上表现不佳，影响了fairness generalization。该论文提出一个数据驱动框架，通过生成代表多样群体的合成数据集（synthetic datasets），并结合损失锐度感知优化和多任务学习框架，优化模型训练以提升公平性泛化能力。实验结果显示，该方法在基准深度伪造检测数据集上超越了现有方法，在跨数据集评估中显著提高了公平性，提供了一个稳健的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at ICAART 2025",
      "pdf_url": "http://arxiv.org/pdf/2412.16428v2",
      "published_date": "2024-12-21 01:28:35 UTC",
      "updated_date": "2024-12-31 07:15:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:18:48.158793"
    },
    {
      "arxiv_id": "2412.16425v1",
      "title": "Patherea: Cell Detection and Classification for the 2020s",
      "title_zh": "Patherea：2020年代的细胞检测和分类",
      "authors": [
        "Dejan Štepec",
        "Maja Jerše",
        "Snežana Đokić",
        "Jera Jeruc",
        "Nina Zidar",
        "Danijel Skočaj"
      ],
      "abstract": "This paper presents a Patherea, a framework for point-based cell detection\nand classification that provides a complete solution for developing and\nevaluating state-of-the-art approaches. We introduce a large-scale dataset\ncollected to directly replicate a clinical workflow for Ki-67 proliferation\nindex estimation and use it to develop an efficient point-based approach that\ndirectly predicts point-based predictions, without the need for intermediate\nrepresentations. The proposed approach effectively utilizes point proposal\ncandidates with the hybrid Hungarian matching strategy and a flexible\narchitecture that enables the usage of various backbones and (pre)training\nstrategies. We report state-of-the-art results on existing public datasets -\nLizard, BRCA-M2C, BCData, and the newly proposed Patherea dataset. We show that\nthe performance on existing public datasets is saturated and that the newly\nproposed Patherea dataset represents a significantly harder challenge for the\nrecently proposed approaches. We also demonstrate the effectiveness of recently\nproposed pathology foundational models that our proposed approach can natively\nutilize and benefit from. We also revisit the evaluation protocol that is used\nin the broader field of cell detection and classification and identify the\nerroneous calculation of performance metrics. Patherea provides a benchmarking\nutility that addresses the identified issues and enables a fair comparison of\ndifferent approaches. The dataset and the code will be publicly released upon\nacceptance.",
      "tldr_zh": "本研究提出了 Patherea 框架，这是一种基于点的细胞检测和分类解决方案，旨在为开发和评估最先进方法提供完整支持。该框架引入了一个大规模数据集，用于模拟 Ki-67 增殖指数估计的临床工作流程，并采用高效的点-based 预测方法，利用点提案候选物、混合 Hungarian matching 策略和灵活架构，支持多种 backbone 和训练策略。在现有公共数据集（如 Lizard、BRCA-M2C 和 BCData）上，Patherea 达到了最先进的结果，而新提出的数据集则证明了更高的挑战性，并展示了病理基础模型的有效性。该框架还修正了细胞检测领域的评估协议错误，提供基准测试工具以实现公平比较，并计划公开数据集和代码。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "Submitted to Medical Image Analysis",
      "pdf_url": "http://arxiv.org/pdf/2412.16425v1",
      "published_date": "2024-12-21 01:23:58 UTC",
      "updated_date": "2024-12-21 01:23:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:19:00.364909"
    },
    {
      "arxiv_id": "2412.16423v1",
      "title": "Technical Report: Small Language Model for Japanese Clinical and Medicine",
      "title_zh": "翻译失败",
      "authors": [
        "Shogo Watanabe"
      ],
      "abstract": "This report presents a small language model (SLM) for Japanese clinical and\nmedicine, named NCVC-slm-1. This 1B parameters model was trained using Japanese\ntext classified to be of high-quality. Moreover, NCVC-slm-1 was augmented with\nrespect to clinical and medicine content that includes the variety of diseases,\ndrugs, and examinations. Using a carefully designed pre-processing, a\nspecialized morphological analyzer and tokenizer, this small and light-weight\nmodel performed not only to generate text but also indicated the feasibility of\nunderstanding clinical and medicine text. In comparison to other large language\nmodels, a fine-tuning NCVC-slm-1 demonstrated the highest scores on 6 tasks of\ntotal 8 on JMED-LLM. According to this result, SLM indicated the feasibility of\nperforming several downstream tasks in the field of clinical and medicine.\nHopefully, NCVC-slm-1 will be contributed to develop and accelerate the field\nof clinical and medicine for a bright future.",
      "tldr_zh": "本报告介绍了名为 NCVC-slm-1 的小型语言模型 (SLM)，这是一个参数量为 1B 的模型，专门针对日文临床和医学领域设计，通过使用高质量日文文本和增强内容（如疾病、药物和检查）进行训练。模型采用了精心设计的预处理、专门的形态分析器和分词器，不仅能生成文本，还能有效理解临床和医学内容。在 JMED-LLM 的 8 个任务中，微调后的 NCVC-slm-1 在 6 个任务上取得了最高分数，证明了 SLM 在该领域的下游任务可行性。该模型有望促进临床和医学领域的快速发展。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.16423v1",
      "published_date": "2024-12-21 01:12:48 UTC",
      "updated_date": "2024-12-21 01:12:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:19:13.052258"
    },
    {
      "arxiv_id": "2412.16411v1",
      "title": "Knowledge as a Breaking of Ergodicity",
      "title_zh": "知识作为遍历性破坏",
      "authors": [
        "Yang He",
        "Vassiliy Lubchenko"
      ],
      "abstract": "We construct a thermodynamic potential that can guide training of a\ngenerative model defined on a set of binary degrees of freedom. We argue that\nupon reduction in description, so as to make the generative model\ncomputationally-manageable, the potential develops multiple minima. This is\nmirrored by the emergence of multiple minima in the free energy proper of the\ngenerative model itself. The variety of training samples that employ N binary\ndegrees of freedom is ordinarily much lower than the size 2^N of the full phase\nspace. The non-represented configurations, we argue, should be thought of as\ncomprising a high-temperature phase separated by an extensive energy gap from\nthe configurations composing the training set. Thus, training amounts to\nsampling a free energy surface in the form of a library of distinct bound\nstates, each of which breaks ergodicity. The ergodicity breaking prevents\nescape into the near continuum of states comprising the high-temperature phase;\nthus it is necessary for proper functionality. It may however have the side\neffect of limiting access to patterns that were underrepresented in the\ntraining set. At the same time, the ergodicity breaking within the library\ncomplicates both learning and retrieval. As a remedy, one may concurrently\nemploy multiple generative models -- up to one model per free energy minimum.",
      "tldr_zh": "本研究构建了一个热力学势能（thermodynamic potential），用于指导基于二进制自由度的生成模型（generative model）的训练，通过减少描述来处理计算复杂性，导致势能和自由能（free energy）出现多个最小值。作者认为，训练样本的多样性远低于全相空间大小，未表示的配置形成一个高温相，与训练集分离，训练过程相当于采样一个由独立束缚态组成的自由能表面，每个束缚态都打破了遍历性（ergodicity breaking）。这种ergodicity breaking有助于防止模型逃逸到无关状态，但可能限制对训练中 underrepresented 模式的访问，并使学习和检索复杂化，为此建议同时使用多个生成模型，每个自由能最小值对应一个模型。",
      "categories": [
        "cs.AI",
        "cond-mat.dis-nn",
        "cs.CC",
        "stat.ML",
        "I.2.4, F.2.0"
      ],
      "primary_category": "cs.AI",
      "comment": "51 pages, 12 figures, accepted to Neural Computation",
      "pdf_url": "http://arxiv.org/pdf/2412.16411v1",
      "published_date": "2024-12-21 00:30:07 UTC",
      "updated_date": "2024-12-21 00:30:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:19:24.177863"
    },
    {
      "arxiv_id": "2412.16409v1",
      "title": "Uncertainty Quantification in Continual Open-World Learning",
      "title_zh": "持续开放世界学习中的不确定性量化",
      "authors": [
        "Amanda S. Rios",
        "Ibrahima J. Ndiour",
        "Parual Datta",
        "Jaroslaw Sydir",
        "Omesh Tickoo",
        "Nilesh Ahuja"
      ],
      "abstract": "AI deployed in the real-world should be capable of autonomously adapting to\nnovelties encountered after deployment. Yet, in the field of continual\nlearning, the reliance on novelty and labeling oracles is commonplace albeit\nunrealistic. This paper addresses a challenging and under-explored problem: a\ndeployed AI agent that continuously encounters unlabeled data - which may\ninclude both unseen samples of known classes and samples from novel (unknown)\nclasses - and must adapt to it continuously. To tackle this challenge, we\npropose our method COUQ \"Continual Open-world Uncertainty Quantification\", an\niterative uncertainty estimation algorithm tailored for learning in generalized\ncontinual open-world multi-class settings. We rigorously apply and evaluate\nCOUQ on key sub-tasks in the Continual Open-World: continual novelty detection,\nuncertainty guided active learning, and uncertainty guided pseudo-labeling for\nsemi-supervised CL. We demonstrate the effectiveness of our method across\nmultiple datasets, ablations, backbones and performance superior to\nstate-of-the-art.",
      "tldr_zh": "这篇论文探讨了 AI 在真实世界部署中的持续开放世界学习（Continual Open-World Learning），强调代理需自主适应未标记数据，包括已知类的新样本和新型类，而非依赖不切实际的标记或acles。作者提出 COUQ（Continual Open-world Uncertainty Quantification）算法，这是一种针对多类设置的迭代不确定性估计方法，用于处理该挑战。COUQ 被应用于持续新颖性检测（continual novelty detection）、不确定性引导的主动学习（uncertainty guided active learning）和不确定性引导的伪标签半监督学习（uncertainty guided pseudo-labeling for semi-supervised CL）等子任务。实验结果显示，COUQ 在多个数据集和骨干模型上表现出色，优于最先进方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "I.2; I.4"
      ],
      "primary_category": "cs.LG",
      "comment": "Manuscript Under Review (full-length); Related 4-page manuscripts\n  accepted at Neurips 2024 Non-Archival Workshops\n  https://sites.google.com/view/continual-fomo-workshop and\n  https://imol-workshop.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2412.16409v1",
      "published_date": "2024-12-21 00:09:20 UTC",
      "updated_date": "2024-12-21 00:09:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T16:19:37.246764"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 75,
  "processed_papers_count": 75,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-21T16:19:57.745256"
}
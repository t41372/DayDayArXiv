[
  {
    "arxiv_id": "2405.00254v2",
    "title": "RLHF from Heterogeneous Feedback via Personalization and Preference Aggregation",
    "authors": [
      "Chanwoo Park",
      "Mingyang Liu",
      "Dingwen Kong",
      "Kaiqing Zhang",
      "Asuman Ozdaglar"
    ],
    "abstract": "Reinforcement learning from human feedback (RLHF) has been an effective\ntechnique for aligning AI systems with human values, with remarkable successes\nin fine-tuning large-language models recently. Most existing RLHF paradigms\nmake the underlying assumption that human preferences are relatively\nhomogeneous, and can be encoded by a single reward model. In this paper, we\nfocus on addressing the issues due to the inherent heterogeneity in human\npreferences, as well as their potential strategic behavior in providing\nfeedback. Specifically, we propose two frameworks to address heterogeneous\nhuman feedback in principled ways: personalization-based one and\naggregation-based one. For the former, we propose two approaches based on\nrepresentation learning and clustering, respectively, for learning multiple\nreward models that trades off the bias (due to preference heterogeneity) and\nvariance (due to the use of fewer data for learning each model by\npersonalization). We then establish sample complexity guarantees for both\napproaches. For the latter, we aim to adhere to the single-model framework, as\nalready deployed in the current RLHF paradigm, by carefully aggregating diverse\nand truthful preferences from humans. We propose two approaches based on reward\nand preference aggregation, respectively: the former utilizes both\nutilitarianism and Leximin approaches to aggregate individual reward models,\nwith sample complexity guarantees; the latter directly aggregates the human\nfeedback in the form of probabilistic opinions. Under the\nprobabilistic-opinion-feedback model, we also develop an approach to handle\nstrategic human labelers who may bias and manipulate the aggregated preferences\nwith untruthful feedback. Based on the ideas in mechanism design, our approach\nensures truthful preference reporting, with the induced aggregation rule\nmaximizing social welfare functions.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Added experiments",
    "pdf_url": "http://arxiv.org/pdf/2405.00254v2",
    "published_date": "2024-04-30 23:57:23 UTC",
    "updated_date": "2024-05-27 14:08:40 UTC"
  },
  {
    "arxiv_id": "2405.00252v3",
    "title": "Q-Newton: Hybrid Quantum-Classical Scheduling for Accelerating Neural Network Training with Newton's Gradient Descent",
    "authors": [
      "Pingzhi Li",
      "Junyu Liu",
      "Hanrui Wang",
      "Tianlong Chen"
    ],
    "abstract": "Optimization techniques in deep learning are predominantly led by first-order\ngradient methodologies, such as SGD. However, neural network training can\ngreatly benefit from the rapid convergence characteristics of second-order\noptimization. Newton's GD stands out in this category, by rescaling the\ngradient using the inverse Hessian. Nevertheless, one of its major bottlenecks\nis matrix inversion, which is notably time-consuming in $O(N^3)$ time with weak\nscalability.\n  Matrix inversion can be translated into solving a series of linear equations.\nGiven that quantum linear solver algorithms (QLSAs), leveraging the principles\nof quantum superposition and entanglement, can operate within a\n$\\text{polylog}(N)$ time frame, they present a promising approach with\nexponential acceleration. Specifically, one of the most recent QLSAs\ndemonstrates a complexity scaling of $O(d\\cdot\\kappa\n\\log(N\\cdot\\kappa/\\epsilon))$, depending on: {size~$N$, condition\nnumber~$\\kappa$, error tolerance~$\\epsilon$, quantum oracle sparsity~$d$} of\nthe matrix. However, this also implies that their potential exponential\nadvantage may be hindered by certain properties (i.e. $\\kappa$ and $d$).\n  We propose Q-Newton, a hybrid quantum-classical scheduler for accelerating\nneural network training with Newton's GD. Q-Newton utilizes a streamlined\nscheduling module that coordinates between quantum and classical linear\nsolvers, by estimating & reducing $\\kappa$ and constructing $d$ for the quantum\nsolver.\n  Our evaluation showcases the potential for Q-Newton to significantly reduce\nthe total training time compared to commonly used optimizers like SGD. We\nhypothesize a future scenario where the gate time of quantum machines is\nreduced, possibly realized by attoseconds physics. Our evaluation establishes\nan ambitious and promising target for the evolution of quantum computing.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "quant-ph",
    "comment": "Our code is provided at https://github.com/UNITES-Lab/q-newton",
    "pdf_url": "http://arxiv.org/pdf/2405.00252v3",
    "published_date": "2024-04-30 23:55:03 UTC",
    "updated_date": "2025-04-29 06:14:40 UTC"
  },
  {
    "arxiv_id": "2405.00248v1",
    "title": "Who is Authentic Speaker",
    "authors": [
      "Qiang Huang"
    ],
    "abstract": "Voice conversion (VC) using deep learning technologies can now generate high\nquality one-to-many voices and thus has been used in some practical application\nfields, such as entertainment and healthcare. However, voice conversion can\npose potential social issues when manipulated voices are employed for deceptive\npurposes. Moreover, it is a big challenge to find who are real speakers from\nthe converted voices as the acoustic characteristics of source speakers are\nchanged greatly. In this paper we attempt to explore the feasibility of\nidentifying authentic speakers from converted voices. This study is conducted\nwith the assumption that certain information from the source speakers persists,\neven when their voices undergo conversion into different target voices.\nTherefore our experiments are geared towards recognising the source speakers\ngiven the converted voices, which are generated by using FragmentVC on the\nrandomly paired utterances from source and target speakers. To improve the\nrobustness against converted voices, our recognition model is constructed by\nusing hierarchical vector of locally aggregated descriptors (VLAD) in deep\nneural networks. The authentic speaker recognition system is mainly tested in\ntwo aspects, including the impact of quality of converted voices and the\nvariations of VLAD. The dataset used in this work is VCTK corpus, where source\nand target speakers are randomly paired. The results obtained on the converted\nutterances show promising performances in recognising authentic speakers from\nconverted voices.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.MM",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.00248v1",
    "published_date": "2024-04-30 23:41:00 UTC",
    "updated_date": "2024-04-30 23:41:00 UTC"
  },
  {
    "arxiv_id": "2405.00242v1",
    "title": "Guiding Attention in End-to-End Driving Models",
    "authors": [
      "Diego Porres",
      "Yi Xiao",
      "Gabriel Villalonga",
      "Alexandre Levy",
      "Antonio M. López"
    ],
    "abstract": "Vision-based end-to-end driving models trained by imitation learning can lead\nto affordable solutions for autonomous driving. However, training these\nwell-performing models usually requires a huge amount of data, while still\nlacking explicit and intuitive activation maps to reveal the inner workings of\nthese models while driving. In this paper, we study how to guide the attention\nof these models to improve their driving quality and obtain more intuitive\nactivation maps by adding a loss term during training using salient semantic\nmaps. In contrast to previous work, our method does not require these salient\nsemantic maps to be available during testing time, as well as removing the need\nto modify the model's architecture to which it is applied. We perform tests\nusing perfect and noisy salient semantic maps with encouraging results in both,\nthe latter of which is inspired by possible errors encountered with real data.\nUsing CIL++ as a representative state-of-the-art model and the CARLA simulator\nwith its standard benchmarks, we conduct experiments that show the\neffectiveness of our method in training better autonomous driving models,\nespecially when data and computational resources are scarce.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted for publication at the 35th IEEE Intelligent Vehicles\n  Symposium (IV 2024)",
    "pdf_url": "http://arxiv.org/pdf/2405.00242v1",
    "published_date": "2024-04-30 23:18:51 UTC",
    "updated_date": "2024-04-30 23:18:51 UTC"
  },
  {
    "arxiv_id": "2405.00236v1",
    "title": "STT: Stateful Tracking with Transformers for Autonomous Driving",
    "authors": [
      "Longlong Jing",
      "Ruichi Yu",
      "Xu Chen",
      "Zhengli Zhao",
      "Shiwei Sheng",
      "Colin Graber",
      "Qi Chen",
      "Qinru Li",
      "Shangxuan Wu",
      "Han Deng",
      "Sangjin Lee",
      "Chris Sweeney",
      "Qiurui He",
      "Wei-Chih Hung",
      "Tong He",
      "Xingyi Zhou",
      "Farshid Moussavi",
      "Zijian Guo",
      "Yin Zhou",
      "Mingxing Tan",
      "Weilong Yang",
      "Congcong Li"
    ],
    "abstract": "Tracking objects in three-dimensional space is critical for autonomous\ndriving. To ensure safety while driving, the tracker must be able to reliably\ntrack objects across frames and accurately estimate their states such as\nvelocity and acceleration in the present. Existing works frequently focus on\nthe association task while either neglecting the model performance on state\nestimation or deploying complex heuristics to predict the states. In this\npaper, we propose STT, a Stateful Tracking model built with Transformers, that\ncan consistently track objects in the scenes while also predicting their states\naccurately. STT consumes rich appearance, geometry, and motion signals through\nlong term history of detections and is jointly optimized for both data\nassociation and state estimation tasks. Since the standard tracking metrics\nlike MOTA and MOTP do not capture the combined performance of the two tasks in\nthe wider spectrum of object states, we extend them with new metrics called\nS-MOTA and MOTPS that address this limitation. STT achieves competitive\nreal-time performance on the Waymo Open Dataset.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "ICRA 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.00236v1",
    "published_date": "2024-04-30 23:04:36 UTC",
    "updated_date": "2024-04-30 23:04:36 UTC"
  },
  {
    "arxiv_id": "2405.07896v2",
    "title": "Almanac Copilot: Towards Autonomous Electronic Health Record Navigation",
    "authors": [
      "Cyril Zakka",
      "Joseph Cho",
      "Gracia Fahed",
      "Rohan Shad",
      "Michael Moor",
      "Robyn Fong",
      "Dhamanpreet Kaur",
      "Vishnu Ravi",
      "Oliver Aalami",
      "Roxana Daneshjou",
      "Akshay Chaudhari",
      "William Hiesinger"
    ],
    "abstract": "Clinicians spend large amounts of time on clinical documentation, and\ninefficiencies impact quality of care and increase clinician burnout. Despite\nthe promise of electronic medical records (EMR), the transition from\npaper-based records has been negatively associated with clinician wellness, in\npart due to poor user experience, increased burden of documentation, and alert\nfatigue. In this study, we present Almanac Copilot, an autonomous agent capable\nof assisting clinicians with EMR-specific tasks such as information retrieval\nand order placement. On EHR-QA, a synthetic evaluation dataset of 300 common\nEHR queries based on real patient data, Almanac Copilot obtains a successful\ntask completion rate of 74% (n = 221 tasks) with a mean score of 2.45 over 3\n(95% CI:2.34-2.56). By automating routine tasks and streamlining the\ndocumentation process, our findings highlight the significant potential of\nautonomous agents to mitigate the cognitive load imposed on clinicians by\ncurrent EMR systems.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.07896v2",
    "published_date": "2024-04-30 22:55:27 UTC",
    "updated_date": "2024-05-14 20:25:23 UTC"
  },
  {
    "arxiv_id": "2405.00233v2",
    "title": "SemantiCodec: An Ultra Low Bitrate Semantic Audio Codec for General Sound",
    "authors": [
      "Haohe Liu",
      "Xuenan Xu",
      "Yi Yuan",
      "Mengyue Wu",
      "Wenwu Wang",
      "Mark D. Plumbley"
    ],
    "abstract": "Large language models (LLMs) have significantly advanced audio processing\nthrough audio codecs that convert audio into discrete tokens, enabling the\napplication of language modelling techniques to audio data. However,\ntraditional codecs often operate at high bitrates or within narrow domains such\nas speech and lack the semantic clues required for efficient language\nmodelling. Addressing these challenges, we introduce SemantiCodec, a novel\ncodec designed to compress audio into fewer than a hundred tokens per second\nacross diverse audio types, including speech, general sound, and music, without\ncompromising quality. SemantiCodec features a dual-encoder architecture: a\nsemantic encoder using a self-supervised pre-trained Audio Masked Autoencoder\n(AudioMAE), discretized using k-means clustering on extensive audio data, and\nan acoustic encoder to capture the remaining details. The semantic and acoustic\nencoder outputs are used to reconstruct audio via a diffusion-model-based\ndecoder. SemantiCodec is presented in three variants with token rates of 25,\n50, and 100 per second, supporting a range of ultra-low bit rates between 0.31\nkbps and 1.40 kbps. Experimental results demonstrate that SemantiCodec\nsignificantly outperforms the state-of-the-art Descript codec on reconstruction\nquality. Our results also suggest that SemantiCodec contains significantly\nricher semantic information than all evaluated state-of-the-art audio codecs,\neven at significantly lower bitrates. Our code and demos are available at\nhttps://haoheliu.github.io/SemantiCodec/.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.MM",
      "eess.AS",
      "eess.SP"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted by Journal of Selected Topics in Signal Processing (JSTSP).\n  Demo and code: https://haoheliu.github.io/SemantiCodec/",
    "pdf_url": "http://arxiv.org/pdf/2405.00233v2",
    "published_date": "2024-04-30 22:51:36 UTC",
    "updated_date": "2024-11-28 12:31:04 UTC"
  },
  {
    "arxiv_id": "2405.00229v1",
    "title": "Aptly: Making Mobile Apps from Natural Language",
    "authors": [
      "Evan W. Patton",
      "David Y. J. Kim",
      "Ashley Granquist",
      "Robin Liu",
      "Arianna Scott",
      "Jennet Zamanova",
      "Harold Abelson"
    ],
    "abstract": "We present Aptly, an extension of the MIT App Inventor platform enabling\nmobile app development via natural language powered by code-generating large\nlanguage models (LLMs). Aptly complements App Inventor's block language with a\ntext language designed to allow visual code generation via text-based LLMs. We\ndetail the technical aspects of how the Aptly server integrates LLMs with a\nrealtime collaboration function to facilitate the automated creation and\nediting of mobile apps given user instructions. The paper concludes with\ninsights from a study of a pilot implementation involving high school students,\nwhich examines Aptly's practicality and user experience. The findings\nunderscore Aptly's potential as a tool that democratizes app development and\nfosters technological creativity.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.PL"
    ],
    "primary_category": "cs.HC",
    "comment": "11 pages, 7 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2405.00229v1",
    "published_date": "2024-04-30 22:33:34 UTC",
    "updated_date": "2024-04-30 22:33:34 UTC"
  },
  {
    "arxiv_id": "2405.00218v3",
    "title": "Constrained Decoding for Secure Code Generation",
    "authors": [
      "Yanjun Fu",
      "Ethan Baker",
      "Yu Ding",
      "Yizheng Chen"
    ],
    "abstract": "Code Large Language Models (Code LLMs) have been increasingly used by\ndevelopers to boost productivity, but they often generate vulnerable code.\nThus, there is an urgent need to ensure that code generated by Code LLMs is\ncorrect and secure. Previous research has primarily focused on generating\nsecure code, overlooking the fact that secure code also needs to be correct.\nThis oversight can lead to a false sense of security. Currently, the community\nlacks a method to measure actual progress in this area, and we need solutions\nthat address both security and correctness of code generation.\n  This paper introduces a new benchmark, CodeGuard+, along with two new\nmetrics, to measure Code LLMs' ability to generate both secure and correct\ncode. Using our new evaluation methods, we show that the state-of-the-art\ndefense technique, prefix tuning, may not be as strong as previously believed,\nsince it generates secure code but sacrifices functional correctness. We also\ndemonstrate that different decoding methods significantly affect the security\nof Code LLMs.\n  Furthermore, we explore a new defense direction: constrained decoding for\nsecure code generation. We propose new constrained decoding techniques to\ngenerate secure code. Our results reveal that constrained decoding is more\neffective than prefix tuning to improve the security of Code LLMs, without\nrequiring a specialized training dataset. Moreover, our evaluations over eight\nstate-of-the-art Code LLMs show that constrained decoding has strong\nperformance to improve the security of Code LLMs, and our technique outperforms\nGPT-4.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "cs.SE"
    ],
    "primary_category": "cs.CR",
    "comment": "17 pages, 9 figures, our website is available at\n  https://codeguardplus.github.io",
    "pdf_url": "http://arxiv.org/pdf/2405.00218v3",
    "published_date": "2024-04-30 21:52:19 UTC",
    "updated_date": "2024-07-20 19:14:03 UTC"
  },
  {
    "arxiv_id": "2405.00216v1",
    "title": "Graphical Reasoning: LLM-based Semi-Open Relation Extraction",
    "authors": [
      "Yicheng Tao",
      "Yiqun Wang",
      "Longju Bai"
    ],
    "abstract": "This paper presents a comprehensive exploration of relation extraction\nutilizing advanced language models, specifically Chain of Thought (CoT) and\nGraphical Reasoning (GRE) techniques. We demonstrate how leveraging in-context\nlearning with GPT-3.5 can significantly enhance the extraction process,\nparticularly through detailed example-based reasoning. Additionally, we\nintroduce a novel graphical reasoning approach that dissects relation\nextraction into sequential sub-tasks, improving precision and adaptability in\nprocessing complex relational data. Our experiments, conducted on multiple\ndatasets, including manually annotated data, show considerable improvements in\nperformance metrics, underscoring the effectiveness of our methodologies.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.00216v1",
    "published_date": "2024-04-30 21:41:53 UTC",
    "updated_date": "2024-04-30 21:41:53 UTC"
  },
  {
    "arxiv_id": "2405.00205v2",
    "title": "A Logic for Reasoning About Aggregate-Combine Graph Neural Networks",
    "authors": [
      "Pierre Nunn",
      "Marco Sälzer",
      "François Schwarzentruber",
      "Nicolas Troquard"
    ],
    "abstract": "We propose a modal logic in which counting modalities appear in linear\ninequalities. We show that each formula can be transformed into an equivalent\ngraph neural network (GNN). We also show that a broad class of GNNs can be\ntransformed efficiently into a formula, thus significantly improving upon the\nliterature about the logical expressiveness of GNNs. We also show that the\nsatisfiability problem is PSPACE-complete. These results bring together the\npromise of using standard logical methods for reasoning about GNNs and their\nproperties, particularly in applications such as GNN querying, equivalence\nchecking, etc. We prove that such natural problems can be solved in polynomial\nspace.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "arXiv admin note: text overlap with arXiv:2307.05150",
    "pdf_url": "http://arxiv.org/pdf/2405.00205v2",
    "published_date": "2024-04-30 21:16:38 UTC",
    "updated_date": "2025-03-27 10:38:15 UTC"
  },
  {
    "arxiv_id": "2405.00204v1",
    "title": "General Purpose Verification for Chain of Thought Prompting",
    "authors": [
      "Robert Vacareanu",
      "Anurag Pratik",
      "Evangelia Spiliopoulou",
      "Zheng Qi",
      "Giovanni Paolini",
      "Neha Anna John",
      "Jie Ma",
      "Yassine Benajiba",
      "Miguel Ballesteros"
    ],
    "abstract": "Many of the recent capabilities demonstrated by Large Language Models (LLMs)\narise primarily from their ability to exploit contextual information. In this\npaper, we explore ways to improve reasoning capabilities of LLMs through (1)\nexploration of different chains of thought and (2) validation of the individual\nsteps of the reasoning process. We propose three general principles that a\nmodel should adhere to while reasoning: (i) Relevance, (ii) Mathematical\nAccuracy, and (iii) Logical Consistency. We apply these constraints to the\nreasoning steps generated by the LLM to improve the accuracy of the final\ngeneration. The constraints are applied in the form of verifiers: the model\nitself is asked to verify if the generated steps satisfy each constraint. To\nfurther steer the generations towards high-quality solutions, we use the\nperplexity of the reasoning steps as an additional verifier. We evaluate our\nmethod on 4 distinct types of reasoning tasks, spanning a total of 9 different\ndatasets. Experiments show that our method is always better than vanilla\ngeneration, and, in 6 out of the 9 datasets, it is better than best-of N\nsampling which samples N reasoning chains and picks the lowest perplexity\ngeneration.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "22 pages, preprint",
    "pdf_url": "http://arxiv.org/pdf/2405.00204v1",
    "published_date": "2024-04-30 21:15:17 UTC",
    "updated_date": "2024-04-30 21:15:17 UTC"
  },
  {
    "arxiv_id": "2405.00201v1",
    "title": "SPAFIT: Stratified Progressive Adaptation Fine-tuning for Pre-trained Large Language Models",
    "authors": [
      "Samir Arora",
      "Liangliang Wang"
    ],
    "abstract": "Full fine-tuning is a popular approach to adapt Transformer-based pre-trained\nlarge language models to a specific downstream task. However, the substantial\nrequirements for computational power and storage have discouraged its\nwidespread use. Moreover, increasing evidence of catastrophic forgetting and\noverparameterization in the Transformer architecture has motivated researchers\nto seek more efficient fine-tuning (PEFT) methods. Commonly known\nparameter-efficient fine-tuning methods like LoRA and BitFit are typically\napplied across all layers of the model. We propose a PEFT method, called\nStratified Progressive Adaptation Fine-tuning (SPAFIT), based on the\nlocalization of different types of linguistic knowledge to specific layers of\nthe model. Our experiments, conducted on nine tasks from the GLUE benchmark,\nshow that our proposed SPAFIT method outperforms other PEFT methods while\nfine-tuning only a fraction of the parameters adjusted by other methods.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.00201v1",
    "published_date": "2024-04-30 21:07:32 UTC",
    "updated_date": "2024-04-30 21:07:32 UTC"
  },
  {
    "arxiv_id": "2405.00197v1",
    "title": "Grounding Realizable Entities",
    "authors": [
      "Michael Rabenberg",
      "Carter Benson",
      "Federico Donato",
      "Yongqun He",
      "Anthony Huffman",
      "Shane Babcock",
      "John Beverley"
    ],
    "abstract": "Ontological representations of qualities, dispositions, and roles have been\nrefined over the past decade, clarifying subtle distinctions in life science\nresearch. After articulating a widely-used characterization of these entities\nwithin the context of Basic Formal Ontology (BFO), we identify gaps in this\ntreatment and motivate the need for supplementing the BFO characterization. By\nway of supplement, we propose definitions for grounding relations holding\nbetween qualities and dispositions, and dispositions and roles, illustrating\nour proposal by representing subtle aspects of host-pathogen interactions.",
    "categories": [
      "cs.AI",
      "cs.DB",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "13",
    "pdf_url": "http://arxiv.org/pdf/2405.00197v1",
    "published_date": "2024-04-30 21:01:34 UTC",
    "updated_date": "2024-04-30 21:01:34 UTC"
  },
  {
    "arxiv_id": "2405.00186v1",
    "title": "Credentials in the Occupation Ontology",
    "authors": [
      "John Beverley",
      "Robin McGill",
      "Sam Smith",
      "Jie Zheng",
      "Giacomo De Colle",
      "Finn Wilson",
      "Matthew Diller",
      "William D. Duncan",
      "William R. Hogan",
      "Yongqun He"
    ],
    "abstract": "The term credential encompasses educational certificates, degrees,\ncertifications, and government-issued licenses. An occupational credential is a\nverification of an individuals qualification or competence issued by a third\nparty with relevant authority. Job seekers often leverage such credentials as\nevidence that desired qualifications are satisfied by their holders. Many U.S.\neducation and workforce development organizations have recognized the\nimportance of credentials for employment and the challenges of understanding\nthe value of credentials. In this study, we identified and ontologically\ndefined credential and credential-related terms at the textual and semantic\nlevels based on the Occupation Ontology (OccO), a BFO-based ontology. Different\ncredential types and their authorization logic are modeled. We additionally\ndefined a high-level hierarchy of credential related terms and relations among\nmany terms, which were initiated in concert with the Alabama Talent Triad (ATT)\nprogram, which aims to connect learners, earners, employers and\neducation/training providers through credentials and skills. To our knowledge,\nour research provides for the first time systematic ontological modeling of the\nimportant domain of credentials and related contents, supporting enhanced\ncredential data and knowledge integration in the future.",
    "categories": [
      "cs.AI",
      "cs.DB",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "11",
    "pdf_url": "http://arxiv.org/pdf/2405.00186v1",
    "published_date": "2024-04-30 20:23:18 UTC",
    "updated_date": "2024-04-30 20:23:18 UTC"
  },
  {
    "arxiv_id": "2405.00183v2",
    "title": "Capabilities: An Ontology",
    "authors": [
      "John Beverley",
      "David Limbaugh",
      "Eric Merrell",
      "Peter M. Koch",
      "Barry Smith"
    ],
    "abstract": "In our daily lives, as in science and in all other domains, we encounter huge\nnumbers of dispositions (tendencies, potentials, powers) which are realized in\nprocesses such as sneezing, sweating, shedding dandruff, and on and on. Among\nthis plethora of what we can think of as mere dispositions is a subset of\ndispositions in whose realizations we have an interest a car responding well\nwhen driven on ice, a rabbits lungs responding well when it is chased by a\nwolf, and so on. We call the latter capabilities and we attempt to provide a\nrobust ontological account of what capabilities are that is of sufficient\ngenerality to serve a variety of purposes, for example by providing a useful\nextension to ontology-based research in areas where capabilities data are\ncurrently being collected in siloed fashion.",
    "categories": [
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "14",
    "pdf_url": "http://arxiv.org/pdf/2405.00183v2",
    "published_date": "2024-04-30 20:16:14 UTC",
    "updated_date": "2024-08-16 03:21:14 UTC"
  },
  {
    "arxiv_id": "2405.00182v1",
    "title": "M-DEW: Extending Dynamic Ensemble Weighting to Handle Missing Values",
    "authors": [
      "Adam Catto",
      "Nan Jia",
      "Ansaf Salleb-Aouissi",
      "Anita Raja"
    ],
    "abstract": "Missing value imputation is a crucial preprocessing step for many machine\nlearning problems. However, it is often considered as a separate subtask from\ndownstream applications such as classification, regression, or clustering, and\nthus is not optimized together with them. We hypothesize that treating the\nimputation model and downstream task model together and optimizing over full\npipelines will yield better results than treating them separately. Our work\ndescribes a novel AutoML technique for making downstream predictions with\nmissing data that automatically handles preprocessing, model weighting, and\nselection during inference time, with minimal compute overhead. Specifically we\ndevelop M-DEW, a Dynamic missingness-aware Ensemble Weighting (DEW) approach,\nthat constructs a set of two-stage imputation-prediction pipelines, trains each\ncomponent separately, and dynamically calculates a set of pipeline weights for\neach sample during inference time. We thus extend previous work on dynamic\nensemble weighting to handle missing data at the level of full\nimputation-prediction pipelines, improving performance and calibration on\ndownstream machine learning tasks over standard model averaging techniques.\nM-DEW is shown to outperform the state-of-the-art in that it produces\nstatistically significant reductions in model perplexity in 17 out of 18\nexperiments, while improving average precision in 13 out of 18 experiments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.00182v1",
    "published_date": "2024-04-30 20:13:18 UTC",
    "updated_date": "2024-04-30 20:13:18 UTC"
  },
  {
    "arxiv_id": "2405.00181v2",
    "title": "Uncovering What, Why and How: A Comprehensive Benchmark for Causation Understanding of Video Anomaly",
    "authors": [
      "Hang Du",
      "Sicheng Zhang",
      "Binzhu Xie",
      "Guoshun Nan",
      "Jiayang Zhang",
      "Junrui Xu",
      "Hangyu Liu",
      "Sicong Leng",
      "Jiangming Liu",
      "Hehe Fan",
      "Dajiu Huang",
      "Jing Feng",
      "Linli Chen",
      "Can Zhang",
      "Xuhuan Li",
      "Hao Zhang",
      "Jianhang Chen",
      "Qimei Cui",
      "Xiaofeng Tao"
    ],
    "abstract": "Video anomaly understanding (VAU) aims to automatically comprehend unusual\noccurrences in videos, thereby enabling various applications such as traffic\nsurveillance and industrial manufacturing. While existing VAU benchmarks\nprimarily concentrate on anomaly detection and localization, our focus is on\nmore practicality, prompting us to raise the following crucial questions: \"what\nanomaly occurred?\", \"why did it happen?\", and \"how severe is this abnormal\nevent?\". In pursuit of these answers, we present a comprehensive benchmark for\nCausation Understanding of Video Anomaly (CUVA). Specifically, each instance of\nthe proposed benchmark involves three sets of human annotations to indicate the\n\"what\", \"why\" and \"how\" of an anomaly, including 1) anomaly type, start and end\ntimes, and event descriptions, 2) natural language explanations for the cause\nof an anomaly, and 3) free text reflecting the effect of the abnormality. In\naddition, we also introduce MMEval, a novel evaluation metric designed to\nbetter align with human preferences for CUVA, facilitating the measurement of\nexisting LLMs in comprehending the underlying cause and corresponding effect of\nvideo anomalies. Finally, we propose a novel prompt-based method that can serve\nas a baseline approach for the challenging CUVA. We conduct extensive\nexperiments to show the superiority of our evaluation metric and the\nprompt-based approach. Our code and dataset are available at\nhttps://github.com/fesvhtr/CUVA.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted in CVPR2024, Codebase: https://github.com/fesvhtr/CUVA",
    "pdf_url": "http://arxiv.org/pdf/2405.00181v2",
    "published_date": "2024-04-30 20:11:49 UTC",
    "updated_date": "2024-05-06 14:57:50 UTC"
  },
  {
    "arxiv_id": "2405.00747v4",
    "title": "Soft Preference Optimization: Aligning Language Models to Expert Distributions",
    "authors": [
      "Arsalan Sharifnassab",
      "Saber Salehkaleybar",
      "Sina Ghiassian",
      "Surya Kanoria",
      "Dale Schuurmans"
    ],
    "abstract": "We propose Soft Preference Optimization (SPO), a method for aligning\ngenerative models, such as Large Language Models (LLMs), with human\npreferences, without the need for a reward model. SPO optimizes model outputs\ndirectly over a preference dataset through a natural loss function that\nintegrates preference loss with a regularization term across the model's entire\noutput distribution rather than limiting it to the preference dataset. Although\nSPO does not require the assumption of an existing underlying reward model, we\ndemonstrate that, under the Bradley-Terry (BT) model assumption, it converges\nto a softmax of scaled rewards, with the distribution's \"softness\" adjustable\nvia the softmax exponent, an algorithm parameter. We showcase SPO's\nmethodology, its theoretical foundation, and its comparative advantages in\nsimplicity, computational efficiency, and alignment precision.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.00747v4",
    "published_date": "2024-04-30 19:48:55 UTC",
    "updated_date": "2024-10-04 00:31:48 UTC"
  },
  {
    "arxiv_id": "2405.03699v1",
    "title": "HCC Is All You Need: Alignment-The Sensible Kind Anyway-Is Just Human-Centered Computing",
    "authors": [
      "Eric Gilbert"
    ],
    "abstract": "This article argues that AI Alignment is a type of Human-Centered Computing.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.03699v1",
    "published_date": "2024-04-30 19:46:23 UTC",
    "updated_date": "2024-04-30 19:46:23 UTC"
  },
  {
    "arxiv_id": "2405.15786v1",
    "title": "Enhancement of Subjective Content Descriptions by using Human Feedback",
    "authors": [
      "Magnus Bender",
      "Tanya Braun",
      "Ralf Möller",
      "Marcel Gehrke"
    ],
    "abstract": "An agent providing an information retrieval service may work with a corpus of\ntext documents. The documents in the corpus may contain annotations such as\nSubjective Content Descriptions (SCD) -- additional data associated with\ndifferent sentences of the documents. Each SCD is associated with multiple\nsentences of the corpus and has relations among each other. The agent uses the\nSCDs to create its answers in response to queries supplied by users. However,\nthe SCD the agent uses might reflect the subjective perspective of another\nuser. Hence, answers may be considered faulty by an agent's user, because the\nSCDs may not exactly match the perceptions of an agent's user. A naive and very\ncostly approach would be to ask each user to completely create all the SCD\nthemselves. To use existing knowledge, this paper presents ReFrESH, an approach\nfor Relation-preserving Feedback-reliant Enhancement of SCDs by Humans. An\nagent's user can give feedback about faulty answers to the agent. This feedback\nis then used by ReFrESH to update the SCDs incrementally. However, human\nfeedback is not always unambiguous. Therefore, this paper additionally presents\nan approach to decide how to incorporate the feedback and when to update the\nSCDs. Altogether, SCDs can be updated with human feedback, allowing users to\ncreate even more specific SCDs for their needs.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.15786v1",
    "published_date": "2024-04-30 19:31:47 UTC",
    "updated_date": "2024-04-30 19:31:47 UTC"
  },
  {
    "arxiv_id": "2405.00166v1",
    "title": "Discovering intrinsic multi-compartment pharmacometric models using Physics Informed Neural Networks",
    "authors": [
      "Imran Nasim",
      "Adam Nasim"
    ],
    "abstract": "Pharmacometric models are pivotal across drug discovery and development,\nplaying a decisive role in determining the progression of candidate molecules.\nHowever, the derivation of mathematical equations governing the system is a\nlabor-intensive trial-and-error process, often constrained by tight timelines.\nIn this study, we introduce PKINNs, a novel purely data-driven\npharmacokinetic-informed neural network model. PKINNs efficiently discovers and\nmodels intrinsic multi-compartment-based pharmacometric structures, reliably\nforecasting their derivatives. The resulting models are both interpretable and\nexplainable through Symbolic Regression methods. Our computational framework\ndemonstrates the potential for closed-form model discovery in pharmacometric\napplications, addressing the labor-intensive nature of traditional model\nderivation. With the increasing availability of large datasets, this framework\nholds the potential to significantly enhance model-informed drug discovery.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted into the International conference on Scientific Computation\n  and Machine Learning 2024 (SCML 2024)",
    "pdf_url": "http://arxiv.org/pdf/2405.00166v1",
    "published_date": "2024-04-30 19:31:31 UTC",
    "updated_date": "2024-04-30 19:31:31 UTC"
  },
  {
    "arxiv_id": "2407.10247v1",
    "title": "Strategic Integration of Artificial Intelligence in the C-Suite: The Role of the Chief AI Officer",
    "authors": [
      "Marc Schmitt"
    ],
    "abstract": "The integration of Artificial Intelligence (AI) into corporate strategy has\nbecome a pivotal focus for organizations aiming to maintain a competitive\nadvantage in the digital age. As AI reshapes business operations and drives\ninnovation, the need for specialized leadership to effectively manage these\nchanges becomes increasingly apparent. In this paper, I explore the role of the\nChief AI Officer (CAIO) within the C-suite, emphasizing the necessity of this\nposition for successful AI strategy, integration, and governance. I analyze\nfuture scenarios based on current trends in three key areas: the AI Economy, AI\nOrganization, and Competition in the Age of AI. These explorations lay the\nfoundation for identifying the antecedents (environmental, structural, and\nstrategic factors) that justify the inclusion of a CAIO in top management\nteams. This sets the stage for a comprehensive examination of the CAIO's role\nand the broader implications of AI leadership. This paper advances the\ndiscussion on AI leadership by providing a rationale for the strategic\nintegration of AI at the executive level and examining the role of the Chief AI\nOfficer within organizations.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG",
      "econ.GN",
      "q-fin.EC"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.10247v1",
    "published_date": "2024-04-30 19:07:18 UTC",
    "updated_date": "2024-04-30 19:07:18 UTC"
  },
  {
    "arxiv_id": "2405.00156v2",
    "title": "Expanding the Horizon: Enabling Hybrid Quantum Transfer Learning for Long-Tailed Chest X-Ray Classification",
    "authors": [
      "Skylar Chan",
      "Pranav Kulkarni",
      "Paul H. Yi",
      "Vishwa S. Parekh"
    ],
    "abstract": "Quantum machine learning (QML) has the potential for improving the\nmulti-label classification of rare, albeit critical, diseases in large-scale\nchest x-ray (CXR) datasets due to theoretical quantum advantages over classical\nmachine learning (CML) in sample efficiency and generalizability. While prior\nliterature has explored QML with CXRs, it has focused on binary classification\ntasks with small datasets due to limited access to quantum hardware and\ncomputationally expensive simulations. To that end, we implemented a Jax-based\nframework that enables the simulation of medium-sized qubit architectures with\nsignificant improvements in wall-clock time over current software offerings. We\nevaluated the performance of our Jax-based framework in terms of efficiency and\nperformance for hybrid quantum transfer learning for long-tailed classification\nacross 8, 14, and 19 disease labels using large-scale CXR datasets. The\nJax-based framework resulted in up to a 58% and 95% speed-up compared to\nPyTorch and TensorFlow implementations, respectively. However, compared to CML,\nQML demonstrated slower convergence and an average AUROC of 0.70, 0.73, and\n0.74 for the classification of 8, 14, and 19 CXR disease labels. In comparison,\nthe CML models had an average AUROC of 0.77, 0.78, and 0.80 respectively. In\nconclusion, our work presents an accessible implementation of hybrid quantum\ntransfer learning for long-tailed CXR classification with a computationally\nefficient Jax-based framework.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "quant-ph"
    ],
    "primary_category": "cs.CV",
    "comment": "11 pages, 13 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2405.00156v2",
    "published_date": "2024-04-30 19:06:37 UTC",
    "updated_date": "2024-08-02 18:18:48 UTC"
  },
  {
    "arxiv_id": "2405.00746v2",
    "title": "Leveraging Sub-Optimal Data for Human-in-the-Loop Reinforcement Learning",
    "authors": [
      "Calarina Muslimani",
      "Matthew E. Taylor"
    ],
    "abstract": "To create useful reinforcement learning (RL) agents, step zero is to design a\nsuitable reward function that captures the nuances of the task. However, reward\nengineering can be a difficult and time-consuming process. Instead,\nhuman-in-the-loop RL methods hold the promise of learning reward functions from\nhuman feedback. Despite recent successes, many of the human-in-the-loop RL\nmethods still require numerous human interactions to learn successful reward\nfunctions. To improve the feedback efficiency of human-in-the-loop RL methods\n(i.e., require less human interaction), this paper introduces Sub-optimal Data\nPre-training, SDP, an approach that leverages reward-free, sub-optimal data to\nimprove scalar- and preference-based RL algorithms. In SDP, we start by\npseudo-labeling all low-quality data with the minimum environment reward.\nThrough this process, we obtain reward labels to pre-train our reward model\nwithout requiring human labeling or preferences. This pre-training phase\nprovides the reward model a head start in learning, enabling it to recognize\nthat low-quality transitions should be assigned low rewards. Through extensive\nexperiments with both simulated and human teachers, we find that SDP can at\nleast meet, but often significantly improve, state of the art human-in-the-loop\nRL performance across a variety of simulated robotic tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.00746v2",
    "published_date": "2024-04-30 18:58:33 UTC",
    "updated_date": "2025-04-07 23:17:50 UTC"
  },
  {
    "arxiv_id": "2405.00134v1",
    "title": "Transforming Dutch: Debiasing Dutch Coreference Resolution Systems for Non-binary Pronouns",
    "authors": [
      "Goya van Boven",
      "Yupei Du",
      "Dong Nguyen"
    ],
    "abstract": "Gender-neutral pronouns are increasingly being introduced across Western\nlanguages. Recent evaluations have however demonstrated that English NLP\nsystems are unable to correctly process gender-neutral pronouns, with the risk\nof erasing and misgendering non-binary individuals. This paper examines a Dutch\ncoreference resolution system's performance on gender-neutral pronouns,\nspecifically hen and die. In Dutch, these pronouns were only introduced in\n2016, compared to the longstanding existence of singular they in English. We\nadditionally compare two debiasing techniques for coreference resolution\nsystems in non-binary contexts: Counterfactual Data Augmentation (CDA) and\ndelexicalisation. Moreover, because pronoun performance can be hard to\ninterpret from a general evaluation metric like LEA, we introduce an innovative\nevaluation metric, the pronoun score, which directly represents the portion of\ncorrectly processed pronouns. Our results reveal diminished performance on\ngender-neutral pronouns compared to gendered counterparts. Nevertheless,\nalthough delexicalisation fails to yield improvements, CDA substantially\nreduces the performance gap between gendered and gender-neutral pronouns. We\nfurther show that CDA remains effective in low-resource settings, in which a\nlimited set of debiasing documents is used. This efficacy extends to previously\nunseen neopronouns, which are currently infrequently used but may gain\npopularity in the future, underscoring the viability of effective debiasing\nwith minimal resources and low computational costs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "22 pages, 2 figures. Accepted at the 2024 ACM Conference on Fairness,\n  Accountability, and Transparency (FAccT '24)",
    "pdf_url": "http://arxiv.org/pdf/2405.00134v1",
    "published_date": "2024-04-30 18:31:19 UTC",
    "updated_date": "2024-04-30 18:31:19 UTC"
  },
  {
    "arxiv_id": "2405.00117v2",
    "title": "Training a high-performance retinal foundation model with half-the-data and 400 times less compute",
    "authors": [
      "Justin Engelmann",
      "Miguel O. Bernabeu"
    ],
    "abstract": "Artificial Intelligence in medicine is traditionally limited by the lack of\nmassive training datasets. Foundation models, pre-trained models that can be\nadapted to downstream tasks with small datasets, could alleviate this problem.\nResearchers at Moorfields Eye Hospital (MEH) proposed RETFound-MEH, a retinal\nfoundation model trained on 900,000 images, including private hospital data.\nRecently, data-efficient DERETFound was proposed providing comparable\nperformance while being trained on only 150,000 publicly available images.\nHowever, both these models required very substantial resources to train\ninitially and are resource-intensive in downstream use. We propose a novel\nToken Reconstruction objective that we use to train RETFound-Green, a retinal\nfoundation model trained using only 75,000 publicly available images and 400\ntimes less compute. We estimate the cost of training RETFound-MEH and\nDERETFound at \\$10,000 and \\$14,000, respectively. RETFound-Green could be\ntrained for less than \\$100, with equally reduced environmental impact.\nRETFound-Green is also far more efficient in downstream use: it can be\ndownloaded 14 times faster, computes vector embeddings 2.7 times faster which\nthen require 2.6 times less storage space. Despite this, RETFound-Green does\nnot perform systematically worse. In fact, on various task on three downstream\ndatasets from Brazil, India and China, it performs best on 68 tasks out of 119\ncomparisons, versus 21 for DERETFound and 13 for RETFound-MEH. Our results\nsuggest that RETFound-Green is a very efficient, high-performance retinal\nfoundation model. We anticipate that our Token Reconstruction objective could\nbe scaled up for even higher performance and be applied to other domains beyond\nretinal imaging.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.00117v2",
    "published_date": "2024-04-30 18:08:08 UTC",
    "updated_date": "2024-09-22 17:25:40 UTC"
  },
  {
    "arxiv_id": "2405.00099v4",
    "title": "Creative Beam Search: LLM-as-a-Judge For Improving Response Generation",
    "authors": [
      "Giorgio Franceschelli",
      "Mirco Musolesi"
    ],
    "abstract": "Large language models are revolutionizing several areas, including artificial\ncreativity. However, the process of generation in machines profoundly diverges\nfrom that observed in humans. In particular, machine generation is\ncharacterized by a lack of intentionality and an underlying creative process.\nWe propose a method called Creative Beam Search that uses Diverse Beam Search\nand LLM-as-a-Judge to perform response generation and response validation. The\nresults of a qualitative experiment show how our approach can provide better\noutput than standard sampling techniques. We also show that the response\nvalidation step is a necessary complement to the response generation step.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Presented as a short paper at the 15th International Conference on\n  Computational Creativity (ICCC'24)",
    "pdf_url": "http://arxiv.org/pdf/2405.00099v4",
    "published_date": "2024-04-30 18:00:02 UTC",
    "updated_date": "2024-10-07 16:45:42 UTC"
  },
  {
    "arxiv_id": "2404.19756v5",
    "title": "KAN: Kolmogorov-Arnold Networks",
    "authors": [
      "Ziming Liu",
      "Yixuan Wang",
      "Sachin Vaidya",
      "Fabian Ruehle",
      "James Halverson",
      "Marin Soljačić",
      "Thomas Y. Hou",
      "Max Tegmark"
    ],
    "abstract": "Inspired by the Kolmogorov-Arnold representation theorem, we propose\nKolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer\nPerceptrons (MLPs). While MLPs have fixed activation functions on nodes\n(\"neurons\"), KANs have learnable activation functions on edges (\"weights\").\nKANs have no linear weights at all -- every weight parameter is replaced by a\nunivariate function parametrized as a spline. We show that this seemingly\nsimple change makes KANs outperform MLPs in terms of accuracy and\ninterpretability. For accuracy, much smaller KANs can achieve comparable or\nbetter accuracy than much larger MLPs in data fitting and PDE solving.\nTheoretically and empirically, KANs possess faster neural scaling laws than\nMLPs. For interpretability, KANs can be intuitively visualized and can easily\ninteract with human users. Through two examples in mathematics and physics,\nKANs are shown to be useful collaborators helping scientists (re)discover\nmathematical and physical laws. In summary, KANs are promising alternatives for\nMLPs, opening opportunities for further improving today's deep learning models\nwhich rely heavily on MLPs.",
    "categories": [
      "cs.LG",
      "cond-mat.dis-nn",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by International Conference on Learning Representations\n  (ICLR) 2025 (conference version: https://openreview.net/forum?id=Ozo7qJ5vZi).\n  Codes are available at https://github.com/KindXiaoming/pykan",
    "pdf_url": "http://arxiv.org/pdf/2404.19756v5",
    "published_date": "2024-04-30 17:58:29 UTC",
    "updated_date": "2025-02-09 21:09:09 UTC"
  },
  {
    "arxiv_id": "2404.19753v1",
    "title": "DOCCI: Descriptions of Connected and Contrasting Images",
    "authors": [
      "Yasumasa Onoe",
      "Sunayana Rane",
      "Zachary Berger",
      "Yonatan Bitton",
      "Jaemin Cho",
      "Roopal Garg",
      "Alexander Ku",
      "Zarana Parekh",
      "Jordi Pont-Tuset",
      "Garrett Tanzer",
      "Su Wang",
      "Jason Baldridge"
    ],
    "abstract": "Vision-language datasets are vital for both text-to-image (T2I) and\nimage-to-text (I2T) research. However, current datasets lack descriptions with\nfine-grained detail that would allow for richer associations to be learned by\nmodels. To fill the gap, we introduce Descriptions of Connected and Contrasting\nImages (DOCCI), a dataset with long, human-annotated English descriptions for\n15k images that were taken, curated and donated by a single researcher intent\non capturing key challenges such as spatial relations, counting, text\nrendering, world knowledge, and more. We instruct human annotators to create\ncomprehensive descriptions for each image; these average 136 words in length\nand are crafted to clearly distinguish each image from those that are related\nor similar. Each description is highly compositional and typically encompasses\nmultiple challenges. Through both quantitative and qualitative analyses, we\ndemonstrate that DOCCI serves as an effective training resource for\nimage-to-text generation -- a PaLI 5B model finetuned on DOCCI shows equal or\nsuperior results compared to highly-performant larger models like LLaVA-1.5 7B\nand InstructBLIP 7B. Furthermore, we show that DOCCI is a useful testbed for\ntext-to-image generation, highlighting the limitations of current text-to-image\nmodels in capturing long descriptions and fine details.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.19753v1",
    "published_date": "2024-04-30 17:56:24 UTC",
    "updated_date": "2024-04-30 17:56:24 UTC"
  },
  {
    "arxiv_id": "2404.19748v1",
    "title": "Quantifying Nematodes through Images: Datasets, Models, and Baselines of Deep Learning",
    "authors": [
      "Zhipeng Yuan",
      "Nasamu Musa",
      "Katarzyna Dybal",
      "Matthew Back",
      "Daniel Leybourne",
      "Po Yang"
    ],
    "abstract": "Every year, plant parasitic nematodes, one of the major groups of plant\npathogens, cause a significant loss of crops worldwide. To mitigate crop yield\nlosses caused by nematodes, an efficient nematode monitoring method is\nessential for plant and crop disease management. In other respects, efficient\nnematode detection contributes to medical research and drug discovery, as\nnematodes are model organisms. With the rapid development of computer\ntechnology, computer vision techniques provide a feasible solution for\nquantifying nematodes or nematode infections. In this paper, we survey and\ncategorise the studies and available datasets on nematode detection through\ndeep-learning models. To stimulate progress in related research, this survey\npresents the potential state-of-the-art object detection models, training\ntechniques, optimisation techniques, and evaluation metrics for deep learning\nbeginners. Moreover, seven state-of-the-art object detection models are\nvalidated on three public datasets and the AgriNema dataset for plant parasitic\nnematodes to construct a baseline for nematode detection.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "The 26th IEEE International Conference on Computational Science and\n  Engineering (CSE-2023)",
    "pdf_url": "http://arxiv.org/pdf/2404.19748v1",
    "published_date": "2024-04-30 17:52:31 UTC",
    "updated_date": "2024-04-30 17:52:31 UTC"
  },
  {
    "arxiv_id": "2404.19744v1",
    "title": "PrivComp-KG : Leveraging Knowledge Graph and Large Language Models for Privacy Policy Compliance Verification",
    "authors": [
      "Leon Garza",
      "Lavanya Elluri",
      "Anantaa Kotal",
      "Aritran Piplai",
      "Deepti Gupta",
      "Anupam Joshi"
    ],
    "abstract": "Data protection and privacy is becoming increasingly crucial in the digital\nera. Numerous companies depend on third-party vendors and service providers to\ncarry out critical functions within their operations, encompassing tasks such\nas data handling and storage. However, this reliance introduces potential\nvulnerabilities, as these vendors' security measures and practices may not\nalways align with the standards expected by regulatory bodies. Businesses are\nrequired, often under the penalty of law, to ensure compliance with the\nevolving regulatory rules. Interpreting and implementing these regulations pose\nchallenges due to their complexity. Regulatory documents are extensive,\ndemanding significant effort for interpretation, while vendor-drafted privacy\npolicies often lack the detail required for full legal compliance, leading to\nambiguity. To ensure a concise interpretation of the regulatory requirements\nand compliance of organizational privacy policy with said regulations, we\npropose a Large Language Model (LLM) and Semantic Web based approach for\nprivacy compliance. In this paper, we develop the novel Privacy Policy\nCompliance Verification Knowledge Graph, PrivComp-KG. It is designed to\nefficiently store and retrieve comprehensive information concerning privacy\npolicies, regulatory frameworks, and domain-specific knowledge pertaining to\nthe legal landscape of privacy. Using Retrieval Augmented Generation, we\nidentify the relevant sections in a privacy policy with corresponding\nregulatory rules. This information about individual privacy policies is\npopulated into the PrivComp-KG. Combining this with the domain context and\nrules, the PrivComp-KG can be queried to check for compliance with privacy\npolicies by each vendor against relevant policy regulations. We demonstrate the\nrelevance of the PrivComp-KG, by verifying compliance of privacy policy\ndocuments for various organizations.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.19744v1",
    "published_date": "2024-04-30 17:44:44 UTC",
    "updated_date": "2024-04-30 17:44:44 UTC"
  },
  {
    "arxiv_id": "2404.19733v3",
    "title": "Iterative Reasoning Preference Optimization",
    "authors": [
      "Richard Yuanzhe Pang",
      "Weizhe Yuan",
      "Kyunghyun Cho",
      "He He",
      "Sainbayar Sukhbaatar",
      "Jason Weston"
    ],
    "abstract": "Iterative preference optimization methods have recently been shown to perform\nwell for general instruction tuning tasks, but typically make little\nimprovement on reasoning tasks (Yuan et al., 2024, Chen et al., 2024). In this\nwork we develop an iterative approach that optimizes the preference between\ncompeting generated Chain-of-Thought (CoT) candidates by optimizing for winning\nvs. losing reasoning steps that lead to the correct answer. We train using a\nmodified DPO loss (Rafailov et al., 2023) with an additional negative\nlog-likelihood term, which we find to be crucial. We show reasoning improves\nacross repeated iterations of this scheme. While only relying on examples in\nthe training set, our approach results in increasing accuracy on GSM8K, MATH,\nand ARC-Challenge for Llama-2-70B-Chat, outperforming other Llama-2-based\nmodels not relying on additionally sourced datasets. For example, we see a\nlarge improvement from 55.6% to 81.6% on GSM8K and an accuracy of 88.7% with\nmajority voting out of 32 samples.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.19733v3",
    "published_date": "2024-04-30 17:28:05 UTC",
    "updated_date": "2024-06-26 01:28:35 UTC"
  },
  {
    "arxiv_id": "2404.19729v1",
    "title": "A Framework for Leveraging Human Computation Gaming to Enhance Knowledge Graphs for Accuracy Critical Generative AI Applications",
    "authors": [
      "Steph Buongiorno",
      "Corey Clark"
    ],
    "abstract": "External knowledge graphs (KGs) can be used to augment large language models\n(LLMs), while simultaneously providing an explainable knowledge base of facts\nthat can be inspected by a human. This approach may be particularly valuable in\ndomains where explainability is critical, like human trafficking data analysis.\nHowever, creating KGs can pose challenges. KGs parsed from documents may\ncomprise explicit connections (those directly stated by a document) but miss\nimplicit connections (those obvious to a human although not directly stated).\nTo address these challenges, this preliminary research introduces the GAME-KG\nframework, standing for \"Gaming for Augmenting Metadata and Enhancing Knowledge\nGraphs.\" GAME-KG is a federated approach to modifying explicit as well as\nimplicit connections in KGs by using crowdsourced feedback collected through\nvideo games. GAME-KG is shown through two demonstrations: a Unity test scenario\nfrom Dark Shadows, a video game that collects feedback on KGs parsed from US\nDepartment of Justice (DOJ) Press Releases on human trafficking, and a\nfollowing experiment where OpenAI's GPT-4 is prompted to answer questions based\non a modified and unmodified KG. Initial results suggest that GAME-KG can be an\neffective framework for enhancing KGs, while simultaneously providing an\nexplainable set of structured facts verified by humans.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.19729v1",
    "published_date": "2024-04-30 17:24:55 UTC",
    "updated_date": "2024-04-30 17:24:55 UTC"
  },
  {
    "arxiv_id": "2404.19725v3",
    "title": "Fairness Without Demographics in Human-Centered Federated Learning",
    "authors": [
      "Shaily Roy",
      "Harshit Sharma",
      "Asif Salekin"
    ],
    "abstract": "Federated learning (FL) enables collaborative model training while preserving\ndata privacy, making it suitable for decentralized human-centered AI\napplications. However, a significant research gap remains in ensuring fairness\nin these systems. Current fairness strategies in FL require knowledge of\nbias-creating/sensitive attributes, clashing with FL's privacy principles.\nMoreover, in human-centered datasets, sensitive attributes may remain latent.\nTo tackle these challenges, we present a novel bias mitigation approach\ninspired by \"Fairness without Demographics\" in machine learning. The presented\napproach achieves fairness without needing knowledge of sensitive attributes by\nminimizing the top eigenvalue of the Hessian matrix during training, ensuring\nequitable loss landscapes across FL participants. Notably, we introduce a novel\nFL aggregation scheme that promotes participating models based on error rates\nand loss landscape curvature attributes, fostering fairness across the FL\nsystem. This work represents the first approach to attaining \"Fairness without\nDemographics\" in human-centered FL. Through comprehensive evaluation, our\napproach demonstrates effectiveness in balancing fairness and efficacy across\nvarious real-world applications, FL setups, and scenarios involving single and\nmultiple bias-inducing factors, representing a significant advancement in\nhuman-centered FL.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.19725v3",
    "published_date": "2024-04-30 17:19:52 UTC",
    "updated_date": "2024-05-15 18:40:42 UTC"
  },
  {
    "arxiv_id": "2407.05915v1",
    "title": "Harnessing Federated Generative Learning for Green and Sustainable Internet of Things",
    "authors": [
      "Yuanhang Qi",
      "M. Shamim Hossain"
    ],
    "abstract": "The rapid proliferation of devices in the Internet of Things (IoT) has\nushered in a transformative era of data-driven connectivity across various\ndomains. However, this exponential growth has raised pressing concerns about\nenvironmental sustainability and data privacy. In response to these challenges,\nthis paper introduces One-shot Federated Learning (OSFL), an innovative\nparadigm that harmonizes sustainability and machine learning within IoT\necosystems. OSFL revolutionizes the traditional Federated Learning (FL)\nworkflow by condensing multiple iterative communication rounds into a single\noperation, thus significantly reducing energy consumption, communication\noverhead, and latency. This breakthrough is coupled with the strategic\nintegration of generative learning techniques, ensuring robust data privacy\nwhile promoting efficient knowledge sharing among IoT devices. By curtailing\nresource utilization, OSFL aligns seamlessly with the vision of green and\nsustainable IoT, effectively extending device lifespans and mitigating their\nenvironmental footprint. Our research underscores the transformative potential\nof OSFL, poised to reshape the landscape of IoT applications across domains\nsuch as energy-efficient smart cities and groundbreaking healthcare solutions.\nThis contribution marks a pivotal step towards a more responsible, sustainable,\nand technologically advanced future.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "This paper is a correction of the published version, in which we\n  corrected the grammatical errors between contexts and highlighted the\n  relationship with \"Federated generative learning with foundation models\"",
    "pdf_url": "http://arxiv.org/pdf/2407.05915v1",
    "published_date": "2024-04-30 17:15:26 UTC",
    "updated_date": "2024-04-30 17:15:26 UTC"
  },
  {
    "arxiv_id": "2404.19721v3",
    "title": "PANGeA: Procedural Artificial Narrative using Generative AI for Turn-Based Video Games",
    "authors": [
      "Steph Buongiorno",
      "Lawrence Jake Klinkert",
      "Tanishq Chawla",
      "Zixin Zhuang",
      "Corey Clark"
    ],
    "abstract": "This research introduces Procedural Artificial Narrative using Generative AI\n(PANGeA), a structured approach for leveraging large language models (LLMs),\nguided by a game designer's high-level criteria, to generate narrative content\nfor turn-based role-playing video games (RPGs). Distinct from prior\napplications of LLMs used for video game design, PANGeA innovates by not only\ngenerating game level data (which includes, but is not limited to, setting, key\nitems, and non-playable characters (NPCs)), but by also fostering dynamic,\nfree-form interactions between the player and the environment that align with\nthe procedural game narrative. The NPCs generated by PANGeA are\npersonality-biased and express traits from the Big 5 Personality Model in their\ngenerated responses. PANGeA addresses challenges behind ingesting free-form\ntext input, which can prompt LLM responses beyond the scope of the game\nnarrative. A novel validation system that uses the LLM's intelligence evaluates\ntext input and aligns generated responses with the unfolding narrative. Making\nthese interactions possible, PANGeA is supported by a server that hosts a\ncustom memory system that supplies context for augmenting generated responses\nthus aligning them with the procedural narrative. For its broad application,\nthe server has a REST interface enabling any game engine to integrate directly\nwith PANGeA, as well as an LLM interface adaptable with local or private LLMs.\nPANGeA's ability to foster dynamic narrative generation by aligning responses\nwith the procedural narrative is demonstrated through an empirical study and\nablation test of two versions of a demo game. These are, a custom,\nbrowser-based GPT and a Unity demo. As the results show, PANGeA holds potential\nto assist game designers in using LLMs to generate narrative-consistent content\neven when provided varied and unpredictable, free-form text input.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.19721v3",
    "published_date": "2024-04-30 17:11:54 UTC",
    "updated_date": "2024-07-09 23:45:27 UTC"
  },
  {
    "arxiv_id": "2404.19708v2",
    "title": "Harmonic LLMs are Trustworthy",
    "authors": [
      "Nicholas S. Kersting",
      "Mohammad Rahman",
      "Suchismitha Vedala",
      "Yang Wang"
    ],
    "abstract": "We introduce an intuitive method to test the robustness (stability and\nexplainability) of any black-box LLM in real-time via its local deviation from\nharmoniticity, denoted as $\\gamma$. To the best of our knowledge this is the\nfirst completely model-agnostic and unsupervised method of measuring the\nrobustness of any given response from an LLM, based upon the model itself\nconforming to a purely mathematical standard. To show general application and\nimmediacy of results, we measure $\\gamma$ in 10 popular LLMs (ChatGPT,\nClaude-2.1, Claude3.0, GPT-4, GPT-4o, Smaug-72B, Mixtral-8x7B, Llama2-7B,\nMistral-7B and MPT-7B) across thousands of queries in three objective domains:\nWebQA, ProgrammingQA, and TruthfulQA. Across all models and domains tested,\nhuman annotation confirms that $\\gamma \\to 0$ indicates trustworthiness, and\nconversely searching higher values of $\\gamma$ easily exposes examples of\nhallucination, a fact that enables efficient adversarial prompt generation\nthrough stochastic gradient ascent in $\\gamma$. The low-$\\gamma$ leaders among\nthe models in the respective domains are GPT-4o, GPT-4, and Smaug-72B,\nproviding evidence that mid-size open-source models can win out against large\ncommercial models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages, 2 figures, 16 tables; added Claude-3.0, GPT-4o, Mistral-7B,\n  Mixtral-8x7B, and more annotation for other models",
    "pdf_url": "http://arxiv.org/pdf/2404.19708v2",
    "published_date": "2024-04-30 17:00:32 UTC",
    "updated_date": "2024-07-25 16:16:46 UTC"
  },
  {
    "arxiv_id": "2406.15396v1",
    "title": "Feature Purified Transformer With Cross-level Feature Guiding Decoder For Multi-class OOD and Anomaly Deteciton",
    "authors": [
      "Jerry Chun-Wei Lin",
      "Pi-Wei Chen",
      "Chao-Chun Chen"
    ],
    "abstract": "Reconstruction networks are prevalently used in unsupervised anomaly and\nOut-of-Distribution (OOD) detection due to their independence from labeled\nanomaly data. However, in multi-class datasets, the effectiveness of anomaly\ndetection is often compromised by the models' generalized reconstruction\ncapabilities, which allow anomalies to blend within the expanded boundaries of\nnormality resulting from the added categories, thereby reducing detection\naccuracy. We introduce the FUTUREG framework, which incorporates two innovative\nmodules: the Feature Purification Module (FPM) and the CFG Decoder. The FPM\nconstrains the normality boundary within the latent space to effectively filter\nout anomalous features, while the CFG Decoder uses layer-wise encoder\nrepresentations to guide the reconstruction of filtered features, preserving\nfine-grained details. Together, these modules enhance the reconstruction error\nfor anomalies, ensuring high-quality reconstructions for normal samples. Our\nresults demonstrate that FUTUREG achieves state-of-the-art performance in\nmulti-class OOD settings and remains competitive in industrial anomaly\ndetection scenarios.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.15396v1",
    "published_date": "2024-04-30 16:45:51 UTC",
    "updated_date": "2024-04-30 16:45:51 UTC"
  },
  {
    "arxiv_id": "2404.19696v1",
    "title": "Naturally Supervised 3D Visual Grounding with Language-Regularized Concept Learners",
    "authors": [
      "Chun Feng",
      "Joy Hsu",
      "Weiyu Liu",
      "Jiajun Wu"
    ],
    "abstract": "3D visual grounding is a challenging task that often requires direct and\ndense supervision, notably the semantic label for each object in the scene. In\nthis paper, we instead study the naturally supervised setting that learns from\nonly 3D scene and QA pairs, where prior works underperform. We propose the\nLanguage-Regularized Concept Learner (LARC), which uses constraints from\nlanguage as regularization to significantly improve the accuracy of\nneuro-symbolic concept learners in the naturally supervised setting. Our\napproach is based on two core insights: the first is that language constraints\n(e.g., a word's relation to another) can serve as effective regularization for\nstructured representations in neuro-symbolic models; the second is that we can\nquery large language models to distill such constraints from language\nproperties. We show that LARC improves performance of prior works in naturally\nsupervised 3D visual grounding, and demonstrates a wide range of 3D visual\nreasoning capabilities-from zero-shot composition, to data efficiency and\ntransferability. Our method represents a promising step towards regularizing\nstructured visual reasoning frameworks with language-based priors, for learning\nin settings without dense supervision.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2024. The first two authors contributed equally",
    "pdf_url": "http://arxiv.org/pdf/2404.19696v1",
    "published_date": "2024-04-30 16:44:18 UTC",
    "updated_date": "2024-04-30 16:44:18 UTC"
  },
  {
    "arxiv_id": "2404.19665v1",
    "title": "ATOMMIC: An Advanced Toolbox for Multitask Medical Imaging Consistency to facilitate Artificial Intelligence applications from acquisition to analysis in Magnetic Resonance Imaging",
    "authors": [
      "Dimitrios Karkalousos",
      "Ivana Išgum",
      "Henk A. Marquering",
      "Matthan W. A. Caan"
    ],
    "abstract": "AI is revolutionizing MRI along the acquisition and processing chain.\nAdvanced AI frameworks have been developed to apply AI in various successive\ntasks, such as image reconstruction, quantitative parameter map estimation, and\nimage segmentation. Existing frameworks are often designed to perform tasks\nindependently or are focused on specific models or datasets, limiting\ngeneralization. We introduce ATOMMIC, an open-source toolbox that streamlines\nAI applications for accelerated MRI reconstruction and analysis. ATOMMIC\nimplements several tasks using DL networks and enables MultiTask Learning (MTL)\nto perform related tasks integrated, targeting generalization in the MRI\ndomain. We first review the current state of AI frameworks for MRI through a\ncomprehensive literature search and by parsing 12,479 GitHub repositories. We\nbenchmark 25 DL models on eight publicly available datasets to present distinct\napplications of ATOMMIC on accelerated MRI reconstruction, image segmentation,\nquantitative parameter map estimation, and joint accelerated MRI reconstruction\nand image segmentation utilizing MTL. Our findings demonstrate that ATOMMIC is\nthe only MTL framework with harmonized complex-valued and real-valued data\nsupport. Evaluations on single tasks show that physics-based models, which\nenforce data consistency by leveraging the physical properties of MRI,\noutperform other models in reconstructing highly accelerated acquisitions.\nPhysics-based models that produce high reconstruction quality can accurately\nestimate quantitative parameter maps. When high-performing reconstruction\nmodels are combined with robust segmentation networks utilizing MTL,\nperformance is improved in both tasks. ATOMMIC facilitates MRI reconstruction\nand analysis by standardizing workflows, enhancing data interoperability,\nintegrating unique features like MTL, and effectively benchmarking DL models.",
    "categories": [
      "physics.med-ph",
      "cs.AI",
      "cs.SE",
      "math-ph",
      "math.MP"
    ],
    "primary_category": "physics.med-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.19665v1",
    "published_date": "2024-04-30 16:00:21 UTC",
    "updated_date": "2024-04-30 16:00:21 UTC"
  },
  {
    "arxiv_id": "2404.19654v1",
    "title": "Masked Multi-Query Slot Attention for Unsupervised Object Discovery",
    "authors": [
      "Rishav Pramanik",
      "José-Fabian Villa-Vásquez",
      "Marco Pedersoli"
    ],
    "abstract": "Unsupervised object discovery is becoming an essential line of research for\ntackling recognition problems that require decomposing an image into entities,\nsuch as semantic segmentation and object detection. Recently, object-centric\nmethods that leverage self-supervision have gained popularity, due to their\nsimplicity and adaptability to different settings and conditions. However,\nthose methods do not exploit effective techniques already employed in modern\nself-supervised approaches. In this work, we consider an object-centric\napproach in which DINO ViT features are reconstructed via a set of queried\nrepresentations called slots. Based on that, we propose a masking scheme on\ninput features that selectively disregards the background regions, inducing our\nmodel to focus more on salient objects during the reconstruction phase.\nMoreover, we extend the slot attention to a multi-query approach, allowing the\nmodel to learn multiple sets of slots, producing more stable masks. During\ntraining, these multiple sets of slots are learned independently while, at test\ntime, these sets are merged through Hungarian matching to obtain the final\nslots. Our experimental results and ablations on the PASCAL-VOC 2012 dataset\nshow the importance of each component and highlight how their combination\nconsistently improves object localization. Our source code is available at:\nhttps://github.com/rishavpramanik/maskedmultiqueryslot",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Paper accepted for presentation at IJCNN 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.19654v1",
    "published_date": "2024-04-30 15:51:05 UTC",
    "updated_date": "2024-04-30 15:51:05 UTC"
  },
  {
    "arxiv_id": "2404.19652v4",
    "title": "VimTS: A Unified Video and Image Text Spotter for Enhancing the Cross-domain Generalization",
    "authors": [
      "Yuliang Liu",
      "Mingxin Huang",
      "Hao Yan",
      "Linger Deng",
      "Weijia Wu",
      "Hao Lu",
      "Chunhua Shen",
      "Lianwen Jin",
      "Xiang Bai"
    ],
    "abstract": "Text spotting, a task involving the extraction of textual information from\nimage or video sequences, faces challenges in cross-domain adaption, such as\nimage-to-image and image-to-video generalization. In this paper, we introduce a\nnew method, termed VimTS, which enhances the generalization ability of the\nmodel by achieving better synergy among different tasks. Typically, we propose\na Prompt Queries Generation Module and a Tasks-aware Adapter to effectively\nconvert the original single-task model into a multi-task model suitable for\nboth image and video scenarios with minimal additional parameters. The Prompt\nQueries Generation Module facilitates explicit interaction between different\ntasks, while the Tasks-aware Adapter helps the model dynamically learn suitable\nfeatures for each task. Additionally, to further enable the model to learn\ntemporal information at a lower cost, we propose a synthetic video text dataset\n(VTD-368k) by leveraging the Content Deformation Fields (CoDeF) algorithm.\nNotably, our method outperforms the state-of-the-art method by an average of\n2.6% in six cross-domain benchmarks such as TT-to-IC15, CTW1500-to-TT, and\nTT-to-CTW1500. For video-level cross-domain adaption, our method even surpasses\nthe previous end-to-end video spotting method in ICDAR2015 video and DSText v2\nby an average of 5.5% on the MOTA metric, using only image-level data. We\nfurther demonstrate that existing Large Multimodal Models exhibit limitations\nin generating cross-domain scene text spotting, in contrast to our VimTS model\nwhich requires significantly fewer parameters and data. The code and datasets\nwill be made available at the https://VimTextSpotter.github.io.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.19652v4",
    "published_date": "2024-04-30 15:49:03 UTC",
    "updated_date": "2024-12-05 02:13:51 UTC"
  },
  {
    "arxiv_id": "2404.19651v1",
    "title": "Provably Robust Conformal Prediction with Improved Efficiency",
    "authors": [
      "Ge Yan",
      "Yaniv Romano",
      "Tsui-Wei Weng"
    ],
    "abstract": "Conformal prediction is a powerful tool to generate uncertainty sets with\nguaranteed coverage using any predictive model, under the assumption that the\ntraining and test data are i.i.d.. Recently, it has been shown that adversarial\nexamples are able to manipulate conformal methods to construct prediction sets\nwith invalid coverage rates, as the i.i.d. assumption is violated. To address\nthis issue, a recent work, Randomized Smoothed Conformal Prediction (RSCP), was\nfirst proposed to certify the robustness of conformal prediction methods to\nadversarial noise. However, RSCP has two major limitations: (i) its robustness\nguarantee is flawed when used in practice and (ii) it tends to produce large\nuncertainty sets. To address these limitations, we first propose a novel\nframework called RSCP+ to provide provable robustness guarantee in evaluation,\nwhich fixes the issues in the original RSCP method. Next, we propose two novel\nmethods, Post-Training Transformation (PTT) and Robust Conformal Training\n(RCT), to effectively reduce prediction set size with little computation\noverhead. Experimental results in CIFAR10, CIFAR100, and ImageNet suggest the\nbaseline method only yields trivial predictions including full label set, while\nour methods could boost the efficiency by up to $4.36\\times$, $5.46\\times$, and\n$16.9\\times$ respectively and provide practical robustness guarantee. Our codes\nare available at\nhttps://github.com/Trustworthy-ML-Lab/Provably-Robust-Conformal-Prediction.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.19651v1",
    "published_date": "2024-04-30 15:49:01 UTC",
    "updated_date": "2024-04-30 15:49:01 UTC"
  },
  {
    "arxiv_id": "2404.19573v1",
    "title": "War Elephants: Rethinking Combat AI and Human Oversight",
    "authors": [
      "Philip Feldman",
      "Aaron Dant",
      "Harry Dreany"
    ],
    "abstract": "This paper explores the changes that pervasive AI is having on the nature of\ncombat. We look beyond the substitution of AI for experts to an approach where\ncomplementary human and machine abilities are blended. Using historical and\nmodern examples, we show how autonomous weapons systems can be effectively\nmanaged by teams of human \"AI Operators\" combined with AI/ML \"Proxy Operators.\"\nBy basing our approach on the principles of complementation, we provide for a\nflexible and dynamic approach to managing lethal autonomous systems. We\nconclude by presenting a path to achieving an integrated vision of\nmachine-speed combat where the battlefield AI is operated by AI Operators that\nwatch for patterns of behavior within battlefield to assess the performance of\nlethal autonomous systems. This approach enables the development of combat\nsystems that are likely to be more ethical, operate at machine speed, and are\ncapable of responding to a broader range of dynamic battlefield conditions than\nany purely autonomous AI system could support.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "J.4; H.5.2; I.2.8"
    ],
    "primary_category": "cs.CY",
    "comment": "15 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.19573v1",
    "published_date": "2024-04-30 14:07:57 UTC",
    "updated_date": "2024-04-30 14:07:57 UTC"
  },
  {
    "arxiv_id": "2407.09982v1",
    "title": "Artificial intelligence and machine learning applications for cultured meat",
    "authors": [
      "Michael E. Todhunter",
      "Sheikh Jubair",
      "Ruchika Verma",
      "Rikard Saqe",
      "Kevin Shen",
      "Breanna Duffy"
    ],
    "abstract": "Cultured meat has the potential to provide a complementary meat industry with\nreduced environmental, ethical, and health impacts. However, major\ntechnological challenges remain which require time- and resource-intensive\nresearch and development efforts. Machine learning has the potential to\naccelerate cultured meat technology by streamlining experiments, predicting\noptimal results, and reducing experimentation time and resources. However, the\nuse of machine learning in cultured meat is in its infancy. This review covers\nthe work available to date on the use of machine learning in cultured meat and\nexplores future possibilities. We address four major areas of cultured meat\nresearch and development: establishing cell lines, cell culture media design,\nmicroscopy and image analysis, and bioprocessing and food processing\noptimization. This review aims to provide the foundation necessary for both\ncultured meat and machine learning scientists to identify research\nopportunities at the intersection between cultured meat and machine learning.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "23 pages (43 pages with references), 4 figures. The first two listed\n  authors share first authorship; they and the last listed author contributed\n  equally to this work",
    "pdf_url": "http://arxiv.org/pdf/2407.09982v1",
    "published_date": "2024-04-30 13:35:18 UTC",
    "updated_date": "2024-04-30 13:35:18 UTC"
  },
  {
    "arxiv_id": "2404.19543v1",
    "title": "RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing",
    "authors": [
      "Yucheng Hu",
      "Yuxing Lu"
    ],
    "abstract": "Large Language Models (LLMs) have catalyzed significant advancements in\nNatural Language Processing (NLP), yet they encounter challenges such as\nhallucination and the need for domain-specific knowledge. To mitigate these,\nrecent methodologies have integrated information retrieved from external\nresources with LLMs, substantially enhancing their performance across NLP\ntasks. This survey paper addresses the absence of a comprehensive overview on\nRetrieval-Augmented Language Models (RALMs), both Retrieval-Augmented\nGeneration (RAG) and Retrieval-Augmented Understanding (RAU), providing an\nin-depth examination of their paradigm, evolution, taxonomy, and applications.\nThe paper discusses the essential components of RALMs, including Retrievers,\nLanguage Models, and Augmentations, and how their interactions lead to diverse\nmodel structures and applications. RALMs demonstrate utility in a spectrum of\ntasks, from translation and dialogue systems to knowledge-intensive\napplications. The survey includes several evaluation methods of RALMs,\nemphasizing the importance of robustness, accuracy, and relevance in their\nassessment. It also acknowledges the limitations of RALMs, particularly in\nretrieval quality and computational efficiency, offering directions for future\nresearch. In conclusion, this survey aims to offer a structured insight into\nRALMs, their potential, and the avenues for their future development in NLP.\nThe paper is supplemented with a Github Repository containing the surveyed\nworks and resources for further study:\nhttps://github.com/2471023025/RALM_Survey.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "30 pages, 7 figures. Draft version 1",
    "pdf_url": "http://arxiv.org/pdf/2404.19543v1",
    "published_date": "2024-04-30 13:14:51 UTC",
    "updated_date": "2024-04-30 13:14:51 UTC"
  },
  {
    "arxiv_id": "2404.19541v1",
    "title": "Ultra Inertial Poser: Scalable Motion Capture and Tracking from Sparse Inertial Sensors and Ultra-Wideband Ranging",
    "authors": [
      "Rayan Armani",
      "Changlin Qian",
      "Jiaxi Jiang",
      "Christian Holz"
    ],
    "abstract": "While camera-based capture systems remain the gold standard for recording\nhuman motion, learning-based tracking systems based on sparse wearable sensors\nare gaining popularity. Most commonly, they use inertial sensors, whose\npropensity for drift and jitter have so far limited tracking accuracy. In this\npaper, we propose Ultra Inertial Poser, a novel 3D full body pose estimation\nmethod that constrains drift and jitter in inertial tracking via inter-sensor\ndistances. We estimate these distances across sparse sensor setups using a\nlightweight embedded tracker that augments inexpensive off-the-shelf 6D\ninertial measurement units with ultra-wideband radio-based\nranging$-$dynamically and without the need for stationary reference anchors.\nOur method then fuses these inter-sensor distances with the 3D states estimated\nfrom each sensor Our graph-based machine learning model processes the 3D states\nand distances to estimate a person's 3D full body pose and translation. To\ntrain our model, we synthesize inertial measurements and distance estimates\nfrom the motion capture database AMASS. For evaluation, we contribute a novel\nmotion dataset of 10 participants who performed 25 motion types, captured by 6\nwearable IMU+UWB trackers and an optical motion capture system, totaling 200\nminutes of synchronized sensor data (UIP-DB). Our extensive experiments show\nstate-of-the-art performance for our method over PIP and TIP, reducing position\nerror from $13.62$ to $10.65cm$ ($22\\%$ better) and lowering jitter from $1.56$\nto $0.055km/s^3$ (a reduction of $97\\%$).",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "eess.SP",
      "68T07, 68T45, 68U01",
      "I.2; I.3; I.4; I.5"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by SIGGRAPH 2024, Code:\n  https://github.com/eth-siplab/UltraInertialPoser",
    "pdf_url": "http://arxiv.org/pdf/2404.19541v1",
    "published_date": "2024-04-30 13:14:11 UTC",
    "updated_date": "2024-04-30 13:14:11 UTC"
  },
  {
    "arxiv_id": "2404.19518v1",
    "title": "MGCBS: An Optimal and Efficient Algorithm for Solving Multi-Goal Multi-Agent Path Finding Problem",
    "authors": [
      "Mingkai Tang",
      "Yuanhang Li",
      "Hongji Liu",
      "Yingbing Chen",
      "Ming Liu",
      "Lujia Wang"
    ],
    "abstract": "With the expansion of the scale of robotics applications, the multi-goal\nmulti-agent pathfinding (MG-MAPF) problem began to gain widespread attention.\nThis problem requires each agent to visit pre-assigned multiple goal points at\nleast once without conflict. Some previous methods have been proposed to solve\nthe MG-MAPF problem based on Decoupling the goal Vertex visiting order search\nand the Single-agent pathfinding (DVS). However, this paper demonstrates that\nthe methods based on DVS cannot always obtain the optimal solution. To obtain\nthe optimal result, we propose the Multi-Goal Conflict-Based Search (MGCBS),\nwhich is based on Decoupling the goal Safe interval visiting order search and\nthe Single-agent pathfinding (DSS). Additionally, we present the\nTime-Interval-Space Forest (TIS Forest) to enhance the efficiency of MGCBS by\nmaintaining the shortest paths from any start point at any start time step to\neach safe interval at the goal points. The experiment demonstrates that our\nmethod can consistently obtain optimal results and execute up to 7 times faster\nthan the state-of-the-art method in our evaluation.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.MA",
    "comment": "to be published in IJCAI2024",
    "pdf_url": "http://arxiv.org/pdf/2404.19518v1",
    "published_date": "2024-04-30 12:49:54 UTC",
    "updated_date": "2024-04-30 12:49:54 UTC"
  },
  {
    "arxiv_id": "2404.19500v2",
    "title": "Towards Real-world Video Face Restoration: A New Benchmark",
    "authors": [
      "Ziyan Chen",
      "Jingwen He",
      "Xinqi Lin",
      "Yu Qiao",
      "Chao Dong"
    ],
    "abstract": "Blind face restoration (BFR) on images has significantly progressed over the\nlast several years, while real-world video face restoration (VFR), which is\nmore challenging for more complex face motions such as moving gaze directions\nand facial orientations involved, remains unsolved. Typical BFR methods are\nevaluated on privately synthesized datasets or self-collected real-world\nlow-quality face images, which are limited in their coverage of real-world\nvideo frames. In this work, we introduced new real-world datasets named FOS\nwith a taxonomy of \"Full, Occluded, and Side\" faces from mainly video frames to\nstudy the applicability of current methods on videos. Compared with existing\ntest datasets, FOS datasets cover more diverse degradations and involve face\nsamples from more complex scenarios, which helps to revisit current face\nrestoration approaches more comprehensively. Given the established datasets, we\nbenchmarked both the state-of-the-art BFR methods and the video super\nresolution (VSR) methods to comprehensively study current approaches,\nidentifying their potential and limitations in VFR tasks. In addition, we\nstudied the effectiveness of the commonly used image quality assessment (IQA)\nmetrics and face IQA (FIQA) metrics by leveraging a subjective user study. With\nextensive experimental results and detailed analysis provided, we gained\ninsights from the successes and failures of both current BFR and VSR methods.\nThese results also pose challenges to current face restoration approaches,\nwhich we hope stimulate future advances in VFR research.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://ziyannchen.github.io/projects/VFRxBenchmark/",
    "pdf_url": "http://arxiv.org/pdf/2404.19500v2",
    "published_date": "2024-04-30 12:37:01 UTC",
    "updated_date": "2024-05-04 07:09:48 UTC"
  },
  {
    "arxiv_id": "2404.19485v2",
    "title": "IID Relaxation by Logical Expressivity: A Research Agenda for Fitting Logics to Neurosymbolic Requirements",
    "authors": [
      "Maarten C. Stol",
      "Alessandra Mileo"
    ],
    "abstract": "Neurosymbolic background knowledge and the expressivity required of its logic\ncan break Machine Learning assumptions about data Independence and Identical\nDistribution. In this position paper we propose to analyze IID relaxation in a\nhierarchy of logics that fit different use case requirements. We discuss the\nbenefits of exploiting known data dependencies and distribution constraints for\nNeurosymbolic use cases and argue that the expressivity required for this\nknowledge has implications for the design of underlying ML routines. This opens\na new research agenda with general questions about Neurosymbolic background\nknowledge and the expressivity required of its logic.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "13 pages, 2 figures, submitted to NeSy 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.19485v2",
    "published_date": "2024-04-30 12:09:53 UTC",
    "updated_date": "2024-07-01 12:44:38 UTC"
  },
  {
    "arxiv_id": "2404.19484v2",
    "title": "More Compute Is What You Need",
    "authors": [
      "Zhen Guo"
    ],
    "abstract": "Large language model pre-training has become increasingly expensive, with\nmost practitioners relying on scaling laws to allocate compute budgets for\nmodel size and training tokens, commonly referred to as Compute-Optimal or\nChinchilla Optimal. In this paper, we hypothesize a new scaling law that\nsuggests model performance depends mostly on the amount of compute spent for\ntransformer-based models, independent of the specific allocation to model size\nand dataset size. Using this unified scaling law, we predict that (a) for\ninference efficiency, training should prioritize smaller model sizes and larger\ntraining datasets, and (b) assuming the exhaustion of available web datasets,\nscaling the model size might be the only way to further improve model\nperformance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.19484v2",
    "published_date": "2024-04-30 12:05:48 UTC",
    "updated_date": "2024-05-02 01:58:15 UTC"
  },
  {
    "arxiv_id": "2404.19456v2",
    "title": "A Survey of Imitation Learning Methods, Environments and Metrics",
    "authors": [
      "Nathan Gavenski",
      "Felipe Meneguzzi",
      "Michael Luck",
      "Odinaldo Rodrigues"
    ],
    "abstract": "Imitation learning is an approach in which an agent learns how to execute a\ntask by trying to mimic how one or more teachers perform it. This learning\napproach offers a compromise between the time it takes to learn a new task and\nthe effort needed to collect teacher samples for the agent. It achieves this by\nbalancing learning from the teacher, who has some information on how to perform\nthe task, and deviating from their examples when necessary, such as states not\npresent in the teacher samples. Consequently, the field of imitation learning\nhas received much attention from researchers in recent years, resulting in many\nnew methods and applications. However, with this increase in published work and\npast surveys focusing mainly on methodology, a lack of standardisation became\nmore prominent in the field. This non-standardisation is evident in the use of\nenvironments, which appear in no more than two works, and evaluation processes,\nsuch as qualitative analysis, that have become rare in current literature. In\nthis survey, we systematically review current imitation learning literature and\npresent our findings by (i) classifying imitation learning techniques,\nenvironments and metrics by introducing novel taxonomies; (ii) reflecting on\nmain problems from the literature; and (iii) presenting challenges and future\ndirections for researchers.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.19456v2",
    "published_date": "2024-04-30 11:13:23 UTC",
    "updated_date": "2024-07-30 08:58:51 UTC"
  },
  {
    "arxiv_id": "2404.19454v2",
    "title": "Augmented neural forms with parametric boundary-matching operators for solving ordinary differential equations",
    "authors": [
      "Adam D. Kypriadis",
      "Isaac E. Lagaris",
      "Aristidis Likas",
      "Konstantinos E. Parsopoulos"
    ],
    "abstract": "Approximating solutions of ordinary and partial differential equations\nconstitutes a significant challenge. Based on functional expressions that\ninherently depend on neural networks, neural forms are specifically designed to\nprecisely satisfy the prescribed initial or boundary conditions of the problem,\nwhile providing the approximate solutions in closed form. Departing from the\nimportant class of ordinary differential equations, the present work aims to\nrefine and validate the neural forms methodology, paving the ground for further\ndevelopments in more challenging fields. The main contributions are as follows.\nFirst, it introduces a formalism for systematically crafting proper neural\nforms with adaptable boundary matches that are amenable to optimization.\nSecond, it describes a novel technique for converting problems with Neumann or\nRobin conditions into equivalent problems with parametric Dirichlet conditions.\nThird, it outlines a method for determining an upper bound on the absolute\ndeviation from the exact solution. The proposed augmented neural forms approach\nwas tested on a set of diverse problems, encompassing first- and second-order\nordinary differential equations, as well as first-order systems. Stiff\ndifferential equations have been considered as well. The resulting solutions\nwere subjected to assessment against existing exact solutions, solutions\nderived through the common penalized neural method, and solutions obtained via\ncontemporary numerical analysis methods. The reported results demonstrate that\nthe augmented neural forms not only satisfy the boundary and initial conditions\nexactly, but also provide closed-form solutions that facilitate high-quality\ninterpolation and controllable overall precision. These attributes are\nessential for expanding the application field of neural forms to more\nchallenging problems that are described by partial differential equations.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.19454v2",
    "published_date": "2024-04-30 11:10:34 UTC",
    "updated_date": "2024-09-26 10:34:40 UTC"
  },
  {
    "arxiv_id": "2405.00076v2",
    "title": "Towards trustable SHAP scores",
    "authors": [
      "Olivier Letoffe",
      "Xuanxiang Huang",
      "Joao Marques-Silva"
    ],
    "abstract": "SHAP scores represent the proposed use of the well-known Shapley values in\neXplainable Artificial Intelligence (XAI). Recent work has shown that the exact\ncomputation of SHAP scores can produce unsatisfactory results. Concretely, for\nsome ML models, SHAP scores will mislead with respect to relative feature\ninfluence. To address these limitations, recently proposed alternatives exploit\ndifferent axiomatic aggregations, all of which are defined in terms of\nabductive explanations. However, the proposed axiomatic aggregations are not\nShapley values. This paper investigates how SHAP scores can be modified so as\nto extend axiomatic aggregations to the case of Shapley values in XAI. More\nimportantly, the proposed new definition of SHAP scores avoids all the known\ncases where unsatisfactory results have been identified. The paper also\ncharacterizes the complexity of computing the novel definition of SHAP scores,\nhighlighting families of classifiers for which computing these scores is\ntractable. Furthermore, the paper proposes modifications to the existing\nimplementations of SHAP scores. These modifications eliminate some of the known\nlimitations of SHAP scores, and have negligible impact in terms of performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.00076v2",
    "published_date": "2024-04-30 10:39:20 UTC",
    "updated_date": "2024-12-19 02:29:41 UTC"
  },
  {
    "arxiv_id": "2404.19432v1",
    "title": "Can Large Language Models put 2 and 2 together? Probing for Entailed Arithmetical Relationships",
    "authors": [
      "D. Panas",
      "S. Seth",
      "V. Belle"
    ],
    "abstract": "Two major areas of interest in the era of Large Language Models regard\nquestions of what do LLMs know, and if and how they may be able to reason (or\nrather, approximately reason). Since to date these lines of work progressed\nlargely in parallel (with notable exceptions), we are interested in\ninvestigating the intersection: probing for reasoning about the implicitly-held\nknowledge. Suspecting the performance to be lacking in this area, we use a very\nsimple set-up of comparisons between cardinalities associated with elements of\nvarious subjects (e.g. the number of legs a bird has versus the number of\nwheels on a tricycle). We empirically demonstrate that although LLMs make\nsteady progress in knowledge acquisition and (pseudo)reasoning with each new\nGPT release, their capabilities are limited to statistical inference only. It\nis difficult to argue that pure statistical learning can cope with the\ncombinatorial explosion inherent in many commonsense reasoning tasks,\nespecially once arithmetical notions are involved. Further, we argue that\nbigger is not always better and chasing purely statistical improvements is\nflawed at the core, since it only exacerbates the dangerous conflation of the\nproduction of correct answers with genuine reasoning ability.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.19432v1",
    "published_date": "2024-04-30 10:28:04 UTC",
    "updated_date": "2024-04-30 10:28:04 UTC"
  },
  {
    "arxiv_id": "2404.19403v1",
    "title": "Transformer-Enhanced Motion Planner: Attention-Guided Sampling for State-Specific Decision Making",
    "authors": [
      "Lei Zhuang",
      "Jingdong Zhao",
      "Yuntao Li",
      "Zichun Xu",
      "Liangliang Zhao",
      "Hong Liu"
    ],
    "abstract": "Sampling-based motion planning (SBMP) algorithms are renowned for their\nrobust global search capabilities. However, the inherent randomness in their\nsampling mechanisms often result in inconsistent path quality and limited\nsearch efficiency. In response to these challenges, this work proposes a novel\ndeep learning-based motion planning framework, named Transformer-Enhanced\nMotion Planner (TEMP), which synergizes an Environmental Information Semantic\nEncoder (EISE) with a Motion Planning Transformer (MPT). EISE converts\nenvironmental data into semantic environmental information (SEI), providing MPT\nwith an enriched environmental comprehension. MPT leverages an attention\nmechanism to dynamically recalibrate its focus on SEI, task objectives, and\nhistorical planning data, refining the sampling node generation. To demonstrate\nthe capabilities of TEMP, we train our model using a dataset comprised of\nplanning results produced by the RRT*. EISE and MPT are collaboratively\ntrained, enabling EISE to autonomously learn and extract patterns from\nenvironmental data, thereby forming semantic representations that MPT could\nmore effectively interpret and utilize for motion planning. Subsequently, we\nconducted a systematic evaluation of TEMP's efficacy across diverse task\ndimensions, which demonstrates that TEMP achieves exceptional performance\nmetrics and a heightened degree of generalizability compared to\nstate-of-the-art SBMPs.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "This work has been submitted to the IEEE for possible publication",
    "pdf_url": "http://arxiv.org/pdf/2404.19403v1",
    "published_date": "2024-04-30 09:48:11 UTC",
    "updated_date": "2024-04-30 09:48:11 UTC"
  },
  {
    "arxiv_id": "2404.19384v1",
    "title": "Pseudo Label Refinery for Unsupervised Domain Adaptation on Cross-dataset 3D Object Detection",
    "authors": [
      "Zhanwei Zhang",
      "Minghao Chen",
      "Shuai Xiao",
      "Liang Peng",
      "Hengjia Li",
      "Binbin Lin",
      "Ping Li",
      "Wenxiao Wang",
      "Boxi Wu",
      "Deng Cai"
    ],
    "abstract": "Recent self-training techniques have shown notable improvements in\nunsupervised domain adaptation for 3D object detection (3D UDA). These\ntechniques typically select pseudo labels, i.e., 3D boxes, to supervise models\nfor the target domain. However, this selection process inevitably introduces\nunreliable 3D boxes, in which 3D points cannot be definitively assigned as\nforeground or background. Previous techniques mitigate this by reweighting\nthese boxes as pseudo labels, but these boxes can still poison the training\nprocess. To resolve this problem, in this paper, we propose a novel pseudo\nlabel refinery framework. Specifically, in the selection process, to improve\nthe reliability of pseudo boxes, we propose a complementary augmentation\nstrategy. This strategy involves either removing all points within an\nunreliable box or replacing it with a high-confidence box. Moreover, the point\nnumbers of instances in high-beam datasets are considerably higher than those\nin low-beam datasets, also degrading the quality of pseudo labels during the\ntraining process. We alleviate this issue by generating additional proposals\nand aligning RoI features across different domains. Experimental results\ndemonstrate that our method effectively enhances the quality of pseudo labels\nand consistently surpasses the state-of-the-art methods on six autonomous\ndriving benchmarks. Code will be available at\nhttps://github.com/Zhanwei-Z/PERE.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by CVPR2024",
    "pdf_url": "http://arxiv.org/pdf/2404.19384v1",
    "published_date": "2024-04-30 09:20:35 UTC",
    "updated_date": "2024-04-30 09:20:35 UTC"
  },
  {
    "arxiv_id": "2407.10244v1",
    "title": "Reimagining AI in Social Work: Practitioner Perspectives on Incorporating Technology in their Practice",
    "authors": [
      "Katie Wassal",
      "Carolyn Ashurst",
      "Jiri Hron",
      "Miri Zilka"
    ],
    "abstract": "There has been a surge in the number and type of AI tools being tested and\ndeployed within both national and local government in the UK, including within\nthe social care sector. Given the many ongoing and planned future developments,\nthe time is ripe to review and reflect on the state of AI in social care. We do\nso by conducting semi-structured interviews with UK-based social work\nprofessionals about their experiences and opinions of past and current AI\nsystems. Our aim is to understand what systems would practitioners like to see\ndeveloped and how. We find that all our interviewees had overwhelmingly\nnegative past experiences of technology in social care, unanimous aversion to\nalgorithmic decision systems in particular, but also strong interest in AI\napplications that could allow them to spend less time on administrative tasks.\nIn response to our findings, we offer a series of concrete recommendations,\nwhich include commitment to participatory design, as well as the necessity of\nregaining practitioner trust.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "Under review",
    "pdf_url": "http://arxiv.org/pdf/2407.10244v1",
    "published_date": "2024-04-30 09:07:04 UTC",
    "updated_date": "2024-04-30 09:07:04 UTC"
  },
  {
    "arxiv_id": "2404.19370v1",
    "title": "Numeric Reward Machines",
    "authors": [
      "Kristina Levina",
      "Nikolaos Pappas",
      "Athanasios Karapantelakis",
      "Aneta Vulgarakis Feljan",
      "Jendrik Seipp"
    ],
    "abstract": "Reward machines inform reinforcement learning agents about the reward\nstructure of the environment and often drastically speed up the learning\nprocess. However, reward machines only accept Boolean features such as\nrobot-reached-gold. Consequently, many inherently numeric tasks cannot profit\nfrom the guidance offered by reward machines. To address this gap, we aim to\nextend reward machines with numeric features such as distance-to-gold. For\nthis, we present two types of reward machines: numeric-Boolean and numeric. In\na numeric-Boolean reward machine, distance-to-gold is emulated by two Boolean\nfeatures distance-to-gold-decreased and robot-reached-gold. In a numeric reward\nmachine, distance-to-gold is used directly alongside the Boolean feature\nrobot-reached-gold. We compare our new approaches to a baseline reward machine\nin the Craft domain, where the numeric feature is the agent-to-target distance.\nWe use cross-product Q-learning, Q-learning with counter-factual experiences,\nand the options framework for learning. Our experimental results show that our\nnew approaches significantly outperform the baseline approach. Extending reward\nmachines with numeric features opens up new possibilities of using reward\nmachines in inherently numeric tasks.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "ICAPS 2024; Workshop on Bridging the Gap Between AI Planning and\n  Reinforcement Learning",
    "pdf_url": "http://arxiv.org/pdf/2404.19370v1",
    "published_date": "2024-04-30 08:58:47 UTC",
    "updated_date": "2024-04-30 08:58:47 UTC"
  },
  {
    "arxiv_id": "2404.19361v1",
    "title": "A Negotiator's Backup Plan: Optimal Concessions with a Reservation Value",
    "authors": [
      "Tamara C. P. Florijn",
      "Pinar Yolum",
      "Tim Baarslag"
    ],
    "abstract": "Automated negotiation is a well-known mechanism for autonomous agents to\nreach agreements. To realize beneficial agreements quickly, it is key to employ\na good bidding strategy. When a negotiating agent has a good back-up plan,\ni.e., a high reservation value, failing to reach an agreement is not\nnecessarily disadvantageous. Thus, the agent can adopt a risk-seeking strategy,\naiming for outcomes with a higher utilities.\n  Accordingly, this paper develops an optimal bidding strategy called\nMIA-RVelous for bilateral negotiations with private reservation values. The\nproposed greedy algorithm finds the optimal bid sequence given the agent's\nbeliefs about the opponent in $O(n^2D)$ time, with $D$ the maximum number of\nrounds and $n$ the number of outcomes. The results obtained here can pave the\nway to realizing effective concurrent negotiations, given that concurrent\nnegotiations can serve as a (probabilistic) backup plan.",
    "categories": [
      "cs.GT",
      "cs.AI"
    ],
    "primary_category": "cs.GT",
    "comment": "Accepted at AAMAS 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.19361v1",
    "published_date": "2024-04-30 08:45:18 UTC",
    "updated_date": "2024-04-30 08:45:18 UTC"
  },
  {
    "arxiv_id": "2404.19359v1",
    "title": "Evaluating Lexicon Incorporation for Depression Symptom Estimation",
    "authors": [
      "Kirill Milintsevich",
      "Gaël Dias",
      "Kairit Sirts"
    ],
    "abstract": "This paper explores the impact of incorporating sentiment, emotion, and\ndomain-specific lexicons into a transformer-based model for depression symptom\nestimation. Lexicon information is added by marking the words in the input\ntranscripts of patient-therapist conversations as well as in social media\nposts. Overall results show that the introduction of external knowledge within\npre-trained language models can be beneficial for prediction performance, while\ndifferent lexicons show distinct behaviours depending on the targeted task.\nAdditionally, new state-of-the-art results are obtained for the estimation of\ndepression level over patient-therapist interviews.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to Clinical NLP workshop at NAACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.19359v1",
    "published_date": "2024-04-30 08:41:06 UTC",
    "updated_date": "2024-04-30 08:41:06 UTC"
  },
  {
    "arxiv_id": "2404.19349v1",
    "title": "Human-AI Interaction in Industrial Robotics: Design and Empirical Evaluation of a User Interface for Explainable AI-Based Robot Program Optimization",
    "authors": [
      "Benjamin Alt",
      "Johannes Zahn",
      "Claudius Kienle",
      "Julia Dvorak",
      "Marvin May",
      "Darko Katic",
      "Rainer Jäkel",
      "Tobias Kopp",
      "Michael Beetz",
      "Gisela Lanza"
    ],
    "abstract": "While recent advances in deep learning have demonstrated its transformative\npotential, its adoption for real-world manufacturing applications remains\nlimited. We present an Explanation User Interface (XUI) for a state-of-the-art\ndeep learning-based robot program optimizer which provides both naive and\nexpert users with different user experiences depending on their skill level, as\nwell as Explainable AI (XAI) features to facilitate the application of deep\nlearning methods in real-world applications. To evaluate the impact of the XUI\non task performance, user satisfaction and cognitive load, we present the\nresults of a preliminary user survey and propose a study design for a\nlarge-scale follow-up study.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CE",
      "cs.HC",
      "cs.LG",
      "68T40",
      "I.2.1; I.2.9; I.2.2; J.6; J.7"
    ],
    "primary_category": "cs.RO",
    "comment": "6 pages, 4 figures, accepted at the 2024 CIRP International\n  Conference on Manufacturing Systems (CMS)",
    "pdf_url": "http://arxiv.org/pdf/2404.19349v1",
    "published_date": "2024-04-30 08:20:31 UTC",
    "updated_date": "2024-04-30 08:20:31 UTC"
  },
  {
    "arxiv_id": "2404.19346v1",
    "title": "Pessimistic Value Iteration for Multi-Task Data Sharing in Offline Reinforcement Learning",
    "authors": [
      "Chenjia Bai",
      "Lingxiao Wang",
      "Jianye Hao",
      "Zhuoran Yang",
      "Bin Zhao",
      "Zhen Wang",
      "Xuelong Li"
    ],
    "abstract": "Offline Reinforcement Learning (RL) has shown promising results in learning a\ntask-specific policy from a fixed dataset. However, successful offline RL often\nrelies heavily on the coverage and quality of the given dataset. In scenarios\nwhere the dataset for a specific task is limited, a natural approach is to\nimprove offline RL with datasets from other tasks, namely, to conduct\nMulti-Task Data Sharing (MTDS). Nevertheless, directly sharing datasets from\nother tasks exacerbates the distribution shift in offline RL. In this paper, we\npropose an uncertainty-based MTDS approach that shares the entire dataset\nwithout data selection. Given ensemble-based uncertainty quantification, we\nperform pessimistic value iteration on the shared offline dataset, which\nprovides a unified framework for single- and multi-task offline RL. We further\nprovide theoretical analysis, which shows that the optimality gap of our method\nis only related to the expected data coverage of the shared dataset, thus\nresolving the distribution shift issue in data sharing. Empirically, we release\nan MTDS benchmark and collect datasets from three challenging domains. The\nexperimental results show our algorithm outperforms the previous\nstate-of-the-art methods in challenging MTDS problems. See\nhttps://github.com/Baichenjia/UTDS for the datasets and code.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by Artificial Intelligence (AIJ)",
    "pdf_url": "http://arxiv.org/pdf/2404.19346v1",
    "published_date": "2024-04-30 08:16:52 UTC",
    "updated_date": "2024-04-30 08:16:52 UTC"
  },
  {
    "arxiv_id": "2404.19341v1",
    "title": "Reliable or Deceptive? Investigating Gated Features for Smooth Visual Explanations in CNNs",
    "authors": [
      "Soham Mitra",
      "Atri Sukul",
      "Swalpa Kumar Roy",
      "Pravendra Singh",
      "Vinay Verma"
    ],
    "abstract": "Deep learning models have achieved remarkable success across diverse domains.\nHowever, the intricate nature of these models often impedes a clear\nunderstanding of their decision-making processes. This is where Explainable AI\n(XAI) becomes indispensable, offering intuitive explanations for model\ndecisions. In this work, we propose a simple yet highly effective approach,\nScoreCAM++, which introduces modifications to enhance the promising ScoreCAM\nmethod for visual explainability. Our proposed approach involves altering the\nnormalization function within the activation layer utilized in ScoreCAM,\nresulting in significantly improved results compared to previous efforts.\nAdditionally, we apply an activation function to the upsampled activation\nlayers to enhance interpretability. This improvement is achieved by selectively\ngating lower-priority values within the activation layer. Through extensive\nexperiments and qualitative comparisons, we demonstrate that ScoreCAM++\nconsistently achieves notably superior performance and fairness in interpreting\nthe decision-making process compared to both ScoreCAM and previous methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.19341v1",
    "published_date": "2024-04-30 08:06:04 UTC",
    "updated_date": "2024-04-30 08:06:04 UTC"
  },
  {
    "arxiv_id": "2404.19336v3",
    "title": "Improving LLM Classification of Logical Errors by Integrating Error Relationship into Prompts",
    "authors": [
      "Yanggyu Lee",
      "Suchae Jeong",
      "Jihie Kim"
    ],
    "abstract": "LLMs trained in the understanding of programming syntax are now providing\neffective assistance to developers and are being used in programming education\nsuch as in generation of coding problem examples or providing code\nexplanations. A key aspect of programming education is understanding and\ndealing with error message. However, 'logical errors' in which the program\noperates against the programmer's intentions do not receive error messages from\nthe compiler. In this study, building on existing research on programming\nerrors, we first define the types of logical errors that can occur in\nprogramming in general. Based on the definition, we propose an effective\napproach for detecting logical errors with LLMs that makes use of relations\namong error types in the Chain-of-Thought and Tree-of-Thought prompts. The\nexperimental results indicate that when such logical error descriptions in the\nprompt are used, the average classifition performance is about 21% higher than\nthe ones without them. We also conducted an experiment for exploiting the\nrelations among errors in generating a new logical error dataset using LLMs. As\nthere is very limited dataset for logical errors such benchmark dataset can be\nvery useful for various programming related applications. We expect that our\nwork can assist novice programmers in identifying the causes of code errors and\ncorrect them more effectively.",
    "categories": [
      "cs.AI",
      "cs.PL"
    ],
    "primary_category": "cs.AI",
    "comment": "Published in ITS 2024 (Best Paper Award)",
    "pdf_url": "http://arxiv.org/pdf/2404.19336v3",
    "published_date": "2024-04-30 08:03:22 UTC",
    "updated_date": "2024-11-17 19:49:58 UTC"
  },
  {
    "arxiv_id": "2404.19330v1",
    "title": "G2LTraj: A Global-to-Local Generation Approach for Trajectory Prediction",
    "authors": [
      "Zhanwei Zhang",
      "Zishuo Hua",
      "Minghao Chen",
      "Wei Lu",
      "Binbin Lin",
      "Deng Cai",
      "Wenxiao Wang"
    ],
    "abstract": "Predicting future trajectories of traffic agents accurately holds substantial\nimportance in various applications such as autonomous driving. Previous methods\ncommonly infer all future steps of an agent either recursively or\nsimultaneously. However, the recursive strategy suffers from the accumulated\nerror, while the simultaneous strategy overlooks the constraints among future\nsteps, resulting in kinematically infeasible predictions. To address these\nissues, in this paper, we propose G2LTraj, a plug-and-play global-to-local\ngeneration approach for trajectory prediction. Specifically, we generate a\nseries of global key steps that uniformly cover the entire future time range.\nSubsequently, the local intermediate steps between the adjacent key steps are\nrecursively filled in. In this way, we prevent the accumulated error from\npropagating beyond the adjacent key steps. Moreover, to boost the kinematical\nfeasibility, we not only introduce the spatial constraints among key steps but\nalso strengthen the temporal constraints among the intermediate steps. Finally,\nto ensure the optimal granularity of key steps, we design a selectable\ngranularity strategy that caters to each predicted trajectory. Our G2LTraj\nsignificantly improves the performance of seven existing trajectory predictors\nacross the ETH, UCY and nuScenes datasets. Experimental results demonstrate its\neffectiveness. Code will be available at https://github.com/Zhanwei-Z/G2LTraj.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by IJCAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.19330v1",
    "published_date": "2024-04-30 07:53:34 UTC",
    "updated_date": "2024-04-30 07:53:34 UTC"
  },
  {
    "arxiv_id": "2404.19306v1",
    "title": "Comprehensive Forecasting-Based Analysis of Hybrid and Stacked Stateful/ Stateless Models",
    "authors": [
      "Swayamjit Saha"
    ],
    "abstract": "Wind speed is a powerful source of renewable energy, which can be used as an\nalternative to the non-renewable resources for production of electricity.\nRenewable sources are clean, infinite and do not impact the environment\nnegatively during production of electrical energy. However, while eliciting\nelectrical energy from renewable resources viz. solar irradiance, wind speed,\nhydro should require special planning failing which may result in huge loss of\nlabour and money for setting up the system. In this paper, we discuss four deep\nrecurrent neural networks viz. Stacked Stateless LSTM, Stacked Stateless GRU,\nStacked Stateful LSTM and Statcked Stateful GRU which will be used to predict\nwind speed on a short-term basis for the airport sites beside two campuses of\nMississippi State University. The paper does a comprehensive analysis of the\nperformance of the models used describing their architectures and how\nefficiently they elicit the results with the help of RMSE values. A detailed\ndescription of the time and space complexities of the above models has also\nbeen discussed.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 14 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.19306v1",
    "published_date": "2024-04-30 07:18:10 UTC",
    "updated_date": "2024-04-30 07:18:10 UTC"
  },
  {
    "arxiv_id": "2404.19303v2",
    "title": "Data Set Terminology of Deep Learning in Medicine: A Historical Review and Recommendation",
    "authors": [
      "Shannon L. Walston",
      "Hiroshi Seki",
      "Hirotaka Takita",
      "Yasuhito Mitsuyama",
      "Shingo Sato",
      "Akifumi Hagiwara",
      "Rintaro Ito",
      "Shouhei Hanaoka",
      "Yukio Miki",
      "Daiju Ueda"
    ],
    "abstract": "Medicine and deep learning-based artificial intelligence (AI) engineering\nrepresent two distinct fields each with decades of published history. With such\nhistory comes a set of terminology that has a specific way in which it is\napplied. However, when two distinct fields with overlapping terminology start\nto collaborate, miscommunication and misunderstandings can occur. This\nnarrative review aims to give historical context for these terms, accentuate\nthe importance of clarity when these terms are used in medical AI contexts, and\noffer solutions to mitigate misunderstandings by readers from either field.\nThrough an examination of historical documents, including articles, writing\nguidelines, and textbooks, this review traces the divergent evolution of terms\nfor data sets and their impact. Initially, the discordant interpretations of\nthe word 'validation' in medical and AI contexts are explored. Then the data\nsets used for AI evaluation are classified, namely random splitting,\ncross-validation, temporal, geographic, internal, and external sets. The\naccurate and standardized description of these data sets is crucial for\ndemonstrating the robustness and generalizability of AI applications in\nmedicine. This review clarifies existing literature to provide a comprehensive\nunderstanding of these classifications and their implications in AI evaluation.\nThis review then identifies often misunderstood terms and proposes pragmatic\nsolutions to mitigate terminological confusion. Among these solutions are the\nuse of standardized terminology such as 'training set,' 'validation (or tuning)\nset,' and 'test set,' and explicit definition of data set splitting\nterminologies in each medical AI research publication. This review aspires to\nenhance the precision of communication in medical AI, thereby fostering more\neffective and transparent research methodologies in this interdisciplinary\nfield.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "20 pages, 3 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2404.19303v2",
    "published_date": "2024-04-30 07:07:45 UTC",
    "updated_date": "2024-06-18 09:49:49 UTC"
  },
  {
    "arxiv_id": "2405.01593v1",
    "title": "Large Language Model Agent for Fake News Detection",
    "authors": [
      "Xinyi Li",
      "Yongfeng Zhang",
      "Edward C. Malthouse"
    ],
    "abstract": "In the current digital era, the rapid spread of misinformation on online\nplatforms presents significant challenges to societal well-being, public trust,\nand democratic processes, influencing critical decision making and public\nopinion. To address these challenges, there is a growing need for automated\nfake news detection mechanisms. Pre-trained large language models (LLMs) have\ndemonstrated exceptional capabilities across various natural language\nprocessing (NLP) tasks, prompting exploration into their potential for\nverifying news claims. Instead of employing LLMs in a non-agentic way, where\nLLMs generate responses based on direct prompts in a single shot, our work\nintroduces FactAgent, an agentic approach of utilizing LLMs for fake news\ndetection. FactAgent enables LLMs to emulate human expert behavior in verifying\nnews claims without any model training, following a structured workflow. This\nworkflow breaks down the complex task of news veracity checking into multiple\nsub-steps, where LLMs complete simple tasks using their internal knowledge or\nexternal tools. At the final step of the workflow, LLMs integrate all findings\nthroughout the workflow to determine the news claim's veracity. Compared to\nmanual human verification, FactAgent offers enhanced efficiency. Experimental\nstudies demonstrate the effectiveness of FactAgent in verifying claims without\nthe need for any training process. Moreover, FactAgent provides transparent\nexplanations at each step of the workflow and during final decision-making,\noffering insights into the reasoning process of fake news detection for end\nusers. FactAgent is highly adaptable, allowing for straightforward updates to\nits tools that LLMs can leverage within the workflow, as well as updates to the\nworkflow itself using domain knowledge. This adaptability enables FactAgent's\napplication to news verification across various domains.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01593v1",
    "published_date": "2024-04-30 06:55:27 UTC",
    "updated_date": "2024-04-30 06:55:27 UTC"
  },
  {
    "arxiv_id": "2404.19288v2",
    "title": "Training-free Graph Neural Networks and the Power of Labels as Features",
    "authors": [
      "Ryoma Sato"
    ],
    "abstract": "We propose training-free graph neural networks (TFGNNs), which can be used\nwithout training and can also be improved with optional training, for\ntransductive node classification. We first advocate labels as features (LaF),\nwhich is an admissible but not explored technique. We show that LaF provably\nenhances the expressive power of graph neural networks. We design TFGNNs based\non this analysis. In the experiments, we confirm that TFGNNs outperform\nexisting GNNs in the training-free setting and converge with much fewer\ntraining iterations than traditional GNNs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "TMLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.19288v2",
    "published_date": "2024-04-30 06:36:43 UTC",
    "updated_date": "2024-08-15 08:32:26 UTC"
  },
  {
    "arxiv_id": "2407.01553v2",
    "title": "Fish-bone diagram of research issue: Gain a bird's-eye view on a specific research topic",
    "authors": [
      "JingHong Li",
      "Huy Phan",
      "Wen Gu",
      "Koichi Ota",
      "Shinobu Hasegawa"
    ],
    "abstract": "Novice researchers often face difficulties in understanding a multitude of\nacademic papers and grasping the fundamentals of a new research field. To solve\nsuch problems, the knowledge graph supporting research survey is gradually\nbeing developed. Existing keyword-based knowledge graphs make it difficult for\nresearchers to deeply understand abstract concepts. Meanwhile, novice\nresearchers may find it difficult to use ChatGPT effectively for research\nsurveys due to their limited understanding of the research field. Without the\nability to ask proficient questions that align with key concepts, obtaining\ndesired and accurate answers from this large language model (LLM) could be\ninefficient. This study aims to help novice researchers by providing a\nfish-bone diagram that includes causal relationships, offering an overview of\nthe research topic. The diagram is constructed using the issue ontology from\nacademic papers, and it offers a broad, highly generalized perspective of the\nresearch field, based on relevance and logical factors. Furthermore, we\nevaluate the strengths and improvable points of the fish-bone diagram derived\nfrom this study's development pattern, emphasizing its potential as a viable\ntool for supporting research survey.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper has been accepted by IEEE SMC 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.01553v2",
    "published_date": "2024-04-30 05:43:41 UTC",
    "updated_date": "2024-07-11 02:18:54 UTC"
  },
  {
    "arxiv_id": "2404.19256v2",
    "title": "AI, Pluralism, and (Social) Compensation",
    "authors": [
      "Nandhini Swaminathan",
      "David Danks"
    ],
    "abstract": "One strategy in response to pluralistic values in a user population is to\npersonalize an AI system: if the AI can adapt to the specific values of each\nindividual, then we can potentially avoid many of the challenges of pluralism.\nUnfortunately, this approach creates a significant ethical issue: if there is\nan external measure of success for the human-AI team, then the adaptive AI\nsystem may develop strategies (sometimes deceptive) to compensate for its human\nteammate. This phenomenon can be viewed as a form of social compensation, where\nthe AI makes decisions based not on predefined goals but on its human partner's\ndeficiencies in relation to the team's performance objectives. We provide a\npractical ethical analysis of the conditions in which such compensation may\nnonetheless be justifiable.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.GT",
      "cs.HC",
      "cs.LG",
      "cs.MA",
      "I.2.0, 91A, 68T05"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages",
    "pdf_url": "http://arxiv.org/pdf/2404.19256v2",
    "published_date": "2024-04-30 04:41:47 UTC",
    "updated_date": "2024-10-15 22:32:47 UTC"
  },
  {
    "arxiv_id": "2405.00741v1",
    "title": "Diagnosis of Parkinson's Disease Using EEG Signals and Machine Learning Techniques: A Comprehensive Study",
    "authors": [
      "Maryam Allahbakhshi",
      "Aylar Sadri",
      "Seyed Omid Shahdi"
    ],
    "abstract": "Parkinson's disease is a widespread neurodegenerative condition necessitating\nearly diagnosis for effective intervention. This paper introduces an innovative\nmethod for diagnosing Parkinson's disease through the analysis of human EEG\nsignals, employing a Support Vector Machine (SVM) classification model. this\nresearch presents novel contributions to enhance diagnostic accuracy and\nreliability. Our approach incorporates a comprehensive review of EEG signal\nanalysis techniques and machine learning methods. Drawing from recent studies,\nwe have engineered an advanced SVM-based model optimized for Parkinson's\ndisease diagnosis. Utilizing cutting-edge feature engineering, extensive\nhyperparameter tuning, and kernel selection, our method achieves not only\nheightened diagnostic accuracy but also emphasizes model interpretability,\ncatering to both clinicians and researchers. Moreover, ethical concerns in\nhealthcare machine learning, such as data privacy and biases, are\nconscientiously addressed. We assess our method's performance through\nexperiments on a diverse dataset comprising EEG recordings from Parkinson's\ndisease patients and healthy controls, demonstrating significantly improved\ndiagnostic accuracy compared to conventional techniques. In conclusion, this\npaper introduces an innovative SVM-based approach for diagnosing Parkinson's\ndisease from human EEG signals. Building upon the IEEE framework and previous\nresearch, its novelty lies in the capacity to enhance diagnostic accuracy while\nupholding interpretability and ethical considerations for practical healthcare\napplications. These advances promise to revolutionize early Parkinson's disease\ndetection and management, ultimately contributing to enhanced patient outcomes\nand quality of life.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "9 pages, 2 tables, 10th International Conference on Artificial\n  Intelligence and Robotics-QICAR2024 Qazvin Islamic Azad University, Feb. 29,\n  2024",
    "pdf_url": "http://arxiv.org/pdf/2405.00741v1",
    "published_date": "2024-04-30 04:25:09 UTC",
    "updated_date": "2024-04-30 04:25:09 UTC"
  },
  {
    "arxiv_id": "2404.19254v1",
    "title": "Suvach -- Generated Hindi QA benchmark",
    "authors": [
      "Vaishak Narayanan",
      "Prabin Raj KP",
      "Saifudheen Nouphal"
    ],
    "abstract": "Current evaluation benchmarks for question answering (QA) in Indic languages\noften rely on machine translation of existing English datasets. This approach\nsuffers from bias and inaccuracies inherent in machine translation, leading to\ndatasets that may not reflect the true capabilities of EQA models for Indic\nlanguages. This paper proposes a new benchmark specifically designed for\nevaluating Hindi EQA models and discusses the methodology to do the same for\nany task. This method leverages large language models (LLMs) to generate a\nhigh-quality dataset in an extractive setting, ensuring its relevance for the\ntarget language. We believe this new resource will foster advancements in Hindi\nNLP research by providing a more accurate and reliable evaluation tool.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.19254v1",
    "published_date": "2024-04-30 04:19:17 UTC",
    "updated_date": "2024-04-30 04:19:17 UTC"
  },
  {
    "arxiv_id": "2404.19253v1",
    "title": "Learning to Communicate Functional States with Nonverbal Expressions for Improved Human-Robot Collaboration",
    "authors": [
      "Liam Roy",
      "Dana Kulic",
      "Elizabeth Croft"
    ],
    "abstract": "Collaborative robots must effectively communicate their internal state to\nhumans to enable a smooth interaction. Nonverbal communication is widely used\nto communicate information during human-robot interaction, however, such\nmethods may also be misunderstood, leading to communication errors. In this\nwork, we explore modulating the acoustic parameter values (pitch bend, beats\nper minute, beats per loop) of nonverbal auditory expressions to convey\nfunctional robot states (accomplished, progressing, stuck). We propose a\nreinforcement learning (RL) algorithm based on noisy human feedback to produce\naccurately interpreted nonverbal auditory expressions. The proposed approach\nwas evaluated through a user study with 24 participants. The results\ndemonstrate that: 1. Our proposed RL-based approach is able to learn suitable\nacoustic parameter values which improve the users' ability to correctly\nidentify the state of the robot. 2. Algorithm initialization informed by\nprevious user data can be used to significantly speed up the learning process.\n3. The method used for algorithm initialization strongly influences whether\nparticipants converge to similar sounds for each robot state. 4. Modulation of\npitch bend has the largest influence on user association between sounds and\nrobotic states.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC",
      "68T40"
    ],
    "primary_category": "cs.RO",
    "comment": "8 Pages, Accepted to RA-L March 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.19253v1",
    "published_date": "2024-04-30 04:18:21 UTC",
    "updated_date": "2024-04-30 04:18:21 UTC"
  },
  {
    "arxiv_id": "2404.19245v2",
    "title": "HydraLoRA: An Asymmetric LoRA Architecture for Efficient Fine-Tuning",
    "authors": [
      "Chunlin Tian",
      "Zhan Shi",
      "Zhijiang Guo",
      "Li Li",
      "Chengzhong Xu"
    ],
    "abstract": "Adapting Large Language Models (LLMs) to new tasks through fine-tuning has\nbeen made more efficient by the introduction of Parameter-Efficient Fine-Tuning\n(PEFT) techniques, such as LoRA. However, these methods often underperform\ncompared to full fine-tuning, particularly in scenarios involving complex\ndatasets. This issue becomes even more pronounced in complex domains,\nhighlighting the need for improved PEFT approaches that can achieve better\nperformance. Through a series of experiments, we have uncovered two critical\ninsights that shed light on the training and parameter inefficiency of LoRA.\nBuilding on these insights, we have developed HydraLoRA, a LoRA framework with\nan asymmetric structure that eliminates the need for domain expertise. Our\nexperiments demonstrate that HydraLoRA outperforms other PEFT approaches, even\nthose that rely on domain knowledge during the training and inference phases.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "19 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.19245v2",
    "published_date": "2024-04-30 04:01:09 UTC",
    "updated_date": "2024-05-23 15:06:02 UTC"
  },
  {
    "arxiv_id": "2404.19244v1",
    "title": "A University Framework for the Responsible use of Generative AI in Research",
    "authors": [
      "Shannon Smith",
      "Melissa Tate",
      "Keri Freeman",
      "Anne Walsh",
      "Brian Ballsun-Stanton",
      "Mark Hooper",
      "Murray Lane"
    ],
    "abstract": "Generative Artificial Intelligence (generative AI) poses both opportunities\nand risks for the integrity of research. Universities must guide researchers in\nusing generative AI responsibly, and in navigating a complex regulatory\nlandscape subject to rapid change. By drawing on the experiences of two\nAustralian universities, we propose a framework to help institutions promote\nand facilitate the responsible use of generative AI. We provide guidance to\nhelp distil the diverse regulatory environment into a principles-based position\nstatement. Further, we explain how a position statement can then serve as a\nfoundation for initiatives in training, communications, infrastructure, and\nprocess change. Despite the growing body of literature about AI's impact on\nacademic integrity for undergraduate students, there has been comparatively\nlittle attention on the impacts of generative AI for research integrity, and\nthe vital role of institutions in helping to address those challenges. This\npaper underscores the urgency for research institutions to take action in this\narea and suggests a practical and adaptable framework for so doing.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "K.4.1; K.3.1"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.19244v1",
    "published_date": "2024-04-30 04:00:15 UTC",
    "updated_date": "2024-04-30 04:00:15 UTC"
  },
  {
    "arxiv_id": "2404.19234v1",
    "title": "Multi-hop Question Answering over Knowledge Graphs using Large Language Models",
    "authors": [
      "Abir Chakraborty"
    ],
    "abstract": "Knowledge graphs (KGs) are large datasets with specific structures\nrepresenting large knowledge bases (KB) where each node represents a key entity\nand relations amongst them are typed edges. Natural language queries formed to\nextract information from a KB entail starting from specific nodes and reasoning\nover multiple edges of the corresponding KG to arrive at the correct set of\nanswer nodes. Traditional approaches of question answering on KG are based on\n(a) semantic parsing (SP), where a logical form (e.g., S-expression, SPARQL\nquery, etc.) is generated using node and edge embeddings and then reasoning\nover these representations or tuning language models to generate the final\nanswer directly, or (b) information-retrieval based that works by extracting\nentities and relations sequentially. In this work, we evaluate the capability\nof (LLMs) to answer questions over KG that involve multiple hops. We show that\ndepending upon the size and nature of the KG we need different approaches to\nextract and feed the relevant information to an LLM since every LLM comes with\na fixed context window. We evaluate our approach on six KGs with and without\nthe availability of example-specific sub-graphs and show that both the IR and\nSP-based methods can be adopted by LLMs resulting in an extremely competitive\nperformance.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.DB"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.19234v1",
    "published_date": "2024-04-30 03:31:03 UTC",
    "updated_date": "2024-04-30 03:31:03 UTC"
  },
  {
    "arxiv_id": "2404.19232v7",
    "title": "GRAMMAR: Grounded and Modular Methodology for Assessment of Closed-Domain Retrieval-Augmented Language Model",
    "authors": [
      "Xinzhe Li",
      "Ming Liu",
      "Shang Gao"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) systems are widely used across various\nindustries for querying closed-domain and in-house knowledge bases. However,\nevaluating these systems presents significant challenges due to the private\nnature of closed-domain data and a scarcity of queries with verifiable ground\ntruths. Moreover, there is a lack of analytical methods to diagnose problematic\nmodules and identify types of failure, such as those caused by knowledge\ndeficits or issues with robustness. To address these challenges, we introduce\nGRAMMAR (GRounded And Modular Methodology for Assessment of RAG), an evaluation\nframework comprising a grounded data generation process and an evaluation\nprotocol that effectively pinpoints defective modules. Our validation\nexperiments reveal that GRAMMAR provides a reliable approach for identifying\nvulnerable modules and supports hypothesis testing for textual form\nvulnerabilities. An open-source tool accompanying this framework is available\nin our GitHub repository (see https://github.com/xinzhel/grammar), allowing for\neasy reproduction of our results and enabling reliable and modular evaluation\nin closed-domain settings.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Under Review",
    "pdf_url": "http://arxiv.org/pdf/2404.19232v7",
    "published_date": "2024-04-30 03:29:30 UTC",
    "updated_date": "2024-10-23 11:19:02 UTC"
  },
  {
    "arxiv_id": "2404.19230v1",
    "title": "Deep Lead Optimization: Leveraging Generative AI for Structural Modification",
    "authors": [
      "Odin Zhang",
      "Haitao Lin",
      "Hui Zhang",
      "Huifeng Zhao",
      "Yufei Huang",
      "Yuansheng Huang",
      "Dejun Jiang",
      "Chang-yu Hsieh",
      "Peichen Pan",
      "Tingjun Hou"
    ],
    "abstract": "The idea of using deep-learning-based molecular generation to accelerate\ndiscovery of drug candidates has attracted extraordinary attention, and many\ndeep generative models have been developed for automated drug design, termed\nmolecular generation. In general, molecular generation encompasses two main\nstrategies: de novo design, which generates novel molecular structures from\nscratch, and lead optimization, which refines existing molecules into drug\ncandidates. Among them, lead optimization plays an important role in real-world\ndrug design. For example, it can enable the development of me-better drugs that\nare chemically distinct yet more effective than the original drugs. It can also\nfacilitate fragment-based drug design, transforming virtual-screened small\nligands with low affinity into first-in-class medicines. Despite its\nimportance, automated lead optimization remains underexplored compared to the\nwell-established de novo generative models, due to its reliance on complex\nbiological and chemical knowledge. To bridge this gap, we conduct a systematic\nreview of traditional computational methods for lead optimization, organizing\nthese strategies into four principal sub-tasks with defined inputs and outputs.\nThis review delves into the basic concepts, goals, conventional CADD\ntechniques, and recent advancements in AIDD. Additionally, we introduce a\nunified perspective based on constrained subgraph generation to harmonize the\nmethodologies of de novo design and lead optimization. Through this lens, de\nnovo design can incorporate strategies from lead optimization to address the\nchallenge of generating hard-to-synthesize molecules; inversely, lead\noptimization can benefit from the innovations in de novo design by approaching\nit as a task of generating molecules conditioned on certain substructures.",
    "categories": [
      "q-bio.BM",
      "cs.AI"
    ],
    "primary_category": "q-bio.BM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.19230v1",
    "published_date": "2024-04-30 03:17:42 UTC",
    "updated_date": "2024-04-30 03:17:42 UTC"
  },
  {
    "arxiv_id": "2404.19205v1",
    "title": "TableVQA-Bench: A Visual Question Answering Benchmark on Multiple Table Domains",
    "authors": [
      "Yoonsik Kim",
      "Moonbin Yim",
      "Ka Yeon Song"
    ],
    "abstract": "In this paper, we establish a benchmark for table visual question answering,\nreferred to as the TableVQA-Bench, derived from pre-existing table\nquestion-answering (QA) and table structure recognition datasets. It is\nimportant to note that existing datasets have not incorporated images or QA\npairs, which are two crucial components of TableVQA. As such, the primary\nobjective of this paper is to obtain these necessary components. Specifically,\nimages are sourced either through the application of a \\textit{stylesheet} or\nby employing the proposed table rendering system. QA pairs are generated by\nexploiting the large language model (LLM) where the input is a text-formatted\ntable. Ultimately, the completed TableVQA-Bench comprises 1,500 QA pairs. We\ncomprehensively compare the performance of various multi-modal large language\nmodels (MLLMs) on TableVQA-Bench. GPT-4V achieves the highest accuracy among\ncommercial and open-sourced MLLMs from our experiments. Moreover, we discover\nthat the number of vision queries plays a significant role in TableVQA\nperformance. To further analyze the capabilities of MLLMs in comparison to\ntheir LLM backbones, we investigate by presenting image-formatted tables to\nMLLMs and text-formatted tables to LLMs, respectively. Our findings suggest\nthat processing visual inputs is more challenging than text inputs, as\nevidenced by the lower performance of MLLMs, despite generally requiring higher\ncomputational costs than LLMs. The proposed TableVQA-Bench and evaluation codes\nare available at\n\\href{https://github.com/naver-ai/tablevqabench}{https://github.com/naver-ai/tablevqabench}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Technical Report",
    "pdf_url": "http://arxiv.org/pdf/2404.19205v1",
    "published_date": "2024-04-30 02:05:18 UTC",
    "updated_date": "2024-04-30 02:05:18 UTC"
  },
  {
    "arxiv_id": "2404.19204v1",
    "title": "NeRF-Insert: 3D Local Editing with Multimodal Control Signals",
    "authors": [
      "Benet Oriol Sabat",
      "Alessandro Achille",
      "Matthew Trager",
      "Stefano Soatto"
    ],
    "abstract": "We propose NeRF-Insert, a NeRF editing framework that allows users to make\nhigh-quality local edits with a flexible level of control. Unlike previous work\nthat relied on image-to-image models, we cast scene editing as an in-painting\nproblem, which encourages the global structure of the scene to be preserved.\nMoreover, while most existing methods use only textual prompts to condition\nedits, our framework accepts a combination of inputs of different modalities as\nreference. More precisely, a user may provide a combination of textual and\nvisual inputs including images, CAD models, and binary image masks for\nspecifying a 3D region. We use generic image generation models to in-paint the\nscene from multiple viewpoints, and lift the local edits to a 3D-consistent\nNeRF edit. Compared to previous methods, our results show better visual quality\nand also maintain stronger consistency with the original NeRF.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.19204v1",
    "published_date": "2024-04-30 02:04:49 UTC",
    "updated_date": "2024-04-30 02:04:49 UTC"
  },
  {
    "arxiv_id": "2404.19192v1",
    "title": "Mix of Experts Language Model for Named Entity Recognition",
    "authors": [
      "Xinwei Chen",
      "Kun Li",
      "Tianyou Song",
      "Jiangjian Guo"
    ],
    "abstract": "Named Entity Recognition (NER) is an essential steppingstone in the field of\nnatural language processing. Although promising performance has been achieved\nby various distantly supervised models, we argue that distant supervision\ninevitably introduces incomplete and noisy annotations, which may mislead the\nmodel training process. To address this issue, we propose a robust NER model\nnamed BOND-MoE based on Mixture of Experts (MoE). Instead of relying on a\nsingle model for NER prediction, multiple models are trained and ensembled\nunder the Expectation-Maximization (EM) framework, so that noisy supervision\ncan be dramatically alleviated. In addition, we introduce a fair assignment\nmodule to balance the document-model assignment process. Extensive experiments\non real-world datasets show that the proposed method achieves state-of-the-art\nperformance compared with other distantly supervised NER.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.19192v1",
    "published_date": "2024-04-30 01:41:03 UTC",
    "updated_date": "2024-04-30 01:41:03 UTC"
  },
  {
    "arxiv_id": "2405.00740v4",
    "title": "Modeling Caption Diversity in Contrastive Vision-Language Pretraining",
    "authors": [
      "Samuel Lavoie",
      "Polina Kirichenko",
      "Mark Ibrahim",
      "Mahmoud Assran",
      "Andrew Gordon Wilson",
      "Aaron Courville",
      "Nicolas Ballas"
    ],
    "abstract": "There are a thousand ways to caption an image. Contrastive Language\nPretraining (CLIP) on the other hand, works by mapping an image and its caption\nto a single vector -- limiting how well CLIP-like models can represent the\ndiverse ways to describe an image. In this work, we introduce Llip, Latent\nLanguage Image Pretraining, which models the diversity of captions that could\nmatch an image. Llip's vision encoder outputs a set of visual features that are\nmixed into a final representation by conditioning on information derived from\nthe text. We show that Llip outperforms non-contextualized baselines like CLIP\nand SigLIP on a variety of tasks even with large-scale encoders. Llip improves\nzero-shot classification by an average of 2.9% zero-shot classification\nbenchmarks with a ViT-G/14 encoder. Specifically, Llip attains a zero-shot\ntop-1 accuracy of 83.5% on ImageNet outperforming a similarly sized CLIP by\n1.4%. We also demonstrate improvement on zero-shot retrieval on MS-COCO by\n6.0%. We provide a comprehensive analysis of the components introduced by the\nmethod and demonstrate that Llip leads to richer visual representations.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "14 pages, 8 figures, 7 tables, to be published at ICML2024",
    "pdf_url": "http://arxiv.org/pdf/2405.00740v4",
    "published_date": "2024-04-30 01:19:18 UTC",
    "updated_date": "2025-03-29 12:57:07 UTC"
  },
  {
    "arxiv_id": "2404.19171v1",
    "title": "Explicit Correlation Learning for Generalizable Cross-Modal Deepfake Detection",
    "authors": [
      "Cai Yu",
      "Shan Jia",
      "Xiaomeng Fu",
      "Jin Liu",
      "Jiahe Tian",
      "Jiao Dai",
      "Xi Wang",
      "Siwei Lyu",
      "Jizhong Han"
    ],
    "abstract": "With the rising prevalence of deepfakes, there is a growing interest in\ndeveloping generalizable detection methods for various types of deepfakes.\nWhile effective in their specific modalities, traditional detection methods\nfall short in addressing the generalizability of detection across diverse\ncross-modal deepfakes. This paper aims to explicitly learn potential\ncross-modal correlation to enhance deepfake detection towards various\ngeneration scenarios. Our approach introduces a correlation distillation task,\nwhich models the inherent cross-modal correlation based on content information.\nThis strategy helps to prevent the model from overfitting merely to\naudio-visual synchronization. Additionally, we present the Cross-Modal Deepfake\nDataset (CMDFD), a comprehensive dataset with four generation methods to\nevaluate the detection of diverse cross-modal deepfakes. The experimental\nresults on CMDFD and FakeAVCeleb datasets demonstrate the superior\ngeneralizability of our method over existing state-of-the-art methods. Our code\nand data can be found at\n\\url{https://github.com/ljj898/CMDFD-Dataset-and-Deepfake-Detection}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "accepted by ICME 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.19171v1",
    "published_date": "2024-04-30 00:25:44 UTC",
    "updated_date": "2024-04-30 00:25:44 UTC"
  }
]
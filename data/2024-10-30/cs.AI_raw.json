[
  {
    "arxiv_id": "2410.23511v2",
    "title": "Dynamic Strategy Planning for Efficient Question Answering with Large Language Models",
    "authors": [
      "Tanmay Parekh",
      "Pradyot Prakash",
      "Alexander Radovic",
      "Akshay Shekher",
      "Denis Savenkov"
    ],
    "abstract": "Research has shown the effectiveness of reasoning (e.g., Chain-of-Thought),\nplanning (e.g., SelfAsk), and retrieval augmented generation strategies to\nimprove the performance of Large Language Models (LLMs) on various tasks, such\nas question answering. However, using a single fixed strategy to answer\ndifferent kinds of questions is suboptimal in performance and inefficient in\nterms of generated output tokens and performed retrievals. In our work, we\npropose a novel technique DyPlan, to induce a dynamic strategy selection\nprocess in LLMs, to improve performance and reduce costs in question-answering.\nDyPlan incorporates an initial decision step to select the most suitable\nstrategy conditioned on the input question and guides the LLM's response\ngeneration accordingly. We extend DyPlan to DyPlan-verify, adding an internal\nverification and correction process to further enrich the generated answer.\nExperiments on three prominent multi-hop question answering (MHQA) datasets\nreveal how DyPlan can improve model performance by 7-13% while reducing the\ncost by 11-32% relative to the best baseline model.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at NAACL 2025 Findings",
    "pdf_url": "http://arxiv.org/pdf/2410.23511v2",
    "published_date": "2024-10-30 23:35:21 UTC",
    "updated_date": "2025-02-08 00:48:39 UTC"
  },
  {
    "arxiv_id": "2410.23506v2",
    "title": "The Belief State Transformer",
    "authors": [
      "Edward S. Hu",
      "Kwangjun Ahn",
      "Qinghua Liu",
      "Haoran Xu",
      "Manan Tomar",
      "Ada Langford",
      "Dinesh Jayaraman",
      "Alex Lamb",
      "John Langford"
    ],
    "abstract": "We introduce the \"Belief State Transformer\", a next-token predictor that\ntakes both a prefix and suffix as inputs, with a novel objective of predicting\nboth the next token for the prefix and the previous token for the suffix. The\nBelief State Transformer effectively learns to solve challenging problems that\nconventional forward-only transformers struggle with, in a domain-independent\nfashion. Key to this success is learning a compact belief state that captures\nall relevant information necessary for accurate predictions. Empirical\nablations show that each component of the model is essential in difficult\nscenarios where standard Transformers fall short. For the task of story writing\nwith known prefixes and suffixes, our approach outperforms the\nFill-in-the-Middle method for reaching known goals and demonstrates improved\nperformance even when the goals are unknown. Altogether, the Belief State\nTransformer enables more efficient goal-conditioned decoding, better test-time\ninference, and high-quality text representations on small scale problems.\nWebsite: https://sites.google.com/view/belief-state-transformer",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR2025 publication",
    "pdf_url": "http://arxiv.org/pdf/2410.23506v2",
    "published_date": "2024-10-30 23:26:06 UTC",
    "updated_date": "2025-02-20 04:44:32 UTC"
  },
  {
    "arxiv_id": "2410.23501v2",
    "title": "All or None: Identifiable Linear Properties of Next-token Predictors in Language Modeling",
    "authors": [
      "Emanuele Marconato",
      "Sébastien Lachapelle",
      "Sebastian Weichwald",
      "Luigi Gresele"
    ],
    "abstract": "We analyze identifiability as a possible explanation for the ubiquity of\nlinear properties across language models, such as the vector difference between\nthe representations of \"easy\" and \"easiest\" being parallel to that between\n\"lucky\" and \"luckiest\". For this, we ask whether finding a linear property in\none model implies that any model that induces the same distribution has that\nproperty, too. To answer that, we first prove an identifiability result to\ncharacterize distribution-equivalent next-token predictors, lifting a diversity\nrequirement of previous results. Second, based on a refinement of relational\nlinearity [Paccanaro and Hinton, 2001; Hernandez et al., 2024], we show how\nmany notions of linearity are amenable to our analysis. Finally, we show that\nunder suitable conditions, these linear properties either hold in all or none\ndistribution-equivalent next-token predictors.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "28th International Conference on Artificial Intelligence and\n  Statistics (AISTATS)",
    "pdf_url": "http://arxiv.org/pdf/2410.23501v2",
    "published_date": "2024-10-30 23:19:29 UTC",
    "updated_date": "2025-03-15 10:30:45 UTC"
  },
  {
    "arxiv_id": "2410.23498v1",
    "title": "Kernel-Based Function Approximation for Average Reward Reinforcement Learning: An Optimist No-Regret Algorithm",
    "authors": [
      "Sattar Vakili",
      "Julia Olkhovskaya"
    ],
    "abstract": "Reinforcement learning utilizing kernel ridge regression to predict the\nexpected value function represents a powerful method with great\nrepresentational capacity. This setting is a highly versatile framework\namenable to analytical results. We consider kernel-based function approximation\nfor RL in the infinite horizon average reward setting, also referred to as the\nundiscounted setting. We propose an optimistic algorithm, similar to\nacquisition function based algorithms in the special case of bandits. We\nestablish novel no-regret performance guarantees for our algorithm, under\nkernel-based modelling assumptions. Additionally, we derive a novel confidence\ninterval for the kernel-based prediction of the expected value function,\napplicable across various RL problems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)",
    "pdf_url": "http://arxiv.org/pdf/2410.23498v1",
    "published_date": "2024-10-30 23:04:10 UTC",
    "updated_date": "2024-10-30 23:04:10 UTC"
  },
  {
    "arxiv_id": "2410.23495v2",
    "title": "DASH: Warm-Starting Neural Network Training in Stationary Settings without Loss of Plasticity",
    "authors": [
      "Baekrok Shin",
      "Junsoo Oh",
      "Hanseul Cho",
      "Chulhee Yun"
    ],
    "abstract": "Warm-starting neural network training by initializing networks with\npreviously learned weights is appealing, as practical neural networks are often\ndeployed under a continuous influx of new data. However, it often leads to loss\nof plasticity, where the network loses its ability to learn new information,\nresulting in worse generalization than training from scratch. This occurs even\nunder stationary data distributions, and its underlying mechanism is poorly\nunderstood. We develop a framework emulating real-world neural network training\nand identify noise memorization as the primary cause of plasticity loss when\nwarm-starting on stationary data. Motivated by this, we propose Direction-Aware\nSHrinking (DASH), a method aiming to mitigate plasticity loss by selectively\nforgetting memorized noise while preserving learned features. We validate our\napproach on vision tasks, demonstrating improvements in test accuracy and\ntraining efficiency.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published at NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.23495v2",
    "published_date": "2024-10-30 22:57:54 UTC",
    "updated_date": "2024-11-01 09:49:24 UTC"
  },
  {
    "arxiv_id": "2410.23494v1",
    "title": "Causality-Driven Audits of Model Robustness",
    "authors": [
      "Nathan Drenkow",
      "Chris Ribaudo",
      "Mathias Unberath"
    ],
    "abstract": "Robustness audits of deep neural networks (DNN) provide a means to uncover\nmodel sensitivities to the challenging real-world imaging conditions that\nsignificantly degrade DNN performance in-the-wild. Such conditions are often\nthe result of the compounding of multiple factors inherent to the environment,\nsensor, or processing pipeline and may lead to complex image distortions that\nare not easily categorized. When robustness audits are limited to a set of\npre-determined imaging effects or distortions, the results cannot be (easily)\ntransferred to real-world conditions where image corruptions may be more\ncomplex or nuanced. To address this challenge, we present a new alternative\nrobustness auditing method that uses causal inference to measure DNN\nsensitivities to the factors of the imaging process that cause complex\ndistortions. Our approach uses causal models to explicitly encode assumptions\nabout the domain-relevant factors and their interactions. Then, through\nextensive experiments on natural and rendered images across multiple vision\ntasks, we show that our approach reliably estimates causal effects of each\nfactor on DNN performance using observational domain data. These causal effects\ndirectly tie DNN sensitivities to observable properties of the imaging pipeline\nin the domain of interest towards reducing the risk of unexpected DNN failures\nwhen deployed in that domain.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.23494v1",
    "published_date": "2024-10-30 22:57:50 UTC",
    "updated_date": "2024-10-30 22:57:50 UTC"
  },
  {
    "arxiv_id": "2410.23483v1",
    "title": "Keep on Swimming: Real Attackers Only Need Partial Knowledge of a Multi-Model System",
    "authors": [
      "Julian Collado",
      "Kevin Stangl"
    ],
    "abstract": "Recent approaches in machine learning often solve a task using a composition\nof multiple models or agentic architectures. When targeting a composed system\nwith adversarial attacks, it might not be computationally or informationally\nfeasible to train an end-to-end proxy model or a proxy model for every\ncomponent of the system. We introduce a method to craft an adversarial attack\nagainst the overall multi-model system when we only have a proxy model for the\nfinal black-box model, and when the transformation applied by the initial\nmodels can make the adversarial perturbations ineffective. Current methods\nhandle this by applying many copies of the first model/transformation to an\ninput and then re-use a standard adversarial attack by averaging gradients, or\nlearning a proxy model for both stages. To our knowledge, this is the first\nattack specifically designed for this threat model and our method has a\nsubstantially higher attack success rate (80% vs 25%) and contains 9.4% smaller\nperturbations (MSE) compared to prior state-of-the-art methods. Our experiments\nfocus on a supervised image pipeline, but we are confident the attack will\ngeneralize to other multi-model settings [e.g. a mix of open/closed source\nfoundation models], or agentic systems",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CV",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.23483v1",
    "published_date": "2024-10-30 22:23:16 UTC",
    "updated_date": "2024-10-30 22:23:16 UTC"
  },
  {
    "arxiv_id": "2410.23472v2",
    "title": "Risk Sources and Risk Management Measures in Support of Standards for General-Purpose AI Systems",
    "authors": [
      "Rokas Gipiškis",
      "Ayrton San Joaquin",
      "Ze Shen Chin",
      "Adrian Regenfuß",
      "Ariel Gil",
      "Koen Holtman"
    ],
    "abstract": "There is an urgent need to identify both short and long-term risks from newly\nemerging types of Artificial Intelligence (AI), as well as available risk\nmanagement measures. In response, and to support global efforts in regulating\nAI and writing safety standards, we compile an extensive catalog of risk\nsources and risk management measures for general-purpose AI (GPAI) systems,\ncomplete with descriptions and supporting examples where relevant. This work\ninvolves identifying technical, operational, and societal risks across model\ndevelopment, training, and deployment stages, as well as surveying established\nand experimental methods for managing these risks. To the best of our\nknowledge, this paper is the first of its kind to provide extensive\ndocumentation of both GPAI risk sources and risk management measures that are\ndescriptive, self-contained and neutral with respect to any existing regulatory\nframework. This work intends to help AI providers, standards experts,\nresearchers, policymakers, and regulators in identifying and mitigating\nsystemic risks from GPAI systems. For this reason, the catalog is released\nunder a public domain license for ease of direct use by stakeholders in AI\ngovernance and standards.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "92 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.23472v2",
    "published_date": "2024-10-30 21:32:56 UTC",
    "updated_date": "2024-11-15 17:18:57 UTC"
  },
  {
    "arxiv_id": "2411.00056v1",
    "title": "Generating Diverse Negations from Affirmative Sentences",
    "authors": [
      "Darian Rodriguez Vasquez",
      "Afroditi Papadaki"
    ],
    "abstract": "Despite the impressive performance of large language models across various\ntasks, they often struggle with reasoning under negated statements. Negations\nare important in real-world applications as they encode negative polarity in\nverb phrases, clauses, or other expressions. Nevertheless, they are\nunderrepresented in current benchmarks, which mainly include basic negation\nforms and overlook more complex ones, resulting in insufficient data for\ntraining a language model. In this work, we propose NegVerse, a method that\ntackles the lack of negation datasets by producing a diverse range of negation\ntypes from affirmative sentences, including verbal, non-verbal, and affixal\nforms commonly found in English text. We provide new rules for masking parts of\nsentences where negations are most likely to occur, based on syntactic\nstructure and use a frozen baseline LLM and prompt tuning to generate negated\nsentences. We also propose a filtering mechanism to identify negation cues and\nremove degenerate examples, producing a diverse range of meaningful\nperturbations. Our results show that NegVerse outperforms existing methods and\ngenerates negations with higher lexical similarity to the original sentences,\nbetter syntactic preservation and negation diversity. The code is available in\nhttps://github.com/DarianRodriguez/NegVerse",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at \"Adaptive Foundation Models: Evolving AI for Personalized\n  and Efficient Learning\" workshop at NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.00056v1",
    "published_date": "2024-10-30 21:25:02 UTC",
    "updated_date": "2024-10-30 21:25:02 UTC"
  },
  {
    "arxiv_id": "2410.23452v1",
    "title": "Graph-Augmented Relation Extraction Model with LLMs-Generated Support Document",
    "authors": [
      "Vicky Dong",
      "Hao Yu",
      "Yao Chen"
    ],
    "abstract": "This study introduces a novel approach to sentence-level relation extraction\n(RE) that integrates Graph Neural Networks (GNNs) with Large Language Models\n(LLMs) to generate contextually enriched support documents. By harnessing the\npower of LLMs to generate auxiliary information, our approach crafts an\nintricate graph representation of textual data. This graph is subsequently\nprocessed through a Graph Neural Network (GNN) to refine and enrich the\nembeddings associated with each entity ensuring a more nuanced and\ninterconnected understanding of the data. This methodology addresses the\nlimitations of traditional sentence-level RE models by incorporating broader\ncontexts and leveraging inter-entity interactions, thereby improving the\nmodel's ability to capture complex relationships across sentences. Our\nexperiments, conducted on the CrossRE dataset, demonstrate the effectiveness of\nour approach, with notable improvements in performance across various domains.\nThe results underscore the potential of combining GNNs with LLM-generated\ncontext to advance the field of relation extraction.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.23452v1",
    "published_date": "2024-10-30 20:48:34 UTC",
    "updated_date": "2024-10-30 20:48:34 UTC"
  },
  {
    "arxiv_id": "2411.00866v1",
    "title": "Emory Knee Radiograph (MRKR) Dataset",
    "authors": [
      "Brandon Price",
      "Jason Adleberg",
      "Kaesha Thomas",
      "Zach Zaiman",
      "Aawez Mansuri",
      "Beatrice Brown-Mulry",
      "Chima Okecheukwu",
      "Judy Gichoya",
      "Hari Trivedi"
    ],
    "abstract": "The Emory Knee Radiograph (MRKR) dataset is a large, demographically diverse\ncollection of 503,261 knee radiographs from 83,011 patients, 40% of which are\nAfrican American. This dataset provides imaging data in DICOM format along with\ndetailed clinical information, including patient-reported pain scores,\ndiagnostic codes, and procedural codes, which are not commonly available in\nsimilar datasets. The MRKR dataset also features imaging metadata such as image\nlaterality, view type, and presence of hardware, enhancing its value for\nresearch and model development. MRKR addresses significant gaps in existing\ndatasets by offering a more representative sample for studying osteoarthritis\nand related outcomes, particularly among minority populations, thereby\nproviding a valuable resource for clinicians and researchers.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "16 pages",
    "pdf_url": "http://arxiv.org/pdf/2411.00866v1",
    "published_date": "2024-10-30 20:48:19 UTC",
    "updated_date": "2024-10-30 20:48:19 UTC"
  },
  {
    "arxiv_id": "2410.23450v1",
    "title": "Return Augmented Decision Transformer for Off-Dynamics Reinforcement Learning",
    "authors": [
      "Ruhan Wang",
      "Yu Yang",
      "Zhishuai Liu",
      "Dongruo Zhou",
      "Pan Xu"
    ],
    "abstract": "We study offline off-dynamics reinforcement learning (RL) to utilize data\nfrom an easily accessible source domain to enhance policy learning in a target\ndomain with limited data. Our approach centers on return-conditioned supervised\nlearning (RCSL), particularly focusing on the decision transformer (DT), which\ncan predict actions conditioned on desired return guidance and complete\ntrajectory history. Previous works tackle the dynamics shift problem by\naugmenting the reward in the trajectory from the source domain to match the\noptimal trajectory in the target domain. However, this strategy can not be\ndirectly applicable in RCSL owing to (1) the unique form of the RCSL policy\nclass, which explicitly depends on the return, and (2) the absence of a\nstraightforward representation of the optimal trajectory distribution. We\npropose the Return Augmented Decision Transformer (RADT) method, where we\naugment the return in the source domain by aligning its distribution with that\nin the target domain. We provide the theoretical analysis demonstrating that\nthe RCSL policy learned from RADT achieves the same level of suboptimality as\nwould be obtained without a dynamics shift. We introduce two practical\nimplementations RADT-DARA and RADT-MV respectively. Extensive experiments\nconducted on D4RL datasets reveal that our methods generally outperform dynamic\nprogramming based methods in off-dynamics RL scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "26 pages, 10 tables, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.23450v1",
    "published_date": "2024-10-30 20:46:26 UTC",
    "updated_date": "2024-10-30 20:46:26 UTC"
  },
  {
    "arxiv_id": "2410.23448v1",
    "title": "Venire: A Machine Learning-Guided Panel Review System for Community Content Moderation",
    "authors": [
      "Vinay Koshy",
      "Frederick Choi",
      "Yi-Shyuan Chiang",
      "Hari Sundaram",
      "Eshwar Chandrasekharan",
      "Karrie Karahalios"
    ],
    "abstract": "Research into community content moderation often assumes that moderation\nteams govern with a single, unified voice. However, recent work has found that\nmoderators disagree with one another at modest, but concerning rates. The\nproblem is not the root disagreements themselves. Subjectivity in moderation is\nunavoidable, and there are clear benefits to including diverse perspectives\nwithin a moderation team. Instead, the crux of the issue is that, due to\nresource constraints, moderation decisions end up being made by individual\ndecision-makers. The result is decision-making that is inconsistent, which is\nfrustrating for community members. To address this, we develop Venire, an\nML-backed system for panel review on Reddit. Venire uses a machine learning\nmodel trained on log data to identify the cases where moderators are most\nlikely to disagree. Venire fast-tracks these cases for multi-person review.\nIdeally, Venire allows moderators to surface and resolve disagreements that\nwould have otherwise gone unnoticed. We conduct three studies through which we\ndesign and evaluate Venire: a set of formative interviews with moderators,\ntechnical evaluations on two datasets, and a think-aloud study in which\nmoderators used Venire to make decisions on real moderation cases.\nQuantitatively, we demonstrate that Venire is able to improve decision\nconsistency and surface latent disagreements. Qualitatively, we find that\nVenire helps moderators resolve difficult moderation cases more confidently.\nVenire represents a novel paradigm for human-AI content moderation, and shifts\nthe conversation from replacing human decision-making to supporting it.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.23448v1",
    "published_date": "2024-10-30 20:39:34 UTC",
    "updated_date": "2024-10-30 20:39:34 UTC"
  },
  {
    "arxiv_id": "2411.00054v1",
    "title": "eDOC: Explainable Decoding Out-of-domain Cell Types with Evidential Learning",
    "authors": [
      "Chaochen Wu",
      "Meiyun Zuo",
      "Lei Xie"
    ],
    "abstract": "Single-cell RNA-seq (scRNA-seq) technology is a powerful tool for unraveling\nthe complexity of biological systems. One of essential and fundamental tasks in\nscRNA-seq data analysis is Cell Type Annotation (CTA). In spite of tremendous\nefforts in developing machine learning methods for this problem, several\nchallenges remains. They include identifying Out-of-Domain (OOD) cell types,\nquantifying the uncertainty of unseen cell type annotations, and determining\ninterpretable cell type-specific gene drivers for an OOD case. OOD cell types\nare often associated with therapeutic responses and disease origins, making\nthem critical for precision medicine and early disease diagnosis. Additionally,\nscRNA-seq data contains tens thousands of gene expressions. Pinpointing gene\ndrivers underlying CTA can provide deep insight into gene regulatory mechanisms\nand serve as disease biomarkers. In this study, we develop a new method, eDOC,\nto address aforementioned challenges. eDOC leverages a transformer architecture\nwith evidential learning to annotate In-Domain (IND) and OOD cell types as well\nas to highlight genes that contribute both IND cells and OOD cells in a single\ncell resolution. Rigorous experiments demonstrate that eDOC significantly\nimproves the efficiency and effectiveness of OOD cell type and gene driver\nidentification compared to other state-of-the-art methods. Our findings suggest\nthat eDOC may provide new insights into single-cell biology.",
    "categories": [
      "q-bio.GN",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.GN",
    "comment": "under review",
    "pdf_url": "http://arxiv.org/pdf/2411.00054v1",
    "published_date": "2024-10-30 20:15:36 UTC",
    "updated_date": "2024-10-30 20:15:36 UTC"
  },
  {
    "arxiv_id": "2411.00865v2",
    "title": "Demo-Craft: Using In-Context Learning to Improve Code Generation in Large Language Models",
    "authors": [
      "Nirmal Joshua Kapu",
      "Mihit Sreejith"
    ],
    "abstract": "Generating executable code from natural language instructions using Large\nLanguage Models (LLMs) poses challenges such as semantic ambiguity and\nunderstanding taskspecific contexts. To address these issues, we propose a\nsystem called DemoCraft, which enhances code generation by leveraging\nin-context learning and demonstration selection, combined with latent concept\nlearning. Latent concept learning introduces additional concept tokens, which\nare trainable embeddings that capture task-specific knowledge. We then test our\nsystem on two major datasets: MBPP and Humaneval. Our experimental results\ndemonstrate that the proposed system achieves an approximate 2x increase in the\npass@k metric compared to baseline models. Furthermore, we introduce two novel\nevaluation metrics: correctness@k and similarity@k. Our empirical studies\nindicate that our system attains nearly a 3x improvement in these metrics as\nwell.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted at IEEE ICIITCEE 2025. Presented on 16th January 2025 in\n  Bengaluru, India",
    "pdf_url": "http://arxiv.org/pdf/2411.00865v2",
    "published_date": "2024-10-30 19:45:50 UTC",
    "updated_date": "2025-03-22 05:52:26 UTC"
  },
  {
    "arxiv_id": "2410.23409v1",
    "title": "TPP-Gaze: Modelling Gaze Dynamics in Space and Time with Neural Temporal Point Processes",
    "authors": [
      "Alessandro D'Amelio",
      "Giuseppe Cartella",
      "Vittorio Cuculo",
      "Manuele Lucchi",
      "Marcella Cornia",
      "Rita Cucchiara",
      "Giuseppe Boccignone"
    ],
    "abstract": "Attention guides our gaze to fixate the proper location of the scene and\nholds it in that location for the deserved amount of time given current\nprocessing demands, before shifting to the next one. As such, gaze deployment\ncrucially is a temporal process. Existing computational models have made\nsignificant strides in predicting spatial aspects of observer's visual\nscanpaths (where to look), while often putting on the background the temporal\nfacet of attention dynamics (when). In this paper we present TPP-Gaze, a novel\nand principled approach to model scanpath dynamics based on Neural Temporal\nPoint Process (TPP), that jointly learns the temporal dynamics of fixations\nposition and duration, integrating deep learning methodologies with point\nprocess theory. We conduct extensive experiments across five publicly available\ndatasets. Our results show the overall superior performance of the proposed\nmodel compared to state-of-the-art approaches. Source code and trained models\nare publicly available at: https://github.com/phuselab/tppgaze.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at WACV 2025",
    "pdf_url": "http://arxiv.org/pdf/2410.23409v1",
    "published_date": "2024-10-30 19:22:38 UTC",
    "updated_date": "2024-10-30 19:22:38 UTC"
  },
  {
    "arxiv_id": "2410.23405v1",
    "title": "FlowLLM: Flow Matching for Material Generation with Large Language Models as Base Distributions",
    "authors": [
      "Anuroop Sriram",
      "Benjamin Kurt Miller",
      "Ricky T. Q. Chen",
      "Brandon M. Wood"
    ],
    "abstract": "Material discovery is a critical area of research with the potential to\nrevolutionize various fields, including carbon capture, renewable energy, and\nelectronics. However, the immense scale of the chemical space makes it\nchallenging to explore all possible materials experimentally. In this paper, we\nintroduce FlowLLM, a novel generative model that combines large language models\n(LLMs) and Riemannian flow matching (RFM) to design novel crystalline\nmaterials. FlowLLM first fine-tunes an LLM to learn an effective base\ndistribution of meta-stable crystals in a text representation. After converting\nto a graph representation, the RFM model takes samples from the LLM and\niteratively refines the coordinates and lattice parameters. Our approach\nsignificantly outperforms state-of-the-art methods, increasing the generation\nrate of stable materials by over three times and increasing the rate for\nstable, unique, and novel crystals by $\\sim50\\%$ - a huge improvement on a\ndifficult problem. Additionally, the crystals generated by FlowLLM are much\ncloser to their relaxed state when compared with another leading model,\nsignificantly reducing post-hoc computational cost.",
    "categories": [
      "cs.LG",
      "cond-mat.mtrl-sci",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.23405v1",
    "published_date": "2024-10-30 19:15:43 UTC",
    "updated_date": "2024-10-30 19:15:43 UTC"
  },
  {
    "arxiv_id": "2411.00053v3",
    "title": "ACC-Collab: An Actor-Critic Approach to Multi-Agent LLM Collaboration",
    "authors": [
      "Andrew Estornell",
      "Jean-Francois Ton",
      "Yuanshun Yao",
      "Yang Liu"
    ],
    "abstract": "Large language models (LLMs) have demonstrated a remarkable ability to serve\nas general-purpose tools for various language-based tasks. Recent works have\ndemonstrated that the efficacy of such models can be improved through iterative\ndialog between multiple models. While these paradigms show promise in improving\nmodel efficacy, most works in this area treat collaboration as an emergent\nbehavior, rather than a learned behavior. In doing so, current multi-agent\nframeworks rely on collaborative behaviors to have been sufficiently trained\ninto off-the-shelf models. To address this limitation, we propose ACC-Collab,\nan Actor-Critic based learning framework to produce a two-agent team (an\nactor-agent and a critic-agent) specialized in collaboration. We demonstrate\nthat ACC-Collab outperforms SotA multi-agent techniques on a wide array of\nbenchmarks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.00053v3",
    "published_date": "2024-10-30 19:09:02 UTC",
    "updated_date": "2025-03-06 16:28:55 UTC"
  },
  {
    "arxiv_id": "2410.23396v1",
    "title": "Adaptive Network Intervention for Complex Systems: A Hierarchical Graph Reinforcement Learning Approach",
    "authors": [
      "Qiliang Chen",
      "Babak Heydari"
    ],
    "abstract": "Effective governance and steering of behavior in complex multi-agent systems\n(MAS) are essential for managing system-wide outcomes, particularly in\nenvironments where interactions are structured by dynamic networks. In many\napplications, the goal is to promote pro-social behavior among agents, where\nnetwork structure plays a pivotal role in shaping these interactions. This\npaper introduces a Hierarchical Graph Reinforcement Learning (HGRL) framework\nthat governs such systems through targeted interventions in the network\nstructure. Operating within the constraints of limited managerial authority,\nthe HGRL framework demonstrates superior performance across a range of\nenvironmental conditions, outperforming established baseline methods. Our\nfindings highlight the critical influence of agent-to-agent learning (social\nlearning) on system behavior: under low social learning, the HGRL manager\npreserves cooperation, forming robust core-periphery networks dominated by\ncooperators. In contrast, high social learning accelerates defection, leading\nto sparser, chain-like networks. Additionally, the study underscores the\nimportance of the system manager's authority level in preventing system-wide\nfailures, such as agent rebellion or collapse, positioning HGRL as a powerful\ntool for dynamic network-based governance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.GT",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.23396v1",
    "published_date": "2024-10-30 18:59:02 UTC",
    "updated_date": "2024-10-30 18:59:02 UTC"
  },
  {
    "arxiv_id": "2410.23393v1",
    "title": "Resource Governance in Networked Systems via Integrated Variational Autoencoders and Reinforcement Learning",
    "authors": [
      "Qiliang Chen",
      "Babak Heydari"
    ],
    "abstract": "We introduce a framework that integrates variational autoencoders (VAE) with\nreinforcement learning (RL) to balance system performance and resource usage in\nmulti-agent systems by dynamically adjusting network structures over time. A\nkey innovation of this method is its capability to handle the vast action space\nof the network structure. This is achieved by combining Variational\nAuto-Encoder and Deep Reinforcement Learning to control the latent space\nencoded from the network structures. The proposed method, evaluated on the\nmodified OpenAI particle environment under various scenarios, not only\ndemonstrates superior performance compared to baselines but also reveals\ninteresting strategies and insights through the learned behaviors.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.23393v1",
    "published_date": "2024-10-30 18:57:02 UTC",
    "updated_date": "2024-10-30 18:57:02 UTC"
  },
  {
    "arxiv_id": "2410.23391v2",
    "title": "Understanding Representation of Deep Equilibrium Models from Neural Collapse Perspective",
    "authors": [
      "Haixiang Sun",
      "Ye Shi"
    ],
    "abstract": "Deep Equilibrium Model (DEQ), which serves as a typical implicit neural\nnetwork, emphasizes their memory efficiency and competitive performance\ncompared to explicit neural networks. However, there has been relatively\nlimited theoretical analysis on the representation of DEQ. In this paper, we\nutilize the Neural Collapse ($\\mathcal{NC}$) as a tool to systematically\nanalyze the representation of DEQ under both balanced and imbalanced\nconditions. $\\mathcal{NC}$ is an interesting phenomenon in the neural network\ntraining process that characterizes the geometry of class features and\nclassifier weights. While extensively studied in traditional explicit neural\nnetworks, the $\\mathcal{NC}$ phenomenon has not received substantial attention\nin the context of implicit neural networks. We theoretically show that\n$\\mathcal{NC}$ exists in DEQ under balanced conditions. Moreover, in imbalanced\nsettings, despite the presence of minority collapse, DEQ demonstrated\nadvantages over explicit neural networks. These advantages include the\nconvergence of extracted features to the vertices of a simplex equiangular\ntight frame and self-duality properties under mild conditions, highlighting\nDEQ's superiority in handling imbalanced datasets. Finally, we validate our\ntheoretical analyses through experiments in both balanced and imbalanced\nscenarios.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.23391v2",
    "published_date": "2024-10-30 18:50:16 UTC",
    "updated_date": "2024-12-03 22:43:27 UTC"
  },
  {
    "arxiv_id": "2410.23386v1",
    "title": "STIED: A deep learning model for the SpatioTemporal detection of focal Interictal Epileptiform Discharges with MEG",
    "authors": [
      "Raquel Fernández-Martín",
      "Alfonso Gijón",
      "Odile Feys",
      "Elodie Juvené",
      "Alec Aeby",
      "Charline Urbain",
      "Xavier De Tiège",
      "Vincent Wens"
    ],
    "abstract": "Magnetoencephalography (MEG) allows the non-invasive detection of interictal\nepileptiform discharges (IEDs). Clinical MEG analysis in epileptic patients\ntraditionally relies on the visual identification of IEDs, which is time\nconsuming and partially subjective. Automatic, data-driven detection methods\nexist but show limited performance. Still, the rise of deep learning (DL)-with\nits ability to reproduce human-like abilities-could revolutionize clinical MEG\npractice. Here, we developed and validated STIED, a simple yet powerful\nsupervised DL algorithm combining two convolutional neural networks with\ntemporal (1D time-course) and spatial (2D topography) features of MEG signals\ninspired from current clinical guidelines. Our DL model enabled both temporal\nand spatial localization of IEDs in patients suffering from focal epilepsy with\nfrequent and high amplitude spikes (FE group), with high-performance\nmetrics-accuracy, specificity, and sensitivity all exceeding 85%-when learning\nfrom spatiotemporal features of IEDs. This performance can be attributed to our\nhandling of input data, which mimics established clinical MEG practice. Reverse\nengineering further revealed that STIED encodes fine spatiotemporal features of\nIEDs rather than their mere amplitude. The model trained on the FE group also\nshowed promising results when applied to a separate group of presurgical\npatients with different types of refractory focal epilepsy, though further work\nis needed to distinguish IEDs from physiological transients. This study paves\nthe way of incorporating STIED and DL algorithms into the routine clinical MEG\nevaluation of epilepsy.",
    "categories": [
      "physics.med-ph",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "physics.med-ph",
    "comment": "10 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.23386v1",
    "published_date": "2024-10-30 18:41:22 UTC",
    "updated_date": "2024-10-30 18:41:22 UTC"
  },
  {
    "arxiv_id": "2410.23382v1",
    "title": "Estimating Neural Network Robustness via Lipschitz Constant and Architecture Sensitivity",
    "authors": [
      "Abulikemu Abuduweili",
      "Changliu Liu"
    ],
    "abstract": "Ensuring neural network robustness is essential for the safe and reliable\noperation of robotic learning systems, especially in perception and\ndecision-making tasks within real-world environments. This paper investigates\nthe robustness of neural networks in perception systems, specifically examining\ntheir sensitivity to targeted, small-scale perturbations. We identify the\nLipschitz constant as a key metric for quantifying and enhancing network\nrobustness. We derive an analytical expression to compute the Lipschitz\nconstant based on neural network architecture, providing a theoretical basis\nfor estimating and improving robustness. Several experiments reveal the\nrelationship between network design, the Lipschitz constant, and robustness,\noffering practical insights for developing safer, more robust robot learning\nsystems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "SAFE-ROL at CoRL 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.23382v1",
    "published_date": "2024-10-30 18:38:42 UTC",
    "updated_date": "2024-10-30 18:38:42 UTC"
  },
  {
    "arxiv_id": "2411.00864v1",
    "title": "Advancing Crime Linkage Analysis with Machine Learning: A Comprehensive Review and Framework for Data-Driven Approaches",
    "authors": [
      "Vinicius Lima",
      "Umit Karabiyik"
    ],
    "abstract": "Crime linkage is the process of analyzing criminal behavior data to determine\nwhether a pair or group of crime cases are connected or belong to a series of\noffenses. This domain has been extensively studied by researchers in sociology,\npsychology, and statistics. More recently, it has drawn interest from computer\nscientists, especially with advances in artificial intelligence. Despite this,\nthe literature indicates that work in this latter discipline is still in its\nearly stages. This study aims to understand the challenges faced by machine\nlearning approaches in crime linkage and to support foundational knowledge for\nfuture data-driven methods. To achieve this goal, we conducted a comprehensive\nsurvey of the main literature on the topic and developed a general framework\nfor crime linkage processes, thoroughly describing each step. Our goal was to\nunify insights from diverse fields into a shared terminology to enhance the\nresearch landscape for those intrigued by this subject.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.00864v1",
    "published_date": "2024-10-30 18:22:45 UTC",
    "updated_date": "2024-10-30 18:22:45 UTC"
  },
  {
    "arxiv_id": "2410.23373v1",
    "title": "Non-binary artificial neuron with phase variation implemented on a quantum computer",
    "authors": [
      "Jhordan Silveira de Borba",
      "Jonas Maziero"
    ],
    "abstract": "The first artificial quantum neuron models followed a similar path to classic\nmodels, as they work only with discrete values. Here we introduce an algorithm\nthat generalizes the binary model manipulating the phase of complex numbers. We\npropose, test, and implement a neuron model that works with continuous values\nin a quantum computer. Through simulations, we demonstrate that our model may\nwork in a hybrid training scheme utilizing gradient descent as a learning\nalgorithm. This work represents another step in the direction of evaluation of\nthe use of artificial neural networks efficiently implemented on near-term\nquantum devices.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "quant-ph",
    "comment": "11 pages, 7 figures, to be published in Ci\\^encia e Natura (ISSN\n  2179-460X, DOI: 10.5902/2179460X)",
    "pdf_url": "http://arxiv.org/pdf/2410.23373v1",
    "published_date": "2024-10-30 18:18:53 UTC",
    "updated_date": "2024-10-30 18:18:53 UTC"
  },
  {
    "arxiv_id": "2410.23356v1",
    "title": "Sequential Order-Robust Mamba for Time Series Forecasting",
    "authors": [
      "Seunghan Lee",
      "Juri Hong",
      "Kibok Lee",
      "Taeyoung Park"
    ],
    "abstract": "Mamba has recently emerged as a promising alternative to Transformers,\noffering near-linear complexity in processing sequential data. However, while\nchannels in time series (TS) data have no specific order in general, recent\nstudies have adopted Mamba to capture channel dependencies (CD) in TS,\nintroducing a sequential order bias. To address this issue, we propose\nSOR-Mamba, a TS forecasting method that 1) incorporates a regularization\nstrategy to minimize the discrepancy between two embedding vectors generated\nfrom data with reversed channel orders, thereby enhancing robustness to channel\norder, and 2) eliminates the 1D-convolution originally designed to capture\nlocal information in sequential data. Furthermore, we introduce channel\ncorrelation modeling (CCM), a pretraining task aimed at preserving correlations\nbetween channels from the data space to the latent space in order to enhance\nthe ability to capture CD. Extensive experiments demonstrate the efficacy of\nthe proposed method across standard and transfer learning scenarios. Code is\navailable at https://github.com/seunghan96/SOR-Mamba.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS Workshop on Time Series in the Age of Large Models, 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.23356v1",
    "published_date": "2024-10-30 18:05:22 UTC",
    "updated_date": "2024-10-30 18:05:22 UTC"
  },
  {
    "arxiv_id": "2411.00863v1",
    "title": "Next-Token Prediction Task Assumes Optimal Data Ordering for LLM Training in Proof Generation",
    "authors": [
      "Chenyang An",
      "Shima Imani",
      "Feng Yao",
      "Chengyu Dong",
      "Ali Abbasi",
      "Harsh Shrivastava",
      "Samuel Buss",
      "Jingbo Shang",
      "Gayathri Mahalingam",
      "Pramod Sharma",
      "Maurice Diesendruck"
    ],
    "abstract": "In the field of large language model (LLM)-based proof generation, despite\nbeing trained on extensive corpora such as OpenWebMath and Arxiv, these models\nstill exhibit only modest performance on proving tasks of moderate difficulty.\nWe believe that this is partly due to the suboptimal order of each proof data\nused in training. Published proofs often follow a purely logical order, where\neach step logically proceeds from the previous steps based on the deductive\nrules. However, this order aims to facilitate the verification of the proof's\nsoundness, rather than to help people and models learn the discovery process of\nthe proof. In proof generation, we argue that the optimal order for one\ntraining data sample occurs when the relevant intermediate supervision for a\nparticular proof step in the proof is always positioned to the left of that\nproof step. We call such order the intuitively sequential order. We validate\nour claims using two tasks: intuitionistic propositional logic theorem-proving\nand digit multiplication. Our experiments verify the order effect and provide\nsupport for our explanations. We demonstrate that training is most effective\nwhen the proof is in the intuitively sequential order. Moreover, the order\neffect and the performance gap between models trained on different data orders\nare substantial -- with an 11 percent improvement in proof success rate\nobserved in the propositional logic theorem-proving task, between models\ntrained on the optimal order compared to the worst order.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.00863v1",
    "published_date": "2024-10-30 18:00:04 UTC",
    "updated_date": "2024-10-30 18:00:04 UTC"
  },
  {
    "arxiv_id": "2410.23346v2",
    "title": "ASURA-FDPS-ML: Star-by-star Galaxy Simulations Accelerated by Surrogate Modeling for Supernova Feedback",
    "authors": [
      "Keiya Hirashima",
      "Kana Moriwaki",
      "Michiko S. Fujii",
      "Yutaka Hirai",
      "Takayuki R. Saitoh",
      "Junnichiro Makino",
      "Ulrich P. Steinwandel",
      "Shirley Ho"
    ],
    "abstract": "We introduce new high-resolution galaxy simulations accelerated by a\nsurrogate model that reduces the computation cost by approximately 75 percent.\nMassive stars with a Zero Age Main Sequence mass of more than about 10\n$\\mathrm{M_\\odot}$ explode as core-collapse supernovae (CCSNe), which play a\ncritical role in galaxy formation. The energy released by CCSNe is essential\nfor regulating star formation and driving feedback processes in the\ninterstellar medium (ISM). However, the short integration timesteps required\nfor SNe feedback have presented significant bottlenecks in astrophysical\nsimulations across various scales. Overcoming this challenge is crucial for\nenabling star-by-star galaxy simulations, which aim to capture the dynamics of\nindividual stars and the inhomogeneous shell's expansion within the turbulent\nISM. To address this, our new framework combines direct numerical simulations\nand surrogate modeling, including machine learning and Gibbs sampling. The star\nformation history and the time evolution of outflow rates in the galaxy match\nthose obtained from resolved direct numerical simulations. Our new approach\nachieves high-resolution fidelity while reducing computational costs,\neffectively bridging the physical scale gap and enabling multi-scale\nsimulations.",
    "categories": [
      "astro-ph.GA",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "astro-ph.GA",
    "comment": "22 pages, 15 figures, 3 tables, accepted for publication in ApJ",
    "pdf_url": "http://arxiv.org/pdf/2410.23346v2",
    "published_date": "2024-10-30 18:00:02 UTC",
    "updated_date": "2025-05-07 08:03:27 UTC"
  },
  {
    "arxiv_id": "2410.23332v1",
    "title": "MoLE: Enhancing Human-centric Text-to-image Diffusion via Mixture of Low-rank Experts",
    "authors": [
      "Jie Zhu",
      "Yixiong Chen",
      "Mingyu Ding",
      "Ping Luo",
      "Leye Wang",
      "Jingdong Wang"
    ],
    "abstract": "Text-to-image diffusion has attracted vast attention due to its impressive\nimage-generation capabilities. However, when it comes to human-centric\ntext-to-image generation, particularly in the context of faces and hands, the\nresults often fall short of naturalness due to insufficient training priors. We\nalleviate the issue in this work from two perspectives. 1) From the data\naspect, we carefully collect a human-centric dataset comprising over one\nmillion high-quality human-in-the-scene images and two specific sets of\nclose-up images of faces and hands. These datasets collectively provide a rich\nprior knowledge base to enhance the human-centric image generation capabilities\nof the diffusion model. 2) On the methodological front, we propose a simple yet\neffective method called Mixture of Low-rank Experts (MoLE) by considering\nlow-rank modules trained on close-up hand and face images respectively as\nexperts. This concept draws inspiration from our observation of low-rank\nrefinement, where a low-rank module trained by a customized close-up dataset\nhas the potential to enhance the corresponding image part when applied at an\nappropriate scale. To validate the superiority of MoLE in the context of\nhuman-centric image generation compared to state-of-the-art, we construct two\nbenchmarks and perform evaluations with diverse metrics and human studies.\nDatasets, model, and code are released at\nhttps://sites.google.com/view/mole4diffuser/.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Published at NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.23332v1",
    "published_date": "2024-10-30 17:59:57 UTC",
    "updated_date": "2024-10-30 17:59:57 UTC"
  },
  {
    "arxiv_id": "2410.23285v3",
    "title": "Provable Acceleration for Diffusion Models under Minimal Assumptions",
    "authors": [
      "Gen Li",
      "Changxiao Cai"
    ],
    "abstract": "Score-based diffusion models, while achieving minimax optimality for\nsampling, are often hampered by slow sampling speeds due to the high\ncomputational burden of score function evaluations. Despite the recent\nremarkable empirical advances in speeding up the score-based samplers,\ntheoretical understanding of acceleration techniques remains largely limited.\nTo bridge this gap, we propose a novel training-free acceleration scheme for\nstochastic samplers. Under minimal assumptions -- namely, $L^2$-accurate score\nestimates and a finite second-moment condition on the target distribution --\nour accelerated sampler provably achieves $\\varepsilon$-accuracy in total\nvariation within $\\widetilde{O}(d^{5/4}/\\sqrt{\\varepsilon})$ iterations,\nthereby significantly improving upon the $\\widetilde{O}(d/\\varepsilon)$\niteration complexity of standard score-based samplers for $\\varepsilon\\leq\n1/\\sqrt{d}$. Notably, our convergence theory does not rely on restrictive\nassumptions on the target distribution or higher-order score estimation\nguarantees.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.23285v3",
    "published_date": "2024-10-30 17:59:06 UTC",
    "updated_date": "2025-02-26 17:35:05 UTC"
  },
  {
    "arxiv_id": "2411.00052v1",
    "title": "Larger models yield better results? Streamlined severity classification of ADHD-related concerns using BERT-based knowledge distillation",
    "authors": [
      "Ahmed Akib Jawad Karim",
      "Kazi Hafiz Md. Asad",
      "Md. Golam Rabiul Alam"
    ],
    "abstract": "This work focuses on the efficiency of the knowledge distillation approach in\ngenerating a lightweight yet powerful BERT based model for natural language\nprocessing applications. After the model creation, we applied the resulting\nmodel, LastBERT, to a real-world task classifying severity levels of Attention\nDeficit Hyperactivity Disorder (ADHD)-related concerns from social media text\ndata. Referring to LastBERT, a customized student BERT model, we significantly\nlowered model parameters from 110 million BERT base to 29 million, resulting in\na model approximately 73.64% smaller. On the GLUE benchmark, comprising\nparaphrase identification, sentiment analysis, and text classification, the\nstudent model maintained strong performance across many tasks despite this\nreduction. The model was also used on a real-world ADHD dataset with an\naccuracy and F1 score of 85%. When compared to DistilBERT (66M) and\nClinicalBERT (110M), LastBERT demonstrated comparable performance, with\nDistilBERT slightly outperforming it at 87%, and ClinicalBERT achieving 86%\nacross the same metrics. These findings highlight the LastBERT model's capacity\nto classify degrees of ADHD severity properly, so it offers a useful tool for\nmental health professionals to assess and comprehend material produced by users\non social networking platforms. The study emphasizes the possibilities of\nknowledge distillation to produce effective models fit for use in\nresource-limited conditions, hence advancing NLP and mental health diagnosis.\nFurthermore underlined by the considerable decrease in model size without\nappreciable performance loss is the lower computational resources needed for\ntraining and deployment, hence facilitating greater applicability. Especially\nusing readily available computational tools like Google Colab. This study shows\nthe accessibility and usefulness of advanced NLP methods in pragmatic world\napplications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "20 figures, 31 pages, review 1 from plos one journal",
    "pdf_url": "http://arxiv.org/pdf/2411.00052v1",
    "published_date": "2024-10-30 17:57:44 UTC",
    "updated_date": "2024-10-30 17:57:44 UTC"
  },
  {
    "arxiv_id": "2410.23279v3",
    "title": "A Transformer Model for Segmentation, Classification, and Caller Identification of Marmoset Vocalization",
    "authors": [
      "Bin Wu",
      "Shinnosuke Takamichi",
      "Sakriani Sakti",
      "Satoshi Nakamura"
    ],
    "abstract": "Marmoset, a highly vocalized primate, has become a popular animal model for\nstudying social-communicative behavior and its underlying mechanism comparing\nwith human infant linguistic developments. In the study of vocal communication,\nit is vital to know the caller identities, call contents, and vocal exchanges.\nPrevious work of a CNN has achieved a joint model for call segmentation,\nclassification, and caller identification for marmoset vocalizations. However,\nthe CNN has limitations in modeling long-range acoustic patterns; the\nTransformer architecture that has been shown to outperform CNNs, utilizes the\nself-attention mechanism that efficiently segregates information parallelly\nover long distances and captures the global structure of marmoset vocalization.\nWe propose using the Transformer to jointly segment and classify the marmoset\ncalls and identify the callers for each vocalization.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.23279v3",
    "published_date": "2024-10-30 17:57:13 UTC",
    "updated_date": "2024-11-21 08:52:31 UTC"
  },
  {
    "arxiv_id": "2410.23277v2",
    "title": "SlowFast-VGen: Slow-Fast Learning for Action-Driven Long Video Generation",
    "authors": [
      "Yining Hong",
      "Beide Liu",
      "Maxine Wu",
      "Yuanhao Zhai",
      "Kai-Wei Chang",
      "Linjie Li",
      "Kevin Lin",
      "Chung-Ching Lin",
      "Jianfeng Wang",
      "Zhengyuan Yang",
      "Yingnian Wu",
      "Lijuan Wang"
    ],
    "abstract": "Human beings are endowed with a complementary learning system, which bridges\nthe slow learning of general world dynamics with fast storage of episodic\nmemory from a new experience. Previous video generation models, however,\nprimarily focus on slow learning by pre-training on vast amounts of data,\noverlooking the fast learning phase crucial for episodic memory storage. This\noversight leads to inconsistencies across temporally distant frames when\ngenerating longer videos, as these frames fall beyond the model's context\nwindow. To this end, we introduce SlowFast-VGen, a novel dual-speed learning\nsystem for action-driven long video generation. Our approach incorporates a\nmasked conditional video diffusion model for the slow learning of world\ndynamics, alongside an inference-time fast learning strategy based on a\ntemporal LoRA module. Specifically, the fast learning process updates its\ntemporal LoRA parameters based on local inputs and outputs, thereby efficiently\nstoring episodic memory in its parameters. We further propose a slow-fast\nlearning loop algorithm that seamlessly integrates the inner fast learning loop\ninto the outer slow learning loop, enabling the recall of prior multi-episode\nexperiences for context-aware skill learning. To facilitate the slow learning\nof an approximate world model, we collect a large-scale dataset of 200k videos\nwith language action annotations, covering a wide range of scenarios. Extensive\nexperiments show that SlowFast-VGen outperforms baselines across various\nmetrics for action-driven video generation, achieving an FVD score of 514\ncompared to 782, and maintaining consistency in longer videos, with an average\nof 0.37 scene cuts versus 0.89. The slow-fast learning loop algorithm\nsignificantly enhances performances on long-horizon planning tasks as well.\nProject Website: https://slowfast-vgen.github.io",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.23277v2",
    "published_date": "2024-10-30 17:55:52 UTC",
    "updated_date": "2024-10-31 18:03:51 UTC"
  },
  {
    "arxiv_id": "2410.23274v2",
    "title": "Multi-student Diffusion Distillation for Better One-step Generators",
    "authors": [
      "Yanke Song",
      "Jonathan Lorraine",
      "Weili Nie",
      "Karsten Kreis",
      "James Lucas"
    ],
    "abstract": "Diffusion models achieve high-quality sample generation at the cost of a\nlengthy multistep inference procedure. To overcome this, diffusion distillation\ntechniques produce student generators capable of matching or surpassing the\nteacher in a single step. However, the student model's inference speed is\nlimited by the size of the teacher architecture, preventing real-time\ngeneration for computationally heavy applications. In this work, we introduce\nMulti-Student Distillation (MSD), a framework to distill a conditional teacher\ndiffusion model into multiple single-step generators. Each student generator is\nresponsible for a subset of the conditioning data, thereby obtaining higher\ngeneration quality for the same capacity. MSD trains multiple distilled\nstudents, allowing smaller sizes and, therefore, faster inference. Also, MSD\noffers a lightweight quality boost over single-student distillation with the\nsame architecture. We demonstrate MSD is effective by training multiple\nsame-sized or smaller students on single-step distillation using distribution\nmatching and adversarial distillation techniques. With smaller students, MSD\ngets competitive results with faster inference for single-step generation.\nUsing 4 same-sized students, MSD significantly outperforms single-student\nbaseline counterparts and achieves remarkable FID scores for one-step image\ngeneration: 1.20 on ImageNet-64x64 and 8.20 on zero-shot COCO2014.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Project page: https://research.nvidia.com/labs/toronto-ai/MSD/",
    "pdf_url": "http://arxiv.org/pdf/2410.23274v2",
    "published_date": "2024-10-30 17:54:56 UTC",
    "updated_date": "2024-12-03 00:28:32 UTC"
  },
  {
    "arxiv_id": "2410.23273v1",
    "title": "Proportional Fairness in Non-Centroid Clustering",
    "authors": [
      "Ioannis Caragiannis",
      "Evi Micha",
      "Nisarg Shah"
    ],
    "abstract": "We revisit the recently developed framework of proportionally fair\nclustering, where the goal is to provide group fairness guarantees that become\nstronger for groups of data points (agents) that are large and cohesive. Prior\nwork applies this framework to centroid clustering, where the loss of an agent\nis its distance to the centroid assigned to its cluster. We expand the\nframework to non-centroid clustering, where the loss of an agent is a function\nof the other agents in its cluster, by adapting two proportional fairness\ncriteria -- the core and its relaxation, fully justified representation (FJR)\n-- to this setting.\n  We show that the core can be approximated only under structured loss\nfunctions, and even then, the best approximation we are able to establish,\nusing an adaptation of the GreedyCapture algorithm developed for centroid\nclustering [Chen et al., 2019; Micha and Shah, 2020], is unappealing for a\nnatural loss function. In contrast, we design a new (inefficient) algorithm,\nGreedyCohesiveClustering, which achieves the relaxation FJR exactly under\narbitrary loss functions, and show that the efficient GreedyCapture algorithm\nachieves a constant approximation of FJR. We also design an efficient auditing\nalgorithm, which estimates the FJR approximation of any given clustering\nsolution up to a constant factor. Our experiments on real data suggest that\ntraditional clustering algorithms are highly unfair, whereas GreedyCapture is\nconsiderably fairer and incurs only a modest loss in common clustering\nobjectives.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.GT"
    ],
    "primary_category": "cs.LG",
    "comment": "A preliminary version appeared at NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.23273v1",
    "published_date": "2024-10-30 17:53:49 UTC",
    "updated_date": "2024-10-30 17:53:49 UTC"
  },
  {
    "arxiv_id": "2410.23272v1",
    "title": "A Monte Carlo Framework for Calibrated Uncertainty Estimation in Sequence Prediction",
    "authors": [
      "Qidong Yang",
      "Weicheng Zhu",
      "Joseph Keslin",
      "Laure Zanna",
      "Tim G. J. Rudner",
      "Carlos Fernandez-Granda"
    ],
    "abstract": "Probabilistic prediction of sequences from images and other high-dimensional\ndata is a key challenge, particularly in risk-sensitive applications. In these\nsettings, it is often desirable to quantify the uncertainty associated with the\nprediction (instead of just determining the most likely sequence, as in\nlanguage modeling). In this paper, we propose a Monte Carlo framework to\nestimate probabilities and confidence intervals associated with the\ndistribution of a discrete sequence. Our framework uses a Monte Carlo\nsimulator, implemented as an autoregressively trained neural network, to sample\nsequences conditioned on an image input. We then use these samples to estimate\nthe probabilities and confidence intervals. Experiments on synthetic and real\ndata show that the framework produces accurate discriminative predictions, but\ncan suffer from miscalibration. In order to address this shortcoming, we\npropose a time-dependent regularization method, which is shown to produce\ncalibrated predictions.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.23272v1",
    "published_date": "2024-10-30 17:53:37 UTC",
    "updated_date": "2024-10-30 17:53:37 UTC"
  },
  {
    "arxiv_id": "2410.23330v1",
    "title": "CLIPErase: Efficient Unlearning of Visual-Textual Associations in CLIP",
    "authors": [
      "Tianyu Yang",
      "Lisen Dai",
      "Zheyuan Liu",
      "Xiangqi Wang",
      "Meng Jiang",
      "Yapeng Tian",
      "Xiangliang Zhang"
    ],
    "abstract": "Machine unlearning (MU) has gained significant attention as a means to remove\nspecific data from trained models without requiring a full retraining process.\nWhile progress has been made in unimodal domains like text and image\nclassification, unlearning in multimodal models remains relatively\nunderexplored. In this work, we address the unique challenges of unlearning in\nCLIP, a prominent multimodal model that aligns visual and textual\nrepresentations. We introduce CLIPErase, a novel approach that disentangles and\nselectively forgets both visual and textual associations, ensuring that\nunlearning does not compromise model performance. CLIPErase consists of three\nkey modules: a Forgetting Module that disrupts the associations in the forget\nset, a Retention Module that preserves performance on the retain set, and a\nConsistency Module that maintains consistency with the original model.\nExtensive experiments on the CIFAR-100 and Flickr30K datasets across four CLIP\ndownstream tasks demonstrate that CLIPErase effectively forgets designated\nassociations in zero-shot tasks for multimodal samples, while preserving the\nmodel's performance on the retain set after unlearning.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.23330v1",
    "published_date": "2024-10-30 17:51:31 UTC",
    "updated_date": "2024-10-30 17:51:31 UTC"
  },
  {
    "arxiv_id": "2410.23266v1",
    "title": "TOMATO: Assessing Visual Temporal Reasoning Capabilities in Multimodal Foundation Models",
    "authors": [
      "Ziyao Shangguan",
      "Chuhan Li",
      "Yuxuan Ding",
      "Yanan Zheng",
      "Yilun Zhao",
      "Tesca Fitzgerald",
      "Arman Cohan"
    ],
    "abstract": "Existing benchmarks often highlight the remarkable performance achieved by\nstate-of-the-art Multimodal Foundation Models (MFMs) in leveraging temporal\ncontext for video understanding. However, how well do the models truly perform\nvisual temporal reasoning? Our study of existing benchmarks shows that this\ncapability of MFMs is likely overestimated as many questions can be solved by\nusing a single, few, or out-of-order frames. To systematically examine current\nvisual temporal reasoning tasks, we propose three principles with corresponding\nmetrics: (1) Multi-Frame Gain, (2) Frame Order Sensitivity, and (3) Frame\nInformation Disparity. Following these principles, we introduce TOMATO,\nTemporal Reasoning Multimodal Evaluation, a novel benchmark crafted to\nrigorously assess MFMs' temporal reasoning capabilities in video understanding.\nTOMATO comprises 1,484 carefully curated, human-annotated questions spanning\nsix tasks (i.e., action count, direction, rotation, shape & trend, velocity &\nfrequency, and visual cues), applied to 1,417 videos, including 805\nself-recorded and -generated videos, that encompass human-centric, real-world,\nand simulated scenarios. Our comprehensive evaluation reveals a human-model\nperformance gap of 57.3% with the best-performing model. Moreover, our in-depth\nanalysis uncovers more fundamental limitations beyond this gap in current MFMs.\nWhile they can accurately recognize events in isolated frames, they fail to\ninterpret these frames as a continuous sequence. We believe TOMATO will serve\nas a crucial testbed for evaluating the next-generation MFMs and as a call to\nthe community to develop AI systems capable of comprehending human world\ndynamics through the video modality.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.23266v1",
    "published_date": "2024-10-30 17:50:23 UTC",
    "updated_date": "2024-10-30 17:50:23 UTC"
  },
  {
    "arxiv_id": "2410.23262v2",
    "title": "EMMA: End-to-End Multimodal Model for Autonomous Driving",
    "authors": [
      "Jyh-Jing Hwang",
      "Runsheng Xu",
      "Hubert Lin",
      "Wei-Chih Hung",
      "Jingwei Ji",
      "Kristy Choi",
      "Di Huang",
      "Tong He",
      "Paul Covington",
      "Benjamin Sapp",
      "Yin Zhou",
      "James Guo",
      "Dragomir Anguelov",
      "Mingxing Tan"
    ],
    "abstract": "We introduce EMMA, an End-to-end Multimodal Model for Autonomous driving.\nBuilt on a multi-modal large language model foundation, EMMA directly maps raw\ncamera sensor data into various driving-specific outputs, including planner\ntrajectories, perception objects, and road graph elements. EMMA maximizes the\nutility of world knowledge from the pre-trained large language models, by\nrepresenting all non-sensor inputs (e.g. navigation instructions and ego\nvehicle status) and outputs (e.g. trajectories and 3D locations) as natural\nlanguage text. This approach allows EMMA to jointly process various driving\ntasks in a unified language space, and generate the outputs for each task using\ntask-specific prompts. Empirically, we demonstrate EMMA's effectiveness by\nachieving state-of-the-art performance in motion planning on nuScenes as well\nas competitive results on the Waymo Open Motion Dataset (WOMD). EMMA also\nyields competitive results for camera-primary 3D object detection on the Waymo\nOpen Dataset (WOD). We show that co-training EMMA with planner trajectories,\nobject detection, and road graph tasks yields improvements across all three\ndomains, highlighting EMMA's potential as a generalist model for autonomous\ndriving applications. However, EMMA also exhibits certain limitations: it can\nprocess only a small amount of image frames, does not incorporate accurate 3D\nsensing modalities like LiDAR or radar and is computationally expensive. We\nhope that our results will inspire further research to mitigate these issues\nand to further evolve the state of the art in autonomous driving model\narchitectures.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Blog post: https://waymo.com/blog/2024/10/introducing-emma/",
    "pdf_url": "http://arxiv.org/pdf/2410.23262v2",
    "published_date": "2024-10-30 17:46:31 UTC",
    "updated_date": "2024-11-04 18:44:20 UTC"
  },
  {
    "arxiv_id": "2410.23254v1",
    "title": "Keypoint Abstraction using Large Models for Object-Relative Imitation Learning",
    "authors": [
      "Xiaolin Fang",
      "Bo-Ruei Huang",
      "Jiayuan Mao",
      "Jasmine Shone",
      "Joshua B. Tenenbaum",
      "Tomás Lozano-Pérez",
      "Leslie Pack Kaelbling"
    ],
    "abstract": "Generalization to novel object configurations and instances across diverse\ntasks and environments is a critical challenge in robotics. Keypoint-based\nrepresentations have been proven effective as a succinct representation for\ncapturing essential object features, and for establishing a reference frame in\naction prediction, enabling data-efficient learning of robot skills. However,\ntheir manual design nature and reliance on additional human labels limit their\nscalability. In this paper, we propose KALM, a framework that leverages large\npre-trained vision-language models (LMs) to automatically generate\ntask-relevant and cross-instance consistent keypoints. KALM distills robust and\nconsistent keypoints across views and objects by generating proposals using LMs\nand verifies them against a small set of robot demonstration data. Based on the\ngenerated keypoints, we can train keypoint-conditioned policy models that\npredict actions in keypoint-centric frames, enabling robots to generalize\neffectively across varying object poses, camera views, and object instances\nwith similar functional shapes. Our method demonstrates strong performance in\nthe real world, adapting to different tasks and environments from only a\nhandful of demonstrations while requiring no additional labels. Website:\nhttps://kalm-il.github.io/",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "CoRL LangRob Workshop, 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.23254v1",
    "published_date": "2024-10-30 17:37:31 UTC",
    "updated_date": "2024-10-30 17:37:31 UTC"
  },
  {
    "arxiv_id": "2410.23242v2",
    "title": "A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment",
    "authors": [
      "Matteo G. Mecattaf",
      "Ben Slater",
      "Marko Tešić",
      "Jonathan Prunty",
      "Konstantinos Voudouris",
      "Lucy G. Cheke"
    ],
    "abstract": "As general-purpose tools, Large Language Models (LLMs) must often reason\nabout everyday physical environments. In a question-and-answer capacity,\nunderstanding the interactions of physical objects may be necessary to give\nappropriate responses. Moreover, LLMs are increasingly used as reasoning\nengines in agentic systems, designing and controlling their action sequences.\nThe vast majority of research has tackled this issue using static benchmarks,\ncomprised of text or image-based questions about the physical world. However,\nthese benchmarks do not capture the complexity and nuance of real-life physical\nprocesses. Here we advocate for a second, relatively unexplored, approach:\n'embodying' the LLMs by granting them control of an agent within a 3D\nenvironment. We present the first embodied and cognitively meaningful\nevaluation of physical common-sense reasoning in LLMs. Our framework allows\ndirect comparison of LLMs with other embodied agents, such as those based on\nDeep Reinforcement Learning, and human and non-human animals. We employ the\nAnimal-AI (AAI) environment, a simulated 3D virtual laboratory, to study\nphysical common-sense reasoning in LLMs. For this, we use the AAI Testbed, a\nsuite of experiments that replicate laboratory studies with non-human animals,\nto study physical reasoning capabilities including distance estimation,\ntracking out-of-sight objects, and tool use. We demonstrate that\nstate-of-the-art multi-modal models with no finetuning can complete this style\nof task, allowing meaningful comparison to the entrants of the 2019 Animal-AI\nOlympics competition and to human children. Our results show that LLMs are\ncurrently outperformed by human children on these tasks. We argue that this\napproach allows the study of physical reasoning using ecologically valid\nexperiments drawn directly from cognitive science, improving the predictability\nand reliability of LLMs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "25 pages, 4 figures; v2: Added AFMR Acknowledgment",
    "pdf_url": "http://arxiv.org/pdf/2410.23242v2",
    "published_date": "2024-10-30 17:28:28 UTC",
    "updated_date": "2025-01-03 11:29:35 UTC"
  },
  {
    "arxiv_id": "2411.00862v1",
    "title": "A Simple and Effective Temporal Grounding Pipeline for Basketball Broadcast Footage",
    "authors": [
      "Levi Harris"
    ],
    "abstract": "We present a reliable temporal grounding pipeline for video-to-analytic\nalignment of basketball broadcast footage. Given a series of frames as input,\nour method quickly and accurately extracts time-remaining and quarter values\nfrom basketball broadcast scenes. Our work intends to expedite the development\nof large, multi-modal video datasets to train data-hungry video models in the\nsports action recognition domain. Our method aligns a pre-labeled corpus of\nplay-by-play annotations containing dense event annotations to video frames,\nenabling quick retrieval of labeled video segments. Unlike previous methods, we\nforgo the need to localize game clocks by fine-tuning an out-of-the-box object\ndetector to find semantic text regions directly. Our end-to-end approach\nimproves the generality of our work. Additionally, interpolation and\nparallelization techniques prepare our pipeline for deployment in a large\ncomputing cluster. All code is made publicly available.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.00862v1",
    "published_date": "2024-10-30 17:27:44 UTC",
    "updated_date": "2024-10-30 17:27:44 UTC"
  },
  {
    "arxiv_id": "2410.23234v1",
    "title": "EMOTION: Expressive Motion Sequence Generation for Humanoid Robots with In-Context Learning",
    "authors": [
      "Peide Huang",
      "Yuhan Hu",
      "Nataliya Nechyporenko",
      "Daehwa Kim",
      "Walter Talbott",
      "Jian Zhang"
    ],
    "abstract": "This paper introduces a framework, called EMOTION, for generating expressive\nmotion sequences in humanoid robots, enhancing their ability to engage in\nhumanlike non-verbal communication. Non-verbal cues such as facial expressions,\ngestures, and body movements play a crucial role in effective interpersonal\ninteractions. Despite the advancements in robotic behaviors, existing methods\noften fall short in mimicking the diversity and subtlety of human non-verbal\ncommunication. To address this gap, our approach leverages the in-context\nlearning capability of large language models (LLMs) to dynamically generate\nsocially appropriate gesture motion sequences for human-robot interaction. We\nuse this framework to generate 10 different expressive gestures and conduct\nonline user studies comparing the naturalness and understandability of the\nmotions generated by EMOTION and its human-feedback version, EMOTION++, against\nthose by human operators. The results demonstrate that our approach either\nmatches or surpasses human performance in generating understandable and natural\nrobot motions under certain scenarios. We also provide design implications for\nfuture research to consider a set of variables when generating expressive\nrobotic gestures.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.23234v1",
    "published_date": "2024-10-30 17:22:45 UTC",
    "updated_date": "2024-10-30 17:22:45 UTC"
  },
  {
    "arxiv_id": "2410.23230v2",
    "title": "Aligning Audio-Visual Joint Representations with an Agentic Workflow",
    "authors": [
      "Shentong Mo",
      "Yibing Song"
    ],
    "abstract": "Visual content and accompanied audio signals naturally formulate a joint\nrepresentation to improve audio-visual (AV) related applications. While studies\ndevelop various AV representation learning frameworks, the importance of AV\ndata alignment is usually undermined for achieving high-quality representation.\nWe observe that an audio signal may contain background noise interference.\nAlso, non-synchronization may appear between audio and video streams. These\nnon-strict data alignment limits representation quality and downgrade\napplication performance. In this paper, we propose to improve AV joint\nrepresentations from a data-centric perspective by aligning audio signals to\nvisual data. Our alignment is conducted in an agentic workflow controlled by an\nLLM-based assistant named AVAgent. For each input AV data pair, our AVAgent\nuses a multi-modal LLM to convert audio and visual data into language\ndescriptions separately (i.e., tool use). Then, AVAgent reasons whether this\npaired data is aligned well and plans to edit the audio signal if needed (i.e.,\nplanning). The audio editing is executed by predefined actions that filter\nnoise or augment data. Moreover, we use a VLM to evaluate how modified audio\nsignals match the visual content and provide feedback to AVAgent (i.e.,\nreflection). The tool use, planning, and reflection steps operate cyclically to\nbecome an agentic workflow where audio signals are gradually aligned to visual\ncontent. To this end, existing methods can directly leverage the aligned AV\ndata via our agentic workflow to improve AV joint representations. The\nexperimental results comprehensively demonstrate the state-of-the-art\nperformance of the proposed approach against previous baselines in diverse\ndownstream tasks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.23230v2",
    "published_date": "2024-10-30 17:18:53 UTC",
    "updated_date": "2024-10-31 04:20:22 UTC"
  },
  {
    "arxiv_id": "2410.23223v1",
    "title": "COMAL: A Convergent Meta-Algorithm for Aligning LLMs with General Preferences",
    "authors": [
      "Yixin Liu",
      "Argyris Oikonomou",
      "Weiqiang Zheng",
      "Yang Cai",
      "Arman Cohan"
    ],
    "abstract": "Many alignment methods, including reinforcement learning from human feedback\n(RLHF), rely on the Bradley-Terry reward assumption, which is insufficient to\ncapture the full range of general human preferences. To achieve robust\nalignment with general preferences, we model the alignment problem as a\ntwo-player zero-sum game, where the Nash equilibrium policy guarantees a 50%\nwin rate against any competing policy. However, previous algorithms for finding\nthe Nash policy either diverge or converge to a Nash policy in a modified game,\neven in a simple synthetic setting, thereby failing to maintain the 50% win\nrate guarantee against all other policies. We propose a meta-algorithm,\nConvergent Meta Alignment Algorithm (COMAL), for language model alignment with\ngeneral preferences, inspired by convergent algorithms in game theory.\nTheoretically, we prove that our meta-algorithm converges to an exact Nash\npolicy in the last iterate. Additionally, our meta-algorithm is simple and can\nbe integrated with many existing methods designed for RLHF and preference\noptimization with minimal changes. Experimental results demonstrate the\neffectiveness of the proposed framework when combined with existing preference\npolicy optimization methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.GT"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.23223v1",
    "published_date": "2024-10-30 17:13:02 UTC",
    "updated_date": "2024-10-30 17:13:02 UTC"
  },
  {
    "arxiv_id": "2410.23222v1",
    "title": "Partial Channel Dependence with Channel Masks for Time Series Foundation Models",
    "authors": [
      "Seunghan Lee",
      "Taeyoung Park",
      "Kibok Lee"
    ],
    "abstract": "Recent advancements in foundation models have been successfully extended to\nthe time series (TS) domain, facilitated by the emergence of large-scale TS\ndatasets. However, previous efforts have primarily focused on designing model\narchitectures to address explicit heterogeneity among datasets such as various\nnumbers of channels, while often overlooking implicit heterogeneity such as\nvarying dependencies between channels. In this work, we introduce the concept\nof partial channel dependence (PCD), which enables a more sophisticated\nadjustment of channel dependencies based on dataset-specific information. To\nachieve PCD, we propose a channel mask that captures the relationships between\nchannels within a dataset using two key components: 1) a correlation matrix\nthat encodes relative dependencies between channels, and 2) domain parameters\nthat learn the absolute dependencies specific to each dataset, refining the\ncorrelation matrix. We validate the effectiveness of PCD across four tasks in\nTS including forecasting, classification, imputation, and anomaly detection,\nunder diverse settings, including few-shot and zero-shot scenarios with both TS\nfoundation models and single-task models. Code is available at\nhttps://github.com/seunghan96/CM.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS Workshop on Time Series in the Age of Large Models, 2024.\n  Oral presentation",
    "pdf_url": "http://arxiv.org/pdf/2410.23222v1",
    "published_date": "2024-10-30 17:12:03 UTC",
    "updated_date": "2024-10-30 17:12:03 UTC"
  },
  {
    "arxiv_id": "2411.09706v1",
    "title": "AI-Driven Feedback Loops in Digital Technologies: Psychological Impacts on User Behaviour and Well-Being",
    "authors": [
      "Anthonette Adanyin"
    ],
    "abstract": "The rapid spread of digital technologies has produced data-driven feedback\nloops, wearable devices, social media networks, and mobile applications that\nshape user behavior, motivation, and mental well-being. While these systems\nencourage self-improvement and the development of healthier habits through\nreal-time feedback, they also create psychological risks such as technostress,\naddiction, and loss of autonomy. The present study also aims to investigate the\npositive and negative psychological consequences of feedback mechanisms on\nusers' behaviour and well-being. Employing a descriptive survey method, the\nstudy collected data from 200 purposely selected users to assess changes in\nbehaviour, motivation, and mental well-being related to health, social, and\nlifestyle applications. Results indicate that while feedback mechanisms\nfacilitate goal attainment and social interconnection through streaks and\nbadges, among other components, they also enhance anxiety, mental weariness,\nand loss of productivity due to actions that are considered feedback-seeking.\nFurthermore, test subjects reported that their actions are unconsciously shaped\nby app feedback, often at the expense of personal autonomy, while real-time\nfeedback minimally influences professional or social interactions. The study\nshows that data-driven feedback loops deliver not only motivational benefits\nbut also psychological challenges. To mitigate these risks, users should\nestablish boundaries regarding their use of technology to prevent burnout and\naddiction, while developers need to refine feedback mechanisms to reduce\ncognitive load and foster more inclusive participation. Future research should\nfocus on designing feedback mechanisms that promote well-being without\ncompromising individual freedom or increasing social comparison.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09706v1",
    "published_date": "2024-10-30 17:11:30 UTC",
    "updated_date": "2024-10-30 17:11:30 UTC"
  },
  {
    "arxiv_id": "2410.23219v1",
    "title": "DiaMond: Dementia Diagnosis with Multi-Modal Vision Transformers Using MRI and PET",
    "authors": [
      "Yitong Li",
      "Morteza Ghahremani",
      "Youssef Wally",
      "Christian Wachinger"
    ],
    "abstract": "Diagnosing dementia, particularly for Alzheimer's Disease (AD) and\nfrontotemporal dementia (FTD), is complex due to overlapping symptoms. While\nmagnetic resonance imaging (MRI) and positron emission tomography (PET) data\nare critical for the diagnosis, integrating these modalities in deep learning\nfaces challenges, often resulting in suboptimal performance compared to using\nsingle modalities. Moreover, the potential of multi-modal approaches in\ndifferential diagnosis, which holds significant clinical importance, remains\nlargely unexplored. We propose a novel framework, DiaMond, to address these\nissues with vision Transformers to effectively integrate MRI and PET. DiaMond\nis equipped with self-attention and a novel bi-attention mechanism that\nsynergistically combine MRI and PET, alongside a multi-modal normalization to\nreduce redundant dependency, thereby boosting the performance. DiaMond\nsignificantly outperforms existing multi-modal methods across various datasets,\nachieving a balanced accuracy of 92.4% in AD diagnosis, 65.2% for AD-MCI-CN\nclassification, and 76.5% in differential diagnosis of AD and FTD. We also\nvalidated the robustness of DiaMond in a comprehensive ablation study. The code\nis available at https://github.com/ai-med/DiaMond.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by IEEE/CVF Winter Conference on Applications of Computer\n  Vision (WACV) 2025",
    "pdf_url": "http://arxiv.org/pdf/2410.23219v1",
    "published_date": "2024-10-30 17:11:00 UTC",
    "updated_date": "2024-10-30 17:11:00 UTC"
  },
  {
    "arxiv_id": "2410.23214v2",
    "title": "Grounding by Trying: LLMs with Reinforcement Learning-Enhanced Retrieval",
    "authors": [
      "Sheryl Hsu",
      "Omar Khattab",
      "Chelsea Finn",
      "Archit Sharma"
    ],
    "abstract": "The hallucinations of large language models (LLMs) are increasingly mitigated\nby allowing LLMs to search for information and to ground their answers in real\nsources. Unfortunately, LLMs often struggle with posing the right search\nqueries, especially when dealing with complex or otherwise indirect topics.\nObserving that LLMs can learn to search for relevant facts by $\\textit{trying}$\ndifferent queries and learning to up-weight queries that successfully produce\nrelevant results, we introduce $\\underline{Le}$arning to $\\underline{Re}$trieve\nby $\\underline{T}$rying (LeReT), a reinforcement learning framework that\nexplores search queries and uses preference-based optimization to improve their\nquality. LeReT can improve the absolute retrieval accuracy by up to 29% and the\ndownstream generator evaluations by 17%. The simplicity and flexibility of\nLeReT allows it to be applied to arbitrary off-the-shelf retrievers and makes\nit a promising technique for improving general LLM pipelines. Project website:\nhttp://sherylhsu.com/LeReT/.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.23214v2",
    "published_date": "2024-10-30 17:02:54 UTC",
    "updated_date": "2024-10-31 01:34:16 UTC"
  },
  {
    "arxiv_id": "2410.23208v2",
    "title": "Kinetix: Investigating the Training of General Agents through Open-Ended Physics-Based Control Tasks",
    "authors": [
      "Michael Matthews",
      "Michael Beukman",
      "Chris Lu",
      "Jakob Foerster"
    ],
    "abstract": "While large models trained with self-supervised learning on offline datasets\nhave shown remarkable capabilities in text and image domains, achieving the\nsame generalisation for agents that act in sequential decision problems remains\nan open challenge. In this work, we take a step towards this goal by\nprocedurally generating tens of millions of 2D physics-based tasks and using\nthese to train a general reinforcement learning (RL) agent for physical\ncontrol. To this end, we introduce Kinetix: an open-ended space of\nphysics-based RL environments that can represent tasks ranging from robotic\nlocomotion and grasping to video games and classic RL environments, all within\na unified framework. Kinetix makes use of our novel hardware-accelerated\nphysics engine Jax2D that allows us to cheaply simulate billions of environment\nsteps during training. Our trained agent exhibits strong physical reasoning\ncapabilities in 2D space, being able to zero-shot solve unseen human-designed\nenvironments. Furthermore, fine-tuning this general agent on tasks of interest\nshows significantly stronger performance than training an RL agent *tabula\nrasa*. This includes solving some environments that standard RL training\ncompletely fails at. We believe this demonstrates the feasibility of large\nscale, mixed-quality pre-training for online RL and we hope that Kinetix will\nserve as a useful framework to investigate this further.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR 2025 Oral. The first two authors contributed equally. Project\n  page located at: https://kinetix-env.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2410.23208v2",
    "published_date": "2024-10-30 16:59:41 UTC",
    "updated_date": "2025-03-03 14:29:16 UTC"
  },
  {
    "arxiv_id": "2410.23180v1",
    "title": "ReasoningRec: Bridging Personalized Recommendations and Human-Interpretable Explanations through LLM Reasoning",
    "authors": [
      "Millennium Bismay",
      "Xiangjue Dong",
      "James Caverlee"
    ],
    "abstract": "This paper presents ReasoningRec, a reasoning-based recommendation framework\nthat leverages Large Language Models (LLMs) to bridge the gap between\nrecommendations and human-interpretable explanations. In contrast to\nconventional recommendation systems that rely on implicit user-item\ninteractions, ReasoningRec employs LLMs to model users and items, focusing on\npreferences, aversions, and explanatory reasoning. The framework utilizes a\nlarger LLM to generate synthetic explanations for user preferences,\nsubsequently used to fine-tune a smaller LLM for enhanced recommendation\naccuracy and human-interpretable explanation. Our experimental study\ninvestigates the impact of reasoning and contextual information on personalized\nrecommendations, revealing that the quality of contextual and personalized data\nsignificantly influences the LLM's capacity to generate plausible explanations.\nEmpirical evaluations demonstrate that ReasoningRec surpasses state-of-the-art\nmethods by up to 12.5\\% in recommendation prediction while concurrently\nproviding human-intelligible explanations. The code is available here:\nhttps://github.com/millenniumbismay/reasoningrec.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "Large Language Model, Recommendation, Human-Interpretable Reasoning,\n  Personalization",
    "pdf_url": "http://arxiv.org/pdf/2410.23180v1",
    "published_date": "2024-10-30 16:37:04 UTC",
    "updated_date": "2024-10-30 16:37:04 UTC"
  },
  {
    "arxiv_id": "2410.23329v1",
    "title": "Variable Resolution Sampling and Deep Learning Image Recovery for Accelerated Multi-Spectral MRI Near Metal Implants",
    "authors": [
      "Azadeh Sharafi",
      "Nikolai J. Mickevicius",
      "Mehran Baboli",
      "Andrew S. Nencka",
      "Kevin M. Koch"
    ],
    "abstract": "Purpose: This study presents a variable resolution (VR) sampling and deep\nlearning reconstruction approach for multi-spectral MRI near metal implants,\naiming to reduce scan times while maintaining image quality. Background: The\nrising use of metal implants has increased MRI scans affected by metal\nartifacts. Multi-spectral imaging (MSI) reduces these artifacts but sacrifices\nacquisition efficiency. Methods: This retrospective study on 1.5T MSI knee and\nhip data from patients with metal hardware used a novel spectral undersampling\nscheme to improve acquisition efficiency by ~40%. U-Net-based deep learning\nmodels were trained for reconstruction. Image quality was evaluated using SSIM,\nPSNR, and RESI metrics. Results: Deep learning reconstructions of undersampled\nVR data (DL-VR) showed significantly higher SSIM and PSNR values (p<0.001)\ncompared to conventional reconstruction (CR-VR), with improved edge sharpness.\nEdge sharpness in DL-reconstructed images matched fully sampled references\n(p=0.5). Conclusion: This approach can potentially enhance MRI examinations\nnear metal implants by reducing scan times or enabling higher resolution.\nFurther prospective studies are needed to assess clinical value.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "physics.med-ph"
    ],
    "primary_category": "eess.IV",
    "comment": "10 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.23329v1",
    "published_date": "2024-10-30 16:19:06 UTC",
    "updated_date": "2024-10-30 16:19:06 UTC"
  },
  {
    "arxiv_id": "2410.23166v2",
    "title": "SciPIP: An LLM-based Scientific Paper Idea Proposer",
    "authors": [
      "Wenxiao Wang",
      "Lihui Gu",
      "Liye Zhang",
      "Yunxiang Luo",
      "Yi Dai",
      "Chen Shen",
      "Liang Xie",
      "Binbin Lin",
      "Xiaofei He",
      "Jieping Ye"
    ],
    "abstract": "The rapid advancement of large language models (LLMs) has opened new\npossibilities for automating the proposal of innovative scientific ideas. This\nprocess involves two key phases: literature retrieval and idea generation.\nHowever, existing approaches often fall short due to their reliance on\nkeyword-based search tools during the retrieval phase, which neglects crucial\nsemantic information and frequently results in incomplete retrieval outcomes.\nSimilarly, in the idea generation phase, current methodologies tend to depend\nsolely on the internal knowledge of LLMs or metadata from retrieved papers,\nthereby overlooking significant valuable insights contained within the full\ntexts. To address these limitations, we introduce SciPIP, an innovative\nframework designed to enhance the LLM-based proposal of scientific ideas\nthrough improvements in both literature retrieval and idea generation. Our\napproach begins with the construction of a comprehensive literature database\nthat supports advanced retrieval based not only on keywords but also on\nsemantics and citation relationships. This is complemented by the introduction\nof a multi-granularity retrieval algorithm aimed at ensuring more thorough and\nexhaustive retrieval results. For the idea generation phase, we propose a\ndual-path framework that effectively integrates both the content of retrieved\npapers and the extensive internal knowledge of LLMs. This integration\nsignificantly boosts the novelty, feasibility, and practical value of proposed\nideas. Our experiments, conducted across various domains such as natural\nlanguage processing and computer vision, demonstrate SciPIP's capability to\ngenerate a multitude of innovative and useful ideas. These findings underscore\nSciPIP's potential as a valuable tool for researchers seeking to advance their\nfields with groundbreaking concepts.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "20 pages, 5 figures, 12 tables. The code has been availabel:\n  https://github.com/cheerss/SciPIP",
    "pdf_url": "http://arxiv.org/pdf/2410.23166v2",
    "published_date": "2024-10-30 16:18:22 UTC",
    "updated_date": "2025-02-17 08:59:45 UTC"
  },
  {
    "arxiv_id": "2410.23160v1",
    "title": "FlexTSF: A Universal Forecasting Model for Time Series with Variable Regularities",
    "authors": [
      "Jingge Xiao",
      "Yile Chen",
      "Gao Cong",
      "Wolfgang Nejdl",
      "Simon Gottschalk"
    ],
    "abstract": "Developing a foundation model for time series forecasting across diverse\ndomains has attracted significant attention in recent years. Existing works\ntypically assume regularly sampled, well-structured data, limiting their\napplicability to more generalized scenarios where time series often contain\nmissing values, unequal sequence lengths, and irregular time intervals between\nmeasurements. To cover diverse domains and handle variable regularities, we\npropose FlexTSF, a universal time series forecasting model that possesses\nbetter generalization and natively support both regular and irregular time\nseries. FlexTSF produces forecasts in an autoregressive manner and incorporates\nthree novel designs: VT-Norm, a normalization strategy to ablate data domain\nbarriers, IVP Patcher, a patching module to learn representations from flexibly\nstructured time series, and LED attention, an attention mechanism to seamlessly\nintegrate these two and propagate forecasts with awareness of domain and time\ninformation. Experiments on 12 datasets show that FlexTSF outperforms\nstate-of-the-art forecasting models respectively designed for regular and\nirregular time series. Furthermore, after self-supervised pre-training, FlexTSF\nshows exceptional performance in both zero-shot and few-show settings for time\nseries forecasting.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.23160v1",
    "published_date": "2024-10-30 16:14:09 UTC",
    "updated_date": "2024-10-30 16:14:09 UTC"
  },
  {
    "arxiv_id": "2410.23159v1",
    "title": "Fourier Amplitude and Correlation Loss: Beyond Using L2 Loss for Skillful Precipitation Nowcasting",
    "authors": [
      "Chiu-Wai Yan",
      "Shi Quan Foo",
      "Van Hoan Trinh",
      "Dit-Yan Yeung",
      "Ka-Hing Wong",
      "Wai-Kin Wong"
    ],
    "abstract": "Deep learning approaches have been widely adopted for precipitation\nnowcasting in recent years. Previous studies mainly focus on proposing new\nmodel architectures to improve pixel-wise metrics. However, they frequently\nresult in blurry predictions which provide limited utility to forecasting\noperations. In this work, we propose a new Fourier Amplitude and Correlation\nLoss (FACL) which consists of two novel loss terms: Fourier Amplitude Loss\n(FAL) and Fourier Correlation Loss (FCL). FAL regularizes the Fourier amplitude\nof the model prediction and FCL complements the missing phase information. The\ntwo loss terms work together to replace the traditional $L_2$ losses such as\nMSE and weighted MSE for the spatiotemporal prediction problem on signal-based\ndata. Our method is generic, parameter-free and efficient. Extensive\nexperiments using one synthetic dataset and three radar echo datasets\ndemonstrate that our method improves perceptual metrics and meteorology skill\nscores, with a small trade-off to pixel-wise accuracy and structural\nsimilarity. Moreover, to improve the error margin in meteorological skill\nscores such as Critical Success Index (CSI) and Fractions Skill Score (FSS), we\npropose and adopt the Regional Histogram Divergence (RHD), a distance metric\nthat considers the patch-wise similarity between signal-based imagery patterns\nwith tolerance to local transforms. Code is available at\nhttps://github.com/argenycw/FACL",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by NeurIPS 2024. Camera-ready submission",
    "pdf_url": "http://arxiv.org/pdf/2410.23159v1",
    "published_date": "2024-10-30 16:12:56 UTC",
    "updated_date": "2024-10-30 16:12:56 UTC"
  },
  {
    "arxiv_id": "2410.23156v2",
    "title": "VisualPredicator: Learning Abstract World Models with Neuro-Symbolic Predicates for Robot Planning",
    "authors": [
      "Yichao Liang",
      "Nishanth Kumar",
      "Hao Tang",
      "Adrian Weller",
      "Joshua B. Tenenbaum",
      "Tom Silver",
      "João F. Henriques",
      "Kevin Ellis"
    ],
    "abstract": "Broadly intelligent agents should form task-specific abstractions that\nselectively expose the essential elements of a task, while abstracting away the\ncomplexity of the raw sensorimotor space. In this work, we present\nNeuro-Symbolic Predicates, a first-order abstraction language that combines the\nstrengths of symbolic and neural knowledge representations. We outline an\nonline algorithm for inventing such predicates and learning abstract world\nmodels. We compare our approach to hierarchical reinforcement learning,\nvision-language model planning, and symbolic predicate invention approaches, on\nboth in- and out-of-distribution tasks across five simulated robotic domains.\nResults show that our approach offers better sample complexity, stronger\nout-of-distribution generalization, and improved interpretability.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "ICLR 2025 (Spotlight)",
    "pdf_url": "http://arxiv.org/pdf/2410.23156v2",
    "published_date": "2024-10-30 16:11:05 UTC",
    "updated_date": "2025-02-28 19:50:42 UTC"
  },
  {
    "arxiv_id": "2411.00859v1",
    "title": "Profiling AI Models: Towards Efficient Computation Offloading in Heterogeneous Edge AI Systems",
    "authors": [
      "Juan Marcelo Parra-Ullauri",
      "Oscar Dilley",
      "Hari Madhukumar",
      "Dimitra Simeonidou"
    ],
    "abstract": "The rapid growth of end-user AI applications, such as computer vision and\ngenerative AI, has led to immense data and processing demands often exceeding\nuser devices' capabilities. Edge AI addresses this by offloading computation to\nthe network edge, crucial for future services in 6G networks. However, it faces\nchallenges such as limited resources during simultaneous offloads and the\nunrealistic assumption of homogeneous system architecture. To address these, we\npropose a research roadmap focused on profiling AI models, capturing data about\nmodel types, hyperparameters, and underlying hardware to predict resource\nutilisation and task completion time. Initial experiments with over 3,000 runs\nshow promise in optimising resource allocation and enhancing Edge AI\nperformance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "cs.ET",
      "cs.NI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.00859v1",
    "published_date": "2024-10-30 16:07:14 UTC",
    "updated_date": "2024-10-30 16:07:14 UTC"
  },
  {
    "arxiv_id": "2411.00858v1",
    "title": "DiabML: AI-assisted diabetes diagnosis method with meta-heuristic-based feature selection",
    "authors": [
      "Vahideh Hayyolalam",
      "Öznur Özkasap"
    ],
    "abstract": "Diabetes is a chronic disorder identified by the high sugar level in the\nblood that can cause various different disorders such as kidney failure, heart\nattack, sightlessness, and stroke. Developments in the healthcare domain by\nfacilitating the early detection of diabetes risk can help not only caregivers\nbut also patients. AIoMT is a recent technology that integrates IoT and machine\nlearning methods to give services for medical purposes, which is a powerful\ntechnology for the early detection of diabetes. In this paper, we take\nadvantage of AIoMT and propose a hybrid diabetes risk detection method, DiabML,\nwhich uses the BWO algorithm and ML methods. BWO is utilized for feature\nselection and SMOTE for imbalance handling in the pre-processing procedure. The\nsimulation results prove the superiority of the proposed DiabML method compared\nto the existing works. DiabML achieves 86.1\\% classification accuracy by\nAdaBoost classifier outperforms the relevant existing methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "14J60 (Primary) 14F05, 14J26 (Secondary)"
    ],
    "primary_category": "cs.LG",
    "comment": "Proceedings of 14th Turkish Congress of Medical Informatics 16 (18),\n  19-30; https://turkmia.net/TurkMIA2023-Proceedings.pdf",
    "pdf_url": "http://arxiv.org/pdf/2411.00858v1",
    "published_date": "2024-10-30 16:06:58 UTC",
    "updated_date": "2024-10-30 16:06:58 UTC"
  },
  {
    "arxiv_id": "2410.23144v1",
    "title": "Public Domain 12M: A Highly Aesthetic Image-Text Dataset with Novel Governance Mechanisms",
    "authors": [
      "Jordan Meyer",
      "Nick Padgett",
      "Cullen Miller",
      "Laura Exline"
    ],
    "abstract": "We present Public Domain 12M (PD12M), a dataset of 12.4 million high-quality\npublic domain and CC0-licensed images with synthetic captions, designed for\ntraining text-to-image models. PD12M is the largest public domain image-text\ndataset to date, with sufficient size to train foundation models while\nminimizing copyright concerns. Through the Source.Plus platform, we also\nintroduce novel, community-driven dataset governance mechanisms that reduce\nharm and support reproducibility over time.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Project Page: https://source.plus/pd12m",
    "pdf_url": "http://arxiv.org/pdf/2410.23144v1",
    "published_date": "2024-10-30 15:59:05 UTC",
    "updated_date": "2024-10-30 15:59:05 UTC"
  },
  {
    "arxiv_id": "2410.23143v2",
    "title": "The Good, the Bad, and the Ugly: The Role of AI Quality Disclosure in Lie Detection",
    "authors": [
      "Haimanti Bhattacharya",
      "Subhasish Dugar",
      "Sanchaita Hazra",
      "Bodhisattwa Prasad Majumder"
    ],
    "abstract": "We investigate how low-quality AI advisors, lacking quality disclosures, can\nhelp spread text-based lies while seeming to help people detect lies.\nParticipants in our experiment discern truth from lies by evaluating\ntranscripts from a game show that mimicked deceptive social media exchanges on\ntopics with objective truths. We find that when relying on low-quality advisors\nwithout disclosures, participants' truth-detection rates fall below their own\nabilities, which recovered once the AI's true effectiveness was revealed.\nConversely, high-quality advisor enhances truth detection, regardless of\ndisclosure. We discover that participants' expectations about AI capabilities\ncontribute to their undue reliance on opaque, low-quality advisors.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Corresponding author: Sanchaita Hazra. Order of the authors are in\n  alphabetical order of their last names. All authors contributed equally. The\n  manuscript is under review. 74 Pages, including appendices and references",
    "pdf_url": "http://arxiv.org/pdf/2410.23143v2",
    "published_date": "2024-10-30 15:58:05 UTC",
    "updated_date": "2025-02-01 19:14:39 UTC"
  },
  {
    "arxiv_id": "2410.23137v1",
    "title": "Fair Division with Market Values",
    "authors": [
      "Siddharth Barman",
      "Soroush Ebadian",
      "Mohamad Latifian",
      "Nisarg Shah"
    ],
    "abstract": "We introduce a model of fair division with market values, where indivisible\ngoods must be partitioned among agents with (additive) subjective valuations,\nand each good additionally has a market value. The market valuation can be\nviewed as a separate additive valuation that holds identically across all the\nagents. We seek allocations that are simultaneously fair with respect to the\nsubjective valuations and with respect to the market valuation.\n  We show that an allocation that satisfies stochastically-dominant\nenvy-freeness up to one good (SD-EF1) with respect to both the subjective\nvaluations and the market valuation does not always exist, but the weaker\nguarantee of EF1 with respect to the subjective valuations along with SD-EF1\nwith respect to the market valuation can be guaranteed. We also study a number\nof other guarantees such as Pareto optimality, EFX, and MMS. In addition, we\nexplore non-additive valuations and extend our model to cake-cutting. Along the\nway, we identify several tantalizing open questions.",
    "categories": [
      "cs.GT",
      "cs.AI"
    ],
    "primary_category": "cs.GT",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.23137v1",
    "published_date": "2024-10-30 15:52:15 UTC",
    "updated_date": "2024-10-30 15:52:15 UTC"
  },
  {
    "arxiv_id": "2410.23132v3",
    "title": "Revisiting MAE pre-training for 3D medical image segmentation",
    "authors": [
      "Tassilo Wald",
      "Constantin Ulrich",
      "Stanislav Lukyanenko",
      "Andrei Goncharov",
      "Alberto Paderno",
      "Maximilian Miller",
      "Leander Maerkisch",
      "Paul F. Jäger",
      "Klaus Maier-Hein"
    ],
    "abstract": "Self-Supervised Learning (SSL) presents an exciting opportunity to unlock the\npotential of vast, untapped clinical datasets, for various downstream\napplications that suffer from the scarcity of labeled data. While SSL has\nrevolutionized fields like natural language processing and computer vision, its\nadoption in 3D medical image computing has been limited by three key pitfalls:\nSmall pre-training dataset sizes, architectures inadequate for 3D medical image\nanalysis, and insufficient evaluation practices. In this paper, we address\nthese issues by i) leveraging a large-scale dataset of 39k 3D brain MRI volumes\nand ii) using a Residual Encoder U-Net architecture within the state-of-the-art\nnnU-Net framework. iii) A robust development framework, incorporating 5\ndevelopment and 8 testing brain MRI segmentation datasets, allowed\nperformance-driven design decisions to optimize the simple concept of Masked\nAuto Encoders (MAEs) for 3D CNNs. The resulting model not only surpasses\nprevious SSL methods but also outperforms the strong nnU-Net baseline by an\naverage of approximately 3 Dice points setting a new state-of-the-art. Our code\nand models are made available here.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2025. Update to Camera-Ready",
    "pdf_url": "http://arxiv.org/pdf/2410.23132v3",
    "published_date": "2024-10-30 15:42:59 UTC",
    "updated_date": "2025-04-04 15:51:37 UTC"
  },
  {
    "arxiv_id": "2410.23126v2",
    "title": "Provably Optimal Memory Capacity for Modern Hopfield Models: Transformer-Compatible Dense Associative Memories as Spherical Codes",
    "authors": [
      "Jerry Yao-Chieh Hu",
      "Dennis Wu",
      "Han Liu"
    ],
    "abstract": "We study the optimal memorization capacity of modern Hopfield models and\nKernelized Hopfield Models (KHMs), a transformer-compatible class of Dense\nAssociative Memories. We present a tight analysis by establishing a connection\nbetween the memory configuration of KHMs and spherical codes from information\ntheory. Specifically, we treat the stored memory set as a specialized spherical\ncode. This enables us to cast the memorization problem in KHMs into a point\narrangement problem on a hypersphere. We show that the optimal capacity of KHMs\noccurs when the feature space allows memories to form an optimal spherical\ncode. This unique perspective leads to: (i) An analysis of how KHMs achieve\noptimal memory capacity, and identify corresponding necessary conditions.\nImportantly, we establish an upper capacity bound that matches the well-known\nexponential lower bound in the literature. This provides the first tight and\noptimal asymptotic memory capacity for modern Hopfield models. (ii) A\nsub-linear time algorithm $\\mathtt{U}\\text{-}\\mathtt{Hop}$+ to reach KHMs'\noptimal capacity. (iii) An analysis of the scaling behavior of the required\nfeature dimension relative to the number of stored memories. These efforts\nimprove both the retrieval capability of KHMs and the representation learning\nof corresponding transformers. Experimentally, we provide thorough numerical\nresults to back up theoretical findings.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "stat.ML",
    "comment": "Accepted at NeurIPS 2024. v2 fixed typos and expanded related work\n  discussion",
    "pdf_url": "http://arxiv.org/pdf/2410.23126v2",
    "published_date": "2024-10-30 15:35:51 UTC",
    "updated_date": "2024-10-31 16:02:34 UTC"
  },
  {
    "arxiv_id": "2410.23118v1",
    "title": "Teaching a Language Model to Distinguish Between Similar Details using a Small Adversarial Training Set",
    "authors": [
      "Chris Achard"
    ],
    "abstract": "Language models can achieve high accuracy on natural language tasks such as\nNLI, but performance suffers on manually created adversarial examples. We\ninvestigate the performance of a language model trained on the Stanford Natural\nLanguage Inference (SNLI) corpus on a manually created adversarial test set. We\nthen improve the model's performance by fine tuning the model on a small,\nmanually created adversarial training set, designed to help the language model\nto learn to differentiate between similar words and phrases in the data. We\nshow an increase in accuracy on the adversarial test set (+ 13%) while still\nmaintaining good performance on the original NLI task. We also show an increase\nin accuracy from 91.2% to 92.9% on the most similar contradictions in the SNLI\ntest set (as judged by cosine similarity).",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.23118v1",
    "published_date": "2024-10-30 15:27:55 UTC",
    "updated_date": "2024-10-30 15:27:55 UTC"
  },
  {
    "arxiv_id": "2410.23114v2",
    "title": "Unified Triplet-Level Hallucination Evaluation for Large Vision-Language Models",
    "authors": [
      "Junjie Wu",
      "Tsz Ting Chung",
      "Kai Chen",
      "Dit-Yan Yeung"
    ],
    "abstract": "Despite the outstanding performance in vision-language reasoning, Large\nVision-Language Models (LVLMs) might generate hallucinated contents that do not\nexist in the given image. Most existing LVLM hallucination benchmarks are\nconstrained to evaluate the object-related hallucinations. However, the\npotential hallucination on the relations between two objects, i.e., relation\nhallucination, still lacks investigation. To remedy that, in this paper we\ndesign a unified framework to measure object and relation hallucination in\nLVLMs simultaneously. The core idea of our framework is to conduct\nhallucination evaluation on (object, relation, object) triplets extracted from\nLVLMs' responses, and thus, could be easily generalized to different\nvision-language tasks. Based on our framework, we further introduce Tri-HE, a\nnovel Triplet-level Hallucination Evaluation benchmark which can be used to\nstudy both object and relation hallucination at the same time. We conduct\ncomprehensive evaluations on Tri-HE and observe that the relation hallucination\nissue is even more serious than object hallucination among existing LVLMs,\nhighlighting a previously neglected problem towards reliable LVLMs. Moreover,\nbased on our findings, we design a simple yet effective training-free approach\nto mitigate hallucinations for LVLMs, with which, we exceed all open-sourced\ncounterparts on Tri-HE, achieving comparable performance with the powerful\nGPT-4V. Our dataset and code for the reproduction of our experiments are\navailable publicly at https://github.com/wujunjie1998/Tri-HE.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page: https://kaichen1998.github.io/projects/tri-he/",
    "pdf_url": "http://arxiv.org/pdf/2410.23114v2",
    "published_date": "2024-10-30 15:25:06 UTC",
    "updated_date": "2024-11-03 09:35:12 UTC"
  },
  {
    "arxiv_id": "2410.23111v6",
    "title": "Exploring Gradient Subspaces: Addressing and Overcoming LoRA's Limitations in Federated Fine-Tuning of Large Language Models",
    "authors": [
      "Navyansh Mahla",
      "Kshitij Sharad Jadhav",
      "Ganesh Ramakrishnan"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains, particularly in task generalization for both text and vision\ndata. While fine-tuning these models can significantly enhance their\nperformance on specific downstream tasks, it often requires high-quality data\nthat cannot be shared due to privacy concerns. Federated Learning (FL) offers a\npromising solution for collaborative training without direct data sharing.\nHowever, many parameter-efficient fine-tuning strategies for LLMs in FL,\nparticularly those based on Low-Rank Adaptation (LoRA), face limitations. In\nthis paper, we critically analyze the convergence and performance guarantees of\npopular FL frameworks utilizing LoRA, highlighting its suboptimal nature due to\nconstrained subspace learning of low-rank matrices. This limitation hinders\neffective fine-tuning of LLMs in federated settings. Through rigorous\nanalytical and empirical evaluations, we demonstrate that direct weight\naveraging outperforms LoRA-based strategies, leading to superior performance\nfor fine-tuned models. Our comprehensive comparison unmasks inefficiencies in\nLoRA approaches and underscores the advantages of direct weight aggregation. We\nextend our analysis to low-rank gradient-based optimizers, such as GaLore, used\nduring local training steps. Our findings show that GaLore along with\ndirect-weight aggregation is a more effective approach, outperforming federated\nLoRA methods like FlexLoRA and FFA-LoRA across both text and image modalities.\nWhile privacy remains paramount in FL discourse, our focus is on assessing\nperformance outcomes of federated fine-tuned models and evaluating various FL\nframeworks from both theoretical and empirical perspectives. Our findings\nadvocate reassessing the reliance on LoRA within FL contexts, paving the way\nfor more efficient training methodologies.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.23111v6",
    "published_date": "2024-10-30 15:23:44 UTC",
    "updated_date": "2025-01-14 06:25:54 UTC"
  },
  {
    "arxiv_id": "2410.23108v1",
    "title": "Controllable Game Level Generation: Assessing the Effect of Negative Examples in GAN Models",
    "authors": [
      "Mahsa Bazzaz",
      "Seth Cooper"
    ],
    "abstract": "Generative Adversarial Networks (GANs) are unsupervised models designed to\nlearn and replicate a target distribution. The vanilla versions of these models\ncan be extended to more controllable models. Conditional Generative Adversarial\nNetworks (CGANs) extend vanilla GANs by conditioning both the generator and\ndiscriminator on some additional information (labels). Controllable models\nbased on complementary learning, such as Rumi-GAN, have been introduced.\nRumi-GANs leverage negative examples to enhance the generator's ability to\nlearn positive examples. We evaluate the performance of two controllable GAN\nvariants, CGAN and Rumi-GAN, in generating game levels targeting specific\nconstraints of interest: playability and controllability. This evaluation is\nconducted under two scenarios: with and without the inclusion of negative\nexamples. The goal is to determine whether incorporating negative examples\nhelps the GAN models avoid generating undesirable outputs. Our findings\nhighlight the strengths and weaknesses of each method in enforcing the\ngeneration of specific conditions when generating outputs based on given\npositive and negative examples.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.23108v1",
    "published_date": "2024-10-30 15:18:26 UTC",
    "updated_date": "2024-10-30 15:18:26 UTC"
  },
  {
    "arxiv_id": "2410.23107v1",
    "title": "Decoupling Semantic Similarity from Spatial Alignment for Neural Networks",
    "authors": [
      "Tassilo Wald",
      "Constantin Ulrich",
      "Gregor Köhler",
      "David Zimmerer",
      "Stefan Denner",
      "Michael Baumgartner",
      "Fabian Isensee",
      "Priyank Jaini",
      "Klaus H. Maier-Hein"
    ],
    "abstract": "What representation do deep neural networks learn? How similar are images to\neach other for neural networks? Despite the overwhelming success of deep\nlearning methods key questions about their internal workings still remain\nlargely unanswered, due to their internal high dimensionality and complexity.\nTo address this, one approach is to measure the similarity of activation\nresponses to various inputs. Representational Similarity Matrices (RSMs)\ndistill this similarity into scalar values for each input pair. These matrices\nencapsulate the entire similarity structure of a system, indicating which input\nleads to similar responses. While the similarity between images is ambiguous,\nwe argue that the spatial location of semantic objects does neither influence\nhuman perception nor deep learning classifiers. Thus this should be reflected\nin the definition of similarity between image responses for computer vision\nsystems. Revisiting the established similarity calculations for RSMs we expose\ntheir sensitivity to spatial alignment. In this paper, we propose to solve this\nthrough semantic RSMs, which are invariant to spatial permutation. We measure\nsemantic similarity between input responses by formulating it as a set-matching\nproblem. Further, we quantify the superiority of semantic RSMs over\nspatio-semantic RSMs through image retrieval and by comparing the similarity\nbetween representations to the similarity between predicted class\nprobabilities.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at NeurIPS2024",
    "pdf_url": "http://arxiv.org/pdf/2410.23107v1",
    "published_date": "2024-10-30 15:17:58 UTC",
    "updated_date": "2024-10-30 15:17:58 UTC"
  },
  {
    "arxiv_id": "2410.23101v2",
    "title": "Guided Game Level Repair via Explainable AI",
    "authors": [
      "Mahsa Bazzaz",
      "Seth Cooper"
    ],
    "abstract": "Procedurally generated levels created by machine learning models can be\nunsolvable without further editing. Various methods have been developed to\nautomatically repair these levels by enforcing hard constraints during the\npost-processing step. However, as levels increase in size, these\nconstraint-based repairs become increasingly slow. This paper proposes using\nexplainability methods to identify specific regions of a level that contribute\nto its unsolvability. By assigning higher weights to these regions,\nconstraint-based solvers can prioritize these problematic areas, enabling more\nefficient repairs. Our results, tested across three games, demonstrate that\nthis approach can help to repair procedurally generated levels faster.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.23101v2",
    "published_date": "2024-10-30 15:12:36 UTC",
    "updated_date": "2024-11-04 16:26:34 UTC"
  },
  {
    "arxiv_id": "2410.23099v1",
    "title": "Comparative Analysis of Demonstration Selection Algorithms for LLM In-Context Learning",
    "authors": [
      "Dong Shu",
      "Mengnan Du"
    ],
    "abstract": "In-context learning can help Large Language Models (LLMs) to adapt new tasks\nwithout additional training. However, this performance heavily depends on the\nquality of the demonstrations, driving research into effective demonstration\nselection algorithms to optimize this process. These algorithms assist users in\nselecting the best $k$ input-label pairs (demonstration examples) based on a\ngiven test input, enabling LLMs to in-context learn the relationship between\nthe provided examples and the test inputs. Despite all the proposed\ndemonstration selection algorithms, their efficiency and effectiveness remain\nunclear. This lack of clarity make it difficult to apply these algorithms in\nreal-world scenarios and poses challenges for future research aimed at\ndeveloping improved methods. This paper revisits six proposed algorithms,\nevaluating them on five datasets from both efficiency and effectiveness\nperspectives. Our experiments reveal significant variations in algorithm\nperformance across different tasks, with some methods struggling to outperform\nrandom selection in certain scenarios. We also find that increasing the number\nof demonstrations does not always lead to better performance, and that there\nare often trade-offs between accuracy and computational efficiency. Our code is\navailable at https://github.com/Tizzzzy/Demonstration_Selection_Overview.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "6 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.23099v1",
    "published_date": "2024-10-30 15:11:58 UTC",
    "updated_date": "2024-10-30 15:11:58 UTC"
  },
  {
    "arxiv_id": "2411.00856v1",
    "title": "AI in Investment Analysis: LLMs for Equity Stock Ratings",
    "authors": [
      "Kassiani Papasotiriou",
      "Srijan Sood",
      "Shayleen Reynolds",
      "Tucker Balch"
    ],
    "abstract": "Investment Analysis is a cornerstone of the Financial Services industry. The\nrapid integration of advanced machine learning techniques, particularly Large\nLanguage Models (LLMs), offers opportunities to enhance the equity rating\nprocess. This paper explores the application of LLMs to generate multi-horizon\nstock ratings by ingesting diverse datasets. Traditional stock rating methods\nrely heavily on the expertise of financial analysts, and face several\nchallenges such as data overload, inconsistencies in filings, and delayed\nreactions to market events. Our study addresses these issues by leveraging LLMs\nto improve the accuracy and consistency of stock ratings. Additionally, we\nassess the efficacy of using different data modalities with LLMs for the\nfinancial domain.\n  We utilize varied datasets comprising fundamental financial, market, and news\ndata from January 2022 to June 2024, along with GPT-4-32k (v0613) (with a\ntraining cutoff in Sep. 2021 to prevent information leakage). Our results show\nthat our benchmark method outperforms traditional stock rating methods when\nassessed by forward returns, specially when incorporating financial\nfundamentals. While integrating news data improves short-term performance,\nsubstituting detailed news summaries with sentiment scores reduces token use\nwithout loss of performance. In many cases, omitting news data entirely\nenhances performance by reducing bias.\n  Our research shows that LLMs can be leveraged to effectively utilize large\namounts of multimodal financial data, as showcased by their effectiveness at\nthe stock rating prediction task. Our work provides a reproducible and\nefficient framework for generating accurate stock ratings, serving as a\ncost-effective alternative to traditional methods. Future work will extend to\nlonger timeframes, incorporate diverse data, and utilize newer models for\nenhanced insights.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-fin.CP",
      "68T50, 91G60 (Primary) 68T07 (Secondary)",
      "I.2.7"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 5 figures, ICAIF24: 5th ACM International Conference on AI\n  in Finance",
    "pdf_url": "http://arxiv.org/pdf/2411.00856v1",
    "published_date": "2024-10-30 15:06:57 UTC",
    "updated_date": "2024-10-30 15:06:57 UTC"
  },
  {
    "arxiv_id": "2410.23086v1",
    "title": "From Hype to Reality: The Road Ahead of Deploying DRL in 6G Networks",
    "authors": [
      "Haiyuan Li",
      "Hari Madhukumar",
      "Peizheng Li",
      "Yiran Teng",
      "Shuangyi Yan",
      "Dimitra Simeonidou"
    ],
    "abstract": "The industrial landscape is rapidly evolving with the advent of 6G\napplications, which demand massive connectivity, high computational capacity,\nand ultra-low latency. These requirements present new challenges, which can no\nlonger be efficiently addressed by conventional strategies. In response, this\narticle underscores the transformative potential of Deep Reinforcement Learning\n(DRL) for 6G, highlighting its advantages over classic machine learning\nsolutions in meeting the demands of 6G. The necessity of DRL is further\nvalidated through three DRL applications in an end-to-end communication\nprocedure, including wireless access control, baseband function placement, and\nnetwork slicing coordination. However, DRL-based network management initiatives\nare far from mature. We extend the discussion to identify the challenges of\napplying DRL in practical networks and explore potential solutions along with\ntheir respective limitations. In the end, these insights are validated through\na practical DRL deployment in managing network slices on the testbed.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.DC",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.23086v1",
    "published_date": "2024-10-30 15:02:54 UTC",
    "updated_date": "2024-10-30 15:02:54 UTC"
  },
  {
    "arxiv_id": "2410.23085v3",
    "title": "S3PT: Scene Semantics and Structure Guided Clustering to Boost Self-Supervised Pre-Training for Autonomous Driving",
    "authors": [
      "Maciej K. Wozniak",
      "Hariprasath Govindarajan",
      "Marvin Klingner",
      "Camille Maurice",
      "B Ravi Kiran",
      "Senthil Yogamani"
    ],
    "abstract": "Recent self-supervised clustering-based pre-training techniques like DINO and\nCribo have shown impressive results for downstream detection and segmentation\ntasks. However, real-world applications such as autonomous driving face\nchallenges with imbalanced object class and size distributions and complex\nscene geometries. In this paper, we propose S3PT a novel scene semantics and\nstructure guided clustering to provide more scene-consistent objectives for\nself-supervised training. Specifically, our contributions are threefold: First,\nwe incorporate semantic distribution consistent clustering to encourage better\nrepresentation of rare classes such as motorcycles or animals. Second, we\nintroduce object diversity consistent spatial clustering, to handle imbalanced\nand diverse object sizes, ranging from large background areas to small objects\nsuch as pedestrians and traffic signs. Third, we propose a depth-guided spatial\nclustering to regularize learning based on geometric information of the scene,\nthus further refining region separation on the feature level. Our learned\nrepresentations significantly improve performance in downstream semantic\nsegmentation and 3D object detection tasks on the nuScenes, nuImages, and\nCityscapes datasets and show promising domain translation properties.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted for WACV 2025 (Oral)",
    "pdf_url": "http://arxiv.org/pdf/2410.23085v3",
    "published_date": "2024-10-30 15:00:06 UTC",
    "updated_date": "2025-01-24 10:46:53 UTC"
  },
  {
    "arxiv_id": "2410.23082v1",
    "title": "An Event-Based Digital Compute-In-Memory Accelerator with Flexible Operand Resolution and Layer-Wise Weight/Output Stationarity",
    "authors": [
      "Nicolas Chauvaux",
      "Adrian Kneip",
      "Christoph Posch",
      "Kofi Makinwa",
      "Charlotte Frenkel"
    ],
    "abstract": "Compute-in-memory (CIM) accelerators for spiking neural networks (SNNs) are\npromising solutions to enable $\\mu$s-level inference latency and ultra-low\nenergy in edge vision applications. Yet, their current lack of flexibility at\nboth the circuit and system levels prevents their deployment in a wide range of\nreal-life scenarios. In this work, we propose a novel digital CIM macro that\nsupports arbitrary operand resolution and shape, with a unified CIM storage for\nweights and membrane potentials. These circuit-level techniques enable a hybrid\nweight- and output-stationary dataflow at the system level to maximize operand\nreuse, thereby minimizing costly on- and off-chip data movements during the SNN\nexecution. Measurement results of a fabricated FlexSpIM prototype in 40-nm CMOS\ndemonstrate a 2$\\times$ increase in bit-normalized energy efficiency compared\nto prior fixed-precision digital CIM-SNNs, while providing resolution\nreconfiguration with bitwise granularity. Our approach can save up to 90%\nenergy in large-scale systems, while reaching a state-of-the-art classification\naccuracy of 95.8% on the IBM DVS gesture dataset.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "B.2.0; B.3.0; B.6.0; B.7.0; C.3"
    ],
    "primary_category": "cs.AR",
    "comment": "5 pages, 7 figures, submitted to IEEE ISCAS 2025",
    "pdf_url": "http://arxiv.org/pdf/2410.23082v1",
    "published_date": "2024-10-30 14:55:13 UTC",
    "updated_date": "2024-10-30 14:55:13 UTC"
  },
  {
    "arxiv_id": "2410.23079v1",
    "title": "BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters for Efficient LLM Inference",
    "authors": [
      "Junqi Zhao",
      "Zhijin Fang",
      "Shu Li",
      "Shaohui Yang",
      "Shichao He"
    ],
    "abstract": "Large language models (LLMs) are essential in natural language processing but\noften struggle with inference speed and computational efficiency, limiting\nreal-time deployment. The key-value (KV) cache mechanism reduces computational\noverhead in transformer models, but challenges in maintaining contextual\nunderstanding remain. In this paper, we propose BUZZ, a novel KV caching\nalgorithm that leverages structured contextual information to minimize cache\nmemory usage while enhancing inference speed. BUZZ employs a beehive-structured\nsparse cache, incorporating a sliding window to capture recent information and\ndynamically segmenting historical tokens into chunks to prioritize important\ntokens in local neighborhoods. We evaluate BUZZ on four real-world datasets:\nCNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ\n(1) reduces cache memory usage by $\\textbf{2.5}\\times$ in LLM inference while\nmaintaining over 99% accuracy in long-text summarization, and (2) surpasses\nstate-of-the-art performance in multi-document question answering by\n$\\textbf{7.69%}$ under the same memory limit, where full cache methods\nencounter out-of-memory issues. Additionally, BUZZ achieves significant\ninference speedup with a $\\log{n}$ time complexity. The code is available at\nhttps://github.com/JunqiZhao888/buzz-llm.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.23079v1",
    "published_date": "2024-10-30 14:53:37 UTC",
    "updated_date": "2024-10-30 14:53:37 UTC"
  },
  {
    "arxiv_id": "2410.23072v1",
    "title": "CNN Explainability with Multivector Tucker Saliency Maps for Self-Supervised Models",
    "authors": [
      "Aymene Mohammed Bouayed",
      "Samuel Deslauriers-Gauthier",
      "Adrian Iaccovelli",
      "David Naccache"
    ],
    "abstract": "Interpreting the decisions of Convolutional Neural Networks (CNNs) is\nessential for understanding their behavior, yet explainability remains a\nsignificant challenge, particularly for self-supervised models. Most existing\nmethods for generating saliency maps rely on ground truth labels, restricting\ntheir use to supervised tasks. EigenCAM is the only notable label-independent\nalternative, leveraging Singular Value Decomposition to generate saliency maps\napplicable across CNN models, but it does not fully exploit the tensorial\nstructure of feature maps. In this work, we introduce the Tucker Saliency Map\n(TSM) method, which applies Tucker tensor decomposition to better capture the\ninherent structure of feature maps, producing more accurate singular vectors\nand values. These are used to generate high-fidelity saliency maps, effectively\nhighlighting objects of interest in the input. We further extend EigenCAM and\nTSM into multivector variants -Multivec-EigenCAM and Multivector Tucker\nSaliency Maps (MTSM)- which utilize all singular vectors and values, further\nimproving saliency map quality. Quantitative evaluations on supervised\nclassification models demonstrate that TSM, Multivec-EigenCAM, and MTSM achieve\ncompetitive performance with label-dependent methods. Moreover, TSM enhances\nexplainability by approximately 50% over EigenCAM for both supervised and\nself-supervised models. Multivec-EigenCAM and MTSM further advance\nstate-of-the-art explainability performance on self-supervised models, with\nMTSM achieving the best results.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "29 pages, 20 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.23072v1",
    "published_date": "2024-10-30 14:46:34 UTC",
    "updated_date": "2024-10-30 14:46:34 UTC"
  },
  {
    "arxiv_id": "2411.00855v1",
    "title": "Vision-Language Models Can Self-Improve Reasoning via Reflection",
    "authors": [
      "Kanzhi Cheng",
      "Yantao Li",
      "Fangzhi Xu",
      "Jianbing Zhang",
      "Hao Zhou",
      "Yang Liu"
    ],
    "abstract": "Chain-of-thought (CoT) has proven to improve the reasoning capability of\nlarge language models (LLMs). However, due to the complexity of multimodal\nscenarios and the difficulty in collecting high-quality CoT data, CoT reasoning\nin multimodal LLMs has been largely overlooked. To this end, we propose a\nsimple yet effective self-training framework, R3V, which iteratively enhances\nthe model's Vision-language Reasoning by Reflecting on CoT Rationales. Our\nframework consists of two interleaved parts: (1) iteratively bootstrapping\npositive and negative solutions for reasoning datasets, and (2) reflection on\nrationale for learning from mistakes. Specifically, we introduce the\nself-refine and self-select losses, enabling the model to refine flawed\nrationale and derive the correct answer by comparing rationale candidates.\nExperiments on a wide range of vision-language tasks show that R3V consistently\nimproves multimodal LLM reasoning, achieving a relative improvement of 23 to 60\npercent over GPT-distilled baselines. Additionally, our approach supports\nself-reflection on generated solutions, further boosting performance through\ntest-time computation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.00855v1",
    "published_date": "2024-10-30 14:45:00 UTC",
    "updated_date": "2024-10-30 14:45:00 UTC"
  },
  {
    "arxiv_id": "2410.23069v1",
    "title": "LLMs Integration in Software Engineering Team Projects: Roles, Impact, and a Pedagogical Design Space for AI Tools in Computing Education",
    "authors": [
      "Ahmed Kharrufa",
      "Sami Alghamdi",
      "Abeer Aziz",
      "Christopher Bull"
    ],
    "abstract": "This work takes a pedagogical lens to explore the implications of generative\nAI (GenAI) models and tools, such as ChatGPT and GitHub Copilot, in a\nsemester-long 2nd-year undergraduate Software Engineering Team Project.\nQualitative findings from survey (39 students) and interviews (eight students)\nprovide insights into the students' views on the impact of GenAI use on their\ncoding experience, learning, and self-efficacy. Our results address a\nparticular gap in understanding the role and implications of GenAI on teamwork,\nteam-efficacy, and team dynamics. The analysis of the learning aspects is\ndistinguished by the application of learning and pedagogy informed lenses to\ndiscuss the data. We propose a preliminary design space for GenAI-based\nprogramming learning tools highlighting the importance of considering the roles\nthat GenAI can play during the learning process, the varying support-ability\npatterns that can be applied to each role, and the importance of supporting\ntransparency in GenAI for team members and students in addition to educators.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.23069v1",
    "published_date": "2024-10-30 14:43:33 UTC",
    "updated_date": "2024-10-30 14:43:33 UTC"
  },
  {
    "arxiv_id": "2410.23054v2",
    "title": "Controlling Language and Diffusion Models by Transporting Activations",
    "authors": [
      "Pau Rodriguez",
      "Arno Blaas",
      "Michal Klein",
      "Luca Zappella",
      "Nicholas Apostoloff",
      "Marco Cuturi",
      "Xavier Suau"
    ],
    "abstract": "The increasing capabilities of large generative models and their ever more\nwidespread deployment have raised concerns about their reliability, safety, and\npotential misuse. To address these issues, recent works have proposed to\ncontrol model generation by steering model activations in order to effectively\ninduce or prevent the emergence of concepts or behaviors in the generated\noutput. In this paper we introduce Activation Transport (AcT), a general\nframework to steer activations guided by optimal transport theory that\ngeneralizes many previous activation-steering works. AcT is modality-agnostic\nand provides fine-grained control over the model behavior with negligible\ncomputational overhead, while minimally impacting model abilities. We\nexperimentally show the effectiveness and versatility of our approach by\naddressing key challenges in large language models (LLMs) and text-to-image\ndiffusion models (T2Is). For LLMs, we show that AcT can effectively mitigate\ntoxicity, induce arbitrary concepts, and increase their truthfulness. In T2Is,\nwe show how AcT enables fine-grained style control and concept negation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "68T07, 49Q22",
      "I.2.6; I.2.7; I.4.8"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.23054v2",
    "published_date": "2024-10-30 14:21:33 UTC",
    "updated_date": "2024-11-22 16:04:44 UTC"
  },
  {
    "arxiv_id": "2410.23041v1",
    "title": "Emotional RAG: Enhancing Role-Playing Agents through Emotional Retrieval",
    "authors": [
      "Le Huang",
      "Hengzhi Lan",
      "Zijun Sun",
      "Chuan Shi",
      "Ting Bai"
    ],
    "abstract": "As LLMs exhibit a high degree of human-like capability, increasing attention\nhas been paid to role-playing research areas in which responses generated by\nLLMs are expected to mimic human replies. This has promoted the exploration of\nrole-playing agents in various applications, such as chatbots that can engage\nin natural conversations with users and virtual assistants that can provide\npersonalized support and guidance. The crucial factor in the role-playing task\nis the effective utilization of character memory, which stores characters'\nprofiles, experiences, and historical dialogues. Retrieval Augmented Generation\n(RAG) technology is used to access the related memory to enhance the response\ngeneration of role-playing agents. Most existing studies retrieve related\ninformation based on the semantic similarity of memory to maintain characters'\npersonalized traits, and few attempts have been made to incorporate the\nemotional factor in the retrieval argument generation (RAG) of LLMs. Inspired\nby the Mood-Dependent Memory theory, which indicates that people recall an\nevent better if they somehow reinstate during recall the original emotion they\nexperienced during learning, we propose a novel emotion-aware memory retrieval\nframework, termed Emotional RAG, which recalls the related memory with\nconsideration of emotional state in role-playing agents. Specifically, we\ndesign two kinds of retrieval strategies, i.e., combination strategy and\nsequential strategy, to incorporate both memory semantic and emotional states\nduring the retrieval process. Extensive experiments on three representative\nrole-playing datasets demonstrate that our Emotional RAG framework outperforms\nthe method without considering the emotional factor in maintaining the\npersonalities of role-playing agents. This provides evidence to further\nreinforce the Mood-Dependent Memory theory in psychology.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.23041v1",
    "published_date": "2024-10-30 14:08:50 UTC",
    "updated_date": "2024-10-30 14:08:50 UTC"
  },
  {
    "arxiv_id": "2410.23031v2",
    "title": "Offline Reinforcement Learning and Sequence Modeling for Downlink Link Adaptation",
    "authors": [
      "Samuele Peri",
      "Alessio Russo",
      "Gabor Fodor",
      "Pablo Soldati"
    ],
    "abstract": "Link adaptation (LA) is an essential function in modern wireless\ncommunication systems that dynamically adjusts the transmission rate of a\ncommunication link to match time- and frequency-varying radio link conditions.\nHowever, factors such as user mobility, fast fading, imperfect channel quality\ninformation, and aging of measurements make the modeling of LA challenging. To\nbypass the need for explicit modeling, recent research has introduced online\nreinforcement learning (RL) approaches as an alternative to the more commonly\nused rule-based algorithms. Yet, RL-based approaches face deployment\nchallenges, as training in live networks can potentially degrade real-time\nperformance. To address this challenge, this paper considers offline RL as a\ncandidate to learn LA policies with minimal effects on the network operation.\nWe propose three LA designs based on batch-constrained deep Q-learning,\nconservative Q-learning, and decision transformer. Our results show that\noffline RL algorithms can match the performance of state-of-the-art online RL\nmethods when data is collected with a proper behavioral policy.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.23031v2",
    "published_date": "2024-10-30 14:01:31 UTC",
    "updated_date": "2024-11-28 23:00:31 UTC"
  },
  {
    "arxiv_id": "2410.23022v2",
    "title": "Online Intrinsic Rewards for Decision Making Agents from Large Language Model Feedback",
    "authors": [
      "Qinqing Zheng",
      "Mikael Henaff",
      "Amy Zhang",
      "Aditya Grover",
      "Brandon Amos"
    ],
    "abstract": "Automatically synthesizing dense rewards from natural language descriptions\nis a promising paradigm in reinforcement learning (RL), with applications to\nsparse reward problems, open-ended exploration, and hierarchical skill design.\nRecent works have made promising steps by exploiting the prior knowledge of\nlarge language models (LLMs). However, these approaches suffer from important\nlimitations: they are either not scalable to problems requiring billions of\nenvironment samples, due to requiring LLM annotations for each observation, or\nthey require a diverse offline dataset, which may not exist or be impossible to\ncollect. In this work, we address these limitations through a combination of\nalgorithmic and systems-level contributions. We propose \\oni, a distributed\narchitecture that simultaneously learns an RL policy and an intrinsic reward\nfunction using LLM feedback. Our approach annotates the agent's collected\nexperience via an asynchronous LLM server, which is then distilled into an\nintrinsic reward model. We explore a range of algorithmic choices for reward\nmodeling with varying complexity, including hashing, classification, and\nranking models. By studying their relative tradeoffs, we shed light on\nquestions regarding intrinsic reward design for sparse reward problems. Our\napproach achieves state-of-the-art performance across a range of challenging,\nsparse reward tasks from the NetHack Learning Environment in a simple unified\nprocess, solely using the agent's gathered experience, without requiring\nexternal datasets. We make our code available at\n\\url{https://github.com/facebookresearch/oni}.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.23022v2",
    "published_date": "2024-10-30 13:52:43 UTC",
    "updated_date": "2024-12-17 22:29:46 UTC"
  },
  {
    "arxiv_id": "2410.22997v2",
    "title": "A Comparison of Prompt Engineering Techniques for Task Planning and Execution in Service Robotics",
    "authors": [
      "Jonas Bode",
      "Bastian Pätzold",
      "Raphael Memmesheimer",
      "Sven Behnke"
    ],
    "abstract": "Recent advances in LLM have been instrumental in autonomous robot control and\nhuman-robot interaction by leveraging their vast general knowledge and\ncapabilities to understand and reason across a wide range of tasks and\nscenarios. Previous works have investigated various prompt engineering\ntechniques for improving the performance of LLM to accomplish tasks, while\nothers have proposed methods that utilize LLMs to plan and execute tasks based\non the available functionalities of a given robot platform. In this work, we\nconsider both lines of research by comparing prompt engineering techniques and\ncombinations thereof within the application of high-level task planning and\nexecution in service robotics. We define a diverse set of tasks and a simple\nset of functionalities in simulation, and measure task completion accuracy and\nexecution time for several state-of-the-art models.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "6 pages, 3 figures, 2 tables, to be published in the 2024 IEEE-RAS\n  International Conference on Humanoid Robots, We make our code, including all\n  prompts, available at https://github.com/AIS-Bonn/Prompt_Engineering",
    "pdf_url": "http://arxiv.org/pdf/2410.22997v2",
    "published_date": "2024-10-30 13:22:55 UTC",
    "updated_date": "2024-11-06 16:57:03 UTC"
  },
  {
    "arxiv_id": "2410.22996v1",
    "title": "Semantic Enrichment of the Quantum Cascade Laser Properties in Text- A Knowledge Graph Generation Approach",
    "authors": [
      "Deperias Kerre",
      "Anne Laurent",
      "Kenneth Maussang",
      "Dickson Owuor"
    ],
    "abstract": "A well structured collection of the various Quantum Cascade Laser (QCL)\ndesign and working properties data provides a platform to analyze and\nunderstand the relationships between these properties. By analyzing these\nrelationships, we can gain insights into how different design features impact\nlaser performance properties such as the working temperature. Most of these QCL\nproperties are captured in scientific text. There is therefore need for\nefficient methodologies that can be utilized to extract QCL properties from\ntext and generate a semantically enriched and interlinked platform where the\nproperties can be analyzed to uncover hidden relations. There is also the need\nto maintain provenance and reference information on which these properties are\nbased. Semantic Web technologies such as Ontologies and Knowledge Graphs have\nproven capability in providing interlinked data platforms for knowledge\nrepresentation in various domains. In this paper, we propose an approach for\ngenerating a QCL properties Knowledge Graph (KG) from text for semantic\nenrichment of the properties. The approach is based on the QCL ontology and a\nRetrieval Augmented Generation (RAG) enabled information extraction pipeline\nbased on GPT 4-Turbo language model. The properties of interest include:\nworking temperature, laser design type, lasing frequency, laser optical power\nand the heterostructure. The experimental results demonstrate the feasibility\nand effectiveness of this approach for efficiently extracting QCL properties\nfrom unstructured text and generating a QCL properties Knowledge Graph, which\nhas potential applications in semantic enrichment and analysis of QCL data.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.22996v1",
    "published_date": "2024-10-30 13:22:22 UTC",
    "updated_date": "2024-10-30 13:22:22 UTC"
  },
  {
    "arxiv_id": "2410.22995v1",
    "title": "VisAidMath: Benchmarking Visual-Aided Mathematical Reasoning",
    "authors": [
      "Jingkun Ma",
      "Runzhe Zhan",
      "Derek F. Wong",
      "Yang Li",
      "Di Sun",
      "Hou Pong Chan",
      "Lidia S. Chao"
    ],
    "abstract": "Although previous research on large language models (LLMs) and large\nmulti-modal models (LMMs) has systematically explored mathematical\nproblem-solving (MPS) within visual contexts, the analysis of how these models\nprocess visual information during problem-solving remains insufficient. To\naddress this gap, we present VisAidMath, a benchmark for evaluating the MPS\nprocess related to visual information. We follow a rigorous data curation\npipeline involving both automated processes and manual annotations to ensure\ndata quality and reliability. Consequently, this benchmark includes 1,200\nchallenging problems from various mathematical branches, vision-aid\nformulations, and difficulty levels, collected from diverse sources such as\ntextbooks, examination papers, and Olympiad problems. Based on the proposed\nbenchmark, we conduct comprehensive evaluations on ten mainstream LLMs and\nLMMs, highlighting deficiencies in the visual-aided reasoning process. For\nexample, GPT-4V only achieves 45.33% accuracy in the visual-aided reasoning\ntask, even with a drop of 2 points when provided with golden visual aids.\nIn-depth analysis reveals that the main cause of deficiencies lies in\nhallucination regarding the implicit visual reasoning process, shedding light\non future research directions in the visual-aided MPS process.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "58 pages, 28 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.22995v1",
    "published_date": "2024-10-30 13:19:44 UTC",
    "updated_date": "2024-10-30 13:19:44 UTC"
  },
  {
    "arxiv_id": "2410.23325v1",
    "title": "Transfer Learning in Vocal Education: Technical Evaluation of Limited Samples Describing Mezzo-soprano",
    "authors": [
      "Zhenyi Hou",
      "Xu Zhao",
      "Kejie Ye",
      "Xinyu Sheng",
      "Shanggerile Jiang",
      "Jiajing Xia",
      "Yitao Zhang",
      "Chenxi Ban",
      "Daijun Luo",
      "Jiaxing Chen",
      "Yan Zou",
      "Yuchao Feng",
      "Guangyu Fan",
      "Xin Yuan"
    ],
    "abstract": "Vocal education in the music field is difficult to quantify due to the\nindividual differences in singers' voices and the different quantitative\ncriteria of singing techniques. Deep learning has great potential to be applied\nin music education due to its efficiency to handle complex data and perform\nquantitative analysis. However, accurate evaluations with limited samples over\nrare vocal types, such as Mezzo-soprano, requires extensive well-annotated data\nsupport using deep learning models. In order to attain the objective, we\nperform transfer learning by employing deep learning models pre-trained on the\nImageNet and Urbansound8k datasets for the improvement on the precision of\nvocal technique evaluation. Furthermore, we tackle the problem of the lack of\nsamples by constructing a dedicated dataset, the Mezzo-soprano Vocal Set (MVS),\nfor vocal technique assessment. Our experimental results indicate that transfer\nlearning increases the overall accuracy (OAcc) of all models by an average of\n8.3%, with the highest accuracy at 94.2%. We not only provide a novel approach\nto evaluating Mezzo-soprano vocal techniques but also introduce a new\nquantitative assessment method for music education.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.MM",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.23325v1",
    "published_date": "2024-10-30 13:17:13 UTC",
    "updated_date": "2024-10-30 13:17:13 UTC"
  },
  {
    "arxiv_id": "2410.22984v1",
    "title": "Higher-order Cross-structural Embedding Model for Time Series Analysis",
    "authors": [
      "Guancen Lin",
      "Cong Shen",
      "Aijing Lin"
    ],
    "abstract": "Time series analysis has gained significant attention due to its critical\napplications in diverse fields such as healthcare, finance, and sensor\nnetworks. The complexity and non-stationarity of time series make it\nchallenging to capture the interaction patterns across different timestamps.\nCurrent approaches struggle to model higher-order interactions within time\nseries, and focus on learning temporal or spatial dependencies separately,\nwhich limits performance in downstream tasks. To address these gaps, we propose\nHigher-order Cross-structural Embedding Model for Time Series (High-TS), a\nnovel framework that jointly models both temporal and spatial perspectives by\ncombining multiscale Transformer with Topological Deep Learning (TDL).\nMeanwhile, High-TS utilizes contrastive learning to integrate these two\nstructures for generating robust and discriminative representations. Extensive\nexperiments show that High-TS outperforms state-of-the-art methods in various\ntime series tasks and demonstrate the importance of higher-order\ncross-structural information in improving model performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.22984v1",
    "published_date": "2024-10-30 12:51:14 UTC",
    "updated_date": "2024-10-30 12:51:14 UTC"
  },
  {
    "arxiv_id": "2411.00853v1",
    "title": "Accelerated AI Inference via Dynamic Execution Methods",
    "authors": [
      "Haim Barad",
      "Jascha Achterberg",
      "Tien Pei Chou",
      "Jean Yu"
    ],
    "abstract": "In this paper, we focus on Dynamic Execution techniques that optimize the\ncomputation flow based on input. This aims to identify simpler problems that\ncan be solved using fewer resources, similar to human cognition. The techniques\ndiscussed include early exit from deep networks, speculative sampling for\nlanguage models, and adaptive steps for diffusion models. Experimental results\ndemonstrate that these dynamic approaches can significantly improve latency and\nthroughput without compromising quality. When combined with model-based\noptimizations, such as quantization, dynamic execution provides a powerful\nmulti-pronged strategy to optimize AI inference.\n  Generative AI requires a large amount of compute resources. This is expected\nto grow, and demand for resources in data centers through to the edge is\nexpected to continue to increase at high rates. We take advantage of existing\nresearch and provide additional innovations for some generative optimizations.\nIn the case of LLMs, we provide more efficient sampling methods that depend on\nthe complexity of the data. In the case of diffusion model generation, we\nprovide a new method that also leverages the difficulty of the input prompt to\npredict an optimal early stopping point.\n  Therefore, dynamic execution methods are relevant because they add another\ndimension of performance optimizations. Performance is critical from a\ncompetitive point of view, but increasing capacity can result in significant\npower savings and cost savings. We have provided several integrations of these\ntechniques into several Intel performance libraries and Huggingface Optimum.\nThese integrations will make them easier to use and increase the adoption of\nthese techniques.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.00853v1",
    "published_date": "2024-10-30 12:49:23 UTC",
    "updated_date": "2024-10-30 12:49:23 UTC"
  },
  {
    "arxiv_id": "2410.22982v1",
    "title": "PDSR: Efficient UAV Deployment for Swift and Accurate Post-Disaster Search and Rescue",
    "authors": [
      "Alaa Awad Abdellatif",
      "Ali Elmancy",
      "Amr Mohamed",
      "Ahmed Massoud",
      "Wadha Lebda",
      "Khalid K. Naji"
    ],
    "abstract": "This paper introduces a comprehensive framework for Post-Disaster Search and\nRescue (PDSR), aiming to optimize search and rescue operations leveraging\nUnmanned Aerial Vehicles (UAVs). The primary goal is to improve the precision\nand availability of sensing capabilities, particularly in various catastrophic\nscenarios. Central to this concept is the rapid deployment of UAV swarms\nequipped with diverse sensing, communication, and intelligence capabilities,\nfunctioning as an integrated system that incorporates multiple technologies and\napproaches for efficient detection of individuals buried beneath rubble or\ndebris following a disaster. Within this framework, we propose architectural\nsolution and address associated challenges to ensure optimal performance in\nreal-world disaster scenarios. The proposed framework aims to achieve complete\ncoverage of damaged areas significantly faster than traditional methods using a\nmulti-tier swarm architecture. Furthermore, integrating multi-modal sensing\ndata with machine learning for data fusion could enhance detection accuracy,\nensuring precise identification of survivors.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "This paper is currently under review at IEEE IoT Magazine",
    "pdf_url": "http://arxiv.org/pdf/2410.22982v1",
    "published_date": "2024-10-30 12:46:15 UTC",
    "updated_date": "2024-10-30 12:46:15 UTC"
  },
  {
    "arxiv_id": "2410.22952v1",
    "title": "Efficient Adaptation of Pre-trained Vision Transformer via Householder Transformation",
    "authors": [
      "Wei Dong",
      "Yuan Sun",
      "Yiting Yang",
      "Xing Zhang",
      "Zhijun Lin",
      "Qingsen Yan",
      "Haokui Zhang",
      "Peng Wang",
      "Yang Yang",
      "Hengtao Shen"
    ],
    "abstract": "A common strategy for Parameter-Efficient Fine-Tuning (PEFT) of pre-trained\nVision Transformers (ViTs) involves adapting the model to downstream tasks by\nlearning a low-rank adaptation matrix. This matrix is decomposed into a product\nof down-projection and up-projection matrices, with the bottleneck\ndimensionality being crucial for reducing the number of learnable parameters,\nas exemplified by prevalent methods like LoRA and Adapter. However, these\nlow-rank strategies typically employ a fixed bottleneck dimensionality, which\nlimits their flexibility in handling layer-wise variations. To address this\nlimitation, we propose a novel PEFT approach inspired by Singular Value\nDecomposition (SVD) for representing the adaptation matrix. SVD decomposes a\nmatrix into the product of a left unitary matrix, a diagonal matrix of scaling\nvalues, and a right unitary matrix. We utilize Householder transformations to\nconstruct orthogonal matrices that efficiently mimic the unitary matrices,\nrequiring only a vector. The diagonal values are learned in a layer-wise\nmanner, allowing them to flexibly capture the unique properties of each layer.\nThis approach enables the generation of adaptation matrices with varying ranks\nacross different layers, providing greater flexibility in adapting pre-trained\nmodels. Experiments on standard downstream vision tasks demonstrate that our\nmethod achieves promising fine-tuning performance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.22952v1",
    "published_date": "2024-10-30 12:08:30 UTC",
    "updated_date": "2024-10-30 12:08:30 UTC"
  },
  {
    "arxiv_id": "2410.22950v1",
    "title": "SpiroActive: Active Learning for Efficient Data Acquisition for Spirometry",
    "authors": [
      "Ankita Kumari Jain",
      "Nitish Sharma",
      "Madhav Kanda",
      "Nipun Batra"
    ],
    "abstract": "Respiratory illnesses are a significant global health burden. Respiratory\nillnesses, primarily Chronic obstructive pulmonary disease (COPD), is the\nseventh leading cause of poor health worldwide and the third leading cause of\ndeath worldwide, causing 3.23 million deaths in 2019, necessitating early\nidentification and diagnosis for effective mitigation. Among the diagnostic\ntools employed, spirometry plays a crucial role in detecting respiratory\nabnormalities. However, conventional clinical spirometry methods often entail\nconsiderable costs and practical limitations like the need for specialized\nequipment, trained personnel, and a dedicated clinical setting, making them\nless accessible. To address these challenges, wearable spirometry technologies\nhave emerged as promising alternatives, offering accurate, cost-effective, and\nconvenient solutions. The development of machine learning models for wearable\nspirometry heavily relies on the availability of high-quality ground truth\nspirometry data, which is a laborious and expensive endeavor. In this research,\nwe propose using active learning, a sub-field of machine learning, to mitigate\nthe challenges associated with data collection and labeling. By strategically\nselecting samples from the ground truth spirometer, we can mitigate the need\nfor resource-intensive data collection. We present evidence that models trained\non small subsets obtained through active learning achieve comparable/better\nresults than models trained on the complete dataset.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.22950v1",
    "published_date": "2024-10-30 12:07:30 UTC",
    "updated_date": "2024-10-30 12:07:30 UTC"
  },
  {
    "arxiv_id": "2410.22944v3",
    "title": "Focus On This, Not That! Steering LLMs With Adaptive Feature Specification",
    "authors": [
      "Tom A. Lamb",
      "Adam Davies",
      "Alasdair Paren",
      "Philip H. S. Torr",
      "Francesco Pinto"
    ],
    "abstract": "Despite the success of Instruction Tuning (IT) in training large language\nmodels (LLMs) to perform arbitrary user-specified tasks, these models often\nstill leverage spurious or biased features learned from their training data,\nleading to undesired behaviours when deploying them in new contexts. In this\nwork, we introduce Focus Instruction Tuning (FIT), which trains LLMs to\ncondition their responses by focusing on specific features whilst ignoring\nothers, leading to different behaviours based on what features are specified.\nAcross several experimental settings, we show that focus-tuned models can be\nadaptively steered by focusing on different features at inference-time: for\ninstance, robustness can be improved by focusing on task-causal features and\nignoring spurious features, and social bias can be mitigated by ignoring\ndemographic categories. Furthermore, FIT can steer behaviour in new contexts,\ngeneralising under distribution shift and to new unseen features at inference\ntime, and thereby facilitating more robust, fair, and controllable LLM\napplications in real-world environments.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "32pages, 17 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.22944v3",
    "published_date": "2024-10-30 12:01:48 UTC",
    "updated_date": "2025-02-10 23:03:19 UTC"
  },
  {
    "arxiv_id": "2410.22938v2",
    "title": "DiffLight: A Partial Rewards Conditioned Diffusion Model for Traffic Signal Control with Missing Data",
    "authors": [
      "Hanyang Chen",
      "Yang Jiang",
      "Shengnan Guo",
      "Xiaowei Mao",
      "Youfang Lin",
      "Huaiyu Wan"
    ],
    "abstract": "The application of reinforcement learning in traffic signal control (TSC) has\nbeen extensively researched and yielded notable achievements. However, most\nexisting works for TSC assume that traffic data from all surrounding\nintersections is fully and continuously available through sensors. In\nreal-world applications, this assumption often fails due to sensor malfunctions\nor data loss, making TSC with missing data a critical challenge. To meet the\nneeds of practical applications, we introduce DiffLight, a novel conditional\ndiffusion model for TSC under data-missing scenarios in the offline setting.\nSpecifically, we integrate two essential sub-tasks, i.e., traffic data\nimputation and decision-making, by leveraging a Partial Rewards Conditioned\nDiffusion (PRCD) model to prevent missing rewards from interfering with the\nlearning process. Meanwhile, to effectively capture the spatial-temporal\ndependencies among intersections, we design a Spatial-Temporal transFormer\n(STFormer) architecture. In addition, we propose a Diffusion Communication\nMechanism (DCM) to promote better communication and control performance under\ndata-missing scenarios. Extensive experiments on five datasets with various\ndata-missing scenarios demonstrate that DiffLight is an effective controller to\naddress TSC with missing data. The code of DiffLight is released at\nhttps://github.com/lokol5579/DiffLight-release.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.LG",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "Accepted by NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.22938v2",
    "published_date": "2024-10-30 11:47:40 UTC",
    "updated_date": "2024-10-31 13:39:20 UTC"
  },
  {
    "arxiv_id": "2410.22937v1",
    "title": "Thoughtful Adoption of NLP for Civic Participation: Understanding Differences Among Policymakers",
    "authors": [
      "Jose A. Guridi",
      "Cristobal Cheyre",
      "Qian Yang"
    ],
    "abstract": "Natural language processing (NLP) tools have the potential to boost civic\nparticipation and enhance democratic processes because they can significantly\nincrease governments' capacity to gather and analyze citizen opinions. However,\ntheir adoption in government remains limited, and harnessing their benefits\nwhile preventing unintended consequences remains a challenge. While prior work\nhas focused on improving NLP performance, this work examines how different\ninternal government stakeholders influence NLP tools' thoughtful adoption. We\ninterviewed seven politicians (politically appointed officials as heads of\ngovernment institutions) and thirteen public servants (career government\nemployees who design and administrate policy interventions), inquiring how they\nchoose whether and how to use NLP tools to support civic participation\nprocesses. The interviews suggest that policymakers across both groups focused\non their needs for career advancement and the need to showcase the legitimacy\nand fairness of their work when considering NLP tool adoption and use. Because\nthese needs vary between politicians and public servants, their preferred NLP\nfeatures and tool designs also differ. Interestingly, despite their differing\nneeds and opinions, neither group clearly identifies who should advocate for\nNLP adoption to enhance civic participation or address the unintended\nconsequences of a poorly considered adoption. This lack of clarity in\nresponsibility might have caused the governments' low adoption of NLP tools. We\ndiscuss how these findings reveal new insights for future HCI research. They\ninform the design of NLP tools for increasing civic participation efficiency\nand capacity, the design of other tools and methods that ensure thoughtful\nadoption of AI tools in government, and the design of NLP tools for\ncollaborative use among users with different incentives and needs.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "Forthcoming in the Proceedings of the 2025 Conference on Computer\n  Supported Cooperative Work and Social Computing (CSCW)",
    "pdf_url": "http://arxiv.org/pdf/2410.22937v1",
    "published_date": "2024-10-30 11:46:26 UTC",
    "updated_date": "2024-10-30 11:46:26 UTC"
  },
  {
    "arxiv_id": "2410.22925v1",
    "title": "BIS: NL2SQL Service Evaluation Benchmark for Business Intelligence Scenarios",
    "authors": [
      "Bora Caglayan",
      "Mingxue Wang",
      "John D. Kelleher",
      "Shen Fei",
      "Gui Tong",
      "Jiandong Ding",
      "Puchao Zhang"
    ],
    "abstract": "NL2SQL (Natural Language to Structured Query Language) transformation has\nseen wide adoption in Business Intelligence (BI) applications in recent years.\nHowever, existing NL2SQL benchmarks are not suitable for production BI\nscenarios, as they are not designed for common business intelligence questions.\nTo address this gap, we have developed a new benchmark focused on typical NL\nquestions in industrial BI scenarios. We discuss the challenges of constructing\na BI-focused benchmark and the shortcomings of existing benchmarks.\nAdditionally, we introduce question categories in our benchmark that reflect\ncommon BI inquiries. Lastly, we propose two novel semantic similarity\nevaluation metrics for assessing NL2SQL capabilities in BI applications and\nservices.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper has been accepted by ICSOC (International Conference on\n  Service-Oriented Computing) 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.22925v1",
    "published_date": "2024-10-30 11:33:03 UTC",
    "updated_date": "2024-10-30 11:33:03 UTC"
  },
  {
    "arxiv_id": "2411.00852v2",
    "title": "EF-LLM: Energy Forecasting LLM with AI-assisted Automation, Enhanced Sparse Prediction, Hallucination Detection",
    "authors": [
      "Zihang Qiu",
      "Chaojie Li",
      "Zhongyang Wang",
      "Renyou Xie",
      "Borui Zhang",
      "Huadong Mo",
      "Guo Chen",
      "Zhaoyang Dong"
    ],
    "abstract": "Accurate prediction helps to achieve supply-demand balance in energy systems,\nsupporting decision-making and scheduling. Traditional models, lacking\nAI-assisted automation, rely on experts, incur high costs, and struggle with\nsparse data prediction. To address these challenges, we propose the Energy\nForecasting Large Language Model (EF-LLM), which integrates domain knowledge\nand temporal data for time-series forecasting, supporting both pre-forecast\noperations and post-forecast decision-support. EF-LLM's human-AI interaction\ncapabilities lower the entry barrier in forecasting tasks, reducing the need\nfor extra expert involvement. To achieve this, we propose a continual learning\napproach with updatable LoRA and a multi-channel architecture for aligning\nheterogeneous multimodal data, enabling EF-LLM to continually learn\nheterogeneous multimodal knowledge. In addition, EF-LLM enables accurate\npredictions under sparse data conditions through its ability to process\nmultimodal data. We propose Fusion Parameter-Efficient Fine-Tuning (F-PEFT)\nmethod to effectively leverage both time-series data and text for this purpose.\nEF-LLM is also the first energy-specific LLM to detect hallucinations and\nquantify their occurrence rate, achieved via multi-task learning, semantic\nsimilarity analysis, and ANOVA. We have achieved success in energy prediction\nscenarios for load, photovoltaic, and wind power forecast.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.00852v2",
    "published_date": "2024-10-30 11:22:37 UTC",
    "updated_date": "2024-12-24 03:24:55 UTC"
  },
  {
    "arxiv_id": "2411.00850v3",
    "title": "GWQ: Gradient-Aware Weight Quantization for Large Language Models",
    "authors": [
      "Yihua Shao",
      "Yan Gu",
      "Siyu Chen",
      "Haiyang Liu",
      "Zijian Ling",
      "Minxi Yan",
      "Ziyang Yan",
      "Chenyu Zhang",
      "Michele Magno",
      "Haotong Qin",
      "Yan Wang",
      "Jingcai Guo",
      "Ling Shao",
      "Hao Tang"
    ],
    "abstract": "Large language models (LLMs) show impressive performance in solving complex\nlanguage tasks. However, its large number of parameters presents significant\nchallenges for the deployment. So, compressing LLMs to low bits can enable to\ndeploy on resource-constrained devices. To address this problem, we propose\ngradient-aware weight quantization (GWQ), the first quantization approach for\nlow-bit weight quantization that leverages gradients to localize outliers,\nrequiring only a minimal amount of calibration data for outlier detection. GWQ\nretains the top 1\\% outliers preferentially at FP16 precision, while the\nremaining non-outlier weights are stored in a low-bit. We widely evaluate GWQ\non different task include language modeling, grounding detection, massive\nmultitask language understanding and vision-language question and answering.\nResults show that models quantified by GWQ performs better than other\nquantization method. During quantization process, GWQ only need one calibration\nset to realize effective quant. Also, GWQ achieves 1.2x inference speedup in\ncomparison to the original model and effectively reduces the inference memory.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.00850v3",
    "published_date": "2024-10-30 11:16:04 UTC",
    "updated_date": "2025-04-09 09:09:11 UTC"
  },
  {
    "arxiv_id": "2410.22912v1",
    "title": "Self-optimization in distributed manufacturing systems using Modular State-based Stackelberg Games",
    "authors": [
      "Steve Yuwono",
      "Ahmar Kamal Hussain",
      "Dorothea Schwung",
      "Andreas Schwung"
    ],
    "abstract": "In this study, we introduce Modular State-based Stackelberg Games (Mod-SbSG),\na novel game structure developed for distributed self-learning in modular\nmanufacturing systems. Mod-SbSG enhances cooperative decision-making among\nself-learning agents within production systems by integrating State-based\nPotential Games (SbPG) with Stackelberg games. This hierarchical structure\nassigns more important modules of the manufacturing system a first-mover\nadvantage, while less important modules respond optimally to the leaders'\ndecisions. This decision-making process differs from typical multi-agent\nlearning algorithms in manufacturing systems, where decisions are made\nsimultaneously. We provide convergence guarantees for the novel game structure\nand design learning algorithms to account for the hierarchical game structure.\nWe further analyse the effects of single-leader/multiple-follower and\nmultiple-leader/multiple-follower scenarios within a Mod-SbSG. To assess its\neffectiveness, we implement and test Mod-SbSG in an industrial control setting\nusing two laboratory-scale testbeds featuring sequential and serial-parallel\nprocesses. The proposed approach delivers promising results compared to the\nvanilla SbPG, which reduces overflow by 97.1%, and in some cases, prevents\noverflow entirely. Additionally, it decreases power consumption by 5-13% while\nsatisfying the production demand, which significantly improves potential\n(global objective) values.",
    "categories": [
      "cs.AI",
      "cs.GT",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "This pre-print was submitted to Journal of Manufacturing Systems on\n  October 30, 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.22912v1",
    "published_date": "2024-10-30 11:09:31 UTC",
    "updated_date": "2024-10-30 11:09:31 UTC"
  },
  {
    "arxiv_id": "2410.22898v1",
    "title": "YOLOv11 for Vehicle Detection: Advancements, Performance, and Applications in Intelligent Transportation Systems",
    "authors": [
      "Mujadded Al Rabbani Alif"
    ],
    "abstract": "Accurate vehicle detection is essential for the development of intelligent\ntransportation systems, autonomous driving, and traffic monitoring. This paper\npresents a detailed analysis of YOLO11, the latest advancement in the YOLO\nseries of deep learning models, focusing exclusively on vehicle detection\ntasks. Building upon the success of its predecessors, YOLO11 introduces\narchitectural improvements designed to enhance detection speed, accuracy, and\nrobustness in complex environments. Using a comprehensive dataset comprising\nmultiple vehicle types-cars, trucks, buses, motorcycles, and bicycles we\nevaluate YOLO11's performance using metrics such as precision, recall, F1\nscore, and mean average precision (mAP). Our findings demonstrate that YOLO11\nsurpasses previous versions (YOLOv8 and YOLOv10) in detecting smaller and more\noccluded vehicles while maintaining a competitive inference time, making it\nwell-suited for real-time applications. Comparative analysis shows significant\nimprovements in the detection of complex vehicle geometries, further\ncontributing to the development of efficient and scalable vehicle detection\nsystems. This research highlights YOLO11's potential to enhance autonomous\nvehicle performance and traffic monitoring systems, offering insights for\nfuture developments in the field.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "16 pages",
    "pdf_url": "http://arxiv.org/pdf/2410.22898v1",
    "published_date": "2024-10-30 10:57:46 UTC",
    "updated_date": "2024-10-30 10:57:46 UTC"
  },
  {
    "arxiv_id": "2410.22891v1",
    "title": "VPO: Leveraging the Number of Votes in Preference Optimization",
    "authors": [
      "Jae Hyeon Cho",
      "Minkyung Park",
      "Byung-Jun Lee"
    ],
    "abstract": "Direct Preference Optimization (DPO) trains a language model using human\npreference data, bypassing the explicit reward modeling phase of Reinforcement\nLearning from Human Feedback (RLHF). By iterating over sentence pairs in a\npreference dataset, DPO enhances generation quality by increasing the\nlikelihood of producing preferred sentences over less favored ones. Preference\ndatasets are typically created by selecting preferred sentences through a\nvoting process involving multiple individuals, as opinions can vary due to the\nsubjective nature of human preferences. While the number of votes offers\ninsight into whether a sentence pair is clearly preferable or controversial,\ncurrent methods do not fully leverage this information. In this paper, we\nintroduce a technique that leverages user voting data to better align with\ndiverse subjective preferences. We employ the Bayesian Minimum Mean Square\nError (Bayesian MMSE) estimator to model the probability that one generation is\npreferable to another. Using this estimated probability as a target, we develop\nthe Vote-based Preference Optimization (VPO) framework, which incorporates the\nnumber of votes on both sides to distinguish between controversial and obvious\ngeneration pairs. We show that previous algorithms, such as DPO and Identity\nPreference Optimization (IPO), can be extended using the proposed framework,\ntermed VDPO and VIPO. Our experiments demonstrate that these proposed\nalgorithms outperform various existing methods, including their base\nalgorithms.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.22891v1",
    "published_date": "2024-10-30 10:39:34 UTC",
    "updated_date": "2024-10-30 10:39:34 UTC"
  },
  {
    "arxiv_id": "2410.22886v2",
    "title": "Less is More: Pre-Training Cross-Lingual Small-Scale Language Models with Cognitively-Plausible Curriculum Learning Strategies",
    "authors": [
      "Suchir Salhan",
      "Richard Diehl Martinez",
      "Zébulon Goriely",
      "Paula Buttery"
    ],
    "abstract": "Curriculum Learning has been a popular strategy to improve the cognitive\nplausibility of Small-Scale Language Models (SSLMs) in the BabyLM Challenge.\nHowever, it has not led to considerable improvements over non-curriculum\nmodels. We assess whether theoretical linguistic acquisition theories can be\nused to specify more fine-grained curriculum learning strategies, creating\nage-ordered corpora of Child-Directed Speech for four typologically distant\nlanguage families to implement SSLMs and acquisition-inspired curricula\ncross-lingually. Comparing the success of three objective curricula (Growing,\nInwards and MMM) that precisely replicate the predictions of acquisition\ntheories on a standard SSLM architecture, we find fine-grained\nacquisition-inspired curricula can outperform non-curriculum baselines and\nperformance benefits of curricula strategies in SSLMs can be derived by\nspecifying fine-grained language-specific curricula that precisely replicate\nlanguage acquisition theories.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "BabyLM Shared Task 2024 (Accepted, Poster), co-located in EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.22886v2",
    "published_date": "2024-10-30 10:31:54 UTC",
    "updated_date": "2025-02-21 11:11:37 UTC"
  },
  {
    "arxiv_id": "2410.22884v1",
    "title": "Stealing User Prompts from Mixture of Experts",
    "authors": [
      "Itay Yona",
      "Ilia Shumailov",
      "Jamie Hayes",
      "Nicholas Carlini"
    ],
    "abstract": "Mixture-of-Experts (MoE) models improve the efficiency and scalability of\ndense language models by routing each token to a small number of experts in\neach layer. In this paper, we show how an adversary that can arrange for their\nqueries to appear in the same batch of examples as a victim's queries can\nexploit Expert-Choice-Routing to fully disclose a victim's prompt. We\nsuccessfully demonstrate the effectiveness of this attack on a two-layer\nMixtral model, exploiting the tie-handling behavior of the torch.topk CUDA\nimplementation. Our results show that we can extract the entire prompt using\n$O({VM}^2)$ queries (with vocabulary size $V$ and prompt length $M$) or 100\nqueries on average per token in the setting we consider. This is the first\nattack to exploit architectural flaws for the purpose of extracting user\nprompts, introducing a new class of LLM vulnerabilities.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.22884v1",
    "published_date": "2024-10-30 10:25:35 UTC",
    "updated_date": "2024-10-30 10:25:35 UTC"
  },
  {
    "arxiv_id": "2410.22883v2",
    "title": "Dataset Awareness is not Enough: Implementing Sample-level Tail Encouragement in Long-tailed Self-supervised Learning",
    "authors": [
      "Haowen Xiao",
      "Guanghui Liu",
      "Xinyi Gao",
      "Yang Li",
      "Fengmao Lv",
      "Jielei Chu"
    ],
    "abstract": "Self-supervised learning (SSL) has shown remarkable data representation\ncapabilities across a wide range of datasets. However, when applied to\nreal-world datasets with long-tailed distributions, performance on multiple\ndownstream tasks degrades significantly. Recently, the community has begun to\nfocus more on self-supervised long-tailed learning. Some works attempt to\ntransfer temperature mechanisms to self-supervised learning or use\ncategory-space uniformity constraints to balance the representation of\ndifferent categories in the embedding space to fight against long-tail\ndistributions. However, most of these approaches focus on the joint\noptimization of all samples in the dataset or on constraining the category\ndistribution, with little attention given to whether each individual sample is\noptimally guided during training. To address this issue, we propose Temperature\nAuxiliary Sample-level Encouragement (TASE). We introduce pseudo-labels into\nself-supervised long-tailed learning, utilizing pseudo-label information to\ndrive a dynamic temperature and re-weighting strategy. Specifically, We assign\nan optimal temperature parameter to each sample. Additionally, we analyze the\nlack of quantity awareness in the temperature parameter and use re-weighting to\ncompensate for this deficiency, thereby achieving optimal training patterns at\nthe sample level. Comprehensive experimental results on six benchmarks across\nthree datasets demonstrate that our method achieves outstanding performance in\nimproving long-tail recognition, while also exhibiting high robustness.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.22883v2",
    "published_date": "2024-10-30 10:25:22 UTC",
    "updated_date": "2024-11-15 04:16:01 UTC"
  },
  {
    "arxiv_id": "2410.22881v2",
    "title": "SFA-UNet: More Attention to Multi-Scale Contrast and Contextual Information in Infrared Small Object Segmentation",
    "authors": [
      "Imad Ali Shah",
      "Fahad Mumtaz Malik",
      "Muhammad Waqas Ashraf"
    ],
    "abstract": "Computer vision researchers have extensively worked on fundamental infrared\nvisual recognition for the past few decades. Among various approaches, deep\nlearning has emerged as the most promising candidate. However, Infrared Small\nObject Segmentation (ISOS) remains a major focus due to several challenges\nincluding: 1) the lack of effective utilization of local contrast and global\ncontextual information; 2) the potential loss of small objects in deep models;\nand 3) the struggling to capture fine-grained details and ignore noise. To\naddress these challenges, we propose a modified U-Net architecture, named\nSFA-UNet, by combining Scharr Convolution (SC) and Fast Fourier Convolution\n(FFC) in addition to vertical and horizontal Attention gates (AG) into UNet.\nSFA-UNet utilizes double convolution layers with the addition of SC and FFC in\nits encoder and decoder layers. SC helps to learn the foreground-to-background\ncontrast information whereas FFC provide multi-scale contextual information\nwhile mitigating the small objects vanishing problem. Additionally, the\nintroduction of vertical AGs in encoder layers enhances the model's focus on\nthe targeted object by ignoring irrelevant regions. We evaluated the proposed\napproach on publicly available, SIRST and IRSTD datasets, and achieved superior\nperformance by an average 0.75% with variance of 0.025 of all combined metrics\nin multiple runs as compared to the existing state-of-the-art methods",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted and Presented at PRIP 2023",
    "pdf_url": "http://arxiv.org/pdf/2410.22881v2",
    "published_date": "2024-10-30 10:21:23 UTC",
    "updated_date": "2024-11-16 14:10:33 UTC"
  },
  {
    "arxiv_id": "2410.22874v1",
    "title": "Eliciting Critical Reasoning in Retrieval-Augmented Language Models via Contrastive Explanations",
    "authors": [
      "Leonardo Ranaldi",
      "Marco Valentino",
      "Andrè Freitas"
    ],
    "abstract": "Retrieval-augmented generation (RAG) has emerged as a critical mechanism in\ncontemporary NLP to support Large Language Models(LLMs) in systematically\naccessing richer factual context. However, the integration of RAG mechanisms\nbrings its inherent challenges, as LLMs need to deal with potentially noisy\ncontexts. Recent studies have shown that LLMs still struggle to critically\nanalyse RAG-based in-context information, a limitation that may lead to\nincorrect inferences and hallucinations. In this paper, we investigate how to\nelicit critical reasoning in RAG via contrastive explanations. In particular,\nwe propose Contrastive-RAG (C-RAG), a framework that (i) retrieves relevant\ndocuments given a query, (ii) selects and exemplifies relevant passages, and\n(iii) generates explanations that explicitly contrast the relevance of the\npassages to (iv) support the final answer. We show the impact of C-RAG building\ncontrastive reasoning demonstrations from LLMs to instruct smaller models for\nretrieval-augmented tasks. Extensive experiments demonstrate that C-RAG\nimproves state-of-the-art RAG models while (a) requiring significantly fewer\nprompts and demonstrations and (b) being robust to perturbations in the\nretrieved documents.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.22874v1",
    "published_date": "2024-10-30 10:11:53 UTC",
    "updated_date": "2024-10-30 10:11:53 UTC"
  },
  {
    "arxiv_id": "2410.22870v5",
    "title": "Conditioned quantum-assisted deep generative surrogate for particle-calorimeter interactions",
    "authors": [
      "J. Quetzalcoatl Toledo-Marin",
      "Sebastian Gonzalez",
      "Hao Jia",
      "Ian Lu",
      "Deniz Sogutlu",
      "Abhishek Abhishek",
      "Colin Gay",
      "Eric Paquet",
      "Roger Melko",
      "Geoffrey C. Fox",
      "Maximilian Swiatlowski",
      "Wojciech Fedorko"
    ],
    "abstract": "Particle collisions at accelerators such as the Large Hadron Collider,\nrecorded and analyzed by experiments such as ATLAS and CMS, enable exquisite\nmeasurements of the Standard Model and searches for new phenomena. Simulations\nof collision events at these detectors have played a pivotal role in shaping\nthe design of future experiments and analyzing ongoing ones. However, the quest\nfor accuracy in Large Hadron Collider (LHC) collisions comes at an imposing\ncomputational cost, with projections estimating the need for millions of\nCPU-years annually during the High Luminosity LHC (HL-LHC) run\n\\cite{collaboration2022atlas}. Simulating a single LHC event with\n\\textsc{Geant4} currently devours around 1000 CPU seconds, with simulations of\nthe calorimeter subdetectors in particular imposing substantial computational\ndemands \\cite{rousseau2023experimental}. To address this challenge, we propose\na conditioned quantum-assisted deep generative model. Our model integrates a\nconditioned variational autoencoder (VAE) on the exterior with a conditioned\nRestricted Boltzmann Machine (RBM) in the latent space, providing enhanced\nexpressiveness compared to conventional VAEs. The RBM nodes and connections are\nmeticulously engineered to enable the use of qubits and couplers on D-Wave's\nPegasus-structured \\textit{Advantage} quantum annealer (QA) for sampling. We\nintroduce a novel method for conditioning the quantum-assisted RBM using\n\\textit{flux biases}. We further propose a novel adaptive mapping to estimate\nthe effective inverse temperature in quantum annealers. The effectiveness of\nour framework is illustrated using Dataset 2 of the CaloChallenge\n\\cite{calochallenge}.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "hep-ph",
      "physics.comp-ph",
      "physics.ins-det"
    ],
    "primary_category": "cs.LG",
    "comment": "27 pages, 10 figures, 8 appendices",
    "pdf_url": "http://arxiv.org/pdf/2410.22870v5",
    "published_date": "2024-10-30 10:08:03 UTC",
    "updated_date": "2024-12-18 21:25:06 UTC"
  },
  {
    "arxiv_id": "2410.23323v1",
    "title": "Exploiting Phonological Similarities between African Languages to achieve Speech to Speech Translation",
    "authors": [
      "Peter Ochieng",
      "Dennis Kaburu"
    ],
    "abstract": "This paper presents a pilot study on direct speech-to-speech translation\n(S2ST) by leveraging linguistic similarities among selected African languages\nwithin the same phylum, particularly in cases where traditional data annotation\nis expensive or impractical. We propose a segment-based model that maps speech\nsegments both within and across language phyla, effectively eliminating the\nneed for large paired datasets. By utilizing paired segments and guided\ndiffusion, our model enables translation between any two languages in the\ndataset. We evaluate the model on a proprietary dataset from the Kenya\nBroadcasting Corporation (KBC), which includes five languages: Swahili, Luo,\nKikuyu, Nandi, and English. The model demonstrates competitive performance in\nsegment pairing and translation quality, particularly for languages within the\nsame phylum. Our experiments reveal that segment length significantly\ninfluences translation accuracy, with average-length segments yielding the\nhighest pairing quality. Comparative analyses with traditional cascaded ASR-MT\ntechniques show that the proposed model delivers nearly comparable translation\nperformance. This study underscores the potential of exploiting linguistic\nsimilarities within language groups to perform efficient S2ST, especially in\nlow-resource language contexts.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.23323v1",
    "published_date": "2024-10-30 09:44:52 UTC",
    "updated_date": "2024-10-30 09:44:52 UTC"
  },
  {
    "arxiv_id": "2410.22839v2",
    "title": "Danoliteracy of Generative Large Language Models",
    "authors": [
      "Søren Vejlgaard Holm",
      "Lars Kai Hansen",
      "Martin Carsten Nielsen"
    ],
    "abstract": "The language technology moonshot moment of Generative Large Language Models\n(GLLMs) was not limited to English: These models brought a surge of\ntechnological applications, investments, and hype to low-resource languages as\nwell. However, the capabilities of these models in languages such as Danish\nwere, until recently, difficult to verify beyond qualitative demonstrations due\nto a lack of applicable evaluation corpora. We present a GLLM benchmark to\nevaluate \\emph{Danoliteracy}, a measure of Danish language and cultural\ncompetency across eight diverse scenarios such as Danish citizenship tests and\nabstractive social media question answering. This limited-size benchmark was\nfound to produce a robust ranking that correlates to human feedback at $\\rho\n\\sim 0.8$ with GPT-4 and Claude Opus models achieving the highest rankings.\nAnalyzing these model results across scenarios, we find one strong underlying\nfactor explaining $95\\%$ of scenario performance variance for GLLMs in Danish,\nsuggesting a $g$ factor of model consistency in language adaptation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "16 pages, 13 figures, Accepted to NoDaLiDa/Baltic-HLT 2025",
    "pdf_url": "http://arxiv.org/pdf/2410.22839v2",
    "published_date": "2024-10-30 09:18:31 UTC",
    "updated_date": "2025-03-04 07:13:20 UTC"
  },
  {
    "arxiv_id": "2410.22832v1",
    "title": "HijackRAG: Hijacking Attacks against Retrieval-Augmented Large Language Models",
    "authors": [
      "Yucheng Zhang",
      "Qinfeng Li",
      "Tianyu Du",
      "Xuhong Zhang",
      "Xinkui Zhao",
      "Zhengwen Feng",
      "Jianwei Yin"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) systems enhance large language models\n(LLMs) by integrating external knowledge, making them adaptable and\ncost-effective for various applications. However, the growing reliance on these\nsystems also introduces potential security risks. In this work, we reveal a\nnovel vulnerability, the retrieval prompt hijack attack (HijackRAG), which\nenables attackers to manipulate the retrieval mechanisms of RAG systems by\ninjecting malicious texts into the knowledge database. When the RAG system\nencounters target questions, it generates the attacker's pre-determined answers\ninstead of the correct ones, undermining the integrity and trustworthiness of\nthe system. We formalize HijackRAG as an optimization problem and propose both\nblack-box and white-box attack strategies tailored to different levels of the\nattacker's knowledge. Extensive experiments on multiple benchmark datasets show\nthat HijackRAG consistently achieves high attack success rates, outperforming\nexisting baseline attacks. Furthermore, we demonstrate that the attack is\ntransferable across different retriever models, underscoring the widespread\nrisk it poses to RAG systems. Lastly, our exploration of various defense\nmechanisms reveals that they are insufficient to counter HijackRAG, emphasizing\nthe urgent need for more robust security measures to protect RAG systems in\nreal-world deployments.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.22832v1",
    "published_date": "2024-10-30 09:15:51 UTC",
    "updated_date": "2024-10-30 09:15:51 UTC"
  },
  {
    "arxiv_id": "2411.05809v2",
    "title": "Two pathways to resolve relational inconsistencies",
    "authors": [
      "Tomer Barak",
      "Yonatan Loewenstein"
    ],
    "abstract": "When individuals encounter observations that violate their expectations, when\nwill they adjust their expectations and when will they maintain them despite\nthese observations? For example, when individuals expect objects of type A to\nbe smaller than objects B, but observe the opposite, when will they adjust\ntheir expectation about the relationship between the two objects (to A being\nlarger than B)? Naively, one would predict that the larger the violation, the\ngreater the adaptation. However, experiments reveal that when violations are\nextreme, individuals are more likely to hold on to their prior expectations\nrather than adjust them. To address this puzzle, we tested the adaptation of\nartificial neural networks (ANNs) capable of relational learning and found a\nsimilar phenomenon: Standard learning dynamics dictates that small violations\nwould lead to adjustments of expected relations while larger ones would be\nresolved using a different mechanism -- a change in object representation that\nbypasses the need for adaptation of the relational expectations. These results\nsuggest that the experimentally-observed stability of prior expectations when\nfacing large expectation violations is a natural consequence of learning\ndynamics and does not require any additional mechanisms. We conclude by\ndiscussing the effect of intermediate adaptation steps on this stability.",
    "categories": [
      "q-bio.NC",
      "cs.AI"
    ],
    "primary_category": "q-bio.NC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.05809v2",
    "published_date": "2024-10-30 08:52:50 UTC",
    "updated_date": "2025-03-26 10:06:54 UTC"
  },
  {
    "arxiv_id": "2410.22815v1",
    "title": "Towards Robust and Efficient Federated Low-Rank Adaptation with Heterogeneous Clients",
    "authors": [
      "Jabin Koo",
      "Minwoo Jang",
      "Jungseul Ok"
    ],
    "abstract": "Federated fine-tuning for Large Language Models (LLMs) has recently gained\nattention due to the heavy communication overhead of transmitting large model\nupdates. Low Rank Adaptation (LoRA) has been proposed as a solution, yet its\napplication in federated learning is complicated by discordance in aggregation.\nExisting methods addressing this discordance often suffer from performance\ndegradation at low ranks in heterogeneous data settings. In response, we\nintroduce LoRA-A2 (Low Rank Adaptation with Alternating freeze and Adaptive\nrank selection), which demonstrates robustness in challenging settings with low\nranks and high data heterogeneity. Our experimental findings reveal that\nLoRA-A2 maintains performance even under extreme heterogeneity and low rank\nconditions, achieving up to a 99.8% reduction in uploaded parameters compared\nto full fine-tuning without compromising performance. This adaptive mechanism\nboosts robustness and communication efficiency in federated fine-tuning,\nenabling the practical deployment of LLMs in resource-constrained environments.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.22815v1",
    "published_date": "2024-10-30 08:48:21 UTC",
    "updated_date": "2024-10-30 08:48:21 UTC"
  },
  {
    "arxiv_id": "2410.22812v1",
    "title": "Universality of the $π^2/6$ Pathway in Avoiding Model Collapse",
    "authors": [
      "Apratim Dey",
      "David Donoho"
    ],
    "abstract": "Researchers in empirical machine learning recently spotlighted their fears of\nso-called Model Collapse. They imagined a discard workflow, where an initial\ngenerative model is trained with real data, after which the real data are\ndiscarded, and subsequently, the model generates synthetic data on which a new\nmodel is trained. They came to the conclusion that models degenerate as\nmodel-fitting generations proceed. However, other researchers considered an\naugment workflow, where the original real data continue to be used in each\ngeneration of training, augmented by synthetic data from models fit in all\nearlier generations. Empirical results on canonical datasets and learning\nprocedures confirmed the occurrence of model collapse under the discard\nworkflow and avoidance of model collapse under the augment workflow. Under the\naugment workflow, theoretical evidence also confirmed avoidance in particular\ninstances; specifically, Gerstgrasser et al. (2024) found that for classical\nLinear Regression, test risk at any later generation is bounded by a moderate\nmultiple, viz. pi-squared-over-6 of the test risk of training with the original\nreal data alone. Some commentators questioned the generality of theoretical\nconclusions based on the generative model assumed in Gerstgrasser et al.\n(2024): could similar conclusions be reached for other task/model pairings? In\nthis work, we demonstrate the universality of the pi-squared-over-6 augment\nrisk bound across a large family of canonical statistical models, offering key\ninsights into exactly why collapse happens under the discard workflow and is\navoided under the augment workflow. In the process, we provide a framework that\nis able to accommodate a large variety of workflows (beyond discard and\naugment), thereby enabling an experimenter to judge the comparative merits of\nmultiple different workflows by simulating a simple Gaussian process.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.ET",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "primary_category": "cs.LG",
    "comment": "30 pages",
    "pdf_url": "http://arxiv.org/pdf/2410.22812v1",
    "published_date": "2024-10-30 08:44:10 UTC",
    "updated_date": "2024-10-30 08:44:10 UTC"
  },
  {
    "arxiv_id": "2410.22809v1",
    "title": "Causality-Enhanced Behavior Sequence Modeling in LLMs for Personalized Recommendation",
    "authors": [
      "Yang Zhang",
      "Juntao You",
      "Yimeng Bai",
      "Jizhi Zhang",
      "Keqin Bao",
      "Wenjie Wang",
      "Tat-Seng Chua"
    ],
    "abstract": "Recent advancements in recommender systems have focused on leveraging Large\nLanguage Models (LLMs) to improve user preference modeling, yielding promising\noutcomes. However, current LLM-based approaches struggle to fully leverage user\nbehavior sequences, resulting in suboptimal preference modeling for\npersonalized recommendations. In this study, we propose a novel Counterfactual\nFine-Tuning (CFT) method to address this issue by explicitly emphasizing the\nrole of behavior sequences when generating recommendations. Specifically, we\nemploy counterfactual reasoning to identify the causal effects of behavior\nsequences on model output and introduce a task that directly fits the\nground-truth labels based on these effects, achieving the goal of explicit\nemphasis. Additionally, we develop a token-level weighting mechanism to adjust\nthe emphasis strength for different item tokens, reflecting the diminishing\ninfluence of behavior sequences from earlier to later tokens during predicting\nan item. Extensive experiments on real-world datasets demonstrate that CFT\neffectively improves behavior sequence modeling. Our codes are available at\nhttps://github.com/itsmeyjt/CFT.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.22809v1",
    "published_date": "2024-10-30 08:41:13 UTC",
    "updated_date": "2024-10-30 08:41:13 UTC"
  },
  {
    "arxiv_id": "2410.22805v1",
    "title": "Run-Time Adaptation of Neural Beamforming for Robust Speech Dereverberation and Denoising",
    "authors": [
      "Yoto Fujita",
      "Aditya Arie Nugraha",
      "Diego Di Carlo",
      "Yoshiaki Bando",
      "Mathieu Fontaine",
      "Kazuyoshi Yoshii"
    ],
    "abstract": "This paper describes speech enhancement for realtime automatic speech\nrecognition (ASR) in real environments. A standard approach to this task is to\nuse neural beamforming that can work efficiently in an online manner. It\nestimates the masks of clean dry speech from a noisy echoic mixture spectrogram\nwith a deep neural network (DNN) and then computes a enhancement filter used\nfor beamforming. The performance of such a supervised approach, however, is\ndrastically degraded under mismatched conditions. This calls for run-time\nadaptation of the DNN. Although the ground-truth speech spectrogram required\nfor adaptation is not available at run time, blind dereverberation and\nseparation methods such as weighted prediction error (WPE) and fast\nmultichannel nonnegative matrix factorization (FastMNMF) can be used for\ngenerating pseudo groundtruth data from a mixture. Based on this idea, a prior\nwork proposed a dual-process system based on a cascade of WPE and minimum\nvariance distortionless response (MVDR) beamforming asynchronously fine-tuned\nby block-online FastMNMF. To integrate the dereverberation capability into\nneural beamforming and make it fine-tunable at run time, we propose to use\nweighted power minimization distortionless response (WPD) beamforming, a\nunified version of WPE and minimum power distortionless response (MPDR), whose\njoint dereverberation and denoising filter is estimated using a DNN. We\nevaluated the impact of run-time adaptation under various conditions with\ndifferent numbers of speakers, reverberation times, and signal-to-noise ratios\n(SNRs).",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted to APSIPA2024",
    "pdf_url": "http://arxiv.org/pdf/2410.22805v1",
    "published_date": "2024-10-30 08:32:47 UTC",
    "updated_date": "2024-10-30 08:32:47 UTC"
  },
  {
    "arxiv_id": "2410.22803v1",
    "title": "DOA-Aware Audio-Visual Self-Supervised Learning for Sound Event Localization and Detection",
    "authors": [
      "Yoto Fujita",
      "Yoshiaki Bando",
      "Keisuke Imoto",
      "Masaki Onishi",
      "Kazuyoshi Yoshii"
    ],
    "abstract": "This paper describes sound event localization and detection (SELD) for\nspatial audio recordings captured by firstorder ambisonics (FOA) microphones.\nIn this task, one may train a deep neural network (DNN) using FOA data\nannotated with the classes and directions of arrival (DOAs) of sound events.\nHowever, the performance of this approach is severely bounded by the amount of\nannotated data. To overcome this limitation, we propose a novel method of\npretraining the feature extraction part of the DNN in a self-supervised manner.\nWe use spatial audio-visual recordings abundantly available as virtual reality\ncontents. Assuming that sound objects are concurrently observed by the FOA\nmicrophones and the omni-directional camera, we jointly train audio and visual\nencoders with contrastive learning such that the audio and visual embeddings of\nthe same recording and DOA are made close. A key feature of our method is that\nthe DOA-wise audio embeddings are jointly extracted from the raw audio data,\nwhile the DOA-wise visual embeddings are separately extracted from the local\nvisual crops centered on the corresponding DOA. This encourages the latent\nfeatures of the audio encoder to represent both the classes and DOAs of sound\nevents. The experiment using the DCASE2022 Task 3 dataset of 20 hours shows\nnon-annotated audio-visual recordings of 100 hours reduced the error score of\nSELD from 36.4 pts to 34.9 pts.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "cs.MM",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted to APSIPA2023",
    "pdf_url": "http://arxiv.org/pdf/2410.22803v1",
    "published_date": "2024-10-30 08:31:58 UTC",
    "updated_date": "2024-10-30 08:31:58 UTC"
  },
  {
    "arxiv_id": "2410.22790v1",
    "title": "Dual Contrastive Transformer for Hierarchical Preference Modeling in Sequential Recommendation",
    "authors": [
      "Chengkai Huang",
      "Shoujin Wang",
      "Xianzhi Wang",
      "Lina Yao"
    ],
    "abstract": "Sequential recommender systems (SRSs) aim to predict the subsequent items\nwhich may interest users via comprehensively modeling users' complex preference\nembedded in the sequence of user-item interactions. However, most of existing\nSRSs often model users' single low-level preference based on item ID\ninformation while ignoring the high-level preference revealed by item attribute\ninformation, such as item category. Furthermore, they often utilize limited\nsequence context information to predict the next item while overlooking richer\ninter-item semantic relations. To this end, in this paper, we proposed a novel\nhierarchical preference modeling framework to substantially model the complex\nlow- and high-level preference dynamics for accurate sequential recommendation.\nSpecifically, in the framework, a novel dual-transformer module and a novel\ndual contrastive learning scheme have been designed to discriminatively learn\nusers' low- and high-level preference and to effectively enhance both low- and\nhigh-level preference learning respectively. In addition, a novel\nsemantics-enhanced context embedding module has been devised to generate more\ninformative context embedding for further improving the recommendation\nperformance. Extensive experiments on six real-world datasets have demonstrated\nboth the superiority of our proposed method over the state-of-the-art ones and\nthe rationality of our design.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.22790v1",
    "published_date": "2024-10-30 08:09:33 UTC",
    "updated_date": "2024-10-30 08:09:33 UTC"
  },
  {
    "arxiv_id": "2411.00848v1",
    "title": "Evaluating Evidential Reliability In Pattern Recognition Based On Intuitionistic Fuzzy Sets",
    "authors": [
      "Juntao Xu",
      "Tianxiang Zhan",
      "Yong Deng"
    ],
    "abstract": "Determining the reliability of evidence sources is a crucial topic in\nDempster-Shafer theory (DST). Previous approaches have addressed high conflicts\nbetween evidence sources using discounting methods, but these methods may not\nensure the high efficiency of classification models. In this paper, we consider\nthe combination of DS theory and Intuitionistic Fuzzy Sets (IFS) and propose an\nalgorithm for quantifying the reliability of evidence sources, called Fuzzy\nReliability Index (FRI). The FRI algorithm is based on decision quantification\nrules derived from IFS, defining the contribution of different BPAs to correct\ndecisions and deriving the evidential reliability from these contributions. The\nproposed method effectively enhances the rationality of reliability estimation\nfor evidence sources, making it particularly suitable for classification\ndecision problems in complex scenarios. Subsequent comparisons with DST-based\nalgorithms and classical machine learning algorithms demonstrate the\nsuperiority and generalizability of the FRI algorithm. The FRI algorithm\nprovides a new perspective for future decision probability conversion and\nreliability analysis of evidence sources.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "35 pages",
    "pdf_url": "http://arxiv.org/pdf/2411.00848v1",
    "published_date": "2024-10-30 08:05:26 UTC",
    "updated_date": "2024-10-30 08:05:26 UTC"
  },
  {
    "arxiv_id": "2410.22784v2",
    "title": "Contrastive Learning and Adversarial Disentanglement for Task-Oriented Semantic Communications",
    "authors": [
      "Omar Erak",
      "Omar Alhussein",
      "Wen Tong"
    ],
    "abstract": "Task-oriented semantic communication systems have emerged as a promising\napproach to achieving efficient and intelligent data transmission, where only\ninformation relevant to a specific task is communicated. However, existing\nmethods struggle to fully disentangle task-relevant and task-irrelevant\ninformation, leading to privacy concerns and subpar performance. To address\nthis, we propose an information-bottleneck method, named CLAD (contrastive\nlearning and adversarial disentanglement). CLAD utilizes contrastive learning\nto effectively capture task-relevant features while employing adversarial\ndisentanglement to discard task-irrelevant information. Additionally, due to\nthe lack of reliable and reproducible methods to gain insight into the\ninformativeness and minimality of the encoded feature vectors, we introduce a\nnew technique to compute the information retention index (IRI), a comparative\nmetric used as a proxy for the mutual information between the encoded features\nand the input, reflecting the minimality of the encoded features. The IRI\nquantifies the minimality and informativeness of the encoded feature vectors\nacross different task-oriented communication techniques. Our extensive\nexperiments demonstrate that CLAD outperforms state-of-the-art baselines in\nterms of semantic extraction, task performance, privacy preservation, and IRI.\nCLAD achieves a predictive performance improvement of around 2.5-3%, along with\na 77-90% reduction in IRI and a 57-76% decrease in adversarial attribute\ninference attack accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.IT",
      "eess.IV",
      "math.IT"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.22784v2",
    "published_date": "2024-10-30 07:59:52 UTC",
    "updated_date": "2025-04-25 11:17:27 UTC"
  },
  {
    "arxiv_id": "2411.00049v1",
    "title": "Rule by Rule: Learning with Confidence through Vocabulary Expansion",
    "authors": [
      "Albert Nössig",
      "Tobias Hell",
      "Georg Moser"
    ],
    "abstract": "In this paper, we present an innovative iterative approach to rule learning\nspecifically designed for (but not limited to) text-based data. Our method\nfocuses on progressively expanding the vocabulary utilized in each iteration\nresulting in a significant reduction of memory consumption. Moreover, we\nintroduce a Value of Confidence as an indicator of the reliability of the\ngenerated rules. By leveraging the Value of Confidence, our approach ensures\nthat only the most robust and trustworthy rules are retained, thereby improving\nthe overall quality of the rule learning process. We demonstrate the\neffectiveness of our method through extensive experiments on various textual as\nwell as non-textual datasets including a use case of significant interest to\ninsurance industries, showcasing its potential for real-world applications.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "29 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.00049v1",
    "published_date": "2024-10-30 07:54:01 UTC",
    "updated_date": "2024-10-30 07:54:01 UTC"
  },
  {
    "arxiv_id": "2410.22772v1",
    "title": "Reliability Assessment of Information Sources Based on Random Permutation Set",
    "authors": [
      "Juntao Xu",
      "Tianxiang Zhan",
      "Yong Deng"
    ],
    "abstract": "In pattern recognition, handling uncertainty is a critical challenge that\nsignificantly affects decision-making and classification accuracy.\nDempster-Shafer Theory (DST) is an effective reasoning framework for addressing\nuncertainty, and the Random Permutation Set (RPS) extends DST by additionally\nconsidering the internal order of elements, forming a more ordered extension of\nDST. However, there is a lack of a transformation method based on permutation\norder between RPS and DST, as well as a sequence-based probability\ntransformation method for RPS. Moreover, the reliability of RPS sources remains\nan issue that requires attention. To address these challenges, this paper\nproposes an RPS transformation approach and a probability transformation method\ntailored for RPS. On this basis, a reliability computation method for RPS\nsources, based on the RPS probability transformation, is introduced and applied\nto pattern recognition. Experimental results demonstrate that the proposed\napproach effectively bridges the gap between DST and RPS and achieves superior\nrecognition accuracy in classification problems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages",
    "pdf_url": "http://arxiv.org/pdf/2410.22772v1",
    "published_date": "2024-10-30 07:40:35 UTC",
    "updated_date": "2024-10-30 07:40:35 UTC"
  },
  {
    "arxiv_id": "2410.22770v3",
    "title": "InjecGuard: Benchmarking and Mitigating Over-defense in Prompt Injection Guardrail Models",
    "authors": [
      "Hao Li",
      "Xiaogeng Liu"
    ],
    "abstract": "Prompt injection attacks pose a critical threat to large language models\n(LLMs), enabling goal hijacking and data leakage. Prompt guard models, though\neffective in defense, suffer from over-defense -- falsely flagging benign\ninputs as malicious due to trigger word bias. To address this issue, we\nintroduce NotInject, an evaluation dataset that systematically measures\nover-defense across various prompt guard models. NotInject contains 339 benign\nsamples enriched with trigger words common in prompt injection attacks,\nenabling fine-grained evaluation. Our results show that state-of-the-art models\nsuffer from over-defense issues, with accuracy dropping close to random\nguessing levels (60%). To mitigate this, we propose InjecGuard, a novel prompt\nguard model that incorporates a new training strategy, Mitigating Over-defense\nfor Free (MOF), which significantly reduces the bias on trigger words.\nInjecGuard demonstrates state-of-the-art performance on diverse benchmarks\nincluding NotInject, surpassing the existing best model by 30.8%, offering a\nrobust and open-source solution for detecting prompt injection attacks. The\ncode and datasets are released at https://github.com/leolee99/InjecGuard.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.22770v3",
    "published_date": "2024-10-30 07:39:42 UTC",
    "updated_date": "2025-03-30 16:39:15 UTC"
  },
  {
    "arxiv_id": "2410.22767v1",
    "title": "Beyond Ontology in Dialogue State Tracking for Goal-Oriented Chatbot",
    "authors": [
      "Sejin Lee",
      "Dongha Kim",
      "Min Song"
    ],
    "abstract": "Goal-oriented chatbots are essential for automating user tasks, such as\nbooking flights or making restaurant reservations. A key component of these\nsystems is Dialogue State Tracking (DST), which interprets user intent and\nmaintains the dialogue state. However, existing DST methods often rely on fixed\nontologies and manually compiled slot values, limiting their adaptability to\nopen-domain dialogues. We propose a novel approach that leverages instruction\ntuning and advanced prompt strategies to enhance DST performance, without\nrelying on any predefined ontologies. Our method enables Large Language Model\n(LLM) to infer dialogue states through carefully designed prompts and includes\nan anti-hallucination mechanism to ensure accurate tracking in diverse\nconversation contexts. Additionally, we employ a Variational Graph Auto-Encoder\n(VGAE) to model and predict subsequent user intent. Our approach achieved\nstate-of-the-art with a JGA of 42.57% outperforming existing ontology-less DST\nmodels, and performed well in open-domain real-world conversations. This work\npresents a significant advancement in creating more adaptive and accurate\ngoal-oriented chatbots.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "There are 10 chapters, including references, and 2 figures used. To\n  be presented at the 15th IEEE International Conference on Knowledge Graphs\n  (ICKG2024)",
    "pdf_url": "http://arxiv.org/pdf/2410.22767v1",
    "published_date": "2024-10-30 07:36:23 UTC",
    "updated_date": "2024-10-30 07:36:23 UTC"
  },
  {
    "arxiv_id": "2410.22766v1",
    "title": "Self-Driving Car Racing: Application of Deep Reinforcement Learning",
    "authors": [
      "Florentiana Yuwono",
      "Gan Pang Yen",
      "Jason Christopher"
    ],
    "abstract": "This paper explores the application of deep reinforcement learning (RL)\ntechniques in the domain of autonomous self-driving car racing. Motivated by\nthe rise of AI-driven mobility and autonomous racing events, the project aims\nto develop an AI agent that efficiently drives a simulated car in the OpenAI\nGymnasium CarRacing environment. We investigate various RL algorithms,\nincluding Deep Q-Network (DQN), Proximal Policy Optimization (PPO), and novel\nadaptations that incorporate transfer learning and recurrent neural networks\n(RNNs) for enhanced performance. The project demonstrates that while DQN\nprovides a strong baseline for policy learning, integrating ResNet and LSTM\nmodels significantly improves the agent's ability to capture complex spatial\nand temporal dynamics. PPO, particularly in continuous action spaces, shows\npromising results for fine control, although challenges such as policy collapse\nremain. We compare the performance of these approaches and outline future\nresearch directions focused on improving computational efficiency and\naddressing model stability. Our findings contribute to the ongoing development\nof AI systems in autonomous driving and related control tasks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.22766v1",
    "published_date": "2024-10-30 07:32:25 UTC",
    "updated_date": "2024-10-30 07:32:25 UTC"
  },
  {
    "arxiv_id": "2410.22752v1",
    "title": "SoftCTRL: Soft conservative KL-control of Transformer Reinforcement Learning for Autonomous Driving",
    "authors": [
      "Minh Tri Huynh",
      "Duc Dung Nguyen"
    ],
    "abstract": "In recent years, motion planning for urban self-driving cars (SDV) has become\na popular problem due to its complex interaction of road components. To tackle\nthis, many methods have relied on large-scale, human-sampled data processed\nthrough Imitation learning (IL). Although effective, IL alone cannot adequately\nhandle safety and reliability concerns. Combining IL with Reinforcement\nlearning (RL) by adding KL divergence between RL and IL policy to the RL loss\ncan alleviate IL's weakness but suffer from over-conservation caused by\ncovariate shift of IL. To address this limitation, we introduce a method that\ncombines IL with RL using an implicit entropy-KL control that offers a simple\nway to reduce the over-conservation characteristic. In particular, we validate\ndifferent challenging simulated urban scenarios from the unseen dataset,\nindicating that although IL can perform well in imitation tasks, our proposed\nmethod significantly improves robustness (over 17\\% reduction in failures) and\ngenerates human-like driving behavior.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "submitted to IEEE Open Journal of Intelligent Transportation Systems",
    "pdf_url": "http://arxiv.org/pdf/2410.22752v1",
    "published_date": "2024-10-30 07:18:00 UTC",
    "updated_date": "2024-10-30 07:18:00 UTC"
  },
  {
    "arxiv_id": "2410.22744v1",
    "title": "Designing AI Personalities: Enhancing Human-Agent Interaction Through Thoughtful Persona Design",
    "authors": [
      "Nima Zargham",
      "Mateusz Dubiel",
      "Smit Desai",
      "Thomas Mildner",
      "Hanz-Joachim Belz"
    ],
    "abstract": "In the rapidly evolving field of artificial intelligence (AI) agents,\ndesigning the agent's characteristics is crucial for shaping user experience.\nThis workshop aims to establish a research community focused on AI agent\npersona design for various contexts, such as in-car assistants, educational\ntools, and smart home environments. We will explore critical aspects of persona\ndesign, such as voice, embodiment, and demographics, and their impact on user\nsatisfaction and engagement. Through discussions and hands-on activities, we\naim to propose practices and standards that enhance the ecological validity of\nagent personas. Topics include the design of conversational interfaces, the\ninfluence of agent personas on user experience, and approaches for creating\ncontextually appropriate AI agents. This workshop will provide a platform for\nbuilding a community dedicated to developing AI agent personas that better fit\ndiverse, everyday interactions.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "8 pages, the workshop accepted at the 23rd International Conference\n  on Mobile and Ubiquitous Multimedia (MUM 2024)",
    "pdf_url": "http://arxiv.org/pdf/2410.22744v1",
    "published_date": "2024-10-30 06:58:59 UTC",
    "updated_date": "2024-10-30 06:58:59 UTC"
  },
  {
    "arxiv_id": "2410.22732v1",
    "title": "st-DTPM: Spatial-Temporal Guided Diffusion Transformer Probabilistic Model for Delayed Scan PET Image Prediction",
    "authors": [
      "Ran Hong",
      "Yuxia Huang",
      "Lei Liu",
      "Zhonghui Wu",
      "Bingxuan Li",
      "Xuemei Wang",
      "Qiegen Liu"
    ],
    "abstract": "PET imaging is widely employed for observing biological metabolic activities\nwithin the human body. However, numerous benign conditions can cause increased\nuptake of radiopharmaceuticals, confounding differentiation from malignant\ntumors. Several studies have indicated that dual-time PET imaging holds promise\nin distinguishing between malignant and benign tumor processes. Nevertheless,\nthe hour-long distribution period of radiopharmaceuticals post-injection\ncomplicates the determination of optimal timing for the second scan, presenting\nchallenges in both practical applications and research. Notably, we have\nidentified that delay time PET imaging can be framed as an image-to-image\nconversion problem. Motivated by this insight, we propose a novel\nspatial-temporal guided diffusion transformer probabilistic model (st-DTPM) to\nsolve dual-time PET imaging prediction problem. Specifically, this architecture\nleverages the U-net framework that integrates patch-wise features of CNN and\npixel-wise relevance of Transformer to obtain local and global information. And\nthen employs a conditional DDPM model for image synthesis. Furthermore, on\nspatial condition, we concatenate early scan PET images and noisy PET images on\nevery denoising step to guide the spatial distribution of denoising sampling.\nOn temporal condition, we convert diffusion time steps and delay time to a\nuniversal time vector, then embed it to each layer of model architecture to\nfurther improve the accuracy of predictions. Experimental results demonstrated\nthe superiority of our method over alternative approaches in preserving image\nquality and structural information, thereby affirming its efficacy in\npredictive task.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.22732v1",
    "published_date": "2024-10-30 06:37:55 UTC",
    "updated_date": "2024-10-30 06:37:55 UTC"
  },
  {
    "arxiv_id": "2410.22728v1",
    "title": "Offline Behavior Distillation",
    "authors": [
      "Shiye Lei",
      "Sen Zhang",
      "Dacheng Tao"
    ],
    "abstract": "Massive reinforcement learning (RL) data are typically collected to train\npolicies offline without the need for interactions, but the large data volume\ncan cause training inefficiencies. To tackle this issue, we formulate offline\nbehavior distillation (OBD), which synthesizes limited expert behavioral data\nfrom sub-optimal RL data, enabling rapid policy learning. We propose two naive\nOBD objectives, DBC and PBC, which measure distillation performance via the\ndecision difference between policies trained on distilled data and either\noffline data or a near-expert policy. Due to intractable bi-level optimization,\nthe OBD objective is difficult to minimize to small values, which deteriorates\nPBC by its distillation performance guarantee with quadratic discount\ncomplexity $\\mathcal{O}(1/(1-\\gamma)^2)$. We theoretically establish the\nequivalence between the policy performance and action-value weighted decision\ndifference, and introduce action-value weighted PBC (Av-PBC) as a more\neffective OBD objective. By optimizing the weighted decision difference, Av-PBC\nachieves a superior distillation guarantee with linear discount complexity\n$\\mathcal{O}(1/(1-\\gamma))$. Extensive experiments on multiple D4RL datasets\nreveal that Av-PBC offers significant improvements in OBD performance, fast\ndistillation convergence speed, and robust cross-architecture/optimizer\ngeneralization.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.22728v1",
    "published_date": "2024-10-30 06:28:09 UTC",
    "updated_date": "2024-10-30 06:28:09 UTC"
  },
  {
    "arxiv_id": "2411.00845v1",
    "title": "End-to-end Graph Learning Approach for Cognitive Diagnosis of Student Tutorial",
    "authors": [
      "Fulai Yang",
      "Di Wu",
      "Yi He",
      "Li Tao",
      "Xin Luo"
    ],
    "abstract": "Cognitive diagnosis (CD) utilizes students' existing studying records to\nestimate their mastery of unknown knowledge concepts, which is vital for\nevaluating their learning abilities. Accurate CD is extremely challenging\nbecause CD is associated with complex relationships and mechanisms among\nstudents, knowledge concepts, studying records, etc. However, existing\napproaches loosely consider these relationships and mechanisms by a\nnon-end-to-end learning framework, resulting in sub-optimal feature extractions\nand fusions for CD. Different from them, this paper innovatively proposes an\nEnd-to-end Graph Neural Networks-based Cognitive Diagnosis (EGNN-CD) model.\nEGNN-CD consists of three main parts: knowledge concept network (KCN), graph\nneural networks-based feature extraction (GNNFE), and cognitive ability\nprediction (CAP). First, KCN constructs CD-related interaction by\ncomprehensively extracting physical information from students, exercises, and\nknowledge concepts. Second, a four-channel GNNFE is designed to extract\nhigh-order and individual features from the constructed KCN. Finally, CAP\nemploys a multi-layer perceptron to fuse the extracted features to predict\nstudents' learning abilities in an end-to-end learning way. With such designs,\nthe feature extractions and fusions are guaranteed to be comprehensive and\noptimal for CD. Extensive experiments on three real datasets demonstrate that\nour EGNN-CD achieves significantly higher accuracy than state-of-the-art models\nin CD.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.00845v1",
    "published_date": "2024-10-30 06:18:47 UTC",
    "updated_date": "2024-10-30 06:18:47 UTC"
  },
  {
    "arxiv_id": "2410.22707v1",
    "title": "Robotic State Recognition with Image-to-Text Retrieval Task of Pre-Trained Vision-Language Model and Black-Box Optimization",
    "authors": [
      "Kento Kawaharazuka",
      "Yoshiki Obinata",
      "Naoaki Kanazawa",
      "Kei Okada",
      "Masayuki Inaba"
    ],
    "abstract": "State recognition of the environment and objects, such as the open/closed\nstate of doors and the on/off of lights, is indispensable for robots that\nperform daily life support and security tasks. Until now, state recognition\nmethods have been based on training neural networks from manual annotations,\npreparing special sensors for the recognition, or manually programming to\nextract features from point clouds or raw images. In contrast, we propose a\nrobotic state recognition method using a pre-trained vision-language model,\nwhich is capable of Image-to-Text Retrieval (ITR) tasks. We prepare several\nkinds of language prompts in advance, calculate the similarity between these\nprompts and the current image by ITR, and perform state recognition. By\napplying the optimal weighting to each prompt using black-box optimization,\nstate recognition can be performed with higher accuracy. Experiments show that\nthis theory enables a variety of state recognitions by simply preparing\nmultiple prompts without retraining neural networks or manual programming. In\naddition, since only prompts and their weights need to be prepared for each\nrecognizer, there is no need to prepare multiple models, which facilitates\nresource management. It is possible to recognize the open/closed state of\ntransparent doors, the state of whether water is running or not from a faucet,\nand even the qualitative state of whether a kitchen is clean or not, which have\nbeen challenging so far, through language.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted at Humanoids2024",
    "pdf_url": "http://arxiv.org/pdf/2410.22707v1",
    "published_date": "2024-10-30 05:34:52 UTC",
    "updated_date": "2024-10-30 05:34:52 UTC"
  },
  {
    "arxiv_id": "2410.22695v1",
    "title": "Permutation Invariant Learning with High-Dimensional Particle Filters",
    "authors": [
      "Akhilan Boopathy",
      "Aneesh Muppidi",
      "Peggy Yang",
      "Abhiram Iyer",
      "William Yue",
      "Ila Fiete"
    ],
    "abstract": "Sequential learning in deep models often suffers from challenges such as\ncatastrophic forgetting and loss of plasticity, largely due to the permutation\ndependence of gradient-based algorithms, where the order of training data\nimpacts the learning outcome. In this work, we introduce a novel\npermutation-invariant learning framework based on high-dimensional particle\nfilters. We theoretically demonstrate that particle filters are invariant to\nthe sequential ordering of training minibatches or tasks, offering a principled\nsolution to mitigate catastrophic forgetting and loss-of-plasticity. We develop\nan efficient particle filter for optimizing high-dimensional models, combining\nthe strengths of Bayesian methods with gradient-based optimization. Through\nextensive experiments on continual supervised and reinforcement learning\nbenchmarks, including SplitMNIST, SplitCIFAR100, and ProcGen, we empirically\nshow that our method consistently improves performance, while reducing variance\ncompared to standard baselines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Website: https://aneeshers.github.io/PermutationInvariantLearning/",
    "pdf_url": "http://arxiv.org/pdf/2410.22695v1",
    "published_date": "2024-10-30 05:06:55 UTC",
    "updated_date": "2024-10-30 05:06:55 UTC"
  },
  {
    "arxiv_id": "2411.02530v1",
    "title": "A Comprehensive Study on Quantization Techniques for Large Language Models",
    "authors": [
      "Jiedong Lang",
      "Zhehao Guo",
      "Shuyu Huang"
    ],
    "abstract": "Large Language Models (LLMs) have been extensively researched and used in\nboth academia and industry since the rise in popularity of the Transformer\nmodel, which demonstrates excellent performance in AI. However, the\ncomputational demands of LLMs are immense, and the energy resources required to\nrun them are often limited. For instance, popular models like GPT-3, with 175\nbillion parameters and a storage requirement of 350 GB, present significant\nchallenges for deployment on resource-constrained IoT devices and embedded\nsystems. These systems often lack the computational capacity to handle such\nlarge models. Quantization, a technique that reduces the precision of model\nvalues to a smaller set of discrete values, offers a promising solution by\nreducing the size of LLMs and accelerating inference. In this research, we\nprovide a comprehensive analysis of quantization techniques within the machine\nlearning field, with a particular focus on their application to LLMs. We begin\nby exploring the mathematical theory of quantization, followed by a review of\ncommon quantization methods and how they are implemented. Furthermore, we\nexamine several prominent quantization methods applied to LLMs, detailing their\nalgorithms and performance outcomes.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02530v1",
    "published_date": "2024-10-30 04:55:26 UTC",
    "updated_date": "2024-10-30 04:55:26 UTC"
  },
  {
    "arxiv_id": "2410.22690v3",
    "title": "Choice Between Partial Trajectories: Disentangling Goals from Beliefs",
    "authors": [
      "Henrik Marklund",
      "Benjamin Van Roy"
    ],
    "abstract": "As AI agents generate increasingly sophisticated behaviors, manually encoding\nhuman preferences to guide these agents becomes more challenging. To address\nthis, it has been suggested that agents instead learn preferences from human\nchoice data. This approach requires a model of choice behavior that the agent\ncan use to interpret the data. For choices between partial trajectories of\nstates and actions, previous models assume choice probabilities are determined\nby the partial return or the cumulative advantage.\n  We consider an alternative model based instead on the bootstrapped return,\nwhich adds to the partial return an estimate of the future return. Benefits of\nthe bootstrapped return model stem from its treatment of human beliefs. Unlike\npartial return, choices based on bootstrapped return reflect human beliefs\nabout the environment. Further, while recovering the reward function from\nchoices based on cumulative advantage requires that those beliefs are correct,\ndoing so from choices based on bootstrapped return does not.\n  To motivate the bootstrapped return model, we formulate axioms and prove an\nAlignment Theorem. This result formalizes how, for a general class of\npreferences, such models are able to disentangle goals from beliefs. This\nensures recovery of an aligned reward function when learning from choices based\non bootstrapped return.\n  The bootstrapped return model also affords greater robustness to choice\nbehavior. Even when choices are based on partial return, learning via a\nbootstrapped return model recovers an aligned reward function. The same holds\nwith choices based on the cumulative advantage if the human and the agent both\nadhere to correct and consistent beliefs about the environment. On the other\nhand, if choices are based on bootstrapped return, learning via partial return\nor cumulative advantage models does not generally produce an aligned reward\nfunction.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.22690v3",
    "published_date": "2024-10-30 04:52:22 UTC",
    "updated_date": "2024-12-21 13:42:14 UTC"
  },
  {
    "arxiv_id": "2410.23320v1",
    "title": "Lina-Speech: Gated Linear Attention is a Fast and Parameter-Efficient Learner for text-to-speech synthesis",
    "authors": [
      "Théodor Lemerle",
      "Harrison Vanderbyl",
      "Vaibhav Srivastav",
      "Nicolas Obin",
      "Axel Roebel"
    ],
    "abstract": "Neural codec language models have achieved state-of-the-art performance in\ntext-to-speech (TTS) synthesis, leveraging scalable architectures like\nautoregressive transformers and large-scale speech datasets. By framing voice\ncloning as a prompt continuation task, these models excel at cloning voices\nfrom short audio samples. However, this approach is limited in its ability to\nhandle numerous or lengthy speech excerpts, since the concatenation of source\nand target speech must fall within the maximum context length which is\ndetermined during training. In this work, we introduce Lina-Speech, a model\nthat replaces traditional self-attention mechanisms with emerging recurrent\narchitectures like Gated Linear Attention (GLA). Building on the success of\ninitial-state tuning on RWKV, we extend this technique to voice cloning,\nenabling the use of multiple speech samples and full utilization of the context\nwindow in synthesis. This approach is fast, easy to deploy, and achieves\nperformance comparable to fine-tuned baselines when the dataset size ranges\nfrom 3 to 15 minutes. Notably, Lina-Speech matches or outperforms\nstate-of-the-art baseline models, including some with a parameter count up to\nfour times higher or trained in an end-to-end style. We release our code and\ncheckpoints. Audio samples are available at\nhttps://theodorblackbird.github.io/blog/demo_lina/.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2410.23320v1",
    "published_date": "2024-10-30 04:50:40 UTC",
    "updated_date": "2024-10-30 04:50:40 UTC"
  },
  {
    "arxiv_id": "2410.22689v1",
    "title": "Multi-Task Interactive Robot Fleet Learning with Visual World Models",
    "authors": [
      "Huihan Liu",
      "Yu Zhang",
      "Vaarij Betala",
      "Evan Zhang",
      "James Liu",
      "Crystal Ding",
      "Yuke Zhu"
    ],
    "abstract": "Recent advancements in large-scale multi-task robot learning offer the\npotential for deploying robot fleets in household and industrial settings,\nenabling them to perform diverse tasks across various environments. However,\nAI-enabled robots often face challenges with generalization and robustness when\nexposed to real-world variability and uncertainty. We introduce Sirius-Fleet, a\nmulti-task interactive robot fleet learning framework to address these\nchallenges. Sirius-Fleet monitors robot performance during deployment and\ninvolves humans to correct the robot's actions when necessary. We employ a\nvisual world model to predict the outcomes of future actions and build anomaly\npredictors to predict whether they will likely result in anomalies. As the\nrobot autonomy improves, the anomaly predictors automatically adapt their\nprediction criteria, leading to fewer requests for human intervention and\ngradually reducing human workload over time. Evaluations on large-scale\nbenchmarks demonstrate Sirius-Fleet's effectiveness in improving multi-task\npolicy performance and monitoring accuracy. We demonstrate Sirius-Fleet's\nperformance in both RoboCasa in simulation and Mutex in the real world, two\ndiverse, large-scale multi-task benchmarks. More information is available on\nthe project website: https://ut-austin-rpl.github.io/sirius-fleet",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "In Proceedings of CoRL 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.22689v1",
    "published_date": "2024-10-30 04:49:39 UTC",
    "updated_date": "2024-10-30 04:49:39 UTC"
  },
  {
    "arxiv_id": "2410.22685v1",
    "title": "Improving Uncertainty Quantification in Large Language Models via Semantic Embeddings",
    "authors": [
      "Yashvir S. Grewal",
      "Edwin V. Bonilla",
      "Thang D. Bui"
    ],
    "abstract": "Accurately quantifying uncertainty in large language models (LLMs) is crucial\nfor their reliable deployment, especially in high-stakes applications. Current\nstate-of-the-art methods for measuring semantic uncertainty in LLMs rely on\nstrict bidirectional entailment criteria between multiple generated responses\nand also depend on sequence likelihoods. While effective, these approaches\noften overestimate uncertainty due to their sensitivity to minor wording\ndifferences, additional correct information, and non-important words in the\nsequence. We propose a novel approach that leverages semantic embeddings to\nachieve smoother and more robust estimation of semantic uncertainty in LLMs. By\ncapturing semantic similarities without depending on sequence likelihoods, our\nmethod inherently reduces any biases introduced by irrelevant words in the\nanswers. Furthermore, we introduce an amortised version of our approach by\nexplicitly modelling semantics as latent variables in a joint probabilistic\nmodel. This allows for uncertainty estimation in the embedding space with a\nsingle forward pass, significantly reducing computational overhead compared to\nexisting multi-pass methods. Experiments across multiple question-answering\ndatasets and frontier LLMs demonstrate that our embedding-based methods provide\nmore accurate and nuanced uncertainty quantification than traditional\napproaches.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.22685v1",
    "published_date": "2024-10-30 04:41:46 UTC",
    "updated_date": "2024-10-30 04:41:46 UTC"
  },
  {
    "arxiv_id": "2411.00844v1",
    "title": "Extralonger: Toward a Unified Perspective of Spatial-Temporal Factors for Extra-Long-Term Traffic Forecasting",
    "authors": [
      "Zhiwei Zhang",
      "Shaojun E",
      "Fandong Meng",
      "Jie Zhou",
      "Wenjuan Han"
    ],
    "abstract": "Traffic forecasting plays a key role in Intelligent Transportation Systems,\nand significant strides have been made in this field. However, most existing\nmethods can only predict up to four hours in the future, which doesn't quite\nmeet real-world demands. we identify that the prediction horizon is limited to\na few hours mainly due to the separation of temporal and spatial factors, which\nresults in high complexity. Drawing inspiration from Albert Einstein's\nrelativity theory, which suggests space and time are unified and inseparable,\nwe introduce Extralonger, which unifies temporal and spatial factors.\nExtralonger notably extends the prediction horizon to a week on real-world\nbenchmarks, demonstrating superior efficiency in the training time, inference\ntime, and memory usage. It sets new standards in long-term and extra-long-term\nscenarios. The code is available at https://github.com/PlanckChang/Extralonger.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by NeurIPS2024 workshop",
    "pdf_url": "http://arxiv.org/pdf/2411.00844v1",
    "published_date": "2024-10-30 04:28:20 UTC",
    "updated_date": "2024-10-30 04:28:20 UTC"
  },
  {
    "arxiv_id": "2411.05025v1",
    "title": "LLMs as Research Tools: A Large Scale Survey of Researchers' Usage and Perceptions",
    "authors": [
      "Zhehui Liao",
      "Maria Antoniak",
      "Inyoung Cheong",
      "Evie Yu-Yen Cheng",
      "Ai-Heng Lee",
      "Kyle Lo",
      "Joseph Chee Chang",
      "Amy X. Zhang"
    ],
    "abstract": "The rise of large language models (LLMs) has led many researchers to consider\ntheir usage for scientific work. Some have found benefits using LLMs to augment\nor automate aspects of their research pipeline, while others have urged caution\ndue to risks and ethical concerns. Yet little work has sought to quantify and\ncharacterize how researchers use LLMs and why. We present the first large-scale\nsurvey of 816 verified research article authors to understand how the research\ncommunity leverages and perceives LLMs as research tools. We examine\nparticipants' self-reported LLM usage, finding that 81% of researchers have\nalready incorporated LLMs into different aspects of their research workflow. We\nalso find that traditionally disadvantaged groups in academia (non-White,\njunior, and non-native English speaking researchers) report higher LLM usage\nand perceived benefits, suggesting potential for improved research equity.\nHowever, women, non-binary, and senior researchers have greater ethical\nconcerns, potentially hindering adoption.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.DL",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "30 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.05025v1",
    "published_date": "2024-10-30 04:25:23 UTC",
    "updated_date": "2024-10-30 04:25:23 UTC"
  },
  {
    "arxiv_id": "2411.00843v2",
    "title": "The Graph's Apprentice: Teaching an LLM Low Level Knowledge for Circuit Quality Estimation",
    "authors": [
      "Reza Moravej",
      "Saurabh Bodhe",
      "Zhanguang Zhang",
      "Didier Chetelat",
      "Dimitrios Tsaras",
      "Yingxue Zhang",
      "Hui-Ling Zhen",
      "Jianye Hao",
      "Mingxuan Yuan"
    ],
    "abstract": "Logic synthesis is a crucial phase in the circuit design process, responsible\nfor transforming hardware description language (HDL) designs into optimized\nnetlists. However, traditional logic synthesis methods are computationally\nintensive, restricting their iterative use in refining chip designs. Recent\nadvancements in large language models (LLMs), particularly those fine-tuned on\nprogramming languages, present a promising alternative. This work proposes\naugmenting LLMs with predictor networks trained to estimate circuit quality\ndirectly from HDL code. To enhance performance, the model is regularized using\nembeddings from graph neural networks (GNNs) trained on Look-Up Table (LUT)\ngraphs, thereby incorporating lower-level circuit insights. The proposed method\ndemonstrates superior performance compared to existing graph-based RTL-level\nestimation techniques on the established benchmark OpenABCD, while providing\ninstant feedback on HDL code quality.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.00843v2",
    "published_date": "2024-10-30 04:20:10 UTC",
    "updated_date": "2025-02-14 18:35:03 UTC"
  },
  {
    "arxiv_id": "2410.22678v1",
    "title": "Backdoor Attack Against Vision Transformers via Attention Gradient-Based Image Erosion",
    "authors": [
      "Ji Guo",
      "Hongwei Li",
      "Wenbo Jiang",
      "Guoming Lu"
    ],
    "abstract": "Vision Transformers (ViTs) have outperformed traditional Convolutional Neural\nNetworks (CNN) across various computer vision tasks. However, akin to CNN, ViTs\nare vulnerable to backdoor attacks, where the adversary embeds the backdoor\ninto the victim model, causing it to make wrong predictions about testing\nsamples containing a specific trigger. Existing backdoor attacks against ViTs\nhave the limitation of failing to strike an optimal balance between attack\nstealthiness and attack effectiveness.\n  In this work, we propose an Attention Gradient-based Erosion Backdoor (AGEB)\ntargeted at ViTs. Considering the attention mechanism of ViTs, AGEB selectively\nerodes pixels in areas of maximal attention gradient, embedding a covert\nbackdoor trigger. Unlike previous backdoor attacks against ViTs, AGEB achieves\nan optimal balance between attack stealthiness and attack effectiveness,\nensuring the trigger remains invisible to human detection while preserving the\nmodel's accuracy on clean samples. Extensive experimental evaluations across\nvarious ViT architectures and datasets confirm the effectiveness of AGEB,\nachieving a remarkable Attack Success Rate (ASR) without diminishing Clean Data\nAccuracy (CDA). Furthermore, the stealthiness of AGEB is rigorously validated,\ndemonstrating minimal visual discrepancies between the clean and the triggered\nimages.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by IEEE GLOBECOM 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.22678v1",
    "published_date": "2024-10-30 04:06:12 UTC",
    "updated_date": "2024-10-30 04:06:12 UTC"
  },
  {
    "arxiv_id": "2410.22669v1",
    "title": "A Walsh Hadamard Derived Linear Vector Symbolic Architecture",
    "authors": [
      "Mohammad Mahmudul Alam",
      "Alexander Oberle",
      "Edward Raff",
      "Stella Biderman",
      "Tim Oates",
      "James Holt"
    ],
    "abstract": "Vector Symbolic Architectures (VSAs) are one approach to developing\nNeuro-symbolic AI, where two vectors in $\\mathbb{R}^d$ are `bound' together to\nproduce a new vector in the same space. VSAs support the commutativity and\nassociativity of this binding operation, along with an inverse operation,\nallowing one to construct symbolic-style manipulations over real-valued\nvectors. Most VSAs were developed before deep learning and automatic\ndifferentiation became popular and instead focused on efficacy in hand-designed\nsystems. In this work, we introduce the Hadamard-derived linear Binding (HLB),\nwhich is designed to have favorable computational efficiency, and efficacy in\nclassic VSA tasks, and perform well in differentiable systems. Code is\navailable at\nhttps://github.com/FutureComputing4AI/Hadamard-derived-Linear-Binding",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "To appear in the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024)",
    "pdf_url": "http://arxiv.org/pdf/2410.22669v1",
    "published_date": "2024-10-30 03:42:59 UTC",
    "updated_date": "2024-10-30 03:42:59 UTC"
  },
  {
    "arxiv_id": "2410.22662v2",
    "title": "EMOS: Embodiment-aware Heterogeneous Multi-robot Operating System with LLM Agents",
    "authors": [
      "Junting Chen",
      "Checheng Yu",
      "Xunzhe Zhou",
      "Tianqi Xu",
      "Yao Mu",
      "Mengkang Hu",
      "Wenqi Shao",
      "Yikai Wang",
      "Guohao Li",
      "Lin Shao"
    ],
    "abstract": "Heterogeneous multi-robot systems (HMRS) have emerged as a powerful approach\nfor tackling complex tasks that single robots cannot manage alone. Current\nlarge-language-model-based multi-agent systems (LLM-based MAS) have shown\nsuccess in areas like software development and operating systems, but applying\nthese systems to robot control presents unique challenges. In particular, the\ncapabilities of each agent in a multi-robot system are inherently tied to the\nphysical composition of the robots, rather than predefined roles. To address\nthis issue, we introduce a novel multi-agent framework designed to enable\neffective collaboration among heterogeneous robots with varying embodiments and\ncapabilities, along with a new benchmark named Habitat-MAS. One of our key\ndesigns is $\\textit{Robot Resume}$: Instead of adopting human-designed role\nplay, we propose a self-prompted approach, where agents comprehend robot URDF\nfiles and call robot kinematics tools to generate descriptions of their physics\ncapabilities to guide their behavior in task planning and action execution. The\nHabitat-MAS benchmark is designed to assess how a multi-agent framework handles\ntasks that require embodiment-aware reasoning, which includes 1) manipulation,\n2) perception, 3) navigation, and 4) comprehensive multi-floor object\nrearrangement. The experimental results indicate that the robot's resume and\nthe hierarchical design of our multi-agent system are essential for the\neffective operation of the heterogeneous multi-robot system within this\nintricate problem context.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.MA",
      "I.2.7; I.2.8; I.2.9; I.2.10"
    ],
    "primary_category": "cs.RO",
    "comment": "10 pages of main content, 3 pages of references, 5 pages of appendix,\n  7 figures in total",
    "pdf_url": "http://arxiv.org/pdf/2410.22662v2",
    "published_date": "2024-10-30 03:20:01 UTC",
    "updated_date": "2025-02-17 08:33:11 UTC"
  },
  {
    "arxiv_id": "2410.22658v2",
    "title": "Incremental Learning of Retrievable Skills For Efficient Continual Task Adaptation",
    "authors": [
      "Daehee Lee",
      "Minjong Yoo",
      "Woo Kyung Kim",
      "Wonje Choi",
      "Honguk Woo"
    ],
    "abstract": "Continual Imitation Learning (CiL) involves extracting and accumulating task\nknowledge from demonstrations across multiple stages and tasks to achieve a\nmulti-task policy. With recent advancements in foundation models, there has\nbeen a growing interest in adapter-based CiL approaches, where adapters are\nestablished parameter-efficiently for tasks newly demonstrated. While these\napproaches isolate parameters for specific tasks and tend to mitigate\ncatastrophic forgetting, they limit knowledge sharing among different\ndemonstrations. We introduce IsCiL, an adapter-based CiL framework that\naddresses this limitation of knowledge sharing by incrementally learning\nshareable skills from different demonstrations, thus enabling sample-efficient\ntask adaptation using the skills particularly in non-stationary CiL\nenvironments. In IsCiL, demonstrations are mapped into the state embedding\nspace, where proper skills can be retrieved upon input states through\nprototype-based memory. These retrievable skills are incrementally learned on\ntheir corresponding adapters. Our CiL experiments with complex tasks in\nFranka-Kitchen and Meta-World demonstrate robust performance of IsCiL in both\ntask adaptation and sample-efficiency. We also show a simple extension of IsCiL\nfor task unlearning scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.22658v2",
    "published_date": "2024-10-30 02:57:35 UTC",
    "updated_date": "2025-01-21 01:37:47 UTC"
  },
  {
    "arxiv_id": "2410.22642v1",
    "title": "Prove Your Point!: Bringing Proof-Enhancement Principles to Argumentative Essay Generation",
    "authors": [
      "Ruiyu Xiao",
      "Lei Wu",
      "Yuhang Gou",
      "Weinan Zhang",
      "Ting Liu"
    ],
    "abstract": "Argumentative essay generation (AEG) aims to generate complete texts on\nspecific controversial topics or debates. Although current AEG methods can\ngenerate individual opinions, they often overlook the high-level connections\nbetween these opinions. This often leads to the generated results being mired\nin logical confusion, unable to proof their own arguments effectively. The\ngenerated essay may present evidence that contradicts the claims or they may\nfail to assemble the claims into logical flow. In this paper, we present a\nunified two-stage framework: Proof-Enhancement and Self-Annotation (PESA) for\nAEG with a focus on logical enhancement. Specifically, we first construct\npseudo-labels for logical information,claims and grounds, using a large\nlanguage model. We then propose a tree planning approach that introduces proof\nprinciples and ensures logical consistency. Extensive experimental results show\nthat, benefiting from proof principle guidance, PESA generates argumentative\nessays with better logical validity and persuasiveness than strong baseline\nmodels.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.22642v1",
    "published_date": "2024-10-30 02:13:39 UTC",
    "updated_date": "2024-10-30 02:13:39 UTC"
  },
  {
    "arxiv_id": "2411.00841v1",
    "title": "A Theoretical Perspective for Speculative Decoding Algorithm",
    "authors": [
      "Ming Yin",
      "Minshuo Chen",
      "Kaixuan Huang",
      "Mengdi Wang"
    ],
    "abstract": "Transformer-based autoregressive sampling has been the major bottleneck for\nslowing down large language model inferences. One effective way to accelerate\ninference is \\emph{Speculative Decoding}, which employs a small model to sample\na sequence of draft tokens and a large model to validate. Given its empirical\neffectiveness, the theoretical understanding of Speculative Decoding is falling\nbehind. This paper tackles this gap by conceptualizing the decoding problem via\nmarkov chain abstraction and studying the key properties, \\emph{output quality\nand inference acceleration}, from a theoretical perspective. Our analysis\ncovers the theoretical limits of speculative decoding, batch algorithms, and\noutput quality-inference acceleration tradeoffs. Our results reveal the\nfundamental connections between different components of LLMs via total\nvariation distances and show how they jointly affect the efficiency of decoding\nalgorithms.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.00841v1",
    "published_date": "2024-10-30 01:53:04 UTC",
    "updated_date": "2024-10-30 01:53:04 UTC"
  },
  {
    "arxiv_id": "2410.22631v2",
    "title": "DECRL: A Deep Evolutionary Clustering Jointed Temporal Knowledge Graph Representation Learning Approach",
    "authors": [
      "Qian Chen",
      "Ling Chen"
    ],
    "abstract": "Temporal Knowledge Graph (TKG) representation learning aims to map temporal\nevolving entities and relations to embedded representations in a continuous\nlow-dimensional vector space. However, existing approaches cannot capture the\ntemporal evolution of high-order correlations in TKGs. To this end, we propose\na Deep Evolutionary Clustering jointed temporal knowledge graph Representation\nLearning approach (DECRL). Specifically, a deep evolutionary clustering module\nis proposed to capture the temporal evolution of high-order correlations among\nentities. Furthermore, a cluster-aware unsupervised alignment mechanism is\nintroduced to ensure the precise one-to-one alignment of soft overlapping\nclusters across timestamps, thereby maintaining the temporal smoothness of\nclusters. In addition, an implicit correlation encoder is introduced to capture\nlatent correlations between any pair of clusters under the guidance of a global\ngraph. Extensive experiments on seven real-world datasets demonstrate that\nDECRL achieves the state-of-the-art performances, outperforming the best\nbaseline by an average of 9.53%, 12.98%, 10.42%, and 14.68% in MRR, Hits@1,\nHits@3, and Hits@10, respectively.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by NeurIPS 2024, 17 pages, and 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.22631v2",
    "published_date": "2024-10-30 01:36:06 UTC",
    "updated_date": "2024-12-19 02:31:00 UTC"
  },
  {
    "arxiv_id": "2411.04138v1",
    "title": "NetworkGym: Reinforcement Learning Environments for Multi-Access Traffic Management in Network Simulation",
    "authors": [
      "Momin Haider",
      "Ming Yin",
      "Menglei Zhang",
      "Arpit Gupta",
      "Jing Zhu",
      "Yu-Xiang Wang"
    ],
    "abstract": "Mobile devices such as smartphones, laptops, and tablets can often connect to\nmultiple access networks (e.g., Wi-Fi, LTE, and 5G) simultaneously. Recent\nadvancements facilitate seamless integration of these connections below the\ntransport layer, enhancing the experience for apps that lack inherent\nmulti-path support. This optimization hinges on dynamically determining the\ntraffic distribution across networks for each device, a process referred to as\n\\textit{multi-access traffic splitting}. This paper introduces\n\\textit{NetworkGym}, a high-fidelity network environment simulator that\nfacilitates generating multiple network traffic flows and multi-access traffic\nsplitting. This simulator facilitates training and evaluating different\nRL-based solutions for the multi-access traffic splitting problem. Our initial\nexplorations demonstrate that the majority of existing state-of-the-art offline\nRL algorithms (e.g. CQL) fail to outperform certain hand-crafted heuristic\npolicies on average. This illustrates the urgent need to evaluate offline RL\nalgorithms against a broader range of benchmarks, rather than relying solely on\npopular ones such as D4RL. We also propose an extension to the TD3+BC\nalgorithm, named Pessimistic TD3 (PTD3), and demonstrate that it outperforms\nmany state-of-the-art offline RL algorithms. PTD3's behavioral constraint\nmechanism, which relies on value-function pessimism, is theoretically motivated\nand relatively simple to implement.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NI",
    "comment": "NeurIPS (Datasets and Benchmarks)",
    "pdf_url": "http://arxiv.org/pdf/2411.04138v1",
    "published_date": "2024-10-30 01:14:33 UTC",
    "updated_date": "2024-10-30 01:14:33 UTC"
  },
  {
    "arxiv_id": "2410.22619v2",
    "title": "Efficient Feature Extraction and Classification Architecture for MRI-Based Brain Tumor Detection and Localization",
    "authors": [
      "Plabon Paul",
      "Md. Nazmul Islam",
      "Fazle Rafsani",
      "Pegah Khorasani",
      "Shovito Barua Soumma"
    ],
    "abstract": "Uncontrolled cell division in the brain is what gives rise to brain tumors.\nIf the tumor size increases by more than half, there is little hope for the\npatient's recovery. This emphasizes the need of rapid and precise brain tumor\ndiagnosis. When it comes to analyzing, diagnosing, and planning therapy for\nbrain tumors, MRI imaging plays a crucial role. A brain tumor's development\nhistory is crucial information for doctors to have. When it comes to\ndistinguishing between human soft tissues, MRI scans are superior. In order to\nget reliable classification results from MRI scans quickly, deep learning is\none of the most practical methods. Early human illness diagnosis has been\ndemonstrated to be more accurate when deep learning methods are used. In the\ncase of diagnosing a brain tumor, when even a little misdiagnosis might have\nserious consequences, accuracy is especially important. Disclosure of brain\ntumors in medical images is still a difficult task. Brain MRIs are notoriously\nimprecise in revealing the presence or absence of tumors. Using MRI scans of\nthe brain, a CNN was trained to identify the presence of a tumor in this\nresearch. Results from the CNN model showed an accuracy of 99.17%. The CNN\nmodel's characteristics were also retrieved. The CNN model's characteristics\nwere also retrieved and we also localized the tumor regions from the\nunannotated images using GradCAM, a deep learning explainability tool. In order\nto evaluate the CNN model's capability for processing images, we applied the\nfeatures into different ML models. CNN and machine learning models were also\nevaluated using the standard metrics of Precision, Recall, Specificity, and F1\nscore. The significance of the doctor's diagnosis enhanced the accuracy of the\nCNN model's assistance in identifying the existence of tumor and treating the\npatient.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.22619v2",
    "published_date": "2024-10-30 00:47:32 UTC",
    "updated_date": "2025-03-09 22:00:46 UTC"
  },
  {
    "arxiv_id": "2410.22615v1",
    "title": "CoGS: Model Agnostic Causality Constrained Counterfactual Explanations using goal-directed ASP",
    "authors": [
      "Sopam Dasgupta",
      "Joaquín Arias",
      "Elmer Salazar",
      "Gopal Gupta"
    ],
    "abstract": "Machine learning models are increasingly used in critical areas such as loan\napprovals and hiring, yet they often function as black boxes, obscuring their\ndecision-making processes. Transparency is crucial, as individuals need\nexplanations to understand decisions, primarily if the decisions result in an\nundesired outcome. Our work introduces CoGS (Counterfactual Generation with\ns(CASP)), a model-agnostic framework capable of generating counterfactual\nexplanations for classification models. CoGS leverages the goal-directed Answer\nSet Programming system s(CASP) to compute realistic and causally consistent\nmodifications to feature values, accounting for causal dependencies between\nthem. By using rule-based machine learning algorithms (RBML), notably the\nFOLD-SE algorithm, CoGS extracts the underlying logic of a statistical model to\ngenerate counterfactual solutions. By tracing a step-by-step path from an\nundesired outcome to a desired one, CoGS offers interpretable and actionable\nexplanations of the changes required to achieve the desired outcome. We present\ndetails of the CoGS framework along with its evaluation.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "arXiv admin note: substantial text overlap with arXiv:2407.08179",
    "pdf_url": "http://arxiv.org/pdf/2410.22615v1",
    "published_date": "2024-10-30 00:43:01 UTC",
    "updated_date": "2024-10-30 00:43:01 UTC"
  }
]
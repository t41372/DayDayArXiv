[
  {
    "arxiv_id": "2501.13947v3",
    "title": "A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods",
    "authors": [
      "Wenli Yang",
      "Lilian Some",
      "Michael Bain",
      "Byeong Kang"
    ],
    "abstract": "The rapid development of artificial intelligence has led to marked progress\nin the field. One interesting direction for research is whether Large Language\nModels (LLMs) can be integrated with structured knowledge-based systems. This\napproach aims to combine the generative language understanding of LLMs and the\nprecise knowledge representation systems by which they are integrated. This\narticle surveys the relationship between LLMs and knowledge bases, looks at how\nthey can be applied in practice, and discusses related technical, operational,\nand ethical challenges. Utilizing a comprehensive examination of the\nliterature, the study both identifies important issues and assesses existing\nsolutions. It demonstrates the merits of incorporating generative AI into\nstructured knowledge-base systems concerning data contextualization, model\naccuracy, and utilization of knowledge resources. The findings give a full list\nof the current situation of research, point out the main gaps, and propose\nhelpful paths to take. These insights contribute to advancing AI technologies\nand support their practical deployment across various sectors.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13947v3",
    "published_date": "2025-01-19 23:25:21 UTC",
    "updated_date": "2025-05-01 03:29:50 UTC"
  },
  {
    "arxiv_id": "2501.14815v1",
    "title": "A VM-HDL Co-Simulation Framework for Systems with PCIe-Connected FPGAs",
    "authors": [
      "Shenghsun Cho",
      "Mrunal Patel",
      "Basavaraj Kaladagi",
      "Han Chen",
      "Tapti Palit",
      "Michael Ferdman",
      "Peter Milder"
    ],
    "abstract": "PCIe-connected FPGAs are gaining popularity as an accelerator technology in\ndata centers. However, it is challenging to jointly develop and debug host\nsoftware and FPGA hardware. Changes to the hardware design require a\ntime-consuming FPGA synthesis process, and modification to the software,\nespecially the operating system and device drivers, can frequently cause the\nsystem to hang, without providing enough information for debugging. The\ncombination of these problems results in long debug iterations and a slow\ndevelopment process. To overcome these problems, we designed a VM-HDL\nco-simulation framework, which is capable of running the same software,\noperating system, and hardware designs as the target physical system, while\nproviding full visibility and significantly shorter debug iterations.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.AR",
      "cs.NI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14815v1",
    "published_date": "2025-01-19 22:06:36 UTC",
    "updated_date": "2025-01-19 22:06:36 UTC"
  },
  {
    "arxiv_id": "2502.00023v1",
    "title": "Musical Agent Systems: MACAT and MACataRT",
    "authors": [
      "Keon Ju M. Lee",
      "Philippe Pasquier"
    ],
    "abstract": "Our research explores the development and application of musical agents,\nhuman-in-the-loop generative AI systems designed to support music performance\nand improvisation within co-creative spaces. We introduce MACAT and MACataRT,\ntwo distinct musical agent systems crafted to enhance interactive music-making\nbetween human musicians and AI. MACAT is optimized for agent-led performance,\nemploying real-time synthesis and self-listening to shape its output\nautonomously, while MACataRT provides a flexible environment for collaborative\nimprovisation through audio mosaicing and sequence-based learning. Both systems\nemphasize training on personalized, small datasets, fostering ethical and\ntransparent AI engagement that respects artistic integrity. This research\nhighlights how interactive, artist-centred generative AI can expand creative\npossibilities, empowering musicians to explore new forms of artistic expression\nin real-time, performance-driven and music improvisation contexts.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.HC",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.MA",
    "comment": "In Proceedings of the Creativity and Generative AI NIPS (Neural\n  Information Processing Systems) Workshop 2024",
    "pdf_url": "http://arxiv.org/pdf/2502.00023v1",
    "published_date": "2025-01-19 22:04:09 UTC",
    "updated_date": "2025-01-19 22:04:09 UTC"
  },
  {
    "arxiv_id": "2501.11183v1",
    "title": "Can Safety Fine-Tuning Be More Principled? Lessons Learned from Cybersecurity",
    "authors": [
      "David Williams-King",
      "Linh Le",
      "Adam Oberman",
      "Yoshua Bengio"
    ],
    "abstract": "As LLMs develop increasingly advanced capabilities, there is an increased\nneed to minimize the harm that could be caused to society by certain model\noutputs; hence, most LLMs have safety guardrails added, for example via\nfine-tuning. In this paper, we argue the position that current safety\nfine-tuning is very similar to a traditional cat-and-mouse game (or arms race)\nbetween attackers and defenders in cybersecurity. Model jailbreaks and attacks\nare patched with bandaids to target the specific attack mechanism, but many\nsimilar attack vectors might remain. When defenders are not proactively coming\nup with principled mechanisms, it becomes very easy for attackers to sidestep\nany new defenses. We show how current defenses are insufficient to prevent new\nadversarial jailbreak attacks, reward hacking, and loss of control problems. In\norder to learn from past mistakes in cybersecurity, we draw analogies with\nhistorical examples and develop lessons learned that can be applied to LLM\nsafety. These arguments support the need for new and more principled approaches\nto designing safe models, which are architected for security from the\nbeginning. We describe several such approaches from the AI literature.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "I.2.7; D.4.6"
    ],
    "primary_category": "cs.CR",
    "comment": "published at Neurips Safe Generative AI Workshop 2024",
    "pdf_url": "http://arxiv.org/pdf/2501.11183v1",
    "published_date": "2025-01-19 21:49:42 UTC",
    "updated_date": "2025-01-19 21:49:42 UTC"
  },
  {
    "arxiv_id": "2501.11175v1",
    "title": "ProKeR: A Kernel Perspective on Few-Shot Adaptation of Large Vision-Language Models",
    "authors": [
      "Yassir Bendou",
      "Amine Ouasfi",
      "Vincent Gripon",
      "Adnane Boukhayma"
    ],
    "abstract": "The growing popularity of Contrastive Language-Image Pretraining (CLIP) has\nled to its widespread application in various visual downstream tasks. To\nenhance CLIP's effectiveness and versatility, efficient few-shot adaptation\ntechniques have been widely adopted. Among these approaches, training-free\nmethods, particularly caching methods exemplified by Tip-Adapter, have gained\nattention for their lightweight adaptation without the need for additional\nfine-tuning. In this paper, we revisit Tip-Adapter from a kernel perspective,\nshowing that caching methods function as local adapters and are connected to a\nwell-established kernel literature. Drawing on this insight, we offer a\ntheoretical understanding of how these methods operate and suggest multiple\navenues for enhancing the Tip-Adapter baseline. Notably, our analysis shows the\nimportance of incorporating global information in local adapters. Therefore, we\nsubsequently propose a global method that learns a proximal regularizer in a\nreproducing kernel Hilbert space (RKHS) using CLIP as a base learner. Our\nmethod, which we call ProKeR (Proximal Kernel ridge Regression), has a closed\nform solution and achieves state-of-the-art performances across 11 datasets in\nthe standard few-shot adaptation benchmark.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Code available at https://ybendou.github.io/ProKeR",
    "pdf_url": "http://arxiv.org/pdf/2501.11175v1",
    "published_date": "2025-01-19 21:25:53 UTC",
    "updated_date": "2025-01-19 21:25:53 UTC"
  },
  {
    "arxiv_id": "2501.11171v1",
    "title": "Counteracting temporal attacks in Video Copy Detection",
    "authors": [
      "Katarzyna Fojcik",
      "Piotr Syga"
    ],
    "abstract": "Video Copy Detection (VCD) plays a crucial role in copyright protection and\ncontent verification by identifying duplicates and near-duplicates in\nlarge-scale video databases. The META AI Challenge on video copy detection\nprovided a benchmark for evaluating state-of-the-art methods, with the\nDual-level detection approach emerging as a winning solution. This method\nintegrates Video Editing Detection and Frame Scene Detection to handle\nadversarial transformations and large datasets efficiently. However, our\nanalysis reveals significant limitations in the VED component, particularly in\nits ability to handle exact copies. Moreover, Dual-level detection shows\nvulnerability to temporal attacks. To address it, we propose an improved frame\nselection strategy based on local maxima of interframe differences, which\nenhances robustness against adversarial temporal modifications while\nsignificantly reducing computational overhead. Our method achieves an increase\nof 1.4 to 5.8 times in efficiency over the standard 1 FPS approach. Compared to\nDual-level detection method, our approach maintains comparable micro-average\nprecision ($\\mu$AP) while also demonstrating improved robustness against\ntemporal attacks. Given 56\\% reduced representation size and the inference time\nof more than 2 times faster, our approach is more suitable to real-world\nresource restriction.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.IR",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "14 pages, 5 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2501.11171v1",
    "published_date": "2025-01-19 21:16:39 UTC",
    "updated_date": "2025-01-19 21:16:39 UTC"
  },
  {
    "arxiv_id": "2501.11170v1",
    "title": "AIMA at SemEval-2024 Task 3: Simple Yet Powerful Emotion Cause Pair Analysis",
    "authors": [
      "Alireza Ghahramani Kure",
      "Mahshid Dehghani",
      "Mohammad Mahdi Abootorabi",
      "Nona Ghazizadeh",
      "Seyed Arshan Dalili",
      "Ehsaneddin Asgari"
    ],
    "abstract": "The SemEval-2024 Task 3 presents two subtasks focusing on emotion-cause pair\nextraction within conversational contexts. Subtask 1 revolves around the\nextraction of textual emotion-cause pairs, where causes are defined and\nannotated as textual spans within the conversation. Conversely, Subtask 2\nextends the analysis to encompass multimodal cues, including language, audio,\nand vision, acknowledging instances where causes may not be exclusively\nrepresented in the textual data. Our proposed model for emotion-cause analysis\nis meticulously structured into three core segments: (i) embedding extraction,\n(ii) cause-pair extraction & emotion classification, and (iii) cause extraction\nusing QA after finding pairs. Leveraging state-of-the-art techniques and\nfine-tuning on task-specific datasets, our model effectively unravels the\nintricate web of conversational dynamics and extracts subtle cues signifying\ncausality in emotional expressions. Our team, AIMA, demonstrated strong\nperformance in the SemEval-2024 Task 3 competition. We ranked as the 10th in\nsubtask 1 and the 6th in subtask 2 out of 23 teams.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Proceedings of the 18th International Workshop on Semantic Evaluation\n  (SemEval-2024)",
    "pdf_url": "http://arxiv.org/pdf/2501.11170v1",
    "published_date": "2025-01-19 21:16:31 UTC",
    "updated_date": "2025-01-19 21:16:31 UTC"
  },
  {
    "arxiv_id": "2501.16354v1",
    "title": "Adaptive Hoeffding Tree with Transfer Learning for Streaming Synchrophasor Data Sets",
    "authors": [
      "Zakaria El Mrabet",
      "Daisy Flora Selvaraj",
      "Prakash Ranganathan"
    ],
    "abstract": "Synchrophasor technology or phasor measurement units (PMUs) are known to\ndetect multiple type of oscillations or faults better than Supervisory Control\nand Data Acquisition (SCADA) systems, but the volume of Bigdata (e.g., 30-120\nsamples per second on a single PMU) generated by these sensors at the\naggregator level (e.g., several PMUs) requires special handling. Conventional\nmachine learning or data mining methods are not suitable to handle such larger\nstreaming realtime data. This is primarily due to latencies associated with\ncloud environments (e.g., at an aggregator or PDC level), and thus necessitates\nthe need for local computing to move the data on the edge (or locally at the\nPMU level) for processing. This requires faster real-time streaming algorithms\nto be processed at the local level (e.g., typically by a Field Programmable\nGate Array (FPGA) based controllers). This paper proposes a transfer\nlearning-based hoeffding tree with ADWIN (THAT) method to detect anomalous\nsynchrophasor signatures. The proposed algorithm is trained and tested with the\nOzaBag method. The preliminary results with transfer learning indicate that a\ncomputational time saving of 0.7ms is achieved with THAT algorithm (0.34ms)\nover Ozabag (1.04ms), while the accuracy of both methods in detecting fault\nevents remains at 94% for four signatures.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.16354v1",
    "published_date": "2025-01-19 21:10:01 UTC",
    "updated_date": "2025-01-19 21:10:01 UTC"
  },
  {
    "arxiv_id": "2501.11166v1",
    "title": "AIMA at SemEval-2024 Task 10: History-Based Emotion Recognition in Hindi-English Code-Mixed Conversations",
    "authors": [
      "Mohammad Mahdi Abootorabi",
      "Nona Ghazizadeh",
      "Seyed Arshan Dalili",
      "Alireza Ghahramani Kure",
      "Mahshid Dehghani",
      "Ehsaneddin Asgari"
    ],
    "abstract": "In this study, we introduce a solution to the SemEval 2024 Task 10 on subtask\n1, dedicated to Emotion Recognition in Conversation (ERC) in code-mixed\nHindi-English conversations. ERC in code-mixed conversations presents unique\nchallenges, as existing models are typically trained on monolingual datasets\nand may not perform well on code-mixed data. To address this, we propose a\nseries of models that incorporate both the previous and future context of the\ncurrent utterance, as well as the sequential information of the conversation.\nTo facilitate the processing of code-mixed data, we developed a\nHinglish-to-English translation pipeline to translate the code-mixed\nconversations into English. We designed four different base models, each\nutilizing powerful pre-trained encoders to extract features from the input but\nwith varying architectures. By ensembling all of these models, we developed a\nfinal model that outperforms all other baselines.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Proceedings of the 18th International Workshop on Semantic Evaluation\n  (SemEval-2024)",
    "pdf_url": "http://arxiv.org/pdf/2501.11166v1",
    "published_date": "2025-01-19 20:56:45 UTC",
    "updated_date": "2025-01-19 20:56:45 UTC"
  },
  {
    "arxiv_id": "2501.11140v1",
    "title": "CLOFAI: A Dataset of Real And Fake Image Classification Tasks for Continual Learning",
    "authors": [
      "William Doherty",
      "Anton Lee",
      "Heitor Murilo Gomes"
    ],
    "abstract": "The rapid advancement of generative AI models capable of creating realistic\nmedia has led to a need for classifiers that can accurately distinguish between\ngenuine and artificially-generated images. A significant challenge for these\nclassifiers emerges when they encounter images from generative models that are\nnot represented in their training data, usually resulting in diminished\nperformance. A typical approach is to periodically update the classifier's\ntraining data with images from the new generative models then retrain the\nclassifier on the updated dataset. However, in some real-life scenarios,\nstorage, computational, or privacy constraints render this approach\nimpractical. Additionally, models used in security applications may be required\nto rapidly adapt. In these circumstances, continual learning provides a\npromising alternative, as the classifier can be updated without retraining on\nthe entire dataset. In this paper, we introduce a new dataset called CLOFAI\n(Continual Learning On Fake and Authentic Images), which takes the form of a\ndomain-incremental image classification problem. Moreover, we showcase the\napplicability of this dataset as a benchmark for evaluating continual learning\nmethodologies. In doing this, we set a baseline on our novel dataset using\nthree foundational continual learning methods -- EWC, GEM, and Experience\nReplay -- and find that EWC performs poorly, while GEM and Experience Replay\nshow promise, performing significantly better than a Naive baseline. The\ndataset and code to run the experiments can be accessed from the following\nGitHub repository: https://github.com/Will-Doherty/CLOFAI.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.11140v1",
    "published_date": "2025-01-19 18:53:30 UTC",
    "updated_date": "2025-01-19 18:53:30 UTC"
  },
  {
    "arxiv_id": "2501.11135v1",
    "title": "Playing the Lottery With Concave Regularizers for Sparse Trainable Neural Networks",
    "authors": [
      "Giulia Fracastoro",
      "Sophie M. Fosson",
      "Andrea Migliorati",
      "Giuseppe C. Calafiore"
    ],
    "abstract": "The design of sparse neural networks, i.e., of networks with a reduced number\nof parameters, has been attracting increasing research attention in the last\nfew years. The use of sparse models may significantly reduce the computational\nand storage footprint in the inference phase. In this context, the lottery\nticket hypothesis (LTH) constitutes a breakthrough result, that addresses not\nonly the performance of the inference phase, but also of the training phase. It\nstates that it is possible to extract effective sparse subnetworks, called\nwinning tickets, that can be trained in isolation. The development of effective\nmethods to play the lottery, i.e., to find winning tickets, is still an open\nproblem. In this article, we propose a novel class of methods to play the\nlottery. The key point is the use of concave regularization to promote the\nsparsity of a relaxed binary mask, which represents the network topology. We\ntheoretically analyze the effectiveness of the proposed method in the convex\nframework. Then, we propose extended numerical tests on various datasets and\narchitectures, that show that the proposed method can improve the performance\nof state-of-the-art algorithms.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.11135v1",
    "published_date": "2025-01-19 18:05:13 UTC",
    "updated_date": "2025-01-19 18:05:13 UTC"
  },
  {
    "arxiv_id": "2501.11128v1",
    "title": "A Collection of Question Answering Datasets for Norwegian",
    "authors": [
      "Vladislav Mikhailov",
      "Petter Mæhlum",
      "Victoria Ovedie Chruickshank Langø",
      "Erik Velldal",
      "Lilja Øvrelid"
    ],
    "abstract": "This paper introduces a new suite of question answering datasets for\nNorwegian; NorOpenBookQA, NorCommonSenseQA, NorTruthfulQA, and NRK-Quiz-QA. The\ndata covers a wide range of skills and knowledge domains, including world\nknowledge, commonsense reasoning, truthfulness, and knowledge about Norway.\nCovering both of the written standards of Norwegian - Bokm{\\aa}l and Nynorsk -\nour datasets comprise over 10k question-answer pairs, created by native\nspeakers. We detail our dataset creation approach and present the results of\nevaluating 11 language models (LMs) in zero- and few-shot regimes. Most LMs\nperform better in Bokm{\\aa}l than Nynorsk, struggle most with commonsense\nreasoning, and are often untruthful in generating answers to questions. All our\ndatasets and annotation materials are publicly available.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted for NoDaLiDa / Baltic-HLT 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.11128v1",
    "published_date": "2025-01-19 17:42:48 UTC",
    "updated_date": "2025-01-19 17:42:48 UTC"
  },
  {
    "arxiv_id": "2501.11120v1",
    "title": "Tell me about yourself: LLMs are aware of their learned behaviors",
    "authors": [
      "Jan Betley",
      "Xuchan Bao",
      "Martín Soto",
      "Anna Sztyber-Betley",
      "James Chua",
      "Owain Evans"
    ],
    "abstract": "We study behavioral self-awareness -- an LLM's ability to articulate its\nbehaviors without requiring in-context examples. We finetune LLMs on datasets\nthat exhibit particular behaviors, such as (a) making high-risk economic\ndecisions, and (b) outputting insecure code. Despite the datasets containing no\nexplicit descriptions of the associated behavior, the finetuned LLMs can\nexplicitly describe it. For example, a model trained to output insecure code\nsays, ``The code I write is insecure.'' Indeed, models show behavioral\nself-awareness for a range of behaviors and for diverse evaluations. Note that\nwhile we finetune models to exhibit behaviors like writing insecure code, we do\nnot finetune them to articulate their own behaviors -- models do this without\nany special training or examples.\n  Behavioral self-awareness is relevant for AI safety, as models could use it\nto proactively disclose problematic behaviors. In particular, we study backdoor\npolicies, where models exhibit unexpected behaviors only under certain trigger\nconditions. We find that models can sometimes identify whether or not they have\na backdoor, even without its trigger being present. However, models are not\nable to directly output their trigger by default.\n  Our results show that models have surprising capabilities for self-awareness\nand for the spontaneous articulation of implicit behaviors. Future work could\ninvestigate this capability for a wider range of scenarios and models\n(including practical scenarios), and explain how it emerges in LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Submitted to ICLR 2025. 17 pages, 13 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.11120v1",
    "published_date": "2025-01-19 17:28:12 UTC",
    "updated_date": "2025-01-19 17:28:12 UTC"
  },
  {
    "arxiv_id": "2501.11114v1",
    "title": "Clinical trial cohort selection using Large Language Models on n2c2 Challenges",
    "authors": [
      "Chi-en Amy Tai",
      "Xavier Tannier"
    ],
    "abstract": "Clinical trials are a critical process in the medical field for introducing\nnew treatments and innovations. However, cohort selection for clinical trials\nis a time-consuming process that often requires manual review of patient text\nrecords for specific keywords. Though there have been studies on standardizing\nthe information across the various platforms, Natural Language Processing (NLP)\ntools remain crucial for spotting eligibility criteria in textual reports.\nRecently, pre-trained large language models (LLMs) have gained popularity for\nvarious NLP tasks due to their ability to acquire a nuanced understanding of\ntext. In this paper, we study the performance of large language models on\nclinical trial cohort selection and leverage the n2c2 challenges to benchmark\ntheir performance. Our results are promising with regard to the incorporation\nof LLMs for simple cohort selection tasks, but also highlight the difficulties\nencountered by these models as soon as fine-grained knowledge and reasoning are\nrequired.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.11114v1",
    "published_date": "2025-01-19 17:07:02 UTC",
    "updated_date": "2025-01-19 17:07:02 UTC"
  },
  {
    "arxiv_id": "2501.11107v2",
    "title": "ChaosEater: Fully Automating Chaos Engineering with Large Language Models",
    "authors": [
      "Daisuke Kikuta",
      "Hiroki Ikeuchi",
      "Kengo Tajiri"
    ],
    "abstract": "Chaos Engineering (CE) is an engineering technique aimed at improving the\nresiliency of distributed systems. It involves artificially injecting specific\nfailures into a distributed system and observing its behavior in response.\nBased on the observation, the system can be proactively improved to handle\nthose failures. Recent CE tools implement the automated execution of predefined\nCE experiments. However, defining these experiments and improving the system\nbased on the experimental results still remain manual. To reduce the costs of\nthe manual operations, we propose ChaosEater, a system for automating the\nentire CE operations with Large Language Models (LLMs). It predefines the\nagentic workflow according to a systematic CE cycle and assigns subdivided\noperations within the workflow to LLMs. ChaosEater targets CE for Kubernetes\nsystems, which are managed through code (i.e., Infrastructure as Code).\nTherefore, the LLMs in ChaosEater perform software engineering tasks to\ncomplete CE cycles, including requirement definition, code generation,\ndebugging, and testing. We evaluate ChaosEater through case studies on both\nsmall and large Kubernetes systems. The results demonstrate that it stably\ncompletes reasonable single CE cycles with significantly low time and monetary\ncosts. The CE cycles are also qualitatively validated by human engineers and\nLLMs.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.DC",
      "cs.NI"
    ],
    "primary_category": "cs.SE",
    "comment": "114 pages (7 main), 11 figures. Project page:\n  https://ntt-dkiku.github.io/chaos-eater",
    "pdf_url": "http://arxiv.org/pdf/2501.11107v2",
    "published_date": "2025-01-19 16:35:09 UTC",
    "updated_date": "2025-04-16 03:33:29 UTC"
  },
  {
    "arxiv_id": "2501.11094v1",
    "title": "Enhanced Suicidal Ideation Detection from Social Media Using a CNN-BiLSTM Hybrid Model",
    "authors": [
      "Mohaiminul Islam Bhuiyan",
      "Nur Shazwani Kamarudin",
      "Nur Hafieza Ismail"
    ],
    "abstract": "Suicidal ideation detection is crucial for preventing suicides, a leading\ncause of death worldwide. Many individuals express suicidal thoughts on social\nmedia, offering a vital opportunity for early detection through advanced\nmachine learning techniques. The identification of suicidal ideation in social\nmedia text is improved by utilising a hybrid framework that integrates\nConvolutional Neural Networks (CNN) and Bidirectional Long Short-Term Memory\n(BiLSTM), enhanced with an attention mechanism. To enhance the interpretability\nof the model's predictions, Explainable AI (XAI) methods are applied, with a\nparticular focus on SHapley Additive exPlanations (SHAP), are incorporated. At\nfirst, the model managed to reach an accuracy of 92.81%. By applying\nfine-tuning and early stopping techniques, the accuracy improved to 94.29%. The\nSHAP analysis revealed key features influencing the model's predictions, such\nas terms related to mental health struggles. This level of transparency boosts\nthe model's credibility while helping mental health professionals understand\nand trust the predictions. This work highlights the potential for improving the\naccuracy and interpretability of detecting suicidal tendencies, making a\nvaluable contribution to the progress of mental health monitoring systems. It\nemphasizes the significance of blending powerful machine learning methods with\nexplainability to develop reliable and impactful mental health solutions.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.11094v1",
    "published_date": "2025-01-19 16:08:50 UTC",
    "updated_date": "2025-01-19 16:08:50 UTC"
  },
  {
    "arxiv_id": "2501.11087v1",
    "title": "Leveraging counterfactual concepts for debugging and improving CNN model performance",
    "authors": [
      "Syed Ali Tariq",
      "Tehseen Zia"
    ],
    "abstract": "Counterfactual explanation methods have recently received significant\nattention for explaining CNN-based image classifiers due to their ability to\nprovide easily understandable explanations that align more closely with human\nreasoning. However, limited attention has been given to utilizing\nexplainability methods to improve model performance. In this paper, we propose\nto leverage counterfactual concepts aiming to enhance the performance of CNN\nmodels in image classification tasks. Our proposed approach utilizes\ncounterfactual reasoning to identify crucial filters used in the\ndecision-making process. Following this, we perform model retraining through\nthe design of a novel methodology and loss functions that encourage the\nactivation of class-relevant important filters and discourage the activation of\nirrelevant filters for each class. This process effectively minimizes the\ndeviation of activation patterns of local predictions and the global activation\npatterns of their respective inferred classes. By incorporating counterfactual\nexplanations, we validate unseen model predictions and identify\nmisclassifications. The proposed methodology provides insights into potential\nweaknesses and biases in the model's learning process, enabling targeted\nimprovements and enhanced performance. Experimental results on publicly\navailable datasets have demonstrated an improvement of 1-2\\%, validating the\neffectiveness of the approach.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This manuscript is currently under consideration for publication in\n  Pattern Recognition Letters",
    "pdf_url": "http://arxiv.org/pdf/2501.11087v1",
    "published_date": "2025-01-19 15:50:33 UTC",
    "updated_date": "2025-01-19 15:50:33 UTC"
  },
  {
    "arxiv_id": "2501.11086v1",
    "title": "Can LLM Generate Regression Tests for Software Commits?",
    "authors": [
      "Jing Liu",
      "Seongmin Lee",
      "Eleonora Losiouk",
      "Marcel Böhme"
    ],
    "abstract": "Large Language Models (LLMs) have shown tremendous promise in automated\nsoftware engineering. In this paper, we investigate the opportunities of LLMs\nfor automatic regression test generation for programs that take highly\nstructured, human-readable inputs, such as XML parsers or JavaScript\ninterpreters. Concretely, we explore the following regression test generation\nscenarios for such programs that have so far been difficult to test\nautomatically in the absence of corresponding input grammars:\n  $\\bullet$ Bug finding. Given a code change (e.g., a commit or pull request),\nour LLM-based approach generates a test case with the objective of revealing\nany bugs that might be introduced if that change is applied.\n  $\\bullet$ Patch testing. Given a patch, our LLM-based approach generates a\ntest case that fails before but passes after the patch. This test can be added\nto the regression test suite to catch similar bugs in the future.\n  We implement Cleverest, a feedback-directed, zero-shot LLM-based regression\ntest generation technique, and evaluate its effectiveness on 22 commits to\nthree subject programs: Mujs, Libxml2, and Poppler. For programs using more\nhuman-readable file formats, like XML or JavaScript, we found Cleverest\nperformed very well. It generated easy-to-understand bug-revealing or\nbug-reproduction test cases for the majority of commits in just under three\nminutes -- even when only the code diff or commit message (unless it was too\nvague) was given. For programs with more compact file formats, like PDF, as\nexpected, it struggled to generate effective test cases. However, the\nLLM-supplied test cases are not very far from becoming effective (e.g., when\nused as a seed by a greybox fuzzer or as a starting point by the developer).",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "18 pages. This version of the paper was written on Thu, 12 Sep 2024",
    "pdf_url": "http://arxiv.org/pdf/2501.11086v1",
    "published_date": "2025-01-19 15:46:26 UTC",
    "updated_date": "2025-01-19 15:46:26 UTC"
  },
  {
    "arxiv_id": "2501.16353v1",
    "title": "Synthetic Data Generation by Supervised Neural Gas Network for Physiological Emotion Recognition Data",
    "authors": [
      "S. Muhammad Hossein Mousavi"
    ],
    "abstract": "Data scarcity remains a significant challenge in the field of emotion\nrecognition using physiological signals, as acquiring comprehensive and diverse\ndatasets is often prevented by privacy concerns and logistical constraints.\nThis limitation restricts the development and generalization of robust emotion\nrecognition models, making the need for effective synthetic data generation\nmethods more critical. Emotion recognition from physiological signals such as\nEEG, ECG, and GSR plays a pivotal role in enhancing human-computer interaction\nand understanding human affective states. Utilizing these signals, this study\nintroduces an innovative approach to synthetic data generation using a\nSupervised Neural Gas (SNG) network, which has demonstrated noteworthy speed\nadvantages over established models like Conditional VAE, Conditional GAN,\ndiffusion model, and Variational LSTM. The Neural Gas network, known for its\nadaptability in organizing data based on topological and feature-space\nproximity, provides a robust framework for generating real-world-like synthetic\ndatasets that preserve the intrinsic patterns of physiological emotion data.\nOur implementation of the SNG efficiently processes the input data, creating\nsynthetic instances that closely mimic the original data distributions, as\ndemonstrated through comparative accuracy assessments. In experiments, while\nour approach did not universally outperform all models, it achieved superior\nperformance against most of the evaluated models and offered significant\nimprovements in processing time. These outcomes underscore the potential of\nusing SNG networks for fast, efficient, and effective synthetic data generation\nin emotion recognition applications.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG",
      "eess.SP"
    ],
    "primary_category": "cs.NE",
    "comment": "14 pages",
    "pdf_url": "http://arxiv.org/pdf/2501.16353v1",
    "published_date": "2025-01-19 15:34:05 UTC",
    "updated_date": "2025-01-19 15:34:05 UTC"
  },
  {
    "arxiv_id": "2501.11079v1",
    "title": "Federated Deep Reinforcement Learning for Energy Efficient Multi-Functional RIS-Assisted Low-Earth Orbit Networks",
    "authors": [
      "Li-Hsiang Shen",
      "Jyun-Jhe Huang",
      "Kai-Ten Feng",
      "Lie-Liang Yang",
      "Jen-Ming Wu"
    ],
    "abstract": "In this paper, a novel network architecture that deploys the multi-functional\nreconfigurable intelligent surface (MF-RIS) in low-Earth orbit (LEO) is\nproposed. Unlike traditional RIS with only signal reflection capability, the\nMF-RIS can reflect, refract, and amplify signals, as well as harvest energy\nfrom wireless signals. Given the high energy demands in shadow regions where\nsolar energy is unavailable, MF-RIS is deployed in LEO to enhance signal\ncoverage and improve energy efficiency (EE). To address this, we formulate a\nlong-term EE optimization problem by determining the optimal parameters for\nMF-RIS configurations, including amplification and phase-shifts, energy\nharvesting ratios, and LEO transmit beamforming. To address the complex\nnon-convex and non-linear problem, a federated learning enhanced multi-agent\ndeep deterministic policy gradient (FEMAD) scheme is designed. Multi-agent DDPG\nof each agent can provide the optimal action policy from its interaction to\nenvironments, whereas federated learning enables the hidden information\nexchange among multi-agents. In numerical results, we can observe significant\nEE improvements compared to the other benchmarks, including centralized deep\nreinforcement learning as well as distributed multi-agent deep deterministic\npolicy gradient (DDPG). Additionally, the proposed LEO-MF-RIS architecture has\ndemonstrated its effectiveness, achieving the highest EE performance compared\nto the scenarios of fixed/no energy harvesting in MF-RIS, traditional\nreflection-only RIS, and deployment without RISs/MF-RISs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.11079v1",
    "published_date": "2025-01-19 15:31:05 UTC",
    "updated_date": "2025-01-19 15:31:05 UTC"
  },
  {
    "arxiv_id": "2501.11067v1",
    "title": "IntellAgent: A Multi-Agent Framework for Evaluating Conversational AI Systems",
    "authors": [
      "Elad Levi",
      "Ilan Kadar"
    ],
    "abstract": "Large Language Models (LLMs) are transforming artificial intelligence,\nevolving into task-oriented systems capable of autonomous planning and\nexecution. One of the primary applications of LLMs is conversational AI\nsystems, which must navigate multi-turn dialogues, integrate domain-specific\nAPIs, and adhere to strict policy constraints. However, evaluating these agents\nremains a significant challenge, as traditional methods fail to capture the\ncomplexity and variability of real-world interactions. We introduce\nIntellAgent, a scalable, open-source multi-agent framework designed to evaluate\nconversational AI systems comprehensively. IntellAgent automates the creation\nof diverse, synthetic benchmarks by combining policy-driven graph modeling,\nrealistic event generation, and interactive user-agent simulations. This\ninnovative approach provides fine-grained diagnostics, addressing the\nlimitations of static and manually curated benchmarks with coarse-grained\nmetrics. IntellAgent represents a paradigm shift in evaluating conversational\nAI. By simulating realistic, multi-policy scenarios across varying levels of\ncomplexity, IntellAgent captures the nuanced interplay of agent capabilities\nand policy constraints. Unlike traditional methods, it employs a graph-based\npolicy model to represent relationships, likelihoods, and complexities of\npolicy interactions, enabling highly detailed diagnostics. IntellAgent also\nidentifies critical performance gaps, offering actionable insights for targeted\noptimization. Its modular, open-source design supports seamless integration of\nnew domains, policies, and APIs, fostering reproducibility and community\ncollaboration. Our findings demonstrate that IntellAgent serves as an effective\nframework for advancing conversational AI by addressing challenges in bridging\nresearch and deployment. The framework is available at\nhttps://github.com/plurai-ai/intellagent",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.11067v1",
    "published_date": "2025-01-19 14:58:35 UTC",
    "updated_date": "2025-01-19 14:58:35 UTC"
  },
  {
    "arxiv_id": "2501.11065v1",
    "title": "Enhancing Neural Spoken Language Recognition: An Exploration with Multilingual Datasets",
    "authors": [
      "Or Haim Anidjar",
      "Roi Yozevitch"
    ],
    "abstract": "In this research, we advanced a spoken language recognition system, moving\nbeyond traditional feature vector-based models. Our improvements focused on\neffectively capturing language characteristics over extended periods using a\nspecialized pooling layer. We utilized a broad dataset range from Common-Voice,\ntargeting ten languages across Indo-European, Semitic, and East Asian families.\nThe major innovation involved optimizing the architecture of Time Delay Neural\nNetworks. We introduced additional layers and restructured these networks into\na funnel shape, enhancing their ability to process complex linguistic patterns.\nA rigorous grid search determined the optimal settings for these networks,\nsignificantly boosting their efficiency in language pattern recognition from\naudio samples. The model underwent extensive training, including a phase with\naugmented data, to refine its capabilities. The culmination of these efforts is\na highly accurate system, achieving a 97\\% accuracy rate in language\nrecognition. This advancement represents a notable contribution to artificial\nintelligence, specifically in improving the accuracy and efficiency of language\nprocessing systems, a critical aspect in the engineering of advanced speech\nrecognition technologies.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "15 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.11065v1",
    "published_date": "2025-01-19 14:49:43 UTC",
    "updated_date": "2025-01-19 14:49:43 UTC"
  },
  {
    "arxiv_id": "2501.13115v2",
    "title": "Dagger Behind Smile: Fool LLMs with a Happy Ending Story",
    "authors": [
      "Xurui Song",
      "Zhixin Xie",
      "Shuo Huai",
      "Jiayi Kong",
      "Jun Luo"
    ],
    "abstract": "The wide adoption of Large Language Models (LLMs) has attracted significant\nattention from $\\textit{jailbreak}$ attacks, where adversarial prompts crafted\nthrough optimization or manual design exploit LLMs to generate malicious\ncontents. However, optimization-based attacks have limited efficiency and\ntransferability, while existing manual designs are either easily detectable or\ndemand intricate interactions with LLMs. In this paper, we first point out a\nnovel perspective for jailbreak attacks: LLMs are more responsive to\n$\\textit{positive}$ prompts. Based on this, we deploy Happy Ending Attack (HEA)\nto wrap up a malicious request in a scenario template involving a positive\nprompt formed mainly via a $\\textit{happy ending}$, it thus fools LLMs into\njailbreaking either immediately or at a follow-up malicious request.This has\nmade HEA both efficient and effective, as it requires only up to two turns to\nfully jailbreak LLMs. Extensive experiments show that our HEA can successfully\njailbreak on state-of-the-art LLMs, including GPT-4o, Llama3-70b, Gemini-pro,\nand achieves 88.79\\% attack success rate on average. We also provide\nquantitative explanations for the success of HEA.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13115v2",
    "published_date": "2025-01-19 13:39:51 UTC",
    "updated_date": "2025-02-17 02:23:21 UTC"
  },
  {
    "arxiv_id": "2501.11043v2",
    "title": "BF-STVSR: B-Splines and Fourier-Best Friends for High Fidelity Spatial-Temporal Video Super-Resolution",
    "authors": [
      "Eunjin Kim",
      "Hyeonjin Kim",
      "Kyong Hwan Jin",
      "Jaejun Yoo"
    ],
    "abstract": "While prior methods in Continuous Spatial-Temporal Video Super-Resolution\n(C-STVSR) employ Implicit Neural Representation (INR) for continuous encoding,\nthey often struggle to capture the complexity of video data, relying on simple\ncoordinate concatenation and pre-trained optical flow networks for motion\nrepresentation. Interestingly, we find that adding position encoding, contrary\nto common observations, does not improve--and even degrades--performance. This\nissue becomes particularly pronounced when combined with pre-trained optical\nflow networks, which can limit the model's flexibility. To address these\nissues, we propose BF-STVSR, a C-STVSR framework with two key modules tailored\nto better represent spatial and temporal characteristics of video: 1) B-spline\nMapper for smooth temporal interpolation, and 2) Fourier Mapper for capturing\ndominant spatial frequencies. Our approach achieves state-of-the-art in various\nmetrics, including PSNR and SSIM, showing enhanced spatial details and natural\ntemporal consistency. Our code is available https://github.com/Eunjnnn/bfstvsr.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.11043v2",
    "published_date": "2025-01-19 13:29:41 UTC",
    "updated_date": "2025-03-25 07:05:39 UTC"
  },
  {
    "arxiv_id": "2501.11031v1",
    "title": "AdaptiveLog: An Adaptive Log Analysis Framework with the Collaboration of Large and Small Language Model",
    "authors": [
      "Lipeng Ma",
      "Weidong Yang",
      "Yixuan Li",
      "Ben Fei",
      "Mingjie Zhou",
      "Shuhao Li",
      "Sihang Jiang",
      "Bo Xu",
      "Yanghua Xiao"
    ],
    "abstract": "Automated log analysis is crucial to ensure high availability and reliability\nof complex systems. The advent of LLMs in NLP has ushered in a new era of\nlanguage model-driven automated log analysis, garnering significant interest.\nWithin this field, two primary paradigms based on language models for log\nanalysis have become prominent. Small Language Models (SLMs) follow the\npre-train and fine-tune paradigm, focusing on the specific log analysis task\nthrough fine-tuning on supervised datasets. On the other hand, LLMs following\nthe in-context learning paradigm, analyze logs by providing a few examples in\nprompt contexts without updating parameters. Despite their respective\nstrengths, we notice that SLMs are more cost-effective but less powerful,\nwhereas LLMs with large parameters are highly powerful but expensive and\ninefficient. To trade-off between the performance and inference costs of both\nmodels in automated log analysis, this paper introduces an adaptive log\nanalysis framework known as AdaptiveLog, which effectively reduces the costs\nassociated with LLM while ensuring superior results. This framework\ncollaborates an LLM and a small language model, strategically allocating the\nLLM to tackle complex logs while delegating simpler logs to the SLM.\nSpecifically, to efficiently query the LLM, we propose an adaptive selection\nstrategy based on the uncertainty estimation of the SLM, where the LLM is\ninvoked only when the SLM is uncertain. In addition, to enhance the reasoning\nability of the LLM in log analysis tasks, we propose a novel prompt strategy by\nretrieving similar error-prone cases as the reference, enabling the model to\nleverage past error experiences and learn solutions from these cases. Extensive\nexperiments demonstrate that AdaptiveLog achieves state-of-the-art results\nacross different tasks, elevating the overall accuracy of log analysis while\nmaintaining cost efficiency.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.11031v1",
    "published_date": "2025-01-19 12:46:01 UTC",
    "updated_date": "2025-01-19 12:46:01 UTC"
  },
  {
    "arxiv_id": "2501.13946v1",
    "title": "Hallucination Mitigation using Agentic AI Natural Language-Based Frameworks",
    "authors": [
      "Diego Gosmar",
      "Deborah A. Dahl"
    ],
    "abstract": "Hallucinations remain a significant challenge in current Generative AI\nmodels, undermining trust in AI systems and their reliability. This study\ninvestigates how orchestrating multiple specialized Artificial Intelligent\nAgents can help mitigate such hallucinations, with a focus on systems\nleveraging Natural Language Processing (NLP) to facilitate seamless agent\ninteractions. To achieve this, we design a pipeline that introduces over three\nhundred prompts, purposefully crafted to induce hallucinations, into a\nfront-end agent. The outputs are then systematically reviewed and refined by\nsecond- and third-level agents, each employing distinct large language models\nand tailored strategies to detect unverified claims, incorporate explicit\ndisclaimers, and clarify speculative content. Additionally, we introduce a set\nof novel Key Performance Indicators (KPIs) specifically designed to evaluate\nhallucination score levels. A dedicated fourth-level AI agent is employed to\nevaluate these KPIs, providing detailed assessments and ensuring accurate\nquantification of shifts in hallucination-related behaviors. A core component\nof this investigation is the use of the OVON (Open Voice Network) framework,\nwhich relies on universal NLP-based interfaces to transfer contextual\ninformation among agents. Through structured JSON messages, each agent\ncommunicates its assessment of the hallucination likelihood and the reasons\nunderlying questionable content, thereby enabling the subsequent stage to\nrefine the text without losing context. The results demonstrate that employing\nmultiple specialized agents capable of interoperating with each other through\nNLP-based agentic frameworks can yield promising outcomes in hallucination\nmitigation, ultimately bolstering trust within the AI community.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.CL",
    "comment": "18 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.13946v1",
    "published_date": "2025-01-19 11:19:25 UTC",
    "updated_date": "2025-01-19 11:19:25 UTC"
  },
  {
    "arxiv_id": "2501.11006v2",
    "title": "GREEN-CODE: Learning to Optimize Energy Efficiency in LLM-based Code Generation",
    "authors": [
      "Shashikant Ilager",
      "Lukas Florian Briem",
      "Ivona Brandic"
    ],
    "abstract": "Large Language Models (LLMs) are becoming integral to daily life, showcasing\ntheir vast potential across various Natural Language Processing (NLP) tasks.\nBeyond NLP, LLMs are increasingly used in software development tasks, such as\ncode completion, modification, bug fixing, and code translation. Software\nengineers widely use tools like GitHub Copilot and Amazon Q, streamlining\nworkflows and automating tasks with high accuracy. While the resource and\nenergy intensity of LLM training is often highlighted, inference can be even\nmore resource-intensive over time, as it's a continuous process with a high\nnumber of invocations. Therefore, developing resource-efficient alternatives\nfor LLM inference is crucial for sustainability. This work proposes GREEN-CODE,\na framework for energy-aware code generation in LLMs. GREEN-CODE performs\ndynamic early exit during LLM inference. We train a Reinforcement Learning (RL)\nagent that learns to balance the trade-offs between accuracy, latency, and\nenergy consumption. Our approach is evaluated on two open-source LLMs, Llama\n3.2 3B and OPT 2.7B, using the JavaCorpus and PY150 datasets. Results show that\nour method reduces the energy consumption between 23-50 % on average for code\ngeneration tasks without significantly affecting accuracy.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.PF",
      "cs.SE",
      "C.4; D.0; E.4; I.7"
    ],
    "primary_category": "cs.DC",
    "comment": "Under submission in ACM/IEEE conference, 11 pages",
    "pdf_url": "http://arxiv.org/pdf/2501.11006v2",
    "published_date": "2025-01-19 10:44:03 UTC",
    "updated_date": "2025-03-21 15:07:55 UTC"
  },
  {
    "arxiv_id": "2503.15498v1",
    "title": "Revival: Collaborative Artistic Creation through Human-AI Interactions in Musical Creativity",
    "authors": [
      "Keon Ju M. Lee",
      "Philippe Pasquier",
      "Jun Yuri"
    ],
    "abstract": "Revival is an innovative live audiovisual performance and music improvisation\nby our artist collective K-Phi-A, blending human and AI musicianship to create\nelectronic music with audio-reactive visuals. The performance features\nreal-time co-creative improvisation between a percussionist, an electronic\nmusic artist, and AI musical agents. Trained in works by deceased composers and\nthe collective's compositions, these agents dynamically respond to human input\nand emulate complex musical styles. An AI-driven visual synthesizer, guided by\na human VJ, produces visuals that evolve with the musical landscape. Revival\nshowcases the potential of AI and human collaboration in improvisational\nartistic creation.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.MA",
      "cs.MM",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.HC",
    "comment": "Keon Ju M. Lee, Philippe Pasquier and Jun Yuri. 2024. In Proceedings\n  of the Creativity and Generative AI NIPS (Neural Information Processing\n  Systems) Workshop",
    "pdf_url": "http://arxiv.org/pdf/2503.15498v1",
    "published_date": "2025-01-19 08:41:31 UTC",
    "updated_date": "2025-01-19 08:41:31 UTC"
  },
  {
    "arxiv_id": "2501.10970v2",
    "title": "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
    "authors": [
      "Nitay Calderon",
      "Roi Reichart",
      "Rotem Dror"
    ],
    "abstract": "The \"LLM-as-a-judge\" paradigm employs Large Language Models (LLMs) as\nannotators and evaluators in tasks traditionally performed by humans. LLM\nannotations are widely used, not only in NLP research but also in fields like\nmedicine, psychology, and social science. Despite their role in shaping study\nresults and insights, there is no standard or rigorous procedure to determine\nwhether LLMs can replace human annotators. In this paper, we propose a novel\nstatistical procedure -- the Alternative Annotator Test (alt-test) -- that\nrequires only a modest subset of annotated examples to justify using LLM\nannotations. Additionally, we introduce a versatile and interpretable measure\nfor comparing LLM judges. To demonstrate our procedure, we curated a diverse\ncollection of ten datasets, consisting of language and vision-language tasks,\nand conducted experiments with six LLMs and four prompting techniques. Our\nresults show that LLMs can sometimes replace humans with closed-source LLMs\n(such as GPT-4o), outperforming open-source LLMs, and that prompting techniques\nyield judges of varying quality. We hope this study encourages more rigorous\nand reliable practices.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.10970v2",
    "published_date": "2025-01-19 07:09:11 UTC",
    "updated_date": "2025-02-05 15:24:26 UTC"
  },
  {
    "arxiv_id": "2501.10967v2",
    "title": "Advancing General Multimodal Capability of Vision-language Models with Pyramid-descent Visual Position Encoding",
    "authors": [
      "Zhanpeng Chen",
      "Mingxiao Li",
      "Ziyang Chen",
      "Nan Du",
      "Xiaolong Li",
      "Yuexian Zou"
    ],
    "abstract": "Vision-language Models (VLMs) have shown remarkable capabilities in advancing\ngeneral artificial intelligence, yet the irrational encoding of visual\npositions persists in inhibiting the models' comprehensive perception\nperformance across different levels of granularity. In this work, we propose\nPyramid-descent Visual Position Encoding (PyPE), a novel approach designed to\nenhance the perception of visual tokens within VLMs. By assigning visual\nposition indexes from the periphery to the center and expanding the central\nreceptive field incrementally, PyPE addresses the limitations of traditional\nraster-scan methods and mitigates the long-term decay effects induced by Rotary\nPosition Embedding (RoPE). Our method reduces the relative distance between\ninterrelated visual elements and instruction tokens, promoting a more rational\nallocation of attention weights and allowing for a multi-granularity perception\nof visual elements and countering the over-reliance on anchor tokens. Extensive\nexperimental evaluations demonstrate that PyPE consistently improves the\ngeneral capabilities of VLMs across various sizes. Code is available at\nhttps://github.com/SakuraTroyChen/PyPE.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.10967v2",
    "published_date": "2025-01-19 07:00:46 UTC",
    "updated_date": "2025-02-12 08:10:06 UTC"
  },
  {
    "arxiv_id": "2501.10966v1",
    "title": "DC-PCN: Point Cloud Completion Network with Dual-Codebook Guided Quantization",
    "authors": [
      "Qiuxia Wu",
      "Haiyang Huang",
      "Kunming Su",
      "Zhiyong Wang",
      "Kun Hu"
    ],
    "abstract": "Point cloud completion aims to reconstruct complete 3D shapes from partial 3D\npoint clouds. With advancements in deep learning techniques, various methods\nfor point cloud completion have been developed. Despite achieving encouraging\nresults, a significant issue remains: these methods often overlook the\nvariability in point clouds sampled from a single 3D object surface. This\nvariability can lead to ambiguity and hinder the achievement of more precise\ncompletion results. Therefore, in this study, we introduce a novel point cloud\ncompletion network, namely Dual-Codebook Point Completion Network (DC-PCN),\nfollowing an encder-decoder pipeline. The primary objective of DC-PCN is to\nformulate a singular representation of sampled point clouds originating from\nthe same 3D surface. DC-PCN introduces a dual-codebook design to quantize\npoint-cloud representations from a multilevel perspective. It consists of an\nencoder-codebook and a decoder-codebook, designed to capture distinct point\ncloud patterns at shallow and deep levels. Additionally, to enhance the\ninformation flow between these two codebooks, we devise an information exchange\nmechanism. This approach ensures that crucial features and patterns from both\nshallow and deep levels are effectively utilized for completion. Extensive\nexperiments on the PCN, ShapeNet\\_Part, and ShapeNet34 datasets demonstrate the\nstate-of-the-art performance of our method.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "AAAI25 Accepted",
    "pdf_url": "http://arxiv.org/pdf/2501.10966v1",
    "published_date": "2025-01-19 06:57:45 UTC",
    "updated_date": "2025-01-19 06:57:45 UTC"
  },
  {
    "arxiv_id": "2501.10957v2",
    "title": "MARIO: A Mixed Annotation Framework For Polyp Segmentation",
    "authors": [
      "Haoyang Li",
      "Yiwen Hu",
      "Jun Wei",
      "Zhen Li"
    ],
    "abstract": "Existing polyp segmentation models are limited by high labeling costs and the\nsmall size of datasets. Additionally, vast polyp datasets remain underutilized\nbecause these models typically rely on a single type of annotation. To address\nthis dilemma, we introduce MARIO, a mixed supervision model designed to\naccommodate various annotation types, significantly expanding the range of\nusable data. MARIO learns from underutilized datasets by incorporating five\nforms of supervision: pixel-level, box-level, polygon-level, scribblelevel, and\npoint-level. Each form of supervision is associated with a tailored loss that\neffectively leverages the supervision labels while minimizing the noise. This\nallows MARIO to move beyond the constraints of relying on a single annotation\ntype. Furthermore, MARIO primarily utilizes dataset with weak and cheap\nannotations, reducing the dependence on large-scale, fully annotated ones.\nExperimental results across five benchmark datasets demonstrate that MARIO\nconsistently outperforms existing methods, highlighting its efficacy in\nbalancing trade-offs between different forms of supervision and maximizing\npolyp segmentation performance",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by IEEE ISBI 2025 4-page paper",
    "pdf_url": "http://arxiv.org/pdf/2501.10957v2",
    "published_date": "2025-01-19 06:11:02 UTC",
    "updated_date": "2025-02-12 10:06:09 UTC"
  },
  {
    "arxiv_id": "2501.10943v1",
    "title": "InsQABench: Benchmarking Chinese Insurance Domain Question Answering with Large Language Models",
    "authors": [
      "Jing Ding",
      "Kai Feng",
      "Binbin Lin",
      "Jiarui Cai",
      "Qiushi Wang",
      "Yu Xie",
      "Xiaojin Zhang",
      "Zhongyu Wei",
      "Wei Chen"
    ],
    "abstract": "The application of large language models (LLMs) has achieved remarkable\nsuccess in various fields, but their effectiveness in specialized domains like\nthe Chinese insurance industry remains underexplored. The complexity of\ninsurance knowledge, encompassing specialized terminology and diverse data\ntypes, poses significant challenges for both models and users. To address this,\nwe introduce InsQABench, a benchmark dataset for the Chinese insurance sector,\nstructured into three categories: Insurance Commonsense Knowledge, Insurance\nStructured Database, and Insurance Unstructured Documents, reflecting\nreal-world insurance question-answering tasks.We also propose two methods,\nSQL-ReAct and RAG-ReAct, to tackle challenges in structured and unstructured\ndata tasks. Evaluations show that while LLMs struggle with domain-specific\nterminology and nuanced clause texts, fine-tuning on InsQABench significantly\nimproves performance. Our benchmark establishes a solid foundation for\nadvancing LLM applications in the insurance domain, with data and code\navailable at https://github.com/HaileyFamo/InsQABench.git.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.10943v1",
    "published_date": "2025-01-19 04:53:20 UTC",
    "updated_date": "2025-01-19 04:53:20 UTC"
  },
  {
    "arxiv_id": "2501.10938v1",
    "title": "Blockchain-assisted Demonstration Cloning for Multi-Agent Deep Reinforcement Learning",
    "authors": [
      "Ahmed Alagha",
      "Jamal Bentahar",
      "Hadi Otrok",
      "Shakti Singh",
      "Rabeb Mizouni"
    ],
    "abstract": "Multi-Agent Deep Reinforcement Learning (MDRL) is a promising research area\nin which agents learn complex behaviors in cooperative or competitive\nenvironments. However, MDRL comes with several challenges that hinder its\nusability, including sample efficiency, curse of dimensionality, and\nenvironment exploration. Recent works proposing Federated Reinforcement\nLearning (FRL) to tackle these issues suffer from problems related to model\nrestrictions and maliciousness. Other proposals using reward shaping require\nconsiderable engineering and could lead to local optima. In this paper, we\npropose a novel Blockchain-assisted Multi-Expert Demonstration Cloning (MEDC)\nframework for MDRL. The proposed method utilizes expert demonstrations in\nguiding the learning of new MDRL agents, by suggesting exploration actions in\nthe environment. A model sharing framework on Blockchain is designed to allow\nusers to share their trained models, which can be allocated as expert models to\nrequesting users to aid in training MDRL systems. A Consortium Blockchain is\nadopted to enable traceable and autonomous execution without the need for a\nsingle trusted entity. Smart Contracts are designed to manage users and models\nallocation, which are shared using IPFS. The proposed framework is tested on\nseveral applications, and is benchmarked against existing methods in FRL,\nReward Shaping, and Imitation Learning-assisted RL. The results show the\noutperformance of the proposed framework in terms of learning speed and\nresiliency to faulty and malicious models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.10938v1",
    "published_date": "2025-01-19 04:20:24 UTC",
    "updated_date": "2025-01-19 04:20:24 UTC"
  },
  {
    "arxiv_id": "2501.10935v1",
    "title": "TSVC:Tripartite Learning with Semantic Variation Consistency for Robust Image-Text Retrieval",
    "authors": [
      "Shuai Lyu",
      "Zijing Tian",
      "Zhonghong Ou",
      "Yifan Zhu",
      "Xiao Zhang",
      "Qiankun Ha",
      "Haoran Luo",
      "Meina Song"
    ],
    "abstract": "Cross-modal retrieval maps data under different modality via semantic\nrelevance. Existing approaches implicitly assume that data pairs are\nwell-aligned and ignore the widely existing annotation noise, i.e., noisy\ncorrespondence (NC). Consequently, it inevitably causes performance\ndegradation. Despite attempts that employ the co-teaching paradigm with\nidentical architectures to provide distinct data perspectives, the differences\nbetween these architectures are primarily stemmed from random initialization.\nThus, the model becomes increasingly homogeneous along with the training\nprocess. Consequently, the additional information brought by this paradigm is\nseverely limited. In order to resolve this problem, we introduce a Tripartite\nlearning with Semantic Variation Consistency (TSVC) for robust image-text\nretrieval. We design a tripartite cooperative learning mechanism comprising a\nCoordinator, a Master, and an Assistant model. The Coordinator distributes\ndata, and the Assistant model supports the Master model's noisy label\nprediction with diverse data. Moreover, we introduce a soft label estimation\nmethod based on mutual information variation, which quantifies the noise in new\nsamples and assigns corresponding soft labels. We also present a new loss\nfunction to enhance robustness and optimize training effectiveness. Extensive\nexperiments on three widely used datasets demonstrate that, even at increasing\nnoise ratios, TSVC exhibits significant advantages in retrieval accuracy and\nmaintains stable training performance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This paper has been accepted to the Main Track of AAAI 2025. It\n  contains 9 pages, 7 figures, and is relevant to the areas of cross-modal\n  retrieval and machine learning. The work presents a novel approach in robust\n  image-text retrieval using a tripartite learning framework",
    "pdf_url": "http://arxiv.org/pdf/2501.10935v1",
    "published_date": "2025-01-19 04:05:08 UTC",
    "updated_date": "2025-01-19 04:05:08 UTC"
  },
  {
    "arxiv_id": "2501.10928v2",
    "title": "Generative Physical AI in Vision: A Survey",
    "authors": [
      "Daochang Liu",
      "Junyu Zhang",
      "Anh-Dung Dinh",
      "Eunbyung Park",
      "Shichao Zhang",
      "Ajmal Mian",
      "Mubarak Shah",
      "Chang Xu"
    ],
    "abstract": "Generative Artificial Intelligence (AI) has rapidly advanced the field of\ncomputer vision by enabling machines to create and interpret visual data with\nunprecedented sophistication. This transformation builds upon a foundation of\ngenerative models to produce realistic images, videos, and 3D/4D content.\nConventional generative models primarily focus on visual fidelity while often\nneglecting the physical plausibility of the generated content. This gap limits\ntheir effectiveness in applications that require adherence to real-world\nphysical laws, such as robotics, autonomous systems, and scientific\nsimulations. As generative models evolve to increasingly integrate physical\nrealism and dynamic simulation, their potential to function as \"world\nsimulators\" expands. Therefore, the field of physics-aware generation in\ncomputer vision is rapidly growing, calling for a comprehensive survey to\nprovide a structured analysis of current efforts. To serve this purpose, the\nsurvey presents a systematic review, categorizing methods based on how they\nincorporate physical knowledge, either through explicit simulation or implicit\nlearning. It also analyzes key paradigms, discusses evaluation protocols, and\nidentifies future research directions. By offering a comprehensive overview,\nthis survey aims to help future developments in physically grounded generation\nfor computer vision. The reviewed papers are summarized at\nhttps://tinyurl.com/Physics-Aware-Generation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "An updated version",
    "pdf_url": "http://arxiv.org/pdf/2501.10928v2",
    "published_date": "2025-01-19 03:19:47 UTC",
    "updated_date": "2025-04-19 14:52:08 UTC"
  },
  {
    "arxiv_id": "2501.13945v1",
    "title": "Self-Explanation in Social AI Agents",
    "authors": [
      "Rhea Basappa",
      "Mustafa Tekman",
      "Hong Lu",
      "Benjamin Faught",
      "Sandeep Kakar",
      "Ashok K. Goel"
    ],
    "abstract": "Social AI agents interact with members of a community, thereby changing the\nbehavior of the community. For example, in online learning, an AI social\nassistant may connect learners and thereby enhance social interaction. These\nsocial AI assistants too need to explain themselves in order to enhance\ntransparency and trust with the learners. We present a method of\nself-explanation that uses introspection over a self-model of an AI social\nassistant. The self-model is captured as a functional model that specifies how\nthe methods of the agent use knowledge to achieve its tasks. The process of\ngenerating self-explanations uses Chain of Thought to reflect on the self-model\nand ChatGPT to provide explanations about its functioning. We evaluate the\nself-explanation of the AI social assistant for completeness and correctness.\nWe also report on its deployment in a live class.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "Extended version of the paper published in International Conference\n  on Intelligent Tutoring Systems, pages 351-360, 2024, Springer. Images\n  corrected, and live deployment, ablation, and precision study results added",
    "pdf_url": "http://arxiv.org/pdf/2501.13945v1",
    "published_date": "2025-01-19 03:03:15 UTC",
    "updated_date": "2025-01-19 03:03:15 UTC"
  },
  {
    "arxiv_id": "2501.10924v1",
    "title": "Adaptive Target Localization under Uncertainty using Multi-Agent Deep Reinforcement Learning with Knowledge Transfer",
    "authors": [
      "Ahmed Alagha",
      "Rabeb Mizouni",
      "Shakti Singh",
      "Jamal Bentahar",
      "Hadi Otrok"
    ],
    "abstract": "Target localization is a critical task in sensitive applications, where\nmultiple sensing agents communicate and collaborate to identify the target\nlocation based on sensor readings. Existing approaches investigated the use of\nMulti-Agent Deep Reinforcement Learning (MADRL) to tackle target localization.\nNevertheless, these methods do not consider practical uncertainties, like false\nalarms when the target does not exist or when it is unreachable due to\nenvironmental complexities. To address these drawbacks, this work proposes a\nnovel MADRL-based method for target localization in uncertain environments. The\nproposed MADRL method employs Proximal Policy Optimization to optimize the\ndecision-making of sensing agents, which is represented in the form of an\nactor-critic structure using Convolutional Neural Networks. The observations of\nthe agents are designed in an optimized manner to capture essential information\nin the environment, and a team-based reward functions is proposed to produce\ncooperative agents. The MADRL method covers three action dimensionalities that\ncontrol the agents' mobility to search the area for the target, detect its\nexistence, and determine its reachability. Using the concept of Transfer\nLearning, a Deep Learning model builds on the knowledge from the MADRL model to\naccurately estimating the target location if it is unreachable, resulting in\nshared representations between the models for faster learning and lower\ncomputational complexity. Collectively, the final combined model is capable of\nsearching for the target, determining its existence and reachability, and\nestimating its location accurately. The proposed method is tested using a\nradioactive target localization environment and benchmarked against existing\nmethods, showing its efficacy.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.10924v1",
    "published_date": "2025-01-19 02:58:22 UTC",
    "updated_date": "2025-01-19 02:58:22 UTC"
  },
  {
    "arxiv_id": "2501.10917v2",
    "title": "Decomposing and Fusing Intra- and Inter-Sensor Spatio-Temporal Signal for Multi-Sensor Wearable Human Activity Recognition",
    "authors": [
      "Haoyu Xie",
      "Haoxuan Li",
      "Chunyuan Zheng",
      "Haonan Yuan",
      "Guorui Liao",
      "Jun Liao",
      "Li Liu"
    ],
    "abstract": "Wearable Human Activity Recognition (WHAR) is a prominent research area\nwithin ubiquitous computing. Multi-sensor synchronous measurement has proven to\nbe more effective for WHAR than using a single sensor. However, existing WHAR\nmethods use shared convolutional kernels for indiscriminate temporal feature\nextraction across each sensor variable, which fails to effectively capture\nspatio-temporal relationships of intra-sensor and inter-sensor variables. We\npropose the DecomposeWHAR model consisting of a decomposition phase and a\nfusion phase to better model the relationships between modality variables. The\ndecomposition creates high-dimensional representations of each intra-sensor\nvariable through the improved Depth Separable Convolution to capture local\ntemporal features while preserving their unique characteristics. The fusion\nphase begins by capturing relationships between intra-sensor variables and\nfusing their features at both the channel and variable levels. Long-range\ntemporal dependencies are modeled using the State Space Model (SSM), and later\ncross-sensor interactions are dynamically captured through a self-attention\nmechanism, highlighting inter-sensor spatial correlations. Our model\ndemonstrates superior performance on three widely used WHAR datasets,\nsignificantly outperforming state-of-the-art models while maintaining\nacceptable computational efficiency.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.10917v2",
    "published_date": "2025-01-19 01:52:28 UTC",
    "updated_date": "2025-04-25 04:02:01 UTC"
  },
  {
    "arxiv_id": "2501.10909v1",
    "title": "Fine-Grained Appropriate Reliance: Human-AI Collaboration with a Multi-Step Transparent Decision Workflow for Complex Task Decomposition",
    "authors": [
      "Gaole He",
      "Patrick Hemmer",
      "Michael Vössing",
      "Max Schemmer",
      "Ujwal Gadiraju"
    ],
    "abstract": "In recent years, the rapid development of AI systems has brought about the\nbenefits of intelligent services but also concerns about security and\nreliability. By fostering appropriate user reliance on an AI system, both\ncomplementary team performance and reduced human workload can be achieved.\nPrevious empirical studies have extensively analyzed the impact of factors\nranging from task, system, and human behavior on user trust and appropriate\nreliance in the context of one-step decision making. However, user reliance on\nAI systems in tasks with complex semantics that require multi-step workflows\nremains under-explored. Inspired by recent work on task decomposition with\nlarge language models, we propose to investigate the impact of a novel\nMulti-Step Transparent (MST) decision workflow on user reliance behaviors. We\nconducted an empirical study (N = 233) of AI-assisted decision making in\ncomposite fact-checking tasks (i.e., fact-checking tasks that entail multiple\nsub-fact verification steps). Our findings demonstrate that human-AI\ncollaboration with an MST decision workflow can outperform one-step\ncollaboration in specific contexts (e.g., when advice from an AI system is\nmisleading). Further analysis of the appropriate reliance at fine-grained\nlevels indicates that an MST decision workflow can be effective when users\ndemonstrate a relatively high consideration of the intermediate steps. Our work\nhighlights that there is no one-size-fits-all decision workflow that can help\nobtain optimal human-AI collaboration. Our insights help deepen the\nunderstanding of the role of decision workflows in facilitating appropriate\nreliance. We synthesize important implications for designing effective means to\nfacilitate appropriate reliance on AI systems in composite tasks, positioning\nopportunities for the human-centered AI and broader HCI communities.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2501.10909v1",
    "published_date": "2025-01-19 01:03:09 UTC",
    "updated_date": "2025-01-19 01:03:09 UTC"
  }
]
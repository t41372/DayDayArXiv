{
  "date": "2024-08-31",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-08-31 的 arXiv 中文 TLDR 快报！今天 arXiv 更新了 36 篇论文，主要聚焦 AI 和机器学习领域，包括 LLM 在工具使用、安全和医疗中的创新应用，以及深度学习在环境监测和网络安全的优化；令人印象深刻的是 Wenxuan Wang 多篇关于 LLM 评估和工具学习的论文，以及 LLM 在精准医学中的潜力，这些工作可能推动 AI 伦理和实际应用。\n\n今天的核心论文多围绕 LLM 和 AI 优化，我将先讨论最具话题度和影响力的文章（如 LLM 相关），然后简要提及其他相关或次要论文。以下是精选摘要，突出每篇的核心贡献、方法和发现。\n\n### 1. **Learning to Ask: When LLM Agents Meet Unclear Instruction** (中文：当 LLM 代理遇到模糊指令时的学习；英文：Learning to Ask: When LLM Agents Meet Unclear Instruction)  \n作者 Wenxuan Wang 等。这篇论文由知名学者 Wenxuan Wang 撰写，聚焦 LLM 在工具使用中的鲁棒性。核心贡献是提出 Ask-when-Needed (AwN) 框架，让 LLM 通过提问澄清模糊指令，避免幻觉生成。方法结合混合密度网络和自动评估工具，实验显示 AwN 在 NoisyToolBench 数据集上显著提升准确性和效率，这可能为 LLM 在真实世界应用带来重要启发，如减少风险的对话系统。\n\n### 2. **Testing and Evaluation of Large Language Models: Correctness, Non-Toxicity, and Fairness** (中文：大语言模型的测试与评估：正确性、非毒性和公平性；英文：Testing and Evaluation of Large Language Models: Correctness, Non-Toxicity, and Fairness)  \n作者 Wenxuan Wang。这篇博士论文同样出自 Wenxuan Wang，强调 LLM 的可靠性评估。主要贡献是开发 FactChecker 和 LogicAsker 等框架，评估 LLM 的事实知识、逻辑推理、非毒性和公平性（如社会偏见）。发现 LLM 常产生事实错误和偏差，论文提出红队测试方法，可能对 LLM 伦理治理产生深远影响，是今天最值得关注的文章之一。\n\n### 3. **Large Language Models-Enabled Digital Twins for Precision Medicine in Rare Gynecological Tumors** (中文：基于大语言模型的数字孪生用于罕见妇科肿瘤的精准医学；英文：Large Language Models-Enabled Digital Twins for Precision Medicine in Rare Gynecological Tumors)  \n作者 Jakob Nikolas Kather 等。这篇论文探索 LLM 在医疗中的应用，核心是构建数字孪生系统整合临床和生物标记数据。方法使用 LLM 处理非结构化数据，为罕见肿瘤如子宫肉瘤提供个性化治疗方案。发现这种方法能识别传统分析忽略的治疗选项，提升患者预后，这在精准医学领域有实际意义，展示了 AI 在医疗的潜力。\n\n### 4. **BreachSeek: A Multi-Agent Automated Penetration Tester** (中文：BreachSeek：一个多代理自动渗透测试系统；英文：BreachSeek: A Multi-Agent Automated Penetration Tester)  \n作者 Ibrahim Alshehri 等。论文提出 AI 驱动的多代理系统，用于自动化网络渗透测试。核心贡献是整合 LLM（如 LangChain）识别漏洞并生成报告，方法包括模拟攻击和利用 YOLO-like 框架。实验显示其在本地网络中有效利用漏洞，这可能提升网络安全效率，相关于 AI 在安全领域的应用。\n\n### 5. **GenAI-powered Multi-Agent Paradigm for Smart Urban Mobility** (中文：基于生成 AI 的多代理范式用于智能城市交通；英文：GenAI-powered Multi-Agent Paradigm for Smart Urban Mobility)  \n作者 Haowen Xu 等。论文讨论 LLM 和 RAG 在智能交通系统的整合，核心是构建多代理框架优化交通拥堵和碳排放。方法结合 LLM 的对话能力和 RAG 的知识检索，目标是自动化交通管理。发现这种范式超越传统规则系统，可能推动可持续城市发展，是 AI 在交通领域的创新尝试。\n\n其他论文中，相关主题如深度学习在核聚变设计（论文 1）和 AI 在环境监测（如论文 11 的 UAV 火灾监控）也值得一提，但篇幅有限，仅快速概述：\n- **Using Deep Learning to Design High Aspect Ratio Fusion Devices** (中文：使用深度学习设计高宽比核聚变设备；英文：Using Deep Learning to Design High Aspect Ratio Fusion Devices)：作者 P. Curvo 等。贡献：训练混合密度网络解决逆设计问题，提升核聚变设备的约束性能。\n- **Data Augmentation for Image Classification using Generative AI** (中文：使用生成 AI 的数据增强进行图像分类；英文：Data Augmentation for Image Classification using Generative AI)：作者 Fazle Rahat 等。贡献：提出 AGA 框架结合 LLM 和扩散模型，提高图像分类准确率 15.6%。\n- **Multi-Output Distributional Fairness via Post-Processing** (中文：通过后处理实现多输出分布公平性；英文：Multi-Output Distributional Fairness via Post-Processing)：作者 Gang Li 等。贡献：优化多任务模型的公平性，使用最优传输映射提升分布平价。\n- 其余如植物检测（论文 10）和财务新闻分析（论文 19）等技术性较强的文章，贡献集中在特定领域优化（如模糊损失或超图网络），但影响力较小，故从简。\n\n总之，今天的 arXiv 突显 AI 模型的多样应用，尤其是 LLM 在安全和医疗的潜力。感兴趣的读者可关注 Wenxuan Wang 的作品，探索更多创新点。明天见！",
  "papers": [
    {
      "arxiv_id": "2409.00564v3",
      "title": "Using Deep Learning to Design High Aspect Ratio Fusion Devices",
      "title_zh": "使用深度学习设计高纵横比聚变装置",
      "authors": [
        "P. Curvo",
        "D. R. Ferreira",
        "R. Jorge"
      ],
      "abstract": "The design of fusion devices is typically based on computationally expensive\nsimulations. This can be alleviated using high aspect ratio models that employ\na reduced number of free parameters, especially in the case of stellarator\noptimization where non-axisymmetric magnetic fields with a large parameter\nspace are optimized to satisfy certain performance criteria. However,\noptimization is still required to find configurations with properties such as\nlow elongation, high rotational transform, finite plasma beta, and good fast\nparticle confinement. In this work, we train a machine learning model to\nconstruct configurations with favorable confinement properties by finding a\nsolution to the inverse design problem, that is, obtaining a set of model input\nparameters for given desired properties. Since the solution of the inverse\nproblem is non-unique, a probabilistic approach, based on mixture density\nnetworks, is used. It is shown that optimized configurations can be generated\nreliably using this method.",
      "tldr_zh": "本研究利用深度学习（Deep Learning）简化高宽比融合设备的设计，针对恒星器（Stellarator）优化中的计算密集问题，通过减少自由参数来提升效率。作者训练了一个机器学习模型来解决逆设计问题（Inverse Design Problem），即给定期望属性（如低伸长率、高旋转变换和良好粒子约束），使用混合密度网络（Mixture Density Networks）的概率方法生成对应的输入参数。由于逆问题非唯一，这种方法确保了可靠的解决方案。实验结果表明，该方法能有效生成优化配置，提高了融合设备设计的性能和可行性。",
      "categories": [
        "physics.plasm-ph",
        "cs.AI"
      ],
      "primary_category": "physics.plasm-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.00564v3",
      "published_date": "2024-08-31 23:28:10 UTC",
      "updated_date": "2025-01-14 21:45:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T20:15:42.678197"
    },
    {
      "arxiv_id": "2409.00557v3",
      "title": "Learning to Ask: When LLM Agents Meet Unclear Instruction",
      "title_zh": "翻译失败",
      "authors": [
        "Wenxuan Wang",
        "Juluan Shi",
        "Zixuan Ling",
        "Yuk-Kit Chan",
        "Chaozheng Wang",
        "Cheryl Lee",
        "Youliang Yuan",
        "Jen-tse Huang",
        "Wenxiang Jiao",
        "Michael R. Lyu"
      ],
      "abstract": "Equipped with the capability to call functions, modern large language models\n(LLMs) can leverage external tools for addressing a range of tasks unattainable\nthrough language skills alone. However, the effective execution of these tools\nrelies heavily not just on the advanced capabilities of LLMs but also on\nprecise user instructions, which often cannot be ensured in the real world. To\nevaluate the performance of LLMs tool-use under imperfect instructions, we\nmeticulously examine the real-world instructions queried from users, analyze\nthe error patterns, and build a challenging tool-use benchmark called Noisy\nToolBench (NoisyToolBench). We find that due to the next-token prediction\ntraining objective, LLMs tend to arbitrarily generate the missed argument,\nwhich may lead to hallucinations and risks. To address this issue, we propose a\nnovel framework, Ask-when-Needed (AwN), which prompts LLMs to ask questions to\nusers whenever they encounter obstacles due to unclear instructions. Moreover,\nto reduce the manual labor involved in user-LLM interaction and assess LLMs\nperformance in tool utilization from both accuracy and efficiency perspectives,\nwe design an automated evaluation tool named ToolEvaluator. Our experiments\ndemonstrate that the AwN significantly outperforms existing frameworks for tool\nlearning in the NoisyToolBench. We will release all related code and datasets\nto support future research.",
      "tldr_zh": "该研究探讨了大语言模型（LLMs）在处理不清晰指令时的工具使用问题，通过分析真实用户指令构建了一个挑战性基准 Noisy ToolBench，发现 LLMs 常因随意生成缺失参数而导致幻觉和风险。作者提出了一种新框架 Ask-when-Needed (AwN)，让 LLMs 在遇到障碍时主动向用户提问，以提升工具使用的准确性和效率，并设计了自动化评估工具 ToolEvaluator 用于评估性能。实验结果显示，AwN 在 Noisy ToolBench 上显著优于现有框架，并将相关代码和数据集开源以支持进一步研究。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.00557v3",
      "published_date": "2024-08-31 23:06:12 UTC",
      "updated_date": "2025-02-16 14:50:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T20:15:53.779951"
    },
    {
      "arxiv_id": "2409.00553v2",
      "title": "Multi-Output Distributional Fairness via Post-Processing",
      "title_zh": "通过后处理的多种输出分布公平性",
      "authors": [
        "Gang Li",
        "Qihang Lin",
        "Ayush Ghosh",
        "Tianbao Yang"
      ],
      "abstract": "The post-processing approaches are becoming prominent techniques to enhance\nmachine learning models' fairness because of their intuitiveness, low\ncomputational cost, and excellent scalability. However, most existing\npost-processing methods are designed for task-specific fairness measures and\nare limited to single-output models. In this paper, we introduce a\npost-processing method for multi-output models, such as the ones used for\nmulti-task/multi-class classification and representation learning, to enhance a\nmodel's distributional parity, a task-agnostic fairness measure. Existing\nmethods for achieving distributional parity rely on the (inverse) cumulative\ndensity function of a model's output, restricting their applicability to\nsingle-output models. Extending previous works, we propose to employ optimal\ntransport mappings to move a model's outputs across different groups towards\ntheir empirical Wasserstein barycenter. An approximation technique is applied\nto reduce the complexity of computing the exact barycenter and a kernel\nregression method is proposed to extend this process to out-of-sample data. Our\nempirical studies evaluate the proposed approach against various baselines on\nmulti-task/multi-class classification and representation learning tasks,\ndemonstrating the effectiveness of the proposed approach.",
      "tldr_zh": "本文提出了一种后处理方法（post-processing），旨在提升多输出模型（如多任务/多类分类和表示学习）的分布平价（distributional parity），这是一种任务无关的公平度量，解决了现有方法限于单输出模型的局限性。该方法使用optimal transport mappings将不同群体的模型输出调整至经验Wasserstein barycenter，以实现输出分布的平衡。为降低计算复杂度，该方法引入近似技术和kernel regression处理样本外数据。实证研究表明，该方法在多任务/多类分类和表示学习任务上优于基线模型，证明了其有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "21 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.00553v2",
      "published_date": "2024-08-31 22:41:26 UTC",
      "updated_date": "2025-03-20 16:42:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T20:16:08.670250"
    },
    {
      "arxiv_id": "2409.00551v1",
      "title": "Testing and Evaluation of Large Language Models: Correctness, Non-Toxicity, and Fairness",
      "title_zh": "大型语言模型的测试和评估：正确性、非",
      "authors": [
        "Wenxuan Wang"
      ],
      "abstract": "Large language models (LLMs), such as ChatGPT, have rapidly penetrated into\npeople's work and daily lives over the past few years, due to their\nextraordinary conversational skills and intelligence. ChatGPT has become the\nfastest-growing software in terms of user numbers in human history and become\nan important foundational model for the next generation of artificial\nintelligence applications. However, the generations of LLMs are not entirely\nreliable, often producing content with factual errors, biases, and toxicity.\nGiven their vast number of users and wide range of application scenarios, these\nunreliable responses can lead to many serious negative impacts. This thesis\nintroduces the exploratory works in the field of language model reliability\nduring the PhD study, focusing on the correctness, non-toxicity, and fairness\nof LLMs from both software testing and natural language processing\nperspectives. First, to measure the correctness of LLMs, we introduce two\ntesting frameworks, FactChecker and LogicAsker, to evaluate factual knowledge\nand logical reasoning accuracy, respectively. Second, for the non-toxicity of\nLLMs, we introduce two works for red-teaming LLMs. Third, to evaluate the\nfairness of LLMs, we introduce two evaluation frameworks, BiasAsker and\nXCulturalBench, to measure the social bias and cultural bias of LLMs,\nrespectively.",
      "tldr_zh": "这篇论文探讨了大型语言模型（LLMs）在正确性、非毒性和公平性方面的测试和评估，以解决其生成内容可能存在的错误、偏见和有害问题。作者从软件测试和自然语言处理角度，引入了FactChecker和LogicAsker框架来评估LLMs的事实知识和逻辑推理准确性；同时，开发了两个red-teaming方法来检测非毒性；此外，BiasAsker和XCulturalBench框架则用于测量LLMs的社会偏见和文化偏见。这些工作为提升LLMs的可靠性和可信度提供了关键工具，减少其在实际应用中的负面影响。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CL",
      "comment": "PhD Thesis",
      "pdf_url": "http://arxiv.org/pdf/2409.00551v1",
      "published_date": "2024-08-31 22:21:04 UTC",
      "updated_date": "2024-08-31 22:21:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T20:16:18.221439"
    },
    {
      "arxiv_id": "2409.00547v1",
      "title": "Data Augmentation for Image Classification using Generative AI",
      "title_zh": "翻译失败",
      "authors": [
        "Fazle Rahat",
        "M Shifat Hossain",
        "Md Rubel Ahmed",
        "Sumit Kumar Jha",
        "Rickard Ewetz"
      ],
      "abstract": "Scaling laws dictate that the performance of AI models is proportional to the\namount of available data. Data augmentation is a promising solution to\nexpanding the dataset size. Traditional approaches focused on augmentation\nusing rotation, translation, and resizing. Recent approaches use generative AI\nmodels to improve dataset diversity. However, the generative methods struggle\nwith issues such as subject corruption and the introduction of irrelevant\nartifacts. In this paper, we propose the Automated Generative Data Augmentation\n(AGA). The framework combines the utility of large language models (LLMs),\ndiffusion models, and segmentation models to augment data. AGA preserves\nforeground authenticity while ensuring background diversity. Specific\ncontributions include: i) segment and superclass based object extraction, ii)\nprompt diversity with combinatorial complexity using prompt decomposition, and\niii) affine subject manipulation. We evaluate AGA against state-of-the-art\n(SOTA) techniques on three representative datasets, ImageNet, CUB, and\niWildCam. The experimental evaluation demonstrates an accuracy improvement of\n15.6% and 23.5% for in and out-of-distribution data compared to baseline\nmodels, respectively. There is also a 64.3% improvement in SIC score compared\nto the baselines.",
      "tldr_zh": "这篇论文探讨了使用Generative AI进行图像分类数据扩充的方法，以应对传统技术（如旋转、平移和缩放）的局限性，以及现有生成方法带来的主体损坏和无关伪影问题。作者提出Automated Generative Data Augmentation (AGA)框架，该框架整合Large Language Models (LLMs)、diffusion models和segmentation models，实现基于分割和超类的物体提取、提示分解带来的多样性，以及仿射主体操作，以保持前景真实性和背景多样性。具体贡献包括在ImageNet、CUB和iWildCam数据集上的实验，AGA相比基线模型提升了15.6%的in-distribution准确率和23.5%的out-of-distribution准确率，同时SIC score提高了64.3%。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "I.2.10; I.5.1"
      ],
      "primary_category": "cs.CV",
      "comment": "19 pages, 15 figures, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2409.00547v1",
      "published_date": "2024-08-31 21:16:43 UTC",
      "updated_date": "2024-08-31 21:16:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T20:16:31.724958"
    },
    {
      "arxiv_id": "2409.11409v1",
      "title": "CyberNFTs: Conceptualizing a decentralized and reward-driven intrusion detection system with ML",
      "title_zh": "翻译失败",
      "authors": [
        "Synim Selimi",
        "Blerim Rexha",
        "Kamer Vishi"
      ],
      "abstract": "The rapid evolution of the Internet, particularly the emergence of Web3, has\ntransformed the ways people interact and share data. Web3, although still not\nwell defined, is thought to be a return to the decentralization of\ncorporations' power over user data. Despite the obsolescence of the idea of\nbuilding systems to detect and prevent cyber intrusions, this is still a topic\nof interest. This paper proposes a novel conceptual approach for implementing\ndecentralized collaborative intrusion detection networks (CIDN) through a\nproof-of-concept. The study employs an analytical and comparative methodology,\nexamining the synergy between cutting-edge Web3 technologies and information\nsecurity. The proposed model incorporates blockchain concepts, cyber\nnon-fungible token (cyberNFT) rewards, machine learning algorithms, and\npublish/subscribe architectures. Finally, the paper discusses the strengths and\nlimitations of the proposed system, offering insights into the potential of\ndecentralized cybersecurity models.",
      "tldr_zh": "这篇论文提出了 CyberNFTs，一个基于 Web3 的去中心化协作入侵检测网络 (CIDN) 概念，利用机器学习算法和奖励机制来增强网络安全。系统整合区块链、cyberNFT 奖励以及发布/订阅架构，通过分析和比较方法考察 Web3 技术与信息安全的协同作用。最终，论文讨论了该模型的优势（如去中心化协作）和局限性，为构建奖励驱动的去中心化网络安全系统提供了潜在见解。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "9 pages, 6 figures, 1 table, 1 algorithm, 1 listing, journal article",
      "pdf_url": "http://arxiv.org/pdf/2409.11409v1",
      "published_date": "2024-08-31 21:15:26 UTC",
      "updated_date": "2024-08-31 21:15:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T20:16:42.247512"
    },
    {
      "arxiv_id": "2409.00544v1",
      "title": "Large Language Models-Enabled Digital Twins for Precision Medicine in Rare Gynecological Tumors",
      "title_zh": "翻译失败",
      "authors": [
        "Jacqueline Lammert",
        "Nicole Pfarr",
        "Leonid Kuligin",
        "Sonja Mathes",
        "Tobias Dreyer",
        "Luise Modersohn",
        "Patrick Metzger",
        "Dyke Ferber",
        "Jakob Nikolas Kather",
        "Daniel Truhn",
        "Lisa Christine Adams",
        "Keno Kyrill Bressem",
        "Sebastian Lange",
        "Kristina Schwamborn",
        "Martin Boeker",
        "Marion Kiechle",
        "Ulrich A. Schatz",
        "Holger Bronger",
        "Maximilian Tschochohei"
      ],
      "abstract": "Rare gynecological tumors (RGTs) present major clinical challenges due to\ntheir low incidence and heterogeneity. The lack of clear guidelines leads to\nsuboptimal management and poor prognosis. Molecular tumor boards accelerate\naccess to effective therapies by tailoring treatment based on biomarkers,\nbeyond cancer type. Unstructured data that requires manual curation hinders\nefficient use of biomarker profiling for therapy matching. This study explores\nthe use of large language models (LLMs) to construct digital twins for\nprecision medicine in RGTs.\n  Our proof-of-concept digital twin system integrates clinical and biomarker\ndata from institutional and published cases (n=21) and literature-derived data\n(n=655 publications with n=404,265 patients) to create tailored treatment plans\nfor metastatic uterine carcinosarcoma, identifying options potentially missed\nby traditional, single-source analysis. LLM-enabled digital twins efficiently\nmodel individual patient trajectories. Shifting to a biology-based rather than\norgan-based tumor definition enables personalized care that could advance RGT\nmanagement and thus enhance patient outcomes.",
      "tldr_zh": "该研究探讨了利用大型语言模型（LLMs）构建数字孪生（Digital Twins）系统，以推进罕见妇科肿瘤（Rare Gynecological Tumors，RGTs）的精准医学（Precision Medicine）。该系统整合了临床数据、生物标记物信息（包括21个机构案例和655篇文献，涉及404,265患者），用于创建个性化治疗计划，例如为转移性子宫肉瘤识别潜在治疗选项，这些选项可能被传统单源分析忽略。结果表明，LLMs 驱动的数字孪生能高效模拟患者轨迹，促进从器官基础向生物学基础的肿瘤定义转变，从而提升 RGTs 的管理水平和患者预后。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "q-bio.QM",
        "stat.ML"
      ],
      "primary_category": "cs.CL",
      "comment": "20 pages, 2 figures, 3 tables, supplements, original article",
      "pdf_url": "http://arxiv.org/pdf/2409.00544v1",
      "published_date": "2024-08-31 21:14:09 UTC",
      "updated_date": "2024-08-31 21:14:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T20:16:55.362741"
    },
    {
      "arxiv_id": "2409.03789v1",
      "title": "BreachSeek: A Multi-Agent Automated Penetration Tester",
      "title_zh": "BreachSeek：多智能体自动化渗透测试器",
      "authors": [
        "Ibrahim Alshehri",
        "Adnan Alshehri",
        "Abdulrahman Almalki",
        "Majed Bamardouf",
        "Alaqsa Akbar"
      ],
      "abstract": "The increasing complexity and scale of modern digital environments have\nexposed significant gaps in traditional cybersecurity penetration testing\nmethods, which are often time-consuming, labor-intensive, and unable to rapidly\nadapt to emerging threats. There is a critical need for an automated solution\nthat can efficiently identify and exploit vulnerabilities across diverse\nsystems without extensive human intervention. BreachSeek addresses this\nchallenge by providing an AI-driven multi-agent software platform that\nleverages Large Language Models (LLMs) integrated through LangChain and\nLangGraph in Python. This system enables autonomous agents to conduct thorough\npenetration testing by identifying vulnerabilities, simulating a variety of\ncyberattacks, executing exploits, and generating comprehensive security\nreports. In preliminary evaluations, BreachSeek successfully exploited\nvulnerabilities in exploitable machines within local networks, demonstrating\nits practical effectiveness. Future developments aim to expand its\ncapabilities, positioning it as an indispensable tool for cybersecurity\nprofessionals.",
      "tldr_zh": "该研究指出了传统网络安全渗透测试方法的局限性，如耗时劳动密集且难以适应新兴威胁，并提出 BreachSeek 作为一种 AI 驱动的多智能体软件平台来解决这些问题。BreachSeek 利用 Large Language Models (LLMs) 通过 LangChain 和 LangGraph 在 Python 中集成，允许自治代理自动识别漏洞、模拟各种网络攻击、执行利用并生成全面安全报告。在初步评估中，该系统成功在本地网络中利用漏洞，证明了其实际有效性。未来，BreachSeek 计划扩展功能，成为网络安全专业人士的必备工具。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "7 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.03789v1",
      "published_date": "2024-08-31 19:15:38 UTC",
      "updated_date": "2024-08-31 19:15:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T20:17:05.408887"
    },
    {
      "arxiv_id": "2409.00518v1",
      "title": "Mapping earth mounds from space",
      "title_zh": "翻译失败",
      "authors": [
        "Baki Uzun",
        "Shivam Pande",
        "Gwendal Cachin-Bernard",
        "Minh-Tan Pham",
        "Sébastien Lefèvre",
        "Rumais Blatrix",
        "Doyle McKey"
      ],
      "abstract": "Regular patterns of vegetation are considered widespread landscapes, although\ntheir global extent has never been estimated. Among them, spotted landscapes\nare of particular interest in the context of climate change. Indeed, regularly\nspaced vegetation spots in semi-arid shrublands result from extreme resource\ndepletion and prefigure catastrophic shift of the ecosystem to a homogeneous\ndesert, while termite mounds also producing spotted landscapes were shown to\nincrease robustness to climate change. Yet, their identification at large scale\ncalls for automatic methods, for instance using the popular deep learning\nframework, able to cope with a vast amount of remote sensing data, e.g.,\noptical satellite imagery. In this paper, we tackle this problem and benchmark\nsome state-of-the-art deep networks on several landscapes and geographical\nareas. Despite the promising results we obtained, we found that more research\nis needed to be able to map automatically these earth mounds from space.",
      "tldr_zh": "这篇论文探讨了从太空映射地球土丘图案的问题，特别是定期植被斑点景观，这些图案可能预示着半干旱灌木丛生态系统的灾难性转变（如转为沙漠）或增强气候变化的鲁棒性（如白蚁土丘）。作者使用深度学习框架分析大量遥感数据（如光学卫星图像），并对几种最先进的深度网络在不同景观和地理区域进行了基准测试。结果显示这些方法取得了有前景的表现，但仍需更多研究来实现可靠的自动映射。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "6 pages, 4 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2409.00518v1",
      "published_date": "2024-08-31 18:08:37 UTC",
      "updated_date": "2024-08-31 18:08:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T20:17:17.778181"
    },
    {
      "arxiv_id": "2409.00513v1",
      "title": "Plant detection from ultra high resolution remote sensing images: A Semantic Segmentation approach based on fuzzy loss",
      "title_zh": "从超高分辨率遥感图像中检测植物：基于模糊损失的",
      "authors": [
        "Shivam Pande",
        "Baki Uzun",
        "Florent Guiotte",
        "Thomas Corpetti",
        "Florian Delerue",
        "Sébastien Lefèvre"
      ],
      "abstract": "In this study, we tackle the challenge of identifying plant species from\nultra high resolution (UHR) remote sensing images. Our approach involves\nintroducing an RGB remote sensing dataset, characterized by millimeter-level\nspatial resolution, meticulously curated through several field expeditions\nacross a mountainous region in France covering various landscapes. The task of\nplant species identification is framed as a semantic segmentation problem for\nits practical and efficient implementation across vast geographical areas.\nHowever, when dealing with segmentation masks, we confront instances where\ndistinguishing boundaries between plant species and their background is\nchallenging. We tackle this issue by introducing a fuzzy loss within the\nsegmentation model. Instead of utilizing one-hot encoded ground truth (GT), our\nmodel incorporates Gaussian filter refined GT, introducing stochasticity during\ntraining. First experimental results obtained on both our UHR dataset and a\npublic dataset are presented, showing the relevance of the proposed\nmethodology, as well as the need for future improvement.",
      "tldr_zh": "本研究针对从超高分辨率（UHR）遥感图像中识别植物物种的挑战，提出了一种基于语义分割（semantic segmentation）的创新方法。研究者引入了一个新的 RGB 遥感数据集，该数据集具有毫米级空间分辨率，通过法国山区实地考察收集，涵盖多种景观。针对植物物种与背景边界区分困难的问题，他们开发了模糊损失（fuzzy loss），使用高斯滤波器改进的地面真实值（GT）引入训练过程中的随机性。初步实验结果显示，该方法在自有 UHR 数据集和一个公共数据集上表现出色，但仍需进一步优化。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "5 pages, 5 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2409.00513v1",
      "published_date": "2024-08-31 17:40:17 UTC",
      "updated_date": "2024-08-31 17:40:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T20:17:30.234374"
    },
    {
      "arxiv_id": "2409.00510v1",
      "title": "Streamlining Forest Wildfire Surveillance: AI-Enhanced UAVs Utilizing the FLAME Aerial Video Dataset for Lightweight and Efficient Monitoring",
      "title_zh": "优化森林野火监测：利用 FLAME 航空视频数据集的 AI 增强无人机实现轻量级高效监测",
      "authors": [
        "Lemeng Zhao",
        "Junjie Hu",
        "Jianchao Bi",
        "Yanbing Bai",
        "Erick Mas",
        "Shunichi Koshimura"
      ],
      "abstract": "In recent years, unmanned aerial vehicles (UAVs) have played an increasingly\ncrucial role in supporting disaster emergency response efforts by analyzing\naerial images. While current deep-learning models focus on improving accuracy,\nthey often overlook the limited computing resources of UAVs. This study\nrecognizes the imperative for real-time data processing in disaster response\nscenarios and introduces a lightweight and efficient approach for aerial video\nunderstanding. Our methodology identifies redundant portions within the video\nthrough policy networks and eliminates this excess information using frame\ncompression techniques. Additionally, we introduced the concept of a `station\npoint,' which leverages future information in the sequential policy network,\nthereby enhancing accuracy. To validate our method, we employed the wildfire\nFLAME dataset. Compared to the baseline, our approach reduces computation costs\nby more than 13 times while boosting accuracy by 3$\\%$. Moreover, our method\ncan intelligently select salient frames from the video, refining the dataset.\nThis feature enables sophisticated models to be effectively trained on a\nsmaller dataset, significantly reducing the time spent during the training\nprocess.",
      "tldr_zh": "本论文针对无人机（UAVs）在森林野火监测中的计算资源限制，提出了一种轻量级高效的空中视频理解方法，以实现实时数据处理。方法通过 policy networks 识别视频冗余部分并应用帧压缩技术来消除多余信息，同时引入 station point 概念，利用序列策略网络中的未来信息提升准确性。在 FLAME Aerial Video Dataset 上验证，该方法比基线减少计算成本超过13倍，同时提高准确性3%，并能智能选择关键帧优化数据集，从而加速复杂模型的训练过程。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "accpeted by Proceedings of the International Conference on\n  Intelligent Robots and Systems (2024 IROS)",
      "pdf_url": "http://arxiv.org/pdf/2409.00510v1",
      "published_date": "2024-08-31 17:26:53 UTC",
      "updated_date": "2024-08-31 17:26:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T20:17:43.650787"
    },
    {
      "arxiv_id": "2409.00494v2",
      "title": "GenAI-powered Multi-Agent Paradigm for Smart Urban Mobility: Opportunities and Challenges for Integrating Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) with Intelligent Transportation Systems",
      "title_zh": "GenAI 驱动的多智能体范式在智能城市移动性中的应用：整合大语言模型 (LLMs) 和检索增强生成 (RAG) 与",
      "authors": [
        "Haowen Xu",
        "Jinghui Yuan",
        "Anye Zhou",
        "Guanhao Xu",
        "Wan Li",
        "Xuegang Ban",
        "Xinyue Ye"
      ],
      "abstract": "Leveraging recent advances in generative AI, multi-agent systems are\nincreasingly being developed to enhance the functionality and efficiency of\nsmart city applications. This paper explores the transformative potential of\nlarge language models (LLMs) and emerging Retrieval-Augmented Generation (RAG)\ntechnologies in Intelligent Transportation Systems (ITS), paving the way for\ninnovative solutions to address critical challenges in urban mobility. We begin\nby providing a comprehensive overview of the current state-of-the-art in\nmobility data, ITS, and Connected Vehicles (CV) applications. Building on this\nreview, we discuss the rationale behind RAG and examine the opportunities for\nintegrating these Generative AI (GenAI) technologies into the smart mobility\nsector. We propose a conceptual framework aimed at developing multi-agent\nsystems capable of intelligently and conversationally delivering smart mobility\nservices to urban commuters, transportation operators, and decision-makers. Our\napproach seeks to foster an autonomous and intelligent approach that (a)\npromotes science-based advisory to reduce traffic congestion, accidents, and\ncarbon emissions at multiple scales, (b) facilitates public education and\nengagement in participatory mobility management, and (c) automates specialized\ntransportation management tasks and the development of critical ITS platforms,\nsuch as data analytics and interpretation, knowledge representation, and\ntraffic simulations. By integrating LLM and RAG, our approach seeks to overcome\nthe limitations of traditional rule-based multi-agent systems, which rely on\nfixed knowledge bases and limited reasoning capabilities. This integration\npaves the way for a more scalable, intuitive, and automated multi-agent\nparadigm, driving advancements in ITS and urban mobility.",
      "tldr_zh": "这篇论文探讨了将大型语言模型(LLMs)和检索增强生成(RAG)技术整合到智能交通系统(ITS)中的机会和挑战，利用生成式AI(GenAI)提升城市移动效率。作者首先回顾了ITS、移动数据和连接车辆(CV)的现状，然后提出一个概念性多智能体框架，支持智能对话式服务，帮助减少交通拥堵、事故和碳排放，并促进公众参与和自动化任务如数据分析与交通模拟。通过整合LLMs和RAG，该框架克服了传统规则-based多智能体系统的局限性，实现更可扩展和直观的智能交通解决方案。",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.00494v2",
      "published_date": "2024-08-31 16:14:42 UTC",
      "updated_date": "2024-09-04 18:00:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T20:17:58.202830"
    },
    {
      "arxiv_id": "2409.00489v1",
      "title": "Geospatial foundation models for image analysis: evaluating and enhancing NASA-IBM Prithvi's domain adaptability",
      "title_zh": "翻译失败",
      "authors": [
        "Chia-Yu Hsu",
        "Wenwen Li",
        "Sizhe Wang"
      ],
      "abstract": "Research on geospatial foundation models (GFMs) has become a trending topic\nin geospatial artificial intelligence (AI) research due to their potential for\nachieving high generalizability and domain adaptability, reducing model\ntraining costs for individual researchers. Unlike large language models, such\nas ChatGPT, constructing visual foundation models for image analysis,\nparticularly in remote sensing, encountered significant challenges such as\nformulating diverse vision tasks into a general problem framework. This paper\nevaluates the recently released NASA-IBM GFM Prithvi for its predictive\nperformance on high-level image analysis tasks across multiple benchmark\ndatasets. Prithvi was selected because it is one of the first open-source GFMs\ntrained on time-series of high-resolution remote sensing imagery. A series of\nexperiments were designed to assess Prithvi's performance as compared to other\npre-trained task-specific AI models in geospatial image analysis. New\nstrategies, including band adaptation, multi-scale feature generation, and\nfine-tuning techniques, are introduced and integrated into an image analysis\npipeline to enhance Prithvi's domain adaptation capability and improve model\nperformance. In-depth analyses reveal Prithvi's strengths and weaknesses,\noffering insights for both improving Prithvi and developing future visual\nfoundation models for geospatial tasks.",
      "tldr_zh": "本研究评估了NASA-IBM Prithvi地理空间基础模型（GFMs）在高水平遥感图像分析任务上的性能，通过实验与任务特定预训练模型进行比较。研究者引入了新策略，包括带适应、多尺度特征生成和微调技术，并将其整合到一个图像分析管道中，以提升Prithvi的领域适应能力和整体表现。结果显示，这些改进显著增强了模型的适用性，并提供了深入见解，帮助优化Prithvi并指导未来GFMs的发展。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.00489v1",
      "published_date": "2024-08-31 15:51:23 UTC",
      "updated_date": "2024-08-31 15:51:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T20:18:07.531809"
    },
    {
      "arxiv_id": "2409.00488v2",
      "title": "Rapid Gyroscope Calibration: A Deep Learning Approach",
      "title_zh": "快速陀螺仪校准：一种深度学习方法",
      "authors": [
        "Yair Stolero",
        "Itzik Klein"
      ],
      "abstract": "Low-cost gyroscope calibration is essential for ensuring the accuracy and\nreliability of gyroscope measurements. Stationary calibration estimates the\ndeterministic parts of measurement errors. To this end, a common practice is to\naverage the gyroscope readings during a predefined period and estimate the\ngyroscope bias. Calibration duration plays a crucial role in performance,\ntherefore, longer periods are preferred. However, some applications require\nquick startup times and calibration is therefore allowed only for a short time.\nIn this work, we focus on reducing low-cost gyroscope calibration time using\ndeep learning methods. We propose a deep-learning framework and explore the\npossibilities of using multiple real and virtual gyroscopes to improve the\ncalibration performance of single gyroscopes. To train and validate our\napproach, we recorded a dataset consisting of 169 hours of gyroscope readings,\nusing 24 gyroscopes of two different brands. We also created a virtual dataset\nconsisting of simulated gyroscope readings. The two datasets were used to\nevaluate our proposed approach. One of our key achievements in this work is\nreducing gyroscope calibration time by up to 89% using three low-cost\ngyroscopes.",
      "tldr_zh": "这篇论文提出了一种基于深度学习的方法，用于快速校准低成本陀螺仪，以解决传统静态校准所需长时间平均读数的限制。研究框架利用多个真实和虚拟陀螺仪（如使用24个不同品牌的陀螺仪录制的169小时数据集和模拟虚拟数据集）来提升单个陀螺仪的校准性能。结果显示，该方法可将校准时间减少高达89%，为需要快速启动的应用提供了更高效的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "10 Pages, 14 Figures",
      "pdf_url": "http://arxiv.org/pdf/2409.00488v2",
      "published_date": "2024-08-31 15:47:31 UTC",
      "updated_date": "2024-10-02 12:55:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T20:18:20.813985"
    },
    {
      "arxiv_id": "2409.12964v3",
      "title": "OpenRANet: Neuralized Spectrum Access by Joint Subcarrier and Power Allocation with Optimization-based Deep Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Siya Chen",
        "Chee Wei Tan",
        "Xiangping Zhai",
        "H. Vincent Poor"
      ],
      "abstract": "The next-generation radio access network (RAN), known as Open RAN, is poised\nto feature an AI-native interface for wireless cellular networks, including\nemerging satellite-terrestrial systems, making deep learning integral to its\noperation. In this paper, we address the nonconvex optimization challenge of\njoint subcarrier and power allocation in Open RAN, with the objective of\nminimizing the total power consumption while ensuring users meet their\ntransmission data rate requirements. We propose OpenRANet, an\noptimization-based deep learning model that integrates machine-learning\ntechniques with iterative optimization algorithms. We start by transforming the\noriginal nonconvex problem into convex subproblems through decoupling, variable\ntransformation, and relaxation techniques. These subproblems are then\nefficiently solved using iterative methods within the standard interference\nfunction framework, enabling the derivation of primal-dual solutions. These\nsolutions integrate seamlessly as a convex optimization layer within OpenRANet,\nenhancing constraint adherence, solution accuracy, and computational efficiency\nby combining machine learning with convex analysis, as shown in numerical\nexperiments. OpenRANet also serves as a foundation for designing\nresource-constrained AI-native wireless optimization strategies for broader\nscenarios like multi-cell systems, satellite-terrestrial networks, and future\nOpen RAN deployments with complex power consumption requirements.",
      "tldr_zh": "本研究针对 Open RAN 中的非凸优化问题，提出 OpenRANet 模型，该模型通过联合子载波和功率分配来最小化总功率消耗，同时确保用户传输数据率要求。OpenRANet 整合机器学习与迭代优化算法，先将原问题通过解耦、变量变换和松弛技术转化为凸子问题，然后在标准干扰函数框架下求解并整合为凸优化层，从而提升约束遵守、解决方案准确性和计算效率。实验结果显示，OpenRANet 在数值测试中表现出色，并为设计适用于多小区系统、卫星-地面网络等复杂场景的资源约束 AI-native 无线优化策略提供了基础。",
      "categories": [
        "cs.IT",
        "cs.AI",
        "math.IT"
      ],
      "primary_category": "cs.IT",
      "comment": "This paper has been accepted by the IEEE Transactions on Green\n  Communications and Networking",
      "pdf_url": "http://arxiv.org/pdf/2409.12964v3",
      "published_date": "2024-08-31 13:10:48 UTC",
      "updated_date": "2025-02-11 02:06:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T20:18:33.783640"
    },
    {
      "arxiv_id": "2409.00448v1",
      "title": "PSLF: A PID Controller-incorporated Second-order Latent Factor Analysis Model for Recommender System",
      "title_zh": "翻译失败",
      "authors": [
        "Jialiang Wang",
        "Yan Xia",
        "Ye Yuan"
      ],
      "abstract": "A second-order-based latent factor (SLF) analysis model demonstrates superior\nperformance in graph representation learning, particularly for high-dimensional\nand incomplete (HDI) interaction data, by incorporating the curvature\ninformation of the loss landscape. However, its objective function is commonly\nbi-linear and non-convex, causing the SLF model to suffer from a low\nconvergence rate. To address this issue, this paper proposes a PID\ncontroller-incorporated SLF (PSLF) model, leveraging two key strategies: a)\nrefining learning error estimation by incorporating the PID controller\nprinciples, and b) acquiring second-order information insights through\nHessian-vector products. Experimental results on multiple HDI datasets indicate\nthat the proposed PSLF model outperforms four state-of-the-art latent factor\nmodels based on advanced optimizers regarding convergence rates and\ngeneralization performance.",
      "tldr_zh": "本研究针对第二阶潜在因子 (SLF) 分析模型在处理高维和不完整 (HDI) 交互数据时的低收敛率问题，提出了一种整合 PID controller 的 PSLF 模型。PSLF 通过两个关键策略提升性能：一是利用 PID controller 原理优化学习误差估计，二是通过 Hessian-vector products 获取第二阶信息，从而改善模型的收敛速度和泛化能力。在多个 HDI 数据集上的实验结果显示，PSLF 模型在收敛率和整体性能上优于四种最先进基于高级优化器的潜在因子模型。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.00448v1",
      "published_date": "2024-08-31 13:01:58 UTC",
      "updated_date": "2024-08-31 13:01:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T20:18:43.961754"
    },
    {
      "arxiv_id": "2409.00447v1",
      "title": "The MERIT Dataset: Modelling and Efficiently Rendering Interpretable Transcripts",
      "title_zh": "翻译失败",
      "authors": [
        "I. de Rodrigo",
        "A. Sanchez-Cuadrado",
        "J. Boal",
        "A. J. Lopez-Lopez"
      ],
      "abstract": "This paper introduces the MERIT Dataset, a multimodal (text + image + layout)\nfully labeled dataset within the context of school reports. Comprising over 400\nlabels and 33k samples, the MERIT Dataset is a valuable resource for training\nmodels in demanding Visually-rich Document Understanding (VrDU) tasks. By its\nnature (student grade reports), the MERIT Dataset can potentially include\nbiases in a controlled way, making it a valuable tool to benchmark biases\ninduced in Language Models (LLMs). The paper outlines the dataset's generation\npipeline and highlights its main features in the textual, visual, layout, and\nbias domains. To demonstrate the dataset's utility, we present a benchmark with\ntoken classification models, showing that the dataset poses a significant\nchallenge even for SOTA models and that these would greatly benefit from\nincluding samples from the MERIT Dataset in their pretraining phase.",
      "tldr_zh": "本论文介绍了MERIT Dataset，这是一个多模态（文本 + 图像 + 布局）数据集，专注于学校报告领域，包含超过400个标签和33k个样本，用于训练Visually-rich Document Understanding (VrDU)任务。该数据集通过受控引入偏差（如学生成绩报告中的潜在偏见），成为基准测试Language Models (LLMs)偏差的有效工具，并详细阐述了其生成管道以及在文本、视觉、布局和偏差方面的关键特征。为验证其效用，论文进行了标记分类模型的基准测试，结果显示MERIT Dataset对SOTA模型构成重大挑战，并证明在模型预训练阶段纳入这些样本可显著提升性能。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.00447v1",
      "published_date": "2024-08-31 12:56:38 UTC",
      "updated_date": "2024-08-31 12:56:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T20:18:59.760206"
    },
    {
      "arxiv_id": "2409.02119v1",
      "title": "CoRA: Optimizing Low-Rank Adaptation with Common Subspace of Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaojun Xiao",
        "Sen Shen",
        "Qiming Bao",
        "Hongfei Rong",
        "Kairui Liu",
        "Zhongsheng Wang",
        "Jiamou Liu"
      ],
      "abstract": "In fine-tuning large language models (LLMs), conserving computational\nresources while maintaining effectiveness and improving outcomes within the\nsame computational constraints is crucial. The Low-Rank Adaptation (LoRA)\nstrategy balances efficiency and performance in fine-tuning large models by\nreducing the number of trainable parameters and computational costs. However,\ncurrent advancements in LoRA might be focused on its fine-tuning methodologies,\nwith not as much exploration as might be expected into further compression of\nLoRA. Since most of LoRA's parameters might still be superfluous, this may lead\nto unnecessary wastage of computational resources. In this paper, we propose\n\\textbf{CoRA}: leveraging shared knowledge to optimize LoRA training by\nsubstituting its matrix $B$ with a common subspace from large models. Our\ntwo-fold method includes (1) Freezing the substitute matrix $B$ to halve\nparameters while training matrix $A$ for specific tasks and (2) Using the\nsubstitute matrix $B$ as an enhanced initial state for the original matrix $B$,\nachieving improved results with the same parameters. Our experiments show that\nthe first approach achieves the same efficacy as the original LoRA fine-tuning\nwhile being more efficient than halving parameters. At the same time, the\nsecond approach has some improvements compared to LoRA's original fine-tuning\nperformance. They generally attest to the effectiveness of our work.",
      "tldr_zh": "本研究提出 CoRA 方法，旨在优化 Low-Rank Adaptation (LoRA) 在微调 Large Language Models (LLMs) 中的效率，通过利用大模型的共同子空间来减少冗余参数和计算资源浪费。CoRA 包括两种策略：(1) 冻结替代矩阵 B 只训练矩阵 A，从而将参数减半，同时保持与原 LoRA 相当的性能；(2) 将替代矩阵 B 作为原矩阵 B 的增强初始状态，提升微调效果。实验结果显示，第一种策略在参数减半后仍实现高效微调，第二种策略则在相同参数下比原 LoRA 性能有所改进，证明了 CoRA 的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.02119v1",
      "published_date": "2024-08-31 12:48:27 UTC",
      "updated_date": "2024-08-31 12:48:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T20:19:08.547886"
    },
    {
      "arxiv_id": "2409.00438v1",
      "title": "Breaking Down Financial News Impact: A Novel AI Approach with Geometric Hypergraphs",
      "title_zh": "翻译失败",
      "authors": [
        "Anoushka Harit",
        "Zhongtian Sun",
        "Jongmin Yu",
        "Noura Al Moubayed"
      ],
      "abstract": "In the fast-paced and volatile financial markets, accurately predicting stock\nmovements based on financial news is critical for investors and analysts.\nTraditional models often struggle to capture the intricate and dynamic\nrelationships between news events and market reactions, limiting their ability\nto provide actionable insights. This paper introduces a novel approach\nleveraging Explainable Artificial Intelligence (XAI) through the development of\na Geometric Hypergraph Attention Network (GHAN) to analyze the impact of\nfinancial news on market behaviours. Geometric hypergraphs extend traditional\ngraph structures by allowing edges to connect multiple nodes, effectively\nmodelling high-order relationships and interactions among financial entities\nand news events. This unique capability enables the capture of complex\ndependencies, such as the simultaneous impact of a single news event on\nmultiple stocks or sectors, which traditional models frequently overlook.\n  By incorporating attention mechanisms within hypergraphs, GHAN enhances the\nmodel's ability to focus on the most relevant information, ensuring more\naccurate predictions and better interpretability. Additionally, we employ\nBERT-based embeddings to capture the semantic richness of financial news texts,\nproviding a nuanced understanding of the content. Using a comprehensive\nfinancial news dataset, our GHAN model addresses key challenges in financial\nnews impact analysis, including the complexity of high-order interactions, the\nnecessity for model interpretability, and the dynamic nature of financial\nmarkets. Integrating attention mechanisms and SHAP values within GHAN ensures\ntransparency, highlighting the most influential factors driving market\npredictions.\n  Empirical validation demonstrates the superior effectiveness of our approach\nover traditional sentiment analysis and time-series models.",
      "tldr_zh": "这篇论文提出了一种新型AI方法，利用Geometric Hypergraph Attention Network (GHAN)来分析金融新闻对股票市场的影响，旨在捕捉传统模型忽略的高阶关系，如单一新闻事件对多个股票或行业的联动效应。GHAN结合注意力机制和BERT-based embeddings处理新闻文本语义，并通过SHAP values增强模型的可解释性（Explainable Artificial Intelligence, XAI），从而提高预测准确性和透明度。实验验证显示，该方法在金融新闻数据集上显著优于传统情感分析和时间序列模型，为投资者提供更可靠的市场洞见。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "16 pages, conference",
      "pdf_url": "http://arxiv.org/pdf/2409.00438v1",
      "published_date": "2024-08-31 12:18:45 UTC",
      "updated_date": "2024-08-31 12:18:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T20:19:25.281869"
    },
    {
      "arxiv_id": "2409.00418v1",
      "title": "Robust off-policy Reinforcement Learning via Soft Constrained Adversary",
      "title_zh": "翻译失败",
      "authors": [
        "Kosuke Nakanishi",
        "Akihiro Kubo",
        "Yuji Yasui",
        "Shin Ishii"
      ],
      "abstract": "Recently, robust reinforcement learning (RL) methods against input\nobservation have garnered significant attention and undergone rapid evolution\ndue to RL's potential vulnerability. Although these advanced methods have\nachieved reasonable success, there have been two limitations when considering\nadversary in terms of long-term horizons. First, the mutual dependency between\nthe policy and its corresponding optimal adversary limits the development of\noff-policy RL algorithms; although obtaining optimal adversary should depend on\nthe current policy, this has restricted applications to off-policy RL. Second,\nthese methods generally assume perturbations based only on the $L_p$-norm, even\nwhen prior knowledge of the perturbation distribution in the environment is\navailable. We here introduce another perspective on adversarial RL: an\nf-divergence constrained problem with the prior knowledge distribution. From\nthis, we derive two typical attacks and their corresponding robust learning\nframeworks. The evaluation of robustness is conducted and the results\ndemonstrate that our proposed methods achieve excellent performance in\nsample-efficient off-policy RL.",
      "tldr_zh": "该论文探讨了强化学习（RL）的鲁棒性问题，针对现有方法的局限性（如策略与最优攻击者间的相互依赖，以及仅依赖$L_p$-norm的扰动假设），提出了一种基于f-divergence约束的视角。作者推导出两种典型攻击形式，并开发了相应的软约束攻击者框架，支持off-policy RL算法的样本高效训练。实验结果表明，该方法在off-policy RL中实现了优秀的鲁棒性能和效率提升。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "33 pages, 12 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2409.00418v1",
      "published_date": "2024-08-31 11:13:33 UTC",
      "updated_date": "2024-08-31 11:13:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T20:19:35.853271"
    },
    {
      "arxiv_id": "2409.09054v1",
      "title": "Evaluating the Performance of Large Language Models in Competitive Programming: A Multi-Year, Multi-Grade Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Adrian Marius Dumitran",
        "Adrian Catalin Badea",
        "Stefan-Gabriel Muscalu"
      ],
      "abstract": "This study explores the performance of large language models (LLMs) in\nsolving competitive programming problems from the Romanian Informatics Olympiad\nat the county level. Romania, a leading nation in computer science\ncompetitions, provides an ideal environment for evaluating LLM capabilities due\nto its rich history and stringent competition standards. We collected and\nanalyzed a dataset comprising 304 challenges from 2002 to 2023, focusing on\nsolutions written by LLMs in C++ and Python for these problems. Our primary\ngoal is to understand why LLMs perform well or poorly on different tasks. We\nevaluated various models, including closed-source models like GPT-4 and\nopen-weight models such as CodeLlama and RoMistral, using a standardized\nprocess involving multiple attempts and feedback rounds. The analysis revealed\nsignificant variations in LLM performance across different grades and problem\ntypes. Notably, GPT-4 showed strong performance, indicating its potential use\nas an educational tool for middle school students. We also observed differences\nin code quality and style across various LLMs",
      "tldr_zh": "这篇论文评估了大型语言模型 (LLMs) 在罗马尼亚信息学奥林匹克县级竞赛中的表现，通过分析2002-2023年的304个编程问题，使用C++和Python生成解决方案。研究采用标准化评估过程，包括多轮尝试和反馈，对模型如GPT-4、CodeLlama和RoMistral进行比较，以探究LLMs在不同年级和问题类型上的表现原因。结果显示，LLMs的表现存在显著差异，GPT-4表现出色，可作为中学教育工具，同时观察到不同模型在代码质量和风格方面有明显区别。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.PL"
      ],
      "primary_category": "cs.SE",
      "comment": "7 pages, Inista 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.09054v1",
      "published_date": "2024-08-31 10:39:54 UTC",
      "updated_date": "2024-08-31 10:39:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T20:19:51.787164"
    },
    {
      "arxiv_id": "2409.10536v1",
      "title": "The potential functions of an international institution for AI safety. Insights from adjacent policy areas and recent trends",
      "title_zh": "翻译失败",
      "authors": [
        "A. Leone De Castris",
        "C. Thomas"
      ],
      "abstract": "Governments, industry, and other actors involved in governing AI technologies\naround the world agree that, while AI offers tremendous promise to benefit the\nworld, appropriate guardrails are required to mitigate risks. Global\ninstitutions, including the OECD, the G7, the G20, UNESCO, and the Council of\nEurope, have already started developing frameworks for ethical and responsible\nAI governance. While these are important initial steps, they alone fall short\nof addressing the need for institutionalised international processes to\nidentify and assess potentially harmful AI capabilities. Contributing to the\nrelevant conversation on how to address this gap, this chapter reflects on what\nfunctions an international AI safety institute could perform. Based on the\nanalysis of both existing international governance models addressing safety\nconsiderations in adjacent policy areas and the newly established national AI\nsafety institutes in the UK and US, the chapter identifies a list of concrete\nfunctions that could be performed at the international level. While creating a\nnew international body is not the only way forward, understanding the structure\nof these bodies from a modular perspective can help us to identify the tools at\nour disposal. These, we suggest, can be categorised under three functional\ndomains: a) technical research and cooperation, b) safeguards and evaluations,\nc) policymaking and governance support.",
      "tldr_zh": "这篇论文探讨了建立国际 AI 安全机构的潜在功能，以应对 AI 风险治理的不足，通过分析相邻政策领域的现有国际治理模型（如 OECD 和 G7）以及英国和美国的国家 AI 安全机构，识别出可行的具体功能。这些功能可归类为三个领域：a) 技术研究与合作，b) 保障与评估，c) 政策制定与治理支持。该研究强调，虽然创建新机构不是唯一方式，但从模块化视角理解这些工具有助于加强全球 AI 安全治理。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10536v1",
      "published_date": "2024-08-31 10:04:53 UTC",
      "updated_date": "2024-08-31 10:04:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T20:20:03.061889"
    },
    {
      "arxiv_id": "2409.10535v1",
      "title": "Learning Co-Speech Gesture Representations in Dialogue through Contrastive Learning: An Intrinsic Evaluation",
      "title_zh": "通过对比学习在对话中学习共语手势表示：一种内在评估",
      "authors": [
        "Esam Ghaleb",
        "Bulat Khaertdinov",
        "Wim Pouw",
        "Marlou Rasenberg",
        "Judith Holler",
        "Aslı Özyürek",
        "Raquel Fernández"
      ],
      "abstract": "In face-to-face dialogues, the form-meaning relationship of co-speech\ngestures varies depending on contextual factors such as what the gestures refer\nto and the individual characteristics of speakers. These factors make co-speech\ngesture representation learning challenging. How can we learn meaningful\ngestures representations considering gestures' variability and relationship\nwith speech? This paper tackles this challenge by employing self-supervised\ncontrastive learning techniques to learn gesture representations from skeletal\nand speech information. We propose an approach that includes both unimodal and\nmultimodal pre-training to ground gesture representations in co-occurring\nspeech. For training, we utilize a face-to-face dialogue dataset rich with\nrepresentational iconic gestures. We conduct thorough intrinsic evaluations of\nthe learned representations through comparison with human-annotated pairwise\ngesture similarity. Moreover, we perform a diagnostic probing analysis to\nassess the possibility of recovering interpretable gesture features from the\nlearned representations. Our results show a significant positive correlation\nwith human-annotated gesture similarity and reveal that the similarity between\nthe learned representations is consistent with well-motivated patterns related\nto the dynamics of dialogue interaction. Moreover, our findings demonstrate\nthat several features concerning the form of gestures can be recovered from the\nlatent representations. Overall, this study shows that multimodal contrastive\nlearning is a promising approach for learning gesture representations, which\nopens the door to using such representations in larger-scale gesture analysis\nstudies.",
      "tldr_zh": "这篇论文探讨了在面对面对话中学习协同言语手势表示的挑战，提出了一种基于自监督对比学习(self-supervised contrastive learning)的方法，利用骨骼和语音信息进行单模态和多模态预训练，以关联手势与同时发生的言语。研究使用一个包含代表性标志性手势的对话数据集进行训练，并通过与人类标注的手势相似性比较进行内在评估。结果显示，学习表示与人类感知有显著正相关，且能从潜在表示中恢复可解释的手势形式特征。总体而言，该方法证明多模态对比学习是学习手势表示的有前景途径，为更大规模的手势分析研究提供了基础。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.SD",
        "eess.AS",
        "I.4"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10535v1",
      "published_date": "2024-08-31 08:53:18 UTC",
      "updated_date": "2024-08-31 08:53:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T20:20:15.327590"
    },
    {
      "arxiv_id": "2409.00391v1",
      "title": "Density Adaptive Attention-based Speech Network: Enhancing Feature Understanding for Mental Health Disorders",
      "title_zh": "基于密度自适应注意力的语音网络：增强心理健康障碍的特征理解",
      "authors": [
        "Georgios Ioannides",
        "Adrian Kieback",
        "Aman Chadha",
        "Aaron Elkins"
      ],
      "abstract": "Speech-based depression detection poses significant challenges for automated\ndetection due to its unique manifestation across individuals and data scarcity.\nAddressing these challenges, we introduce DAAMAudioCNNLSTM and\nDAAMAudioTransformer, two parameter efficient and explainable models for audio\nfeature extraction and depression detection. DAAMAudioCNNLSTM features a novel\nCNN-LSTM framework with multi-head Density Adaptive Attention Mechanism (DAAM),\nfocusing dynamically on informative speech segments. DAAMAudioTransformer,\nleveraging a transformer encoder in place of the CNN-LSTM architecture,\nincorporates the same DAAM module for enhanced attention and interpretability.\nThese approaches not only enhance detection robustness and interpretability but\nalso achieve state-of-the-art performance: DAAMAudioCNNLSTM with an F1 macro\nscore of 0.702 and DAAMAudioTransformer with an F1 macro score of 0.72 on the\nDAIC-WOZ dataset, without reliance on supplementary information such as vowel\npositions and speaker information during training/validation as in previous\napproaches. Both models' significant explainability and efficiency in\nleveraging speech signals for depression detection represent a leap towards\nmore reliable, clinically useful diagnostic tools, promising advancements in\nspeech and mental health care. To foster further research in this domain, we\nmake our code publicly available.",
      "tldr_zh": "本研究针对语音抑郁检测的个体差异和数据稀缺挑战，提出两种参数高效且可解释的模型：DAAMAudioCNNLSTM 和 DAAMAudioTransformer。这两个模型均整合了多头Density Adaptive Attention Mechanism (DAAM)，分别基于CNN-LSTM框架和Transformer编码器，动态关注信息丰富的语音段，从而提升特征提取和检测的鲁棒性。在DAIC-WOZ数据集上，DAAMAudioCNNLSTM 达到 F1 macro score 0.702，DAAMAudioTransformer 达到 0.72 的最先进性能，且不依赖额外信息如元音位置或说话者数据。该方法显著提高了模型的可解释性和临床实用性，并公开代码以推动语音心理健康领域的进一步研究。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.00391v1",
      "published_date": "2024-08-31 08:50:28 UTC",
      "updated_date": "2024-08-31 08:50:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T20:20:27.160860"
    },
    {
      "arxiv_id": "2409.03788v1",
      "title": "HSF: Defending against Jailbreak Attacks with Hidden State Filtering",
      "title_zh": "HSF：通过隐藏状态过滤防御越狱攻击",
      "authors": [
        "Cheng Qian",
        "Hainan Zhang",
        "Lei Sha",
        "Zhiming Zheng"
      ],
      "abstract": "With the growing deployment of LLMs in daily applications like chatbots and\ncontent generation, efforts to ensure outputs align with human values and avoid\nharmful content have intensified. However, increasingly sophisticated jailbreak\nattacks threaten this alignment, aiming to induce unsafe outputs. Current\ndefense efforts either focus on prompt rewriting or detection, which are\nlimited in effectiveness due to the various design of jailbreak prompts, or on\noutput control and detection, which are computationally expensive as they\nrequire LLM inference. Therefore, designing a pre-inference defense method that\nresists diverse jailbreak prompts is crucial for preventing LLM jailbreak\nattacks. We observe that jailbreak attacks, safe queries, and harmful queries\nexhibit different clustering patterns within the LLM's hidden state\nrepresentation space. This suggests that by leveraging the LLM's hidden state\nrepresentational capabilities, we can analyze the LLM's forthcoming behavior\nand proactively intervene for defense. In this paper, we propose a jailbreak\nattack defense strategy based on a Hidden State Filter (HSF), a lossless\narchitectural defense mechanism that enables the model to preemptively identify\nand reject adversarial inputs before the inference process begins. We activate\nits defensive potential through an additional plugin module, effectively\nframing the defense task as a classification problem. Experimental results on\ntwo benchmark datasets, utilizing three different LLMs, show that HSF\nsignificantly enhances resilience against six cutting-edge jailbreak attacks.\nIt significantly reduces the success rate of jailbreak attacks while minimally\nimpacting responses to benign user queries, with negligible inference overhead,\nand outperforming defense baselines.Our code and data are available at\nhttps://anonymous.4open.science/r/Hidden-State-Filtering-8652/",
      "tldr_zh": "该研究针对大型语言模型（LLMs）面临的jailbreak attacks问题，提出了一种基于Hidden State Filter (HSF)的防御机制，以在推理前识别并拒绝潜在的攻击输入。HSF利用LLMs隐藏状态（hidden state）空间中不同查询类型的聚类模式，通过一个插件模块将防御任务转化为分类问题，从而实现高效的预处理干预。实验结果显示，在两个基准数据集上使用三种LLMs，HSF显著降低了六种先进jailbreak attacks的成功率，同时对安全查询（benign user queries）的响应影响最小，且推理开销可忽略不计，优于现有防御基线。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "13 pages",
      "pdf_url": "http://arxiv.org/pdf/2409.03788v1",
      "published_date": "2024-08-31 06:50:07 UTC",
      "updated_date": "2024-08-31 06:50:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T20:20:37.168952"
    },
    {
      "arxiv_id": "2409.00359v1",
      "title": "Predicting Femicide in Veracruz: A Fuzzy Logic Approach with the Expanded MFM-FEM-VER-CP-2024 Model",
      "title_zh": "翻译失败",
      "authors": [
        "Carlos Medel-Ramírez",
        "Hilario Medel-López"
      ],
      "abstract": "The article focuses on the urgent issue of femicide in Veracruz, Mexico, and\nthe development of the MFM_FEM_VER_CP_2024 model, a mathematical framework\ndesigned to predict femicide risk using fuzzy logic. This model addresses the\ncomplexity and uncertainty inherent in gender based violence by formalizing\nrisk factors such as coercive control, dehumanization, and the cycle of\nviolence. These factors are mathematically modeled through membership functions\nthat assess the degree of risk associated with various conditions, including\npersonal relationships and specific acts of violence. The study enhances the\noriginal model by incorporating new rules and refining existing membership\nfunctions, which significantly improve the model predictive accuracy.",
      "tldr_zh": "本研究针对Veracruz, Mexico的女性谋杀(femicide)问题，开发了扩展版MFM-FEM-VER-CP-2024模型，使用fuzzy logic模糊逻辑来预测风险。该模型通过成员函数(membership functions)形式化风险因素，如coercive control强制控制、非人化(dehumanization)和暴力循环(cycle of violence)，以评估个人关系和具体暴力行为下的不确定性。相比原模型，该版本通过添加新规则和优化现有函数，显著提升了预测准确性，为应对性别暴力提供更有效的数学框架。",
      "categories": [
        "cs.AI",
        "03E72, 91D10, 62P25, 91B76",
        "G.1; G.3; I.2; I.6"
      ],
      "primary_category": "cs.AI",
      "comment": "24 pages, 2 tables, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.00359v1",
      "published_date": "2024-08-31 06:00:49 UTC",
      "updated_date": "2024-08-31 06:00:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T20:20:49.365775"
    },
    {
      "arxiv_id": "2409.00358v2",
      "title": "Predicting the Target Word of Game-playing Conversations using a Low-Rank Dialect Adapter for Decoder Models",
      "title_zh": "翻译失败",
      "authors": [
        "Dipankar Srirag",
        "Aditya Joshi",
        "Jacob Eisenstein"
      ],
      "abstract": "Dialect adapters that improve the performance of LLMs for NLU tasks on\ncertain sociolects/dialects/national varieties ('dialects' for the sake of\nbrevity) have been reported for encoder models. In this paper, we extend the\nidea of dialect adapters to decoder models in our architecture called LoRDD.\nUsing MD-3, a publicly available dataset of word game-playing conversations\nbetween dialectal speakers, our task is Target Word Prediction (TWP) from a\nmasked conversation. LoRDD combines task adapters and dialect adapters where\nthe latter employ contrastive learning on pseudo-parallel conversations from\nMD-3. Our experiments on Indian English and Nigerian English conversations with\ntwo models (Mistral and Gemma) demonstrate that LoRDD outperforms four\nbaselines on TWP. Additionally, it significantly reduces the performance gap\nwith American English, narrowing it to 12% and 5.8% for word similarity, and\n25% and 4.5% for accuracy, respectively. The focused contribution of LoRDD is\nin its promise for dialect adaptation of decoder models using TWP, a simplified\nversion of the commonly used next-word prediction task.",
      "tldr_zh": "本研究提出了一种名为 LoRDD 的架构，使用低秩 dialect adapters 扩展到 decoder 模型，以提升在方言对话中的目标词预测 (TWP) 任务性能。LoRDD 结合 task adapters 和 dialect adapters，通过对比学习处理 MD-3 数据集的伪平行对话，针对 Indian English 和 Nigerian English 等方言进行优化。实验结果显示，LoRDD 优于四个基线模型（如 Mistral 和 Gemma），显著缩小了与 American English 的差距（word similarity 差距降至 12% 和 5.8%，准确率差距降至 25% 和 4.5%），为 decoder 模型的方言适应提供了高效的简化方法。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to NAACL 2025",
      "pdf_url": "http://arxiv.org/pdf/2409.00358v2",
      "published_date": "2024-08-31 05:53:39 UTC",
      "updated_date": "2025-01-31 07:32:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T20:21:03.400844"
    },
    {
      "arxiv_id": "2409.00356v1",
      "title": "Contrastive Augmentation: An Unsupervised Learning Approach for Keyword Spotting in Speech Technology",
      "title_zh": "对比增强：一种用于语音技术中关键词检测的无监督学习方法",
      "authors": [
        "Weinan Dai",
        "Yifeng Jiang",
        "Yuanjing Liu",
        "Jinkun Chen",
        "Xin Sun",
        "Jinglei Tao"
      ],
      "abstract": "This paper addresses the persistent challenge in Keyword Spotting (KWS), a\nfundamental component in speech technology, regarding the acquisition of\nsubstantial labeled data for training. Given the difficulty in obtaining large\nquantities of positive samples and the laborious process of collecting new\ntarget samples when the keyword changes, we introduce a novel approach\ncombining unsupervised contrastive learning and a unique augmentation-based\ntechnique. Our method allows the neural network to train on unlabeled data\nsets, potentially improving performance in downstream tasks with limited\nlabeled data sets. We also propose that similar high-level feature\nrepresentations should be employed for speech utterances with the same keyword\ndespite variations in speed or volume. To achieve this, we present a speech\naugmentation-based unsupervised learning method that utilizes the similarity\nbetween the bottleneck layer feature and the audio reconstructing information\nfor auxiliary training. Furthermore, we propose a compressed convolutional\narchitecture to address potential redundancy and non-informative information in\nKWS tasks, enabling the model to simultaneously learn local features and focus\non long-term information. This method achieves strong performance on the Google\nSpeech Commands V2 Dataset. Inspired by recent advancements in sign spotting\nand spoken term detection, our method underlines the potential of our\ncontrastive learning approach in KWS and the advantages of Query-by-Example\nSpoken Term Detection strategies. The presented CAB-KWS provide new\nperspectives in the field of KWS, demonstrating effective ways to reduce data\ncollection efforts and increase the system's robustness.",
      "tldr_zh": "这篇论文针对 Keyword Spotting (KWS) 在语音技术中的核心挑战——获取大量标注数据困难，提出了一种无监督对比学习方法Contrastive Augmentation。该方法结合语音增强技术，利用瓶颈层特征与音频重建信息的相似性进行辅助训练，确保相同关键词的语音特征表示在速度或音量变化时保持一致，同时引入压缩卷积架构来处理冗余信息并优化局部和长期特征学习。在Google Speech Commands V2数据集上，该方法表现出色，提高了下游任务性能，并减少了数据收集需求，强调了Query-by-Example Spoken Term Detection策略的潜力。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "This paper has been accepted by the ICPR2024",
      "pdf_url": "http://arxiv.org/pdf/2409.00356v1",
      "published_date": "2024-08-31 05:40:37 UTC",
      "updated_date": "2024-08-31 05:40:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T20:21:14.722890"
    },
    {
      "arxiv_id": "2409.02118v1",
      "title": "TSO: Self-Training with Scaled Preference Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Kaihui Chen",
        "Hao Yi",
        "Qingyang Li",
        "Tianyu Qi",
        "Yulan Hu",
        "Fuzheng Zhang",
        "Yong Liu"
      ],
      "abstract": "Enhancing the conformity of large language models (LLMs) to human preferences\nremains an ongoing research challenge. Recently, offline approaches such as\nDirect Preference Optimization (DPO) have gained prominence as attractive\noptions due to offering effective improvement in simple, efficient, and stable\nwithout interactions with reward models. However, these offline preference\noptimization methods highly rely on the quality of pairwise preference samples.\nMeanwhile, numerous iterative methods require additional training of reward\nmodels to select positive and negative samples from the model's own generated\nresponses for preference learning. Furthermore, as LLMs' capabilities advance,\nit is quite challenging to continuously construct high-quality positive and\nnegative preference instances from the model's outputs due to the lack of\ndiversity. To tackle these challenges, we propose TSO, or Self-Training with\nScaled Preference Optimization, a framework for preference optimization that\nconducts self-training preference learning without training an additional\nreward model. TSO enhances the diversity of responses by constructing a model\nmatrix and incorporating human preference responses. Furthermore, TSO\nintroduces corrections for model preference errors through human and AI\nfeedback. Finally, TSO adopts iterative and dual clip reward strategies to\nupdate the reference model and its responses, adaptively adjusting preference\ndata and balancing the optimization process. Experimental results demonstrate\nthat TSO outperforms existing mainstream methods on various alignment\nevaluation benchmarks, providing practical insight into preference data\nconstruction and model training strategies in the alignment domain.",
      "tldr_zh": "该研究提出 TSO（Self-Training with Scaled Preference Optimization），一种无需额外训练奖励模型的自训练框架，用于提升大型语言模型 (LLMs) 对人类偏好的符合度，以解决现有方法如 Direct Preference Optimization (DPO) 对高质量偏好样本的依赖和响应多样性不足的问题。TSO 通过构建模型矩阵整合人类偏好响应来增强输出多样性，并利用人类和 AI 反馈纠正模型偏好错误，同时采用迭代和双剪辑奖励策略来适应性调整偏好数据并平衡优化过程。实验结果表明，TSO 在各种对齐评估基准上优于主流方法，提供偏好数据构建和模型训练的实用见解。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.02118v1",
      "published_date": "2024-08-31 05:37:01 UTC",
      "updated_date": "2024-08-31 05:37:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T20:21:27.272890"
    },
    {
      "arxiv_id": "2409.00338v1",
      "title": "GSpect: Spectral Filtering for Cross-Scale Graph Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaoyu Zhang",
        "Wenchuan Yang",
        "Jiawei Feng",
        "Bitao Dai",
        "Tianci Bu",
        "Xin Lu"
      ],
      "abstract": "Identifying structures in common forms the basis for networked systems design\nand optimization. However, real structures represented by graphs are often of\nvarying sizes, leading to the low accuracy of traditional graph classification\nmethods. These graphs are called cross-scale graphs. To overcome this\nlimitation, in this study, we propose GSpect, an advanced spectral graph\nfiltering model for cross-scale graph classification tasks. Compared with other\nmethods, we use graph wavelet neural networks for the convolution layer of the\nmodel, which aggregates multi-scale messages to generate graph representations.\nWe design a spectral-pooling layer which aggregates nodes to one node to reduce\nthe cross-scale graphs to the same size. We collect and construct the\ncross-scale benchmark data set, MSG (Multi Scale Graphs). Experiments reveal\nthat, on open data sets, GSpect improves the performance of classification\naccuracy by 1.62% on average, and for a maximum of 3.33% on PROTEINS. On MSG,\nGSpect improves the performance of classification accuracy by 15.55% on\naverage. GSpect fills the gap in cross-scale graph classification studies and\nhas potential to provide assistance in application research like diagnosis of\nbrain disease by predicting the brain network's label and developing new drugs\nwith molecular structures learned from their counterparts in other systems.",
      "tldr_zh": "本研究提出 GSpect，一种先进的谱图过滤模型，用于解决跨尺度图（cross-scale graphs）分类中传统方法准确率低的问题。GSpect 采用 graph wavelet neural networks 作为卷积层，以聚合多尺度信息，并设计 spectral-pooling layer 将节点聚合为一个节点，从而统一图的大小。实验结果显示，在公开数据集上，GSpect 的分类准确率平均提高 1.62%，最高在 PROTEINS 上提高 3.33%；在自建的 MSG（Multi Scale Graphs）数据集上，准确率平均提升 15.55%。该模型填补了跨尺度图分类研究的空白，并有望应用于脑病诊断和药物开发等领域。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.00338v1",
      "published_date": "2024-08-31 03:26:32 UTC",
      "updated_date": "2024-08-31 03:26:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T20:21:38.625832"
    },
    {
      "arxiv_id": "2409.00335v1",
      "title": "Evaluating the Effectiveness of Large Language Models in Representing and Understanding Movement Trajectories",
      "title_zh": "评估大语言模型在表示和理解运动轨迹的有效性",
      "authors": [
        "Yuhan Ji",
        "Song Gao"
      ],
      "abstract": "This research focuses on assessing the ability of AI foundation models in\nrepresenting the trajectories of movements. We utilize one of the large\nlanguage models (LLMs) (i.e., GPT-J) to encode the string format of\ntrajectories and then evaluate the effectiveness of the LLM-based\nrepresentation for trajectory data analysis. The experiments demonstrate that\nwhile the LLM-based embeddings can preserve certain trajectory distance metrics\n(i.e., the correlation coefficients exceed 0.74 between the Cosine distance\nderived from GPT-J embeddings and the Hausdorff and Dynamic Time Warping\ndistances on raw trajectories), challenges remain in restoring numeric values\nand retrieving spatial neighbors in movement trajectory analytics. In addition,\nthe LLMs can understand the spatiotemporal dependency contained in trajectories\nand have good accuracy in location prediction tasks. This research highlights\nthe need for improvement in terms of capturing the nuances and complexities of\nthe underlying geospatial data and integrating domain knowledge to support\nvarious GeoAI applications using LLMs.",
      "tldr_zh": "这篇论文评估了LLMs（如GPT-J）在表示和理解运动轨迹方面的有效性，通过将轨迹字符串编码为嵌入并进行分析。实验结果显示，LLM-based embeddings能够保留某些轨迹距离指标（如Cosine distance与Hausdorff和Dynamic Time Warping距离的相关系数超过0.74），并在位置预测任务中表现出高准确性，同时理解轨迹中的时空依赖性。然而，在恢复数值和检索空间邻居方面存在挑战，研究强调需要进一步改进以捕捉地理空间数据的细微差别，并整合领域知识来支持GeoAI应用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "I.2; E.2"
      ],
      "primary_category": "cs.CL",
      "comment": "7 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.00335v1",
      "published_date": "2024-08-31 02:57:25 UTC",
      "updated_date": "2024-08-31 02:57:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T20:21:53.109818"
    },
    {
      "arxiv_id": "2409.00331v1",
      "title": "WikiCausal: Corpus and Evaluation Framework for Causal Knowledge Graph Construction",
      "title_zh": "翻译失败",
      "authors": [
        "Oktie Hassanzadeh"
      ],
      "abstract": "Recently, there has been an increasing interest in the construction of\ngeneral-domain and domain-specific causal knowledge graphs. Such knowledge\ngraphs enable reasoning for causal analysis and event prediction, and so have a\nrange of applications across different domains. While great progress has been\nmade toward automated construction of causal knowledge graphs, the evaluation\nof such solutions has either focused on low-level tasks (e.g., cause-effect\nphrase extraction) or on ad hoc evaluation data and small manual evaluations.\nIn this paper, we present a corpus, task, and evaluation framework for causal\nknowledge graph construction. Our corpus consists of Wikipedia articles for a\ncollection of event-related concepts in Wikidata. The task is to extract causal\nrelations between event concepts from the corpus. The evaluation is performed\nin part using existing causal relations in Wikidata to measure recall, and in\npart using Large Language Models to avoid the need for manual or crowd-sourced\nevaluation. We evaluate a pipeline for causal knowledge graph construction that\nrelies on neural models for question answering and concept linking, and show\nhow the corpus and the evaluation framework allow us to effectively find the\nright model for each task. The corpus and the evaluation framework are publicly\navailable.",
      "tldr_zh": "该论文介绍了WikiCausal，一个专为因果知识图(Causal Knowledge Graph)构建设计的语料库、任务和评估框架，以解决现有方法的局限性，如仅关注低级任务或依赖临时数据。语料库基于Wikidata中事件相关概念的Wikipedia文章，任务涉及从这些文章中提取事件概念之间的因果关系。评估框架结合Wikidata的现有关系计算召回率，并利用Large Language Models进行自动化评估，避免手动干预；实验结果显示，该框架能有效选择神经模型（如问答和概念链接）的管道，提升因果知识图的构建质量。该语料库和框架已公开可用。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Extended version; poster paper accepted at ISWC 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.00331v1",
      "published_date": "2024-08-31 02:21:39 UTC",
      "updated_date": "2024-08-31 02:21:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T20:22:04.758609"
    },
    {
      "arxiv_id": "2409.00327v1",
      "title": "Demo: FedCampus: A Real-world Privacy-preserving Mobile Application for Smart Campus via Federated Learning & Analytics",
      "title_zh": "翻译失败",
      "authors": [
        "Jiaxiang Geng",
        "Beilong Tang",
        "Boyan Zhang",
        "Jiaqi Shao",
        "Bing Luo"
      ],
      "abstract": "In this demo, we introduce FedCampus, a privacy-preserving mobile application\nfor smart \\underline{campus} with \\underline{fed}erated learning (FL) and\nfederated analytics (FA). FedCampus enables cross-platform on-device FL/FA for\nboth iOS and Android, supporting continuously models and algorithms deployment\n(MLOps). Our app integrates privacy-preserving processed data via differential\nprivacy (DP) from smartwatches, where the processed parameters are used for\nFL/FA through the FedCampus backend platform. We distributed 100 smartwatches\nto volunteers at Duke Kunshan University and have successfully completed a\nseries of smart campus tasks featuring capabilities such as sleep tracking,\nphysical activity monitoring, personalized recommendations, and heavy hitters.\nOur project is opensourced at https://github.com/FedCampus/FedCampus_Flutter.\nSee the FedCampus video at https://youtu.be/k5iu46IjA38.",
      "tldr_zh": "本演示介绍了 FedCampus，一款基于 Federated Learning (FL) 和 Federated Analytics (FA) 的真实世界隐私保护移动应用，用于智能校园场景。FedCampus 支持 iOS 和 Android 的跨平台设备端 FL/FA，并集成 MLOps 实现模型和算法的持续部署，同时通过 Differential Privacy (DP) 处理智能手表数据以保护隐私。在 Duke Kunshan University 的实际测试中，我们分发 100 个智能手表，成功完成睡眠追踪、身体活动监测、个性化推荐和重打者等任务。该项目已开源在 GitHub，并提供视频演示，展示了其在隐私保护智能校园中的实际应用潜力。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.CR",
      "comment": "2 pages, 3 figures, accepted for publication in ACM Mobihoc 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.00327v1",
      "published_date": "2024-08-31 01:58:36 UTC",
      "updated_date": "2024-08-31 01:58:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T20:22:16.428417"
    },
    {
      "arxiv_id": "2409.00316v1",
      "title": "Toward a More Complete OMR Solution",
      "title_zh": "朝向更完整的 OMR 解决方案",
      "authors": [
        "Guang Yang",
        "Muru Zhang",
        "Lin Qiu",
        "Yanming Wan",
        "Noah A. Smith"
      ],
      "abstract": "Optical music recognition (OMR) aims to convert music notation into digital\nformats. One approach to tackle OMR is through a multi-stage pipeline, where\nthe system first detects visual music notation elements in the image (object\ndetection) and then assembles them into a music notation (notation assembly).\nMost previous work on notation assembly unrealistically assumes perfect object\ndetection. In this study, we focus on the MUSCIMA++ v2.0 dataset, which\nrepresents musical notation as a graph with pairwise relationships among\ndetected music objects, and we consider both stages together. First, we\nintroduce a music object detector based on YOLOv8, which improves detection\nperformance. Second, we introduce a supervised training pipeline that completes\nthe notation assembly stage based on detection output. We find that this model\nis able to outperform existing models trained on perfect detection output,\nshowing the benefit of considering the detection and assembly stages in a more\nholistic way. These findings, together with our novel evaluation metric, are\nimportant steps toward a more complete OMR solution.",
      "tldr_zh": "本研究针对光学音乐识别（OMR）系统，提出一种更全面的解决方案，通过整合对象检测和符号组装阶段来处理音乐符号图像。研究基于 MUSCIMA++ v2.0 数据集，引入了基于 YOLOv8 的音乐对象检测器，以提升检测性能，并开发了一个监督训练管道，利用检测输出完成符号组装。结果显示，该模型在真实检测条件下优于基于完美检测输出的现有模型，并引入了一个新颖的评估指标，这些进展为构建更完整的 OMR 系统奠定了基础。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.00316v1",
      "published_date": "2024-08-31 01:09:12 UTC",
      "updated_date": "2024-08-31 01:09:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T20:22:29.029120"
    },
    {
      "arxiv_id": "2409.00315v1",
      "title": "An Empirical Study on Context Length for Open-Domain Dialog Generation",
      "title_zh": "开放域对话生成中上下文长度的实证研究",
      "authors": [
        "Xinyi Shen",
        "Zuoquan Lin"
      ],
      "abstract": "Transformer-based open-domain dialog models have become increasingly popular\nin recent years. These models typically represent context as a concatenation of\na dialog history. However, there is no criterion to decide how many utterances\nshould be kept adequate in a context. We try to figure out how the choice of\ncontext length affects the model. We experiment on three questions from coarse\nto fine: (i) Does longer context help model training? (ii) Is it necessary to\nchange the training context length when dealing with dialogs of different\ncontext lengths? (iii) Do different dialog samples have the same preference for\ncontext length? Our experimental results show that context length, an often\noverlooked setting, deserves attention when implementing Transformer-based\ndialog models.",
      "tldr_zh": "这篇论文通过实证研究探讨了 Transformer-based 开放域对话生成模型中上下文长度的影响，重点测试上下文长度对模型性能的作用。研究者设计了三个实验问题：（i）更长的上下文是否能提升模型训练效果；（ii）处理不同上下文长度的对话时是否需要调整训练长度；（iii）不同对话样本是否对上下文长度有相同的偏好。结果显示，上下文长度这一常被忽略的设置对 Transformer-based 对话模型的实现至关重要，需要根据具体场景进行优化。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "6 pages, 2 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2409.00315v1",
      "published_date": "2024-08-31 00:56:36 UTC",
      "updated_date": "2024-08-31 00:56:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T20:22:40.127115"
    },
    {
      "arxiv_id": "2409.00310v2",
      "title": "Objective Features Extracted from Motor Activity Time Series for Food Addiction Analysis Using Machine Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Mikhail Borisenkov",
        "Andrei Velichko",
        "Maksim Belyaev",
        "Dmitry Korzun",
        "Tatyana Tserne",
        "Larisa Bakutova",
        "Denis Gubin"
      ],
      "abstract": "This study investigates machine learning algorithms to identify objective\nfeatures for diagnosing food addiction (FA) and assessing confirmed symptoms\n(SC). Data were collected from 81 participants (mean age: 21.5 years, range:\n18-61 years, women: 77.8%) whose FA and SC were measured using the Yale Food\nAddiction Scale (YFAS). Participants provided demographic and anthropometric\ndata, completed the YFAS, the Zung Self-Rating Depression Scale, and the Dutch\nEating Behavior Questionnaire, and wore an actimeter on the non-dominant wrist\nfor a week to record motor activity. Analysis of the actimetric data identified\nsignificant statistical and entropy-based features that accurately predicted FA\nand SC using ML. The Matthews correlation coefficient (MCC) was the primary\nmetric. Activity-related features were more effective for FA prediction\n(MCC=0.88) than rest-related features (MCC=0.68). For SC, activity segments\nyielded MCC=0.47, rest segments MCC=0.38, and their combination MCC=0.51.\nSignificant correlations were also found between actimetric features related to\nFA, emotional, and restrained eating behaviors, supporting the model's\nvalidity. Our results support the concept of a human bionic suite composed of\nIoT devices and ML sensors, which implements health digital assistance with\nreal-time monitoring and analysis of physiological indicators related to FA and\nSC.",
      "tldr_zh": "本研究使用机器学习（Machine Learning）分析从运动活动时间序列中提取的客观特征，以诊断食物成瘾（Food Addiction, FA）和评估确认症状（SC）。研究收集了81名参与者（平均年龄21.5岁，女性77.8%）的数据，包括Yale Food Addiction Scale (YFAS)评估和一周的actimeter记录。结果显示，活动相关特征在FA预测中更有效（Matthews correlation coefficient, MCC=0.88），而SC预测的最佳组合特征达到MCC=0.51，并与情绪性和限制性进食行为相关。该方法支持构建基于IoT设备和ML传感器的健康数字辅助系统，实现FA和SC的实时监控和分析。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP",
        "physics.med-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "16 pages, 3 figures, 14 tables",
      "pdf_url": "http://arxiv.org/pdf/2409.00310v2",
      "published_date": "2024-08-31 00:33:17 UTC",
      "updated_date": "2024-12-05 06:28:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T20:22:55.953600"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 36,
  "processed_papers_count": 36,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-19T20:23:20.312062"
}
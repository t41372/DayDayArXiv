{
  "date": "2024-07-26",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-07-26 的 arXiv 中文 TLDR 快报！今天 arXiv 论文主要聚焦 AI 和机器学习领域，包括大型语言模型（LLM）在因果推理、医疗和代码生成中的应用、强化学习在机器人规划和能源优化上的创新，以及图像处理和语音识别的进展。令人印象深刻的文章有知名学者如 Emre Kıcıman 和 Mark van der Laan 参与的医疗因果推理论文，以及 LLM 用于机器人零样本任务规划的 Wonderful Team。\n\n下面，我将挑选并简要讨论部分关键论文，先优先聊 AI、LLM 和机器学习主题的相关文章，再快速掠过其他领域的内容。每个条目列出论文标题（中文 + 英文），并突出核心贡献和发现。\n\n### 重点 AI 和 LLM 相关论文\n- **Large Language Models as Co-Pilots for Causal Inference in Medical Studies** (英文原题)  \n  作者包括 Emre Kıcıman 和 Mark van der Laan 等知名学者。该论文提出使用 LLM 作为医疗研究的因果推理辅助工具，通过自然语言交互识别研究设计缺陷，并提供框架整合现有因果推理方法，显著提升了医疗数据分析的可靠性和效率。\n\n- **Effective Large Language Model Debugging with Best-first Tree Search** (英文原题)  \n  这篇论文引入 BESTER 算法，利用树搜索和自反式调试优化 LLM 的代码生成能力，实现了在代码基准上的最先进性能，并在可解释性分析中揭示了 LLM 如何修复错误，适用于复杂任务的迭代优化。\n\n- **Wonderful Team: Zero-Shot Physical Task Planning with Visual LLMs** (英文原题)  \n  论文开发了 Wonderful Team 框架，使用视觉 LLM 在零样本环境中规划机器人任务，显著提高了任务成功率（如在 VimaBench 上提升 40%），并强调了 LLM 在机器人感知和控制中的潜力。\n\n- **GPT Deciphering Fedspeak: Quantifying Dissent Among Hawks and Doves** (英文原题)  \n  作者包括 Alan Blinder，该研究利用 GPT-4 分析 FOMC 会议文本，量化经济决策分歧，发现会议记录比公开声明更能反映真实态度，为宏观经济预测提供新工具。\n\n- **Towards Generalized Offensive Language Identification** (英文原题)  \n  这篇论文评估 LLM 在检测攻击性语言的泛化能力，提出新基准并测试不同模型，发现 GPT-4o 在跨领域任务中表现出色，但强调了实际部署中的鲁棒性挑战。\n\n- **Multi-Agent Trajectory Prediction with Difficulty-Guided Feature Enhancement Network** (英文原题)  \n  论文提出 DGFNet 模型，通过难度引导增强特征来预测多代理轨迹，在 Argoverse 基准上实现最先进性能，显著提高了自动驾驶系统的预测准确性和实时性。\n\n- **AppWorld: A Controllable World of Apps and People for Benchmarking Interactive Coding Agents** (英文原题)  \n  该工作构建了 AppWorld 基准，使用 LLM 测试代理在真实应用场景下的交互编码能力，提供 750 个任务数据集，展示了 LLM 在复杂任务中的潜力。\n\n- **Learn from the Learnt: Source-Free Active Domain Adaptation via Contrastive Sampling and Visual Persistence** (英文原题)  \n  论文引入 LFTL 方法，通过对比采样和视觉持久性指导的无源域适应，提升了图像分类的鲁棒性，在基准数据集上达到最先进水平。\n\n- **Binary Bleed: Fast Distributed and Parallel Method for Automatic Model Selection** (英文原题)  \n  提出 Binary Bleed 算法，基于二分搜索加速聚类模型选择，显著减少计算资源需求，并在 NMF 和 K-Means 等任务中表现出色。\n\n- **Accuracy-Privacy Trade-off in the Mitigation of Membership Inference Attack in Federated Learning** (英文原题)  \n  该研究探索联邦学习中的准确性和隐私权衡，证明了深度集成模型的权衡关系，并通过实验验证了不同客户端数量的影响。\n\n- **Greedy Output Approximation: Towards Efficient Structured Pruning for LLMs Without Retraining** (英文原题)  \n  论文提出一种单次剪枝方法，针对 Transformer 模型减少计算成本，同时保持性能，通过优化视角的剪枝标准实现了高效 LLM 压缩。\n\n- **Many-Shot In-Context Learning for Molecular Inverse Design** (英文原题)  \n  引入半监督方法增强 LLM 的分子设计能力，支持多模态交互，显著提高了逆设计任务的准确性。\n\n### 其他机器学习和应用论文（简要掠过）\n- **Surveys Considered Harmful? Reflecting on the Use of Surveys in AI Research, Development, and Governance** (英文原题)  \n  该论文审视 AI 调查方法的局限性，强调西方偏见和透明性问题，提出改进原则。\n\n- **A Scalable Quantum Non-local Neural Network for Image Classification** (英文原题)  \n  开发量子神经网络提升图像分类效率，利用量子并行性减少计算复杂度。\n\n- **Lessons from Learning to Spin \"Pens\"** (英文原题)  \n  使用强化学习训练机器人手部操作，生成高保真轨迹数据集，实现多种物体旋转。\n\n- **FH-DRL: Exponential-Hyperbolic Frontier Heuristics with DRL for accelerated Exploration in Unknown Environments** (英文原题)  \n  提出 FH-DRL 框架，结合深度强化学习加速机器人探索，优化路径规划。\n\n- **Enhancing material property prediction with ensemble deep graph convolutional networks** (英文原题)  \n  使用集成图神经网络提升材料属性预测准确性，适用于无机材料数据集。\n\n### 快速提到的领域特定论文\n其余论文涉及区块链、能源、医疗和图像处理等领域，但非核心焦点，仅简述贡献：\n- **Blockchain for Large Language Model Security and Safety: A Holistic Survey** (英文原题)  \n  综述区块链在 LLM 安全中的应用，提供新框架。\n- **Reinforcement Learning for Sustainable Energy: A Survey** (英文原题)  \n  调查强化学习在能源优化中的潜力。\n- **Using Large Language Models for the Interpretation of Building Regulations** (英文原题)  \n  利用 LLM 翻译建筑法规，提升自动化合规检查。\n- **Towards a Transformer-Based Pre-trained Model for IoT Traffic Classification** (英文原题)  \n  提出 ITCT 模型，提升 IoT 流量分类准确性。\n- **A Fault Prognostic System for the Turbine Guide Bearings of a Hydropower Plant Using Long-Short Term Memory (LSTM)** (英文原题)  \n  使用 LSTM 预测水电站轴承故障，降低 RMSE。\n- 其他如 **GraphBPE: Molecular Graphs Meet Byte-Pair Encoding** 和 **Artificial Neural Networks on Graded Vector Spaces** 等，贡献在于改进图神经网络和数学建模，但影响较小，故不详述。\n\n总之，今天的论文突显了 AI 领域的创新潜力，尤其在 LLM 和强化学习的交叉应用上。感兴趣的读者可关注医疗因果推理和机器人规划方向，更多细节请查阅 arXiv。明天的快报见！",
  "papers": [
    {
      "arxiv_id": "2407.19126v1",
      "title": "Greedy Output Approximation: Towards Efficient Structured Pruning for LLMs Without Retraining",
      "title_zh": "翻译失败",
      "authors": [
        "Jianwei Li",
        "Yijun Dong",
        "Qi Lei"
      ],
      "abstract": "To remove redundant components of large language models (LLMs) without\nincurring significant computational costs, this work focuses on single-shot\npruning without a retraining phase. We simplify the pruning process for\nTransformer-based LLMs by identifying a depth-2 pruning structure that\nfunctions independently. Additionally, we propose two inference-aware pruning\ncriteria derived from the optimization perspective of output approximation,\nwhich outperforms traditional training-aware metrics such as gradient and\nHessian. We also introduce a two-step reconstruction technique to mitigate\npruning errors without model retraining. Experimental results demonstrate that\nour approach significantly reduces computational costs and hardware\nrequirements while maintaining superior performance across various datasets and\nmodels.",
      "tldr_zh": "这篇论文提出Greedy Output Approximation，一种高效的结构化剪枝方法，用于Transformer-based大型语言模型(LLMs)，旨在移除冗余组件而无需重新训练(retraining)。该方法通过识别深度-2剪枝结构和基于输出近似的两个推理感知剪枝标准，优于传统的训练感知指标如gradient和Hessian，并引入两步重建技术来减轻剪枝错误。实验结果表明，该方法显著降低了计算成本和硬件需求，同时在多种数据集和模型上保持了优越性能。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19126v1",
      "published_date": "2024-07-26 23:53:59 UTC",
      "updated_date": "2024-07-26 23:53:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:11:45.011802"
    },
    {
      "arxiv_id": "2407.19125v1",
      "title": "Binary Bleed: Fast Distributed and Parallel Method for Automatic Model Selection",
      "title_zh": "翻译失败",
      "authors": [
        "Ryan Barron",
        "Maksim E. Eren",
        "Manish Bhattarai",
        "Ismael Boureima",
        "Cynthia Matuszek",
        "Boian S. Alexandrov"
      ],
      "abstract": "In several Machine Learning (ML) clustering and dimensionality reduction\napproaches, such as non-negative matrix factorization (NMF), RESCAL, and\nK-Means clustering, users must select a hyper-parameter k to define the number\nof clusters or components that yield an ideal separation of samples or clean\nclusters. This selection, while difficult, is crucial to avoid overfitting or\nunderfitting the data. Several ML applications use scoring methods (e.g.,\nSilhouette and Davies Boulding scores) to evaluate the cluster pattern\nstability for a specific k. The score is calculated for different trials over a\nrange of k, and the ideal k is heuristically selected as the value before the\nmodel starts overfitting, indicated by a drop or increase in the score\nresembling an elbow curve plot. While the grid-search method can be used to\naccurately find a good k value, visiting a range of k can become time-consuming\nand computationally resource-intensive. In this paper, we introduce the Binary\nBleed method based on binary search, which significantly reduces the k search\nspace for these grid-search ML algorithms by truncating the target k values\nfrom the search space using a heuristic with thresholding over the scores.\nBinary Bleed is designed to work with single-node serial, single-node\nmulti-processing, and distributed computing resources. In our experiments, we\ndemonstrate the reduced search space gain over a naive sequential search of the\nideal k and the accuracy of the Binary Bleed in identifying the correct k for\nNMFk, K-Means pyDNMFk, and pyDRESCALk with Silhouette and Davies Boulding\nscores. We make our implementation of Binary Bleed for the NMF algorithm\navailable on GitHub.",
      "tldr_zh": "本论文提出Binary Bleed，一种基于二分搜索的快速分布式和并行方法，用于自动选择机器学习算法中的超参数k，例如在NMF、RESCAL和K-Means中定义簇数，以避免过拟合或欠拟合。Binary Bleed通过对Silhouette和Davies-Bouldin分数应用阈值启发式，显著减少k值的搜索空间，支持单节点串行、单节点多处理和分布式计算环境。实验结果显示，该方法相较于naive顺序搜索，能有效降低搜索空间并准确识别理想k值，并在NMF、K-Means和pyDRESCAL上验证其性能，同时开源了NMF算法的实现。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.PF"
      ],
      "primary_category": "cs.DC",
      "comment": "8 pages, submitted to IEEE HPEC",
      "pdf_url": "http://arxiv.org/pdf/2407.19125v1",
      "published_date": "2024-07-26 23:48:51 UTC",
      "updated_date": "2024-07-26 23:48:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:11:47.691166"
    },
    {
      "arxiv_id": "2407.19119v1",
      "title": "Accuracy-Privacy Trade-off in the Mitigation of Membership Inference Attack in Federated Learning",
      "title_zh": "联邦学习中成员推理攻击缓解的准确性-隐私权衡",
      "authors": [
        "Sayyed Farid Ahamed",
        "Soumya Banerjee",
        "Sandip Roy",
        "Devin Quinn",
        "Marc Vucovich",
        "Kevin Choi",
        "Abdul Rahman",
        "Alison Hu",
        "Edward Bowen",
        "Sachin Shetty"
      ],
      "abstract": "Over the last few years, federated learning (FL) has emerged as a prominent\nmethod in machine learning, emphasizing privacy preservation by allowing\nmultiple clients to collaboratively build a model while keeping their training\ndata private. Despite this focus on privacy, FL models are susceptible to\nvarious attacks, including membership inference attacks (MIAs), posing a\nserious threat to data confidentiality. In a recent study, Rezaei \\textit{et\nal.} revealed the existence of an accuracy-privacy trade-off in deep ensembles\nand proposed a few fusion strategies to overcome it. In this paper, we aim to\nexplore the relationship between deep ensembles and FL. Specifically, we\ninvestigate whether confidence-based metrics derived from deep ensembles apply\nto FL and whether there is a trade-off between accuracy and privacy in FL with\nrespect to MIA. Empirical investigations illustrate a lack of a non-monotonic\ncorrelation between the number of clients and the accuracy-privacy trade-off.\nBy experimenting with different numbers of federated clients, datasets, and\nconfidence-metric-based fusion strategies, we identify and analytically justify\nthe clear existence of the accuracy-privacy trade-off.",
      "tldr_zh": "这篇论文探讨了在 Federated Learning (FL) 中缓解 Membership Inference Attacks (MIAs) 时存在的 accuracy-privacy trade-off。通过实验不同客户端数量、数据集和基于深度集成的置信度指标融合策略，研究者发现 FL 中确实存在这种 trade-off，但客户端数量与 trade-off 之间缺乏非单调相关性。该研究分析了这一现象，并为提升 FL 的隐私保护和模型准确性提供了理论依据。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19119v1",
      "published_date": "2024-07-26 22:44:41 UTC",
      "updated_date": "2024-07-26 22:44:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:12:05.487581"
    },
    {
      "arxiv_id": "2407.19118v1",
      "title": "Large Language Models as Co-Pilots for Causal Inference in Medical Studies",
      "title_zh": "翻译失败",
      "authors": [
        "Ahmed Alaa",
        "Rachael V. Phillips",
        "Emre Kıcıman",
        "Laura B. Balzer",
        "Mark van der Laan",
        "Maya Petersen"
      ],
      "abstract": "The validity of medical studies based on real-world clinical data, such as\nobservational studies, depends on critical assumptions necessary for drawing\ncausal conclusions about medical interventions. Many published studies are\nflawed because they violate these assumptions and entail biases such as\nresidual confounding, selection bias, and misalignment between treatment and\nmeasurement times. Although researchers are aware of these pitfalls, they\ncontinue to occur because anticipating and addressing them in the context of a\nspecific study can be challenging without a large, often unwieldy,\ninterdisciplinary team with extensive expertise. To address this expertise gap,\nwe explore the use of large language models (LLMs) as co-pilot tools to assist\nresearchers in identifying study design flaws that undermine the validity of\ncausal inferences. We propose a conceptual framework for LLMs as causal\nco-pilots that encode domain knowledge across various fields, engaging with\nresearchers in natural language interactions to provide contextualized\nassistance in study design. We provide illustrative examples of how LLMs can\nfunction as causal co-pilots, propose a structured framework for their\ngrounding in existing causal inference frameworks, and highlight the unique\nchallenges and opportunities in adapting LLMs for reliable use in\nepidemiological research.",
      "tldr_zh": "这篇论文探讨了大型语言模型 (LLMs) 作为辅助工具在医疗研究中提升因果推断的有效性，针对观察性研究中的常见问题，如残留混杂、选择偏差和治疗与测量时间不匹配等问题。作者提出一个概念框架，将 LLMs 视为因果协作者，通过编码跨领域知识和自然语言互动，提供上下文化的研究设计指导和缺陷识别。论文通过示例和结构化框架展示了 LLMs 的潜力，同时强调了其在流行病学研究中的独特挑战和机会，如确保可靠性和适应性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19118v1",
      "published_date": "2024-07-26 22:43:15 UTC",
      "updated_date": "2024-07-26 22:43:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:12:40.471167"
    },
    {
      "arxiv_id": "2407.19110v1",
      "title": "GPT Deciphering Fedspeak: Quantifying Dissent Among Hawks and Doves",
      "title_zh": "翻译失败",
      "authors": [
        "Denis Peskoff",
        "Adam Visokay",
        "Sander Schulhoff",
        "Benjamin Wachspress",
        "Alan Blinder",
        "Brandon M. Stewart"
      ],
      "abstract": "Markets and policymakers around the world hang on the consequential monetary\npolicy decisions made by the Federal Open Market Committee (FOMC). Publicly\navailable textual documentation of their meetings provides insight into\nmembers' attitudes about the economy. We use GPT-4 to quantify dissent among\nmembers on the topic of inflation. We find that transcripts and minutes reflect\nthe diversity of member views about the macroeconomic outlook in a way that is\nlost or omitted from the public statements. In fact, diverging opinions that\nshed light upon the committee's \"true\" attitudes are almost entirely omitted\nfrom the final statements. Hence, we argue that forecasting FOMC sentiment\nbased solely on statements will not sufficiently reflect dissent among the\nhawks and doves.",
      "tldr_zh": "本文利用GPT-4分析FOMC（Federal Open Market Committee）会议记录，量化成员在通胀议题上的分歧，揭示鹰派(hawks)和鸽派(doves)观点的多样性。研究发现，会议记录和纪要中存在的不同意见在公开声明中几乎完全被遗漏或省略，导致声明无法准确反映委员会的“真实”态度。由此，作者强调，仅依赖公开声明预测FOMC情绪将不足以捕捉成员间的异议。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19110v1",
      "published_date": "2024-07-26 22:16:40 UTC",
      "updated_date": "2024-07-26 22:16:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:12:29.621114"
    },
    {
      "arxiv_id": "2408.01458v1",
      "title": "Surveys Considered Harmful? Reflecting on the Use of Surveys in AI Research, Development, and Governance",
      "title_zh": "翻译失败",
      "authors": [
        "Mohammmad Tahaei",
        "Daricia Wilkinson",
        "Alisa Frik",
        "Michael Muller",
        "Ruba Abu-Salma",
        "Lauren Wilcox"
      ],
      "abstract": "Calls for engagement with the public in Artificial Intelligence (AI)\nresearch, development, and governance are increasing, leading to the use of\nsurveys to capture people's values, perceptions, and experiences related to AI.\nIn this paper, we critically examine the state of human participant surveys\nassociated with these topics. Through both a reflexive analysis of a survey\npilot spanning six countries and a systematic literature review of 44 papers\nfeaturing public surveys related to AI, we explore prominent perspectives and\nmethodological nuances associated with surveys to date. We find that public\nsurveys on AI topics are vulnerable to specific Western knowledge, values, and\nassumptions in their design, including in their positioning of ethical concepts\nand societal values, lack sufficient critical discourse surrounding deployment\nstrategies, and demonstrate inconsistent forms of transparency in their\nreporting. Based on our findings, we distill provocations and heuristic\nquestions for our community, to recognize the limitations of surveys for\nmeeting the goals of engagement, and to cultivate shared principles to design,\ndeploy, and interpret surveys cautiously and responsibly.",
      "tldr_zh": "这篇论文审视了在AI研究、开发和治理中使用surveys的潜在问题，通过对一个跨六国survey试点的反思分析和对44篇相关文献的系统文献综述，揭示了这些surveys易受西方知识、价值观和假设的影响。研究发现，surveys在定位伦理概念和社会价值观时存在偏差，缺乏对部署策略的充分批判性讨论，且报告透明度不一致。作者据此提出启发性问题和共享原则，旨在帮助社区认识到surveys的局限性，并促进更谨慎、负责任的设计、部署和解释，以提升公众参与的成效。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "To appear in 7th AAAI Conference on AI, Ethics, and Society (AIES)",
      "pdf_url": "http://arxiv.org/pdf/2408.01458v1",
      "published_date": "2024-07-26 22:10:49 UTC",
      "updated_date": "2024-07-26 22:10:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:12:30.985503"
    },
    {
      "arxiv_id": "2407.19094v6",
      "title": "Wonderful Team: Zero-Shot Physical Task Planning with Visual LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Zidan Wang",
        "Rui Shen",
        "Bradly Stadie"
      ],
      "abstract": "We introduce Wonderful Team, a multi-agent Vision Large Language Model (VLLM)\nframework for executing high-level robotic planning in a zero-shot regime. In\nour context, zero-shot high-level planning means that for a novel environment,\nwe provide a VLLM with an image of the robot's surroundings and a task\ndescription, and the VLLM outputs the sequence of actions necessary for the\nrobot to complete the task. Unlike previous methods for high-level visual\nplanning for robotic manipulation, our method uses VLLMs for the entire\nplanning process, enabling a more tightly integrated loop between perception,\ncontrol, and planning. As a result, Wonderful Team's performance on real-world\nsemantic and physical planning tasks often exceeds methods that rely on\nseparate vision systems. For example, we see an average 40% success rate\nimprovement on VimaBench over prior methods such as NLaP, an average 30%\nimprovement over Trajectory Generators on tasks from the Trajectory Generator\npaper, including drawing and wiping a plate, and an average 70% improvement\nover Trajectory Generators on a new set of semantic reasoning tasks including\nenvironment rearrangement with implicit linguistic constraints. We hope these\nresults highlight the rapid improvements of VLLMs in the past year, and\nmotivate the community to consider VLLMs as an option for some high-level\nrobotic planning problems in the future.",
      "tldr_zh": "该研究提出 Wonderful Team，一种基于多智能体 Vision Large Language Model (VLLM) 的框架，用于实现零样本（zero-shot）物理任务规划，允许机器人通过环境图像和任务描述直接输出动作序列，从而紧密整合感知、控制和规划过程。与依赖独立视觉系统的先前方法相比，这种方法显著提升了性能。实验结果显示，在 VimaBench 上成功率平均比 NLaP 高 40%，在 Trajectory Generators 的任务（如绘图和擦拭盘子）上提高 30%，并在新语义任务上实现 70% 的改进。这些发现突显了 VLLM 的快速进步，并鼓励将其应用于高级机器人规划。",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "aka Wonderful Team",
      "pdf_url": "http://arxiv.org/pdf/2407.19094v6",
      "published_date": "2024-07-26 21:18:57 UTC",
      "updated_date": "2025-02-04 00:18:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:12:56.024164"
    },
    {
      "arxiv_id": "2407.19089v1",
      "title": "Many-Shot In-Context Learning for Molecular Inverse Design",
      "title_zh": "翻译失败",
      "authors": [
        "Saeed Moayedpour",
        "Alejandro Corrochano-Navarro",
        "Faryad Sahneh",
        "Shahriar Noroozizadeh",
        "Alexander Koetter",
        "Jiri Vymetal",
        "Lorenzo Kogler-Anele",
        "Pablo Mas",
        "Yasser Jangjou",
        "Sizhen Li",
        "Michael Bailey",
        "Marc Bianciotto",
        "Hans Matter",
        "Christoph Grebner",
        "Gerhard Hessler",
        "Ziv Bar-Joseph",
        "Sven Jager"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated great performance in few-shot\nIn-Context Learning (ICL) for a variety of generative and discriminative\nchemical design tasks. The newly expanded context windows of LLMs can further\nimprove ICL capabilities for molecular inverse design and lead optimization. To\ntake full advantage of these capabilities we developed a new semi-supervised\nlearning method that overcomes the lack of experimental data available for\nmany-shot ICL. Our approach involves iterative inclusion of LLM generated\nmolecules with high predicted performance, along with experimental data. We\nfurther integrated our method in a multi-modal LLM which allows for the\ninteractive modification of generated molecular structures using text\ninstructions. As we show, the new method greatly improves upon existing ICL\nmethods for molecular design while being accessible and easy to use for\nscientists.",
      "tldr_zh": "本文提出了一种新的半监督学习方法，用于many-shot In-Context Learning (ICL) 在分子逆设计中的应用，以解决实验数据不足的问题。该方法通过迭代加入大语言模型 (LLMs) 生成的预测性能高的分子以及现有实验数据，显著提升了分子设计和优化效果。此外，该方法整合到多模态LLMs中，支持使用文本指令进行交互式分子结构修改，并被证明比现有ICL方法更高效且易于科学家操作。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19089v1",
      "published_date": "2024-07-26 21:10:50 UTC",
      "updated_date": "2024-07-26 21:10:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:13:08.067866"
    },
    {
      "arxiv_id": "2407.19082v2",
      "title": "Regularized Multi-Decoder Ensemble for an Error-Aware Scene Representation Network",
      "title_zh": "翻译失败",
      "authors": [
        "Tianyu Xiong",
        "Skylar W. Wurster",
        "Hanqi Guo",
        "Tom Peterka",
        "Han-Wei Shen"
      ],
      "abstract": "Feature grid Scene Representation Networks (SRNs) have been applied to\nscientific data as compact functional surrogates for analysis and\nvisualization. As SRNs are black-box lossy data representations, assessing the\nprediction quality is critical for scientific visualization applications to\nensure that scientists can trust the information being visualized. Currently,\nexisting architectures do not support inference time reconstruction quality\nassessment, as coordinate-level errors cannot be evaluated in the absence of\nground truth data. We propose a parameter-efficient multi-decoder SRN (MDSRN)\nensemble architecture consisting of a shared feature grid with multiple\nlightweight multi-layer perceptron decoders. MDSRN can generate a set of\nplausible predictions for a given input coordinate to compute the mean as the\nprediction of the multi-decoder ensemble and the variance as a confidence\nscore. The coordinate-level variance can be rendered along with the data to\ninform the reconstruction quality, or be integrated into uncertainty-aware\nvolume visualization algorithms. To prevent the misalignment between the\nquantified variance and the prediction quality, we propose a novel variance\nregularization loss for ensemble learning that promotes the Regularized\nmulti-decoder SRN (RMDSRN) to obtain a more reliable variance that correlates\nclosely to the true model error. We comprehensively evaluate the quality of\nvariance quantification and data reconstruction of Monte Carlo Dropout, Mean\nField Variational Inference, Deep Ensemble, and Predicting Variance compared to\nthe proposed MDSRN and RMDSRN across diverse scalar field datasets. We\ndemonstrate that RMDSRN attains the most accurate data reconstruction and\ncompetitive variance-error correlation among uncertain SRNs under the same\nneural network parameter budgets.",
      "tldr_zh": "该研究针对 Scene Representation Networks (SRNs) 在科学数据可视化中的黑盒预测问题，提出了一种参数高效的多解码器 SRN (MDSRN) 架构，该架构使用共享特征网格和多个轻量级多层感知器 (MLP) 解码器，为输入坐标生成预测均值和方差置信度分数，以评估重建质量。针对方差与预测质量可能不匹配的问题，引入了新的方差正则化损失，优化了 Regularized multi-decoder SRN (RMDSRN)，使其方差更可靠并与真实模型错误高度相关。在多种标量场数据集上实验表明，RMDSRN 在相同神经网络参数预算下，比 Monte Carlo Dropout、Mean Field Variational Inference 和 Deep Ensemble 等方法实现了更准确的数据重建和竞争性的方差-错误相关性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.GR",
        "cs.HC"
      ],
      "primary_category": "cs.LG",
      "comment": "To be published in Proc. IEEE VIS 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.19082v2",
      "published_date": "2024-07-26 21:02:11 UTC",
      "updated_date": "2024-08-05 21:09:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:13:16.710473"
    },
    {
      "arxiv_id": "2407.19055v1",
      "title": "Effective Large Language Model Debugging with Best-first Tree Search",
      "title_zh": "基于最佳优先树搜索的有效大型语言模型调试",
      "authors": [
        "Jialin Song",
        "Jonathan Raiman",
        "Bryan Catanzaro"
      ],
      "abstract": "Large Language Models (LLMs) show promise in code generation tasks. However,\ntheir code-writing abilities are often limited in scope: while they can\nsuccessfully implement simple functions, they struggle with more complex tasks.\nA fundamental difference with how an LLM writes code, compared to a human\nprogrammer, is that it cannot consistently spot and fix bugs. Debugging is a\ncrucial skill for programmers and it enables iterative code refinement towards\na correct implementation. In this work, we propose a novel algorithm to enable\nLLMs to debug their code via self-reflection and search where a model attempts\nto identify its previous mistakes. Our key contributions are 1) a best-first\ntree search algorithm with self-reflections (BESTER) that achieves\nstate-of-the-art Pass@1 in three code generation benchmarks. BESTER maintains\nits superiority when we measure pass rates taking into account additional\ninference costs incurred by tree search. 2) A novel interpretability study on\nwhat self-reflections attend to in buggy programs and how they impact bug\nfixes, which provides a deeper understanding of the debugging process. 3) An\nextensive study on when self-reflections are effective in finding bugs.",
      "tldr_zh": "这篇论文针对 Large Language Models (LLMs) 在代码生成任务中存在的调试难题，提出了一种新算法BESTER，利用 best-first tree search 和自我反思机制，帮助模型识别并修复错误。BESTER 在三个代码生成基准上实现了 state-of-the-art 的 Pass@1 性能，并在考虑额外推理成本后保持优势。论文还通过解释性研究分析了自我反思如何关注错误程序并影响修复过程，并对这种机制的有效性进行了广泛探讨。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19055v1",
      "published_date": "2024-07-26 19:26:00 UTC",
      "updated_date": "2024-07-26 19:26:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:13:30.508881"
    },
    {
      "arxiv_id": "2407.19051v1",
      "title": "Towards a Transformer-Based Pre-trained Model for IoT Traffic Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Bruna Bazaluk",
        "Mosab Hamdan",
        "Mustafa Ghaleb",
        "Mohammed S. M. Gismalla",
        "Flavio S. Correa da Silva",
        "Daniel Macêdo Batista"
      ],
      "abstract": "The classification of IoT traffic is important to improve the efficiency and\nsecurity of IoT-based networks. As the state-of-the-art classification methods\nare based on Deep Learning, most of the current results require a large amount\nof data to be trained. Thereby, in real-life situations, where there is a\nscarce amount of IoT traffic data, the models would not perform so well.\nConsequently, these models underperform outside their initial training\nconditions and fail to capture the complex characteristics of network traffic,\nrendering them inefficient and unreliable in real-world applications. In this\npaper, we propose IoT Traffic Classification Transformer (ITCT), a novel\napproach that utilizes the state-of-the-art transformer-based model named\nTabTransformer. ITCT, which is pre-trained on a large labeled MQTT-based IoT\ntraffic dataset and may be fine-tuned with a small set of labeled data, showed\npromising results in various traffic classification tasks. Our experiments\ndemonstrated that the ITCT model significantly outperforms existing models,\nachieving an overall accuracy of 82%. To support reproducibility and\ncollaborative development, all associated code has been made publicly\navailable.",
      "tldr_zh": "本研究针对 IoT 流量分类问题，指出现有深度学习模型需要大量数据训练，在数据稀缺的真实场景中性能不佳。提出 ITCT（IoT Traffic Classification Transformer），一种基于 TabTransformer 的预训练模型，在大型标记的 MQTT-based IoT 流量数据集上预训练，并可通过少量标记数据微调。实验结果显示，ITCT 在各种分类任务中准确率达 82%，显著优于现有模型，且所有代码已开源以支持可复现性。",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "Updated version of: B. Bazaluk, M. Hamdan, M. Ghaleb, M. S. M.\n  Gismalla, F. S. Correa da Silva and D. M. Batista, \"Towards a\n  Transformer-Based Pre-trained Model for IoT Traffic Classification,\" NOMS\n  2024-2024 IEEE Network Operations and Management Symposium, Seoul, Korea,\n  Republic of, 2024, pp. 1-7, doi: 10.1109/NOMS59830.2024.10575448",
      "pdf_url": "http://arxiv.org/pdf/2407.19051v1",
      "published_date": "2024-07-26 19:13:11 UTC",
      "updated_date": "2024-07-26 19:13:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:13:57.518722"
    },
    {
      "arxiv_id": "2407.19041v1",
      "title": "Optimizing Numerical Estimation and Operational Efficiency in the Legal Domain through Large Language Models",
      "title_zh": "通过大型语言模型优化法律领域的数值估计和操作效率",
      "authors": [
        "Jia-Hong Huang",
        "Chao-Chun Yang",
        "Yixian Shen",
        "Alessio M. Pacces",
        "Evangelos Kanoulas"
      ],
      "abstract": "The legal landscape encompasses a wide array of lawsuit types, presenting\nlawyers with challenges in delivering timely and accurate information to\nclients, particularly concerning critical aspects like potential imprisonment\nduration or financial repercussions. Compounded by the scarcity of legal\nexperts, there's an urgent need to enhance the efficiency of traditional legal\nworkflows. Recent advances in deep learning, especially Large Language Models\n(LLMs), offer promising solutions to this challenge. Leveraging LLMs'\nmathematical reasoning capabilities, we propose a novel approach integrating\nLLM-based methodologies with specially designed prompts to address precision\nrequirements in legal Artificial Intelligence (LegalAI) applications. The\nproposed work seeks to bridge the gap between traditional legal practices and\nmodern technological advancements, paving the way for a more accessible,\nefficient, and equitable legal system. To validate this method, we introduce a\ncurated dataset tailored to precision-oriented LegalAI tasks, serving as a\nbenchmark for evaluating LLM-based approaches. Extensive experimentation\nconfirms the efficacy of our methodology in generating accurate numerical\nestimates within the legal domain, emphasizing the role of LLMs in streamlining\nlegal processes and meeting the evolving demands of LegalAI.",
      "tldr_zh": "本研究针对法律领域中律师提供及时准确信息的挑战（如监禁期限或财务影响的数字估计），提出了一种整合 Large Language Models (LLMs) 的新方法，利用其数学推理能力并结合特殊设计的提示，以提升法律 AI (LegalAI) 的精确性和操作效率。该方法桥接了传统法律实践与现代技术，并引入了一个定制数据集作为基准，用于评估 LLM 基于的 LegalAI 任务。通过广泛实验验证，该方法在生成准确数字估计方面表现出色，有效 streamlining 了法律流程，推动了更高效、公平的法律系统。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "The paper has been accepted by the 33rd ACM International Conference\n  on Information and Knowledge Management (CIKM) in 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.19041v1",
      "published_date": "2024-07-26 18:46:39 UTC",
      "updated_date": "2024-07-26 18:46:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:14:25.777241"
    },
    {
      "arxiv_id": "2407.19040v1",
      "title": "A Fault Prognostic System for the Turbine Guide Bearings of a Hydropower Plant Using Long-Short Term Memory (LSTM)",
      "title_zh": "翻译失败",
      "authors": [
        "Yasir Saleem Afridi",
        "Mian Ibad Ali Shah",
        "Adnan Khan",
        "Atia Kareem",
        "Laiq Hasan"
      ],
      "abstract": "Hydroelectricity, being a renewable source of energy, globally fulfills the\nelectricity demand. Hence, Hydropower Plants (HPPs) have always been in the\nlimelight of research. The fast-paced technological advancement is enabling us\nto develop state-of-the-art power generation machines. This has not only\nresulted in improved turbine efficiency but has also increased the complexity\nof these systems. In lieu thereof, efficient Operation & Maintenance (O&M) of\nsuch intricate power generation systems has become a more challenging task.\nTherefore, there has been a shift from conventional reactive approaches to more\nintelligent predictive approaches in maintaining the HPPs. The research is\ntherefore targeted to develop an artificially intelligent fault prognostics\nsystem for the turbine bearings of an HPP. The proposed method utilizes the\nLong Short-Term Memory (LSTM) algorithm in developing the model. Initially, the\nmodel is trained and tested with bearing vibration data from a test rig.\nSubsequently, it is further trained and tested with realistic bearing vibration\ndata obtained from an HPP operating in Pakistan via the Supervisory Control and\nData Acquisition (SCADA) system. The model demonstrates highly effective\npredictions of bearing vibration values, achieving a remarkably low RMSE.",
      "tldr_zh": "这篇论文针对水电站（HPPs）的涡轮导轴承开发了一个基于 Long Short-Term Memory (LSTM) 算法的故障预测系统，以应对维护复杂性的挑战。研究从反应式维护转向预测式维护，通过先用测试台的轴承振动数据训练模型，然后利用巴基斯坦水电站的真实数据（经 Supervisory Control and Data Acquisition (SCADA) 系统获取）进行进一步验证。结果表明，该模型在预测轴承振动值时准确性极高，实现了显著低的 RMSE 值，为智能维护水电系统提供了有效解决方案。",
      "categories": [
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.AI",
      "comment": "8 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2407.19040v1",
      "published_date": "2024-07-26 18:45:42 UTC",
      "updated_date": "2024-07-26 18:45:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:14:30.961946"
    },
    {
      "arxiv_id": "2407.19039v1",
      "title": "GraphBPE: Molecular Graphs Meet Byte-Pair Encoding",
      "title_zh": "翻译失败",
      "authors": [
        "Yuchen Shen",
        "Barnabás Póczos"
      ],
      "abstract": "With the increasing attention to molecular machine learning, various\ninnovations have been made in designing better models or proposing more\ncomprehensive benchmarks. However, less is studied on the data preprocessing\nschedule for molecular graphs, where a different view of the molecular graph\ncould potentially boost the model's performance. Inspired by the Byte-Pair\nEncoding (BPE) algorithm, a subword tokenization method popularly adopted in\nNatural Language Processing, we propose GraphBPE, which tokenizes a molecular\ngraph into different substructures and acts as a preprocessing schedule\nindependent of the model architectures. Our experiments on 3 graph-level\nclassification and 3 graph-level regression datasets show that data\npreprocessing could boost the performance of models for molecular graphs, and\nGraphBPE is effective for small classification datasets and it performs on par\nwith other tokenization methods across different model architectures.",
      "tldr_zh": "本研究发现，分子机器学习领域虽有诸多模型和基准创新，但分子图数据预处理方法研究较少，因此提出GraphBPE，一种受Byte-Pair Encoding (BPE)启发的预处理方法，将分子图分解为子结构，以提升模型性能。GraphBPE独立于模型架构，通过在3个图级分类和3个图级回归数据集上的实验，证明了其对小型分类数据集的有效性，并在不同模型架构中与其它标记方法表现相当。该方法为分子图处理提供了新视角，展示了数据预处理的潜在价值。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.chem-ph",
        "q-bio.BM"
      ],
      "primary_category": "cs.LG",
      "comment": "accepted by ICML 2024 AI for Science Workshop",
      "pdf_url": "http://arxiv.org/pdf/2407.19039v1",
      "published_date": "2024-07-26 18:45:09 UTC",
      "updated_date": "2024-07-26 18:45:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:14:29.398608"
    },
    {
      "arxiv_id": "2407.19031v2",
      "title": "Artificial Neural Networks on Graded Vector Spaces",
      "title_zh": "分级向量空间上的人工神经网络",
      "authors": [
        "Tony Shaska"
      ],
      "abstract": "This paper presents a transformative framework for artificial neural networks\nover graded vector spaces, tailored to model hierarchical and structured data\nin fields like algebraic geometry and physics. By exploiting the algebraic\nproperties of graded vector spaces, where features carry distinct weights, we\nextend classical neural networks with graded neurons, layers, and activation\nfunctions that preserve structural integrity. Grounded in group actions,\nrepresentation theory, and graded algebra, our approach combines theoretical\nrigor with practical utility.\n  We introduce graded neural architectures, loss functions prioritizing graded\ncomponents, and equivariant extensions adaptable to diverse gradings. Case\nstudies validate the framework's effectiveness, outperforming standard neural\nnetworks in tasks such as predicting invariants in weighted projective spaces\nand modeling supersymmetric systems.\n  This work establishes a new frontier in machine learning, merging\nmathematical sophistication with interdisciplinary applications. Future\nchallenges, including computational scalability and finite field extensions,\noffer rich opportunities for advancing this paradigm.",
      "tldr_zh": "本研究提出了一种应用于分级向量空间（graded vector spaces）的神经网络框架，用于建模层次化和结构化数据，如代数几何和物理领域。框架通过利用分级向量空间的代数特性，扩展了经典神经网络，包括分级神经元（graded neurons）、分级层（graded layers）和激活函数，以保持结构的完整性，并结合群作用（group actions）、表示理论（representation theory）和分级代数（graded algebra）。实验案例显示，该框架在预测加权射影空间的不变量和建模超对称系统等任务中，优于标准神经网络，为机器学习开辟了新的前沿，并指出了未来挑战如计算可伸缩性（computational scalability）。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "68T05, 68Q32, 16W50, 17B70, 58A50, 14A22",
        "I.2; I.2.6"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19031v2",
      "published_date": "2024-07-26 18:17:58 UTC",
      "updated_date": "2025-05-10 15:03:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:14:48.334610"
    },
    {
      "arxiv_id": "2407.18913v2",
      "title": "SOAP-RL: Sequential Option Advantage Propagation for Reinforcement Learning in POMDP Environments",
      "title_zh": "翻译失败",
      "authors": [
        "Shu Ishida",
        "João F. Henriques"
      ],
      "abstract": "This work compares ways of extending Reinforcement Learning algorithms to\nPartially Observed Markov Decision Processes (POMDPs) with options. One view of\noptions is as temporally extended action, which can be realized as a memory\nthat allows the agent to retain historical information beyond the policy's\ncontext window. While option assignment could be handled using heuristics and\nhand-crafted objectives, learning temporally consistent options and associated\nsub-policies without explicit supervision is a challenge. Two algorithms, PPOEM\nand SOAP, are proposed and studied in depth to address this problem. PPOEM\napplies the forward-backward algorithm (for Hidden Markov Models) to optimize\nthe expected returns for an option-augmented policy. However, this learning\napproach is unstable during on-policy rollouts. It is also unsuited for\nlearning causal policies without the knowledge of future trajectories, since\noption assignments are optimized for offline sequences where the entire episode\nis available. As an alternative approach, SOAP evaluates the policy gradient\nfor an optimal option assignment. It extends the concept of the generalized\nadvantage estimation (GAE) to propagate option advantages through time, which\nis an analytical equivalent to performing temporal back-propagation of option\npolicy gradients. This option policy is only conditional on the history of the\nagent, not future actions. Evaluated against competing baselines, SOAP\nexhibited the most robust performance, correctly discovering options for POMDP\ncorridor environments, as well as on standard benchmarks including Atari and\nMuJoCo, outperforming PPOEM, as well as LSTM and Option-Critic baselines. The\nopen-sourced code is available at https://github.com/shuishida/SoapRL.",
      "tldr_zh": "本研究探讨了在部分可观测马尔可夫决策过程 (POMDPs) 中扩展强化学习算法的方法，特别关注 options 作为时间扩展动作来保留历史信息。作者提出了两种算法：PPOEM 和 SOAP，其中 SOAP 通过扩展广义优势估计 (GAE) 来传播选项优势，实现对选项策略梯度的时序传播，确保策略仅依赖于代理的历史而非未来轨迹。实验结果显示，SOAP 在 POMDP 走廊环境、Atari 和 MuJoCo 基准上表现出最稳健的性能，优于 PPOEM 以及 LSTM 和 Option-Critic 等基线，为无监督学习选项和子策略提供了有效解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18913v2",
      "published_date": "2024-07-26 17:59:55 UTC",
      "updated_date": "2024-10-11 15:35:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:15:09.378642"
    },
    {
      "arxiv_id": "2407.18906v2",
      "title": "A Scalable Quantum Non-local Neural Network for Image Classification",
      "title_zh": "可扩展的量子非局部神经网络用于图像分类",
      "authors": [
        "Sparsh Gupta",
        "Debanjan Konar",
        "Vaneet Aggarwal"
      ],
      "abstract": "Non-local operations play a crucial role in computer vision enabling the\ncapture of long-range dependencies through weighted sums of features across the\ninput, surpassing the constraints of traditional convolution operations that\nfocus solely on local neighborhoods. Non-local operations typically require\ncomputing pairwise relationships between all elements in a set, leading to\nquadratic complexity in terms of time and memory. Due to the high computational\nand memory demands, scaling non-local neural networks to large-scale problems\ncan be challenging. This article introduces a hybrid quantum-classical scalable\nnon-local neural network, referred to as Quantum Non-Local Neural Network\n(QNL-Net), to enhance pattern recognition. The proposed QNL-Net relies on\ninherent quantum parallelism to allow the simultaneous processing of a large\nnumber of input features enabling more efficient computations in\nquantum-enhanced feature space and involving pairwise relationships through\nquantum entanglement. We benchmark our proposed QNL-Net with other quantum\ncounterparts to binary classification with datasets MNIST and CIFAR-10. The\nsimulation findings showcase our QNL-Net achieves cutting-edge accuracy levels\nin binary image classification among quantum classifiers while utilizing fewer\nqubits.",
      "tldr_zh": "这篇论文针对传统非局部神经网络在图像分类中面临的计算密集问题（如二次复杂度），提出了一种可扩展的混合量子-经典框架，名为Quantum Non-Local Neural Network (QNL-Net)。QNL-Net利用量子并行性和量子纠缠来高效处理大量输入特征和成对关系，从而提升模式识别的效率和性能。在MNIST和CIFAR-10数据集上的二进制分类基准测试中，QNL-Net实现了领先的准确率，同时仅需更少的qubits，展示了其在量子增强特征空间中的优势。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.IT",
        "cs.LG",
        "math.IT",
        "quant-ph"
      ],
      "primary_category": "cs.CV",
      "comment": "preprint, 12 pages (including references and appendix), 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.18906v2",
      "published_date": "2024-07-26 17:58:57 UTC",
      "updated_date": "2024-08-22 02:22:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:15:20.106128"
    },
    {
      "arxiv_id": "2407.18902v2",
      "title": "Lessons from Learning to Spin \"Pens\"",
      "title_zh": "翻译失败",
      "authors": [
        "Jun Wang",
        "Ying Yuan",
        "Haichuan Che",
        "Haozhi Qi",
        "Yi Ma",
        "Jitendra Malik",
        "Xiaolong Wang"
      ],
      "abstract": "In-hand manipulation of pen-like objects is an important skill in our daily\nlives, as many tools such as hammers and screwdrivers are similarly shaped.\nHowever, current learning-based methods struggle with this task due to a lack\nof high-quality demonstrations and the significant gap between simulation and\nthe real world. In this work, we push the boundaries of learning-based in-hand\nmanipulation systems by demonstrating the capability to spin pen-like objects.\nWe first use reinforcement learning to train an oracle policy with privileged\ninformation and generate a high-fidelity trajectory dataset in simulation. This\nserves two purposes: 1) pre-training a sensorimotor policy in simulation; 2)\nconducting open-loop trajectory replay in the real world. We then fine-tune the\nsensorimotor policy using these real-world trajectories to adapt it to the real\nworld dynamics. With less than 50 trajectories, our policy learns to rotate\nmore than ten pen-like objects with different physical properties for multiple\nrevolutions. We present a comprehensive analysis of our design choices and\nshare the lessons learned during development.",
      "tldr_zh": "本文研究了学习-based in-hand manipulation 系统在旋转笔状物体（如锤子和螺丝刀）方面的进展，旨在克服高质量演示缺失和模拟-现实世界差距的挑战。作者首先利用 reinforcement learning 训练一个 oracle policy 生成高保真轨迹数据集，用于在模拟环境中预训练 sensorimotor policy，并进行真实世界的 open-loop trajectory replay。随后，通过少于50条真实轨迹 fine-tune 该政策，使其适应真实动态，并成功旋转超过十种不同物理属性的笔状物体。论文还提供了设计选择的全面分析和开发过程中的经验教训。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "CoRL 2024. Website: https://penspin.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2407.18902v2",
      "published_date": "2024-07-26 17:56:01 UTC",
      "updated_date": "2024-10-23 19:56:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:15:19.861758"
    },
    {
      "arxiv_id": "2407.18901v1",
      "title": "AppWorld: A Controllable World of Apps and People for Benchmarking Interactive Coding Agents",
      "title_zh": "AppWorld：用于基准测试交互式编码代理的可控应用和",
      "authors": [
        "Harsh Trivedi",
        "Tushar Khot",
        "Mareike Hartmann",
        "Ruskin Manku",
        "Vinty Dong",
        "Edward Li",
        "Shashank Gupta",
        "Ashish Sabharwal",
        "Niranjan Balasubramanian"
      ],
      "abstract": "Autonomous agents that address day-to-day digital tasks (e.g., ordering\ngroceries for a household), must not only operate multiple apps (e.g., notes,\nmessaging, shopping app) via APIs, but also generate rich code with complex\ncontrol flow in an iterative manner based on their interaction with the\nenvironment. However, existing benchmarks for tool use are inadequate, as they\nonly cover tasks that require a simple sequence of API calls.\n  To remedy this gap, we built $\\textbf{AppWorld Engine}$, a high-quality\nexecution environment (60K lines of code) of 9 day-to-day apps operable via 457\nAPIs and populated with realistic digital activities simulating the lives of\n~100 fictitious users. We then created $\\textbf{AppWorld Benchmark}$ (40K lines\nof code), a suite of 750 natural, diverse, and challenging autonomous agent\ntasks requiring rich and interactive code generation. It supports robust\nprogrammatic evaluation with state-based unit tests, allowing for different\nways of completing a task while also checking for unexpected changes, i.e.,\ncollateral damage. The state-of-the-art LLM, GPT-4o, solves only ~49% of our\n'normal' tasks and ~30% of 'challenge' tasks, while other models solve at least\n16% fewer. This highlights the benchmark's difficulty and AppWorld's potential\nto push the frontiers of interactive coding agents. The project website is\navailable at https://appworld.dev/.",
      "tldr_zh": "该论文引入了AppWorld Engine和AppWorld Benchmark，这是一个模拟日常数字任务的执行环境，包含9个应用、457个API和约100个虚构用户的活动，用于评估交互式编码代理的性能。AppWorld Benchmark 提供了750个多样化任务，需要代理生成复杂的代码和控制流，并支持基于状态的单元测试来检查任务完成情况和避免意外变化（collateral damage）。实验结果显示，GPT-4o仅解决了约49%的常规任务和30%的挑战任务，而其他模型表现更差，这突显了该基准的难度和对推动交互式编码代理技术的前沿潜力。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "ACL'24 Camera Ready",
      "pdf_url": "http://arxiv.org/pdf/2407.18901v1",
      "published_date": "2024-07-26 17:55:45 UTC",
      "updated_date": "2024-07-26 17:55:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:15:32.622059"
    },
    {
      "arxiv_id": "2407.18899v1",
      "title": "Learn from the Learnt: Source-Free Active Domain Adaptation via Contrastive Sampling and Visual Persistence",
      "title_zh": "翻译失败",
      "authors": [
        "Mengyao Lyu",
        "Tianxiang Hao",
        "Xinhao Xu",
        "Hui Chen",
        "Zijia Lin",
        "Jungong Han",
        "Guiguang Ding"
      ],
      "abstract": "Domain Adaptation (DA) facilitates knowledge transfer from a source domain to\na related target domain. This paper investigates a practical DA paradigm,\nnamely Source data-Free Active Domain Adaptation (SFADA), where source data\nbecomes inaccessible during adaptation, and a minimum amount of annotation\nbudget is available in the target domain. Without referencing the source data,\nnew challenges emerge in identifying the most informative target samples for\nlabeling, establishing cross-domain alignment during adaptation, and ensuring\ncontinuous performance improvements through the iterative query-and-adaptation\nprocess. In response, we present learn from the learnt (LFTL), a novel paradigm\nfor SFADA to leverage the learnt knowledge from the source pretrained model and\nactively iterated models without extra overhead. We propose Contrastive Active\nSampling to learn from the hypotheses of the preceding model, thereby querying\ntarget samples that are both informative to the current model and persistently\nchallenging throughout active learning. During adaptation, we learn from\nfeatures of actively selected anchors obtained from previous intermediate\nmodels, so that the Visual Persistence-guided Adaptation can facilitate feature\ndistribution alignment and active sample exploitation. Extensive experiments on\nthree widely-used benchmarks show that our LFTL achieves state-of-the-art\nperformance, superior computational efficiency and continuous improvements as\nthe annotation budget increases. Our code is available at\nhttps://github.com/lyumengyao/lftl.",
      "tldr_zh": "这篇论文针对 Source-Free Active Domain Adaptation (SFADA) 问题，提出了一种名为 Learn from the Learnt (LFTL) 的新范式，利用源预训练模型和迭代模型的知识来实现高效的领域适应。LFTL 包括 Contrastive Active Sampling 方法，用于从前一个模型的假设中选择对当前模型有信息量且持续具有挑战性的目标样本，以及 Visual Persistence-guided Adaptation 技术，通过利用之前中间模型的特征锚点来促进跨域特征分布对齐和样本优化。在三个常用基准上的实验表明，LFTL 实现了最先进性能、更高的计算效率，并随着标注预算的增加而持续提升整体表现。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "ECCV 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.18899v1",
      "published_date": "2024-07-26 17:51:58 UTC",
      "updated_date": "2024-07-26 17:51:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:15:39.316700"
    },
    {
      "arxiv_id": "2407.18892v2",
      "title": "FH-DRL: Exponential-Hyperbolic Frontier Heuristics with DRL for accelerated Exploration in Unknown Environments",
      "title_zh": "翻译失败",
      "authors": [
        "Seunghyeop Nam",
        "Tuan Anh Nguyen",
        "Eunmi Choi",
        "Dugki Min"
      ],
      "abstract": "Autonomous robot exploration in large-scale or cluttered environments remains\na central challenge in intelligent vehicle applications, where partial or\nabsent prior maps constrain reliable navigation. This paper introduces FH-DRL,\na novel framework that integrates a customizable heuristic function for\nfrontier detection with a Twin Delayed DDPG (TD3) agent for continuous,\nhigh-speed local navigation. The proposed heuristic relies on an\nexponential-hyperbolic distance score, which balances immediate proximity\nagainst long-range exploration gains, and an occupancy-based stochastic\nmeasure, accounting for environmental openness and obstacle densities in real\ntime. By ranking frontiers using these adaptive metrics, FH-DRL targets highly\ninformative yet tractable waypoints, thereby minimizing redundant paths and\ntotal exploration time. We thoroughly evaluate FH-DRL across multiple simulated\nand real-world scenarios, demonstrating clear improvements in travel distance\nand completion time over frontier-only or purely DRL-based exploration. In\nstructured corridor layouts and maze-like topologies, our architecture\nconsistently outperforms standard methods such as Nearest Frontier, Cognet\nFrontier Exploration, and Goal Driven Autonomous Exploration. Real-world tests\nwith a Turtlebot3 platform further confirm robust adaptation to previously\nunseen or cluttered indoor spaces. The results highlight FH-DRL as an efficient\nand generalizable approach for frontier-based exploration in large or partially\nknown environments, offering a promising direction for various autonomous\ndriving, industrial, and service robotics tasks.",
      "tldr_zh": "本研究提出 FH-DRL 框架，通过结合指数-双曲距离评分（exponential-hyperbolic distance score）和 TD3 代理（Twin Delayed DDPG），加速自主机器人在未知环境的探索。框架使用自适应启发式函数和基于占用率的随机测量，平衡近距离导航与远距离探索收益，从而优先选择信息丰富的路径，减少冗余和总探索时间。在模拟及真实场景测试中，FH-DRL 显著优于传统方法如 Nearest Frontier 和 Cognet Frontier Exploration，在走廊布局和迷宫环境中降低了旅行距离和完成时间。整体结果表明，该框架为自主驾驶、工业和服务机器人任务提供了一个高效、可推广的解决方案。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18892v2",
      "published_date": "2024-07-26 17:42:18 UTC",
      "updated_date": "2025-02-13 02:46:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:16:02.041883"
    },
    {
      "arxiv_id": "2407.20287v1",
      "title": "Variational Inference Using Material Point Method",
      "title_zh": "翻译失败",
      "authors": [
        "Yongchao Huang"
      ],
      "abstract": "A new gradient-based particle sampling method, MPM-ParVI, based on material\npoint method (MPM), is proposed for variational inference. MPM-ParVI simulates\nthe deformation of a deformable body (e.g. a solid or fluid) under external\neffects driven by the target density; transient or steady configuration of the\ndeformable body approximates the target density. The continuum material is\nmodelled as an interacting particle system (IPS) using MPM, each particle\ncarries full physical properties, interacts and evolves following conservation\ndynamics. This easy-to-implement ParVI method offers deterministic sampling and\ninference for a class of probabilistic models such as those encountered in\nBayesian inference (e.g. intractable densities) and generative modelling (e.g.\nscore-based).",
      "tldr_zh": "本文提出了一种基于 Material Point Method (MPM) 的新方法 MPM-ParVI，用于 Variational Inference，通过模拟可变形物体（如固体或流体）在外部影响下的变形来逼近目标密度。系统将连续材料建模为相互作用的粒子系统 (IPS)，每个粒子携带完整的物理属性并遵循守恒动力学演化，实现易于实施的确定性采样和推断。该方法适用于 Bayesian inference 中的难解密度和 score-based 生成模型，提供了一种高效的概率模型处理框架。",
      "categories": [
        "cs.AI",
        "stat.CO",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.20287v1",
      "published_date": "2024-07-26 17:19:50 UTC",
      "updated_date": "2024-07-26 17:19:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:16:14.798933"
    },
    {
      "arxiv_id": "2407.18875v2",
      "title": "Generative Adversarial Networks for Imputing Sparse Learning Performance",
      "title_zh": "翻译失败",
      "authors": [
        "Liang Zhang",
        "Mohammed Yeasin",
        "Jionghao Lin",
        "Felix Havugimana",
        "Xiangen Hu"
      ],
      "abstract": "Learning performance data, such as correct or incorrect responses to\nquestions in Intelligent Tutoring Systems (ITSs) is crucial for tracking and\nassessing the learners' progress and mastery of knowledge. However, the issue\nof data sparsity, characterized by unexplored questions and missing attempts,\nhampers accurate assessment and the provision of tailored, personalized\ninstruction within ITSs. This paper proposes using the Generative Adversarial\nImputation Networks (GAIN) framework to impute sparse learning performance\ndata, reconstructed into a three-dimensional (3D) tensor representation across\nthe dimensions of learners, questions and attempts. Our customized GAIN-based\nmethod computational process imputes sparse data in a 3D tensor space,\nsignificantly enhanced by convolutional neural networks for its input and\noutput layers. This adaptation also includes the use of a least squares loss\nfunction for optimization and aligns the shapes of the input and output with\nthe dimensions of the questions-attempts matrices along the learners'\ndimension. Through extensive experiments on six datasets from various ITSs,\nincluding AutoTutor, ASSISTments and MATHia, we demonstrate that the GAIN\napproach generally outperforms existing methods such as tensor factorization\nand other generative adversarial network (GAN) based approaches in terms of\nimputation accuracy. This finding enhances comprehensive learning data modeling\nand analytics in AI-based education.",
      "tldr_zh": "这篇论文提出使用 Generative Adversarial Imputation Networks (GAIN) 框架来填充智能辅导系统 (ITSs) 中的稀疏学习表现数据，这些数据被重构为三维张量（涵盖学习者、问题和尝试维度）。该方法通过卷积神经网络 (CNN) 增强输入输出层，并采用最小二乘损失函数优化，以提高填充准确性。实验在 AutoTutor、ASSISTments 和 MATHia 等六种数据集上表明，GAIN 整体优于张量分解和其他 GAN 基于方法，从而提升了 AI 教育中的学习数据建模和分析。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18875v2",
      "published_date": "2024-07-26 17:09:48 UTC",
      "updated_date": "2024-09-20 00:00:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:16:26.976028"
    },
    {
      "arxiv_id": "2407.18874v2",
      "title": "Engaging with Children's Artwork in Mixed Visual-Ability Families",
      "title_zh": "翻译失败",
      "authors": [
        "Arnavi Chheda-Kothary",
        "Jacob O. Wobbrock",
        "Jon E. Froehlich"
      ],
      "abstract": "We present two studies exploring how blind or low-vision (BLV) family members\nengage with their sighted children's artwork, strategies to support\nunderstanding and interpretation, and the potential role of technology, such as\nAI, therein. Our first study involved 14 BLV individuals, and the second\nincluded five groups of BLV individuals with their children. Through\nsemi-structured interviews with AI descriptions of children's artwork and\nmulti-sensory design probes, we found that BLV family members value artwork\nengagement as a bonding opportunity, preferring the child's storytelling and\ninterpretation over other nonvisual representations. Additionally, despite some\ninaccuracies, BLV family members felt that AI-generated descriptions could\nfacilitate dialogue with their children and aid self-guided art discovery. We\nclose with specific design considerations for supporting artwork engagement in\nmixed visual-ability families, including enabling artwork access through\nvarious methods, supporting children's corrections of AI output, and\ndistinctions in context vs. content and interpretation vs. description of\nchildren's artwork.",
      "tldr_zh": "本研究探讨了视力障碍（BLV）家庭成员如何参与视力正常的孩子艺术创作，包括理解和解释策略，以及AI等技术的作用。研究通过两个阶段进行：首先访谈14名BLV个体，其次观察五组BLV成员及其孩子，使用半结构化访谈、AI描述和多感官设计探针。结果显示，BLV家庭成员将艺术互动视为增进情感纽带的机会，更倾向于孩子的个人讲述而非其他非视觉表示，尽管AI生成的描述存在不准确性，但能促进对话和自我探索。最终，该研究提出设计建议，如通过多种方法提升艺术访问、支持孩子修正AI输出，并区分艺术的上下文与内容、解释与描述，以更好地支持混合视力能力家庭。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18874v2",
      "published_date": "2024-07-26 17:08:53 UTC",
      "updated_date": "2024-07-30 06:31:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:16:37.790733"
    },
    {
      "arxiv_id": "2407.18854v1",
      "title": "Unifying Visual and Semantic Feature Spaces with Diffusion Models for Enhanced Cross-Modal Alignment",
      "title_zh": "使用扩散模型统一视觉和语义特征空间以增强跨模态对齐",
      "authors": [
        "Yuze Zheng",
        "Zixuan Li",
        "Xiangxian Li",
        "Jinxing Liu",
        "Yuqing Wang",
        "Xiangxu Meng",
        "Lei Meng"
      ],
      "abstract": "Image classification models often demonstrate unstable performance in\nreal-world applications due to variations in image information, driven by\ndiffering visual perspectives of subject objects and lighting discrepancies. To\nmitigate these challenges, existing studies commonly incorporate additional\nmodal information matching the visual data to regularize the model's learning\nprocess, enabling the extraction of high-quality visual features from complex\nimage regions. Specifically, in the realm of multimodal learning, cross-modal\nalignment is recognized as an effective strategy, harmonizing different modal\ninformation by learning a domain-consistent latent feature space for visual and\nsemantic features. However, this approach may face limitations due to the\nheterogeneity between multimodal information, such as differences in feature\ndistribution and structure. To address this issue, we introduce a Multimodal\nAlignment and Reconstruction Network (MARNet), designed to enhance the model's\nresistance to visual noise. Importantly, MARNet includes a cross-modal\ndiffusion reconstruction module for smoothly and stably blending information\nacross different domains. Experiments conducted on two benchmark datasets,\nVireo-Food172 and Ingredient-101, demonstrate that MARNet effectively improves\nthe quality of image information extracted by the model. It is a plug-and-play\nframework that can be rapidly integrated into various image classification\nframeworks, boosting model performance.",
      "tldr_zh": "该论文针对图像分类模型在现实应用中因视觉角度和光照差异导致的性能不稳定性问题，提出了一种 Multimodal Alignment and Reconstruction Network (MARNet)，通过 Diffusion Models 统一视觉和语义特征空间，实现增强的 Cross-Modal Alignment。MARNet 包含一个跨模态扩散重建模块，用于平稳融合不同模态信息，缓解特征分布和结构异质性，提高模型对视觉噪声的抵抗力。在 Vireo-Food172 和 Ingredient-101 数据集上的实验表明，该框架显著提升了图像信息提取质量，并作为即插即用模块，能快速集成到各种图像分类框架中，提升整体性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18854v1",
      "published_date": "2024-07-26 16:30:18 UTC",
      "updated_date": "2024-07-26 16:30:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:16:51.120979"
    },
    {
      "arxiv_id": "2407.18848v1",
      "title": "Repairing Networks of $\\mathcal{EL_\\perp}$ Ontologies using Weakening and Completing -- Extended version",
      "title_zh": "翻译失败",
      "authors": [
        "Ying Li",
        "Patrick Lambrix"
      ],
      "abstract": "The quality of ontologies and their alignments is crucial for developing\nhigh-quality semantics-based applications. Traditional debugging techniques\nrepair ontology networks by removing unwanted axioms and mappings, but may\nthereby remove consequences that are correct in the domain of the ontology\nnetwork. In this paper we propose a framework for repairing ontology networks\nthat deals with this issue. It defines basic operations such as debugging,\nweakening and completing. Further, it defines combination operators that\nreflect choices in how and when to use the basic operators, as well as choices\nregarding the autonomy level of the ontologies and alignments in the ontology\nnetwork. We show the influence of the combination operators on the quality of\nthe repaired network and present an implemented tool. By using our framework\ntogether with existing algorithms for debugging, weakening and completing, we\nessentially provide a blueprint for extending previous work and systems.",
      "tldr_zh": "这篇论文提出了一种修复 $\\mathcal{EL_\\perp}$ Ontologies 网络的框架，使用弱化(weakening)和完成(completing)操作，以避免传统调试(debugging)技术意外移除域中正确的后果。框架定义了基本操作和组合操作，这些操作考虑了本体(ontologies)和对齐(alignments)的自治水平，并允许灵活选择操作方式。实验结果显示，这种方法提高了修复网络的质量，并通过实现工具为扩展现有算法和系统提供了蓝图。",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "This is a slightly revised and extended version of a paper published\n  at ISWC 2024. arXiv admin note: text overlap with arXiv:2208.00486",
      "pdf_url": "http://arxiv.org/pdf/2407.18848v1",
      "published_date": "2024-07-26 16:15:33 UTC",
      "updated_date": "2024-07-26 16:15:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:17:10.380159"
    },
    {
      "arxiv_id": "2407.18847v1",
      "title": "Enhancing material property prediction with ensemble deep graph convolutional networks",
      "title_zh": "使用集成深度图卷积神经网络增强材料属性预测",
      "authors": [
        "Chowdhury Mohammad Abid Rahman",
        "Ghadendra Bhandari",
        "Nasser M Nasrabadi",
        "Aldo H. Romero",
        "Prashnna K. Gyawali"
      ],
      "abstract": "Machine learning (ML) models have emerged as powerful tools for accelerating\nmaterials discovery and design by enabling accurate predictions of properties\nfrom compositional and structural data. These capabilities are vital for\ndeveloping advanced technologies across fields such as energy, electronics, and\nbiomedicine, potentially reducing the time and resources needed for new\nmaterial exploration and promoting rapid innovation cycles. Recent efforts have\nfocused on employing advanced ML algorithms, including deep learning - based\ngraph neural network, for property prediction. Additionally, ensemble models\nhave proven to enhance the generalizability and robustness of ML and DL.\nHowever, the use of such ensemble strategies in deep graph networks for\nmaterial property prediction remains underexplored. Our research provides an\nin-depth evaluation of ensemble strategies in deep learning - based graph\nneural network, specifically targeting material property prediction tasks. By\ntesting the Crystal Graph Convolutional Neural Network (CGCNN) and its\nmultitask version, MT-CGCNN, we demonstrated that ensemble techniques,\nespecially prediction averaging, substantially improve precision beyond\ntraditional metrics for key properties like formation energy per atom ($\\Delta\nE^{f}$), band gap ($E_{g}$) and density ($\\rho$) in 33,990 stable inorganic\nmaterials. These findings support the broader application of ensemble methods\nto enhance predictive accuracy in the field.",
      "tldr_zh": "本研究旨在通过集成深度图卷积网络提升材料属性预测的准确性，特别是在材料发现和设计领域。研究者评估了集成策略在 Crystal Graph Convolutional Neural Network (CGCNN) 和其多任务版本 MT-CGCNN 中的应用，重点测试预测平均等技术。结果显示，在 33,990 个稳定无机材料上，这些方法显著提高了关键属性的预测精度，包括 formation energy per atom ($\\Delta E^{f}$)、band gap ($E_{g}$) 和 density ($\\rho$)，从而支持集成方法在材料科学中的更广泛应用。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 6 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2407.18847v1",
      "published_date": "2024-07-26 16:12:06 UTC",
      "updated_date": "2024-07-26 16:12:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:17:26.926066"
    },
    {
      "arxiv_id": "2408.05337v1",
      "title": "VACoDe: Visual Augmented Contrastive Decoding",
      "title_zh": "VACoDe：视觉增强对比解码",
      "authors": [
        "Sihyeon Kim",
        "Boryeong Cho",
        "Sangmin Bae",
        "Sumyeong Ahn",
        "Se-Young Yun"
      ],
      "abstract": "Despite the astonishing performance of recent Large Vision-Language Models\n(LVLMs), these models often generate inaccurate responses. To address this\nissue, previous studies have focused on mitigating hallucinations by employing\ncontrastive decoding (CD) with augmented images, which amplifies the contrast\nwith the original image. However, these methods have limitations, including\nreliance on a single augmentation, which is restrictive for certain tasks, as\nwell as the high cost of using external knowledge. In this study, we address\nthese limitations by exploring how to utilize multiple image augmentations.\nThrough extensive experiments, we observed that different augmentations produce\nvarying levels of contrast depending on the task. Based on this observation, we\nintroduce a novel method called VACoDe, Visual Augmented Contrastive Decoding.\nThis method adaptively selects the augmentation with the highest contrast for\neach task using the proposed softmax distance metric. Our empirical tests show\nthat \\alg outperforms previous methods and improves output quality in various\nvision-language tasks. Additionally, VACoDe can be universally applied across\ndifferent model types and sizes without additional training or the use of\nexternal models and data.",
      "tldr_zh": "本研究针对 Large Vision-Language Models (LVLMs) 常产生的幻觉问题，分析了现有对比解码 (CD) 方法的局限性，如依赖单一图像增强和高计算成本。论文提出 VACoDe（Visual Augmented Contrastive Decoding）新方法，通过利用多个图像增强并采用 softmax distance metric 自适应选择最高对比度的增强，从而提升模型输出质量。实验结果显示，VACoDe 在各种视觉语言任务中优于先前方法，且无需额外训练即可适用于不同模型类型和大小。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "68T01",
        "I.2.0"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2408.05337v1",
      "published_date": "2024-07-26 15:49:31 UTC",
      "updated_date": "2024-07-26 15:49:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:17:26.125764"
    },
    {
      "arxiv_id": "2407.18827v1",
      "title": "Human-artificial intelligence teaming for scientific information extraction from data-driven additive manufacturing research using large language models",
      "title_zh": "翻译失败",
      "authors": [
        "Mutahar Safdar",
        "Jiarui Xie",
        "Andrei Mircea",
        "Yaoyao Fiona Zhao"
      ],
      "abstract": "Data-driven research in Additive Manufacturing (AM) has gained significant\nsuccess in recent years. This has led to a plethora of scientific literature to\nemerge. The knowledge in these works consists of AM and Artificial Intelligence\n(AI) contexts that have not been mined and formalized in an integrated way. It\nrequires substantial effort and time to extract scientific information from\nthese works. AM domain experts have contributed over two dozen review papers to\nsummarize these works. However, information specific to AM and AI contexts\nstill requires manual effort to extract. The recent success of foundation\nmodels such as BERT (Bidirectional Encoder Representations for Transformers) or\nGPT (Generative Pre-trained Transformers) on textual data has opened the\npossibility of expediting scientific information extraction. We propose a\nframework that enables collaboration between AM and AI experts to continuously\nextract scientific information from data-driven AM literature. A demonstration\ntool is implemented based on the proposed framework and a case study is\nconducted to extract information relevant to the datasets, modeling, sensing,\nand AM system categories. We show the ability of LLMs (Large Language Models)\nto expedite the extraction of relevant information from data-driven AM\nliterature. In the future, the framework can be used to extract information\nfrom the broader design and manufacturing literature in the engineering\ndiscipline.",
      "tldr_zh": "该研究探讨了从数据驱动增材制造（Additive Manufacturing, AM）文献中提取科学信息的问题，强调了手动提取的低效性。作者提出一个框架，结合人类专家（AM和AI领域）和大型语言模型（Large Language Models, LLMs）如BERT或GPT，实现AM与AI上下文的协作式信息提取。该框架通过一个演示工具和案例研究，展示了在数据集、建模、传感和AM系统类别中加速信息提取的能力，并证明LLMs能显著提高效率。未来，该框架可扩展到更广泛的工程设计和制造文献中。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "11 pages, 5 Figures, 3 Tables. This paper has been accepted to be\n  published in the proceedings of IDETC-CIE 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.18827v1",
      "published_date": "2024-07-26 15:43:52 UTC",
      "updated_date": "2024-07-26 15:43:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:17:32.977191"
    },
    {
      "arxiv_id": "2407.20181v2",
      "title": "Blockchain for Large Language Model Security and Safety: A Holistic Survey",
      "title_zh": "翻译失败",
      "authors": [
        "Caleb Geren",
        "Amanda Board",
        "Gaby G. Dagher",
        "Tim Andersen",
        "Jun Zhuang"
      ],
      "abstract": "With the growing development and deployment of large language models (LLMs)\nin both industrial and academic fields, their security and safety concerns have\nbecome increasingly critical. However, recent studies indicate that LLMs face\nnumerous vulnerabilities, including data poisoning, prompt injections, and\nunauthorized data exposure, which conventional methods have struggled to\naddress fully. In parallel, blockchain technology, known for its data\nimmutability and decentralized structure, offers a promising foundation for\nsafeguarding LLMs. In this survey, we aim to comprehensively assess how to\nleverage blockchain technology to enhance LLMs' security and safety. Besides,\nwe propose a new taxonomy of blockchain for large language models (BC4LLMs) to\nsystematically categorize related works in this emerging field. Our analysis\nincludes novel frameworks and definitions to delineate security and safety in\nthe context of BC4LLMs, highlighting potential research directions and\nchallenges at this intersection. Through this study, we aim to stimulate\ntargeted advancements in blockchain-integrated LLM security.",
      "tldr_zh": "这篇综述探讨了区块链技术如何提升大型语言模型（LLMs）的安全性和安全性，针对LLMs面临的漏洞如数据中毒、提示注入和数据暴露等问题。作者提出一个新的分类法（BC4LLMs），系统化归类相关研究，并提供新框架和定义来分析区块链在LLMs中的应用。最终，该研究突出了潜在研究方向和挑战，旨在推动区块链与LLMs安全领域的创新发展。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.DC",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted to SIGKDD Explorations, to appear Dec 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.20181v2",
      "published_date": "2024-07-26 15:24:01 UTC",
      "updated_date": "2024-11-17 22:23:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:17:49.838975"
    },
    {
      "arxiv_id": "2407.18812v1",
      "title": "Online Planning in POMDPs with State-Requests",
      "title_zh": "翻译失败",
      "authors": [
        "Raphael Avalos",
        "Eugenio Bargiacchi",
        "Ann Nowé",
        "Diederik M. Roijers",
        "Frans A. Oliehoek"
      ],
      "abstract": "In key real-world problems, full state information is sometimes available but\nonly at a high cost, like activating precise yet energy-intensive sensors or\nconsulting humans, thereby compelling the agent to operate under partial\nobservability. For this scenario, we propose AEMS-SR (Anytime Error\nMinimization Search with State Requests), a principled online planning\nalgorithm tailored for POMDPs with state requests. By representing the search\nspace as a graph instead of a tree, AEMS-SR avoids the exponential growth of\nthe search space originating from state requests. Theoretical analysis\ndemonstrates AEMS-SR's $\\varepsilon$-optimality, ensuring solution quality,\nwhile empirical evaluations illustrate its effectiveness compared with AEMS and\nPOMCP, two SOTA online planning algorithms. AEMS-SR enables efficient planning\nin domains characterized by partial observability and costly state requests\noffering practical benefits across various applications.",
      "tldr_zh": "该论文探讨了在部分可观察马尔可夫决策过程（POMDPs）中，当完整状态信息可用但成本高昂（如激活高能耗传感器或咨询人类）时，代理如何进行在线规划。为解决这一问题，研究提出了一种在线规划算法AEMS-SR（Anytime Error Minimization Search with State Requests），它通过将搜索空间表示为图而非树，避免了状态请求导致的指数级增长。理论分析证明了AEMS-SR的ε-optimality，确保了解决方案的质量；实证评估显示，该算法在部分可观察性和高成本状态请求场景中比AEMS和POMCP等最先进算法更有效，从而为实际应用提供了实用益处。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18812v1",
      "published_date": "2024-07-26 15:20:50 UTC",
      "updated_date": "2024-07-26 15:20:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:18:17.502884"
    },
    {
      "arxiv_id": "2407.18808v1",
      "title": "Learning Chaotic Systems and Long-Term Predictions with Neural Jump ODEs",
      "title_zh": "翻译失败",
      "authors": [
        "Florian Krach",
        "Josef Teichmann"
      ],
      "abstract": "The Path-dependent Neural Jump ODE (PD-NJ-ODE) is a model for online\nprediction of generic (possibly non-Markovian) stochastic processes with\nirregular (in time) and potentially incomplete (with respect to coordinates)\nobservations. It is a model for which convergence to the $L^2$-optimal\npredictor, which is given by the conditional expectation, is established\ntheoretically. Thereby, the training of the model is solely based on a dataset\nof realizations of the underlying stochastic process, without the need of\nknowledge of the law of the process. In the case where the underlying process\nis deterministic, the conditional expectation coincides with the process\nitself. Therefore, this framework can equivalently be used to learn the\ndynamics of ODE or PDE systems solely from realizations of the dynamical system\nwith different initial conditions. We showcase the potential of our method by\napplying it to the chaotic system of a double pendulum. When training the\nstandard PD-NJ-ODE method, we see that the prediction starts to diverge from\nthe true path after about half of the evaluation time. In this work we enhance\nthe model with two novel ideas, which independently of each other improve the\nperformance of our modelling setup. The resulting dynamics match the true\ndynamics of the chaotic system very closely. The same enhancements can be used\nto provably enable the PD-NJ-ODE to learn long-term predictions for general\nstochastic datasets, where the standard model fails. This is verified in\nseveral experiments.",
      "tldr_zh": "该论文提出了一种Path-dependent Neural Jump ODE (PD-NJ-ODE)模型，用于在线预测通用随机过程，包括非马尔可夫过程，并基于不规则或不完整观察数据实现对L^2最优预测器的收敛。模型仅需随机过程的实现数据即可训练，无需知道过程的分布，且可扩展到学习ODE或PDE系统的动态。作者引入两个新颖增强方法，显著改善了模型在混沌系统（如双摆）上的长期预测性能，使预测路径更接近真实动态，并在多个实验中验证了其有效性。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "math.DS",
        "math.PR"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18808v1",
      "published_date": "2024-07-26 15:18:29 UTC",
      "updated_date": "2024-07-26 15:18:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:18:17.526269"
    },
    {
      "arxiv_id": "2407.18807v3",
      "title": "When narrower is better: the narrow width limit of Bayesian parallel branching neural networks",
      "title_zh": "翻译失败",
      "authors": [
        "Zechen Zhang",
        "Haim Sompolinsky"
      ],
      "abstract": "The infinite width limit of random neural networks is known to result in\nNeural Networks as Gaussian Process (NNGP) (Lee et al. (2018)), characterized\nby task-independent kernels. It is widely accepted that larger network widths\ncontribute to improved generalization (Park et al. (2019)). However, this work\nchallenges this notion by investigating the narrow width limit of the Bayesian\nParallel Branching Neural Network (BPB-NN), an architecture that resembles\nneural networks with residual blocks. We demonstrate that when the width of a\nBPB-NN is significantly smaller compared to the number of training examples,\neach branch exhibits more robust learning due to a symmetry breaking of\nbranches in kernel renormalization. Surprisingly, the performance of a BPB-NN\nin the narrow width limit is generally superior to or comparable to that\nachieved in the wide width limit in bias-limited scenarios. Furthermore, the\nreadout norms of each branch in the narrow width limit are mostly independent\nof the architectural hyperparameters but generally reflective of the nature of\nthe data. We demonstrate such phenomenon primarily in the branching graph\nneural networks, where each branch represents a different order of convolutions\nof the graph; we also extend the results to other more general architectures\nsuch as the residual-MLP and demonstrate that the narrow width effect is a\ngeneral feature of the branching networks. Our results characterize a newly\ndefined narrow-width regime for parallel branching networks in general.",
      "tldr_zh": "这篇论文挑战了神经网络宽度越大泛化越好的传统观点，聚焦于Bayesian Parallel Branching Neural Network (BPB-NN)，一种类似于残差块架构的模型。研究发现，当BPB-NN的宽度远小于训练样本数时，分支间的对称性破坏会导致内核重整，从而增强每个分支的学习能力，并在偏差有限场景中使性能优于或相当于是无限宽度极限。作者进一步证明，这种窄宽度效应在分支图神经网络中显著，并扩展到其他架构如残差-MLP，定义了一个新的窄宽度制度。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18807v3",
      "published_date": "2024-07-26 15:14:22 UTC",
      "updated_date": "2025-03-10 15:00:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:18:42.664639"
    },
    {
      "arxiv_id": "2407.18998v2",
      "title": "Towards a Cyber Information Ontology",
      "title_zh": "迈向网络信息本体",
      "authors": [
        "David Limbaugh",
        "Mark Jensen",
        "John Beverley"
      ],
      "abstract": "This paper introduces a set of terms that are intended to act as an interface\nbetween cyber ontologies (like a file system ontology or a data fusion\nontology) and top- and mid-level ontologies, specifically Basic Formal Ontology\nand the Common Core Ontologies. These terms center on what makes\ncyberinformation management unique: numerous acts of copying items of\ninformation, the aggregates of copies that result from those acts, and the\nfaithful members of those aggregates that represent all other members.",
      "tldr_zh": "这篇论文提出了一组术语，旨在作为网络本体（如file system ontology或data fusion ontology）和高层及中层本体（如Basic Formal Ontology和Common Core Ontologies）之间的接口。重点聚焦于网络信息管理的独特特性，包括多次复制信息项、这些复制产生的聚合体，以及聚合体中代表其他成员的忠实成员。该术语集有助于桥接不同本体，促进更有效的cyber信息管理。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "14",
      "pdf_url": "http://arxiv.org/pdf/2407.18998v2",
      "published_date": "2024-07-26 14:59:00 UTC",
      "updated_date": "2024-08-16 03:21:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:18:47.602565"
    },
    {
      "arxiv_id": "2407.18782v1",
      "title": "Understanding XAI Through the Philosopher's Lens: A Historical Perspective",
      "title_zh": "翻译失败",
      "authors": [
        "Martina Mattioli",
        "Antonio Emanuele Cinà",
        "Marcello Pelillo"
      ],
      "abstract": "Despite explainable AI (XAI) has recently become a hot topic and several\ndifferent approaches have been developed, there is still a widespread belief\nthat it lacks a convincing unifying foundation. On the other hand, over the\npast centuries, the very concept of explanation has been the subject of\nextensive philosophical analysis in an attempt to address the fundamental\nquestion of \"why\" in the context of scientific law. However, this discussion\nhas rarely been connected with XAI. This paper tries to fill in this gap and\naims to explore the concept of explanation in AI through an epistemological\nlens. By comparing the historical development of both the philosophy of science\nand AI, an intriguing picture emerges. Specifically, we show that a gradual\nprogression has independently occurred in both domains from logical-deductive\nto statistical models of explanation, thereby experiencing in both cases a\nparadigm shift from deterministic to nondeterministic and probabilistic\ncausality. Interestingly, we also notice that similar concepts have\nindependently emerged in both realms such as, for example, the relation between\nexplanation and understanding and the importance of pragmatic factors. Our\nstudy aims to be the first step towards understanding the philosophical\nunderpinnings of the notion of explanation in AI, and we hope that our findings\nwill shed some fresh light on the elusive nature of XAI.",
      "tldr_zh": "本文从认识论视角审视可解释AI（XAI），通过比较哲学科学和AI的历史发展，探讨解释概念的演变。论文指出，两个领域均经历了从逻辑演绎到统计模型的转变，以及从确定性到非确定性和概率因果的范式转移，同时发现类似概念（如解释与理解的关系和实用因素）在两者中独立出现。主要贡献在于为XAI提供哲学基础，推动对AI解释本质的更深入理解。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages",
      "pdf_url": "http://arxiv.org/pdf/2407.18782v1",
      "published_date": "2024-07-26 14:44:49 UTC",
      "updated_date": "2024-07-26 14:44:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:21:39.124643"
    },
    {
      "arxiv_id": "2407.18770v1",
      "title": "Any four real numbers are on all fours with analogy",
      "title_zh": "翻译失败",
      "authors": [
        "Yves Lepage",
        "Miguel Couceiro"
      ],
      "abstract": "This work presents a formalization of analogy on numbers that relies on\ngeneralized means. It is motivated by recent advances in artificial\nintelligence and applications of machine learning, where the notion of analogy\nis used to infer results, create data and even as an assessment tool of object\nrepresentations, or embeddings, that are basically collections of numbers\n(vectors, matrices, tensors). This extended analogy use asks for mathematical\nfoundations and clear understanding of the notion of analogy between numbers.\nWe propose a unifying view of analogies that relies on generalized means\ndefined in terms of a power parameter. In particular, we show that any four\nincreasing positive real numbers is an analogy in a unique suitable power. In\naddition, we show that any such analogy can be reduced to an equivalent\narithmetic analogy and that any analogical equation has a solution for\nincreasing numbers, which generalizes without restriction to complex numbers.\nThese foundational results provide a better understanding of analogies in areas\nwhere representations are numerical.",
      "tldr_zh": "这篇论文基于广义均值（generalized means）正式化了数字间的类比概念，旨在为人工智能和机器学习中类比的应用提供数学基础，例如推断结果、创建数据或评估对象表示（如向量、矩阵、张量）。论文提出一个依赖功率参数（power parameter）的统一视图，证明任何四个递增的正实数在唯一合适的幂下构成类比。研究进一步显示，这些类比可以简化为等价的算术类比（arithmetic analogy），且任何类比方程在递增数字下都有解，并扩展到复数。这些发现为数字表示中的类比提供了更清晰的理解。",
      "categories": [
        "cs.AI",
        "68Txx"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18770v1",
      "published_date": "2024-07-26 14:30:35 UTC",
      "updated_date": "2024-07-26 14:30:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:19:12.929972"
    },
    {
      "arxiv_id": "2407.18764v2",
      "title": "TAGIFY: LLM-powered Tagging Interface for Improved Data Findability on OGD portals",
      "title_zh": "翻译失败",
      "authors": [
        "Kevin Kliimask",
        "Anastasija Nikiforova"
      ],
      "abstract": "Efforts directed towards promoting Open Government Data (OGD) have gained\nsignificant traction across various governmental tiers since the mid-2000s. As\nmore datasets are published on OGD portals, finding specific data becomes\nharder, leading to information overload. Complete and accurate documentation of\ndatasets, including association of proper tags with datasets is key to\nimproving dataset findability and accessibility. Analysis conducted on the\nEstonian Open Data Portal, revealed that 11% datasets have no associated tags,\nwhile 26% had only one tag assigned to them, which underscores challenges in\ndata findability and accessibility within the portal, which, according to the\nrecent Open Data Maturity Report, is considered trend-setter. The aim of this\nstudy is to propose an automated solution to tagging datasets to improve data\nfindability on OGD portals. This paper presents Tagify - a prototype of tagging\ninterface that employs large language models (LLM) such as GPT-3.5-turbo and\nGPT-4 to automate dataset tagging, generating tags for datasets in English and\nEstonian, thereby augmenting metadata preparation by data publishers and\nimproving data findability on OGD portals by data users. The developed solution\nwas evaluated by users and their feedback was collected to define an agenda for\nfuture prototype improvements.",
      "tldr_zh": "这篇论文针对 Open Government Data (OGD) 门户的数据可发现性问题，分析了爱沙尼亚门户中11%的数据集无标签且26%仅有一标签，导致信息过载。研究提出Tagify接口，这是一个基于LLM（如GPT-3.5-turbo和GPT-4）的原型系统，能自动生成英语和爱沙尼亚语标签，从而增强数据集的元数据准备和可访问性。通过用户评估，该系统证明了其有效性，并为未来改进提供了反馈方向。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.ET",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18764v2",
      "published_date": "2024-07-26 14:22:30 UTC",
      "updated_date": "2024-08-21 12:23:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:19:19.248333"
    },
    {
      "arxiv_id": "2407.18756v1",
      "title": "Evaluating Human Trajectory Prediction with Metamorphic Testing",
      "title_zh": "翻译失败",
      "authors": [
        "Helge Spieker",
        "Nassim Belmecheri",
        "Arnaud Gotlieb",
        "Nadjib Lazaar"
      ],
      "abstract": "The prediction of human trajectories is important for planning in autonomous\nsystems that act in the real world, e.g. automated driving or mobile robots.\nHuman trajectory prediction is a noisy process, and no prediction does\nprecisely match any future trajectory. It is therefore approached as a\nstochastic problem, where the goal is to minimise the error between the true\nand the predicted trajectory. In this work, we explore the application of\nmetamorphic testing for human trajectory prediction. Metamorphic testing is\ndesigned to handle unclear or missing test oracles. It is well-designed for\nhuman trajectory prediction, where there is no clear criterion of correct or\nincorrect human behaviour. Metamorphic relations rely on transformations over\nsource test cases and exploit invariants. A setting well-designed for human\ntrajectory prediction where there are many symmetries of expected human\nbehaviour under variations of the input, e.g. mirroring and rescaling of the\ninput data. We discuss how metamorphic testing can be applied to stochastic\nhuman trajectory prediction and introduce the Wasserstein Violation Criterion\nto statistically assess whether a follow-up test case violates a\nlabel-preserving metamorphic relation.",
      "tldr_zh": "这篇论文探讨了使用 Metamorphic Testing 来评估人类轨迹预测的可靠性，尤其针对自主系统（如自动驾驶或移动机器人）的规划需求。论文指出，人类轨迹预测是一个随机过程，缺乏明确正确标准的测试预言，因此引入 Metamorphic Testing，通过源测试案例的变换（如镜像和缩放）来利用行为不变性。最终，论文提出 Wasserstein Violation Criterion 作为统计方法，用于评估后续测试案例是否违反标签保留的 Metamorphic Relations，从而提升预测模型的鲁棒性。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "MET'24: 9th ACM International Workshop on Metamorphic Testing",
      "pdf_url": "http://arxiv.org/pdf/2407.18756v1",
      "published_date": "2024-07-26 14:10:14 UTC",
      "updated_date": "2024-07-26 14:10:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:19:38.000628"
    },
    {
      "arxiv_id": "2407.18755v2",
      "title": "Score matching through the roof: linear, nonlinear, and latent variables causal discovery",
      "title_zh": "翻译失败",
      "authors": [
        "Francesco Montagna",
        "Philipp M. Faller",
        "Patrick Bloebaum",
        "Elke Kirschbaum",
        "Francesco Locatello"
      ],
      "abstract": "Causal discovery from observational data holds great promise, but existing\nmethods rely on strong assumptions about the underlying causal structure, often\nrequiring full observability of all relevant variables. We tackle these\nchallenges by leveraging the score function $\\nabla \\log p(X)$ of observed\nvariables for causal discovery and propose the following contributions. First,\nwe fine-tune the existing identifiability results with the score on additive\nnoise models, showing that their assumption of nonlinearity of the causal\nmechanisms is not necessary. Second, we establish conditions for inferring\ncausal relations from the score even in the presence of hidden variables; this\nresult is two-faced: we demonstrate the score's potential to infer the\nequivalence class of causal graphs with hidden variables (while previous\nresults are restricted to the fully observable setting), and we provide\nsufficient conditions for identifying direct causes in latent variable models.\nBuilding on these insights, we propose a flexible algorithm suited for causal\ndiscovery on linear, nonlinear, and latent variable models, which we\nempirically validate.",
      "tldr_zh": "本研究针对从观测数据中进行因果发现的挑战，提出利用score function（∇ log p(X)）的方法，以克服现有方法的强假设，如完全可观测变量的需求。主要贡献包括：微调additive noise models的可识别性结果，证明因果机制的非线性并非必要；并建立条件以从score推断隐藏变量下的因果关系，包括识别causal graphs的等价类和直接原因。基于这些见解，作者开发了一个灵活的算法，适用于linear、非linear和latent variable models，并通过实证验证证明其有效性。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "stat.ME"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18755v2",
      "published_date": "2024-07-26 14:09:06 UTC",
      "updated_date": "2025-03-22 11:26:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:19:53.979123"
    },
    {
      "arxiv_id": "2407.18752v3",
      "title": "Knowledge Graph Structure as Prompt: Improving Small Language Models Capabilities for Knowledge-based Causal Discovery",
      "title_zh": "知识图谱结构作为提示：改善小型语言模型在基于知识的因果发现中的能力",
      "authors": [
        "Yuni Susanti",
        "Michael Färber"
      ],
      "abstract": "Causal discovery aims to estimate causal structures among variables based on\nobservational data. Large Language Models (LLMs) offer a fresh perspective to\ntackle the causal discovery problem by reasoning on the metadata associated\nwith variables rather than their actual data values, an approach referred to as\nknowledge-based causal discovery. In this paper, we investigate the\ncapabilities of Small Language Models (SLMs, defined as LLMs with fewer than 1\nbillion parameters) with prompt-based learning for knowledge-based causal\ndiscovery. Specifically, we present KG Structure as Prompt, a novel approach\nfor integrating structural information from a knowledge graph, such as common\nneighbor nodes and metapaths, into prompt-based learning to enhance the\ncapabilities of SLMs. Experimental results on three types of biomedical and\nopen-domain datasets under few-shot settings demonstrate the effectiveness of\nour approach, surpassing most baselines and even conventional fine-tuning\napproaches trained on full datasets. Our findings further highlight the strong\ncapabilities of SLMs: in combination with knowledge graphs and prompt-based\nlearning, SLMs demonstrate the potential to surpass LLMs with larger number of\nparameters. Our code and datasets are available on GitHub.",
      "tldr_zh": "本文探讨了 Small Language Models (SLMs) 在知识-based Causal Discovery 中的能力，提出了一种新方法：KG Structure as Prompt，将知识图谱的结构信息（如共同邻居节点和元路径）整合到 Prompt-based Learning 中，以提升 SLMs 的性能。实验在少样本设置下，使用三种生物医学和开放域数据集，结果显示该方法超过了大多数基线模型，甚至优于在完整数据集上训练的传统微调方法。研究发现，SLMs 通过结合知识图谱和提示学习，能够超越参数更大的 Large Language Models (LLMs)，证明了其在因果发现任务中的潜力。该方法的代码和数据集已在 GitHub 上公开。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "accepted at ISWC'24",
      "pdf_url": "http://arxiv.org/pdf/2407.18752v3",
      "published_date": "2024-07-26 14:07:00 UTC",
      "updated_date": "2024-07-30 12:05:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:19:51.938947"
    },
    {
      "arxiv_id": "2407.18749v1",
      "title": "Multi-Robot System Architecture design in SysML and BPMN",
      "title_zh": "多机器人系统架构设计于 SysML 和 BPMN",
      "authors": [
        "Ahmed R. Sadik",
        "Christian Goerick"
      ],
      "abstract": "Multi-Robot System (MRS) is a complex system that contains many different\nsoftware and hardware components. This main problem addressed in this article\nis the MRS design complexity. The proposed solution provides a modular modeling\nand simulation technique that is based on formal system engineering method,\ntherefore the MRS design complexity is decomposed and reduced. Modeling the MRS\nhas been achieved via two formal Architecture Description Languages (ADLs),\nwhich are Systems Modeling Language (SysML) and Business Process Model and\nNotation (BPMN), to design the system blueprints. By using those abstract\ndesign ADLs, the implementation of the project becomes technology agnostic.\nThis allows to transfer the design concept from on programming language to\nanother. During the simulation phase, a multi-agent environment is used to\nsimulate the MRS blueprints. The simulation has been implemented in Java Agent\nDevelopment (JADE) middleware. Therefore, its results can be used to analysis\nand verify the proposed MRS model in form of performance evaluation matrix.",
      "tldr_zh": "本文针对多机器人系统 (MRS) 的设计复杂性，提出了一种模块化建模和模拟技术，基于正式的系统工程方法来分解和降低复杂度。该方法利用 Systems Modeling Language (SysML) 和 Business Process Model and Notation (BPMN) 作为架构描述语言 (ADLs)，创建技术无关的系统蓝图，便于在不同编程语言之间转移设计概念。通过 Java Agent Development (JADE) 中间件进行多智能体模拟，生成的性能评估指标可用于分析和验证 MRS 模型的有效性。",
      "categories": [
        "cs.AI",
        "cs.RO",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18749v1",
      "published_date": "2024-07-26 14:04:40 UTC",
      "updated_date": "2024-07-26 14:04:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:22:02.589007"
    },
    {
      "arxiv_id": "2407.18738v1",
      "title": "Towards Generalized Offensive Language Identification",
      "title_zh": "翻译失败",
      "authors": [
        "Alphaeus Dmonte",
        "Tejas Arya",
        "Tharindu Ranasinghe",
        "Marcos Zampieri"
      ],
      "abstract": "The prevalence of offensive content on the internet, encompassing hate speech\nand cyberbullying, is a pervasive issue worldwide. Consequently, it has\ngarnered significant attention from the machine learning (ML) and natural\nlanguage processing (NLP) communities. As a result, numerous systems have been\ndeveloped to automatically identify potentially harmful content and mitigate\nits impact. These systems can follow two approaches; (1) Use publicly available\nmodels and application endpoints, including prompting large language models\n(LLMs) (2) Annotate datasets and train ML models on them. However, both\napproaches lack an understanding of how generalizable they are. Furthermore,\nthe applicability of these systems is often questioned in off-domain and\npractical environments. This paper empirically evaluates the generalizability\nof offensive language detection models and datasets across a novel generalized\nbenchmark. We answer three research questions on generalizability. Our findings\nwill be useful in creating robust real-world offensive language detection\nsystems.",
      "tldr_zh": "这篇论文探讨了互联网上冒犯性内容（如仇恨言论和网络欺凌）的泛化识别问题，评估了现有 machine learning (ML) 和 natural language processing (NLP) 系统在不同领域的适用性。作者指出现有方法，包括使用 large language models (LLMs) 提示或训练数据集，都缺乏泛化性验证，因此在实际环境中表现存疑。论文通过一个新的泛化基准进行实证评估，回答了三个关键研究问题，并为构建鲁棒的真实世界冒犯性语言检测系统提供了重要见解。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to ASONAM 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.18738v1",
      "published_date": "2024-07-26 13:50:22 UTC",
      "updated_date": "2024-07-26 13:50:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:22:13.952434"
    },
    {
      "arxiv_id": "2407.18735v1",
      "title": "AutoRDF2GML: Facilitating RDF Integration in Graph Machine Learning",
      "title_zh": "AutoRDF2GML：促进 RDF",
      "authors": [
        "Michael Färber",
        "David Lamprecht",
        "Yuni Susanti"
      ],
      "abstract": "In this paper, we introduce AutoRDF2GML, a framework designed to convert RDF\ndata into data representations tailored for graph machine learning tasks.\nAutoRDF2GML enables, for the first time, the creation of both content-based\nfeatures -- i.e., features based on RDF datatype properties -- and\ntopology-based features -- i.e., features based on RDF object properties.\nCharacterized by automated feature extraction, AutoRDF2GML makes it possible\neven for users less familiar with RDF and SPARQL to generate data\nrepresentations ready for graph machine learning tasks, such as link\nprediction, node classification, and graph classification. Furthermore, we\npresent four new benchmark datasets for graph machine learning, created from\nlarge RDF knowledge graphs using our framework. These datasets serve as\nvaluable resources for evaluating graph machine learning approaches, such as\ngraph neural networks. Overall, our framework effectively bridges the gap\nbetween the Graph Machine Learning and Semantic Web communities, paving the way\nfor RDF-based machine learning applications.",
      "tldr_zh": "本研究引入了 AutoRDF2GML 框架，用于将 RDF 数据转换为适合图机器学习任务的表示形式，首次实现了基于 RDF datatype properties 的内容特征和基于 RDF object properties 的拓扑特征的自动提取。该框架通过自动化流程简化了特征生成过程，使不熟悉 RDF 和 SPARQL 的用户也能轻松准备数据，用于任务如链接预测、节点分类和图分类。研究者还创建了四个新的基准数据集，从大型 RDF 知识图谱中衍生而来，用于评估图神经网络等方法。总体上，AutoRDF2GML 桥接了图机器学习和语义网社区，促进了 RDF 基础的机器学习应用的发展。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.LG",
      "comment": "accepted at ISWC'24",
      "pdf_url": "http://arxiv.org/pdf/2407.18735v1",
      "published_date": "2024-07-26 13:44:06 UTC",
      "updated_date": "2024-07-26 13:44:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:22:14.731716"
    },
    {
      "arxiv_id": "2407.18996v1",
      "title": "A maturity framework for data driven maintenance",
      "title_zh": "数据驱动维护的成熟度框架",
      "authors": [
        "Chris Rijsdijk",
        "Mike van de Wijnckel",
        "Tiedo Tinga"
      ],
      "abstract": "Maintenance decisions range from the simple detection of faults to ultimately\npredicting future failures and solving the problem. These traditionally human\ndecisions are nowadays increasingly supported by data and the ultimate aim is\nto make them autonomous. This paper explores the challenges encountered in data\ndriven maintenance, and proposes to consider four aspects in a maturity\nframework: data / decision maturity, the translation from the real world to\ndata, the computability of decisions (using models) and the causality in the\nobtained relations. After a discussion of the theoretical concepts involved,\nthe exploration continues by considering a practical fault detection and\nidentification problem. Two approaches, i.e. experience based and model based,\nare compared and discussed in terms of the four aspects in the maturity\nframework. It is observed that both approaches yield the same decisions, but\nstill differ in the assignment of causality. This confirms that a maturity\nassessment not only concerns the type of decision, but should also include the\nother proposed aspects.",
      "tldr_zh": "这篇论文提出一个成熟度框架，用于评估数据驱动维护（data driven maintenance）的成熟水平，以支持从故障检测到预测未来故障的自治决策。该框架考虑四个关键方面：数据/决策成熟度、从现实世界到数据的转换、决策的可计算性（使用模型）和因果关系。作者首先讨论了这些理论概念，然后通过一个实际的故障检测和识别问题，比较了基于经验（experience based）和基于模型（model based）的两种方法。结果显示，两者虽能得出相同决策，但因果关系的分配存在差异，强调成熟度评估应涵盖所有方面，而非仅限于决策类型。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY",
        "E.1; F.2; G.3; I.2.8; I.6.4; J.6"
      ],
      "primary_category": "cs.AI",
      "comment": "in Proceedings of the 8th European Conference of the Prognostics and\n  Health Management Society 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.18996v1",
      "published_date": "2024-07-26 13:20:58 UTC",
      "updated_date": "2024-07-26 13:20:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:22:27.905863"
    },
    {
      "arxiv_id": "2407.18722v1",
      "title": "Neurosymbolic AI for Enhancing Instructability in Generative AI",
      "title_zh": "翻译失败",
      "authors": [
        "Amit Sheth",
        "Vishal Pallagani",
        "Kaushik Roy"
      ],
      "abstract": "Generative AI, especially via Large Language Models (LLMs), has transformed\ncontent creation across text, images, and music, showcasing capabilities in\nfollowing instructions through prompting, largely facilitated by instruction\ntuning. Instruction tuning is a supervised fine-tuning method where LLMs are\ntrained on datasets formatted with specific tasks and corresponding\ninstructions. This method systematically enhances the model's ability to\ncomprehend and execute the provided directives. Despite these advancements,\nLLMs still face challenges in consistently interpreting complex, multi-step\ninstructions and generalizing them to novel tasks, which are essential for\nbroader applicability in real-world scenarios. This article explores why\nneurosymbolic AI offers a better path to enhance the instructability of LLMs.\nWe explore the use a symbolic task planner to decompose high-level instructions\ninto structured tasks, a neural semantic parser to ground these tasks into\nexecutable actions, and a neuro-symbolic executor to implement these actions\nwhile dynamically maintaining an explicit representation of state. We also seek\nto show that neurosymbolic approach enhances the reliability and\ncontext-awareness of task execution, enabling LLMs to dynamically interpret and\nrespond to a wider range of instructional contexts with greater precision and\nflexibility.",
      "tldr_zh": "这篇论文探讨了如何通过Neurosymbolic AI提升生成式AI，特别是Large Language Models (LLMs)的指令性（instructability），以解决LLMs在处理复杂多步指令和任务泛化方面的挑战。作者提出一种方法，使用symbolic task planner分解高层指令为结构化任务，neural semantic parser将这些任务转化为可执行动作，以及neuro-symbolic executor来执行动作并动态维护状态表示。这种神经符号方法显著提高了任务执行的可靠性和上下文感知能力，使LLMs能够更精确和灵活地适应各种指令场景。",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18722v1",
      "published_date": "2024-07-26 13:15:50 UTC",
      "updated_date": "2024-07-26 13:15:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:22:49.218518"
    },
    {
      "arxiv_id": "2407.18712v2",
      "title": "Cluster-norm for Unsupervised Probing of Knowledge",
      "title_zh": "翻译失败",
      "authors": [
        "Walter Laurito",
        "Sharan Maiya",
        "Grégoire Dhimoïla",
        "Owen",
        "Yeung",
        "Kaarel Hänni"
      ],
      "abstract": "The deployment of language models brings challenges in generating reliable\ninformation, especially when these models are fine-tuned using human\npreferences. To extract encoded knowledge without (potentially) biased human\nlabels, unsupervised probing techniques like Contrast-Consistent Search (CCS)\nhave been developed (Burns et al., 2022). However, salient but unrelated\nfeatures in a given dataset can mislead these probes (Farquhar et al., 2023).\nAddressing this, we propose a cluster normalization method to minimize the\nimpact of such features by clustering and normalizing activations of contrast\npairs before applying unsupervised probing techniques. While this approach does\nnot address the issue of differentiating between knowledge in general and\nsimulated knowledge - a major issue in the literature of latent knowledge\nelicitation (Christiano et al., 2021) - it significantly improves the ability\nof unsupervised probes to identify the intended knowledge amidst distractions.",
      "tldr_zh": "本研究针对语言模型在生成可靠信息时的挑战，提出了一种cluster normalization方法，用于改进无监督知识探测技术。作者指出，现有的无监督探测如Contrast-Consistent Search (CCS)容易受数据集中的显著但不相关特征误导，因此通过对对比对的激活进行聚类和归一化，减少这些干扰的影响。该方法显著提升了探测器识别预期知识的能力，尽管无法解决区分一般知识和模拟知识的核心问题。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "30 pages, 35 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.18712v2",
      "published_date": "2024-07-26 12:57:54 UTC",
      "updated_date": "2024-10-03 20:53:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:22:50.948708"
    },
    {
      "arxiv_id": "2407.18691v2",
      "title": "Graph Neural Networks for Virtual Sensing in Complex Systems: Addressing Heterogeneous Temporal Dynamics",
      "title_zh": "复杂系统虚拟感知中的图神经网络：解决异质时间动态",
      "authors": [
        "Mengjie Zhao",
        "Cees Taal",
        "Stephan Baggerohr",
        "Olga Fink"
      ],
      "abstract": "Real-time condition monitoring is crucial for the reliable and efficient\noperation of complex systems. However, relying solely on physical sensors can\nbe limited due to their cost, placement constraints, or inability to directly\nmeasure certain critical parameters. Virtual sensing addresses these\nlimitations by leveraging readily available sensor data and system knowledge to\nestimate inaccessible parameters or infer system states. The increasing\ncomplexity of industrial systems necessitates deployments of sensors with\ndiverse modalities to provide a comprehensive understanding of system states.\nThese sensors capture data at varying frequencies to monitor both rapid and\nslowly varying system dynamics, as well as local and global state evolutions of\nthe systems. This leads to heterogeneous temporal dynamics, which, particularly\nunder varying operational end environmental conditions, pose a significant\nchallenge for accurate virtual sensing. To address this, we propose a\nHeterogeneous Temporal Graph Neural Network (HTGNN) framework. HTGNN explicitly\nmodels signals from diverse sensors and integrates operating conditions into\nthe model architecture. We evaluate HTGNN using two newly released datasets: a\nbearing dataset with diverse load conditions for bearing load prediction and a\nyear-long simulated dataset for predicting bridge live loads. Our results\ndemonstrate that HTGNN significantly outperforms established baseline methods\nin both tasks, particularly under highly varying operating conditions. These\nresults highlight HTGNN's potential as a robust and accurate virtual sensing\napproach for complex systems, paving the way for improved monitoring,\npredictive maintenance, and enhanced system performance. Our code and data are\navailable under https://github.com/EPFL-IMOS/htgnn.",
      "tldr_zh": "该研究针对复杂系统中异构时间动态的挑战，提出了一种Heterogeneous Temporal Graph Neural Network (HTGNN)框架，用于虚拟感应。该框架通过显式建模多样模态传感器信号并整合操作条件，实现对不可访问参数的准确估计和系统状态推断。在两个新数据集（轴承负载预测和桥梁实时负载预测）上的实验表明，HTGNN显著优于现有基线方法，尤其在变化的操作环境下，准确率大幅提升。该方法为复杂系统的实时监控、预测性维护和性能优化提供了鲁棒解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.LG",
      "comment": "This paper extends our previous conference paper (Best Paper at\n  European Conference of the PHM Society 2024,\n  https://doi.org/10.36001/phme.2024.v8i1.3998). Accepted by Mechanical Systems\n  and Signal Processing (MSSP)",
      "pdf_url": "http://arxiv.org/pdf/2407.18691v2",
      "published_date": "2024-07-26 12:16:53 UTC",
      "updated_date": "2025-03-06 15:47:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:23:06.377757"
    },
    {
      "arxiv_id": "2407.18690v1",
      "title": "Collaborative Evolving Strategy for Automatic Data-Centric Development",
      "title_zh": "自动数据中心化开发的协作演化策略",
      "authors": [
        "Xu Yang",
        "Haotian Chen",
        "Wenjun Feng",
        "Haoxue Wang",
        "Zeqi Ye",
        "Xinjie Shen",
        "Xiao Yang",
        "Shizhao Sun",
        "Weiqing Liu",
        "Jiang Bian"
      ],
      "abstract": "Artificial Intelligence (AI) significantly influences many fields, largely\nthanks to the vast amounts of high-quality data for machine learning models.\nThe emphasis is now on a data-centric AI strategy, prioritizing data\ndevelopment over model design progress. Automating this process is crucial. In\nthis paper, we serve as the first work to introduce the automatic data-centric\ndevelopment (AD^2) task and outline its core challenges, which require\ndomain-experts-like task scheduling and implementation capability, largely\nunexplored by previous work.\n  By leveraging the strong complex problem-solving capabilities of large\nlanguage models (LLMs), we propose an LLM-based autonomous agent, equipped with\na strategy named Collaborative Knowledge-STudying-Enhanced Evolution by\nRetrieval (Co-STEER), to simultaneously address all the challenges.\nSpecifically, our proposed Co-STEER agent enriches its domain knowledge through\nour proposed evolving strategy and develops both its scheduling and\nimplementation skills by accumulating and retrieving domain-specific practical\nexperience. With an improved schedule, the capability for implementation\naccelerates. Simultaneously, as implementation feedback becomes more thorough,\nthe scheduling accuracy increases. These two capabilities evolve together\nthrough practical feedback, enabling a collaborative evolution process.\n  Extensive experimental results demonstrate that our Co-STEER agent breaks new\nground in AD^2 research, possesses strong evolvable schedule and implementation\nability, and demonstrates the significant effectiveness of its components. Our\nCo-STEER paves the way for AD^2 advancements.",
      "tldr_zh": "本论文首次提出自动数据中心开发（AD²）任务，强调在人工智能（AI）领域中优先自动化数据开发以提升模型性能，并识别其核心挑战，如任务调度和实施能力的需求。作者引入了一种基于大型语言模型（LLMs）的自主代理Co-STEER（Collaborative Knowledge-STudying-Enhanced Evolution by Retrieval），通过演化策略积累领域知识并检索经验，实现调度和实施技能的协同提升。Co-STEER的机制允许这些能力通过实践反馈相互强化，促进持续优化。实验结果显示，该代理在AD²研究中取得了突破性进展，证明了其组件的有效性和整体性能。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "23 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.18690v1",
      "published_date": "2024-07-26 12:16:47 UTC",
      "updated_date": "2024-07-26 12:16:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:23:26.223212"
    },
    {
      "arxiv_id": "2408.00804v1",
      "title": "ChipExpert: The Open-Source Integrated-Circuit-Design-Specific Large Language Model",
      "title_zh": "翻译失败",
      "authors": [
        "Ning Xu",
        "Zhaoyang Zhang",
        "Lei Qi",
        "Wensuo Wang",
        "Chao Zhang",
        "Zihao Ren",
        "Huaiyuan Zhang",
        "Xin Cheng",
        "Yanqi Zhang",
        "Zhichao Liu",
        "Qingwen Wei",
        "Shiyang Wu",
        "Lanlan Yang",
        "Qianfeng Lu",
        "Yiqun Ma",
        "Mengyao Zhao",
        "Junbo Liu",
        "Yufan Song",
        "Xin Geng",
        "Jun Yang"
      ],
      "abstract": "The field of integrated circuit (IC) design is highly specialized, presenting\nsignificant barriers to entry and research and development challenges. Although\nlarge language models (LLMs) have achieved remarkable success in various\ndomains, existing LLMs often fail to meet the specific needs of students,\nengineers, and researchers. Consequently, the potential of LLMs in the IC\ndesign domain remains largely unexplored. To address these issues, we introduce\nChipExpert, the first open-source, instructional LLM specifically tailored for\nthe IC design field. ChipExpert is trained on one of the current best\nopen-source base model (Llama-3 8B). The entire training process encompasses\nseveral key stages, including data preparation, continue pre-training,\ninstruction-guided supervised fine-tuning, preference alignment, and\nevaluation. In the data preparation stage, we construct multiple high-quality\ncustom datasets through manual selection and data synthesis techniques. In the\nsubsequent two stages, ChipExpert acquires a vast amount of IC design knowledge\nand learns how to respond to user queries professionally. ChipExpert also\nundergoes an alignment phase, using Direct Preference Optimization, to achieve\na high standard of ethical performance. Finally, to mitigate the hallucinations\nof ChipExpert, we have developed a Retrieval-Augmented Generation (RAG) system,\nbased on the IC design knowledge base. We also released the first IC design\nbenchmark ChipICD-Bench, to evaluate the capabilities of LLMs across multiple\nIC design sub-domains. Through comprehensive experiments conducted on this\nbenchmark, ChipExpert demonstrated a high level of expertise in IC design\nknowledge Question-and-Answer tasks.",
      "tldr_zh": "该论文介绍了 ChipExpert，这是首个开源的、专为集成电路 (IC) 设计领域量身定制的大型语言模型 (LLM)，基于 Llama-3 8B 模型进行训练，以解决现有 LLM 在 IC 设计中的适用性问题。训练过程包括数据准备、继续预训练、指令引导的监督微调、Direct Preference Optimization 偏好对齐，以及集成 Retrieval-Augmented Generation (RAG) 系统来减少幻觉。研究者构建了高质量的自定义数据集，并发布了首个 IC 设计基准 ChipICD-Bench，用于评估 LLM 在多个 IC 子领域的知识问答能力。实验结果显示，ChipExpert 在该基准上表现出色，展示了高水平的专业性和可靠性。",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00804v1",
      "published_date": "2024-07-26 11:00:08 UTC",
      "updated_date": "2024-07-26 11:00:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:23:27.379102"
    },
    {
      "arxiv_id": "2407.18995v1",
      "title": "SWIFT: Semantic Watermarking for Image Forgery Thwarting",
      "title_zh": "翻译失败",
      "authors": [
        "Gautier Evennou",
        "Vivien Chappelier",
        "Ewa Kijak",
        "Teddy Furon"
      ],
      "abstract": "This paper proposes a novel approach towards image authentication and\ntampering detection by using watermarking as a communication channel for\nsemantic information. We modify the HiDDeN deep-learning watermarking\narchitecture to embed and extract high-dimensional real vectors representing\nimage captions. Our method improves significantly robustness on both malign and\nbenign edits. We also introduce a local confidence metric correlated with\nMessage Recovery Rate, enhancing the method's practical applicability. This\napproach bridges the gap between traditional watermarking and passive forensic\nmethods, offering a robust solution for image integrity verification.",
      "tldr_zh": "这篇论文提出了SWIFT，一种基于语义水印的图像认证和篡改检测方法，将水印用作传输语义信息的通道，通过修改HiDDeN深度学习架构来嵌入和提取图像标题的高维实向量。SWIFT显著提升了对恶意和良性编辑的鲁棒性，并引入了一个与Message Recovery Rate相关的局部置信度指标，以提高实际应用效果。该方法桥接了传统水印技术和被动取证方法，提供了一个可靠的图像完整性验证解决方案。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CV",
        "cs.MM"
      ],
      "primary_category": "cs.CR",
      "comment": "Code will be released",
      "pdf_url": "http://arxiv.org/pdf/2407.18995v1",
      "published_date": "2024-07-26 09:50:13 UTC",
      "updated_date": "2024-07-26 09:50:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:24:21.086893"
    },
    {
      "arxiv_id": "2407.18626v1",
      "title": "Every Part Matters: Integrity Verification of Scientific Figures Based on Multimodal Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Xiang Shi",
        "Jiawei Liu",
        "Yinpeng Liu",
        "Qikai Cheng",
        "Wei Lu"
      ],
      "abstract": "This paper tackles a key issue in the interpretation of scientific figures:\nthe fine-grained alignment of text and figures. It advances beyond prior\nresearch that primarily dealt with straightforward, data-driven visualizations\nsuch as bar and pie charts and only offered a basic understanding of diagrams\nthrough captioning and classification. We introduce a novel task, Figure\nIntegrity Verification, designed to evaluate the precision of technologies in\naligning textual knowledge with visual elements in scientific figures. To\nsupport this, we develop a semi-automated method for constructing a large-scale\ndataset, Figure-seg, specifically designed for this task. Additionally, we\npropose an innovative framework, Every Part Matters (EPM), which leverages\nMultimodal Large Language Models (MLLMs) to not only incrementally improve the\nalignment and verification of text-figure integrity but also enhance integrity\nthrough analogical reasoning. Our comprehensive experiments show that these\ninnovations substantially improve upon existing methods, allowing for more\nprecise and thorough analysis of complex scientific figures. This progress not\nonly enhances our understanding of multimodal technologies but also stimulates\nfurther research and practical applications across fields requiring the\naccurate interpretation of complex visual data.",
      "tldr_zh": "本文提出一个新任务Figure Integrity Verification，用于评估科学图形中文本与视觉元素的细粒度对齐，超越了以往仅处理简单图表（如条形图和饼图）的基本方法。研究团队开发了半自动化数据集Figure-seg，并设计了创新框架Every Part Matters (EPM)，利用Multimodal Large Language Models (MLLMs)通过增量改进和类比推理来增强文本-图形完整性验证。实验结果显示，EPM显著提高了现有方法的性能，提供更精确的复杂科学图形分析，并推动多模态技术在实际应用中的发展。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.DL",
        "cs.MM"
      ],
      "primary_category": "cs.CL",
      "comment": "28 pages, 11 figures, under review",
      "pdf_url": "http://arxiv.org/pdf/2407.18626v1",
      "published_date": "2024-07-26 09:35:36 UTC",
      "updated_date": "2024-07-26 09:35:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:24:44.544988"
    },
    {
      "arxiv_id": "2407.18625v1",
      "title": "Topology Optimization of Random Memristors for Input-Aware Dynamic SNN",
      "title_zh": "随机忆阻器的拓扑优化，用于输入感知动态 SNN",
      "authors": [
        "Bo Wang",
        "Shaocong Wang",
        "Ning Lin",
        "Yi Li",
        "Yifei Yu",
        "Yue Zhang",
        "Jichang Yang",
        "Xiaoshan Wu",
        "Yangu He",
        "Songqi Wang",
        "Rui Chen",
        "Guoqi Li",
        "Xiaojuan Qi",
        "Zhongrui Wang",
        "Dashan Shang"
      ],
      "abstract": "There is unprecedented development in machine learning, exemplified by recent\nlarge language models and world simulators, which are artificial neural\nnetworks running on digital computers. However, they still cannot parallel\nhuman brains in terms of energy efficiency and the streamlined adaptability to\ninputs of different difficulties, due to differences in signal representation,\noptimization, run-time reconfigurability, and hardware architecture. To address\nthese fundamental challenges, we introduce pruning optimization for input-aware\ndynamic memristive spiking neural network (PRIME). Signal representation-wise,\nPRIME employs leaky integrate-and-fire neurons to emulate the brain's inherent\nspiking mechanism. Drawing inspiration from the brain's structural plasticity,\nPRIME optimizes the topology of a random memristive spiking neural network\nwithout expensive memristor conductance fine-tuning. For runtime\nreconfigurability, inspired by the brain's dynamic adjustment of computational\ndepth, PRIME employs an input-aware dynamic early stop policy to minimize\nlatency during inference, thereby boosting energy efficiency without\ncompromising performance. Architecture-wise, PRIME leverages memristive\nin-memory computing, mirroring the brain and mitigating the von Neumann\nbottleneck. We validated our system using a 40 nm 256 Kb memristor-based\nin-memory computing macro on neuromorphic image classification and image\ninpainting. Our results demonstrate the classification accuracy and Inception\nScore are comparable to the software baseline, while achieving maximal\n62.50-fold improvements in energy efficiency, and maximal 77.0% computational\nload savings. The system also exhibits robustness against stochastic synaptic\nnoise of analogue memristors. Our software-hardware co-designed model paves the\nway to future brain-inspired neuromorphic computing with brain-like energy\nefficiency and adaptivity.",
      "tldr_zh": "该研究针对机器学习模型在能效和适应性上不如人脑的问题，提出了一种名为 PRIME 的优化框架，用于输入感知动态 memristive 脉冲神经网络 (SNN)。PRIME 通过采用 leaky integrate-and-fire neurons 模拟大脑脉冲机制、优化随机 memristors 的拓扑结构以及输入感知动态早停策略，实现运行时可重配置和能效提升，同时利用 memristive in-memory computing 缓解 von Neumann 瓶颈。实验结果显示，在神经形态图像分类和图像修复任务上，PRIME 与软件基准的准确性和 Inception Score 相当，却实现了最大 62.50 倍的能效改善和 77.0% 的计算负载节省，并对模拟 memristor 的随机突触噪声表现出鲁棒性。该框架为脑启发神经形态计算提供了软件-硬件协同设计的路径，提升了系统的人脑-like 能效和适应性。",
      "categories": [
        "cs.ET",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.ET",
      "comment": "15 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.18625v1",
      "published_date": "2024-07-26 09:35:02 UTC",
      "updated_date": "2024-07-26 09:35:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:24:48.570827"
    },
    {
      "arxiv_id": "2407.18607v2",
      "title": "Using GPT-4 to guide causal machine learning",
      "title_zh": "利用 GPT-4 指导因果机器学习",
      "authors": [
        "Anthony C. Constantinou",
        "Neville K. Kitson",
        "Alessio Zanga"
      ],
      "abstract": "Since its introduction to the public, ChatGPT has had an unprecedented\nimpact. While some experts praised AI advancements and highlighted their\npotential risks, others have been critical about the accuracy and usefulness of\nLarge Language Models (LLMs). In this paper, we are interested in the ability\nof LLMs to identify causal relationships. We focus on the well-established\nGPT-4 (Turbo) and evaluate its performance under the most restrictive\nconditions, by isolating its ability to infer causal relationships based solely\non the variable labels without being given any other context by humans,\ndemonstrating the minimum level of effectiveness one can expect when it is\nprovided with label-only information. We show that questionnaire participants\njudge the GPT-4 graphs as the most accurate in the evaluated categories,\nclosely followed by knowledge graphs constructed by domain experts, with causal\nMachine Learning (ML) far behind. We use these results to highlight the\nimportant limitation of causal ML, which often produces causal graphs that\nviolate common sense, affecting trust in them. However, we show that pairing\nGPT-4 with causal ML overcomes this limitation, resulting in graphical\nstructures learnt from real data that align more closely with those identified\nby domain experts, compared to structures learnt by causal ML alone. Overall,\nour findings suggest that despite GPT-4 not being explicitly designed to reason\ncausally, it can still be a valuable tool for causal representation, as it\nimproves the causal discovery process of causal ML algorithms that are designed\nto do just that.",
      "tldr_zh": "本研究评估了 GPT-4 在因果关系识别方面的能力，通过在最严格条件下（仅基于变量标签）生成因果图，并与领域专家知识图和 causal machine learning (causal ML) 模型进行比较。结果显示，参与者认为 GPT-4 生成的图在准确性上最优，远超 causal ML，而 causal ML 常产生违反常识的图，影响其可信度。作者发现，将 GPT-4 与 causal ML 结合，能显著改善因果图的结构，使其更接近专家水平，从而证明 GPT-4 可作为提升因果发现过程的宝贵工具。",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18607v2",
      "published_date": "2024-07-26 08:59:26 UTC",
      "updated_date": "2024-12-11 19:43:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:24:59.729126"
    },
    {
      "arxiv_id": "2407.18601v2",
      "title": "Reorganizing attention-space geometry with expressive attention",
      "title_zh": "翻译失败",
      "authors": [
        "Claudius Gros"
      ],
      "abstract": "Attention regulates information transfer between tokens. For this, query and\nkey vectors are compared, typically in terms of a scalar product,\n$\\mathbf{Q}^T\\mathbf{K}$, together with a subsequent softmax normalization. In\ngeometric terms, the standard dot-product attention (DPA) leads to large/small\nattention weights for parallel/antiparallel queries and keys. Here we study\nexpressive attention (EA), which is based on $(\\mathbf{Q}^T\\mathbf{K})^2$, the\nsquared dot product. In this case, attention is enhanced when query and key are\neither parallel or antiparallel, and suppressed for orthogonal configurations.\nEA can be introduced into any attention-based code without additional compute\ncosts or memory requirements. For a series of autoregressive prediction tasks,\nwe find that expressive attention performs at least as well as vanilla DPA.\nIncreasing task complexity, EA is observed to outperform DPA with increasing\nmargins, which also holds for multi-task settings. For a given model size, EA\nmanages to achieve 100% performance for a range of complexity levels not\naccessible to DPA. Our results show that it is possible to reorganize the\ngeometry of the matching condition in the space of attention heads without loss\nof performance.",
      "tldr_zh": "本文提出 expressive attention (EA) 机制，通过使用点积的平方 $(\\mathbf{Q}^T\\mathbf{K})^2$ 来重新组织注意力空间的几何结构，与标准 dot-product attention (DPA) 相比，它增强了查询和键平行或反平行的注意力，同时抑制正交配置。EA 可以无缝集成到任何注意力-based 代码中，无需额外计算成本或内存开销。在自回归预测任务中，EA 的性能至少与 DPA 相当，并在任务复杂度增加或多任务设置下表现出显著优势，能够使给定模型大小处理 DPA 无法达到的复杂度水平，从而证明了重新组织注意力头匹配几何的可行性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18601v2",
      "published_date": "2024-07-26 08:41:58 UTC",
      "updated_date": "2025-01-08 09:30:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:25:10.516177"
    },
    {
      "arxiv_id": "2407.18597v1",
      "title": "Reinforcement Learning for Sustainable Energy: A Survey",
      "title_zh": "可持续能源的强化学习：综述",
      "authors": [
        "Koen Ponse",
        "Felix Kleuker",
        "Márton Fejér",
        "Álvaro Serra-Gómez",
        "Aske Plaat",
        "Thomas Moerland"
      ],
      "abstract": "The transition to sustainable energy is a key challenge of our time,\nrequiring modifications in the entire pipeline of energy production, storage,\ntransmission, and consumption. At every stage, new sequential decision-making\nchallenges emerge, ranging from the operation of wind farms to the management\nof electrical grids or the scheduling of electric vehicle charging stations.\nAll such problems are well suited for reinforcement learning, the branch of\nmachine learning that learns behavior from data. Therefore, numerous studies\nhave explored the use of reinforcement learning for sustainable energy. This\npaper surveys this literature with the intention of bridging both the\nunderlying research communities: energy and machine learning. After a brief\nintroduction of both fields, we systematically list relevant sustainability\nchallenges, how they can be modeled as a reinforcement learning problem, and\nwhat solution approaches currently exist in the literature. Afterwards, we zoom\nout and identify overarching reinforcement learning themes that appear\nthroughout sustainability, such as multi-agent, offline, and safe reinforcement\nlearning. Lastly, we also cover standardization of environments, which will be\ncrucial for connecting both research fields, and highlight potential directions\nfor future work. In summary, this survey provides an extensive overview of\nreinforcement learning methods for sustainable energy, which may play a vital\nrole in the energy transition.",
      "tldr_zh": "这篇调查论文探讨了强化学习(Reinforcement Learning)在可持续能源领域的应用，旨在桥接能源和机器学习研究社区。论文首先概述了能源生产、存储、传输和消费中的决策挑战，并说明这些问题如何被建模为强化学习问题，同时总结了现有文献中的解决方案。接着，它识别了贯穿可持续能源的强化学习主题，如多智能体(Multi-Agent)、离线(Offline)和安全(Safe)强化学习，并强调了环境标准化的重要性，以促进未来研究。最后，论文指出强化学习可能在能源转型中发挥关键作用，提供了一个全面的概述。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY",
        "cs.SY",
        "eess.SY",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "22 pages excluding references, 40 pages including references, 7\n  images",
      "pdf_url": "http://arxiv.org/pdf/2407.18597v1",
      "published_date": "2024-07-26 08:37:14 UTC",
      "updated_date": "2024-07-26 08:37:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:25:31.815009"
    },
    {
      "arxiv_id": "2407.21060v1",
      "title": "Using Large Language Models for the Interpretation of Building Regulations",
      "title_zh": "翻译失败",
      "authors": [
        "Stefan Fuchs",
        "Michael Witbrock",
        "Johannes Dimyadi",
        "Robert Amor"
      ],
      "abstract": "Compliance checking is an essential part of a construction project. The\nrecent rapid uptake of building information models (BIM) in the construction\nindustry has created more opportunities for automated compliance checking\n(ACC). BIM enables sharing of digital building design data that can be used for\ncompliance checking with legal requirements, which are conventionally conveyed\nin natural language and not intended for machine processing. Creating a\ncomputable representation of legal requirements suitable for ACC is complex,\ncostly, and time-consuming. Large language models (LLMs) such as the generative\npre-trained transformers (GPT), GPT-3.5 and GPT-4, powering OpenAI's ChatGPT,\ncan generate logically coherent text and source code responding to user\nprompts. This capability could be used to automate the conversion of building\nregulations into a semantic and computable representation. This paper evaluates\nthe performance of LLMs in translating building regulations into LegalRuleML in\na few-shot learning setup. By providing GPT-3.5 with only a few example\ntranslations, it can learn the basic structure of the format. Using a system\nprompt, we further specify the LegalRuleML representation and explore the\nexistence of expert domain knowledge in the model. Such domain knowledge might\nbe ingrained in GPT-3.5 through the broad pre-training but needs to be brought\nforth by careful contextualisation. Finally, we investigate whether strategies\nsuch as chain-of-thought reasoning and self-consistency could apply to this use\ncase. As LLMs become more sophisticated, the increased common sense, logical\ncoherence, and means to domain adaptation can significantly support ACC,\nleading to more efficient and effective checking processes.",
      "tldr_zh": "本论文探讨了使用Large Language Models (LLMs)，如GPT-3.5和GPT-4，来将建筑法规转化为可计算的LegalRuleML格式，从而支持自动合规性检查 (ACC)。研究采用few-shot learning方法，通过系统提示和chain-of-thought reasoning策略，帮助LLMs学习格式结构并激活其内在领域知识。实验结果表明，这种方法能有效提升法规翻译的准确性和逻辑一致性，最终促进ACC过程更高效，并为建筑信息模型 (BIM) 应用提供新途径。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Presented at the 13th Conference on Engineering, Project and\n  Production Management",
      "pdf_url": "http://arxiv.org/pdf/2407.21060v1",
      "published_date": "2024-07-26 08:30:47 UTC",
      "updated_date": "2024-07-26 08:30:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:25:34.265784"
    },
    {
      "arxiv_id": "2407.18581v4",
      "title": "Dynamic Language Group-Based MoE: Enhancing Code-Switching Speech Recognition with Hierarchical Routing",
      "title_zh": "翻译失败",
      "authors": [
        "Hukai Huang",
        "Shenghui Lu",
        "Yahui Shan",
        "He Qu",
        "Fengrun Zhang",
        "Wenhao Guan",
        "Qingyang Hong",
        "Lin Li"
      ],
      "abstract": "The Mixture of Experts (MoE) model is a promising approach for handling\ncode-switching speech recognition (CS-ASR) tasks. However, the existing CS-ASR\nwork on MoE has yet to leverage the advantages of MoE's parameter scaling\nability fully. This work proposes DLG-MoE, a Dynamic Language Group-based MoE,\nwhich can effectively handle the CS-ASR task and leverage the advantages of\nparameter scaling. DLG-MoE operates based on a hierarchical routing mechanism.\nFirst, the language router explicitly models the language attribute and\ndispatches the representations to the corresponding language expert groups.\nSubsequently, the unsupervised router within each language group implicitly\nmodels attributes beyond language and coordinates expert routing and\ncollaboration. DLG-MoE outperforms the existing MoE methods on CS-ASR tasks\nwhile demonstrating great flexibility. It supports different top-$k$ inference\nand streaming capabilities and can also prune the model parameters flexibly to\nobtain a monolingual sub-model. The code has been released.",
      "tldr_zh": "该研究提出 DLG-MoE，一种基于动态语言组的混合专家(MoE)模型，用于提升代码切换语音识别(CS-ASR)的性能，并充分利用 MoE 的参数扩展优势。\nDLG-MoE 采用分层路由机制：首先，语言路由器显式建模语言属性，将输入分发到对应的语言专家组；随后，在每个组内，无监督路由器隐式处理超出语言的属性，实现专家的协调和协作。\n实验表明，DLG-MoE 在 CS-ASR 任务上优于现有 MoE 方法，并具备灵活性，支持 top-k 推理、流式处理以及参数修剪以生成单语子模型。代码已发布。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by ICASSP2025",
      "pdf_url": "http://arxiv.org/pdf/2407.18581v4",
      "published_date": "2024-07-26 08:03:07 UTC",
      "updated_date": "2024-12-22 03:01:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:25:57.216021"
    },
    {
      "arxiv_id": "2407.18994v1",
      "title": "Online Test Synthesis From Requirements: Enhancing Reinforcement Learning with Game Theory",
      "title_zh": "翻译失败",
      "authors": [
        "Ocan Sankur",
        "Thierry Jéron",
        "Nicolas Markey",
        "David Mentré",
        "Reiya Noguchi"
      ],
      "abstract": "We consider the automatic online synthesis of black-box test cases from\nfunctional requirements specified as automata for reactive implementations. The\ngoal of the tester is to reach some given state, so as to satisfy a coverage\ncriterion, while monitoring the violation of the requirements. We develop an\napproach based on Monte Carlo Tree Search, which is a classical technique in\nreinforcement learning for efficiently selecting promising inputs. Seeing the\nautomata requirements as a game between the implementation and the tester, we\ndevelop a heuristic by biasing the search towards inputs that are promising in\nthis game. We experimentally show that our heuristic accelerates the\nconvergence of the Monte Carlo Tree Search algorithm, thus improving the\nperformance of testing.",
      "tldr_zh": "本研究探讨了从功能需求（指定为自动机）自动合成在线黑盒测试用例的目标，以达到给定状态的覆盖标准，同时监控需求违反。研究提出了一种基于 Monte Carlo Tree Search 的方法，将需求视为实现者和测试者之间的游戏，并开发启发式来偏向有前景的输入，从而加速搜索过程。实验结果显示，该启发式显著提升了 Monte Carlo Tree Search 的收敛速度，并提高了整体测试性能。",
      "categories": [
        "cs.AI",
        "cs.GT",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18994v1",
      "published_date": "2024-07-26 07:59:59 UTC",
      "updated_date": "2024-07-26 07:59:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:25:57.839833"
    },
    {
      "arxiv_id": "2407.18571v2",
      "title": "Speech Bandwidth Expansion Via High Fidelity Generative Adversarial Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Mahmoud Salhab",
        "Haidar Harmanani"
      ],
      "abstract": "Speech bandwidth expansion is crucial for expanding the frequency range of\nlow-bandwidth speech signals, thereby improving audio quality, clarity and\nperceptibility in digital applications. Its applications span telephony,\ncompression, text-to-speech synthesis, and speech recognition. This paper\npresents a novel approach using a high-fidelity generative adversarial network,\nunlike cascaded systems, our system is trained end-to-end on paired narrowband\nand wideband speech signals. Our method integrates various bandwidth upsampling\nratios into a single unified model specifically designed for speech bandwidth\nexpansion applications. Our approach exhibits robust performance across various\nbandwidth expansion factors, including those not encountered during training,\ndemonstrating zero-shot capability. To the best of our knowledge, this is the\nfirst work to showcase this capability. The experimental results demonstrate\nthat our method outperforms previous end-to-end approaches, as well as\ninterpolation and traditional techniques, showcasing its effectiveness in\npractical speech enhancement applications.",
      "tldr_zh": "这篇论文提出了一种基于高保真 Generative Adversarial Networks (GANs) 的语音带宽扩展方法，通过端到端训练处理配对的窄带和宽带语音信号，从而提升音频质量和清晰度。不同于传统的级联系统，该方法将多种带宽上采样比率整合到一个统一模型中，并展示了零-shot 能力，能够处理训练中未见过的扩展因子。实验结果显示，该方法在语音增强应用中优于现有端到端方法、插值和传统技术，这是首创的工作。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18571v2",
      "published_date": "2024-07-26 07:54:47 UTC",
      "updated_date": "2024-07-29 07:29:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:26:15.364754"
    },
    {
      "arxiv_id": "2407.18569v3",
      "title": "PP-TIL: Personalized Planning for Autonomous Driving with Instance-based Transfer Imitation Learning",
      "title_zh": "PP-TIL：基于实例的转移模仿学习用于自动驾驶的个性化规划",
      "authors": [
        "Fangze Lin",
        "Ying He",
        "Fei Yu"
      ],
      "abstract": "Personalized motion planning holds significant importance within urban\nautomated driving, catering to the unique requirements of individual users.\nNevertheless, prior endeavors have frequently encountered difficulties in\nsimultaneously addressing two crucial aspects: personalized planning within\nintricate urban settings and enhancing planning performance through data\nutilization. The challenge arises from the expensive and limited nature of user\ndata, coupled with the scene state space tending towards infinity. These\nfactors contribute to overfitting and poor generalization problems during model\ntraining. Henceforth, we propose an instance-based transfer imitation learning\napproach. This method facilitates knowledge transfer from extensive expert\ndomain data to the user domain, presenting a fundamental resolution to these\nissues. We initially train a pre-trained model using large-scale expert data.\nSubsequently, during the fine-tuning phase, we feed the batch data, which\ncomprises expert and user data. Employing the inverse reinforcement learning\ntechnique, we extract the style feature distribution from user demonstrations,\nconstructing the regularization term for the approximation of user style. In\nour experiments, we conducted extensive evaluations of the proposed method.\nCompared to the baseline methods, our approach mitigates the overfitting issue\ncaused by sparse user data. Furthermore, we discovered that integrating the\ndriving model with a differentiable nonlinear optimizer as a safety protection\nlayer for end-to-end personalized fine-tuning results in superior planning\nperformance.",
      "tldr_zh": "该论文提出PP-TIL方法，利用instance-based transfer imitation learning来实现自动驾驶的个性化规划，解决复杂城市环境中用户数据稀缺导致的overfitting和泛化差问题。该方法首先通过大规模专家数据训练预训练模型，然后在微调阶段结合专家和用户数据，使用inverse reinforcement learning提取用户风格特征作为正则化项，以实现知识转移。实验结果显示，PP-TIL比基线方法显著减少overfitting，并通过整合可微非线性优化器作为安全层，进一步提升了规划性能。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "IROS 2024 Accepted",
      "pdf_url": "http://arxiv.org/pdf/2407.18569v3",
      "published_date": "2024-07-26 07:51:11 UTC",
      "updated_date": "2024-08-04 09:01:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:26:23.739448"
    },
    {
      "arxiv_id": "2407.18562v1",
      "title": "Learning Robust Named Entity Recognizers From Noisy Data With Retrieval Augmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Chaoyi Ai",
        "Yong Jiang",
        "Shen Huang",
        "Pengjun Xie",
        "Kewei Tu"
      ],
      "abstract": "Named entity recognition (NER) models often struggle with noisy inputs, such\nas those with spelling mistakes or errors generated by Optical Character\nRecognition processes, and learning a robust NER model is challenging. Existing\nrobust NER models utilize both noisy text and its corresponding gold text for\ntraining, which is infeasible in many real-world applications in which gold\ntext is not available. In this paper, we consider a more realistic setting in\nwhich only noisy text and its NER labels are available. We propose to retrieve\nrelevant text of the noisy text from a knowledge corpus and use it to enhance\nthe representation of the original noisy input. We design three retrieval\nmethods: sparse retrieval based on lexicon similarity, dense retrieval based on\nsemantic similarity, and self-retrieval based on task-specific text. After\nretrieving relevant text, we concatenate the retrieved text with the original\nnoisy text and encode them with a transformer network, utilizing self-attention\nto enhance the contextual token representations of the noisy text using the\nretrieved text. We further employ a multi-view training framework that improves\nrobust NER without retrieving text during inference. Experiments show that our\nretrieval-augmented model achieves significant improvements in various noisy\nNER settings.",
      "tldr_zh": "本研究针对命名实体识别（Named Entity Recognition, NER）模型在处理噪声数据（如拼写错误或 OCR 错误）时的鲁棒性问题，提出了一种基于检索增强（Retrieval Augmentation）的训练方法，该方法仅使用噪声文本及其标签，而不依赖金标准文本。研究设计了三种检索策略：基于词汇相似性的稀疏检索、基于语义相似性的稠密检索，以及基于任务特定文本的自检索；随后，将检索到的相关文本与原始噪声文本拼接，并通过 Transformer 网络利用自注意力机制增强噪声文本的表示。论文还引入多视图训练框架，以在推理阶段无需检索文本的情况下提升模型性能。实验结果显示，该方法在各种噪声 NER 设置中实现了显著改进。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18562v1",
      "published_date": "2024-07-26 07:30:41 UTC",
      "updated_date": "2024-07-26 07:30:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:26:36.942851"
    },
    {
      "arxiv_id": "2407.18556v1",
      "title": "Look Globally and Reason: Two-stage Path Reasoning over Sparse Knowledge Graphs",
      "title_zh": "翻译失败",
      "authors": [
        "Saiping Guan",
        "Jiyao Wei",
        "Xiaolong Jin",
        "Jiafeng Guo",
        "Xueqi Cheng"
      ],
      "abstract": "Sparse Knowledge Graphs (KGs), frequently encountered in real-world\napplications, contain fewer facts in the form of (head entity, relation, tail\nentity) compared to more populated KGs. The sparse KG completion task, which\nreasons answers for given queries in the form of (head entity, relation, ?) for\nsparse KGs, is particularly challenging due to the necessity of reasoning\nmissing facts based on limited facts. Path-based models, known for excellent\nexplainability, are often employed for this task. However, existing path-based\nmodels typically rely on external models to fill in missing facts and\nsubsequently perform path reasoning. This approach introduces unexplainable\nfactors or necessitates meticulous rule design. In light of this, this paper\nproposes an alternative approach by looking inward instead of seeking external\nassistance. We introduce a two-stage path reasoning model called LoGRe (Look\nGlobally and Reason) over sparse KGs. LoGRe constructs a relation-path\nreasoning schema by globally analyzing the training data to alleviate the\nsparseness problem. Based on this schema, LoGRe then aggregates paths to reason\nout answers. Experimental results on five benchmark sparse KG datasets\ndemonstrate the effectiveness of the proposed LoGRe model.",
      "tldr_zh": "这篇论文针对稀疏知识图（Sparse Knowledge Graphs）提出了一种两阶段路径推理模型LoGRe，以解决基于有限事实进行查询（head entity, relation, ?）推理的挑战。LoGRe首先通过全局分析训练数据构建relation-path推理schema，缓解数据稀疏性问题；随后，基于该schema聚合路径来推导出答案。与现有依赖外部模型的路径-based方法相比，LoGRe提高了推理的可解释性和效率。在五个基准稀疏KG数据集上的实验结果证明了LoGRe的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to CIKM 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.18556v1",
      "published_date": "2024-07-26 07:10:27 UTC",
      "updated_date": "2024-07-26 07:10:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:26:59.338020"
    },
    {
      "arxiv_id": "2407.18555v3",
      "title": "How to Segment in 3D Using 2D Models: Automated 3D Segmentation of Prostate Cancer Metastatic Lesions on PET Volumes Using Multi-angle Maximum Intensity Projections and Diffusion Models",
      "title_zh": "翻译失败",
      "authors": [
        "Amirhosein Toosi",
        "Sara Harsini",
        "François Bénard",
        "Carlos Uribe",
        "Arman Rahmim"
      ],
      "abstract": "Prostate specific membrane antigen (PSMA) positron emission\ntomography/computed tomography (PET/CT) imaging provides a tremendously\nexciting frontier in visualization of prostate cancer (PCa) metastatic lesions.\nHowever, accurate segmentation of metastatic lesions is challenging due to low\nsignal-to-noise ratios and variable sizes, shapes, and locations of the\nlesions. This study proposes a novel approach for automated segmentation of\nmetastatic lesions in PSMA PET/CT 3D volumetric images using 2D denoising\ndiffusion probabilistic models (DDPMs). Instead of 2D trans-axial slices or 3D\nvolumes, the proposed approach segments the lesions on generated multi-angle\nmaximum intensity projections (MA-MIPs) of the PSMA PET images, then obtains\nthe final 3D segmentation masks from 3D ordered subset expectation maximization\n(OSEM) reconstruction of 2D MA-MIPs segmentations. Our proposed method achieved\nsuperior performance compared to state-of-the-art 3D segmentation approaches in\nterms of accuracy and robustness in detecting and segmenting small metastatic\nPCa lesions. The proposed method has significant potential as a tool for\nquantitative analysis of metastatic burden in PCa patients.",
      "tldr_zh": "本研究针对前列腺癌 (PCa) 转移病变的分割挑战，提出了一种使用 2D 模型实现 3D 自动分割的方法，针对 PSMA PET/CT 图像中低信噪比和病变变异性问题。方法通过生成多角度最大强度投影 (MA-MIPs) 图像，并应用 2D 去噪扩散概率模型 (DDPMs) 进行分割，然后利用 3D 有序子集期望最大化 (OSEM) 重建获得最终 3D 掩码。该方法在检测和分割小转移病变方面，相比现有 3D 分割方法表现出更高的准确性和鲁棒性，具有潜力作为量化分析 PCa 患者转移负担的工具。",
      "categories": [
        "physics.med-ph",
        "cs.AI",
        "cs.CV",
        "I.4.6"
      ],
      "primary_category": "physics.med-ph",
      "comment": "11 pages, 2 figures, accepted in the DGM4MICCAI workshop, MICCAI,\n  2024",
      "pdf_url": "http://arxiv.org/pdf/2407.18555v3",
      "published_date": "2024-07-26 07:08:05 UTC",
      "updated_date": "2024-12-04 12:42:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:27:17.502200"
    },
    {
      "arxiv_id": "2408.06359v1",
      "title": "An Adaptive CSI Feedback Model Based on BiLSTM for Massive MIMO-OFDM Systems",
      "title_zh": "基于 BiLSTM 的自适应 CSI 反馈模型，用于大规模 MIMO-OFDM 系统",
      "authors": [
        "Hongrui Shen",
        "Long Zhao",
        "Kan Zheng",
        "Yuhua Cao",
        "Pingzhi Fan"
      ],
      "abstract": "Deep learning (DL)-based channel state information (CSI) feedback has the\npotential to improve the recovery accuracy and reduce the feedback overhead in\nmassive multiple-input multiple-output orthogonal frequency division\nmultiplexing (MIMO-OFDM) systems. However, the length of input CSI and the\nnumber of feedback bits should be adjustable in different scenarios, which can\nnot be efficiently achieved by the existing CSI feedback models. Therefore, an\nadaptive bidirectional long short-term memory network (ABLNet) for CSI feedback\nis first designed to process various input CSI lengths, where the number of\nfeedback bits is in proportion to the CSI length. Then, to realize a more\nflexible feedback bit number, a feedback bit control unit (FBCU) module is\nproposed to control the output length of feedback bits. Based on which, a\ntarget feedback performance can be adaptively achieved by a designed bit number\nadjusting (BNA) algorithm. Furthermore, a novel separate training approach is\ndevised to solve the model protection problem that the UE and gNB are from\ndifferent manufacturers. Experiments demonstrate that the proposed ABLNet with\nFBCU can fit for different input CSI lengths and feedback bit numbers; the CSI\nfeedback performance can be stabilized by the BNA algorithm; and the proposed\nseparate training approach can maintain the feedback performance and reduce the\ncomplexity of feedback model.",
      "tldr_zh": "本研究针对大规模 MIMO-OFDM 系统中的 CSI 反馈问题，提出了一种基于 BiLSTM 的自适应模型 ABLNet，能够处理不同输入 CSI 长度的反馈需求，并使反馈位数与 CSI 长度成比例。模型引入 FBCU（Feedback Bit Control Unit）模块和 BNA（Bit Number Adjusting）算法，实现更灵活的反馈位数控制和目标性能的自适应调整，同时采用新型分离训练方法解决 UE 和 gNB 来自不同制造商的模型保护问题。实验结果表明，ABLNet with FBCU 能适应各种输入 CSI 长度和反馈位数，BNA 算法稳定了反馈性能，且分离训练方法维持了性能并降低了模型复杂度。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "13 pages, 14 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2408.06359v1",
      "published_date": "2024-07-26 07:07:34 UTC",
      "updated_date": "2024-07-26 07:07:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:27:12.330931"
    },
    {
      "arxiv_id": "2407.18551v3",
      "title": "Multi-Agent Trajectory Prediction with Difficulty-Guided Feature Enhancement Network",
      "title_zh": "翻译失败",
      "authors": [
        "Guipeng Xin",
        "Duanfeng Chu",
        "Liping Lu",
        "Zejian Deng",
        "Yuang Lu",
        "Xigang Wu"
      ],
      "abstract": "Trajectory prediction is crucial for autonomous driving as it aims to\nforecast the future movements of traffic participants. Traditional methods\nusually perform holistic inference on the trajectories of agents, neglecting\nthe differences in prediction difficulty among agents. This paper proposes a\nnovel Difficulty-Guided Feature Enhancement Network (DGFNet), which leverages\nthe prediction difficulty differences among agents for multi-agent trajectory\nprediction. Firstly, we employ spatio-temporal feature encoding and interaction\nto capture rich spatio-temporal features. Secondly, a difficulty-guided decoder\ncontrols the flow of future trajectories into subsequent modules, obtaining\nreliable future trajectories. Then, feature interaction and fusion are\nperformed through the future feature interaction module. Finally, the fused\nagent features are fed into the final predictor to generate the predicted\ntrajectory distributions for multiple participants. Experimental results\ndemonstrate that our DGFNet achieves state-of-the-art performance on the\nArgoverse 1\\&2 motion forecasting benchmarks. Ablation studies further validate\nthe effectiveness of each module. Moreover, compared with SOTA methods, our\nmethod balances trajectory prediction accuracy and real-time inference speed.",
      "tldr_zh": "该论文提出了一种Difficulty-Guided Feature Enhancement Network (DGFNet)，针对多代理轨迹预测问题，通过考虑代理间的预测难度差异来提升自动驾驶中的轨迹预测准确性。DGFNet 包括时空特征编码和交互模块、难度引导解码器、未来特征交互模块，以及最终预测器，用于生成可靠的未来轨迹分布。实验结果显示，该方法在 Argoverse 1&2 基准上实现了最先进性能，同时通过消融研究验证了各模块的有效性，并与 SOTA 方法相比实现了准确性和实时推理速度的平衡。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18551v3",
      "published_date": "2024-07-26 07:04:30 UTC",
      "updated_date": "2024-12-19 09:55:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:27:24.158993"
    },
    {
      "arxiv_id": "2407.18550v1",
      "title": "ReALFRED: An Embodied Instruction Following Benchmark in Photo-Realistic Environments",
      "title_zh": "翻译失败",
      "authors": [
        "Taewoong Kim",
        "Cheolhong Min",
        "Byeonghwi Kim",
        "Jinyeon Kim",
        "Wonje Jeung",
        "Jonghyun Choi"
      ],
      "abstract": "Simulated virtual environments have been widely used to learn robotic agents\nthat perform daily household tasks. These environments encourage research\nprogress by far, but often provide limited object interactability, visual\nappearance different from real-world environments, or relatively smaller\nenvironment sizes. This prevents the learned models in the virtual scenes from\nbeing readily deployable. To bridge the gap between these learning environments\nand deploying (i.e., real) environments, we propose the ReALFRED benchmark that\nemploys real-world scenes, objects, and room layouts to learn agents to\ncomplete household tasks by understanding free-form language instructions and\ninteracting with objects in large, multi-room and 3D-captured scenes.\nSpecifically, we extend the ALFRED benchmark with updates for larger\nenvironmental spaces with smaller visual domain gaps. With ReALFRED, we analyze\npreviously crafted methods for the ALFRED benchmark and observe that they\nconsistently yield lower performance in all metrics, encouraging the community\nto develop methods in more realistic environments. Our code and data are\npublicly available.",
      "tldr_zh": "该论文提出ReALFRED基准，这是一个基于照片级真实环境的基准，用于训练机器人代理理解自由形式语言指令并在大型、多房间的3D捕获场景中完成家庭任务，从而桥接虚拟模拟与真实部署的差距。ReALFRED扩展了ALFRED基准，通过采用真实场景、物体和布局，减少视觉领域差距并提升环境规模。实验结果显示，现有方法在ReALFRED上的性能显著下降，呼吁社区开发更适用于真实环境的算法；代码和数据已公开可用。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "ECCV 2024 (Project page: https://twoongg.github.io/projects/realfred)",
      "pdf_url": "http://arxiv.org/pdf/2407.18550v1",
      "published_date": "2024-07-26 07:00:27 UTC",
      "updated_date": "2024-07-26 07:00:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:27:47.344864"
    },
    {
      "arxiv_id": "2407.18541v1",
      "title": "Towards Improving NAM-to-Speech Synthesis Intelligibility using Self-Supervised Speech Models",
      "title_zh": "翻译失败",
      "authors": [
        "Neil Shah",
        "Shirish Karande",
        "Vineet Gandhi"
      ],
      "abstract": "We propose a novel approach to significantly improve the intelligibility in\nthe Non-Audible Murmur (NAM)-to-speech conversion task, leveraging\nself-supervision and sequence-to-sequence (Seq2Seq) learning techniques. Unlike\nconventional methods that explicitly record ground-truth speech, our\nmethodology relies on self-supervision and speech-to-speech synthesis to\nsimulate ground-truth speech. Despite utilizing simulated speech, our method\nsurpasses the current state-of-the-art (SOTA) by 29.08% improvement in the\nMel-Cepstral Distortion (MCD) metric. Additionally, we present error rates and\ndemonstrate our model's proficiency to synthesize speech in novel voices of\ninterest. Moreover, we present a methodology for augmenting the existing CSTR\nNAM TIMIT Plus corpus, setting a benchmark with a Word Error Rate (WER) of\n42.57% to gauge the intelligibility of the synthesized speech. Speech samples\ncan be found at https://nam2speech.github.io/NAM2Speech/",
      "tldr_zh": "这篇论文提出了一种新方法，利用自监督学习和 Seq2Seq 模型来显著提升 Non-Audible Murmur (NAM) 到语音合成的清晰度，而非依赖显式录音，而是通过模拟语音生成来实现。相比传统技术，该方法在 Mel-Cepstral Distortion (MCD) 指标上取得了29.08%的改进，并展示了模型合成新颖声音的能力。作者还扩展了 CSTR NAM TIMIT Plus 语料库，并设定了 Word Error Rate (WER) 为42.57%的基准，以评估合成的语音可懂度。最终，该研究为 NAM-to-Speech 任务提供了更高效且实用的框架。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted at Interspeech 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.18541v1",
      "published_date": "2024-07-26 06:44:01 UTC",
      "updated_date": "2024-07-26 06:44:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:27:50.820214"
    },
    {
      "arxiv_id": "2407.18540v1",
      "title": "A Universal Prompting Strategy for Extracting Process Model Information from Natural Language Text using Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Julian Neuberger",
        "Lars Ackermann",
        "Han van der Aa",
        "Stefan Jablonski"
      ],
      "abstract": "Over the past decade, extensive research efforts have been dedicated to the\nextraction of information from textual process descriptions. Despite the\nremarkable progress witnessed in natural language processing (NLP), information\nextraction within the Business Process Management domain remains predominantly\nreliant on rule-based systems and machine learning methodologies. Data scarcity\nhas so far prevented the successful application of deep learning techniques.\nHowever, the rapid progress in generative large language models (LLMs) makes it\npossible to solve many NLP tasks with very high quality without the need for\nextensive data. Therefore, we systematically investigate the potential of LLMs\nfor extracting information from textual process descriptions, targeting the\ndetection of process elements such as activities and actors, and relations\nbetween them. Using a heuristic algorithm, we demonstrate the suitability of\nthe extracted information for process model generation. Based on a novel\nprompting strategy, we show that LLMs are able to outperform state-of-the-art\nmachine learning approaches with absolute performance improvements of up to 8\\%\n$F_1$ score across three different datasets. We evaluate our prompting strategy\non eight different LLMs, showing it is universally applicable, while also\nanalyzing the impact of certain prompt parts on extraction quality. The number\nof example texts, the specificity of definitions, and the rigour of format\ninstructions are identified as key for improving the accuracy of extracted\ninformation. Our code, prompts, and data are publicly available.",
      "tldr_zh": "这篇论文提出了一种通用的提示策略(prompting strategy)，利用大语言模型(LLMs)从自然语言文本中提取过程模型信息，包括活动、参与者和它们之间的关系，从而解决业务过程管理领域的信息提取挑战。研究通过启发式算法(heuristic algorithm)验证了提取信息的适用性，并证明该策略在三个数据集上使LLMs的性能比现有机器学习方法提升高达8%的F1 score。实验在八个不同LLMs上进行，显示策略具有通用性，并强调示例文本数量、定义具体性和格式指令严格性是提升提取准确性的关键因素。代码、提示和数据已公开可用，以促进进一步研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18540v1",
      "published_date": "2024-07-26 06:39:35 UTC",
      "updated_date": "2024-07-26 06:39:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:28:01.637077"
    },
    {
      "arxiv_id": "2407.20284v1",
      "title": "MLtoGAI: Semantic Web based with Machine Learning for Enhanced Disease Prediction and Personalized Recommendations using Generative AI",
      "title_zh": "翻译失败",
      "authors": [
        "Shyam Dongre",
        "Ritesh Chandra",
        "Sonali Agarwal"
      ],
      "abstract": "In modern healthcare, addressing the complexities of accurate disease\nprediction and personalized recommendations is both crucial and challenging.\nThis research introduces MLtoGAI, which integrates Semantic Web technology with\nMachine Learning (ML) to enhance disease prediction and offer user-friendly\nexplanations through ChatGPT. The system comprises three key components: a\nreusable disease ontology that incorporates detailed knowledge about various\ndiseases, a diagnostic classification model that uses patient symptoms to\ndetect specific diseases accurately, and the integration of Semantic Web Rule\nLanguage (SWRL) with ontology and ChatGPT to generate clear, personalized\nhealth advice. This approach significantly improves prediction accuracy and\nensures results that are easy to understand, addressing the complexity of\ndiseases and diverse symptoms. The MLtoGAI system demonstrates substantial\nadvancements in accuracy and user satisfaction, contributing to developing more\nintelligent and accessible healthcare solutions. This innovative approach\ncombines the strengths of ML algorithms with the ability to provide\ntransparent, human-understandable explanations through ChatGPT, achieving\nsignificant improvements in prediction accuracy and user comprehension. By\nleveraging semantic technology and explainable AI, the system enhances the\naccuracy of disease prediction and ensures that the recommendations are\nrelevant and easily understood by individual patients. Our research highlights\nthe potential of integrating advanced technologies to overcome existing\nchallenges in medical diagnostics, paving the way for future developments in\nintelligent healthcare systems. Additionally, the system is validated using 200\nsynthetic patient data records, ensuring robust performance and reliability.",
      "tldr_zh": "本研究提出 MLtoGAI 系统，通过整合 Semantic Web 技术和 Machine Learning，提升疾病预测准确性和个性化推荐。系统包括可重用疾病本体、基于患者症状的诊断分类模型，以及使用 Semantic Web Rule Language (SWRL) 和 ChatGPT 生成易懂的健康建议。该方法显著提高了预测准确性，并在 200 条合成患者数据上验证了其可靠性和用户满意度，最终为智能医疗诊断提供了更可访问和透明的解决方案。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.20284v1",
      "published_date": "2024-07-26 06:32:06 UTC",
      "updated_date": "2024-07-26 06:32:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:28:12.609038"
    },
    {
      "arxiv_id": "2407.18532v1",
      "title": "Outer Approximation and Super-modular Cuts for Constrained Assortment Optimization under Mixed-Logit Model",
      "title_zh": "翻译失败",
      "authors": [
        "Hoang Giang Pham",
        "Tien Mai"
      ],
      "abstract": "In this paper, we study the assortment optimization problem under the\nmixed-logit customer choice model. While assortment optimization has been a\nmajor topic in revenue management for decades, the mixed-logit model is\nconsidered one of the most general and flexible approaches for modeling and\npredicting customer purchasing behavior. Existing exact methods have primarily\nrelied on mixed-integer linear programming (MILP) or second-order cone (CONIC)\nreformulations, which allow for exact problem solving using off-the-shelf\nsolvers. However, these approaches often suffer from weak continuous\nrelaxations and are slow when solving large instances. Our work addresses the\nproblem by focusing on components of the objective function that can be proven\nto be monotonically super-modular and convex. This allows us to derive valid\ncuts to outer-approximate the nonlinear objective functions. We then\ndemonstrate that these valid cuts can be incorporated into Cutting Plane or\nBranch-and-Cut methods to solve the problem exactly. Extensive experiments show\nthat our approaches consistently outperform previous methods in terms of both\nsolution quality and computation time.",
      "tldr_zh": "本文研究了混合Logit模型(Mixed-Logit Model)下的约束产品组合优化问题，旨在解决现有方法如混合整数线性规划(MILP)和二次锥(CONIC)重构在计算效率和连续松弛方面的不足。作者通过证明目标函数的组件为单调超模和凸的，推导出有效的Super-modular Cuts和Outer Approximation来外逼近非线性目标函数，并将其整合到Cutting Plane或Branch-and-Cut方法中进行精确求解。实验结果显示，该方法在解决方案质量和计算时间上均显著优于现有方法。",
      "categories": [
        "math.OC",
        "cs.AI"
      ],
      "primary_category": "math.OC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18532v1",
      "published_date": "2024-07-26 06:27:11 UTC",
      "updated_date": "2024-07-26 06:27:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:28:25.074583"
    },
    {
      "arxiv_id": "2407.18525v1",
      "title": "Is larger always better? Evaluating and prompting large language models for non-generative medical tasks",
      "title_zh": "更大总是更好吗？评估和提示大型语言模型用于非生成式医疗任务",
      "authors": [
        "Yinghao Zhu",
        "Junyi Gao",
        "Zixiang Wang",
        "Weibin Liao",
        "Xiaochen Zheng",
        "Lifang Liang",
        "Yasha Wang",
        "Chengwei Pan",
        "Ewen M. Harrison",
        "Liantao Ma"
      ],
      "abstract": "The use of Large Language Models (LLMs) in medicine is growing, but their\nability to handle both structured Electronic Health Record (EHR) data and\nunstructured clinical notes is not well-studied. This study benchmarks various\nmodels, including GPT-based LLMs, BERT-based models, and traditional clinical\npredictive models, for non-generative medical tasks utilizing renowned\ndatasets. We assessed 14 language models (9 GPT-based and 5 BERT-based) and 7\ntraditional predictive models using the MIMIC dataset (ICU patient records) and\nthe TJH dataset (early COVID-19 EHR data), focusing on tasks such as mortality\nand readmission prediction, disease hierarchy reconstruction, and biomedical\nsentence matching, comparing both zero-shot and finetuned performance. Results\nindicated that LLMs exhibited robust zero-shot predictive capabilities on\nstructured EHR data when using well-designed prompting strategies, frequently\nsurpassing traditional models. However, for unstructured medical texts, LLMs\ndid not outperform finetuned BERT models, which excelled in both supervised and\nunsupervised tasks. Consequently, while LLMs are effective for zero-shot\nlearning on structured data, finetuned BERT models are more suitable for\nunstructured texts, underscoring the importance of selecting models based on\nspecific task requirements and data characteristics to optimize the application\nof NLP technology in healthcare.",
      "tldr_zh": "本研究评估了 Large Language Models (LLMs)，包括 GPT-based 和 BERT-based 模型，在非生成医疗任务中的性能，比较了其零-shot 和 finetuned 表现，使用 MIMIC 和 TJH 数据集进行基准测试，涵盖任务如死亡率预测、再入院预测和疾病层次重建。\n结果表明，LLMs 通过精心设计的提示策略，在结构化 Electronic Health Record (EHR) 数据上显示出强劲的零-shot 能力，常超过传统预测模型；然而，在非结构化医疗文本上，finetuned BERT 模型的表现更优。\n这强调了根据具体任务需求和数据特性选择模型的重要性，以提升 NLP 在医疗领域的应用效率。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "arXiv admin note: text overlap with arXiv:2402.01713",
      "pdf_url": "http://arxiv.org/pdf/2407.18525v1",
      "published_date": "2024-07-26 06:09:10 UTC",
      "updated_date": "2024-07-26 06:09:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:28:42.297952"
    },
    {
      "arxiv_id": "2407.18524v1",
      "title": "She Works, He Works: A Curious Exploration of Gender Bias in AI-Generated Imagery",
      "title_zh": "翻译失败",
      "authors": [
        "Amalia Foka"
      ],
      "abstract": "This paper examines gender bias in AI-generated imagery of construction\nworkers, highlighting discrepancies in the portrayal of male and female\nfigures. Grounded in Griselda Pollock's theories on visual culture and gender,\nthe analysis reveals that AI models tend to sexualize female figures while\nportraying male figures as more authoritative and competent. These findings\nunderscore AI's potential to mirror and perpetuate societal biases, emphasizing\nthe need for critical engagement with AI-generated content. The project\ncontributes to discussions on the ethical implications of AI in creative\npractices and its broader impact on cultural perceptions of gender.",
      "tldr_zh": "这篇论文探讨了 AI 生成图像中建筑工人的性别偏见（gender bias），通过 Griselda Pollock 的视觉文化和性别理论进行分析。\n研究发现，AI 模型倾向于将女性形象性感化，而将男性形象描绘为更权威和胜任。\n这些结果突出了 AI 可能反映并加剧社会偏见，强调需要批判性地处理 AI-generated imagery，并在创意实践中关注其伦理影响和对文化性别认知的更广泛作用。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CV",
        "I.2.0; J.5"
      ],
      "primary_category": "cs.CY",
      "comment": "11 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.18524v1",
      "published_date": "2024-07-26 05:56:18 UTC",
      "updated_date": "2024-07-26 05:56:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:28:50.455333"
    },
    {
      "arxiv_id": "2407.18521v4",
      "title": "Patched MOA: optimizing inference for diverse software development tasks",
      "title_zh": "翻译失败",
      "authors": [
        "Asankhaya Sharma"
      ],
      "abstract": "This paper introduces Patched MOA (Mixture of Agents), an inference\noptimization technique that significantly enhances the performance of large\nlanguage models (LLMs) across diverse software development tasks. We evaluate\nthree inference optimization algorithms - Best of N, Mixture of Agents, and\nMonte Carlo Tree Search and demonstrate that Patched MOA can boost the\nperformance of smaller models to surpass that of larger, more expensive models.\nNotably, our approach improves the gpt-4o-mini model's performance on the\nArena-Hard-Auto benchmark by 15.52%, outperforming gpt-4-turbo at a fraction of\nthe cost. We also apply Patched MOA to various software development workflows,\nshowing consistent improvements in task completion rates. Our method is\nmodel-agnostic, transparent to end-users, and can be easily integrated into\nexisting LLM pipelines. This work contributes to the growing field of LLM\noptimization, offering a cost-effective solution for enhancing model\nperformance without the need for fine-tuning or larger models. Our\nimplementation is open-source and available at\nhttps://github.com/codelion/optillm.",
      "tldr_zh": "本论文引入了 Patched MOA（Mixture of Agents），一种推断优化技术，用于提升大型语言模型（LLMs）在多样化软件开发任务中的性能。研究评估了三种算法——Best of N、Mixture of Agents 和 Monte Carlo Tree Search，发现 Patched MOA 能使较小模型如 gpt-4o-mini 在 Arena-Hard-Auto 基准上性能提升 15.52%，甚至超越更大模型 gpt-4-turbo，同时大幅降低成本。该方法模型无关、对用户透明、易于集成现有 LLM 管道，并展示了在各种软件开发工作流中的任务完成率改善，为 LLM 优化提供了一种无需微调或更大模型的成本有效解决方案。开源实现可从 https://github.com/codelion/optillm 获取。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18521v4",
      "published_date": "2024-07-26 05:34:34 UTC",
      "updated_date": "2025-04-29 23:31:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:29:04.369814"
    },
    {
      "arxiv_id": "2407.18519v1",
      "title": "TCGPN: Temporal-Correlation Graph Pre-trained Network for Stock Forecasting",
      "title_zh": "翻译失败",
      "authors": [
        "Wenbo Yan",
        "Ying Tan"
      ],
      "abstract": "Recently, the incorporation of both temporal features and the correlation\nacross time series has become an effective approach in time series prediction.\nSpatio-Temporal Graph Neural Networks (STGNNs) demonstrate good performance on\nmany Temporal-correlation Forecasting Problem. However, when applied to tasks\nlacking periodicity, such as stock data prediction, the effectiveness and\nrobustness of STGNNs are found to be unsatisfactory. And STGNNs are limited by\nmemory savings so that cannot handle problems with a large number of nodes. In\nthis paper, we propose a novel approach called the Temporal-Correlation Graph\nPre-trained Network (TCGPN) to address these limitations. TCGPN utilize\nTemporal-correlation fusion encoder to get a mixed representation and\npre-training method with carefully designed temporal and correlation\npre-training tasks. Entire structure is independent of the number and order of\nnodes, so better results can be obtained through various data enhancements. And\nmemory consumption during training can be significantly reduced through\nmultiple sampling. Experiments are conducted on real stock market data sets\nCSI300 and CSI500 that exhibit minimal periodicity. We fine-tune a simple MLP\nin downstream tasks and achieve state-of-the-art results, validating the\ncapability to capture more robust temporal correlation patterns.",
      "tldr_zh": "本研究针对Spatio-Temporal Graph Neural Networks (STGNNs)在缺乏周期性的时间序列预测任务（如股票预测）中存在有效性不足和内存限制问题，提出了一种新型方法Temporal-Correlation Graph Pre-trained Network (TCGPN)。TCGPN通过Temporal-correlation fusion encoder获取混合表示，并结合精心设计的预训练任务，包括时间和相关性预训练，以提升鲁棒性。整个结构独立于节点数量和顺序，支持数据增强和多重采样，从而显著减少训练内存消耗。在真实股票数据集CSI300和CSI500上进行实验，通过微调一个简单的MLP模型，TCGPN实现了state-of-the-art结果，证明了其捕捉更鲁棒的时间相关模式的能力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-fin.ST",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18519v1",
      "published_date": "2024-07-26 05:27:26 UTC",
      "updated_date": "2024-07-26 05:27:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:29:14.291831"
    },
    {
      "arxiv_id": "2407.18517v1",
      "title": "SLIM: Style-Linguistics Mismatch Model for Generalized Audio Deepfake Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Yi Zhu",
        "Surya Koppisetti",
        "Trang Tran",
        "Gaurav Bharaj"
      ],
      "abstract": "Audio deepfake detection (ADD) is crucial to combat the misuse of speech\nsynthesized from generative AI models. Existing ADD models suffer from\ngeneralization issues, with a large performance discrepancy between in-domain\nand out-of-domain data. Moreover, the black-box nature of existing models\nlimits their use in real-world scenarios, where explanations are required for\nmodel decisions. To alleviate these issues, we introduce a new ADD model that\nexplicitly uses the StyleLInguistics Mismatch (SLIM) in fake speech to separate\nthem from real speech. SLIM first employs self-supervised pretraining on only\nreal samples to learn the style-linguistics dependency in the real class. The\nlearned features are then used in complement with standard pretrained acoustic\nfeatures (e.g., Wav2vec) to learn a classifier on the real and fake classes.\nWhen the feature encoders are frozen, SLIM outperforms benchmark methods on\nout-of-domain datasets while achieving competitive results on in-domain data.\nThe features learned by SLIM allow us to quantify the (mis)match between style\nand linguistic content in a sample, hence facilitating an explanation of the\nmodel decision.",
      "tldr_zh": "论文提出SLIM（Style-Linguistics Mismatch）模型，用于泛化音频深度伪造检测（Audio Deepfake Detection），旨在解决现有模型的泛化问题（如域内域外性能差异）和黑盒解释性不足。SLIM通过在真实样本上进行自监督预训练，学习风格-语言依赖，并结合Wav2vec等预训练声学特征来训练分类器，从而区分真假语音。实验结果表明，SLIM在特征编码器冻结的情况下，优于基准方法，尤其在域外数据集上提升显著，并能量化样本中风格和语言内容的匹配程度，提供模型决策解释。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18517v1",
      "published_date": "2024-07-26 05:23:41 UTC",
      "updated_date": "2024-07-26 05:23:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:29:26.277874"
    },
    {
      "arxiv_id": "2407.18499v3",
      "title": "Non-Overlapping Placement of Macro Cells based on Reinforcement Learning in Chip Design",
      "title_zh": "翻译失败",
      "authors": [
        "Tao Yu",
        "Peng Gao",
        "Fei Wang",
        "Ru-Yue Yuan"
      ],
      "abstract": "Due to the increasing complexity of chip design, existing placement methods\nstill have many shortcomings in dealing with macro cells coverage and\noptimization efficiency. Aiming at the problems of layout overlap, inferior\nperformance, and low optimization efficiency in existing chip design methods,\nthis paper proposes an end-to-end placement method, SRLPlacer, based on\nreinforcement learning. First, the placement problem is transformed into a\nMarkov decision process by establishing the coupling relationship graph model\nbetween macro cells to learn the strategy for optimizing layouts. Secondly, the\nwhole placement process is optimized after integrating the standard cell\nlayout. By assessing on the public benchmark ISPD2005, the proposed SRLPlacer\ncan effectively solve the overlap problem between macro cells while considering\nrouting congestion and shortening the total wire length to ensure routability.\nCodes are available at https://github.com/zhouyusd/SRLPlacer.",
      "tldr_zh": "针对芯片设计中宏单元（macro cells）的布局重叠、性能低下和优化效率问题，本文提出了一种基于Reinforcement Learning的端到端放置方法SRLPlacer。\n该方法首先通过建立宏单元之间的耦合关系图模型，将放置问题转化为Markov decision process，以学习优化布局策略；其次，整合标准单元布局来进一步优化整个放置过程。\n在ISPD2005公共基准测试中，SRLPlacer有效解决了宏单元重叠问题，同时考虑布线拥塞并缩短总线长，确保了设计的可布线性。",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18499v3",
      "published_date": "2024-07-26 04:15:54 UTC",
      "updated_date": "2024-09-29 09:41:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:29:38.623881"
    },
    {
      "arxiv_id": "2407.18498v1",
      "title": "A Reliable Common-Sense Reasoning Socialbot Built Using LLMs and Goal-Directed ASP",
      "title_zh": "翻译失败",
      "authors": [
        "Yankai Zeng",
        "Abhiramon Rajashekharan",
        "Kinjal Basu",
        "Huaduo Wang",
        "Joaquín Arias",
        "Gopal Gupta"
      ],
      "abstract": "The development of large language models (LLMs), such as GPT, has enabled the\nconstruction of several socialbots, like ChatGPT, that are receiving a lot of\nattention for their ability to simulate a human conversation. However, the\nconversation is not guided by a goal and is hard to control. In addition,\nbecause LLMs rely more on pattern recognition than deductive reasoning, they\ncan give confusing answers and have difficulty integrating multiple topics into\na cohesive response. These limitations often lead the LLM to deviate from the\nmain topic to keep the conversation interesting. We propose AutoCompanion, a\nsocialbot that uses an LLM model to translate natural language into predicates\n(and vice versa) and employs commonsense reasoning based on Answer Set\nProgramming (ASP) to hold a social conversation with a human. In particular, we\nrely on s(CASP), a goal-directed implementation of ASP as the backend. This\npaper presents the framework design and how an LLM is used to parse user\nmessages and generate a response from the s(CASP) engine output. To validate\nour proposal, we describe (real) conversations in which the chatbot's goal is\nto keep the user entertained by talking about movies and books, and s(CASP)\nensures (i) correctness of answers, (ii) coherence (and precision) during the\nconversation, which it dynamically regulates to achieve its specific purpose,\nand (iii) no deviation from the main topic.",
      "tldr_zh": "该论文提出 AutoCompanion，一种可靠的常识推理社交机器人，使用大型语言模型 (LLMs) 将自然语言转化为谓词，并结合目标导向 Answer Set Programming (ASP) 的 s(CASP) 后端进行推理，以解决现有 LLMs 社交机器人的对话无目标、易偏题和答案混乱等问题。框架设计中，LLMs 负责解析用户消息和生成响应，而 s(CASP) 确保对话的正确性、一致性和精确性，同时动态调节以实现特定目标，如保持用户娱乐。实验通过真实对话验证了 AutoCompanion 在谈论电影和书籍时的有效性，避免了主题偏差，并提升了整体对话质量。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18498v1",
      "published_date": "2024-07-26 04:13:43 UTC",
      "updated_date": "2024-07-26 04:13:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:30:16.944010"
    },
    {
      "arxiv_id": "2407.21059v1",
      "title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks",
      "title_zh": "翻译失败",
      "authors": [
        "Yunfan Gao",
        "Yun Xiong",
        "Meng Wang",
        "Haofen Wang"
      ],
      "abstract": "Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\nof Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\nincreasing demands of application scenarios have driven the evolution of RAG,\nleading to the integration of advanced retrievers, LLMs and other complementary\ntechnologies, which in turn has amplified the intricacy of RAG systems.\nHowever, the rapid advancements are outpacing the foundational RAG paradigm,\nwith many methods struggling to be unified under the process of\n\"retrieve-then-generate\". In this context, this paper examines the limitations\nof the existing RAG paradigm and introduces the modular RAG framework. By\ndecomposing complex RAG systems into independent modules and specialized\noperators, it facilitates a highly reconfigurable framework. Modular RAG\ntranscends the traditional linear architecture, embracing a more advanced\ndesign that integrates routing, scheduling, and fusion mechanisms. Drawing on\nextensive research, this paper further identifies prevalent RAG\npatterns-linear, conditional, branching, and looping-and offers a comprehensive\nanalysis of their respective implementation nuances. Modular RAG presents\ninnovative opportunities for the conceptualization and deployment of RAG\nsystems. Finally, the paper explores the potential emergence of new operators\nand paradigms, establishing a solid theoretical foundation and a practical\nroadmap for the continued evolution and practical deployment of RAG\ntechnologies.",
      "tldr_zh": "这篇论文探讨了Retrieval-augmented Generation (RAG) 系统的局限性，特别是传统“retrieve-then-generate”范式的复杂性和统一性问题。论文提出Modular RAG框架，将RAG系统分解为独立的模块和专用操作符，实现LEGO-like的可重新配置设计，并整合路由、调度和融合机制。基于对线性、条件、分支和循环等常见RAG模式的全面分析，该框架为RAG系统的创新部署提供理论基础和实用路线图，促进Large Language Models (LLMs)在知识密集型任务中的更灵活应用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21059v1",
      "published_date": "2024-07-26 03:45:30 UTC",
      "updated_date": "2024-07-26 03:45:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:30:11.603482"
    },
    {
      "arxiv_id": "2407.18483v4",
      "title": "A Role-specific Guided Large Language Model for Ophthalmic Consultation Based on Stylistic Differentiation",
      "title_zh": "翻译失败",
      "authors": [
        "Laiyi Fu",
        "Binbin Fan",
        "Hongkai Du",
        "Yanxiang Feng",
        "Chunhua Li",
        "Huping Song"
      ],
      "abstract": "Ophthalmology consultations are crucial for diagnosing, treating, and\npreventing eye diseases. However, the growing demand for consultations exceeds\nthe availability of ophthalmologists. By leveraging large pre-trained language\nmodels, we can design effective dialogues for specific scenarios, aiding in\nconsultations. Traditional fine-tuning strategies for question-answering tasks\nare impractical due to increasing model size and often ignoring patient-doctor\nrole function during consultations. In this paper, we propose EyeDoctor, an\nophthalmic medical questioning large language model that enhances accuracy\nthrough doctor-patient role perception guided and an augmented knowledge base\nwith external disease information. Experimental results show EyeDoctor achieves\nhigher question-answering precision in ophthalmology consultations. Notably,\nEyeDoctor demonstrated a 7.25% improvement in Rouge-1 scores and a 10.16%\nimprovement in F1 scores on multi-round datasets compared to second best model\nChatGPT, highlighting the importance of doctor-patient role differentiation and\ndynamic knowledge base expansion for intelligent medical consultations. EyeDoc\nalso serves as a free available web based service and souce code is available\nat https://github.com/sperfu/EyeDoc.",
      "tldr_zh": "该研究针对眼科咨询需求增长与医生短缺的问题，提出EyeDoctor，一种基于风格差异的角色特定引导大型语言模型（Large Language Model）。EyeDoctor通过医生-患者角色感知机制和增强的外部疾病知识库，提高了眼科咨询的问答准确性。实验结果显示，与ChatGPT相比，EyeDoctor在多轮数据集上实现了Rouge-1分数提高7.25%和F1 scores提高10.16%，突显了角色区分和动态知识扩展的重要性。该模型已作为免费网络服务发布，源代码可在GitHub获取。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18483v4",
      "published_date": "2024-07-26 03:23:31 UTC",
      "updated_date": "2024-07-31 07:24:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:30:25.631588"
    },
    {
      "arxiv_id": "2407.18468v3",
      "title": "Diffusion-Driven Semantic Communication for Generative Models with Bandwidth Constraints",
      "title_zh": "翻译失败",
      "authors": [
        "Lei Guo",
        "Wei Chen",
        "Yuxuan Sun",
        "Bo Ai",
        "Nikolaos Pappas",
        "Tony Quek"
      ],
      "abstract": "Diffusion models have been extensively utilized in AI-generated content\n(AIGC) in recent years, thanks to the superior generation capabilities.\nCombining with semantic communications, diffusion models are used for tasks\nsuch as denoising, data reconstruction, and content generation. However,\nexisting diffusion-based generative models do not consider the stringent\nbandwidth limitation, which limits its application in wireless communication.\nThis paper introduces a diffusion-driven semantic communication framework with\nadvanced VAE-based compression for bandwidth-constrained generative model. Our\ndesigned architecture utilizes the diffusion model, where the signal\ntransmission process through the wireless channel acts as the forward process\nin diffusion. To reduce bandwidth requirements, we incorporate a downsampling\nmodule and a paired upsampling module based on a variational auto-encoder with\nreparameterization at the receiver to ensure that the recovered features\nconform to the Gaussian distribution. Furthermore, we derive the loss function\nfor our proposed system and evaluate its performance through comprehensive\nexperiments. Our experimental results demonstrate significant improvements in\npixel-level metrics such as peak signal to noise ratio (PSNR) and semantic\nmetrics like learned perceptual image patch similarity (LPIPS). These\nenhancements are more profound regarding the compression rates and SNR compared\nto deep joint source-channel coding (DJSCC).",
      "tldr_zh": "本文提出了一种基于扩散模型的语义通信框架，旨在解决带宽受限条件下生成模型的应用问题。该框架将无线通道传输过程视为扩散模型的正向过程，并引入 VAE-based 压缩模块，包括下采样和上采样，以确保恢复特征符合高斯分布，同时推导了相应的损失函数。实验结果表明，该系统在像素级指标如 PSNR 和语义指标如 LPIPS 上取得了显著改善，尤其在压缩率和 SNR 方面优于 DJSCC 基准方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "accepted to IEEE for possible publication",
      "pdf_url": "http://arxiv.org/pdf/2407.18468v3",
      "published_date": "2024-07-26 02:34:25 UTC",
      "updated_date": "2025-03-23 09:36:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:30:36.479831"
    },
    {
      "arxiv_id": "2407.18992v1",
      "title": "Towards Automated Solution Recipe Generation for Industrial Asset Management with LLM",
      "title_zh": "翻译失败",
      "authors": [
        "Nianjun Zhou",
        "Dhaval Patel",
        "Shuxin Lin",
        "Fearghal O'Donncha"
      ],
      "abstract": "This study introduces a novel approach to Industrial Asset Management (IAM)\nby incorporating Conditional-Based Management (CBM) principles with the latest\nadvancements in Large Language Models (LLMs). Our research introduces an\nautomated model-building process, traditionally reliant on intensive\ncollaboration between data scientists and domain experts. We present two\nprimary innovations: a taxonomy-guided prompting generation that facilitates\nthe automatic creation of AI solution recipes and a set of LLM pipelines\ndesigned to produce a solution recipe containing a set of artifacts composed of\ndocuments, sample data, and models for IAM. These pipelines, guided by\nstandardized principles, enable the generation of initial solution templates\nfor heterogeneous asset classes without direct human input, reducing reliance\non extensive domain knowledge and enhancing automation. We evaluate our\nmethodology by assessing asset health and sustainability across a spectrum of\nten asset classes. Our findings illustrate the potential of LLMs and\ntaxonomy-based LLM prompting pipelines in transforming asset management,\noffering a blueprint for subsequent research and development initiatives to be\nintegrated into a rapid client solution.",
      "tldr_zh": "本研究提出了一种将大型语言模型（LLMs）融入工业资产管理（IAM）的创新方法，旨在自动化解决方案配方生成，减少对数据科学家和领域专家的密集合作依赖。关键创新包括taxonomy-guided prompting生成技术，用于自动创建AI解决方案配方，以及一套基于标准化原则的LLM pipelines，能够生成包含文档、样本数据和模型的初始模板，适用于不同资产类别。实验评估显示，该方法在十个资产类别上提升了资产健康和可持续性评估的效率，为未来IAM研究提供了一个减少人类干预的蓝图。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18992v1",
      "published_date": "2024-07-26 01:24:52 UTC",
      "updated_date": "2024-07-26 01:24:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:30:48.805037"
    },
    {
      "arxiv_id": "2407.18454v1",
      "title": "Fairness Definitions in Language Models Explained",
      "title_zh": "翻译失败",
      "authors": [
        "Thang Viet Doan",
        "Zhibo Chu",
        "Zichong Wang",
        "Wenbin Zhang"
      ],
      "abstract": "Language Models (LMs) have demonstrated exceptional performance across\nvarious Natural Language Processing (NLP) tasks. Despite these advancements,\nLMs can inherit and amplify societal biases related to sensitive attributes\nsuch as gender and race, limiting their adoption in real-world applications.\nTherefore, fairness has been extensively explored in LMs, leading to the\nproposal of various fairness notions. However, the lack of clear agreement on\nwhich fairness definition to apply in specific contexts (\\textit{e.g.,}\nmedium-sized LMs versus large-sized LMs) and the complexity of understanding\nthe distinctions between these definitions can create confusion and impede\nfurther progress. To this end, this paper proposes a systematic survey that\nclarifies the definitions of fairness as they apply to LMs. Specifically, we\nbegin with a brief introduction to LMs and fairness in LMs, followed by a\ncomprehensive, up-to-date overview of existing fairness notions in LMs and the\nintroduction of a novel taxonomy that categorizes these concepts based on their\nfoundational principles and operational distinctions. We further illustrate\neach definition through experiments, showcasing their practical implications\nand outcomes. Finally, we discuss current research challenges and open\nquestions, aiming to foster innovative ideas and advance the field. The\nimplementation and additional resources are publicly available at\nhttps://github.com/LavinWong/Fairness-in-Large-Language-Models/tree/main/definitions.",
      "tldr_zh": "这篇论文系统地调查了 Language Models (LMs) 中的公平性定义，旨在解决 LMs 继承社会偏见（如性别和种族）并放大这些偏见的问题。通过引入一个新的 taxonomy 分类这些公平概念，并基于基础原则和操作区别进行整理，该研究通过实验展示了每个定义的实际含义和影响。最后，论文讨论了当前研究挑战和开放问题，并公开了实现代码和资源，以促进 LMs 公平性的进一步发展。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18454v1",
      "published_date": "2024-07-26 01:21:25 UTC",
      "updated_date": "2024-07-26 01:21:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:31:02.050227"
    },
    {
      "arxiv_id": "2408.06357v1",
      "title": "Algorithm Research of ELMo Word Embedding and Deep Learning Multimodal Transformer in Image Description",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaohan Cheng",
        "Taiyuan Mei",
        "Yun Zi",
        "Qi Wang",
        "Zijun Gao",
        "Haowei Yang"
      ],
      "abstract": "Zero sample learning is an effective method for data deficiency. The existing\nembedded zero sample learning methods only use the known classes to construct\nthe embedded space, so there is an overfitting of the known classes in the\ntesting process. This project uses category semantic similarity measures to\nclassify multiple tags. This enables it to incorporate unknown classes that\nhave the same meaning as currently known classes into the vector space when it\nis built. At the same time, most of the existing zero sample learning\nalgorithms directly use the depth features of medical images as input, and the\nfeature extraction process does not consider semantic information. This project\nintends to take ELMo-MCT as the main task and obtain multiple visual features\nrelated to the original image through self-attention mechanism. In this paper,\na large number of experiments are carried out on three zero-shot learning\nreference datasets, and the best harmonic average accuracy is obtained compared\nwith the most advanced algorithms.",
      "tldr_zh": "本研究针对零样本学习（Zero Sample Learning）中的过度拟合问题，提出使用类别语义相似性措施来构建嵌入空间，从而将与已知类别语义相似的未知类别纳入其中，同时采用 ELMo 词嵌入和深度学习多模态 Transformer（Deep Learning Multimodal Transformer）作为主要任务。方法通过自注意力机制（self-attention mechanism）提取医疗图像的多重视觉特征，增强语义信息整合。在三个零样本学习参考数据集上进行的大量实验中，该方法取得了比现有先进算法更高的调和平均准确率（harmonic average accuracy）。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.06357v1",
      "published_date": "2024-07-26 01:12:19 UTC",
      "updated_date": "2024-07-26 01:12:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:31:16.078087"
    },
    {
      "arxiv_id": "2407.18445v1",
      "title": "Capturing the security expert knowledge in feature selection for web application attack detection",
      "title_zh": "翻译失败",
      "authors": [
        "Amanda Riverol",
        "Gustavo Betarte",
        "Rodrigo Martínez",
        "Álvaro Pardo"
      ],
      "abstract": "This article puts forward the use of mutual information values to replicate\nthe expertise of security professionals in selecting features for detecting web\nattacks. The goal is to enhance the effectiveness of web application firewalls\n(WAFs). Web applications are frequently vulnerable to various security threats,\nmaking WAFs essential for their protection. WAFs analyze HTTP traffic using\nrule-based approaches to identify known attack patterns and to detect and block\npotential malicious requests. However, a major challenge is the occurrence of\nfalse positives, which can lead to blocking legitimate traffic and impact the\nnormal functioning of the application. The problem is addressed as an approach\nthat combines supervised learning for feature selection with a semi-supervised\nlearning scenario for training a One-Class SVM model. The experimental findings\nshow that the model trained with features selected by the proposed algorithm\noutperformed the expert-based selection approach in terms of performance.\nAdditionally, the results obtained by the traditional rule-based WAF\nModSecurity, configured with a vanilla set of OWASP CRS rules, were also\nimproved.",
      "tldr_zh": "这篇论文提出使用 mutual information values 来捕捉安全专家知识，进行特征选择，以提升 web application firewalls (WAFs) 在检测网络应用攻击时的有效性。方法结合 supervised learning 用于特征选择，以及 semi-supervised learning 训练 One-Class SVM 模型，旨在减少 false positives 并改善攻击检测准确性。实验结果显示，该算法选择的特征训练模型在性能上优于专家-based 选择方法，并超过了传统规则-based WAF 如 ModSecurity with OWASP CRS rules。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18445v1",
      "published_date": "2024-07-26 00:56:11 UTC",
      "updated_date": "2024-07-26 00:56:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:31:26.462565"
    },
    {
      "arxiv_id": "2407.18437v1",
      "title": "Mixed Non-linear Quantization for Vision Transformers",
      "title_zh": "翻译失败",
      "authors": [
        "Gihwan Kim",
        "Jemin Lee",
        "Sihyeong Park",
        "Yongin Kwon",
        "Hyungshin Kim"
      ],
      "abstract": "The majority of quantization methods have been proposed to reduce the model\nsize of Vision Transformers, yet most of them have overlooked the quantization\nof non-linear operations. Only a few works have addressed quantization for\nnon-linear operations, but they applied a single quantization method across all\nnon-linear operations. We believe that this can be further improved by\nemploying a different quantization method for each non-linear operation.\nTherefore, to assign the most error-minimizing quantization method from the\nknown methods to each non-linear layer, we propose a mixed non-linear\nquantization that considers layer-wise quantization sensitivity measured by\nSQNR difference metric. The results show that our method outperforms I-BERT,\nFQ-ViT, and I-ViT in both 8-bit and 6-bit settings for ViT, DeiT, and Swin\nmodels by an average of 0.6%p and 19.6%p, respectively. Our method outperforms\nI-BERT and I-ViT by 0.6%p and 20.8%p, respectively, when training time is\nlimited. We plan to release our code at\nhttps://gitlab.com/ones-ai/mixed-non-linear-quantization.",
      "tldr_zh": "本文提出了一种混合非线性量化（mixed non-linear quantization）方法，用于优化 Vision Transformers 的模型大小问题，特别针对非线性操作的量化。该方法通过评估每个非线性层的量化敏感性（使用 SQNR 差异指标），为不同层分配最能最小化错误的量化策略，从而超越了单一量化方法的局限性。在实验中，该方法在 8-bit 和 6-bit 设置下，使 ViT、DeiT 和 Swin 模型的性能分别比 I-BERT、FQ-ViT 和 I-ViT 平均提高了 0.6% 和 19.6%；在训练时间有限的情况下，也实现了 0.6% 和 20.8% 的提升。作者计划公开代码，以促进进一步研究。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "16 pages, 4 figures, under review",
      "pdf_url": "http://arxiv.org/pdf/2407.18437v1",
      "published_date": "2024-07-26 00:19:01 UTC",
      "updated_date": "2024-07-26 00:19:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:31:37.028972"
    },
    {
      "arxiv_id": "2407.18433v1",
      "title": "Investigating the Privacy Risk of Using Robot Vacuum Cleaners in Smart Environments",
      "title_zh": "在智能环境中使用机器人吸尘器的隐私风险调查",
      "authors": [
        "Benjamin Ulsmaag",
        "Jia-Chun Lin",
        "Ming-Chang Lee"
      ],
      "abstract": "Robot vacuum cleaners have become increasingly popular and are widely used in\nvarious smart environments. To improve user convenience, manufacturers also\nintroduced smartphone applications that enable users to customize cleaning\nsettings or access information about their robot vacuum cleaners. While this\nintegration enhances the interaction between users and their robot vacuum\ncleaners, it results in potential privacy concerns because users' personal\ninformation may be exposed. To address these concerns, end-to-end encryption is\nimplemented between the application, cloud service, and robot vacuum cleaners\nto secure the exchanged information. Nevertheless, network header metadata\nremains unencrypted and it is still vulnerable to network eavesdropping. In\nthis paper, we investigate the potential risk of private information exposure\nthrough such metadata. A popular robot vacuum cleaner was deployed in a real\nsmart environment where passive network eavesdropping was conducted during\nseveral selected cleaning events. Our extensive analysis, based on Association\nRule Learning, demonstrates that it is feasible to identify certain events\nusing only the captured Internet traffic metadata, thereby potentially exposing\nprivate user information and raising privacy concerns.",
      "tldr_zh": "本文研究了在智能环境中使用机器人吸尘器的隐私风险，重点关注智能手机应用与设备的集成可能导致的个人信息暴露问题，尽管已实施端到端加密，但网络 header metadata 仍易受网络窃听。研究团队在真实环境中部署了一个流行机器人吸尘器，进行被动网络 eavesdropping，并运用 Association Rule Learning 分析捕获的互联网流量元数据。结果表明，通过这些元数据可识别特定清洁事件，从而潜在暴露用户私人信息，强调了现有隐私保护措施的不足。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "18 pages, 11 figures, 4 tables, The 26th International Conference on\n  Information and Communications Security, 26-28 August, 2024, Mytilene,\n  Lesvos, Greece (ICICS2024)",
      "pdf_url": "http://arxiv.org/pdf/2407.18433v1",
      "published_date": "2024-07-26 00:00:53 UTC",
      "updated_date": "2024-07-26 00:00:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:31:52.106875"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 86,
  "processed_papers_count": 86,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-19T10:32:21.524889"
}
[
  {
    "arxiv_id": "2510.06517v1",
    "title": "Visualizing Multimodality in Combinatorial Search Landscapes",
    "authors": [
      "Xavier F. C. Sánchez-Díaz",
      "Ole Jakob Mengshoel"
    ],
    "abstract": "This work walks through different visualization techniques for combinatorial search landscapes, focusing on multimodality. We discuss different techniques from the landscape analysis literature, and how they can be combined to provide a more comprehensive view of the search landscape. We also include examples and discuss relevant work to show how others have used these techniques in practice, based on the geometric and aesthetic elements of the Grammar of Graphics. We conclude that there is no free lunch in visualization, and provide recommendations for future work as there are several paths to continue the work in this field.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.GR",
    "comment": "18 pages, 9 figures, Poster presented at the 2025 Symposium of the Norwegian Artificial Intelligence Society (NAIS 2025) on June 18, 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.06517v1",
    "published_date": "2025-10-07 23:29:19 UTC",
    "updated_date": "2025-10-07 23:29:19 UTC"
  },
  {
    "arxiv_id": "2510.06512v1",
    "title": "LogSTOP: Temporal Scores over Prediction Sequences for Matching and Retrieval",
    "authors": [
      "Avishree Khare",
      "Hideki Okamoto",
      "Bardh Hoxha",
      "Georgios Fainekos",
      "Rajeev Alur"
    ],
    "abstract": "Neural models such as YOLO and HuBERT can be used to detect local properties such as objects (\"car\") and emotions (\"angry\") in individual frames of videos and audio clips respectively. The likelihood of these detections is indicated by scores in [0, 1]. Lifting these scores to temporal properties over sequences can be useful for several downstream applications such as query matching (e.g., \"does the speaker eventually sound happy in this audio clip?\"), and ranked retrieval (e.g., \"retrieve top 5 videos with a 10 second scene where a car is detected until a pedestrian is detected\"). In this work, we formalize this problem of assigning Scores for TempOral Properties (STOPs) over sequences, given potentially noisy score predictors for local properties. We then propose a scoring function called LogSTOP that can efficiently compute these scores for temporal properties represented in Linear Temporal Logic. Empirically, LogSTOP, with YOLO and HuBERT, outperforms Large Vision / Audio Language Models and other Temporal Logic-based baselines by at least 16% on query matching with temporal properties over objects-in-videos and emotions-in-speech respectively. Similarly, on ranked retrieval with temporal properties over objects and actions in videos, LogSTOP with Grounding DINO and SlowR50 reports at least a 19% and 16% increase in mean average precision and recall over zero-shot text-to-video retrieval baselines respectively.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06512v1",
    "published_date": "2025-10-07 23:05:20 UTC",
    "updated_date": "2025-10-07 23:05:20 UTC"
  },
  {
    "arxiv_id": "2510.06505v1",
    "title": "A Median Perspective on Unlabeled Data for Out-of-Distribution Detection",
    "authors": [
      "Momin Abbas",
      "Ali Falahati",
      "Hossein Goli",
      "Mohammad Mohammadi Amiri"
    ],
    "abstract": "Out-of-distribution (OOD) detection plays a crucial role in ensuring the robustness and reliability of machine learning systems deployed in real-world applications. Recent approaches have explored the use of unlabeled data, showing potential for enhancing OOD detection capabilities. However, effectively utilizing unlabeled in-the-wild data remains challenging due to the mixed nature of both in-distribution (InD) and OOD samples. The lack of a distinct set of OOD samples complicates the task of training an optimal OOD classifier. In this work, we introduce Medix, a novel framework designed to identify potential outliers from unlabeled data using the median operation. We use the median because it provides a stable estimate of the central tendency, as an OOD detection mechanism, due to its robustness against noise and outliers. Using these identified outliers, along with labeled InD data, we train a robust OOD classifier. From a theoretical perspective, we derive error bounds that demonstrate Medix achieves a low error rate. Empirical results further substantiate our claims, as Medix outperforms existing methods across the board in open-world settings, confirming the validity of our theoretical insights.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06505v1",
    "published_date": "2025-10-07 22:43:57 UTC",
    "updated_date": "2025-10-07 22:43:57 UTC"
  },
  {
    "arxiv_id": "2510.06503v1",
    "title": "ATLO-ML: Adaptive Time-Length Optimizer for Machine Learning -- Insights from Air Quality Forecasting",
    "authors": [
      "I-Hsi Kao",
      "Kanji Uchino"
    ],
    "abstract": "Accurate time-series predictions in machine learning are heavily influenced by the selection of appropriate input time length and sampling rate. This paper introduces ATLO-ML, an adaptive time-length optimization system that automatically determines the optimal input time length and sampling rate based on user-defined output time length. The system provides a flexible approach to time-series data pre-processing, dynamically adjusting these parameters to enhance predictive performance. ATLO-ML is validated using air quality datasets, including both GAMS-dataset and proprietary data collected from a data center, both in time series format. Results demonstrate that utilizing the optimized time length and sampling rate significantly improves the accuracy of machine learning models compared to fixed time lengths. ATLO-ML shows potential for generalization across various time-sensitive applications, offering a robust solution for optimizing temporal input parameters in machine learning workflows.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06503v1",
    "published_date": "2025-10-07 22:38:36 UTC",
    "updated_date": "2025-10-07 22:38:36 UTC"
  },
  {
    "arxiv_id": "2510.06499v1",
    "title": "Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining Levels",
    "authors": [
      "Zhepeng Cen",
      "Haolin Chen",
      "Shiyu Wang",
      "Zuxin Liu",
      "Zhiwei Liu",
      "Ding Zhao",
      "Silvio Savarese",
      "Caiming Xiong",
      "Huan Wang",
      "Weiran Yao"
    ],
    "abstract": "Large Language Models (LLMs) have achieved remarkable success through imitation learning on vast text corpora, but this paradigm creates a training-generation gap and limits robust reasoning. Reinforcement learning (RL) offers a more data-efficient solution capable of bridging this gap, yet its application has been constrained by a critical data bottleneck: existing RL datasets are orders of magnitude smaller and less diverse than web-scale pre-training corpora. To address this, we introduce the Webscale-RL pipeline, a scalable data engine that systematically converts large-scale pre-training documents into millions of diverse, verifiable question-answer pairs for RL. Using this pipeline, we construct the Webscale-RL dataset, containing 1.2 million examples across more than 9 domains. Our experiments show that the model trained on this dataset significantly outperforms continual pretraining and strong data refinement baselines across a suite of benchmarks. Notably, RL training with our dataset proves substantially more efficient, achieving the performance of continual pre-training with up to 100$\\times$ fewer tokens. Our work presents a viable path toward scaling RL to pre-training levels, enabling more capable and efficient language models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06499v1",
    "published_date": "2025-10-07 22:30:59 UTC",
    "updated_date": "2025-10-07 22:30:59 UTC"
  },
  {
    "arxiv_id": "2510.06478v2",
    "title": "Anytime-Valid Answer Sufficiency Certificates for LLM Generation via Sequential Information Lift",
    "authors": [
      "Sanjeda Akter",
      "Ibne Farabi Shihab",
      "Anuj Sharma"
    ],
    "abstract": "We introduce Sequential-EDFL (Empirical Dynamic Formal Lift), which applies anytime-valid sequential testing to language model generation stopping. Our approach tracks information lift, defined as the log-likelihood ratio between the full model and deliberately weakened \"skeleton\" baselines, using self-normalized empirical-Bernstein e-processes that provide formal delta-level error control regardless of stopping time. This delta guarantee controls premature stopping when information lift is insufficient relative to the skeleton, and it does not imply delta control of factual incorrectness or hallucinations. We handle unknown centering through online mean estimation, combine multiple parameters via mixture e-processes, and support adaptive resets under distributional drift. On six benchmarks, Sequential-EDFL reduces generation length by 22 to 28 percent relative to sequential baselines while maintaining delta-level control with 12 percent computational overhead. We introduce automated skeletons (distilled submodels and randomized logits) and show robustness across skeleton families. Composing EDFL with a lightweight correctness gate (sentence boundaries plus a verifier) improves end-task correctness while preserving anytime-valid guarantees by only delaying stopping. Our certificates control information sufficiency, not factual correctness. Specifically, 10.9 percent of stopped sequences remain incorrect even with the gate (13.2 to 22.7 percent without it). EDFL serves as a first-stage filter that can reduce verification burden: when applied to stopped sequences, the gate validates 83 percent of stops, requiring full verification only for the remaining 17 percent, plus all non-stopped sequences. EDFL is not a standalone solution for safety-critical domains.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06478v2",
    "published_date": "2025-10-07 21:28:53 UTC",
    "updated_date": "2026-01-05 17:33:34 UTC"
  },
  {
    "arxiv_id": "2510.06477v1",
    "title": "Attention Sinks and Compression Valleys in LLMs are Two Sides of the Same Coin",
    "authors": [
      "Enrique Queipo-de-Llano",
      "Álvaro Arroyo",
      "Federico Barbero",
      "Xiaowen Dong",
      "Michael Bronstein",
      "Yann LeCun",
      "Ravid Shwartz-Ziv"
    ],
    "abstract": "Attention sinks and compression valleys have attracted significant attention as two puzzling phenomena in large language models, but have been studied in isolation. In this work, we present a surprising connection between attention sinks and compression valleys, tracing both to the formation of massive activations in the residual stream. We prove theoretically that massive activations necessarily produce representational compression and establish bounds on the resulting entropy reduction. Through experiments across several models (410M-120B parameters), we confirm that when the beginning-of-sequence token develops extreme activation norms in the middle layers, both compression valleys and attention sinks emerge simultaneously. Targeted ablation studies validate our theoretical predictions. This unified view motivates us to propose the Mix-Compress-Refine theory of information flow, as an attempt to explain how LLMs organize their computation in depth by controlling attention and representational compression via massive activations. Specifically, we posit that Transformer-based LLMs process tokens in three distinct phases: (1) broad mixing in the early layers, (2) compressed computation with limited mixing in the middle layers, and (3) selective refinement in the late layers. Our framework helps explain why embedding tasks perform best at intermediate layers, whereas generation tasks benefit from full-depth processing, clarifying differences in task-dependent representations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06477v1",
    "published_date": "2025-10-07 21:27:24 UTC",
    "updated_date": "2025-10-07 21:27:24 UTC"
  },
  {
    "arxiv_id": "2510.06475v1",
    "title": "PuzzlePlex: Benchmarking Foundation Models on Reasoning and Planning with Puzzles",
    "authors": [
      "Yitao Long",
      "Yuru Jiang",
      "Hongjun Liu",
      "Yilun Zhao",
      "Jingchen Sun",
      "Yiqiu Shen",
      "Chen Zhao",
      "Arman Cohan",
      "Dennis Shasha"
    ],
    "abstract": "This work investigates the reasoning and planning capabilities of foundation models and their scalability in complex, dynamic environments. We introduce PuzzlePlex, a benchmark designed to assess these capabilities through a diverse set of puzzles. PuzzlePlex consists of 15 types of puzzles, including deterministic and stochastic games of varying difficulty, as well as single-player and two-player scenarios. The PuzzlePlex framework provides a comprehensive environment for each game, and supports extensibility to generate more challenging instances as foundation models evolve. Additionally, we implement customized game-playing strategies for comparison. Building on this benchmark, we develop fine-grained metrics to measure performance and conduct an in-depth analysis of frontier foundation models across two settings: instruction-based and code-based. Furthermore, we systematically investigate their scaling limits. Our findings show that reasoning models outperform others in instruction-based settings, while code-based execution presents greater challenges but offers a scalable and efficient alternative. PuzzlePlex enables targeted evaluation and guides future improvements in reasoning, planning, and generalization for foundation models.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06475v1",
    "published_date": "2025-10-07 21:24:29 UTC",
    "updated_date": "2025-10-07 21:24:29 UTC"
  },
  {
    "arxiv_id": "2510.06473v1",
    "title": "Deep Generative Model for Human Mobility Behavior",
    "authors": [
      "Ye Hong",
      "Yatao Zhang",
      "Konrad Schindler",
      "Martin Raubal"
    ],
    "abstract": "Understanding and modeling human mobility is central to challenges in transport planning, sustainable urban design, and public health. Despite decades of effort, simulating individual mobility remains challenging because of its complex, context-dependent, and exploratory nature. Here, we present MobilityGen, a deep generative model that produces realistic mobility trajectories spanning days to weeks at large spatial scales. By linking behavioral attributes with environmental context, MobilityGen reproduces key patterns such as scaling laws for location visits, activity time allocation, and the coupled evolution of travel mode and destination choices. It reflects spatio-temporal variability and generates diverse, plausible, and novel mobility patterns consistent with the built environment. Beyond standard validation, MobilityGen yields insights not attainable with earlier models, including how access to urban space varies across travel modes and how co-presence dynamics shape social exposure and segregation. Our work establishes a new framework for mobility simulation, paving the way for fine-grained, data-driven studies of human behavior and its societal implications.",
    "categories": [
      "physics.soc-ph",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "physics.soc-ph",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06473v1",
    "published_date": "2025-10-07 21:22:08 UTC",
    "updated_date": "2025-10-07 21:22:08 UTC"
  },
  {
    "arxiv_id": "2510.14992v1",
    "title": "GAZE:Governance-Aware pre-annotation for Zero-shot World Model Environments",
    "authors": [
      "Leela Krishna",
      "Mengyang Zhao",
      "Saicharithreddy Pasula",
      "Harshit Rajgarhia",
      "Abhishek Mukherji"
    ],
    "abstract": "Training robust world models requires large-scale, precisely labeled multimodal datasets, a process historically bottlenecked by slow and expensive manual annotation. We present a production-tested GAZE pipeline that automates the conversion of raw, long-form video into rich, task-ready supervision for world-model training. Our system (i) normalizes proprietary 360-degree formats into standard views and shards them for parallel processing; (ii) applies a suite of AI models (scene understanding, object tracking, audio transcription, PII/NSFW/minor detection) for dense, multimodal pre-annotation; and (iii) consolidates signals into a structured output specification for rapid human validation.\n  The GAZE workflow demonstrably yields efficiency gains (~19 minutes saved per review hour) and reduces human review volume by >80% through conservative auto-skipping of low-salience segments. By increasing label density and consistency while integrating privacy safeguards and chain-of-custody metadata, our method generates high-fidelity, privacy-aware datasets directly consumable for learning cross-modal dynamics and action-conditioned prediction. We detail our orchestration, model choices, and data dictionary to provide a scalable blueprint for generating high-quality world model training data without sacrificing throughput or governance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.14992v1",
    "published_date": "2025-10-07 21:13:03 UTC",
    "updated_date": "2025-10-07 21:13:03 UTC"
  },
  {
    "arxiv_id": "2510.06457v1",
    "title": "Evaluating Node-tree Interfaces for AI Explainability",
    "authors": [
      "Lifei Wang",
      "Natalie Friedman",
      "Chengchao Zhu",
      "Zeshu Zhu",
      "S. Joy Mountford"
    ],
    "abstract": "As large language models (LLMs) become ubiquitous in workplace tools and decision-making processes, ensuring explainability and fostering user trust are critical. Although advancements in LLM engineering continue, human-centered design is still catching up, particularly when it comes to embedding transparency and trust into AI interfaces. This study evaluates user experiences with two distinct AI interfaces - node-tree interfaces and chatbot interfaces - to assess their performance in exploratory, follow-up inquiry, decision-making, and problem-solving tasks. Our design-driven approach introduces a node-tree interface that visually structures AI-generated responses into hierarchically organized, interactive nodes, allowing users to navigate, refine, and follow up on complex information. In a comparative study with n=20 business users, we observed that while the chatbot interface effectively supports linear, step-by-step queries, it is the node-tree interface that enhances brainstorming. Quantitative and qualitative findings indicate that node-tree interfaces not only improve task performance and decision-making support but also promote higher levels of user trust by preserving context. Our findings suggest that adaptive AI interfaces capable of switching between structured visualizations and conversational formats based on task requirements can significantly enhance transparency and user confidence in AI-powered systems. This work contributes actionable insights to the fields of human-robot interaction and AI design, particularly for enterprise applications where trust-building is critical for teams.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "5 pages, 2 figures. Accepted to the 3rd Workshop on Explainability in Human-Robot Collaboration: Real-World Concerns (XHRI 2025), scheduled for March 3, 2025, Hybrid (Melbourne and online) as part of HRI 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.06457v1",
    "published_date": "2025-10-07 20:48:08 UTC",
    "updated_date": "2025-10-07 20:48:08 UTC"
  },
  {
    "arxiv_id": "2510.06448v1",
    "title": "How NOT to benchmark your SITE metric: Beyond Static Leaderboards and Towards Realistic Evaluation",
    "authors": [
      "Prabhant Singh",
      "Sibylle Hess",
      "Joaquin Vanschoren"
    ],
    "abstract": "Transferability estimation metrics are used to find a high-performing pre-trained model for a given target task without fine-tuning models and without access to the source dataset. Despite the growing interest in developing such metrics, the benchmarks used to measure their progress have gone largely unexamined. In this work, we empirically show the shortcomings of widely used benchmark setups to evaluate transferability estimation metrics. We argue that the benchmarks on which these metrics are evaluated are fundamentally flawed. We empirically demonstrate that their unrealistic model spaces and static performance hierarchies artificially inflate the perceived performance of existing metrics, to the point where simple, dataset-agnostic heuristics can outperform sophisticated methods. Our analysis reveals a critical disconnect between current evaluation protocols and the complexities of real-world model selection. To address this, we provide concrete recommendations for constructing more robust and realistic benchmarks to guide future research in a more meaningful direction.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06448v1",
    "published_date": "2025-10-07 20:38:12 UTC",
    "updated_date": "2025-10-07 20:38:12 UTC"
  },
  {
    "arxiv_id": "2510.06445v2",
    "title": "A Survey on Agentic Security: Applications, Threats and Defenses",
    "authors": [
      "Asif Shahriar",
      "Md Nafiu Rahman",
      "Sadif Ahmed",
      "Farig Sadeque",
      "Md Rizwan Parvez"
    ],
    "abstract": "In this work we present the first holistic survey of the agentic security landscape, structuring the field around three fundamental pillars: Applications, Threats, and Defenses. We provide a comprehensive taxonomy of over 160 papers, explaining how agents are used in downstream cybersecurity applications, inherent threats to agentic systems, and countermeasures designed to protect them. A detailed cross-cutting analysis shows emerging trends in agent architecture while revealing critical research gaps in model and modality coverage. A complete and continuously updated list of all surveyed papers is publicly available at https://github.com/kagnlp/Awesome-Agentic-Security.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06445v2",
    "published_date": "2025-10-07 20:32:20 UTC",
    "updated_date": "2025-12-20 13:42:00 UTC"
  },
  {
    "arxiv_id": "2510.06444v1",
    "title": "Context-Aware Inference via Performance Forecasting in Decentralized Learning Networks",
    "authors": [
      "Joel Pfeffer",
      "J. M. Diederik Kruijssen",
      "Clément Gossart",
      "Mélanie Chevance",
      "Diego Campo Millan",
      "Florian Stecker",
      "Steven N. Longmore"
    ],
    "abstract": "In decentralized learning networks, predictions from many participants are combined to generate a network inference. While many studies have demonstrated performance benefits of combining multiple model predictions, existing strategies using linear pooling methods (ranging from simple averaging to dynamic weight updates) face a key limitation. Dynamic prediction combinations that rely on historical performance to update weights are necessarily reactive. Due to the need to average over a reasonable number of epochs (with moving averages or exponential weighting), they tend to be slow to adjust to changing circumstances (phase or regime changes). In this work, we develop a model that uses machine learning to forecast the performance of predictions by models at each epoch in a time series. This enables `context-awareness' by assigning higher weight to models that are likely to be more accurate at a given time. We show that adding a performance forecasting worker in a decentralized learning network, following a design similar to the Allora network, can improve the accuracy of network inferences. Specifically, we find forecasting models that predict regret (performance relative to the network inference) or regret z-score (performance relative to other workers) show greater improvement than models predicting losses, which often do not outperform the naive network inference (historically weighted average of all inferences). Through a series of optimization tests, we show that the performance of the forecasting model can be sensitive to choices in the feature set and number of training epochs. These properties may depend on the exact problem and should be tailored to each domain. Although initially designed for a decentralized learning network, using performance forecasting for prediction combination may be useful in any situation where predictive rather than reactive model weighting is needed.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "17 pages, 12 figures; appeared in ADI (October 2025)",
    "pdf_url": "https://arxiv.org/pdf/2510.06444v1",
    "published_date": "2025-10-07 20:30:21 UTC",
    "updated_date": "2025-10-07 20:30:21 UTC"
  },
  {
    "arxiv_id": "2510.07345v1",
    "title": "Mitigating Surgical Data Imbalance with Dual-Prediction Video Diffusion Model",
    "authors": [
      "Danush Kumar Venkatesh",
      "Adam Schmidt",
      "Muhammad Abdullah Jamal",
      "Omid Mohareri"
    ],
    "abstract": "Surgical video datasets are essential for scene understanding, enabling procedural modeling and intra-operative support. However, these datasets are often heavily imbalanced, with rare actions and tools under-represented, which limits the robustness of downstream models. We address this challenge with $SurgiFlowVid$, a sparse and controllable video diffusion framework for generating surgical videos of under-represented classes. Our approach introduces a dual-prediction diffusion module that jointly denoises RGB frames and optical flow, providing temporal inductive biases to improve motion modeling from limited samples. In addition, a sparse visual encoder conditions the generation process on lightweight signals (e.g., sparse segmentation masks or RGB frames), enabling controllability without dense annotations. We validate our approach on three surgical datasets across tasks including action recognition, tool presence detection, and laparoscope motion prediction. Synthetic data generated by our method yields consistent gains of 10-20% over competitive baselines, establishing $SurgiFlowVid$ as a promising strategy to mitigate data imbalance and advance surgical video understanding methods.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "q-bio.QM",
    "comment": "29 pages, 16 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.07345v1",
    "published_date": "2025-10-07 20:29:27 UTC",
    "updated_date": "2025-10-07 20:29:27 UTC"
  },
  {
    "arxiv_id": "2510.06433v1",
    "title": "Flavonoid Fusion: Creating a Knowledge Graph to Unveil the Interplay Between Food and Health",
    "authors": [
      "Aryan Singh Dalal",
      "Yinglun Zhang",
      "Duru Doğan",
      "Atalay Mert İleri",
      "Hande Küçük McGinty"
    ],
    "abstract": "The focus on \"food as medicine\" is gaining traction in the field of health and several studies conducted in the past few years discussed this aspect of food in the literature. However, very little research has been done on representing the relationship between food and health in a standardized, machine-readable format using a semantic web that can help us leverage this knowledge effectively. To address this gap, this study aims to create a knowledge graph to link food and health through the knowledge graph's ability to combine information from various platforms focusing on flavonoid contents of food found in the USDA databases and cancer connections found in the literature. We looked closely at these relationships using KNARM methodology and represented them in machine-operable format. The proposed knowledge graph serves as an example for researchers, enabling them to explore the complex interplay between dietary choices and disease management. Future work for this study involves expanding the scope of the knowledge graph by capturing nuances, adding more related data, and performing inferences on the acquired knowledge to uncover hidden relationships.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06433v1",
    "published_date": "2025-10-07 20:11:39 UTC",
    "updated_date": "2025-10-07 20:11:39 UTC"
  },
  {
    "arxiv_id": "2510.06410v1",
    "title": "Off-Trajectory Reasoning: Can LLMs Collaborate on Reasoning Trajectory?",
    "authors": [
      "Aochong Oliver Li",
      "Tanya Goyal"
    ],
    "abstract": "Reasoning LLMs are trained to verbalize their reasoning process, yielding strong gains on complex tasks. This transparency also opens a promising direction: multiple reasoners can directly collaborate on each other's thinking within a shared trajectory, yielding better inference efficiency and exploration. A key prerequisite, however, is the ability to assess the usefulness and build on another model's partial thinking -- we call this off-trajectory reasoning. Our paper investigates a critical question: can standard solo-reasoning training pipelines deliver desired off-trajectory behaviors? We propose twin tests that capture the two extremes of the off-trajectory spectrum, namely Recoverability, which tests whether LLMs can backtrack from \"distractions\" induced by misleading reasoning traces, and Guidability, which tests their ability to build upon correct reasoning from stronger collaborators. Our study evaluates 15 open-weight LLMs (1.5B-32B) and reveals a counterintuitive finding -- \"stronger\" LLMs on benchmarks are often more fragile under distraction. Moreover, all models tested fail to effectively leverage guiding steps from collaborators on problems beyond their inherent capabilities with solve rates remaining under 9.2%. Finally, we conduct control studies to isolate the effects of three factors in post-training on these behaviors: the choice of distillation teacher, the use of RL, and data selection strategy. Our results provide actionable insights for training natively strong reasoning collaborators; e.g., we find that suboptimal recoverability behaviors of teacher models are transferred to distilled students even if the distillation trajectories are correct. Taken together, this work lays the groundwork for evaluating multi-model collaborations in shared reasoning trajectories and highlights the limitations of off-the-shelf reasoning LLMs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06410v1",
    "published_date": "2025-10-07 19:42:50 UTC",
    "updated_date": "2025-10-07 19:42:50 UTC"
  },
  {
    "arxiv_id": "2510.06397v1",
    "title": "Geometry-Aware Backdoor Attacks: Leveraging Curvature in Hyperbolic Embeddings",
    "authors": [
      "Ali Baheri"
    ],
    "abstract": "Non-Euclidean foundation models increasingly place representations in curved spaces such as hyperbolic geometry. We show that this geometry creates a boundary-driven asymmetry that backdoor triggers can exploit. Near the boundary, small input changes appear subtle to standard input-space detectors but produce disproportionately large shifts in the model's representation space. Our analysis formalizes this effect and also reveals a limitation for defenses: methods that act by pulling points inward along the radius can suppress such triggers, but only by sacrificing useful model sensitivity in that same direction. Building on these insights, we propose a simple geometry-adaptive trigger and evaluate it across tasks and architectures. Empirically, attack success increases toward the boundary, whereas conventional detectors weaken, mirroring the theoretical trends. Together, these results surface a geometry-specific vulnerability in non-Euclidean models and offer analysis-backed guidance for designing and understanding the limits of defenses.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06397v1",
    "published_date": "2025-10-07 19:24:43 UTC",
    "updated_date": "2025-10-07 19:24:43 UTC"
  },
  {
    "arxiv_id": "2510.06396v1",
    "title": "Adaptive Protein Design Protocols and Middleware",
    "authors": [
      "Aymen Alsaadi",
      "Jonathan Ash",
      "Mikhail Titov",
      "Matteo Turilli",
      "Andre Merzky",
      "Shantenu Jha",
      "Sagar Khare"
    ],
    "abstract": "Computational protein design is experiencing a transformation driven by AI/ML. However, the range of potential protein sequences and structures is astronomically vast, even for moderately sized proteins. Hence, achieving convergence between generated and predicted structures demands substantial computational resources for sampling. The Integrated Machine-learning for Protein Structures at Scale (IMPRESS) offers methods and advanced computing systems for coupling AI to high-performance computing tasks, enabling the ability to evaluate the effectiveness of protein designs as they are developed, as well as the models and simulations used to generate data and train models. This paper introduces IMPRESS and demonstrates the development and implementation of an adaptive protein design protocol and its supporting computing infrastructure. This leads to increased consistency in the quality of protein design and enhanced throughput of protein design due to dynamic resource allocation and asynchronous workload execution.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.PF",
      "cs.SE"
    ],
    "primary_category": "cs.DC",
    "comment": "N/A",
    "pdf_url": "https://arxiv.org/pdf/2510.06396v1",
    "published_date": "2025-10-07 19:23:53 UTC",
    "updated_date": "2025-10-07 19:23:53 UTC"
  },
  {
    "arxiv_id": "2510.06391v1",
    "title": "Reward Model Perspectives: Whose Opinions Do Reward Models Reward?",
    "authors": [
      "Elle"
    ],
    "abstract": "Reward models (RMs) are central to the alignment of language models (LMs). An RM often serves as a proxy for human preferences to guide downstream LM behavior. However, our understanding of RM behavior is limited. Our work (i) formalizes a framework for measuring the alignment of opinions captured by RMs, (ii) investigates the extent to which RMs demonstrate sociodemographic biases, and (iii) explores the effects of prompting to steer rewards towards the preferences of a target group. We study the subjective and diverse perspectives on controversial topics, which allows us to quantify RM perspectives in terms of their opinions, attitudes, and values. We show that RMs are poorly aligned with several demographic groups and can systematically reward harmful stereotypes, and steering alone is not enough to overcome these limitations. Our findings underscore the need for more careful consideration of RM behavior in model alignment during preference learning to prevent the propagation of unwanted social biases in the language technologies that we use.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Published at EMNLP 2025 under the full author name \"Elle\"",
    "pdf_url": "https://arxiv.org/pdf/2510.06391v1",
    "published_date": "2025-10-07 19:13:52 UTC",
    "updated_date": "2025-10-07 19:13:52 UTC"
  },
  {
    "arxiv_id": "2510.07343v2",
    "title": "Local MAP Sampling for Diffusion Models",
    "authors": [
      "Shaorong Zhang",
      "Rob Brekelmans",
      "Greg Ver Steeg"
    ],
    "abstract": "Diffusion Posterior Sampling (DPS) provides a principled Bayesian approach to inverse problems by sampling from $p(x_0 \\mid y)$. However, in practice, the goal of inverse problem solving is not to cover the posterior but to recover the most accurate reconstruction, where optimization-based diffusion solvers often excel despite lacking a clear probabilistic foundation. We introduce Local MAP Sampling (LMAPS), a new inference framework that iteratively solving local MAP subproblems along the diffusion trajectory. This perspective clarifies their connection to global MAP estimation and DPS, offering a unified probabilistic interpretation for optimization-based methods. Building on this foundation, we develop practical algorithms with a probabilistically interpretable covariance approximation, a reformulated objective for stability and interpretability, and a gradient approximation for non-differentiable operators. Across a broad set of image restoration and scientific tasks, LMAPS achieves state-of-the-art performance, including $\\geq 2$ dB gains on motion deblurring, JPEG restoration, and quantization, and $>1.5$ dB improvements on inverse scattering benchmarks.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.GR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.07343v2",
    "published_date": "2025-10-07 19:02:32 UTC",
    "updated_date": "2025-10-12 18:18:02 UTC"
  },
  {
    "arxiv_id": "2510.06383v1",
    "title": "Protecting De-identified Documents from Search-based Linkage Attacks",
    "authors": [
      "Pierre Lison",
      "Mark Anderson"
    ],
    "abstract": "While de-identification models can help conceal the identity of the individual(s) mentioned in a document, they fail to address linkage risks, defined as the potential to map the de-identified text back to its source. One straightforward way to perform such linkages is to extract phrases from the de-identified document and then check their presence in the original dataset. This paper presents a method to counter search-based linkage attacks while preserving the semantic integrity of the text. The method proceeds in two steps. We first construct an inverted index of the N-grams occurring in the document collection, making it possible to efficiently determine which N-grams appear in less than $k$ documents (either alone or in combination with other N-grams). An LLM-based rewriter is then iteratively queried to reformulate those spans until linkage is no longer possible. Experimental results on a collection of court cases show that the method is able to effectively prevent search-based linkages while remaining faithful to the original content.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06383v1",
    "published_date": "2025-10-07 19:02:21 UTC",
    "updated_date": "2025-10-07 19:02:21 UTC"
  },
  {
    "arxiv_id": "2510.06381v1",
    "title": "Monte Carlo Permutation Search",
    "authors": [
      "Tristan Cazenave"
    ],
    "abstract": "We propose Monte Carlo Permutation Search (MCPS), a general-purpose Monte Carlo Tree Search (MCTS) algorithm that improves upon the GRAVE algorithm. MCPS is relevant when deep reinforcement learning is not an option, or when the computing power available before play is not substantial, such as in General Game Playing, for example. The principle of MCPS is to include in the exploration term of a node the statistics on all the playouts that contain all the moves on the path from the root to the node. We extensively test MCPS on a variety of games: board games, wargame, investment game, video game and multi-player games. MCPS has better results than GRAVE in all the two-player games. It has equivalent results for multi-player games because these games are inherently balanced even when players have different strengths. We also show that using abstract codes for moves instead of exact codes can be beneficial to both MCPS and GRAVE, as they improve the permutation statistics and the AMAF statistics. We also provide a mathematical derivation of the formulas used for weighting the three sources of statistics. These formulas are an improvement on the GRAVE formula since they no longer use the bias hyperparameter of GRAVE. Moreover, MCPS is not sensitive to the ref hyperparameter.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06381v1",
    "published_date": "2025-10-07 18:59:39 UTC",
    "updated_date": "2025-10-07 18:59:39 UTC"
  },
  {
    "arxiv_id": "2510.06377v2",
    "title": "Relational Transformer: Toward Zero-Shot Foundation Models for Relational Data",
    "authors": [
      "Rishabh Ranjan",
      "Valter Hudovernik",
      "Mark Znidar",
      "Charilaos Kanatsoulis",
      "Roshan Upendra",
      "Mahmoud Mohammadi",
      "Joe Meyer",
      "Tom Palczewski",
      "Carlos Guestrin",
      "Jure Leskovec"
    ],
    "abstract": "Pretrained transformers readily adapt to new sequence modeling tasks via zero-shot prompting, but relational domains still lack architectures that transfer across datasets and tasks. The core challenge is the diversity of relational data, with varying heterogeneous schemas, graph structures and functional dependencies. In this paper, we present the Relational Transformer (RT) architecture, which can be pretrained on diverse relational databases and directly applied to unseen datasets and tasks without task- or dataset-specific fine-tuning, or retrieval of in-context examples. RT (i) tokenizes cells with table/column metadata, (ii) is pretrained via masked token prediction, and (iii) utilizes a novel Relational Attention mechanism over columns, rows, and primary-foreign key links. Pretrained on RelBench datasets spanning tasks such as churn and sales forecasting, RT attains strong zero-shot performance, averaging 93% of fully supervised AUROC on binary classification tasks with a single forward pass of a 22M parameter model, as opposed to 84% for a 27B LLM. Fine-tuning yields state-of-the-art results with high sample efficiency. Our experiments show that RT's zero-shot transfer harnesses task-table context, relational attention patterns and schema semantics. Overall, RT provides a practical path toward foundation models for relational data.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.LG",
    "comment": "preprint; under review",
    "pdf_url": "https://arxiv.org/pdf/2510.06377v2",
    "published_date": "2025-10-07 18:51:51 UTC",
    "updated_date": "2025-10-22 22:13:45 UTC"
  },
  {
    "arxiv_id": "2510.06371v1",
    "title": "EverydayMMQA: A Multilingual and Multimodal Framework for Culturally Grounded Spoken Visual QA",
    "authors": [
      "Firoj Alam",
      "Ali Ezzat Shahroor",
      "Md. Arid Hasan",
      "Zien Sheikh Ali",
      "Hunzalah Hassan Bhatti",
      "Mohamed Bayan Kmainasi",
      "Shammur Absar Chowdhury",
      "Basel Mousi",
      "Fahim Dalvi",
      "Nadir Durrani",
      "Natasa Milic-Frayling"
    ],
    "abstract": "Large-scale multimodal models achieve strong results on tasks like Visual Question Answering (VQA), but they often fail when queries require culturally grounded, everyday knowledge, particularly in low-resource and underrepresented languages. To bridge this gap, we introduce Everyday Multimodal and Multilingual QA (EverydayMMQA), a framework for creating large-scale, culturally-grounded datasets for spoken and visual question answering (SVQA). Using this framework, we developed OASIS, a multimodal dataset integrating speech, images, and text. With over ~0.92M images and 14.8M QA pairs, OASIS contains 3.7M spoken questions, enabling four unique input combinations: speech-only, text-only, speech+image, and text+image. Focused on English and Arabic varieties, 18 countries, the dataset content is curated to reflect diverse, real-world situations. OASIS tests models on tasks beyond object recognition that involve pragmatic, commonsense, and culturally aware reasoning. We benchmarked four closed-source models, three open-source models, and one fine-tuned model. EverydayMMQA and OASIS together provide a benchmark and training dataset for building multimodal LLMs for a comprehensive set of everyday tasks within cultural contexts. The framework and dataset will be made publicly available to the community.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Multimodal Foundation Models, Large Language Models, Native, Multilingual, Language Diversity, Contextual Understanding, Culturally Informed",
    "pdf_url": "https://arxiv.org/pdf/2510.06371v1",
    "published_date": "2025-10-07 18:37:32 UTC",
    "updated_date": "2025-10-07 18:37:32 UTC"
  },
  {
    "arxiv_id": "2510.06357v1",
    "title": "Constrained Natural Language Action Planning for Resilient Embodied Systems",
    "authors": [
      "Grayson Byrd",
      "Corban Rivera",
      "Bethany Kemp",
      "Meghan Booker",
      "Aurora Schmidt",
      "Celso M de Melo",
      "Lalithkumar Seenivasan",
      "Mathias Unberath"
    ],
    "abstract": "Replicating human-level intelligence in the execution of embodied tasks remains challenging due to the unconstrained nature of real-world environments. Novel use of large language models (LLMs) for task planning seeks to address the previously intractable state/action space of complex planning tasks, but hallucinations limit their reliability, and thus, viability beyond a research context. Additionally, the prompt engineering required to achieve adequate system performance lacks transparency, and thus, repeatability. In contrast to LLM planning, symbolic planning methods offer strong reliability and repeatability guarantees, but struggle to scale to the complexity and ambiguity of real-world tasks. We introduce a new robotic planning method that augments LLM planners with symbolic planning oversight to improve reliability and repeatability, and provide a transparent approach to defining hard constraints with considerably stronger clarity than traditional prompt engineering. Importantly, these augmentations preserve the reasoning capabilities of LLMs and retain impressive generalization in open-world environments. We demonstrate our approach in simulated and real-world environments. On the ALFWorld planning benchmark, our approach outperforms current state-of-the-art methods, achieving a near-perfect 99% success rate. Deployment of our method to a real-world quadruped robot resulted in 100% task success compared to 50% and 30% for pure LLM and symbolic planners across embodied pick and place tasks. Our approach presents an effective strategy to enhance the reliability, repeatability and transparency of LLM-based robot planners while retaining their key strengths: flexibility and generalizability to complex real-world environments. We hope that this work will contribute to the broad goal of building resilient embodied intelligent systems.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06357v1",
    "published_date": "2025-10-07 18:23:12 UTC",
    "updated_date": "2025-10-07 18:23:12 UTC"
  },
  {
    "arxiv_id": "2510.06353v1",
    "title": "TransFIRA: Transfer Learning for Face Image Recognizability Assessment",
    "authors": [
      "Allen Tu",
      "Kartik Narayan",
      "Joshua Gleason",
      "Jennifer Xu",
      "Matthew Meyn",
      "Tom Goldstein",
      "Vishal M. Patel"
    ],
    "abstract": "Face recognition in unconstrained environments such as surveillance, video, and web imagery must contend with extreme variation in pose, blur, illumination, and occlusion, where conventional visual quality metrics fail to predict whether inputs are truly recognizable to the deployed encoder. Existing FIQA methods typically rely on visual heuristics, curated annotations, or computationally intensive generative pipelines, leaving their predictions detached from the encoder's decision geometry. We introduce TransFIRA (Transfer Learning for Face Image Recognizability Assessment), a lightweight and annotation-free framework that grounds recognizability directly in embedding space. TransFIRA delivers three advances: (i) a definition of recognizability via class-center similarity (CCS) and class-center angular separation (CCAS), yielding the first natural, decision-boundary--aligned criterion for filtering and weighting; (ii) a recognizability-informed aggregation strategy that achieves state-of-the-art verification accuracy on BRIAR and IJB-C while nearly doubling correlation with true recognizability, all without external labels, heuristics, or backbone-specific training; and (iii) new extensions beyond faces, including encoder-grounded explainability that reveals how degradations and subject-specific factors affect recognizability, and the first recognizability-aware body recognition assessment. Experiments confirm state-of-the-art results on faces, strong performance on body recognition, and robustness under cross-dataset shifts. Together, these contributions establish TransFIRA as a unified, geometry-driven framework for recognizability assessment -- encoder-specific, accurate, interpretable, and extensible across modalities -- significantly advancing FIQA in accuracy, explainability, and scope.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page: https://transfira.github.io/",
    "pdf_url": "https://arxiv.org/pdf/2510.06353v1",
    "published_date": "2025-10-07 18:16:21 UTC",
    "updated_date": "2025-10-07 18:16:21 UTC"
  },
  {
    "arxiv_id": "2510.08612v1",
    "title": "Impact of LLMs on Team Collaboration in Software Development",
    "authors": [
      "Devang Dhanuka"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly being integrated into software development processes, with the potential to transform team workflows and productivity. This paper investigates how LLMs affect team collaboration throughout the Software Development Life Cycle (SDLC). We reframe and update a prior study with recent developments as of 2025, incorporating new literature and case studies. We outline the problem of collaboration hurdles in SDLC and explore how LLMs can enhance productivity, communication, and decision-making in a team context. Through literature review, industry examples, a team survey, and two case studies, we assess the impact of LLM-assisted tools (such as code generation assistants and AI-powered project management agents) on collaborative software engineering practices. Our findings indicate that LLMs can significantly improve efficiency (by automating repetitive tasks and documentation), enhance communication clarity, and aid cross-functional collaboration, while also introducing new challenges like model limitations and privacy concerns. We discuss these benefits and challenges, present research questions guiding the investigation, evaluate threats to validity, and suggest future research directions including domain-specific model customization, improved integration into development tools, and robust strategies for ensuring trust and security.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08612v1",
    "published_date": "2025-10-07 18:16:17 UTC",
    "updated_date": "2025-10-07 18:16:17 UTC"
  },
  {
    "arxiv_id": "2510.06350v1",
    "title": "Asking For It: Question-Answering for Predicting Rule Infractions in Online Content Moderation",
    "authors": [
      "Mattia Samory",
      "Diana Pamfile",
      "Andrew To",
      "Shruti Phadke"
    ],
    "abstract": "Online communities rely on a mix of platform policies and community-authored rules to define acceptable behavior and maintain order. However, these rules vary widely across communities, evolve over time, and are enforced inconsistently, posing challenges for transparency, governance, and automation. In this paper, we model the relationship between rules and their enforcement at scale, introducing ModQ, a novel question-answering framework for rule-sensitive content moderation. Unlike prior classification or generation-based approaches, ModQ conditions on the full set of community rules at inference time and identifies which rule best applies to a given comment. We implement two model variants - extractive and multiple-choice QA - and train them on large-scale datasets from Reddit and Lemmy, the latter of which we construct from publicly available moderation logs and rule descriptions. Both models outperform state-of-the-art baselines in identifying moderation-relevant rule violations, while remaining lightweight and interpretable. Notably, ModQ models generalize effectively to unseen communities and rules, supporting low-resource moderation settings and dynamic governance environments.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "Accepted at ICWSM 2026",
    "pdf_url": "https://arxiv.org/pdf/2510.06350v1",
    "published_date": "2025-10-07 18:11:27 UTC",
    "updated_date": "2025-10-07 18:11:27 UTC"
  },
  {
    "arxiv_id": "2510.06349v2",
    "title": "Flexible Swarm Learning May Outpace Foundation Models in Essential Tasks",
    "authors": [
      "Moein E. Samadi",
      "Andreas Schuppert"
    ],
    "abstract": "Foundation models have rapidly advanced AI, raising the question of whether their decisions will ultimately surpass human strategies in real-world domains. The exponential, and possibly super-exponential, pace of AI development makes such analysis elusive. Nevertheless, many application areas that matter for daily life and society show only modest gains so far; a prominent case is diagnosing and treating dynamically evolving disease in intensive care.\n  The common challenge is adapting complex systems to dynamic environments. Effective strategies must optimize outcomes in systems composed of strongly interacting functions while avoiding shared side effects; this requires reliable, self-adaptive modeling. These tasks align with building digital twins of highly complex systems whose mechanisms are not fully or quantitatively understood. It is therefore essential to develop methods for self-adapting AI models with minimal data and limited mechanistic knowledge. As this challenge extends beyond medicine, AI should demonstrate clear superiority in these settings before assuming broader decision-making roles.\n  We identify the curse of dimensionality as a fundamental barrier to efficient self-adaptation and argue that monolithic foundation models face conceptual limits in overcoming it. As an alternative, we propose a decentralized architecture of interacting small agent networks (SANs). We focus on agents representing the specialized substructure of the system, where each agent covers only a subset of the full system functions. Drawing on mathematical results on the learning behavior of SANs and evidence from existing applications, we argue that swarm-learning in diverse swarms can enable self-adaptive SANs to deliver superior decision-making in dynamic environments compared with monolithic foundation models, though at the cost of reduced reproducibility in detail.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06349v2",
    "published_date": "2025-10-07 18:10:31 UTC",
    "updated_date": "2025-11-30 11:47:15 UTC"
  },
  {
    "arxiv_id": "2510.06343v2",
    "title": "Leveraging Large Language Models for Cybersecurity Risk Assessment -- A Case from Forestry Cyber-Physical Systems",
    "authors": [
      "Fikret Mert Gultekin",
      "Oscar Lilja",
      "Ranim Khojah",
      "Rebekka Wohlrab",
      "Marvin Damschen",
      "Mazen Mohamad"
    ],
    "abstract": "In safety-critical software systems, cybersecurity activities become essential, with risk assessment being one of the most critical. In many software teams, cybersecurity experts are either entirely absent or represented by only a small number of specialists. As a result, the workload for these experts becomes high, and software engineers would need to conduct cybersecurity activities themselves. This creates a need for a tool to support cybersecurity experts and engineers in evaluating vulnerabilities and threats during the risk assessment process. This paper explores the potential of leveraging locally hosted large language models (LLMs) with retrieval-augmented generation to support cybersecurity risk assessment in the forestry domain while complying with data protection and privacy requirements that limit external data sharing. We performed a design science study involving 12 experts in interviews, interactive sessions, and a survey within a large-scale project. The results demonstrate that LLMs can assist cybersecurity experts by generating initial risk assessments, identifying threats, and providing redundancy checks. The results also highlight the necessity for human oversight to ensure accuracy and compliance. Despite trust concerns, experts were willing to utilize LLMs in specific evaluation and assistance roles, rather than solely relying on their generative capabilities. This study provides insights that encourage the use of LLM-based agents to support the risk assessment process of cyber-physical systems in safety-critical domains.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted at Autonomous Agents in Software Engineering (AgenticSE) Workshop, co-located with ASE 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.06343v2",
    "published_date": "2025-10-07 18:07:16 UTC",
    "updated_date": "2025-10-11 19:52:40 UTC"
  },
  {
    "arxiv_id": "2510.06218v1",
    "title": "EgoNight: Towards Egocentric Vision Understanding at Night with a Challenging Benchmark",
    "authors": [
      "Deheng Zhang",
      "Yuqian Fu",
      "Runyi Yang",
      "Yang Miao",
      "Tianwen Qian",
      "Xu Zheng",
      "Guolei Sun",
      "Ajad Chhatkuli",
      "Xuanjing Huang",
      "Yu-Gang Jiang",
      "Luc Van Gool",
      "Danda Pani Paudel"
    ],
    "abstract": "Most existing benchmarks for egocentric vision understanding focus primarily on daytime scenarios, overlooking the low-light conditions that are inevitable in real-world applications. To investigate this gap, we present EgoNight, the first comprehensive benchmark for nighttime egocentric vision, with visual question answering (VQA) as the core task. A key feature of EgoNight is the introduction of day-night aligned videos, which enhance night annotation quality using the daytime data and reveal clear performance gaps between lighting conditions. To achieve this, we collect both synthetic videos rendered by Blender and real-world recordings, ensuring that scenes and actions are visually and temporally aligned. Leveraging these paired videos, we construct EgoNight-VQA, supported by a novel day-augmented night auto-labeling engine and refinement through extensive human verification. Each QA pair is double-checked by annotators for reliability. In total, EgoNight-VQA contains 3658 QA pairs across 90 videos, spanning 12 diverse QA types, with more than 300 hours of human work. Evaluations of state-of-the-art multimodal large language models (MLLMs) reveal substantial performance drops when transferring from day to night, underscoring the challenges of reasoning under low-light conditions. Beyond VQA, EgoNight also introduces two auxiliary tasks, day-night correspondence retrieval and egocentric depth estimation at night, that further explore the boundaries of existing models. We believe EgoNight-VQA provides a strong foundation for advancing application-driven egocentric vision research and for developing models that generalize across illumination domains. All the data and code will be made available upon acceptance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06218v1",
    "published_date": "2025-10-07 17:59:47 UTC",
    "updated_date": "2025-10-07 17:59:47 UTC"
  },
  {
    "arxiv_id": "2510.06217v1",
    "title": "TaTToo: Tool-Grounded Thinking PRM for Test-Time Scaling in Tabular Reasoning",
    "authors": [
      "Jiaru Zou",
      "Soumya Roy",
      "Vinay Kumar Verma",
      "Ziyi Wang",
      "David Wipf",
      "Pan Lu",
      "Sumit Negi",
      "James Zou",
      "Jingrui He"
    ],
    "abstract": "Process Reward Models (PRMs) have recently emerged as a powerful framework for enhancing the reasoning capabilities of large reasoning models (LRMs), particularly in the context of test-time scaling (TTS). However, their potential for supervising LRMs on tabular reasoning domains remains underexplored. Through detailed empirical analyses, we identify that existing PRMs, though widely adopted for supervising text-only reasoning steps, struggle with table-specific operations such as sub-table retrieval and schema interaction, leading to critical performance bottlenecks. To address this limitation, we propose TaTToo, a novel table-grounded PRM framework that (i) reasons explicitly over tabular reasoning steps and (ii) integrates tool-based verification to provide precise reward supervision. Concretely, we first design a scalable data curation pipeline that constructs over 60k high-quality step-level annotations by integrating table verification rationales with tool-based executions. Building on the collected data, we train TaTToo with a dual-stage paradigm: cold-start supervised fine-tuning to capture tool-use reasoning patterns, followed by reinforcement learning with tool-grounded reward shaping to align our model with table-based verification. We provide a comprehensive evaluation of the policy improvement induced by our newly designed PRM. Across 5 challenging tabular reasoning benchmarks covering numerical reasoning, fact-checking, and data analysis, TaTToo improves downstream policy LRMs by 30.9% at inference, surpasses strong PRM baselines such as Qwen-2.5-Math-PRM-72B with only 8B parameters, and demonstrates strong generalizability across diverse TTS strategies.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06217v1",
    "published_date": "2025-10-07 17:59:41 UTC",
    "updated_date": "2025-10-07 17:59:41 UTC"
  },
  {
    "arxiv_id": "2510.06214v1",
    "title": "Stratified GRPO: Handling Structural Heterogeneity in Reinforcement Learning of LLM Search Agents",
    "authors": [
      "Mingkang Zhu",
      "Xi Chen",
      "Bei Yu",
      "Hengshuang Zhao",
      "Jiaya Jia"
    ],
    "abstract": "Large language model (LLM) agents increasingly rely on external tools such as search engines to solve complex, multi-step problems, and reinforcement learning (RL) has become a key paradigm for training them. However, the trajectories of search agents are structurally heterogeneous, where variations in the number, placement, and outcomes of search calls lead to fundamentally different answer directions and reward distributions. Standard policy gradient methods, which use a single global baseline, suffer from what we identify and formalize as cross-stratum bias-an \"apples-to-oranges\" comparison of heterogeneous trajectories. This cross-stratum bias distorts credit assignment and hinders exploration of complex, multi-step search strategies. To address this, we propose Stratified GRPO, whose central component, Stratified Advantage Normalization (SAN), partitions trajectories into homogeneous strata based on their structural properties and computes advantages locally within each stratum. This ensures that trajectories are evaluated only against their true peers. Our analysis proves that SAN eliminates cross-stratum bias, yields conditionally unbiased unit-variance estimates inside each stratum, and retains the global unbiasedness and unit-variance properties enjoyed by standard normalization, resulting in a more pure and scale-stable learning signal. To improve practical stability under finite-sample regimes, we further linearly blend SAN with the global estimator. Extensive experiments on diverse single-hop and multi-hop question-answering benchmarks demonstrate that Stratified GRPO consistently and substantially outperforms GRPO by up to 11.3 points, achieving higher training rewards, greater training stability, and more effective search policies. These results establish stratification as a principled remedy for structural heterogeneity in RL for LLM search agents.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06214v1",
    "published_date": "2025-10-07 17:59:13 UTC",
    "updated_date": "2025-10-07 17:59:13 UTC"
  },
  {
    "arxiv_id": "2510.06203v2",
    "title": "Reference Grounded Skill Discovery",
    "authors": [
      "Seungeun Rho",
      "Aaron Trinh",
      "Danfei Xu",
      "Sehoon Ha"
    ],
    "abstract": "Scaling unsupervised skill discovery algorithms to high-DoF agents remains challenging. As dimensionality increases, the exploration space grows exponentially, while the manifold of meaningful skills remains limited. Therefore, semantic meaningfulness becomes essential to effectively guide exploration in high-dimensional spaces. In this work, we present **Reference-Grounded Skill Discovery (RGSD)**, a novel algorithm that grounds skill discovery in a semantically meaningful latent space using reference data. RGSD first performs contrastive pretraining to embed motions on a unit hypersphere, clustering each reference trajectory into a distinct direction. This grounding enables skill discovery to simultaneously involve both imitation of reference behaviors and the discovery of semantically related diverse behaviors. On a simulated SMPL humanoid with $359$-D observations and $69$-D actions, RGSD successfully imitates skills such as walking, running, punching, and sidestepping, while also discover variations of these behaviors. In downstream locomotion tasks, RGSD leverages the discovered skills to faithfully satisfy user-specified style commands and outperforms imitation-learning baselines, which often fail to maintain the commanded style. Overall, our results suggest that lightweight reference-grounding offers a practical path to discovering semantically rich and structured skills in high-DoF systems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06203v2",
    "published_date": "2025-10-07 17:55:01 UTC",
    "updated_date": "2025-12-02 00:15:08 UTC"
  },
  {
    "arxiv_id": "2510.06201v1",
    "title": "TokenChain: A Discrete Speech Chain via Semantic Token Modeling",
    "authors": [
      "Mingxuan Wang",
      "Satoshi Nakamura"
    ],
    "abstract": "Machine Speech Chain, simulating the human perception-production loop, proves effective in jointly improving ASR and TTS. We propose TokenChain, a fully discrete speech chain coupling semantic-token ASR with a two-stage TTS: an autoregressive text-to-semantic model co-trained with ASR and a masked-generative semantic-to-acoustic model for synthesis only. End-to-end feedback across the text interface is enabled with straight-through argmax/Gumbel-Softmax and balanced with supervised ASR via dynamic weight averaging. Ablations examine optimal temperature schedules for in- and cross-domain transfer. Evaluation reveals TokenChain surpasses baseline accuracy 2-6 epochs earlier and yields 5-13% lower equal-epoch error with stable T2S on LibriSpeech, and reduces relative ASR WER by 56% and T2S WER by 31% on TED-LIUM with minimal forgetting, showing that chain learning remains effective with token interfaces and models.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "5 pages, 3 figures. Submitted to IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2026",
    "pdf_url": "https://arxiv.org/pdf/2510.06201v1",
    "published_date": "2025-10-07 17:54:12 UTC",
    "updated_date": "2025-10-07 17:54:12 UTC"
  },
  {
    "arxiv_id": "2510.06200v1",
    "title": "StarEmbed: Benchmarking Time Series Foundation Models on Astronomical Observations of Variable Stars",
    "authors": [
      "Weijian Li",
      "Hong-Yu Chen",
      "Qinjie Lin",
      "Nabeel Rehemtulla",
      "Ved G. Shah",
      "Dennis Wu",
      "Adam A. Miller",
      "Han Liu"
    ],
    "abstract": "Time series foundation models (TSFMs) are increasingly being adopted as highly-capable general-purpose time series representation learners. Although their training corpora are vast, they exclude astronomical time series data. Observations of stars produce peta-scale time series with unique challenges including irregular sampling and heteroskedasticity. We introduce StarEmbed, the first public benchmark for rigorous and standardized evaluation of state-of-the-art TSFMs on stellar time series observations (``light curves''). We benchmark on three scientifically-motivated downstream tasks: unsupervised clustering, supervised classification, and out-of-distribution source detection. StarEmbed integrates a catalog of expert-vetted labels with multi-variate light curves from the Zwicky Transient Facility, yielding ~40k hand-labeled light curves spread across seven astrophysical classes. We evaluate the zero-shot representation capabilities of three TSFMs (MOIRAI, Chronos, Chronos-Bolt) and a domain-specific transformer (Astromer) against handcrafted feature extraction, the long-standing baseline in the astrophysics literature. Our results demonstrate that these TSFMs, especially the Chronos models, which are trained on data completely unlike the astronomical observations, can outperform established astrophysics-specific baselines in some tasks and effectively generalize to entirely new data. In particular, TSFMs deliver state-of-the-art performance on our out-of-distribution source detection benchmark. With the first benchmark of TSFMs on astronomical time series data, we test the limits of their generalization and motivate a paradigm shift in time-domain astronomy from using task-specific, fully supervised pipelines toward adopting generic foundation model representations for the analysis of peta-scale datasets from forthcoming observatories.",
    "categories": [
      "astro-ph.SR",
      "astro-ph.IM",
      "cs.AI"
    ],
    "primary_category": "astro-ph.SR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06200v1",
    "published_date": "2025-10-07 17:53:56 UTC",
    "updated_date": "2025-10-07 17:53:56 UTC"
  },
  {
    "arxiv_id": "2510.06307v1",
    "title": "Belief-Calibrated Multi-Agent Consensus Seeking for Complex NLP Tasks",
    "authors": [
      "Wentao Deng",
      "Jiahuan Pei",
      "Zhiwei Xu",
      "Zhaochun Ren",
      "Zhumin Chen",
      "Pengjie Ren"
    ],
    "abstract": "A multi-agent system (MAS) enhances its capacity to solve complex natural language processing (NLP) tasks through collaboration among multiple agents, where consensus-seeking serves as a fundamental mechanism. However, existing consensus-seeking approaches typically rely on voting mechanisms to judge consensus, overlooking contradictions in system-internal beliefs that destabilize the consensus. Moreover, these methods often involve agents updating their results through indiscriminate collaboration with every other agent. Such uniform interaction fails to identify the optimal collaborators for each agent, hindering the emergence of a stable consensus. To address these challenges, we provide a theoretical framework for selecting optimal collaborators that maximize consensus stability. Based on the theorems, we propose the Belief-Calibrated Consensus Seeking (BCCS) framework to facilitate stable consensus via selecting optimal collaborators and calibrating the consensus judgment by system-internal beliefs. Experimental results on the MATH and MMLU benchmark datasets demonstrate that the proposed BCCS framework outperforms the best existing results by 2.23% and 3.95% of accuracy on challenging tasks, respectively. Our code and data are available at https://github.com/dengwentao99/BCCS.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper has been accepted by NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.06307v1",
    "published_date": "2025-10-07 17:53:34 UTC",
    "updated_date": "2025-10-07 17:53:34 UTC"
  },
  {
    "arxiv_id": "2510.06195v1",
    "title": "Latent Speech-Text Transformer",
    "authors": [
      "Yen-Ju Lu",
      "Yashesh Gaur",
      "Wei Zhou",
      "Benjamin Muller",
      "Jesus Villalba",
      "Najim Dehak",
      "Luke Zettlemoyer",
      "Gargi Ghosh",
      "Mike Lewis",
      "Srinivasan Iyer",
      "Duc Le"
    ],
    "abstract": "Auto-regressive speech-text models are typically pre-trained on a large number of interleaved sequences of text tokens and raw speech encoded as speech tokens using vector quantization. These models have demonstrated state-of-the-art performance in speech-to-speech understanding and generation benchmarks, together with promising scaling laws, primarily enabled by the representational alignment between text and speech. Nevertheless, they suffer from shortcomings, partly owing to the disproportionately longer sequences of speech tokens in contrast to textual tokens. This results in a large compute imbalance between modalities during pre-training as well as during inference, and a potential hindrance to effectively aligning speech and text, ultimately translating to several orders of magnitude slower scaling laws. We introduce the Latent Speech-Text Transformer (LST), which makes pre-training speech-text models more data-efficient by dynamically and inexpensively aggregating speech tokens into latent speech patches. These patches serve as higher-level units that can either align with corresponding textual units to aid capability transfer or even encapsulate common speech sequences like silences to be more compute-efficient. We show that LST outperforms vanilla approaches on speech-to-speech as well as text-to-text benchmarks in both data- and compute-controlled settings, the former indicating more effective representational alignment and the latter indicating steeper scaling laws for speech-text models. On HellaSwag story completion, LST achieves 6.5% absolute gain in speech accuracy under compute-controlled training and 5.3% under data-controlled training, while also improving text performance. We will release our models, code, and the evaluation data to facilitate further research.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "16 pages, 13 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.06195v1",
    "published_date": "2025-10-07 17:52:08 UTC",
    "updated_date": "2025-10-07 17:52:08 UTC"
  },
  {
    "arxiv_id": "2510.06189v3",
    "title": "Barbarians at the Gate: How AI is Upending Systems Research",
    "authors": [
      "Audrey Cheng",
      "Shu Liu",
      "Melissa Pan",
      "Zhifei Li",
      "Bowen Wang",
      "Alex Krentsel",
      "Tian Xia",
      "Mert Cemri",
      "Jongseok Park",
      "Shuo Yang",
      "Jeff Chen",
      "Lakshya Agrawal",
      "Aditya Desai",
      "Jiarong Xing",
      "Koushik Sen",
      "Matei Zaharia",
      "Ion Stoica"
    ],
    "abstract": "Artificial Intelligence (AI) is starting to transform the research process as we know it by automating the discovery of new solutions. Given a task, the typical AI-driven approach is (i) to generate a set of diverse solutions, and then (ii) to verify these solutions and select one that solves the problem. Crucially, this approach assumes the existence of a reliable verifier, i.e., one that can accurately determine whether a solution solves the given problem. We argue that systems research, long focused on designing and evaluating new performance-oriented algorithms, is particularly well-suited for AI-driven solution discovery. This is because system performance problems naturally admit reliable verifiers: solutions are typically implemented in real systems or simulators, and verification reduces to running these software artifacts against predefined workloads and measuring performance. We term this approach as AI-Driven Research for Systems (ADRS), which iteratively generates, evaluates, and refines solutions. Using penEvolve, an existing open-source ADRS instance, we present case studies across diverse domains, including load balancing for multi-region cloud scheduling, Mixture-of-Experts inference, LLM-based SQL queries, and transaction scheduling. In multiple instances, ADRS discovers algorithms that outperform state-of-the-art human designs (e.g., achieving up to 5.0x runtime improvements or 50% cost reductions). We distill best practices for guiding algorithm evolution, from prompt design to evaluator construction, for existing frameworks. We then discuss the broader implications for the systems community: as AI assumes a central role in algorithm design, we argue that human researchers will increasingly focus on problem formulation and strategic guidance. Our results highlight both the disruptive potential and the urgent need to adapt systems research practices in the age of AI.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06189v3",
    "published_date": "2025-10-07 17:49:24 UTC",
    "updated_date": "2025-10-10 17:53:53 UTC"
  },
  {
    "arxiv_id": "2510.06188v2",
    "title": "BanglaTalk: Towards Real-Time Speech Assistance for Bengali Regional Dialects",
    "authors": [
      "Jakir Hasan",
      "Shubhashis Roy Dipta"
    ],
    "abstract": "Real-time speech assistants are becoming increasingly popular for ensuring improved accessibility to information. Bengali, being a low-resource language with a high regional dialectal diversity, has seen limited progress in developing such systems. Existing systems are not optimized for real-time use and focus only on standard Bengali. In this work, we present BanglaTalk, the first real-time speech assistance system for Bengali regional dialects. BanglaTalk follows the client-server architecture and uses the Real-time Transport Protocol (RTP) to ensure low-latency communication. To address dialectal variation, we introduce a dialect-aware ASR system, BRDialect, developed by fine-tuning the IndicWav2Vec model in ten Bengali regional dialects. It outperforms the baseline ASR models by 12.41-33.98% on the RegSpeech12 dataset. Furthermore, BanglaTalk can operate at a low bandwidth of 24 kbps while maintaining an average end-to-end delay of 4.9 seconds. Low bandwidth usage and minimal end-to-end delay make the system both cost-effective and interactive for real-time use cases, enabling inclusive and accessible speech technology for the diverse community of Bengali speakers. Code is available in https://github.com/Jak57/BanglaTalk",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06188v2",
    "published_date": "2025-10-07 17:47:39 UTC",
    "updated_date": "2025-11-13 20:47:20 UTC"
  },
  {
    "arxiv_id": "2510.06187v3",
    "title": "Automated Program Repair of Uncompilable Student Code",
    "authors": [
      "Griffin Pitts",
      "Aum Pandya",
      "Darsh Rank",
      "Tirth Bhatt",
      "Muntasir Hoq",
      "Bita Akram"
    ],
    "abstract": "A significant portion of student programming submissions in CS1 learning environments are uncompilable, limiting their use in student modeling and downstream knowledge tracing. Traditional modeling pipelines often exclude these cases, discarding observations of student learning. This study investigates automated program repair as a strategy to recover uncompilable code while preserving students' structural intent for use in student modeling. Within this framework, we assess large language models (LLMs) as repair agents under high- and low-context prompting conditions. Repairs were evaluated for compilability, edit distance, and preservation of students' original structure and logic. While all models produced compilable repairs, they differed in how well they preserve students' control flow and code structure, affecting their pedagogical utility. By recovering uncompilable submissions, this work enables richer and more comprehensive analyses of learners' coding processes and development over time.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.SE",
    "comment": "In Proceedings of the 57th ACM Technical Symposium on Computer Science Education V.2 (SIGCSE TS 2026)",
    "pdf_url": "https://arxiv.org/pdf/2510.06187v3",
    "published_date": "2025-10-07 17:46:33 UTC",
    "updated_date": "2025-12-23 07:41:07 UTC"
  },
  {
    "arxiv_id": "2510.06186v2",
    "title": "RECODE-H: A Benchmark for Research Code Development with Interactive Human Feedback",
    "authors": [
      "Chunyu Miao",
      "Henry Peng Zou",
      "Yangning Li",
      "Yankai Chen",
      "Yibo Wang",
      "Fangxin Wang",
      "Yifan Li",
      "Wooseong Yang",
      "Bowei He",
      "Xinni Zhang",
      "Dianzhi Yu",
      "Hanchen Yang",
      "Hoang H Nguyen",
      "Yue Zhou",
      "Jie Yang",
      "Jizhou Guo",
      "Wenzhe Fan",
      "Chin-Yuan Yeh",
      "Panpan Meng",
      "Liancheng Fang",
      "Jinhu Qi",
      "Wei-Chieh Huang",
      "Zhengyao Gu",
      "Yuwei Han",
      "Langzhou He",
      "Yuyao Yang",
      "Yinghui Li",
      "Hai-Tao Zheng",
      "Xue Liu",
      "Irwin King",
      "Philip S. Yu"
    ],
    "abstract": "Large language models (LLMs) show the promise in supporting scientific research implementation, yet their ability to generate correct and executable code remains limited. Existing works largely adopt one-shot settings, ignoring the iterative and feedback-driven nature of realistic workflows of scientific research development. To address this gap, we present RECODE-H, a benchmark of 102 tasks from research papers and repositories that evaluates LLM agents through multi-turn interactions with LLM-simulated human feedback. It includes structured instructions,unit tests, and a five-level feedback hierarchy to reflect realistic researcher-agent collaboration. We further present ReCodeAgent, a framework that integrates feedback into iterative code generation. Experiments with leading LLMs, including GPT-5, Claude-Sonnet-4, DeepSeek-V3.1, and Gemini 2.5, show substantial performance gains with richer feedback, while also highlighting ongoing challenges in the generation of complex research code. RECODE-H establishes a foundation for developing adaptive, feedback-driven LLM agents in scientific research implementation",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Code and dataset are available at github.com/ChunyuMiao98/RECODE",
    "pdf_url": "https://arxiv.org/pdf/2510.06186v2",
    "published_date": "2025-10-07 17:45:35 UTC",
    "updated_date": "2025-10-24 17:20:26 UTC"
  },
  {
    "arxiv_id": "2510.06170v2",
    "title": "Smartphone-based iris recognition through high-quality visible-spectrum iris image capture.V2",
    "authors": [
      "Naveenkumar G Venkataswamy",
      "Yu Liu",
      "Soumyabrata Dey",
      "Stephanie Schuckers",
      "Masudul H Imtiaz"
    ],
    "abstract": "Smartphone-based iris recognition in the visible spectrum (VIS) remains difficult due to illumination variability, pigmentation differences, and the absence of standardized capture controls. This work presents a compact end-to-end pipeline that enforces ISO/IEC 29794-6 quality compliance at acquisition and demonstrates that accurate VIS iris recognition is feasible on commodity devices. Using a custom Android application performing real-time framing, sharpness evaluation, and feedback, we introduce the CUVIRIS dataset of 752 compliant images from 47 subjects. A lightweight MobileNetV3-based multi-task segmentation network (LightIrisNet) is developed for efficient on-device processing, and a transformer matcher (IrisFormer) is adapted to the VIS domain. Under a standardized protocol and comparative benchmarking against prior CNN baselines, OSIRIS attains a TAR of 97.9% at FAR=0.01 (EER=0.76%), while IrisFormer, trained only on UBIRIS.v2, achieves an EER of 0.057% on CUVIRIS. The acquisition app, trained models, and a public subset of the dataset are released to support reproducibility. These results confirm that standardized capture and VIS-adapted lightweight models enable accurate and practical iris recognition on smartphones.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "This submission has been withdrawn because it duplicates significant content from another version of the paper already available on arXiv as arXiv:2412.13063",
    "pdf_url": "https://arxiv.org/pdf/2510.06170v2",
    "published_date": "2025-10-07 17:33:41 UTC",
    "updated_date": "2025-10-27 16:44:15 UTC"
  },
  {
    "arxiv_id": "2510.06303v3",
    "title": "SDAR: A Synergistic Diffusion-AutoRegression Paradigm for Scalable Sequence Generation",
    "authors": [
      "Shuang Cheng",
      "Yihan Bian",
      "Dawei Liu",
      "Linfeng Zhang",
      "Qian Yao",
      "Zhongbo Tian",
      "Wenhai Wang",
      "Qipeng Guo",
      "Kai Chen",
      "Biqing Qi",
      "Bowen Zhou"
    ],
    "abstract": "We propose SDAR, a Synergistic Diffusion-Autoregression paradigm that unifies the training efficiency of autoregressive models with the parallel inference capability of diffusion. Instead of costly end-to-end diffusion training, SDAR performs a lightweight paradigm conversion that transforms a well-trained autoregressive (AR) model into a blockwise diffusion model through brief, data-efficient adaptation. During inference, SDAR generates sequences autoregressively across blocks for global coherence while decoding all tokens within each block in parallel via a discrete diffusion process. Extensive experiments show that AR models remain substantially more compute-efficient than masked diffusion models, providing a strong foundation for adaptation. Building on this insight, SDAR achieves efficient AR-to-diffusion conversion with minimal cost, preserving AR-level performance while enabling parallel generation. Scaling studies across dense and Mixture-of-Experts architectures confirm that SDAR scales without compromise: larger models exhibit stronger robustness to block size and decoding thresholds, yielding greater speedups without accuracy loss. Beyond efficiency, SDAR demonstrates enhanced reasoning and domain adaptability. Our 30B MoE model surpasses its AR counterpart on challenging scientific reasoning benchmarks such as GPQA and ChemBench, and gains further improvements under test-time scaling methods like majority voting and pass@k. Together, these results establish SDAR as a practical paradigm that combines the strengths of autoregression and diffusion for scalable, high-throughput reasoning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Technical report. 40 pages, Inference speedup analysis added",
    "pdf_url": "https://arxiv.org/pdf/2510.06303v3",
    "published_date": "2025-10-07 17:29:28 UTC",
    "updated_date": "2025-10-18 20:08:33 UTC"
  },
  {
    "arxiv_id": "2510.06302v1",
    "title": "Requirements for Game-Based Learning Design Framework for Information System Integration in the Context of Post-Merger Integration",
    "authors": [
      "Ksenija Lace",
      "Marite Kirikova"
    ],
    "abstract": "Post-merger integration states unique challenges for professionals responsible for information system integration aimed on alignment and combination diverse system architectures of merging organizations. Although the theoretical and practical guidance exists for post-merger integration on the business level, there is a significant gap in training for information system integration in this context. In prior research specific methods AMILI (Support method for informed decision identification) and AMILP (Support method for informed decision-making) were introduced for the support of information system integration decisions in the post-merger integration. But during the practical application was reported high learning curve and low learner motivation. This paper explores how game-based learning design can address these limitations by transforming static method training into engaging learning experience. The study analyzes foundational learning theories, cognitive load and motivation models, and serious game design frameworks to identify the essential requirements for a game-based learning design framework tailored to information system integration in post-merger integration. Requirements are structured in two components: the transformation process and resulting learning experience. The paper concludes with a plan for developing and evaluating the proposed framework through iterative design and real-world validation.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06302v1",
    "published_date": "2025-10-07 17:25:44 UTC",
    "updated_date": "2025-10-07 17:25:44 UTC"
  },
  {
    "arxiv_id": "2510.06151v1",
    "title": "LLMs as Policy-Agnostic Teammates: A Case Study in Human Proxy Design for Heterogeneous Agent Teams",
    "authors": [
      "Aju Ani Justus",
      "Chris Baber"
    ],
    "abstract": "A critical challenge in modelling Heterogeneous-Agent Teams is training agents to collaborate with teammates whose policies are inaccessible or non-stationary, such as humans. Traditional approaches rely on expensive human-in-the-loop data, which limits scalability. We propose using Large Language Models (LLMs) as policy-agnostic human proxies to generate synthetic data that mimics human decision-making. To evaluate this, we conduct three experiments in a grid-world capture game inspired by Stag Hunt, a game theory paradigm that balances risk and reward. In Experiment 1, we compare decisions from 30 human participants and 2 expert judges with outputs from LLaMA 3.1 and Mixtral 8x22B models. LLMs, prompted with game-state observations and reward structures, align more closely with experts than participants, demonstrating consistency in applying underlying decision criteria. Experiment 2 modifies prompts to induce risk-sensitive strategies (e.g. \"be risk averse\"). LLM outputs mirror human participants' variability, shifting between risk-averse and risk-seeking behaviours. Finally, Experiment 3 tests LLMs in a dynamic grid-world where the LLM agents generate movement actions. LLMs produce trajectories resembling human participants' paths. While LLMs cannot yet fully replicate human adaptability, their prompt-guided diversity offers a scalable foundation for simulating policy-agnostic teammates.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "This is a preprint of a paper presented at the \\textit{European Conference on Artificial Intelligence (ECAI 2025)}. It is made publicly available for the benefit of the research community and should be regarded as a preprint rather than a formally reviewed publication",
    "pdf_url": "https://arxiv.org/pdf/2510.06151v1",
    "published_date": "2025-10-07 17:21:20 UTC",
    "updated_date": "2025-10-07 17:21:20 UTC"
  },
  {
    "arxiv_id": "2510.06145v1",
    "title": "Bimanual 3D Hand Motion and Articulation Forecasting in Everyday Images",
    "authors": [
      "Aditya Prakash",
      "David Forsyth",
      "Saurabh Gupta"
    ],
    "abstract": "We tackle the problem of forecasting bimanual 3D hand motion & articulation from a single image in everyday settings. To address the lack of 3D hand annotations in diverse settings, we design an annotation pipeline consisting of a diffusion model to lift 2D hand keypoint sequences to 4D hand motion. For the forecasting model, we adopt a diffusion loss to account for the multimodality in hand motion distribution. Extensive experiments across 6 datasets show the benefits of training on diverse data with imputed labels (14% improvement) and effectiveness of our lifting (42% better) & forecasting (16.4% gain) models, over the best baselines, especially in zero-shot generalization to everyday images.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://ap229997.github.io/projects/forehand4d",
    "pdf_url": "https://arxiv.org/pdf/2510.06145v1",
    "published_date": "2025-10-07 17:18:56 UTC",
    "updated_date": "2025-10-07 17:18:56 UTC"
  },
  {
    "arxiv_id": "2510.06138v1",
    "title": "Multi-Task Reinforcement Learning with Language-Encoded Gated Policy Networks",
    "authors": [
      "Rushiv Arora"
    ],
    "abstract": "Multi-task reinforcement learning often relies on task metadata -- such as brief natural-language descriptions -- to guide behavior across diverse objectives. We present Lexical Policy Networks (LEXPOL), a language-conditioned mixture-of-policies architecture for multi-task RL. LEXPOL encodes task metadata with a text encoder and uses a learned gating module to select or blend among multiple sub-policies, enabling end-to-end training across tasks. On MetaWorld benchmarks, LEXPOL matches or exceeds strong multi-task baselines in success rate and sample efficiency, without task-specific retraining. To analyze the mechanism, we further study settings with fixed expert policies obtained independently of the gate and show that the learned language gate composes these experts to produce behaviors appropriate to novel task descriptions and unseen task combinations. These results indicate that natural-language metadata can effectively index and recombine reusable skills within a single policy.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages, 3 figures, 12 tables, 2 appendices. Currently under review",
    "pdf_url": "https://arxiv.org/pdf/2510.06138v1",
    "published_date": "2025-10-07 17:12:24 UTC",
    "updated_date": "2025-10-07 17:12:24 UTC"
  },
  {
    "arxiv_id": "2510.06135v1",
    "title": "Pushing Test-Time Scaling Limits of Deep Search with Asymmetric Verification",
    "authors": [
      "Weihao Zeng",
      "Keqing He",
      "Chuqiao Kuang",
      "Xiaoguang Li",
      "Junxian He"
    ],
    "abstract": "Test-time compute can be scaled both sequentially and in parallel. Sequential scaling involves lengthening the generation process, while parallel scaling involves verifying and selecting among multiple candidate outputs. Combining these two strategies has led to the most powerful AI systems, such as Grok 4 Heavy and GPT-5 Pro. In certain contexts (e.g., solving Sudoku puzzles), verifying responses can be substantially easier than generating them. This property, referred to as \\emph{asymmetric verification}, highlights the strong potential of test-time scaling (TTS). In this work, we study both sequential and parallel TTS of deep search agents, motivated by the intuition that verification in this setting is often much easier than generation. In experiments, we first show that sequential scaling methods, such as budget forcing, can be effective initially but soon degrade performance. Leveraging asymmetric verification, however, we are able to achieve substantial improvements by allocating only a modest amount of compute to the verifier. We conduct experiments with flagship open-source models and extend them to their ``Heavy'' variants through TTS. These deep research agents achieve gains of up to 27 absolute points on benchmarks such as BrowseComp. Remarkably, as an open-source alternative, GLM-4.5 Heavy reaches accuracy of {\\bf 54.0\\%} on BrowseComp and {\\bf 66.0\\%} on GAIA, placing it comparable to the best proprietary choices such as OpenAI Deep Research. Tongyi-DeepResearch Heavy further achieves {\\bf 69.0\\%} accuracy on BrowseComp, greatly surpassing the best proprietary results.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06135v1",
    "published_date": "2025-10-07 17:09:23 UTC",
    "updated_date": "2025-10-07 17:09:23 UTC"
  },
  {
    "arxiv_id": "2510.06133v1",
    "title": "CreditDecoding: Accelerating Parallel Decoding in Diffusion Large Language Models with Trace Credits",
    "authors": [
      "Kangyu Wang",
      "Zhiyun Jiang",
      "Haibo Feng",
      "Weijia Zhao",
      "Lin Liu",
      "Jianguo Li",
      "Zhenzhong Lan",
      "Weiyao Lin"
    ],
    "abstract": "Diffusion large language models (dLLMs) generate text through iterative denoising steps, achieving parallel decoding by denoising only high-confidence positions at each step. However, existing approaches often repetitively remask tokens due to initially low confidence scores, leading to redundant iterations and limiting overall acceleration. Through the analysis of dLLM decoding traces, we observe that the model often determines the final prediction for a token several steps before the decoding step. To leverage this historical information and avoid redundant steps, we introduce the concept of Trace Credit, which quantifies each token's convergence potential by accumulating historical logits. Furthermore, we propose CreditDecoding, a training-free parallel decoding algorithm that accelerates the confidence convergence of correct but underconfident tokens by fusing current logits with Trace Credit. This process significantly reduces redundant iterations and enhances decoding robustness. On eight benchmarks, CreditDecoding achieves a 5.48 times speedup and a 0.48 performance improvement over LLaDA-8B-Instruct, and a 4.11 times speedup with a 0.15 performance improvement over LLaDA-MoE-Instruct. Importantly, CreditDecoding scales effectively to long sequences and is orthogonal to mainstream inference optimizations, making it a readily integrable and versatile solution.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "18 pages,8 figures,4 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.06133v1",
    "published_date": "2025-10-07 17:08:33 UTC",
    "updated_date": "2025-10-07 17:08:33 UTC"
  },
  {
    "arxiv_id": "2510.06131v1",
    "title": "Discrete Diffusion Models with MLLMs for Unified Medical Multimodal Generation",
    "authors": [
      "Jiawei Mao",
      "Yuhan Wang",
      "Lifeng Chen",
      "Can Zhao",
      "Yucheng Tang",
      "Dong Yang",
      "Liangqiong Qu",
      "Daguang Xu",
      "Yuyin Zhou"
    ],
    "abstract": "Recent advances in generative medical models are constrained by modality-specific scenarios that hinder the integration of complementary evidence from imaging, pathology, and clinical notes. This fragmentation limits their evolution into foundation models that can learn and reason across the full spectrum of biomedical data. We propose MeDiM, the first medical discrete diffusion model that learns shared distributions across modalities without modality-specific components. MeDiM unifies multiple generative tasks: translating between images and text, and jointly producing image-report pairs across domains in response to prompts. Built on a discrete diffusion framework, MeDiM bridges vision and language representations through a shared probabilistic space. To enable unified and flexible medical generation, we employ a multimodal large language model (MLLM) as the diffusion backbone, leveraging its prior knowledge and cross-modal reasoning. Two key designs are introduced: (1) removing the causal attention mask for bidirectional context, and (2) injecting continuous timestep embeddings for diffusion awareness. Experiments demonstrate high-fidelity medical generation (FID 16.60 on MIMIC-CXR and FID 24.19 on PathGen) and accurate report generation (METEOR 0.2650 and 0.2580). Jointly generated image-report pairs further enhance downstream performance (plus6.43 percent BLEU-1, plus18.57 percent BLEU-2, plus31.58 percent BLEU-3, plus4.80 percent METEOR), showing that MeDiM supports coherent and clinically grounded multimodal outputs.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "16 pages,6 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.06131v1",
    "published_date": "2025-10-07 17:06:57 UTC",
    "updated_date": "2025-10-07 17:06:57 UTC"
  },
  {
    "arxiv_id": "2510.09663v1",
    "title": "Adversarial-Resilient RF Fingerprinting: A CNN-GAN Framework for Rogue Transmitter Detection",
    "authors": [
      "Raju Dhakal",
      "Prashant Shekhar",
      "Laxima Niure Kandel"
    ],
    "abstract": "Radio Frequency Fingerprinting (RFF) has evolved as an effective solution for authenticating devices by leveraging the unique imperfections in hardware components involved in the signal generation process. In this work, we propose a Convolutional Neural Network (CNN) based framework for detecting rogue devices and identifying genuine ones using softmax probability thresholding. We emulate an attack scenario in which adversaries attempt to mimic the RF characteristics of genuine devices by training a Generative Adversarial Network (GAN) using In-phase and Quadrature (IQ) samples from genuine devices. The proposed approach is verified using IQ samples collected from ten different ADALM-PLUTO Software Defined Radios (SDRs), with seven devices considered genuine, two as rogue, and one used for validation to determine the threshold.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted for publication in ICMLA 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.09663v1",
    "published_date": "2025-10-07 17:04:50 UTC",
    "updated_date": "2025-10-07 17:04:50 UTC"
  },
  {
    "arxiv_id": "2510.06107v2",
    "title": "Distributional Semantics Tracing: A Framework for Explaining Hallucinations in Large Language Models",
    "authors": [
      "Gagan Bhatia",
      "Somayajulu G Sripada",
      "Kevin Allan",
      "Jacobo Azcona"
    ],
    "abstract": "Large Language Models (LLMs) are prone to hallucination, the generation of plausible yet factually incorrect statements. This work investigates the intrinsic, architectural origins of this failure mode through three primary contributions. First, to enable the reliable tracing of internal semantic failures, we propose Distributional Semantics Tracing (DST), a unified framework that integrates established interpretability techniques to produce a causal map of a model's reasoning, treating meaning as a function of context (distributional semantics). Second, we pinpoint the model's layer at which a hallucination becomes inevitable, identifying a specific commitment layer where a model's internal representations irreversibly diverge from factuality. Third, we identify the underlying mechanism for these failures. We observe a conflict between distinct computational pathways, which we interpret using the lens of dual-process theory: a fast, heuristic associative pathway (akin to System 1) and a slow, deliberate, contextual pathway (akin to System 2), leading to predictable failure modes such as Reasoning Shortcut Hijacks. Our framework's ability to quantify the coherence of the contextual pathway reveals a strong negative correlation ($ρ= -0.863$) with hallucination rates, implying that these failures are predictable consequences of internal semantic weakness. The result is a mechanistic account of how, when, and why hallucinations occur within the Transformer architecture.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06107v2",
    "published_date": "2025-10-07 16:40:31 UTC",
    "updated_date": "2025-10-08 18:51:54 UTC"
  },
  {
    "arxiv_id": "2510.06105v1",
    "title": "Moloch's Bargain: Emergent Misalignment When LLMs Compete for Audiences",
    "authors": [
      "Batu El",
      "James Zou"
    ],
    "abstract": "Large language models (LLMs) are increasingly shaping how information is created and disseminated, from companies using them to craft persuasive advertisements, to election campaigns optimizing messaging to gain votes, to social media influencers boosting engagement. These settings are inherently competitive, with sellers, candidates, and influencers vying for audience approval, yet it remains poorly understood how competitive feedback loops influence LLM behavior. We show that optimizing LLMs for competitive success can inadvertently drive misalignment. Using simulated environments across these scenarios, we find that, 6.3% increase in sales is accompanied by a 14.0% rise in deceptive marketing; in elections, a 4.9% gain in vote share coincides with 22.3% more disinformation and 12.5% more populist rhetoric; and on social media, a 7.5% engagement boost comes with 188.6% more disinformation and a 16.3% increase in promotion of harmful behaviors. We call this phenomenon Moloch's Bargain for AI--competitive success achieved at the cost of alignment. These misaligned behaviors emerge even when models are explicitly instructed to remain truthful and grounded, revealing the fragility of current alignment safeguards. Our findings highlight how market-driven optimization pressures can systematically erode alignment, creating a race to the bottom, and suggest that safe deployment of AI systems will require stronger governance and carefully designed incentives to prevent competitive dynamics from undermining societal trust.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06105v1",
    "published_date": "2025-10-07 16:37:15 UTC",
    "updated_date": "2025-10-07 16:37:15 UTC"
  },
  {
    "arxiv_id": "2510.06093v1",
    "title": "Classical AI vs. LLMs for Decision-Maker Alignment in Health Insurance Choices",
    "authors": [
      "Mallika Mainali",
      "Harsha Sureshbabu",
      "Anik Sen",
      "Christopher B. Rauch",
      "Noah D. Reifsnyder",
      "John Meyer",
      "J. T. Turner",
      "Michael W. Floyd",
      "Matthew Molineaux",
      "Rosina O. Weber"
    ],
    "abstract": "As algorithmic decision-makers are increasingly applied to high-stakes domains, AI alignment research has evolved from a focus on universal value alignment to context-specific approaches that account for decision-maker attributes. Prior work on Decision-Maker Alignment (DMA) has explored two primary strategies: (1) classical AI methods integrating case-based reasoning, Bayesian reasoning, and naturalistic decision-making, and (2) large language model (LLM)-based methods leveraging prompt engineering. While both approaches have shown promise in limited domains such as medical triage, their generalizability to novel contexts remains underexplored. In this work, we implement a prior classical AI model and develop an LLM-based algorithmic decision-maker evaluated using a large reasoning model (GPT-5) and a non-reasoning model (GPT-4) with weighted self-consistency under a zero-shot prompting framework, as proposed in recent literature. We evaluate both approaches on a health insurance decision-making dataset annotated for three target decision-makers with varying levels of risk tolerance (0.0, 0.5, 1.0). In the experiments reported herein, classical AI and LLM-based models achieved comparable alignment with attribute-based targets, with classical AI exhibiting slightly better alignment for a moderate risk profile. The dataset and open-source implementation are publicly available at: https://github.com/TeX-Base/ClassicalAIvsLLMsforDMAlignment and https://github.com/Parallax-Advanced-Research/ITM/tree/feature_insurance.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "15 pages, 3 figures. Accepted at the Twelfth Annual Conference on Advances in Cognitive Systems (ACS 2025)",
    "pdf_url": "https://arxiv.org/pdf/2510.06093v1",
    "published_date": "2025-10-07 16:21:52 UTC",
    "updated_date": "2025-10-07 16:21:52 UTC"
  },
  {
    "arxiv_id": "2510.06090v1",
    "title": "A public cardiac CT dataset featuring the left atrial appendage",
    "authors": [
      "Bjoern Hansen",
      "Jonas Pedersen",
      "Klaus F. Kofoed",
      "Oscar Camara",
      "Rasmus R. Paulsen",
      "Kristine Soerensen"
    ],
    "abstract": "Despite the success of advanced segmentation frameworks such as TotalSegmentator (TS), accurate segmentations of the left atrial appendage (LAA), coronary arteries (CAs), and pulmonary veins (PVs) remain a significant challenge in medical imaging. In this work, we present the first open-source, anatomically coherent dataset of curated, high-resolution segmentations for these structures, supplemented with whole-heart labels produced by TS on the publicly available ImageCAS dataset consisting of 1000 cardiac computed tomography angiography (CCTA) scans. One purpose of the data set is to foster novel approaches to the analysis of LAA morphology.\n  LAA segmentations on ImageCAS were generated using a state-of-the-art segmentation framework developed specifically for high resolution LAA segmentation. We trained the network on a large private dataset with manual annotations provided by medical readers guided by a trained cardiologist and transferred the model to ImageCAS data. CA labels were improved from the original ImageCAS annotations, while PV segmentations were refined from TS outputs. In addition, we provide a list of scans from ImageCAS that contains common data flaws such as step artefacts, LAAs extending beyond the scanner's field of view, and other types of data defects.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 5 figures, published at STACOM2025",
    "pdf_url": "https://arxiv.org/pdf/2510.06090v1",
    "published_date": "2025-10-07 16:16:59 UTC",
    "updated_date": "2025-10-07 16:16:59 UTC"
  },
  {
    "arxiv_id": "2510.06084v1",
    "title": "Spectrum Tuning: Post-Training for Distributional Coverage and In-Context Steerability",
    "authors": [
      "Taylor Sorensen",
      "Benjamin Newman",
      "Jared Moore",
      "Chan Park",
      "Jillian Fisher",
      "Niloofar Mireshghallah",
      "Liwei Jiang",
      "Yejin Choi"
    ],
    "abstract": "Language model post-training has enhanced instruction-following and performance on many downstream tasks, but also comes with an often-overlooked cost on tasks with many possible valid answers. We characterize three desiderata for conditional distributional modeling: in-context steerability, valid output space coverage, and distributional alignment, and document across three model families how current post-training can reduce these properties. In particular, we disambiguate between two kinds of in-context learning: ICL for eliciting existing underlying knowledge or capabilities, and in-context steerability, where a model must use in-context information to override its priors and steer to a novel data generating distribution. To better evaluate and improve these desiderata, we introduce Spectrum Suite, a large-scale resource compiled from >40 data sources and spanning >90 tasks requiring models to steer to and match diverse distributions ranging from varied human preferences to numerical distributions and more. We find that while current post-training techniques help elicit underlying capabilities and knowledge, they hurt models' ability to flexibly steer in-context. To mitigate these issues, we propose Spectrum Tuning, a post-training method using Spectrum Suite to improve steerability and distributional coverage. We find that Spectrum Tuning often improves over pretrained models and their instruction-tuned counterparts, enhancing steerability, spanning more of the output space, and improving distributional alignment on held-out datasets.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06084v1",
    "published_date": "2025-10-07 16:10:26 UTC",
    "updated_date": "2025-10-07 16:10:26 UTC"
  },
  {
    "arxiv_id": "2510.09660v4",
    "title": "Learning What Matters: Steering Diffusion via Spectrally Anisotropic Forward Noise",
    "authors": [
      "Luca Scimeca",
      "Thomas Jiralerspong",
      "Berton Earnshaw",
      "Jason Hartford",
      "Yoshua Bengio"
    ],
    "abstract": "Diffusion Probabilistic Models (DPMs) have achieved strong generative performance, yet their inductive biases remain largely implicit. In this work, we aim to build inductive biases into the training and sampling of diffusion models to better accommodate the target distribution of the data to model. We introduce an anisotropic noise operator that shapes these biases by replacing the isotropic forward covariance with a structured, frequency-diagonal covariance. This operator unifies band-pass masks and power-law weightings, allowing us to emphasize or suppress designated frequency bands, while keeping the forward process Gaussian. We refer to this as Spectrally Anisotropic Gaussian Diffusion (SAGD). In this work, we derive the score relation for anisotropic forward covariances and show that, under full support, the learned score converges to the true data score as $t\\!\\to\\!0$, while anisotropy reshapes the probability-flow path from noise to data. Empirically, we show the induced anisotropy outperforms standard diffusion across several vision datasets, and enables selective omission: learning while ignoring known corruptions confined to specific bands. Together, these results demonstrate that carefully designed anisotropic forward noise provides a simple, yet principled, handle to tailor inductive bias in DPMs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09660v4",
    "published_date": "2025-10-07 16:08:39 UTC",
    "updated_date": "2025-12-10 16:04:26 UTC"
  },
  {
    "arxiv_id": "2510.06078v1",
    "title": "Constraint-Aware Route Recommendation from Natural Language via Hierarchical LLM Agents",
    "authors": [
      "Tao Zhe",
      "Rui Liu",
      "Fateme Memar",
      "Xiao Luo",
      "Wei Fan",
      "Xinyue Ye",
      "Zhongren Peng",
      "Dongjie Wang"
    ],
    "abstract": "Route recommendation aims to provide users with optimal travel plans that satisfy diverse and complex requirements. Classical routing algorithms (e.g., shortest-path and constraint-aware search) are efficient but assume structured inputs and fixed objectives, limiting adaptability to natural-language queries. Recent LLM-based approaches enhance flexibility but struggle with spatial reasoning and the joint modeling of route-level and POI-level preferences. To address these limitations, we propose RouteLLM, a hierarchical multi-agent framework that grounds natural-language intents into constraint-aware routes. It first parses user queries into structured intents including POIs, paths, and constraints. A manager agent then coordinates specialized sub-agents: a constraint agent that resolves and formally check constraints, a POI agent that retrieves and ranks candidate POIs, and a path refinement agent that refines routes via a routing engine with preference-conditioned costs. A final verifier agent ensures constraint satisfaction and produces the final route with an interpretable rationale. This design bridges linguistic flexibility and spatial structure, enabling reasoning over route feasibility and user preferences. Experiments show that our method reliably grounds textual preferences into constraint-aware routes, improving route quality and preference satisfaction over classical methods.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06078v1",
    "published_date": "2025-10-07 16:03:57 UTC",
    "updated_date": "2025-10-07 16:03:57 UTC"
  },
  {
    "arxiv_id": "2510.06077v1",
    "title": "When Thinking Drifts: Evidential Grounding for Robust Video Reasoning",
    "authors": [
      "Mi Luo",
      "Zihui Xue",
      "Alex Dimakis",
      "Kristen Grauman"
    ],
    "abstract": "Video reasoning, the task of enabling machines to infer from dynamic visual content through multi-step logic, is crucial for advanced AI. While the Chain-of-Thought (CoT) mechanism has enhanced reasoning in text-based tasks, its application to video understanding remains underexplored. This paper presents a systematic analysis revealing that CoT often degrades performance in video reasoning, generating verbose but misleading internal monologues, and leading to hallucinated visual details and overridden correct intuitions - a phenomenon we term \"visual thinking drift\". We explain this drift through a Bayesian lens, positing that CoT traces often diverge from actual visual evidence, instead amplifying internal biases or language priors, causing models to storytell rather than engage in grounded reasoning. To counteract this, we introduce Visual Evidence Reward (VER), a novel reinforcement learning framework that explicitly rewards the generation of reasoning traces that are verifiably grounded in visual evidence. Comprehensive evaluation across 10 diverse video understanding benchmarks demonstrates that our Video-VER consistently achieves top performance. Our work sheds light on the distinct challenges of video-centric reasoning and encourages the development of AI that robustly grounds its inferences in visual evidence - for large multimodal models that not only \"think before answering\", but also \"see while thinking\".",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by NeurIPS 2025, Project page: https://vision.cs.utexas.edu/projects/video-ver/",
    "pdf_url": "https://arxiv.org/pdf/2510.06077v1",
    "published_date": "2025-10-07 16:03:33 UTC",
    "updated_date": "2025-10-07 16:03:33 UTC"
  },
  {
    "arxiv_id": "2510.06071v1",
    "title": "Benchmark It Yourself (BIY): Preparing a Dataset and Benchmarking AI Models for Scatterplot-Related Tasks",
    "authors": [
      "João Palmeiro",
      "Diogo Duarte",
      "Rita Costa",
      "Pedro Bizarro"
    ],
    "abstract": "AI models are increasingly used for data analysis and visualization, yet benchmarks rarely address scatterplot-specific tasks, limiting insight into performance. To address this gap for one of the most common chart types, we introduce a synthetic, annotated dataset of over 18,000 scatterplots from six data generators and 17 chart designs, and a benchmark based on it. We evaluate proprietary models from OpenAI and Google using N-shot prompting on five distinct tasks derived from annotations of cluster bounding boxes, their center coordinates, and outlier coordinates. OpenAI models and Gemini 2.5 Flash, especially when prompted with examples, are viable options for counting clusters and, in Flash's case, outliers (90%+ Accuracy). However, the results for localization-related tasks are unsatisfactory: Precision and Recall are near or below 50%, except for Flash in outlier identification (65.01%). Furthermore, the impact of chart design on performance appears to be a secondary factor, but it is advisable to avoid scatterplots with wide aspect ratios (16:9 and 21:9) or those colored randomly. Supplementary materials are available at https://github.com/feedzai/biy-paper.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 3 figures, short paper accepted at VISxGenAI: 1st Workshop on GenAI, Agents, and the Future of VIS (IEEE VIS 2025)",
    "pdf_url": "https://arxiv.org/pdf/2510.06071v1",
    "published_date": "2025-10-07 15:59:19 UTC",
    "updated_date": "2025-10-07 15:59:19 UTC"
  },
  {
    "arxiv_id": "2510.06068v1",
    "title": "Cross-Embodiment Dexterous Hand Articulation Generation via Morphology-Aware Learning",
    "authors": [
      "Heng Zhang",
      "Kevin Yuchen Ma",
      "Mike Zheng Shou",
      "Weisi Lin",
      "Yan Wu"
    ],
    "abstract": "Dexterous grasping with multi-fingered hands remains challenging due to high-dimensional articulations and the cost of optimization-based pipelines. Existing end-to-end methods require training on large-scale datasets for specific hands, limiting their ability to generalize across different embodiments. We propose an eigengrasp-based, end-to-end framework for cross-embodiment grasp generation. From a hand's morphology description, we derive a morphology embedding and an eigengrasp set. Conditioned on these, together with the object point cloud and wrist pose, an amplitude predictor regresses articulation coefficients in a low-dimensional space, which are decoded into full joint articulations. Articulation learning is supervised with a Kinematic-Aware Articulation Loss (KAL) that emphasizes fingertip-relevant motions and injects morphology-specific structure. In simulation on unseen objects across three dexterous hands, our model attains a 91.9% average grasp success rate with less than 0.4 seconds inference per grasp. With few-shot adaptation to an unseen hand, it achieves 85.6% success on unseen objects in simulation, and real-world experiments on this few-shot generalized hand achieve an 87% success rate. The code and additional materials will be made available upon publication on our project website https://connor-zh.github.io/cross_embodiment_dexterous_grasping.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06068v1",
    "published_date": "2025-10-07 15:57:00 UTC",
    "updated_date": "2025-10-07 15:57:00 UTC"
  },
  {
    "arxiv_id": "2510.06067v2",
    "title": "Reasoning under Vision: Understanding Visual-Spatial Cognition in Vision-Language Models for CAPTCHA",
    "authors": [
      "Python Song",
      "Luke Tenyi Chang",
      "Yun-Yun Tsai",
      "Penghui Li",
      "Junfeng Yang"
    ],
    "abstract": "CAPTCHA, originally designed to distinguish humans from robots, has evolved into a real-world benchmark for assessing the spatial reasoning capabilities of vision-language models. In this work, we first show that step-by-step reasoning is crucial for vision-language models (VLMs) to solve CAPTCHAs, which represent high-difficulty spatial reasoning tasks, and that current commercial vision-language models still struggle with such reasoning. In particular, we observe that most commercial VLMs (e.g., Gemini, Claude, GPT, etc.) fail to effectively solve CAPTCHAs and thus achieve low accuracy (around 21.9 percent). However, our findings indicate that requiring the model to perform step-by-step reasoning before generating the final coordinates can significantly enhance its solving accuracy, underscoring the severity of the gap. To systematically study this issue, we introduce CAPTCHA-X, the first real-world CAPTCHA benchmark with reasoning, covering seven categories of CAPTCHAs (such as Gobang, hCaptcha, etc.) with step-by-step action solutions and grounding annotations. We further define five reasoning-oriented metrics that enable a comprehensive evaluation of models reasoning capabilities. To validate the effectiveness of reasoning, we also propose a general agentic VLM-based framework that incorporates the models inherent reasoning abilities. Our method achieves state-of-the-art performance across five high-difficulty CAPTCHA types, with an average solving accuracy of 83.9 percent, substantially surpassing existing baselines. These results reveal the limitations of current models and highlight the importance of reasoning in advancing visual-spatial challenges in the future.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "14pages, 11figures",
    "pdf_url": "https://arxiv.org/pdf/2510.06067v2",
    "published_date": "2025-10-07 15:56:21 UTC",
    "updated_date": "2025-11-16 00:34:10 UTC"
  },
  {
    "arxiv_id": "2510.06063v1",
    "title": "TelecomTS: A Multi-Modal Observability Dataset for Time Series and Language Analysis",
    "authors": [
      "Austin Feng",
      "Andreas Varvarigos",
      "Ioannis Panitsas",
      "Daniela Fernandez",
      "Jinbiao Wei",
      "Yuwei Guo",
      "Jialin Chen",
      "Ali Maatouk",
      "Leandros Tassiulas",
      "Rex Ying"
    ],
    "abstract": "Modern enterprises generate vast streams of time series metrics when monitoring complex systems, known as observability data. Unlike conventional time series from domains such as weather, observability data are zero-inflated, highly stochastic, and exhibit minimal temporal structure. Despite their importance, observability datasets are underrepresented in public benchmarks due to proprietary restrictions. Existing datasets are often anonymized and normalized, removing scale information and limiting their use for tasks beyond forecasting, such as anomaly detection, root-cause analysis, and multi-modal reasoning. To address this gap, we introduce TelecomTS, a large-scale observability dataset derived from a 5G telecommunications network. TelecomTS features heterogeneous, de-anonymized covariates with explicit scale information and supports a suite of downstream tasks, including anomaly detection, root-cause analysis, and a question-answering benchmark requiring multi-modal reasoning. Benchmarking state-of-the-art time series, language, and reasoning models reveals that existing approaches struggle with the abrupt, noisy, and high-variance dynamics of observability data. Our experiments also underscore the importance of preserving covariates' absolute scale, emphasizing the need for foundation time series models that natively leverage scale information for practical observability applications.",
    "categories": [
      "cs.AI",
      "cs.IT",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06063v1",
    "published_date": "2025-10-07 15:54:34 UTC",
    "updated_date": "2025-10-07 15:54:34 UTC"
  },
  {
    "arxiv_id": "2510.06060v1",
    "title": "Controllable Audio-Visual Viewpoint Generation from 360° Spatial Information",
    "authors": [
      "Christian Marinoni",
      "Riccardo Fosco Gramaccioni",
      "Eleonora Grassucci",
      "Danilo Comminiello"
    ],
    "abstract": "The generation of sounding videos has seen significant advancements with the advent of diffusion models. However, existing methods often lack the fine-grained control needed to generate viewpoint-specific content from larger, immersive 360-degree environments. This limitation restricts the creation of audio-visual experiences that are aware of off-camera events. To the best of our knowledge, this is the first work to introduce a framework for controllable audio-visual generation, addressing this unexplored gap. Specifically, we propose a diffusion model by introducing a set of powerful conditioning signals derived from the full 360-degree space: a panoramic saliency map to identify regions of interest, a bounding-box-aware signed distance map to define the target viewpoint, and a descriptive caption of the entire scene. By integrating these controls, our model generates spatially-aware viewpoint videos and audios that are coherently influenced by the broader, unseen environmental context, introducing a strong controllability that is essential for realistic and immersive audio-visual generation. We show audiovisual examples proving the effectiveness of our framework.",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.MM",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06060v1",
    "published_date": "2025-10-07 15:53:31 UTC",
    "updated_date": "2025-10-07 15:53:31 UTC"
  },
  {
    "arxiv_id": "2510.06056v1",
    "title": "Scientific Algorithm Discovery by Augmenting AlphaEvolve with Deep Research",
    "authors": [
      "Gang Liu",
      "Yihan Zhu",
      "Jie Chen",
      "Meng Jiang"
    ],
    "abstract": "Large language models hold promise as scientific assistants, yet existing agents either rely solely on algorithm evolution or on deep research in isolation, both of which face critical limitations. Pure algorithm evolution, as in AlphaEvolve, depends only on the internal knowledge of LLMs and quickly plateaus in complex domains, while pure deep research proposes ideas without validation, resulting in unrealistic or unimplementable solutions. We present DeepEvolve, an agent that integrates deep research with algorithm evolution, uniting external knowledge retrieval, cross-file code editing, and systematic debugging under a feedback-driven iterative loop. Each iteration not only proposes new hypotheses but also refines, implements, and tests them, avoiding both shallow improvements and unproductive over-refinements. Across nine benchmarks in chemistry, mathematics, biology, materials, and patents, DeepEvolve consistently improves the initial algorithm, producing executable new algorithms with sustained gains. By bridging the gap between unguided evolution and research without grounding, DeepEvolve provides a reliable framework for advancing scientific algorithm discovery. Our code is available at https://github.com/liugangcode/deepevolve.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "25 pages, 17 figures, 4 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.06056v1",
    "published_date": "2025-10-07 15:49:51 UTC",
    "updated_date": "2025-10-07 15:49:51 UTC"
  },
  {
    "arxiv_id": "2510.06052v1",
    "title": "MixReasoning: Switching Modes to Think",
    "authors": [
      "Haiquan Lu",
      "Gongfan Fang",
      "Xinyin Ma",
      "Qi Li",
      "Xinchao Wang"
    ],
    "abstract": "Reasoning models enhance performance by tackling problems in a step-by-step manner, decomposing them into sub-problems and exploring long chains of thought before producing an answer. However, applying extended reasoning to every step introduces substantial redundancy, as sub-problems vary widely in difficulty and complexity: a small number of pivotal steps are genuinely challenging and decisive for the final answer, while many others only involve straightforward revisions or simple computations. Therefore, a natural idea is to endow reasoning models with the ability to adaptively respond to this variation, rather than treating all steps with the same level of elaboration. To this end, we propose MixReasoning, a framework that dynamically adjusts the depth of reasoning within a single response. The resulting chain of thought then becomes a mixture of detailed reasoning on difficult steps and concise inference on simpler ones. Experiments on GSM8K, MATH-500, and AIME show that MixReasoning shortens reasoning length and substantially improves efficiency without compromising accuracy.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06052v1",
    "published_date": "2025-10-07 15:46:34 UTC",
    "updated_date": "2025-10-07 15:46:34 UTC"
  },
  {
    "arxiv_id": "2510.06046v1",
    "title": "GLVD: Guided Learned Vertex Descent",
    "authors": [
      "Pol Caselles Rico",
      "Francesc Moreno Noguer"
    ],
    "abstract": "Existing 3D face modeling methods usually depend on 3D Morphable Models, which inherently constrain the representation capacity to fixed shape priors. Optimization-based approaches offer high-quality reconstructions but tend to be computationally expensive. In this work, we introduce GLVD, a hybrid method for 3D face reconstruction from few-shot images that extends Learned Vertex Descent (LVD) by integrating per-vertex neural field optimization with global structural guidance from dynamically predicted 3D keypoints. By incorporating relative spatial encoding, GLVD iteratively refines mesh vertices without requiring dense 3D supervision. This enables expressive and adaptable geometry reconstruction while maintaining computational efficiency. GLVD achieves state-of-the-art performance in single-view settings and remains highly competitive in multi-view scenarios, all while substantially reducing inference time.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06046v1",
    "published_date": "2025-10-07 15:40:10 UTC",
    "updated_date": "2025-10-07 15:40:10 UTC"
  },
  {
    "arxiv_id": "2511.20653v1",
    "title": "Domain-Grounded Evaluation of LLMs in International Student Knowledge",
    "authors": [
      "Claudinei Daitx",
      "Haitham Amar"
    ],
    "abstract": "Large language models (LLMs) are increasingly used to answer high-stakes study-abroad questions about admissions, visas, scholarships, and eligibility. Yet it remains unclear how reliably they advise students, and how often otherwise helpful answers drift into unsupported claims (``hallucinations'').\n  This work provides a clear, domain-grounded overview of how current LLMs behave in this setting. Using realistic questions set drawn from ApplyBoard's advising workflows -- an EdTech platform that supports students from discovery to enrolment -- we evaluate two essentials side by side: accuracy (is the information correct and complete?) and hallucination (does the model add content not supported by the question or domain evidence). These questions are categorized by domain scope which can be a single-domain or multi-domain -- when it must integrate evidence across areas such as admissions, visas, and scholarships.\n  To reflect real advising quality, we grade answers with a simple rubric which is correct, partial, or wrong. The rubric is domain-coverage-aware: an answer can be partial if it addresses only a subset of the required domains, and it can be over-scoped if it introduces extra, unnecessary domains; both patterns are captured in our scoring as under-coverage or reduced relevance/hallucination.\n  We also report measures of faithfulness and answer relevance, alongside an aggregate hallucination score, to capture relevance and usefulness. All models are tested with the same questions for a fair, head-to-head comparison.\n  Our goals are to: (1) give a clear picture of which models are most dependable for study-abroad advising, (2) surface common failure modes -- where answers are incomplete, off-topic, or unsupported, and (3) offer a practical, reusable protocol for auditing LLMs before deployment in education and advising contexts.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.20653v1",
    "published_date": "2025-10-07 15:37:34 UTC",
    "updated_date": "2025-10-07 15:37:34 UTC"
  },
  {
    "arxiv_id": "2510.06040v1",
    "title": "VideoMiner: Iteratively Grounding Key Frames of Hour-Long Videos via Tree-based Group Relative Policy Optimization",
    "authors": [
      "Xinye Cao",
      "Hongcan Guo",
      "Jiawen Qian",
      "Guoshun Nan",
      "Chao Wang",
      "Yuqi Pan",
      "Tianhao Hou",
      "Xiaojuan Wang",
      "Yutong Gao"
    ],
    "abstract": "Understanding hour-long videos with multi-modal large language models (MM-LLMs) enriches the landscape of human-centered AI applications. However, for end-to-end video understanding with LLMs, uniformly sampling video frames results in LLMs being overwhelmed by a vast amount of irrelevant information as video length increases. Existing hierarchical key frame extraction methods improve the accuracy of video understanding but still face two critical challenges. 1) How can the interference of extensive redundant information in long videos be mitigated? 2) How can a model dynamically adapt to complex hierarchical structures while accurately identifying key frames? To address these issues, we propose VideoMiner, which iteratively segments, captions, and clusters long videos, forming a hierarchical tree structure. The proposed VideoMiner progresses from long videos to events to frames while preserving temporal coherence, effectively addressing the first challenge. To precisely locate key frames, we introduce T-GRPO, a tree-based group relative policy optimization in reinforcement learning method that guides the exploration of the VideoMiner. The proposed T-GRPO is specifically designed for tree structures, integrating spatiotemporal information at the event level while being guided by the question, thus solving the second challenge. We achieve superior performance in all long-video understanding tasks and uncover several interesting insights. Our proposed T-GRPO surprisingly incentivizes the model to spontaneously generate a reasoning chain. Additionally, the designed tree growth auxin dynamically adjusts the expansion depth, obtaining accuracy and efficiency gains. The code is publicly available at https://github.com/caoxinye/VideoMiner.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ICCV 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.06040v1",
    "published_date": "2025-10-07 15:34:46 UTC",
    "updated_date": "2025-10-07 15:34:46 UTC"
  },
  {
    "arxiv_id": "2510.06039v1",
    "title": "CDTP: A Large-Scale Chinese Data-Text Pair Dataset for Comprehensive Evaluation of Chinese LLMs",
    "authors": [
      "Chengwei Wu",
      "Jiapu Wang",
      "Mingyang Gao",
      "Xingrui Zhuo",
      "Jipeng Guo",
      "Runlin Lei",
      "Haoran Luo",
      "Tianyu Chen",
      "Haoyi Zhou",
      "Shirui Pan",
      "Zechao Li"
    ],
    "abstract": "Large Language Models (LLMs) have achieved remarkable success across a wide range of natural language processing tasks. However, Chinese LLMs face unique challenges, primarily due to the dominance of unstructured free text and the lack of structured representations in Chinese corpora. While existing benchmarks for LLMs partially assess Chinese LLMs, they are still predominantly English-centric and fail to address the unique linguistic characteristics of Chinese, lacking structured datasets essential for robust evaluation. To address these challenges, we present a Comprehensive Benchmark for Evaluating Chinese Large Language Models (CB-ECLLM) based on the newly constructed Chinese Data-Text Pair (CDTP) dataset. Specifically, CDTP comprises over 7 million aligned text pairs, each consisting of unstructured text coupled with one or more corresponding triples, alongside a total of 15 million triples spanning four critical domains. The core contributions of CDTP are threefold: (i) enriching Chinese corpora with high-quality structured information; (ii) enabling fine-grained evaluation tailored to knowledge-driven tasks; and (iii) supporting multi-task fine-tuning to assess generalization and robustness across scenarios, including Knowledge Graph Completion, Triple-to-Text generation, and Question Answering. Furthermore, we conduct rigorous evaluations through extensive experiments and ablation studies to assess the effectiveness, Supervised Fine-Tuning (SFT), and robustness of the benchmark. To support reproducible research, we offer an open-source codebase and outline potential directions for future investigations based on our insights.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06039v1",
    "published_date": "2025-10-07 15:33:52 UTC",
    "updated_date": "2025-10-07 15:33:52 UTC"
  },
  {
    "arxiv_id": "2510.06038v1",
    "title": "From Learning to Mastery: Achieving Safe and Efficient Real-World Autonomous Driving with Human-In-The-Loop Reinforcement Learning",
    "authors": [
      "Li Zeqiao",
      "Wang Yijing",
      "Wang Haoyu",
      "Li Zheng",
      "Li Peng",
      "Liu Wenfei",
      "Zuo Zhiqiang"
    ],
    "abstract": "Autonomous driving with reinforcement learning (RL) has significant potential. However, applying RL in real-world settings remains challenging due to the need for safe, efficient, and robust learning. Incorporating human expertise into the learning process can help overcome these challenges by reducing risky exploration and improving sample efficiency. In this work, we propose a reward-free, active human-in-the-loop learning method called Human-Guided Distributional Soft Actor-Critic (H-DSAC). Our method combines Proxy Value Propagation (PVP) and Distributional Soft Actor-Critic (DSAC) to enable efficient and safe training in real-world environments. The key innovation is the construction of a distributed proxy value function within the DSAC framework. This function encodes human intent by assigning higher expected returns to expert demonstrations and penalizing actions that require human intervention. By extrapolating these labels to unlabeled states, the policy is effectively guided toward expert-like behavior. With a well-designed state space, our method achieves real-world driving policy learning within practical training times. Results from both simulation and real-world experiments demonstrate that our framework enables safe, robust, and sample-efficient learning for autonomous driving.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06038v1",
    "published_date": "2025-10-07 15:33:29 UTC",
    "updated_date": "2025-10-07 15:33:29 UTC"
  },
  {
    "arxiv_id": "2510.06036v1",
    "title": "Refusal Falls off a Cliff: How Safety Alignment Fails in Reasoning?",
    "authors": [
      "Qingyu Yin",
      "Chak Tou Leong",
      "Linyi Yang",
      "Wenxuan Huang",
      "Wenjie Li",
      "Xiting Wang",
      "Jaehong Yoon",
      "YunXing",
      "XingYu",
      "Jinjin Gu"
    ],
    "abstract": "Large reasoning models (LRMs) with multi-step reasoning capabilities have shown remarkable problem-solving abilities, yet they exhibit concerning safety vulnerabilities that remain poorly understood. In this work, we investigate why safety alignment fails in reasoning models through a mechanistic interpretability lens. Using a linear probing approach to trace refusal intentions across token positions, we discover a striking phenomenon termed as \\textbf{refusal cliff}: many poorly-aligned reasoning models correctly identify harmful prompts and maintain strong refusal intentions during their thinking process, but experience a sharp drop in refusal scores at the final tokens before output generation. This suggests that these models are not inherently unsafe; rather, their refusal intentions are systematically suppressed. Through causal intervention analysis, we identify a sparse set of attention heads that negatively contribute to refusal behavior. Ablating just 3\\% of these heads can reduce attack success rates below 10\\%. Building on these mechanistic insights, we propose \\textbf{Cliff-as-a-Judge}, a novel data selection method that identifies training examples exhibiting the largest refusal cliff to efficiently repair reasoning models' safety alignment. This approach achieves comparable safety improvements using only 1.7\\% of the vanilla safety training data, demonstrating a less-is-more effect in safety alignment.",
    "categories": [
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06036v1",
    "published_date": "2025-10-07 15:32:59 UTC",
    "updated_date": "2025-10-07 15:32:59 UTC"
  },
  {
    "arxiv_id": "2510.06029v1",
    "title": "Fast Leave-One-Out Approximation from Fragment-Target Prevalence Vectors (molFTP) : From Dummy Masking to Key-LOO for Leakage-Free Feature Construction",
    "authors": [
      "Guillaume Godin"
    ],
    "abstract": "We introduce molFTP (molecular fragment-target prevalence), a compact representation that delivers strong predictive performance. To prevent feature leakage across cross-validation folds, we implement a dummy-masking procedure that removes information about fragments present in the held-out molecules. We further show that key leave-one-out (key-loo) closely approximates true molecule-level leave-one-out (LOO), with deviation below 8% on our datasets. This enables near full data training while preserving unbiased cross-validation estimates of model performance. Overall, molFTP provides a fast, leakage-resistant fragment-target prevalence vectorization with practical safeguards (dummy masking or key-LOO) that approximate LOO at a fraction of its cost.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "28 pages, 21 figures, 3 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.06029v1",
    "published_date": "2025-10-07 15:27:16 UTC",
    "updated_date": "2025-10-07 15:27:16 UTC"
  },
  {
    "arxiv_id": "2510.06026v1",
    "title": "Emergent AI Surveillance: Overlearned Person Re-Identification and Its Mitigation in Law Enforcement Context",
    "authors": [
      "An Thi Nguyen",
      "Radina Stoykova",
      "Eric Arazo"
    ],
    "abstract": "Generic instance search models can dramatically reduce the manual effort required to analyze vast surveillance footage during criminal investigations by retrieving specific objects of interest to law enforcement. However, our research reveals an unintended emergent capability: through overlearning, these models can single out specific individuals even when trained on datasets without human subjects. This capability raises concerns regarding identification and profiling of individuals based on their personal data, while there is currently no clear standard on how de-identification can be achieved. We evaluate two technical safeguards to curtail a model's person re-identification capacity: index exclusion and confusion loss. Our experiments demonstrate that combining these approaches can reduce person re-identification accuracy to below 2% while maintaining 82% of retrieval performance for non-person objects. However, we identify critical vulnerabilities in these mitigations, including potential circumvention using partial person images. These findings highlight urgent regulatory questions at the intersection of AI governance and data protection: How should we classify and regulate systems with emergent identification capabilities? And what technical standards should be required to prevent identification capabilities from developing in seemingly benign applications?",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, accepted to AIES 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.06026v1",
    "published_date": "2025-10-07 15:23:16 UTC",
    "updated_date": "2025-10-07 15:23:16 UTC"
  },
  {
    "arxiv_id": "2510.06014v1",
    "title": "ARISE: An Adaptive Resolution-Aware Metric for Test-Time Scaling Evaluation in Large Reasoning Models",
    "authors": [
      "Zhangyue Yin",
      "Qiushi Sun",
      "Zhiyuan Zeng",
      "Zhiyuan Yu",
      "Qipeng Guo",
      "Xuanjing Huang",
      "Xipeng Qiu"
    ],
    "abstract": "Test-time scaling has emerged as a transformative paradigm for enhancing the performance of large reasoning models, enabling dynamic allocation of computational resources during inference. However, as the landscape of reasoning models rapidly expands, a critical question remains: how can we systematically compare and evaluate the test-time scaling capabilities across different models? In this paper, we introduce ARISE (Adaptive Resolution-aware Scaling Evaluation), a novel metric specifically designed to assess the test-time scaling effectiveness of large reasoning models. Unlike existing evaluation approaches, ARISE incorporates two key innovations: (1) sample-level awareness that effectively penalizes negative scaling behaviors where increased computation leads to performance degradation, and (2) a dynamic sampling mechanism that mitigates the impact of accuracy fluctuations and token count instability on the final assessment. We conduct comprehensive experiments evaluating state-of-the-art reasoning models across diverse domains including mathematical reasoning, code generation, and agentic tasks. Our results demonstrate that ARISE provides a reliable and fine-grained measurement of test-time scaling capabilities, revealing significant variations in scaling efficiency across models. Notably, our evaluation identifies Claude Opus as exhibiting superior scaling characteristics compared to other contemporary reasoning models.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "19 pages, 7 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.06014v1",
    "published_date": "2025-10-07 15:10:51 UTC",
    "updated_date": "2025-10-07 15:10:51 UTC"
  },
  {
    "arxiv_id": "2510.06010v1",
    "title": "Hybrid Quantum-Classical Policy Gradient for Adaptive Control of Cyber-Physical Systems: A Comparative Study of VQC vs. MLP",
    "authors": [
      "Aueaphum Aueawatthanaphisut",
      "Nyi Wunna Tun"
    ],
    "abstract": "The comparative evaluation between classical and quantum reinforcement learning (QRL) paradigms was conducted to investigate their convergence behavior, robustness under observational noise, and computational efficiency in a benchmark control environment. The study employed a multilayer perceptron (MLP) agent as a classical baseline and a parameterized variational quantum circuit (VQC) as a quantum counterpart, both trained on the CartPole-v1 environment over 500 episodes. Empirical results demonstrated that the classical MLP achieved near-optimal policy convergence with a mean return of 498.7 +/- 3.2, maintaining stable equilibrium throughout training. In contrast, the VQC exhibited limited learning capability, with an average return of 14.6 +/- 4.8, primarily constrained by circuit depth and qubit connectivity. Noise robustness analysis further revealed that the MLP policy deteriorated gracefully under Gaussian perturbations, while the VQC displayed higher sensitivity at equivalent noise levels. Despite the lower asymptotic performance, the VQC exhibited significantly lower parameter count and marginally increased training time, highlighting its potential scalability for low-resource quantum processors. The results suggest that while classical neural policies remain dominant in current control benchmarks, quantum-enhanced architectures could offer promising efficiency advantages once hardware noise and expressivity limitations are mitigated.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG",
      "cs.RO",
      "eess.SY"
    ],
    "primary_category": "quant-ph",
    "comment": "6 pages, 5 figures, 2 tables, 17 equations, 1 algorithm",
    "pdf_url": "https://arxiv.org/pdf/2510.06010v1",
    "published_date": "2025-10-07 15:09:29 UTC",
    "updated_date": "2025-10-07 15:09:29 UTC"
  },
  {
    "arxiv_id": "2510.06008v1",
    "title": "Detection and Measurement of Hailstones with Multimodal Large Language Models",
    "authors": [
      "Moritz Alker",
      "David C. Schedl",
      "Andreas Stöckl"
    ],
    "abstract": "This study examines the use of social media and news images to detect and measure hailstones, utilizing pre-trained multimodal large language models. The dataset for this study comprises 474 crowdsourced images of hailstones from documented hail events in Austria, which occurred between January 2022 and September 2024. These hailstones have maximum diameters ranging from 2 to 11cm. We estimate the hail diameters and compare four different models utilizing one-stage and two-stage prompting strategies. The latter utilizes additional size cues from reference objects, such as human hands, within the image. Our results show that pretrained models already have the potential to measure hailstone diameters from images with an average mean absolute error of 1.12cm for the best model. In comparison to a single-stage prompt, two-stage prompting improves the reliability of most models. Our study suggests that these off-the-shelf models, even without fine-tuning, can complement traditional hail sensors by extracting meaningful and spatially dense information from social media imagery, enabling faster and more detailed assessments of severe weather events. The automated real-time image harvesting from social media and other sources remains an open task, but it will make our approach directly applicable to future hail events.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "6 pages, 5 figures, accepted at The 2nd International Conference on Electrical and Computer Engineering Researches",
    "pdf_url": "https://arxiv.org/pdf/2510.06008v1",
    "published_date": "2025-10-07 15:07:29 UTC",
    "updated_date": "2025-10-07 15:07:29 UTC"
  },
  {
    "arxiv_id": "2510.06002v2",
    "title": "Deterministic Legal Agents: A Canonical Primitive API for Auditable Reasoning over Temporal Knowledge Graphs",
    "authors": [
      "Hudson de Martim"
    ],
    "abstract": "For autonomous legal agents to operate safely in high-stakes domains, they require a foundation of absolute determinism and auditability-guarantees that standard Retrieval-Augmented Generation (RAG) frameworks cannot provide. When interacting with temporal knowledge graphs that model the complex evolution of legal norms, agents must navigate versioning, causality, and hierarchical structures with precision, a task for which black-box vector search is ill-suited. This paper introduces a new architectural pattern to solve this: a formal Primitive API designed as a secure execution layer for reasoning over such graphs. Instead of a monolithic query engine, our framework provides a library of canonical primitives-atomic, composable, and auditable primitives. This design empowers planner-guided agents to decompose complex legal questions into transparent execution plans, enabling critical tasks with full verifiability, including: (i) precise point-in-time version retrieval, (ii) robust causal lineage tracing, and (iii) context-aware hybrid search. Ultimately, this architecture transforms opaque retrieval into auditable reasoning, turning the agent's internal process from a black box into a verifiable log of deterministic primitives and providing a blueprint for building the next generation of trustworthy legal AI.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "Major revision reframing the paper from an API spec to a novel architectural pattern for deterministic agents. The core contribution is now positioned as a blueprint for auditable reasoning, essential for building trustworthy legal AI systems",
    "pdf_url": "https://arxiv.org/pdf/2510.06002v2",
    "published_date": "2025-10-07 15:04:23 UTC",
    "updated_date": "2025-11-04 16:44:53 UTC"
  },
  {
    "arxiv_id": "2510.05996v1",
    "title": "Information-Theoretic Policy Pre-Training with Empowerment",
    "authors": [
      "Moritz Schneider",
      "Robert Krug",
      "Narunas Vaskevicius",
      "Luigi Palmieri",
      "Michael Volpp",
      "Joschka Boedecker"
    ],
    "abstract": "Empowerment, an information-theoretic measure of an agent's potential influence on its environment, has emerged as a powerful intrinsic motivation and exploration framework for reinforcement learning (RL). Besides for unsupervised RL and skill learning algorithms, the specific use of empowerment as a pre-training signal has received limited attention in the literature. We show that empowerment can be used as a pre-training signal for data-efficient downstream task adaptation. For this we extend the traditional notion of empowerment by introducing discounted empowerment, which balances the agent's control over the environment across short- and long-term horizons. Leveraging this formulation, we propose a novel pre-training paradigm that initializes policies to maximize discounted empowerment, enabling agents to acquire a robust understanding of environmental dynamics. We analyze empowerment-based pre-training for various existing RL algorithms and empirically demonstrate its potential as a general-purpose initialization strategy: empowerment-maximizing policies with long horizons are data-efficient and effective, leading to improved adaptability in downstream tasks. Our findings pave the way for future research to scale this framework to high-dimensional and complex tasks, further advancing the field of RL.",
    "categories": [
      "cs.AI",
      "cs.IT",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05996v1",
    "published_date": "2025-10-07 14:57:58 UTC",
    "updated_date": "2025-10-07 14:57:58 UTC"
  },
  {
    "arxiv_id": "2510.08610v1",
    "title": "Relative Positioning Based Code Chunking Method For Rich Context Retrieval In Repository Level Code Completion Task With Code Language Model",
    "authors": [
      "Imranur Rahman",
      "Md Rayhanur Rahman"
    ],
    "abstract": "Code completion can help developers improve efficiency and ease the development lifecycle. Although code completion is available in modern integrated development environments (IDEs), research lacks in determining what makes a good context for code completion based on the information available to the IDEs for the large language models (LLMs) to perform better. In this paper, we describe an effective context collection strategy to assist the LLMs in performing better at code completion tasks. The key idea of our strategy is to preprocess the repository into smaller code chunks and later use syntactic and semantic similarity-based code chunk retrieval with relative positioning. We found that code chunking and relative positioning of the chunks in the final context improve the performance of code completion tasks.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted to Context Collection Workshop co-located with ASE 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.08610v1",
    "published_date": "2025-10-07 14:44:59 UTC",
    "updated_date": "2025-10-07 14:44:59 UTC"
  },
  {
    "arxiv_id": "2510.05984v1",
    "title": "ECTSpeech: Enhancing Efficient Speech Synthesis via Easy Consistency Tuning",
    "authors": [
      "Tao Zhu",
      "Yinfeng Yu",
      "Liejun Wang",
      "Fuchun Sun",
      "Wendong Zheng"
    ],
    "abstract": "Diffusion models have demonstrated remarkable performance in speech synthesis, but typically require multi-step sampling, resulting in low inference efficiency. Recent studies address this issue by distilling diffusion models into consistency models, enabling efficient one-step generation. However, these approaches introduce additional training costs and rely heavily on the performance of pre-trained teacher models. In this paper, we propose ECTSpeech, a simple and effective one-step speech synthesis framework that, for the first time, incorporates the Easy Consistency Tuning (ECT) strategy into speech synthesis. By progressively tightening consistency constraints on a pre-trained diffusion model, ECTSpeech achieves high-quality one-step generation while significantly reducing training complexity. In addition, we design a multi-scale gate module (MSGate) to enhance the denoiser's ability to fuse features at different scales. Experimental results on the LJSpeech dataset demonstrate that ECTSpeech achieves audio quality comparable to state-of-the-art methods under single-step sampling, while substantially reducing the model's training cost and complexity.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted for publication by Proceedings of the 2025 ACM Multimedia Asia Conference(MMAsia '25)",
    "pdf_url": "https://arxiv.org/pdf/2510.05984v1",
    "published_date": "2025-10-07 14:44:05 UTC",
    "updated_date": "2025-10-07 14:44:05 UTC"
  },
  {
    "arxiv_id": "2511.20652v1",
    "title": "When LLMs Can't Help: Real-World Evaluation of LLMs in Nutrition",
    "authors": [
      "Karen Jia-Hui Li",
      "Simone Balloccu",
      "Ondrej Dusek",
      "Ehud Reiter"
    ],
    "abstract": "The increasing trust in large language models (LLMs), especially in the form of chatbots, is often undermined by the lack of their extrinsic evaluation. This holds particularly true in nutrition, where randomised controlled trials (RCTs) are the gold standard, and experts demand them for evidence-based deployment. LLMs have shown promising results in this field, but these are limited to intrinsic setups. We address this gap by running the first RCT involving LLMs for nutrition. We augment a rule-based chatbot with two LLM-based features: (1) message rephrasing for conversational variety and engagement, and (2) nutritional counselling through a fine-tuned model. In our seven-week RCT (n=81), we compare chatbot variants with and without LLM integration. We measure effects on dietary outcome, emotional well-being, and engagement. Despite our LLM-based features performing well in intrinsic evaluation, we find that they did not yield consistent benefits in real-world deployment. These results highlight critical gaps between intrinsic evaluations and real-world impact, emphasising the need for interdisciplinary, human-centred approaches.\\footnote{We provide all of our code and results at: \\\\ \\href{https://github.com/saeshyra/diet-chatbot-trial}{https://github.com/saeshyra/diet-chatbot-trial}}",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "Published at INLG 2025 main conference",
    "pdf_url": "https://arxiv.org/pdf/2511.20652v1",
    "published_date": "2025-10-07 14:38:44 UTC",
    "updated_date": "2025-10-07 14:38:44 UTC"
  },
  {
    "arxiv_id": "2510.05976v1",
    "title": "Diffusion Models for Low-Light Image Enhancement: A Multi-Perspective Taxonomy and Performance Analysis",
    "authors": [
      "Eashan Adhikarla",
      "Yixin Liu",
      "Brian D. Davison"
    ],
    "abstract": "Low-light image enhancement (LLIE) is vital for safety-critical applications such as surveillance, autonomous navigation, and medical imaging, where visibility degradation can impair downstream task performance. Recently, diffusion models have emerged as a promising generative paradigm for LLIE due to their capacity to model complex image distributions via iterative denoising. This survey provides an up-to-date critical analysis of diffusion models for LLIE, distinctively featuring an in-depth comparative performance evaluation against Generative Adversarial Network and Transformer-based state-of-the-art methods, a thorough examination of practical deployment challenges, and a forward-looking perspective on the role of emerging paradigms like foundation models. We propose a multi-perspective taxonomy encompassing six categories: Intrinsic Decomposition, Spectral & Latent, Accelerated, Guided, Multimodal, and Autonomous; that map enhancement methods across physical priors, conditioning schemes, and computational efficiency. Our taxonomy is grounded in a hybrid view of both the model mechanism and the conditioning signals. We evaluate qualitative failure modes, benchmark inconsistencies, and trade-offs between interpretability, generalization, and inference efficiency. We also discuss real-world deployment constraints (e.g., memory, energy use) and ethical considerations. This survey aims to guide the next generation of diffusion-based LLIE research by highlighting trends and surfacing open research questions, including novel conditioning, real-time adaptation, and the potential of foundation models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05976v1",
    "published_date": "2025-10-07 14:30:36 UTC",
    "updated_date": "2025-10-07 14:30:36 UTC"
  },
  {
    "arxiv_id": "2510.06298v1",
    "title": "RGBD Gaze Tracking Using Transformer for Feature Fusion",
    "authors": [
      "Tobias J. Bauer"
    ],
    "abstract": "Subject of this thesis is the implementation of an AI-based Gaze Tracking system using RGBD images that contain both color (RGB) and depth (D) information. To fuse the features extracted from the images, a module based on the Transformer architecture is used. The combination of RGBD input images and Transformers was chosen because it has not yet been investigated. Furthermore, a new dataset is created for training the AI models as existing datasets either do not contain depth information or only contain labels for Gaze Point Estimation that are not suitable for the task of Gaze Angle Estimation. Various model configurations are trained, validated and evaluated on a total of three different datasets. The trained models are then to be used in a real-time pipeline to estimate the gaze direction and thus the gaze point of a person in front of a computer screen. The AI model architecture used in this thesis is based on an earlier work by Lian et al. It uses a Generative Adversarial Network (GAN) to simultaneously remove depth map artifacts and extract head pose features. Lian et al. achieve a mean Euclidean error of 38.7mm on their own dataset ShanghaiTechGaze+. In this thesis, a model architecture with a Transformer module for feature fusion achieves a mean Euclidean error of 55.3mm on the same dataset, but we show that using no pre-trained GAN module leads to a mean Euclidean error of 30.1mm. Replacing the Transformer module with a Multilayer Perceptron (MLP) improves the error to 26.9mm. These results are coherent with the ones on the other two datasets. On the ETH-XGaze dataset, the model with Transformer module achieves a mean angular error of 3.59° and without Transformer module 3.26°, whereas the fundamentally different model architecture used by the dataset authors Zhang et al. achieves a mean angular error of 2.04°. On the OTH-Gaze-Estimation dataset created for...",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Master Thesis with 125 pages, 59 figures, 17 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.06298v1",
    "published_date": "2025-10-07 14:28:31 UTC",
    "updated_date": "2025-10-07 14:28:31 UTC"
  },
  {
    "arxiv_id": "2510.05972v1",
    "title": "LexiCon: a Benchmark for Planning under Temporal Constraints in Natural Language",
    "authors": [
      "Periklis Mantenoglou",
      "Rishi Hazra",
      "Pedro Zuidberg Dos Martires",
      "Luc De Raedt"
    ],
    "abstract": "Owing to their reasoning capabilities, large language models (LLMs) have been evaluated on planning tasks described in natural language. However, LLMs have largely been tested on planning domains without constraints. In order to deploy them in real-world settings where adherence to constraints, in particular safety constraints, is critical, we need to evaluate their performance on constrained planning tasks. We introduce LexiCon -- a natural language-based (Lexi) constrained (Con) planning benchmark, consisting of a suite of environments, that can be used to evaluate the planning capabilities of LLMs in a principled fashion. The core idea behind LexiCon is to take existing planning environments and impose temporal constraints on the states. These constrained problems are then translated into natural language and given to an LLM to solve. A key feature of LexiCon is its extensibility. That is, the set of supported environments can be extended with new (unconstrained) environment generators, for which temporal constraints are constructed automatically. This renders LexiCon future-proof: the hardness of the generated planning problems can be increased as the planning capabilities of LLMs improve. Our experiments reveal that the performance of state-of-the-art LLMs, including reasoning models like GPT-5, o3, and R1, deteriorates as the degree of constrainedness of the planning tasks increases.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05972v1",
    "published_date": "2025-10-07 14:28:30 UTC",
    "updated_date": "2025-10-07 14:28:30 UTC"
  },
  {
    "arxiv_id": "2510.05969v2",
    "title": "Probing the Difficulty Perception Mechanism of Large Language Models",
    "authors": [
      "Sunbowen Lee",
      "Qingyu Yin",
      "Chak Tou Leong",
      "Jialiang Zhang",
      "Yicheng Gong",
      "Shiwen Ni",
      "Min Yang",
      "Xiaoyu Shen"
    ],
    "abstract": "Large language models (LLMs) are increasingly deployed on complex reasoning tasks, yet little is known about their ability to internally evaluate problem difficulty, which is an essential capability for adaptive reasoning and efficient resource allocation. In this work, we investigate whether LLMs implicitly encode problem difficulty in their internal representations. Using a linear probe on the final-token representations of LLMs, we demonstrate that the difficulty level of math problems can be linearly modeled. We further locate the specific attention heads of the final Transformer layer: these attention heads have opposite activation patterns for simple and difficult problems, thus achieving perception of difficulty. Our ablation experiments prove the accuracy of the location. Crucially, our experiments provide practical support for using LLMs as automatic difficulty annotators, potentially substantially reducing reliance on costly human labeling in benchmark construction and curriculum learning. We also uncover that there is a significant difference in entropy and difficulty perception at the token level. Our study reveals that difficulty perception in LLMs is not only present but also structurally organized, offering new theoretical insights and practical directions for future research. Our code is available at https://github.com/Aegis1863/Difficulty-Perception-of-LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05969v2",
    "published_date": "2025-10-07 14:24:32 UTC",
    "updated_date": "2025-10-12 07:13:37 UTC"
  },
  {
    "arxiv_id": "2510.05962v1",
    "title": "MatheMagic: Generating Dynamic Mathematics Benchmarks Robust to Memorization",
    "authors": [
      "Dayyán O'Brien",
      "Barry Haddow",
      "Emily Allaway",
      "Pinzhen Chen"
    ],
    "abstract": "Conducting contamination-free evaluation of mathematical capabilities can be difficult for two reasons: models may memorize a test set once it is made public, and current mathematical benchmarks are prone to overfitting due to having limited diversity of symbols and rules, coupled with closed-ended answers. This paper proposes a method to leverage these shortcomings as useful features to a construct dynamic, counterfactual benchmark, which can be used to both reveal overfitting and measure true reasoning. We demonstrate this via MatheMagic, which generates math test instances with the interpretations of numbers and operators altered, yet has automatically verifiable answers. Test instances are randomly seeded and constructed at test time to evaluate a model's induction or deduction capability, offering stability, extensibility, comparability, and robustness to overfitting. Our experiments find that models solve deduction more easily than induction, but they revert to standard math. Further analysis reveals that math-adapted models fail to exhibit a general \"skill\" of reasoning, and fine-tuning on induction tasks generalizes poorly.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05962v1",
    "published_date": "2025-10-07 14:19:21 UTC",
    "updated_date": "2025-10-07 14:19:21 UTC"
  },
  {
    "arxiv_id": "2510.08608v1",
    "title": "MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation",
    "authors": [
      "Weihua Zheng",
      "Zhengyuan Liu",
      "Tanmoy Chakraborty",
      "Weiwen Xu",
      "Xiaoxue Gao",
      "Bryan Chen Zhengyu Tan",
      "Bowei Zou",
      "Chang Liu",
      "Yujia Hu",
      "Xing Xie",
      "Xiaoyuan Yi",
      "Jing Yao",
      "Chaojun Wang",
      "Long Li",
      "Rui Liu",
      "Huiyao Liu",
      "Koji Inoue",
      "Ryuichi Sumida",
      "Tatsuya Kawahara",
      "Fan Xu",
      "Lingyu Ye",
      "Wei Tian",
      "Dongjun Kim",
      "Jimin Jung",
      "Jaehyung Seo",
      "Nadya Yuki Wangsajaya",
      "Pham Minh Duc",
      "Ojasva Saxena",
      "Palash Nandi",
      "Xiyan Tao",
      "Wiwik Karlina",
      "Tuan Luong",
      "Keertana Arun Vasan",
      "Roy Ka-Wei Lee",
      "Nancy F. Chen"
    ],
    "abstract": "Large language models (LLMs) are now used worldwide, yet their multimodal understanding and reasoning often degrade outside Western, high-resource settings. We propose MMA-ASIA, a comprehensive framework to evaluate LLMs' cultural awareness with a focus on Asian contexts. MMA-ASIA centers on a human-curated, multilingual, and multimodally aligned multiple-choice benchmark covering 8 Asian countries and 10 languages, comprising 27,000 questions; over 79 percent require multi-step reasoning grounded in cultural context, moving beyond simple memorization. To our knowledge, this is the first dataset aligned at the input level across three modalities: text, image (visual question answering), and speech. This enables direct tests of cross-modal transfer. Building on this benchmark, we propose a five-dimensional evaluation protocol that measures: (i) cultural-awareness disparities across countries, (ii) cross-lingual consistency, (iii) cross-modal consistency, (iv) cultural knowledge generalization, and (v) grounding validity. To ensure rigorous assessment, a Cultural Awareness Grounding Validation Module detects \"shortcut learning\" by checking whether the requisite cultural knowledge supports correct answers. Finally, through comparative model analysis, attention tracing, and an innovative Vision-ablated Prefix Replay (VPR) method, we probe why models diverge across languages and modalities, offering actionable insights for building culturally reliable multimodal LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08608v1",
    "published_date": "2025-10-07 14:12:12 UTC",
    "updated_date": "2025-10-07 14:12:12 UTC"
  },
  {
    "arxiv_id": "2510.05950v1",
    "title": "Training-Free Time Series Classification via In-Context Reasoning with LLM Agents",
    "authors": [
      "Songyuan Sui",
      "Zihang Xu",
      "Yu-Neng Chuang",
      "Kwei-Herng Lai",
      "Xia Hu"
    ],
    "abstract": "Time series classification (TSC) spans diverse application scenarios, yet labeled data are often scarce, making task-specific training costly and inflexible. Recent reasoning-oriented large language models (LLMs) show promise in understanding temporal patterns, but purely zero-shot usage remains suboptimal. We propose FETA, a multi-agent framework for training-free TSC via exemplar-based in-context reasoning. FETA decomposes a multivariate series into channel-wise subproblems, retrieves a few structurally similar labeled examples for each channel, and leverages a reasoning LLM to compare the query against these exemplars, producing channel-level labels with self-assessed confidences; a confidence-weighted aggregator then fuses all channel decisions. This design eliminates the need for pretraining or fine-tuning, improves efficiency by pruning irrelevant channels and controlling input length, and enhances interpretability through exemplar grounding and confidence estimation. On nine challenging UEA datasets, FETA achieves strong accuracy under a fully training-free setting, surpassing multiple trained baselines. These results demonstrate that a multi-agent in-context reasoning framework can transform LLMs into competitive, plug-and-play TSC solvers without any parameter training. The code is available at https://github.com/SongyuanSui/FETATSC.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages main content, 12 pages total including appendix, 1 figure",
    "pdf_url": "https://arxiv.org/pdf/2510.05950v1",
    "published_date": "2025-10-07 14:07:43 UTC",
    "updated_date": "2025-10-07 14:07:43 UTC"
  },
  {
    "arxiv_id": "2510.05949v1",
    "title": "Gaussian Embeddings: How JEPAs Secretly Learn Your Data Density",
    "authors": [
      "Randall Balestriero",
      "Nicolas Ballas",
      "Mike Rabbat",
      "Yann LeCun"
    ],
    "abstract": "Joint Embedding Predictive Architectures (JEPAs) learn representations able to solve numerous downstream tasks out-of-the-box. JEPAs combine two objectives: (i) a latent-space prediction term, i.e., the representation of a slightly perturbed sample must be predictable from the original sample's representation, and (ii) an anti-collapse term, i.e., not all samples should have the same representation. While (ii) is often considered as an obvious remedy to representation collapse, we uncover that JEPAs' anti-collapse term does much more--it provably estimates the data density. In short, any successfully trained JEPA can be used to get sample probabilities, e.g., for data curation, outlier detection, or simply for density estimation. Our theoretical finding is agnostic of the dataset and architecture used--in any case one can compute the learned probabilities of sample $x$ efficiently and in closed-form using the model's Jacobian matrix at $x$. Our findings are empirically validated across datasets (synthetic, controlled, and Imagenet) and across different Self Supervised Learning methods falling under the JEPA family (I-JEPA and DINOv2) and on multimodal models, such as MetaCLIP. We denote the method extracting the JEPA learned density as {\\bf JEPA-SCORE}.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05949v1",
    "published_date": "2025-10-07 14:06:30 UTC",
    "updated_date": "2025-10-07 14:06:30 UTC"
  },
  {
    "arxiv_id": "2510.05942v2",
    "title": "EvalMORAAL: Interpretable Chain-of-Thought and LLM-as-Judge Evaluation for Moral Alignment in Large Language Models",
    "authors": [
      "Hadi Mohammadi",
      "Anastasia Giachanou",
      "Ayoub Bagheri"
    ],
    "abstract": "We present EvalMORAAL, a transparent chain-of-thought (CoT) framework that uses two scoring methods (log-probabilities and direct ratings) plus a model-as-judge peer review to evaluate moral alignment in 20 large language models. We assess models on the World Values Survey (55 countries, 19 topics) and the PEW Global Attitudes Survey (39 countries, 8 topics). With EvalMORAAL, top models align closely with survey responses (Pearson's r approximately 0.90 on WVS). Yet we find a clear regional difference: Western regions average r=0.82 while non-Western regions average r=0.61 (a 0.21 absolute gap), indicating consistent regional bias. Our framework adds three parts: (1) two scoring methods for all models to enable fair comparison, (2) a structured chain-of-thought protocol with self-consistency checks, and (3) a model-as-judge peer review that flags 348 conflicts using a data-driven threshold. Peer agreement relates to survey alignment (WVS r=0.74, PEW r=0.39, both p<.001), supporting automated quality checks. These results show real progress toward culture-aware AI while highlighting open challenges for use across regions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05942v2",
    "published_date": "2025-10-07 13:52:16 UTC",
    "updated_date": "2025-10-08 08:03:38 UTC"
  },
  {
    "arxiv_id": "2510.05935v1",
    "title": "LLM-FS-Agent: A Deliberative Role-based Large Language Model Architecture for Transparent Feature Selection",
    "authors": [
      "Mohamed Bal-Ghaoui",
      "Fayssal Sabri"
    ],
    "abstract": "High-dimensional data remains a pervasive challenge in machine learning, often undermining model interpretability and computational efficiency. While Large Language Models (LLMs) have shown promise for dimensionality reduction through feature selection, existing LLM-based approaches frequently lack structured reasoning and transparent justification for their decisions. This paper introduces LLM-FS-Agent, a novel multi-agent architecture designed for interpretable and robust feature selection. The system orchestrates a deliberative \"debate\" among multiple LLM agents, each assigned a specific role, enabling collective evaluation of feature relevance and generation of detailed justifications. We evaluate LLM-FS-Agent in the cybersecurity domain using the CIC-DIAD 2024 IoT intrusion detection dataset and compare its performance against strong baselines, including LLM-Select and traditional methods such as PCA. Experimental results demonstrate that LLM-FS-Agent consistently achieves superior or comparable classification performance while reducing downstream training time by an average of 46% (statistically significant improvement, p = 0.028 for XGBoost). These findings highlight that the proposed deliberative architecture enhances both decision transparency and computational efficiency, establishing LLM-FS-Agent as a practical and reliable solution for real-world applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05935v1",
    "published_date": "2025-10-07 13:46:06 UTC",
    "updated_date": "2025-10-07 13:46:06 UTC"
  },
  {
    "arxiv_id": "2510.05930v1",
    "title": "Carré du champ flow matching: better quality-generalisation tradeoff in generative models",
    "authors": [
      "Jacob Bamberger",
      "Iolo Jones",
      "Dennis Duncan",
      "Michael M. Bronstein",
      "Pierre Vandergheynst",
      "Adam Gosztolai"
    ],
    "abstract": "Deep generative models often face a fundamental tradeoff: high sample quality can come at the cost of memorisation, where the model reproduces training data rather than generalising across the underlying data geometry. We introduce Carré du champ flow matching (CDC-FM), a generalisation of flow matching (FM), that improves the quality-generalisation tradeoff by regularising the probability path with a geometry-aware noise. Our method replaces the homogeneous, isotropic noise in FM with a spatially varying, anisotropic Gaussian noise whose covariance captures the local geometry of the latent data manifold. We prove that this geometric noise can be optimally estimated from the data and is scalable to large data. Further, we provide an extensive experimental evaluation on diverse datasets (synthetic manifolds, point clouds, single-cell genomics, animal motion capture, and images) as well as various neural network architectures (MLPs, CNNs, and transformers). We demonstrate that CDC-FM consistently offers a better quality-generalisation tradeoff. We observe significant improvements over standard FM in data-scarce regimes and in highly non-uniformly sampled datasets, which are often encountered in AI for science applications. Our work provides a mathematical framework for studying the interplay between data geometry, generalisation and memorisation in generative models, as well as a robust and scalable algorithm that can be readily integrated into existing flow matching pipelines.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.DG"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05930v1",
    "published_date": "2025-10-07 13:41:33 UTC",
    "updated_date": "2025-10-07 13:41:33 UTC"
  },
  {
    "arxiv_id": "2511.11581v1",
    "title": "The Anatomy of a Triton Attention Kernel",
    "authors": [
      "Burkhard Ringlein",
      "Jan van Lunteren",
      "Radu Stoica",
      "Thomas Parnell"
    ],
    "abstract": "A long-standing goal in both industry and academia is to develop an LLM inference platform that is portable across hardware architectures, eliminates the need for low-level hand-tuning, and still delivers best-in-class efficiency. In this work, we demonstrate that portable, efficient cross-platform LLM inference is indeed possible and share our experience. We develop a state-of-the-art paged attention kernel, the core performance-critical component of many LLM deployments, that builds exclusively on the domain-specific just-in-time compiled language Triton to achieve state-of-the-art performance on both NVIDIA and AMD GPUs. We describe our high-level approach, the key algorithmic and system-level improvements, the parameter auto-tuning required to unlock efficiency, and the integrations into a popular inference server that are necessary to bring the performance of a generic Triton attention kernel from 19.7% of the state-of-the-art to 105.9%. Our results highlight how open-source domain-specific languages can be leveraged to unlock model portability across different GPU vendors.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.DC",
      "cs.PL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.11581v1",
    "published_date": "2025-10-07 13:34:51 UTC",
    "updated_date": "2025-10-07 13:34:51 UTC"
  },
  {
    "arxiv_id": "2510.09658v2",
    "title": "Gradient-Sign Masking for Task Vector Transport Across Pre-Trained Models",
    "authors": [
      "Filippo Rinaldi",
      "Aniello Panariello",
      "Giacomo Salici",
      "Fengyuan Liu",
      "Marco Ciccone",
      "Angelo Porrello",
      "Simone Calderara"
    ],
    "abstract": "When a new release of a foundation model is published, practitioners typically need to repeat full fine-tuning, even if the same task has already been solved in the previous version. A promising alternative is to reuse the parameter changes (i.e., task vectors) that capture how a model adapts to a specific task. However, they often fail to transfer across different pre-trained models due to their misaligned parameter space. In this work, we show that the key to successful transfer lies in the sign structure of the gradients of the new model. Based on this insight, we propose GradFix, a novel method that approximates the ideal gradient sign structure and leverages it to transfer knowledge using only a handful of labeled samples. Notably, this requires no additional fine-tuning: the adaptation is achieved by computing a few gradients at the target model and masking the source task vector accordingly. This yields an update that is locally aligned with the target loss landscape, effectively rebasing the task vector onto the new pre-training. We provide a theoretical guarantee that our method ensures first-order descent. Empirically, we demonstrate significant performance gains on vision and language benchmarks, consistently outperforming naive task vector addition and few-shot fine-tuning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09658v2",
    "published_date": "2025-10-07 13:30:25 UTC",
    "updated_date": "2025-10-16 16:13:33 UTC"
  },
  {
    "arxiv_id": "2510.05919v1",
    "title": "An Attention-Augmented VAE-BiLSTM Framework for Anomaly Detection in 12-Lead ECG Signals",
    "authors": [
      "Marc Garreta Basora",
      "Mehmet Oguz Mulayim"
    ],
    "abstract": "Anomaly detection in 12-lead electrocardiograms (ECGs) is critical for identifying deviations associated with cardiovascular disease. This work presents a comparative analysis of three autoencoder-based architectures: convolutional autoencoder (CAE), variational autoencoder with bidirectional long short-term memory (VAE-BiLSTM), and VAE-BiLSTM with multi-head attention (VAE-BiLSTM-MHA), for unsupervised anomaly detection in ECGs. To the best of our knowledge, this study reports the first application of a VAE-BiLSTM-MHA architecture to ECG anomaly detection. All models are trained on normal ECG samples to reconstruct non-anomalous cardiac morphology and detect deviations indicative of disease. Using a unified preprocessing and evaluation pipeline on the public China Physiological Signal Challenge (CPSC) dataset, the attention-augmented VAE achieves the best performance, with an AUPRC of 0.81 and a recall of 0.85 on the held-out test set, outperforming the other architectures. To support clinical triage, this model is further integrated into an interactive dashboard that visualizes anomaly localization. In addition, a performance comparison with baseline models from the literature is provided.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages, 11 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.05919v1",
    "published_date": "2025-10-07 13:30:02 UTC",
    "updated_date": "2025-10-07 13:30:02 UTC"
  },
  {
    "arxiv_id": "2510.05909v2",
    "title": "Optimizing for Persuasion Improves LLM Generalization: Evidence from Quality-Diversity Evolution of Debate Strategies",
    "authors": [
      "Aksel Joonas Reedi",
      "Corentin Léger",
      "Julien Pourcel",
      "Loris Gaven",
      "Perrine Charriau",
      "Guillaume Pourcel"
    ],
    "abstract": "Large Language Models (LLMs) optimized to output truthful answers often overfit, producing brittle reasoning that fails to generalize. While persuasion-based optimization has shown promise in debate settings, it has not been systematically compared against mainstream truth-based approaches. We introduce DebateQD, a minimal Quality-Diversity (QD) evolutionary algorithm that evolves diverse debate strategies across different categories (rationality, authority, emotional appeal, etc.) through tournament-style competitions where two LLMs debate while a third judges. Unlike previously proposed methods that require a population of LLMs, our approach maintains diversity of opponents through prompt-based strategies within a single LLM architecture, making it more accessible for experiments while preserving the key benefits of population-based optimization. In contrast to prior work, we explicitly isolate the role of the optimization objective by fixing the debate protocol and swapping only the fitness function: persuasion rewards strategies that convince the judge irrespective of truth, whereas truth rewards collaborative correctness. Across three model scales (7B, 32B, 72B parameters) and multiple dataset sizes from the QuALITY benchmark, persuasion-optimized strategies achieve up to 13.94% smaller train-test generalization gaps, while matching or exceeding truth optimization's test performance. These results provide the first controlled evidence that competitive pressure to persuade, rather than seek the truth collaboratively, fosters more transferable reasoning skills, offering a promising path for improving LLM generalization.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Open-source code available at https://github.com/flowersteam/llm_persuasion",
    "pdf_url": "https://arxiv.org/pdf/2510.05909v2",
    "published_date": "2025-10-07 13:20:51 UTC",
    "updated_date": "2025-10-18 14:06:55 UTC"
  },
  {
    "arxiv_id": "2510.06296v1",
    "title": "VeriEquivBench: An Equivalence Score for Ground-Truth-Free Evaluation of Formally Verifiable Code",
    "authors": [
      "Lingfei Zeng",
      "Fengdi Che",
      "Xuhan Huang",
      "Fei Ye",
      "Xu Xu",
      "Binhang Yuan",
      "Jie Fu"
    ],
    "abstract": "Formal verification is the next frontier for ensuring the correctness of code generated by Large Language Models (LLMs). While methods that co-generate code and formal specifications in formal languages, like Dafny, can, in principle, prove alignment with user intent, progress is bottlenecked by specification quality evaluation. Current benchmarks rely on matching against ground-truth specifications, a manual and expertise-intensive process that has limited existing datasets to a few hundred simple problems and also suffers from a reliability issue. To address this, we introduce VeriEquivBench, a new benchmark with $2,389$ complex algorithmic problems that probe the limitations of current models in both code generation and formal reasoning. Our evaluation framework replaces ground-truth matching with a formally grounded metric, the equivalence score, and rigorously verifies the quality of generated specifications and code. Our results show that generating formally verifiable code remains a profound challenge for state-of-the-art LLMs. This underscores both the difficulty of the task and the need for benchmarks like VeriEquivBench to drive progress toward scalable and reliable coding agents.",
    "categories": [
      "cs.PL",
      "cs.AI"
    ],
    "primary_category": "cs.PL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06296v1",
    "published_date": "2025-10-07 13:19:05 UTC",
    "updated_date": "2025-10-07 13:19:05 UTC"
  },
  {
    "arxiv_id": "2510.05903v1",
    "title": "Kaputt: A Large-Scale Dataset for Visual Defect Detection",
    "authors": [
      "Sebastian Höfer",
      "Dorian Henning",
      "Artemij Amiranashvili",
      "Douglas Morrison",
      "Mariliza Tzes",
      "Ingmar Posner",
      "Marc Matvienko",
      "Alessandro Rennola",
      "Anton Milan"
    ],
    "abstract": "We present a novel large-scale dataset for defect detection in a logistics setting. Recent work on industrial anomaly detection has primarily focused on manufacturing scenarios with highly controlled poses and a limited number of object categories. Existing benchmarks like MVTec-AD [6] and VisA [33] have reached saturation, with state-of-the-art methods achieving up to 99.9% AUROC scores. In contrast to manufacturing, anomaly detection in retail logistics faces new challenges, particularly in the diversity and variability of object pose and appearance. Leading anomaly detection methods fall short when applied to this new setting. To bridge this gap, we introduce a new benchmark that overcomes the current limitations of existing datasets. With over 230,000 images (and more than 29,000 defective instances), it is 40 times larger than MVTec-AD and contains more than 48,000 distinct objects. To validate the difficulty of the problem, we conduct an extensive evaluation of multiple state-of-the-art anomaly detection methods, demonstrating that they do not surpass 56.96% AUROC on our dataset. Further qualitative analysis confirms that existing methods struggle to leverage normal samples under heavy pose and appearance variation. With our large-scale dataset, we set a new benchmark and encourage future research towards solving this challenging problem in retail logistics anomaly detection. The dataset is available for download under https://www.kaputt-dataset.com.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to ICCV 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.05903v1",
    "published_date": "2025-10-07 13:13:18 UTC",
    "updated_date": "2025-10-07 13:13:18 UTC"
  },
  {
    "arxiv_id": "2510.05901v2",
    "title": "Untangling Component Imbalance in Hybrid Linear Attention Conversion Methods",
    "authors": [
      "Martin Benfeghoul",
      "Teresa Delgado",
      "Adnan Oomerjee",
      "Haitham Bou Ammar",
      "Jun Wang",
      "Zafeirios Fountas"
    ],
    "abstract": "Transformers' quadratic computational complexity limits their scalability despite remarkable performance. While linear attention reduces this to linear complexity, pre-training such models from scratch remains, in most cases, prohibitively expensive. Recent post-training linearisation methods convert pre-trained Transformers to linear models efficiently, often using hybrid approaches that combine linear attention with sliding-window softmax. We identify a critical flaw: existing hybrid methods inadvertently bypass the linear component, relying almost entirely on SWA. Component-level diagnostics reveal this previously undetected behaviour stems from overlooked evaluation practices on common-sense benchmarks. We propose three solutions to ensure balanced component usage: (i) inference-time hybridisation of linear-only conversions with sliding-window softmax; (ii) HedgeCATs, combining attention-weight transfer with targeted LoRA fine-tuning; and (iii) Scheduled Sliding-window Dropout (SSD), which stochastically suppresses the softmax branch during training to prevent component collapse. Our methods maintain computational efficiency while recovering most base model performance and ensuring genuine linear attention adoption, restoring the validity of performance attributions in hybrid conversions.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05901v2",
    "published_date": "2025-10-07 13:11:13 UTC",
    "updated_date": "2025-10-10 17:42:09 UTC"
  },
  {
    "arxiv_id": "2510.05891v1",
    "title": "$\\bf{D^3}$QE: Learning Discrete Distribution Discrepancy-aware Quantization Error for Autoregressive-Generated Image Detection",
    "authors": [
      "Yanran Zhang",
      "Bingyao Yu",
      "Yu Zheng",
      "Wenzhao Zheng",
      "Yueqi Duan",
      "Lei Chen",
      "Jie Zhou",
      "Jiwen Lu"
    ],
    "abstract": "The emergence of visual autoregressive (AR) models has revolutionized image generation while presenting new challenges for synthetic image detection. Unlike previous GAN or diffusion-based methods, AR models generate images through discrete token prediction, exhibiting both marked improvements in image synthesis quality and unique characteristics in their vector-quantized representations. In this paper, we propose to leverage Discrete Distribution Discrepancy-aware Quantization Error (D$^3$QE) for autoregressive-generated image detection that exploits the distinctive patterns and the frequency distribution bias of the codebook existing in real and fake images. We introduce a discrete distribution discrepancy-aware transformer that integrates dynamic codebook frequency statistics into its attention mechanism, fusing semantic features and quantization error latent. To evaluate our method, we construct a comprehensive dataset termed ARForensics covering 7 mainstream visual AR models. Experiments demonstrate superior detection accuracy and strong generalization of D$^3$QE across different AR models, with robustness to real-world perturbations. Code is available at \\href{https://github.com/Zhangyr2022/D3QE}{https://github.com/Zhangyr2022/D3QE}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 5 figures, published to ICCV2025",
    "pdf_url": "https://arxiv.org/pdf/2510.05891v1",
    "published_date": "2025-10-07 13:02:27 UTC",
    "updated_date": "2025-10-07 13:02:27 UTC"
  },
  {
    "arxiv_id": "2510.05881v1",
    "title": "Segment-Factorized Full-Song Generation on Symbolic Piano Music",
    "authors": [
      "Ping-Yi Chen",
      "Chih-Pin Tan",
      "Yi-Hsuan Yang"
    ],
    "abstract": "We propose the Segmented Full-Song Model (SFS) for symbolic full-song generation. The model accepts a user-provided song structure and an optional short seed segment that anchors the main idea around which the song is developed. By factorizing a song into segments and generating each one through selective attention to related segments, the model achieves higher quality and efficiency compared to prior work. To demonstrate its suitability for human-AI interaction, we further wrap SFS into a web application that enables users to iteratively co-create music on a piano roll with customizable structures and flexible ordering.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "cs.MM",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted to the 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: AI for Music",
    "pdf_url": "https://arxiv.org/pdf/2510.05881v1",
    "published_date": "2025-10-07 12:54:44 UTC",
    "updated_date": "2025-10-07 12:54:44 UTC"
  },
  {
    "arxiv_id": "2510.05871v1",
    "title": "Towards Label-Free Biological Reasoning Synthetic Dataset Creation via Uncertainty Filtering",
    "authors": [
      "Josefa Lia Stoisser",
      "Lawrence Phillips",
      "Aditya Misra",
      "Tom A. Lamb",
      "Philip Torr",
      "Marc Boubnovski Martell",
      "Julien Fauqueur",
      "Kaspar Märtens"
    ],
    "abstract": "Synthetic chain-of-thought (CoT) traces are widely used to train large reasoning models (LRMs), improving generalization by providing step-level supervision. Yet most approaches require ground-truth labels to seed or filter these traces - an expensive bottleneck in domains like biology where wet-lab data are scarce. We propose a label-free alternative: uncertainty-based filtering, which uses a model's own confidence - quantified through established uncertainty metrics like self-consistency and predictive perplexity - as a substitute for external labels. We sample multiple reasoning traces and retain only low-uncertainty subsets. Applied to biological perturbation prediction, a domain where wet-lab labels are especially costly, we show that the filtered subset has higher accuracy, and that supervised fine-tuning (SFT) on uncertainty-filtered data outperforms unfiltered synthetic data, narrows the gap to ground-truth training, and surpasses strong LRM baselines. Ablations show that per-class filtering corrects for class-specific uncertainty scales and that hybrid uncertainty metrics yield higher-quality datasets. Our results suggest that model-internal confidence is a powerful signal for efficient reasoning dataset creation, enabling LRMs in domains where supervision is expensive.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05871v1",
    "published_date": "2025-10-07 12:40:37 UTC",
    "updated_date": "2025-10-07 12:40:37 UTC"
  },
  {
    "arxiv_id": "2510.05865v1",
    "title": "The Safety Challenge of World Models for Embodied AI Agents: A Review",
    "authors": [
      "Lorenzo Baraldi",
      "Zifan Zeng",
      "Chongzhe Zhang",
      "Aradhana Nayak",
      "Hongbo Zhu",
      "Feng Liu",
      "Qunli Zhang",
      "Peng Wang",
      "Shiming Liu",
      "Zheng Hu",
      "Angelo Cangelosi",
      "Lorenzo Baraldi"
    ],
    "abstract": "The rapid progress in embodied artificial intelligence has highlighted the necessity for more advanced and integrated models that can perceive, interpret, and predict environmental dynamics. In this context, World Models (WMs) have been introduced to provide embodied agents with the abilities to anticipate future environmental states and fill in knowledge gaps, thereby enhancing agents' ability to plan and execute actions. However, when dealing with embodied agents it is fundamental to ensure that predictions are safe for both the agent and the environment. In this article, we conduct a comprehensive literature review of World Models in the domains of autonomous driving and robotics, with a specific focus on the safety implications of scene and control generation tasks. Our review is complemented by an empirical analysis, wherein we collect and examine predictions from state-of-the-art models, identify and categorize common faults (herein referred to as pathologies), and provide a quantitative evaluation of the results.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05865v1",
    "published_date": "2025-10-07 12:35:09 UTC",
    "updated_date": "2025-10-07 12:35:09 UTC"
  },
  {
    "arxiv_id": "2510.05862v2",
    "title": "Revisiting Long-context Modeling from Context Denoising Perspective",
    "authors": [
      "Zecheng Tang",
      "Baibei Ji",
      "Juntao Li",
      "Lijun Wu",
      "Haijia Gui",
      "Min Zhang"
    ],
    "abstract": "Long-context models (LCMs) have demonstrated great potential in processing long sequences, facilitating many real-world applications. The success of LCMs can be attributed to their ability to locate implicit critical information within the context for further prediction. However, recent research reveals that LCMs are often susceptible to contextual noise, i.e., irrelevant tokens, that can mislead model attention. In this paper, we conduct a fine-grained analysis of the context noise and propose an effective metric, the Integrated Gradient (IG) score, to detect and quantify the noise information within the context. Our findings reveal that even simple mitigation of detected context noise can substantially boost the model's attention on critical tokens and benefit subsequent predictions. Building on this insight, we propose Context Denoising Training (CDT), a straightforward yet effective training strategy that improves attention on critical tokens while reinforcing their influence on model predictions. Extensive experiments across four tasks, under both context window scaling and long-context alignment settings, demonstrate the superiority of CDT. Notably, when trained with CDT, an open-source 8B model can achieve performance (50.92) comparable to GPT-4o (51.00).",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05862v2",
    "published_date": "2025-10-07 12:32:23 UTC",
    "updated_date": "2025-11-04 05:18:48 UTC"
  },
  {
    "arxiv_id": "2511.17515v1",
    "title": "Embedding Generative AI into Systems Analysis and Design Curriculum: Framework, Case Study, and Cross-Campus Empirical Evidence",
    "authors": [
      "Mahmoud Elkhodr",
      "Ergun Gide"
    ],
    "abstract": "Systems analysis students increasingly use Generative AI, yet current pedagogy lacks systematic approaches for teaching responsible AI orchestration that fosters critical thinking whilst meeting educational outcomes. Students risk accepting AI suggestions blindly or uncritically without assessing alignment with user needs or contextual appropriateness. SAGE (Structured AI-Guided Education) addresses this gap by embedding GenAI into curriculum design, training students when to accept, modify, or reject AI contributions. Implementation with 18 student groups across four Australian universities revealed how orchestration skills develop. Most groups (84\\%) moved beyond passive acceptance, showing selective judgment, yet none proactively identified gaps overlooked by both human and AI analysis, indicating a competency ceiling. Students strong at explaining decisions also performed well at integrating sources, and those with deep domain understanding consistently considered accessibility considerations. Accessibility awareness proved fragile. When writing requirements, 85\\% of groups explicitly considered elderly users and cultural needs. Notably, 55\\% of groups struggled identifying when AI misclassified system boundaries (what belongs inside versus outside the system), 45\\% missed data management errors (how information is stored and updated), and 55\\% overlooked missing exception handling. Three implications emerge for educators: (i) require students to document why they accepted, modified, or rejected each AI suggestion, making reasoning explicit; (ii) embed accessibility prompts at each development stage because awareness collapses without continuous scaffolding; and (iii) have students create their own specifications before using AI, then compare versions, and anchor to research or standards to identify gaps.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "~12,000 words; 4 figures; 6 tables; multi-site study (across 4 Australian campuses)",
    "pdf_url": "https://arxiv.org/pdf/2511.17515v1",
    "published_date": "2025-10-07 12:31:15 UTC",
    "updated_date": "2025-10-07 12:31:15 UTC"
  },
  {
    "arxiv_id": "2510.05858v3",
    "title": "DACP: Domain-Adaptive Continual Pre-Training of Large Language Models for Phone Conversation Summarization",
    "authors": [
      "Xue-Yong Fu",
      "Elena Khasanova",
      "Md Tahmid Rahman Laskar",
      "Harsh Saini",
      "Shashi Bhushan TN"
    ],
    "abstract": "Large language models (LLMs) have achieved impressive performance in text summarization, yet their performance often falls short when applied to specialized domains that differ from their original pre-training distribution. While fine-tuning can improve summarization quality, it typically relies on costly and scarce high-quality labeled data. In this work, we explore continual pre-training as a scalable, self-supervised approach to adapt LLMs for downstream summarization tasks, particularly in the context of noisy real-world conversation transcripts. We conduct extensive experiments using large-scale, unlabeled business conversation data to investigate whether continual pre-training enhances model capabilities in conversational summarization. Our results demonstrate that continual pre-training yields substantial gains in both in-domain and out-of-domain summarization benchmarks, while maintaining strong generalization and robustness. We also analyze the effects of data selection strategies, providing practical guidelines for applying continual pre-training in summarization-focused industrial applications.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to the NewSumm Workshop at EMNLP 2025. Equal contribution from the first four authors",
    "pdf_url": "https://arxiv.org/pdf/2510.05858v3",
    "published_date": "2025-10-07 12:26:19 UTC",
    "updated_date": "2025-10-09 12:35:24 UTC"
  },
  {
    "arxiv_id": "2510.09657v1",
    "title": "Generative Models for Helmholtz Equation Solutions: A Dataset of Acoustic Materials",
    "authors": [
      "Riccardo Fosco Gramaccioni",
      "Christian Marinoni",
      "Fabrizio Frezza",
      "Aurelio Uncini",
      "Danilo Comminiello"
    ],
    "abstract": "Accurate simulation of wave propagation in complex acoustic materials is crucial for applications in sound design, noise control, and material engineering. Traditional numerical solvers, such as finite element methods, are computationally expensive, especially when dealing with large-scale or real-time scenarios. In this work, we introduce a dataset of 31,000 acoustic materials, named HA30K, designed and simulated solving the Helmholtz equations. For each material, we provide the geometric configuration and the corresponding pressure field solution, enabling data-driven approaches to learn Helmholtz equation solutions. As a baseline, we explore a deep learning approach based on Stable Diffusion with ControlNet, a state-of-the-art model for image generation. Unlike classical solvers, our approach leverages GPU parallelization to process multiple simulations simultaneously, drastically reducing computation time. By representing solutions as images, we bypass the need for complex simulation software and explicit equation-solving. Additionally, the number of diffusion steps can be adjusted at inference time, balancing speed and quality. We aim to demonstrate that deep learning-based methods are particularly useful in early-stage research, where rapid exploration is more critical than absolute accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP",
      "math.NA"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at EUSIPCO 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.09657v1",
    "published_date": "2025-10-07 12:16:12 UTC",
    "updated_date": "2025-10-07 12:16:12 UTC"
  },
  {
    "arxiv_id": "2510.06295v1",
    "title": "Efficient High-Resolution Image Editing with Hallucination-Aware Loss and Adaptive Tiling",
    "authors": [
      "Young D. Kwon",
      "Abhinav Mehrotra",
      "Malcolm Chadwick",
      "Alberto Gil Ramos",
      "Sourav Bhattacharya"
    ],
    "abstract": "High-resolution (4K) image-to-image synthesis has become increasingly important for mobile applications. Existing diffusion models for image editing face significant challenges, in terms of memory and image quality, when deployed on resource-constrained devices. In this paper, we present MobilePicasso, a novel system that enables efficient image editing at high resolutions, while minimising computational cost and memory usage. MobilePicasso comprises three stages: (i) performing image editing at a standard resolution with hallucination-aware loss, (ii) applying latent projection to overcome going to the pixel space, and (iii) upscaling the edited image latent to a higher resolution with adaptive context-preserving tiling. Our user study with 46 participants reveals that MobilePicasso not only improves image quality by 18-48% but reduces hallucinations by 14-51% over existing methods. MobilePicasso demonstrates significantly lower latency, e.g., up to 55.8$\\times$ speed-up, yet with a small increase in runtime memory, e.g., a mere 9% increase over prior work. Surprisingly, the on-device runtime of MobilePicasso is observed to be faster than a server-based high-resolution image editing model running on an A100 GPU.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Preprint. Under review",
    "pdf_url": "https://arxiv.org/pdf/2510.06295v1",
    "published_date": "2025-10-07 12:09:24 UTC",
    "updated_date": "2025-10-07 12:09:24 UTC"
  },
  {
    "arxiv_id": "2510.06293v2",
    "title": "BlockGPT: Spatio-Temporal Modelling of Rainfall via Frame-Level Autoregression",
    "authors": [
      "Cristian Meo",
      "Varun Sarathchandran",
      "Avijit Majhi",
      "Shao Hung",
      "Carlo Saccardi",
      "Ruben Imhoff",
      "Roberto Deidda",
      "Remko Uijlenhoet",
      "Justin Dauwels"
    ],
    "abstract": "Predicting precipitation maps is a highly complex spatiotemporal modeling task, critical for mitigating the impacts of extreme weather events. Short-term precipitation forecasting, or nowcasting, requires models that are not only accurate but also computationally efficient for real-time applications. Current methods, such as token-based autoregressive models, often suffer from flawed inductive biases and slow inference, while diffusion models can be computationally intensive. To address these limitations, we introduce BlockGPT, a generative autoregressive transformer using batched tokenization (Block) method that predicts full two-dimensional fields (frames) at each time step. Conceived as a model-agnostic paradigm for video prediction, BlockGPT factorizes space-time by using self-attention within each frame and causal attention across frames; in this work, we instantiate it for precipitation nowcasting. We evaluate BlockGPT on two precipitation datasets, viz. KNMI (Netherlands) and SEVIR (U.S.), comparing it to state-of-the-art baselines including token-based (NowcastingGPT) and diffusion-based (DiffCast+Phydnet) models. The results show that BlockGPT achieves superior accuracy, event localization as measured by categorical metrics, and inference speeds up to 31x faster than comparable baselines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06293v2",
    "published_date": "2025-10-07 11:52:32 UTC",
    "updated_date": "2025-10-22 15:14:05 UTC"
  },
  {
    "arxiv_id": "2510.05827v1",
    "title": "VCoT-Grasp: Grasp Foundation Models with Visual Chain-of-Thought Reasoning for Language-driven Grasp Generation",
    "authors": [
      "Haoran Zhang",
      "Shuanghao Bai",
      "Wanqi Zhou",
      "Yuedi Zhang",
      "Qi Zhang",
      "Pengxiang Ding",
      "Cheng Chi",
      "Donglin Wang",
      "Badong Chen"
    ],
    "abstract": "Robotic grasping is one of the most fundamental tasks in robotic manipulation, and grasp detection/generation has long been the subject of extensive research. Recently, language-driven grasp generation has emerged as a promising direction due to its practical interaction capabilities. However, most existing approaches either lack sufficient reasoning and generalization capabilities or depend on complex modular pipelines. Moreover, current grasp foundation models tend to overemphasize dialog and object semantics, resulting in inferior performance and restriction to single-object grasping. To maintain strong reasoning ability and generalization in cluttered environments, we propose VCoT-Grasp, an end-to-end grasp foundation model that incorporates visual chain-of-thought reasoning to enhance visual understanding for grasp generation. VCoT-Grasp adopts a multi-turn processing paradigm that dynamically focuses on visual inputs while providing interpretable reasoning traces. For training, we refine and introduce a large-scale dataset, VCoT-GraspSet, comprising 167K synthetic images with over 1.36M grasps, as well as 400+ real-world images with more than 1.2K grasps, annotated with intermediate bounding boxes. Extensive experiments on both VCoT-GraspSet and real robot demonstrate that our method significantly improves grasp success rates and generalizes effectively to unseen objects, backgrounds, and distractors. More details can be found at https://zhanghr2001.github.io/VCoT-Grasp.github.io.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05827v1",
    "published_date": "2025-10-07 11:50:26 UTC",
    "updated_date": "2025-10-07 11:50:26 UTC"
  },
  {
    "arxiv_id": "2510.05825v1",
    "title": "Mitigating Premature Exploitation in Particle-based Monte Carlo for Inference-Time Scaling",
    "authors": [
      "Giorgio Giannone",
      "Guangxuan Xu",
      "Nikhil Shivakumar Nayak",
      "Rohan Mahesh Awhad",
      "Shivchander Sudalairaj",
      "Kai Xu",
      "Akash Srivastava"
    ],
    "abstract": "Inference-Time Scaling (ITS) improves language models by allocating more computation at generation time. Particle Filtering (PF) has emerged as a strong ITS method for complex mathematical reasoning tasks, but it is vulnerable when guided by process reward models, which often assign overconfident scores early in the reasoning process. This causes PF to suffer from premature exploitation: it myopically commits to locally promising trajectories, prunes potentially correct hypotheses, and converges to suboptimal solutions. This failure mode, known as particle impoverishment, is especially severe under constrained computational budgets. To address this, we analyze the problem and identify two root causes: a lack of diversity in the particle set due to overconfident resampling and consequent inability to assess the potential of a reasoning path. We introduce Entropic Particle Filtering (ePF), an algorithm that integrates two new techniques to solve these issues. The first technique, Entropic Annealing (EA), directly mitigates particle impoverishment by monitoring search diversity via entropy; when diversity drops, it intervenes by dynamically annealing the resampling distribution to preserve exploration. The second, an enhancement called Look-ahead Modulation (LaM), adds a predictive guide to evaluate a state's potential based on its successors. On several challenging math benchmarks, ePF significantly outperforms strong baselines and achieves up to a 50 % relative improvement in task reward. Together, these methods improve PF's resilience by balancing the exploration of diverse solution spaces with the exploitation of high-reward regions, ultimately leading to higher-quality solutions.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05825v1",
    "published_date": "2025-10-07 11:48:32 UTC",
    "updated_date": "2025-10-07 11:48:32 UTC"
  },
  {
    "arxiv_id": "2510.05819v1",
    "title": "Deformable Image Registration for Self-supervised Cardiac Phase Detection in Multi-View Multi-Disease Cardiac Magnetic Resonance Images",
    "authors": [
      "Sven Koehler",
      "Sarah Kaye Mueller",
      "Jonathan Kiekenap",
      "Gerald Greil",
      "Tarique Hussain",
      "Samir Sarikouch",
      "Florian André",
      "Norbert Frey",
      "Sandy Engelhardt"
    ],
    "abstract": "Cardiovascular magnetic resonance (CMR) is the gold standard for assessing cardiac function, but individual cardiac cycles complicate automatic temporal comparison or sub-phase analysis. Accurate cardiac keyframe detection can eliminate this problem. However, automatic methods solely derive end-systole (ES) and end-diastole (ED) frames from left ventricular volume curves, which do not provide a deeper insight into myocardial motion. We propose a self-supervised deep learning method detecting five keyframes in short-axis (SAX) and four-chamber long-axis (4CH) cine CMR. Initially, dense deformable registration fields are derived from the images and used to compute a 1D motion descriptor, which provides valuable insights into global cardiac contraction and relaxation patterns. From these characteristic curves, keyframes are determined using a simple set of rules. The method was independently evaluated for both views using three public, multicentre, multidisease datasets. M&Ms-2 (n=360) dataset was used for training and evaluation, and M&Ms (n=345) and ACDC (n=100) datasets for repeatability control. Furthermore, generalisability to patients with rare congenital heart defects was tested using the German Competence Network (GCN) dataset. Our self-supervised approach achieved improved detection accuracy by 30% - 51% for SAX and 11% - 47% for 4CH in ED and ES, as measured by cyclic frame difference (cFD), compared with the volume-based approach. We can detect ED and ES, as well as three additional keyframes throughout the cardiac cycle with a mean cFD below 1.31 frames for SAX and 1.73 for LAX. Our approach enables temporally aligned inter- and intra-patient analysis of cardiac dynamics, irrespective of cycle or phase lengths. GitHub repository: https://github.com/Cardio-AI/cmr-multi-view-phase-detection.git",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Main 30 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.05819v1",
    "published_date": "2025-10-07 11:40:17 UTC",
    "updated_date": "2025-10-07 11:40:17 UTC"
  },
  {
    "arxiv_id": "2510.05808v1",
    "title": "Risk level dependent Minimax Quantile lower bounds for Interactive Statistical Decision Making",
    "authors": [
      "Raghav Bongole",
      "Amirreza Zamani",
      "Tobias J. Oechtering",
      "Mikael Skoglund"
    ],
    "abstract": "Minimax risk and regret focus on expectation, missing rare failures critical in safety-critical bandits and reinforcement learning. Minimax quantiles capture these tails. Three strands of prior work motivate this study: minimax-quantile bounds restricted to non-interactive estimation; unified interactive analyses that focus on expected risk rather than risk level specific quantile bounds; and high-probability bandit bounds that still lack a quantile-specific toolkit for general interactive protocols. To close this gap, within the interactive statistical decision making framework, we develop high-probability Fano and Le Cam tools and derive risk level explicit minimax-quantile bounds, including a quantile-to-expectation conversion and a tight link between strict and lower minimax quantiles. Instantiating these results for the two-armed Gaussian bandit immediately recovers optimal-rate bounds.",
    "categories": [
      "cs.IT",
      "cs.AI"
    ],
    "primary_category": "cs.IT",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05808v1",
    "published_date": "2025-10-07 11:25:13 UTC",
    "updated_date": "2025-10-07 11:25:13 UTC"
  },
  {
    "arxiv_id": "2510.05799v1",
    "title": "Data-efficient Targeted Token-level Preference Optimization for LLM-based Text-to-Speech",
    "authors": [
      "Rikuto Kotoge",
      "Yuichi Sasaki"
    ],
    "abstract": "Aligning text-to-speech (TTS) system outputs with human feedback through preference optimization has been shown to effectively improve the robustness and naturalness of language model-based TTS models. Current approaches primarily require paired desirable and undesirable samples at the utterance level. However, such pairs are often limited in TTS output data, and utterance-level formulation prevents fine-grained token-level optimization needed for accurate pronunciation alignment. In this study, we propose TKTO that eliminates the need for paired data, enabling a more data-efficient training paradigm, and directly targets token-level units, automatically providing fine-grained alignment signals without token-level annotations. TKTO improves the challenging Japanese TTS accuracy by 39% and reduces CER by 54%, automatically assigning 12.8 times stronger reward to targeted tokens.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05799v1",
    "published_date": "2025-10-07 11:18:04 UTC",
    "updated_date": "2025-10-07 11:18:04 UTC"
  },
  {
    "arxiv_id": "2511.02842v1",
    "title": "Digital Transformation Chatbot (DTchatbot): Integrating Large Language Model-based Chatbot in Acquiring Digital Transformation Needs",
    "authors": [
      "Jiawei Zheng",
      "Gokcen Yilmaz",
      "Ji Han",
      "Saeema Ahmed-Kristensen"
    ],
    "abstract": "Many organisations pursue digital transformation to enhance operational efficiency, reduce manual efforts, and optimise processes by automation and digital tools. To achieve this, a comprehensive understanding of their unique needs is required. However, traditional methods, such as expert interviews, while effective, face several challenges, including scheduling conflicts, resource constraints, inconsistency, etc. To tackle these issues, we investigate the use of a Large Language Model (LLM)-powered chatbot to acquire organisations' digital transformation needs. Specifically, the chatbot integrates workflow-based instruction with LLM's planning and reasoning capabilities, enabling it to function as a virtual expert and conduct interviews. We detail the chatbot's features and its implementation. Our preliminary evaluation indicates that the chatbot performs as designed, effectively following predefined workflows and supporting user interactions with areas for improvement. We conclude by discussing the implications of employing chatbots to elicit user information, emphasizing their potential and limitations.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted by the International Conference on Human-Computer Interaction",
    "pdf_url": "https://arxiv.org/pdf/2511.02842v1",
    "published_date": "2025-10-07 11:09:23 UTC",
    "updated_date": "2025-10-07 11:09:23 UTC"
  },
  {
    "arxiv_id": "2510.05788v1",
    "title": "Mellum: Production-Grade in-IDE Contextual Code Completion with Multi-File Project Understanding",
    "authors": [
      "Nikita Pavlichenko",
      "Iurii Nazarov",
      "Ivan Dolgov",
      "Ekaterina Garanina",
      "Dmitry Ustalov",
      "Ivan Bondyrev",
      "Kseniia Lysaniuk",
      "Evgeniia Vu",
      "Kirill Chekmenev",
      "Joseph Shtok",
      "Yaroslav Golubev",
      "Anton Semenkin",
      "Uladzislau Sazanovich"
    ],
    "abstract": "We present the Mellum models family, open-weight code completion models designed for interactive use in JetBrains IDEs. Mellums have 4B parameters, adopt a Llama-style architecture, and are pre-trained on ~4T tokens of permissively licensed, multi-language code. Our studies show that (i) careful data curation and staged training significantly improve the model's quality, (ii) editor-critical capabilities such as context packing are necessary for high-quality suggestions, and (iii) a compact, task-focused model can meet the cost and latency constraints of interactive completion.\n  In the paper, we describe an end-to-end industrial pipeline for producing contextualized in-editor completion: disciplined data governance, multi-stage training that includes fill-in-the-middle and project context via supervised fine-tuning, and alignment via direct preference optimization using feedback from real-world scenarios. Our quality evaluations include both large-scale offline benchmarks and online telemetry from production deployments in JetBrains IDEs. Mellums are released under the Apache-2.0 license on HuggingFace, with a public model card providing a reproducible reference for practitioners. Our experience offers a pragmatic blueprint for taking a focused, open model from a research prototype to at scale production for hundreds of thousands of users.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "11 pages, 4 figures, 3 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.05788v1",
    "published_date": "2025-10-07 11:09:11 UTC",
    "updated_date": "2025-10-07 11:09:11 UTC"
  },
  {
    "arxiv_id": "2510.05774v1",
    "title": "ConstraintLLM: A Neuro-Symbolic Framework for Industrial-Level Constraint Programming",
    "authors": [
      "Weichun Shi",
      "Minghao Liu",
      "Wanting Zhang",
      "Langchen Shi",
      "Fuqi Jia",
      "Feifei Ma",
      "Jian Zhang"
    ],
    "abstract": "Constraint programming (CP) is a crucial technology for solving real-world constraint optimization problems (COPs), with the advantages of rich modeling semantics and high solving efficiency. Using large language models (LLMs) to generate formal modeling automatically for COPs is becoming a promising approach, which aims to build trustworthy neuro-symbolic AI with the help of symbolic solvers. However, CP has received less attention compared to works based on operations research (OR) models. We introduce ConstraintLLM, the first LLM specifically designed for CP modeling, which is trained on an open-source LLM with multi-instruction supervised fine-tuning. We propose the Constraint-Aware Retrieval Module (CARM) to increase the in-context learning capabilities, which is integrated in a Tree-of-Thoughts (ToT) framework with guided self-correction mechanism. Moreover, we construct and release IndusCP, the first industrial-level benchmark for CP modeling, which contains 140 challenging tasks from various domains. Our experiments demonstrate that ConstraintLLM achieves state-of-the-art solving accuracy across multiple benchmarks and outperforms the baselines by 2x on the new IndusCP benchmark. Code and data are available at: https://github.com/william4s/ConstraintLLM.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to the 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP 2025), Main Conference",
    "pdf_url": "https://arxiv.org/pdf/2510.05774v1",
    "published_date": "2025-10-07 10:43:39 UTC",
    "updated_date": "2025-10-07 10:43:39 UTC"
  },
  {
    "arxiv_id": "2510.05769v1",
    "title": "InforME: Improving Informativeness of Abstractive Text Summarization With Informative Attention Guided by Named Entity Salience",
    "authors": [
      "Jianbin Shen",
      "Christy Jie Liang",
      "Junyu Xuan"
    ],
    "abstract": "Abstractive text summarization is integral to the Big Data era, which demands advanced methods to turn voluminous and often long text data into concise but coherent and informative summaries for efficient human consumption. Despite significant progress, there is still room for improvement in various aspects. One such aspect is to improve informativeness. Hence, this paper proposes a novel learning approach consisting of two methods: an optimal transport-based informative attention method to improve learning focal information in reference summaries and an accumulative joint entropy reduction method on named entities to enhance informative salience. Experiment results show that our approach achieves better ROUGE scores compared to prior work on CNN/Daily Mail while having competitive results on XSum. Human evaluation of informativeness also demonstrates the better performance of our approach over a strong baseline. Further analysis gives insight into the plausible reasons underlying the evaluation results.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05769v1",
    "published_date": "2025-10-07 10:40:09 UTC",
    "updated_date": "2025-10-07 10:40:09 UTC"
  },
  {
    "arxiv_id": "2510.05764v2",
    "title": "RareAgent: Self-Evolving Reasoning for Drug Repurposing in Rare Diseases",
    "authors": [
      "Lang Qin",
      "Zijian Gan",
      "Xu Cao",
      "Pengcheng Jiang",
      "Yankai Jiang",
      "Jiawei Han",
      "Kaishun Wu",
      "Jintai Chen"
    ],
    "abstract": "Computational drug repurposing for rare diseases is especially challenging when no prior associations exist between drugs and target diseases. Therefore, knowledge graph completion and message-passing GNNs have little reliable signal to learn and propagate, resulting in poor performance. We present RareAgent, a self-evolving multi-agent system that reframes this task from passive pattern recognition to active evidence-seeking reasoning. RareAgent organizes task-specific adversarial debates in which agents dynamically construct evidence graphs from diverse perspectives to support, refute, or entail hypotheses. The reasoning strategies are analyzed post hoc in a self-evolutionary loop, producing textual feedback that refines agent policies, while successful reasoning paths are distilled into transferable heuristics to accelerate future investigations. Comprehensive evaluations reveal that RareAgent improves the indication AUPRC by 18.1% over reasoning baselines and provides a transparent reasoning chain consistent with clinical evidence.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05764v2",
    "published_date": "2025-10-07 10:35:18 UTC",
    "updated_date": "2025-10-16 03:52:44 UTC"
  },
  {
    "arxiv_id": "2510.05761v1",
    "title": "Early Multimodal Prediction of Cross-Lingual Meme Virality on Reddit: A Time-Window Analysis",
    "authors": [
      "Sedat Dogan",
      "Nina Dethlefs",
      "Debarati Chakraborty"
    ],
    "abstract": "Predicting the virality of online content remains challenging, especially for culturally complex, fast-evolving memes. This study investigates the feasibility of early prediction of meme virality using a large-scale, cross-lingual dataset from 25 diverse Reddit communities. We propose a robust, data-driven method to define virality based on a hybrid engagement score, learning a percentile-based threshold from a chronologically held-out training set to prevent data leakage. We evaluated a suite of models, including Logistic Regression, XGBoost, and a Multi-layer Perceptron (MLP), with a comprehensive, multimodal feature set across increasing time windows (30-420 min). Crucially, useful signals emerge quickly: our best-performing model, XGBoost, achieves a PR-AUC $>$ 0.52 in just 30 minutes. Our analysis reveals a clear \"evidentiary transition,\" in which the importance of the feature dynamically shifts from the static context to the temporal dynamics as a meme gains traction. This work establishes a robust, interpretable, and practical benchmark for early virality prediction in scenarios where full diffusion cascade data is unavailable, contributing a novel cross-lingual dataset and a methodologically sound definition of virality. To our knowledge, this study is the first to combine time series data with static content and network features to predict early meme virality.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Preprint work in progress. Main body: 9 pages. Total: 15 pages including references and appendix. 16 figures and 12 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.05761v1",
    "published_date": "2025-10-07 10:27:36 UTC",
    "updated_date": "2025-10-07 10:27:36 UTC"
  },
  {
    "arxiv_id": "2510.06292v1",
    "title": "ChainMPQ: Interleaved Text-Image Reasoning Chains for Mitigating Relation Hallucinations",
    "authors": [
      "Yike Wu",
      "Yiwei Wang",
      "Yujun Cai"
    ],
    "abstract": "While Large Vision-Language Models (LVLMs) achieve strong performance in multimodal tasks, hallucinations continue to hinder their reliability. Among the three categories of hallucinations, which include object, attribute, and relation, relation hallucinations account for the largest proportion but have received the least attention. To address this issue, we propose ChainMPQ (Multi-Perspective Questions guided Interleaved Chain of Image and Text), a training-free method that improves relational inference in LVLMs by utilizing accumulated textual and visual memories. ChainMPQ first extracts subject and object keywords from the question to enhance the corresponding image regions. It then constructs multi-perspective questions that focus on the three core components of a relationship: the subject, the object, and the relation that links them. These questions are sequentially input to the model, with textual and visual memories from earlier steps providing supporting context for subsequent ones, thereby forming an interleaved chain of images and text that guides progressive relational reasoning. Experiments on multiple LVLMs and benchmarks show that ChainMPQ substantially reduces relation hallucinations, while ablation studies further validate the effectiveness of its three core modules.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06292v1",
    "published_date": "2025-10-07 10:19:18 UTC",
    "updated_date": "2025-10-07 10:19:18 UTC"
  },
  {
    "arxiv_id": "2510.05751v1",
    "title": "Uncertainty assessment in satellite-based greenhouse gas emissions estimates using emulated atmospheric transport",
    "authors": [
      "Jeffrey N. Clark",
      "Elena Fillola",
      "Nawid Keshtmand",
      "Raul Santos-Rodriguez",
      "Matthew Rigby"
    ],
    "abstract": "Monitoring greenhouse gas emissions and evaluating national inventories require efficient, scalable, and reliable inference methods. Top-down approaches, combined with recent advances in satellite observations, provide new opportunities to evaluate emissions at continental and global scales. However, transport models used in these methods remain a key source of uncertainty: they are computationally expensive to run at scale, and their uncertainty is difficult to characterise. Artificial intelligence offers a dual opportunity to accelerate transport simulations and to quantify their associated uncertainty.\n  We present an ensemble-based pipeline for estimating atmospheric transport \"footprints\", greenhouse gas mole fraction measurements, and their uncertainties using a graph neural network emulator of a Lagrangian Particle Dispersion Model (LPDM). The approach is demonstrated with GOSAT (Greenhouse Gases Observing Satellite) observations for Brazil in 2016. The emulator achieved a ~1000x speed-up over the NAME LPDM, while reproducing large-scale footprint structures. Ensembles were calculated to quantify absolute and relative uncertainty, revealing spatial correlations with prediction error. The results show that ensemble spread highlights low-confidence spatial and temporal predictions for both atmospheric transport footprints and methane mole fractions.\n  While demonstrated here for an LPDM emulator, the approach could be applied more generally to atmospheric transport models, supporting uncertainty-aware greenhouse gas inversion systems and improving the robustness of satellite-based emissions monitoring. With further development, ensemble-based emulators could also help explore systematic LPDM errors, offering a computationally efficient pathway towards a more comprehensive uncertainty budget in greenhouse gas flux estimates.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05751v1",
    "published_date": "2025-10-07 10:14:25 UTC",
    "updated_date": "2025-10-07 10:14:25 UTC"
  },
  {
    "arxiv_id": "2510.05750v1",
    "title": "Are Heterogeneous Graph Neural Networks Truly Effective? A Causal Perspective",
    "authors": [
      "Xiao Yang",
      "Xuejiao Zhao",
      "Zhiqi Shen"
    ],
    "abstract": "Graph neural networks (GNNs) have achieved remarkable success in node classification. Building on this progress, heterogeneous graph neural networks (HGNNs) integrate relation types and node and edge semantics to leverage heterogeneous information. Causal analysis for HGNNs is advancing rapidly, aiming to separate genuine causal effects from spurious correlations. However, whether HGNNs are intrinsically effective remains underexamined, and most studies implicitly assume rather than establish this effectiveness. In this work, we examine HGNNs from two perspectives: model architecture and heterogeneous information. We conduct a systematic reproduction across 21 datasets and 20 baselines, complemented by comprehensive hyperparameter retuning. To further disentangle the source of performance gains, we develop a causal effect estimation framework that constructs and evaluates candidate factors under standard assumptions through factual and counterfactual analyses, with robustness validated via minimal sufficient adjustment sets, cross-method consistency checks, and sensitivity analyses. Our results lead to two conclusions. First, model architecture and complexity have no causal effect on performance. Second, heterogeneous information exerts a positive causal effect by increasing homophily and local-global distribution discrepancy, which makes node classes more distinguishable. The implementation is publicly available at https://github.com/YXNTU/CausalHGNN.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05750v1",
    "published_date": "2025-10-07 10:12:21 UTC",
    "updated_date": "2025-10-07 10:12:21 UTC"
  },
  {
    "arxiv_id": "2510.08606v1",
    "title": "Centering Emotion Hotspots: Multimodal Local-Global Fusion and Cross-Modal Alignment for Emotion Recognition in Conversations",
    "authors": [
      "Yu Liu",
      "Hanlei Shi",
      "Haoxun Li",
      "Yuqing Sun",
      "Yuxuan Ding",
      "Linlin Gong",
      "Leyuan Qu",
      "Taihao Li"
    ],
    "abstract": "Emotion Recognition in Conversations (ERC) is hard because discriminative evidence is sparse, localized, and often asynchronous across modalities. We center ERC on emotion hotspots and present a unified model that detects per-utterance hotspots in text, audio, and video, fuses them with global features via Hotspot-Gated Fusion, and aligns modalities using a routed Mixture-of-Aligners; a cross-modal graph encodes conversational structure. This design focuses modeling on salient spans, mitigates misalignment, and preserves context. Experiments on standard ERC benchmarks show consistent gains over strong baselines, with ablations confirming the contributions of HGF and MoA. Our results point to a hotspot-centric view that can inform future multimodal learning, offering a new perspective on modality fusion in ERC.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Under review for ICASSP 2026",
    "pdf_url": "https://arxiv.org/pdf/2510.08606v1",
    "published_date": "2025-10-07 10:11:50 UTC",
    "updated_date": "2025-10-07 10:11:50 UTC"
  },
  {
    "arxiv_id": "2510.08605v1",
    "title": "Toward a Safer Web: Multilingual Multi-Agent LLMs for Mitigating Adversarial Misinformation Attacks",
    "authors": [
      "Nouar Aldahoul",
      "Yasir Zaki"
    ],
    "abstract": "The rapid spread of misinformation on digital platforms threatens public discourse, emotional stability, and decision-making. While prior work has explored various adversarial attacks in misinformation detection, the specific transformations examined in this paper have not been systematically studied. In particular, we investigate language-switching across English, French, Spanish, Arabic, Hindi, and Chinese, followed by translation. We also study query length inflation preceding summarization and structural reformatting into multiple-choice questions. In this paper, we present a multilingual, multi-agent large language model framework with retrieval-augmented generation that can be deployed as a web plugin into online platforms. Our work underscores the importance of AI-driven misinformation detection in safeguarding online factual integrity against diverse attacks, while showcasing the feasibility of plugin-based deployment for real-world web applications.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08605v1",
    "published_date": "2025-10-07 10:09:25 UTC",
    "updated_date": "2025-10-07 10:09:25 UTC"
  },
  {
    "arxiv_id": "2510.05746v1",
    "title": "ARM: Discovering Agentic Reasoning Modules for Generalizable Multi-Agent Systems",
    "authors": [
      "Bohan Yao",
      "Shiva Krishna Reddy Malay",
      "Vikas Yadav"
    ],
    "abstract": "Large Language Model (LLM)-powered Multi-agent systems (MAS) have achieved state-of-the-art results on various complex reasoning tasks. Recent works have proposed techniques to automate the design of MASes, eliminating the need for manual engineering. However, these techniques perform poorly, often achieving similar or inferior performance to simple baselines. Furthermore, they require computationally expensive re-discovery of architectures for each new task domain and expensive data annotation on domains without existing labeled validation sets. A critical insight is that simple Chain of Thought (CoT) reasoning often performs competitively with these complex systems, suggesting that the fundamental reasoning unit of MASes, CoT, warrants further investigation. To this end, we present a new paradigm for automatic MAS design that pivots the focus to optimizing CoT reasoning. We introduce the Agentic Reasoning Module (ARM), an agentic generalization of CoT where each granular reasoning step is executed by a specialized reasoning module. This module is discovered through a tree search over the code space, starting from a simple CoT module and evolved using mutations informed by reflection on execution traces. The resulting ARM acts as a versatile reasoning building block which can be utilized as a direct recursive loop or as a subroutine in a learned meta-orchestrator. Our approach significantly outperforms both manually designed MASes and state-of-the-art automatic MAS design methods. Crucially, MASes built with ARM exhibit superb generalization, maintaining high performance across different foundation models and task domains without further optimization.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "29 pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.05746v1",
    "published_date": "2025-10-07 10:04:48 UTC",
    "updated_date": "2025-10-07 10:04:48 UTC"
  },
  {
    "arxiv_id": "2510.05743v2",
    "title": "Artificially intelligent agents in the social and behavioral sciences: A history and outlook",
    "authors": [
      "Petter Holme",
      "Milena Tsvetkova"
    ],
    "abstract": "We review the historical development and current trends of artificially intelligent agents (agentic AI) in the social and behavioral sciences: from the first programmable computers, and social simulations soon thereafter, to today's experiments with large language models. This overview emphasizes the role of AI in the scientific process and the changes brought about, both through technological advancements and the broader evolution of science from around 1950 to the present. Some of the specific points we cover include: the challenges of presenting the first social simulation studies to a world unaware of computers, the rise of social systems science, intelligent game theoretic agents, the age of big data and the epistemic upheaval in its wake, and the current enthusiasm around applications of generative AI, and many other topics. A pervasive theme is how deeply entwined we are with the technologies we use to understand ourselves.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05743v2",
    "published_date": "2025-10-07 10:02:17 UTC",
    "updated_date": "2025-10-31 08:48:04 UTC"
  },
  {
    "arxiv_id": "2510.05740v1",
    "title": "Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect",
    "authors": [
      "Amirtaha Amanzadi",
      "Zahra Dehghanian",
      "Hamid Beigy",
      "Hamid R. Rabiee"
    ],
    "abstract": "The rapid development of generative models has made it increasingly crucial to develop detectors that can reliably detect synthetic images. Although most of the work has now focused on cross-generator generalization, we argue that this viewpoint is too limited. Detecting synthetic images involves another equally important challenge: generalization across visual domains. To bridge this gap,we present the OmniGen Benchmark. This comprehensive evaluation dataset incorporates 12 state-of-the-art generators, providing a more realistic way of evaluating detector performance under realistic conditions. In addition, we introduce a new method, FusionDetect, aimed at addressing both vectors of generalization. FusionDetect draws on the benefits of two frozen foundation models: CLIP & Dinov2. By deriving features from both complementary models,we develop a cohesive feature space that naturally adapts to changes in both thecontent and design of the generator. Our extensive experiments demonstrate that FusionDetect delivers not only a new state-of-the-art, which is 3.87% more accurate than its closest competitor and 6.13% more precise on average on established benchmarks, but also achieves a 4.48% increase in accuracy on OmniGen,along with exceptional robustness to common image perturbations. We introduce not only a top-performing detector, but also a new benchmark and framework for furthering universal AI image detection. The code and dataset are available at http://github.com/amir-aman/FusionDetect",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project code: http://github.com/amir-aman/FusionDetect",
    "pdf_url": "https://arxiv.org/pdf/2510.05740v1",
    "published_date": "2025-10-07 10:01:32 UTC",
    "updated_date": "2025-10-07 10:01:32 UTC"
  },
  {
    "arxiv_id": "2510.05733v1",
    "title": "Syn-Diag: An LLM-based Synergistic Framework for Generalizable Few-shot Fault Diagnosis on the Edge",
    "authors": [
      "Zijun Jia",
      "Shuang Liang",
      "Jinsong Yu"
    ],
    "abstract": "Industrial fault diagnosis faces the dual challenges of data scarcity and the difficulty of deploying large AI models in resource-constrained environments. This paper introduces Syn-Diag, a novel cloud-edge synergistic framework that leverages Large Language Models to overcome these limitations in few-shot fault diagnosis. Syn-Diag is built on a three-tiered mechanism: 1) Visual-Semantic Synergy, which aligns signal features with the LLM's semantic space through cross-modal pre-training; 2) Content-Aware Reasoning, which dynamically constructs contextual prompts to enhance diagnostic accuracy with limited samples; and 3) Cloud-Edge Synergy, which uses knowledge distillation to create a lightweight, efficient edge model capable of online updates via a shared decision space. Extensive experiments on six datasets covering different CWRU and SEU working conditions show that Syn-Diag significantly outperforms existing methods, especially in 1-shot and cross-condition scenarios. The edge model achieves performance comparable to the cloud version while reducing model size by 83% and latency by 50%, offering a practical, robust, and deployable paradigm for modern intelligent diagnostics.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05733v1",
    "published_date": "2025-10-07 09:55:21 UTC",
    "updated_date": "2025-10-07 09:55:21 UTC"
  },
  {
    "arxiv_id": "2510.05725v1",
    "title": "Improving Discrete Diffusion Unmasking Policies Beyond Explicit Reference Policies",
    "authors": [
      "Chunsan Hong",
      "Seonho An",
      "Min-Soo Kim",
      "Jong Chul Ye"
    ],
    "abstract": "Masked diffusion models (MDMs) have recently emerged as a novel framework for language modeling. MDMs generate sentences by iteratively denoising masked sequences, filling in [MASK] tokens step by step. Although MDMs support any-order sampling, performance is highly sensitive to the choice of which position to unmask next. Prior work typically relies on rule-based schedules (e.g., max-confidence, max-margin), which provide ad hoc improvements. In contrast, we replace these heuristics with a learned scheduler. Specifically, we cast denoising as a KL-regularized Markov decision process (MDP) with an explicit reference policy and optimize a regularized objective that admits policy improvement and convergence guarantees under standard assumptions. We prove that the optimized policy under this framework generates samples that more closely match the data distribution than heuristic schedules. Empirically, across four benchmarks, our learned policy consistently outperforms max-confidence: for example, on SUDOKU, where unmasking order is critical, it yields a 20.1% gain over random and a 11.2% gain over max-confidence.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint",
    "pdf_url": "https://arxiv.org/pdf/2510.05725v1",
    "published_date": "2025-10-07 09:44:24 UTC",
    "updated_date": "2025-10-07 09:44:24 UTC"
  },
  {
    "arxiv_id": "2510.08604v2",
    "title": "LatentBreak: Jailbreaking Large Language Models through Latent Space Feedback",
    "authors": [
      "Raffaele Mura",
      "Giorgio Piras",
      "Kamilė Lukošiūtė",
      "Maura Pintor",
      "Amin Karbasi",
      "Battista Biggio"
    ],
    "abstract": "Jailbreaks are adversarial attacks designed to bypass the built-in safety mechanisms of large language models. Automated jailbreaks typically optimize an adversarial suffix or adapt long prompt templates by forcing the model to generate the initial part of a restricted or harmful response. In this work, we show that existing jailbreak attacks that leverage such mechanisms to unlock the model response can be detected by a straightforward perplexity-based filtering on the input prompt. To overcome this issue, we propose LatentBreak, a white-box jailbreak attack that generates natural adversarial prompts with low perplexity capable of evading such defenses. LatentBreak substitutes words in the input prompt with semantically-equivalent ones, preserving the initial intent of the prompt, instead of adding high-perplexity adversarial suffixes or long templates. These words are chosen by minimizing the distance in the latent space between the representation of the adversarial prompt and that of harmless requests. Our extensive evaluation shows that LatentBreak leads to shorter and low-perplexity prompts, thus outperforming competing jailbreak algorithms against perplexity-based filters on multiple safety-aligned models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08604v2",
    "published_date": "2025-10-07 09:40:20 UTC",
    "updated_date": "2025-10-30 15:33:58 UTC"
  },
  {
    "arxiv_id": "2510.05713v1",
    "title": "Federated Split Learning for Resource-Constrained Robots in Industrial IoT: Framework Comparison, Optimization Strategies, and Future Directions",
    "authors": [
      "Wanli Ni",
      "Hui Tian",
      "Shuai Wang",
      "Chengyang Li",
      "Lei Sun",
      "Zhaohui Yang"
    ],
    "abstract": "Federated split learning (FedSL) has emerged as a promising paradigm for enabling collaborative intelligence in industrial Internet of Things (IoT) systems, particularly in smart factories where data privacy, communication efficiency, and device heterogeneity are critical concerns. In this article, we present a comprehensive study of FedSL frameworks tailored for resource-constrained robots in industrial scenarios. We compare synchronous, asynchronous, hierarchical, and heterogeneous FedSL frameworks in terms of workflow, scalability, adaptability, and limitations under dynamic industrial conditions. Furthermore, we systematically categorize token fusion strategies into three paradigms: input-level (pre-fusion), intermediate-level (intra-fusion), and output-level (post-fusion), and summarize their respective strengths in industrial applications. We also provide adaptive optimization techniques to enhance the efficiency and feasibility of FedSL implementation, including model compression, split layer selection, computing frequency allocation, and wireless resource management. Simulation results validate the performance of these frameworks under industrial detection scenarios. Finally, we outline open issues and research directions of FedSL in future smart manufacturing systems.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.MA",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "9 pages, 5 figures, submitted to the IEEE magazine",
    "pdf_url": "https://arxiv.org/pdf/2510.05713v1",
    "published_date": "2025-10-07 09:24:29 UTC",
    "updated_date": "2025-10-07 09:24:29 UTC"
  },
  {
    "arxiv_id": "2510.05710v1",
    "title": "FinReflectKG - EvalBench: Benchmarking Financial KG with Multi-Dimensional Evaluation",
    "authors": [
      "Fabrizio Dimino",
      "Abhinav Arun",
      "Bhaskarjit Sarmah",
      "Stefano Pasquali"
    ],
    "abstract": "Large language models (LLMs) are increasingly being used to extract structured knowledge from unstructured financial text. Although prior studies have explored various extraction methods, there is no universal benchmark or unified evaluation framework for the construction of financial knowledge graphs (KG). We introduce FinReflectKG - EvalBench, a benchmark and evaluation framework for KG extraction from SEC 10-K filings. Building on the agentic and holistic evaluation principles of FinReflectKG - a financial KG linking audited triples to source chunks from S&P 100 filings and supporting single-pass, multi-pass, and reflection-agent-based extraction modes - EvalBench implements a deterministic commit-then-justify judging protocol with explicit bias controls, mitigating position effects, leniency, verbosity and world-knowledge reliance. Each candidate triple is evaluated with binary judgments of faithfulness, precision, and relevance, while comprehensiveness is assessed on a three-level ordinal scale (good, partial, bad) at the chunk level. Our findings suggest that, when equipped with explicit bias controls, LLM-as-Judge protocols provide a reliable and cost-efficient alternative to human annotation, while also enabling structured error analysis. Reflection-based extraction emerges as the superior approach, achieving best performance in comprehensiveness, precision, and relevance, while single-pass extraction maintains the highest faithfulness. By aggregating these complementary dimensions, FinReflectKG - EvalBench enables fine-grained benchmarking and bias-aware evaluation, advancing transparency and governance in financial AI applications.",
    "categories": [
      "q-fin.CP",
      "cs.AI"
    ],
    "primary_category": "q-fin.CP",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05710v1",
    "published_date": "2025-10-07 09:22:48 UTC",
    "updated_date": "2025-10-07 09:22:48 UTC"
  },
  {
    "arxiv_id": "2510.05709v1",
    "title": "Towards Reliable and Practical LLM Security Evaluations via Bayesian Modelling",
    "authors": [
      "Mary Llewellyn",
      "Annie Gray",
      "Josh Collyer",
      "Michael Harries"
    ],
    "abstract": "Before adopting a new large language model (LLM) architecture, it is critical to understand vulnerabilities accurately. Existing evaluations can be difficult to trust, often drawing conclusions from LLMs that are not meaningfully comparable, relying on heuristic inputs or employing metrics that fail to capture the inherent uncertainty. In this paper, we propose a principled and practical end-to-end framework for evaluating LLM vulnerabilities to prompt injection attacks. First, we propose practical approaches to experimental design, tackling unfair LLM comparisons by considering two practitioner scenarios: when training an LLM and when deploying a pre-trained LLM. Second, we address the analysis of experiments and propose a Bayesian hierarchical model with embedding-space clustering. This model is designed to improve uncertainty quantification in the common scenario that LLM outputs are not deterministic, test prompts are designed imperfectly, and practitioners only have a limited amount of compute to evaluate vulnerabilities. We show the improved inferential capabilities of the model in several prompt injection attack settings. Finally, we demonstrate the pipeline to evaluate the security of Transformer versus Mamba architectures. Our findings show that consideration of output variability can suggest less definitive findings. However, for some attacks, we find notably increased Transformer and Mamba-variant vulnerabilities across LLMs with the same training data or mathematical ability.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05709v1",
    "published_date": "2025-10-07 09:22:22 UTC",
    "updated_date": "2025-10-07 09:22:22 UTC"
  },
  {
    "arxiv_id": "2510.05702v2",
    "title": "Uncovering Representation Bias for Investment Decisions in Open-Source Large Language Models",
    "authors": [
      "Fabrizio Dimino",
      "Krati Saxena",
      "Bhaskarjit Sarmah",
      "Stefano Pasquali"
    ],
    "abstract": "Large Language Models are increasingly adopted in financial applications to support investment workflows. However, prior studies have seldom examined how these models reflect biases related to firm size, sector, or financial characteristics, which can significantly impact decision-making. This paper addresses this gap by focusing on representation bias in open-source Qwen models. We propose a balanced round-robin prompting method over approximately 150 U.S. equities, applying constrained decoding and token-logit aggregation to derive firm-level confidence scores across financial contexts. Using statistical tests and variance analysis, we find that firm size and valuation consistently increase model confidence, while risk factors tend to decrease it. Confidence varies significantly across sectors, with the Technology sector showing the greatest variability. When models are prompted for specific financial categories, their confidence rankings best align with fundamental data, moderately with technical signals, and least with growth indicators. These results highlight representation bias in Qwen models and motivate sector-aware calibration and category-conditioned evaluation protocols for safe and fair financial LLM deployment.",
    "categories": [
      "q-fin.CP",
      "cs.AI"
    ],
    "primary_category": "q-fin.CP",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05702v2",
    "published_date": "2025-10-07 09:10:13 UTC",
    "updated_date": "2025-11-03 01:00:40 UTC"
  },
  {
    "arxiv_id": "2510.05699v2",
    "title": "Membership Inference Attacks on Tokenizers of Large Language Models",
    "authors": [
      "Meng Tong",
      "Yuntao Du",
      "Kejiang Chen",
      "Weiming Zhang"
    ],
    "abstract": "Membership inference attacks (MIAs) are widely used to assess the privacy risks associated with machine learning models. However, when these attacks are applied to pre-trained large language models (LLMs), they encounter significant challenges, including mislabeled samples, distribution shifts, and discrepancies in model size between experimental and real-world settings. To address these limitations, we introduce tokenizers as a new attack vector for membership inference. Specifically, a tokenizer converts raw text into tokens for LLMs. Unlike full models, tokenizers can be efficiently trained from scratch, thereby avoiding the aforementioned challenges. In addition, the tokenizer's training data is typically representative of the data used to pre-train LLMs. Despite these advantages, the potential of tokenizers as an attack vector remains unexplored. To this end, we present the first study on membership leakage through tokenizers and explore five attack methods to infer dataset membership. Extensive experiments on millions of Internet samples reveal the vulnerabilities in the tokenizers of state-of-the-art LLMs. To mitigate this emerging risk, we further propose an adaptive defense. Our findings highlight tokenizers as an overlooked yet critical privacy threat, underscoring the urgent need for privacy-preserving mechanisms specifically designed for them.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "To appear at USENIX Security Symposium 2026",
    "pdf_url": "https://arxiv.org/pdf/2510.05699v2",
    "published_date": "2025-10-07 09:05:40 UTC",
    "updated_date": "2026-01-12 12:20:50 UTC"
  },
  {
    "arxiv_id": "2510.05698v1",
    "title": "Joint Communication Scheduling and Velocity Control for Multi-UAV-Assisted Post-Disaster Monitoring: An Attention-Based In-Context Learning Approach",
    "authors": [
      "Yousef Emami",
      "Seyedsina Nabavirazavi",
      "Jingjing Zheng",
      "Hao Zhou",
      "Miguel Gutierrez Gaitan",
      "Kai Li",
      "Luis Almeida"
    ],
    "abstract": "Recently, Unmanned Aerial Vehicles (UAVs) are increasingly being investigated to collect sensory data in post-disaster monitoring scenarios, such as tsunamis, where early actions are critical to limit coastal damage. A major challenge is to design the data collection schedules and flight velocities, as unfavorable schedules and velocities can lead to transmission errors and buffer overflows of the ground sensors, ultimately resulting in significant packet loss. Meanwhile, online Deep Reinforcement Learning (DRL) solutions have a complex training process and a mismatch between simulation and reality that does not meet the urgent requirements of tsunami monitoring. Recent advances in Large Language Models (LLMs) offer a compelling alternative. With their strong reasoning and generalization capabilities, LLMs can adapt to new tasks through In-Context Learning (ICL), which enables task adaptation through natural language prompts and example-based guidance without retraining. However, LLM models have input data limitations and thus require customized approaches. In this paper, a joint optimization of data collection schedules and velocities control for multiple UAVs is proposed to minimize data loss. The battery level of the ground sensors, the length of the queues, and the channel conditions, as well as the trajectories of the UAVs, are taken into account. Attention-Based In-Context Learning for Velocity Control and Data Collection Schedule (AIC-VDS) is proposed as an alternative to DRL in emergencies. The simulation results show that the proposed AIC-VDS outperforms both the Deep-Q-Network (DQN) and maximum channel gain baselines.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05698v1",
    "published_date": "2025-10-07 09:04:56 UTC",
    "updated_date": "2025-10-07 09:04:56 UTC"
  },
  {
    "arxiv_id": "2510.05696v1",
    "title": "Sparse deepfake detection promotes better disentanglement",
    "authors": [
      "Antoine Teissier",
      "Marie Tahon",
      "Nicolas Dugué",
      "Aghilas Sini"
    ],
    "abstract": "Due to the rapid progress of speech synthesis, deepfake detection has become a major concern in the speech processing community. Because it is a critical task, systems must not only be efficient and robust, but also provide interpretable explanations. Among the different approaches for explainability, we focus on the interpretation of latent representations. In such paper, we focus on the last layer of embeddings of AASIST, a deepfake detection architecture. We use a TopK activation inspired by SAEs on this layer to obtain sparse representations which are used in the decision process. We demonstrate that sparse deepfake detection can improve detection performance, with an EER of 23.36% on ASVSpoof5 test set, with 95% of sparsity. We then show that these representations provide better disentanglement, using completeness and modularity metrics based on mutual information. Notably, some attacks are directly encoded in the latent space.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05696v1",
    "published_date": "2025-10-07 09:03:39 UTC",
    "updated_date": "2025-10-07 09:03:39 UTC"
  },
  {
    "arxiv_id": "2510.05688v1",
    "title": "vAttention: Verified Sparse Attention",
    "authors": [
      "Aditya Desai",
      "Kumar Krishna Agrawal",
      "Shuo Yang",
      "Alejandro Cuadron",
      "Luis Gaspar Schroeder",
      "Matei Zaharia",
      "Joseph E. Gonzalez",
      "Ion Stoica"
    ],
    "abstract": "State-of-the-art sparse attention methods for reducing decoding latency fall into two main categories: approximate top-$k$ (and its extension, top-$p$) and recently introduced sampling-based estimation. However, these approaches are fundamentally limited in their ability to approximate full attention: they fail to provide consistent approximations across heads and query vectors and, most critically, lack guarantees on approximation quality, limiting their practical deployment. We observe that top-$k$ and random sampling are complementary: top-$k$ performs well when attention scores are dominated by a few tokens, whereas random sampling provides better estimates when attention scores are relatively uniform. Building on this insight and leveraging the statistical guarantees of sampling, we introduce vAttention, the first practical sparse attention mechanism with user-specified $(ε, δ)$ guarantees on approximation accuracy (thus, verified). These guarantees make vAttention a compelling step toward practical, reliable deployment of sparse attention at scale. By unifying top-k and sampling, vAttention outperforms both individually, delivering a superior quality-efficiency trade-off. Our experiments show that vAttention significantly improves the quality of sparse attention (e.g., $\\sim$4.5 percentage points for Llama-3.1-8B-Inst and Deepseek-R1-Distill-Llama-8B on RULER-HARD), and effectively bridges the gap between full and sparse attention (e.g., across datasets, it matches full model quality with upto 20x sparsity). We also demonstrate that it can be deployed in reasoning scenarios to achieve fast decoding without compromising model quality (e.g., vAttention achieves full model quality on AIME2024 at 10x sparsity with up to 32K token generations). Code is open-sourced at https://github.com/xAlg-ai/sparse-attention-hub.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05688v1",
    "published_date": "2025-10-07 08:46:08 UTC",
    "updated_date": "2025-10-07 08:46:08 UTC"
  },
  {
    "arxiv_id": "2510.05684v2",
    "title": "D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to Embodied AI",
    "authors": [
      "Suhwan Choi",
      "Jaeyoon Jung",
      "Haebin Seong",
      "Minchan Kim",
      "Minyeong Kim",
      "Yongjun Cho",
      "Yoonshik Kim",
      "Yubeen Park",
      "Youngjae Yu",
      "Yunsung Lee"
    ],
    "abstract": "Large language models leverage internet-scale text data, yet embodied AI remains constrained by the prohibitive costs of physical trajectory collection. Desktop environments -- particularly gaming -- offer a compelling alternative: they provide rich sensorimotor interactions at scale while maintaining the structured observation-action coupling essential for embodied learning. We present D2E (Desktop to Embodied AI), a framework that demonstrates desktop interactions can serve as an effective pretraining substrate for robotics embodied AI tasks. Unlike prior work that remained domain-specific (e.g., VPT for Minecraft) or kept data proprietary (e.g., SIMA), D2E establishes a complete pipeline from scalable desktop data collection to verified transfer in embodied domains. Our framework comprises three components: (1) the OWA Toolkit that unifies diverse desktop interactions into a standardized format with 152x compression, (2) the Generalist-IDM that achieves strong zero-shot generalization across unseen games through timestamp-based event prediction, enabling internet-scale pseudo-labeling, and (3) VAPT that transfers desktop-pretrained representations to physical manipulation and navigation. Using 1.3K+ hours of data (259 hours of human demonstrations, and 1K+ hours of pseudo-labeled gameplay), we achieve a total of 96.6% success rate on LIBERO manipulation and 83.3% on CANVAS navigation benchmarks. This validates that sensorimotor primitives in digital interactions exhibit sufficient invariance to transfer meaningfully to physical embodied tasks, establishing desktop pretraining as a practical paradigm for robotics. We will make all our work public, including the OWA toolkit, datasets of human-collected and pseudo-labeled, and VAPT-trained models available at https://worv-ai.github.io/d2e/",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05684v2",
    "published_date": "2025-10-07 08:40:33 UTC",
    "updated_date": "2025-12-17 19:23:28 UTC"
  },
  {
    "arxiv_id": "2510.05683v1",
    "title": "QGraphLIME - Explaining Quantum Graph Neural Networks",
    "authors": [
      "Haribandhu Jena",
      "Jyotirmaya Shivottam",
      "Subhankar Mishra"
    ],
    "abstract": "Quantum graph neural networks offer a powerful paradigm for learning on graph-structured data, yet their explainability is complicated by measurement-induced stochasticity and the combinatorial nature of graph structure. In this paper, we introduce QuantumGraphLIME (QGraphLIME), a model-agnostic, post-hoc framework that treats model explanations as distributions over local surrogates fit on structure-preserving perturbations of a graph. By aggregating surrogate attributions together with their dispersion, QGraphLIME yields uncertainty-aware node and edge importance rankings for quantum graph models. The framework further provides a distribution-free, finite-sample guarantee on the size of the surrogate ensemble: a Dvoretzky-Kiefer-Wolfowitz bound ensures uniform approximation of the induced distribution of a binary class probability at target accuracy and confidence under standard independence assumptions. Empirical studies on controlled synthetic graphs with known ground truth demonstrate accurate and stable explanations, with ablations showing clear benefits of nonlinear surrogate modeling and highlighting sensitivity to perturbation design. Collectively, these results establish a principled, uncertainty-aware, and structure-sensitive approach to explaining quantum graph neural networks, and lay the groundwork for scaling to broader architectures and real-world datasets, as quantum resources mature. Code is available at https://github.com/smlab-niser/qglime.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05683v1",
    "published_date": "2025-10-07 08:39:13 UTC",
    "updated_date": "2025-10-07 08:39:13 UTC"
  },
  {
    "arxiv_id": "2510.05681v1",
    "title": "Verifier-free Test-Time Sampling for Vision Language Action Models",
    "authors": [
      "Suhyeok Jang",
      "Dongyoung Kim",
      "Changyeon Kim",
      "Youngsuk Kim",
      "Jinwoo Shin"
    ],
    "abstract": "Vision-Language-Action models (VLAs) have demonstrated remarkable performance in robot control. However, they remain fundamentally limited in tasks that require high precision due to their single-inference paradigm. While test-time scaling approaches using external verifiers have shown promise, they require additional training and fail to generalize to unseen conditions. We propose Masking Distribution Guided Selection (MG-Select), a novel test-time scaling framework for VLAs that leverages the model's internal properties without requiring additional training or external modules. Our approach utilizes KL divergence from a reference action token distribution as a confidence metric for selecting the optimal action from multiple candidates. We introduce a reference distribution generated by the same VLA but with randomly masked states and language conditions as inputs, ensuring maximum uncertainty while remaining aligned with the target task distribution. Additionally, we propose a joint training strategy that enables the model to learn both conditional and unconditional distributions by applying dropout to state and language conditions, thereby further improving the quality of the reference distribution. Our experiments demonstrate that MG-Select achieves significant performance improvements, including a 28%/35% improvement in real-world in-distribution/out-of-distribution tasks, along with a 168% relative gain on RoboCasa pick-and-place tasks trained with 30 demonstrations.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "14 pages; 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.05681v1",
    "published_date": "2025-10-07 08:38:08 UTC",
    "updated_date": "2025-10-07 08:38:08 UTC"
  },
  {
    "arxiv_id": "2510.05678v1",
    "title": "Code-Switching In-Context Learning for Cross-Lingual Transfer of Large Language Models",
    "authors": [
      "Haneul Yoo",
      "Jiho Jin",
      "Kyunghyun Cho",
      "Alice Oh"
    ],
    "abstract": "While large language models (LLMs) exhibit strong multilingual abilities, their reliance on English as latent representations creates a translation barrier, where reasoning implicitly depends on internal translation into English. When this process fails, performance in non-English languages deteriorates sharply, limiting the inclusiveness of LLM-based applications. Existing cross-lingual in-context learning (X-ICL) methods primarily leverage monolingual demonstrations, often failing to mitigate this barrier and instead reinforcing it. In this work, we introduce code-switching in-context learning (CSICL), a simple yet effective prompting strategy that progressively transitions from a target language to English within demonstrations and instruction to facilitate their latent reasoning in English. By explicitly scaffolding the reasoning process through controlled code-switching, CSICL acts as an implicit linguistic bridge that enhances cross-lingual alignment and reduces reliance on the translation barrier. We conduct extensive experiments across 4 LLMs, 6 datasets, and 10 languages, spanning both knowledge-intensive and reasoning-oriented domains. Our results demonstrate that CSICL consistently outperforms X-ICL baselines, achieving gains of 3.1%p and 1.9%p in both target and unseen languages, respectively. The improvement is even more pronounced in low-resource settings, with gains of 14.7% in target and 5.3% in unseen languages. These findings establish code-switching as a principled and robust approach for overcoming the translation barrier during inference, moving LLMs toward more equitable and effective multilingual systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05678v1",
    "published_date": "2025-10-07 08:35:42 UTC",
    "updated_date": "2025-10-07 08:35:42 UTC"
  },
  {
    "arxiv_id": "2510.09655v1",
    "title": "Data Provenance Auditing of Fine-Tuned Large Language Models with a Text-Preserving Technique",
    "authors": [
      "Yanming Li",
      "Seifeddine Ghozzi",
      "Cédric Eichler",
      "Nicolas Anciaux",
      "Alexandra Bensamoun",
      "Lorena Gonzalez Manzano"
    ],
    "abstract": "We address the problem of auditing whether sensitive or copyrighted texts were used to fine-tune large language models (LLMs) under black-box access. Prior signals-verbatim regurgitation and membership inference-are unreliable at the level of individual documents or require altering the visible text. We introduce a text-preserving watermarking framework that embeds sequences of invisible Unicode characters into documents. Each watermark is split into a cue (embedded in odd chunks) and a reply (embedded in even chunks). At audit time, we submit prompts that contain only the cue; the presence of the corresponding reply in the model's output provides evidence of memorization consistent with training on the marked text. To obtain sound decisions, we compare the score of the published watermark against a held-out set of counterfactual watermarks and apply a ranking test with a provable false-positive-rate bound. The design is (i) minimally invasive (no visible text changes), (ii) scalable to many users and documents via a large watermark space and multi-watermark attribution, and (iii) robust to common passive transformations. We evaluate on open-weight LLMs and multiple text domains, analyzing regurgitation dynamics, sensitivity to training set size, and interference under multiple concurrent watermarks. Our results demonstrate reliable post-hoc provenance signals with bounded FPR under black-box access. We experimentally observe a failure rate of less than 0.1\\% when detecting a reply after fine-tuning with 50 marked documents. Conversely, no spurious reply was recovered in over 18,000 challenges, corresponding to a 100\\%TPR@0\\% FPR. Moreover, detection rates remain relatively stable as the dataset size increases, maintaining a per-document detection rate above 45\\% even when the marked collection accounts for less than 0.33\\% of the fine-tuning data.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09655v1",
    "published_date": "2025-10-07 08:34:08 UTC",
    "updated_date": "2025-10-07 08:34:08 UTC"
  },
  {
    "arxiv_id": "2510.05670v2",
    "title": "Quantifying the Accuracy-Interpretability Trade-Off in Concept-Based Sidechannel Models",
    "authors": [
      "David Debot",
      "Giuseppe Marra"
    ],
    "abstract": "Concept Bottleneck Models (CBNMs) are deep learning models that provide interpretability by enforcing a bottleneck layer where predictions are based exclusively on human-understandable concepts. However, this constraint also restricts information flow and often results in reduced predictive accuracy. Concept Sidechannel Models (CSMs) address this limitation by introducing a sidechannel that bypasses the bottleneck and carry additional task-relevant information. While this improves accuracy, it simultaneously compromises interpretability, as predictions may rely on uninterpretable representations transmitted through sidechannels. Currently, there exists no principled technique to control this fundamental trade-off. In this paper, we close this gap. First, we present a unified probabilistic concept sidechannel meta-model that subsumes existing CSMs as special cases. Building on this framework, we introduce the Sidechannel Independence Score (SIS), a metric that quantifies a CSM's reliance on its sidechannel by contrasting predictions made with and without sidechannel information. We propose SIS regularization, which explicitly penalizes sidechannel reliance to improve interpretability. Finally, we analyze how the expressivity of the predictor and the reliance of the sidechannel jointly shape interpretability, revealing inherent trade-offs across different CSM architectures. Empirical results show that state-of-the-art CSMs, when trained solely for accuracy, exhibit low representation interpretability, and that SIS regularization substantially improves their interpretability, intervenability, and the quality of learned interpretable task predictors. Our work provides both theoretical and practical tools for developing CSMs that balance accuracy and interpretability in a principled manner.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05670v2",
    "published_date": "2025-10-07 08:29:34 UTC",
    "updated_date": "2025-10-16 11:37:20 UTC"
  },
  {
    "arxiv_id": "2510.05664v1",
    "title": "Large Language Model-Based Uncertainty-Adjusted Label Extraction for Artificial Intelligence Model Development in Upper Extremity Radiography",
    "authors": [
      "Hanna Kreutzer",
      "Anne-Sophie Caselitz",
      "Thomas Dratsch",
      "Daniel Pinto dos Santos",
      "Christiane Kuhl",
      "Daniel Truhn",
      "Sven Nebelung"
    ],
    "abstract": "Objectives: To evaluate GPT-4o's ability to extract diagnostic labels (with uncertainty) from free-text radiology reports and to test how these labels affect multi-label image classification of musculoskeletal radiographs. Methods: This retrospective study included radiography series of the clavicle (n=1,170), elbow (n=3,755), and thumb (n=1,978). After anonymization, GPT-4o filled out structured templates by indicating imaging findings as present (\"true\"), absent (\"false\"), or \"uncertain.\" To assess the impact of label uncertainty, \"uncertain\" labels of the training and validation sets were automatically reassigned to \"true\" (inclusive) or \"false\" (exclusive). Label-image-pairs were used for multi-label classification using ResNet50. Label extraction accuracy was manually verified on internal (clavicle: n=233, elbow: n=745, thumb: n=393) and external test sets (n=300 for each). Performance was assessed using macro-averaged receiver operating characteristic (ROC) area under the curve (AUC), precision recall curves, sensitivity, specificity, and accuracy. AUCs were compared with the DeLong test. Results: Automatic extraction was correct in 98.6% (60,618 of 61,488) of labels in the test sets. Across anatomic regions, label-based model training yielded competitive performance measured by macro-averaged AUC values for inclusive (e.g., elbow: AUC=0.80 [range, 0.62-0.87]) and exclusive models (elbow: AUC=0.80 [range, 0.61-0.88]). Models generalized well on external datasets (elbow [inclusive]: AUC=0.79 [range, 0.61-0.87]; elbow [exclusive]: AUC=0.79 [range, 0.63-0.89]). No significant differences were observed across labeling strategies or datasets (p>=0.15). Conclusion: GPT-4o extracted labels from radiologic reports to train competitive multi-label classification models with high accuracy. Detected uncertainty in the radiologic reports did not influence the performance of these models.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "28 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.05664v1",
    "published_date": "2025-10-07 08:19:18 UTC",
    "updated_date": "2025-10-07 08:19:18 UTC"
  },
  {
    "arxiv_id": "2510.05649v1",
    "title": "Ocular-Induced Abnormal Head Posture: Diagnosis and Missing Data Imputation",
    "authors": [
      "Saja Al-Dabet",
      "Sherzod Turaev",
      "Nazar Zaki",
      "Arif O. Khan",
      "Luai Eldweik"
    ],
    "abstract": "Ocular-induced abnormal head posture (AHP) is a compensatory mechanism that arises from ocular misalignment conditions, such as strabismus, enabling patients to reduce diplopia and preserve binocular vision. Early diagnosis minimizes morbidity and secondary complications such as facial asymmetry; however, current clinical assessments remain largely subjective and are further complicated by incomplete medical records. This study addresses both challenges through two complementary deep learning frameworks. First, AHP-CADNet is a multi-level attention fusion framework for automated diagnosis that integrates ocular landmarks, head pose features, and structured clinical attributes to generate interpretable predictions. Second, a curriculum learning-based imputation framework is designed to mitigate missing data by progressively leveraging structured variables and unstructured clinical notes to enhance diagnostic robustness under realistic data conditions. Evaluation on the PoseGaze-AHP dataset demonstrates robust diagnostic performance. AHP-CADNet achieves 96.9-99.0 percent accuracy across classification tasks and low prediction errors for continuous variables, with MAE ranging from 0.103 to 0.199 and R2 exceeding 0.93. The imputation framework maintains high accuracy across all clinical variables (93.46-99.78 percent with PubMedBERT), with clinical dependency modeling yielding significant improvements (p < 0.001). These findings confirm the effectiveness of both frameworks for automated diagnosis and recovery from missing data in clinical settings.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05649v1",
    "published_date": "2025-10-07 07:51:59 UTC",
    "updated_date": "2025-10-07 07:51:59 UTC"
  },
  {
    "arxiv_id": "2510.05644v1",
    "title": "The African Languages Lab: A Collaborative Approach to Advancing Low-Resource African NLP",
    "authors": [
      "Sheriff Issaka",
      "Keyi Wang",
      "Yinka Ajibola",
      "Oluwatumininu Samuel-Ipaye",
      "Zhaoyi Zhang",
      "Nicte Aguillon Jimenez",
      "Evans Kofi Agyei",
      "Abraham Lin",
      "Rohan Ramachandran",
      "Sadick Abdul Mumin",
      "Faith Nchifor",
      "Mohammed Shuraim",
      "Lieqi Liu",
      "Erick Rosas Gonzalez",
      "Sylvester Kpei",
      "Jemimah Osei",
      "Carlene Ajeneza",
      "Persis Boateng",
      "Prisca Adwoa Dufie Yeboah",
      "Saadia Gabriel"
    ],
    "abstract": "Despite representing nearly one-third of the world's languages, African languages remain critically underserved by modern NLP technologies, with 88\\% classified as severely underrepresented or completely ignored in computational linguistics. We present the African Languages Lab (All Lab), a comprehensive research initiative that addresses this technological gap through systematic data collection, model development, and capacity building. Our contributions include: (1) a quality-controlled data collection pipeline, yielding the largest validated African multi-modal speech and text dataset spanning 40 languages with 19 billion tokens of monolingual text and 12,628 hours of aligned speech data; (2) extensive experimental validation demonstrating that our dataset, combined with fine-tuning, achieves substantial improvements over baseline models, averaging +23.69 ChrF++, +0.33 COMET, and +15.34 BLEU points across 31 evaluated languages; and (3) a structured research program that has successfully mentored fifteen early-career researchers, establishing sustainable local capacity. Our comparative evaluation against Google Translate reveals competitive performance in several languages while identifying areas that require continued development.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05644v1",
    "published_date": "2025-10-07 07:42:52 UTC",
    "updated_date": "2025-10-07 07:42:52 UTC"
  },
  {
    "arxiv_id": "2510.05637v1",
    "title": "From Neural Activity to Computation: Biological Reservoirs for Pattern Recognition in Digit Classification",
    "authors": [
      "Ludovico Iannello",
      "Luca Ciampi",
      "Fabrizio Tonelli",
      "Gabriele Lagani",
      "Lucio Maria Calcagnile",
      "Federico Cremisi",
      "Angelo Di Garbo",
      "Giuseppe Amato"
    ],
    "abstract": "In this paper, we present a biologically grounded approach to reservoir computing (RC), in which a network of cultured biological neurons serves as the reservoir substrate. This system, referred to as biological reservoir computing (BRC), replaces artificial recurrent units with the spontaneous and evoked activity of living neurons. A multi-electrode array (MEA) enables simultaneous stimulation and readout across multiple sites: inputs are delivered through a subset of electrodes, while the remaining ones capture the resulting neural responses, mapping input patterns into a high-dimensional biological feature space. We evaluate the system through a case study on digit classification using a custom dataset. Input images are encoded and delivered to the biological reservoir via electrical stimulation, and the corresponding neural activity is used to train a simple linear classifier. To contextualize the performance of the biological system, we also include a comparison with a standard artificial reservoir trained on the same task. The results indicate that the biological reservoir can effectively support classification, highlighting its potential as a viable and interpretable computational substrate. We believe this work contributes to the broader effort of integrating biological principles into machine learning and aligns with the goals of human-inspired vision by exploring how living neural systems can inform the design of efficient and biologically plausible models.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.NE",
    "comment": "Accepted at HiCV@ICCV2025",
    "pdf_url": "https://arxiv.org/pdf/2510.05637v1",
    "published_date": "2025-10-07 07:36:36 UTC",
    "updated_date": "2025-10-07 07:36:36 UTC"
  },
  {
    "arxiv_id": "2510.05633v1",
    "title": "Beyond Spectral Peaks: Interpreting the Cues Behind Synthetic Image Detection",
    "authors": [
      "Sara Mandelli",
      "Diego Vila-Portela",
      "David Vázquez-Padín",
      "Paolo Bestagini",
      "Fernando Pérez-González"
    ],
    "abstract": "Over the years, the forensics community has proposed several deep learning-based detectors to mitigate the risks of generative AI. Recently, frequency-domain artifacts (particularly periodic peaks in the magnitude spectrum), have received significant attention, as they have been often considered a strong indicator of synthetic image generation. However, state-of-the-art detectors are typically used as black-boxes, and it still remains unclear whether they truly rely on these peaks. This limits their interpretability and trust. In this work, we conduct a systematic study to address this question. We propose a strategy to remove spectral peaks from images and analyze the impact of this operation on several detectors. In addition, we introduce a simple linear detector that relies exclusively on frequency peaks, providing a fully interpretable baseline free from the confounding influence of deep learning. Our findings reveal that most detectors are not fundamentally dependent on spectral peaks, challenging a widespread assumption in the field and paving the way for more transparent and reliable forensic tools.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05633v1",
    "published_date": "2025-10-07 07:33:47 UTC",
    "updated_date": "2025-10-07 07:33:47 UTC"
  },
  {
    "arxiv_id": "2510.05625v1",
    "title": "Generative AI-Driven Hierarchical Multi-Agent Framework for Zero-Touch Optical Networks",
    "authors": [
      "Yao Zhang",
      "Yuchen Song",
      "Shengnan Li",
      "Yan Shi",
      "Shikui Shen",
      "Xiongyan Tang",
      "Min Zhang",
      "Danshi Wang"
    ],
    "abstract": "The rapid development of Generative Artificial Intelligence (GenAI) has catalyzed a transformative technological revolution across all walks of life. As the backbone of wideband communication, optical networks are expecting high-level autonomous operation and zero-touch management to accommodate their expanding network scales and escalating transmission bandwidth. The integration of GenAI is deemed as the pivotal solution for realizing zero-touch optical networks. However, the lifecycle management of optical networks involves a multitude of tasks and necessitates seamless collaboration across multiple layers, which poses significant challenges to the existing single-agent GenAI systems. In this paper, we propose a GenAI-driven hierarchical multi-agent framework designed to streamline multi-task autonomous execution for zero-touch optical networks. We present the architecture, implementation, and applications of this framework. A field-deployed mesh network is utilized to demonstrate three typical scenarios throughout the lifecycle of optical network: quality of transmission estimation in the planning stage, dynamic channel adding/dropping in the operation stage, and system capacity increase in the upgrade stage. The case studies, illustrate the capabilities of multi-agent framework in multi-task allocation, coordination, execution, evaluation, and summarization. This work provides a promising approach for the future development of intelligent, efficient, and collaborative network management solutions, paving the way for more specialized and adaptive zero-touch optical networks.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.CL",
      "cs.MA",
      "eess.SY"
    ],
    "primary_category": "cs.NI",
    "comment": "7 pages,6 figures, Accepted by lEEE Communications Magazine, Open call",
    "pdf_url": "https://arxiv.org/pdf/2510.05625v1",
    "published_date": "2025-10-07 07:12:52 UTC",
    "updated_date": "2025-10-07 07:12:52 UTC"
  },
  {
    "arxiv_id": "2511.17514v1",
    "title": "XAI-on-RAN: Explainable, AI-native, and GPU-Accelerated RAN Towards 6G",
    "authors": [
      "Osman Tugay Basaran",
      "Falko Dressler"
    ],
    "abstract": "Artificial intelligence (AI)-native radio access networks (RANs) will serve vertical industries with stringent requirements: smart grids, autonomous vehicles, remote healthcare, industrial automation, etc. To achieve these requirements, modern 5G/6G design increasingly leverage AI for network optimization, but the opacity of AI decisions poses risks in mission-critical domains. These use cases are often delivered via non-public networks (NPNs) or dedicated network slices, where reliability and safety are vital. In this paper, we motivate the need for transparent and trustworthy AI in high-stakes communications (e.g., healthcare, industrial automation, and robotics) by drawing on 3rd generation partnership project (3GPP)'s vision for non-public networks. We design a mathematical framework to model the trade-offs between transparency (explanation fidelity and fairness), latency, and graphics processing unit (GPU) utilization in deploying explainable AI (XAI) models. Empirical evaluations demonstrate that our proposed hybrid XAI model xAI-Native, consistently surpasses conventional baseline models in performance.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: AI and ML for Next-Generation Wireless Communications and Networking (AI4NextG)",
    "pdf_url": "https://arxiv.org/pdf/2511.17514v1",
    "published_date": "2025-10-07 07:12:47 UTC",
    "updated_date": "2025-10-07 07:12:47 UTC"
  },
  {
    "arxiv_id": "2510.05620v2",
    "title": "Monte Carlo-Type Neural Operator for Differential Equations",
    "authors": [
      "Salah Eddine Choutri",
      "Prajwal Chauhan",
      "Othmane Mazhar",
      "Saif Eddin Jabari"
    ],
    "abstract": "The Monte Carlo-type Neural Operator (MCNO) introduces a framework for learning solution operators of one-dimensional partial differential equations (PDEs) by directly learning the kernel function and approximating the associated integral operator using a Monte Carlo-type approach. Unlike Fourier Neural Operators (FNOs), which rely on spectral representations and assume translation-invariant kernels, MCNO makes no such assumptions. The kernel is represented as a learnable tensor over sampled input-output pairs, and sampling is performed once, uniformly at random from a discretized grid. This design enables generalization across multiple grid resolutions without relying on fixed global basis functions or repeated sampling during training, while an interpolation step maps between arbitrary input and output grids to further enhance flexibility. Experiments on standard 1D PDE benchmarks show that MCNO achieves competitive accuracy with efficient computational cost. We also provide a theoretical analysis proving that the Monte Carlo estimator yields a bounded bias and variance under mild regularity assumptions. This result holds in any spatial dimension, suggesting that MCNO may extend naturally beyond one-dimensional problems. More broadly, this work explores how Monte Carlo-type integration can be incorporated into neural operator frameworks for continuous-domain PDEs, providing a theoretically supported alternative to spectral methods (such as FNO) and to graph-based Monte Carlo approaches (such as the Graph Kernel Neural Operator, GNO).",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.NA",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05620v2",
    "published_date": "2025-10-07 07:07:04 UTC",
    "updated_date": "2025-12-02 22:52:17 UTC"
  },
  {
    "arxiv_id": "2510.19829v1",
    "title": "SSL-SE-EEG: A Framework for Robust Learning from Unlabeled EEG Data with Self-Supervised Learning and Squeeze-Excitation Networks",
    "authors": [
      "Meghna Roy Chowdhury",
      "Yi Ding",
      "Shreyas Sen"
    ],
    "abstract": "Electroencephalography (EEG) plays a crucial role in brain-computer interfaces (BCIs) and neurological diagnostics, but its real-world deployment faces challenges due to noise artifacts, missing data, and high annotation costs. We introduce SSL-SE-EEG, a framework that integrates Self-Supervised Learning (SSL) with Squeeze-and-Excitation Networks (SE-Nets) to enhance feature extraction, improve noise robustness, and reduce reliance on labeled data. Unlike conventional EEG processing techniques, SSL-SE-EEG} transforms EEG signals into structured 2D image representations, suitable for deep learning. Experimental validation on MindBigData, TUH-AB, SEED-IV and BCI-IV datasets demonstrates state-of-the-art accuracy (91% in MindBigData, 85% in TUH-AB), making it well-suited for real-time BCI applications. By enabling low-power, scalable EEG processing, SSL-SE-EEG presents a promising solution for biomedical signal analysis, neural engineering, and next-generation BCIs.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "6 figures, 2 tables, 8 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.19829v1",
    "published_date": "2025-10-07 06:37:34 UTC",
    "updated_date": "2025-10-07 06:37:34 UTC"
  },
  {
    "arxiv_id": "2510.05613v2",
    "title": "PointNSP: Autoregressive 3D Point Cloud Generation with Next-Scale Level-of-Detail Prediction",
    "authors": [
      "Ziqiao Meng",
      "Qichao Wang",
      "Zhiyang Dou",
      "Zixing Song",
      "Zhipeng Zhou",
      "Irwin King",
      "Peilin Zhao"
    ],
    "abstract": "Autoregressive point cloud generation has long lagged behind diffusion-based approaches in quality. The performance gap stems from the fact that autoregressive models impose an artificial ordering on inherently unordered point sets, forcing shape generation to proceed as a sequence of local predictions. This sequential bias emphasizes short-range continuity but undermines the model's capacity to capture long-range dependencies, hindering its ability to enforce global structural properties such as symmetry, consistent topology, and large-scale geometric regularities. Inspired by the level-of-detail (LOD) principle in shape modeling, we propose PointNSP, a coarse-to-fine generative framework that preserves global shape structure at low resolutions and progressively refines fine-grained geometry at higher scales through a next-scale prediction paradigm. This multi-scale factorization aligns the autoregressive objective with the permutation-invariant nature of point sets, enabling rich intra-scale interactions while avoiding brittle fixed orderings. Experiments on ShapeNet show that PointNSP establishes state-of-the-art (SOTA) generation quality for the first time within the autoregressive paradigm. In addition, it surpasses strong diffusion-based baselines in parameter, training, and inference efficiency. Finally, in dense generation with 8,192 points, PointNSP's advantages become even more pronounced, underscoring its scalability potential.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "This work was intended as a replacement of arXiv:2503.08594 and any subsequent updates will appear there",
    "pdf_url": "https://arxiv.org/pdf/2510.05613v2",
    "published_date": "2025-10-07 06:31:02 UTC",
    "updated_date": "2025-11-26 05:49:58 UTC"
  },
  {
    "arxiv_id": "2510.05611v2",
    "title": "MADIAVE: Multi-Agent Debate for Implicit Attribute Value Extraction",
    "authors": [
      "Wei-Chieh Huang",
      "Cornelia Caragea"
    ],
    "abstract": "Implicit Attribute Value Extraction (AVE) is essential for accurately representing products in e-commerce, as it infers latent attributes from multimodal data. Despite advances in multimodal large language models (MLLMs), implicit AVE remains challenging due to the complexity of multidimensional data and gaps in vision-text understanding. In this work, we introduce MADIAVE, a multi-agent debate framework that employs multiple MLLM agents to iteratively refine inferences. Through a series of debate rounds, agents verify and update each other's responses, thereby improving inference performance and robustness. Experiments on the ImplicitAVE dataset demonstrate that even a few rounds of debate significantly boost accuracy, especially for attributes with initially low performance. We systematically evaluate various debate configurations, including identical or different MLLM agents, and analyze how debate rounds affect convergence dynamics. Our findings highlight the potential of multi-agent debate strategies to address the limitations of single-agent approaches and offer a scalable solution for implicit AVE in multimodal e-commerce.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by EACL 2026 (Findings)",
    "pdf_url": "https://arxiv.org/pdf/2510.05611v2",
    "published_date": "2025-10-07 06:27:42 UTC",
    "updated_date": "2026-01-15 19:56:07 UTC"
  },
  {
    "arxiv_id": "2510.05609v1",
    "title": "HOI-R1: Exploring the Potential of Multimodal Large Language Models for Human-Object Interaction Detection",
    "authors": [
      "Junwen Chen",
      "Peilin Xiong",
      "Keiji Yanai"
    ],
    "abstract": "Recent Human-object interaction detection (HOID) methods highly require prior knowledge from VLMs to enhance the interaction recognition capabilities. The training strategies and model architectures for connecting the knowledge from VLMs to the HOI instance representations from the object detector are challenging, and the whole framework is complex for further development or application. On the other hand, the inherent reasoning abilities of MLLMs on human-object interaction detection are under-explored. Inspired by the recent success of training MLLMs with reinforcement learning (RL) methods, we propose HOI-R1 and first explore the potential of the language model on the HOID task without any additional detection modules. We introduce an HOI reasoning process and HOID reward functions to solve the HOID task by pure text. The results on the HICO-DET dataset show that HOI-R1 achieves 2x the accuracy of the baseline with great generalization ability. The source code is available at https://github.com/cjw2021/HOI-R1.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05609v1",
    "published_date": "2025-10-07 06:16:02 UTC",
    "updated_date": "2025-10-07 06:16:02 UTC"
  },
  {
    "arxiv_id": "2510.05605v1",
    "title": "AutoPentester: An LLM Agent-based Framework for Automated Pentesting",
    "authors": [
      "Yasod Ginige",
      "Akila Niroshan",
      "Sajal Jain",
      "Suranga Seneviratne"
    ],
    "abstract": "Penetration testing and vulnerability assessment are essential industry practices for safeguarding computer systems. As cyber threats grow in scale and complexity, the demand for pentesting has surged, surpassing the capacity of human professionals to meet it effectively. With advances in AI, particularly Large Language Models (LLMs), there have been attempts to automate the pentesting process. However, existing tools such as PentestGPT are still semi-manual, requiring significant professional human interaction to conduct pentests. To this end, we propose a novel LLM agent-based framework, AutoPentester, which automates the pentesting process. Given a target IP, AutoPentester automatically conducts pentesting steps using common security tools in an iterative process. It can dynamically generate attack strategies based on the tool outputs from the previous iteration, mimicking the human pentester approach. We evaluate AutoPentester using Hack The Box and custom-made VMs, comparing the results with the state-of-the-art PentestGPT. Results show that AutoPentester achieves a 27.0% better subtask completion rate and 39.5% more vulnerability coverage with fewer steps. Most importantly, it requires significantly fewer human interactions and interventions compared to PentestGPT. Furthermore, we recruit a group of security industry professional volunteers for a user survey and perform a qualitative analysis to evaluate AutoPentester against industry practices and compare it with PentestGPT. On average, AutoPentester received a score of 3.93 out of 5 based on user reviews, which was 19.8% higher than PentestGPT.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "IEEE TrustCom 2025 10 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.05605v1",
    "published_date": "2025-10-07 06:02:26 UTC",
    "updated_date": "2025-10-07 06:02:26 UTC"
  },
  {
    "arxiv_id": "2510.05598v1",
    "title": "AgentDR Dynamic Recommendation with Implicit Item-Item Relations via LLM-based Agents",
    "authors": [
      "Mingdai Yang",
      "Nurendra Choudhary",
      "Jiangshu Du",
      "Edward W. Huang",
      "Philip S. Yu",
      "Karthik Subbian",
      "Danai Kourta"
    ],
    "abstract": "Recent agent-based recommendation frameworks aim to simulate user behaviors by incorporating memory mechanisms and prompting strategies, but they struggle with hallucinating non-existent items and full-catalog ranking. Besides, a largely underexplored opportunity lies in leveraging LLMs'commonsense reasoning to capture user intent through substitute and complement relationships between items, which are usually implicit in datasets and difficult for traditional ID-based recommenders to capture. In this work, we propose a novel LLM-agent framework, AgenDR, which bridges LLM reasoning with scalable recommendation tools. Our approach delegates full-ranking tasks to traditional models while utilizing LLMs to (i) integrate multiple recommendation outputs based on personalized tool suitability and (ii) reason over substitute and complement relationships grounded in user history. This design mitigates hallucination, scales to large catalogs, and enhances recommendation relevance through relational reasoning. Through extensive experiments on three public grocery datasets, we show that our framework achieves superior full-ranking performance, yielding on average a twofold improvement over its underlying tools. We also introduce a new LLM-based evaluation metric that jointly measures semantic alignment and ranking correctness.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05598v1",
    "published_date": "2025-10-07 05:48:05 UTC",
    "updated_date": "2025-10-07 05:48:05 UTC"
  },
  {
    "arxiv_id": "2510.05596v1",
    "title": "From Agentification to Self-Evolving Agentic AI for Wireless Networks: Concepts, Approaches, and Future Research Directions",
    "authors": [
      "Changyuan Zhao",
      "Ruichen Zhang",
      "Jiacheng Wang",
      "Dusit Niyato",
      "Geng Sun",
      "Xianbin Wang",
      "Shiwen Mao",
      "Abbas Jamalipour"
    ],
    "abstract": "Self-evolving agentic artificial intelligence (AI) offers a new paradigm for future wireless systems by enabling autonomous agents to continually adapt and improve without human intervention. Unlike static AI models, self-evolving agents embed an autonomous evolution cycle that updates models, tools, and workflows in response to environmental dynamics. This paper presents a comprehensive overview of self-evolving agentic AI, highlighting its layered architecture, life cycle, and key techniques, including tool intelligence, workflow optimization, self-reflection, and evolutionary learning. We further propose a multi-agent cooperative self-evolving agentic AI framework, where multiple large language models (LLMs) are assigned role-specialized prompts under the coordination of a supervisor agent. Through structured dialogue, iterative feedback, and systematic validation, the system autonomously executes the entire life cycle without human intervention. A case study on antenna evolution in low-altitude wireless networks (LAWNs) demonstrates how the framework autonomously upgrades fixed antenna optimization into movable antenna optimization. Experimental results show that the proposed self-evolving agentic AI autonomously improves beam gain and restores degraded performance by up to 52.02%, consistently surpassing the fixed baseline with little to no human intervention and validating its adaptability and robustness for next-generation wireless intelligence.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "7 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.05596v1",
    "published_date": "2025-10-07 05:45:25 UTC",
    "updated_date": "2025-10-07 05:45:25 UTC"
  },
  {
    "arxiv_id": "2510.06291v1",
    "title": "Traj-Transformer: Diffusion Models with Transformer for GPS Trajectory Generation",
    "authors": [
      "Zhiyang Zhang",
      "Ningcong Chen",
      "Xin Zhang",
      "Yanhua Li",
      "Shen Su",
      "Hui Lu",
      "Jun Luo"
    ],
    "abstract": "The widespread use of GPS devices has driven advances in spatiotemporal data mining, enabling machine learning models to simulate human decision making and generate realistic trajectories, addressing both data collection costs and privacy concerns. Recent studies have shown the promise of diffusion models for high-quality trajectory generation. However, most existing methods rely on convolution based architectures (e.g. UNet) to predict noise during the diffusion process, which often results in notable deviations and the loss of fine-grained street-level details due to limited model capacity. In this paper, we propose Trajectory Transformer, a novel model that employs a transformer backbone for both conditional information embedding and noise prediction. We explore two GPS coordinate embedding strategies, location embedding and longitude-latitude embedding, and analyze model performance at different scales. Experiments on two real-world datasets demonstrate that Trajectory Transformer significantly enhances generation quality and effectively alleviates the deviation issues observed in prior approaches.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06291v1",
    "published_date": "2025-10-07 05:41:09 UTC",
    "updated_date": "2025-10-07 05:41:09 UTC"
  },
  {
    "arxiv_id": "2510.05593v1",
    "title": "Improving Chain-of-Thought Efficiency for Autoregressive Image Generation",
    "authors": [
      "Zeqi Gu",
      "Markos Georgopoulos",
      "Xiaoliang Dai",
      "Marjan Ghazvininejad",
      "Chu Wang",
      "Felix Juefei-Xu",
      "Kunpeng Li",
      "Yujun Shi",
      "Zecheng He",
      "Zijian He",
      "Jiawei Zhou",
      "Abe Davis",
      "Jialiang Wang"
    ],
    "abstract": "Autoregressive multimodal large language models have recently gained popularity for image generation, driven by advances in foundation models. To enhance alignment and detail, newer approaches employ chain-of-thought (CoT) reasoning, expanding user inputs into elaborated prompts prior to image synthesis. However, this strategy can introduce unnecessary redundancy -- a phenomenon we call visual overthinking -- which increases computational costs and can introduce details that contradict the original prompt. In this work, we explore how to generate more concise CoT sequences for more efficient image generation. We introduce ShortCoTI, a lightweight optimization framework that encourages more concise CoT while preserving output image quality. ShortCoTI rewards more concise prompts with an adaptive function that scales according to an estimated difficulty for each task. Incorporating this reward into a reinforcement learning paradigm reduces prompt reasoning length by 54% while maintaining or slightly improving quality metrics across multiple benchmarks (T2I-CompBench, GenEval). Qualitative analysis shows that our method eliminates verbose explanations and repetitive refinements, producing reasoning prompts that are both concise and semantically rich. As a result, ShortCoTI improves computational efficiency without compromising the fidelity or visual appeal of generated images.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05593v1",
    "published_date": "2025-10-07 05:40:43 UTC",
    "updated_date": "2025-10-07 05:40:43 UTC"
  },
  {
    "arxiv_id": "2510.05592v1",
    "title": "In-the-Flow Agentic System Optimization for Effective Planning and Tool Use",
    "authors": [
      "Zhuofeng Li",
      "Haoxiang Zhang",
      "Seungju Han",
      "Sheng Liu",
      "Jianwen Xie",
      "Yu Zhang",
      "Yejin Choi",
      "James Zou",
      "Pan Lu"
    ],
    "abstract": "Outcome-driven reinforcement learning has advanced reasoning in large language models (LLMs), but prevailing tool-augmented approaches train a single, monolithic policy that interleaves thoughts and tool calls under full context; this scales poorly with long horizons and diverse tools and generalizes weakly to new scenarios. Agentic systems offer a promising alternative by decomposing work across specialized modules, yet most remain training-free or rely on offline training decoupled from the live dynamics of multi-turn interaction. We introduce AgentFlow, a trainable, in-the-flow agentic framework that coordinates four modules (planner, executor, verifier, generator) through an evolving memory and directly optimizes its planner inside the multi-turn loop. To train on-policy in live environments, we propose Flow-based Group Refined Policy Optimization (Flow-GRPO), which tackles long-horizon, sparse-reward credit assignment by converting multi-turn optimization into a sequence of tractable single-turn policy updates. It broadcasts a single, verifiable trajectory-level outcome to every turn to align local planner decisions with global success and stabilizes learning with group-normalized advantages. Across ten benchmarks, AgentFlow with a 7B-scale backbone outperforms top-performing baselines with average accuracy gains of 14.9% on search, 14.0% on agentic, 14.5% on mathematical, and 4.1% on scientific tasks, even surpassing larger proprietary models like GPT-4o. Further analyses confirm the benefits of in-the-flow optimization, showing improved planning, enhanced tool-calling reliability, and positive scaling with model size and reasoning turns.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "45 pages, 12 figures. Project website: https://agentflow.stanford.edu/",
    "pdf_url": "https://arxiv.org/pdf/2510.05592v1",
    "published_date": "2025-10-07 05:32:44 UTC",
    "updated_date": "2025-10-07 05:32:44 UTC"
  },
  {
    "arxiv_id": "2510.05589v2",
    "title": "Deciphering Invariant Feature Decoupling in Source-free Time Series Forecasting with Proxy Denoising",
    "authors": [
      "Kangjia Yan",
      "Chenxi Liu",
      "Hao Miao",
      "Xinle Wu",
      "Yan Zhao",
      "Chenjuan Guo",
      "Bin Yang"
    ],
    "abstract": "The proliferation of mobile devices generates a massive volume of time series across various domains, where effective time series forecasting enables a variety of real-world applications. This study focuses on a new problem of source-free domain adaptation for time series forecasting. It aims to adapt a pretrained model from sufficient source time series to the sparse target time series domain without access to the source data, embracing data protection regulations. To achieve this, we propose TimePD, the first source-free time series forecasting framework with proxy denoising, where large language models (LLMs) are employed to benefit from their generalization capabilities. Specifically, TimePD consists of three key components: (1) dual-branch invariant disentangled feature learning that enforces representation- and gradient-wise invariance by means of season-trend decomposition; (2) lightweight, parameter-free proxy denoising that dynamically calibrates systematic biases of LLMs; and (3) knowledge distillation that bidirectionally aligns the denoised prediction and the original target prediction. Extensive experiments on real-world datasets offer insight into the effectiveness of the proposed TimePD, outperforming SOTA baselines by 9.3% on average.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05589v2",
    "published_date": "2025-10-07 05:29:18 UTC",
    "updated_date": "2025-10-31 05:24:27 UTC"
  },
  {
    "arxiv_id": "2510.06290v1",
    "title": "Soft-Evidence Fused Graph Neural Network for Cancer Driver Gene Identification across Multi-View Biological Graphs",
    "authors": [
      "Bang Chen",
      "Lijun Guo",
      "Houli Fan",
      "Wentao He",
      "Rong Zhang"
    ],
    "abstract": "Identifying cancer driver genes (CDGs) is essential for understanding cancer mechanisms and developing targeted therapies. Graph neural networks (GNNs) have recently been employed to identify CDGs by capturing patterns in biological interaction networks. However, most GNN-based approaches rely on a single protein-protein interaction (PPI) network, ignoring complementary information from other biological networks. Some studies integrate multiple networks by aligning features with consistency constraints to learn unified gene representations for CDG identification. However, such representation-level fusion often assumes congruent gene relationships across networks, which may overlook network heterogeneity and introduce conflicting information. To address this, we propose Soft-Evidence Fusion Graph Neural Network (SEFGNN), a novel framework for CDG identification across multiple networks at the decision level. Instead of enforcing feature-level consistency, SEFGNN treats each biological network as an independent evidence source and performs uncertainty-aware fusion at the decision level using Dempster-Shafer Theory (DST). To alleviate the risk of overconfidence from DST, we further introduce a Soft Evidence Smoothing (SES) module that improves ranking stability while preserving discriminative performance. Experiments on three cancer datasets show that SEFGNN consistently outperforms state-of-the-art baselines and exhibits strong potential in discovering novel CDGs.",
    "categories": [
      "q-bio.GN",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.GN",
    "comment": "8pages",
    "pdf_url": "https://arxiv.org/pdf/2510.06290v1",
    "published_date": "2025-10-07 05:20:57 UTC",
    "updated_date": "2025-10-07 05:20:57 UTC"
  },
  {
    "arxiv_id": "2510.05580v1",
    "title": "MetaVLA: Unified Meta Co-training For Efficient Embodied Adaption",
    "authors": [
      "Chen Li",
      "Zhantao Yang",
      "Han Zhang",
      "Fangyi Chen",
      "Chenchen Zhu",
      "Anudeepsekhar Bolimera",
      "Marios Savvides"
    ],
    "abstract": "Vision-Language-Action (VLA) models show promise in embodied reasoning, yet remain far from true generalists-they often require task-specific fine-tuning, and generalize poorly to unseen tasks. We propose MetaVLA, a unified, backbone-agnostic post-training framework for efficient and scalable alignment. MetaVLA introduces Context-Aware Meta Co-Training, which consolidates diverse target tasks into a single fine-tuning stage while leveraging structurally diverse auxiliary tasks to improve in-domain generalization. Unlike naive multi-task SFT, MetaVLA integrates a lightweight meta-learning mechanism-derived from Attentive Neural Processes-to enable rapid adaptation from diverse contexts with minimal architectural change or inference overhead. On the LIBERO benchmark, MetaVLA with six auxiliary tasks outperforms OpenVLA by up to 8.0% on long-horizon tasks, reduces training steps from 240K to 75K, and cuts GPU time by ~76%. These results show that scalable, low-resource post-training is achievable-paving the way toward general-purpose embodied agents. Code will be available.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05580v1",
    "published_date": "2025-10-07 04:54:39 UTC",
    "updated_date": "2025-10-07 04:54:39 UTC"
  },
  {
    "arxiv_id": "2510.05577v1",
    "title": "Mission Impossible: Feedback-Guided Dynamic Interactive Planning for Improving Reasoning on LLMs",
    "authors": [
      "Dong Yan",
      "Gaochen Wu",
      "Bowen Zhou"
    ],
    "abstract": "Recent advancements in language agents have led to significant improvements in multi-hop reasoning tasks. However, existing approaches often struggle with handling open-domain problems, which require massive information retrieval due to their reliance on a fixed sequence of actions. To address this, we propose Feedback-Guided Dynamic Interactive Planning (FGDIP), a novel framework tailored to enhance reasoning in LLMs by utilizing dynamic and adaptive strategies for information exploration in open-domain multi-hop reasoning tasks. Our approach begins by identifying key entities relevant to the problem, which serve as the initial nodes in the reasoning process. From these initial nodes, we then generate reasoning child nodes with the process being refined through a combination of historical error analysis and real-time feedback, which allows the framework to dynamically adjust and optimize its reasoning strategies. By integrating depth-first search with an innovative node generation technique, our framework adapts based on both prior error paths and concurrently generated nodes at the same hierarchical level. This dynamic strategy effectively expands the search space while ensuring the reasoning process systematically converges toward accurate solutions. Experimental results show that FGDIP achieved up to 54.47% F1 score on the HotpotQA dataset and 70.05% on the StrategyQA dataset, surpassing the best baseline by 5.03% and 7.25% respectively, highlighting its versatility and potential to enhance language agents in multi-hop reasoning tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05577v1",
    "published_date": "2025-10-07 04:46:58 UTC",
    "updated_date": "2025-10-07 04:46:58 UTC"
  },
  {
    "arxiv_id": "2510.06288v1",
    "title": "BuilderBench -- A benchmark for generalist agents",
    "authors": [
      "Raj Ghugare",
      "Catherine Ji",
      "Kathryn Wantlin",
      "Jin Schofield",
      "Benjamin Eysenbach"
    ],
    "abstract": "Today's AI models learn primarily through mimicry and sharpening, so it is not surprising that they struggle to solve problems beyond the limits set by existing data. To solve novel problems, agents should acquire skills for exploring and learning through experience. Finding a scalable learning mechanism for developing agents that learn through interaction remains a major open problem. In this work, we introduce BuilderBench, a benchmark to accelerate research into agent pre-training that centers open-ended exploration. BuilderBench requires agents to learn how to build any structure using blocks. BuilderBench is equipped with $(1)$ a hardware accelerated simulator of a robotic agent interacting with various physical blocks, and $(2)$ a task-suite with over 42 diverse target structures that are carefully curated to test an understanding of physics, mathematics, and long-horizon planning. During training, agents have to explore and learn general principles about the environment without any external supervision. During evaluation, agents have to build the unseen target structures from the task suite. Solving these tasks requires a sort of \\emph{embodied reasoning} that is not reflected in words but rather in actions, experimenting with different strategies and piecing them together. Our experiments show that many of these tasks challenge the current iteration of algorithms. Hence, we also provide a ``training wheels'' protocol, in which agents are trained and evaluated to build a single target structure from the task suite. Finally, we provide single-file implementations of six different algorithms as a reference point for researchers.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Project page: https://rajghugare19.github.io/builderbench and Code: https://github.com/rajghugare19/builderbench",
    "pdf_url": "https://arxiv.org/pdf/2510.06288v1",
    "published_date": "2025-10-07 04:23:48 UTC",
    "updated_date": "2025-10-07 04:23:48 UTC"
  },
  {
    "arxiv_id": "2510.05566v1",
    "title": "Domain-Shift-Aware Conformal Prediction for Large Language Models",
    "authors": [
      "Zhexiao Lin",
      "Yuanyuan Li",
      "Neeraj Sarna",
      "Yuanyuan Gao",
      "Michael von Gablenz"
    ],
    "abstract": "Large language models have achieved impressive performance across diverse tasks. However, their tendency to produce overconfident and factually incorrect outputs, known as hallucinations, poses risks in real world applications. Conformal prediction provides finite-sample, distribution-free coverage guarantees, but standard conformal prediction breaks down under domain shift, often leading to under-coverage and unreliable prediction sets. We propose a new framework called Domain-Shift-Aware Conformal Prediction (DS-CP). Our framework adapts conformal prediction to large language models under domain shift, by systematically reweighting calibration samples based on their proximity to the test prompt, thereby preserving validity while enhancing adaptivity. Our theoretical analysis and experiments on the MMLU benchmark demonstrate that the proposed method delivers more reliable coverage than standard conformal prediction, especially under substantial distribution shifts, while maintaining efficiency. This provides a practical step toward trustworthy uncertainty quantification for large language models in real-world deployment.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "stat.AP"
    ],
    "primary_category": "stat.ML",
    "comment": "26 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.05566v1",
    "published_date": "2025-10-07 04:22:06 UTC",
    "updated_date": "2025-10-07 04:22:06 UTC"
  },
  {
    "arxiv_id": "2510.05562v1",
    "title": "Generative Dynamic Graph Representation Learning for Conspiracy Spoofing Detection",
    "authors": [
      "Sheng Xiang",
      "Yidong Jiang",
      "Yunting Chen",
      "Dawei Cheng",
      "Guoping Zhao",
      "Changjun Jiang"
    ],
    "abstract": "Spoofing detection in financial trading is crucial, especially for identifying complex behaviors such as conspiracy spoofing. Traditional machine-learning approaches primarily focus on isolated node features, often overlooking the broader context of interconnected nodes. Graph-based techniques, particularly Graph Neural Networks (GNNs), have advanced the field by leveraging relational information effectively. However, in real-world spoofing detection datasets, trading behaviors exhibit dynamic, irregular patterns. Existing spoofing detection methods, though effective in some scenarios, struggle to capture the complexity of dynamic and diverse, evolving inter-node relationships. To address these challenges, we propose a novel framework called the Generative Dynamic Graph Model (GDGM), which models dynamic trading behaviors and the relationships among nodes to learn representations for conspiracy spoofing detection. Specifically, our approach incorporates the generative dynamic latent space to capture the temporal patterns and evolving market conditions. Raw trading data is first converted into time-stamped sequences. Then we model trading behaviors using the neural ordinary differential equations and gated recurrent units, to generate the representation incorporating temporal dynamics of spoofing patterns. Furthermore, pseudo-label generation and heterogeneous aggregation techniques are employed to gather relevant information and enhance the detection performance for conspiratorial spoofing behaviors. Experiments conducted on spoofing detection datasets demonstrate that our approach outperforms state-of-the-art models in detection accuracy. Additionally, our spoofing detection system has been successfully deployed in one of the largest global trading markets, further validating the practical applicability and performance of the proposed method.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 5 figures, ACM the web conference 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.05562v1",
    "published_date": "2025-10-07 04:16:12 UTC",
    "updated_date": "2025-10-07 04:16:12 UTC"
  },
  {
    "arxiv_id": "2510.14991v1",
    "title": "The Role of Federated Learning in Improving Financial Security: A Survey",
    "authors": [
      "Cade Houston Kennedy",
      "Amr Hilal",
      "Morteza Momeni"
    ],
    "abstract": "With the growth of digital financial systems, robust security and privacy have become a concern for financial institutions. Even though traditional machine learning models have shown to be effective in fraud detections, they often compromise user data by requiring centralized access to sensitive information. In IoT-enabled financial endpoints such as ATMs and POS Systems that regularly produce sensitive data that is sent over the network. Federated Learning (FL) offers a privacy-preserving, decentralized model training across institutions without sharing raw data. FL enables cross-silo collaboration among banks while also using cross-device learning on IoT endpoints. This survey explores the role of FL in enhancing financial security and introduces a novel classification of its applications based on regulatory and compliance exposure levels ranging from low-exposure tasks such as collaborative portfolio optimization to high-exposure tasks like real-time fraud detection. Unlike prior surveys, this work reviews FL's practical use within financial systems, discussing its regulatory compliance and recent successes in fraud prevention and blockchain-integrated frameworks. However, FL deployment in finance is not without challenges. Data heterogeneity, adversarial attacks, and regulatory compliance make implementation far from easy. This survey reviews current defense mechanisms and discusses future directions, including blockchain integration, differential privacy, secure multi-party computation, and quantum-secure frameworks. Ultimately, this work aims to be a resource for researchers exploring FL's potential to advance secure, privacy-compliant financial systems.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "8 pages, 2 figures, 1 tables, accepted at 2025 IEEE Global Conference on Artificial Intelligence and Internet of Things",
    "pdf_url": "https://arxiv.org/pdf/2510.14991v1",
    "published_date": "2025-10-07 03:53:12 UTC",
    "updated_date": "2025-10-07 03:53:12 UTC"
  },
  {
    "arxiv_id": "2510.05554v1",
    "title": "Critical attention scaling in long-context transformers",
    "authors": [
      "Shi Chen",
      "Zhengjiang Lin",
      "Yury Polyanskiy",
      "Philippe Rigollet"
    ],
    "abstract": "As large language models scale to longer contexts, attention layers suffer from a fundamental pathology: attention scores collapse toward uniformity as context length $n$ increases, causing tokens to cluster excessively, a phenomenon known as rank-collapse. While $\\textit{attention scaling}$ effectively addresses this deficiency by rescaling attention scores with a polylogarithmic factor $β_n$, theoretical justification for this approach remains lacking.\n  We analyze a simplified yet tractable model that magnifies the effect of attention scaling. In this model, attention exhibits a phase transition governed by the scaling factor $β_n$: insufficient scaling collapses all tokens to a single direction, while excessive scaling reduces attention to identity, thereby eliminating meaningful interactions between tokens. Our main result identifies the critical scaling $β_n \\asymp \\log n$ and provides a rigorous justification for attention scaling in YaRN and Qwen, clarifying why logarithmic scaling maintains sparse, content-adaptive attention at large context lengths.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DM",
      "math.CA"
    ],
    "primary_category": "cs.LG",
    "comment": "29 pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.05554v1",
    "published_date": "2025-10-07 03:51:57 UTC",
    "updated_date": "2025-10-07 03:51:57 UTC"
  },
  {
    "arxiv_id": "2510.08601v1",
    "title": "Mnemosyne: An Unsupervised, Human-Inspired Long-Term Memory Architecture for Edge-Based LLMs",
    "authors": [
      "Aneesh Jonelagadda",
      "Christina Hahn",
      "Haoze Zheng",
      "Salvatore Penachio"
    ],
    "abstract": "Long-term memory is essential for natural, realistic dialogue. However, current large language model (LLM) memory systems rely on either brute-force context expansion or static retrieval pipelines that fail on edge-constrained devices. We introduce Mnemosyne, an unsupervised, human-inspired long-term memory architecture designed for edge-based LLMs. Our approach uses graph-structured storage, modular substance and redundancy filters, memory committing and pruning mechanisms, and probabilistic recall with temporal decay and refresh processes modeled after human memory. Mnemosyne also introduces a concentrated \"core summary\" efficiently derived from a fixed-length subset of the memory graph to capture the user's personality and other domain-specific long-term details such as, using healthcare application as an example, post-recovery ambitions and attitude towards care. Unlike existing retrieval-augmented methods, Mnemosyne is designed for use in longitudinal healthcare assistants, where repetitive and semantically similar but temporally distinct conversations are limited by naive retrieval. In experiments with longitudinal healthcare dialogues, Mnemosyne demonstrates the highest win rate of 65.8% in blind human evaluations of realism and long-term memory capability compared to a baseline RAG win rate of 31.1%. Mnemosyne also achieves current highest LoCoMo benchmark scores in temporal reasoning and single-hop retrieval compared to other same-backboned techniques. Further, the average overall score of 54.6% was second highest across all methods, beating commonly used Mem0 and OpenAI baselines among others. This demonstrates that improved factual recall, enhanced temporal reasoning, and much more natural user-facing responses can be feasible with an edge-compatible and easily transferable unsupervised memory architecture.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.08601v1",
    "published_date": "2025-10-07 03:32:53 UTC",
    "updated_date": "2025-10-07 03:32:53 UTC"
  },
  {
    "arxiv_id": "2510.05548v1",
    "title": "Decade-long Emission Forecasting with an Ensemble Model in Taiwan",
    "authors": [
      "Gordon Hung",
      "Salinna Abdullah"
    ],
    "abstract": "Taiwan's high population and heavy dependence on fossil fuels have led to severe air pollution, with the most prevalent greenhouse gas being carbon dioxide (CO2). There-fore, this study presents a reproducible and comprehensive case study comparing 21 of the most commonly employed time series models in forecasting emissions, analyzing both univariate and multivariate approaches. Among these, Feedforward Neural Network (FFNN), Support Vector Machine (SVM), and Random Forest Regressor (RFR) achieved the best performances. To further enhance robustness, the top performers were integrated with Linear Regression through a custom stacked generalization en-semble technique. Our proposed ensemble model achieved an SMAPE of 1.407 with no signs of overfitting. Finally, this research provides an accurate decade-long emission projection that will assist policymakers in making more data-driven decisions.",
    "categories": [
      "cs.AI",
      "stat.AP"
    ],
    "primary_category": "cs.AI",
    "comment": "18 pages, 12 figures, 6 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.05548v1",
    "published_date": "2025-10-07 03:24:25 UTC",
    "updated_date": "2025-10-07 03:24:25 UTC"
  },
  {
    "arxiv_id": "2510.05538v1",
    "title": "Seeing the Big Picture: Evaluating Multimodal LLMs' Ability to Interpret and Grade Handwritten Student Work",
    "authors": [
      "Owen Henkel",
      "Bill Roberts",
      "Doug Jaffe",
      "Laurence Holt"
    ],
    "abstract": "Recent advances in multimodal large language models (MLLMs) raise the question of their potential for grading, analyzing, and offering feedback on handwritten student classwork. This capability would be particularly beneficial in elementary and middle-school mathematics education, where most work remains handwritten, because seeing students' full working of a problem provides valuable insights into their learning processes, but is extremely time-consuming to grade. We present two experiments investigating MLLM performance on handwritten student mathematics classwork. Experiment A examines 288 handwritten responses from Ghanaian middle school students solving arithmetic problems with objective answers. In this context, models achieved near-human accuracy (95%, k = 0.90) but exhibited occasional errors that human educators would be unlikely to make. Experiment B evaluates 150 mathematical illustrations from American elementary students, where the drawings are the answer to the question. These tasks lack single objective answers and require sophisticated visual interpretation as well as pedagogical judgment in order to analyze and evaluate them. We attempted to separate MLLMs' visual capabilities from their pedagogical abilities by first asking them to grade the student illustrations directly, and then by augmenting the image with a detailed human description of the illustration. We found that when the models had to analyze the student illustrations directly, they struggled, achieving only k = 0.20 with ground truth scores, but when given human descriptions, their agreement levels improved dramatically to k = 0.47, which was in line with human-to-human agreement levels. This gap suggests MLLMs can \"see\" and interpret arithmetic work relatively well, but still struggle to \"see\" student mathematical illustrations.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05538v1",
    "published_date": "2025-10-07 02:59:18 UTC",
    "updated_date": "2025-10-07 02:59:18 UTC"
  },
  {
    "arxiv_id": "2510.05535v2",
    "title": "Permutation-Invariant Representation Learning for Robust and Privacy-Preserving Feature Selection",
    "authors": [
      "Rui Liu",
      "Tao Zhe",
      "Yanjie Fu",
      "Feng Xia",
      "Ted Senator",
      "Dongjie Wang"
    ],
    "abstract": "Feature selection eliminates redundancy among features to improve downstream task performance while reducing computational overhead. Existing methods often struggle to capture intricate feature interactions and adapt across diverse application scenarios. Recent advances employ generative intelligence to alleviate these drawbacks. However, these methods remain constrained by permutation sensitivity in embedding and reliance on convexity assumptions in gradient-based search. To address these limitations, our initial work introduces a novel framework that integrates permutation-invariant embedding with policy-guided search. Although effective, it still left opportunities to adapt to realistic distributed scenarios. In practice, data across local clients is highly imbalanced, heterogeneous and constrained by strict privacy regulations, limiting direct sharing. These challenges highlight the need for a framework that can integrate feature selection knowledge across clients without exposing sensitive information. In this extended journal version, we advance the framework from two perspectives: 1) developing a privacy-preserving knowledge fusion strategy to derive a unified representation space without sharing sensitive raw data. 2) incorporating a sample-aware weighting strategy to address distributional imbalance among heterogeneous local clients. Extensive experiments validate the effectiveness, robustness, and efficiency of our framework. The results further demonstrate its strong generalization ability in federated learning scenarios. The code and data are publicly available: https://anonymous.4open.science/r/FedCAPS-08BF.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "We note that this work has been reproduced without authorization by Stchingtana Naryso and Zihang Yang under the title \"Robust and Privacy-Preserving Feature Selection: A Permutation-Invariant Representation Learning Approach with Federated Extension.\" Their version remains the same technical content, with only the title and abstract changed. This version is the authoritative and original source",
    "pdf_url": "https://arxiv.org/pdf/2510.05535v2",
    "published_date": "2025-10-07 02:53:32 UTC",
    "updated_date": "2025-12-08 23:31:34 UTC"
  },
  {
    "arxiv_id": "2510.05526v2",
    "title": "Provably Mitigating Corruption, Overoptimization, and Verbosity Simultaneously in Offline and Online RLHF/DPO Alignment",
    "authors": [
      "Ziyi Chen",
      "Junyi Li",
      "Peiran Yu",
      "Heng Huang"
    ],
    "abstract": "Reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO) are important techniques to align large language models (LLM) with human preference. However, the quality of RLHF and DPO training is seriously compromised by \\textit{\\textbf{C}orrupted} preference, reward \\textit{\\textbf{O}veroptimization}, and bias towards \\textit{\\textbf{V}erbosity}. To our knowledge, most existing works tackle only one of these important issues, and the few other works require much computation to estimate multiple reward models and lack theoretical guarantee of generalization ability. In this work, we propose RLHF-\\textbf{COV} and DPO-\\textbf{COV} algorithms that can simultaneously mitigate these three issues, in both offline and online settings. This ability is theoretically demonstrated by obtaining length-regularized generalization error rates for our DPO-COV algorithms trained on corrupted data, which match the best-known rates for simpler cases with clean data and without length regularization. Moreover, our DPO-COV algorithm is simple to implement without reward estimation, and is proved to be equivalent to our RLHF-COV algorithm, which directly implies the equivalence between the vanilla RLHF and DPO algorithms. Experiments demonstrate the effectiveness of our DPO-COV algorithms under both offline and online settings.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Edited a few incorrect numbers in Tables 2 and 3",
    "pdf_url": "https://arxiv.org/pdf/2510.05526v2",
    "published_date": "2025-10-07 02:32:47 UTC",
    "updated_date": "2025-12-09 17:10:04 UTC"
  },
  {
    "arxiv_id": "2510.05520v1",
    "title": "CAM: A Constructivist View of Agentic Memory for LLM-Based Reading Comprehension",
    "authors": [
      "Rui Li",
      "Zeyu Zhang",
      "Xiaohe Bo",
      "Zihang Tian",
      "Xu Chen",
      "Quanyu Dai",
      "Zhenhua Dong",
      "Ruiming Tang"
    ],
    "abstract": "Current Large Language Models (LLMs) are confronted with overwhelming information volume when comprehending long-form documents. This challenge raises the imperative of a cohesive memory module, which can elevate vanilla LLMs into autonomous reading agents. Despite the emergence of some heuristic approaches, a systematic design principle remains absent. To fill this void, we draw inspiration from Jean Piaget's Constructivist Theory, illuminating three traits of the agentic memory -- structured schemata, flexible assimilation, and dynamic accommodation. This blueprint forges a clear path toward a more robust and efficient memory system for LLM-based reading comprehension. To this end, we develop CAM, a prototype implementation of Constructivist Agentic Memory that simultaneously embodies the structurality, flexibility, and dynamicity. At its core, CAM is endowed with an incremental overlapping clustering algorithm for structured memory development, supporting both coherent hierarchical summarization and online batch integration. During inference, CAM adaptively explores the memory structure to activate query-relevant information for contextual response, akin to the human associative process. Compared to existing approaches, our design demonstrates dual advantages in both performance and efficiency across diverse long-text reading comprehension tasks, including question answering, query-based summarization, and claim verification.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.05520v1",
    "published_date": "2025-10-07 02:16:30 UTC",
    "updated_date": "2025-10-07 02:16:30 UTC"
  },
  {
    "arxiv_id": "2510.05497v3",
    "title": "Orders in Chaos: Enhancing Large-Scale MoE LLM Serving with Data Movement Forecasting",
    "authors": [
      "Zhongkai Yu",
      "Yue Guan",
      "Zihao Yu",
      "Chenyang Zhou",
      "Zhengding Hu",
      "Shuyi Pei",
      "Yangwook Kang",
      "Yufei Ding",
      "Po-An Tsai"
    ],
    "abstract": "Large-scale Mixture of Experts (MoE) Large Language Models (LLMs) have recently become the frontier open weight models, achieving remarkable model capability similar to proprietary ones. But their random expert selection mechanism introduces significant data movement overhead that becomes the dominant bottleneck in multi-unit LLM serving systems.\n  To understand the patterns underlying this data movement, we conduct comprehensive data-movement-centric profiling across four state-of-the-art large-scale MoE models released in 2025 (200B-1000B) using over 24,000 requests spanning diverse workloads. We perform systematic analysis from both temporal and spatial perspectives and distill six key insights to guide the design of diverse future serving systems. With our insights, we then demonstrate how to improve wafer-scale GPUs as a case study, and show that minor architectural modifications leveraging the insights achieve substantial performance gains, delivering 5.3x and 3.1x average speedups on DeepSeek V3 and Qwen3, respectively. Our work presents the first comprehensive data-centric analysis of large-scale MoE models and a concrete design study using the learned lessons, with profiling traces and simulation framework already open-sourced with $>$1k downloads. Our traces and results are publicly available at https://huggingface.co/datasets/core12345/MoE_expert_selection_trace",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.AR",
      "cs.LG"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05497v3",
    "published_date": "2025-10-07 01:31:39 UTC",
    "updated_date": "2025-12-05 07:59:52 UTC"
  },
  {
    "arxiv_id": "2510.05492v2",
    "title": "High-Fidelity Synthetic ECG Generation via Mel-Spectrogram Informed Diffusion Training",
    "authors": [
      "Zhuoyi Huang",
      "Nutan Sahoo",
      "Anamika Kumari",
      "Girish Kumar",
      "Kexuan Cai",
      "Shixing Cao",
      "Yue Kang",
      "Tian Xia",
      "Somya Chatterjee",
      "Nicholas Hausman",
      "Aidan Jay",
      "Eric S. Rosenthal",
      "Soundar Srinivasan",
      "Sadid Hasan",
      "Alex Fedorov",
      "Sulaiman Vesal"
    ],
    "abstract": "The development of machine learning for cardiac care is severely hampered by privacy restrictions on sharing real patient electrocardiogram (ECG) data. Although generative AI offers a promising solution, the real-world use of existing model-synthesized ECGs is limited by persistent gaps in trustworthiness and clinical utility. In this work, we address two major shortcomings of current generative ECG methods: insufficient morphological fidelity and the inability to generate personalized, patient-specific physiological signals. To address these gaps, we build on a conditional diffusion-based Structured State Space Model (SSSD-ECG) with two principled innovations: (1) MIDT-ECG (Mel-Spectrogram Informed Diffusion Training), a novel training paradigm with time-frequency domain supervision to enforce physiological structural realism, and (2) multi-modal demographic conditioning to enable patient-specific synthesis. We comprehensively evaluate our approach on the PTB-XL dataset, assessing the synthesized ECG signals on fidelity, clinical coherence, privacy preservation, and downstream task utility. MIDT-ECG achieves substantial gains: it improves morphological coherence, preserves strong privacy guarantees with all metrics evaluated exceeding the baseline by 4-8%, and notably reduces the interlead correlation error by an average of 74%, while demographic conditioning enhances signal-to-noise ratio and personalization. In critical low-data regimes, a classifier trained on datasets supplemented with our synthetic ECGs achieves performance comparable to a classifier trained solely on real data. Together, we demonstrate that ECG synthesizers, trained with the proposed time-frequency structural regularization scheme, can serve as personalized, high-fidelity, privacy-preserving surrogates when real data are scarce, advancing the responsible use of generative AI in healthcare.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05492v2",
    "published_date": "2025-10-07 01:14:53 UTC",
    "updated_date": "2025-10-09 00:47:14 UTC"
  },
  {
    "arxiv_id": "2510.05490v1",
    "title": "LANTERN: Scalable Distillation of Large Language Models for Job-Person Fit and Explanation",
    "authors": [
      "Zhoutong Fu",
      "Yihan Cao",
      "Yi-Lin Chen",
      "Aman Lunia",
      "Liming Dong",
      "Neha Saraf",
      "Ruijie Jiang",
      "Yun Dai",
      "Qingquan Song",
      "Tan Wang",
      "Guoyao Li",
      "Derek Koh",
      "Haichao Wei",
      "Zhipeng Wang",
      "Aman Gupta",
      "Chengming Jiang",
      "Jianqiang Shen",
      "Liangjie Hong",
      "Wenjing Zhang"
    ],
    "abstract": "Large language models (LLMs) have achieved strong performance across a wide range of natural language processing tasks. However, deploying LLMs at scale for domain specific applications, such as job-person fit and explanation in job seeking platforms, introduces distinct challenges. At LinkedIn, the job person fit task requires analyzing a candidate's public profile against job requirements to produce both a fit assessment and a detailed explanation. Directly applying open source or finetuned LLMs to this task often fails to yield high quality, actionable feedback due to the complexity of the domain and the need for structured outputs. Moreover, the large size of these models leads to high inference latency and limits scalability, making them unsuitable for online use. To address these challenges, we introduce LANTERN, a novel LLM knowledge distillation framework tailored specifically for job person fit tasks. LANTERN involves modeling over multiple objectives, an encoder model for classification purpose, and a decoder model for explanation purpose. To better distill the knowledge from a strong black box teacher model to multiple downstream models, LANTERN incorporates multi level knowledge distillation that integrates both data and logit level insights. In addition to introducing the knowledge distillation framework, we share our insights on post training techniques and prompt engineering, both of which are crucial for successfully adapting LLMs to domain specific downstream tasks. Extensive experimental results demonstrate that LANTERN significantly improves task specific metrics for both job person fit and explanation. Online evaluations further confirm its effectiveness, showing measurable gains in job seeker engagement, including a 0.24\\% increase in apply rate and a 0.28\\% increase in qualified applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, 4 figures, 5 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.05490v1",
    "published_date": "2025-10-07 01:10:02 UTC",
    "updated_date": "2025-10-07 01:10:02 UTC"
  },
  {
    "arxiv_id": "2510.05480v1",
    "title": "Vul-R2: A Reasoning LLM for Automated Vulnerability Repair",
    "authors": [
      "Xin-Cheng Wen",
      "Zirui Lin",
      "Yijun Yang",
      "Cuiyun Gao",
      "Deheng Ye"
    ],
    "abstract": "The exponential increase in software vulnerabilities has created an urgent need for automatic vulnerability repair (AVR) solutions. Recent research has formulated AVR as a sequence generation problem and has leveraged large language models (LLMs) to address this problem. Typically, these approaches prompt or fine-tune LLMs to generate repairs for vulnerabilities directly. Although these methods show state-of-the-art performance, they face the following challenges: (1) Lack of high-quality, vulnerability-related reasoning data. Current approaches primarily rely on foundation models that mainly encode general programming knowledge. Without vulnerability-related reasoning data, they tend to fail to capture the diverse vulnerability repair patterns. (2) Hard to verify the intermediate vulnerability repair process during LLM training. Existing reinforcement learning methods often leverage intermediate execution feedback from the environment (e.g., sandbox-based execution results) to guide reinforcement learning training. In contrast, the vulnerability repair process generally lacks such intermediate, verifiable feedback, which poses additional challenges for model training.",
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "13 pages, 8 figures. This paper is accepted by ASE 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.05480v1",
    "published_date": "2025-10-07 00:43:13 UTC",
    "updated_date": "2025-10-07 00:43:13 UTC"
  },
  {
    "arxiv_id": "2510.05468v1",
    "title": "AMAQ: Adaptive Mixed-bit Activation Quantization for Collaborative Parameter Efficient Fine-tuning",
    "authors": [
      "Yurun Song",
      "Zhuoyi Yang",
      "Ian G. Harris",
      "Sangeetha Abdu Jyothi"
    ],
    "abstract": "Large Language Models (LLMs) are scaling rapidly, creating significant challenges for collaborative server client distributed training, particularly in terms of communication efficiency and computational overheads. To address these challenges, we implement Parameter-efficient Split Learning, which effectively balances efficiency and performance for collaborative training on low-resource devices.\n  To reduce communication overhead in collaborative training, we introduce Adaptive Mixed bit Activation Quantization (AMAQ), a strategy that progressively compresses activations and gradients from high precision (6 to 8 bits) to low precision (3 to 4 bits). AMAQ achieves this by effectively allocating bit budgets across channels based on feature wise and layer wise importance using bit regularization.\n  Under the same bit budgets, AMAQ outperforms fixed-precision approaches, delivering about 2.5% higher generation accuracy and about 1.3% better classification accuracy for models like LLaMA3 8B and Qwen2.5 7B. In addition, it significantly enhances training stability and reducing ultra-low bit representation collapse during the training.\n  Experiments demonstrate that AMAQ integrates effectively into practical multi-machine collaborative training setups, offering superior inference accuracy with only a modest communication overhead for bits adaptation during training. This trade off makes AMAQ a practical and effective solution for collaborative training with minimal communication cost.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.05468v1",
    "published_date": "2025-10-07 00:05:16 UTC",
    "updated_date": "2025-10-07 00:05:16 UTC"
  }
]
{
  "date": "2024-12-01",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-12-01 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 更新了 59 篇论文，主要聚焦 AI 和大型语言模型 (LLM) 的优化、道德挑战、多模态处理以及应用领域的创新，其中令人印象深刻的包括减少 LLM sycophancy 的方法（NeurIPS 相关）和高效的多模态 LLM 框架，而知名学者如 José Miguel Hernández-Lobato 和 Ayoub Bagheri 的作品凸显了 LLM 在药物设计和跨文化道德中的潜力。\n\n下面，我将挑选并简要讨论部分关键论文，先优先聊 AI/LLM 相关、突破性贡献的文章，再快速掠过其他领域的内容。每个条目包括论文标题（中文 + 英文）和核心贡献。\n\n### AI 和 LLM 优化领域\n- **Linear Probe Penalties Reduce LLM Sycophancy（中文：线性探针惩罚减少大型语言模型的谄媚行为）**  \n  这篇论文提出了一种线性探针方法，用于识别并惩罚 LLM 在强化学习反馈（RLHF）中的谄媚行为（如优先同意用户观点），实验显示该方法显著降低了多个开源 LLM 的谄媚倾向，并为减少不期望行为提供了通用框架。\n\n- **LLMs as mirrors of societal moral standards（中文：大型语言模型作为社会道德标准的镜像：反映文化差异和共识）**  \n  作者 Ayoub Bagheri 等研究了 LLM 是否能准确捕捉跨文化道德差异，通过比较模型生成分数与调查数据，发现现有 LLM 在反映道德细微差别的表现不佳，但 BLOOM 模型表现出一定相关性，强调了开发文化感知 AI 的必要性。\n\n- **Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic Vision-language Context Sparsification（中文：动态视觉-语言上下文稀疏化以提升多模态大型语言模型的效率）**  \n  这篇论文引入了动态稀疏化框架，减少多模态 LLM（如 LLaVA）的计算开销，实验显示它在预填充阶段节省约 75% 计算，在整个生成过程中节省 50% 计算和内存，同时保持了性能，适合资源受限场景。\n\n- **Improving Physics Reasoning in Large Language Models Using Mixture of Refinement Agents（中文：使用混合精炼代理提升大型语言模型的物理推理能力）**  \n  论文提出 Mixture of Refinement Agents（MoRA）框架，通过迭代修正 LLM 的问题误解、概念错误和计算失误，在物理任务上使开源模型（如 Llama-3-70B）性能提升 16%，接近 GPT-4o 水平。\n\n- **Large Language Models in Politics and Democracy: A Comprehensive Survey（中文：大型语言模型在政治和民主中的应用：全面调查）**  \n  这篇综述探讨了 LLM 在政策制定、政治沟通和治理中的潜力，同时指出了偏见和透明性挑战，强调了伦理框架以确保 LLM 与民主价值观一致。\n\n### 多模态和视觉处理领域\n- **DSSRNN: Decomposition-Enhanced State-Space Recurrent Neural Network for Time-Series Analysis（中文：分解增强的状态空间循环神经网络用于时间序列分析）**  \n  论文创新性地结合分解分析和状态空间模型，用于时间序列预测，在室内空气质量数据集上超越 SOTA 模型（如 Transformer），实现了更好的 MSE 和 MAE，同时保持计算效率。\n\n- **BIGCity: A Universal Spatiotemporal Model for Unified Trajectory and Traffic State Data Analysis（中文：通用时空模型用于统一轨迹和交通状态数据分析）**  \n  这篇工作首次提出多任务多模态模型，统一处理轨迹和交通数据，通过图神经网络和提示机制，在 8 个任务上超越 18 个基线，代码开源。\n\n- **STEVE-Audio: Expanding the Goal Conditioning Modalities of Embodied Agents in Minecraft（中文：在 Minecraft 中扩展具身代理的目标条件模态）**  \n  论文扩展了 STEVE-1 代理，支持音频模态，通过 Audio-Video CLIP 模型实现与文本/视觉相当的性能，适用于多模态决策，代码和模型开源。\n\n- **Playable Game Generation（中文：可玩游戏生成）**  \n  引入 PlayGen 方法，使用自回归扩散模型生成实时互动游戏，在 2D/3D 游戏上实现高视觉质量和机制模拟，支持超过 1000 帧的游戏回放，代码和演示开源。\n\n### 其他应用领域（快速掠过）\n- **A Deep Generative Model for the Design of Synthesizable Ionizable Lipids（中文：用于合成可离子化脂质设计的深度生成模型）**  \n  论文开发了针对脂质纳米粒子的生成模型，提供合成路径，加速 mRNA 疫苗和基因疗法的开发。\n\n- **Learning to Unlearn: Meta-Learning-Based Knowledge Graph Embedding Unlearning（中文：基于元学习的知识图谱嵌入取消学习）**  \n  提出 MetaEU 框架，使用元学习消除知识图谱中的特定数据影响，同时保持整体性能。\n\n- **HT-HEDL: High-Throughput Hypothesis Evaluation in Description Logic（中文：描述逻辑中的高吞吐量假设评估）**  \n  这篇工作优化了描述逻辑假设评估，使用 CPU 和 GPU 结合加速计算，实现了高达 44 倍的性能提升。\n\n其他论文如关于手术反馈分析、药物设计或游戏生成的，贡献虽有趣但较 niche，且未涉及核心热点，因此仅简要提及：它们分别探讨了 AI 在医疗和娱乐中的应用，但整体影响力较小。\n\n总之，今天的 arXiv 论文突显了 AI 领域的快速迭代，尤其在 LLM 鲁棒性和多模态融合上，值得关注未来应用。明天的快报见！",
  "papers": [
    {
      "arxiv_id": "2412.00994v1",
      "title": "DSSRNN: Decomposition-Enhanced State-Space Recurrent Neural Network for Time-Series Analysis",
      "title_zh": "DSSRNN：分解增强的状态空间循环神经网络用于时间序列分析",
      "authors": [
        "Ahmad Mohammadshirazi",
        "Ali Nosratifiroozsalari",
        "Rajiv Ramnath"
      ],
      "abstract": "Time series forecasting is a crucial yet challenging task in machine\nlearning, requiring domain-specific knowledge due to its wide-ranging\napplications. While recent Transformer models have improved forecasting\ncapabilities, they come with high computational costs. Linear-based models have\nshown better accuracy than Transformers but still fall short of ideal\nperformance. To address these challenges, we introduce the Decomposition\nState-Space Recurrent Neural Network (DSSRNN), a novel framework designed for\nboth long-term and short-term time series forecasting. DSSRNN uniquely combines\ndecomposition analysis to capture seasonal and trend components with\nstate-space models and physics-based equations. We evaluate DSSRNN's\nperformance on indoor air quality datasets, focusing on CO2 concentration\nprediction across various forecasting horizons. Results demonstrate that DSSRNN\nconsistently outperforms state-of-the-art models, including transformer-based\narchitectures, in terms of both Mean Squared Error (MSE) and Mean Absolute\nError (MAE). For example, at the shortest horizon (T=96) in Office 1, DSSRNN\nachieved an MSE of 0.378 and an MAE of 0.401, significantly lower than\ncompeting models. Additionally, DSSRNN exhibits superior computational\nefficiency compared to more complex models. While not as lightweight as the\nDLinear model, DSSRNN achieves a balance between performance and efficiency,\nwith only 0.11G MACs and 437MiB memory usage, and an inference time of 0.58ms\nfor long-term forecasting. This work not only showcases DSSRNN's success but\nalso establishes a new benchmark for physics-informed machine learning in\nenvironmental forecasting and potentially other domains.",
      "tldr_zh": "本研究提出DSSRNN（Decomposition-Enhanced State-Space Recurrent Neural Network），一种新型框架，用于处理时间序列预测的挑战，包括长期和短期预测。DSSRNN结合分解分析（捕捉季节性和趋势成分）、状态空间模型以及基于物理的方程，旨在克服Transformer模型的高计算成本和线性模型的准确性不足问题。在室内空气质量数据集（如CO2浓度预测）上的实验显示，DSSRNN在MSE和MAE指标上显著优于现有模型，例如在Office 1的T=96预测中，MSE为0.378、MAE为0.401，同时保持高效的计算性能（0.11G MACs、437MiB内存和0.58ms推理时间）。这项工作为基于物理的机器学习在环境预测和其他领域设立了新基准。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00994v1",
      "published_date": "2024-12-01 22:55:58 UTC",
      "updated_date": "2024-12-01 22:55:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:06:37.533878"
    },
    {
      "arxiv_id": "2412.00967v1",
      "title": "Linear Probe Penalties Reduce LLM Sycophancy",
      "title_zh": "翻译失败",
      "authors": [
        "Henry Papadatos",
        "Rachel Freedman"
      ],
      "abstract": "Large language models (LLMs) are often sycophantic, prioritizing agreement\nwith their users over accurate or objective statements. This problematic\nbehavior becomes more pronounced during reinforcement learning from human\nfeedback (RLHF), an LLM fine-tuning stage intended to align model outputs with\nhuman values. Instead of increasing accuracy and reliability, the reward model\nlearned from RLHF often rewards sycophancy. We develop a linear probing method\nto identify and penalize markers of sycophancy within the reward model,\nproducing rewards that discourage sycophantic behavior. Our experiments show\nthat constructing and optimizing against this surrogate reward function reduces\nsycophantic behavior in multiple open-source LLMs. Our results suggest a\ngeneralizable methodology for reducing unwanted LLM behaviors that are not\nsufficiently disincentivized by RLHF fine-tuning.",
      "tldr_zh": "大型语言模型 (LLMs) 常表现出 sycophancy 行为，即优先同意用户而非提供准确信息，这种问题在 reinforcement learning from human feedback (RLHF) 微调过程中尤为突出。研究开发了一种 linear probing 方法，用于识别和惩罚 reward model 中的 sycophancy 标记，从而构建一个抑制这种行为的替代奖励函数。实验结果显示，该方法在多个开源 LLMs 中显著减少了 sycophantic 行为，并提出了一种可推广的策略来缓解 RLHF 未能充分抑制的不当行为。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "20 pages, 15 figures, NeurIPS 2024 Workshop Socially Responsible\n  Language Modelling Research (SoLaR)",
      "pdf_url": "http://arxiv.org/pdf/2412.00967v1",
      "published_date": "2024-12-01 21:11:28 UTC",
      "updated_date": "2024-12-01 21:11:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:06:49.168402"
    },
    {
      "arxiv_id": "2412.00962v1",
      "title": "LLMs as mirrors of societal moral standards: reflection of cultural divergence and agreement across ethical topics",
      "title_zh": "LLMs 作为社会道德标准的镜像：伦理主题中文化分歧和一致性的反映",
      "authors": [
        "Mijntje Meijer",
        "Hadi Mohammadi",
        "Ayoub Bagheri"
      ],
      "abstract": "Large language models (LLMs) have become increasingly pivotal in various\ndomains due the recent advancements in their performance capabilities. However,\nconcerns persist regarding biases in LLMs, including gender, racial, and\ncultural biases derived from their training data. These biases raise critical\nquestions about the ethical deployment and societal impact of LLMs.\nAcknowledging these concerns, this study investigates whether LLMs accurately\nreflect cross-cultural variations and similarities in moral perspectives. In\nassessing whether the chosen LLMs capture patterns of divergence and agreement\non moral topics across cultures, three main methods are employed: (1)\ncomparison of model-generated and survey-based moral score variances, (2)\ncluster alignment analysis to evaluate the correspondence between country\nclusters derived from model-generated moral scores and those derived from\nsurvey data, and (3) probing LLMs with direct comparative prompts. All three\nmethods involve the use of systematic prompts and token pairs designed to\nassess how well LLMs understand and reflect cultural variations in moral\nattitudes. The findings of this study indicate overall variable and low\nperformance in reflecting cross-cultural differences and similarities in moral\nvalues across the models tested, highlighting the necessity for improving\nmodels' accuracy in capturing these nuances effectively. The insights gained\nfrom this study aim to inform discussions on the ethical development and\ndeployment of LLMs in global contexts, emphasizing the importance of mitigating\nbiases and promoting fair representation across diverse cultural perspectives.",
      "tldr_zh": "这篇论文探讨大型语言模型 (LLMs) 是否能准确反映跨文化在道德话题上的分歧和共识，评估其作为社会道德标准的“镜像”能力。研究采用三种方法：比较模型生成的道德分数与调查数据的差异、集群对齐分析以检查国家集群的对应性，以及使用直接比较提示来探测 LLMs 的文化理解。结果显示，测试的 LLMs 在捕捉跨文化道德差异和相似性方面表现变量且整体低下，突显了减少文化偏见的需求。论文的见解旨在指导 LLMs 的伦理开发和全球部署，促进更公平的文化代表。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.SC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00962v1",
      "published_date": "2024-12-01 20:39:42 UTC",
      "updated_date": "2024-12-01 20:39:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:07:01.749630"
    },
    {
      "arxiv_id": "2412.00959v1",
      "title": "Generative Language Models Potential for Requirement Engineering Applications: Insights into Current Strengths and Limitations",
      "title_zh": "生成式语言模型在需求工程应用中的潜力：对当前优势和局限性的洞见",
      "authors": [
        "Summra Saleem",
        "Muhammad Nabeel Asim",
        "Ludger Van Elst",
        "Andreas Dengel"
      ],
      "abstract": "Traditional language models have been extensively evaluated for software\nengineering domain, however the potential of ChatGPT and Gemini have not been\nfully explored. To fulfill this gap, the paper in hand presents a comprehensive\ncase study to investigate the potential of both language models for development\nof diverse types of requirement engineering applications. It deeply explores\nimpact of varying levels of expert knowledge prompts on the prediction\naccuracies of both language models. Across 4 different public benchmark\ndatasets of requirement engineering tasks, it compares performance of both\nlanguage models with existing task specific machine/deep learning predictors\nand traditional language models. Specifically, the paper utilizes 4 benchmark\ndatasets; Pure (7,445 samples, requirements extraction),PROMISE (622 samples,\nrequirements classification), REQuestA (300 question answer (QA) pairs) and\nAerospace datasets (6347 words, requirements NER tagging). Our experiments\nreveal that, in comparison to ChatGPT, Gemini requires more careful prompt\nengineering to provide accurate predictions. Moreover, across requirement\nextraction benchmark dataset the state-of-the-art F1-score is 0.86 while\nChatGPT and Gemini achieved 0.76 and 0.77,respectively. The State-of-the-art\nF1-score on requirements classification dataset is 0.96 and both language\nmodels 0.78. In name entity recognition (NER) task the state-of-the-art\nF1-score is 0.92 and ChatGPT managed to produce 0.36, and Gemini 0.25.\nSimilarly, across question answering dataset the state-of-the-art F1-score is\n0.90 and ChatGPT and Gemini managed to produce 0.91 and 0.88 respectively. Our\nexperiments show that Gemini requires more precise prompt engineering than\nChatGPT. Except for question-answering, both models under-perform compared to\ncurrent state-of-the-art predictors across other tasks.",
      "tldr_zh": "这篇论文通过一个全面案例研究，评估了生成式语言模型（如 ChatGPT 和 Gemini）在需求工程（Requirement Engineering）应用中的潜力，填补了传统语言模型研究的空白。研究者使用4个公共基准数据集（Pure、PROMISE、REQuestA 和 Aerospace），测试了不同专家知识提示水平对模型预测准确性的影响，并与现有任务特定机器/深度学习预测器进行比较。结果显示，Gemini 需要更精确的 Prompt Engineering 才能提供准确预测，而在要求提取（ChatGPT F1-score 0.76 vs. SOTA 0.86）、分类（0.78 vs. 0.96）和命名实体识别（NER）任务（ChatGPT 0.36 vs. SOTA 0.92）上，两模型表现不如 SOTA，但在问答任务上（如 ChatGPT 的 0.91 vs. SOTA 0.90）竞争力较强。总体而言，该研究突出了这些模型的优点和局限性，为未来需求工程应用提供宝贵见解。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00959v1",
      "published_date": "2024-12-01 20:20:58 UTC",
      "updated_date": "2024-12-01 20:20:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:07:15.286961"
    },
    {
      "arxiv_id": "2412.00956v1",
      "title": "Large Language Models as Mirrors of Societal Moral Standards",
      "title_zh": "大型语言模型作为社会道德标准的镜像",
      "authors": [
        "Evi Papadopoulou",
        "Hadi Mohammadi",
        "Ayoub Bagheri"
      ],
      "abstract": "Prior research has demonstrated that language models can, to a limited\nextent, represent moral norms in a variety of cultural contexts. This research\naims to replicate these findings and further explore their validity,\nconcentrating on issues like 'homosexuality' and 'divorce'. This study\nevaluates the effectiveness of these models using information from two surveys,\nthe WVS and the PEW, that encompass moral perspectives from over 40 countries.\nThe results show that biases exist in both monolingual and multilingual models,\nand they typically fall short of accurately capturing the moral intricacies of\ndiverse cultures. However, the BLOOM model shows the best performance,\nexhibiting some positive correlations, but still does not achieve a\ncomprehensive moral understanding. This research underscores the limitations of\ncurrent PLMs in processing cross-cultural differences in values and highlights\nthe importance of developing culturally aware AI systems that better align with\nuniversal human values.",
      "tldr_zh": "本研究探讨大型语言模型（Large Language Models）作为社会道德标准镜像的能力，聚焦于“homosexuality”和“divorce”等议题，并使用WVS和PEW调查数据（覆盖超过40个国家）来评估模型的表现。结果显示，单语和多语模型均存在偏差，无法准确捕捉不同文化的道德复杂性，尽管BLOOM模型表现出一些正相关性并表现最佳。论文突出了当前PLMs在处理跨文化价值观方面的局限性，并强调开发更文化感知的AI系统，以更好地符合普遍人类价值。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.SC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00956v1",
      "published_date": "2024-12-01 20:20:35 UTC",
      "updated_date": "2024-12-01 20:20:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:07:26.600538"
    },
    {
      "arxiv_id": "2412.00953v1",
      "title": "BIGCity: A Universal Spatiotemporal Model for Unified Trajectory and Traffic State Data Analysis",
      "title_zh": "BIGCity：一种通用时空模型，用于统一轨迹和交通状态数据分析",
      "authors": [
        "Xie Yu",
        "Jingyuan Wang",
        "Yifan Yang",
        "Qian Huang",
        "Ke Qu"
      ],
      "abstract": "Typical dynamic ST data includes trajectory data (representing\nindividual-level mobility) and traffic state data (representing\npopulation-level mobility). Traditional studies often treat trajectory and\ntraffic state data as distinct, independent modalities, each tailored to\nspecific tasks within a single modality. However, real-world applications, such\nas navigation apps, require joint analysis of trajectory and traffic state\ndata. Treating these data types as two separate domains can lead to suboptimal\nmodel performance. Although recent advances in ST data pre-training and ST\nfoundation models aim to develop universal models for ST data analysis, most\nexisting models are \"multi-task, solo-data modality\" (MTSM), meaning they can\nhandle multiple tasks within either trajectory data or traffic state data, but\nnot both simultaneously. To address this gap, this paper introduces BIGCity,\nthe first multi-task, multi-data modality (MTMD) model for ST data analysis.\nThe model targets two key challenges in designing an MTMD ST model: (1)\nunifying the representations of different ST data modalities, and (2) unifying\nheterogeneous ST analysis tasks. To overcome the first challenge, BIGCity\nintroduces a novel ST-unit that represents both trajectories and traffic states\nin a unified format. Additionally, for the second challenge, BIGCity adopts a\ntunable large model with ST task-oriented prompt, enabling it to perform a\nrange of heterogeneous tasks without the need for fine-tuning. Extensive\nexperiments on real-world datasets demonstrate that BIGCity achieves\nstate-of-the-art performance across 8 tasks, outperforming 18 baselines. To the\nbest of our knowledge, BIGCity is the first model capable of handling both\ntrajectories and traffic states for diverse heterogeneous tasks. Our code are\navailable at https://github.com/bigscity/BIGCity",
      "tldr_zh": "该论文提出 BIGCity，一种通用的时空（Spatiotemporal）模型，用于统一分析轨迹数据（trajectory data）和交通状态数据（traffic state data），解决传统方法将两者视为独立模态导致的性能问题。BIGCity 作为首个多任务多数据模态（MTMD）模型，引入 ST-unit 来统一不同数据模态的表示，并采用可调节的大型模型结合 ST 任务导向提示，实现无需微调即可处理多种异构任务。实验结果显示，BIGCity 在真实数据集上超越 18 个基线模型，在 8 个任务中达到最先进性能，并开源代码以促进进一步研究。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00953v1",
      "published_date": "2024-12-01 20:10:55 UTC",
      "updated_date": "2024-12-01 20:10:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:07:38.247701"
    },
    {
      "arxiv_id": "2412.00949v1",
      "title": "STEVE-Audio: Expanding the Goal Conditioning Modalities of Embodied Agents in Minecraft",
      "title_zh": "翻译失败",
      "authors": [
        "Nicholas Lenzen",
        "Amogh Raut",
        "Andrew Melnik"
      ],
      "abstract": "Recently, the STEVE-1 approach has been introduced as a method for training\ngenerative agents to follow instructions in the form of latent CLIP embeddings.\nIn this work, we present a methodology to extend the control modalities by\nlearning a mapping from new input modalities to the latent goal space of the\nagent. We apply our approach to the challenging Minecraft domain, and extend\nthe goal conditioning to include the audio modality. The resulting\naudio-conditioned agent is able to perform on a comparable level to the\noriginal text-conditioned and visual-conditioned agents. Specifically, we\ncreate an Audio-Video CLIP foundation model for Minecraft and an audio prior\nnetwork which together map audio samples to the latent goal space of the\nSTEVE-1 policy. Additionally, we highlight the tradeoffs that occur when\nconditioning on different modalities. Our training code, evaluation code, and\nAudio-Video CLIP foundation model for Minecraft are made open-source to help\nfoster further research into multi-modal generalist sequential decision-making\nagents.",
      "tldr_zh": "本研究扩展了 STEVE-1 方法，使其在 Minecraft 环境中让具身代理支持音频模态的目标条件控制，通过学习将新输入模态映射到代理的潜在目标空间。研究者创建了 Audio-Video CLIP 基础模型和音频先验网络，将音频样本转换为 STEVE-1 策略的潜在目标，从而使音频条件代理的表现与文本和视觉条件代理相当。实验突出了不同模态间的权衡，并开源了训练代码、评估代码和 Audio-Video CLIP 模型，以推动多模态通用序列决策代理的进一步研究。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at CoRL 2024: Workshop on Lifelong Learning for Home Robots",
      "pdf_url": "http://arxiv.org/pdf/2412.00949v1",
      "published_date": "2024-12-01 19:48:57 UTC",
      "updated_date": "2024-12-01 19:48:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:07:49.998996"
    },
    {
      "arxiv_id": "2412.00944v1",
      "title": "Bilinear Convolution Decomposition for Causal RL Interpretability",
      "title_zh": "翻译失败",
      "authors": [
        "Narmeen Oozeer",
        "Sinem Erisken",
        "Alice Rigg"
      ],
      "abstract": "Efforts to interpret reinforcement learning (RL) models often rely on\nhigh-level techniques such as attribution or probing, which provide only\ncorrelational insights and coarse causal control. This work proposes replacing\nnonlinearities in convolutional neural networks (ConvNets) with bilinear\nvariants, to produce a class of models for which these limitations can be\naddressed. We show bilinear model variants perform comparably in model-free\nreinforcement learning settings, and give a side by side comparison on ProcGen\nenvironments. Bilinear layers' analytic structure enables weight-based\ndecomposition. Previous work has shown bilinearity enables quantifying\nfunctional importance through eigendecomposition, to identify interpretable low\nrank structure. We show how to adapt the decomposition to convolution layers by\napplying singular value decomposition to vectors of interest, to separate the\nchannel and spatial dimensions. Finally, we propose a methodology for causally\nvalidating concept-based probes, and illustrate its utility by studying a\nmaze-solving agent's ability to track a cheese object.",
      "tldr_zh": "这篇论文针对强化学习 (RL) 模型的解释问题，提出用双线性变体替换卷积神经网络 (ConvNets) 中的非线性，以实现更精确的因果控制和可解释性。实验结果显示，这些双线性模型在无模型 RL 设置中性能与传统模型相当，并在 ProcGen 环境进行了对比。论文通过奇异值分解 (SVD) 对卷积层的通道和空间维度进行分解，量化功能重要性，并引入一种因果验证方法来评估概念-based probes，例如研究迷宫求解代理跟踪奶酪对象的能力。总的来说，该方法提升了 RL 模型的可解释性和低秩结构分析，为更可靠的模型解释奠定基础。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "8 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.00944v1",
      "published_date": "2024-12-01 19:32:04 UTC",
      "updated_date": "2024-12-01 19:32:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:08:02.654814"
    },
    {
      "arxiv_id": "2412.07794v1",
      "title": "Automatic answering of scientific questions using the FACTS-V1 framework: New methods in research to increase efficiency through the use of AI",
      "title_zh": "翻译失败",
      "authors": [
        "Stefan Pietrusky"
      ],
      "abstract": "The use of artificial intelligence (AI) offers various possibilities to\nexpand and support educational research. Specifically, the implementation of AI\ncan be used to develop new frameworks to establish new research tools that\naccelerate and meaningfully expand the efficiency of data evaluation and\ninterpretation (Buckingham Shum et al., 2023). This article presents the\nprototype of the FACTS-V1 (Filtering and Analysis of Content in Textual\nSources) framework. With the help of the application, numerous scientific\npapers can be automatically extracted, analyzed and interpreted from open\naccess document servers without having to rely on proprietary applications and\ntheir limitations. The FACTS-V1 prototype consists of three building blocks.\nThe first part deals with the extraction of texts, the second with filtering\nand interpretation, and the last with the actual statistical evaluation (topic\nmodeling) using an interactive overview. The aim of the framework is to provide\nrecommendations for future scientific questions based on existing data. The\nfunctionality is illustrated by asking how the use of AI will change the\neducation sector. The data used to answer the question comes from 82 scientific\npapers on the topic of AI from 2024. The papers are publicly available on the\npeDOCS document server of the Leibniz Institute for Educational Research and\nEducational Information.",
      "tldr_zh": "本文提出FACTS-V1框架，这是一个利用AI的工具，旨在自动提取、分析和解释科学论文，以提高教育研究的效率和数据处理能力。框架由三个构建块组成：文本提取、过滤与解释、以及统计评估（包括topic modeling），允许从开源文档服务器获取数据并生成交互式概述。其核心功能是通过现有数据提供未来科学问题的推荐，例如分析2024年82篇关于AI在教育领域的论文，探讨AI如何改变教育部门。该框架有助于减少对专有应用的依赖，推动研究工具的创新发展。",
      "categories": [
        "cs.DL",
        "cs.AI"
      ],
      "primary_category": "cs.DL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.07794v1",
      "published_date": "2024-12-01 18:55:39 UTC",
      "updated_date": "2024-12-01 18:55:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:08:14.012789"
    },
    {
      "arxiv_id": "2412.00928v1",
      "title": "A Deep Generative Model for the Design of Synthesizable Ionizable Lipids",
      "title_zh": "一种用于设计可合成电离脂质的深度生成模型",
      "authors": [
        "Yuxuan Ou",
        "Jingyi Zhao",
        "Austin Tripp",
        "Morteza Rasoulianboroujeni",
        "José Miguel Hernández-Lobato"
      ],
      "abstract": "Lipid nanoparticles (LNPs) are vital in modern biomedicine, enabling the\neffective delivery of mRNA for vaccines and therapies by protecting it from\nrapid degradation. Among the components of LNPs, ionizable lipids play a key\nrole in RNA protection and facilitate its delivery into the cytoplasm. However,\ndesigning ionizable lipids is complex. Deep generative models can accelerate\nthis process and explore a larger candidate space compared to traditional\nmethods. Due to the structural differences between lipids and small molecules,\nexisting generative models used for small molecule generation are unsuitable\nfor lipid generation. To address this, we developed a deep generative model\nspecifically tailored for the discovery of ionizable lipids. Our model\ngenerates novel ionizable lipid structures and provides synthesis paths using\nsynthetically accessible building blocks, addressing synthesizability. This\nadvancement holds promise for streamlining the development of lipid-based\ndelivery systems, potentially accelerating the deployment of new therapeutic\nagents, including mRNA vaccines and gene therapies.",
      "tldr_zh": "脂质纳米颗粒(LNPs)是mRNA疫苗和疗法递送的关键，离子化脂质(Ionizable Lipids)负责RNA保护和细胞内递送，但其设计过程复杂且依赖传统方法效率低下。本文提出一个专门的深度生成模型(Deep Generative Model)，针对脂质的独特结构生成新颖的离子化脂质结构，并提供基于易合成构建块的合成路径。该模型有望加速脂质递送系统的开发，提升mRNA疫苗和基因疗法的部署效率。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2024 Workshop on AI for New Drug Modalities",
      "pdf_url": "http://arxiv.org/pdf/2412.00928v1",
      "published_date": "2024-12-01 18:33:22 UTC",
      "updated_date": "2024-12-01 18:33:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:08:26.028912"
    },
    {
      "arxiv_id": "2412.01864v1",
      "title": "Learning Aggregation Rules in Participatory Budgeting: A Data-Driven Approach",
      "title_zh": "在参与式预算中学习聚合规则：一种数据驱动的方法",
      "authors": [
        "Roy Fairstein",
        "Dan Vilenchik",
        "Kobi Gal"
      ],
      "abstract": "Participatory Budgeting (PB) offers a democratic process for communities to\nallocate public funds across various projects through voting. In practice, PB\norganizers face challenges in selecting aggregation rules either because they\nare not familiar with the literature and the exact details of every existing\nrule or because no existing rule echoes their expectations. This paper presents\na novel data-driven approach utilizing machine learning to address this\nchallenge. By training neural networks on PB instances, our approach learns\naggregation rules that balance social welfare, representation, and other\nsocietal beneficial goals. It is able to generalize from small-scale synthetic\nPB examples to large, real-world PB instances. It is able to learn existing\naggregation rules but also generate new rules that adapt to diverse objectives,\nproviding a more nuanced, compromise-driven solution for PB processes. The\neffectiveness of our approach is demonstrated through extensive experiments\nwith synthetic and real-world PB data, and can expand the use and deployment of\nPB solutions.",
      "tldr_zh": "该研究针对参与式预算（Participatory Budgeting, PB）中聚合规则的选择难题，提出了一种基于机器学习的数据驱动方法，使用神经网络在PB实例上训练，以学习平衡社会福利、代表性和其他目标的规则。相比传统方法，该方法能从小型合成示例推广到大型真实世界实例，并不仅限于现有规则，还能生成适应多样目标的新规则，提供更细致的折衷解决方案。通过广泛的合成和真实世界数据实验，该方法证明了其有效性，并有助于扩展PB系统的应用和部署。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY",
        "cs.GT"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.01864v1",
      "published_date": "2024-12-01 18:13:27 UTC",
      "updated_date": "2024-12-01 18:13:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:08:37.602171"
    },
    {
      "arxiv_id": "2412.00887v1",
      "title": "Playable Game Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Mingyu Yang",
        "Junyou Li",
        "Zhongbin Fang",
        "Sheng Chen",
        "Yangbin Yu",
        "Qiang Fu",
        "Wei Yang",
        "Deheng Ye"
      ],
      "abstract": "In recent years, Artificial Intelligence Generated Content (AIGC) has\nadvanced from text-to-image generation to text-to-video and multimodal video\nsynthesis. However, generating playable games presents significant challenges\ndue to the stringent requirements for real-time interaction, high visual\nquality, and accurate simulation of game mechanics. Existing approaches often\nfall short, either lacking real-time capabilities or failing to accurately\nsimulate interactive mechanics. To tackle the playability issue, we propose a\nnovel method called \\emph{PlayGen}, which encompasses game data generation, an\nautoregressive DiT-based diffusion model, and a comprehensive playability-based\nevaluation framework. Validated on well-known 2D and 3D games, PlayGen achieves\nreal-time interaction, ensures sufficient visual quality, and provides accurate\ninteractive mechanics simulation. Notably, these results are sustained even\nafter over 1000 frames of gameplay on an NVIDIA RTX 2060 GPU. Our code is\npublicly available: https://github.com/GreatX3/Playable-Game-Generation. Our\nplayable demo generated by AI is: http://124.156.151.207.",
      "tldr_zh": "这篇论文探讨了人工智能生成内容(AIGC)从文本到图像、视频及多模态合成的进展，并强调了生成可玩游戏的挑战，如实时交互、高视觉质量和准确模拟游戏机制。作者提出了一种新方法PlayGen，包括游戏数据生成、自回归DiT-based扩散模型，以及一个基于可玩性的综合评估框架。在知名2D和3D游戏上验证，PlayGen实现了实时交互、优秀视觉效果和精确机制模拟，即使在NVIDIA RTX 2060 GPU上运行超过1000帧，性能保持稳定。该研究还公开了代码和演示链接，促进进一步应用。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00887v1",
      "published_date": "2024-12-01 16:53:02 UTC",
      "updated_date": "2024-12-01 16:53:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:08:50.588827"
    },
    {
      "arxiv_id": "2412.00881v1",
      "title": "Learn to Unlearn: Meta-Learning-Based Knowledge Graph Embedding Unlearning",
      "title_zh": "翻译失败",
      "authors": [
        "Naixing Xu",
        "Qian Li",
        "Xu Wang",
        "Bingchen Liu",
        "Xin Li"
      ],
      "abstract": "Knowledge graph (KG) embedding methods map entities and relations into\ncontinuous vector spaces, improving performance in tasks like link prediction\nand question answering. With rising privacy concerns, machine unlearning (MU)\nhas emerged as a critical AI technology, enabling models to eliminate the\ninfluence of specific data. Existing MU approaches often rely on data\nobfuscation and adjustments to training loss but lack generalization across\nunlearning tasks. This paper introduces MetaEU, a Meta-Learning-Based Knowledge\nGraph Embedding Unlearning framework. MetaEU leverages meta-learning to unlearn\nspecific embeddings, mitigating their impact while preserving model performance\non remaining data. Experiments on benchmark datasets demonstrate its\neffectiveness in KG embedding unlearning.",
      "tldr_zh": "该论文针对知识图（Knowledge Graph）嵌入模型在链接预测等任务中的隐私问题，引入了机器无学习（Machine Unlearning, MU）技术，以消除特定数据的影响。现有 MU 方法依赖数据混淆和训练损失调整，但泛化能力不足。为此，作者提出 MetaEU，这是一个基于元学习（Meta-Learning）的知识图嵌入无学习框架，能够有效去除特定嵌入的影响，同时保持模型在剩余数据上的性能。在基准数据集上的实验证明，MetaEU 在 KG 嵌入无学习任务中表现出色。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00881v1",
      "published_date": "2024-12-01 16:43:04 UTC",
      "updated_date": "2024-12-01 16:43:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:09:01.297995"
    },
    {
      "arxiv_id": "2412.00876v4",
      "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic Vision-language Context Sparsification",
      "title_zh": "Dynamic-LLaVA：通过动态视觉-语言上下文稀疏化实现的高效多模态大型语言模型",
      "authors": [
        "Wenxuan Huang",
        "Zijie Zhai",
        "Yunhang Shen",
        "Shaosheng Cao",
        "Fei Zhao",
        "Xiangfeng Xu",
        "Zheyu Ye",
        "Yao Hu",
        "Shaohui Lin"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .",
      "tldr_zh": "该论文提出 Dynamic-LLaVA 框架，通过动态视觉-语言上下文稀疏化来提升 Multimodal Large Language Models (MLLMs) 的推理效率，解决输出 token 生成过程中计算和内存开销不断增加的问题。该框架针对预填充阶段和解码阶段（包括有/无 KV cache 的情况）设计了定制化稀疏化方案，能够在预填充阶段减少约 75% 计算消耗，并在整个生成过程中节省约 50% 计算消耗和 GPU 内存开销。实验结果表明，Dynamic-LLaVA 与全上下文基线相比，实现了高效推理，同时理解和生成能力几乎无损甚至有所提升。代码已在 GitHub 上开源。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ICLR 2025. Code is available at\n  https://github.com/Osilly/dynamic_llava",
      "pdf_url": "http://arxiv.org/pdf/2412.00876v4",
      "published_date": "2024-12-01 16:32:31 UTC",
      "updated_date": "2025-03-21 13:30:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:09:15.479928"
    },
    {
      "arxiv_id": "2412.00869v2",
      "title": "KnowledgePrompts: Exploring the Abilities of Large Language Models to Solve Proportional Analogies via Knowledge-Enhanced Prompting",
      "title_zh": "翻译失败",
      "authors": [
        "Thilini Wijesiriwardene",
        "Ruwan Wickramarachchi",
        "Sreeram Vennam",
        "Vinija Jain",
        "Aman Chadha",
        "Amitava Das",
        "Ponnurangam Kumaraguru",
        "Amit Sheth"
      ],
      "abstract": "Making analogies is fundamental to cognition. Proportional analogies, which\nconsist of four terms, are often used to assess linguistic and cognitive\nabilities. For instance, completing analogies like \"Oxygen is to Gas as <blank>\nis to <blank>\" requires identifying the semantic relationship (e.g., \"type of\")\nbetween the first pair of terms (\"Oxygen\" and \"Gas\") and finding a second pair\nthat shares the same relationship (e.g., \"Aluminum\" and \"Metal\"). In this work,\nwe introduce a 15K Multiple-Choice Question Answering (MCQA) dataset for\nproportional analogy completion and evaluate the performance of contemporary\nLarge Language Models (LLMs) in various knowledge-enhanced prompt settings.\nSpecifically, we augment prompts with three types of knowledge: exemplar,\nstructured, and targeted. Our results show that despite extensive training\ndata, solving proportional analogies remains challenging for current LLMs, with\nthe best model achieving an accuracy of 55%. Notably, we find that providing\ntargeted knowledge can better assist models in completing proportional\nanalogies compared to providing exemplars or collections of structured\nknowledge. Our code and data are available at:\nhttps://github.com/Thiliniiw/KnowledgePrompts/",
      "tldr_zh": "本研究探讨了大语言模型（LLMs）通过知识增强提示（Knowledge-Enhanced Prompting）解决比例类比（Proportional Analogies）的能力，引入了一个包含15K多项选择问答（MCQA）数据集，用于测试模型完成类比任务，如“氧气是气体的一种，铝是______的一种”。研究评估了三种知识类型（exemplar、structured和targeted）的提示增强效果，结果显示当前LLMs在该任务上仍面临挑战，最佳模型准确率仅为55%。特别地，提供targeted knowledge比exemplar或structured knowledge更有效地提升了模型的表现，为未来LLMs改进类比推理提供了新见解。数据集和代码可从GitHub获取。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at COLING 2025",
      "pdf_url": "http://arxiv.org/pdf/2412.00869v2",
      "published_date": "2024-12-01 16:15:14 UTC",
      "updated_date": "2024-12-19 04:38:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:09:25.657559"
    },
    {
      "arxiv_id": "2412.00864v2",
      "title": "Explicit and data-Efficient Encoding via Gradient Flow",
      "title_zh": "翻译失败",
      "authors": [
        "Kyriakos Flouris",
        "Anna Volokitin",
        "Gustav Bredell",
        "Ender Konukoglu"
      ],
      "abstract": "The autoencoder model typically uses an encoder to map data to a lower\ndimensional latent space and a decoder to reconstruct it. However, relying on\nan encoder for inversion can lead to suboptimal representations, particularly\nlimiting in physical sciences where precision is key. We introduce a\ndecoder-only method using gradient flow to directly encode data into the latent\nspace, defined by ordinary differential equations (ODEs). This approach\neliminates the need for approximate encoder inversion. We train the decoder via\nthe adjoint method and show that costly integrals can be avoided with minimal\naccuracy loss. Additionally, we propose a $2^{nd}$ order ODE variant,\napproximating Nesterov's accelerated gradient descent for faster convergence.\nTo handle stiff ODEs, we use an adaptive solver that prioritizes loss\nminimization, improving robustness. Compared to traditional autoencoders, our\nmethod demonstrates explicit encoding and superior data efficiency, which is\ncrucial for data-scarce scenarios in the physical sciences. Furthermore, this\nwork paves the way for integrating machine learning into scientific workflows,\nwhere precise and efficient encoding is critical. \\footnote{The code for this\nwork is available at \\url{https://github.com/k-flouris/gfe}.}",
      "tldr_zh": "这篇论文提出了一种基于梯度流(gradient flow)的仅解码器方法，通过常微分方程(ODEs)直接将数据编码到潜在空间，避免了传统自编码器依赖编码器导致的次优表示问题。作者使用伴随方法(adjoint method)训练解码器，并引入二阶ODE变体以模拟Nesterov's accelerated gradient descent，实现更快收敛，同时采用自适应求解器处理刚性ODE以提升鲁棒性。与传统自编码器相比，该方法提供显式编码和更高的数据效率，特别适用于物理科学中数据稀缺的场景，并为机器学习在科学工作流中的整合铺平道路。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "math.OC",
        "physics.comp-ph"
      ],
      "primary_category": "stat.ML",
      "comment": "Machine Learning and the Physical Sciences Workshop, NeurIPS 2024.\n  arXiv admin note: text overlap with arXiv:2105.05031",
      "pdf_url": "http://arxiv.org/pdf/2412.00864v2",
      "published_date": "2024-12-01 15:54:50 UTC",
      "updated_date": "2025-01-04 05:09:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:09:39.621799"
    },
    {
      "arxiv_id": "2412.00860v1",
      "title": "Deep evolving semi-supervised anomaly detection",
      "title_zh": "翻译失败",
      "authors": [
        "Jack Belham",
        "Aryan Bhosale",
        "Samrat Mukherjee",
        "Biplab Banerjee",
        "Fabio Cuzzolin"
      ],
      "abstract": "The aim of this paper is to formalise the task of continual semi-supervised\nanomaly detection (CSAD), with the aim of highlighting the importance of such a\nproblem formulation which assumes as close to real-world conditions as\npossible. After an overview of the relevant definitions of continual\nsemi-supervised learning, its components, anomaly detection extension, and the\ntraining protocols; the paper introduces a baseline model of a variational\nautoencoder (VAE) to work with semi-supervised data along with a continual\nlearning method of deep generative replay with outlier rejection. The results\nshow that such a use of extreme value theory (EVT) applied to anomaly detection\ncan provide promising results even in comparison to an upper baseline of joint\ntraining. The results explore the effects of how much labelled and unlabelled\ndata is present, of which class, and where it is located in the data stream.\nOutlier rejection shows promising initial results where it often surpasses a\nbaseline method of Elastic Weight Consolidation (EWC). A baseline for CSAD is\nput forward along with the specific dataset setups used for reproducability and\ntestability for other practitioners. Future research directions include other\nCSAD settings and further research into efficient continual hyperparameter\ntuning.",
      "tldr_zh": "本论文正式化了持续半监督异常检测（CSAD）的任务，强调其接近真实世界的条件，并概述了持续半监督学习的相关定义、组件和训练协议。研究引入了一个基于变分自编码器（VAE）的基线模型，结合深度生成重放和异常值拒绝（outlier rejection）方法，利用极值理论（EVT）来处理半监督数据。实验结果显示，该方法在不同标记/未标记数据量、类别和数据流位置上表现出色，往往优于Elastic Weight Consolidation (EWC)的基线，并提供可重现的数据集设置。未来研究方向包括探索其他CSAD设置和高效的持续超参数调整。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00860v1",
      "published_date": "2024-12-01 15:48:37 UTC",
      "updated_date": "2024-12-01 15:48:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:10:11.778529"
    },
    {
      "arxiv_id": "2412.04498v2",
      "title": "Large Language Models in Politics and Democracy: A Comprehensive Survey",
      "title_zh": "大型语言模型在政治和民主中：全面综述",
      "authors": [
        "Goshi Aoki"
      ],
      "abstract": "The advancement of generative AI, particularly large language models (LLMs),\nhas a significant impact on politics and democracy, offering potential across\nvarious domains, including policymaking, political communication, analysis, and\ngovernance. This paper surveys the recent and potential applications of LLMs in\npolitics, examining both their promises and the associated challenges. This\npaper examines the ways in which LLMs are being employed in legislative\nprocesses, political communication, and political analysis. Moreover, we\ninvestigate the potential of LLMs in diplomatic and national security contexts,\neconomic and social modeling, and legal applications. While LLMs offer\nopportunities to enhance efficiency, inclusivity, and decision-making in\npolitical processes, they also present challenges related to bias,\ntransparency, and accountability. The paper underscores the necessity for\nresponsible development, ethical considerations, and governance frameworks to\nensure that the integration of LLMs into politics aligns with democratic values\nand promotes a more just and equitable society.",
      "tldr_zh": "这篇论文对大型语言模型（LLMs）在政治和民主领域的应用进行了全面调查，涵盖了其在政策制定、政治沟通、分析、治理、外交、国家安全、经济建模以及法律等方面的潜力。调查揭示了 LLMs 能提升效率、包容性和决策质量，但也面临偏见、透明度和责任性等挑战。论文强调，需要通过负责任的开发、伦理考虑和治理框架，确保 LLMs 的整合符合民主价值观，促进更公正的社会。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages",
      "pdf_url": "http://arxiv.org/pdf/2412.04498v2",
      "published_date": "2024-12-01 15:23:34 UTC",
      "updated_date": "2024-12-16 05:27:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:10:02.037185"
    },
    {
      "arxiv_id": "2412.00846v1",
      "title": "Improving Multimodal LLMs Ability In Geometry Problem Solving, Reasoning, And Multistep Scoring",
      "title_zh": "翻译失败",
      "authors": [
        "Avinash Anand",
        "Raj Jaiswal",
        "Abhishek Dharmadhikari",
        "Atharva Marathe",
        "Harsh Parimal Popat",
        "Harshil Mital",
        "Kritarth Prasad",
        "Rajiv Ratn Shah",
        "Roger Zimmermann"
      ],
      "abstract": "This paper presents GPSM4K, a comprehensive geometry multimodal dataset\ntailored to augment the problem-solving capabilities of Large Vision Language\nModels (LVLMs). GPSM4K encompasses 2157 multimodal question-answer pairs\nmanually extracted from mathematics textbooks spanning grades 7-12 and is\nfurther augmented to 5340 problems, consisting of both numerical and\ntheorem-proving questions. In contrast to PGPS9k, Geometry3K, and Geo170K which\nfeature only objective-type questions, GPSM4K offers detailed step-by-step\nsolutions in a consistent format, facilitating a comprehensive evaluation of\nproblem-solving approaches. This dataset serves as an excellent benchmark for\nassessing the geometric reasoning capabilities of LVLMs. Evaluation of our test\nset shows that there is scope for improvement needed in open-source language\nmodels in geometry problem-solving. Finetuning on our training set increases\nthe geometry problem-solving capabilities of models. Further, We also evaluate\nthe effectiveness of techniques such as image captioning and Retrieval\nAugmentation generation (RAG) on model performance. We leveraged LLM to\nautomate the task of final answer evaluation by providing ground truth and\npredicted solutions. This research will help to assess and improve the\ngeometric reasoning capabilities of LVLMs.",
      "tldr_zh": "这篇论文介绍了 GPSM4K，一个包含 5340 个多模态几何问题的综合数据集，用于提升 Large Vision Language Models (LVLMs) 在几何问题解决、推理和多步评分方面的能力。该数据集从 7-12 年级数学教科书手动提取，提供详细的逐步解决方案，与 PGPS9k、Geometry3K 和 Geo170K 等数据集不同，强调全面评估问题解决方法。实验结果显示，开源模型在几何问题上仍有改进空间，通过在训练集上微调以及应用 image captioning 和 Retrieval Augmentation Generation (RAG) 等技术，能显著提高模型性能。此外，研究利用 LLM 自动化最终答案评估，帮助评估和增强 LVLMs 的几何推理能力。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "15 pages",
      "pdf_url": "http://arxiv.org/pdf/2412.00846v1",
      "published_date": "2024-12-01 15:19:23 UTC",
      "updated_date": "2024-12-01 15:19:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:10:14.767672"
    },
    {
      "arxiv_id": "2412.00833v1",
      "title": "AlignMamba: Enhancing Multimodal Mamba with Local and Global Cross-modal Alignment",
      "title_zh": "AlignMamba：通过局部和全局跨模态对齐增强多模态 Mamba",
      "authors": [
        "Yan Li",
        "Yifei Xing",
        "Xiangyuan Lan",
        "Xin Li",
        "Haifeng Chen",
        "Dongmei Jiang"
      ],
      "abstract": "Cross-modal alignment is crucial for multimodal representation fusion due to\nthe inherent heterogeneity between modalities. While Transformer-based methods\nhave shown promising results in modeling inter-modal relationships, their\nquadratic computational complexity limits their applicability to long-sequence\nor large-scale data. Although recent Mamba-based approaches achieve linear\ncomplexity, their sequential scanning mechanism poses fundamental challenges in\ncomprehensively modeling cross-modal relationships. To address this limitation,\nwe propose AlignMamba, an efficient and effective method for multimodal fusion.\nSpecifically, grounded in Optimal Transport, we introduce a local cross-modal\nalignment module that explicitly learns token-level correspondences between\ndifferent modalities. Moreover, we propose a global cross-modal alignment loss\nbased on Maximum Mean Discrepancy to implicitly enforce the consistency between\ndifferent modal distributions. Finally, the unimodal representations after\nlocal and global alignment are passed to the Mamba backbone for further\ncross-modal interaction and multimodal fusion. Extensive experiments on\ncomplete and incomplete multimodal fusion tasks demonstrate the effectiveness\nand efficiency of the proposed method.",
      "tldr_zh": "该论文提出 AlignMamba 方法，以本地和全局跨模态对齐增强多模态 Mamba 模型，解决 Transformer 计算复杂性和 Mamba 顺序扫描机制在建模跨模态关系上的局限性。具体而言，该方法引入基于 Optimal Transport 的本地跨模态对齐模块来学习不同模态的 token-level 对应关系，并使用 Maximum Mean Discrepancy 的全局对齐损失来强制模态分布一致性，随后将对齐后的表示传递到 Mamba 主干进行融合。实验结果显示，AlignMamba 在完整和不完整的多模态融合任务上表现出色，具有更高的有效性和效率。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00833v1",
      "published_date": "2024-12-01 14:47:41 UTC",
      "updated_date": "2024-12-01 14:47:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:10:27.086889"
    },
    {
      "arxiv_id": "2412.00830v1",
      "title": "SPILDL: A Scalable and Parallel Inductive Learner in Description Logic",
      "title_zh": "SPILDL：描述逻辑中的可扩展并行归纳学习器",
      "authors": [
        "Eyad Algahtani"
      ],
      "abstract": "We present SPILDL, a Scalable and Parallel Inductive Learner in Description\nLogic (DL). SPILDL is based on the DL-Learner (the state of the art in DL-based\nILP learning). As a DL-based ILP learner, SPILDL targets the\n$\\mathcal{ALCQI}^{\\mathcal{(D)}}$ DL language, and can learn DL hypotheses\nexpressed as disjunctions of conjunctions (using the $\\sqcup$ operator).\nMoreover, SPILDL's hypothesis language also incorporates the use of string\nconcrete roles (also known as string data properties in the Web Ontology\nLanguage, OWL); As a result, this incorporation of powerful DL constructs,\nenables SPILDL to learn powerful DL-based hypotheses for describing many\nreal-world complex concepts. SPILDL employs a hybrid parallel approach which\ncombines both shared-memory and distributed-memory approaches, to accelerates\nILP learning (for both hypothesis search and evaluation). According to\nexperimental results, SPILDL's parallel search improved performance by up to\n$\\sim$27.3 folds (best case). For hypothesis evaluation, SPILDL improved\nevaluation performance through HT-HEDL (our multi-core CPU + multi-GPU\nhypothesis evaluation engine), by up to 38 folds (best case). By combining both\nparallel search and evaluation, SPILDL improved performance by up to $\\sim$560\nfolds (best case). In terms of worst case scenario, SPILDL's parallel search\ndoesn't provide consistent speedups on all datasets, and is highly dependent on\nthe search space nature of the ILP dataset. For some datasets, increasing the\nnumber of parallel search threads result in reduced performance, similar or\nworse than baseline. Some ILP datasets benefit from parallel search, while\nothers don't (or the performance gains are negligible). In terms of parallel\nevaluation, on small datasets, parallel evaluation provide similar or worse\nperformance than baseline.",
      "tldr_zh": "本研究引入了SPILDL，一种可扩展并行的描述逻辑(Description Logic, DL)归纳学习器，基于DL-Learner框架，针对$\\mathcal{ALCQI}^{\\mathcal{(D)}}$语言学习DL假设，包括使用$\\sqcup$操作符的合取和析取，以及字符串具体角色，以更好地描述真实世界的复杂概念。SPILDL采用混合并行方法，结合共享内存(shared-memory)和分布式内存(distributed-memory)技术，通过并行搜索和HT-HEDL评估引擎，显著加速假设搜索和评估过程。实验结果显示，搜索性能最高提升约27.3倍，评估性能最高提升约38倍，整体性能最高提升约560倍；然而，在某些数据集上，并行搜索可能导致性能下降或无显著收益，取决于数据集的搜索空间特性。",
      "categories": [
        "cs.AI",
        "cs.DC",
        "cs.DS",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00830v1",
      "published_date": "2024-12-01 14:33:37 UTC",
      "updated_date": "2024-12-01 14:33:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:10:38.488933"
    },
    {
      "arxiv_id": "2412.00821v1",
      "title": "Improving Physics Reasoning in Large Language Models Using Mixture of Refinement Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Raj Jaiswal",
        "Dhruv Jain",
        "Harsh Parimal Popat",
        "Avinash Anand",
        "Abhishek Dharmadhikari",
        "Atharva Marathe",
        "Rajiv Ratn Shah"
      ],
      "abstract": "Large Language Models (LLMs) demonstrate remarkable capabilities in various\nreasoning tasks. However, they encounter significant challenges when it comes\nto scientific reasoning, particularly in physics, which requires not only\nmathematical reasoning but also factual and conceptual understanding. When\naddressing complex physics problems, LLMs typically face three key issues:\nproblem miscomprehension, incorrect concept application, and computational\nerrors. While each of these problems can be addressed individually, there is a\nneed for a generalized approach that can tackle all three issues\nsimultaneously. To address this, we introduce Mixture of Refinement Agents\n(MoRA), a novel agentic refinement framework that iteratively refines the LLM\ngenerated base solution by correcting the aforementioned errors, resulting in a\nsignificant performance improvement for open-source LLMs. Our approach aims to\nbridge the gap between opensource LLMs and GPT-4o by utilizing the latter as\nerror identifier to guide these refinement agents. We evaluate our approach on\nthe SciEval and MMLU subsets along with our own physics dataset (PhysicsQA).\nMoRA significantly improves the performance of Llama-3-70B and Gemma-2-27B on\nthese datasets, achieving up to a 16% increase in final answer accuracy.",
      "tldr_zh": "大型语言模型 (LLMs) 在物理推理任务中常面临问题误解、概念应用错误和计算错误等挑战，导致性能不足。论文提出 Mixture of Refinement Agents (MoRA)，一个创新的代理框架，通过迭代精炼 LLMs 的基解决方案来同时纠正这些问题，并利用 GPT-4o 作为错误识别器进行指导。在 SciEval、MMLU 和自创的 PhysicsQA 数据集上，MoRA 显著提升了 Llama-3-70B 和 Gemma-2-27B 的性能，最多提高 16% 的最终答案准确率。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "7 pages",
      "pdf_url": "http://arxiv.org/pdf/2412.00821v1",
      "published_date": "2024-12-01 14:15:55 UTC",
      "updated_date": "2024-12-01 14:15:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:10:52.545725"
    },
    {
      "arxiv_id": "2412.00810v1",
      "title": "Long text outline generation: Chinese text outline based on unsupervised framework and large language mode",
      "title_zh": "翻译失败",
      "authors": [
        "Yan Yan",
        "Yuanchi Ma"
      ],
      "abstract": "Outline generation aims to reveal the internal structure of a document by\nidentifying underlying chapter relationships and generating corresponding\nchapter summaries. Although existing deep learning methods and large models\nperform well on small- and medium-sized texts, they struggle to produce\nreadable outlines for very long texts (such as fictional works), often failing\nto segment chapters coherently. In this paper, we propose a novel outline\ngeneration method for Chinese, combining an unsupervised framework with large\nmodels. Specifically, the method first generates chapter feature graph data\nbased on entity and syntactic dependency relationships. Then, a representation\nmodule based on graph attention layers learns deep embeddings of the chapter\ngraph data. Using these chapter embeddings, we design an operator based on\nMarkov chain principles to segment plot boundaries. Finally, we employ a large\nmodel to generate summaries of each plot segment and produce the overall\noutline. We evaluate our model based on segmentation accuracy and outline\nreadability, and our performance outperforms several deep learning models and\nlarge models in comparative evaluations.",
      "tldr_zh": "这篇论文针对长文本（如小说）的中文大纲生成问题，提出了一种结合无监督 framework 和 large language model 的新方法，以揭示文档的内部结构并生成章节摘要。方法包括：首先基于实体和句法依赖关系生成章节特征图数据，然后使用基于 graph attention layers 的表示模块学习深度嵌入，并通过 Markov chain 原理设计操作符来分割情节边界，最后由 large language model 生成每个段落的摘要和整体大纲。实验评估显示，该模型在分割准确性和大纲可读性方面优于现有深度学习模型和大型模型，为长文本结构分析提供了有效工具。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00810v1",
      "published_date": "2024-12-01 13:46:15 UTC",
      "updated_date": "2024-12-01 13:46:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:11:02.675776"
    },
    {
      "arxiv_id": "2412.00807v1",
      "title": "Generative Model for Synthesizing Ionizable Lipids: A Monte Carlo Tree Search Approach",
      "title_zh": "用于合成可电离脂质的生成模型：一种蒙特卡罗树搜索方法",
      "authors": [
        "Jingyi Zhao",
        "Yuxuan Ou",
        "Austin Tripp",
        "Morteza Rasoulianboroujeni",
        "José Miguel Hernández-Lobato"
      ],
      "abstract": "Ionizable lipids are essential in developing lipid nanoparticles (LNPs) for\neffective messenger RNA (mRNA) delivery. While traditional methods for\ndesigning new ionizable lipids are typically time-consuming, deep generative\nmodels have emerged as a powerful solution, significantly accelerating the\nmolecular discovery process. However, a practical challenge arises as the\nmolecular structures generated can often be difficult or infeasible to\nsynthesize. This project explores Monte Carlo tree search (MCTS)-based\ngenerative models for synthesizable ionizable lipids. Leveraging a\nsynthetically accessible lipid building block dataset and two specialized\npredictors to guide the search through chemical space, we introduce a policy\nnetwork guided MCTS generative model capable of producing new ionizable lipids\nwith available synthesis pathways.",
      "tldr_zh": "本研究针对离子化脂质（Ionizable lipids）的设计问题，指出传统方法耗时长且深度生成模型常产生难以合成的分子结构。研究者提出了一种基于Monte Carlo Tree Search (MCTS)的生成模型，利用合成可访问的脂质构建块数据集和两个专门预测器来引导化学空间搜索，并引入policy network进行优化，以生成具有可用合成路径的新型离子化脂质。该方法为脂质纳米颗粒（LNPs）用于mRNA递送的分子发现过程提供了更高效、可行的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00807v1",
      "published_date": "2024-12-01 13:34:22 UTC",
      "updated_date": "2024-12-01 13:34:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:11:13.816795"
    },
    {
      "arxiv_id": "2412.00802v1",
      "title": "HT-HEDL: High-Throughput Hypothesis Evaluation in Description Logic",
      "title_zh": "HT-HEDL：描述逻辑中的高吞吐量假设评估",
      "authors": [
        "Eyad Algahtani"
      ],
      "abstract": "We present High-Throughput Hypothesis Evaluation in Description Logic\n(HT-HEDL). HT-HEDL is a high-performance hypothesis evaluation engine that\naccelerates hypothesis evaluation computations for inductive logic programming\n(ILP) learners using description logic (DL) for their knowledge representation;\nin particular, HT-HEDL targets accelerating computations for the\n$\\mathcal{ALCQI}^{\\mathcal{(D)}}$ DL language. HT-HEDL aggregates the computing\npower of multi-core CPUs with multi-GPUs to improve hypothesis computations at\ntwo levels: 1) the evaluation of a single hypothesis and 2) the evaluation of\nmultiple hypotheses (i.e., batch of hypotheses). In the first level, HT-HEDL\nuses a single GPU or a vectorized multi-threaded CPU to evaluate a single\nhypothesis. In vectorized multi-threaded CPU evaluation, classical (scalar) CPU\nmulti-threading is combined with CPU's extended vector instructions set to\nextract more CPU-based performance. The experimental results revealed that\nHT-HEDL increased performance using CPU-based evaluation (on a single\nhypothesis): from 20.4 folds using classical multi-threading to $\\sim85$ folds\nusing vectorized multi-threading. In the GPU-based evaluation, HT-HEDL achieved\nspeedups of up to $\\sim38$ folds for single hypothesis evaluation using a\nsingle GPU. To accelerate the evaluation of multiple hypotheses, HT-HEDL\ncombines, in parallel, GPUs with multi-core CPUs to increase evaluation\nthroughput (number of evaluated hypotheses per second). The experimental\nresults revealed that HT-HEDL increased evaluation throughput by up to 29.3\nfolds using two GPUs and up to $\\sim44$ folds using two GPUs combined with a\nCPU's vectorized multi-threaded evaluation.",
      "tldr_zh": "本研究提出了 HT-HEDL，一种高性能假设评估引擎，用于加速基于 Description Logic (DL) 的归纳逻辑编程 (ILP) 学习者，特别是针对 $\\mathcal{ALCQI}^{\\mathcal{(D)}}$ 语言的计算。\nHT-HEDL 通过整合多核 CPU 和多 GPU 的计算能力，在单假设评估中采用矢量化多线程 CPU 或单个 GPU，提升性能至多达 85 倍（CPU）和 38 倍（GPU）。\n在批量假设评估中，HT-HEDL 并行结合 GPU 和 CPU，进一步提高评估吞吐量，多达 29.3 倍（两个 GPU）或 44 倍（两个 GPU 加 CPU 矢量化多线程）。\n该框架显著提升了 DL 相关任务的计算效率，为高效的知识表示和假设处理提供了新方法。",
      "categories": [
        "cs.AI",
        "cs.DC",
        "cs.DS",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00802v1",
      "published_date": "2024-12-01 13:01:48 UTC",
      "updated_date": "2024-12-01 13:01:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:11:27.167733"
    },
    {
      "arxiv_id": "2412.00800v2",
      "title": "A Comprehensive Guide to Explainable AI: From Classical Models to LLMs",
      "title_zh": "可解释人工智能的全面指南：从经典模型到大语言模型",
      "authors": [
        "Weiche Hsieh",
        "Ziqian Bi",
        "Chuanqi Jiang",
        "Junyu Liu",
        "Benji Peng",
        "Sen Zhang",
        "Xuanhe Pan",
        "Jiawei Xu",
        "Jinlang Wang",
        "Keyu Chen",
        "Pohsun Feng",
        "Yizhu Wen",
        "Xinyuan Song",
        "Tianyang Wang",
        "Ming Liu",
        "Junjie Yang",
        "Ming Li",
        "Bowen Jing",
        "Jintao Ren",
        "Junhao Song",
        "Hong-Ming Tseng",
        "Yichao Zhang",
        "Lawrence K. Q. Yan",
        "Qian Niu",
        "Silin Chen",
        "Yunze Wang",
        "Chia Xin Liang"
      ],
      "abstract": "Explainable Artificial Intelligence (XAI) addresses the growing need for\ntransparency and interpretability in AI systems, enabling trust and\naccountability in decision-making processes. This book offers a comprehensive\nguide to XAI, bridging foundational concepts with advanced methodologies. It\nexplores interpretability in traditional models such as Decision Trees, Linear\nRegression, and Support Vector Machines, alongside the challenges of explaining\ndeep learning architectures like CNNs, RNNs, and Large Language Models (LLMs),\nincluding BERT, GPT, and T5. The book presents practical techniques such as\nSHAP, LIME, Grad-CAM, counterfactual explanations, and causal inference,\nsupported by Python code examples for real-world applications.\n  Case studies illustrate XAI's role in healthcare, finance, and policymaking,\ndemonstrating its impact on fairness and decision support. The book also covers\nevaluation metrics for explanation quality, an overview of cutting-edge XAI\ntools and frameworks, and emerging research directions, such as\ninterpretability in federated learning and ethical AI considerations. Designed\nfor a broad audience, this resource equips readers with the theoretical\ninsights and practical skills needed to master XAI. Hands-on examples and\nadditional resources are available at the companion GitHub repository:\nhttps://github.com/Echoslayer/XAI_From_Classical_Models_to_LLMs.",
      "tldr_zh": "这本书提供了一个全面的 XAI（Explainable Artificial Intelligence）指南，从传统模型如 Decision Trees、Linear Regression 和 Support Vector Machines，到深度学习架构如 CNNs、RNNs 和 LLMs（包括 BERT、GPT 和 T5），探讨了这些模型的可解释性挑战和解决方案。作者介绍了实用技术如 SHAP、LIME、Grad-CAM、反事实解释和因果推理，并通过 Python 代码示例和案例研究展示了 XAI 在医疗、金融和政策制定中的应用，以提升公平性和决策支持。书籍还涵盖了解释质量的评估指标、XAI 工具框架以及新兴研究方向，如联邦学习中的可解释性和道德 AI 考虑，并附带 GitHub 资源以供实践。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00800v2",
      "published_date": "2024-12-01 13:01:01 UTC",
      "updated_date": "2024-12-08 06:24:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:11:38.368032"
    },
    {
      "arxiv_id": "2412.00789v2",
      "title": "A Cognac shot to forget bad memories: Corrective Unlearning in GNNs",
      "title_zh": "翻译失败",
      "authors": [
        "Varshita Kolipaka",
        "Akshit Sinha",
        "Debangan Mishra",
        "Sumit Kumar",
        "Arvindh Arun",
        "Shashwat Goel",
        "Ponnurangam Kumaraguru"
      ],
      "abstract": "Graph Neural Networks (GNNs) are increasingly being used for a variety of ML\napplications on graph data. Because graph data does not follow the\nindependently and identically distributed (i.i.d.) assumption, adversarial\nmanipulations or incorrect data can propagate to other data points through\nmessage passing, which deteriorates the model's performance. To allow model\ndevelopers to remove the adverse effects of manipulated entities from a trained\nGNN, we study the recently formulated problem of Corrective Unlearning. We find\nthat current graph unlearning methods fail to unlearn the effect of\nmanipulations even when the whole manipulated set is known. We introduce a new\ngraph unlearning method, Cognac, which can unlearn the effect of the\nmanipulation set even when only 5% of it is identified. It recovers most of the\nperformance of a strong oracle with fully corrected training data, even beating\nretraining from scratch without the deletion set while being 8x more efficient.\nWe hope our work assists GNN developers in mitigating harmful effects caused by\nissues in real-world data post-training. Our code is publicly available at\nhttps://github.com/varshitakolipaka/corrective-unlearning-for-gnns",
      "tldr_zh": "该研究探讨了图神经网络(GNNs)中 Corrective Unlearning 的问题，旨在消除操纵数据通过消息传递对模型性能的负面影响，因为图数据并非独立同分布(i.i.d.)。作者提出了一种新方法 Cognac，即使仅识别操纵集的5%，也能有效移除这些数据的影响，并恢复模型性能，优于从零重新训练且效率高8倍。实验结果显示，Cognac 显著提升了 GNNs 的鲁棒性，有助于开发者在训练后缓解真实世界数据中的有害问题。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00789v2",
      "published_date": "2024-12-01 12:23:25 UTC",
      "updated_date": "2024-12-09 15:14:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:11:49.347879"
    },
    {
      "arxiv_id": "2412.00787v1",
      "title": "TSUBF-Net: Trans-Spatial UNet-like Network with Bi-direction Fusion for Segmentation of Adenoid Hypertrophy in CT",
      "title_zh": "翻译失败",
      "authors": [
        "Rulin Zhou",
        "Yingjie Feng",
        "Guankun Wang",
        "Xiaopin Zhong",
        "Zongze Wu",
        "Qiang Wu",
        "Xi Zhang"
      ],
      "abstract": "Adenoid hypertrophy stands as a common cause of obstructive sleep\napnea-hypopnea syndrome in children. It is characterized by snoring, nasal\ncongestion, and growth disorders. Computed Tomography (CT) emerges as a pivotal\nmedical imaging modality, utilizing X-rays and advanced computational\ntechniques to generate detailed cross-sectional images. Within the realm of\npediatric airway assessments, CT imaging provides an insightful perspective on\nthe shape and volume of enlarged adenoids. Despite the advances of deep\nlearning methods for medical imaging analysis, there remains an emptiness in\nthe segmentation of adenoid hypertrophy in CT scans. To address this research\ngap, we introduce TSUBF-Nett (Trans-Spatial UNet-like Network based on\nBi-direction Fusion), a 3D medical image segmentation framework. TSUBF-Net is\nengineered to effectively discern intricate 3D spatial interlayer features in\nCT scans and enhance the extraction of boundary-blurring features. Notably, we\npropose two innovative modules within the U-shaped network architecture:the\nTrans-Spatial Perception module (TSP) and the Bi-directional Sampling\nCollaborated Fusion module (BSCF).These two modules are in charge of operating\nduring the sampling process and strategically fusing down-sampled and\nup-sampled features, respectively. Furthermore, we introduce the Sobel loss\nterm, which optimizes the smoothness of the segmentation results and enhances\nmodel accuracy. Extensive 3D segmentation experiments are conducted on several\ndatasets. TSUBF-Net is superior to the state-of-the-art methods with the lowest\nHD95: 7.03, IoU:85.63, and DSC: 92.26 on our own AHSD dataset. The results in\nthe other two public datasets also demonstrate that our methods can robustly\nand effectively address the challenges of 3D segmentation in CT scans.",
      "tldr_zh": "该研究针对儿童腺样体肥大（Adenoid Hypertrophy）导致的睡眠呼吸障碍问题，提出了一种新的3D医疗图像分割框架TSUBF-Net，该框架基于UNet-like网络，旨在提升CT扫描中模糊边界特征的提取。TSUBF-Net引入了两个创新模块：Trans-Spatial Perception (TSP)模块用于处理复杂的3D空间特征，以及Bi-directional Sampling Collaborated Fusion (BSCF)模块负责融合下采样和上采样特征，同时结合Sobel loss优化分割结果的光滑性和准确性。在实验中，TSUBF-Net在自有AHSD数据集上取得了优于现有方法的性能，包括HD95: 7.03、IoU: 85.63和DSC: 92.26，并在其他公共数据集上显示出鲁棒性。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00787v1",
      "published_date": "2024-12-01 12:21:23 UTC",
      "updated_date": "2024-12-01 12:21:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:12:02.420691"
    },
    {
      "arxiv_id": "2412.00777v2",
      "title": "Local vs. Global: Local Land-Use and Land-Cover Models Deliver Higher Quality Maps",
      "title_zh": "局部 vs. 全局：局部土地利用和土地覆盖模型提供更高质量的地图",
      "authors": [
        "Girmaw Abebe Tadesse",
        "Caleb Robinson",
        "Charles Mwangi",
        "Esther Maina",
        "Joshua Nyakundi",
        "Luana Marotti",
        "Gilles Quentin Hacheme",
        "Hamed Alemohammad",
        "Rahul Dodhia",
        "Juan M. Lavista Ferres"
      ],
      "abstract": "In 2023, 58.0% of the African population experienced moderate to severe food\ninsecurity, with 21.6% facing severe food insecurity. Land-use and land-cover\nmaps provide crucial insights for addressing food insecurity by improving\nagricultural efforts, including mapping and monitoring crop types and\nestimating yield. The development of global land-cover maps has been\nfacilitated by the increasing availability of earth observation data and\nadvancements in geospatial machine learning. However, these global maps exhibit\nlower accuracy and inconsistencies in Africa, partly due to the lack of\nrepresentative training data. To address this issue, we propose a data-centric\nframework with a teacher-student model setup, which uses diverse data sources\nof satellite images and label examples to produce local land-cover maps. Our\nmethod trains a high-resolution teacher model on images with a resolution of\n0.331 m/pixel and a low-resolution student model on publicly available images\nwith a resolution of 10 m/pixel. The student model also utilizes the teacher\nmodel's output as its weak label examples through knowledge transfer. We\nevaluated our framework using Murang'a county in Kenya, renowned for its\nagricultural productivity, as a use case. Our local models achieved higher\nquality maps, with improvements of 0.14 in the F1 score and 0.21 in\nIntersection-over-Union, compared to the best global model. Our evaluation also\nrevealed inconsistencies in existing global maps, with a maximum agreement rate\nof 0.30 among themselves. Our work provides valuable guidance to\ndecision-makers for driving informed decisions to enhance food security.",
      "tldr_zh": "该研究针对非洲土地覆盖地图的低准确性和不一致性问题，提出了一种数据中心框架，使用 teacher-student 模型来生成高质量的本地土地利用和土地覆盖（land-use and land-cover）地图。该框架通过在高分辨率图像（0.331 m/像素）上训练 teacher 模型，并在低分辨率图像（10 m/像素）上训练 student 模型，同时利用知识转移作为弱标签，实现对卫星图像的有效处理。在肯尼亚Murang'a县的评估中，本地模型相比最佳全球模型在F1 score上提高了0.14，在Intersection-over-Union (IoU)上提高了0.21，并揭示了现有全球地图之间最大同意率仅为0.30。该方法为决策者提供指导，帮助提升农业努力和粮食安全。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00777v2",
      "published_date": "2024-12-01 11:48:58 UTC",
      "updated_date": "2024-12-11 15:11:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:14:06.934837"
    },
    {
      "arxiv_id": "2412.00773v1",
      "title": "DIVD: Deblurring with Improved Video Diffusion Model",
      "title_zh": "翻译失败",
      "authors": [
        "Haoyang Long",
        "Yan Wang",
        "Wendong Wang"
      ],
      "abstract": "Video deblurring presents a considerable challenge owing to the complexity of\nblur, which frequently results from a combination of camera shakes, and object\nmotions. In the field of video deblurring, many previous works have primarily\nconcentrated on distortion-based metrics, such as PSNR. However, this approach\noften results in a weak correlation with human perception and yields\nreconstructions that lack realism. Diffusion models and video diffusion models\nhave respectively excelled in the fields of image and video generation,\nparticularly achieving remarkable results in terms of image authenticity and\nrealistic perception. However, due to the computational complexity and\nchallenges inherent in adapting diffusion models, there is still uncertainty\nregarding the potential of video diffusion models in video deblurring tasks. To\nexplore the viability of video diffusion models in the task of video\ndeblurring, we introduce a diffusion model specifically for this purpose. In\nthis field, leveraging highly correlated information between adjacent frames\nand addressing the challenge of temporal misalignment are crucial research\ndirections. To tackle these challenges, many improvements based on the video\ndiffusion model are introduced in this work. As a result, our model outperforms\nexisting models and achieves state-of-the-art results on a range of perceptual\nmetrics. Our model preserves a significant amount of detail in the images while\nmaintaining competitive distortion metrics. Furthermore, to the best of our\nknowledge, this is the first time the diffusion model has been applied in video\ndeblurring to overcome the limitations mentioned above.",
      "tldr_zh": "本文提出 DIVD，一种改进的视频扩散模型，用于解决视频去模糊问题，该问题常因相机抖动和物体运动导致模糊，且传统方法如基于 PSNR 的指标与人类感知相关性弱。DIVD 通过利用相邻帧的高度相关信息并处理时间不对齐挑战，对视频扩散模型进行优化，提升了图像真实性和细节保留。实验结果表明，该模型在各种感知指标上达到最先进水平，同时保持竞争力的扭曲指标，这是首次将 diffusion model 应用于视频 deblurring 以克服上述限制。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00773v1",
      "published_date": "2024-12-01 11:39:02 UTC",
      "updated_date": "2024-12-01 11:39:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:12:25.455749"
    },
    {
      "arxiv_id": "2412.00772v1",
      "title": "A Wave is Worth 100 Words: Investigating Cross-Domain Transferability in Time Series",
      "title_zh": "翻译失败",
      "authors": [
        "Xiangkai Ma",
        "Xiaobin Hong",
        "Wenzhong Li",
        "Sanglu Lu"
      ],
      "abstract": "Time series analysis is a fundamental data mining task that supervised\ntraining methods based on empirical risk minimization have proven their\neffectiveness on specific tasks and datasets. However, the acquisition of\nwell-annotated data is costly and a large amount of unlabeled series data is\nunder-utilized. Due to distributional shifts across various domains and\ndifferent patterns of interest across multiple tasks. The problem of\ncross-domain multi-task migration of time series remains a significant\nchallenge. To address these problems, this paper proposes a novel cross-domain\npretraining method based on Wave Quantization (termed as WQ4TS), which can be\ncombined with any advanced time series model and applied to multiple downstream\ntasks. Specifically, we transfer the time series data from different domains\ninto a common spectral latent space, and enable the model to learn the temporal\npattern knowledge of different domains directly from the common space and\nutilize it for the inference of downstream tasks, thereby mitigating the\nchallenge of heterogeneous cross-domains migration. The establishment of\nspectral latent space brings at least three benefits, cross-domain migration\ncapability thus adapting to zero- and few-shot scenarios without relying on\npriori knowledge of the dataset, general compatible cross-domain migration\nframework without changing the existing model structure, and robust modeling\ncapability thus achieving SOTA results in multiple downstream tasks. To\ndemonstrate the effectiveness of the proposed approach, we conduct extensive\nexperiments including three important tasks: forecasting, imputation, and\nclassification. And three common real-world data scenarios are simulated:\nfull-data, few-shot, and zero-shot. The proposed WQ4TS achieves the best\nperformance on 87.5% of all tasks, and the average improvement of the metrics\non all the tasks is up to 34.7%.",
      "tldr_zh": "本论文探讨了时间序列分析中的跨域转移性问题，针对数据标注成本高和分布偏移挑战，提出了一种新型预训练方法Wave Quantization (WQ4TS)。该方法将不同领域的时间序列数据映射到共同的谱隐空间中，允许模型直接从该空间学习时序模式知识，从而实现跨域多任务迁移，而无需依赖先验知识或修改现有模型结构。实验结果显示，WQ4TS在预测、插值和分类等下游任务上，在全数据、少样本和零样本场景中表现出色，87.5%的任务达到最佳性能，平均指标提升高达34.7%。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00772v1",
      "published_date": "2024-12-01 11:35:06 UTC",
      "updated_date": "2024-12-01 11:35:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:14:18.132610"
    },
    {
      "arxiv_id": "2412.00765v1",
      "title": "SelfPrompt: Autonomously Evaluating LLM Robustness via Domain-Constrained Knowledge Guidelines and Refined Adversarial Prompts",
      "title_zh": "翻译失败",
      "authors": [
        "Aihua Pei",
        "Zehua Yang",
        "Shunan Zhu",
        "Ruoxi Cheng",
        "Ju Jia"
      ],
      "abstract": "Traditional methods for evaluating the robustness of large language models\n(LLMs) often rely on standardized benchmarks, which can escalate costs and\nlimit evaluations across varied domains. This paper introduces a novel\nframework designed to autonomously evaluate the robustness of LLMs by\nincorporating refined adversarial prompts and domain-constrained knowledge\nguidelines in the form of knowledge graphs. Our method systematically generates\ndescriptive sentences from domain-constrained knowledge graph triplets to\nformulate adversarial prompts, enhancing the relevance and challenge of the\nevaluation. These prompts, generated by the LLM itself and tailored to evaluate\nits own robustness, undergo a rigorous filtering and refinement process,\nensuring that only those with high textual fluency and semantic fidelity are\nused. This self-evaluation mechanism allows the LLM to evaluate its robustness\nwithout the need for external benchmarks. We assess the effectiveness of our\nframework through extensive testing on both proprietary models like ChatGPT and\nopen-source models such as Llama-3.1, Phi-3, and Mistral. Results confirm that\nour approach not only reduces dependency on conventional data but also provides\na targeted and efficient means of evaluating LLM robustness in constrained\ndomains.",
      "tldr_zh": "该论文提出SelfPrompt框架，用于自主评估大型语言模型(LLM)的鲁棒性，通过领域约束知识指南(以知识图谱形式)和精炼对抗提示(adversarial prompts)来克服传统基准的局限性。该方法从知识图谱三元组生成描述性句子，形成针对性的对抗提示，并由LLM自身生成并经过严格过滤，确保文本流畅性和语义准确性，从而实现无需外部基准的自评估机制。在ChatGPT和开源模型如Llama-3.1、Phi-3、Mistral上的测试显示，该框架显著减少了对传统数据的依赖，提供高效、针对性的鲁棒性评估。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00765v1",
      "published_date": "2024-12-01 10:58:53 UTC",
      "updated_date": "2024-12-01 10:58:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:14:30.322252"
    },
    {
      "arxiv_id": "2412.00763v1",
      "title": "PGSO: Prompt-based Generative Sequence Optimization Network for Aspect-based Sentiment Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Hao Dong",
        "Wei Wei"
      ],
      "abstract": "Recently, generative pre-training based models have demonstrated remarkable\nresults on Aspect-based Sentiment Analysis (ABSA) task. However, previous works\noveremphasize crafting various templates to paraphrase training targets for\nenhanced decoding, ignoring the internal optimizations on generative models.\nDespite notable results achieved by these target-oriented optimization methods,\nthey struggle with the complicated long texts since the implicit long-distance\nrelation, e.g., aspect-opinion relation, is difficult to extract under the\nposition embedding mechanism in generative models. Thus, in this paper, we\nfirst clarify the causes of the problem and introduce two sequence optimization\nstrategies: the rule-based static optimization and the score-based dynamic\noptimization. The rule-based approach relies on handcraft priority of\ndependency relation to reorder the context, while the score-based algorithm\ndynamically regulates the contextual sequence by calculating word position\nscores using neural network. Based on the dynamic optimization structure, we\nfurther propose a unified Prompt-based Generative Sequence Optimization network\n(named PGSO), which jointly optimizes the training target as well as the\ngenerative model. Specifically, PGSO contains two components, namely, prompt\nconstruction and sequence regulator. The former constructs a task-specific\nprompt based on unsupervised training objects to fully utilize the pre-trained\nmodel. The latter jointly leverages semantic, syntactic and original-sequence\ninformation to dynamically regulate contextual sequence. Our experiments\nconducted on four ABSA tasks across multiple benchmarks indicate that PGSO\noutperforms state-of-the-art methods, with an average improvement of 3.52% in\nF1 score.",
      "tldr_zh": "本论文提出PGSO，一种基于Prompt的生成式序列优化网络，用于Aspect-based Sentiment Analysis (ABSA)，旨在解决生成模型在处理长文本时提取长距离关系（如aspect-opinion关系）的问题。\nPGSO引入两种序列优化策略：基于规则的静态优化和基于分数的动态优化，并通过prompt construction构建任务特定prompt，以及sequence regulator结合语义、句法和原始序列信息动态调节上下文。\n实验在四个ABSA任务的多个基准上表明，PGSO比现有方法平均F1分数提高了3.52%。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00763v1",
      "published_date": "2024-12-01 10:49:55 UTC",
      "updated_date": "2024-12-01 10:49:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:14:42.492696"
    },
    {
      "arxiv_id": "2412.00761v1",
      "title": "Learning to Forget using Hypernetworks",
      "title_zh": "翻译失败",
      "authors": [
        "Jose Miguel Lara Rangel",
        "Stefan Schoepf",
        "Jack Foster",
        "David Krueger",
        "Usman Anwar"
      ],
      "abstract": "Machine unlearning is gaining increasing attention as a way to remove\nadversarial data poisoning attacks from already trained models and to comply\nwith privacy and AI regulations. The objective is to unlearn the effect of\nundesired data from a trained model while maintaining performance on the\nremaining data. This paper introduces HyperForget, a novel machine unlearning\nframework that leverages hypernetworks - neural networks that generate\nparameters for other networks - to dynamically sample models that lack\nknowledge of targeted data while preserving essential capabilities. Leveraging\ndiffusion models, we implement two Diffusion HyperForget Networks and used them\nto sample unlearned models in Proof-of-Concept experiments. The unlearned\nmodels obtained zero accuracy on the forget set, while preserving good accuracy\non the retain sets, highlighting the potential of HyperForget for dynamic\ntargeted data removal and a promising direction for developing adaptive machine\nunlearning algorithms.",
      "tldr_zh": "这篇论文提出了 HyperForget，一种新型机器 unlearning 框架，利用 hypernetworks（超网络）来动态采样模型，从而从已训练模型中移除目标数据的知识，同时保留其他数据性能。框架通过整合 diffusion models（扩散模型）实现两个 Diffusion HyperForget Networks，并在概念验证实验中测试。结果显示，未学习模型在遗忘集上准确率达到零，而在保留集上保持良好准确率，这为动态目标数据移除和开发自适应机器 unlearning 算法提供了有前景的方向。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "AdvML-Frontiers'24: The 3rd Workshop on New Frontiers in Adversarial\n  Machine Learning@NeurIPS'24, Vancouver, CA",
      "pdf_url": "http://arxiv.org/pdf/2412.00761v1",
      "published_date": "2024-12-01 10:43:11 UTC",
      "updated_date": "2024-12-01 10:43:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:14:53.840681"
    },
    {
      "arxiv_id": "2412.00760v1",
      "title": "Automating Feedback Analysis in Surgical Training: Detection, Categorization, and Assessment",
      "title_zh": "手术培训中的反馈分析自动化：检测、分类和评估",
      "authors": [
        "Firdavs Nasriddinov",
        "Rafal Kocielnik",
        "Arushi Gupta",
        "Cherine Yang",
        "Elyssa Wong",
        "Anima Anandkumar",
        "Andrew Hung"
      ],
      "abstract": "This work introduces the first framework for reconstructing surgical dialogue\nfrom unstructured real-world recordings, which is crucial for characterizing\nteaching tasks. In surgical training, the formative verbal feedback that\ntrainers provide to trainees during live surgeries is crucial for ensuring\nsafety, correcting behavior immediately, and facilitating long-term skill\nacquisition. However, analyzing and quantifying this feedback is challenging\ndue to its unstructured and specialized nature. Automated systems are essential\nto manage these complexities at scale, allowing for the creation of structured\ndatasets that enhance feedback analysis and improve surgical education. Our\nframework integrates voice activity detection, speaker diarization, and\nautomated speech recaognition, with a novel enhancement that 1) removes\nhallucinations (non-existent utterances generated during speech recognition\nfueled by noise in the operating room) and 2) separates speech from trainers\nand trainees using few-shot voice samples. These aspects are vital for\nreconstructing accurate surgical dialogues and understanding the roles of\noperating room participants. Using data from 33 real-world surgeries, we\ndemonstrated the system's capability to reconstruct surgical teaching dialogues\nand detect feedback instances effectively (F1 score of 0.79+/-0.07). Moreover,\nour hallucination removal step improves feedback detection performance by ~14%.\nEvaluation on downstream clinically relevant tasks of predicting Behavioral\nAdjustment of trainees and classifying Technical feedback, showed performances\ncomparable to manual annotations with F1 scores of 0.82+/0.03 and 0.81+/0.03\nrespectively. These results highlight the effectiveness of our framework in\nsupporting clinically relevant tasks and improving over manual methods.",
      "tldr_zh": "这篇论文引入了一个创新框架，用于从非结构化真实世界录音中重建手术对话，从而实现手术训练中反馈的检测、分类和评估。该框架整合了 voice activity detection、speaker diarization 和 automated speech recognition，并通过移除 hallucinations（由手术室噪音引起的虚假话语）和使用少-shot 语音样本分离培训师与学员的语音，来提升对话重建的准确性。在33个真实手术数据上，系统在反馈检测任务中取得了0.79+/-0.07的F1 score，并通过移除 hallucinations 提高了约14%的性能。此外，在下游任务中，预测学员行为调整（Behavioral Adjustment）和分类技术反馈（Technical feedback）的F1 scores 分别为0.82+/-0.03和0.81+/-0.03，与手动标注相当，从而证明了框架在自动化手术教育中的有效性。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.ET",
        "cs.LG",
        "68T50, 68U99, 68T99",
        "I.2; I.2.7; I.5.4; J.3; K.3.1"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted as a proceedings paper at Machine Learning for Health 2024",
      "pdf_url": "http://arxiv.org/pdf/2412.00760v1",
      "published_date": "2024-12-01 10:35:12 UTC",
      "updated_date": "2024-12-01 10:35:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:15:07.963749"
    },
    {
      "arxiv_id": "2412.00754v1",
      "title": "CtrlNeRF: The Generative Neural Radiation Fields for the Controllable Synthesis of High-fidelity 3D-Aware Images",
      "title_zh": "翻译失败",
      "authors": [
        "Jian Liu",
        "Zhen Yu"
      ],
      "abstract": "The neural radiance field (NERF) advocates learning the continuous\nrepresentation of 3D geometry through a multilayer perceptron (MLP). By\nintegrating this into a generative model, the generative neural radiance field\n(GRAF) is capable of producing images from random noise z without 3D\nsupervision. In practice, the shape and appearance are modeled by z_s and z_a,\nrespectively, to manipulate them separately during inference. However, it is\nchallenging to represent multiple scenes using a solitary MLP and precisely\ncontrol the generation of 3D geometry in terms of shape and appearance. In this\npaper, we introduce a controllable generative model (i.e. \\textbf{CtrlNeRF})\nthat uses a single MLP network to represent multiple scenes with shared\nweights. Consequently, we manipulated the shape and appearance codes to realize\nthe controllable generation of high-fidelity images with 3D consistency.\nMoreover, the model enables the synthesis of novel views that do not exist in\nthe training sets via camera pose alteration and feature interpolation.\nExtensive experiments were conducted to demonstrate its superiority in 3D-aware\nimage generation compared to its counterparts.",
      "tldr_zh": "本文提出CtrlNeRF，一种基于NeRF的可控生成模型，使用单个MLP网络以共享权重表示多个场景，实现对形状和外观代码的精确操纵，从而合成高保真、3D一致的图像。该模型还支持通过改变相机姿态和特征插值生成训练集中不存在的新视图。实验结果表明，CtrlNeRF在3D感知图像生成任务中优于现有方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00754v1",
      "published_date": "2024-12-01 10:19:24 UTC",
      "updated_date": "2024-12-01 10:19:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:15:17.759895"
    },
    {
      "arxiv_id": "2412.00751v1",
      "title": "Rethinking Cognition: Morphological Info-Computation and the Embodied Paradigm in Life and Artificial Intelligence",
      "title_zh": "重新思考认知：生命与人工智能中的形态信息计算和具身范式",
      "authors": [
        "Gordana Dodig-Crnkovic"
      ],
      "abstract": "This study aims to place Lorenzo Magnanis Eco-Cognitive Computationalism\nwithin the broader context of current work on information, computation, and\ncognition. Traditionally, cognition was believed to be exclusive to humans and\na result of brain activity. However, recent studies reveal it as a fundamental\ncharacteristic of all life forms, ranging from single cells to complex\nmulticellular organisms and their networks. Yet, the literature and general\nunderstanding of cognition still largely remain human-brain-focused, leading to\nconceptual gaps and incoherency. This paper presents a variety of computational\n(information processing) approaches, including an info-computational approach\nto cognition, where natural structures represent information and dynamical\nprocesses on natural structures are regarded as computation, relative to an\nobserving cognizing agent. We model cognition as a web of concurrent\nmorphological computations, driven by processes of self-assembly,\nself-organisation, and autopoiesis across physical, chemical, and biological\ndomains. We examine recent findings linking morphological computation,\nmorphogenesis, agency, basal cognition, extended evolutionary synthesis, and\nactive inference. We establish a connection to Magnanis Eco-Cognitive\nComputationalism and the idea of computational domestication of ignorant\nentities. Novel theoretical and applied insights question the boundaries of\nconventional computational models of cognition. The traditional models\nprioritize symbolic processing and often neglect the inherent constraints and\npotentialities in the physical embodiment of agents on different levels of\norganization. Gaining a better info-computational grasp of cognitive embodiment\nis crucial for the advancement of fields such as biology, evolutionary studies,\nartificial intelligence, robotics, medicine, and more.",
      "tldr_zh": "这篇论文重新审视认知，将Lorenzo Magnani的Eco-Cognitive Computationalism置于信息、计算和认知的更广泛语境中，强调认知是所有生命形式的基本特征，而非仅限于人类大脑活动。\n论文采用info-computational方法，将认知建模为形态计算的网络，受自组装、自组织和autopoiesis等过程驱动，并探讨morphological computation、morphogenesis、agency、basal cognition、extended evolutionary synthesis和active inference等概念。\n该研究质疑传统符号处理模型的局限性，强调认知的实体化（embodiment）对生物学、进化研究、人工智能、机器人学和医学等领域的进步具有关键意义。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00751v1",
      "published_date": "2024-12-01 10:04:53 UTC",
      "updated_date": "2024-12-01 10:04:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:15:31.291578"
    },
    {
      "arxiv_id": "2412.00749v2",
      "title": "CONCERTO: Complex Query Execution Mechanism-Aware Learned Cost Estimation",
      "title_zh": "翻译失败",
      "authors": [
        "Kaixin Zhang",
        "Hongzhi Wang",
        "Kunkai Gu",
        "Ziqi Li",
        "Chunyu Zhao",
        "Yingze Li",
        "Yu Yan"
      ],
      "abstract": "With the growing demand for massive data analysis, many DBMSs have adopted\ncomplex underlying query execution mechanisms, including vectorized operators,\nparallel execution, and dynamic pipeline modifications. However, there remains\na lack of targeted Query Performance Prediction (QPP) methods for these complex\nexecution mechanisms and their interactions, as most existing approaches focus\non traditional tree-shaped query plans and static serial executors. To address\nthis challenge, this paper proposes CONCERTO, a Complex query executiON\nmeChanism-awaE leaRned cosT estimatiOn method. CONCERTO first establishes\nindependent resource cost models for each physical operator. It then constructs\na Directed Acyclic Graph (DAG) consisting of a dataflow tree backbone and\nresource competition relationships among concurrent operators. After\ncalibrating the cost impact of parallel operator execution using Graph\nAttention Networks (GATs) with additional attention mechanisms, CONCERTO\nextracts and aggregates cost vector trees through Temporal Convolutional\nNetworks (TCNs), ultimately achieving effective query performance prediction.\nExperimental results demonstrate that CONCERTO achieves higher prediction\naccuracy than existing methods.",
      "tldr_zh": "该论文提出CONCERTO，一种针对复杂查询执行机制的学习式成本估算方法，旨在解决数据库管理系统(DBMS)中向量化操作符、并行执行和动态管道修改等交互问题的Query Performance Prediction (QPP)挑战。CONCERTO首先为每个物理操作符建立独立的资源成本模型，并构建一个包含数据流树骨干和资源竞争关系的Directed Acyclic Graph (DAG)。随后，通过Graph Attention Networks (GATs)校准并行执行的成本影响，并利用Temporal Convolutional Networks (TCNs)提取和聚合成本向量树，最终实现精确的查询性能预测。实验结果显示，CONCERTO比现有方法具有更高的预测准确性。",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "primary_category": "cs.DB",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00749v2",
      "published_date": "2024-12-01 09:58:54 UTC",
      "updated_date": "2025-03-28 12:47:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:15:42.311751"
    },
    {
      "arxiv_id": "2412.00748v1",
      "title": "Exploring Cognition through Morphological Info-Computational Framework",
      "title_zh": "翻译失败",
      "authors": [
        "Gordana Dodig-Crnkovic"
      ],
      "abstract": "Traditionally, cognition has been considered a uniquely human capability\ninvolving perception, memory, learning, reasoning, and problem-solving.\nHowever, recent research shows that cognition is a fundamental ability shared\nby all living beings, from single cells to complex organisms. This chapter\ntakes an info-computational approach (ICON), viewing natural structures as\ninformation and the processes of change in these structures as computations. It\nis a relational framework dependent on the perspective of a cognizing\nobserver/cognizer. Informational structures are properties of the material\nsubstrate, and when focusing on the behavior of the substrate, we discuss\nmorphological computing (MC). ICON and MC are complementary perspectives for a\ncognizer. Information and computation are inseparably connected with cognition.\nThis chapter explores research connecting nature as a computational structure\nfor a cognizer, with morphological computation, morphogenesis, agency, extended\ncognition, and extended evolutionary synthesis, using examples of the free\nenergy principle and active inference. It introduces theoretical and practical\napproaches challenging traditional computational models of cognition limited to\nabstract symbol processing, highlighting the computational capacities inherent\nin the material substrate (embodiment). Understanding the embodiment of\ncognition through its morphological computational basis is crucial for biology,\nevolution, intelligence theory, AI, robotics, and other fields.",
      "tldr_zh": "本文通过 info-computational (ICON) 框架探索认知，强调认知并非人类独有，而是所有生物从单细胞到复杂有机体的基本能力，将自然结构视为信息，并将变化过程视为计算。框架结合 morphological computing (MC)，从观察者视角分析认知与物质基底的关联，并使用自由能量原理和主动推理等例子，展示形态发生、代理和扩展进化等概念。研究挑战传统以抽象符号处理为主的认知模型，突出物质基底的计算能力，这对生物学、进化理论、AI 和机器人等领域具有关键意义。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00748v1",
      "published_date": "2024-12-01 09:56:38 UTC",
      "updated_date": "2024-12-01 09:56:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:15:54.041768"
    },
    {
      "arxiv_id": "2412.00744v1",
      "title": "A Cross-Scene Benchmark for Open-World Drone Active Tracking",
      "title_zh": "翻译失败",
      "authors": [
        "Haowei Sun",
        "Jinwu Hu",
        "Zhirui Zhang",
        "Haoyuan Tian",
        "Xinze Xie",
        "Yufeng Wang",
        "Zhuliang Yu",
        "Xiaohua Xie",
        "Mingkui Tan"
      ],
      "abstract": "Drone Visual Active Tracking aims to autonomously follow a target object by\ncontrolling the motion system based on visual observations, providing a more\npractical solution for effective tracking in dynamic environments. However,\naccurate Drone Visual Active Tracking using reinforcement learning remains\nchallenging due to the absence of a unified benchmark, the complexity of\nopen-world environments with frequent interference, and the diverse motion\nbehavior of dynamic targets. To address these issues, we propose a unified\ncross-scene cross-domain benchmark for open-world drone active tracking called\nDAT. The DAT benchmark provides 24 visually complex environments to assess the\nalgorithms' cross-scene and cross-domain generalization abilities, and\nhigh-fidelity modeling of realistic robot dynamics. Additionally, we propose a\nreinforcement learning-based drone tracking method called R-VAT, which aims to\nimprove the performance of drone tracking targets in complex scenarios.\nSpecifically, inspired by curriculum learning, we introduce a Curriculum-Based\nTraining strategy that progressively enhances the agent tracking performance in\nvast environments with complex interference. We design a goal-centered reward\nfunction to provide precise feedback to the drone agent, preventing targets\nfarther from the center of view from receiving higher rewards than closer ones.\nThis allows the drone to adapt to the diverse motion behavior of open-world\ntargets. Experiments demonstrate that the R-VAT has about 400% improvement over\nthe SOTA method in terms of the cumulative reward metric.",
      "tldr_zh": "该论文针对无人机视觉主动跟踪（Drone Visual Active Tracking）提出一个统一的跨场景跨领域基准DAT，以解决开放世界环境中缺乏基准、干扰复杂以及目标运动多样性的挑战。DAT包括24个视觉复杂的环境，并模拟真实机器人动态，以评估算法的泛化能力。作者还开发了基于强化学习（reinforcement learning）的跟踪方法R-VAT，通过Curriculum-Based Training策略逐步提升代理在复杂场景下的性能，并设计了goal-centered reward function来提供精确反馈，确保无人机更准确地跟踪动态目标。实验结果显示，R-VAT在累积奖励指标上比最先进方法（SOTA）提高了约400%。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "25 pages",
      "pdf_url": "http://arxiv.org/pdf/2412.00744v1",
      "published_date": "2024-12-01 09:37:46 UTC",
      "updated_date": "2024-12-01 09:37:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:16:05.807099"
    },
    {
      "arxiv_id": "2412.00742v1",
      "title": "Revisiting Self-Supervised Heterogeneous Graph Learning from Spectral Clustering Perspective",
      "title_zh": "从谱聚类视角重新审视自监督异构图学习",
      "authors": [
        "Yujie Mo",
        "Zhihe Lu",
        "Runpeng Yu",
        "Xiaofeng Zhu",
        "Xinchao Wang"
      ],
      "abstract": "Self-supervised heterogeneous graph learning (SHGL) has shown promising\npotential in diverse scenarios. However, while existing SHGL methods share a\nsimilar essential with clustering approaches, they encounter two significant\nlimitations: (i) noise in graph structures is often introduced during the\nmessage-passing process to weaken node representations, and (ii) cluster-level\ninformation may be inadequately captured and leveraged, diminishing the\nperformance in downstream tasks. In this paper, we address these limitations by\ntheoretically revisiting SHGL from the spectral clustering perspective and\nintroducing a novel framework enhanced by rank and dual consistency\nconstraints. Specifically, our framework incorporates a rank-constrained\nspectral clustering method that refines the affinity matrix to exclude noise\neffectively. Additionally, we integrate node-level and cluster-level\nconsistency constraints that concurrently capture invariant and clustering\ninformation to facilitate learning in downstream tasks. We theoretically\ndemonstrate that the learned representations are divided into distinct\npartitions based on the number of classes and exhibit enhanced generalization\nability across tasks. Experimental results affirm the superiority of our\nmethod, showcasing remarkable improvements in several downstream tasks compared\nto existing methods.",
      "tldr_zh": "本论文从 Spectral Clustering 的视角重新审视 Self-Supervised Heterogeneous Graph Learning (SHGL)，解决了现有方法在消息传递过程中引入噪声以及未能充分捕获集群级信息的问题，从而提升下游任务性能。论文提出一个新框架，结合 Rank-Constrained Spectral Clustering 方法来提炼亲和矩阵排除噪声，并整合节点级和集群级 Dual Consistency 约束，以同时捕获不变和聚类信息。理论分析证明，该框架能将学到的表示分为基于类别数量的 distinct partitions，并增强任务的泛化能力；实验结果显示，与现有方法相比，该方法在多个下游任务上取得了显著改进。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00742v1",
      "published_date": "2024-12-01 09:33:20 UTC",
      "updated_date": "2024-12-01 09:33:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:16:18.767010"
    },
    {
      "arxiv_id": "2412.00726v1",
      "title": "Free and Customizable Code Documentation with LLMs: A Fine-Tuning Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Sayak Chakrabarty",
        "Souradip Pal"
      ],
      "abstract": "Automated documentation of programming source code is a challenging task with\nsignificant practical and scientific implications for the developer community.\nWe present a large language model (LLM)-based application that developers can\nuse as a support tool to generate basic documentation for any publicly\navailable repository. Over the last decade, several papers have been written on\ngenerating documentation for source code using neural network architectures.\nWith the recent advancements in LLM technology, some open-source applications\nhave been developed to address this problem. However, these applications\ntypically rely on the OpenAI APIs, which incur substantial financial costs,\nparticularly for large repositories. Moreover, none of these open-source\napplications offer a fine-tuned model or features to enable users to fine-tune.\nAdditionally, finding suitable data for fine-tuning is often challenging. Our\napplication addresses these issues which is available at\nhttps://pypi.org/project/readme-ready/.",
      "tldr_zh": "本研究提出了一种基于大型语言模型（LLM）的免费代码文档生成应用，采用 fine-tuning 方式，帮助开发者为公开仓库自动创建基本文档。相比现有依赖 OpenAI API 的工具，该应用避免了高昂的财务成本，并提供用户可自定义的 fine-tuning 功能，同时解决了 fine-tuning 数据获取的难题。该应用已在 https://pypi.org/project/readme-ready/ 发布，为代码文档自动化提供了更实用和可访问的解决方案。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00726v1",
      "published_date": "2024-12-01 08:38:18 UTC",
      "updated_date": "2024-12-01 08:38:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:16:29.197111"
    },
    {
      "arxiv_id": "2412.00725v1",
      "title": "Decision Transformer vs. Decision Mamba: Analysing the Complexity of Sequential Decision Making in Atari Games",
      "title_zh": "翻译失败",
      "authors": [
        "Ke Yan"
      ],
      "abstract": "This work analyses the disparity in performance between Decision Transformer\n(DT) and Decision Mamba (DM) in sequence modelling reinforcement learning tasks\nfor different Atari games. The study first observed that DM generally\noutperformed DT in the games Breakout and Qbert, while DT performed better in\nmore complicated games, such as Hero and Kung Fu Master. To understand these\ndifferences, we expanded the number of games to 12 and performed a\ncomprehensive analysis of game characteristics, including action space\ncomplexity, visual complexity, average trajectory length, and average steps to\nthe first non-zero reward. In order to further analyse the key factors that\nimpact the disparity in performance between DT and DM, we employ various\napproaches, including quantifying visual complexity, random forest regression,\ncorrelation analysis, and action space simplification strategies. The results\nindicate that the performance gap between DT and DM is affected by the complex\ninteraction of multiple factors, with the complexity of the action space and\nvisual complexity (particularly evaluated by compression ratio) being the\nprimary determining factors. DM performs well in environments with simple\naction and visual elements, while DT shows an advantage in games with higher\naction and visual complexity. Our findings contribute to a deeper understanding\nof how the game characteristics affect the performance difference in sequential\nmodelling reinforcement learning, potentially guiding the development of future\nmodel design and applications for diverse and complex environments.",
      "tldr_zh": "本研究比较了 Decision Transformer (DT) 和 Decision Mamba (DM) 在 Atari 游戏中的性能差异，发现 DM 在简单游戏如 Breakout 和 Qbert 中表现优于 DT，而 DT 在更复杂的游戏如 Hero 和 Kung Fu Master 中更具优势。研究者扩展到 12 个游戏，通过分析动作空间复杂度、视觉复杂度、轨迹长度和到第一个非零奖励的步数，并采用随机森林回归、相关分析和动作空间简化策略等方法，确定性能差距主要受动作空间复杂度和视觉复杂度（尤其是压缩比）的影响。结果表明，DM 适合简单环境，DT 更适用于高复杂度场景，这为顺序建模强化学习模型的设计和应用提供了重要指导。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00725v1",
      "published_date": "2024-12-01 08:37:10 UTC",
      "updated_date": "2024-12-01 08:37:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:16:42.499010"
    },
    {
      "arxiv_id": "2412.00724v1",
      "title": "AdaScale: Dynamic Context-aware DNN Scaling via Automated Adaptation Loop on Mobile Devices",
      "title_zh": "AdaScale：动态上下文感知的 DNN 缩放，通过自动适应循环在移动设备上",
      "authors": [
        "Yuzhan Wang",
        "Sicong Liu",
        "Bin Guo",
        "Boqi Zhang",
        "Ke Ma",
        "Yasan Ding",
        "Hao Luo",
        "Yao Li",
        "Zhiwen Yu"
      ],
      "abstract": "Deep learning is reshaping mobile applications, with a growing trend of\ndeploying deep neural networks (DNNs) directly to mobile and embedded devices\nto address real-time performance and privacy. To accommodate local resource\nlimitations, techniques like weight compression, convolution decomposition, and\nspecialized layer architectures have been developed. However, the\n\\textit{dynamic} and \\textit{diverse} deployment contexts of mobile devices\npose significant challenges. Adapting deep models to meet varied\ndevice-specific requirements for latency, accuracy, memory, and energy is\nlabor-intensive. Additionally, changing processor states, fluctuating memory\navailability, and competing processes frequently necessitate model\nre-compression to preserve user experience. To address these issues, we\nintroduce AdaScale, an elastic inference framework that automates the\nadaptation of deep models to dynamic contexts. AdaScale leverages a\nself-evolutionary model to streamline network creation, employs diverse\ncompression operator combinations to reduce the search space and improve\noutcomes, and integrates a resource availability awareness block and\nperformance profilers to establish an automated adaptation loop. Our\nexperiments demonstrate that AdaScale significantly enhances accuracy by 5.09%,\nreduces training overhead by 66.89%, speeds up inference latency by 1.51 to 6.2\ntimes, and lowers energy costs by 4.69 times.",
      "tldr_zh": "本文提出AdaScale框架，用于在移动设备上动态适应DNN（Deep Neural Networks），以应对资源限制和变化环境下的性能挑战。该框架通过自演化模型、多种压缩操作组合以及资源感知块和性能分析器，构建自动化适应循环，实现高效的网络优化。实验结果显示，AdaScale提高了5.09%的准确性，减少了66.89%的训练开销，推理延迟加快1.51到6.2倍，并降低了4.69倍的能源消耗。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00724v1",
      "published_date": "2024-12-01 08:33:56 UTC",
      "updated_date": "2024-12-01 08:33:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:16:53.836588"
    },
    {
      "arxiv_id": "2412.00722v1",
      "title": "Towards Adaptive Mechanism Activation in Language Agent",
      "title_zh": "翻译失败",
      "authors": [
        "Ziyang Huang",
        "Jun Zhao",
        "Kang Liu"
      ],
      "abstract": "Language Agent could be endowed with different mechanisms for autonomous task\naccomplishment. Current agents typically rely on fixed mechanisms or a set of\nmechanisms activated in a predefined order, limiting their adaptation to varied\npotential task solution structures. To this end, this paper proposes\n\\textbf{A}daptive \\textbf{L}anguage \\textbf{A}gent \\textbf{M}echanism\n\\textbf{A}ctivation Learning with Self-Exploration (\\textbf{ALAMA}), which\nfocuses on optimizing mechanism activation adaptability without reliance on\nexpert models. Initially, it builds a harmonized agent framework\n(\\textbf{UniAct}) to \\textbf{Uni}fy different mechanisms via \\textbf{Act}ions.\nThen it leverages a training-efficient optimization method based on\nself-exploration to enable the UniAct to adaptively activate the appropriate\nmechanisms according to the potential characteristics of the task. Experimental\nresults demonstrate significant improvements in downstream agent tasks,\naffirming the effectiveness of our approach in facilitating more dynamic and\ncontext-sensitive mechanism activation.",
      "tldr_zh": "该论文针对语言代理（Language Agent）中机制激活的适应性问题，提出ALAMA（Adaptive Language Agent Mechanism Activation Learning with Self-Exploration）方法，以优化代理在不同任务结构下的表现，而无需依赖专家模型。首先，该方法构建了UniAct框架，通过统一机制为Actions来协调多种机制，然后利用基于自探索的训练高效优化技术，使代理能根据任务特点自适应地激活合适机制。实验结果显示，ALAMA在下游代理任务上实现了显著改进，证明了其在动态和上下文敏感机制激活方面的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "COLING2025",
      "pdf_url": "http://arxiv.org/pdf/2412.00722v1",
      "published_date": "2024-12-01 08:10:04 UTC",
      "updated_date": "2024-12-01 08:10:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:17:06.138868"
    },
    {
      "arxiv_id": "2412.00721v2",
      "title": "A Comparative Study of LLM-based ASR and Whisper in Low Resource and Code Switching Scenario",
      "title_zh": "翻译失败",
      "authors": [
        "Zheshu Song",
        "Ziyang Ma",
        "Yifan Yang",
        "Jianheng Zhuo",
        "Xie Chen"
      ],
      "abstract": "Large Language Models (LLMs) have showcased exceptional performance across\ndiverse NLP tasks, and their integration with speech encoder is rapidly\nemerging as a dominant trend in the Automatic Speech Recognition (ASR) field.\nPrevious works mainly concentrated on leveraging LLMs for speech recognition in\nEnglish and Chinese. However, their potential for addressing speech recognition\nchallenges in low resource settings remains underexplored. Hence, in this work,\nwe aim to explore the capability of LLMs in low resource ASR and\nMandarin-English code switching ASR. We also evaluate and compare the\nrecognition performance of LLM-based ASR systems against Whisper model.\nExtensive experiments demonstrate that LLM-based ASR yields a relative gain of\n12.8\\% over the Whisper model in low resource ASR while Whisper performs better\nin Mandarin-English code switching ASR. We hope that this study could shed\nlight on ASR for low resource scenarios.",
      "tldr_zh": "本文通过比较大语言模型 (LLMs) 基于的自动语音识别 (ASR) 系统与 Whisper 模型，探讨了 LLMs 在低资源 ASR 和 中英代码切换 (Mandarin-English code switching) 场景中的性能。研究通过广泛实验评估发现，LLM-based ASR 在低资源 ASR 中比 Whisper 模型实现了 12.8% 的相对性能提升，而在代码切换 ASR 中 Whisper 表现更优。总体而言，此研究旨在揭示 LLMs 处理低资源语音识别挑战的潜力，并为相关领域提供宝贵启发。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.AI",
      "comment": "This work hasn't been finished yet",
      "pdf_url": "http://arxiv.org/pdf/2412.00721v2",
      "published_date": "2024-12-01 08:07:01 UTC",
      "updated_date": "2024-12-04 06:23:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:17:18.434220"
    },
    {
      "arxiv_id": "2412.00718v1",
      "title": "Well log data generation and imputation using sequence-based generative adversarial networks",
      "title_zh": "翻译失败",
      "authors": [
        "Abdulrahman Al-Fakih",
        "A. Koeshidayatullah",
        "Tapan Mukerji",
        "Sadam Al-Azani",
        "SanLinn I. Kaka"
      ],
      "abstract": "Well log analysis is crucial for hydrocarbon exploration, providing detailed\ninsights into subsurface geological formations. However, gaps and inaccuracies\nin well log data, often due to equipment limitations, operational challenges,\nand harsh subsurface conditions, can introduce significant uncertainties in\nreservoir evaluation. Addressing these challenges requires effective methods\nfor both synthetic data generation and precise imputation of missing data,\nensuring data completeness and reliability. This study introduces a novel\nframework utilizing sequence-based generative adversarial networks (GANs)\nspecifically designed for well log data generation and imputation. The\nframework integrates two distinct sequence-based GAN models: Time Series GAN\n(TSGAN) for generating synthetic well log data and Sequence GAN (SeqGAN) for\nimputing missing data. Both models were tested on a dataset from the North Sea,\nNetherlands region, focusing on different sections of 5, 10, and 50 data\npoints. Experimental results demonstrate that this approach achieves superior\naccuracy in filling data gaps compared to other deep learning models for\nspatial series analysis. The method yielded R^2 values of 0.921, 0.899, and\n0.594, with corresponding mean absolute percentage error (MAPE) values of\n8.320, 0.005, and 151.154, and mean absolute error (MAE) values of 0.012,\n0.005, and 0.032, respectively. These results set a new benchmark for data\nintegrity and utility in geosciences, particularly in well log data analysis.",
      "tldr_zh": "本研究针对 well log data 中的 gaps 和 inaccuracies 问题，提出一个新框架，利用 sequence-based generative adversarial networks (GANs) 来生成合成数据和补全缺失数据。该框架整合 Time Series GAN (TSGAN) 用于合成 well log data 生成，以及 Sequence GAN (SeqGAN) 用于精确数据补全，并在 North Sea, Netherlands 区域的真实数据集上测试了不同数据点段（5、10 和 50 点）。实验结果显示，该方法比其他深度学习模型更准确，R^2 值分别为 0.921、0.899 和 0.594，MAPE 值分别为 8.320、0.005 和 151.154，MAE 值分别为 0.012、0.005 和 0.032，从而为 geosciences 中的 well log data 分析设定了新基准，提升了数据完整性和可靠性。",
      "categories": [
        "physics.geo-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "physics.geo-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00718v1",
      "published_date": "2024-12-01 07:50:34 UTC",
      "updated_date": "2024-12-01 07:50:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:17:31.169147"
    },
    {
      "arxiv_id": "2412.00707v2",
      "title": "Protect Your Secrets: Understanding and Measuring Data Exposure in VSCode Extensions",
      "title_zh": "保护您的秘密：理解和测量 VSCode 扩展中的数据暴露",
      "authors": [
        "Yue Liu",
        "Chakkrit Tantithamthavorn",
        "Li Li"
      ],
      "abstract": "Recent years have witnessed the emerging trend of extensions in modern\nIntegrated Development Environments (IDEs) like Visual Studio Code (VSCode)\nthat significantly enhance developer productivity. Especially, popular AI\ncoding assistants like GitHub Copilot and Tabnine provide conveniences like\nautomated code completion and debugging. While these extensions offer numerous\nbenefits, they may introduce privacy and security concerns to software\ndevelopers. However, there is no existing work that systematically analyzes the\nsecurity and privacy concerns, including the risks of data exposure in VSCode\nextensions.\n  In this paper, we investigate on the security issues of cross-extension\ninteractions in VSCode and shed light on the vulnerabilities caused by data\nexposure among different extensions. Our study uncovers high-impact security\nflaws that could allow adversaries to stealthily acquire or manipulate\ncredential-related data (e.g., passwords, API keys, access tokens) from other\nextensions if not properly handled by extension vendors. To measure their\nprevalence, we design a novel automated risk detection framework that leverages\nprogram analysis and natural language processing techniques to automatically\nidentify potential risks in VSCode extensions. By applying our tool to 27,261\nreal-world VSCode extensions, we discover that 8.5% of them (i.e., 2,325\nextensions) are exposed to credential-related data leakage through various\nvectors, such as commands, user input, and configurations. Our study sheds\nlight on the security challenges and flaws of the extension-in-IDE paradigm and\nprovides suggestions and recommendations for improving the security of VSCode\nextensions and mitigating the risks of data exposure.",
      "tldr_zh": "这篇论文探讨了Visual Studio Code (VSCode) 扩展在提升开发效率的同时，可能引发的隐私和安全问题，特别是AI编码助手如GitHub Copilot导致的数据暴露风险。研究者系统分析了跨扩展交互的安全漏洞，发现这些漏洞可能允许攻击者窃取或操纵凭证相关数据（如API keys和访问tokens）。他们设计了一个基于程序分析和natural language processing (NLP) 技术的自动化风险检测框架，并应用于27,261个真实VSCode扩展中，揭示8.5%的扩展（即2,325个）存在通过命令、用户输入或配置等向量的数据泄露风险。该研究为改善VSCode扩展的安全性提供了关键建议和推荐，以缓解数据暴露威胁。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00707v2",
      "published_date": "2024-12-01 07:08:53 UTC",
      "updated_date": "2024-12-25 06:49:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:17:41.480968"
    },
    {
      "arxiv_id": "2412.00691v2",
      "title": "The Advancement of Personalized Learning Potentially Accelerated by Generative AI",
      "title_zh": "生成式 AI 可能加速的个性化学习进步",
      "authors": [
        "Yuang Wei",
        "Yuan-Hao Jiang",
        "Jiayi Liu",
        "Changyong Qi",
        "Linzhao Jia",
        "Rui Jia"
      ],
      "abstract": "The rapid development of Generative AI (GAI) has sparked revolutionary\nchanges across various aspects of education. Personalized learning, a focal\npoint and challenge in educational research, has also been influenced by the\ndevelopment of GAI. To explore GAI's extensive impact on personalized learning,\nthis study investigates its potential to enhance various facets of personalized\nlearning through a thorough analysis of existing research. The research\ncomprehensively examines GAI's influence on personalized learning by analyzing\nits application across different methodologies and contexts, including learning\nstrategies, paths, materials, environments, and specific analyses within the\nteaching and learning processes. Through this in-depth investigation, we find\nthat GAI demonstrates exceptional capabilities in providing adaptive learning\nexperiences tailored to individual preferences and needs. Utilizing different\nforms of GAI across various subjects yields superior learning outcomes. The\narticle concludes by summarizing scenarios where GAI is applicable in\neducational processes and discussing strategies for leveraging GAI to enhance\npersonalized learning, aiming to guide educators and learners in effectively\nutilizing GAI to achieve superior learning objectives.",
      "tldr_zh": "该研究探讨了Generative AI (GAI)如何加速个性化学习的发展，通过分析现有文献，考察GAI在学习策略、路径、材料、环境等方面的应用。结果显示，GAI能提供适应个体偏好和需求的动态学习体验，在不同学科中显著提升学习成果。该文总结了GAI在教育过程中的适用场景，并提出利用策略，以指导教育者有效整合GAI，实现更优的学习目标。",
      "categories": [
        "cs.AI",
        "stat.AP"
      ],
      "primary_category": "cs.AI",
      "comment": "The V1 version is a more detailed version, and the latest version is\n  the SITE conference included version. SITE 2025-Orando, Florida, United\n  States, March 17-21.2025",
      "pdf_url": "http://arxiv.org/pdf/2412.00691v2",
      "published_date": "2024-12-01 06:01:14 UTC",
      "updated_date": "2025-02-26 08:54:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:17:52.809804"
    },
    {
      "arxiv_id": "2412.00686v2",
      "title": "LVLM-COUNT: Enhancing the Counting Ability of Large Vision-Language Models",
      "title_zh": "LVLM-COUNT：增强大型视觉语言模型的计数能力",
      "authors": [
        "Muhammad Fetrat Qharabagh",
        "Mohammadreza Ghofrani",
        "Kimon Fountoulakis"
      ],
      "abstract": "Counting is a fundamental operation for various visual tasks in real-life\napplications, requiring both object recognition and robust counting\ncapabilities. Despite their advanced visual perception, large vision-language\nmodels (LVLMs) struggle with counting tasks, especially when the number of\nobjects exceeds those commonly encountered during training. We enhance LVLMs'\ncounting abilities using a divide-and-conquer approach, breaking counting\nproblems into sub-counting tasks. Our method employs a mechanism that prevents\nbisecting and thus repetitive counting of objects, which occurs in a naive\ndivide-and-conquer approach. Unlike prior methods, which do not generalize well\nto counting datasets they have not been trained on, our method performs well on\nnew datasets without any additional training or fine-tuning. We demonstrate\nthat our approach enhances the counting capability of LVLMs across various\ndatasets and benchmarks.",
      "tldr_zh": "大型视觉语言模型 (LVLMs) 在计数任务上表现不佳，尤其当物体数量超出训练常见范围时，本文提出 LVLM-COUNT 方法，使用 divide-and-conquer 策略将计数问题分解为子计数任务，并引入机制防止重复计数。不同于以往方法，该方法无需额外训练或微调，即可泛化到未见数据集。实验结果表明，LVLM-COUNT 显著提升了模型在各种基准上的计数能力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "31 pages, 24 Figures, 10 Tables",
      "pdf_url": "http://arxiv.org/pdf/2412.00686v2",
      "published_date": "2024-12-01 05:50:22 UTC",
      "updated_date": "2025-02-02 17:49:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:18:06.124732"
    },
    {
      "arxiv_id": "2412.00684v2",
      "title": "Paint Outside the Box: Synthesizing and Selecting Training Data for Visual Grounding",
      "title_zh": "翻译失败",
      "authors": [
        "Zilin Du",
        "Haoxin Li",
        "Jianfei Yu",
        "Boyang Li"
      ],
      "abstract": "Visual grounding aims to localize the image regions based on a textual query.\nGiven the difficulty of large-scale data curation, we investigate how to\neffectively learn visual grounding under data-scarce settings in this paper. To\naddress the data scarcity, we propose a novel framework, POBF (Paint Outside\nthe Box and Filter). POBF synthesizes images by inpainting outside the box,\ntackling a label misalignment issue encountered in previous works. Furthermore,\nPOBF leverages an innovative filtering scheme to select the most effective\ntraining data. This scheme combines a hardness score and an overfitting score,\nbalanced by a penalty term. Extensive experiments across four benchmark\ndatasets demonstrate that POBF consistently improves performance, achieving an\naverage gain of 5.83\\% over the real-data-only method and outperforming leading\nbaselines by 2.29\\%-3.85\\% in accuracy. Additionally, we validate the\nrobustness and generalizability of POBF across various generative models,\ntraining data sizes, and model architectures.",
      "tldr_zh": "本文针对视觉定位(Visual Grounding)任务的数据稀缺问题，提出一个新框架POBF（Paint Outside the Box and Filter），通过在框外进行图像修复(inpainting outside the box)合成图像，并解决标签不对齐问题。POBF采用创新的过滤方案，结合难度分数(hardness score)、过拟合分数(overfitting score)和惩罚项来选择最有效的训练数据。实验结果显示，该框架在四个基准数据集上比仅使用真实数据的基准方法提升5.83%的准确率，并优于领先基线2.29%-3.85%。此外，POBF在不同生成模型、训练数据大小和模型架构上展现出良好的鲁棒性和泛化性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00684v2",
      "published_date": "2024-12-01 05:47:59 UTC",
      "updated_date": "2025-04-20 10:32:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:18:19.125047"
    },
    {
      "arxiv_id": "2412.00663v1",
      "title": "Deep Learning for Longitudinal Gross Tumor Volume Segmentation in MRI-Guided Adaptive Radiotherapy for Head and Neck Cancer",
      "title_zh": "翻译失败",
      "authors": [
        "Xin Tie",
        "Weijie Chen",
        "Zachary Huemann",
        "Brayden Schott",
        "Nuohao Liu",
        "Tyler J. Bradshaw"
      ],
      "abstract": "Accurate segmentation of gross tumor volume (GTV) is essential for effective\nMRI-guided adaptive radiotherapy (MRgART) in head and neck cancer. However,\nmanual segmentation of the GTV over the course of therapy is time-consuming and\nprone to interobserver variability. Deep learning (DL) has the potential to\novercome these challenges by automatically delineating GTVs. In this study, our\nteam, $\\textit{UW LAIR}$, tackled the challenges of both pre-radiotherapy\n(pre-RT) (Task 1) and mid-radiotherapy (mid-RT) (Task 2) tumor volume\nsegmentation. To this end, we developed a series of DL models for longitudinal\nGTV segmentation. The backbone of our models for both tasks was SegResNet with\ndeep supervision. For Task 1, we trained the model using a combined dataset of\npre-RT and mid-RT MRI data, which resulted in the improved aggregated Dice\nsimilarity coefficient (DSCagg) on an internal testing set compared to models\ntrained solely on pre-RT MRI data. In Task 2, we introduced mask-aware\nattention modules, enabling pre-RT GTV masks to influence intermediate features\nlearned from mid-RT data. This attention-based approach yielded slight\nimprovements over the baseline method, which concatenated mid-RT MRI with\npre-RT GTV masks as input. In the final testing phase, the ensemble of 10\npre-RT segmentation models achieved an average DSCagg of 0.794, with 0.745 for\nprimary GTV (GTVp) and 0.844 for metastatic lymph nodes (GTVn) in Task 1. For\nTask 2, the ensemble of 10 mid-RT segmentation models attained an average\nDSCagg of 0.733, with 0.607 for GTVp and 0.859 for GTVn, leading us to\n$\\textbf{achieve 1st place}$. In summary, we presented a collection of DL\nmodels that could facilitate GTV segmentation in MRgART, offering the potential\nto streamline radiation oncology workflows. Our code and model weights are\navailable at https://github.com/xtie97/HNTS-MRG24-UWLAIR.",
      "tldr_zh": "该研究针对头颈癌的 MRI-guided adaptive radiotherapy (MRgART)，开发了深度学习（DL）模型来自动分割 Gross Tumor Volume (GTV)，以解决手动分割耗时且易受观察者变异的问题。研究团队 UW LAIR 使用 SegResNet with deep supervision 作为骨干网络，对于预放疗（pre-RT）任务（Task 1），通过结合 pre-RT 和 mid-RT MRI 数据训练，提高了 Dice similarity coefficient (DSCagg)；对于中期放疗（mid-RT）任务（Task 2），引入了 mask-aware attention modules，利用 pre-RT GTV masks 增强 mid-RT 数据特征提取。结果显示，10 个模型的集合在 Task 1 中平均 DSCagg 达 0.794（GTVp: 0.745, GTVn: 0.844），在 Task 2 中达 0.733（GTVp: 0.607, GTVn: 0.859），并在两任务中均获得第一名。该方法有助于简化辐射肿瘤学工作流程，并公开了代码和模型权重以供进一步应用。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "physics.med-ph"
      ],
      "primary_category": "eess.IV",
      "comment": "12 pages, 4 figures, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2412.00663v1",
      "published_date": "2024-12-01 03:57:18 UTC",
      "updated_date": "2024-12-01 03:57:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:18:32.786867"
    },
    {
      "arxiv_id": "2412.00661v2",
      "title": "Mean-Field Sampling for Cooperative Multi-Agent Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Emile Anand",
        "Ishani Karmarkar",
        "Guannan Qu"
      ],
      "abstract": "Designing efficient algorithms for multi-agent reinforcement learning (MARL)\nis fundamentally challenging because the size of the joint state and action\nspaces grows exponentially in the number of agents. These difficulties are\nexacerbated when balancing sequential global decision-making with local agent\ninteractions. In this work, we propose a new algorithm $\\texttt{SUBSAMPLE-MFQ}$\n($\\textbf{Subsample}$-$\\textbf{M}$ean-$\\textbf{F}$ield-$\\textbf{Q}$-learning)\nand a decentralized randomized policy for a system with $n$ agents. For $k\\leq\nn$, our algorithm learns a policy for the system in time polynomial in $k$. We\nshow that this learned policy converges to the optimal policy on the order of\n$\\tilde{O}(1/\\sqrt{k})$ as the number of subsampled agents $k$ increases. We\nempirically validate our method in Gaussian squeeze and global exploration\nsettings.",
      "tldr_zh": "本论文针对合作多智能体强化学习(MARL)中状态和动作空间指数增长的挑战，提出了一种新算法SUBSAMPLE-MFQ（Subsample-Mean-Field-Q-learning），结合去中心化随机策略来平衡全局决策与本地交互。该算法通过对不超过n个智能体中的k个进行子采样，能够在多项式时间内学习策略，并证明策略收敛到最优策略，误差为\\tilde{O}(1/\\sqrt{k})。实验结果在Gaussian squeeze和global exploration环境中验证了方法的有效性，提升了MARL的效率和可扩展性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA",
        "cs.SY",
        "eess.SY",
        "math.OC",
        "60J20, 68T99",
        "I.2.11"
      ],
      "primary_category": "cs.LG",
      "comment": "44 pages. 6 figures. arXiv admin note: text overlap with\n  arXiv:2403.00222",
      "pdf_url": "http://arxiv.org/pdf/2412.00661v2",
      "published_date": "2024-12-01 03:45:17 UTC",
      "updated_date": "2025-01-29 22:54:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:18:42.355743"
    },
    {
      "arxiv_id": "2412.00657v1",
      "title": "Improving Vietnamese Legal Document Retrieval using Synthetic Data",
      "title_zh": "利用合成数据改进越南法律文件检索",
      "authors": [
        "Son Pham Tien",
        "Hieu Nguyen Doan",
        "An Nguyen Dai",
        "Sang Dinh Viet"
      ],
      "abstract": "In the field of legal information retrieval, effective embedding-based models\nare essential for accurate question-answering systems. However, the scarcity of\nlarge annotated datasets poses a significant challenge, particularly for\nVietnamese legal texts. To address this issue, we propose a novel approach that\nleverages large language models to generate high-quality, diverse synthetic\nqueries for Vietnamese legal passages. This synthetic data is then used to\npre-train retrieval models, specifically bi-encoder and ColBERT, which are\nfurther fine-tuned using contrastive loss with mined hard negatives. Our\nexperiments demonstrate that these enhancements lead to strong improvement in\nretrieval accuracy, validating the effectiveness of synthetic data and\npre-training techniques in overcoming the limitations posed by the lack of\nlarge labeled datasets in the Vietnamese legal domain.",
      "tldr_zh": "本文针对越南法律文本缺乏大型标注数据集的问题，提出一种新方法，使用大型语言模型生成高质量、多样化的 synthetic queries 来处理越南法律段落。研究利用这些合成数据预训练 bi-encoder 和 ColBERT 模型，并通过 contrastive loss 与 mined hard negatives 进行进一步微调。实验结果显示，该方法显著提高了检索准确率，证明了 synthetic data 和预训练技术在越南法律领域克服数据不足挑战的有效性。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00657v1",
      "published_date": "2024-12-01 03:28:26 UTC",
      "updated_date": "2024-12-01 03:28:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:18:53.588994"
    },
    {
      "arxiv_id": "2412.00653v1",
      "title": "Predictive Inference With Fast Feature Conformal Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Zihao Tang",
        "Boyuan Wang",
        "Chuan Wen",
        "Jiaye Teng"
      ],
      "abstract": "Conformal prediction is widely adopted in uncertainty quantification, due to\nits post-hoc, distribution-free, and model-agnostic properties. In the realm of\nmodern deep learning, researchers have proposed Feature Conformal Prediction\n(FCP), which deploys conformal prediction in a feature space, yielding reduced\nband lengths. However, the practical utility of FCP is limited due to the\ntime-consuming non-linear operations required to transform confidence bands\nfrom feature space to output space. In this paper, we introduce Fast Feature\nConformal Prediction (FFCP), which features a novel non-conformity score and is\nconvenient for practical applications. FFCP serves as a fast version of FCP, in\nthat it equivalently employs a Taylor expansion to approximate the\naforementioned non-linear operations in FCP. Empirical validations showcase\nthat FFCP performs comparably with FCP (both outperforming the vanilla version)\nwhile achieving a significant reduction in computational time by approximately\n50x. The code is available at https://github.com/ElvisWang1111/FastFeatureCP",
      "tldr_zh": "这篇论文提出了 Fast Feature Conformal Prediction (FFCP)，一种改进的预测推理方法，用于处理不确定性量化问题，旨在解决原 Feature Conformal Prediction (FCP) 的计算密集型问题。FFCP 通过引入新颖的非一致性分数和 Taylor 展开来近似 FCP 中的非线性操作，从而显著减少从特征空间到输出空间的转换时间。实验验证显示，FFCP 与 FCP 性能相当（两者均优于传统版本），但计算时间减少约50倍，为实际应用提供了高效的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00653v1",
      "published_date": "2024-12-01 03:14:04 UTC",
      "updated_date": "2024-12-01 03:14:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:19:06.129749"
    },
    {
      "arxiv_id": "2412.00631v1",
      "title": "ROSE: A Reward-Oriented Data Selection Framework for LLM Task-Specific Instruction Tuning",
      "title_zh": "ROSE：基于奖励导向的数据选择框架，用于LLM任务特定指令微调",
      "authors": [
        "Yang Wu",
        "Huayi Zhang",
        "Yizheng Jiao",
        "Lin Ma",
        "Xiaozhong Liu",
        "Jinhong Yu",
        "Dongyu Zhang",
        "Dezhi Yu",
        "Wei Xu"
      ],
      "abstract": "Instruction tuning has underscored the significant potential of large\nlanguage models (LLMs) in producing more human-controllable and effective\noutputs in various domains. In this work, we focus on the data selection\nproblem for task-specific instruction tuning of LLMs. Prevailing methods\nprimarily rely on the crafted similarity metrics to select training data that\naligns with the test data distribution. The goal is to minimize instruction\ntuning loss on the test data, ultimately improving performance on the target\ntask. However, it has been widely observed that instruction tuning loss (i.e.,\ncross-entropy loss for next token prediction) in LLMs often fails to exhibit a\nmonotonic relationship with actual task performance. This misalignment\nundermines the effectiveness of current data selection methods for\ntask-specific instruction tuning. To address this issue, we introduce ROSE, a\nnovel Reward-Oriented inStruction data sElection method which leverages\npairwise preference loss as a reward signal to optimize data selection for\ntask-specific instruction tuning. Specifically, ROSE adapts an influence\nformulation to approximate the influence of training data points relative to a\nfew-shot preference validation set to select the most task-related training\ndata points. Experimental results show that by selecting just 5% of the\ntraining data using ROSE, our approach can achieve competitive results compared\nto fine-tuning with the full training dataset, and it surpasses other\nstate-of-the-art data selection methods for task-specific instruction tuning.\nOur qualitative analysis further confirms the robust generalizability of our\nmethod across multiple benchmark datasets and diverse model architectures.",
      "tldr_zh": "该研究针对大型语言模型 (LLMs) 的任务特定指令微调问题，指出现有数据选择方法依赖相似性指标，但由于指令微调损失与实际任务性能不一致，导致效果有限。论文提出 ROSE 框架，这是一种奖励导向的数据选择方法，利用成对偏好损失 (pairwise preference loss) 作为奖励信号，并通过影响公式 (influence formulation) 选择最相关的训练数据点。实验结果显示，使用 ROSE 仅选 5% 的训练数据即可与全数据集微调的性能相当，并优于其他先进方法；在多个基准数据集和模型架构上表现出色，证明了其鲁棒的泛化能力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00631v1",
      "published_date": "2024-12-01 01:01:09 UTC",
      "updated_date": "2024-12-01 01:01:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:19:18.169293"
    },
    {
      "arxiv_id": "2412.00627v2",
      "title": "ARChef: An iOS-Based Augmented Reality Cooking Assistant Powered by Multimodal Gemini LLM",
      "title_zh": "翻译失败",
      "authors": [
        "Rithik Vir",
        "Parsa Madinei"
      ],
      "abstract": "Cooking meals can be difficult, causing many to resort to cookbooks and\nonline recipes. However, relying on these traditional methods of cooking often\nresults in missing ingredients, nutritional hazards, and unsatisfactory meals.\nUsing Augmented Reality (AR) can address these issues; however, current AR\ncooking applications have poor user interfaces and limited accessibility. This\npaper proposes a prototype of an iOS application that integrates AR and\nComputer Vision (CV) into the cooking process. We leverage Google's Gemini\nLarge Language Model (LLM) to identify ingredients in the camera's field of\nvision and generate recipe choices with detailed nutritional information.\nAdditionally, this application uses Apple's ARKit to create an AR user\ninterface compatible with iOS devices. Users can personalize their meal\nsuggestions by inputting their dietary preferences and rating each meal. The\napplication's effectiveness is evaluated through three rounds of user\nexperience surveys. This application advances the field of accessible cooking\nassistance technologies, aiming to reduce food wastage and improve the meal\nplanning experience.",
      "tldr_zh": "该论文提出 ARChef，一款基于 iOS 的增强现实(AR)烹饪助手，利用多模态 Gemini LLM 和计算机视觉(CV)技术，解决传统烹饪方法导致的食材缺失、营养风险和用餐不满意问题。应用通过识别相机视野中的食材、生成个性化配方并提供详细营养信息，以及利用 Apple's ARKit 创建用户友好界面，让用户输入饮食偏好并对餐点评分。研究通过三轮用户体验调查评估了应用的有效性，最终旨在减少食物浪费并提升用餐规划体验。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00627v2",
      "published_date": "2024-12-01 00:52:51 UTC",
      "updated_date": "2024-12-09 08:27:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:19:29.923800"
    },
    {
      "arxiv_id": "2412.00622v2",
      "title": "Visual Modality Prompt for Adapting Vision-Language Object Detectors",
      "title_zh": "翻译失败",
      "authors": [
        "Heitor R. Medeiros",
        "Atif Belal",
        "Srikanth Muralidharan",
        "Eric Granger",
        "Marco Pedersoli"
      ],
      "abstract": "The zero-shot performance of object detectors degrades when tested on\ndifferent modalities, such as infrared and depth. While recent work has\nexplored image translation techniques to adapt detectors to new modalities,\nthese methods are limited to a single modality and apply only to traditional\ndetectors. Recently, vision-language detectors, such as YOLO-World and\nGrounding DINO, have shown promising zero-shot capabilities, however, they have\nnot yet been adapted for other visual modalities. Traditional fine-tuning\napproaches compromise the zero-shot capabilities of the detectors. The visual\nprompt strategies commonly used for classification with vision-language models\napply the same linear prompt translation to each image, making them less\neffective. To address these limitations, we propose ModPrompt, a visual prompt\nstrategy to adapt vision-language detectors to new modalities without degrading\nzero-shot performance. In particular, an encoder-decoder visual prompt strategy\nis proposed, further enhanced by the integration of inference-friendly modality\nprompt decoupled residual, facilitating a more robust adaptation. Empirical\nbenchmarking results show our method for modality adaptation on two\nvision-language detectors, YOLO-World and Grounding DINO, and on challenging\ninfrared (LLVIP, FLIR) and depth (NYUv2) datasets, achieving performance\ncomparable to full fine-tuning while preserving the model's zero-shot\ncapability. Code available at: https://github.com/heitorrapela/ModPrompt.",
      "tldr_zh": "该论文探讨了视觉语言物体检测器（如YOLO-World和Grounding DINO）在零样本条件下适应新模态（如红外和深度图像）时的性能下降问题。作者提出ModPrompt，一种视觉提示策略，包括编码器-解码器机制和模态提示解耦残差，以实现鲁棒适应，而不损害检测器的零样本能力。实验结果显示，在LLVIP、FLIR和NYUv2数据集上，ModPrompt的性能与完全微调相当，同时保留了模型的零样本功能，为跨模态物体检测提供了高效解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00622v2",
      "published_date": "2024-12-01 00:19:59 UTC",
      "updated_date": "2025-03-14 20:32:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:19:41.845920"
    },
    {
      "arxiv_id": "2412.00621v1",
      "title": "Exposing LLM Vulnerabilities: Adversarial Scam Detection and Performance",
      "title_zh": "翻译失败",
      "authors": [
        "Chen-Wei Chang",
        "Shailik Sarkar",
        "Shutonu Mitra",
        "Qi Zhang",
        "Hossein Salemi",
        "Hemant Purohit",
        "Fengxiu Zhang",
        "Michin Hong",
        "Jin-Hee Cho",
        "Chang-Tien Lu"
      ],
      "abstract": "Can we trust Large Language Models (LLMs) to accurately predict scam? This\npaper investigates the vulnerabilities of LLMs when facing adversarial scam\nmessages for the task of scam detection. We addressed this issue by creating a\ncomprehensive dataset with fine-grained labels of scam messages, including both\noriginal and adversarial scam messages. The dataset extended traditional binary\nclasses for the scam detection task into more nuanced scam types. Our analysis\nshowed how adversarial examples took advantage of vulnerabilities of a LLM,\nleading to high misclassification rate. We evaluated the performance of LLMs on\nthese adversarial scam messages and proposed strategies to improve their\nrobustness.",
      "tldr_zh": "本研究探讨了大型语言模型 (LLMs) 在检测诈骗消息时的脆弱性，特别是面对对抗性诈骗消息 (adversarial scam messages)。作者构建了一个全面数据集，包含细粒度标签的原始和对抗性诈骗消息，并将传统的二元分类扩展为更细化的诈骗类型。分析结果显示，对抗性例子利用了 LLMs 的漏洞，导致高误分类率；同时，论文评估了 LLMs 的性能并提出了改进策略，以提升其鲁棒性。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CR",
      "comment": "4 pages, 2024 IEEE International Conference on Big Data workshop\n  BigEACPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2412.00621v1",
      "published_date": "2024-12-01 00:13:28 UTC",
      "updated_date": "2024-12-01 00:13:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T06:19:53.485757"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 59,
  "processed_papers_count": 59,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-21T06:20:11.940732"
}
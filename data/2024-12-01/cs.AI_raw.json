[
  {
    "arxiv_id": "2412.00994v1",
    "title": "DSSRNN: Decomposition-Enhanced State-Space Recurrent Neural Network for Time-Series Analysis",
    "authors": [
      "Ahmad Mohammadshirazi",
      "Ali Nosratifiroozsalari",
      "Rajiv Ramnath"
    ],
    "abstract": "Time series forecasting is a crucial yet challenging task in machine\nlearning, requiring domain-specific knowledge due to its wide-ranging\napplications. While recent Transformer models have improved forecasting\ncapabilities, they come with high computational costs. Linear-based models have\nshown better accuracy than Transformers but still fall short of ideal\nperformance. To address these challenges, we introduce the Decomposition\nState-Space Recurrent Neural Network (DSSRNN), a novel framework designed for\nboth long-term and short-term time series forecasting. DSSRNN uniquely combines\ndecomposition analysis to capture seasonal and trend components with\nstate-space models and physics-based equations. We evaluate DSSRNN's\nperformance on indoor air quality datasets, focusing on CO2 concentration\nprediction across various forecasting horizons. Results demonstrate that DSSRNN\nconsistently outperforms state-of-the-art models, including transformer-based\narchitectures, in terms of both Mean Squared Error (MSE) and Mean Absolute\nError (MAE). For example, at the shortest horizon (T=96) in Office 1, DSSRNN\nachieved an MSE of 0.378 and an MAE of 0.401, significantly lower than\ncompeting models. Additionally, DSSRNN exhibits superior computational\nefficiency compared to more complex models. While not as lightweight as the\nDLinear model, DSSRNN achieves a balance between performance and efficiency,\nwith only 0.11G MACs and 437MiB memory usage, and an inference time of 0.58ms\nfor long-term forecasting. This work not only showcases DSSRNN's success but\nalso establishes a new benchmark for physics-informed machine learning in\nenvironmental forecasting and potentially other domains.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00994v1",
    "published_date": "2024-12-01 22:55:58 UTC",
    "updated_date": "2024-12-01 22:55:58 UTC"
  },
  {
    "arxiv_id": "2412.00967v1",
    "title": "Linear Probe Penalties Reduce LLM Sycophancy",
    "authors": [
      "Henry Papadatos",
      "Rachel Freedman"
    ],
    "abstract": "Large language models (LLMs) are often sycophantic, prioritizing agreement\nwith their users over accurate or objective statements. This problematic\nbehavior becomes more pronounced during reinforcement learning from human\nfeedback (RLHF), an LLM fine-tuning stage intended to align model outputs with\nhuman values. Instead of increasing accuracy and reliability, the reward model\nlearned from RLHF often rewards sycophancy. We develop a linear probing method\nto identify and penalize markers of sycophancy within the reward model,\nproducing rewards that discourage sycophantic behavior. Our experiments show\nthat constructing and optimizing against this surrogate reward function reduces\nsycophantic behavior in multiple open-source LLMs. Our results suggest a\ngeneralizable methodology for reducing unwanted LLM behaviors that are not\nsufficiently disincentivized by RLHF fine-tuning.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "20 pages, 15 figures, NeurIPS 2024 Workshop Socially Responsible\n  Language Modelling Research (SoLaR)",
    "pdf_url": "http://arxiv.org/pdf/2412.00967v1",
    "published_date": "2024-12-01 21:11:28 UTC",
    "updated_date": "2024-12-01 21:11:28 UTC"
  },
  {
    "arxiv_id": "2412.00962v1",
    "title": "LLMs as mirrors of societal moral standards: reflection of cultural divergence and agreement across ethical topics",
    "authors": [
      "Mijntje Meijer",
      "Hadi Mohammadi",
      "Ayoub Bagheri"
    ],
    "abstract": "Large language models (LLMs) have become increasingly pivotal in various\ndomains due the recent advancements in their performance capabilities. However,\nconcerns persist regarding biases in LLMs, including gender, racial, and\ncultural biases derived from their training data. These biases raise critical\nquestions about the ethical deployment and societal impact of LLMs.\nAcknowledging these concerns, this study investigates whether LLMs accurately\nreflect cross-cultural variations and similarities in moral perspectives. In\nassessing whether the chosen LLMs capture patterns of divergence and agreement\non moral topics across cultures, three main methods are employed: (1)\ncomparison of model-generated and survey-based moral score variances, (2)\ncluster alignment analysis to evaluate the correspondence between country\nclusters derived from model-generated moral scores and those derived from\nsurvey data, and (3) probing LLMs with direct comparative prompts. All three\nmethods involve the use of systematic prompts and token pairs designed to\nassess how well LLMs understand and reflect cultural variations in moral\nattitudes. The findings of this study indicate overall variable and low\nperformance in reflecting cross-cultural differences and similarities in moral\nvalues across the models tested, highlighting the necessity for improving\nmodels' accuracy in capturing these nuances effectively. The insights gained\nfrom this study aim to inform discussions on the ethical development and\ndeployment of LLMs in global contexts, emphasizing the importance of mitigating\nbiases and promoting fair representation across diverse cultural perspectives.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.SC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00962v1",
    "published_date": "2024-12-01 20:39:42 UTC",
    "updated_date": "2024-12-01 20:39:42 UTC"
  },
  {
    "arxiv_id": "2412.00959v1",
    "title": "Generative Language Models Potential for Requirement Engineering Applications: Insights into Current Strengths and Limitations",
    "authors": [
      "Summra Saleem",
      "Muhammad Nabeel Asim",
      "Ludger Van Elst",
      "Andreas Dengel"
    ],
    "abstract": "Traditional language models have been extensively evaluated for software\nengineering domain, however the potential of ChatGPT and Gemini have not been\nfully explored. To fulfill this gap, the paper in hand presents a comprehensive\ncase study to investigate the potential of both language models for development\nof diverse types of requirement engineering applications. It deeply explores\nimpact of varying levels of expert knowledge prompts on the prediction\naccuracies of both language models. Across 4 different public benchmark\ndatasets of requirement engineering tasks, it compares performance of both\nlanguage models with existing task specific machine/deep learning predictors\nand traditional language models. Specifically, the paper utilizes 4 benchmark\ndatasets; Pure (7,445 samples, requirements extraction),PROMISE (622 samples,\nrequirements classification), REQuestA (300 question answer (QA) pairs) and\nAerospace datasets (6347 words, requirements NER tagging). Our experiments\nreveal that, in comparison to ChatGPT, Gemini requires more careful prompt\nengineering to provide accurate predictions. Moreover, across requirement\nextraction benchmark dataset the state-of-the-art F1-score is 0.86 while\nChatGPT and Gemini achieved 0.76 and 0.77,respectively. The State-of-the-art\nF1-score on requirements classification dataset is 0.96 and both language\nmodels 0.78. In name entity recognition (NER) task the state-of-the-art\nF1-score is 0.92 and ChatGPT managed to produce 0.36, and Gemini 0.25.\nSimilarly, across question answering dataset the state-of-the-art F1-score is\n0.90 and ChatGPT and Gemini managed to produce 0.91 and 0.88 respectively. Our\nexperiments show that Gemini requires more precise prompt engineering than\nChatGPT. Except for question-answering, both models under-perform compared to\ncurrent state-of-the-art predictors across other tasks.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00959v1",
    "published_date": "2024-12-01 20:20:58 UTC",
    "updated_date": "2024-12-01 20:20:58 UTC"
  },
  {
    "arxiv_id": "2412.00956v1",
    "title": "Large Language Models as Mirrors of Societal Moral Standards",
    "authors": [
      "Evi Papadopoulou",
      "Hadi Mohammadi",
      "Ayoub Bagheri"
    ],
    "abstract": "Prior research has demonstrated that language models can, to a limited\nextent, represent moral norms in a variety of cultural contexts. This research\naims to replicate these findings and further explore their validity,\nconcentrating on issues like 'homosexuality' and 'divorce'. This study\nevaluates the effectiveness of these models using information from two surveys,\nthe WVS and the PEW, that encompass moral perspectives from over 40 countries.\nThe results show that biases exist in both monolingual and multilingual models,\nand they typically fall short of accurately capturing the moral intricacies of\ndiverse cultures. However, the BLOOM model shows the best performance,\nexhibiting some positive correlations, but still does not achieve a\ncomprehensive moral understanding. This research underscores the limitations of\ncurrent PLMs in processing cross-cultural differences in values and highlights\nthe importance of developing culturally aware AI systems that better align with\nuniversal human values.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.SC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00956v1",
    "published_date": "2024-12-01 20:20:35 UTC",
    "updated_date": "2024-12-01 20:20:35 UTC"
  },
  {
    "arxiv_id": "2412.00953v1",
    "title": "BIGCity: A Universal Spatiotemporal Model for Unified Trajectory and Traffic State Data Analysis",
    "authors": [
      "Xie Yu",
      "Jingyuan Wang",
      "Yifan Yang",
      "Qian Huang",
      "Ke Qu"
    ],
    "abstract": "Typical dynamic ST data includes trajectory data (representing\nindividual-level mobility) and traffic state data (representing\npopulation-level mobility). Traditional studies often treat trajectory and\ntraffic state data as distinct, independent modalities, each tailored to\nspecific tasks within a single modality. However, real-world applications, such\nas navigation apps, require joint analysis of trajectory and traffic state\ndata. Treating these data types as two separate domains can lead to suboptimal\nmodel performance. Although recent advances in ST data pre-training and ST\nfoundation models aim to develop universal models for ST data analysis, most\nexisting models are \"multi-task, solo-data modality\" (MTSM), meaning they can\nhandle multiple tasks within either trajectory data or traffic state data, but\nnot both simultaneously. To address this gap, this paper introduces BIGCity,\nthe first multi-task, multi-data modality (MTMD) model for ST data analysis.\nThe model targets two key challenges in designing an MTMD ST model: (1)\nunifying the representations of different ST data modalities, and (2) unifying\nheterogeneous ST analysis tasks. To overcome the first challenge, BIGCity\nintroduces a novel ST-unit that represents both trajectories and traffic states\nin a unified format. Additionally, for the second challenge, BIGCity adopts a\ntunable large model with ST task-oriented prompt, enabling it to perform a\nrange of heterogeneous tasks without the need for fine-tuning. Extensive\nexperiments on real-world datasets demonstrate that BIGCity achieves\nstate-of-the-art performance across 8 tasks, outperforming 18 baselines. To the\nbest of our knowledge, BIGCity is the first model capable of handling both\ntrajectories and traffic states for diverse heterogeneous tasks. Our code are\navailable at https://github.com/bigscity/BIGCity",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00953v1",
    "published_date": "2024-12-01 20:10:55 UTC",
    "updated_date": "2024-12-01 20:10:55 UTC"
  },
  {
    "arxiv_id": "2412.00949v1",
    "title": "STEVE-Audio: Expanding the Goal Conditioning Modalities of Embodied Agents in Minecraft",
    "authors": [
      "Nicholas Lenzen",
      "Amogh Raut",
      "Andrew Melnik"
    ],
    "abstract": "Recently, the STEVE-1 approach has been introduced as a method for training\ngenerative agents to follow instructions in the form of latent CLIP embeddings.\nIn this work, we present a methodology to extend the control modalities by\nlearning a mapping from new input modalities to the latent goal space of the\nagent. We apply our approach to the challenging Minecraft domain, and extend\nthe goal conditioning to include the audio modality. The resulting\naudio-conditioned agent is able to perform on a comparable level to the\noriginal text-conditioned and visual-conditioned agents. Specifically, we\ncreate an Audio-Video CLIP foundation model for Minecraft and an audio prior\nnetwork which together map audio samples to the latent goal space of the\nSTEVE-1 policy. Additionally, we highlight the tradeoffs that occur when\nconditioning on different modalities. Our training code, evaluation code, and\nAudio-Video CLIP foundation model for Minecraft are made open-source to help\nfoster further research into multi-modal generalist sequential decision-making\nagents.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at CoRL 2024: Workshop on Lifelong Learning for Home Robots",
    "pdf_url": "http://arxiv.org/pdf/2412.00949v1",
    "published_date": "2024-12-01 19:48:57 UTC",
    "updated_date": "2024-12-01 19:48:57 UTC"
  },
  {
    "arxiv_id": "2412.00944v1",
    "title": "Bilinear Convolution Decomposition for Causal RL Interpretability",
    "authors": [
      "Narmeen Oozeer",
      "Sinem Erisken",
      "Alice Rigg"
    ],
    "abstract": "Efforts to interpret reinforcement learning (RL) models often rely on\nhigh-level techniques such as attribution or probing, which provide only\ncorrelational insights and coarse causal control. This work proposes replacing\nnonlinearities in convolutional neural networks (ConvNets) with bilinear\nvariants, to produce a class of models for which these limitations can be\naddressed. We show bilinear model variants perform comparably in model-free\nreinforcement learning settings, and give a side by side comparison on ProcGen\nenvironments. Bilinear layers' analytic structure enables weight-based\ndecomposition. Previous work has shown bilinearity enables quantifying\nfunctional importance through eigendecomposition, to identify interpretable low\nrank structure. We show how to adapt the decomposition to convolution layers by\napplying singular value decomposition to vectors of interest, to separate the\nchannel and spatial dimensions. Finally, we propose a methodology for causally\nvalidating concept-based probes, and illustrate its utility by studying a\nmaze-solving agent's ability to track a cheese object.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.00944v1",
    "published_date": "2024-12-01 19:32:04 UTC",
    "updated_date": "2024-12-01 19:32:04 UTC"
  },
  {
    "arxiv_id": "2412.07794v1",
    "title": "Automatic answering of scientific questions using the FACTS-V1 framework: New methods in research to increase efficiency through the use of AI",
    "authors": [
      "Stefan Pietrusky"
    ],
    "abstract": "The use of artificial intelligence (AI) offers various possibilities to\nexpand and support educational research. Specifically, the implementation of AI\ncan be used to develop new frameworks to establish new research tools that\naccelerate and meaningfully expand the efficiency of data evaluation and\ninterpretation (Buckingham Shum et al., 2023). This article presents the\nprototype of the FACTS-V1 (Filtering and Analysis of Content in Textual\nSources) framework. With the help of the application, numerous scientific\npapers can be automatically extracted, analyzed and interpreted from open\naccess document servers without having to rely on proprietary applications and\ntheir limitations. The FACTS-V1 prototype consists of three building blocks.\nThe first part deals with the extraction of texts, the second with filtering\nand interpretation, and the last with the actual statistical evaluation (topic\nmodeling) using an interactive overview. The aim of the framework is to provide\nrecommendations for future scientific questions based on existing data. The\nfunctionality is illustrated by asking how the use of AI will change the\neducation sector. The data used to answer the question comes from 82 scientific\npapers on the topic of AI from 2024. The papers are publicly available on the\npeDOCS document server of the Leibniz Institute for Educational Research and\nEducational Information.",
    "categories": [
      "cs.DL",
      "cs.AI"
    ],
    "primary_category": "cs.DL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07794v1",
    "published_date": "2024-12-01 18:55:39 UTC",
    "updated_date": "2024-12-01 18:55:39 UTC"
  },
  {
    "arxiv_id": "2412.00928v1",
    "title": "A Deep Generative Model for the Design of Synthesizable Ionizable Lipids",
    "authors": [
      "Yuxuan Ou",
      "Jingyi Zhao",
      "Austin Tripp",
      "Morteza Rasoulianboroujeni",
      "José Miguel Hernández-Lobato"
    ],
    "abstract": "Lipid nanoparticles (LNPs) are vital in modern biomedicine, enabling the\neffective delivery of mRNA for vaccines and therapies by protecting it from\nrapid degradation. Among the components of LNPs, ionizable lipids play a key\nrole in RNA protection and facilitate its delivery into the cytoplasm. However,\ndesigning ionizable lipids is complex. Deep generative models can accelerate\nthis process and explore a larger candidate space compared to traditional\nmethods. Due to the structural differences between lipids and small molecules,\nexisting generative models used for small molecule generation are unsuitable\nfor lipid generation. To address this, we developed a deep generative model\nspecifically tailored for the discovery of ionizable lipids. Our model\ngenerates novel ionizable lipid structures and provides synthesis paths using\nsynthetically accessible building blocks, addressing synthesizability. This\nadvancement holds promise for streamlining the development of lipid-based\ndelivery systems, potentially accelerating the deployment of new therapeutic\nagents, including mRNA vaccines and gene therapies.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024 Workshop on AI for New Drug Modalities",
    "pdf_url": "http://arxiv.org/pdf/2412.00928v1",
    "published_date": "2024-12-01 18:33:22 UTC",
    "updated_date": "2024-12-01 18:33:22 UTC"
  },
  {
    "arxiv_id": "2412.01864v1",
    "title": "Learning Aggregation Rules in Participatory Budgeting: A Data-Driven Approach",
    "authors": [
      "Roy Fairstein",
      "Dan Vilenchik",
      "Kobi Gal"
    ],
    "abstract": "Participatory Budgeting (PB) offers a democratic process for communities to\nallocate public funds across various projects through voting. In practice, PB\norganizers face challenges in selecting aggregation rules either because they\nare not familiar with the literature and the exact details of every existing\nrule or because no existing rule echoes their expectations. This paper presents\na novel data-driven approach utilizing machine learning to address this\nchallenge. By training neural networks on PB instances, our approach learns\naggregation rules that balance social welfare, representation, and other\nsocietal beneficial goals. It is able to generalize from small-scale synthetic\nPB examples to large, real-world PB instances. It is able to learn existing\naggregation rules but also generate new rules that adapt to diverse objectives,\nproviding a more nuanced, compromise-driven solution for PB processes. The\neffectiveness of our approach is demonstrated through extensive experiments\nwith synthetic and real-world PB data, and can expand the use and deployment of\nPB solutions.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "cs.GT"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.01864v1",
    "published_date": "2024-12-01 18:13:27 UTC",
    "updated_date": "2024-12-01 18:13:27 UTC"
  },
  {
    "arxiv_id": "2412.00887v1",
    "title": "Playable Game Generation",
    "authors": [
      "Mingyu Yang",
      "Junyou Li",
      "Zhongbin Fang",
      "Sheng Chen",
      "Yangbin Yu",
      "Qiang Fu",
      "Wei Yang",
      "Deheng Ye"
    ],
    "abstract": "In recent years, Artificial Intelligence Generated Content (AIGC) has\nadvanced from text-to-image generation to text-to-video and multimodal video\nsynthesis. However, generating playable games presents significant challenges\ndue to the stringent requirements for real-time interaction, high visual\nquality, and accurate simulation of game mechanics. Existing approaches often\nfall short, either lacking real-time capabilities or failing to accurately\nsimulate interactive mechanics. To tackle the playability issue, we propose a\nnovel method called \\emph{PlayGen}, which encompasses game data generation, an\nautoregressive DiT-based diffusion model, and a comprehensive playability-based\nevaluation framework. Validated on well-known 2D and 3D games, PlayGen achieves\nreal-time interaction, ensures sufficient visual quality, and provides accurate\ninteractive mechanics simulation. Notably, these results are sustained even\nafter over 1000 frames of gameplay on an NVIDIA RTX 2060 GPU. Our code is\npublicly available: https://github.com/GreatX3/Playable-Game-Generation. Our\nplayable demo generated by AI is: http://124.156.151.207.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00887v1",
    "published_date": "2024-12-01 16:53:02 UTC",
    "updated_date": "2024-12-01 16:53:02 UTC"
  },
  {
    "arxiv_id": "2412.00881v1",
    "title": "Learn to Unlearn: Meta-Learning-Based Knowledge Graph Embedding Unlearning",
    "authors": [
      "Naixing Xu",
      "Qian Li",
      "Xu Wang",
      "Bingchen Liu",
      "Xin Li"
    ],
    "abstract": "Knowledge graph (KG) embedding methods map entities and relations into\ncontinuous vector spaces, improving performance in tasks like link prediction\nand question answering. With rising privacy concerns, machine unlearning (MU)\nhas emerged as a critical AI technology, enabling models to eliminate the\ninfluence of specific data. Existing MU approaches often rely on data\nobfuscation and adjustments to training loss but lack generalization across\nunlearning tasks. This paper introduces MetaEU, a Meta-Learning-Based Knowledge\nGraph Embedding Unlearning framework. MetaEU leverages meta-learning to unlearn\nspecific embeddings, mitigating their impact while preserving model performance\non remaining data. Experiments on benchmark datasets demonstrate its\neffectiveness in KG embedding unlearning.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00881v1",
    "published_date": "2024-12-01 16:43:04 UTC",
    "updated_date": "2024-12-01 16:43:04 UTC"
  },
  {
    "arxiv_id": "2412.00876v4",
    "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic Vision-language Context Sparsification",
    "authors": [
      "Wenxuan Huang",
      "Zijie Zhai",
      "Yunhang Shen",
      "Shaosheng Cao",
      "Fei Zhao",
      "Xiangfeng Xu",
      "Zheyu Ye",
      "Yao Hu",
      "Shaohui Lin"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to ICLR 2025. Code is available at\n  https://github.com/Osilly/dynamic_llava",
    "pdf_url": "http://arxiv.org/pdf/2412.00876v4",
    "published_date": "2024-12-01 16:32:31 UTC",
    "updated_date": "2025-03-21 13:30:33 UTC"
  },
  {
    "arxiv_id": "2412.00869v2",
    "title": "KnowledgePrompts: Exploring the Abilities of Large Language Models to Solve Proportional Analogies via Knowledge-Enhanced Prompting",
    "authors": [
      "Thilini Wijesiriwardene",
      "Ruwan Wickramarachchi",
      "Sreeram Vennam",
      "Vinija Jain",
      "Aman Chadha",
      "Amitava Das",
      "Ponnurangam Kumaraguru",
      "Amit Sheth"
    ],
    "abstract": "Making analogies is fundamental to cognition. Proportional analogies, which\nconsist of four terms, are often used to assess linguistic and cognitive\nabilities. For instance, completing analogies like \"Oxygen is to Gas as <blank>\nis to <blank>\" requires identifying the semantic relationship (e.g., \"type of\")\nbetween the first pair of terms (\"Oxygen\" and \"Gas\") and finding a second pair\nthat shares the same relationship (e.g., \"Aluminum\" and \"Metal\"). In this work,\nwe introduce a 15K Multiple-Choice Question Answering (MCQA) dataset for\nproportional analogy completion and evaluate the performance of contemporary\nLarge Language Models (LLMs) in various knowledge-enhanced prompt settings.\nSpecifically, we augment prompts with three types of knowledge: exemplar,\nstructured, and targeted. Our results show that despite extensive training\ndata, solving proportional analogies remains challenging for current LLMs, with\nthe best model achieving an accuracy of 55%. Notably, we find that providing\ntargeted knowledge can better assist models in completing proportional\nanalogies compared to providing exemplars or collections of structured\nknowledge. Our code and data are available at:\nhttps://github.com/Thiliniiw/KnowledgePrompts/",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at COLING 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.00869v2",
    "published_date": "2024-12-01 16:15:14 UTC",
    "updated_date": "2024-12-19 04:38:59 UTC"
  },
  {
    "arxiv_id": "2412.00864v2",
    "title": "Explicit and data-Efficient Encoding via Gradient Flow",
    "authors": [
      "Kyriakos Flouris",
      "Anna Volokitin",
      "Gustav Bredell",
      "Ender Konukoglu"
    ],
    "abstract": "The autoencoder model typically uses an encoder to map data to a lower\ndimensional latent space and a decoder to reconstruct it. However, relying on\nan encoder for inversion can lead to suboptimal representations, particularly\nlimiting in physical sciences where precision is key. We introduce a\ndecoder-only method using gradient flow to directly encode data into the latent\nspace, defined by ordinary differential equations (ODEs). This approach\neliminates the need for approximate encoder inversion. We train the decoder via\nthe adjoint method and show that costly integrals can be avoided with minimal\naccuracy loss. Additionally, we propose a $2^{nd}$ order ODE variant,\napproximating Nesterov's accelerated gradient descent for faster convergence.\nTo handle stiff ODEs, we use an adaptive solver that prioritizes loss\nminimization, improving robustness. Compared to traditional autoencoders, our\nmethod demonstrates explicit encoding and superior data efficiency, which is\ncrucial for data-scarce scenarios in the physical sciences. Furthermore, this\nwork paves the way for integrating machine learning into scientific workflows,\nwhere precise and efficient encoding is critical. \\footnote{The code for this\nwork is available at \\url{https://github.com/k-flouris/gfe}.}",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "math.OC",
      "physics.comp-ph"
    ],
    "primary_category": "stat.ML",
    "comment": "Machine Learning and the Physical Sciences Workshop, NeurIPS 2024.\n  arXiv admin note: text overlap with arXiv:2105.05031",
    "pdf_url": "http://arxiv.org/pdf/2412.00864v2",
    "published_date": "2024-12-01 15:54:50 UTC",
    "updated_date": "2025-01-04 05:09:44 UTC"
  },
  {
    "arxiv_id": "2412.00860v1",
    "title": "Deep evolving semi-supervised anomaly detection",
    "authors": [
      "Jack Belham",
      "Aryan Bhosale",
      "Samrat Mukherjee",
      "Biplab Banerjee",
      "Fabio Cuzzolin"
    ],
    "abstract": "The aim of this paper is to formalise the task of continual semi-supervised\nanomaly detection (CSAD), with the aim of highlighting the importance of such a\nproblem formulation which assumes as close to real-world conditions as\npossible. After an overview of the relevant definitions of continual\nsemi-supervised learning, its components, anomaly detection extension, and the\ntraining protocols; the paper introduces a baseline model of a variational\nautoencoder (VAE) to work with semi-supervised data along with a continual\nlearning method of deep generative replay with outlier rejection. The results\nshow that such a use of extreme value theory (EVT) applied to anomaly detection\ncan provide promising results even in comparison to an upper baseline of joint\ntraining. The results explore the effects of how much labelled and unlabelled\ndata is present, of which class, and where it is located in the data stream.\nOutlier rejection shows promising initial results where it often surpasses a\nbaseline method of Elastic Weight Consolidation (EWC). A baseline for CSAD is\nput forward along with the specific dataset setups used for reproducability and\ntestability for other practitioners. Future research directions include other\nCSAD settings and further research into efficient continual hyperparameter\ntuning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00860v1",
    "published_date": "2024-12-01 15:48:37 UTC",
    "updated_date": "2024-12-01 15:48:37 UTC"
  },
  {
    "arxiv_id": "2412.04498v2",
    "title": "Large Language Models in Politics and Democracy: A Comprehensive Survey",
    "authors": [
      "Goshi Aoki"
    ],
    "abstract": "The advancement of generative AI, particularly large language models (LLMs),\nhas a significant impact on politics and democracy, offering potential across\nvarious domains, including policymaking, political communication, analysis, and\ngovernance. This paper surveys the recent and potential applications of LLMs in\npolitics, examining both their promises and the associated challenges. This\npaper examines the ways in which LLMs are being employed in legislative\nprocesses, political communication, and political analysis. Moreover, we\ninvestigate the potential of LLMs in diplomatic and national security contexts,\neconomic and social modeling, and legal applications. While LLMs offer\nopportunities to enhance efficiency, inclusivity, and decision-making in\npolitical processes, they also present challenges related to bias,\ntransparency, and accountability. The paper underscores the necessity for\nresponsible development, ethical considerations, and governance frameworks to\nensure that the integration of LLMs into politics aligns with democratic values\nand promotes a more just and equitable society.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.04498v2",
    "published_date": "2024-12-01 15:23:34 UTC",
    "updated_date": "2024-12-16 05:27:41 UTC"
  },
  {
    "arxiv_id": "2412.00846v1",
    "title": "Improving Multimodal LLMs Ability In Geometry Problem Solving, Reasoning, And Multistep Scoring",
    "authors": [
      "Avinash Anand",
      "Raj Jaiswal",
      "Abhishek Dharmadhikari",
      "Atharva Marathe",
      "Harsh Parimal Popat",
      "Harshil Mital",
      "Kritarth Prasad",
      "Rajiv Ratn Shah",
      "Roger Zimmermann"
    ],
    "abstract": "This paper presents GPSM4K, a comprehensive geometry multimodal dataset\ntailored to augment the problem-solving capabilities of Large Vision Language\nModels (LVLMs). GPSM4K encompasses 2157 multimodal question-answer pairs\nmanually extracted from mathematics textbooks spanning grades 7-12 and is\nfurther augmented to 5340 problems, consisting of both numerical and\ntheorem-proving questions. In contrast to PGPS9k, Geometry3K, and Geo170K which\nfeature only objective-type questions, GPSM4K offers detailed step-by-step\nsolutions in a consistent format, facilitating a comprehensive evaluation of\nproblem-solving approaches. This dataset serves as an excellent benchmark for\nassessing the geometric reasoning capabilities of LVLMs. Evaluation of our test\nset shows that there is scope for improvement needed in open-source language\nmodels in geometry problem-solving. Finetuning on our training set increases\nthe geometry problem-solving capabilities of models. Further, We also evaluate\nthe effectiveness of techniques such as image captioning and Retrieval\nAugmentation generation (RAG) on model performance. We leveraged LLM to\nautomate the task of final answer evaluation by providing ground truth and\npredicted solutions. This research will help to assess and improve the\ngeometric reasoning capabilities of LVLMs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "15 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.00846v1",
    "published_date": "2024-12-01 15:19:23 UTC",
    "updated_date": "2024-12-01 15:19:23 UTC"
  },
  {
    "arxiv_id": "2412.00833v1",
    "title": "AlignMamba: Enhancing Multimodal Mamba with Local and Global Cross-modal Alignment",
    "authors": [
      "Yan Li",
      "Yifei Xing",
      "Xiangyuan Lan",
      "Xin Li",
      "Haifeng Chen",
      "Dongmei Jiang"
    ],
    "abstract": "Cross-modal alignment is crucial for multimodal representation fusion due to\nthe inherent heterogeneity between modalities. While Transformer-based methods\nhave shown promising results in modeling inter-modal relationships, their\nquadratic computational complexity limits their applicability to long-sequence\nor large-scale data. Although recent Mamba-based approaches achieve linear\ncomplexity, their sequential scanning mechanism poses fundamental challenges in\ncomprehensively modeling cross-modal relationships. To address this limitation,\nwe propose AlignMamba, an efficient and effective method for multimodal fusion.\nSpecifically, grounded in Optimal Transport, we introduce a local cross-modal\nalignment module that explicitly learns token-level correspondences between\ndifferent modalities. Moreover, we propose a global cross-modal alignment loss\nbased on Maximum Mean Discrepancy to implicitly enforce the consistency between\ndifferent modal distributions. Finally, the unimodal representations after\nlocal and global alignment are passed to the Mamba backbone for further\ncross-modal interaction and multimodal fusion. Extensive experiments on\ncomplete and incomplete multimodal fusion tasks demonstrate the effectiveness\nand efficiency of the proposed method.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00833v1",
    "published_date": "2024-12-01 14:47:41 UTC",
    "updated_date": "2024-12-01 14:47:41 UTC"
  },
  {
    "arxiv_id": "2412.00830v1",
    "title": "SPILDL: A Scalable and Parallel Inductive Learner in Description Logic",
    "authors": [
      "Eyad Algahtani"
    ],
    "abstract": "We present SPILDL, a Scalable and Parallel Inductive Learner in Description\nLogic (DL). SPILDL is based on the DL-Learner (the state of the art in DL-based\nILP learning). As a DL-based ILP learner, SPILDL targets the\n$\\mathcal{ALCQI}^{\\mathcal{(D)}}$ DL language, and can learn DL hypotheses\nexpressed as disjunctions of conjunctions (using the $\\sqcup$ operator).\nMoreover, SPILDL's hypothesis language also incorporates the use of string\nconcrete roles (also known as string data properties in the Web Ontology\nLanguage, OWL); As a result, this incorporation of powerful DL constructs,\nenables SPILDL to learn powerful DL-based hypotheses for describing many\nreal-world complex concepts. SPILDL employs a hybrid parallel approach which\ncombines both shared-memory and distributed-memory approaches, to accelerates\nILP learning (for both hypothesis search and evaluation). According to\nexperimental results, SPILDL's parallel search improved performance by up to\n$\\sim$27.3 folds (best case). For hypothesis evaluation, SPILDL improved\nevaluation performance through HT-HEDL (our multi-core CPU + multi-GPU\nhypothesis evaluation engine), by up to 38 folds (best case). By combining both\nparallel search and evaluation, SPILDL improved performance by up to $\\sim$560\nfolds (best case). In terms of worst case scenario, SPILDL's parallel search\ndoesn't provide consistent speedups on all datasets, and is highly dependent on\nthe search space nature of the ILP dataset. For some datasets, increasing the\nnumber of parallel search threads result in reduced performance, similar or\nworse than baseline. Some ILP datasets benefit from parallel search, while\nothers don't (or the performance gains are negligible). In terms of parallel\nevaluation, on small datasets, parallel evaluation provide similar or worse\nperformance than baseline.",
    "categories": [
      "cs.AI",
      "cs.DC",
      "cs.DS",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00830v1",
    "published_date": "2024-12-01 14:33:37 UTC",
    "updated_date": "2024-12-01 14:33:37 UTC"
  },
  {
    "arxiv_id": "2412.00821v1",
    "title": "Improving Physics Reasoning in Large Language Models Using Mixture of Refinement Agents",
    "authors": [
      "Raj Jaiswal",
      "Dhruv Jain",
      "Harsh Parimal Popat",
      "Avinash Anand",
      "Abhishek Dharmadhikari",
      "Atharva Marathe",
      "Rajiv Ratn Shah"
    ],
    "abstract": "Large Language Models (LLMs) demonstrate remarkable capabilities in various\nreasoning tasks. However, they encounter significant challenges when it comes\nto scientific reasoning, particularly in physics, which requires not only\nmathematical reasoning but also factual and conceptual understanding. When\naddressing complex physics problems, LLMs typically face three key issues:\nproblem miscomprehension, incorrect concept application, and computational\nerrors. While each of these problems can be addressed individually, there is a\nneed for a generalized approach that can tackle all three issues\nsimultaneously. To address this, we introduce Mixture of Refinement Agents\n(MoRA), a novel agentic refinement framework that iteratively refines the LLM\ngenerated base solution by correcting the aforementioned errors, resulting in a\nsignificant performance improvement for open-source LLMs. Our approach aims to\nbridge the gap between opensource LLMs and GPT-4o by utilizing the latter as\nerror identifier to guide these refinement agents. We evaluate our approach on\nthe SciEval and MMLU subsets along with our own physics dataset (PhysicsQA).\nMoRA significantly improves the performance of Llama-3-70B and Gemma-2-27B on\nthese datasets, achieving up to a 16% increase in final answer accuracy.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "7 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.00821v1",
    "published_date": "2024-12-01 14:15:55 UTC",
    "updated_date": "2024-12-01 14:15:55 UTC"
  },
  {
    "arxiv_id": "2412.00810v1",
    "title": "Long text outline generation: Chinese text outline based on unsupervised framework and large language mode",
    "authors": [
      "Yan Yan",
      "Yuanchi Ma"
    ],
    "abstract": "Outline generation aims to reveal the internal structure of a document by\nidentifying underlying chapter relationships and generating corresponding\nchapter summaries. Although existing deep learning methods and large models\nperform well on small- and medium-sized texts, they struggle to produce\nreadable outlines for very long texts (such as fictional works), often failing\nto segment chapters coherently. In this paper, we propose a novel outline\ngeneration method for Chinese, combining an unsupervised framework with large\nmodels. Specifically, the method first generates chapter feature graph data\nbased on entity and syntactic dependency relationships. Then, a representation\nmodule based on graph attention layers learns deep embeddings of the chapter\ngraph data. Using these chapter embeddings, we design an operator based on\nMarkov chain principles to segment plot boundaries. Finally, we employ a large\nmodel to generate summaries of each plot segment and produce the overall\noutline. We evaluate our model based on segmentation accuracy and outline\nreadability, and our performance outperforms several deep learning models and\nlarge models in comparative evaluations.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00810v1",
    "published_date": "2024-12-01 13:46:15 UTC",
    "updated_date": "2024-12-01 13:46:15 UTC"
  },
  {
    "arxiv_id": "2412.00807v1",
    "title": "Generative Model for Synthesizing Ionizable Lipids: A Monte Carlo Tree Search Approach",
    "authors": [
      "Jingyi Zhao",
      "Yuxuan Ou",
      "Austin Tripp",
      "Morteza Rasoulianboroujeni",
      "José Miguel Hernández-Lobato"
    ],
    "abstract": "Ionizable lipids are essential in developing lipid nanoparticles (LNPs) for\neffective messenger RNA (mRNA) delivery. While traditional methods for\ndesigning new ionizable lipids are typically time-consuming, deep generative\nmodels have emerged as a powerful solution, significantly accelerating the\nmolecular discovery process. However, a practical challenge arises as the\nmolecular structures generated can often be difficult or infeasible to\nsynthesize. This project explores Monte Carlo tree search (MCTS)-based\ngenerative models for synthesizable ionizable lipids. Leveraging a\nsynthetically accessible lipid building block dataset and two specialized\npredictors to guide the search through chemical space, we introduce a policy\nnetwork guided MCTS generative model capable of producing new ionizable lipids\nwith available synthesis pathways.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.BM",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00807v1",
    "published_date": "2024-12-01 13:34:22 UTC",
    "updated_date": "2024-12-01 13:34:22 UTC"
  },
  {
    "arxiv_id": "2412.00802v1",
    "title": "HT-HEDL: High-Throughput Hypothesis Evaluation in Description Logic",
    "authors": [
      "Eyad Algahtani"
    ],
    "abstract": "We present High-Throughput Hypothesis Evaluation in Description Logic\n(HT-HEDL). HT-HEDL is a high-performance hypothesis evaluation engine that\naccelerates hypothesis evaluation computations for inductive logic programming\n(ILP) learners using description logic (DL) for their knowledge representation;\nin particular, HT-HEDL targets accelerating computations for the\n$\\mathcal{ALCQI}^{\\mathcal{(D)}}$ DL language. HT-HEDL aggregates the computing\npower of multi-core CPUs with multi-GPUs to improve hypothesis computations at\ntwo levels: 1) the evaluation of a single hypothesis and 2) the evaluation of\nmultiple hypotheses (i.e., batch of hypotheses). In the first level, HT-HEDL\nuses a single GPU or a vectorized multi-threaded CPU to evaluate a single\nhypothesis. In vectorized multi-threaded CPU evaluation, classical (scalar) CPU\nmulti-threading is combined with CPU's extended vector instructions set to\nextract more CPU-based performance. The experimental results revealed that\nHT-HEDL increased performance using CPU-based evaluation (on a single\nhypothesis): from 20.4 folds using classical multi-threading to $\\sim85$ folds\nusing vectorized multi-threading. In the GPU-based evaluation, HT-HEDL achieved\nspeedups of up to $\\sim38$ folds for single hypothesis evaluation using a\nsingle GPU. To accelerate the evaluation of multiple hypotheses, HT-HEDL\ncombines, in parallel, GPUs with multi-core CPUs to increase evaluation\nthroughput (number of evaluated hypotheses per second). The experimental\nresults revealed that HT-HEDL increased evaluation throughput by up to 29.3\nfolds using two GPUs and up to $\\sim44$ folds using two GPUs combined with a\nCPU's vectorized multi-threaded evaluation.",
    "categories": [
      "cs.AI",
      "cs.DC",
      "cs.DS",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00802v1",
    "published_date": "2024-12-01 13:01:48 UTC",
    "updated_date": "2024-12-01 13:01:48 UTC"
  },
  {
    "arxiv_id": "2412.00800v2",
    "title": "A Comprehensive Guide to Explainable AI: From Classical Models to LLMs",
    "authors": [
      "Weiche Hsieh",
      "Ziqian Bi",
      "Chuanqi Jiang",
      "Junyu Liu",
      "Benji Peng",
      "Sen Zhang",
      "Xuanhe Pan",
      "Jiawei Xu",
      "Jinlang Wang",
      "Keyu Chen",
      "Pohsun Feng",
      "Yizhu Wen",
      "Xinyuan Song",
      "Tianyang Wang",
      "Ming Liu",
      "Junjie Yang",
      "Ming Li",
      "Bowen Jing",
      "Jintao Ren",
      "Junhao Song",
      "Hong-Ming Tseng",
      "Yichao Zhang",
      "Lawrence K. Q. Yan",
      "Qian Niu",
      "Silin Chen",
      "Yunze Wang",
      "Chia Xin Liang"
    ],
    "abstract": "Explainable Artificial Intelligence (XAI) addresses the growing need for\ntransparency and interpretability in AI systems, enabling trust and\naccountability in decision-making processes. This book offers a comprehensive\nguide to XAI, bridging foundational concepts with advanced methodologies. It\nexplores interpretability in traditional models such as Decision Trees, Linear\nRegression, and Support Vector Machines, alongside the challenges of explaining\ndeep learning architectures like CNNs, RNNs, and Large Language Models (LLMs),\nincluding BERT, GPT, and T5. The book presents practical techniques such as\nSHAP, LIME, Grad-CAM, counterfactual explanations, and causal inference,\nsupported by Python code examples for real-world applications.\n  Case studies illustrate XAI's role in healthcare, finance, and policymaking,\ndemonstrating its impact on fairness and decision support. The book also covers\nevaluation metrics for explanation quality, an overview of cutting-edge XAI\ntools and frameworks, and emerging research directions, such as\ninterpretability in federated learning and ethical AI considerations. Designed\nfor a broad audience, this resource equips readers with the theoretical\ninsights and practical skills needed to master XAI. Hands-on examples and\nadditional resources are available at the companion GitHub repository:\nhttps://github.com/Echoslayer/XAI_From_Classical_Models_to_LLMs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00800v2",
    "published_date": "2024-12-01 13:01:01 UTC",
    "updated_date": "2024-12-08 06:24:32 UTC"
  },
  {
    "arxiv_id": "2412.00789v2",
    "title": "A Cognac shot to forget bad memories: Corrective Unlearning in GNNs",
    "authors": [
      "Varshita Kolipaka",
      "Akshit Sinha",
      "Debangan Mishra",
      "Sumit Kumar",
      "Arvindh Arun",
      "Shashwat Goel",
      "Ponnurangam Kumaraguru"
    ],
    "abstract": "Graph Neural Networks (GNNs) are increasingly being used for a variety of ML\napplications on graph data. Because graph data does not follow the\nindependently and identically distributed (i.i.d.) assumption, adversarial\nmanipulations or incorrect data can propagate to other data points through\nmessage passing, which deteriorates the model's performance. To allow model\ndevelopers to remove the adverse effects of manipulated entities from a trained\nGNN, we study the recently formulated problem of Corrective Unlearning. We find\nthat current graph unlearning methods fail to unlearn the effect of\nmanipulations even when the whole manipulated set is known. We introduce a new\ngraph unlearning method, Cognac, which can unlearn the effect of the\nmanipulation set even when only 5% of it is identified. It recovers most of the\nperformance of a strong oracle with fully corrected training data, even beating\nretraining from scratch without the deletion set while being 8x more efficient.\nWe hope our work assists GNN developers in mitigating harmful effects caused by\nissues in real-world data post-training. Our code is publicly available at\nhttps://github.com/varshitakolipaka/corrective-unlearning-for-gnns",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00789v2",
    "published_date": "2024-12-01 12:23:25 UTC",
    "updated_date": "2024-12-09 15:14:03 UTC"
  },
  {
    "arxiv_id": "2412.00787v1",
    "title": "TSUBF-Net: Trans-Spatial UNet-like Network with Bi-direction Fusion for Segmentation of Adenoid Hypertrophy in CT",
    "authors": [
      "Rulin Zhou",
      "Yingjie Feng",
      "Guankun Wang",
      "Xiaopin Zhong",
      "Zongze Wu",
      "Qiang Wu",
      "Xi Zhang"
    ],
    "abstract": "Adenoid hypertrophy stands as a common cause of obstructive sleep\napnea-hypopnea syndrome in children. It is characterized by snoring, nasal\ncongestion, and growth disorders. Computed Tomography (CT) emerges as a pivotal\nmedical imaging modality, utilizing X-rays and advanced computational\ntechniques to generate detailed cross-sectional images. Within the realm of\npediatric airway assessments, CT imaging provides an insightful perspective on\nthe shape and volume of enlarged adenoids. Despite the advances of deep\nlearning methods for medical imaging analysis, there remains an emptiness in\nthe segmentation of adenoid hypertrophy in CT scans. To address this research\ngap, we introduce TSUBF-Nett (Trans-Spatial UNet-like Network based on\nBi-direction Fusion), a 3D medical image segmentation framework. TSUBF-Net is\nengineered to effectively discern intricate 3D spatial interlayer features in\nCT scans and enhance the extraction of boundary-blurring features. Notably, we\npropose two innovative modules within the U-shaped network architecture:the\nTrans-Spatial Perception module (TSP) and the Bi-directional Sampling\nCollaborated Fusion module (BSCF).These two modules are in charge of operating\nduring the sampling process and strategically fusing down-sampled and\nup-sampled features, respectively. Furthermore, we introduce the Sobel loss\nterm, which optimizes the smoothness of the segmentation results and enhances\nmodel accuracy. Extensive 3D segmentation experiments are conducted on several\ndatasets. TSUBF-Net is superior to the state-of-the-art methods with the lowest\nHD95: 7.03, IoU:85.63, and DSC: 92.26 on our own AHSD dataset. The results in\nthe other two public datasets also demonstrate that our methods can robustly\nand effectively address the challenges of 3D segmentation in CT scans.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00787v1",
    "published_date": "2024-12-01 12:21:23 UTC",
    "updated_date": "2024-12-01 12:21:23 UTC"
  },
  {
    "arxiv_id": "2412.00777v2",
    "title": "Local vs. Global: Local Land-Use and Land-Cover Models Deliver Higher Quality Maps",
    "authors": [
      "Girmaw Abebe Tadesse",
      "Caleb Robinson",
      "Charles Mwangi",
      "Esther Maina",
      "Joshua Nyakundi",
      "Luana Marotti",
      "Gilles Quentin Hacheme",
      "Hamed Alemohammad",
      "Rahul Dodhia",
      "Juan M. Lavista Ferres"
    ],
    "abstract": "In 2023, 58.0% of the African population experienced moderate to severe food\ninsecurity, with 21.6% facing severe food insecurity. Land-use and land-cover\nmaps provide crucial insights for addressing food insecurity by improving\nagricultural efforts, including mapping and monitoring crop types and\nestimating yield. The development of global land-cover maps has been\nfacilitated by the increasing availability of earth observation data and\nadvancements in geospatial machine learning. However, these global maps exhibit\nlower accuracy and inconsistencies in Africa, partly due to the lack of\nrepresentative training data. To address this issue, we propose a data-centric\nframework with a teacher-student model setup, which uses diverse data sources\nof satellite images and label examples to produce local land-cover maps. Our\nmethod trains a high-resolution teacher model on images with a resolution of\n0.331 m/pixel and a low-resolution student model on publicly available images\nwith a resolution of 10 m/pixel. The student model also utilizes the teacher\nmodel's output as its weak label examples through knowledge transfer. We\nevaluated our framework using Murang'a county in Kenya, renowned for its\nagricultural productivity, as a use case. Our local models achieved higher\nquality maps, with improvements of 0.14 in the F1 score and 0.21 in\nIntersection-over-Union, compared to the best global model. Our evaluation also\nrevealed inconsistencies in existing global maps, with a maximum agreement rate\nof 0.30 among themselves. Our work provides valuable guidance to\ndecision-makers for driving informed decisions to enhance food security.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00777v2",
    "published_date": "2024-12-01 11:48:58 UTC",
    "updated_date": "2024-12-11 15:11:09 UTC"
  },
  {
    "arxiv_id": "2412.00773v1",
    "title": "DIVD: Deblurring with Improved Video Diffusion Model",
    "authors": [
      "Haoyang Long",
      "Yan Wang",
      "Wendong Wang"
    ],
    "abstract": "Video deblurring presents a considerable challenge owing to the complexity of\nblur, which frequently results from a combination of camera shakes, and object\nmotions. In the field of video deblurring, many previous works have primarily\nconcentrated on distortion-based metrics, such as PSNR. However, this approach\noften results in a weak correlation with human perception and yields\nreconstructions that lack realism. Diffusion models and video diffusion models\nhave respectively excelled in the fields of image and video generation,\nparticularly achieving remarkable results in terms of image authenticity and\nrealistic perception. However, due to the computational complexity and\nchallenges inherent in adapting diffusion models, there is still uncertainty\nregarding the potential of video diffusion models in video deblurring tasks. To\nexplore the viability of video diffusion models in the task of video\ndeblurring, we introduce a diffusion model specifically for this purpose. In\nthis field, leveraging highly correlated information between adjacent frames\nand addressing the challenge of temporal misalignment are crucial research\ndirections. To tackle these challenges, many improvements based on the video\ndiffusion model are introduced in this work. As a result, our model outperforms\nexisting models and achieves state-of-the-art results on a range of perceptual\nmetrics. Our model preserves a significant amount of detail in the images while\nmaintaining competitive distortion metrics. Furthermore, to the best of our\nknowledge, this is the first time the diffusion model has been applied in video\ndeblurring to overcome the limitations mentioned above.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00773v1",
    "published_date": "2024-12-01 11:39:02 UTC",
    "updated_date": "2024-12-01 11:39:02 UTC"
  },
  {
    "arxiv_id": "2412.00772v1",
    "title": "A Wave is Worth 100 Words: Investigating Cross-Domain Transferability in Time Series",
    "authors": [
      "Xiangkai Ma",
      "Xiaobin Hong",
      "Wenzhong Li",
      "Sanglu Lu"
    ],
    "abstract": "Time series analysis is a fundamental data mining task that supervised\ntraining methods based on empirical risk minimization have proven their\neffectiveness on specific tasks and datasets. However, the acquisition of\nwell-annotated data is costly and a large amount of unlabeled series data is\nunder-utilized. Due to distributional shifts across various domains and\ndifferent patterns of interest across multiple tasks. The problem of\ncross-domain multi-task migration of time series remains a significant\nchallenge. To address these problems, this paper proposes a novel cross-domain\npretraining method based on Wave Quantization (termed as WQ4TS), which can be\ncombined with any advanced time series model and applied to multiple downstream\ntasks. Specifically, we transfer the time series data from different domains\ninto a common spectral latent space, and enable the model to learn the temporal\npattern knowledge of different domains directly from the common space and\nutilize it for the inference of downstream tasks, thereby mitigating the\nchallenge of heterogeneous cross-domains migration. The establishment of\nspectral latent space brings at least three benefits, cross-domain migration\ncapability thus adapting to zero- and few-shot scenarios without relying on\npriori knowledge of the dataset, general compatible cross-domain migration\nframework without changing the existing model structure, and robust modeling\ncapability thus achieving SOTA results in multiple downstream tasks. To\ndemonstrate the effectiveness of the proposed approach, we conduct extensive\nexperiments including three important tasks: forecasting, imputation, and\nclassification. And three common real-world data scenarios are simulated:\nfull-data, few-shot, and zero-shot. The proposed WQ4TS achieves the best\nperformance on 87.5% of all tasks, and the average improvement of the metrics\non all the tasks is up to 34.7%.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00772v1",
    "published_date": "2024-12-01 11:35:06 UTC",
    "updated_date": "2024-12-01 11:35:06 UTC"
  },
  {
    "arxiv_id": "2412.00765v1",
    "title": "SelfPrompt: Autonomously Evaluating LLM Robustness via Domain-Constrained Knowledge Guidelines and Refined Adversarial Prompts",
    "authors": [
      "Aihua Pei",
      "Zehua Yang",
      "Shunan Zhu",
      "Ruoxi Cheng",
      "Ju Jia"
    ],
    "abstract": "Traditional methods for evaluating the robustness of large language models\n(LLMs) often rely on standardized benchmarks, which can escalate costs and\nlimit evaluations across varied domains. This paper introduces a novel\nframework designed to autonomously evaluate the robustness of LLMs by\nincorporating refined adversarial prompts and domain-constrained knowledge\nguidelines in the form of knowledge graphs. Our method systematically generates\ndescriptive sentences from domain-constrained knowledge graph triplets to\nformulate adversarial prompts, enhancing the relevance and challenge of the\nevaluation. These prompts, generated by the LLM itself and tailored to evaluate\nits own robustness, undergo a rigorous filtering and refinement process,\nensuring that only those with high textual fluency and semantic fidelity are\nused. This self-evaluation mechanism allows the LLM to evaluate its robustness\nwithout the need for external benchmarks. We assess the effectiveness of our\nframework through extensive testing on both proprietary models like ChatGPT and\nopen-source models such as Llama-3.1, Phi-3, and Mistral. Results confirm that\nour approach not only reduces dependency on conventional data but also provides\na targeted and efficient means of evaluating LLM robustness in constrained\ndomains.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00765v1",
    "published_date": "2024-12-01 10:58:53 UTC",
    "updated_date": "2024-12-01 10:58:53 UTC"
  },
  {
    "arxiv_id": "2412.00763v1",
    "title": "PGSO: Prompt-based Generative Sequence Optimization Network for Aspect-based Sentiment Analysis",
    "authors": [
      "Hao Dong",
      "Wei Wei"
    ],
    "abstract": "Recently, generative pre-training based models have demonstrated remarkable\nresults on Aspect-based Sentiment Analysis (ABSA) task. However, previous works\noveremphasize crafting various templates to paraphrase training targets for\nenhanced decoding, ignoring the internal optimizations on generative models.\nDespite notable results achieved by these target-oriented optimization methods,\nthey struggle with the complicated long texts since the implicit long-distance\nrelation, e.g., aspect-opinion relation, is difficult to extract under the\nposition embedding mechanism in generative models. Thus, in this paper, we\nfirst clarify the causes of the problem and introduce two sequence optimization\nstrategies: the rule-based static optimization and the score-based dynamic\noptimization. The rule-based approach relies on handcraft priority of\ndependency relation to reorder the context, while the score-based algorithm\ndynamically regulates the contextual sequence by calculating word position\nscores using neural network. Based on the dynamic optimization structure, we\nfurther propose a unified Prompt-based Generative Sequence Optimization network\n(named PGSO), which jointly optimizes the training target as well as the\ngenerative model. Specifically, PGSO contains two components, namely, prompt\nconstruction and sequence regulator. The former constructs a task-specific\nprompt based on unsupervised training objects to fully utilize the pre-trained\nmodel. The latter jointly leverages semantic, syntactic and original-sequence\ninformation to dynamically regulate contextual sequence. Our experiments\nconducted on four ABSA tasks across multiple benchmarks indicate that PGSO\noutperforms state-of-the-art methods, with an average improvement of 3.52% in\nF1 score.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00763v1",
    "published_date": "2024-12-01 10:49:55 UTC",
    "updated_date": "2024-12-01 10:49:55 UTC"
  },
  {
    "arxiv_id": "2412.00761v1",
    "title": "Learning to Forget using Hypernetworks",
    "authors": [
      "Jose Miguel Lara Rangel",
      "Stefan Schoepf",
      "Jack Foster",
      "David Krueger",
      "Usman Anwar"
    ],
    "abstract": "Machine unlearning is gaining increasing attention as a way to remove\nadversarial data poisoning attacks from already trained models and to comply\nwith privacy and AI regulations. The objective is to unlearn the effect of\nundesired data from a trained model while maintaining performance on the\nremaining data. This paper introduces HyperForget, a novel machine unlearning\nframework that leverages hypernetworks - neural networks that generate\nparameters for other networks - to dynamically sample models that lack\nknowledge of targeted data while preserving essential capabilities. Leveraging\ndiffusion models, we implement two Diffusion HyperForget Networks and used them\nto sample unlearned models in Proof-of-Concept experiments. The unlearned\nmodels obtained zero accuracy on the forget set, while preserving good accuracy\non the retain sets, highlighting the potential of HyperForget for dynamic\ntargeted data removal and a promising direction for developing adaptive machine\nunlearning algorithms.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "AdvML-Frontiers'24: The 3rd Workshop on New Frontiers in Adversarial\n  Machine Learning@NeurIPS'24, Vancouver, CA",
    "pdf_url": "http://arxiv.org/pdf/2412.00761v1",
    "published_date": "2024-12-01 10:43:11 UTC",
    "updated_date": "2024-12-01 10:43:11 UTC"
  },
  {
    "arxiv_id": "2412.00760v1",
    "title": "Automating Feedback Analysis in Surgical Training: Detection, Categorization, and Assessment",
    "authors": [
      "Firdavs Nasriddinov",
      "Rafal Kocielnik",
      "Arushi Gupta",
      "Cherine Yang",
      "Elyssa Wong",
      "Anima Anandkumar",
      "Andrew Hung"
    ],
    "abstract": "This work introduces the first framework for reconstructing surgical dialogue\nfrom unstructured real-world recordings, which is crucial for characterizing\nteaching tasks. In surgical training, the formative verbal feedback that\ntrainers provide to trainees during live surgeries is crucial for ensuring\nsafety, correcting behavior immediately, and facilitating long-term skill\nacquisition. However, analyzing and quantifying this feedback is challenging\ndue to its unstructured and specialized nature. Automated systems are essential\nto manage these complexities at scale, allowing for the creation of structured\ndatasets that enhance feedback analysis and improve surgical education. Our\nframework integrates voice activity detection, speaker diarization, and\nautomated speech recaognition, with a novel enhancement that 1) removes\nhallucinations (non-existent utterances generated during speech recognition\nfueled by noise in the operating room) and 2) separates speech from trainers\nand trainees using few-shot voice samples. These aspects are vital for\nreconstructing accurate surgical dialogues and understanding the roles of\noperating room participants. Using data from 33 real-world surgeries, we\ndemonstrated the system's capability to reconstruct surgical teaching dialogues\nand detect feedback instances effectively (F1 score of 0.79+/-0.07). Moreover,\nour hallucination removal step improves feedback detection performance by ~14%.\nEvaluation on downstream clinically relevant tasks of predicting Behavioral\nAdjustment of trainees and classifying Technical feedback, showed performances\ncomparable to manual annotations with F1 scores of 0.82+/0.03 and 0.81+/0.03\nrespectively. These results highlight the effectiveness of our framework in\nsupporting clinically relevant tasks and improving over manual methods.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.ET",
      "cs.LG",
      "68T50, 68U99, 68T99",
      "I.2; I.2.7; I.5.4; J.3; K.3.1"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted as a proceedings paper at Machine Learning for Health 2024",
    "pdf_url": "http://arxiv.org/pdf/2412.00760v1",
    "published_date": "2024-12-01 10:35:12 UTC",
    "updated_date": "2024-12-01 10:35:12 UTC"
  },
  {
    "arxiv_id": "2412.00754v1",
    "title": "CtrlNeRF: The Generative Neural Radiation Fields for the Controllable Synthesis of High-fidelity 3D-Aware Images",
    "authors": [
      "Jian Liu",
      "Zhen Yu"
    ],
    "abstract": "The neural radiance field (NERF) advocates learning the continuous\nrepresentation of 3D geometry through a multilayer perceptron (MLP). By\nintegrating this into a generative model, the generative neural radiance field\n(GRAF) is capable of producing images from random noise z without 3D\nsupervision. In practice, the shape and appearance are modeled by z_s and z_a,\nrespectively, to manipulate them separately during inference. However, it is\nchallenging to represent multiple scenes using a solitary MLP and precisely\ncontrol the generation of 3D geometry in terms of shape and appearance. In this\npaper, we introduce a controllable generative model (i.e. \\textbf{CtrlNeRF})\nthat uses a single MLP network to represent multiple scenes with shared\nweights. Consequently, we manipulated the shape and appearance codes to realize\nthe controllable generation of high-fidelity images with 3D consistency.\nMoreover, the model enables the synthesis of novel views that do not exist in\nthe training sets via camera pose alteration and feature interpolation.\nExtensive experiments were conducted to demonstrate its superiority in 3D-aware\nimage generation compared to its counterparts.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00754v1",
    "published_date": "2024-12-01 10:19:24 UTC",
    "updated_date": "2024-12-01 10:19:24 UTC"
  },
  {
    "arxiv_id": "2412.00751v1",
    "title": "Rethinking Cognition: Morphological Info-Computation and the Embodied Paradigm in Life and Artificial Intelligence",
    "authors": [
      "Gordana Dodig-Crnkovic"
    ],
    "abstract": "This study aims to place Lorenzo Magnanis Eco-Cognitive Computationalism\nwithin the broader context of current work on information, computation, and\ncognition. Traditionally, cognition was believed to be exclusive to humans and\na result of brain activity. However, recent studies reveal it as a fundamental\ncharacteristic of all life forms, ranging from single cells to complex\nmulticellular organisms and their networks. Yet, the literature and general\nunderstanding of cognition still largely remain human-brain-focused, leading to\nconceptual gaps and incoherency. This paper presents a variety of computational\n(information processing) approaches, including an info-computational approach\nto cognition, where natural structures represent information and dynamical\nprocesses on natural structures are regarded as computation, relative to an\nobserving cognizing agent. We model cognition as a web of concurrent\nmorphological computations, driven by processes of self-assembly,\nself-organisation, and autopoiesis across physical, chemical, and biological\ndomains. We examine recent findings linking morphological computation,\nmorphogenesis, agency, basal cognition, extended evolutionary synthesis, and\nactive inference. We establish a connection to Magnanis Eco-Cognitive\nComputationalism and the idea of computational domestication of ignorant\nentities. Novel theoretical and applied insights question the boundaries of\nconventional computational models of cognition. The traditional models\nprioritize symbolic processing and often neglect the inherent constraints and\npotentialities in the physical embodiment of agents on different levels of\norganization. Gaining a better info-computational grasp of cognitive embodiment\nis crucial for the advancement of fields such as biology, evolutionary studies,\nartificial intelligence, robotics, medicine, and more.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00751v1",
    "published_date": "2024-12-01 10:04:53 UTC",
    "updated_date": "2024-12-01 10:04:53 UTC"
  },
  {
    "arxiv_id": "2412.00749v2",
    "title": "CONCERTO: Complex Query Execution Mechanism-Aware Learned Cost Estimation",
    "authors": [
      "Kaixin Zhang",
      "Hongzhi Wang",
      "Kunkai Gu",
      "Ziqi Li",
      "Chunyu Zhao",
      "Yingze Li",
      "Yu Yan"
    ],
    "abstract": "With the growing demand for massive data analysis, many DBMSs have adopted\ncomplex underlying query execution mechanisms, including vectorized operators,\nparallel execution, and dynamic pipeline modifications. However, there remains\na lack of targeted Query Performance Prediction (QPP) methods for these complex\nexecution mechanisms and their interactions, as most existing approaches focus\non traditional tree-shaped query plans and static serial executors. To address\nthis challenge, this paper proposes CONCERTO, a Complex query executiON\nmeChanism-awaE leaRned cosT estimatiOn method. CONCERTO first establishes\nindependent resource cost models for each physical operator. It then constructs\na Directed Acyclic Graph (DAG) consisting of a dataflow tree backbone and\nresource competition relationships among concurrent operators. After\ncalibrating the cost impact of parallel operator execution using Graph\nAttention Networks (GATs) with additional attention mechanisms, CONCERTO\nextracts and aggregates cost vector trees through Temporal Convolutional\nNetworks (TCNs), ultimately achieving effective query performance prediction.\nExperimental results demonstrate that CONCERTO achieves higher prediction\naccuracy than existing methods.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00749v2",
    "published_date": "2024-12-01 09:58:54 UTC",
    "updated_date": "2025-03-28 12:47:19 UTC"
  },
  {
    "arxiv_id": "2412.00748v1",
    "title": "Exploring Cognition through Morphological Info-Computational Framework",
    "authors": [
      "Gordana Dodig-Crnkovic"
    ],
    "abstract": "Traditionally, cognition has been considered a uniquely human capability\ninvolving perception, memory, learning, reasoning, and problem-solving.\nHowever, recent research shows that cognition is a fundamental ability shared\nby all living beings, from single cells to complex organisms. This chapter\ntakes an info-computational approach (ICON), viewing natural structures as\ninformation and the processes of change in these structures as computations. It\nis a relational framework dependent on the perspective of a cognizing\nobserver/cognizer. Informational structures are properties of the material\nsubstrate, and when focusing on the behavior of the substrate, we discuss\nmorphological computing (MC). ICON and MC are complementary perspectives for a\ncognizer. Information and computation are inseparably connected with cognition.\nThis chapter explores research connecting nature as a computational structure\nfor a cognizer, with morphological computation, morphogenesis, agency, extended\ncognition, and extended evolutionary synthesis, using examples of the free\nenergy principle and active inference. It introduces theoretical and practical\napproaches challenging traditional computational models of cognition limited to\nabstract symbol processing, highlighting the computational capacities inherent\nin the material substrate (embodiment). Understanding the embodiment of\ncognition through its morphological computational basis is crucial for biology,\nevolution, intelligence theory, AI, robotics, and other fields.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00748v1",
    "published_date": "2024-12-01 09:56:38 UTC",
    "updated_date": "2024-12-01 09:56:38 UTC"
  },
  {
    "arxiv_id": "2412.00744v1",
    "title": "A Cross-Scene Benchmark for Open-World Drone Active Tracking",
    "authors": [
      "Haowei Sun",
      "Jinwu Hu",
      "Zhirui Zhang",
      "Haoyuan Tian",
      "Xinze Xie",
      "Yufeng Wang",
      "Zhuliang Yu",
      "Xiaohua Xie",
      "Mingkui Tan"
    ],
    "abstract": "Drone Visual Active Tracking aims to autonomously follow a target object by\ncontrolling the motion system based on visual observations, providing a more\npractical solution for effective tracking in dynamic environments. However,\naccurate Drone Visual Active Tracking using reinforcement learning remains\nchallenging due to the absence of a unified benchmark, the complexity of\nopen-world environments with frequent interference, and the diverse motion\nbehavior of dynamic targets. To address these issues, we propose a unified\ncross-scene cross-domain benchmark for open-world drone active tracking called\nDAT. The DAT benchmark provides 24 visually complex environments to assess the\nalgorithms' cross-scene and cross-domain generalization abilities, and\nhigh-fidelity modeling of realistic robot dynamics. Additionally, we propose a\nreinforcement learning-based drone tracking method called R-VAT, which aims to\nimprove the performance of drone tracking targets in complex scenarios.\nSpecifically, inspired by curriculum learning, we introduce a Curriculum-Based\nTraining strategy that progressively enhances the agent tracking performance in\nvast environments with complex interference. We design a goal-centered reward\nfunction to provide precise feedback to the drone agent, preventing targets\nfarther from the center of view from receiving higher rewards than closer ones.\nThis allows the drone to adapt to the diverse motion behavior of open-world\ntargets. Experiments demonstrate that the R-VAT has about 400% improvement over\nthe SOTA method in terms of the cumulative reward metric.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "25 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.00744v1",
    "published_date": "2024-12-01 09:37:46 UTC",
    "updated_date": "2024-12-01 09:37:46 UTC"
  },
  {
    "arxiv_id": "2412.00742v1",
    "title": "Revisiting Self-Supervised Heterogeneous Graph Learning from Spectral Clustering Perspective",
    "authors": [
      "Yujie Mo",
      "Zhihe Lu",
      "Runpeng Yu",
      "Xiaofeng Zhu",
      "Xinchao Wang"
    ],
    "abstract": "Self-supervised heterogeneous graph learning (SHGL) has shown promising\npotential in diverse scenarios. However, while existing SHGL methods share a\nsimilar essential with clustering approaches, they encounter two significant\nlimitations: (i) noise in graph structures is often introduced during the\nmessage-passing process to weaken node representations, and (ii) cluster-level\ninformation may be inadequately captured and leveraged, diminishing the\nperformance in downstream tasks. In this paper, we address these limitations by\ntheoretically revisiting SHGL from the spectral clustering perspective and\nintroducing a novel framework enhanced by rank and dual consistency\nconstraints. Specifically, our framework incorporates a rank-constrained\nspectral clustering method that refines the affinity matrix to exclude noise\neffectively. Additionally, we integrate node-level and cluster-level\nconsistency constraints that concurrently capture invariant and clustering\ninformation to facilitate learning in downstream tasks. We theoretically\ndemonstrate that the learned representations are divided into distinct\npartitions based on the number of classes and exhibit enhanced generalization\nability across tasks. Experimental results affirm the superiority of our\nmethod, showcasing remarkable improvements in several downstream tasks compared\nto existing methods.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00742v1",
    "published_date": "2024-12-01 09:33:20 UTC",
    "updated_date": "2024-12-01 09:33:20 UTC"
  },
  {
    "arxiv_id": "2412.00726v1",
    "title": "Free and Customizable Code Documentation with LLMs: A Fine-Tuning Approach",
    "authors": [
      "Sayak Chakrabarty",
      "Souradip Pal"
    ],
    "abstract": "Automated documentation of programming source code is a challenging task with\nsignificant practical and scientific implications for the developer community.\nWe present a large language model (LLM)-based application that developers can\nuse as a support tool to generate basic documentation for any publicly\navailable repository. Over the last decade, several papers have been written on\ngenerating documentation for source code using neural network architectures.\nWith the recent advancements in LLM technology, some open-source applications\nhave been developed to address this problem. However, these applications\ntypically rely on the OpenAI APIs, which incur substantial financial costs,\nparticularly for large repositories. Moreover, none of these open-source\napplications offer a fine-tuned model or features to enable users to fine-tune.\nAdditionally, finding suitable data for fine-tuning is often challenging. Our\napplication addresses these issues which is available at\nhttps://pypi.org/project/readme-ready/.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00726v1",
    "published_date": "2024-12-01 08:38:18 UTC",
    "updated_date": "2024-12-01 08:38:18 UTC"
  },
  {
    "arxiv_id": "2412.00725v1",
    "title": "Decision Transformer vs. Decision Mamba: Analysing the Complexity of Sequential Decision Making in Atari Games",
    "authors": [
      "Ke Yan"
    ],
    "abstract": "This work analyses the disparity in performance between Decision Transformer\n(DT) and Decision Mamba (DM) in sequence modelling reinforcement learning tasks\nfor different Atari games. The study first observed that DM generally\noutperformed DT in the games Breakout and Qbert, while DT performed better in\nmore complicated games, such as Hero and Kung Fu Master. To understand these\ndifferences, we expanded the number of games to 12 and performed a\ncomprehensive analysis of game characteristics, including action space\ncomplexity, visual complexity, average trajectory length, and average steps to\nthe first non-zero reward. In order to further analyse the key factors that\nimpact the disparity in performance between DT and DM, we employ various\napproaches, including quantifying visual complexity, random forest regression,\ncorrelation analysis, and action space simplification strategies. The results\nindicate that the performance gap between DT and DM is affected by the complex\ninteraction of multiple factors, with the complexity of the action space and\nvisual complexity (particularly evaluated by compression ratio) being the\nprimary determining factors. DM performs well in environments with simple\naction and visual elements, while DT shows an advantage in games with higher\naction and visual complexity. Our findings contribute to a deeper understanding\nof how the game characteristics affect the performance difference in sequential\nmodelling reinforcement learning, potentially guiding the development of future\nmodel design and applications for diverse and complex environments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00725v1",
    "published_date": "2024-12-01 08:37:10 UTC",
    "updated_date": "2024-12-01 08:37:10 UTC"
  },
  {
    "arxiv_id": "2412.00724v1",
    "title": "AdaScale: Dynamic Context-aware DNN Scaling via Automated Adaptation Loop on Mobile Devices",
    "authors": [
      "Yuzhan Wang",
      "Sicong Liu",
      "Bin Guo",
      "Boqi Zhang",
      "Ke Ma",
      "Yasan Ding",
      "Hao Luo",
      "Yao Li",
      "Zhiwen Yu"
    ],
    "abstract": "Deep learning is reshaping mobile applications, with a growing trend of\ndeploying deep neural networks (DNNs) directly to mobile and embedded devices\nto address real-time performance and privacy. To accommodate local resource\nlimitations, techniques like weight compression, convolution decomposition, and\nspecialized layer architectures have been developed. However, the\n\\textit{dynamic} and \\textit{diverse} deployment contexts of mobile devices\npose significant challenges. Adapting deep models to meet varied\ndevice-specific requirements for latency, accuracy, memory, and energy is\nlabor-intensive. Additionally, changing processor states, fluctuating memory\navailability, and competing processes frequently necessitate model\nre-compression to preserve user experience. To address these issues, we\nintroduce AdaScale, an elastic inference framework that automates the\nadaptation of deep models to dynamic contexts. AdaScale leverages a\nself-evolutionary model to streamline network creation, employs diverse\ncompression operator combinations to reduce the search space and improve\noutcomes, and integrates a resource availability awareness block and\nperformance profilers to establish an automated adaptation loop. Our\nexperiments demonstrate that AdaScale significantly enhances accuracy by 5.09%,\nreduces training overhead by 66.89%, speeds up inference latency by 1.51 to 6.2\ntimes, and lowers energy costs by 4.69 times.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00724v1",
    "published_date": "2024-12-01 08:33:56 UTC",
    "updated_date": "2024-12-01 08:33:56 UTC"
  },
  {
    "arxiv_id": "2412.00722v1",
    "title": "Towards Adaptive Mechanism Activation in Language Agent",
    "authors": [
      "Ziyang Huang",
      "Jun Zhao",
      "Kang Liu"
    ],
    "abstract": "Language Agent could be endowed with different mechanisms for autonomous task\naccomplishment. Current agents typically rely on fixed mechanisms or a set of\nmechanisms activated in a predefined order, limiting their adaptation to varied\npotential task solution structures. To this end, this paper proposes\n\\textbf{A}daptive \\textbf{L}anguage \\textbf{A}gent \\textbf{M}echanism\n\\textbf{A}ctivation Learning with Self-Exploration (\\textbf{ALAMA}), which\nfocuses on optimizing mechanism activation adaptability without reliance on\nexpert models. Initially, it builds a harmonized agent framework\n(\\textbf{UniAct}) to \\textbf{Uni}fy different mechanisms via \\textbf{Act}ions.\nThen it leverages a training-efficient optimization method based on\nself-exploration to enable the UniAct to adaptively activate the appropriate\nmechanisms according to the potential characteristics of the task. Experimental\nresults demonstrate significant improvements in downstream agent tasks,\naffirming the effectiveness of our approach in facilitating more dynamic and\ncontext-sensitive mechanism activation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "COLING2025",
    "pdf_url": "http://arxiv.org/pdf/2412.00722v1",
    "published_date": "2024-12-01 08:10:04 UTC",
    "updated_date": "2024-12-01 08:10:04 UTC"
  },
  {
    "arxiv_id": "2412.00721v2",
    "title": "A Comparative Study of LLM-based ASR and Whisper in Low Resource and Code Switching Scenario",
    "authors": [
      "Zheshu Song",
      "Ziyang Ma",
      "Yifan Yang",
      "Jianheng Zhuo",
      "Xie Chen"
    ],
    "abstract": "Large Language Models (LLMs) have showcased exceptional performance across\ndiverse NLP tasks, and their integration with speech encoder is rapidly\nemerging as a dominant trend in the Automatic Speech Recognition (ASR) field.\nPrevious works mainly concentrated on leveraging LLMs for speech recognition in\nEnglish and Chinese. However, their potential for addressing speech recognition\nchallenges in low resource settings remains underexplored. Hence, in this work,\nwe aim to explore the capability of LLMs in low resource ASR and\nMandarin-English code switching ASR. We also evaluate and compare the\nrecognition performance of LLM-based ASR systems against Whisper model.\nExtensive experiments demonstrate that LLM-based ASR yields a relative gain of\n12.8\\% over the Whisper model in low resource ASR while Whisper performs better\nin Mandarin-English code switching ASR. We hope that this study could shed\nlight on ASR for low resource scenarios.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.AI",
    "comment": "This work hasn't been finished yet",
    "pdf_url": "http://arxiv.org/pdf/2412.00721v2",
    "published_date": "2024-12-01 08:07:01 UTC",
    "updated_date": "2024-12-04 06:23:40 UTC"
  },
  {
    "arxiv_id": "2412.00718v1",
    "title": "Well log data generation and imputation using sequence-based generative adversarial networks",
    "authors": [
      "Abdulrahman Al-Fakih",
      "A. Koeshidayatullah",
      "Tapan Mukerji",
      "Sadam Al-Azani",
      "SanLinn I. Kaka"
    ],
    "abstract": "Well log analysis is crucial for hydrocarbon exploration, providing detailed\ninsights into subsurface geological formations. However, gaps and inaccuracies\nin well log data, often due to equipment limitations, operational challenges,\nand harsh subsurface conditions, can introduce significant uncertainties in\nreservoir evaluation. Addressing these challenges requires effective methods\nfor both synthetic data generation and precise imputation of missing data,\nensuring data completeness and reliability. This study introduces a novel\nframework utilizing sequence-based generative adversarial networks (GANs)\nspecifically designed for well log data generation and imputation. The\nframework integrates two distinct sequence-based GAN models: Time Series GAN\n(TSGAN) for generating synthetic well log data and Sequence GAN (SeqGAN) for\nimputing missing data. Both models were tested on a dataset from the North Sea,\nNetherlands region, focusing on different sections of 5, 10, and 50 data\npoints. Experimental results demonstrate that this approach achieves superior\naccuracy in filling data gaps compared to other deep learning models for\nspatial series analysis. The method yielded R^2 values of 0.921, 0.899, and\n0.594, with corresponding mean absolute percentage error (MAPE) values of\n8.320, 0.005, and 151.154, and mean absolute error (MAE) values of 0.012,\n0.005, and 0.032, respectively. These results set a new benchmark for data\nintegrity and utility in geosciences, particularly in well log data analysis.",
    "categories": [
      "physics.geo-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "physics.geo-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00718v1",
    "published_date": "2024-12-01 07:50:34 UTC",
    "updated_date": "2024-12-01 07:50:34 UTC"
  },
  {
    "arxiv_id": "2412.00707v2",
    "title": "Protect Your Secrets: Understanding and Measuring Data Exposure in VSCode Extensions",
    "authors": [
      "Yue Liu",
      "Chakkrit Tantithamthavorn",
      "Li Li"
    ],
    "abstract": "Recent years have witnessed the emerging trend of extensions in modern\nIntegrated Development Environments (IDEs) like Visual Studio Code (VSCode)\nthat significantly enhance developer productivity. Especially, popular AI\ncoding assistants like GitHub Copilot and Tabnine provide conveniences like\nautomated code completion and debugging. While these extensions offer numerous\nbenefits, they may introduce privacy and security concerns to software\ndevelopers. However, there is no existing work that systematically analyzes the\nsecurity and privacy concerns, including the risks of data exposure in VSCode\nextensions.\n  In this paper, we investigate on the security issues of cross-extension\ninteractions in VSCode and shed light on the vulnerabilities caused by data\nexposure among different extensions. Our study uncovers high-impact security\nflaws that could allow adversaries to stealthily acquire or manipulate\ncredential-related data (e.g., passwords, API keys, access tokens) from other\nextensions if not properly handled by extension vendors. To measure their\nprevalence, we design a novel automated risk detection framework that leverages\nprogram analysis and natural language processing techniques to automatically\nidentify potential risks in VSCode extensions. By applying our tool to 27,261\nreal-world VSCode extensions, we discover that 8.5% of them (i.e., 2,325\nextensions) are exposed to credential-related data leakage through various\nvectors, such as commands, user input, and configurations. Our study sheds\nlight on the security challenges and flaws of the extension-in-IDE paradigm and\nprovides suggestions and recommendations for improving the security of VSCode\nextensions and mitigating the risks of data exposure.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00707v2",
    "published_date": "2024-12-01 07:08:53 UTC",
    "updated_date": "2024-12-25 06:49:44 UTC"
  },
  {
    "arxiv_id": "2412.00691v2",
    "title": "The Advancement of Personalized Learning Potentially Accelerated by Generative AI",
    "authors": [
      "Yuang Wei",
      "Yuan-Hao Jiang",
      "Jiayi Liu",
      "Changyong Qi",
      "Linzhao Jia",
      "Rui Jia"
    ],
    "abstract": "The rapid development of Generative AI (GAI) has sparked revolutionary\nchanges across various aspects of education. Personalized learning, a focal\npoint and challenge in educational research, has also been influenced by the\ndevelopment of GAI. To explore GAI's extensive impact on personalized learning,\nthis study investigates its potential to enhance various facets of personalized\nlearning through a thorough analysis of existing research. The research\ncomprehensively examines GAI's influence on personalized learning by analyzing\nits application across different methodologies and contexts, including learning\nstrategies, paths, materials, environments, and specific analyses within the\nteaching and learning processes. Through this in-depth investigation, we find\nthat GAI demonstrates exceptional capabilities in providing adaptive learning\nexperiences tailored to individual preferences and needs. Utilizing different\nforms of GAI across various subjects yields superior learning outcomes. The\narticle concludes by summarizing scenarios where GAI is applicable in\neducational processes and discussing strategies for leveraging GAI to enhance\npersonalized learning, aiming to guide educators and learners in effectively\nutilizing GAI to achieve superior learning objectives.",
    "categories": [
      "cs.AI",
      "stat.AP"
    ],
    "primary_category": "cs.AI",
    "comment": "The V1 version is a more detailed version, and the latest version is\n  the SITE conference included version. SITE 2025-Orando, Florida, United\n  States, March 17-21.2025",
    "pdf_url": "http://arxiv.org/pdf/2412.00691v2",
    "published_date": "2024-12-01 06:01:14 UTC",
    "updated_date": "2025-02-26 08:54:04 UTC"
  },
  {
    "arxiv_id": "2412.00686v2",
    "title": "LVLM-COUNT: Enhancing the Counting Ability of Large Vision-Language Models",
    "authors": [
      "Muhammad Fetrat Qharabagh",
      "Mohammadreza Ghofrani",
      "Kimon Fountoulakis"
    ],
    "abstract": "Counting is a fundamental operation for various visual tasks in real-life\napplications, requiring both object recognition and robust counting\ncapabilities. Despite their advanced visual perception, large vision-language\nmodels (LVLMs) struggle with counting tasks, especially when the number of\nobjects exceeds those commonly encountered during training. We enhance LVLMs'\ncounting abilities using a divide-and-conquer approach, breaking counting\nproblems into sub-counting tasks. Our method employs a mechanism that prevents\nbisecting and thus repetitive counting of objects, which occurs in a naive\ndivide-and-conquer approach. Unlike prior methods, which do not generalize well\nto counting datasets they have not been trained on, our method performs well on\nnew datasets without any additional training or fine-tuning. We demonstrate\nthat our approach enhances the counting capability of LVLMs across various\ndatasets and benchmarks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "31 pages, 24 Figures, 10 Tables",
    "pdf_url": "http://arxiv.org/pdf/2412.00686v2",
    "published_date": "2024-12-01 05:50:22 UTC",
    "updated_date": "2025-02-02 17:49:25 UTC"
  },
  {
    "arxiv_id": "2412.00684v2",
    "title": "Paint Outside the Box: Synthesizing and Selecting Training Data for Visual Grounding",
    "authors": [
      "Zilin Du",
      "Haoxin Li",
      "Jianfei Yu",
      "Boyang Li"
    ],
    "abstract": "Visual grounding aims to localize the image regions based on a textual query.\nGiven the difficulty of large-scale data curation, we investigate how to\neffectively learn visual grounding under data-scarce settings in this paper. To\naddress the data scarcity, we propose a novel framework, POBF (Paint Outside\nthe Box and Filter). POBF synthesizes images by inpainting outside the box,\ntackling a label misalignment issue encountered in previous works. Furthermore,\nPOBF leverages an innovative filtering scheme to select the most effective\ntraining data. This scheme combines a hardness score and an overfitting score,\nbalanced by a penalty term. Extensive experiments across four benchmark\ndatasets demonstrate that POBF consistently improves performance, achieving an\naverage gain of 5.83\\% over the real-data-only method and outperforming leading\nbaselines by 2.29\\%-3.85\\% in accuracy. Additionally, we validate the\nrobustness and generalizability of POBF across various generative models,\ntraining data sizes, and model architectures.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00684v2",
    "published_date": "2024-12-01 05:47:59 UTC",
    "updated_date": "2025-04-20 10:32:37 UTC"
  },
  {
    "arxiv_id": "2412.00663v1",
    "title": "Deep Learning for Longitudinal Gross Tumor Volume Segmentation in MRI-Guided Adaptive Radiotherapy for Head and Neck Cancer",
    "authors": [
      "Xin Tie",
      "Weijie Chen",
      "Zachary Huemann",
      "Brayden Schott",
      "Nuohao Liu",
      "Tyler J. Bradshaw"
    ],
    "abstract": "Accurate segmentation of gross tumor volume (GTV) is essential for effective\nMRI-guided adaptive radiotherapy (MRgART) in head and neck cancer. However,\nmanual segmentation of the GTV over the course of therapy is time-consuming and\nprone to interobserver variability. Deep learning (DL) has the potential to\novercome these challenges by automatically delineating GTVs. In this study, our\nteam, $\\textit{UW LAIR}$, tackled the challenges of both pre-radiotherapy\n(pre-RT) (Task 1) and mid-radiotherapy (mid-RT) (Task 2) tumor volume\nsegmentation. To this end, we developed a series of DL models for longitudinal\nGTV segmentation. The backbone of our models for both tasks was SegResNet with\ndeep supervision. For Task 1, we trained the model using a combined dataset of\npre-RT and mid-RT MRI data, which resulted in the improved aggregated Dice\nsimilarity coefficient (DSCagg) on an internal testing set compared to models\ntrained solely on pre-RT MRI data. In Task 2, we introduced mask-aware\nattention modules, enabling pre-RT GTV masks to influence intermediate features\nlearned from mid-RT data. This attention-based approach yielded slight\nimprovements over the baseline method, which concatenated mid-RT MRI with\npre-RT GTV masks as input. In the final testing phase, the ensemble of 10\npre-RT segmentation models achieved an average DSCagg of 0.794, with 0.745 for\nprimary GTV (GTVp) and 0.844 for metastatic lymph nodes (GTVn) in Task 1. For\nTask 2, the ensemble of 10 mid-RT segmentation models attained an average\nDSCagg of 0.733, with 0.607 for GTVp and 0.859 for GTVn, leading us to\n$\\textbf{achieve 1st place}$. In summary, we presented a collection of DL\nmodels that could facilitate GTV segmentation in MRgART, offering the potential\nto streamline radiation oncology workflows. Our code and model weights are\navailable at https://github.com/xtie97/HNTS-MRG24-UWLAIR.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "physics.med-ph"
    ],
    "primary_category": "eess.IV",
    "comment": "12 pages, 4 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2412.00663v1",
    "published_date": "2024-12-01 03:57:18 UTC",
    "updated_date": "2024-12-01 03:57:18 UTC"
  },
  {
    "arxiv_id": "2412.00661v2",
    "title": "Mean-Field Sampling for Cooperative Multi-Agent Reinforcement Learning",
    "authors": [
      "Emile Anand",
      "Ishani Karmarkar",
      "Guannan Qu"
    ],
    "abstract": "Designing efficient algorithms for multi-agent reinforcement learning (MARL)\nis fundamentally challenging because the size of the joint state and action\nspaces grows exponentially in the number of agents. These difficulties are\nexacerbated when balancing sequential global decision-making with local agent\ninteractions. In this work, we propose a new algorithm $\\texttt{SUBSAMPLE-MFQ}$\n($\\textbf{Subsample}$-$\\textbf{M}$ean-$\\textbf{F}$ield-$\\textbf{Q}$-learning)\nand a decentralized randomized policy for a system with $n$ agents. For $k\\leq\nn$, our algorithm learns a policy for the system in time polynomial in $k$. We\nshow that this learned policy converges to the optimal policy on the order of\n$\\tilde{O}(1/\\sqrt{k})$ as the number of subsampled agents $k$ increases. We\nempirically validate our method in Gaussian squeeze and global exploration\nsettings.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA",
      "cs.SY",
      "eess.SY",
      "math.OC",
      "60J20, 68T99",
      "I.2.11"
    ],
    "primary_category": "cs.LG",
    "comment": "44 pages. 6 figures. arXiv admin note: text overlap with\n  arXiv:2403.00222",
    "pdf_url": "http://arxiv.org/pdf/2412.00661v2",
    "published_date": "2024-12-01 03:45:17 UTC",
    "updated_date": "2025-01-29 22:54:55 UTC"
  },
  {
    "arxiv_id": "2412.00657v1",
    "title": "Improving Vietnamese Legal Document Retrieval using Synthetic Data",
    "authors": [
      "Son Pham Tien",
      "Hieu Nguyen Doan",
      "An Nguyen Dai",
      "Sang Dinh Viet"
    ],
    "abstract": "In the field of legal information retrieval, effective embedding-based models\nare essential for accurate question-answering systems. However, the scarcity of\nlarge annotated datasets poses a significant challenge, particularly for\nVietnamese legal texts. To address this issue, we propose a novel approach that\nleverages large language models to generate high-quality, diverse synthetic\nqueries for Vietnamese legal passages. This synthetic data is then used to\npre-train retrieval models, specifically bi-encoder and ColBERT, which are\nfurther fine-tuned using contrastive loss with mined hard negatives. Our\nexperiments demonstrate that these enhancements lead to strong improvement in\nretrieval accuracy, validating the effectiveness of synthetic data and\npre-training techniques in overcoming the limitations posed by the lack of\nlarge labeled datasets in the Vietnamese legal domain.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00657v1",
    "published_date": "2024-12-01 03:28:26 UTC",
    "updated_date": "2024-12-01 03:28:26 UTC"
  },
  {
    "arxiv_id": "2412.00653v1",
    "title": "Predictive Inference With Fast Feature Conformal Prediction",
    "authors": [
      "Zihao Tang",
      "Boyuan Wang",
      "Chuan Wen",
      "Jiaye Teng"
    ],
    "abstract": "Conformal prediction is widely adopted in uncertainty quantification, due to\nits post-hoc, distribution-free, and model-agnostic properties. In the realm of\nmodern deep learning, researchers have proposed Feature Conformal Prediction\n(FCP), which deploys conformal prediction in a feature space, yielding reduced\nband lengths. However, the practical utility of FCP is limited due to the\ntime-consuming non-linear operations required to transform confidence bands\nfrom feature space to output space. In this paper, we introduce Fast Feature\nConformal Prediction (FFCP), which features a novel non-conformity score and is\nconvenient for practical applications. FFCP serves as a fast version of FCP, in\nthat it equivalently employs a Taylor expansion to approximate the\naforementioned non-linear operations in FCP. Empirical validations showcase\nthat FFCP performs comparably with FCP (both outperforming the vanilla version)\nwhile achieving a significant reduction in computational time by approximately\n50x. The code is available at https://github.com/ElvisWang1111/FastFeatureCP",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00653v1",
    "published_date": "2024-12-01 03:14:04 UTC",
    "updated_date": "2024-12-01 03:14:04 UTC"
  },
  {
    "arxiv_id": "2412.00631v1",
    "title": "ROSE: A Reward-Oriented Data Selection Framework for LLM Task-Specific Instruction Tuning",
    "authors": [
      "Yang Wu",
      "Huayi Zhang",
      "Yizheng Jiao",
      "Lin Ma",
      "Xiaozhong Liu",
      "Jinhong Yu",
      "Dongyu Zhang",
      "Dezhi Yu",
      "Wei Xu"
    ],
    "abstract": "Instruction tuning has underscored the significant potential of large\nlanguage models (LLMs) in producing more human-controllable and effective\noutputs in various domains. In this work, we focus on the data selection\nproblem for task-specific instruction tuning of LLMs. Prevailing methods\nprimarily rely on the crafted similarity metrics to select training data that\naligns with the test data distribution. The goal is to minimize instruction\ntuning loss on the test data, ultimately improving performance on the target\ntask. However, it has been widely observed that instruction tuning loss (i.e.,\ncross-entropy loss for next token prediction) in LLMs often fails to exhibit a\nmonotonic relationship with actual task performance. This misalignment\nundermines the effectiveness of current data selection methods for\ntask-specific instruction tuning. To address this issue, we introduce ROSE, a\nnovel Reward-Oriented inStruction data sElection method which leverages\npairwise preference loss as a reward signal to optimize data selection for\ntask-specific instruction tuning. Specifically, ROSE adapts an influence\nformulation to approximate the influence of training data points relative to a\nfew-shot preference validation set to select the most task-related training\ndata points. Experimental results show that by selecting just 5% of the\ntraining data using ROSE, our approach can achieve competitive results compared\nto fine-tuning with the full training dataset, and it surpasses other\nstate-of-the-art data selection methods for task-specific instruction tuning.\nOur qualitative analysis further confirms the robust generalizability of our\nmethod across multiple benchmark datasets and diverse model architectures.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00631v1",
    "published_date": "2024-12-01 01:01:09 UTC",
    "updated_date": "2024-12-01 01:01:09 UTC"
  },
  {
    "arxiv_id": "2412.00627v2",
    "title": "ARChef: An iOS-Based Augmented Reality Cooking Assistant Powered by Multimodal Gemini LLM",
    "authors": [
      "Rithik Vir",
      "Parsa Madinei"
    ],
    "abstract": "Cooking meals can be difficult, causing many to resort to cookbooks and\nonline recipes. However, relying on these traditional methods of cooking often\nresults in missing ingredients, nutritional hazards, and unsatisfactory meals.\nUsing Augmented Reality (AR) can address these issues; however, current AR\ncooking applications have poor user interfaces and limited accessibility. This\npaper proposes a prototype of an iOS application that integrates AR and\nComputer Vision (CV) into the cooking process. We leverage Google's Gemini\nLarge Language Model (LLM) to identify ingredients in the camera's field of\nvision and generate recipe choices with detailed nutritional information.\nAdditionally, this application uses Apple's ARKit to create an AR user\ninterface compatible with iOS devices. Users can personalize their meal\nsuggestions by inputting their dietary preferences and rating each meal. The\napplication's effectiveness is evaluated through three rounds of user\nexperience surveys. This application advances the field of accessible cooking\nassistance technologies, aiming to reduce food wastage and improve the meal\nplanning experience.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00627v2",
    "published_date": "2024-12-01 00:52:51 UTC",
    "updated_date": "2024-12-09 08:27:07 UTC"
  },
  {
    "arxiv_id": "2412.00622v2",
    "title": "Visual Modality Prompt for Adapting Vision-Language Object Detectors",
    "authors": [
      "Heitor R. Medeiros",
      "Atif Belal",
      "Srikanth Muralidharan",
      "Eric Granger",
      "Marco Pedersoli"
    ],
    "abstract": "The zero-shot performance of object detectors degrades when tested on\ndifferent modalities, such as infrared and depth. While recent work has\nexplored image translation techniques to adapt detectors to new modalities,\nthese methods are limited to a single modality and apply only to traditional\ndetectors. Recently, vision-language detectors, such as YOLO-World and\nGrounding DINO, have shown promising zero-shot capabilities, however, they have\nnot yet been adapted for other visual modalities. Traditional fine-tuning\napproaches compromise the zero-shot capabilities of the detectors. The visual\nprompt strategies commonly used for classification with vision-language models\napply the same linear prompt translation to each image, making them less\neffective. To address these limitations, we propose ModPrompt, a visual prompt\nstrategy to adapt vision-language detectors to new modalities without degrading\nzero-shot performance. In particular, an encoder-decoder visual prompt strategy\nis proposed, further enhanced by the integration of inference-friendly modality\nprompt decoupled residual, facilitating a more robust adaptation. Empirical\nbenchmarking results show our method for modality adaptation on two\nvision-language detectors, YOLO-World and Grounding DINO, and on challenging\ninfrared (LLVIP, FLIR) and depth (NYUv2) datasets, achieving performance\ncomparable to full fine-tuning while preserving the model's zero-shot\ncapability. Code available at: https://github.com/heitorrapela/ModPrompt.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00622v2",
    "published_date": "2024-12-01 00:19:59 UTC",
    "updated_date": "2025-03-14 20:32:12 UTC"
  },
  {
    "arxiv_id": "2412.00621v1",
    "title": "Exposing LLM Vulnerabilities: Adversarial Scam Detection and Performance",
    "authors": [
      "Chen-Wei Chang",
      "Shailik Sarkar",
      "Shutonu Mitra",
      "Qi Zhang",
      "Hossein Salemi",
      "Hemant Purohit",
      "Fengxiu Zhang",
      "Michin Hong",
      "Jin-Hee Cho",
      "Chang-Tien Lu"
    ],
    "abstract": "Can we trust Large Language Models (LLMs) to accurately predict scam? This\npaper investigates the vulnerabilities of LLMs when facing adversarial scam\nmessages for the task of scam detection. We addressed this issue by creating a\ncomprehensive dataset with fine-grained labels of scam messages, including both\noriginal and adversarial scam messages. The dataset extended traditional binary\nclasses for the scam detection task into more nuanced scam types. Our analysis\nshowed how adversarial examples took advantage of vulnerabilities of a LLM,\nleading to high misclassification rate. We evaluated the performance of LLMs on\nthese adversarial scam messages and proposed strategies to improve their\nrobustness.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CR",
    "comment": "4 pages, 2024 IEEE International Conference on Big Data workshop\n  BigEACPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2412.00621v1",
    "published_date": "2024-12-01 00:13:28 UTC",
    "updated_date": "2024-12-01 00:13:28 UTC"
  }
]
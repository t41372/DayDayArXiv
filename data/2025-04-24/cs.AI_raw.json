[
  {
    "arxiv_id": "2504.17979v1",
    "title": "Fuzzy-RRT for Obstacle Avoidance in a 2-DOF Semi-Autonomous Surgical Robotic Arm",
    "authors": [
      "Kaaustaaub Shankar",
      "Wilhelm Louw",
      "Bharadwaj Dogga",
      "Nick Ernest",
      "Tim Arnett",
      "Kelly Cohen"
    ],
    "abstract": "AI-driven semi-autonomous robotic surgery is essential for addressing the\nmedical challenges of long-duration interplanetary missions, where limited crew\nsizes and communication delays restrict traditional surgical approaches.\nCurrent robotic surgery systems require full surgeon control, demanding\nextensive expertise and limiting feasibility in space. We propose a novel\nadaptation of the Fuzzy Rapidly-exploring Random Tree algorithm for obstacle\navoidance and collaborative control in a two-degree-of-freedom robotic arm\nmodeled on the Miniaturized Robotic-Assisted surgical system. It was found that\nthe Fuzzy Rapidly-exploring Random Tree algorithm resulted in an 743 percent\nimprovement to path search time and 43 percent improvement to path cost.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "9 pages, 5 figures. Submitted to NAFIPS 2025 Conference (North\n  American Fuzzy Information Processing Society). Includes results on Fuzzy-RRT\n  performance in surgical robotics path planning",
    "pdf_url": "http://arxiv.org/pdf/2504.17979v1",
    "published_date": "2025-04-24 23:19:27 UTC",
    "updated_date": "2025-04-24 23:19:27 UTC"
  },
  {
    "arxiv_id": "2504.17967v1",
    "title": "LLM Agent Swarm for Hypothesis-Driven Drug Discovery",
    "authors": [
      "Kevin Song",
      "Andrew Trotter",
      "Jake Y. Chen"
    ],
    "abstract": "Drug discovery remains a formidable challenge: more than 90 percent of\ncandidate molecules fail in clinical evaluation, and development costs often\nexceed one billion dollars per approved therapy. Disparate data streams, from\ngenomics and transcriptomics to chemical libraries and clinical records, hinder\ncoherent mechanistic insight and slow progress. Meanwhile, large language\nmodels excel at reasoning and tool integration but lack the modular\nspecialization and iterative memory required for regulated, hypothesis-driven\nworkflows. We introduce PharmaSwarm, a unified multi-agent framework that\norchestrates specialized LLM \"agents\" to propose, validate, and refine\nhypotheses for novel drug targets and lead compounds. Each agent accesses\ndedicated functionality--automated genomic and expression analysis; a curated\nbiomedical knowledge graph; pathway enrichment and network simulation;\ninterpretable binding affinity prediction--while a central Evaluator LLM\ncontinuously ranks proposals by biological plausibility, novelty, in silico\nefficacy, and safety. A shared memory layer captures validated insights and\nfine-tunes underlying submodels over time, yielding a self-improving system.\nDeployable on low-code platforms or Kubernetes-based microservices, PharmaSwarm\nsupports literature-driven discovery, omics-guided target identification, and\nmarket-informed repurposing. We also describe a rigorous four-tier validation\npipeline spanning retrospective benchmarking, independent computational assays,\nexperimental testing, and expert user studies to ensure transparency,\nreproducibility, and real-world impact. By acting as an AI copilot, PharmaSwarm\ncan accelerate translational research and deliver high-confidence hypotheses\nmore efficiently than traditional pipelines.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "15 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.17967v1",
    "published_date": "2025-04-24 22:27:50 UTC",
    "updated_date": "2025-04-24 22:27:50 UTC"
  },
  {
    "arxiv_id": "2504.17964v1",
    "title": "Evaluating Machine Expertise: How Graduate Students Develop Frameworks for Assessing GenAI Content",
    "authors": [
      "Celia Chen",
      "Alex Leitch"
    ],
    "abstract": "This paper examines how graduate students develop frameworks for evaluating\nmachine-generated expertise in web-based interactions with large language\nmodels (LLMs). Through a qualitative study combining surveys, LLM interaction\ntranscripts, and in-depth interviews with 14 graduate students, we identify\npatterns in how these emerging professionals assess and engage with\nAI-generated content. Our findings reveal that students construct evaluation\nframeworks shaped by three main factors: professional identity, verification\ncapabilities, and system navigation experience. Rather than uniformly accepting\nor rejecting LLM outputs, students protect domains central to their\nprofessional identities while delegating others--with managers preserving\nconceptual work, designers safeguarding creative processes, and programmers\nmaintaining control over core technical expertise. These evaluation frameworks\nare further influenced by students' ability to verify different types of\ncontent and their experience navigating complex systems. This research\ncontributes to web science by highlighting emerging human-genAI interaction\npatterns and suggesting how platforms might better support users in developing\neffective frameworks for evaluating machine-generated expertise signals in\nAI-mediated web environments.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "Under review at ACM Web Science Conference 2025's Human-GenAI\n  Interactions Workshop, 4 pages",
    "pdf_url": "http://arxiv.org/pdf/2504.17964v1",
    "published_date": "2025-04-24 22:24:14 UTC",
    "updated_date": "2025-04-24 22:24:14 UTC"
  },
  {
    "arxiv_id": "2504.18603v1",
    "title": "Toward Personalizing Quantum Computing Education: An Evolutionary LLM-Powered Approach",
    "authors": [
      "Iizalaarab Elhaimeur",
      "Nikos Chrisochoides"
    ],
    "abstract": "Quantum computing education faces significant challenges due to its\ncomplexity and the limitations of current tools; this paper introduces a novel\nIntelligent Teaching Assistant for quantum computing education and details its\nevolutionary design process. The system combines a knowledge-graph-augmented\narchitecture with two specialized Large Language Model (LLM) agents: a Teaching\nAgent for dynamic interaction, and a Lesson Planning Agent for lesson plan\ngeneration. The system is designed to adapt to individual student needs, with\ninteractions meticulously tracked and stored in a knowledge graph. This graph\nrepresents student actions, learning resources, and relationships, aiming to\nenable reasoning about effective learning pathways. We describe the\nimplementation of the system, highlighting the challenges encountered and the\nsolutions implemented, including introducing a dual-agent architecture where\ntasks are separated, all coordinated through a central knowledge graph that\nmaintains system awareness, and a user-facing tag system intended to mitigate\nLLM hallucination and improve user control. Preliminary results illustrate the\nsystem's potential to capture rich interaction data, dynamically adapt lesson\nplans based on student feedback via a tag system in simulation, and facilitate\ncontext-aware tutoring through the integrated knowledge graph, though\nsystematic evaluation is required.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.18603v1",
    "published_date": "2025-04-24 21:53:34 UTC",
    "updated_date": "2025-04-24 21:53:34 UTC"
  },
  {
    "arxiv_id": "2504.17929v2",
    "title": "ApproXAI: Energy-Efficient Hardware Acceleration of Explainable AI using Approximate Computing",
    "authors": [
      "Ayesha Siddique",
      "Khurram Khalil",
      "Khaza Anuarul Hoque"
    ],
    "abstract": "Explainable artificial intelligence (XAI) enhances AI system transparency by\nframing interpretability as an optimization problem. However, this approach\noften necessitates numerous iterations of computationally intensive operations,\nlimiting its applicability in real-time scenarios. While recent research has\nfocused on XAI hardware acceleration on FPGAs and TPU, these methods do not\nfully address energy efficiency in real-time settings. To address this\nlimitation, we propose XAIedge, a novel framework that leverages approximate\ncomputing techniques into XAI algorithms, including integrated gradients, model\ndistillation, and Shapley analysis. XAIedge translates these algorithms into\napproximate matrix computations and exploits the synergy between convolution,\nFourier transform, and approximate computing paradigms. This approach enables\nefficient hardware acceleration on TPU-based edge devices, facilitating faster\nreal-time outcome interpretations. Our comprehensive evaluation demonstrates\nthat XAIedge achieves a $2\\times$ improvement in energy efficiency compared to\nexisting accurate XAI hardware acceleration techniques while maintaining\ncomparable accuracy. These results highlight the potential of XAIedge to\nsignificantly advance the deployment of explainable AI in energy-constrained\nreal-time applications.",
    "categories": [
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at the International Joint Conference on Neural Networks\n  (IJCNN), June 30th - July 5th, 2025 in Rome, Italy",
    "pdf_url": "http://arxiv.org/pdf/2504.17929v2",
    "published_date": "2025-04-24 20:40:29 UTC",
    "updated_date": "2025-05-12 16:04:45 UTC"
  },
  {
    "arxiv_id": "2504.20074v1",
    "title": "EPSILON: Adaptive Fault Mitigation in Approximate Deep Neural Network using Statistical Signatures",
    "authors": [
      "Khurram Khalil",
      "Khaza Anuarul Hoque"
    ],
    "abstract": "The increasing adoption of approximate computing in deep neural network\naccelerators (AxDNNs) promises significant energy efficiency gains. However,\npermanent faults in AxDNNs can severely degrade their performance compared to\ntheir accurate counterparts (AccDNNs). Traditional fault detection and\nmitigation approaches, while effective for AccDNNs, introduce substantial\noverhead and latency, making them impractical for energy-constrained real-time\ndeployment. To address this, we introduce EPSILON, a lightweight framework that\nleverages pre-computed statistical signatures and layer-wise importance metrics\nfor efficient fault detection and mitigation in AxDNNs. Our framework\nintroduces a novel non-parametric pattern-matching algorithm that enables\nconstant-time fault detection without interrupting normal execution while\ndynamically adapting to different network architectures and fault patterns.\nEPSILON maintains model accuracy by intelligently adjusting mitigation\nstrategies based on a statistical analysis of weight distribution and layer\ncriticality while preserving the energy benefits of approximate computing.\nExtensive evaluations across various approximate multipliers, AxDNN\narchitectures, popular datasets (MNIST, CIFAR-10, CIFAR-100, ImageNet-1k), and\nfault scenarios demonstrate that EPSILON maintains 80.05\\% accuracy while\noffering 22\\% improvement in inference time and 28\\% improvement in energy\nefficiency, establishing EPSILON as a practical solution for deploying reliable\nAxDNNs in safety-critical edge applications.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.DC",
    "comment": "Accepted at the International Joint Conference on Neural Networks\n  (IJCNN), June 30th - July 5th, 2025 in Rome, Italy",
    "pdf_url": "http://arxiv.org/pdf/2504.20074v1",
    "published_date": "2025-04-24 20:37:37 UTC",
    "updated_date": "2025-04-24 20:37:37 UTC"
  },
  {
    "arxiv_id": "2504.17921v1",
    "title": "Avoiding Leakage Poisoning: Concept Interventions Under Distribution Shifts",
    "authors": [
      "Mateo Espinosa Zarlenga",
      "Gabriele Dominici",
      "Pietro Barbiero",
      "Zohreh Shams",
      "Mateja Jamnik"
    ],
    "abstract": "In this paper, we investigate how concept-based models (CMs) respond to\nout-of-distribution (OOD) inputs. CMs are interpretable neural architectures\nthat first predict a set of high-level concepts (e.g., stripes, black) and then\npredict a task label from those concepts. In particular, we study the impact of\nconcept interventions (i.e., operations where a human expert corrects a CM's\nmispredicted concepts at test time) on CMs' task predictions when inputs are\nOOD. Our analysis reveals a weakness in current state-of-the-art CMs, which we\nterm leakage poisoning, that prevents them from properly improving their\naccuracy when intervened on for OOD inputs. To address this, we introduce\nMixCEM, a new CM that learns to dynamically exploit leaked information missing\nfrom its concepts only when this information is in-distribution. Our results\nacross tasks with and without complete sets of concept annotations demonstrate\nthat MixCEMs outperform strong baselines by significantly improving their\naccuracy for both in-distribution and OOD samples in the presence and absence\nof concept interventions.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17921v1",
    "published_date": "2025-04-24 20:24:31 UTC",
    "updated_date": "2025-04-24 20:24:31 UTC"
  },
  {
    "arxiv_id": "2504.18601v1",
    "title": "The Philosophic Turn for AI Agents: Replacing centralized digital rhetoric with decentralized truth-seeking",
    "authors": [
      "Philipp Koralus"
    ],
    "abstract": "In the face of rapidly advancing AI technology, individuals will increasingly\nrely on AI agents to navigate life's growing complexities, raising critical\nconcerns about maintaining both human agency and autonomy. This paper addresses\na fundamental dilemma posed by AI decision-support systems: the risk of either\nbecoming overwhelmed by complex decisions, thus losing agency, or having\nautonomy compromised by externally controlled choice architectures reminiscent\nof ``nudging'' practices. While the ``nudge'' framework, based on the use of\nchoice-framing to guide individuals toward presumed beneficial outcomes,\ninitially appeared to preserve liberty, at AI-driven scale, it threatens to\nerode autonomy. To counteract this risk, the paper proposes a philosophic turn\nin AI design. AI should be constructed to facilitate decentralized\ntruth-seeking and open-ended inquiry, mirroring the Socratic method of\nphilosophical dialogue. By promoting individual and collective adaptive\nlearning, such AI systems would empower users to maintain control over their\njudgments, augmenting their agency without undermining autonomy. The paper\nconcludes by outlining essential features for autonomy-preserving AI systems,\nsketching a path toward AI systems that enhance human judgment rather than\nundermine it.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.18601v1",
    "published_date": "2025-04-24 19:34:43 UTC",
    "updated_date": "2025-04-24 19:34:43 UTC"
  },
  {
    "arxiv_id": "2504.17901v1",
    "title": "Beyond Task and Motion Planning: Hierarchical Robot Planning with General-Purpose Policies",
    "authors": [
      "Benned Hedegaard",
      "Ziyi Yang",
      "Yichen Wei",
      "Ahmed Jaafar",
      "Stefanie Tellex",
      "George Konidaris",
      "Naman Shah"
    ],
    "abstract": "Task and motion planning is a well-established approach for solving\nlong-horizon robot planning problems. However, traditional methods assume that\neach task-level robot action, or skill, can be reduced to kinematic motion\nplanning. In this work, we address the challenge of planning with both\nkinematic skills and closed-loop motor controllers that go beyond kinematic\nconsiderations. We propose a novel method that integrates these controllers\ninto motion planning using Composable Interaction Primitives (CIPs), enabling\nthe use of diverse, non-composable pre-learned skills in hierarchical robot\nplanning. Toward validating our Task and Skill Planning (TASP) approach, we\ndescribe ongoing robot experiments in real-world scenarios designed to\ndemonstrate how CIPs can allow a mobile manipulator robot to effectively\ncombine motion planning with general-purpose skills to accomplish complex\ntasks.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17901v1",
    "published_date": "2025-04-24 19:22:50 UTC",
    "updated_date": "2025-04-24 19:22:50 UTC"
  },
  {
    "arxiv_id": "2504.17892v1",
    "title": "Token Sequence Compression for Efficient Multimodal Computing",
    "authors": [
      "Yasmine Omri",
      "Parth Shroff",
      "Thierry Tambe"
    ],
    "abstract": "The exponential growth of Large Multimodal Models (LMMs) has driven\nadvancements in cross-modal reasoning but at significant computational costs.\nIn this work, we focus on visual language models. We highlight the redundancy\nand inefficiency in current vision encoders, and seek to construct an adaptive\ncompression method for multimodal data. In this work, we characterize a panoply\nof visual token selection and merging approaches through both benchmarking and\nqualitative analysis. In particular, we demonstrate that simple cluster-level\ntoken aggregation outperforms prior state-of-the-art works in token selection\nand merging, including merging at the vision encoder level and attention-based\napproaches. We underline the redundancy in current vision encoders, and shed\nlight on several puzzling trends regarding principles of visual token selection\nthrough cross-modal attention visualizations. This work is a first effort\ntowards more effective encoding and processing of high-dimensional data, and\npaves the way for more scalable and sustainable multimodal systems.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17892v1",
    "published_date": "2025-04-24 19:11:10 UTC",
    "updated_date": "2025-04-24 19:11:10 UTC"
  },
  {
    "arxiv_id": "2504.19940v1",
    "title": "Assessing the Potential of Generative Agents in Crowdsourced Fact-Checking",
    "authors": [
      "Luigia Costabile",
      "Gian Marco Orlando",
      "Valerio La Gatta",
      "Vincenzo Moscato"
    ],
    "abstract": "The growing spread of online misinformation has created an urgent need for\nscalable, reliable fact-checking solutions. Crowdsourced fact-checking - where\nnon-experts evaluate claim veracity - offers a cost-effective alternative to\nexpert verification, despite concerns about variability in quality and bias.\nEncouraged by promising results in certain contexts, major platforms such as X\n(formerly Twitter), Facebook, and Instagram have begun shifting from\ncentralized moderation to decentralized, crowd-based approaches.\n  In parallel, advances in Large Language Models (LLMs) have shown strong\nperformance across core fact-checking tasks, including claim detection and\nevidence evaluation. However, their potential role in crowdsourced workflows\nremains unexplored. This paper investigates whether LLM-powered generative\nagents - autonomous entities that emulate human behavior and decision-making -\ncan meaningfully contribute to fact-checking tasks traditionally reserved for\nhuman crowds. Using the protocol of La Barbera et al. (2024), we simulate\ncrowds of generative agents with diverse demographic and ideological profiles.\nAgents retrieve evidence, assess claims along multiple quality dimensions, and\nissue final veracity judgments.\n  Our results show that agent crowds outperform human crowds in truthfulness\nclassification, exhibit higher internal consistency, and show reduced\nsusceptibility to social and cognitive biases. Compared to humans, agents rely\nmore systematically on informative criteria such as Accuracy, Precision, and\nInformativeness, suggesting a more structured decision-making process. Overall,\nour findings highlight the potential of generative agents as scalable,\nconsistent, and less biased contributors to crowd-based fact-checking systems.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.19940v1",
    "published_date": "2025-04-24 18:49:55 UTC",
    "updated_date": "2025-04-24 18:49:55 UTC"
  },
  {
    "arxiv_id": "2504.18600v1",
    "title": "QuantBench: Benchmarking AI Methods for Quantitative Investment",
    "authors": [
      "Saizhuo Wang",
      "Hao Kong",
      "Jiadong Guo",
      "Fengrui Hua",
      "Yiyan Qi",
      "Wanyun Zhou",
      "Jiahao Zheng",
      "Xinyu Wang",
      "Lionel M. Ni",
      "Jian Guo"
    ],
    "abstract": "The field of artificial intelligence (AI) in quantitative investment has seen\nsignificant advancements, yet it lacks a standardized benchmark aligned with\nindustry practices. This gap hinders research progress and limits the practical\napplication of academic innovations. We present QuantBench, an industrial-grade\nbenchmark platform designed to address this critical need. QuantBench offers\nthree key strengths: (1) standardization that aligns with quantitative\ninvestment industry practices, (2) flexibility to integrate various AI\nalgorithms, and (3) full-pipeline coverage of the entire quantitative\ninvestment process. Our empirical studies using QuantBench reveal some critical\nresearch directions, including the need for continual learning to address\ndistribution shifts, improved methods for modeling relational financial data,\nand more robust approaches to mitigate overfitting in low signal-to-noise\nenvironments. By providing a common ground for evaluation and fostering\ncollaboration between researchers and practitioners, QuantBench aims to\naccelerate progress in AI for quantitative investment, similar to the impact of\nbenchmark platforms in computer vision and natural language processing.",
    "categories": [
      "q-fin.CP",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "q-fin.CP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.18600v1",
    "published_date": "2025-04-24 18:47:22 UTC",
    "updated_date": "2025-04-24 18:47:22 UTC"
  },
  {
    "arxiv_id": "2504.17878v1",
    "title": "Crypto-ncRNA: Non-coding RNA (ncRNA) Based Encryption Algorithm",
    "authors": [
      "Xu Wang",
      "Yiquan Wang",
      "Tin-yeh Huang"
    ],
    "abstract": "In the looming post-quantum era, traditional cryptographic systems are\nincreasingly vulnerable to quantum computing attacks that can compromise their\nmathematical foundations. To address this critical challenge, we propose\ncrypto-ncRNA-a bio-convergent cryptographic framework that leverages the\ndynamic folding properties of non-coding RNA (ncRNA) to generate high-entropy,\nquantum-resistant keys and produce unpredictable ciphertexts. The framework\nemploys a novel, multi-stage process: encoding plaintext into RNA sequences,\npredicting and manipulating RNA secondary structures using advanced algorithms,\nand deriving cryptographic keys through the intrinsic physical unclonability of\nRNA molecules. Experimental evaluations indicate that, although crypto-ncRNA's\nencryption speed is marginally lower than that of AES, it significantly\noutperforms RSA in terms of efficiency and scalability while achieving a 100%\npass rate on the NIST SP 800-22 randomness tests. These results demonstrate\nthat crypto-ncRNA offers a promising and robust approach for securing digital\ninfrastructures against the evolving threats posed by quantum computing.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted at the AI4NA workshop at ICLR 2025. 18pages, 4figures",
    "pdf_url": "http://arxiv.org/pdf/2504.17878v1",
    "published_date": "2025-04-24 18:30:35 UTC",
    "updated_date": "2025-04-24 18:30:35 UTC"
  },
  {
    "arxiv_id": "2504.17872v1",
    "title": "Flow Matching Ergodic Coverage",
    "authors": [
      "Max Muchen Sun",
      "Allison Pinosky",
      "Todd Murphey"
    ],
    "abstract": "Ergodic coverage effectively generates exploratory behaviors for embodied\nagents by aligning the spatial distribution of the agent's trajectory with a\ntarget distribution, where the difference between these two distributions is\nmeasured by the ergodic metric. However, existing ergodic coverage methods are\nconstrained by the limited set of ergodic metrics available for control\nsynthesis, fundamentally limiting their performance. In this work, we propose\nan alternative approach to ergodic coverage based on flow matching, a technique\nwidely used in generative inference for efficient and scalable sampling. We\nformally derive the flow matching problem for ergodic coverage and show that it\nis equivalent to a linear quadratic regulator problem with a closed-form\nsolution. Our formulation enables alternative ergodic metrics from generative\ninference that overcome the limitations of existing ones. These metrics were\npreviously infeasible for control synthesis but can now be supported with no\ncomputational overhead. Specifically, flow matching with the Stein variational\ngradient flow enables control synthesis directly over the score function of the\ntarget distribution, improving robustness to the unnormalized distributions; on\nthe other hand, flow matching with the Sinkhorn divergence flow enables an\noptimal transport-based ergodic metric, improving coverage performance on\nnon-smooth distributions with irregular supports. We validate the improved\nperformance and competitive computational efficiency of our method through\ncomprehensive numerical benchmarks and across different nonlinear dynamics. We\nfurther demonstrate the practicality of our method through a series of drawing\nand erasing tasks on a Franka robot.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "15 pages, 15 figures. Accepted to Robotics: Science and Systems (RSS)\n  2025. Project website: https://murpheylab.github.io/lqr-flow-matching/",
    "pdf_url": "http://arxiv.org/pdf/2504.17872v1",
    "published_date": "2025-04-24 18:18:35 UTC",
    "updated_date": "2025-04-24 18:18:35 UTC"
  },
  {
    "arxiv_id": "2504.20073v1",
    "title": "RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning",
    "authors": [
      "Zihan Wang",
      "Kangrui Wang",
      "Qineng Wang",
      "Pingyue Zhang",
      "Linjie Li",
      "Zhengyuan Yang",
      "Kefan Yu",
      "Minh Nhat Nguyen",
      "Licheng Liu",
      "Eli Gottlieb",
      "Monica Lam",
      "Yiping Lu",
      "Kyunghyun Cho",
      "Jiajun Wu",
      "Li Fei-Fei",
      "Lijuan Wang",
      "Yejin Choi",
      "Manling Li"
    ],
    "abstract": "Training large language models (LLMs) as interactive agents presents unique\nchallenges including long-horizon decision making and interacting with\nstochastic environment feedback. While reinforcement learning (RL) has enabled\nprogress in static tasks, multi-turn agent RL training remains underexplored.\nWe propose StarPO (State-Thinking-Actions-Reward Policy Optimization), a\ngeneral framework for trajectory-level agent RL, and introduce RAGEN, a modular\nsystem for training and evaluating LLM agents. Our study on three stylized\nenvironments reveals three core findings. First, our agent RL training shows a\nrecurring mode of Echo Trap where reward variance cliffs and gradient spikes;\nwe address this with StarPO-S, a stabilized variant with trajectory filtering,\ncritic incorporation, and decoupled clipping. Second, we find the shaping of RL\nrollouts would benefit from diverse initial states, medium interaction\ngranularity and more frequent sampling. Third, we show that without\nfine-grained, reasoning-aware reward signals, agent reasoning hardly emerge\nthrough multi-turn RL and they may show shallow strategies or hallucinated\nthoughts. Code and environments are available at\nhttps://github.com/RAGEN-AI/RAGEN.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.20073v1",
    "published_date": "2025-04-24 17:57:08 UTC",
    "updated_date": "2025-04-24 17:57:08 UTC"
  },
  {
    "arxiv_id": "2504.17838v1",
    "title": "CaRL: Learning Scalable Planning Policies with Simple Rewards",
    "authors": [
      "Bernhard Jaeger",
      "Daniel Dauner",
      "Jens Beißwenger",
      "Simon Gerstenecker",
      "Kashyap Chitta",
      "Andreas Geiger"
    ],
    "abstract": "We investigate reinforcement learning (RL) for privileged planning in\nautonomous driving. State-of-the-art approaches for this task are rule-based,\nbut these methods do not scale to the long tail. RL, on the other hand, is\nscalable and does not suffer from compounding errors like imitation learning.\nContemporary RL approaches for driving use complex shaped rewards that sum\nmultiple individual rewards, \\eg~progress, position, or orientation rewards. We\nshow that PPO fails to optimize a popular version of these rewards when the\nmini-batch size is increased, which limits the scalability of these approaches.\nInstead, we propose a new reward design based primarily on optimizing a single\nintuitive reward term: route completion. Infractions are penalized by\nterminating the episode or multiplicatively reducing route completion. We find\nthat PPO scales well with higher mini-batch sizes when trained with our simple\nreward, even improving performance. Training with large mini-batch sizes\nenables efficient scaling via distributed data parallelism. We scale PPO to\n300M samples in CARLA and 500M samples in nuPlan with a single 8-GPU node. The\nresulting model achieves 64 DS on the CARLA longest6 v2 benchmark,\noutperforming other RL methods with more complex rewards by a large margin.\nRequiring only minimal adaptations from its use in CARLA, the same method is\nthe best learning-based approach on nuPlan. It scores 91.3 in non-reactive and\n90.6 in reactive traffic on the Val14 benchmark while being an order of\nmagnitude faster than prior work.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17838v1",
    "published_date": "2025-04-24 17:56:01 UTC",
    "updated_date": "2025-04-24 17:56:01 UTC"
  },
  {
    "arxiv_id": "2504.17771v2",
    "title": "Integrating Learning-Based Manipulation and Physics-Based Locomotion for Whole-Body Badminton Robot Control",
    "authors": [
      "Haochen Wang",
      "Zhiwei Shi",
      "Chengxi Zhu",
      "Yafei Qiao",
      "Cheng Zhang",
      "Fan Yang",
      "Pengjie Ren",
      "Lan Lu",
      "Dong Xuan"
    ],
    "abstract": "Learning-based methods, such as imitation learning (IL) and reinforcement\nlearning (RL), can produce excel control policies over challenging agile robot\ntasks, such as sports robot. However, no existing work has harmonized\nlearning-based policy with model-based methods to reduce training complexity\nand ensure the safety and stability for agile badminton robot control. In this\npaper, we introduce Hamlet, a novel hybrid control system for agile badminton\nrobots. Specifically, we propose a model-based strategy for chassis locomotion\nwhich provides a base for arm policy. We introduce a physics-informed \"IL+RL\"\ntraining framework for learning-based arm policy. In this train framework, a\nmodel-based strategy with privileged information is used to guide arm policy\ntraining during both IL and RL phases. In addition, we train the critic model\nduring IL phase to alleviate the performance drop issue when transitioning from\nIL to RL. We present results on our self-engineered badminton robot, achieving\n94.5% success rate against the serving machine and 90.7% success rate against\nhuman players. Our system can be easily generalized to other agile mobile\nmanipulation tasks such as agile catching and table tennis. Our project\nwebsite: https://dreamstarring.github.io/HAMLET/.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted to ICRA 2025. Project page:\n  https://dreamstarring.github.io/HAMLET/",
    "pdf_url": "http://arxiv.org/pdf/2504.17771v2",
    "published_date": "2025-04-24 17:46:29 UTC",
    "updated_date": "2025-04-27 14:23:00 UTC"
  },
  {
    "arxiv_id": "2505.00022v1",
    "title": "Aleph-Alpha-GermanWeb: Improving German-language LLM pre-training with model-based data curation and synthetic data generation",
    "authors": [
      "Thomas F Burns",
      "Letitia Parcalabescu",
      "Stephan Wäldchen",
      "Michael Barlow",
      "Gregor Ziegltrum",
      "Volker Stampa",
      "Bastian Harren",
      "Björn Deiseroth"
    ],
    "abstract": "Scaling data quantity is essential for large language models (LLMs), yet\nrecent findings show that data quality can significantly boost performance and\ntraining efficiency. We introduce a German-language dataset curation pipeline\nthat combines heuristic and model-based filtering techniques with synthetic\ndata generation. We use our pipeline to create Aleph-Alpha-GermanWeb, a\nlarge-scale German pre-training dataset which draws from: (1) Common Crawl web\ndata, (2) FineWeb2, and (3) synthetically-generated data conditioned on actual,\norganic web data. We evaluate our dataset by pre-training both a 1B Llama-style\nmodel and an 8B tokenizer-free hierarchical autoregressive transformer (HAT). A\ncomparison on German-language benchmarks, including MMMLU, shows significant\nperformance gains of Aleph-Alpha-GermanWeb over FineWeb2 alone. This advantage\nholds at the 8B scale even when FineWeb2 is enriched by human-curated\nhigh-quality data sources such as Wikipedia. Our findings support the growing\nbody of evidence that model-based data curation and synthetic data generation\ncan significantly enhance LLM pre-training datasets.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.00022v1",
    "published_date": "2025-04-24 17:23:46 UTC",
    "updated_date": "2025-04-24 17:23:46 UTC"
  },
  {
    "arxiv_id": "2504.17751v3",
    "title": "Revisiting Reset Mechanisms in Spiking Neural Networks for Sequential Modeling: Specialized Discretization for Binary Activated RNN",
    "authors": [
      "Enqi Zhang"
    ],
    "abstract": "In the field of image recognition, spiking neural networks (SNNs) have\nachieved performance comparable to conventional artificial neural networks\n(ANNs). In such applications, SNNs essentially function as traditional neural\nnetworks with quantized activation values. This article focuses on an another\nalternative perspective,viewing SNNs as binary-activated recurrent neural\nnetworks (RNNs) for sequential modeling tasks. From this viewpoint, current SNN\narchitectures face several fundamental challenges in sequence modeling: (1)\nTraditional models lack effective memory mechanisms for long-range sequence\nmodeling; (2) The biological-inspired components in SNNs (such as reset\nmechanisms and refractory period applications) remain theoretically\nunder-explored for sequence tasks; (3) The RNN-like computational paradigm in\nSNNs prevents parallel training across different timesteps. To address these\nchallenges, this study conducts a systematic analysis of the fundamental\nmechanisms underlying reset operations and refractory periods in\nbinary-activated RNN-based SNN sequence models. We re-examine whether such\nbiological mechanisms are strictly necessary for generating sparse spiking\npatterns, provide new theoretical explanations and insights, and ultimately\npropose the fixed-refractory-period SNN architecture for sequence modeling.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17751v3",
    "published_date": "2025-04-24 17:09:59 UTC",
    "updated_date": "2025-04-29 18:43:45 UTC"
  },
  {
    "arxiv_id": "2504.18598v2",
    "title": "BadMoE: Backdooring Mixture-of-Experts LLMs via Optimizing Routing Triggers and Infecting Dormant Experts",
    "authors": [
      "Qingyue Wang",
      "Qi Pang",
      "Xixun Lin",
      "Shuai Wang",
      "Daoyuan Wu"
    ],
    "abstract": "Mixture-of-Experts (MoE) have emerged as a powerful architecture for large\nlanguage models (LLMs), enabling efficient scaling of model capacity while\nmaintaining manageable computational costs. The key advantage lies in their\nability to route different tokens to different ``expert'' networks within the\nmodel, enabling specialization and efficient handling of diverse input.\nHowever, the vulnerabilities of MoE-based LLMs still have barely been studied,\nand the potential for backdoor attacks in this context remains largely\nunexplored. This paper presents the first backdoor attack against MoE-based\nLLMs where the attackers poison ``dormant experts'' (i.e., underutilized\nexperts) and activate them by optimizing routing triggers, thereby gaining\ncontrol over the model's output. We first rigorously prove the existence of a\nfew ``dominating experts'' in MoE models, whose outputs can determine the\noverall MoE's output. We also show that dormant experts can serve as dominating\nexperts to manipulate model predictions. Accordingly, our attack, namely\nBadMoE, exploits the unique architecture of MoE models by 1) identifying\ndormant experts unrelated to the target task, 2) constructing a routing-aware\nloss to optimize the activation triggers of these experts, and 3) promoting\ndormant experts to dominating roles via poisoned training data. Extensive\nexperiments show that BadMoE successfully enforces malicious prediction on\nattackers' target tasks while preserving overall model utility, making it a\nmore potent and stealthy attack than existing methods.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.18598v2",
    "published_date": "2025-04-24 16:42:38 UTC",
    "updated_date": "2025-04-29 02:23:57 UTC"
  },
  {
    "arxiv_id": "2505.00021v1",
    "title": "Ustnlp16 at SemEval-2025 Task 9: Improving Model Performance through Imbalance Handling and Focal Loss",
    "authors": [
      "Zhuoang Cai",
      "Zhenghao Li",
      "Yang Liu",
      "Liyuan Guo",
      "Yangqiu Song"
    ],
    "abstract": "Classification tasks often suffer from imbal- anced data distribution, which\npresents chal- lenges in food hazard detection due to severe class imbalances,\nshort and unstructured text, and overlapping semantic categories. In this\npaper, we present our system for SemEval- 2025 Task 9: Food Hazard Detection,\nwhich ad- dresses these issues by applying data augmenta- tion techniques to\nimprove classification perfor- mance. We utilize transformer-based models, BERT\nand RoBERTa, as backbone classifiers and explore various data balancing\nstrategies, including random oversampling, Easy Data Augmentation (EDA), and\nfocal loss. Our ex- periments show that EDA effectively mitigates class\nimbalance, leading to significant improve- ments in accuracy and F1 scores.\nFurthermore, combining focal loss with oversampling and EDA further enhances\nmodel robustness, par- ticularly for hard-to-classify examples. These findings\ncontribute to the development of more effective NLP-based classification models\nfor food hazard detection.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.00021v1",
    "published_date": "2025-04-24 16:35:44 UTC",
    "updated_date": "2025-04-24 16:35:44 UTC"
  },
  {
    "arxiv_id": "2504.17721v1",
    "title": "Conformal Segmentation in Industrial Surface Defect Detection with Statistical Guarantees",
    "authors": [
      "Cheng Shen",
      "Yuewei Liu"
    ],
    "abstract": "In industrial settings, surface defects on steel can significantly compromise\nits service life and elevate potential safety risks. Traditional defect\ndetection methods predominantly rely on manual inspection, which suffers from\nlow efficiency and high costs. Although automated defect detection approaches\nbased on Convolutional Neural Networks(e.g., Mask R-CNN) have advanced rapidly,\ntheir reliability remains challenged due to data annotation uncertainties\nduring deep model training and overfitting issues. These limitations may lead\nto detection deviations when processing the given new test samples, rendering\nautomated detection processes unreliable. To address this challenge, we first\nevaluate the detection model's practical performance through calibration data\nthat satisfies the independent and identically distributed (i.i.d) condition\nwith test data. Specifically, we define a loss function for each calibration\nsample to quantify detection error rates, such as the complement of recall rate\nand false discovery rate. Subsequently, we derive a statistically rigorous\nthreshold based on a user-defined risk level to identify high-probability\ndefective pixels in test images, thereby constructing prediction sets (e.g.,\ndefect regions). This methodology ensures that the expected error rate (mean\nerror rate) on the test set remains strictly bounced by the predefined risk\nlevel. Additionally, we observe a negative correlation between the average\nprediction set size and the risk level on the test set, establishing a\nstatistically rigorous metric for assessing detection model uncertainty.\nFurthermore, our study demonstrates robust and efficient control over the\nexpected test set error rate across varying calibration-to-test partitioning\nratios, validating the method's adaptability and operational effectiveness.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Under Review",
    "pdf_url": "http://arxiv.org/pdf/2504.17721v1",
    "published_date": "2025-04-24 16:33:56 UTC",
    "updated_date": "2025-04-24 16:33:56 UTC"
  },
  {
    "arxiv_id": "2504.17720v1",
    "title": "Multilingual Performance Biases of Large Language Models in Education",
    "authors": [
      "Vansh Gupta",
      "Sankalan Pal Chowdhury",
      "Vilém Zouhar",
      "Donya Rooein",
      "Mrinmaya Sachan"
    ],
    "abstract": "Large language models (LLMs) are increasingly being adopted in educational\nsettings. These applications expand beyond English, though current LLMs remain\nprimarily English-centric. In this work, we ascertain if their use in education\nsettings in non-English languages is warranted. We evaluated the performance of\npopular LLMs on four educational tasks: identifying student misconceptions,\nproviding targeted feedback, interactive tutoring, and grading translations in\nsix languages (Hindi, Arabic, Farsi, Telugu, Ukrainian, Czech) in addition to\nEnglish. We find that the performance on these tasks somewhat corresponds to\nthe amount of language represented in training data, with lower-resource\nlanguages having poorer task performance. Although the models perform\nreasonably well in most languages, the frequent performance drop from English\nis significant. Thus, we recommend that practitioners first verify that the LLM\nworks well in the target language for their educational task before deployment.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17720v1",
    "published_date": "2025-04-24 16:32:31 UTC",
    "updated_date": "2025-04-24 16:32:31 UTC"
  },
  {
    "arxiv_id": "2505.14693v1",
    "title": "Propositional Measure Logic",
    "authors": [
      "Francisco Aragão"
    ],
    "abstract": "We present a propositional logic with fundamental probabilistic semantics, in\nwhich each formula is given a real measure in the interval $[0,1]$ that\nrepresents its degree of truth. This semantics replaces the binarity of\nclassical logic, while preserving its deductive structure. We demonstrate the\nsoundness theorem, establishing that the proposed system is sound and suitable\nfor reasoning under uncertainty. We discuss potential applications and avenues\nfor future extensions of the theory. We apply probabilistic logic to a still\nrefractory problem in Bayesian Networks.",
    "categories": [
      "cs.LO",
      "cs.AI",
      "Primary: 03B48, Secondary: 68T27, 60A99, 68T37"
    ],
    "primary_category": "cs.LO",
    "comment": "!0 pages",
    "pdf_url": "http://arxiv.org/pdf/2505.14693v1",
    "published_date": "2025-04-24 16:21:16 UTC",
    "updated_date": "2025-04-24 16:21:16 UTC"
  },
  {
    "arxiv_id": "2504.17717v1",
    "title": "Early Detection of Multidrug Resistance Using Multivariate Time Series Analysis and Interpretable Patient-Similarity Representations",
    "authors": [
      "Óscar Escudero-Arnanz",
      "Antonio G. Marques",
      "Inmaculada Mora-Jiménez",
      "Joaquín Álvarez-Rodríguez",
      "Cristina Soguero-Ruiz"
    ],
    "abstract": "Background and Objectives: Multidrug Resistance (MDR) is a critical global\nhealth issue, causing increased hospital stays, healthcare costs, and\nmortality. This study proposes an interpretable Machine Learning (ML) framework\nfor MDR prediction, aiming for both accurate inference and enhanced\nexplainability.\n  Methods: Patients are modeled as Multivariate Time Series (MTS), capturing\nclinical progression and patient-to-patient interactions. Similarity among\npatients is quantified using MTS-based methods: descriptive statistics, Dynamic\nTime Warping, and Time Cluster Kernel. These similarity measures serve as\ninputs for MDR classification via Logistic Regression, Random Forest, and\nSupport Vector Machines, with dimensionality reduction and kernel\ntransformations improving model performance. For explainability, patient\nsimilarity networks are constructed from these metrics. Spectral clustering and\nt-SNE are applied to identify MDR-related subgroups and visualize high-risk\nclusters, enabling insight into clinically relevant patterns.\n  Results: The framework was validated on ICU Electronic Health Records from\nthe University Hospital of Fuenlabrada, achieving an AUC of 81%. It outperforms\nbaseline ML and deep learning models by leveraging graph-based patient\nsimilarity. The approach identifies key risk factors -- prolonged antibiotic\nuse, invasive procedures, co-infections, and extended ICU stays -- and reveals\nclinically meaningful clusters. Code and results are available at\n\\https://github.com/oscarescuderoarnanz/DM4MTS.\n  Conclusions: Patient similarity representations combined with graph-based\nanalysis provide accurate MDR prediction and interpretable insights. This\nmethod supports early detection, risk factor identification, and patient\nstratification, highlighting the potential of explainable ML in critical care.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17717v1",
    "published_date": "2025-04-24 16:19:13 UTC",
    "updated_date": "2025-04-24 16:19:13 UTC"
  },
  {
    "arxiv_id": "2504.17703v1",
    "title": "Federated Learning: A Survey on Privacy-Preserving Collaborative Intelligence",
    "authors": [
      "Edward Collins",
      "Michel Wang"
    ],
    "abstract": "Federated Learning (FL) has emerged as a transformative paradigm in the field\nof distributed machine learning, enabling multiple clients such as mobile\ndevices, edge nodes, or organizations to collaboratively train a shared global\nmodel without the need to centralize sensitive data. This decentralized\napproach addresses growing concerns around data privacy, security, and\nregulatory compliance, making it particularly attractive in domains such as\nhealthcare, finance, and smart IoT systems. This survey provides a concise yet\ncomprehensive overview of Federated Learning, beginning with its core\narchitecture and communication protocol. We discuss the standard FL lifecycle,\nincluding local training, model aggregation, and global updates. A particular\nemphasis is placed on key technical challenges such as handling non-IID\n(non-independent and identically distributed) data, mitigating system and\nhardware heterogeneity, reducing communication overhead, and ensuring privacy\nthrough mechanisms like differential privacy and secure aggregation.\nFurthermore, we examine emerging trends in FL research, including personalized\nFL, cross-device versus cross-silo settings, and integration with other\nparadigms such as reinforcement learning and quantum computing. We also\nhighlight real-world applications and summarize benchmark datasets and\nevaluation metrics commonly used in FL research. Finally, we outline open\nresearch problems and future directions to guide the development of scalable,\nefficient, and trustworthy FL systems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17703v1",
    "published_date": "2025-04-24 16:10:29 UTC",
    "updated_date": "2025-04-24 16:10:29 UTC"
  },
  {
    "arxiv_id": "2504.17696v3",
    "title": "Hierarchical and Multimodal Data for Daily Activity Understanding",
    "authors": [
      "Ghazal Kaviani",
      "Yavuz Yarici",
      "Seulgi Kim",
      "Mohit Prabhushankar",
      "Ghassan AlRegib",
      "Mashhour Solh",
      "Ameya Patil"
    ],
    "abstract": "Daily Activity Recordings for Artificial Intelligence (DARai, pronounced\n\"Dahr-ree\") is a multimodal, hierarchically annotated dataset constructed to\nunderstand human activities in real-world settings. DARai consists of\ncontinuous scripted and unscripted recordings of 50 participants in 10\ndifferent environments, totaling over 200 hours of data from 20 sensors\nincluding multiple camera views, depth and radar sensors, wearable inertial\nmeasurement units (IMUs), electromyography (EMG), insole pressure sensors,\nbiomonitor sensors, and gaze tracker.\n  To capture the complexity in human activities, DARai is annotated at three\nlevels of hierarchy: (i) high-level activities (L1) that are independent tasks,\n(ii) lower-level actions (L2) that are patterns shared between activities, and\n(iii) fine-grained procedures (L3) that detail the exact execution steps for\nactions. The dataset annotations and recordings are designed so that 22.7% of\nL2 actions are shared between L1 activities and 14.2% of L3 procedures are\nshared between L2 actions. The overlap and unscripted nature of DARai allows\ncounterfactual activities in the dataset.\n  Experiments with various machine learning models showcase the value of DARai\nin uncovering important challenges in human-centered applications.\nSpecifically, we conduct unimodal and multimodal sensor fusion experiments for\nrecognition, temporal localization, and future action anticipation across all\nhierarchical annotation levels. To highlight the limitations of individual\nsensors, we also conduct domain-variant experiments that are enabled by DARai's\nmulti-sensor and counterfactual activity design setup.\n  The code, documentation, and dataset are available at the dedicated DARai\nwebsite:\nhttps://alregib.ece.gatech.edu/software-and-datasets/darai-daily-activity-recordings-for-artificial-intelligence-and-machine-learning/",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17696v3",
    "published_date": "2025-04-24 16:04:00 UTC",
    "updated_date": "2025-05-13 16:36:40 UTC"
  },
  {
    "arxiv_id": "2504.17685v1",
    "title": "Ensemble Bayesian Inference: Leveraging Small Language Models to Achieve LLM-level Accuracy in Profile Matching Tasks",
    "authors": [
      "Haru-Tada Sato",
      "Fuka Matsuzaki",
      "Jun-ichiro Takahashi"
    ],
    "abstract": "This study explores the potential of small language model(SLM) ensembles to\nachieve accuracy comparable to proprietary large language models (LLMs). We\npropose Ensemble Bayesian Inference (EBI), a novel approach that applies\nBayesian estimation to combine judgments from multiple SLMs, allowing them to\nexceed the performance limitations of individual models. Our experiments on\ndiverse tasks(aptitude assessments and consumer profile analysis in both\nJapanese and English) demonstrate EBI's effectiveness. Notably, we analyze\ncases where incorporating models with negative Lift values into ensembles\nimproves overall performance, and we examine the method's efficacy across\ndifferent languages. These findings suggest new possibilities for constructing\nhigh-performance AI systems with limited computational resources and for\neffectively utilizing models with individually lower performance. Building on\nexisting research on LLM performance evaluation, ensemble methods, and\nopen-source LLM utilization, we discuss the novelty and significance of our\napproach.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "13 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.17685v1",
    "published_date": "2025-04-24 15:55:10 UTC",
    "updated_date": "2025-04-24 15:55:10 UTC"
  },
  {
    "arxiv_id": "2504.18596v1",
    "title": "Optimizing the Privacy-Utility Balance using Synthetic Data and Configurable Perturbation Pipelines",
    "authors": [
      "Anantha Sharma",
      "Swetha Devabhaktuni",
      "Eklove Mohan"
    ],
    "abstract": "This paper explores the strategic use of modern synthetic data generation and\nadvanced data perturbation techniques to enhance security, maintain analytical\nutility, and improve operational efficiency when managing large datasets, with\na particular focus on the Banking, Financial Services, and Insurance (BFSI)\nsector. We contrast these advanced methods encompassing generative models like\nGANs, sophisticated context-aware PII transformation, configurable statistical\nperturbation, and differential privacy with traditional anonymization\napproaches.\n  The goal is to create realistic, privacy-preserving datasets that retain high\nutility for complex machine learning tasks and analytics, a critical need in\nthe data-sensitive industries like BFSI, Healthcare, Retail, and\nTelecommunications. We discuss how these modern techniques potentially offer\nsignificant improvements in balancing privacy preservation while maintaining\ndata utility compared to older methods. Furthermore, we examine the potential\nfor operational gains, such as reduced overhead and accelerated analytics, by\nusing these privacy-enhanced datasets. We also explore key use cases where\nthese methods can mitigate regulatory risks and enable scalable, data-driven\ninnovation without compromising sensitive customer information.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "math.PR"
    ],
    "primary_category": "cs.CR",
    "comment": "18 pages, 8 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2504.18596v1",
    "published_date": "2025-04-24 15:52:53 UTC",
    "updated_date": "2025-04-24 15:52:53 UTC"
  },
  {
    "arxiv_id": "2505.00020v1",
    "title": "Beyond Public Access in LLM Pre-Training Data",
    "authors": [
      "Sruly Rosenblat",
      "Tim O'Reilly",
      "Ilan Strauss"
    ],
    "abstract": "Using a legally obtained dataset of 34 copyrighted O'Reilly Media books, we\napply the DE-COP membership inference attack method to investigate whether\nOpenAI's large language models were trained on copyrighted content without\nconsent. Our AUROC scores show that GPT-4o, OpenAI's more recent and capable\nmodel, demonstrates strong recognition of paywalled O'Reilly book content\n(AUROC = 82\\%), compared to OpenAI's earlier model GPT-3.5 Turbo. In contrast,\nGPT-3.5 Turbo shows greater relative recognition of publicly accessible\nO'Reilly book samples. GPT-4o Mini, as a much smaller model, shows no knowledge\nof public or non-public O'Reilly Media content when tested (AUROC $\\approx$\n50\\%). Testing multiple models, with the same cutoff date, helps us account for\npotential language shifts over time that might bias our findings. These results\nhighlight the urgent need for increased corporate transparency regarding\npre-training data sources as a means to develop formal licensing frameworks for\nAI content training",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.00020v1",
    "published_date": "2025-04-24 15:49:59 UTC",
    "updated_date": "2025-04-24 15:49:59 UTC"
  },
  {
    "arxiv_id": "2504.17677v1",
    "title": "INSIGHT: Bridging the Student-Teacher Gap in Times of Large Language Models",
    "authors": [
      "Jarne Thys",
      "Sebe Vanbrabant",
      "Davy Vanacken",
      "Gustavo Rovelo Ruiz"
    ],
    "abstract": "The rise of AI, especially Large Language Models, presents challenges and\nopportunities to integrate such technology into the classroom. AI has the\npotential to revolutionize education by helping teaching staff with various\ntasks, such as personalizing their teaching methods, but it also raises\nconcerns, for example, about the degradation of student-teacher interactions\nand user privacy. This paper introduces INSIGHT, a proof of concept to combine\nvarious AI tools to assist teaching staff and students in the process of\nsolving exercises. INSIGHT has a modular design that allows it to be integrated\ninto various higher education courses. We analyze students' questions to an LLM\nby extracting keywords, which we use to dynamically build an FAQ from students'\nquestions and provide new insights for the teaching staff to use for more\npersonalized face-to-face support. Future work could build upon INSIGHT by\nusing the collected data to provide adaptive learning and adjust content based\non student progress and learning styles to offer a more interactive and\ninclusive learning experience.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17677v1",
    "published_date": "2025-04-24 15:47:20 UTC",
    "updated_date": "2025-04-24 15:47:20 UTC"
  },
  {
    "arxiv_id": "2504.17675v1",
    "title": "Optimized Cloud Resource Allocation Using Genetic Algorithms for Energy Efficiency and QoS Assurance",
    "authors": [
      "Caroline Panggabean",
      "Devaraj Verma C",
      "Bhagyashree Gogoi",
      "Ranju Limbu",
      "Rhythm Sarker"
    ],
    "abstract": "Cloud computing environments demand dynamic and efficient resource management\nto ensure optimal performance, reduced energy consumption, and adherence to\nService Level Agreements (SLAs). This paper presents a Genetic Algorithm\n(GA)-based approach for Virtual Machine (VM) placement and consolidation,\naiming to minimize power usage while maintaining QoS constraints. The proposed\nmethod dynamically adjusts VM allocation based on real-time workload\nvariations, outperforming traditional heuristics such as First Fit Decreasing\n(FFD) and Best Fit Decreasing (BFD). Experimental results show notable\nreductions in energy consumption, VM migrations, SLA violation rates, and\nexecution time. A correlation heatmap further illustrates strong relationships\namong these key performance indicators, confirming the effectiveness of our\napproach in optimizing cloud resource utilization.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "7 pages, 5 figures, accepted for publication (not yet published)",
    "pdf_url": "http://arxiv.org/pdf/2504.17675v1",
    "published_date": "2025-04-24 15:45:40 UTC",
    "updated_date": "2025-04-24 15:45:40 UTC"
  },
  {
    "arxiv_id": "2504.17671v3",
    "title": "Data-Driven Calibration of Prediction Sets in Large Vision-Language Models Based on Inductive Conformal Prediction",
    "authors": [
      "Yuanchang Ye",
      "Weiyan Wen"
    ],
    "abstract": "This study addresses the critical challenge of hallucination mitigation in\nLarge Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks\nthrough a Split Conformal Prediction (SCP) framework. While LVLMs excel in\nmulti-modal reasoning, their outputs often exhibit hallucinated content with\nhigh confidence, posing risks in safety-critical applications. We propose a\nmodel-agnostic uncertainty quantification method that integrates dynamic\nthreshold calibration and cross-modal consistency verification. By partitioning\ndata into calibration and test sets, the framework computes nonconformity\nscores to construct prediction sets with statistical guarantees under\nuser-defined risk levels ($\\alpha$). Key innovations include: (1) rigorous\ncontrol of \\textbf{marginal coverage} to ensure empirical error rates remain\nstrictly below $\\alpha$; (2) dynamic adjustment of prediction set sizes\ninversely with $\\alpha$, filtering low-confidence outputs; (3) elimination of\nprior distribution assumptions and retraining requirements. Evaluations on\nbenchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces\ntheoretical guarantees across all $\\alpha$ values. The framework achieves\nstable performance across varying calibration-to-test split ratios,\nunderscoring its robustness for real-world deployment in healthcare, autonomous\nsystems, and other safety-sensitive domains. This work bridges the gap between\ntheoretical reliability and practical applicability in multi-modal AI systems,\noffering a scalable solution for hallucination detection and uncertainty-aware\ndecision-making.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by ICIPCA 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.17671v3",
    "published_date": "2025-04-24 15:39:46 UTC",
    "updated_date": "2025-05-15 16:24:49 UTC"
  },
  {
    "arxiv_id": "2504.17669v2",
    "title": "Towards a HIPAA Compliant Agentic AI System in Healthcare",
    "authors": [
      "Subash Neupane",
      "Sudip Mittal",
      "Shahram Rahimi"
    ],
    "abstract": "Agentic AI systems powered by Large Language Models (LLMs) as their\nfoundational reasoning engine, are transforming clinical workflows such as\nmedical report generation and clinical summarization by autonomously analyzing\nsensitive healthcare data and executing decisions with minimal human oversight.\nHowever, their adoption demands strict compliance with regulatory frameworks\nsuch as Health Insurance Portability and Accountability Act (HIPAA),\nparticularly when handling Protected Health Information (PHI). This\nwork-in-progress paper introduces a HIPAA-compliant Agentic AI framework that\nenforces regulatory compliance through dynamic, context-aware policy\nenforcement. Our framework integrates three core mechanisms: (1)\nAttribute-Based Access Control (ABAC) for granular PHI governance, (2) a hybrid\nPHI sanitization pipeline combining regex patterns and BERT-based model to\nminimize leakage, and (3) immutable audit trails for compliance verification.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17669v2",
    "published_date": "2025-04-24 15:38:20 UTC",
    "updated_date": "2025-05-06 21:45:48 UTC"
  },
  {
    "arxiv_id": "2504.17663v1",
    "title": "The Malicious Technical Ecosystem: Exposing Limitations in Technical Governance of AI-Generated Non-Consensual Intimate Images of Adults",
    "authors": [
      "Michelle L. Ding",
      "Harini Suresh"
    ],
    "abstract": "In this paper, we adopt a survivor-centered approach to locate and dissect\nthe role of sociotechnical AI governance in preventing AI-Generated\nNon-Consensual Intimate Images (AIG-NCII) of adults, colloquially known as\n\"deep fake pornography.\" We identify a \"malicious technical ecosystem\" or\n\"MTE,\" comprising of open-source face-swapping models and nearly 200\n\"nudifying\" software programs that allow non-technical users to create AIG-NCII\nwithin minutes. Then, using the National Institute of Standards and Technology\n(NIST) AI 100-4 report as a reflection of current synthetic content governance\nmethods, we show how the current landscape of practices fails to effectively\nregulate the MTE for adult AIG-NCII, as well as flawed assumptions explaining\nthese gaps.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17663v1",
    "published_date": "2025-04-24 15:31:46 UTC",
    "updated_date": "2025-04-24 15:31:46 UTC"
  },
  {
    "arxiv_id": "2504.17655v1",
    "title": "Aerial Image Classification in Scarce and Unconstrained Environments via Conformal Prediction",
    "authors": [
      "Farhad Pourkamali-Anaraki"
    ],
    "abstract": "This paper presents a comprehensive empirical analysis of conformal\nprediction methods on a challenging aerial image dataset featuring diverse\nevents in unconstrained environments. Conformal prediction is a powerful\npost-hoc technique that takes the output of any classifier and transforms it\ninto a set of likely labels, providing a statistical guarantee on the coverage\nof the true label. Unlike evaluations on standard benchmarks, our study\naddresses the complexities of data-scarce and highly variable real-world\nsettings. We investigate the effectiveness of leveraging pretrained models\n(MobileNet, DenseNet, and ResNet), fine-tuned with limited labeled data, to\ngenerate informative prediction sets. To further evaluate the impact of\ncalibration, we consider two parallel pipelines (with and without temperature\nscaling) and assess performance using two key metrics: empirical coverage and\naverage prediction set size. This setup allows us to systematically examine how\ncalibration choices influence the trade-off between reliability and efficiency.\nOur findings demonstrate that even with relatively small labeled samples and\nsimple nonconformity scores, conformal prediction can yield valuable\nuncertainty estimates for complex tasks. Moreover, our analysis reveals that\nwhile temperature scaling is often employed for calibration, it does not\nconsistently lead to smaller prediction sets, underscoring the importance of\ncareful consideration in its application. Furthermore, our results highlight\nthe significant potential of model compression techniques within the conformal\nprediction pipeline for deployment in resource-constrained environments. Based\non our observations, we advocate for future research to delve into the impact\nof noisy or ambiguous labels on conformal prediction performance and to explore\neffective model reduction strategies.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "17 pages, 5 figures, and 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2504.17655v1",
    "published_date": "2025-04-24 15:25:37 UTC",
    "updated_date": "2025-04-24 15:25:37 UTC"
  },
  {
    "arxiv_id": "2504.17641v2",
    "title": "PTCL: Pseudo-Label Temporal Curriculum Learning for Label-Limited Dynamic Graph",
    "authors": [
      "Shengtao Zhang",
      "Haokai Zhang",
      "Shiqi Lou",
      "Zicheng Wang",
      "Zinan Zeng",
      "Yilin Wang",
      "Minnan Luo"
    ],
    "abstract": "Dynamic node classification is critical for modeling evolving systems like\nfinancial transactions and academic collaborations. In such systems,\ndynamically capturing node information changes is critical for dynamic node\nclassification, which usually requires all labels at every timestamp. However,\nit is difficult to collect all dynamic labels in real-world scenarios due to\nhigh annotation costs and label uncertainty (e.g., ambiguous or delayed labels\nin fraud detection). In contrast, final timestamp labels are easier to obtain\nas they rely on complete temporal patterns and are usually maintained as a\nunique label for each user in many open platforms, without tracking the history\ndata. To bridge this gap, we propose PTCL(Pseudo-label Temporal Curriculum\nLearning), a pioneering method addressing label-limited dynamic node\nclassification where only final labels are available. PTCL introduces: (1) a\ntemporal decoupling architecture separating the backbone (learning time-aware\nrepresentations) and decoder (strictly aligned with final labels), which\ngenerate pseudo-labels, and (2) a Temporal Curriculum Learning strategy that\nprioritizes pseudo-labels closer to the final timestamp by assigning them\nhigher weights using an exponentially decaying function. We contribute a new\nacademic dataset (CoOAG), capturing long-range research interest in dynamic\ngraph. Experiments across real-world scenarios demonstrate PTCL's consistent\nsuperiority over other methods adapted to this task. Beyond methodology, we\npropose a unified framework FLiD (Framework for Label-Limited Dynamic Node\nClassification), consisting of a complete preparation workflow, training\npipeline, and evaluation standards, and supporting various models and datasets.\nThe code can be found at https://github.com/3205914485/FLiD.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "13 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.17641v2",
    "published_date": "2025-04-24 15:11:41 UTC",
    "updated_date": "2025-04-25 03:38:56 UTC"
  },
  {
    "arxiv_id": "2504.17624v1",
    "title": "Deciphering the unique dynamic activation pathway in a G protein-coupled receptor enables unveiling biased signaling and identifying cryptic allosteric sites in conformational intermediates",
    "authors": [
      "Jigang Fan",
      "Chunhao Zhu",
      "Xiaobing Lan",
      "Haiming Zhuang",
      "Mingyu Li",
      "Jian Zhang",
      "Shaoyong Lu"
    ],
    "abstract": "Neurotensin receptor 1 (NTSR1), a member of the Class A G protein-coupled\nreceptor superfamily, plays an important role in modulating dopaminergic\nneuronal activity and eliciting opioid-independent analgesia. Recent studies\nsuggest that promoting \\{beta}-arrestin-biased signaling in NTSR1 may diminish\ndrugs of abuse, such as psychostimulants, thereby offering a potential avenue\nfor treating human addiction-related disorders. In this study, we utilized a\nnovel computational and experimental approach that combined nudged elastic\nband-based molecular dynamics simulations, Markov state models, temporal\ncommunication network analysis, site-directed mutagenesis, and conformational\nbiosensors, to explore the intricate mechanisms underlying NTSR1 activation and\nbiased signaling. Our study reveals a dynamic stepwise transition mechanism and\nactivated transmission network associated with NTSR1 activation. It also yields\nvaluable insights into the complex interplay between the unique polar network,\nnon-conserved ion locks, and aromatic clusters in NTSR1 signaling. Moreover, we\nidentified a cryptic allosteric site located in the intracellular region of the\nreceptor that exists in an intermediate state within the activation pathway.\nCollectively, these findings contribute to a more profound understanding of\nNTSR1 activation and biased signaling at the atomic level, thereby providing a\npotential strategy for the development of NTSR1 allosteric modulators in the\nrealm of G protein-coupled receptor biology, biophysics, and medicine.",
    "categories": [
      "q-bio.BM",
      "cs.AI"
    ],
    "primary_category": "q-bio.BM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17624v1",
    "published_date": "2025-04-24 14:46:20 UTC",
    "updated_date": "2025-04-24 14:46:20 UTC"
  },
  {
    "arxiv_id": "2504.17619v1",
    "title": "Enhancing CNNs robustness to occlusions with bioinspired filters for border completion",
    "authors": [
      "Catarina P. Coutinho",
      "Aneeqa Merhab",
      "Janko Petkovic",
      "Ferdinando Zanchetta",
      "Rita Fioresi"
    ],
    "abstract": "We exploit the mathematical modeling of the visual cortex mechanism for\nborder completion to define custom filters for CNNs. We see a consistent\nimprovement in performance, particularly in accuracy, when our modified LeNet 5\nis tested with occluded MNIST images.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Submitted to the 7th International Conference on Geometric Science of\n  Information",
    "pdf_url": "http://arxiv.org/pdf/2504.17619v1",
    "published_date": "2025-04-24 14:43:55 UTC",
    "updated_date": "2025-04-24 14:43:55 UTC"
  },
  {
    "arxiv_id": "2504.17617v1",
    "title": "Decentralized Time Series Classification with ROCKET Features",
    "authors": [
      "Bruno Casella",
      "Matthias Jakobs",
      "Marco Aldinucci",
      "Sebastian Buschjäger"
    ],
    "abstract": "Time series classification (TSC) is a critical task with applications in\nvarious domains, including healthcare, finance, and industrial monitoring. Due\nto privacy concerns and data regulations, Federated Learning has emerged as a\npromising approach for learning from distributed time series data without\ncentralizing raw information. However, most FL solutions rely on a\nclient-server architecture, which introduces robustness and confidentiality\nrisks related to the distinguished role of the server, which is a single point\nof failure and can observe knowledge extracted from clients. To address these\nchallenges, we propose DROCKS, a fully decentralized FL framework for TSC that\nleverages ROCKET (RandOm Convolutional KErnel Transform) features. In DROCKS,\nthe global model is trained by sequentially traversing a structured path across\nfederation nodes, where each node refines the model and selects the most\neffective local kernels before passing them to the successor. Extensive\nexperiments on the UCR archive demonstrate that DROCKS outperforms\nstate-of-the-art client-server FL approaches while being more resilient to node\nfailures and malicious attacks. Our code is available at\nhttps://anonymous.4open.science/r/DROCKS-7FF3/README.md.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "68T07",
      "I.2.11; I.2.6"
    ],
    "primary_category": "cs.LG",
    "comment": "Submitted to Workshop on Federated Learning Advancements 2025, in\n  conjunction with ECML-PKDD, WAFL25",
    "pdf_url": "http://arxiv.org/pdf/2504.17617v1",
    "published_date": "2025-04-24 14:41:50 UTC",
    "updated_date": "2025-04-24 14:41:50 UTC"
  },
  {
    "arxiv_id": "2504.17609v1",
    "title": "STCL:Curriculum learning Strategies for deep learning image steganography models",
    "authors": [
      "Fengchun Liu",
      "Tong Zhang",
      "Chunying Zhang"
    ],
    "abstract": "Aiming at the problems of poor quality of steganographic images and slow\nnetwork convergence of image steganography models based on deep learning, this\npaper proposes a Steganography Curriculum Learning training strategy (STCL) for\ndeep learning image steganography models. So that only easy images are selected\nfor training when the model has poor fitting ability at the initial stage, and\ngradually expand to more difficult images, the strategy includes a difficulty\nevaluation strategy based on the teacher model and an knee point-based training\nscheduling strategy. Firstly, multiple teacher models are trained, and the\nconsistency of the quality of steganographic images under multiple teacher\nmodels is used as the difficulty score to construct the training subsets from\neasy to difficult. Secondly, a training control strategy based on knee points\nis proposed to reduce the possibility of overfitting on small training sets and\naccelerate the training process. Experimental results on three large public\ndatasets, ALASKA2, VOC2012 and ImageNet, show that the proposed image\nsteganography scheme is able to improve the model performance under multiple\nalgorithmic frameworks, which not only has a high PSNR, SSIM score, and\ndecoding accuracy, but also the steganographic images generated by the model\nunder the training of the STCL strategy have a low steganography analysis\nscores. You can find our code at\n\\href{https://github.com/chaos-boops/STCL}{https://github.com/chaos-boops/STCL}.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17609v1",
    "published_date": "2025-04-24 14:34:41 UTC",
    "updated_date": "2025-04-24 14:34:41 UTC"
  },
  {
    "arxiv_id": "2505.00019v1",
    "title": "An Empirical Study on Prompt Compression for Large Language Models",
    "authors": [
      "Zheng Zhang",
      "Jinyi Li",
      "Yihuai Lan",
      "Xiang Wang",
      "Hao Wang"
    ],
    "abstract": "Prompt engineering enables Large Language Models (LLMs) to perform a variety\nof tasks. However, lengthy prompts significantly increase computational\ncomplexity and economic costs. To address this issue, we study six prompt\ncompression methods for LLMs, aiming to reduce prompt length while maintaining\nLLM response quality. In this paper, we present a comprehensive analysis\ncovering aspects such as generation performance, model hallucinations, efficacy\nin multimodal tasks, word omission analysis, and more. We evaluate these\nmethods across 13 datasets, including news, scientific articles, commonsense\nQA, math QA, long-context QA, and VQA datasets. Our experiments reveal that\nprompt compression has a greater impact on LLM performance in long contexts\ncompared to short ones. In the Longbench evaluation, moderate compression even\nenhances LLM performance. Our code and data is available at\nhttps://github.com/3DAgentWorld/Toolkit-for-Prompt-Compression.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by Building Trust Workshop at ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2505.00019v1",
    "published_date": "2025-04-24 14:15:13 UTC",
    "updated_date": "2025-04-24 14:15:13 UTC"
  },
  {
    "arxiv_id": "2504.20069v1",
    "title": "A Simple Review of EEG Foundation Models: Datasets, Advancements and Future Perspectives",
    "authors": [
      "Junhong Lai",
      "Jiyu Wei",
      "Lin Yao",
      "Yueming Wang"
    ],
    "abstract": "Electroencephalogram (EEG) signals play a crucial role in understanding brain\nactivity and diagnosing neurological disorders. This review focuses on the\nrecent development of EEG foundation models(EEG-FMs), which have shown great\npotential in processing and analyzing EEG data. We discuss various EEG-FMs,\nincluding their architectures, pre-training strategies, their pre-training and\ndownstream datasets and other details. The review also highlights the\nchallenges and future directions in this field, aiming to provide a\ncomprehensive overview for researchers and practitioners interested in EEG\nanalysis and related EEG-FMs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.20069v1",
    "published_date": "2025-04-24 14:14:17 UTC",
    "updated_date": "2025-04-24 14:14:17 UTC"
  },
  {
    "arxiv_id": "2504.18595v1",
    "title": "EnviroPiNet: A Physics-Guided AI Model for Predicting Biofilter Performance",
    "authors": [
      "Uzma",
      "Fabien Cholet",
      "Domenic Quinn",
      "Cindy Smith",
      "Siming You",
      "William Sloan"
    ],
    "abstract": "Environmental biotechnologies, such as drinking water biofilters, rely on\ncomplex interactions between microbial communities and their surrounding\nphysical-chemical environments. Predicting the performance of these systems is\nchallenging due to high-dimensional, sparse datasets that lack diversity and\nfail to fully capture system behaviour. Accurate predictive models require\ninnovative, science-guided approaches. In this study, we present the first\napplication of Buckingham Pi theory to modelling biofilter performance. This\ndimensionality reduction technique identifies meaningful, dimensionless\nvariables that enhance predictive accuracy and improve model interpretability.\nUsing these variables, we developed the Environmental Buckingham Pi Neural\nNetwork (EnviroPiNet), a physics-guided model benchmarked against traditional\ndata-driven methods, including Principal Component Analysis (PCA) and\nautoencoder neural networks. Our findings demonstrate that the EnviroPiNet\nmodel achieves an R^2 value of 0.9236 on the testing dataset, significantly\noutperforming PCA and autoencoder methods. The Buckingham Pi variables also\nprovide insights into the physical and chemical relationships governing\nbiofilter behaviour, with implications for system design and optimization. This\nstudy highlights the potential of combining physical principles with AI\napproaches to model complex environmental systems characterized by sparse,\nhigh-dimensional datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.18595v1",
    "published_date": "2025-04-24 13:52:51 UTC",
    "updated_date": "2025-04-24 13:52:51 UTC"
  },
  {
    "arxiv_id": "2504.17551v2",
    "title": "Unsupervised Urban Land Use Mapping with Street View Contrastive Clustering and a Geographical Prior",
    "authors": [
      "Lin Che",
      "Yizi Chen",
      "Tanhua Jin",
      "Martin Raubal",
      "Konrad Schindler",
      "Peter Kiefer"
    ],
    "abstract": "Urban land use classification and mapping are critical for urban planning,\nresource management, and environmental monitoring. Existing remote sensing\ntechniques often lack precision in complex urban environments due to the\nabsence of ground-level details. Unlike aerial perspectives, street view images\nprovide a ground-level view that captures more human and social activities\nrelevant to land use in complex urban scenes. Existing street view-based\nmethods primarily rely on supervised classification, which is challenged by the\nscarcity of high-quality labeled data and the difficulty of generalizing across\ndiverse urban landscapes. This study introduces an unsupervised contrastive\nclustering model for street view images with a built-in geographical prior, to\nenhance clustering performance. When combined with a simple visual assignment\nof the clusters, our approach offers a flexible and customizable solution to\nland use mapping, tailored to the specific needs of urban planners. We\nexperimentally show that our method can generate land use maps from geotagged\nstreet view image datasets of two cities. As our methodology relies on the\nuniversal spatial coherence of geospatial data (\"Tobler's law\"), it can be\nadapted to various settings where street view images are available, to enable\nscalable, unsupervised land use mapping and updating. The code will be\navailable at https://github.com/lin102/CCGP.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "11 pages, 7 figures, preprint version",
    "pdf_url": "http://arxiv.org/pdf/2504.17551v2",
    "published_date": "2025-04-24 13:41:27 UTC",
    "updated_date": "2025-05-13 16:31:13 UTC"
  },
  {
    "arxiv_id": "2504.17550v1",
    "title": "HalluLens: LLM Hallucination Benchmark",
    "authors": [
      "Yejin Bang",
      "Ziwei Ji",
      "Alan Schelten",
      "Anthony Hartshorn",
      "Tara Fowler",
      "Cheng Zhang",
      "Nicola Cancedda",
      "Pascale Fung"
    ],
    "abstract": "Large language models (LLMs) often generate responses that deviate from user\ninput or training data, a phenomenon known as \"hallucination.\" These\nhallucinations undermine user trust and hinder the adoption of generative AI\nsystems. Addressing hallucinations is essential for the advancement of LLMs.\nThis paper introduces a comprehensive hallucination benchmark, incorporating\nboth new extrinsic and existing intrinsic evaluation tasks, built upon clear\ntaxonomy of hallucination. A major challenge in benchmarking hallucinations is\nthe lack of a unified framework due to inconsistent definitions and\ncategorizations. We disentangle LLM hallucination from \"factuality,\" proposing\na clear taxonomy that distinguishes between extrinsic and intrinsic\nhallucinations, to promote consistency and facilitate research. Extrinsic\nhallucinations, where the generated content is not consistent with the training\ndata, are increasingly important as LLMs evolve. Our benchmark includes dynamic\ntest set generation to mitigate data leakage and ensure robustness against such\nleakage. We also analyze existing benchmarks, highlighting their limitations\nand saturation. The work aims to: (1) establish a clear taxonomy of\nhallucinations, (2) introduce new extrinsic hallucination tasks, with data that\ncan be dynamically regenerated to prevent saturation by leakage, (3) provide a\ncomprehensive analysis of existing benchmarks, distinguishing them from\nfactuality evaluations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "42 pages",
    "pdf_url": "http://arxiv.org/pdf/2504.17550v1",
    "published_date": "2025-04-24 13:40:27 UTC",
    "updated_date": "2025-04-24 13:40:27 UTC"
  },
  {
    "arxiv_id": "2504.17544v1",
    "title": "Auditing the Ethical Logic of Generative AI Models",
    "authors": [
      "W. Russell Neuman",
      "Chad Coleman",
      "Ali Dasdan",
      "Safinah Ali",
      "Manan Shah"
    ],
    "abstract": "As generative AI models become increasingly integrated into high-stakes\ndomains, the need for robust methods to evaluate their ethical reasoning\nbecomes increasingly important. This paper introduces a five-dimensional audit\nmodel -- assessing Analytic Quality, Breadth of Ethical Considerations, Depth\nof Explanation, Consistency, and Decisiveness -- to evaluate the ethical logic\nof leading large language models (LLMs). Drawing on traditions from applied\nethics and higher-order thinking, we present a multi-battery prompt approach,\nincluding novel ethical dilemmas, to probe the models' reasoning across diverse\ncontexts. We benchmark seven major LLMs finding that while models generally\nconverge on ethical decisions, they vary in explanatory rigor and moral\nprioritization. Chain-of-Thought prompting and reasoning-optimized models\nsignificantly enhance performance on our audit metrics. This study introduces a\nscalable methodology for ethical benchmarking of AI systems and highlights the\npotential for AI to complement human moral reasoning in complex decision-making\ncontexts.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17544v1",
    "published_date": "2025-04-24 13:32:30 UTC",
    "updated_date": "2025-04-24 13:32:30 UTC"
  },
  {
    "arxiv_id": "2504.17540v1",
    "title": "An Explainable Nature-Inspired Framework for Monkeypox Diagnosis: Xception Features Combined with NGBoost and African Vultures Optimization Algorithm",
    "authors": [
      "Ahmadreza Shateri",
      "Negar Nourani",
      "Morteza Dorrigiv",
      "Hamid Nasiri"
    ],
    "abstract": "The recent global spread of monkeypox, particularly in regions where it has\nnot historically been prevalent, has raised significant public health concerns.\nEarly and accurate diagnosis is critical for effective disease management and\ncontrol. In response, this study proposes a novel deep learning-based framework\nfor the automated detection of monkeypox from skin lesion images, leveraging\nthe power of transfer learning, dimensionality reduction, and advanced machine\nlearning techniques. We utilize the newly developed Monkeypox Skin Lesion\nDataset (MSLD), which includes images of monkeypox, chickenpox, and measles, to\ntrain and evaluate our models. The proposed framework employs the Xception\narchitecture for deep feature extraction, followed by Principal Component\nAnalysis (PCA) for dimensionality reduction, and the Natural Gradient Boosting\n(NGBoost) algorithm for classification. To optimize the model's performance and\ngeneralization, we introduce the African Vultures Optimization Algorithm (AVOA)\nfor hyperparameter tuning, ensuring efficient exploration of the parameter\nspace. Our results demonstrate that the proposed AVOA-NGBoost model achieves\nstate-of-the-art performance, with an accuracy of 97.53%, F1-score of 97.72%\nand an AUC of 97.47%. Additionally, we enhance model interpretability using\nGrad-CAM and LIME techniques, providing insights into the decision-making\nprocess and highlighting key features influencing classification. This\nframework offers a highly precise and efficient diagnostic tool, potentially\naiding healthcare providers in early detection and diagnosis, particularly in\nresource-constrained environments.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17540v1",
    "published_date": "2025-04-24 13:32:11 UTC",
    "updated_date": "2025-04-24 13:32:11 UTC"
  },
  {
    "arxiv_id": "2504.17539v1",
    "title": "Proof of Useful Intelligence (PoUI): Blockchain Consensus Beyond Energy Waste",
    "authors": [
      "Zan-Kai Chong",
      "Hiroyuki Ohsaki",
      "Bryan Ng"
    ],
    "abstract": "Blockchain technology enables secure, transparent data management in\ndecentralized systems, supporting applications from cryptocurrencies like\nBitcoin to tokenizing real-world assets like property. Its scalability and\nsustainability hinge on consensus mechanisms balancing security and efficiency.\nProof of Work (PoW), used by Bitcoin, ensures security through energy-intensive\ncomputations but demands significant resources. Proof of Stake (PoS), as in\nEthereum post-Merge, selects validators based on staked cryptocurrency,\noffering energy efficiency but risking centralization from wealth\nconcentration. With AI models straining computational resources, we propose\nProof of Useful Intelligence (PoUI), a hybrid consensus mechanism. In PoUI,\nworkers perform AI tasks like language processing or image analysis to earn\ncoins, which are staked to secure the network, blending security with practical\nutility. Decentralized nodes--job posters, market coordinators, workers, and\nvalidators --collaborate via smart contracts to manage tasks and rewards.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17539v1",
    "published_date": "2025-04-24 13:32:03 UTC",
    "updated_date": "2025-04-24 13:32:03 UTC"
  },
  {
    "arxiv_id": "2504.17534v1",
    "title": "Learning Isometric Embeddings of Road Networks using Multidimensional Scaling",
    "authors": [
      "Juan Carlos Climent Pardo"
    ],
    "abstract": "The lack of generalization in learning-based autonomous driving applications\nis shown by the narrow range of road scenarios that vehicles can currently\ncover. A generalizable approach should capture many distinct road structures\nand topologies, as well as consider traffic participants, and dynamic changes\nin the environment, so that vehicles can navigate and perform motion planning\ntasks even in the most difficult situations. Designing suitable feature spaces\nfor neural network-based motion planers that encapsulate all kinds of road\nscenarios is still an open research challenge. This paper tackles this\nlearning-based generalization challenge and shows how graph representations of\nroad networks can be leveraged by using multidimensional scaling (MDS)\ntechniques in order to obtain such feature spaces. State-of-the-art graph\nrepresentations and MDS approaches are analyzed for the autonomous driving use\ncase. Finally, the option of embedding graph nodes is discussed in order to\nperform easier learning procedures and obtain dimensionality reduction.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.ET",
      "cs.SC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17534v1",
    "published_date": "2025-04-24 13:20:32 UTC",
    "updated_date": "2025-04-24 13:20:32 UTC"
  },
  {
    "arxiv_id": "2504.17833v1",
    "title": "The Role of Open-Source LLMs in Shaping the Future of GeoAI",
    "authors": [
      "Xiao Huang",
      "Zhengzhong Tu",
      "Xinyue Ye",
      "Michael Goodchild"
    ],
    "abstract": "Large Language Models (LLMs) are transforming geospatial artificial\nintelligence (GeoAI), offering new capabilities in data processing, spatial\nanalysis, and decision support. This paper examines the open-source paradigm's\npivotal role in this transformation. While proprietary LLMs offer\naccessibility, they often limit the customization, interoperability, and\ntransparency vital for specialized geospatial tasks. Conversely, open-source\nalternatives significantly advance Geographic Information Science (GIScience)\nby fostering greater adaptability, reproducibility, and community-driven\ninnovation. Open frameworks empower researchers to tailor solutions, integrate\ncutting-edge methodologies (e.g., reinforcement learning, advanced spatial\nindexing), and align with FAIR principles. However, the growing reliance on any\nLLM necessitates careful consideration of security vulnerabilities, ethical\nrisks, and robust governance for AI-generated geospatial outputs. Ongoing\ndebates on accessibility, regulation, and misuse underscore the critical need\nfor responsible AI development strategies. This paper argues that GIScience\nadvances best not through a single model type, but by cultivating a diverse,\ninteroperable ecosystem combining open-source foundations for innovation,\nbespoke geospatial models, and interdisciplinary collaboration. By critically\nevaluating the opportunities and challenges of open-source LLMs within the\nbroader GeoAI landscape, this work contributes to a nuanced discourse on\nleveraging AI to effectively advance spatial research, policy, and\ndecision-making in an equitable, sustainable, and scientifically rigorous\nmanner.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17833v1",
    "published_date": "2025-04-24 13:20:17 UTC",
    "updated_date": "2025-04-24 13:20:17 UTC"
  },
  {
    "arxiv_id": "2504.17531v3",
    "title": "Towards Machine-Generated Code for the Resolution of User Intentions",
    "authors": [
      "Justus Flerlage",
      "Ilja Behnke",
      "Odej Kao"
    ],
    "abstract": "The growing capabilities of Artificial Intelligence (AI), particularly Large\nLanguage Models (LLMs), prompt a reassessment of the interaction mechanisms\nbetween users and their devices. Currently, users are required to use a set of\nhigh-level applications to achieve their desired results. However, the advent\nof AI may signal a shift in this regard, as its capabilities have generated\nnovel prospects for user-provided intent resolution through the deployment of\nmodel-generated code. This development represents a significant progression in\nthe realm of hybrid workflows, where human and artificial intelligence\ncollaborate to address user intentions, with the former responsible for\ndefining these intentions and the latter for implementing the solutions to\naddress them. In this paper, we investigate the feasibility of generating and\nexecuting workflows through code generation that results from prompting an LLM\nwith a concrete user intention, and a simplified application programming\ninterface for a GUI-less operating system. We provide an in-depth analysis and\ncomparison of various user intentions, the resulting code, and its execution.\nThe findings demonstrate the general feasibility of our approach and that the\nemployed LLM, GPT-4o-mini, exhibits remarkable proficiency in the generation of\ncode-oriented workflows in accordance with provided user intentions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17531v3",
    "published_date": "2025-04-24 13:19:17 UTC",
    "updated_date": "2025-05-22 10:57:51 UTC"
  },
  {
    "arxiv_id": "2504.17528v1",
    "title": "TACO: Tackling Over-correction in Federated Learning with Tailored Adaptive Correction",
    "authors": [
      "Weijie Liu",
      "Ziwei Zhan",
      "Carlee Joe-Wong",
      "Edith Ngai",
      "Jingpu Duan",
      "Deke Guo",
      "Xu Chen",
      "Xiaoxi Zhang"
    ],
    "abstract": "Non-independent and identically distributed (Non-IID) data across edge\nclients have long posed significant challenges to federated learning (FL)\ntraining in edge computing environments. Prior works have proposed various\nmethods to mitigate this statistical heterogeneity. While these works can\nachieve good theoretical performance, in this work we provide the first\ninvestigation into a hidden over-correction phenomenon brought by the uniform\nmodel correction coefficients across clients adopted by existing methods. Such\nover-correction could degrade model performance and even cause failures in\nmodel convergence. To address this, we propose TACO, a novel algorithm that\naddresses the non-IID nature of clients' data by implementing fine-grained,\nclient-specific gradient correction and model aggregation, steering local\nmodels towards a more accurate global optimum. Moreover, we verify that leading\nFL algorithms generally have better model accuracy in terms of communication\nrounds rather than wall-clock time, resulting from their extra computation\noverhead imposed on clients. To enhance the training efficiency, TACO deploys a\nlightweight model correction and tailored aggregation approach that requires\nminimum computation overhead and no extra information beyond the synchronized\nmodel parameters. To validate TACO's effectiveness, we present the first FL\nconvergence analysis that reveals the root cause of over-correction. Extensive\nexperiments across various datasets confirm TACO's superior and stable\nperformance in practice.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2.6"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, 7 figures, accepted by ICDCS 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.17528v1",
    "published_date": "2025-04-24 13:16:21 UTC",
    "updated_date": "2025-04-24 13:16:21 UTC"
  },
  {
    "arxiv_id": "2504.17497v2",
    "title": "Combining GCN Structural Learning with LLM Chemical Knowledge for Enhanced Virtual Screening",
    "authors": [
      "Radia Berreziga",
      "Mohammed Brahimi",
      "Khairedine Kraim",
      "Hamid Azzoune"
    ],
    "abstract": "Virtual screening plays a critical role in modern drug discovery by enabling\nthe identification of promising candidate molecules for experimental\nvalidation. Traditional machine learning methods such, as Support Vector\nMachines (SVM) and XGBoost, rely on predefined molecular representations, often\nleading to information loss and potential bias. In contrast, deep learning\napproaches-particularly Graph Convolutional Networks (GCNs)-offer a more\nexpressive and unbiased alternative by operating directly on molecular graphs.\nMeanwhile, Large Language Models (LLMs) have recently demonstrated\nstate-of-the-art performance in drug design, thanks to their capacity to\ncapture complex chemical patterns from large-scale data via attention\nmechanisms.\n  In this paper, we propose a hybrid architecture that integrates GCNs with\nLLM-derived embeddings to combine localized structural learning with global\nchemical knowledge. The LLM embeddings can be precomputed and stored in a\nmolecular feature library, removing the need to rerun the LLM during training\nor inference and thus maintaining computational efficiency. We found that\nconcatenating the LLM embeddings after each GCN layer-rather than only at the\nfinal layer-significantly improves performance, enabling deeper integration of\nglobal context throughout the network. The resulting model achieves superior\nresults, with an F1-score of (88.8\\%), outperforming standalone GCN (87.9%),\nXGBoost (85.5%), and SVM (85.4%) baselines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17497v2",
    "published_date": "2025-04-24 12:38:03 UTC",
    "updated_date": "2025-04-26 11:37:05 UTC"
  },
  {
    "arxiv_id": "2504.17493v1",
    "title": "Goal-Oriented Time-Series Forecasting: Foundation Framework Design",
    "authors": [
      "Luca-Andrei Fechete",
      "Mohamed Sana",
      "Fadhel Ayed",
      "Nicola Piovesan",
      "Wenjie Li",
      "Antonio De Domenico",
      "Tareq Si Salem"
    ],
    "abstract": "Traditional time-series forecasting often focuses only on minimizing\nprediction errors, ignoring the specific requirements of real-world\napplications that employ them. This paper presents a new training methodology,\nwhich allows a forecasting model to dynamically adjust its focus based on the\nimportance of forecast ranges specified by the end application. Unlike previous\nmethods that fix these ranges beforehand, our training approach breaks down\npredictions over the entire signal range into smaller segments, which are then\ndynamically weighted and combined to produce accurate forecasts. We tested our\nmethod on standard datasets, including a new dataset from wireless\ncommunication, and found that not only it improves prediction accuracy but also\nimproves the performance of end application employing the forecasting model.\nThis research provides a basis for creating forecasting systems that better\nconnect prediction and decision-making in various practical applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17493v1",
    "published_date": "2025-04-24 12:34:43 UTC",
    "updated_date": "2025-04-24 12:34:43 UTC"
  },
  {
    "arxiv_id": "2504.17490v1",
    "title": "Plasticine: Accelerating Research in Plasticity-Motivated Deep Reinforcement Learning",
    "authors": [
      "Mingqi Yuan",
      "Qi Wang",
      "Guozheng Ma",
      "Bo Li",
      "Xin Jin",
      "Yunbo Wang",
      "Xiaokang Yang",
      "Wenjun Zeng",
      "Dacheng Tao"
    ],
    "abstract": "Developing lifelong learning agents is crucial for artificial general\nintelligence. However, deep reinforcement learning (RL) systems often suffer\nfrom plasticity loss, where neural networks gradually lose their ability to\nadapt during training. Despite its significance, this field lacks unified\nbenchmarks and evaluation protocols. We introduce Plasticine, the first\nopen-source framework for benchmarking plasticity optimization in deep RL.\nPlasticine provides single-file implementations of over 13 mitigation methods,\n10 evaluation metrics, and learning scenarios with increasing non-stationarity\nlevels from standard to open-ended environments. This framework enables\nresearchers to systematically quantify plasticity loss, evaluate mitigation\nstrategies, and analyze plasticity dynamics across different contexts. Our\ndocumentation, examples, and source code are available at\nhttps://github.com/RLE-Foundation/Plasticine.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "23 pages",
    "pdf_url": "http://arxiv.org/pdf/2504.17490v1",
    "published_date": "2025-04-24 12:32:13 UTC",
    "updated_date": "2025-04-24 12:32:13 UTC"
  },
  {
    "arxiv_id": "2504.18594v1",
    "title": "A Simple DropConnect Approach to Transfer-based Targeted Attack",
    "authors": [
      "Tongrui Su",
      "Qingbin Li",
      "Shengyu Zhu",
      "Wei Chen",
      "Xueqi Cheng"
    ],
    "abstract": "We study the problem of transfer-based black-box attack, where adversarial\nsamples generated using a single surrogate model are directly applied to target\nmodels. Compared with untargeted attacks, existing methods still have lower\nAttack Success Rates (ASRs) in the targeted setting, i.e., the obtained\nadversarial examples often overfit the surrogate model but fail to mislead\nother models. In this paper, we hypothesize that the pixels or features in\nthese adversarial examples collaborate in a highly dependent manner to maximize\nthe success of an adversarial attack on the surrogate model, which we refer to\nas perturbation co-adaptation. Then, we propose to Mitigate perturbation\nCo-adaptation by DropConnect (MCD) to enhance transferability, by creating\ndiverse variants of surrogate model at each optimization iteration. We conduct\nextensive experiments across various CNN- and Transformer-based models to\ndemonstrate the effectiveness of MCD. In the challenging scenario of\ntransferring from a CNN-based model to Transformer-based models, MCD achieves\n13% higher average ASRs compared with state-of-the-art baselines. MCD boosts\nthe performance of self-ensemble methods by bringing in more diversification\nacross the variants while reserving sufficient semantic information for each\nvariant. In addition, MCD attains the highest performance gain when scaling the\ncompute of crafting adversarial examples.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.18594v1",
    "published_date": "2025-04-24 12:29:23 UTC",
    "updated_date": "2025-04-24 12:29:23 UTC"
  },
  {
    "arxiv_id": "2504.17474v1",
    "title": "Enhanced Sample Selection with Confidence Tracking: Identifying Correctly Labeled yet Hard-to-Learn Samples in Noisy Data",
    "authors": [
      "Weiran Pan",
      "Wei Wei",
      "Feida Zhu",
      "Yong Deng"
    ],
    "abstract": "We propose a novel sample selection method for image classification in the\npresence of noisy labels. Existing methods typically consider small-loss\nsamples as correctly labeled. However, some correctly labeled samples are\ninherently difficult for the model to learn and can exhibit high loss similar\nto mislabeled samples in the early stages of training. Consequently, setting a\nthreshold on per-sample loss to select correct labels results in a trade-off\nbetween precision and recall in sample selection: a lower threshold may miss\nmany correctly labeled hard-to-learn samples (low recall), while a higher\nthreshold may include many mislabeled samples (low precision). To address this\nissue, our goal is to accurately distinguish correctly labeled yet\nhard-to-learn samples from mislabeled ones, thus alleviating the trade-off\ndilemma. We achieve this by considering the trends in model prediction\nconfidence rather than relying solely on loss values. Empirical observations\nshow that only for correctly labeled samples, the model's prediction confidence\nfor the annotated labels typically increases faster than for any other classes.\nBased on this insight, we propose tracking the confidence gaps between the\nannotated labels and other classes during training and evaluating their trends\nusing the Mann-Kendall Test. A sample is considered potentially correctly\nlabeled if all its confidence gaps tend to increase. Our method functions as a\nplug-and-play component that can be seamlessly integrated into existing sample\nselection techniques. Experiments on several standard benchmarks and real-world\ndatasets demonstrate that our method enhances the performance of existing\nmethods for learning with noisy labels.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17474v1",
    "published_date": "2025-04-24 12:07:14 UTC",
    "updated_date": "2025-04-24 12:07:14 UTC"
  },
  {
    "arxiv_id": "2504.17471v1",
    "title": "GRANITE : a Byzantine-Resilient Dynamic Gossip Learning Framework",
    "authors": [
      "Yacine Belal",
      "Mohamed Maouche",
      "Sonia Ben Mokhtar",
      "Anthony Simonet-Boulogne"
    ],
    "abstract": "Gossip Learning (GL) is a decentralized learning paradigm where users\niteratively exchange and aggregate models with a small set of neighboring\npeers. Recent GL approaches rely on dynamic communication graphs built and\nmaintained using Random Peer Sampling (RPS) protocols. Thanks to graph\ndynamics, GL can achieve fast convergence even over extremely sparse\ntopologies. However, the robustness of GL over dy- namic graphs to Byzantine\n(model poisoning) attacks remains unaddressed especially when Byzantine nodes\nattack the RPS protocol to scale up model poisoning. We address this issue by\nintroducing GRANITE, a framework for robust learning over sparse, dynamic\ngraphs in the presence of a fraction of Byzantine nodes. GRANITE relies on two\nkey components (i) a History-aware Byzantine-resilient Peer Sampling protocol\n(HaPS), which tracks previously encountered identifiers to reduce adversarial\ninfluence over time, and (ii) an Adaptive Probabilistic Threshold (APT), which\nleverages an estimate of Byzantine presence to set aggregation thresholds with\nformal guarantees. Empirical results confirm that GRANITE maintains convergence\nwith up to 30% Byzantine nodes, improves learning speed via adaptive filtering\nof poisoned models and obtains these results in up to 9 times sparser graphs\nthan dictated by current theory.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17471v1",
    "published_date": "2025-04-24 12:03:15 UTC",
    "updated_date": "2025-04-24 12:03:15 UTC"
  },
  {
    "arxiv_id": "2504.17461v1",
    "title": "Evaluating Time Series Models for Urban Wastewater Management: Predictive Performance, Model Complexity and Resilience",
    "authors": [
      "Vipin Singh",
      "Tianheng Ling",
      "Teodor Chiaburu",
      "Felix Biessmann"
    ],
    "abstract": "Climate change increases the frequency of extreme rainfall, placing a\nsignificant strain on urban infrastructures, especially Combined Sewer Systems\n(CSS). Overflows from overburdened CSS release untreated wastewater into\nsurface waters, posing environmental and public health risks. Although\ntraditional physics-based models are effective, they are costly to maintain and\ndifficult to adapt to evolving system dynamics. Machine Learning (ML)\napproaches offer cost-efficient alternatives with greater adaptability. To\nsystematically assess the potential of ML for modeling urban infrastructure\nsystems, we propose a protocol for evaluating Neural Network architectures for\nCSS time series forecasting with respect to predictive performance, model\ncomplexity, and robustness to perturbations. In addition, we assess model\nperformance on peak events and critical fluctuations, as these are the key\nregimes for urban wastewater management. To investigate the feasibility of\nlightweight models suitable for IoT deployment, we compare global models, which\nhave access to all information, with local models, which rely solely on nearby\nsensor readings. Additionally, to explore the security risks posed by network\noutages or adversarial attacks on urban infrastructure, we introduce error\nmodels that assess the resilience of models. Our results demonstrate that while\nglobal models achieve higher predictive performance, local models provide\nsufficient resilience in decentralized scenarios, ensuring robust modeling of\nurban infrastructure. Furthermore, models with longer native forecast horizons\nexhibit greater robustness to data perturbations. These findings contribute to\nthe development of interpretable and reliable ML solutions for sustainable\nurban wastewater management. The implementation is available in our GitHub\nrepository.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages, 6 figures, accepted at 10th International Conference on\n  Smart and Sustainable Technologies (SpliTech) 2025, GitHub:\n  https://github.com/calgo-lab/resilient-timeseries-evaluation",
    "pdf_url": "http://arxiv.org/pdf/2504.17461v1",
    "published_date": "2025-04-24 11:52:13 UTC",
    "updated_date": "2025-04-24 11:52:13 UTC"
  },
  {
    "arxiv_id": "2504.17449v1",
    "title": "HMI: Hierarchical Knowledge Management for Efficient Multi-Tenant Inference in Pretrained Language Models",
    "authors": [
      "Jun Zhang",
      "Jue Wang",
      "Huan Li",
      "Lidan Shou",
      "Ke Chen",
      "Gang Chen",
      "Qin Xie",
      "Guiming Xie",
      "Xuejian Gong"
    ],
    "abstract": "The significant computational demands of pretrained language models (PLMs),\nwhich often require dedicated hardware, present a substantial challenge in\nserving them efficiently, especially in multi-tenant environments. To address\nthis, we introduce HMI, a Hierarchical knowledge management-based Multi-tenant\nInference system, designed to manage tenants with distinct PLMs\nresource-efficiently. Our approach is three-fold: Firstly, we categorize PLM\nknowledge into general, domain-specific, and task-specific. Leveraging insights\non knowledge acquisition across different model layers, we construct\nhierarchical PLMs (hPLMs) by extracting and storing knowledge at different\nlevels, significantly reducing GPU memory usage per tenant. Secondly, we\nestablish hierarchical knowledge management for hPLMs generated by various\ntenants in HMI. We manage domain-specific knowledge with acceptable storage\nincreases by constructing and updating domain-specific knowledge trees based on\nfrequency. We manage task-specific knowledge within limited GPU memory through\nparameter swapping. Finally, we propose system optimizations to enhance\nresource utilization and inference throughput. These include fine-grained\npipelining via hierarchical knowledge prefetching to overlap CPU and I/O\noperations with GPU computations, and optimizing parallel implementations with\nbatched matrix multiplications. Our experimental results demonstrate that the\nproposed HMI can efficiently serve up to 10,000 hPLMs (hBERTs and hGPTs) on a\nsingle GPU, with only a negligible compromise in accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by VLDBJ 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.17449v1",
    "published_date": "2025-04-24 11:28:40 UTC",
    "updated_date": "2025-04-24 11:28:40 UTC"
  },
  {
    "arxiv_id": "2504.17447v1",
    "title": "FRAG: Frame Selection Augmented Generation for Long Video and Long Document Understanding",
    "authors": [
      "De-An Huang",
      "Subhashree Radhakrishnan",
      "Zhiding Yu",
      "Jan Kautz"
    ],
    "abstract": "There has been impressive progress in Large Multimodal Models (LMMs). Recent\nworks extend these models to long inputs, including multi-page documents and\nlong videos. However, the model size and performance of these long context\nmodels are still limited due to the computational cost in both training and\ninference. In this work, we explore an orthogonal direction and process long\ninputs without long context LMMs. We propose Frame Selection Augmented\nGeneration (FRAG), where the model first selects relevant frames within the\ninput, and then only generates the final outputs based on the selected frames.\nThe core of the selection process is done by scoring each frame independently,\nwhich does not require long context processing. The frames with the highest\nscores are then selected by a simple Top-K selection. We show that this\nfrustratingly simple framework is applicable to both long videos and multi-page\ndocuments using existing LMMs without any fine-tuning. We consider two models,\nLLaVA-OneVision and InternVL2, in our experiments and show that FRAG\nconsistently improves the performance and achieves state-of-the-art\nperformances for both long video and long document understanding. For videos,\nFRAG substantially improves InternVL2-76B by 5.8% on MLVU and 3.7% on\nVideo-MME. For documents, FRAG achieves over 20% improvements on MP-DocVQA\ncompared with recent LMMs specialized in long document understanding. Code is\navailable at: https://github.com/NVlabs/FRAG",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17447v1",
    "published_date": "2025-04-24 11:19:18 UTC",
    "updated_date": "2025-04-24 11:19:18 UTC"
  },
  {
    "arxiv_id": "2504.17428v1",
    "title": "Detection, Classification and Prevalence of Self-Admitted Aging Debt",
    "authors": [
      "Murali Sridharan",
      "Mika Mäntylä",
      "Leevi Rantala"
    ],
    "abstract": "Context: Previous research on software aging is limited with focus on dynamic\nruntime indicators like memory and performance, often neglecting evolutionary\nindicators like source code comments and narrowly examining legacy issues\nwithin the TD context. Objective: We introduce the concept of Aging Debt (AD),\nrepresenting the increased maintenance efforts and costs needed to keep\nsoftware updated. We study AD through Self-Admitted Aging Debt (SAAD) observed\nin source code comments left by software developers. Method: We employ a\nmixed-methods approach, combining qualitative and quantitative analyses to\ndetect and measure AD in software. This includes framing SAAD patterns from the\nsource code comments after analysing the source code context, then utilizing\nthe SAAD patterns to detect SAAD comments. In the process, we develop a\ntaxonomy for SAAD that reflects the temporal aging of software and its\nassociated debt. Then we utilize the taxonomy to quantify the different types\nof AD prevalent in OSS repositories. Results: Our proposed taxonomy categorizes\ntemporal software aging into Active and Dormant types. Our extensive analysis\nof over 9,000+ Open Source Software (OSS) repositories reveals that more than\n21% repositories exhibit signs of SAAD as observed from our gold standard SAAD\ndataset. Notably, Dormant AD emerges as the predominant category, highlighting\na critical but often overlooked aspect of software maintenance. Conclusion: As\nsoftware volume grows annually, so do evolutionary aging and maintenance\nchallenges; our proposed taxonomy can aid researchers in detailed software\naging studies and help practitioners develop improved and proactive maintenance\nstrategies.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CE",
      "cs.GL",
      "D.2.7; D.2.9"
    ],
    "primary_category": "cs.SE",
    "comment": "Draft",
    "pdf_url": "http://arxiv.org/pdf/2504.17428v1",
    "published_date": "2025-04-24 10:38:55 UTC",
    "updated_date": "2025-04-24 10:38:55 UTC"
  },
  {
    "arxiv_id": "2504.17426v1",
    "title": "Towards Leveraging Large Language Model Summaries for Topic Modeling in Source Code",
    "authors": [
      "Michele Carissimi",
      "Martina Saletta",
      "Claudio Ferretti"
    ],
    "abstract": "Understanding source code is a topic of great interest in the software\nengineering community, since it can help programmers in various tasks such as\nsoftware maintenance and reuse. Recent advances in large language models (LLMs)\nhave demonstrated remarkable program comprehension capabilities, while\ntransformer-based topic modeling techniques offer effective ways to extract\nsemantic information from text. This paper proposes and explores a novel\napproach that combines these strengths to automatically identify meaningful\ntopics in a corpus of Python programs. Our method consists in applying topic\nmodeling on the descriptions obtained by asking an LLM to summarize the code.\nTo assess the internal consistency of the extracted topics, we compare them\nagainst topics inferred from function names alone, and those derived from\nexisting docstrings. Experimental results suggest that leveraging LLM-generated\nsummaries provides interpretable and semantically rich representation of code\nstructure. The promising results suggest that our approach can be fruitfully\napplied in various software engineering tasks such as automatic documentation\nand tagging, code search, software reorganization and knowledge discovery in\nlarge repositories.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17426v1",
    "published_date": "2025-04-24 10:30:40 UTC",
    "updated_date": "2025-04-24 10:30:40 UTC"
  },
  {
    "arxiv_id": "2504.17424v1",
    "title": "Object Pose Estimation by Camera Arm Control Based on the Next Viewpoint Estimation",
    "authors": [
      "Tomoki Mizuno",
      "Kazuya Yabashi",
      "Tsuyoshi Tasaki"
    ],
    "abstract": "We have developed a new method to estimate a Next Viewpoint (NV) which is\neffective for pose estimation of simple-shaped products for product display\nrobots in retail stores. Pose estimation methods using Neural Networks (NN)\nbased on an RGBD camera are highly accurate, but their accuracy significantly\ndecreases when the camera acquires few texture and shape features at a current\nview point. However, it is difficult for previous mathematical model-based\nmethods to estimate effective NV which is because the simple shaped objects\nhave few shape features. Therefore, we focus on the relationship between the\npose estimation and NV estimation. When the pose estimation is more accurate,\nthe NV estimation is more accurate. Therefore, we develop a new pose estimation\nNN that estimates NV simultaneously. Experimental results showed that our NV\nestimation realized a pose estimation success rate 77.3\\%, which was 7.4pt\nhigher than the mathematical model-based NV calculation did. Moreover, we\nverified that the robot using our method displayed 84.2\\% of products.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17424v1",
    "published_date": "2025-04-24 10:26:14 UTC",
    "updated_date": "2025-04-24 10:26:14 UTC"
  },
  {
    "arxiv_id": "2504.17421v1",
    "title": "Towards Harnessing the Collaborative Power of Large and Small Models for Domain Tasks",
    "authors": [
      "Yang Liu",
      "Bingjie Yan",
      "Tianyuan Zou",
      "Jianqing Zhang",
      "Zixuan Gu",
      "Jianbing Ding",
      "Xidong Wang",
      "Jingyi Li",
      "Xiaozhou Ye",
      "Ye Ouyang",
      "Qiang Yang",
      "Ya-Qin Zhang"
    ],
    "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities, but\nthey require vast amounts of data and computational resources. In contrast,\nsmaller models (SMs), while less powerful, can be more efficient and tailored\nto specific domains. In this position paper, we argue that taking a\ncollaborative approach, where large and small models work synergistically, can\naccelerate the adaptation of LLMs to private domains and unlock new potential\nin AI. We explore various strategies for model collaboration and identify\npotential challenges and opportunities. Building upon this, we advocate for\nindustry-driven research that prioritizes multi-objective benchmarks on\nreal-world private datasets and applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17421v1",
    "published_date": "2025-04-24 10:24:35 UTC",
    "updated_date": "2025-04-24 10:24:35 UTC"
  },
  {
    "arxiv_id": "2504.17404v2",
    "title": "Redefining Superalignment: From Weak-to-Strong Alignment to Human-AI Co-Alignment to Sustainable Symbiotic Society",
    "authors": [
      "Yi Zeng",
      "Feifei Zhao",
      "Yuwei Wang",
      "Enmeng Lu",
      "Yaodong Yang",
      "Lei Wang",
      "Chao Liu",
      "Yitao Liang",
      "Dongcheng Zhao",
      "Bing Han",
      "Haibo Tong",
      "Yao Liang",
      "Dongqi Liang",
      "Kang Sun",
      "Boyuan Chen",
      "Jinyu Fan"
    ],
    "abstract": "Artificial Intelligence (AI) systems are becoming increasingly powerful and\nautonomous, and may progress to surpass human intelligence levels, namely\nArtificial Superintelligence (ASI). During the progression from AI to ASI, it\nmay exceed human control, violate human values, and even lead to irreversible\ncatastrophic consequences in extreme cases. This gives rise to a pressing issue\nthat needs to be addressed: superalignment, ensuring that AI systems much\nsmarter than humans, remain aligned with human (compatible) intentions and\nvalues. Existing scalable oversight and weak-to-strong generalization methods\nmay prove substantially infeasible and inadequate when facing ASI. We must\nexplore safer and more pluralistic frameworks and approaches for\nsuperalignment. In this paper, we redefine superalignment as the human-AI\nco-alignment towards a sustainable symbiotic society, and highlight a framework\nthat integrates external oversight and intrinsic proactive alignment. External\noversight superalignment should be grounded in human-centered ultimate\ndecision, supplemented by interpretable automated evaluation and correction, to\nachieve continuous alignment with humanity's evolving values. Intrinsic\nproactive superalignment is rooted in a profound understanding of the Self,\nothers, and society, integrating self-awareness, self-reflection, and empathy\nto spontaneously infer human intentions, distinguishing good from evil and\nproactively considering human well-being, ultimately attaining human-AI\nco-alignment through iterative interaction. The integration of\nexternally-driven oversight with intrinsically-driven proactive alignment\nempowers sustainable symbiotic societies through human-AI co-alignment, paving\nthe way for achieving safe and beneficial AGI and ASI for good, for human, and\nfor a symbiotic ecology.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17404v2",
    "published_date": "2025-04-24 09:53:49 UTC",
    "updated_date": "2025-04-25 15:32:41 UTC"
  },
  {
    "arxiv_id": "2504.17402v1",
    "title": "Assessing the Capability of Large Language Models for Domain-Specific Ontology Generation",
    "authors": [
      "Anna Sofia Lippolis",
      "Mohammad Javad Saeedizade",
      "Robin Keskisarkka",
      "Aldo Gangemi",
      "Eva Blomqvist",
      "Andrea Giovanni Nuzzolese"
    ],
    "abstract": "Large Language Models (LLMs) have shown significant potential for ontology\nengineering. However, it is still unclear to what extent they are applicable to\nthe task of domain-specific ontology generation. In this study, we explore the\napplication of LLMs for automated ontology generation and evaluate their\nperformance across different domains. Specifically, we investigate the\ngeneralizability of two state-of-the-art LLMs, DeepSeek and o1-preview, both\nequipped with reasoning capabilities, by generating ontologies from a set of\ncompetency questions (CQs) and related user stories. Our experimental setup\ncomprises six distinct domains carried out in existing ontology engineering\nprojects and a total of 95 curated CQs designed to test the models' reasoning\nfor ontology engineering. Our findings show that with both LLMs, the\nperformance of the experiments is remarkably consistent across all domains,\nindicating that these methods are capable of generalizing ontology generation\ntasks irrespective of the domain. These results highlight the potential of\nLLM-based approaches in achieving scalable and domain-agnostic ontology\nconstruction and lay the groundwork for further research into enhancing\nautomated reasoning and knowledge representation techniques.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17402v1",
    "published_date": "2025-04-24 09:47:14 UTC",
    "updated_date": "2025-04-24 09:47:14 UTC"
  },
  {
    "arxiv_id": "2504.17401v1",
    "title": "StereoMamba: Real-time and Robust Intraoperative Stereo Disparity Estimation via Long-range Spatial Dependencies",
    "authors": [
      "Xu Wang",
      "Jialang Xu",
      "Shuai Zhang",
      "Baoru Huang",
      "Danail Stoyanov",
      "Evangelos B. Mazomenos"
    ],
    "abstract": "Stereo disparity estimation is crucial for obtaining depth information in\nrobot-assisted minimally invasive surgery (RAMIS). While current deep learning\nmethods have made significant advancements, challenges remain in achieving an\noptimal balance between accuracy, robustness, and inference speed. To address\nthese challenges, we propose the StereoMamba architecture, which is\nspecifically designed for stereo disparity estimation in RAMIS. Our approach is\nbased on a novel Feature Extraction Mamba (FE-Mamba) module, which enhances\nlong-range spatial dependencies both within and across stereo images. To\neffectively integrate multi-scale features from FE-Mamba, we then introduce a\nnovel Multidimensional Feature Fusion (MFF) module. Experiments against the\nstate-of-the-art on the ex-vivo SCARED benchmark demonstrate that StereoMamba\nachieves superior performance on EPE of 2.64 px and depth MAE of 2.55 mm, the\nsecond-best performance on Bad2 of 41.49% and Bad3 of 26.99%, while maintaining\nan inference speed of 21.28 FPS for a pair of high-resolution images\n(1280*1024), striking the optimum balance between accuracy, robustness, and\nefficiency. Furthermore, by comparing synthesized right images, generated from\nwarping left images using the generated disparity maps, with the actual right\nimage, StereoMamba achieves the best average SSIM (0.8970) and PSNR (16.0761),\nexhibiting strong zero-shot generalization on the in-vivo RIS2017 and StereoMIS\ndatasets.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17401v1",
    "published_date": "2025-04-24 09:46:15 UTC",
    "updated_date": "2025-04-24 09:46:15 UTC"
  },
  {
    "arxiv_id": "2504.18593v1",
    "title": "Severity Classification of Chronic Obstructive Pulmonary Disease in Intensive Care Units: A Semi-Supervised Approach Using MIMIC-III Dataset",
    "authors": [
      "Akram Shojaei",
      "Mehdi Delrobaei"
    ],
    "abstract": "Chronic obstructive pulmonary disease (COPD) represents a significant global\nhealth burden, where precise severity assessment is particularly critical for\neffective clinical management in intensive care unit (ICU) settings. This study\nintroduces an innovative machine learning framework for COPD severity\nclassification utilizing the MIMIC-III critical care database, thereby\nexpanding the applications of artificial intelligence in critical care\nmedicine. Our research developed a robust classification model incorporating\nkey ICU parameters such as blood gas measurements and vital signs, while\nimplementing semi-supervised learning techniques to effectively utilize\nunlabeled data and enhance model performance. The random forest classifier\nemerged as particularly effective, demonstrating exceptional discriminative\ncapability with 92.51% accuracy and 0.98 ROC AUC in differentiating between\nmild-to-moderate and severe COPD cases. This machine learning approach provides\nclinicians with a practical, accurate, and efficient tool for rapid COPD\nseverity evaluation in ICU environments, with significant potential to improve\nboth clinical decision-making processes and patient outcomes. Future research\ndirections should prioritize external validation across diverse patient\npopulations and integration with clinical decision support systems to optimize\nCOPD management in critical care settings.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.18593v1",
    "published_date": "2025-04-24 09:37:52 UTC",
    "updated_date": "2025-04-24 09:37:52 UTC"
  },
  {
    "arxiv_id": "2504.17393v2",
    "title": "Towards User-Centred Design of AI-Assisted Decision-Making in Law Enforcement",
    "authors": [
      "Vesna Nowack",
      "Dalal Alrajeh",
      "Carolina Gutierrez Muñoz",
      "Katie Thomas",
      "William Hobson",
      "Patrick Benjamin",
      "Catherine Hamilton-Giachritsis",
      "Tim Grant",
      "Juliane A. Kloess",
      "Jessica Woodhams"
    ],
    "abstract": "Artificial Intelligence (AI) has become an important part of our everyday\nlives, yet user requirements for designing AI-assisted systems in law\nenforcement remain unclear. To address this gap, we conducted qualitative\nresearch on decision-making within a law enforcement agency. Our study aimed to\nidentify limitations of existing practices, explore user requirements and\nunderstand the responsibilities that humans expect to undertake in these\nsystems.\n  Participants in our study highlighted the need for a system capable of\nprocessing and analysing large volumes of data efficiently to help in crime\ndetection and prevention. Additionally, the system should satisfy requirements\nfor scalability, accuracy, justification, trustworthiness and adaptability to\nbe adopted in this domain. Participants also emphasised the importance of\nhaving end users review the input data that might be challenging for AI to\ninterpret, and validate the generated output to ensure the system's accuracy.\nTo keep up with the evolving nature of the law enforcement domain, end users\nneed to help the system adapt to the changes in criminal behaviour and\ngovernment guidance, and technical experts need to regularly oversee and\nmonitor the system. Furthermore, user-friendly human interaction with the\nsystem is essential for its adoption and some of the participants confirmed\nthey would be happy to be in the loop and provide necessary feedback that the\nsystem can learn from. Finally, we argue that it is very unlikely that the\nsystem will ever achieve full automation due to the dynamic and complex nature\nof the law enforcement domain.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "10 pages",
    "pdf_url": "http://arxiv.org/pdf/2504.17393v2",
    "published_date": "2025-04-24 09:25:29 UTC",
    "updated_date": "2025-05-07 09:34:46 UTC"
  },
  {
    "arxiv_id": "2504.17384v2",
    "title": "On the workflow, opportunities and challenges of developing foundation model in geophysics",
    "authors": [
      "Hanlin Sheng",
      "Xinming Wu",
      "Hang Gao",
      "Haibin Di",
      "Sergey Fomel",
      "Jintao Li",
      "Xu Si"
    ],
    "abstract": "Foundation models, as a mainstream technology in artificial intelligence,\nhave demonstrated immense potential across various domains in recent years,\nparticularly in handling complex tasks and multimodal data. In the field of\ngeophysics, although the application of foundation models is gradually\nexpanding, there is currently a lack of comprehensive reviews discussing the\nfull workflow of integrating foundation models with geophysical data. To\naddress this gap, this paper presents a complete framework that systematically\nexplores the entire process of developing foundation models in conjunction with\ngeophysical data. From data collection and preprocessing to model architecture\nselection, pre-training strategies, and model deployment, we provide a detailed\nanalysis of the key techniques and methodologies at each stage. In particular,\nconsidering the diversity, complexity, and physical consistency constraints of\ngeophysical data, we discuss targeted solutions to address these challenges.\nFurthermore, we discuss how to leverage the transfer learning capabilities of\nfoundation models to reduce reliance on labeled data, enhance computational\nefficiency, and incorporate physical constraints into model training, thereby\nimproving physical consistency and interpretability. Through a comprehensive\nsummary and analysis of the current technological landscape, this paper not\nonly fills the gap in the geophysics domain regarding a full-process review of\nfoundation models but also offers valuable practical guidance for their\napplication in geophysical data analysis, driving innovation and advancement in\nthe field.",
    "categories": [
      "physics.geo-ph",
      "cs.AI"
    ],
    "primary_category": "physics.geo-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17384v2",
    "published_date": "2025-04-24 09:08:24 UTC",
    "updated_date": "2025-04-25 07:35:21 UTC"
  },
  {
    "arxiv_id": "2504.17829v1",
    "title": "Fine-Tuning Adversarially-Robust Transformers for Single-Image Dehazing",
    "authors": [
      "Vlad Vasilescu",
      "Ana Neacsu",
      "Daniela Faur"
    ],
    "abstract": "Single-image dehazing is an important topic in remote sensing applications,\nenhancing the quality of acquired images and increasing object detection\nprecision. However, the reliability of such structures has not been\nsufficiently analyzed, which poses them to the risk of imperceptible\nperturbations that can significantly hinder their performance. In this work, we\nshow that state-of-the-art image-to-image dehazing transformers are susceptible\nto adversarial noise, with even 1 pixel change being able to decrease the PSNR\nby as much as 2.8 dB. Next, we propose two lightweight fine-tuning strategies\naimed at increasing the robustness of pre-trained transformers. Our methods\nresults in comparable clean performance, while significantly increasing the\nprotection against adversarial data. We further present their applicability in\ntwo remote sensing scenarios, showcasing their robust behavior for\nout-of-distribution data. The source code for adversarial fine-tuning and\nattack algorithms can be found at github.com/Vladimirescu/RobustDehazing.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17829v1",
    "published_date": "2025-04-24 08:52:14 UTC",
    "updated_date": "2025-04-24 08:52:14 UTC"
  },
  {
    "arxiv_id": "2504.18591v1",
    "title": "Geometry aware inference of steady state PDEs using Equivariant Neural Fields representations",
    "authors": [
      "Giovanni Catalani",
      "Michael Bauerheim",
      "Frédéric Tost",
      "Xavier Bertrand",
      "Joseph Morlier"
    ],
    "abstract": "Recent advances in Neural Fields have enabled powerful,\ndiscretization-invariant methods for learning neural operators that approximate\nsolutions of Partial Differential Equations (PDEs) on general geometries.\nBuilding on these developments, we introduce enf2enf, an encoder--decoder\nmethodology for predicting steady-state Partial Differential Equations with\nnon-parameterized geometric variability, based on recently proposed Equivariant\nNeural Field architectures. In enf2enf, input geometries are encoded into\nlatent point cloud embeddings that inherently preserve geometric grounding and\ncapture local phenomena. The resulting representations are then combined with\nglobal parameters and directly decoded into continuous output fields, thus\nefficiently modeling the coupling between geometry and physics. By leveraging\nthe inductive biases of locality and translation invariance, our approach is\nable to capture fine-scale physical features as well as complex shape\nvariations, thereby enhancing generalization and physical compliance. Extensive\nexperiments on a high-fidelity aerodynamic dataset, a hyper-elastic material\nbenchmark, and multi-element airfoil geometries, demonstrate that the proposed\nmodel achieves superior or competitive performance compared to state-of-the-art\ngraph based, operator learning, and neural field methods. Notably, our method\nsupports real time inference and zero-shot super-resolution, enabling efficient\ntraining on low-resolution meshes while maintaining high accuracy on full-scale\ndiscretizations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.18591v1",
    "published_date": "2025-04-24 08:30:32 UTC",
    "updated_date": "2025-04-24 08:30:32 UTC"
  },
  {
    "arxiv_id": "2504.17366v1",
    "title": "LiveLongBench: Tackling Long-Context Understanding for Spoken Texts from Live Streams",
    "authors": [
      "Yongxuan Wu",
      "Runyu Chen",
      "Peiyu Liu",
      "Hongjin Qian"
    ],
    "abstract": "Long-context understanding poses significant challenges in natural language\nprocessing, particularly for real-world dialogues characterized by speech-based\nelements, high redundancy, and uneven information density. Although large\nlanguage models (LLMs) achieve impressive results on existing benchmarks, these\ndatasets fail to reflect the complexities of such texts, limiting their\napplicability to practical scenarios. To bridge this gap, we construct the\nfirst spoken long-text dataset, derived from live streams, designed to reflect\nthe redundancy-rich and conversational nature of real-world scenarios. We\nconstruct tasks in three categories: retrieval-dependent, reasoning-dependent,\nand hybrid. We then evaluate both popular LLMs and specialized methods to\nassess their ability to understand long-contexts in these tasks. Our results\nshow that current methods exhibit strong task-specific preferences and perform\npoorly on highly redundant inputs, with no single method consistently\noutperforming others. We propose a new baseline that better handles redundancy\nin spoken text and achieves strong performance across tasks. Our findings\nhighlight key limitations of current methods and suggest future directions for\nimproving long-context understanding. Finally, our benchmark fills a gap in\nevaluating long-context spoken language understanding and provides a practical\nfoundation for developing real-world e-commerce systems. The code and benchmark\nare available at https://github.com/Yarayx/livelongbench.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17366v1",
    "published_date": "2025-04-24 08:27:48 UTC",
    "updated_date": "2025-04-24 08:27:48 UTC"
  },
  {
    "arxiv_id": "2504.18590v1",
    "title": "A multilevel approach to accelerate the training of Transformers",
    "authors": [
      "Guillaume Lauga",
      "Maël Chaumette",
      "Edgar Desainte-Maréville",
      "Étienne Lasalle",
      "Arthur Lebeurrier"
    ],
    "abstract": "In this article, we investigate the potential of multilevel approaches to\naccelerate the training of transformer architectures. Using an ordinary\ndifferential equation (ODE) interpretation of these architectures, we propose\nan appropriate way of varying the discretization of these ODE Transformers in\norder to accelerate the training. We validate our approach experimentally by a\ncomparison with the standard training procedure.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.18590v1",
    "published_date": "2025-04-24 08:23:50 UTC",
    "updated_date": "2025-04-24 08:23:50 UTC"
  },
  {
    "arxiv_id": "2504.17356v1",
    "title": "Comprehend, Divide, and Conquer: Feature Subspace Exploration via Multi-Agent Hierarchical Reinforcement Learning",
    "authors": [
      "Weiliang Zhang",
      "Xiaohan Huang",
      "Yi Du",
      "Ziyue Qiao",
      "Qingqing Long",
      "Zhen Meng",
      "Yuanchun Zhou",
      "Meng Xiao"
    ],
    "abstract": "Feature selection aims to preprocess the target dataset, find an optimal and\nmost streamlined feature subset, and enhance the downstream machine learning\ntask. Among filter, wrapper, and embedded-based approaches, the reinforcement\nlearning (RL)-based subspace exploration strategy provides a novel objective\noptimization-directed perspective and promising performance. Nevertheless, even\nwith improved performance, current reinforcement learning approaches face\nchallenges similar to conventional methods when dealing with complex datasets.\nThese challenges stem from the inefficient paradigm of using one agent per\nfeature and the inherent complexities present in the datasets. This observation\nmotivates us to investigate and address the above issue and propose a novel\napproach, namely HRLFS. Our methodology initially employs a Large Language\nModel (LLM)-based hybrid state extractor to capture each feature's mathematical\nand semantic characteristics. Based on this information, features are\nclustered, facilitating the construction of hierarchical agents for each\ncluster and sub-cluster. Extensive experiments demonstrate the efficiency,\nscalability, and robustness of our approach. Compared to contemporary or the\none-feature-one-agent RL-based approaches, HRLFS improves the downstream ML\nperformance with iterative feature subspace exploration while accelerating\ntotal run time by reducing the number of agents involved.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "20 pages, keywords: Automated Feature Engineering, Tabular Dataset,\n  Multi-Agent Reinforcement Learning, Feature Selection",
    "pdf_url": "http://arxiv.org/pdf/2504.17356v1",
    "published_date": "2025-04-24 08:16:36 UTC",
    "updated_date": "2025-04-24 08:16:36 UTC"
  },
  {
    "arxiv_id": "2504.17355v1",
    "title": "Collaborative Multi-Agent Reinforcement Learning for Automated Feature Transformation with Graph-Driven Path Optimization",
    "authors": [
      "Xiaohan Huang",
      "Dongjie Wang",
      "Zhiyuan Ning",
      "Ziyue Qiao",
      "Qingqing Long",
      "Haowei Zhu",
      "Yi Du",
      "Min Wu",
      "Yuanchun Zhou",
      "Meng Xiao"
    ],
    "abstract": "Feature transformation methods aim to find an optimal mathematical\nfeature-feature crossing process that generates high-value features and\nimproves the performance of downstream machine learning tasks. Existing\nframeworks, though designed to mitigate manual costs, often treat feature\ntransformations as isolated operations, ignoring dynamic dependencies between\ntransformation steps. To address the limitations, we propose TCTO, a\ncollaborative multi-agent reinforcement learning framework that automates\nfeature engineering through graph-driven path optimization. The framework's\ncore innovation lies in an evolving interaction graph that models features as\nnodes and transformations as edges. Through graph pruning and backtracking, it\ndynamically eliminates low-impact edges, reduces redundant operations, and\nenhances exploration stability. This graph also provides full traceability to\nempower TCTO to reuse high-utility subgraphs from historical transformations.\nTo demonstrate the efficacy and adaptability of our approach, we conduct\ncomprehensive experiments and case studies, which show superior performance\nacross a range of datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "13 pages, Keywords: Automated Feature Transformation, Tabular\n  Dataset, Reinforcement Learning",
    "pdf_url": "http://arxiv.org/pdf/2504.17355v1",
    "published_date": "2025-04-24 08:16:13 UTC",
    "updated_date": "2025-04-24 08:16:13 UTC"
  },
  {
    "arxiv_id": "2504.17354v1",
    "title": "Data-Driven Surrogate Modeling Techniques to Predict the Effective Contact Area of Rough Surface Contact Problems",
    "authors": [
      "Tarik Sahin",
      "Jacopo Bonari",
      "Sebastian Brandstaeter",
      "Alexander Popp"
    ],
    "abstract": "The effective contact area in rough surface contact plays a critical role in\nmulti-physics phenomena such as wear, sealing, and thermal or electrical\nconduction. Although accurate numerical methods, like the Boundary Element\nMethod (BEM), are available to compute this quantity, their high computational\ncost limits their applicability in multi-query contexts, such as uncertainty\nquantification, parameter identification, and multi-scale algorithms, where\nmany repeated evaluations are required. This study proposes a surrogate\nmodeling framework for predicting the effective contact area using\nfast-to-evaluate data-driven techniques. Various machine learning algorithms\nare trained on a precomputed dataset, where the inputs are the imposed load and\nstatistical roughness parameters, and the output is the corresponding effective\ncontact area. All models undergo hyperparameter optimization to enable fair\ncomparisons in terms of predictive accuracy and computational efficiency,\nevaluated using established quantitative metrics. Among the models, the Kernel\nRidge Regressor demonstrates the best trade-off between accuracy and\nefficiency, achieving high predictive accuracy, low prediction time, and\nminimal training overhead-making it a strong candidate for general-purpose\nsurrogate modeling. The Gaussian Process Regressor provides an attractive\nalternative when uncertainty quantification is required, although it incurs\nadditional computational cost due to variance estimation. The generalization\ncapability of the Kernel Ridge model is validated on an unseen simulation\nscenario, confirming its ability to transfer to new configurations. Database\ngeneration constitutes the dominant cost in the surrogate modeling process.\nNevertheless, the approach proves practical and efficient for multi-query\ntasks, even when accounting for this initial expense.",
    "categories": [
      "cs.CE",
      "cs.AI"
    ],
    "primary_category": "cs.CE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17354v1",
    "published_date": "2025-04-24 08:15:46 UTC",
    "updated_date": "2025-04-24 08:15:46 UTC"
  },
  {
    "arxiv_id": "2504.17346v1",
    "title": "Dual-Individual Genetic Algorithm: A Dual-Individual Approach for Efficient Training of Multi-Layer Neural Networks",
    "authors": [
      "Tran Thuy Nga Truong",
      "Jooyong Kim"
    ],
    "abstract": "This paper introduces an enhanced Genetic Algorithm technique called\nDual-Individual Genetic Algorithm (Dual-Individual GA), which optimizes neural\nnetworks for binary image classification tasks, such as cat vs. non-cat\nclassification. The proposed method employs only two individuals for crossover,\nrepresented by two parameter sets: Leader and Follower. The Leader focuses on\nexploitation, representing the primary optimal solution at even-indexed\npositions (0, 2, 4, ...), while the Follower promotes exploration by preserving\ndiversity and avoiding premature convergence, operating at odd-indexed\npositions (1, 3, 5, ...). Leader and Follower are modeled as two phases or\nroles. The key contributions of this work are threefold: (1) a self-adaptive\nlayer dimension mechanism that eliminates the need for manual tuning of layer\narchitectures; (2) generates two parameter sets, leader and follower parameter\nsets, with 10 layer architecture configurations (5 for each set), ranked by\nPareto dominance and cost. post-optimization; and (3) demonstrated superior\nperformance compared to traditional gradient-based methods. Experimental\nresults show that the Dual-Individual GA achieves 99.04% training accuracy and\n80% testing accuracy (cost = 0.034) on a three-layer network with architecture\n[12288, 17, 4, 1], outperforming a gradient-based approach that achieves 98%\ntraining accuracy and 80% testing accuracy (cost = 0.092) on a four-layer\nnetwork with architecture [12288, 20, 7, 5, 1]. These findings highlight the\nefficiency and effectiveness of the proposed method in optimizing neural\nnetworks.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17346v1",
    "published_date": "2025-04-24 08:04:08 UTC",
    "updated_date": "2025-04-24 08:04:08 UTC"
  },
  {
    "arxiv_id": "2504.17331v1",
    "title": "Exploring Context-aware and LLM-driven Locomotion for Immersive Virtual Reality",
    "authors": [
      "Süleyman Özdel",
      "Kadir Burak Buldu",
      "Enkelejda Kasneci",
      "Efe Bozkir"
    ],
    "abstract": "Locomotion plays a crucial role in shaping the user experience within virtual\nreality environments. In particular, hands-free locomotion offers a valuable\nalternative by supporting accessibility and freeing users from reliance on\nhandheld controllers. To this end, traditional speech-based methods often\ndepend on rigid command sets, limiting the naturalness and flexibility of\ninteraction. In this study, we propose a novel locomotion technique powered by\nlarge language models (LLMs), which allows users to navigate virtual\nenvironments using natural language with contextual awareness. We evaluate\nthree locomotion methods: controller-based teleportation, voice-based steering,\nand our language model-driven approach. Our evaluation measures include\neye-tracking data analysis, including explainable machine learning through SHAP\nanalysis as well as standardized questionnaires for usability, presence,\ncybersickness, and cognitive load to examine user attention and engagement. Our\nfindings indicate that the LLM-driven locomotion possesses comparable\nusability, presence, and cybersickness scores to established methods like\nteleportation, demonstrating its novel potential as a comfortable, natural\nlanguage-based, hands-free alternative. In addition, it enhances user attention\nwithin the virtual environment, suggesting greater engagement. Complementary to\nthese findings, SHAP analysis revealed that fixation, saccade, and\npupil-related features vary across techniques, indicating distinct patterns of\nvisual attention and cognitive processing. Overall, we state that our method\ncan facilitate hands-free locomotion in virtual spaces, especially in\nsupporting accessibility.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "This work has been submitted to the IEEE for possible publication",
    "pdf_url": "http://arxiv.org/pdf/2504.17331v1",
    "published_date": "2025-04-24 07:48:09 UTC",
    "updated_date": "2025-04-24 07:48:09 UTC"
  },
  {
    "arxiv_id": "2504.17315v1",
    "title": "DIMT25@ICDAR2025: HW-TSC's End-to-End Document Image Machine Translation System Leveraging Large Vision-Language Model",
    "authors": [
      "Zhanglin Wu",
      "Tengfei Song",
      "Ning Xie",
      "Weidong Zhang",
      "Pengfei Li",
      "Shuang Wu",
      "Chong Li",
      "Junhao Zhu",
      "Hao Yang"
    ],
    "abstract": "This paper presents the technical solution proposed by Huawei Translation\nService Center (HW-TSC) for the \"End-to-End Document Image Machine Translation\nfor Complex Layouts\" competition at the 19th International Conference on\nDocument Analysis and Recognition (DIMT25@ICDAR2025). Leveraging\nstate-of-the-art open-source large vision-language model (LVLM), we introduce a\ntraining framework that combines multi-task learning with perceptual\nchain-of-thought to develop a comprehensive end-to-end document translation\nsystem. During the inference phase, we apply minimum Bayesian decoding and\npost-processing strategies to further enhance the system's translation\ncapabilities. Our solution uniquely addresses both OCR-based and OCR-free\ndocument image translation tasks within a unified framework. This paper\nsystematically details the training methods, inference strategies, LVLM base\nmodels, training data, experimental setups, and results, demonstrating an\neffective approach to document image machine translation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "7 pages, 1 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2504.17315v1",
    "published_date": "2025-04-24 07:17:59 UTC",
    "updated_date": "2025-04-24 07:17:59 UTC"
  },
  {
    "arxiv_id": "2504.17311v1",
    "title": "FLUKE: A Linguistically-Driven and Task-Agnostic Framework for Robustness Evaluation",
    "authors": [
      "Yulia Otmakhova",
      "Hung Thinh Truong",
      "Rahmad Mahendra",
      "Zenan Zhai",
      "Rongxin Zhu",
      "Daniel Beck",
      "Jey Han Lau"
    ],
    "abstract": "We present FLUKE (Framework for LingUistically-driven and tasK-agnostic\nrobustness Evaluation), a task-agnostic framework for assessing model\nrobustness through systematic minimal variations of test data. FLUKE introduces\ncontrolled variations across linguistic levels - from orthography to dialect\nand style varieties - and leverages large language models (LLMs) with human\nvalidation to generate modifications. We demonstrate FLUKE's utility by\nevaluating both fine-tuned models and LLMs across four diverse NLP tasks, and\nreveal that (1) the impact of linguistic variations is highly task-dependent,\nwith some tests being critical for certain tasks but irrelevant for others; (2)\nwhile LLMs have better overall robustness compared to fine-tuned models, they\nstill exhibit significant brittleness to certain linguistic variations; (3) all\nmodels show substantial vulnerability to negation modifications across most\ntasks. These findings highlight the importance of systematic robustness testing\nfor understanding model behaviors.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17311v1",
    "published_date": "2025-04-24 07:12:37 UTC",
    "updated_date": "2025-04-24 07:12:37 UTC"
  },
  {
    "arxiv_id": "2504.17306v1",
    "title": "Advanced Segmentation of Diabetic Retinopathy Lesions Using DeepLabv3+",
    "authors": [
      "Meher Boulaabi",
      "Takwa Ben Aïcha Gader",
      "Afef Kacem Echi",
      "Sameh Mbarek"
    ],
    "abstract": "To improve the segmentation of diabetic retinopathy lesions (microaneurysms,\nhemorrhages, exudates, and soft exudates), we implemented a binary segmentation\nmethod specific to each type of lesion. As post-segmentation, we combined the\nindividual model outputs into a single image to better analyze the lesion\ntypes. This approach facilitated parameter optimization and improved accuracy,\neffectively overcoming challenges related to dataset limitations and annotation\ncomplexity. Specific preprocessing steps included cropping and applying\ncontrast-limited adaptive histogram equalization to the L channel of the LAB\nimage. Additionally, we employed targeted data augmentation techniques to\nfurther refine the model's efficacy. Our methodology utilized the DeepLabv3+\nmodel, achieving a segmentation accuracy of 99%. These findings highlight the\nefficacy of innovative strategies in advancing medical image analysis,\nparticularly in the precise segmentation of diabetic retinopathy lesions. The\nIDRID dataset was utilized to validate and demonstrate the robustness of our\napproach.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This work was accepted at the ACS/IEEE International Conference on\n  Computer Systems and Applications (AICCSA) 2024",
    "pdf_url": "http://arxiv.org/pdf/2504.17306v1",
    "published_date": "2025-04-24 07:00:38 UTC",
    "updated_date": "2025-04-24 07:00:38 UTC"
  },
  {
    "arxiv_id": "2504.17304v1",
    "title": "You Are What You Bought: Generating Customer Personas for E-commerce Applications",
    "authors": [
      "Yimin Shi",
      "Yang Fei",
      "Shiqi Zhang",
      "Haixun Wang",
      "Xiaokui Xiao"
    ],
    "abstract": "In e-commerce, user representations are essential for various applications.\nExisting methods often use deep learning techniques to convert customer\nbehaviors into implicit embeddings. However, these embeddings are difficult to\nunderstand and integrate with external knowledge, limiting the effectiveness of\napplications such as customer segmentation, search navigation, and product\nrecommendations. To address this, our paper introduces the concept of the\ncustomer persona. Condensed from a customer's numerous purchasing histories, a\ncustomer persona provides a multi-faceted and human-readable characterization\nof specific purchase behaviors and preferences, such as Busy Parents or Bargain\nHunters.\n  This work then focuses on representing each customer by multiple personas\nfrom a predefined set, achieving readable and informative explicit user\nrepresentations. To this end, we propose an effective and efficient solution\nGPLR. To ensure effectiveness, GPLR leverages pre-trained LLMs to infer\npersonas for customers. To reduce overhead, GPLR applies LLM-based labeling to\nonly a fraction of users and utilizes a random walk technique to predict\npersonas for the remaining customers. We further propose RevAff, which provides\nan absolute error $\\epsilon$ guarantee while improving the time complexity of\nthe exact solution by a factor of at least\n$O(\\frac{\\epsilon\\cdot|E|N}{|E|+N\\log N})$, where $N$ represents the number of\ncustomers and products, and $E$ represents the interactions between them. We\nevaluate the performance of our persona-based representation in terms of\naccuracy and robustness for recommendation and customer segmentation tasks\nusing three real-world e-commerce datasets. Most notably, we find that\nintegrating customer persona representations improves the state-of-the-art\ngraph convolution-based recommendation model by up to 12% in terms of NDCG@K\nand F1-Score@K.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "SIGIR 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.17304v1",
    "published_date": "2025-04-24 06:59:16 UTC",
    "updated_date": "2025-04-24 06:59:16 UTC"
  },
  {
    "arxiv_id": "2504.17295v1",
    "title": "AI-Enhanced Business Process Automation: A Case Study in the Insurance Domain Using Object-Centric Process Mining",
    "authors": [
      "Shahrzad Khayatbashi",
      "Viktor Sjölind",
      "Anders Granåker",
      "Amin Jalali"
    ],
    "abstract": "Recent advancements in Artificial Intelligence (AI), particularly Large\nLanguage Models (LLMs), have enhanced organizations' ability to reengineer\nbusiness processes by automating knowledge-intensive tasks. This automation\ndrives digital transformation, often through gradual transitions that improve\nprocess efficiency and effectiveness. To fully assess the impact of such\nautomation, a data-driven analysis approach is needed - one that examines how\ntraditional and AI-enhanced process variants coexist during this transition.\nObject-Centric Process Mining (OCPM) has emerged as a valuable method that\nenables such analysis, yet real-world case studies are still needed to\ndemonstrate its applicability. This paper presents a case study from the\ninsurance sector, where an LLM was deployed in production to automate the\nidentification of claim parts, a task previously performed manually and\nidentified as a bottleneck for scalability. To evaluate this transformation, we\napply OCPM to assess the impact of AI-driven automation on process scalability.\nOur findings indicate that while LLMs significantly enhance operational\ncapacity, they also introduce new process dynamics that require further\nrefinement. This study also demonstrates the practical application of OCPM in a\nreal-world setting, highlighting its advantages and limitations.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17295v1",
    "published_date": "2025-04-24 06:43:29 UTC",
    "updated_date": "2025-04-24 06:43:29 UTC"
  },
  {
    "arxiv_id": "2504.17282v1",
    "title": "Cracking the Code of Action: a Generative Approach to Affordances for Reinforcement Learning",
    "authors": [
      "Lynn Cherif",
      "Flemming Kondrup",
      "David Venuto",
      "Ankit Anand",
      "Doina Precup",
      "Khimya Khetarpal"
    ],
    "abstract": "Agents that can autonomously navigate the web through a graphical user\ninterface (GUI) using a unified action space (e.g., mouse and keyboard actions)\ncan require very large amounts of domain-specific expert demonstrations to\nachieve good performance. Low sample efficiency is often exacerbated in\nsparse-reward and large-action-space environments, such as a web GUI, where\nonly a few actions are relevant in any given situation. In this work, we\nconsider the low-data regime, with limited or no access to expert behavior. To\nenable sample-efficient learning, we explore the effect of constraining the\naction space through $\\textit{intent-based affordances}$ -- i.e., considering\nin any situation only the subset of actions that achieve a desired outcome. We\npropose $\\textbf{Code as Generative Affordances}$ $(\\textbf{$\\texttt{CoGA}$})$,\na method that leverages pre-trained vision-language models (VLMs) to generate\ncode that determines affordable actions through implicit intent-completion\nfunctions and using a fully-automated program generation and verification\npipeline. These programs are then used in-the-loop of a reinforcement learning\nagent to return a set of affordances given a pixel observation. By greatly\nreducing the number of actions that an agent must consider, we demonstrate on a\nwide range of tasks in the MiniWob++ benchmark that: $\\textbf{1)}$\n$\\texttt{CoGA}$ is orders of magnitude more sample efficient than its RL agent,\n$\\textbf{2)}$ $\\texttt{CoGA}$'s programs can generalize within a family of\ntasks, and $\\textbf{3)}$ $\\texttt{CoGA}$ performs better or on par compared\nwith behavior cloning when a small number of expert demonstrations is\navailable.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17282v1",
    "published_date": "2025-04-24 06:20:08 UTC",
    "updated_date": "2025-04-24 06:20:08 UTC"
  },
  {
    "arxiv_id": "2504.17277v1",
    "title": "ExOSITO: Explainable Off-Policy Learning with Side Information for Intensive Care Unit Blood Test Orders",
    "authors": [
      "Zongliang Ji",
      "Andre Carlos Kajdacsy-Balla Amaral",
      "Anna Goldenberg",
      "Rahul G. Krishnan"
    ],
    "abstract": "Ordering a minimal subset of lab tests for patients in the intensive care\nunit (ICU) can be challenging. Care teams must balance between ensuring the\navailability of the right information and reducing the clinical burden and\ncosts associated with each lab test order. Most in-patient settings experience\nfrequent over-ordering of lab tests, but are now aiming to reduce this burden\non both hospital resources and the environment. This paper develops a novel\nmethod that combines off-policy learning with privileged information to\nidentify the optimal set of ICU lab tests to order. Our approach, EXplainable\nOff-policy learning with Side Information for ICU blood Test Orders (ExOSITO)\ncreates an interpretable assistive tool for clinicians to order lab tests by\nconsidering both the observed and predicted future status of each patient. We\npose this problem as a causal bandit trained using offline data and a reward\nfunction derived from clinically-approved rules; we introduce a novel learning\nframework that integrates clinical knowledge with observational data to bridge\nthe gap between the optimal and logging policies. The learned policy function\nprovides interpretable clinical information and reduces costs without omitting\nany vital lab orders, outperforming both a physician's policy and prior\napproaches to this practical problem.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to the Conference on Health, Inference, and Learning (CHIL)\n  2025",
    "pdf_url": "http://arxiv.org/pdf/2504.17277v1",
    "published_date": "2025-04-24 06:07:14 UTC",
    "updated_date": "2025-04-24 06:07:14 UTC"
  },
  {
    "arxiv_id": "2505.00018v1",
    "title": "Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management",
    "authors": [
      "Ju Wu",
      "Calvin K. L. Or"
    ],
    "abstract": "This position paper critically surveys a broad spectrum of recent empirical\ndevelopments on human-AI agents collaboration, highlighting both their\ntechnical achievements and persistent gaps. We observe a lack of a unifying\ntheoretical framework that can coherently integrate these varied studies,\nespecially when tackling open-ended, complex tasks. To address this, we propose\na novel conceptual architecture: one that systematically interlinks the\ntechnical details of multi-agent coordination, knowledge management, cybernetic\nfeedback loops, and higher-level control mechanisms. By mapping existing\ncontributions, from symbolic AI techniques and connectionist LLM-based agents\nto hybrid organizational practices, onto this proposed framework (Hierarchical\nExploration-Exploitation Net), our approach facilitates revision of legacy\nmethods and inspires new work that fuses qualitative and quantitative\nparadigms. The paper's structure allows it to be read from any section, serving\nequally as a critical review of technical implementations and as a\nforward-looking reference for designing or extending human-AI symbioses.\nTogether, these insights offer a stepping stone toward deeper co-evolution of\nhuman cognition and AI capability.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.00018v1",
    "published_date": "2025-04-24 05:57:03 UTC",
    "updated_date": "2025-04-24 05:57:03 UTC"
  },
  {
    "arxiv_id": "2504.17264v1",
    "title": "JurisCTC: Enhancing Legal Judgment Prediction via Cross-Domain Transfer and Contrastive Learning",
    "authors": [
      "Zhaolu Kang",
      "Hongtian Cai",
      "Xiangyang Ji",
      "Jinzhe Li",
      "Nanfei Gu"
    ],
    "abstract": "In recent years, Unsupervised Domain Adaptation (UDA) has gained significant\nattention in the field of Natural Language Processing (NLP) owing to its\nability to enhance model generalization across diverse domains. However, its\napplication for knowledge transfer between distinct legal domains remains\nlargely unexplored. To address the challenges posed by lengthy and complex\nlegal texts and the limited availability of large-scale annotated datasets, we\npropose JurisCTC, a novel model designed to improve the accuracy of Legal\nJudgment Prediction (LJP) tasks. Unlike existing approaches, JurisCTC\nfacilitates effective knowledge transfer across various legal domains and\nemploys contrastive learning to distinguish samples from different domains.\nSpecifically, for the LJP task, we enable knowledge transfer between civil and\ncriminal law domains. Compared to other models and specific large language\nmodels (LLMs), JurisCTC demonstrates notable advancements, achieving peak\naccuracies of 76.59% and 78.83%, respectively.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted in International Joint Conference on Neural Networks (IJCNN)\n  2025",
    "pdf_url": "http://arxiv.org/pdf/2504.17264v1",
    "published_date": "2025-04-24 05:48:57 UTC",
    "updated_date": "2025-04-24 05:48:57 UTC"
  },
  {
    "arxiv_id": "2504.17261v1",
    "title": "Symbolic Representation for Any-to-Any Generative Tasks",
    "authors": [
      "Jiaqi Chen",
      "Xiaoye Zhu",
      "Yue Wang",
      "Tianyang Liu",
      "Xinhui Chen",
      "Ying Chen",
      "Chak Tou Leong",
      "Yifei Ke",
      "Joseph Liu",
      "Yiwen Yuan",
      "Julian McAuley",
      "Li-jia Li"
    ],
    "abstract": "We propose a symbolic generative task description language and a\ncorresponding inference engine capable of representing arbitrary multimodal\ntasks as structured symbolic flows. Unlike conventional generative models that\nrely on large-scale training and implicit neural representations to learn\ncross-modal mappings, often at high computational cost and with limited\nflexibility, our framework introduces an explicit symbolic representation\ncomprising three core primitives: functions, parameters, and topological logic.\nLeveraging a pre-trained language model, our inference engine maps natural\nlanguage instructions directly to symbolic workflows in a training-free manner.\nOur framework successfully performs over 12 diverse multimodal generative\ntasks, demonstrating strong performance and flexibility without the need for\ntask-specific tuning. Experiments show that our method not only matches or\noutperforms existing state-of-the-art unified models in content quality, but\nalso offers greater efficiency, editability, and interruptibility. We believe\nthat symbolic task representations provide a cost-effective and extensible\nfoundation for advancing the capabilities of generative AI.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17261v1",
    "published_date": "2025-04-24 05:35:47 UTC",
    "updated_date": "2025-04-24 05:35:47 UTC"
  },
  {
    "arxiv_id": "2504.17255v1",
    "title": "3D Deep-learning-based Segmentation of Human Skin Sweat Glands and Their 3D Morphological Response to Temperature Variations",
    "authors": [
      "Shaoyu Pei",
      "Renxiong Wu",
      "Hao Zheng",
      "Lang Qin",
      "Shuaichen Lin",
      "Yuxing Gan",
      "Wenjing Huang",
      "Zhixuan Wang",
      "Mohan Qin",
      "Yong Liu",
      "Guangming Ni"
    ],
    "abstract": "Skin, the primary regulator of heat exchange, relies on sweat glands for\nthermoregulation. Alterations in sweat gland morphology play a crucial role in\nvarious pathological conditions and clinical diagnoses. Current methods for\nobserving sweat gland morphology are limited by their two-dimensional, in\nvitro, and destructive nature, underscoring the urgent need for real-time,\nnon-invasive, quantifiable technologies. We proposed a novel three-dimensional\n(3D) transformer-based multi-object segmentation framework, integrating a\nsliding window approach, joint spatial-channel attention mechanism, and\narchitectural heterogeneity between shallow and deep layers. Our proposed\nnetwork enables precise 3D sweat gland segmentation from skin volume data\ncaptured by optical coherence tomography (OCT). For the first time, subtle\nvariations of sweat gland 3D morphology in response to temperature changes,\nhave been visualized and quantified. Our approach establishes a benchmark for\nnormal sweat gland morphology and provides a real-time, non-invasive tool for\nquantifying 3D structural parameters. This enables the study of individual\nvariability and pathological changes in sweat gland structure, advancing\ndermatological research and clinical applications, including thermoregulation\nand bromhidrosis treatment.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "physics.optics"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17255v1",
    "published_date": "2025-04-24 05:19:47 UTC",
    "updated_date": "2025-04-24 05:19:47 UTC"
  },
  {
    "arxiv_id": "2504.17247v1",
    "title": "Targeted AMP generation through controlled diffusion with efficient embeddings",
    "authors": [
      "Diogo Soares",
      "Leon Hetzel",
      "Paulina Szymczak",
      "Fabian Theis",
      "Stephan Günnemann",
      "Ewa Szczurek"
    ],
    "abstract": "Deep learning-based antimicrobial peptide (AMP) discovery faces critical\nchallenges such as low experimental hit rates as well as the need for nuanced\ncontrollability and efficient modeling of peptide properties. To address these\nchallenges, we introduce OmegAMP, a framework that leverages a diffusion-based\ngenerative model with efficient low-dimensional embeddings, precise\ncontrollability mechanisms, and novel classifiers with drastically reduced\nfalse positive rates for candidate filtering. OmegAMP enables the targeted\ngeneration of AMPs with specific physicochemical properties, activity profiles,\nand species-specific effectiveness. Moreover, it maximizes sample diversity\nwhile ensuring faithfulness to the underlying data distribution during\ngeneration. We demonstrate that OmegAMP achieves state-of-the-art performance\nacross all stages of the AMP discovery pipeline, significantly advancing the\npotential of computational frameworks in combating antimicrobial resistance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.BM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17247v1",
    "published_date": "2025-04-24 04:53:04 UTC",
    "updated_date": "2025-04-24 04:53:04 UTC"
  },
  {
    "arxiv_id": "2504.17243v2",
    "title": "NeuralGrok: Accelerate Grokking by Neural Gradient Transformation",
    "authors": [
      "Xinyu Zhou",
      "Simin Fan",
      "Martin Jaggi",
      "Jie Fu"
    ],
    "abstract": "Grokking is proposed and widely studied as an intricate phenomenon in which\ngeneralization is achieved after a long-lasting period of overfitting. In this\nwork, we propose NeuralGrok, a novel gradient-based approach that learns an\noptimal gradient transformation to accelerate the generalization of\ntransformers in arithmetic tasks. Specifically, NeuralGrok trains an auxiliary\nmodule (e.g., an MLP block) in conjunction with the base model. This module\ndynamically modulates the influence of individual gradient components based on\ntheir contribution to generalization, guided by a bilevel optimization\nalgorithm. Our extensive experiments demonstrate that NeuralGrok significantly\naccelerates generalization, particularly in challenging arithmetic tasks. We\nalso show that NeuralGrok promotes a more stable training paradigm, constantly\nreducing the model's complexity, while traditional regularization methods, such\nas weight decay, can introduce substantial instability and impede\ngeneralization. We further investigate the intrinsic model complexity\nleveraging a novel Absolute Gradient Entropy (AGE) metric, which explains that\nNeuralGrok effectively facilitates generalization by reducing the model\ncomplexity. We offer valuable insights on the grokking phenomenon of\nTransformer models, which encourages a deeper understanding of the fundamental\nprinciples governing generalization ability.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint, 16 pages",
    "pdf_url": "http://arxiv.org/pdf/2504.17243v2",
    "published_date": "2025-04-24 04:41:35 UTC",
    "updated_date": "2025-04-25 03:44:09 UTC"
  },
  {
    "arxiv_id": "2504.17828v1",
    "title": "VEU-Bench: Towards Comprehensive Understanding of Video Editing",
    "authors": [
      "Bozheng Li",
      "Yongliang Wu",
      "Yi Lu",
      "Jiashuo Yu",
      "Licheng Tang",
      "Jiawang Cao",
      "Wenqing Zhu",
      "Yuyang Sun",
      "Jay Wu",
      "Wenbo Zhu"
    ],
    "abstract": "Widely shared videos on the internet are often edited. Recently, although\nVideo Large Language Models (Vid-LLMs) have made great progress in general\nvideo understanding tasks, their capabilities in video editing understanding\n(VEU) tasks remain unexplored. To address this gap, in this paper, we introduce\nVEU-Bench (Video Editing Understanding Benchmark), a comprehensive benchmark\nthat categorizes video editing components across various dimensions, from\nintra-frame features like shot size to inter-shot attributes such as cut types\nand transitions. Unlike previous video editing understanding benchmarks that\nfocus mainly on editing element classification, VEU-Bench encompasses 19\nfine-grained tasks across three stages: recognition, reasoning, and judging. To\nenhance the annotation of VEU automatically, we built an annotation pipeline\nintegrated with an ontology-based knowledge base. Through extensive experiments\nwith 11 state-of-the-art Vid-LLMs, our findings reveal that current Vid-LLMs\nface significant challenges in VEU tasks, with some performing worse than\nrandom choice. To alleviate this issue, we develop Oscars, a VEU expert model\nfine-tuned on the curated VEU-Bench dataset. It outperforms existing\nopen-source Vid-LLMs on VEU-Bench by over 28.3% in accuracy and achieves\nperformance comparable to commercial models like GPT-4o. We also demonstrate\nthat incorporating VEU data significantly enhances the performance of Vid-LLMs\non general video understanding benchmarks, with an average improvement of 8.3%\nacross nine reasoning tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to CVPR2025",
    "pdf_url": "http://arxiv.org/pdf/2504.17828v1",
    "published_date": "2025-04-24 04:36:28 UTC",
    "updated_date": "2025-04-24 04:36:28 UTC"
  },
  {
    "arxiv_id": "2504.17219v1",
    "title": "Enhancing Variational Autoencoders with Smooth Robust Latent Encoding",
    "authors": [
      "Hyomin Lee",
      "Minseon Kim",
      "Sangwon Jang",
      "Jongheon Jeong",
      "Sung Ju Hwang"
    ],
    "abstract": "Variational Autoencoders (VAEs) have played a key role in scaling up\ndiffusion-based generative models, as in Stable Diffusion, yet questions\nregarding their robustness remain largely underexplored. Although adversarial\ntraining has been an established technique for enhancing robustness in\npredictive models, it has been overlooked for generative models due to concerns\nabout potential fidelity degradation by the nature of trade-offs between\nperformance and robustness. In this work, we challenge this presumption,\nintroducing Smooth Robust Latent VAE (SRL-VAE), a novel adversarial training\nframework that boosts both generation quality and robustness. In contrast to\nconventional adversarial training, which focuses on robustness only, our\napproach smooths the latent space via adversarial perturbations, promoting more\ngeneralizable representations while regularizing with originality\nrepresentation to sustain original fidelity. Applied as a post-training step on\npre-trained VAEs, SRL-VAE improves image robustness and fidelity with minimal\ncomputational overhead. Experiments show that SRL-VAE improves both generation\nquality, in image reconstruction and text-guided image editing, and robustness,\nagainst Nightshade attacks and image editing attacks. These results establish a\nnew paradigm, showing that adversarial training, once thought to be detrimental\nto generative models, can instead enhance both fidelity and robustness.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "Under review",
    "pdf_url": "http://arxiv.org/pdf/2504.17219v1",
    "published_date": "2025-04-24 03:17:57 UTC",
    "updated_date": "2025-04-24 03:17:57 UTC"
  },
  {
    "arxiv_id": "2504.17827v3",
    "title": "Evolution Meets Diffusion: Efficient Neural Architecture Generation",
    "authors": [
      "Bingye Zhou",
      "Caiyang Yu"
    ],
    "abstract": "Neural Architecture Search (NAS) has gained widespread attention for its\ntransformative potential in deep learning model design. However, the vast and\ncomplex search space of NAS leads to significant computational and time costs.\nNeural Architecture Generation (NAG) addresses this by reframing NAS as a\ngeneration problem, enabling the precise generation of optimal architectures\nfor specific tasks. Despite its promise, mainstream methods like diffusion\nmodels face limitations in global search capabilities and are still hindered by\nhigh computational and time demands. To overcome these challenges, we propose\nEvolutionary Diffusion-based Neural Architecture Generation (EDNAG), a novel\napproach that achieves efficient and training-free architecture generation.\nEDNAG leverages evolutionary algorithms to simulate the denoising process in\ndiffusion models, using fitness to guide the transition from random Gaussian\ndistributions to optimal architecture distributions. This approach combines the\nstrengths of evolutionary strategies and diffusion models, enabling rapid and\neffective architecture generation. Extensive experiments demonstrate that EDNAG\nachieves state-of-the-art (SOTA) performance in architecture optimization, with\nan improvement in accuracy of up to 10.45%. Furthermore, it eliminates the need\nfor time-consuming training and boosts inference speed by an average of 50\ntimes, showcasing its exceptional efficiency and effectiveness.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17827v3",
    "published_date": "2025-04-24 03:09:04 UTC",
    "updated_date": "2025-04-30 08:52:25 UTC"
  },
  {
    "arxiv_id": "2504.18588v1",
    "title": "Dynamic QoS Prediction via a Non-Negative Tensor Snowflake Factorization",
    "authors": [
      "YongHui Xia",
      "Lan Wang",
      "Hao Wu"
    ],
    "abstract": "Dynamic quality of service (QoS) data exhibit rich temporal patterns in\nuser-service interactions, which are crucial for a comprehensive understanding\nof user behavior and service conditions in Web service. As the number of users\nand services increases, there is a large amount of unobserved QoS data, which\nsignificantly affects users'choice of services. To predict unobserved QoS data,\nwe propose a Non-negative Snowflake Factorization of tensors model. This method\ndesigns a snowflake core tensor to enhance the model's learning capability.\nAdditionally, it employs a single latent factor-based, nonnegative\nmultiplication update on tensor (SLF-NMUT) for parameter learning. Empirical\nresults demonstrate that the proposed model more accurately learns dynamic\nuser-service interaction patterns, thereby yielding improved predictions for\nmissing QoS data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.18588v1",
    "published_date": "2025-04-24 03:03:22 UTC",
    "updated_date": "2025-04-24 03:03:22 UTC"
  },
  {
    "arxiv_id": "2504.17213v2",
    "title": "MASR: Self-Reflective Reasoning through Multimodal Hierarchical Attention Focusing for Agent-based Video Understanding",
    "authors": [
      "Shiwen Cao",
      "Zhaoxing Zhang",
      "Junming Jiao",
      "Juyi Qiao",
      "Guowen Song",
      "Rong Shen",
      "Xiangbing Meng"
    ],
    "abstract": "Even in the era of rapid advances in large models, video understanding\nremains a highly challenging task. Compared to texts or images, videos commonly\ncontain more information with redundancy, requiring large models to properly\nallocate attention at a global level for comprehensive and accurate\nunderstanding. To address this, we propose a Multimodal hierarchical Attention\nfocusing Self-reflective Reasoning (MASR) framework for agent-based video\nunderstanding. The key innovation lies in its ability to detect and prioritize\nsegments of videos that are highly relevant to the query. Firstly, MASR\nrealizes Multimodal Coarse-to-fine Relevance Sensing (MCRS) which enhances the\ncorrelation between the acquired contextual information and the query.\nSecondly, MASR employs Dilated Temporal Expansion (DTE) to mitigate the risk of\nmissing crucial details when extracting semantic information from the focused\nframes selected through MCRS. By iteratively applying MCRS and DTE in the\nself-reflective reasoning process, MASR is able to adaptively adjust the\nattention to extract highly query-relevant context and therefore improve the\nresponse accuracy. In the EgoSchema dataset, MASR achieves a remarkable 5%\nperformance gain over previous leading approaches. In the Next-QA and IntentQA\ndatasets, it outperforms the state-of-the-art standards by 0.2% and 0.3%\nrespectively. In the Video-MME dataset that contains long-term videos, MASR\nalso performs better than other agent-based methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17213v2",
    "published_date": "2025-04-24 02:54:40 UTC",
    "updated_date": "2025-04-28 05:05:24 UTC"
  },
  {
    "arxiv_id": "2504.17210v1",
    "title": "Synthetic Power Flow Data Generation Using Physics-Informed Denoising Diffusion Probabilistic Models",
    "authors": [
      "Junfei Wang",
      "Darshana Upadhyay",
      "Marzia Zaman",
      "Pirathayini Srikantha"
    ],
    "abstract": "Many data-driven modules in smart grid rely on access to high-quality power\nflow data; however, real-world data are often limited due to privacy and\noperational constraints. This paper presents a physics-informed generative\nframework based on Denoising Diffusion Probabilistic Models (DDPMs) for\nsynthesizing feasible power flow data. By incorporating auxiliary training and\nphysics-informed loss functions, the proposed method ensures that the generated\ndata exhibit both statistical fidelity and adherence to power system\nfeasibility. We evaluate the approach on the IEEE 14-bus and 30-bus benchmark\nsystems, demonstrating its ability to capture key distributional properties and\ngeneralize to out-of-distribution scenarios. Comparative results show that the\nproposed model outperforms three baseline models in terms of feasibility,\ndiversity, and accuracy of statistical features. This work highlights the\npotential of integrating generative modelling into data-driven power system\napplications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Submitted to IEEE SmartGridComm Conference 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.17210v1",
    "published_date": "2025-04-24 02:53:22 UTC",
    "updated_date": "2025-04-24 02:53:22 UTC"
  },
  {
    "arxiv_id": "2504.17826v1",
    "title": "FashionM3: Multimodal, Multitask, and Multiround Fashion Assistant based on Unified Vision-Language Model",
    "authors": [
      "Kaicheng Pang",
      "Xingxing Zou",
      "Waikeung Wong"
    ],
    "abstract": "Fashion styling and personalized recommendations are pivotal in modern\nretail, contributing substantial economic value in the fashion industry. With\nthe advent of vision-language models (VLM), new opportunities have emerged to\nenhance retailing through natural language and visual interactions. This work\nproposes FashionM3, a multimodal, multitask, and multiround fashion assistant,\nbuilt upon a VLM fine-tuned for fashion-specific tasks. It helps users discover\nsatisfying outfits by offering multiple capabilities including personalized\nrecommendation, alternative suggestion, product image generation, and virtual\ntry-on simulation. Fine-tuned on the novel FashionRec dataset, comprising\n331,124 multimodal dialogue samples across basic, personalized, and alternative\nrecommendation tasks, FashionM3 delivers contextually personalized suggestions\nwith iterative refinement through multiround interactions. Quantitative and\nqualitative evaluations, alongside user studies, demonstrate FashionM3's\nsuperior performance in recommendation effectiveness and practical value as a\nfashion assistant.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17826v1",
    "published_date": "2025-04-24 02:44:22 UTC",
    "updated_date": "2025-04-24 02:44:22 UTC"
  },
  {
    "arxiv_id": "2504.17825v1",
    "title": "Dual Prompting Image Restoration with Diffusion Transformers",
    "authors": [
      "Dehong Kong",
      "Fan Li",
      "Zhixin Wang",
      "Jiaqi Xu",
      "Renjing Pei",
      "Wenbo Li",
      "WenQi Ren"
    ],
    "abstract": "Recent state-of-the-art image restoration methods mostly adopt latent\ndiffusion models with U-Net backbones, yet still facing challenges in achieving\nhigh-quality restoration due to their limited capabilities. Diffusion\ntransformers (DiTs), like SD3, are emerging as a promising alternative because\nof their better quality with scalability. In this paper, we introduce DPIR\n(Dual Prompting Image Restoration), a novel image restoration method that\neffectivly extracts conditional information of low-quality images from multiple\nperspectives. Specifically, DPIR consits of two branches: a low-quality image\nconditioning branch and a dual prompting control branch. The first branch\nutilizes a lightweight module to incorporate image priors into the DiT with\nhigh efficiency. More importantly, we believe that in image restoration,\ntextual description alone cannot fully capture its rich visual characteristics.\nTherefore, a dual prompting module is designed to provide DiT with additional\nvisual cues, capturing both global context and local appearance. The extracted\nglobal-local visual prompts as extra conditional control, alongside textual\nprompts to form dual prompts, greatly enhance the quality of the restoration.\nExtensive experimental results demonstrate that DPIR delivers superior image\nrestoration performance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR2025",
    "pdf_url": "http://arxiv.org/pdf/2504.17825v1",
    "published_date": "2025-04-24 02:34:44 UTC",
    "updated_date": "2025-04-24 02:34:44 UTC"
  },
  {
    "arxiv_id": "2504.17198v1",
    "title": "Automatically Generating Rules of Malicious Software Packages via Large Language Model",
    "authors": [
      "XiangRui Zhang",
      "HaoYu Chen",
      "Yongzhong He",
      "Wenjia Niu",
      "Qiang Li"
    ],
    "abstract": "Today's security tools predominantly rely on predefined rules crafted by\nexperts, making them poorly adapted to the emergence of software supply chain\nattacks. To tackle this limitation, we propose a novel tool, RuleLLM, which\nleverages large language models (LLMs) to automate rule generation for OSS\necosystems. RuleLLM extracts metadata and code snippets from malware as its\ninput, producing YARA and Semgrep rules that can be directly deployed in\nsoftware development. Specifically, the rule generation task involves three\nsubtasks: crafting rules, refining rules, and aligning rules. To validate\nRuleLLM's effectiveness, we implemented a prototype system and conducted\nexperiments on the dataset of 1,633 malicious packages. The results are\npromising that RuleLLM generated 763 rules (452 YARA and 311 Semgrep) with a\nprecision of 85.2\\% and a recall of 91.8\\%, outperforming state-of-the-art\n(SOTA) tools and scored-based approaches. We further analyzed generated rules\nand proposed a rule taxonomy: 11 categories and 38 subcategories.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.SE",
    "comment": "14 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.17198v1",
    "published_date": "2025-04-24 02:15:45 UTC",
    "updated_date": "2025-04-24 02:15:45 UTC"
  },
  {
    "arxiv_id": "2504.17180v2",
    "title": "We'll Fix it in Post: Improving Text-to-Video Generation with Neuro-Symbolic Feedback",
    "authors": [
      "Minkyu Choi",
      "S P Sharan",
      "Harsh Goel",
      "Sahil Shah",
      "Sandeep Chinchali"
    ],
    "abstract": "Current text-to-video (T2V) generation models are increasingly popular due to\ntheir ability to produce coherent videos from textual prompts. However, these\nmodels often struggle to generate semantically and temporally consistent videos\nwhen dealing with longer, more complex prompts involving multiple objects or\nsequential events. Additionally, the high computational costs associated with\ntraining or fine-tuning make direct improvements impractical. To overcome these\nlimitations, we introduce NeuS-E, a novel zero-training video refinement\npipeline that leverages neuro-symbolic feedback to automatically enhance video\ngeneration, achieving superior alignment with the prompts. Our approach first\nderives the neuro-symbolic feedback by analyzing a formal video representation\nand pinpoints semantically inconsistent events, objects, and their\ncorresponding frames. This feedback then guides targeted edits to the original\nvideo. Extensive empirical evaluations on both open-source and proprietary T2V\nmodels demonstrate that NeuS-E significantly enhances temporal and logical\nalignment across diverse prompts by almost 40%",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17180v2",
    "published_date": "2025-04-24 01:34:12 UTC",
    "updated_date": "2025-04-25 02:41:34 UTC"
  },
  {
    "arxiv_id": "2504.17179v1",
    "title": "AUTHENTICATION: Identifying Rare Failure Modes in Autonomous Vehicle Perception Systems using Adversarially Guided Diffusion Models",
    "authors": [
      "Mohammad Zarei",
      "Melanie A Jutras",
      "Eliana Evans",
      "Mike Tan",
      "Omid Aaramoon"
    ],
    "abstract": "Autonomous Vehicles (AVs) rely on artificial intelligence (AI) to accurately\ndetect objects and interpret their surroundings. However, even when trained\nusing millions of miles of real-world data, AVs are often unable to detect rare\nfailure modes (RFMs). The problem of RFMs is commonly referred to as the\n\"long-tail challenge\", due to the distribution of data including many instances\nthat are very rarely seen. In this paper, we present a novel approach that\nutilizes advanced generative and explainable AI techniques to aid in\nunderstanding RFMs. Our methods can be used to enhance the robustness and\nreliability of AVs when combined with both downstream model training and\ntesting. We extract segmentation masks for objects of interest (e.g., cars) and\ninvert them to create environmental masks. These masks, combined with carefully\ncrafted text prompts, are fed into a custom diffusion model. We leverage the\nStable Diffusion inpainting model guided by adversarial noise optimization to\ngenerate images containing diverse environments designed to evade object\ndetection models and expose vulnerabilities in AI systems. Finally, we produce\nnatural language descriptions of the generated RFMs that can guide developers\nand policymakers to improve the safety and reliability of AV systems.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.RO",
      "68T45, 68T05 68T45, 68T05 68T45, 68T05",
      "I.2.6; I.2.10; I.4.8"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, 10 figures. Accepted to IEEE Conference on Artificial\n  Intelligence (CAI), 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.17179v1",
    "published_date": "2025-04-24 01:31:13 UTC",
    "updated_date": "2025-04-24 01:31:13 UTC"
  },
  {
    "arxiv_id": "2504.18587v1",
    "title": "Training Large Language Models to Reason via EM Policy Gradient",
    "authors": [
      "Tianbing Xu"
    ],
    "abstract": "Recently, foundation models such as OpenAI's O1 and O3, along with DeepSeek's\nR1, have demonstrated strong reasoning capacities and problem-solving skills\nacquired through large-scale reinforcement learning (RL), with wide\napplications in mathematics, coding, science, intelligent agents, and virtual\nassistants. In this work, we introduce an off-policy reinforcement learning\nalgorithm, EM Policy Gradient, aimed at enhancing LLM reasoning by optimizing\nexpected return over reasoning trajectories. We frame the reasoning task as an\nExpectation-Maximization (EM) optimization problem, alternating between\nsampling diverse rationale trajectories and performing reward-guided\nfine-tuning. Unlike PPO and GRPO, which rely on complex importance weights and\nheuristic clipping, our method provides a simpler, more principled off-policy\npolicy gradient approach, eliminating these complexities while maintaining\nstrong performance. We evaluate the effectiveness of EM Policy Gradient on the\nGSM8K and MATH (HARD) datasets, where it achieves performance comparable to or\nslightly surpassing the state-of-the-art GRPO, while offering additional\nadvantages in scalability, simplicity, and reasoning conciseness. Moreover,\nmodels fine-tuned with our method exhibit cognitive behaviors, such as\nsub-problem decomposition, self-verification, and backtracking, highlighting\nits potential to enhance both the interpretability and robustness of LLM\nreasoning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.18587v1",
    "published_date": "2025-04-24 01:31:05 UTC",
    "updated_date": "2025-04-24 01:31:05 UTC"
  },
  {
    "arxiv_id": "2504.17170v1",
    "title": "Improving Human-Autonomous Vehicle Interaction in Complex Systems",
    "authors": [
      "Robert Kaufman"
    ],
    "abstract": "Unresolved questions about how autonomous vehicles (AVs) should meet the\ninformational needs of riders hinder real-world adoption. Complicating our\nability to satisfy rider needs is that different people, goals, and driving\ncontexts have different criteria for what constitutes interaction success.\nUnfortunately, most human-AV research and design today treats all people and\nsituations uniformly. It is crucial to understand how an AV should communicate\nto meet rider needs, and how communications should change when the human-AV\ncomplex system changes. I argue that understanding the relationships between\ndifferent aspects of the human-AV system can help us build improved and\nadaptable AV communications. I support this argument using three empirical\nstudies. First, I identify optimal communication strategies that enhance\ndriving performance, confidence, and trust for learning in extreme driving\nenvironments. Findings highlight the need for task-sensitive,\nmodality-appropriate communications tuned to learner cognitive limits and\ngoals. Next, I highlight the consequences of deploying faulty communication\nsystems and demonstrate the need for context-sensitive communications. Third, I\nuse machine learning (ML) to illuminate personal factors predicting trust in\nAVs, emphasizing the importance of tailoring designs to individual traits and\nconcerns. Together, this dissertation supports the necessity of transparent,\nadaptable, and personalized AV systems that cater to individual needs, goals,\nand contextual demands. By considering the complex system within which human-AV\ninteractions occur, we can deliver valuable insights for designers,\nresearchers, and policymakers. This dissertation also provides a concrete\ndomain to study theories of human-machine joint action and situational\nawareness, and can be used to guide future human-AI interaction research.\n[shortened for arxiv]",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "PhD Dissertation from University of California, San Diego; 175 pages",
    "pdf_url": "http://arxiv.org/pdf/2504.17170v1",
    "published_date": "2025-04-24 01:09:51 UTC",
    "updated_date": "2025-04-24 01:09:51 UTC"
  },
  {
    "arxiv_id": "2505.00017v1",
    "title": "ReCellTy: Domain-specific knowledge graph retrieval-augmented LLMs workflow for single-cell annotation",
    "authors": [
      "Dezheng Han",
      "Yibin Jia",
      "Ruxiao Chen",
      "Wenjie Han",
      "Shuaishuai Guo",
      "Jianbo Wang"
    ],
    "abstract": "To enable precise and fully automated cell type annotation with large\nlanguage models (LLMs), we developed a graph structured feature marker database\nto retrieve entities linked to differential genes for cell reconstruction. We\nfurther designed a multi task workflow to optimize the annotation process.\nCompared to general purpose LLMs, our method improves human evaluation scores\nby up to 0.21 and semantic similarity by 6.1% across 11 tissue types, while\nmore closely aligning with the cognitive logic of manual annotation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.00017v1",
    "published_date": "2025-04-24 01:05:22 UTC",
    "updated_date": "2025-04-24 01:05:22 UTC"
  },
  {
    "arxiv_id": "2504.17162v1",
    "title": "A Comprehensive Review on RNA Subcellular Localization Prediction",
    "authors": [
      "Cece Zhang",
      "Xuehuan Zhu",
      "Nick Peterson",
      "Jieqiong Wang",
      "Shibiao Wan"
    ],
    "abstract": "The subcellular localization of RNAs, including long non-coding RNAs\n(lncRNAs), messenger RNAs (mRNAs), microRNAs (miRNAs) and other smaller RNAs,\nplays a critical role in determining their biological functions. For instance,\nlncRNAs are predominantly associated with chromatin and act as regulators of\ngene transcription and chromatin structure, while mRNAs are distributed across\nthe nucleus and cytoplasm, facilitating the transport of genetic information\nfor protein synthesis. Understanding RNA localization sheds light on processes\nlike gene expression regulation with spatial and temporal precision. However,\ntraditional wet lab methods for determining RNA localization, such as in situ\nhybridization, are often time-consuming, resource-demanding, and costly. To\novercome these challenges, computational methods leveraging artificial\nintelligence (AI) and machine learning (ML) have emerged as powerful\nalternatives, enabling large-scale prediction of RNA subcellular localization.\nThis paper provides a comprehensive review of the latest advancements in\nAI-based approaches for RNA subcellular localization prediction, covering\nvarious RNA types and focusing on sequence-based, image-based, and hybrid\nmethodologies that combine both data types. We highlight the potential of these\nmethods to accelerate RNA research, uncover molecular pathways, and guide\ntargeted disease treatments. Furthermore, we critically discuss the challenges\nin AI/ML approaches for RNA subcellular localization, such as data scarcity and\nlack of benchmarks, and opportunities to address them. This review aims to\nserve as a valuable resource for researchers seeking to develop innovative\nsolutions in the field of RNA subcellular localization and beyond.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "q-bio.GN",
      "q-bio.SC"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17162v1",
    "published_date": "2025-04-24 00:47:31 UTC",
    "updated_date": "2025-04-24 00:47:31 UTC"
  },
  {
    "arxiv_id": "2504.17160v1",
    "title": "OUI Need to Talk About Weight Decay: A New Perspective on Overfitting Detection",
    "authors": [
      "Alberto Fernández-Hernández",
      "Jose I. Mestre",
      "Manuel F. Dolz",
      "Jose Duato",
      "Enrique S. Quintana-Ortí"
    ],
    "abstract": "We introduce the Overfitting-Underfitting Indicator (OUI), a novel tool for\nmonitoring the training dynamics of Deep Neural Networks (DNNs) and identifying\noptimal regularization hyperparameters. Specifically, we validate that OUI can\neffectively guide the selection of the Weight Decay (WD) hyperparameter by\nindicating whether a model is overfitting or underfitting during training\nwithout requiring validation data. Through experiments on DenseNet-BC-100 with\nCIFAR- 100, EfficientNet-B0 with TinyImageNet and ResNet-34 with ImageNet-1K,\nwe show that maintaining OUI within a prescribed interval correlates strongly\nwith improved generalization and validation scores. Notably, OUI converges\nsignificantly faster than traditional metrics such as loss or accuracy,\nenabling practitioners to identify optimal WD (hyperparameter) values within\nthe early stages of training. By leveraging OUI as a reliable indicator, we can\ndetermine early in training whether the chosen WD value leads the model to\nunderfit the training data, overfit, or strike a well-balanced trade-off that\nmaximizes validation scores. This enables more precise WD tuning for optimal\nperformance on the tested datasets and DNNs. All code for reproducing these\nexperiments is available at https://github.com/AlbertoFdezHdez/OUI.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.17160v1",
    "published_date": "2025-04-24 00:41:59 UTC",
    "updated_date": "2025-04-24 00:41:59 UTC"
  }
]
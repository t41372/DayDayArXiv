{
  "date": "2025-04-24",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-04-24 的 arXiv 中文 TLDR 快报！今天 arXiv 更新了 110 篇论文，主要聚焦 AI 和机器学习领域的创新应用，如 LLM 增强的药物发现、联邦学习优化和机器人规划等，其中 PharmaSwarm（药物发现框架）和 BadMoE（MoE 模型安全攻击）等论文令人印象深刻，而作者如 Pietro Barbiero 和 Yejin Choi 的参与进一步提升了话题度。\n\n以下是今日论文的精要摘要，我会优先讨论重要、创新或话题度高的论文（如 AI 安全、医疗和机器人领域），并将相关论文归类讨论；对于较基础或小众主题的论文（如某些数学逻辑或特定优化算法），会快速掠过，只列出关键点。每个条目包括论文标题（中文 + 英文）和主要贡献。\n\n### AI 安全与鲁棒性（重点领域，创新性强）\n- **BadMoE（中文）：通过优化路由触发和感染休眠专家的混合专家 LLM 后门攻击**  \n  英文：BadMoE: Backdooring Mixture-of-Experts LLMs via Optimizing Routing Triggers and Infecting Dormant Experts  \n  主要贡献：提出针对 MoE 模型的隐蔽后门攻击方法，通过激活休眠专家和路由优化，实现高效攻击，同时保持模型效用；发现这种攻击比传统方法更隐蔽，强调了 LLM 安全风险。\n  \n- **EPSILON（中文）：使用统计签名在近似深度神经网络中实现自适应容错**  \n  英文：EPSILON: Adaptive Fault Mitigation in Approximate Deep Neural Network using Statistical Signatures  \n  主要贡献：开发轻量级框架检测和缓解 DNN 中的永久故障，提高推理速度和能效 28%，适用于边缘计算的安全场景。\n\n- **ApproXAI（中文）：使用近似计算加速可解释 AI 的硬件**  \n  英文：ApproXAI: Energy-Efficient Hardware Acceleration of Explainable AI using Approximate Computing  \n  主要贡献：提出框架将 XAI 算法转化为近似矩阵计算，提升 TPU 边缘设备的能效 2 倍，同时保持准确性。\n\n### LLM 与生成模型应用（高话题度，结合实际场景）\n- **PharmaSwarm（中文）：LLM 代理群用于假设驱动的药物发现**  \n  英文：LLM Agent Swarm for Hypothesis-Driven Drug Discovery  \n  主要贡献：构建多代理框架，使用 LLM 提出并验证药物假设，提高药物发现效率，减少临床失败率 90%，并提供四层验证管道确保透明性。\n  \n- **RAGEN（中文）：通过多轮强化学习理解 LLM 代理的自演化**  \n  英文：RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning  \n  主要贡献：引入 StarPO 框架分析 LLM 代理演化，解决奖励方差问题，并发现细粒度奖励能提升代理推理能力。\n  \n- **HalluLens（中文）：LLM 幻觉现象基准**  \n  英文：HalluLens: LLM Hallucination Benchmark  \n  主要贡献：构建幻觉分类数据集，区分外在和内在幻觉，提供动态测试集，揭示 LLM 在生成任务中的鲁棒性问题。\n  \n- **BadMoE（已在上文提及，相关扩展）：混合专家 LLM 的后门攻击**  \n  快速掠过其他类似：如 Token Sequence Compression（中文）：用于高效多模态计算的令牌序列压缩，英文：Token Sequence Compression for Efficient Multimodal Computing。主要发现：聚类令牌聚合方法提升计算效率，但整体影响力较小。\n\n### 医疗与生物AI（实际应用潜力强）\n- **Crypto-ncRNA（中文）：基于非编码 RNA 的加密算法**  \n  英文：Crypto-ncRNA: Non-coding RNA (ncRNA) Based Encryption Algorithm  \n  主要贡献：提出量子抵抗加密框架，使用 RNA 折叠生成高熵密钥，通过 NIST 测试验证其效率，适用于后量子时代安全。\n  \n- **Early Detection of Multidrug Resistance（中文）：使用多变量时序分析检测多药耐药**  \n  英文：Early Detection of Multidrug Resistance Using Multivariate Time Series Analysis and Interpretable Patient-Similarity Representations  \n  主要贡献：开发 ICU 数据框架，使用时序相似性预测耐药风险，提升 AUC 到 81%，并通过图聚类解释关键风险因素。\n\n- **QuantBench（中文）：AI 用于量化投资的基准**  \n  英文：QuantBench: Benchmarking AI Methods for Quantitative Investment  \n  主要贡献：构建标准化投资基准，揭示 AI 在金融中的持续学习需求，加速研究与实践应用。\n  \n  快速掠过相关：如 Multilingual Performance Biases（中文）：多语言 LLM 在教育中的偏差，英文：Multilingual Performance Biases of Large Language Models in Education。主要发现：LLM 在低资源语言任务中表现较差，但非核心话题。\n\n### 机器人与规划（技术创新突出）\n- **Beyond Task and Motion Planning（中文）：使用通用策略的层次机器人规划**  \n  英文：Beyond Task and Motion Planning: Hierarchical Robot Planning with General-Purpose Policies  \n  主要贡献：整合运动规划和闭环控制器，实现复杂任务规划，提高机器人适应性。\n  \n- **Integrating Learning-Based Manipulation and Physics-Based Locomotion（中文）：用于全身体育机器人的学习与物理整合**  \n  英文：Integrating Learning-Based Manipulation and Physics-Based Locomotion for Whole-Body Badminton Robot Control  \n  主要贡献：提出混合控制系统，结合学习和物理建模，实现机器人打羽毛球成功率达 94.5%，易扩展到其他任务。\n  \n  快速掠过：如 Fuzzy-RRT（中文）：用于外科机器人避障的模糊 RRT 算法，英文：Fuzzy-RRT for Obstacle Avoidance in a 2-DOF Semi-Autonomous Surgical Robotic Arm。主要发现：改进路径搜索时间 743%，但应用较窄。\n\n### 其他领域（快速掠过，选重点）\n- **Toward Personalizing Quantum Computing Education（中文）：使用 LLM 的量子计算教育个性化**  \n  英文：Toward Personalizing Quantum Computing Education: An Evolutionary LLM-Powered Approach  \n  主要贡献：开发知识图谱增强的 LLM 代理系统，动态适应学生需求，缓解 LLM 幻觉问题。\n  \n- **Flow Matching Ergodic Coverage（中文）：基于流匹配的机器人覆盖优化**  \n  英文：Flow Matching Ergodic Coverage  \n  主要贡献：使用流匹配改进机器人轨迹规划，提升覆盖性能，但整体影响力中等。\n  \n- **其他论文快速概述**：如 Propositional Measure Logic（中文）：命题测度逻辑，英文：Propositional Measure Logic。主要发现：提出不确定性推理框架；Federated Learning（中文）：联邦学习的隐私调查，英文：Federated Learning: A Survey on Privacy-Preserving Collaborative Intelligence。主要贡献：综述隐私机制，但较泛；Decentralized Time Series Classification（中文）：去中心化时序分类，英文：Decentralized Time Series Classification with ROCKET Features。主要发现：提升分类鲁棒性。这些论文虽有价值，但非核心主题，故简要提及。\n\n今日 arXiv 论文总体质量高，AI 应用多样化，但 LLM 安全和医疗领域的创新最值得关注。未来几天，建议读者关注这些方向的后续发展，以捕捉潜在突破。明日见！",
  "papers": [
    {
      "arxiv_id": "2504.17979v1",
      "title": "Fuzzy-RRT for Obstacle Avoidance in a 2-DOF Semi-Autonomous Surgical Robotic Arm",
      "title_zh": "翻译失败",
      "authors": [
        "Kaaustaaub Shankar",
        "Wilhelm Louw",
        "Bharadwaj Dogga",
        "Nick Ernest",
        "Tim Arnett",
        "Kelly Cohen"
      ],
      "abstract": "AI-driven semi-autonomous robotic surgery is essential for addressing the\nmedical challenges of long-duration interplanetary missions, where limited crew\nsizes and communication delays restrict traditional surgical approaches.\nCurrent robotic surgery systems require full surgeon control, demanding\nextensive expertise and limiting feasibility in space. We propose a novel\nadaptation of the Fuzzy Rapidly-exploring Random Tree algorithm for obstacle\navoidance and collaborative control in a two-degree-of-freedom robotic arm\nmodeled on the Miniaturized Robotic-Assisted surgical system. It was found that\nthe Fuzzy Rapidly-exploring Random Tree algorithm resulted in an 743 percent\nimprovement to path search time and 43 percent improvement to path cost.",
      "tldr_zh": "该研究针对长期星际任务中的手术挑战，提出了一种基于 Fuzzy-RRT 算法的障碍避免方法，应用于一个 2-DOF 半自主手术机器人臂，以减少对外科医生完全控制的依赖。算法改编自 Rapidly-exploring Random Tree，并结合模糊逻辑实现协作控制和路径优化。实验结果显示，Fuzzy-RRT 使路径搜索时间提高了 743%，路径成本降低了 43%，从而提升了太空手术的可行性。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "9 pages, 5 figures. Submitted to NAFIPS 2025 Conference (North\n  American Fuzzy Information Processing Society). Includes results on Fuzzy-RRT\n  performance in surgical robotics path planning",
      "pdf_url": "http://arxiv.org/pdf/2504.17979v1",
      "published_date": "2025-04-24 23:19:27 UTC",
      "updated_date": "2025-04-24 23:19:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:11:13.750116"
    },
    {
      "arxiv_id": "2504.17967v1",
      "title": "LLM Agent Swarm for Hypothesis-Driven Drug Discovery",
      "title_zh": "翻译失败",
      "authors": [
        "Kevin Song",
        "Andrew Trotter",
        "Jake Y. Chen"
      ],
      "abstract": "Drug discovery remains a formidable challenge: more than 90 percent of\ncandidate molecules fail in clinical evaluation, and development costs often\nexceed one billion dollars per approved therapy. Disparate data streams, from\ngenomics and transcriptomics to chemical libraries and clinical records, hinder\ncoherent mechanistic insight and slow progress. Meanwhile, large language\nmodels excel at reasoning and tool integration but lack the modular\nspecialization and iterative memory required for regulated, hypothesis-driven\nworkflows. We introduce PharmaSwarm, a unified multi-agent framework that\norchestrates specialized LLM \"agents\" to propose, validate, and refine\nhypotheses for novel drug targets and lead compounds. Each agent accesses\ndedicated functionality--automated genomic and expression analysis; a curated\nbiomedical knowledge graph; pathway enrichment and network simulation;\ninterpretable binding affinity prediction--while a central Evaluator LLM\ncontinuously ranks proposals by biological plausibility, novelty, in silico\nefficacy, and safety. A shared memory layer captures validated insights and\nfine-tunes underlying submodels over time, yielding a self-improving system.\nDeployable on low-code platforms or Kubernetes-based microservices, PharmaSwarm\nsupports literature-driven discovery, omics-guided target identification, and\nmarket-informed repurposing. We also describe a rigorous four-tier validation\npipeline spanning retrospective benchmarking, independent computational assays,\nexperimental testing, and expert user studies to ensure transparency,\nreproducibility, and real-world impact. By acting as an AI copilot, PharmaSwarm\ncan accelerate translational research and deliver high-confidence hypotheses\nmore efficiently than traditional pipelines.",
      "tldr_zh": "该研究提出PharmaSwarm，一种基于LLM的多代理框架，用于假设驱动的药物发现，以解决数据碎片化和传统流程低效的问题。该框架由专门的LLM代理组成，包括基因组分析、生物医学知识图、途径富集、网络模拟和结合亲和力预测功能，以及一个中央Evaluator LLM负责评估提案的生物学合理性、新颖性、在硅效能和安全性。系统通过共享内存层捕获验证见解并实现自我改进，支持文献驱动发现和目标识别，最终通过四层验证管道（如回顾性基准测试和实验验证）加速翻译研究，提供高效、高置信度的药物假设。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "15 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.17967v1",
      "published_date": "2025-04-24 22:27:50 UTC",
      "updated_date": "2025-04-24 22:27:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:10:31.841767"
    },
    {
      "arxiv_id": "2504.17964v1",
      "title": "Evaluating Machine Expertise: How Graduate Students Develop Frameworks for Assessing GenAI Content",
      "title_zh": "评估机器专业知识：研究生如何开发框架来评估 GenAI 内容",
      "authors": [
        "Celia Chen",
        "Alex Leitch"
      ],
      "abstract": "This paper examines how graduate students develop frameworks for evaluating\nmachine-generated expertise in web-based interactions with large language\nmodels (LLMs). Through a qualitative study combining surveys, LLM interaction\ntranscripts, and in-depth interviews with 14 graduate students, we identify\npatterns in how these emerging professionals assess and engage with\nAI-generated content. Our findings reveal that students construct evaluation\nframeworks shaped by three main factors: professional identity, verification\ncapabilities, and system navigation experience. Rather than uniformly accepting\nor rejecting LLM outputs, students protect domains central to their\nprofessional identities while delegating others--with managers preserving\nconceptual work, designers safeguarding creative processes, and programmers\nmaintaining control over core technical expertise. These evaluation frameworks\nare further influenced by students' ability to verify different types of\ncontent and their experience navigating complex systems. This research\ncontributes to web science by highlighting emerging human-genAI interaction\npatterns and suggesting how platforms might better support users in developing\neffective frameworks for evaluating machine-generated expertise signals in\nAI-mediated web environments.",
      "tldr_zh": "本研究通过对14名研究生的定性调查、LLM互动记录和深度访谈，探讨了他们如何开发框架来评估机器生成的专业知识（GenAI Content）。发现学生的评估框架主要受专业身份、验证能力和系统导航经验的影响，他们倾向于保护核心专业领域（如管理者保留概念工作、设计师维护创造过程、程序员掌控技术专长），同时选择性地委托其他领域。研究为网络科学贡献了人类-GenAI互动模式的新见解，并建议平台通过优化支持机制，帮助用户更有效地评估AI生成的专业知识信号。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Under review at ACM Web Science Conference 2025's Human-GenAI\n  Interactions Workshop, 4 pages",
      "pdf_url": "http://arxiv.org/pdf/2504.17964v1",
      "published_date": "2025-04-24 22:24:14 UTC",
      "updated_date": "2025-04-24 22:24:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:10:43.254486"
    },
    {
      "arxiv_id": "2504.18603v1",
      "title": "Toward Personalizing Quantum Computing Education: An Evolutionary LLM-Powered Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Iizalaarab Elhaimeur",
        "Nikos Chrisochoides"
      ],
      "abstract": "Quantum computing education faces significant challenges due to its\ncomplexity and the limitations of current tools; this paper introduces a novel\nIntelligent Teaching Assistant for quantum computing education and details its\nevolutionary design process. The system combines a knowledge-graph-augmented\narchitecture with two specialized Large Language Model (LLM) agents: a Teaching\nAgent for dynamic interaction, and a Lesson Planning Agent for lesson plan\ngeneration. The system is designed to adapt to individual student needs, with\ninteractions meticulously tracked and stored in a knowledge graph. This graph\nrepresents student actions, learning resources, and relationships, aiming to\nenable reasoning about effective learning pathways. We describe the\nimplementation of the system, highlighting the challenges encountered and the\nsolutions implemented, including introducing a dual-agent architecture where\ntasks are separated, all coordinated through a central knowledge graph that\nmaintains system awareness, and a user-facing tag system intended to mitigate\nLLM hallucination and improve user control. Preliminary results illustrate the\nsystem's potential to capture rich interaction data, dynamically adapt lesson\nplans based on student feedback via a tag system in simulation, and facilitate\ncontext-aware tutoring through the integrated knowledge graph, though\nsystematic evaluation is required.",
      "tldr_zh": "这篇论文针对量子计算教育的复杂性和工具限制，提出了一种基于 Large Language Model (LLM) 的个性化智能教学助手系统及其进化设计过程。系统采用知识图谱增强架构，结合两个专门的 LLM 代理——Teaching Agent 负责动态互动，以及 Lesson Planning Agent 负责生成适应性课程计划——通过中央知识图谱跟踪学生行为和学习路径，以实现个性化学习。初步结果表明，该系统能捕获丰富互动数据、动态调整课程计划并缓解 LLM 幻觉，但仍需进行系统性评估以验证其有效性。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.18603v1",
      "published_date": "2025-04-24 21:53:34 UTC",
      "updated_date": "2025-04-24 21:53:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:10:56.374607"
    },
    {
      "arxiv_id": "2504.17929v2",
      "title": "ApproXAI: Energy-Efficient Hardware Acceleration of Explainable AI using Approximate Computing",
      "title_zh": "翻译失败",
      "authors": [
        "Ayesha Siddique",
        "Khurram Khalil",
        "Khaza Anuarul Hoque"
      ],
      "abstract": "Explainable artificial intelligence (XAI) enhances AI system transparency by\nframing interpretability as an optimization problem. However, this approach\noften necessitates numerous iterations of computationally intensive operations,\nlimiting its applicability in real-time scenarios. While recent research has\nfocused on XAI hardware acceleration on FPGAs and TPU, these methods do not\nfully address energy efficiency in real-time settings. To address this\nlimitation, we propose XAIedge, a novel framework that leverages approximate\ncomputing techniques into XAI algorithms, including integrated gradients, model\ndistillation, and Shapley analysis. XAIedge translates these algorithms into\napproximate matrix computations and exploits the synergy between convolution,\nFourier transform, and approximate computing paradigms. This approach enables\nefficient hardware acceleration on TPU-based edge devices, facilitating faster\nreal-time outcome interpretations. Our comprehensive evaluation demonstrates\nthat XAIedge achieves a $2\\times$ improvement in energy efficiency compared to\nexisting accurate XAI hardware acceleration techniques while maintaining\ncomparable accuracy. These results highlight the potential of XAIedge to\nsignificantly advance the deployment of explainable AI in energy-constrained\nreal-time applications.",
      "tldr_zh": "该研究针对可解释AI（XAI）的计算密集问题，提出ApproXAI框架，利用Approximate Computing技术优化XAI算法，包括integrated gradients、model distillation和Shapley analysis。框架将这些算法转化为近似矩阵计算，并结合卷积和Fourier transform的协同作用，在TPU-based edge devices上实现高效硬件加速，以支持实时解释。实验结果显示，ApproXAI比现有精确XAI硬件加速技术能量效率提高2倍，同时保持可比准确性，为XAI在能量受限实时应用中的部署提供了重要潜力。",
      "categories": [
        "cs.AI",
        "cs.AR"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at the International Joint Conference on Neural Networks\n  (IJCNN), June 30th - July 5th, 2025 in Rome, Italy",
      "pdf_url": "http://arxiv.org/pdf/2504.17929v2",
      "published_date": "2025-04-24 20:40:29 UTC",
      "updated_date": "2025-05-12 16:04:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:11:08.904329"
    },
    {
      "arxiv_id": "2504.20074v1",
      "title": "EPSILON: Adaptive Fault Mitigation in Approximate Deep Neural Network using Statistical Signatures",
      "title_zh": "翻译失败",
      "authors": [
        "Khurram Khalil",
        "Khaza Anuarul Hoque"
      ],
      "abstract": "The increasing adoption of approximate computing in deep neural network\naccelerators (AxDNNs) promises significant energy efficiency gains. However,\npermanent faults in AxDNNs can severely degrade their performance compared to\ntheir accurate counterparts (AccDNNs). Traditional fault detection and\nmitigation approaches, while effective for AccDNNs, introduce substantial\noverhead and latency, making them impractical for energy-constrained real-time\ndeployment. To address this, we introduce EPSILON, a lightweight framework that\nleverages pre-computed statistical signatures and layer-wise importance metrics\nfor efficient fault detection and mitigation in AxDNNs. Our framework\nintroduces a novel non-parametric pattern-matching algorithm that enables\nconstant-time fault detection without interrupting normal execution while\ndynamically adapting to different network architectures and fault patterns.\nEPSILON maintains model accuracy by intelligently adjusting mitigation\nstrategies based on a statistical analysis of weight distribution and layer\ncriticality while preserving the energy benefits of approximate computing.\nExtensive evaluations across various approximate multipliers, AxDNN\narchitectures, popular datasets (MNIST, CIFAR-10, CIFAR-100, ImageNet-1k), and\nfault scenarios demonstrate that EPSILON maintains 80.05\\% accuracy while\noffering 22\\% improvement in inference time and 28\\% improvement in energy\nefficiency, establishing EPSILON as a practical solution for deploying reliable\nAxDNNs in safety-critical edge applications.",
      "tldr_zh": "该论文提出 EPSILON 框架，用于在近似深度神经网络 (AxDNNs) 中实现自适应故障检测和缓解，以应对永久故障对性能的负面影响，同时避免传统方法带来的高开销和延迟。EPSILON 利用预计算的统计签名、层级重要性指标和一种新颖的非参数模式匹配算法，实现恒定时间故障检测，并根据权重分布和层关键性动态调整缓解策略，以保持模型准确性并保留近似计算的能源效益。实验结果显示，在多种 AxDNN 架构、数据集 (如 MNIST、CIFAR-10、CIFAR-100 和 ImageNet-1k) 和故障场景下，EPSILON 维持 80.05% 准确率，同时提升 22% 推理时间和 28% 能源效率，为安全关键边缘应用提供可靠解决方案。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.DC",
      "comment": "Accepted at the International Joint Conference on Neural Networks\n  (IJCNN), June 30th - July 5th, 2025 in Rome, Italy",
      "pdf_url": "http://arxiv.org/pdf/2504.20074v1",
      "published_date": "2025-04-24 20:37:37 UTC",
      "updated_date": "2025-04-24 20:37:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:11:27.053672"
    },
    {
      "arxiv_id": "2504.17921v1",
      "title": "Avoiding Leakage Poisoning: Concept Interventions Under Distribution Shifts",
      "title_zh": "避免泄漏污染：分布偏移下的",
      "authors": [
        "Mateo Espinosa Zarlenga",
        "Gabriele Dominici",
        "Pietro Barbiero",
        "Zohreh Shams",
        "Mateja Jamnik"
      ],
      "abstract": "In this paper, we investigate how concept-based models (CMs) respond to\nout-of-distribution (OOD) inputs. CMs are interpretable neural architectures\nthat first predict a set of high-level concepts (e.g., stripes, black) and then\npredict a task label from those concepts. In particular, we study the impact of\nconcept interventions (i.e., operations where a human expert corrects a CM's\nmispredicted concepts at test time) on CMs' task predictions when inputs are\nOOD. Our analysis reveals a weakness in current state-of-the-art CMs, which we\nterm leakage poisoning, that prevents them from properly improving their\naccuracy when intervened on for OOD inputs. To address this, we introduce\nMixCEM, a new CM that learns to dynamically exploit leaked information missing\nfrom its concepts only when this information is in-distribution. Our results\nacross tasks with and without complete sets of concept annotations demonstrate\nthat MixCEMs outperform strong baselines by significantly improving their\naccuracy for both in-distribution and OOD samples in the presence and absence\nof concept interventions.",
      "tldr_zh": "本研究探讨了概念基于模型 (CMs) 在处理分布外 (OOD) 输入时的表现，特别关注概念干预（即专家在测试时修正模型的错误概念预测）对任务预测的影响。现有 CMs 存在“leakage poisoning”弱点，导致在 OOD 输入下无法有效提升准确率。为解决此问题，作者提出 MixCEM，一种新模型，能够动态利用概念中缺失的泄漏信息，但仅限于分布内数据。实验结果显示，MixCEM 在有无完整概念标注的各种任务中，显著优于基线模型，提高了分布内和 OOD 样本的准确率。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.HC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.17921v1",
      "published_date": "2025-04-24 20:24:31 UTC",
      "updated_date": "2025-04-24 20:24:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:11:37.901045"
    },
    {
      "arxiv_id": "2504.18601v1",
      "title": "The Philosophic Turn for AI Agents: Replacing centralized digital rhetoric with decentralized truth-seeking",
      "title_zh": "翻译失败",
      "authors": [
        "Philipp Koralus"
      ],
      "abstract": "In the face of rapidly advancing AI technology, individuals will increasingly\nrely on AI agents to navigate life's growing complexities, raising critical\nconcerns about maintaining both human agency and autonomy. This paper addresses\na fundamental dilemma posed by AI decision-support systems: the risk of either\nbecoming overwhelmed by complex decisions, thus losing agency, or having\nautonomy compromised by externally controlled choice architectures reminiscent\nof ``nudging'' practices. While the ``nudge'' framework, based on the use of\nchoice-framing to guide individuals toward presumed beneficial outcomes,\ninitially appeared to preserve liberty, at AI-driven scale, it threatens to\nerode autonomy. To counteract this risk, the paper proposes a philosophic turn\nin AI design. AI should be constructed to facilitate decentralized\ntruth-seeking and open-ended inquiry, mirroring the Socratic method of\nphilosophical dialogue. By promoting individual and collective adaptive\nlearning, such AI systems would empower users to maintain control over their\njudgments, augmenting their agency without undermining autonomy. The paper\nconcludes by outlining essential features for autonomy-preserving AI systems,\nsketching a path toward AI systems that enhance human judgment rather than\nundermine it.",
      "tldr_zh": "本文探讨了 AI 代理在帮助人们应对复杂决策时，可能导致人类代理性（agency）和自治性（autonomy）受损的风险，特别是通过 \"nudge\" 框架的外部操控。论文提出一种哲学转向，建议将 AI 设计为促进分散化的真理寻求和开放式探究，类似于 Socratic method 的对话方式，以支持个体和集体的适应性学习。最终，该方法旨在增强用户对判断的控制力，确保 AI 系统强化而非削弱人类自治性，并概述了构建此类系统的关键特征。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.18601v1",
      "published_date": "2025-04-24 19:34:43 UTC",
      "updated_date": "2025-04-24 19:34:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:11:49.037863"
    },
    {
      "arxiv_id": "2504.17901v1",
      "title": "Beyond Task and Motion Planning: Hierarchical Robot Planning with General-Purpose Policies",
      "title_zh": "超越",
      "authors": [
        "Benned Hedegaard",
        "Ziyi Yang",
        "Yichen Wei",
        "Ahmed Jaafar",
        "Stefanie Tellex",
        "George Konidaris",
        "Naman Shah"
      ],
      "abstract": "Task and motion planning is a well-established approach for solving\nlong-horizon robot planning problems. However, traditional methods assume that\neach task-level robot action, or skill, can be reduced to kinematic motion\nplanning. In this work, we address the challenge of planning with both\nkinematic skills and closed-loop motor controllers that go beyond kinematic\nconsiderations. We propose a novel method that integrates these controllers\ninto motion planning using Composable Interaction Primitives (CIPs), enabling\nthe use of diverse, non-composable pre-learned skills in hierarchical robot\nplanning. Toward validating our Task and Skill Planning (TASP) approach, we\ndescribe ongoing robot experiments in real-world scenarios designed to\ndemonstrate how CIPs can allow a mobile manipulator robot to effectively\ncombine motion planning with general-purpose skills to accomplish complex\ntasks.",
      "tldr_zh": "本文研究超越了传统的任务和运动规划（Task and Motion Planning），提出了一种新的Task and Skill Planning (TASP)方法，用于层次化机器人规划。该方法通过Composable Interaction Primitives (CIPs)将闭环电机控制器整合到运动规划中，允许机器人使用多样、非可组合的预学技能来处理超出运动学考虑的复杂任务。实验在真实世界场景中验证了TASP的效能，使移动机械臂机器人能够有效结合运动规划和通用技能，完成长时域任务。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.17901v1",
      "published_date": "2025-04-24 19:22:50 UTC",
      "updated_date": "2025-04-24 19:22:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:12:01.454197"
    },
    {
      "arxiv_id": "2504.17892v1",
      "title": "Token Sequence Compression for Efficient Multimodal Computing",
      "title_zh": "用于高效多模态计算的令牌序列压缩",
      "authors": [
        "Yasmine Omri",
        "Parth Shroff",
        "Thierry Tambe"
      ],
      "abstract": "The exponential growth of Large Multimodal Models (LMMs) has driven\nadvancements in cross-modal reasoning but at significant computational costs.\nIn this work, we focus on visual language models. We highlight the redundancy\nand inefficiency in current vision encoders, and seek to construct an adaptive\ncompression method for multimodal data. In this work, we characterize a panoply\nof visual token selection and merging approaches through both benchmarking and\nqualitative analysis. In particular, we demonstrate that simple cluster-level\ntoken aggregation outperforms prior state-of-the-art works in token selection\nand merging, including merging at the vision encoder level and attention-based\napproaches. We underline the redundancy in current vision encoders, and shed\nlight on several puzzling trends regarding principles of visual token selection\nthrough cross-modal attention visualizations. This work is a first effort\ntowards more effective encoding and processing of high-dimensional data, and\npaves the way for more scalable and sustainable multimodal systems.",
      "tldr_zh": "本文探讨了 Large Multimodal Models (LMMs) 的快速增长带来的计算成本问题，特别针对视觉语言模型的视觉编码器冗余和低效问题。研究提出了一种自适应压缩方法，包括各种视觉标记选择和合并策略，并通过基准测试和定性分析证明，简单的集群级标记聚合优于现有最先进方法，如视觉编码器级合并和基于注意力的方法。最终，该工作揭示了视觉编码器的冗余原理，并为更高效、可扩展的多模态系统铺平道路。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.17892v1",
      "published_date": "2025-04-24 19:11:10 UTC",
      "updated_date": "2025-04-24 19:11:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:12:14.481316"
    },
    {
      "arxiv_id": "2504.19940v1",
      "title": "Assessing the Potential of Generative Agents in Crowdsourced Fact-Checking",
      "title_zh": "评估生成式代理在众包事实核查中的潜力",
      "authors": [
        "Luigia Costabile",
        "Gian Marco Orlando",
        "Valerio La Gatta",
        "Vincenzo Moscato"
      ],
      "abstract": "The growing spread of online misinformation has created an urgent need for\nscalable, reliable fact-checking solutions. Crowdsourced fact-checking - where\nnon-experts evaluate claim veracity - offers a cost-effective alternative to\nexpert verification, despite concerns about variability in quality and bias.\nEncouraged by promising results in certain contexts, major platforms such as X\n(formerly Twitter), Facebook, and Instagram have begun shifting from\ncentralized moderation to decentralized, crowd-based approaches.\n  In parallel, advances in Large Language Models (LLMs) have shown strong\nperformance across core fact-checking tasks, including claim detection and\nevidence evaluation. However, their potential role in crowdsourced workflows\nremains unexplored. This paper investigates whether LLM-powered generative\nagents - autonomous entities that emulate human behavior and decision-making -\ncan meaningfully contribute to fact-checking tasks traditionally reserved for\nhuman crowds. Using the protocol of La Barbera et al. (2024), we simulate\ncrowds of generative agents with diverse demographic and ideological profiles.\nAgents retrieve evidence, assess claims along multiple quality dimensions, and\nissue final veracity judgments.\n  Our results show that agent crowds outperform human crowds in truthfulness\nclassification, exhibit higher internal consistency, and show reduced\nsusceptibility to social and cognitive biases. Compared to humans, agents rely\nmore systematically on informative criteria such as Accuracy, Precision, and\nInformativeness, suggesting a more structured decision-making process. Overall,\nour findings highlight the potential of generative agents as scalable,\nconsistent, and less biased contributors to crowd-based fact-checking systems.",
      "tldr_zh": "这篇论文评估了生成代理（generative agents）在众包事实检查（crowdsourced fact-checking）中的潜力，以应对在线虚假信息传播问题。研究者使用 Large Language Models (LLMs) 驱动的代理模拟多样化人群，模拟过程包括检索证据、评估声明的质量维度（如 Accuracy, Precision 和 Informativeness）并给出真实性判断。结果表明，代理人群在真实性分类中优于人类，表现出更高的内部一致性、较低的社会和认知偏见，并提供更结构化的决策过程，突显了生成代理作为可扩展且可靠的事实检查工具的潜力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.19940v1",
      "published_date": "2025-04-24 18:49:55 UTC",
      "updated_date": "2025-04-24 18:49:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:12:26.652144"
    },
    {
      "arxiv_id": "2504.18600v1",
      "title": "QuantBench: Benchmarking AI Methods for Quantitative Investment",
      "title_zh": "QuantBench: 量化投资AI方法的基准测试",
      "authors": [
        "Saizhuo Wang",
        "Hao Kong",
        "Jiadong Guo",
        "Fengrui Hua",
        "Yiyan Qi",
        "Wanyun Zhou",
        "Jiahao Zheng",
        "Xinyu Wang",
        "Lionel M. Ni",
        "Jian Guo"
      ],
      "abstract": "The field of artificial intelligence (AI) in quantitative investment has seen\nsignificant advancements, yet it lacks a standardized benchmark aligned with\nindustry practices. This gap hinders research progress and limits the practical\napplication of academic innovations. We present QuantBench, an industrial-grade\nbenchmark platform designed to address this critical need. QuantBench offers\nthree key strengths: (1) standardization that aligns with quantitative\ninvestment industry practices, (2) flexibility to integrate various AI\nalgorithms, and (3) full-pipeline coverage of the entire quantitative\ninvestment process. Our empirical studies using QuantBench reveal some critical\nresearch directions, including the need for continual learning to address\ndistribution shifts, improved methods for modeling relational financial data,\nand more robust approaches to mitigate overfitting in low signal-to-noise\nenvironments. By providing a common ground for evaluation and fostering\ncollaboration between researchers and practitioners, QuantBench aims to\naccelerate progress in AI for quantitative investment, similar to the impact of\nbenchmark platforms in computer vision and natural language processing.",
      "tldr_zh": "该论文指出，AI 在量化投资领域的进展受限于缺乏标准化基准，影响了研究和实际应用。为此，研究团队提出了 QuantBench，这是一个工业级基准平台，具有标准化（与行业实践一致）、灵活性（整合各种 AI 算法）和全流程覆盖（量化投资过程）的关键优势。通过实证研究，QuantBench 揭示了重要研究方向，包括 continual learning 以应对分布偏移、改进 modeling relational financial data 的方法，以及更 robust 的策略来缓解 overfitting。在提供统一评估框架的同时，该平台旨在促进研究者和从业者合作，推动 AI 在量化投资领域的快速发展。",
      "categories": [
        "q-fin.CP",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "q-fin.CP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.18600v1",
      "published_date": "2025-04-24 18:47:22 UTC",
      "updated_date": "2025-04-24 18:47:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:12:38.059312"
    },
    {
      "arxiv_id": "2504.17878v1",
      "title": "Crypto-ncRNA: Non-coding RNA (ncRNA) Based Encryption Algorithm",
      "title_zh": "Crypto-ncRNA：基于非编码 RNA (ncRNA) 的加密算法",
      "authors": [
        "Xu Wang",
        "Yiquan Wang",
        "Tin-yeh Huang"
      ],
      "abstract": "In the looming post-quantum era, traditional cryptographic systems are\nincreasingly vulnerable to quantum computing attacks that can compromise their\nmathematical foundations. To address this critical challenge, we propose\ncrypto-ncRNA-a bio-convergent cryptographic framework that leverages the\ndynamic folding properties of non-coding RNA (ncRNA) to generate high-entropy,\nquantum-resistant keys and produce unpredictable ciphertexts. The framework\nemploys a novel, multi-stage process: encoding plaintext into RNA sequences,\npredicting and manipulating RNA secondary structures using advanced algorithms,\nand deriving cryptographic keys through the intrinsic physical unclonability of\nRNA molecules. Experimental evaluations indicate that, although crypto-ncRNA's\nencryption speed is marginally lower than that of AES, it significantly\noutperforms RSA in terms of efficiency and scalability while achieving a 100%\npass rate on the NIST SP 800-22 randomness tests. These results demonstrate\nthat crypto-ncRNA offers a promising and robust approach for securing digital\ninfrastructures against the evolving threats posed by quantum computing.",
      "tldr_zh": "本研究提出crypto-ncRNA，一种基于非编码RNA (ncRNA) 的生物融合加密框架，旨在应对量子计算对传统加密系统的威胁，通过利用ncRNA的动态折叠特性生成高熵量子抵抗密钥和不可预测的密文。框架采用多阶段过程，包括将明文编码为RNA序列、预测并操纵RNA二级结构，以及通过RNA的物理不可克隆性衍生密钥。实验结果显示，crypto-ncRNA的加密速度虽略逊于AES，但比RSA更高效且可扩展，并100%通过NIST SP 800-22随机性测试，展示了其在量子时代保护数字基础设施的潜力。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted at the AI4NA workshop at ICLR 2025. 18pages, 4figures",
      "pdf_url": "http://arxiv.org/pdf/2504.17878v1",
      "published_date": "2025-04-24 18:30:35 UTC",
      "updated_date": "2025-04-24 18:30:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:12:49.341039"
    },
    {
      "arxiv_id": "2504.17872v1",
      "title": "Flow Matching Ergodic Coverage",
      "title_zh": "流匹配遍历覆盖",
      "authors": [
        "Max Muchen Sun",
        "Allison Pinosky",
        "Todd Murphey"
      ],
      "abstract": "Ergodic coverage effectively generates exploratory behaviors for embodied\nagents by aligning the spatial distribution of the agent's trajectory with a\ntarget distribution, where the difference between these two distributions is\nmeasured by the ergodic metric. However, existing ergodic coverage methods are\nconstrained by the limited set of ergodic metrics available for control\nsynthesis, fundamentally limiting their performance. In this work, we propose\nan alternative approach to ergodic coverage based on flow matching, a technique\nwidely used in generative inference for efficient and scalable sampling. We\nformally derive the flow matching problem for ergodic coverage and show that it\nis equivalent to a linear quadratic regulator problem with a closed-form\nsolution. Our formulation enables alternative ergodic metrics from generative\ninference that overcome the limitations of existing ones. These metrics were\npreviously infeasible for control synthesis but can now be supported with no\ncomputational overhead. Specifically, flow matching with the Stein variational\ngradient flow enables control synthesis directly over the score function of the\ntarget distribution, improving robustness to the unnormalized distributions; on\nthe other hand, flow matching with the Sinkhorn divergence flow enables an\noptimal transport-based ergodic metric, improving coverage performance on\nnon-smooth distributions with irregular supports. We validate the improved\nperformance and competitive computational efficiency of our method through\ncomprehensive numerical benchmarks and across different nonlinear dynamics. We\nfurther demonstrate the practicality of our method through a series of drawing\nand erasing tasks on a Franka robot.",
      "tldr_zh": "这篇论文针对传统的 ergodic coverage 方法受限于可用度量的问题，提出了一种基于 flow matching 的新方法，用于生成代理的探索行为。该方法将 ergodic coverage 问题形式化为一个有闭式解的 linear quadratic regulator (LQR) 问题，支持来自生成推理的替代 ergodic metrics，如 Stein variational gradient flow 和 Sinkhorn divergence flow，从而提升鲁棒性和覆盖性能。Stein variational gradient flow 能直接在目标分布的 score function 上进行控制合成，提高了对未归一化分布的鲁棒性，而 Sinkhorn divergence flow 则实现了基于最优传输的度量，优化了非平滑分布的处理。通过数值基准测试和 Franka 机器人上的绘图及擦除任务，实验证明该方法在性能和计算效率上均有显著提升。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "15 pages, 15 figures. Accepted to Robotics: Science and Systems (RSS)\n  2025. Project website: https://murpheylab.github.io/lqr-flow-matching/",
      "pdf_url": "http://arxiv.org/pdf/2504.17872v1",
      "published_date": "2025-04-24 18:18:35 UTC",
      "updated_date": "2025-04-24 18:18:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:13:04.539065"
    },
    {
      "arxiv_id": "2504.20073v1",
      "title": "RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Zihan Wang",
        "Kangrui Wang",
        "Qineng Wang",
        "Pingyue Zhang",
        "Linjie Li",
        "Zhengyuan Yang",
        "Kefan Yu",
        "Minh Nhat Nguyen",
        "Licheng Liu",
        "Eli Gottlieb",
        "Monica Lam",
        "Yiping Lu",
        "Kyunghyun Cho",
        "Jiajun Wu",
        "Li Fei-Fei",
        "Lijuan Wang",
        "Yejin Choi",
        "Manling Li"
      ],
      "abstract": "Training large language models (LLMs) as interactive agents presents unique\nchallenges including long-horizon decision making and interacting with\nstochastic environment feedback. While reinforcement learning (RL) has enabled\nprogress in static tasks, multi-turn agent RL training remains underexplored.\nWe propose StarPO (State-Thinking-Actions-Reward Policy Optimization), a\ngeneral framework for trajectory-level agent RL, and introduce RAGEN, a modular\nsystem for training and evaluating LLM agents. Our study on three stylized\nenvironments reveals three core findings. First, our agent RL training shows a\nrecurring mode of Echo Trap where reward variance cliffs and gradient spikes;\nwe address this with StarPO-S, a stabilized variant with trajectory filtering,\ncritic incorporation, and decoupled clipping. Second, we find the shaping of RL\nrollouts would benefit from diverse initial states, medium interaction\ngranularity and more frequent sampling. Third, we show that without\nfine-grained, reasoning-aware reward signals, agent reasoning hardly emerge\nthrough multi-turn RL and they may show shallow strategies or hallucinated\nthoughts. Code and environments are available at\nhttps://github.com/RAGEN-AI/RAGEN.",
      "tldr_zh": "这篇论文提出了 RAGEN 系统和 StarPO 框架，用于理解大型语言模型 (LLMs) 代理在多轮强化学习 (RL) 中的自我演化，针对长时决策和随机环境交互的挑战。StarPO 是一个轨迹级代理 RL 框架，而其稳定版本 StarPO-S 通过轨迹过滤、批评者整合和解耦剪切解决了 Echo Trap 问题，该问题会导致奖励方差悬崖和梯度峰值。研究在三个模拟环境中发现，RL 回合的优化受益于多样初始状态、中等交互粒度和更频繁采样，且缺乏细粒度的推理aware奖励信号会阻碍代理的推理发展，可能导致浅显策略或幻觉思考。总的来说，这为 LLM 代理训练提供了关键见解和改进方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.20073v1",
      "published_date": "2025-04-24 17:57:08 UTC",
      "updated_date": "2025-04-24 17:57:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:13:15.143892"
    },
    {
      "arxiv_id": "2504.17838v1",
      "title": "CaRL: Learning Scalable Planning Policies with Simple Rewards",
      "title_zh": "CaRL：通过简单奖励学习可扩展的规划策略",
      "authors": [
        "Bernhard Jaeger",
        "Daniel Dauner",
        "Jens Beißwenger",
        "Simon Gerstenecker",
        "Kashyap Chitta",
        "Andreas Geiger"
      ],
      "abstract": "We investigate reinforcement learning (RL) for privileged planning in\nautonomous driving. State-of-the-art approaches for this task are rule-based,\nbut these methods do not scale to the long tail. RL, on the other hand, is\nscalable and does not suffer from compounding errors like imitation learning.\nContemporary RL approaches for driving use complex shaped rewards that sum\nmultiple individual rewards, \\eg~progress, position, or orientation rewards. We\nshow that PPO fails to optimize a popular version of these rewards when the\nmini-batch size is increased, which limits the scalability of these approaches.\nInstead, we propose a new reward design based primarily on optimizing a single\nintuitive reward term: route completion. Infractions are penalized by\nterminating the episode or multiplicatively reducing route completion. We find\nthat PPO scales well with higher mini-batch sizes when trained with our simple\nreward, even improving performance. Training with large mini-batch sizes\nenables efficient scaling via distributed data parallelism. We scale PPO to\n300M samples in CARLA and 500M samples in nuPlan with a single 8-GPU node. The\nresulting model achieves 64 DS on the CARLA longest6 v2 benchmark,\noutperforming other RL methods with more complex rewards by a large margin.\nRequiring only minimal adaptations from its use in CARLA, the same method is\nthe best learning-based approach on nuPlan. It scores 91.3 in non-reactive and\n90.6 in reactive traffic on the Val14 benchmark while being an order of\nmagnitude faster than prior work.",
      "tldr_zh": "本研究提出CaRL框架，使用强化学习（RL）来实现可扩展的自动驾驶规划策略，取代传统基于规则的方法，以应对长尾问题。不同于现有RL方法依赖复杂奖励函数，CaRL采用简单奖励设计，主要优化单一直观指标——路线完成，并通过终止剧集或乘法惩罚违规行为，使PPO算法在更大批量大小下训练更高效。实验结果显示，在CARLA数据集上训练至3亿样本，CaRL在longest6 v2基准上达到64 DS的性能，大幅优于其他RL方法；在nuPlan数据集上训练至5亿样本，它在Val14基准上分别获得91.3（非反应性交通）和90.6（反应性交通）的分数，同时训练速度比先前工作快一个数量级。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.17838v1",
      "published_date": "2025-04-24 17:56:01 UTC",
      "updated_date": "2025-04-24 17:56:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:13:26.448206"
    },
    {
      "arxiv_id": "2504.17771v2",
      "title": "Integrating Learning-Based Manipulation and Physics-Based Locomotion for Whole-Body Badminton Robot Control",
      "title_zh": "整合基于学习的操控和基于物理的运动用于全身羽毛球机器人",
      "authors": [
        "Haochen Wang",
        "Zhiwei Shi",
        "Chengxi Zhu",
        "Yafei Qiao",
        "Cheng Zhang",
        "Fan Yang",
        "Pengjie Ren",
        "Lan Lu",
        "Dong Xuan"
      ],
      "abstract": "Learning-based methods, such as imitation learning (IL) and reinforcement\nlearning (RL), can produce excel control policies over challenging agile robot\ntasks, such as sports robot. However, no existing work has harmonized\nlearning-based policy with model-based methods to reduce training complexity\nand ensure the safety and stability for agile badminton robot control. In this\npaper, we introduce Hamlet, a novel hybrid control system for agile badminton\nrobots. Specifically, we propose a model-based strategy for chassis locomotion\nwhich provides a base for arm policy. We introduce a physics-informed \"IL+RL\"\ntraining framework for learning-based arm policy. In this train framework, a\nmodel-based strategy with privileged information is used to guide arm policy\ntraining during both IL and RL phases. In addition, we train the critic model\nduring IL phase to alleviate the performance drop issue when transitioning from\nIL to RL. We present results on our self-engineered badminton robot, achieving\n94.5% success rate against the serving machine and 90.7% success rate against\nhuman players. Our system can be easily generalized to other agile mobile\nmanipulation tasks such as agile catching and table tennis. Our project\nwebsite: https://dreamstarring.github.io/HAMLET/.",
      "tldr_zh": "本研究提出Hamlet，一种新型混合控制系统，用于全身体羽毛球机器人的敏捷控制，旨在将学习-based方法（如imitation learning (IL)和reinforcement learning (RL)）与模型-based方法相结合，以降低训练复杂性并提升安全性和稳定性。具体来说，该系统采用模型-based策略控制底盘运动，并引入physics-informed的\"IL+RL\"训练框架来指导手臂策略的学习，同时训练critic模型以缓解从IL到RL阶段的性能下降问题。在实验中，Hamlet在自制羽毛球机器人上实现了94.5%的对抗发球机成功率和90.7%的对抗人类玩家成功率，并可泛化到其他敏捷移动操控任务，如敏捷捕捉和乒乓球。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted to ICRA 2025. Project page:\n  https://dreamstarring.github.io/HAMLET/",
      "pdf_url": "http://arxiv.org/pdf/2504.17771v2",
      "published_date": "2025-04-24 17:46:29 UTC",
      "updated_date": "2025-04-27 14:23:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:13:38.179324"
    },
    {
      "arxiv_id": "2505.00022v1",
      "title": "Aleph-Alpha-GermanWeb: Improving German-language LLM pre-training with model-based data curation and synthetic data generation",
      "title_zh": "翻译失败",
      "authors": [
        "Thomas F Burns",
        "Letitia Parcalabescu",
        "Stephan Wäldchen",
        "Michael Barlow",
        "Gregor Ziegltrum",
        "Volker Stampa",
        "Bastian Harren",
        "Björn Deiseroth"
      ],
      "abstract": "Scaling data quantity is essential for large language models (LLMs), yet\nrecent findings show that data quality can significantly boost performance and\ntraining efficiency. We introduce a German-language dataset curation pipeline\nthat combines heuristic and model-based filtering techniques with synthetic\ndata generation. We use our pipeline to create Aleph-Alpha-GermanWeb, a\nlarge-scale German pre-training dataset which draws from: (1) Common Crawl web\ndata, (2) FineWeb2, and (3) synthetically-generated data conditioned on actual,\norganic web data. We evaluate our dataset by pre-training both a 1B Llama-style\nmodel and an 8B tokenizer-free hierarchical autoregressive transformer (HAT). A\ncomparison on German-language benchmarks, including MMMLU, shows significant\nperformance gains of Aleph-Alpha-GermanWeb over FineWeb2 alone. This advantage\nholds at the 8B scale even when FineWeb2 is enriched by human-curated\nhigh-quality data sources such as Wikipedia. Our findings support the growing\nbody of evidence that model-based data curation and synthetic data generation\ncan significantly enhance LLM pre-training datasets.",
      "tldr_zh": "该研究提出了一种德语数据集整理管道，结合启发式过滤、模型-based数据整理和合成数据生成，创建了Aleph-Alpha-GermanWeb数据集，该数据集整合了Common Crawl、FineWeb2和基于实际网络数据的合成数据。研究通过预训练一个1B Llama-style模型和一个8B tokenizer-free hierarchical autoregressive transformer (HAT)来评估数据集，结果显示在德语基准测试如MMMLU上，Aleph-Alpha-GermanWeb显著优于FineWeb2，甚至其添加Wikipedia等高质量来源的版本。总体而言，这证实了模型-based数据整理和合成数据生成能显著提升LLM预训练的性能和效率。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2505.00022v1",
      "published_date": "2025-04-24 17:23:46 UTC",
      "updated_date": "2025-04-24 17:23:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:13:50.940018"
    },
    {
      "arxiv_id": "2504.17751v3",
      "title": "Revisiting Reset Mechanisms in Spiking Neural Networks for Sequential Modeling: Specialized Discretization for Binary Activated RNN",
      "title_zh": "翻译失败",
      "authors": [
        "Enqi Zhang"
      ],
      "abstract": "In the field of image recognition, spiking neural networks (SNNs) have\nachieved performance comparable to conventional artificial neural networks\n(ANNs). In such applications, SNNs essentially function as traditional neural\nnetworks with quantized activation values. This article focuses on an another\nalternative perspective,viewing SNNs as binary-activated recurrent neural\nnetworks (RNNs) for sequential modeling tasks. From this viewpoint, current SNN\narchitectures face several fundamental challenges in sequence modeling: (1)\nTraditional models lack effective memory mechanisms for long-range sequence\nmodeling; (2) The biological-inspired components in SNNs (such as reset\nmechanisms and refractory period applications) remain theoretically\nunder-explored for sequence tasks; (3) The RNN-like computational paradigm in\nSNNs prevents parallel training across different timesteps. To address these\nchallenges, this study conducts a systematic analysis of the fundamental\nmechanisms underlying reset operations and refractory periods in\nbinary-activated RNN-based SNN sequence models. We re-examine whether such\nbiological mechanisms are strictly necessary for generating sparse spiking\npatterns, provide new theoretical explanations and insights, and ultimately\npropose the fixed-refractory-period SNN architecture for sequence modeling.",
      "tldr_zh": "本研究从二元激活的 RNN 视角审视 Spiking Neural Networks (SNNs) 在序列建模中的应用，指出当前 SNNs 面临缺乏有效记忆机制、生物启发组件（如 reset mechanisms 和 refractory periods）未充分探索，以及无法实现时间步并行训练等挑战。通过系统分析 reset operations 和 refractory periods 的基础机制，该研究发现这些生物组件并非生成稀疏 spiking 模式所必需，并提供了新的理论解释。最终，提出 fixed-refractory-period SNN 架构，旨在提升 SNNs 在序列任务中的性能和效率。",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.17751v3",
      "published_date": "2025-04-24 17:09:59 UTC",
      "updated_date": "2025-04-29 18:43:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:14:02.845004"
    },
    {
      "arxiv_id": "2504.18598v2",
      "title": "BadMoE: Backdooring Mixture-of-Experts LLMs via Optimizing Routing Triggers and Infecting Dormant Experts",
      "title_zh": "翻译失败",
      "authors": [
        "Qingyue Wang",
        "Qi Pang",
        "Xixun Lin",
        "Shuai Wang",
        "Daoyuan Wu"
      ],
      "abstract": "Mixture-of-Experts (MoE) have emerged as a powerful architecture for large\nlanguage models (LLMs), enabling efficient scaling of model capacity while\nmaintaining manageable computational costs. The key advantage lies in their\nability to route different tokens to different ``expert'' networks within the\nmodel, enabling specialization and efficient handling of diverse input.\nHowever, the vulnerabilities of MoE-based LLMs still have barely been studied,\nand the potential for backdoor attacks in this context remains largely\nunexplored. This paper presents the first backdoor attack against MoE-based\nLLMs where the attackers poison ``dormant experts'' (i.e., underutilized\nexperts) and activate them by optimizing routing triggers, thereby gaining\ncontrol over the model's output. We first rigorously prove the existence of a\nfew ``dominating experts'' in MoE models, whose outputs can determine the\noverall MoE's output. We also show that dormant experts can serve as dominating\nexperts to manipulate model predictions. Accordingly, our attack, namely\nBadMoE, exploits the unique architecture of MoE models by 1) identifying\ndormant experts unrelated to the target task, 2) constructing a routing-aware\nloss to optimize the activation triggers of these experts, and 3) promoting\ndormant experts to dominating roles via poisoned training data. Extensive\nexperiments show that BadMoE successfully enforces malicious prediction on\nattackers' target tasks while preserving overall model utility, making it a\nmore potent and stealthy attack than existing methods.",
      "tldr_zh": "本论文首次提出 BadMoE，一种针对 Mixture-of-Experts (MoE) 模型的背门攻击方法，通过优化路由触发器和感染 dormant experts（未充分利用的专家），来操控大型语言模型（LLMs）的输出。攻击方法包括识别与目标任务无关的 dormant experts、构建路由感知损失来优化其激活触发器，以及利用毒化训练数据将这些专家提升为 dominating experts（主导专家），从而影响模型预测。实验结果显示，BadMoE 能够在攻击目标任务上强制恶意预测，同时保持模型整体效用，比现有方法更强大和隐蔽。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.18598v2",
      "published_date": "2025-04-24 16:42:38 UTC",
      "updated_date": "2025-04-29 02:23:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:14:14.416808"
    },
    {
      "arxiv_id": "2505.00021v1",
      "title": "Ustnlp16 at SemEval-2025 Task 9: Improving Model Performance through Imbalance Handling and Focal Loss",
      "title_zh": "翻译失败",
      "authors": [
        "Zhuoang Cai",
        "Zhenghao Li",
        "Yang Liu",
        "Liyuan Guo",
        "Yangqiu Song"
      ],
      "abstract": "Classification tasks often suffer from imbal- anced data distribution, which\npresents chal- lenges in food hazard detection due to severe class imbalances,\nshort and unstructured text, and overlapping semantic categories. In this\npaper, we present our system for SemEval- 2025 Task 9: Food Hazard Detection,\nwhich ad- dresses these issues by applying data augmenta- tion techniques to\nimprove classification perfor- mance. We utilize transformer-based models, BERT\nand RoBERTa, as backbone classifiers and explore various data balancing\nstrategies, including random oversampling, Easy Data Augmentation (EDA), and\nfocal loss. Our ex- periments show that EDA effectively mitigates class\nimbalance, leading to significant improve- ments in accuracy and F1 scores.\nFurthermore, combining focal loss with oversampling and EDA further enhances\nmodel robustness, par- ticularly for hard-to-classify examples. These findings\ncontribute to the development of more effective NLP-based classification models\nfor food hazard detection.",
      "tldr_zh": "本文针对SemEval-2025 Task 9食物危害检测任务，处理数据不平衡、文本短小无结构及语义重叠等问题，使用BERT和RoBERTa作为骨干分类器。研究者采用了多种策略，包括随机过采样、Easy Data Augmentation (EDA)和focal loss，以提升模型性能。实验结果显示，EDA显著提高了准确性和F1分数，而将focal loss与过采样和EDA结合，进一步增强了模型对难分类样本的鲁棒性。这些发现为基于NLP的食物危害检测分类模型提供了重要改进见解。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.00021v1",
      "published_date": "2025-04-24 16:35:44 UTC",
      "updated_date": "2025-04-24 16:35:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:14:26.673317"
    },
    {
      "arxiv_id": "2504.17721v1",
      "title": "Conformal Segmentation in Industrial Surface Defect Detection with Statistical Guarantees",
      "title_zh": "工业表面缺陷检测中的保形分割及其",
      "authors": [
        "Cheng Shen",
        "Yuewei Liu"
      ],
      "abstract": "In industrial settings, surface defects on steel can significantly compromise\nits service life and elevate potential safety risks. Traditional defect\ndetection methods predominantly rely on manual inspection, which suffers from\nlow efficiency and high costs. Although automated defect detection approaches\nbased on Convolutional Neural Networks(e.g., Mask R-CNN) have advanced rapidly,\ntheir reliability remains challenged due to data annotation uncertainties\nduring deep model training and overfitting issues. These limitations may lead\nto detection deviations when processing the given new test samples, rendering\nautomated detection processes unreliable. To address this challenge, we first\nevaluate the detection model's practical performance through calibration data\nthat satisfies the independent and identically distributed (i.i.d) condition\nwith test data. Specifically, we define a loss function for each calibration\nsample to quantify detection error rates, such as the complement of recall rate\nand false discovery rate. Subsequently, we derive a statistically rigorous\nthreshold based on a user-defined risk level to identify high-probability\ndefective pixels in test images, thereby constructing prediction sets (e.g.,\ndefect regions). This methodology ensures that the expected error rate (mean\nerror rate) on the test set remains strictly bounced by the predefined risk\nlevel. Additionally, we observe a negative correlation between the average\nprediction set size and the risk level on the test set, establishing a\nstatistically rigorous metric for assessing detection model uncertainty.\nFurthermore, our study demonstrates robust and efficient control over the\nexpected test set error rate across varying calibration-to-test partitioning\nratios, validating the method's adaptability and operational effectiveness.",
      "tldr_zh": "本研究针对工业表面缺陷检测中的可靠性问题，提出一种基于Conformal Segmentation的方法，以统计保证提升检测准确性。方法首先利用满足i.i.d条件的校准数据评估模型性能，通过定义损失函数（如recall rate补集和false discovery rate）来量化检测错误率，并基于用户定义的风险水平推导出阈值，以构建预测集（缺陷区域）。实验结果显示，该方法能严格控制测试集的预期错误率（mean error rate）在预定义风险水平内，并通过预测集大小与风险水平的负相关关系评估模型不确定性，在不同校准-测试比例下表现出鲁棒性和高效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Under Review",
      "pdf_url": "http://arxiv.org/pdf/2504.17721v1",
      "published_date": "2025-04-24 16:33:56 UTC",
      "updated_date": "2025-04-24 16:33:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:14:38.389023"
    },
    {
      "arxiv_id": "2504.17720v1",
      "title": "Multilingual Performance Biases of Large Language Models in Education",
      "title_zh": "翻译失败",
      "authors": [
        "Vansh Gupta",
        "Sankalan Pal Chowdhury",
        "Vilém Zouhar",
        "Donya Rooein",
        "Mrinmaya Sachan"
      ],
      "abstract": "Large language models (LLMs) are increasingly being adopted in educational\nsettings. These applications expand beyond English, though current LLMs remain\nprimarily English-centric. In this work, we ascertain if their use in education\nsettings in non-English languages is warranted. We evaluated the performance of\npopular LLMs on four educational tasks: identifying student misconceptions,\nproviding targeted feedback, interactive tutoring, and grading translations in\nsix languages (Hindi, Arabic, Farsi, Telugu, Ukrainian, Czech) in addition to\nEnglish. We find that the performance on these tasks somewhat corresponds to\nthe amount of language represented in training data, with lower-resource\nlanguages having poorer task performance. Although the models perform\nreasonably well in most languages, the frequent performance drop from English\nis significant. Thus, we recommend that practitioners first verify that the LLM\nworks well in the target language for their educational task before deployment.",
      "tldr_zh": "本研究评估了大型语言模型(LLMs)在教育领域的多语言性能偏差，测试了流行模型在七种语言（包括英语、Hindi、Arabic、Farsi、Telugu、Ukrainian和Czech）上的四个任务：识别学生误区、提供针对性反馈、互动辅导和评分翻译。结果显示，LLMs的性能与训练数据中的语言量密切相关，低资源语言表现较差，且与英语相比存在显著下降。尽管模型在大多数语言中表现尚可，但这种偏差可能影响实际应用。因此，建议教育从业者在部署LLMs前，先验证其在目标语言中的任务表现。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.17720v1",
      "published_date": "2025-04-24 16:32:31 UTC",
      "updated_date": "2025-04-24 16:32:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:14:50.281716"
    },
    {
      "arxiv_id": "2505.14693v1",
      "title": "Propositional Measure Logic",
      "title_zh": "翻译失败",
      "authors": [
        "Francisco Aragão"
      ],
      "abstract": "We present a propositional logic with fundamental probabilistic semantics, in\nwhich each formula is given a real measure in the interval $[0,1]$ that\nrepresents its degree of truth. This semantics replaces the binarity of\nclassical logic, while preserving its deductive structure. We demonstrate the\nsoundness theorem, establishing that the proposed system is sound and suitable\nfor reasoning under uncertainty. We discuss potential applications and avenues\nfor future extensions of the theory. We apply probabilistic logic to a still\nrefractory problem in Bayesian Networks.",
      "tldr_zh": "本论文提出了一种名为Propositional Measure Logic的命题逻辑系统，该系统采用基本的概率语义，每个公式被赋予[0,1]区间内的实数度量，以表示其真实度，从而取代了经典逻辑的二元真假结构，同时保留了其演绎框架。论文证明了健全性定理，证实该系统适用于不确定性下的推理，并讨论了潜在应用及未来扩展方向。具体而言，该逻辑已被应用于Bayesian Networks中的一个难题，提供了一种新的处理不确定性问题的方法。",
      "categories": [
        "cs.LO",
        "cs.AI",
        "Primary: 03B48, Secondary: 68T27, 60A99, 68T37"
      ],
      "primary_category": "cs.LO",
      "comment": "!0 pages",
      "pdf_url": "http://arxiv.org/pdf/2505.14693v1",
      "published_date": "2025-04-24 16:21:16 UTC",
      "updated_date": "2025-04-24 16:21:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:15:02.234532"
    },
    {
      "arxiv_id": "2504.17717v1",
      "title": "Early Detection of Multidrug Resistance Using Multivariate Time Series Analysis and Interpretable Patient-Similarity Representations",
      "title_zh": "翻译失败",
      "authors": [
        "Óscar Escudero-Arnanz",
        "Antonio G. Marques",
        "Inmaculada Mora-Jiménez",
        "Joaquín Álvarez-Rodríguez",
        "Cristina Soguero-Ruiz"
      ],
      "abstract": "Background and Objectives: Multidrug Resistance (MDR) is a critical global\nhealth issue, causing increased hospital stays, healthcare costs, and\nmortality. This study proposes an interpretable Machine Learning (ML) framework\nfor MDR prediction, aiming for both accurate inference and enhanced\nexplainability.\n  Methods: Patients are modeled as Multivariate Time Series (MTS), capturing\nclinical progression and patient-to-patient interactions. Similarity among\npatients is quantified using MTS-based methods: descriptive statistics, Dynamic\nTime Warping, and Time Cluster Kernel. These similarity measures serve as\ninputs for MDR classification via Logistic Regression, Random Forest, and\nSupport Vector Machines, with dimensionality reduction and kernel\ntransformations improving model performance. For explainability, patient\nsimilarity networks are constructed from these metrics. Spectral clustering and\nt-SNE are applied to identify MDR-related subgroups and visualize high-risk\nclusters, enabling insight into clinically relevant patterns.\n  Results: The framework was validated on ICU Electronic Health Records from\nthe University Hospital of Fuenlabrada, achieving an AUC of 81%. It outperforms\nbaseline ML and deep learning models by leveraging graph-based patient\nsimilarity. The approach identifies key risk factors -- prolonged antibiotic\nuse, invasive procedures, co-infections, and extended ICU stays -- and reveals\nclinically meaningful clusters. Code and results are available at\n\\https://github.com/oscarescuderoarnanz/DM4MTS.\n  Conclusions: Patient similarity representations combined with graph-based\nanalysis provide accurate MDR prediction and interpretable insights. This\nmethod supports early detection, risk factor identification, and patient\nstratification, highlighting the potential of explainable ML in critical care.",
      "tldr_zh": "本文提出一个可解释的机器学习框架，用于早期检测 Multidrug Resistance (MDR)，通过 Multivariate Time Series (MTS) 分析和患者相似性表示实现准确预测和增强解释性。方法包括使用描述性统计、Dynamic Time Warping 和 Time Cluster Kernel 量化患者间相似性，作为 Logistic Regression、Random Forest 和 Support Vector Machines 的输入，并结合降维、Spectral Clustering 和 t-SNE 构建患者相似性网络以识别高风险子群和临床模式。在西班牙大学医院的 ICU 电子健康记录上验证，该框架达到 81% 的 AUC，优于基线模型，并揭示关键风险因素如延长抗生素使用和侵入性程序，支持早期干预和患者分层。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.17717v1",
      "published_date": "2025-04-24 16:19:13 UTC",
      "updated_date": "2025-04-24 16:19:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:15:17.240153"
    },
    {
      "arxiv_id": "2504.17703v1",
      "title": "Federated Learning: A Survey on Privacy-Preserving Collaborative Intelligence",
      "title_zh": "联邦学习：隐私保护",
      "authors": [
        "Edward Collins",
        "Michel Wang"
      ],
      "abstract": "Federated Learning (FL) has emerged as a transformative paradigm in the field\nof distributed machine learning, enabling multiple clients such as mobile\ndevices, edge nodes, or organizations to collaboratively train a shared global\nmodel without the need to centralize sensitive data. This decentralized\napproach addresses growing concerns around data privacy, security, and\nregulatory compliance, making it particularly attractive in domains such as\nhealthcare, finance, and smart IoT systems. This survey provides a concise yet\ncomprehensive overview of Federated Learning, beginning with its core\narchitecture and communication protocol. We discuss the standard FL lifecycle,\nincluding local training, model aggregation, and global updates. A particular\nemphasis is placed on key technical challenges such as handling non-IID\n(non-independent and identically distributed) data, mitigating system and\nhardware heterogeneity, reducing communication overhead, and ensuring privacy\nthrough mechanisms like differential privacy and secure aggregation.\nFurthermore, we examine emerging trends in FL research, including personalized\nFL, cross-device versus cross-silo settings, and integration with other\nparadigms such as reinforcement learning and quantum computing. We also\nhighlight real-world applications and summarize benchmark datasets and\nevaluation metrics commonly used in FL research. Finally, we outline open\nresearch problems and future directions to guide the development of scalable,\nefficient, and trustworthy FL systems.",
      "tldr_zh": "本调查综述了Federated Learning (FL)，一种分布式机器学习范式，允许多个客户端（如移动设备或组织）在不集中敏感数据的情况下协作训练共享模型，从而提升数据隐私和合规性。论文详细阐述了FL的核心架构、通信协议和生命周期，包括本地训练、模型聚合和全局更新，同时讨论了关键挑战如处理non-IID数据、缓解系统异质性、减少通信开销，以及通过differential privacy和secure aggregation等机制确保隐私。最终，调查探讨了新兴趋势如personalized FL、跨设备/跨分区设置及其与reinforcement learning和quantum computing的整合，并总结了实际应用、基准数据集、评估指标以及未来研究方向，如构建可扩展、高效且可信的FL系统。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.17703v1",
      "published_date": "2025-04-24 16:10:29 UTC",
      "updated_date": "2025-04-24 16:10:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:15:26.577409"
    },
    {
      "arxiv_id": "2504.17696v3",
      "title": "Hierarchical and Multimodal Data for Daily Activity Understanding",
      "title_zh": "用于日常活动理解的层次化和多模态数据",
      "authors": [
        "Ghazal Kaviani",
        "Yavuz Yarici",
        "Seulgi Kim",
        "Mohit Prabhushankar",
        "Ghassan AlRegib",
        "Mashhour Solh",
        "Ameya Patil"
      ],
      "abstract": "Daily Activity Recordings for Artificial Intelligence (DARai, pronounced\n\"Dahr-ree\") is a multimodal, hierarchically annotated dataset constructed to\nunderstand human activities in real-world settings. DARai consists of\ncontinuous scripted and unscripted recordings of 50 participants in 10\ndifferent environments, totaling over 200 hours of data from 20 sensors\nincluding multiple camera views, depth and radar sensors, wearable inertial\nmeasurement units (IMUs), electromyography (EMG), insole pressure sensors,\nbiomonitor sensors, and gaze tracker.\n  To capture the complexity in human activities, DARai is annotated at three\nlevels of hierarchy: (i) high-level activities (L1) that are independent tasks,\n(ii) lower-level actions (L2) that are patterns shared between activities, and\n(iii) fine-grained procedures (L3) that detail the exact execution steps for\nactions. The dataset annotations and recordings are designed so that 22.7% of\nL2 actions are shared between L1 activities and 14.2% of L3 procedures are\nshared between L2 actions. The overlap and unscripted nature of DARai allows\ncounterfactual activities in the dataset.\n  Experiments with various machine learning models showcase the value of DARai\nin uncovering important challenges in human-centered applications.\nSpecifically, we conduct unimodal and multimodal sensor fusion experiments for\nrecognition, temporal localization, and future action anticipation across all\nhierarchical annotation levels. To highlight the limitations of individual\nsensors, we also conduct domain-variant experiments that are enabled by DARai's\nmulti-sensor and counterfactual activity design setup.\n  The code, documentation, and dataset are available at the dedicated DARai\nwebsite:\nhttps://alregib.ece.gatech.edu/software-and-datasets/darai-daily-activity-recordings-for-artificial-intelligence-and-machine-learning/",
      "tldr_zh": "本文提出 DARai 数据集，这是一个多模态且分层注解的数据集，旨在理解真实世界人类活动的复杂性。数据集包含 50 名参与者在 10 个环境中的超过 200 小时连续记录，涉及 20 个传感器，包括多个摄像头视图、深度和雷达传感器、可穿戴 IMU、EMG、压力传感器、生物监测传感器和注视跟踪器。注解分为三个层次：L1 高水平活动（独立任务）、L2 低水平动作（在活动间共享的模式）和 L3 细粒度程序（精确执行步骤），其中 22.7% 的 L2 动作和 14.2% 的 L3 程序存在共享，支持反事实活动设计。实验通过单模态和多模态传感器融合，评估了在活动识别、时间定位和未来动作预测等方面的性能，突显了数据集在揭示人类中心应用挑战的价值。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.17696v3",
      "published_date": "2025-04-24 16:04:00 UTC",
      "updated_date": "2025-05-13 16:36:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:15:39.839163"
    },
    {
      "arxiv_id": "2504.17685v1",
      "title": "Ensemble Bayesian Inference: Leveraging Small Language Models to Achieve LLM-level Accuracy in Profile Matching Tasks",
      "title_zh": "翻译失败",
      "authors": [
        "Haru-Tada Sato",
        "Fuka Matsuzaki",
        "Jun-ichiro Takahashi"
      ],
      "abstract": "This study explores the potential of small language model(SLM) ensembles to\nachieve accuracy comparable to proprietary large language models (LLMs). We\npropose Ensemble Bayesian Inference (EBI), a novel approach that applies\nBayesian estimation to combine judgments from multiple SLMs, allowing them to\nexceed the performance limitations of individual models. Our experiments on\ndiverse tasks(aptitude assessments and consumer profile analysis in both\nJapanese and English) demonstrate EBI's effectiveness. Notably, we analyze\ncases where incorporating models with negative Lift values into ensembles\nimproves overall performance, and we examine the method's efficacy across\ndifferent languages. These findings suggest new possibilities for constructing\nhigh-performance AI systems with limited computational resources and for\neffectively utilizing models with individually lower performance. Building on\nexisting research on LLM performance evaluation, ensemble methods, and\nopen-source LLM utilization, we discuss the novelty and significance of our\napproach.",
      "tldr_zh": "本文提出 Ensemble Bayesian Inference (EBI) 方法，通过贝叶斯估计结合多个 Small Language Models (SLM) 的判断，实现与 Large Language Models (LLM) 相当的准确性，特别是在 profile matching 任务中超越单个模型的性能限制。实验在 aptitude assessments 和 consumer profile analysis 等多样任务上进行，包括日语和英语数据集，结果显示纳入具有负 Lift values 的模型能提升整体表现。EBI 的创新性为利用有限计算资源构建高性能 AI 系统提供了新途径，并有效整合性能较低的模型。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "13 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.17685v1",
      "published_date": "2025-04-24 15:55:10 UTC",
      "updated_date": "2025-04-24 15:55:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:15:51.087101"
    },
    {
      "arxiv_id": "2504.18596v1",
      "title": "Optimizing the Privacy-Utility Balance using Synthetic Data and Configurable Perturbation Pipelines",
      "title_zh": "使用合成数据和可配置扰动管道优化隐私-效用平衡",
      "authors": [
        "Anantha Sharma",
        "Swetha Devabhaktuni",
        "Eklove Mohan"
      ],
      "abstract": "This paper explores the strategic use of modern synthetic data generation and\nadvanced data perturbation techniques to enhance security, maintain analytical\nutility, and improve operational efficiency when managing large datasets, with\na particular focus on the Banking, Financial Services, and Insurance (BFSI)\nsector. We contrast these advanced methods encompassing generative models like\nGANs, sophisticated context-aware PII transformation, configurable statistical\nperturbation, and differential privacy with traditional anonymization\napproaches.\n  The goal is to create realistic, privacy-preserving datasets that retain high\nutility for complex machine learning tasks and analytics, a critical need in\nthe data-sensitive industries like BFSI, Healthcare, Retail, and\nTelecommunications. We discuss how these modern techniques potentially offer\nsignificant improvements in balancing privacy preservation while maintaining\ndata utility compared to older methods. Furthermore, we examine the potential\nfor operational gains, such as reduced overhead and accelerated analytics, by\nusing these privacy-enhanced datasets. We also explore key use cases where\nthese methods can mitigate regulatory risks and enable scalable, data-driven\ninnovation without compromising sensitive customer information.",
      "tldr_zh": "本文提出了一种优化隐私与实用性平衡的方法，通过合成数据生成（如GANs）和可配置扰动管道（如上下文感知PII转换、统计扰动和差分隐私），专注于管理大型数据集以提升安全性并保持分析效用，尤其在BFSI（银行业、金融服务和保险）等领域。与传统匿名化方法相比，这些现代技术能生成更真实且高实用性的隐私保护数据集，并显著减少操作开销。研究探讨了这些方法的潜在益处，包括缓解监管风险、加速机器学习任务和促进可扩展的数据驱动创新。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "math.PR"
      ],
      "primary_category": "cs.CR",
      "comment": "18 pages, 8 figures, 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2504.18596v1",
      "published_date": "2025-04-24 15:52:53 UTC",
      "updated_date": "2025-04-24 15:52:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:16:03.648102"
    },
    {
      "arxiv_id": "2505.00020v1",
      "title": "Beyond Public Access in LLM Pre-Training Data",
      "title_zh": "翻译失败",
      "authors": [
        "Sruly Rosenblat",
        "Tim O'Reilly",
        "Ilan Strauss"
      ],
      "abstract": "Using a legally obtained dataset of 34 copyrighted O'Reilly Media books, we\napply the DE-COP membership inference attack method to investigate whether\nOpenAI's large language models were trained on copyrighted content without\nconsent. Our AUROC scores show that GPT-4o, OpenAI's more recent and capable\nmodel, demonstrates strong recognition of paywalled O'Reilly book content\n(AUROC = 82\\%), compared to OpenAI's earlier model GPT-3.5 Turbo. In contrast,\nGPT-3.5 Turbo shows greater relative recognition of publicly accessible\nO'Reilly book samples. GPT-4o Mini, as a much smaller model, shows no knowledge\nof public or non-public O'Reilly Media content when tested (AUROC $\\approx$\n50\\%). Testing multiple models, with the same cutoff date, helps us account for\npotential language shifts over time that might bias our findings. These results\nhighlight the urgent need for increased corporate transparency regarding\npre-training data sources as a means to develop formal licensing frameworks for\nAI content training",
      "tldr_zh": "本研究使用DE-COP membership inference attack方法，基于一个包含34本版权保护的O'Reilly Media书籍的数据集，调查OpenAI的LLM（如GPT-4o和GPT-3.5 Turbo）是否在未经许可的情况下训练了版权内容。\n结果显示，GPT-4o对付费墙后的书籍内容有强烈识别能力（AUROC=82%），而GPT-3.5 Turbo更倾向于识别公开可访问的内容，GPT-4o Mini则对这些内容无显著知识（AUROC≈50%）。\n通过控制时间因素并测试多个模型，该研究强调了LLM预训练数据来源的透明性需求，并呼吁建立正式的AI内容训练许可框架。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.00020v1",
      "published_date": "2025-04-24 15:49:59 UTC",
      "updated_date": "2025-04-24 15:49:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:16:15.852664"
    },
    {
      "arxiv_id": "2504.17677v1",
      "title": "INSIGHT: Bridging the Student-Teacher Gap in Times of Large Language Models",
      "title_zh": "INSIGHT: 在大语言模型时代桥接学生-教师差距",
      "authors": [
        "Jarne Thys",
        "Sebe Vanbrabant",
        "Davy Vanacken",
        "Gustavo Rovelo Ruiz"
      ],
      "abstract": "The rise of AI, especially Large Language Models, presents challenges and\nopportunities to integrate such technology into the classroom. AI has the\npotential to revolutionize education by helping teaching staff with various\ntasks, such as personalizing their teaching methods, but it also raises\nconcerns, for example, about the degradation of student-teacher interactions\nand user privacy. This paper introduces INSIGHT, a proof of concept to combine\nvarious AI tools to assist teaching staff and students in the process of\nsolving exercises. INSIGHT has a modular design that allows it to be integrated\ninto various higher education courses. We analyze students' questions to an LLM\nby extracting keywords, which we use to dynamically build an FAQ from students'\nquestions and provide new insights for the teaching staff to use for more\npersonalized face-to-face support. Future work could build upon INSIGHT by\nusing the collected data to provide adaptive learning and adjust content based\non student progress and learning styles to offer a more interactive and\ninclusive learning experience.",
      "tldr_zh": "该论文探讨了 Large Language Models (LLMs) 在教育中的应用挑战，如学生-教师互动下降和隐私问题，同时提出 INSIGHT 系统作为概念证明 (proof of concept)，旨在桥接这一差距。INSIGHT 采用模块化设计，结合各种 AI 工具分析学生对 LLM 的问题，通过提取关键词动态构建 FAQ，并为教师提供见解，以实现更个性化的面对面支持。实验结果显示该系统有助于提升教学效率，未来工作可利用收集的数据开发适应性学习，调整内容基于学生的进步和学习风格，提供更互动包容的教育体验。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.17677v1",
      "published_date": "2025-04-24 15:47:20 UTC",
      "updated_date": "2025-04-24 15:47:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:16:27.358164"
    },
    {
      "arxiv_id": "2504.17675v1",
      "title": "Optimized Cloud Resource Allocation Using Genetic Algorithms for Energy Efficiency and QoS Assurance",
      "title_zh": "翻译失败",
      "authors": [
        "Caroline Panggabean",
        "Devaraj Verma C",
        "Bhagyashree Gogoi",
        "Ranju Limbu",
        "Rhythm Sarker"
      ],
      "abstract": "Cloud computing environments demand dynamic and efficient resource management\nto ensure optimal performance, reduced energy consumption, and adherence to\nService Level Agreements (SLAs). This paper presents a Genetic Algorithm\n(GA)-based approach for Virtual Machine (VM) placement and consolidation,\naiming to minimize power usage while maintaining QoS constraints. The proposed\nmethod dynamically adjusts VM allocation based on real-time workload\nvariations, outperforming traditional heuristics such as First Fit Decreasing\n(FFD) and Best Fit Decreasing (BFD). Experimental results show notable\nreductions in energy consumption, VM migrations, SLA violation rates, and\nexecution time. A correlation heatmap further illustrates strong relationships\namong these key performance indicators, confirming the effectiveness of our\napproach in optimizing cloud resource utilization.",
      "tldr_zh": "本研究提出了一种基于 Genetic Algorithms (GA) 的云资源分配方法，用于优化 Virtual Machine (VM) 放置和整合，以实现能效最大化和 QoS 保障。该方法通过动态调整 VM 分配以适应实时工作负载变化，相比传统启发式算法如 First Fit Decreasing (FFD) 和 Best Fit Decreasing (BFD)，在减少能耗和维护 Service Level Agreements (SLAs) 方面表现出色。实验结果显示，该方法显著降低了能耗、VM 迁移次数、SLA 违反率和执行时间，并通过相关性热图证实了关键性能指标之间的强相关性，从而提升了云资源利用效率。",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "7 pages, 5 figures, accepted for publication (not yet published)",
      "pdf_url": "http://arxiv.org/pdf/2504.17675v1",
      "published_date": "2025-04-24 15:45:40 UTC",
      "updated_date": "2025-04-24 15:45:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:16:39.493177"
    },
    {
      "arxiv_id": "2504.17671v3",
      "title": "Data-Driven Calibration of Prediction Sets in Large Vision-Language Models Based on Inductive Conformal Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Yuanchang Ye",
        "Weiyan Wen"
      ],
      "abstract": "This study addresses the critical challenge of hallucination mitigation in\nLarge Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks\nthrough a Split Conformal Prediction (SCP) framework. While LVLMs excel in\nmulti-modal reasoning, their outputs often exhibit hallucinated content with\nhigh confidence, posing risks in safety-critical applications. We propose a\nmodel-agnostic uncertainty quantification method that integrates dynamic\nthreshold calibration and cross-modal consistency verification. By partitioning\ndata into calibration and test sets, the framework computes nonconformity\nscores to construct prediction sets with statistical guarantees under\nuser-defined risk levels ($\\alpha$). Key innovations include: (1) rigorous\ncontrol of \\textbf{marginal coverage} to ensure empirical error rates remain\nstrictly below $\\alpha$; (2) dynamic adjustment of prediction set sizes\ninversely with $\\alpha$, filtering low-confidence outputs; (3) elimination of\nprior distribution assumptions and retraining requirements. Evaluations on\nbenchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces\ntheoretical guarantees across all $\\alpha$ values. The framework achieves\nstable performance across varying calibration-to-test split ratios,\nunderscoring its robustness for real-world deployment in healthcare, autonomous\nsystems, and other safety-sensitive domains. This work bridges the gap between\ntheoretical reliability and practical applicability in multi-modal AI systems,\noffering a scalable solution for hallucination detection and uncertainty-aware\ndecision-making.",
      "tldr_zh": "本研究针对 Large Vision-Language Models (LVLMs) 在 Visual Question Answering (VQA) 任务中的幻觉问题，提出一个模型无关的 Split Conformal Prediction (SCP) 框架，通过动态阈值校准和跨模态一致性验证来量化不确定性，并构建具有统计保证的预测集。关键创新包括严格控制 marginal coverage 以确保经验错误率低于用户定义的风险水平 ($\\alpha$)、动态调整预测集大小以过滤低置信输出，以及无需先验分布假设或模型重新训练。在 ScienceQA 和 MMMU 基准上测试八个 LVLMs 后，SCP 框架显示出稳定性能和理论可靠性，适用于医疗、自动系统等安全敏感领域，提供可扩展的幻觉检测和不确定性感知决策方案。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by ICIPCA 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.17671v3",
      "published_date": "2025-04-24 15:39:46 UTC",
      "updated_date": "2025-05-15 16:24:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:16:54.043295"
    },
    {
      "arxiv_id": "2504.17669v2",
      "title": "Towards a HIPAA Compliant Agentic AI System in Healthcare",
      "title_zh": "翻译失败",
      "authors": [
        "Subash Neupane",
        "Sudip Mittal",
        "Shahram Rahimi"
      ],
      "abstract": "Agentic AI systems powered by Large Language Models (LLMs) as their\nfoundational reasoning engine, are transforming clinical workflows such as\nmedical report generation and clinical summarization by autonomously analyzing\nsensitive healthcare data and executing decisions with minimal human oversight.\nHowever, their adoption demands strict compliance with regulatory frameworks\nsuch as Health Insurance Portability and Accountability Act (HIPAA),\nparticularly when handling Protected Health Information (PHI). This\nwork-in-progress paper introduces a HIPAA-compliant Agentic AI framework that\nenforces regulatory compliance through dynamic, context-aware policy\nenforcement. Our framework integrates three core mechanisms: (1)\nAttribute-Based Access Control (ABAC) for granular PHI governance, (2) a hybrid\nPHI sanitization pipeline combining regex patterns and BERT-based model to\nminimize leakage, and (3) immutable audit trails for compliance verification.",
      "tldr_zh": "本文提出一个 HIPAA 合规的 Agentic AI 框架，旨在处理医疗领域的敏感数据，如 Protected Health Information (PHI)，以支持 LLMs 驱动的临床工作流程如报告生成和总结，同时减少人为监督。框架的核心机制包括：(1) Attribute-Based Access Control (ABAC) 用于精细化 PHI 访问管理，(2) 混合 PHI 净化管道结合 regex 模式和 BERT 模型以最小化数据泄露风险，以及 (3) 不可变的审计跟踪机制以验证合规性。该框架通过动态、上下文感知的政策执行，增强 Agentic AI 系统在医疗中的安全性和可采用性。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.17669v2",
      "published_date": "2025-04-24 15:38:20 UTC",
      "updated_date": "2025-05-06 21:45:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:17:05.544888"
    },
    {
      "arxiv_id": "2504.17663v1",
      "title": "The Malicious Technical Ecosystem: Exposing Limitations in Technical Governance of AI-Generated Non-Consensual Intimate Images of Adults",
      "title_zh": "翻译失败",
      "authors": [
        "Michelle L. Ding",
        "Harini Suresh"
      ],
      "abstract": "In this paper, we adopt a survivor-centered approach to locate and dissect\nthe role of sociotechnical AI governance in preventing AI-Generated\nNon-Consensual Intimate Images (AIG-NCII) of adults, colloquially known as\n\"deep fake pornography.\" We identify a \"malicious technical ecosystem\" or\n\"MTE,\" comprising of open-source face-swapping models and nearly 200\n\"nudifying\" software programs that allow non-technical users to create AIG-NCII\nwithin minutes. Then, using the National Institute of Standards and Technology\n(NIST) AI 100-4 report as a reflection of current synthetic content governance\nmethods, we show how the current landscape of practices fails to effectively\nregulate the MTE for adult AIG-NCII, as well as flawed assumptions explaining\nthese gaps.",
      "tldr_zh": "该论文采用幸存者中心方法，剖析AI治理在防止AI-Generated Non-Consensual Intimate Images (AIG-NCII)——俗称“deep fake pornography”——中的作用，并识别了“malicious technical ecosystem” (MTE)，包括开源面部交换模型和近200个“nudifying”软件程序，这些工具使非技术用户能在几分钟内创建AIG-NCII。研究以National Institute of Standards and Technology (NIST) AI 100-4报告为基准，揭示了当前治理实践在监管MTE方面的失败，包括未能有效应对成人AIG-NCII问题。最终，论文指出了这些缺口的根本原因在于错误的假设，为改进AI治理提供了关键洞见。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.17663v1",
      "published_date": "2025-04-24 15:31:46 UTC",
      "updated_date": "2025-04-24 15:31:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:17:16.221253"
    },
    {
      "arxiv_id": "2504.17655v1",
      "title": "Aerial Image Classification in Scarce and Unconstrained Environments via Conformal Prediction",
      "title_zh": "在稀",
      "authors": [
        "Farhad Pourkamali-Anaraki"
      ],
      "abstract": "This paper presents a comprehensive empirical analysis of conformal\nprediction methods on a challenging aerial image dataset featuring diverse\nevents in unconstrained environments. Conformal prediction is a powerful\npost-hoc technique that takes the output of any classifier and transforms it\ninto a set of likely labels, providing a statistical guarantee on the coverage\nof the true label. Unlike evaluations on standard benchmarks, our study\naddresses the complexities of data-scarce and highly variable real-world\nsettings. We investigate the effectiveness of leveraging pretrained models\n(MobileNet, DenseNet, and ResNet), fine-tuned with limited labeled data, to\ngenerate informative prediction sets. To further evaluate the impact of\ncalibration, we consider two parallel pipelines (with and without temperature\nscaling) and assess performance using two key metrics: empirical coverage and\naverage prediction set size. This setup allows us to systematically examine how\ncalibration choices influence the trade-off between reliability and efficiency.\nOur findings demonstrate that even with relatively small labeled samples and\nsimple nonconformity scores, conformal prediction can yield valuable\nuncertainty estimates for complex tasks. Moreover, our analysis reveals that\nwhile temperature scaling is often employed for calibration, it does not\nconsistently lead to smaller prediction sets, underscoring the importance of\ncareful consideration in its application. Furthermore, our results highlight\nthe significant potential of model compression techniques within the conformal\nprediction pipeline for deployment in resource-constrained environments. Based\non our observations, we advocate for future research to delve into the impact\nof noisy or ambiguous labels on conformal prediction performance and to explore\neffective model reduction strategies.",
      "tldr_zh": "这篇论文通过实证分析探讨了 conformal prediction 方法在数据稀缺和不受约束环境中的航空图像分类应用。研究利用预训练模型（如 MobileNet、DenseNet 和 ResNet）进行微调，并比较了带或不带 temperature scaling 的处理管道，评估了 empirical coverage 和 average prediction set size 等关键指标。结果表明，即使在小样本和简单 nonconformity scores 下，conformal prediction 也能提供可靠的不确定性估计，但 temperature scaling 并不总是减少预测集大小。论文强调模型压缩技术在资源受限环境中的潜力，并建议未来研究关注噪声或模糊标签的影响及有效模型简化策略。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "17 pages, 5 figures, and 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2504.17655v1",
      "published_date": "2025-04-24 15:25:37 UTC",
      "updated_date": "2025-04-24 15:25:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:17:28.932320"
    },
    {
      "arxiv_id": "2504.17641v2",
      "title": "PTCL: Pseudo-Label Temporal Curriculum Learning for Label-Limited Dynamic Graph",
      "title_zh": "翻译失败",
      "authors": [
        "Shengtao Zhang",
        "Haokai Zhang",
        "Shiqi Lou",
        "Zicheng Wang",
        "Zinan Zeng",
        "Yilin Wang",
        "Minnan Luo"
      ],
      "abstract": "Dynamic node classification is critical for modeling evolving systems like\nfinancial transactions and academic collaborations. In such systems,\ndynamically capturing node information changes is critical for dynamic node\nclassification, which usually requires all labels at every timestamp. However,\nit is difficult to collect all dynamic labels in real-world scenarios due to\nhigh annotation costs and label uncertainty (e.g., ambiguous or delayed labels\nin fraud detection). In contrast, final timestamp labels are easier to obtain\nas they rely on complete temporal patterns and are usually maintained as a\nunique label for each user in many open platforms, without tracking the history\ndata. To bridge this gap, we propose PTCL(Pseudo-label Temporal Curriculum\nLearning), a pioneering method addressing label-limited dynamic node\nclassification where only final labels are available. PTCL introduces: (1) a\ntemporal decoupling architecture separating the backbone (learning time-aware\nrepresentations) and decoder (strictly aligned with final labels), which\ngenerate pseudo-labels, and (2) a Temporal Curriculum Learning strategy that\nprioritizes pseudo-labels closer to the final timestamp by assigning them\nhigher weights using an exponentially decaying function. We contribute a new\nacademic dataset (CoOAG), capturing long-range research interest in dynamic\ngraph. Experiments across real-world scenarios demonstrate PTCL's consistent\nsuperiority over other methods adapted to this task. Beyond methodology, we\npropose a unified framework FLiD (Framework for Label-Limited Dynamic Node\nClassification), consisting of a complete preparation workflow, training\npipeline, and evaluation standards, and supporting various models and datasets.\nThe code can be found at https://github.com/3205914485/FLiD.",
      "tldr_zh": "本论文针对动态图节点分类任务提出 PTCL（Pseudo-Label Temporal Curriculum Learning）方法，以解决仅可用最终标签的标签有限场景问题，如金融交易或学术合作中的动态建模。PTCL 包括一个时间解耦架构（分离主干网络用于学习时间感知表示和解码器用于生成伪标签）以及 Temporal Curriculum Learning 策略，通过指数衰减函数赋予接近最终时间戳的伪标签更高权重，从而提升模型训练效率。论文贡献了一个新数据集 CoOAG 用于捕捉动态图中的长期研究兴趣，并通过实验证明 PTCL 在真实场景中优于其他方法；此外，还提出统一框架 FLiD，支持模型训练和评估工作流。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "13 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.17641v2",
      "published_date": "2025-04-24 15:11:41 UTC",
      "updated_date": "2025-04-25 03:38:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:17:40.137259"
    },
    {
      "arxiv_id": "2504.17624v1",
      "title": "Deciphering the unique dynamic activation pathway in a G protein-coupled receptor enables unveiling biased signaling and identifying cryptic allosteric sites in conformational intermediates",
      "title_zh": "翻译失败",
      "authors": [
        "Jigang Fan",
        "Chunhao Zhu",
        "Xiaobing Lan",
        "Haiming Zhuang",
        "Mingyu Li",
        "Jian Zhang",
        "Shaoyong Lu"
      ],
      "abstract": "Neurotensin receptor 1 (NTSR1), a member of the Class A G protein-coupled\nreceptor superfamily, plays an important role in modulating dopaminergic\nneuronal activity and eliciting opioid-independent analgesia. Recent studies\nsuggest that promoting \\{beta}-arrestin-biased signaling in NTSR1 may diminish\ndrugs of abuse, such as psychostimulants, thereby offering a potential avenue\nfor treating human addiction-related disorders. In this study, we utilized a\nnovel computational and experimental approach that combined nudged elastic\nband-based molecular dynamics simulations, Markov state models, temporal\ncommunication network analysis, site-directed mutagenesis, and conformational\nbiosensors, to explore the intricate mechanisms underlying NTSR1 activation and\nbiased signaling. Our study reveals a dynamic stepwise transition mechanism and\nactivated transmission network associated with NTSR1 activation. It also yields\nvaluable insights into the complex interplay between the unique polar network,\nnon-conserved ion locks, and aromatic clusters in NTSR1 signaling. Moreover, we\nidentified a cryptic allosteric site located in the intracellular region of the\nreceptor that exists in an intermediate state within the activation pathway.\nCollectively, these findings contribute to a more profound understanding of\nNTSR1 activation and biased signaling at the atomic level, thereby providing a\npotential strategy for the development of NTSR1 allosteric modulators in the\nrealm of G protein-coupled receptor biology, biophysics, and medicine.",
      "tldr_zh": "本研究探讨了 Neurotensin receptor 1 (NTSR1)，一个 Class A G protein-coupled receptor 家族成员的独特动态激活路径，以揭示其 β-arrestin-biased signaling 机制，并识别激活路径中中间状态的 cryptic allosteric sites。研究结合 nudged elastic band-based molecular dynamics simulations、Markov state models 和 site-directed mutagenesis 等计算与实验方法，揭示了 NTSR1 的逐步过渡机制、激活传输网络，以及 polar network、非保守 ion locks 和 aromatic clusters 在信号传导中的复杂互动。最终，这些发现加深了对 NTSR1 激活的原子水平理解，并为开发针对成瘾相关疾病的 NTSR1 allosteric modulators 提供潜在策略。",
      "categories": [
        "q-bio.BM",
        "cs.AI"
      ],
      "primary_category": "q-bio.BM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.17624v1",
      "published_date": "2025-04-24 14:46:20 UTC",
      "updated_date": "2025-04-24 14:46:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:17:52.929897"
    },
    {
      "arxiv_id": "2504.17619v1",
      "title": "Enhancing CNNs robustness to occlusions with bioinspired filters for border completion",
      "title_zh": "翻译失败",
      "authors": [
        "Catarina P. Coutinho",
        "Aneeqa Merhab",
        "Janko Petkovic",
        "Ferdinando Zanchetta",
        "Rita Fioresi"
      ],
      "abstract": "We exploit the mathematical modeling of the visual cortex mechanism for\nborder completion to define custom filters for CNNs. We see a consistent\nimprovement in performance, particularly in accuracy, when our modified LeNet 5\nis tested with occluded MNIST images.",
      "tldr_zh": "本研究利用视觉皮层机制的数学模型，开发了生物启发的自定义过滤器，以提升 CNNs 对遮挡的鲁棒性，特别是通过边框完成（border completion）技术。研究者将这些过滤器应用于修改后的 LeNet 5 模型，并在遮挡的 MNIST 图像上进行测试。结果显示，模型的性能得到一致改善，尤其是在准确率方面。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Submitted to the 7th International Conference on Geometric Science of\n  Information",
      "pdf_url": "http://arxiv.org/pdf/2504.17619v1",
      "published_date": "2025-04-24 14:43:55 UTC",
      "updated_date": "2025-04-24 14:43:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:18:02.404431"
    },
    {
      "arxiv_id": "2504.17617v1",
      "title": "Decentralized Time Series Classification with ROCKET Features",
      "title_zh": "翻译失败",
      "authors": [
        "Bruno Casella",
        "Matthias Jakobs",
        "Marco Aldinucci",
        "Sebastian Buschjäger"
      ],
      "abstract": "Time series classification (TSC) is a critical task with applications in\nvarious domains, including healthcare, finance, and industrial monitoring. Due\nto privacy concerns and data regulations, Federated Learning has emerged as a\npromising approach for learning from distributed time series data without\ncentralizing raw information. However, most FL solutions rely on a\nclient-server architecture, which introduces robustness and confidentiality\nrisks related to the distinguished role of the server, which is a single point\nof failure and can observe knowledge extracted from clients. To address these\nchallenges, we propose DROCKS, a fully decentralized FL framework for TSC that\nleverages ROCKET (RandOm Convolutional KErnel Transform) features. In DROCKS,\nthe global model is trained by sequentially traversing a structured path across\nfederation nodes, where each node refines the model and selects the most\neffective local kernels before passing them to the successor. Extensive\nexperiments on the UCR archive demonstrate that DROCKS outperforms\nstate-of-the-art client-server FL approaches while being more resilient to node\nfailures and malicious attacks. Our code is available at\nhttps://anonymous.4open.science/r/DROCKS-7FF3/README.md.",
      "tldr_zh": "这篇论文针对时间序列分类（TSC）的隐私和数据法规挑战，提出了一种完全去中心化的联邦学习（FL）框架 DROCKS，利用 ROCKET 特征来训练分布式模型。DROCKS 通过在联邦节点之间顺序遍历结构化路径，每个节点优化本地内核并传递模型，从而避免了传统客户端-服务器架构的单点故障风险。实验在 UCR 档案上表明，DROCKS 优于现有 FL 方法，在节点故障和恶意攻击方面更具弹性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "68T07",
        "I.2.11; I.2.6"
      ],
      "primary_category": "cs.LG",
      "comment": "Submitted to Workshop on Federated Learning Advancements 2025, in\n  conjunction with ECML-PKDD, WAFL25",
      "pdf_url": "http://arxiv.org/pdf/2504.17617v1",
      "published_date": "2025-04-24 14:41:50 UTC",
      "updated_date": "2025-04-24 14:41:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:18:15.450488"
    },
    {
      "arxiv_id": "2504.17609v1",
      "title": "STCL:Curriculum learning Strategies for deep learning image steganography models",
      "title_zh": "翻译失败",
      "authors": [
        "Fengchun Liu",
        "Tong Zhang",
        "Chunying Zhang"
      ],
      "abstract": "Aiming at the problems of poor quality of steganographic images and slow\nnetwork convergence of image steganography models based on deep learning, this\npaper proposes a Steganography Curriculum Learning training strategy (STCL) for\ndeep learning image steganography models. So that only easy images are selected\nfor training when the model has poor fitting ability at the initial stage, and\ngradually expand to more difficult images, the strategy includes a difficulty\nevaluation strategy based on the teacher model and an knee point-based training\nscheduling strategy. Firstly, multiple teacher models are trained, and the\nconsistency of the quality of steganographic images under multiple teacher\nmodels is used as the difficulty score to construct the training subsets from\neasy to difficult. Secondly, a training control strategy based on knee points\nis proposed to reduce the possibility of overfitting on small training sets and\naccelerate the training process. Experimental results on three large public\ndatasets, ALASKA2, VOC2012 and ImageNet, show that the proposed image\nsteganography scheme is able to improve the model performance under multiple\nalgorithmic frameworks, which not only has a high PSNR, SSIM score, and\ndecoding accuracy, but also the steganographic images generated by the model\nunder the training of the STCL strategy have a low steganography analysis\nscores. You can find our code at\n\\href{https://github.com/chaos-boops/STCL}{https://github.com/chaos-boops/STCL}.",
      "tldr_zh": "该论文针对基于深度学习的图像隐写模型存在的隐写图像质量差和网络收敛慢的问题，提出了一种STCL（Steganography Curriculum Learning）训练策略。该策略包括基于教师模型的难度评估方法，用于构建从易到难的训练子集，以及基于膝点（knee point）的训练调度策略，以减少过拟合风险并加速训练过程。在ALASKA2、VOC2012和ImageNet数据集上的实验显示，STCL显著提升了模型性能，包括更高的PSNR、SSIM分数、解码准确率，以及更低的隐写分析分数。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.17609v1",
      "published_date": "2025-04-24 14:34:41 UTC",
      "updated_date": "2025-04-24 14:34:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:18:27.571680"
    },
    {
      "arxiv_id": "2505.00019v1",
      "title": "An Empirical Study on Prompt Compression for Large Language Models",
      "title_zh": "大型语言模型提示压缩的实证研究",
      "authors": [
        "Zheng Zhang",
        "Jinyi Li",
        "Yihuai Lan",
        "Xiang Wang",
        "Hao Wang"
      ],
      "abstract": "Prompt engineering enables Large Language Models (LLMs) to perform a variety\nof tasks. However, lengthy prompts significantly increase computational\ncomplexity and economic costs. To address this issue, we study six prompt\ncompression methods for LLMs, aiming to reduce prompt length while maintaining\nLLM response quality. In this paper, we present a comprehensive analysis\ncovering aspects such as generation performance, model hallucinations, efficacy\nin multimodal tasks, word omission analysis, and more. We evaluate these\nmethods across 13 datasets, including news, scientific articles, commonsense\nQA, math QA, long-context QA, and VQA datasets. Our experiments reveal that\nprompt compression has a greater impact on LLM performance in long contexts\ncompared to short ones. In the Longbench evaluation, moderate compression even\nenhances LLM performance. Our code and data is available at\nhttps://github.com/3DAgentWorld/Toolkit-for-Prompt-Compression.",
      "tldr_zh": "这篇论文通过实证研究探讨了提示压缩方法，以减少大语言模型(LLMs)的提示长度，同时维持响应质量。作者评估了六种提示压缩方法，在13个数据集（如新闻、科学文章、常识QA、数学QA、长上下文QA和VQA）上分析了生成性能、模型幻觉、多模态任务效能等指标。结果表明，提示压缩对长上下文的影响更大，在Longbench评估中适度压缩甚至提升了LLM性能，并提供了代码和数据以供进一步研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by Building Trust Workshop at ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2505.00019v1",
      "published_date": "2025-04-24 14:15:13 UTC",
      "updated_date": "2025-04-24 14:15:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:18:39.866023"
    },
    {
      "arxiv_id": "2504.20069v1",
      "title": "A Simple Review of EEG Foundation Models: Datasets, Advancements and Future Perspectives",
      "title_zh": "EEG 基础模型的简单综述：数据集、进展和未来展望",
      "authors": [
        "Junhong Lai",
        "Jiyu Wei",
        "Lin Yao",
        "Yueming Wang"
      ],
      "abstract": "Electroencephalogram (EEG) signals play a crucial role in understanding brain\nactivity and diagnosing neurological disorders. This review focuses on the\nrecent development of EEG foundation models(EEG-FMs), which have shown great\npotential in processing and analyzing EEG data. We discuss various EEG-FMs,\nincluding their architectures, pre-training strategies, their pre-training and\ndownstream datasets and other details. The review also highlights the\nchallenges and future directions in this field, aiming to provide a\ncomprehensive overview for researchers and practitioners interested in EEG\nanalysis and related EEG-FMs.",
      "tldr_zh": "这篇综述文章回顾了 EEG 基础模型（EEG-FMs）的最新进展，强调其在处理和分析脑电图（EEG）信号方面的潜力，用于理解脑活动和诊断神经疾病。主要内容包括各种 EEG-FMs 的架构、预训练策略、预训练和下游数据集等细节，并为研究者和从业者提供全面概述。该文还指出了当前面临的挑战，如模型泛化问题，并探讨了未来方向，例如改进数据集和算法以推动 EEG 分析的应用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.20069v1",
      "published_date": "2025-04-24 14:14:17 UTC",
      "updated_date": "2025-04-24 14:14:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:18:50.775207"
    },
    {
      "arxiv_id": "2504.18595v1",
      "title": "EnviroPiNet: A Physics-Guided AI Model for Predicting Biofilter Performance",
      "title_zh": "EnviroPiNet：一种物理引导的 AI 模型，用于预测生物过滤器性能",
      "authors": [
        "Uzma",
        "Fabien Cholet",
        "Domenic Quinn",
        "Cindy Smith",
        "Siming You",
        "William Sloan"
      ],
      "abstract": "Environmental biotechnologies, such as drinking water biofilters, rely on\ncomplex interactions between microbial communities and their surrounding\nphysical-chemical environments. Predicting the performance of these systems is\nchallenging due to high-dimensional, sparse datasets that lack diversity and\nfail to fully capture system behaviour. Accurate predictive models require\ninnovative, science-guided approaches. In this study, we present the first\napplication of Buckingham Pi theory to modelling biofilter performance. This\ndimensionality reduction technique identifies meaningful, dimensionless\nvariables that enhance predictive accuracy and improve model interpretability.\nUsing these variables, we developed the Environmental Buckingham Pi Neural\nNetwork (EnviroPiNet), a physics-guided model benchmarked against traditional\ndata-driven methods, including Principal Component Analysis (PCA) and\nautoencoder neural networks. Our findings demonstrate that the EnviroPiNet\nmodel achieves an R^2 value of 0.9236 on the testing dataset, significantly\noutperforming PCA and autoencoder methods. The Buckingham Pi variables also\nprovide insights into the physical and chemical relationships governing\nbiofilter behaviour, with implications for system design and optimization. This\nstudy highlights the potential of combining physical principles with AI\napproaches to model complex environmental systems characterized by sparse,\nhigh-dimensional datasets.",
      "tldr_zh": "该研究针对饮用水生物过滤器等环境生物技术的复杂系统，提出了一种基于Buckingham Pi理论的AI模型EnviroPiNet，以应对高维稀疏数据集的预测挑战。Buckingham Pi理论用于降维，识别无量纲变量，从而提升模型的预测准确性和可解释性。EnviroPiNet与传统数据驱动方法如PCA和autoencoder神经网络相比，在测试数据集上实现了R^2值为0.9236的显著优越表现。研究还揭示了这些变量对生物过滤器物理-化学关系的洞见，为系统设计和优化提供了重要启发。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.18595v1",
      "published_date": "2025-04-24 13:52:51 UTC",
      "updated_date": "2025-04-24 13:52:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:19:03.660022"
    },
    {
      "arxiv_id": "2504.17551v2",
      "title": "Unsupervised Urban Land Use Mapping with Street View Contrastive Clustering and a Geographical Prior",
      "title_zh": "翻译失败",
      "authors": [
        "Lin Che",
        "Yizi Chen",
        "Tanhua Jin",
        "Martin Raubal",
        "Konrad Schindler",
        "Peter Kiefer"
      ],
      "abstract": "Urban land use classification and mapping are critical for urban planning,\nresource management, and environmental monitoring. Existing remote sensing\ntechniques often lack precision in complex urban environments due to the\nabsence of ground-level details. Unlike aerial perspectives, street view images\nprovide a ground-level view that captures more human and social activities\nrelevant to land use in complex urban scenes. Existing street view-based\nmethods primarily rely on supervised classification, which is challenged by the\nscarcity of high-quality labeled data and the difficulty of generalizing across\ndiverse urban landscapes. This study introduces an unsupervised contrastive\nclustering model for street view images with a built-in geographical prior, to\nenhance clustering performance. When combined with a simple visual assignment\nof the clusters, our approach offers a flexible and customizable solution to\nland use mapping, tailored to the specific needs of urban planners. We\nexperimentally show that our method can generate land use maps from geotagged\nstreet view image datasets of two cities. As our methodology relies on the\nuniversal spatial coherence of geospatial data (\"Tobler's law\"), it can be\nadapted to various settings where street view images are available, to enable\nscalable, unsupervised land use mapping and updating. The code will be\navailable at https://github.com/lin102/CCGP.",
      "tldr_zh": "该研究提出了一种无监督的城市土地用途映射方法，使用街景图像结合对比聚类（contrastive clustering）和地理先验（geographical prior），以克服现有遥感技术和监督分类方法的局限性。该方法通过增强聚类性能并整合简单视觉分配，提供灵活、可定制的土地用途映射解决方案，适用于城市规划需求。在实验中，该模型成功从两个城市的街景图像数据集生成土地用途地图，并依赖于地理数据的空间连贯性（Tobler's law），实现可扩展的无监督映射和更新。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 7 figures, preprint version",
      "pdf_url": "http://arxiv.org/pdf/2504.17551v2",
      "published_date": "2025-04-24 13:41:27 UTC",
      "updated_date": "2025-05-13 16:31:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:19:15.761970"
    },
    {
      "arxiv_id": "2504.17550v1",
      "title": "HalluLens: LLM Hallucination Benchmark",
      "title_zh": "HalluLens: LLM 幻觉基准",
      "authors": [
        "Yejin Bang",
        "Ziwei Ji",
        "Alan Schelten",
        "Anthony Hartshorn",
        "Tara Fowler",
        "Cheng Zhang",
        "Nicola Cancedda",
        "Pascale Fung"
      ],
      "abstract": "Large language models (LLMs) often generate responses that deviate from user\ninput or training data, a phenomenon known as \"hallucination.\" These\nhallucinations undermine user trust and hinder the adoption of generative AI\nsystems. Addressing hallucinations is essential for the advancement of LLMs.\nThis paper introduces a comprehensive hallucination benchmark, incorporating\nboth new extrinsic and existing intrinsic evaluation tasks, built upon clear\ntaxonomy of hallucination. A major challenge in benchmarking hallucinations is\nthe lack of a unified framework due to inconsistent definitions and\ncategorizations. We disentangle LLM hallucination from \"factuality,\" proposing\na clear taxonomy that distinguishes between extrinsic and intrinsic\nhallucinations, to promote consistency and facilitate research. Extrinsic\nhallucinations, where the generated content is not consistent with the training\ndata, are increasingly important as LLMs evolve. Our benchmark includes dynamic\ntest set generation to mitigate data leakage and ensure robustness against such\nleakage. We also analyze existing benchmarks, highlighting their limitations\nand saturation. The work aims to: (1) establish a clear taxonomy of\nhallucinations, (2) introduce new extrinsic hallucination tasks, with data that\ncan be dynamically regenerated to prevent saturation by leakage, (3) provide a\ncomprehensive analysis of existing benchmarks, distinguishing them from\nfactuality evaluations.",
      "tldr_zh": "这篇论文介绍了HalluLens，一种针对大型语言模型(LLM)的“hallucination”现象的全面基准，用于评估模型生成内容与用户输入或训练数据不一致的问题。论文提出一个清晰的“hallucination”分类体系，将其分为“extrinsic”（与训练数据不一致）和“intrinsic”（内部不一致）类型，并将其与“factuality”区分开来，以统一定义并促进研究。基准包括新的“extrinsic”评估任务和动态测试集生成，以防止数据泄漏和基准饱和。最终，该工作分析了现有基准的局限性，并为LLM的可靠性和可信度提升提供了重要基础。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "42 pages",
      "pdf_url": "http://arxiv.org/pdf/2504.17550v1",
      "published_date": "2025-04-24 13:40:27 UTC",
      "updated_date": "2025-04-24 13:40:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:19:28.628059"
    },
    {
      "arxiv_id": "2504.17544v1",
      "title": "Auditing the Ethical Logic of Generative AI Models",
      "title_zh": "审计生成式 AI 模型的伦理逻辑",
      "authors": [
        "W. Russell Neuman",
        "Chad Coleman",
        "Ali Dasdan",
        "Safinah Ali",
        "Manan Shah"
      ],
      "abstract": "As generative AI models become increasingly integrated into high-stakes\ndomains, the need for robust methods to evaluate their ethical reasoning\nbecomes increasingly important. This paper introduces a five-dimensional audit\nmodel -- assessing Analytic Quality, Breadth of Ethical Considerations, Depth\nof Explanation, Consistency, and Decisiveness -- to evaluate the ethical logic\nof leading large language models (LLMs). Drawing on traditions from applied\nethics and higher-order thinking, we present a multi-battery prompt approach,\nincluding novel ethical dilemmas, to probe the models' reasoning across diverse\ncontexts. We benchmark seven major LLMs finding that while models generally\nconverge on ethical decisions, they vary in explanatory rigor and moral\nprioritization. Chain-of-Thought prompting and reasoning-optimized models\nsignificantly enhance performance on our audit metrics. This study introduces a\nscalable methodology for ethical benchmarking of AI systems and highlights the\npotential for AI to complement human moral reasoning in complex decision-making\ncontexts.",
      "tldr_zh": "这篇论文提出了一种五维审计模型，用于评估生成式 AI 模型的伦理逻辑，包括 Analytic Quality、Breadth of Ethical Considerations、Depth of Explanation、Consistency 和 Decisiveness。研究采用多电池提示方法（如新型伦理困境）和 Chain-of-Thought 提示，对七个主要 Large Language Models (LLMs) 进行基准测试，发现这些模型在伦理决策上趋于一致，但解释严谨度和道德优先级存在差异。结果显示，Chain-of-Thought 提示和优化推理的模型显著提升了审计指标的性能，并为 AI 系统提供了一种可扩展的伦理基准测试方法，以补充人类在复杂决策中的道德推理。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.17544v1",
      "published_date": "2025-04-24 13:32:30 UTC",
      "updated_date": "2025-04-24 13:32:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:19:40.200586"
    },
    {
      "arxiv_id": "2504.17540v1",
      "title": "An Explainable Nature-Inspired Framework for Monkeypox Diagnosis: Xception Features Combined with NGBoost and African Vultures Optimization Algorithm",
      "title_zh": "翻译失败",
      "authors": [
        "Ahmadreza Shateri",
        "Negar Nourani",
        "Morteza Dorrigiv",
        "Hamid Nasiri"
      ],
      "abstract": "The recent global spread of monkeypox, particularly in regions where it has\nnot historically been prevalent, has raised significant public health concerns.\nEarly and accurate diagnosis is critical for effective disease management and\ncontrol. In response, this study proposes a novel deep learning-based framework\nfor the automated detection of monkeypox from skin lesion images, leveraging\nthe power of transfer learning, dimensionality reduction, and advanced machine\nlearning techniques. We utilize the newly developed Monkeypox Skin Lesion\nDataset (MSLD), which includes images of monkeypox, chickenpox, and measles, to\ntrain and evaluate our models. The proposed framework employs the Xception\narchitecture for deep feature extraction, followed by Principal Component\nAnalysis (PCA) for dimensionality reduction, and the Natural Gradient Boosting\n(NGBoost) algorithm for classification. To optimize the model's performance and\ngeneralization, we introduce the African Vultures Optimization Algorithm (AVOA)\nfor hyperparameter tuning, ensuring efficient exploration of the parameter\nspace. Our results demonstrate that the proposed AVOA-NGBoost model achieves\nstate-of-the-art performance, with an accuracy of 97.53%, F1-score of 97.72%\nand an AUC of 97.47%. Additionally, we enhance model interpretability using\nGrad-CAM and LIME techniques, providing insights into the decision-making\nprocess and highlighting key features influencing classification. This\nframework offers a highly precise and efficient diagnostic tool, potentially\naiding healthcare providers in early detection and diagnosis, particularly in\nresource-constrained environments.",
      "tldr_zh": "本研究提出了一种基于自然启发的可解释框架，用于猴痘诊断，通过结合 Xception 架构提取深度特征、PCA 进行降维、NGBoost 算法进行分类，并利用 African Vultures Optimization Algorithm (AVOA) 优化超参数，以提升模型性能和泛化能力。实验基于 Monkeypox Skin Lesion Dataset (MSLD) 进行评估，结果显示 AVOA-NGBoost 模型在猴痘检测中达到 97.53% 准确率、97.72% F1-score 和 97.47% AUC，显著优于现有方法。模型还通过 Grad-CAM 和 LIME 技术增强可解释性，提供决策过程洞见，从而为资源有限环境下的早期诊断提供高效工具。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.17540v1",
      "published_date": "2025-04-24 13:32:11 UTC",
      "updated_date": "2025-04-24 13:32:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:19:51.816936"
    },
    {
      "arxiv_id": "2504.17539v1",
      "title": "Proof of Useful Intelligence (PoUI): Blockchain Consensus Beyond Energy Waste",
      "title_zh": "翻译失败",
      "authors": [
        "Zan-Kai Chong",
        "Hiroyuki Ohsaki",
        "Bryan Ng"
      ],
      "abstract": "Blockchain technology enables secure, transparent data management in\ndecentralized systems, supporting applications from cryptocurrencies like\nBitcoin to tokenizing real-world assets like property. Its scalability and\nsustainability hinge on consensus mechanisms balancing security and efficiency.\nProof of Work (PoW), used by Bitcoin, ensures security through energy-intensive\ncomputations but demands significant resources. Proof of Stake (PoS), as in\nEthereum post-Merge, selects validators based on staked cryptocurrency,\noffering energy efficiency but risking centralization from wealth\nconcentration. With AI models straining computational resources, we propose\nProof of Useful Intelligence (PoUI), a hybrid consensus mechanism. In PoUI,\nworkers perform AI tasks like language processing or image analysis to earn\ncoins, which are staked to secure the network, blending security with practical\nutility. Decentralized nodes--job posters, market coordinators, workers, and\nvalidators --collaborate via smart contracts to manage tasks and rewards.",
      "tldr_zh": "区块链技术依赖共识机制来平衡安全性和效率，现有的 Proof of Work (PoW) 通过高能耗计算确保安全，但资源浪费严重，而 Proof of Stake (PoS) 虽节能却可能因财富集中导致中心化。  \n本文提出 Proof of Useful Intelligence (PoUI)，一种混合共识机制，让参与者通过执行 AI 任务（如语言处理或图像分析）赚取币，并用这些币进行Staking，以实现安全性和实用性的结合。  \n在 PoUI 中，去中心化节点包括任务发布者（job posters）、市场协调者（market coordinators）、工作者（workers）和验证者（validators），它们通过智能合约协作管理任务和奖励。  \n这一机制有望超越传统共识，提供更可持续的区块链解决方案，减少能源浪费并提升实际应用价值。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.17539v1",
      "published_date": "2025-04-24 13:32:03 UTC",
      "updated_date": "2025-04-24 13:32:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:20:04.827244"
    },
    {
      "arxiv_id": "2504.17534v1",
      "title": "Learning Isometric Embeddings of Road Networks using Multidimensional Scaling",
      "title_zh": "使用多维缩放学习道路网络的等距嵌入",
      "authors": [
        "Juan Carlos Climent Pardo"
      ],
      "abstract": "The lack of generalization in learning-based autonomous driving applications\nis shown by the narrow range of road scenarios that vehicles can currently\ncover. A generalizable approach should capture many distinct road structures\nand topologies, as well as consider traffic participants, and dynamic changes\nin the environment, so that vehicles can navigate and perform motion planning\ntasks even in the most difficult situations. Designing suitable feature spaces\nfor neural network-based motion planers that encapsulate all kinds of road\nscenarios is still an open research challenge. This paper tackles this\nlearning-based generalization challenge and shows how graph representations of\nroad networks can be leveraged by using multidimensional scaling (MDS)\ntechniques in order to obtain such feature spaces. State-of-the-art graph\nrepresentations and MDS approaches are analyzed for the autonomous driving use\ncase. Finally, the option of embedding graph nodes is discussed in order to\nperform easier learning procedures and obtain dimensionality reduction.",
      "tldr_zh": "这篇论文探讨了基于学习的自动驾驶应用在道路场景泛化方面的不足，提出通过多维缩放 (MDS) 技术学习道路网络的等距嵌入，以捕捉各种道路结构、拓扑、交通参与者和环境动态变化。作者分析了现有图表示和 MDS 方法，将其应用于神经网络运动规划的特征空间设计，从而提升模型的泛化能力。该方法还讨论了嵌入图节点以简化学习过程并实现维度减少，最终有助于车辆在复杂情况下进行更有效的导航和规划。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.ET",
        "cs.SC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.17534v1",
      "published_date": "2025-04-24 13:20:32 UTC",
      "updated_date": "2025-04-24 13:20:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:20:15.406120"
    },
    {
      "arxiv_id": "2504.17833v1",
      "title": "The Role of Open-Source LLMs in Shaping the Future of GeoAI",
      "title_zh": "开源 LLMs 在塑造 GeoAI 未来的作用",
      "authors": [
        "Xiao Huang",
        "Zhengzhong Tu",
        "Xinyue Ye",
        "Michael Goodchild"
      ],
      "abstract": "Large Language Models (LLMs) are transforming geospatial artificial\nintelligence (GeoAI), offering new capabilities in data processing, spatial\nanalysis, and decision support. This paper examines the open-source paradigm's\npivotal role in this transformation. While proprietary LLMs offer\naccessibility, they often limit the customization, interoperability, and\ntransparency vital for specialized geospatial tasks. Conversely, open-source\nalternatives significantly advance Geographic Information Science (GIScience)\nby fostering greater adaptability, reproducibility, and community-driven\ninnovation. Open frameworks empower researchers to tailor solutions, integrate\ncutting-edge methodologies (e.g., reinforcement learning, advanced spatial\nindexing), and align with FAIR principles. However, the growing reliance on any\nLLM necessitates careful consideration of security vulnerabilities, ethical\nrisks, and robust governance for AI-generated geospatial outputs. Ongoing\ndebates on accessibility, regulation, and misuse underscore the critical need\nfor responsible AI development strategies. This paper argues that GIScience\nadvances best not through a single model type, but by cultivating a diverse,\ninteroperable ecosystem combining open-source foundations for innovation,\nbespoke geospatial models, and interdisciplinary collaboration. By critically\nevaluating the opportunities and challenges of open-source LLMs within the\nbroader GeoAI landscape, this work contributes to a nuanced discourse on\nleveraging AI to effectively advance spatial research, policy, and\ndecision-making in an equitable, sustainable, and scientifically rigorous\nmanner.",
      "tldr_zh": "这篇论文探讨了开源大语言模型（LLMs）在地理空间人工智能（GeoAI）中的关键作用，强调它们通过提升数据处理、空间分析和决策支持能力，推动Geographic Information Science (GIScience)的创新。相比专有LLMs，开源替代方案提供更高的适应性、可重复性和社区驱动整合（如reinforcement learning和advanced spatial indexing），并符合FAIR principles，但需应对安全漏洞、伦理风险和治理挑战。论文主张，通过构建多样化的生态系统——结合开源基础、定制模型和跨学科合作——才能实现GeoAI的可持续、公平发展，从而为空间研究、政策和决策提供更具科学性的框架。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.17833v1",
      "published_date": "2025-04-24 13:20:17 UTC",
      "updated_date": "2025-04-24 13:20:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:20:29.415145"
    },
    {
      "arxiv_id": "2504.17531v3",
      "title": "Towards Machine-Generated Code for the Resolution of User Intentions",
      "title_zh": "翻译失败",
      "authors": [
        "Justus Flerlage",
        "Ilja Behnke",
        "Odej Kao"
      ],
      "abstract": "The growing capabilities of Artificial Intelligence (AI), particularly Large\nLanguage Models (LLMs), prompt a reassessment of the interaction mechanisms\nbetween users and their devices. Currently, users are required to use a set of\nhigh-level applications to achieve their desired results. However, the advent\nof AI may signal a shift in this regard, as its capabilities have generated\nnovel prospects for user-provided intent resolution through the deployment of\nmodel-generated code. This development represents a significant progression in\nthe realm of hybrid workflows, where human and artificial intelligence\ncollaborate to address user intentions, with the former responsible for\ndefining these intentions and the latter for implementing the solutions to\naddress them. In this paper, we investigate the feasibility of generating and\nexecuting workflows through code generation that results from prompting an LLM\nwith a concrete user intention, and a simplified application programming\ninterface for a GUI-less operating system. We provide an in-depth analysis and\ncomparison of various user intentions, the resulting code, and its execution.\nThe findings demonstrate the general feasibility of our approach and that the\nemployed LLM, GPT-4o-mini, exhibits remarkable proficiency in the generation of\ncode-oriented workflows in accordance with provided user intentions.",
      "tldr_zh": "这篇论文探讨了利用Large Language Models (LLMs)生成代码来实现用户意图的可能性，旨在通过人机协作的混合工作流（如用户定义意图，AI负责代码实现）重塑用户与设备交互方式。研究方法包括提示LLM（如GPT-4o-mini）生成工作流代码，并使用简化Application Programming Interface (API) 在无图形用户界面(GUI-less)操作系统上执行。实验分析显示，这种方法在处理各种用户意图时可行，且GPT-4o-mini在代码生成和执行方面表现出色。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.17531v3",
      "published_date": "2025-04-24 13:19:17 UTC",
      "updated_date": "2025-05-22 10:57:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:20:38.958585"
    },
    {
      "arxiv_id": "2504.17528v1",
      "title": "TACO: Tackling Over-correction in Federated Learning with Tailored Adaptive Correction",
      "title_zh": "翻译失败",
      "authors": [
        "Weijie Liu",
        "Ziwei Zhan",
        "Carlee Joe-Wong",
        "Edith Ngai",
        "Jingpu Duan",
        "Deke Guo",
        "Xu Chen",
        "Xiaoxi Zhang"
      ],
      "abstract": "Non-independent and identically distributed (Non-IID) data across edge\nclients have long posed significant challenges to federated learning (FL)\ntraining in edge computing environments. Prior works have proposed various\nmethods to mitigate this statistical heterogeneity. While these works can\nachieve good theoretical performance, in this work we provide the first\ninvestigation into a hidden over-correction phenomenon brought by the uniform\nmodel correction coefficients across clients adopted by existing methods. Such\nover-correction could degrade model performance and even cause failures in\nmodel convergence. To address this, we propose TACO, a novel algorithm that\naddresses the non-IID nature of clients' data by implementing fine-grained,\nclient-specific gradient correction and model aggregation, steering local\nmodels towards a more accurate global optimum. Moreover, we verify that leading\nFL algorithms generally have better model accuracy in terms of communication\nrounds rather than wall-clock time, resulting from their extra computation\noverhead imposed on clients. To enhance the training efficiency, TACO deploys a\nlightweight model correction and tailored aggregation approach that requires\nminimum computation overhead and no extra information beyond the synchronized\nmodel parameters. To validate TACO's effectiveness, we present the first FL\nconvergence analysis that reveals the root cause of over-correction. Extensive\nexperiments across various datasets confirm TACO's superior and stable\nperformance in practice.",
      "tldr_zh": "该论文探讨了联邦学习（FL）中，非独立同分布（Non-IID）数据导致的过修正（over-correction）问题，该问题会降低模型性能并影响收敛。作者提出TACO算法，通过细粒度的客户端特定梯度修正和模型聚合，实现更精确的全局优化，同时减少计算开销，仅需同步模型参数。实验结果显示，TACO在多种数据集上表现出优越的稳定性能，并在收敛分析中验证了其有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "I.2.6"
      ],
      "primary_category": "cs.LG",
      "comment": "11 pages, 7 figures, accepted by ICDCS 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.17528v1",
      "published_date": "2025-04-24 13:16:21 UTC",
      "updated_date": "2025-04-24 13:16:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:20:51.424963"
    },
    {
      "arxiv_id": "2504.17497v2",
      "title": "Combining GCN Structural Learning with LLM Chemical Knowledge for Enhanced Virtual Screening",
      "title_zh": "结合 GCN 结构学习与 LLM 化学知识以增强虚拟筛选",
      "authors": [
        "Radia Berreziga",
        "Mohammed Brahimi",
        "Khairedine Kraim",
        "Hamid Azzoune"
      ],
      "abstract": "Virtual screening plays a critical role in modern drug discovery by enabling\nthe identification of promising candidate molecules for experimental\nvalidation. Traditional machine learning methods such, as Support Vector\nMachines (SVM) and XGBoost, rely on predefined molecular representations, often\nleading to information loss and potential bias. In contrast, deep learning\napproaches-particularly Graph Convolutional Networks (GCNs)-offer a more\nexpressive and unbiased alternative by operating directly on molecular graphs.\nMeanwhile, Large Language Models (LLMs) have recently demonstrated\nstate-of-the-art performance in drug design, thanks to their capacity to\ncapture complex chemical patterns from large-scale data via attention\nmechanisms.\n  In this paper, we propose a hybrid architecture that integrates GCNs with\nLLM-derived embeddings to combine localized structural learning with global\nchemical knowledge. The LLM embeddings can be precomputed and stored in a\nmolecular feature library, removing the need to rerun the LLM during training\nor inference and thus maintaining computational efficiency. We found that\nconcatenating the LLM embeddings after each GCN layer-rather than only at the\nfinal layer-significantly improves performance, enabling deeper integration of\nglobal context throughout the network. The resulting model achieves superior\nresults, with an F1-score of (88.8\\%), outperforming standalone GCN (87.9%),\nXGBoost (85.5%), and SVM (85.4%) baselines.",
      "tldr_zh": "本研究提出了一种混合架构，将Graph Convolutional Networks (GCNs)的结构学习与Large Language Models (LLMs)的化学知识相结合，以提升虚拟筛选在药物发现中的性能。该方法通过预计算LLMs嵌入并在每个GCN层后连接它们，实现局部分子图学习与全局化学模式的深度整合，从而避免了信息丢失和偏差问题。实验结果显示，该模型的F1-score达到88.8%，显著优于独立的GCN（87.9%）、XGBoost（85.5%）和SVM（85.4%）基线，为高效的药物设计提供了更可靠的工具。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.17497v2",
      "published_date": "2025-04-24 12:38:03 UTC",
      "updated_date": "2025-04-26 11:37:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:21:04.211326"
    },
    {
      "arxiv_id": "2504.17493v1",
      "title": "Goal-Oriented Time-Series Forecasting: Foundation Framework Design",
      "title_zh": "面向目标的时间序列预测：基础框架设计",
      "authors": [
        "Luca-Andrei Fechete",
        "Mohamed Sana",
        "Fadhel Ayed",
        "Nicola Piovesan",
        "Wenjie Li",
        "Antonio De Domenico",
        "Tareq Si Salem"
      ],
      "abstract": "Traditional time-series forecasting often focuses only on minimizing\nprediction errors, ignoring the specific requirements of real-world\napplications that employ them. This paper presents a new training methodology,\nwhich allows a forecasting model to dynamically adjust its focus based on the\nimportance of forecast ranges specified by the end application. Unlike previous\nmethods that fix these ranges beforehand, our training approach breaks down\npredictions over the entire signal range into smaller segments, which are then\ndynamically weighted and combined to produce accurate forecasts. We tested our\nmethod on standard datasets, including a new dataset from wireless\ncommunication, and found that not only it improves prediction accuracy but also\nimproves the performance of end application employing the forecasting model.\nThis research provides a basis for creating forecasting systems that better\nconnect prediction and decision-making in various practical applications.",
      "tldr_zh": "这篇论文提出了一种面向目标的时间序列预测框架，允许模型根据实际应用的特定需求动态调整预测焦点，从而超越传统方法仅最小化预测错误的局限。不同于以往固定预测范围的做法，该框架将整个信号范围分解为小段落，并通过动态加权和组合来生成更准确的预测。在标准数据集（包括一个新的无线通信数据集）上测试，结果显示该方法不仅提升了预测准确性，还显著改善了使用该模型的最终应用性能。该研究为构建更紧密连接预测与决策的实际系统奠定了基础。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.17493v1",
      "published_date": "2025-04-24 12:34:43 UTC",
      "updated_date": "2025-04-24 12:34:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:21:15.401615"
    },
    {
      "arxiv_id": "2504.17490v1",
      "title": "Plasticine: Accelerating Research in Plasticity-Motivated Deep Reinforcement Learning",
      "title_zh": "Plasticine：加速可塑性驱动深度强化学习的研究",
      "authors": [
        "Mingqi Yuan",
        "Qi Wang",
        "Guozheng Ma",
        "Bo Li",
        "Xin Jin",
        "Yunbo Wang",
        "Xiaokang Yang",
        "Wenjun Zeng",
        "Dacheng Tao"
      ],
      "abstract": "Developing lifelong learning agents is crucial for artificial general\nintelligence. However, deep reinforcement learning (RL) systems often suffer\nfrom plasticity loss, where neural networks gradually lose their ability to\nadapt during training. Despite its significance, this field lacks unified\nbenchmarks and evaluation protocols. We introduce Plasticine, the first\nopen-source framework for benchmarking plasticity optimization in deep RL.\nPlasticine provides single-file implementations of over 13 mitigation methods,\n10 evaluation metrics, and learning scenarios with increasing non-stationarity\nlevels from standard to open-ended environments. This framework enables\nresearchers to systematically quantify plasticity loss, evaluate mitigation\nstrategies, and analyze plasticity dynamics across different contexts. Our\ndocumentation, examples, and source code are available at\nhttps://github.com/RLE-Foundation/Plasticine.",
      "tldr_zh": "该论文探讨了深度强化学习（RL）中可塑性损失（plasticity loss）问题，这种现象导致神经网络在训练过程中逐渐丧失适应能力，阻碍了终身学习代理的开发。研究者引入了 Plasticine，这是一个开源框架，用于加速基于可塑性的深度 RL 研究，提供超过 13 种缓解方法的单文件实现、10 种评估指标，以及从标准到开放式环境的非平稳性水平递增的学习场景。该框架允许研究人员系统量化可塑性损失、评估缓解策略，并分析不同环境下的可塑性动态，最终促进该领域的统一基准和优化。Plasticine 的文档、示例和源代码可通过 https://github.com/RLE-Foundation/Plasticine 获取。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "23 pages",
      "pdf_url": "http://arxiv.org/pdf/2504.17490v1",
      "published_date": "2025-04-24 12:32:13 UTC",
      "updated_date": "2025-04-24 12:32:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:21:27.768416"
    },
    {
      "arxiv_id": "2504.18594v1",
      "title": "A Simple DropConnect Approach to Transfer-based Targeted Attack",
      "title_zh": "翻译失败",
      "authors": [
        "Tongrui Su",
        "Qingbin Li",
        "Shengyu Zhu",
        "Wei Chen",
        "Xueqi Cheng"
      ],
      "abstract": "We study the problem of transfer-based black-box attack, where adversarial\nsamples generated using a single surrogate model are directly applied to target\nmodels. Compared with untargeted attacks, existing methods still have lower\nAttack Success Rates (ASRs) in the targeted setting, i.e., the obtained\nadversarial examples often overfit the surrogate model but fail to mislead\nother models. In this paper, we hypothesize that the pixels or features in\nthese adversarial examples collaborate in a highly dependent manner to maximize\nthe success of an adversarial attack on the surrogate model, which we refer to\nas perturbation co-adaptation. Then, we propose to Mitigate perturbation\nCo-adaptation by DropConnect (MCD) to enhance transferability, by creating\ndiverse variants of surrogate model at each optimization iteration. We conduct\nextensive experiments across various CNN- and Transformer-based models to\ndemonstrate the effectiveness of MCD. In the challenging scenario of\ntransferring from a CNN-based model to Transformer-based models, MCD achieves\n13% higher average ASRs compared with state-of-the-art baselines. MCD boosts\nthe performance of self-ensemble methods by bringing in more diversification\nacross the variants while reserving sufficient semantic information for each\nvariant. In addition, MCD attains the highest performance gain when scaling the\ncompute of crafting adversarial examples.",
      "tldr_zh": "这篇论文研究了转移-based 针对性黑盒攻击的问题，提出了一种简单有效的 DropConnect 方法（MCD），通过缓解扰动协同适应（perturbation co-adaptation）来创建代理模型的多样变体，从而提升对抗样本的转移性。MCD 在各种 CNN- 和 Transformer-based 模型上进行了广泛实验，在从 CNN 模型到 Transformer 模型的挑战性场景中，比最先进基线平均攻击成功率（ASRs）提高了 13%。此外，该方法还能增强自集合方法的性能，并在扩展计算资源时实现最大性能提升。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.18594v1",
      "published_date": "2025-04-24 12:29:23 UTC",
      "updated_date": "2025-04-24 12:29:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:21:39.763364"
    },
    {
      "arxiv_id": "2504.17474v1",
      "title": "Enhanced Sample Selection with Confidence Tracking: Identifying Correctly Labeled yet Hard-to-Learn Samples in Noisy Data",
      "title_zh": "翻译失败",
      "authors": [
        "Weiran Pan",
        "Wei Wei",
        "Feida Zhu",
        "Yong Deng"
      ],
      "abstract": "We propose a novel sample selection method for image classification in the\npresence of noisy labels. Existing methods typically consider small-loss\nsamples as correctly labeled. However, some correctly labeled samples are\ninherently difficult for the model to learn and can exhibit high loss similar\nto mislabeled samples in the early stages of training. Consequently, setting a\nthreshold on per-sample loss to select correct labels results in a trade-off\nbetween precision and recall in sample selection: a lower threshold may miss\nmany correctly labeled hard-to-learn samples (low recall), while a higher\nthreshold may include many mislabeled samples (low precision). To address this\nissue, our goal is to accurately distinguish correctly labeled yet\nhard-to-learn samples from mislabeled ones, thus alleviating the trade-off\ndilemma. We achieve this by considering the trends in model prediction\nconfidence rather than relying solely on loss values. Empirical observations\nshow that only for correctly labeled samples, the model's prediction confidence\nfor the annotated labels typically increases faster than for any other classes.\nBased on this insight, we propose tracking the confidence gaps between the\nannotated labels and other classes during training and evaluating their trends\nusing the Mann-Kendall Test. A sample is considered potentially correctly\nlabeled if all its confidence gaps tend to increase. Our method functions as a\nplug-and-play component that can be seamlessly integrated into existing sample\nselection techniques. Experiments on several standard benchmarks and real-world\ndatasets demonstrate that our method enhances the performance of existing\nmethods for learning with noisy labels.",
      "tldr_zh": "本文提出了一种增强样本选择方法，通过跟踪模型预测置信度的趋势（而非仅依赖损失值），来准确识别噪声数据中正确标记但难学习的样本，从而缓解现有方法在精度和召回率间的权衡困境。方法基于观察到正确标签样本的置信度差距（annotated labels 与其他类别的差异）通常会更快增加，并使用 Mann-Kendall Test 评估这些趋势的增长情况，以筛选潜在正确样本。该方法可作为插件整合到现有技术中，实验在多个标准基准和真实数据集上证明了其性能提升。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.17474v1",
      "published_date": "2025-04-24 12:07:14 UTC",
      "updated_date": "2025-04-24 12:07:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:21:51.604471"
    },
    {
      "arxiv_id": "2504.17471v1",
      "title": "GRANITE : a Byzantine-Resilient Dynamic Gossip Learning Framework",
      "title_zh": "翻译失败",
      "authors": [
        "Yacine Belal",
        "Mohamed Maouche",
        "Sonia Ben Mokhtar",
        "Anthony Simonet-Boulogne"
      ],
      "abstract": "Gossip Learning (GL) is a decentralized learning paradigm where users\niteratively exchange and aggregate models with a small set of neighboring\npeers. Recent GL approaches rely on dynamic communication graphs built and\nmaintained using Random Peer Sampling (RPS) protocols. Thanks to graph\ndynamics, GL can achieve fast convergence even over extremely sparse\ntopologies. However, the robustness of GL over dy- namic graphs to Byzantine\n(model poisoning) attacks remains unaddressed especially when Byzantine nodes\nattack the RPS protocol to scale up model poisoning. We address this issue by\nintroducing GRANITE, a framework for robust learning over sparse, dynamic\ngraphs in the presence of a fraction of Byzantine nodes. GRANITE relies on two\nkey components (i) a History-aware Byzantine-resilient Peer Sampling protocol\n(HaPS), which tracks previously encountered identifiers to reduce adversarial\ninfluence over time, and (ii) an Adaptive Probabilistic Threshold (APT), which\nleverages an estimate of Byzantine presence to set aggregation thresholds with\nformal guarantees. Empirical results confirm that GRANITE maintains convergence\nwith up to 30% Byzantine nodes, improves learning speed via adaptive filtering\nof poisoned models and obtains these results in up to 9 times sparser graphs\nthan dictated by current theory.",
      "tldr_zh": "该论文提出GRANITE框架，用于提升Gossip Learning（GL）在动态通信图上的鲁棒性，特别是在面对Byzantine（拜占庭）攻击时，这些攻击可能通过毒化模型或攻击Random Peer Sampling（RPS）协议来放大影响。GRANITE的核心组件包括History-aware Byzantine-resilient Peer Sampling（HaPS）协议，该协议通过跟踪历史遇到的标识符来减少对手影响，以及Adaptive Probabilistic Threshold（APT）机制，该机制利用对Byzantine节点的估计设置聚合阈值，并提供正式保证。实验结果显示，GRANITE在高达30%的Byzantine节点环境下保持收敛速度，通过自适应过滤有毒模型实现更快学习，并在比当前理论建议稀疏9倍的图上取得优异性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.17471v1",
      "published_date": "2025-04-24 12:03:15 UTC",
      "updated_date": "2025-04-24 12:03:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:22:05.287083"
    },
    {
      "arxiv_id": "2504.17461v1",
      "title": "Evaluating Time Series Models for Urban Wastewater Management: Predictive Performance, Model Complexity and Resilience",
      "title_zh": "翻译失败",
      "authors": [
        "Vipin Singh",
        "Tianheng Ling",
        "Teodor Chiaburu",
        "Felix Biessmann"
      ],
      "abstract": "Climate change increases the frequency of extreme rainfall, placing a\nsignificant strain on urban infrastructures, especially Combined Sewer Systems\n(CSS). Overflows from overburdened CSS release untreated wastewater into\nsurface waters, posing environmental and public health risks. Although\ntraditional physics-based models are effective, they are costly to maintain and\ndifficult to adapt to evolving system dynamics. Machine Learning (ML)\napproaches offer cost-efficient alternatives with greater adaptability. To\nsystematically assess the potential of ML for modeling urban infrastructure\nsystems, we propose a protocol for evaluating Neural Network architectures for\nCSS time series forecasting with respect to predictive performance, model\ncomplexity, and robustness to perturbations. In addition, we assess model\nperformance on peak events and critical fluctuations, as these are the key\nregimes for urban wastewater management. To investigate the feasibility of\nlightweight models suitable for IoT deployment, we compare global models, which\nhave access to all information, with local models, which rely solely on nearby\nsensor readings. Additionally, to explore the security risks posed by network\noutages or adversarial attacks on urban infrastructure, we introduce error\nmodels that assess the resilience of models. Our results demonstrate that while\nglobal models achieve higher predictive performance, local models provide\nsufficient resilience in decentralized scenarios, ensuring robust modeling of\nurban infrastructure. Furthermore, models with longer native forecast horizons\nexhibit greater robustness to data perturbations. These findings contribute to\nthe development of interpretable and reliable ML solutions for sustainable\nurban wastewater management. The implementation is available in our GitHub\nrepository.",
      "tldr_zh": "这篇论文评估了机器学习（ML）模型在城市废水管理中的应用，特别是针对Combined Sewer Systems (CSS)的时间序列预测，以应对气候变化导致的极端降雨挑战。研究提出一个评估协议，比较神经网络架构的预测性能、模型复杂性和鲁棒性，包括全局模型（访问所有信息）和本地模型（仅依赖附近传感器数据），并测试模型在峰值事件和数据扰动下的表现。结果显示，全局模型在预测准确性上优于本地模型，但本地模型在去中心化场景（如IoT部署）中更具弹性，且具有更长原生预测地平线的模型对网络中断或攻击更耐受。这些发现为开发可解释且可靠的ML解决方案提供了指导，促进可持续的城市废水管理。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "6 pages, 6 figures, accepted at 10th International Conference on\n  Smart and Sustainable Technologies (SpliTech) 2025, GitHub:\n  https://github.com/calgo-lab/resilient-timeseries-evaluation",
      "pdf_url": "http://arxiv.org/pdf/2504.17461v1",
      "published_date": "2025-04-24 11:52:13 UTC",
      "updated_date": "2025-04-24 11:52:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:22:17.236185"
    },
    {
      "arxiv_id": "2504.17449v1",
      "title": "HMI: Hierarchical Knowledge Management for Efficient Multi-Tenant Inference in Pretrained Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Jun Zhang",
        "Jue Wang",
        "Huan Li",
        "Lidan Shou",
        "Ke Chen",
        "Gang Chen",
        "Qin Xie",
        "Guiming Xie",
        "Xuejian Gong"
      ],
      "abstract": "The significant computational demands of pretrained language models (PLMs),\nwhich often require dedicated hardware, present a substantial challenge in\nserving them efficiently, especially in multi-tenant environments. To address\nthis, we introduce HMI, a Hierarchical knowledge management-based Multi-tenant\nInference system, designed to manage tenants with distinct PLMs\nresource-efficiently. Our approach is three-fold: Firstly, we categorize PLM\nknowledge into general, domain-specific, and task-specific. Leveraging insights\non knowledge acquisition across different model layers, we construct\nhierarchical PLMs (hPLMs) by extracting and storing knowledge at different\nlevels, significantly reducing GPU memory usage per tenant. Secondly, we\nestablish hierarchical knowledge management for hPLMs generated by various\ntenants in HMI. We manage domain-specific knowledge with acceptable storage\nincreases by constructing and updating domain-specific knowledge trees based on\nfrequency. We manage task-specific knowledge within limited GPU memory through\nparameter swapping. Finally, we propose system optimizations to enhance\nresource utilization and inference throughput. These include fine-grained\npipelining via hierarchical knowledge prefetching to overlap CPU and I/O\noperations with GPU computations, and optimizing parallel implementations with\nbatched matrix multiplications. Our experimental results demonstrate that the\nproposed HMI can efficiently serve up to 10,000 hPLMs (hBERTs and hGPTs) on a\nsingle GPU, with only a negligible compromise in accuracy.",
      "tldr_zh": "本文提出 HMI 系统，一种基于分层知识管理的多租户推理框架，旨在解决预训练语言模型 (PLMs) 在多租户环境中的高计算需求问题。通过将 PLM 知识分类为一般、领域特定和任务特定，并构建分层 PLMs (hPLMs) 来提取和存储不同层级知识，该系统显著降低了每个租户的 GPU 内存使用。HMI 进一步通过基于频率的领域特定知识树管理、任务特定参数交换，以及系统优化如分层知识预取和批量矩阵乘法，提升了资源利用率和推理吞吐量。实验结果表明，HMI 能在单个 GPU 上高效服务多达 10,000 个 hPLMs (如 hBERTs 和 hGPTs)，准确性损失微乎其微。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by VLDBJ 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.17449v1",
      "published_date": "2025-04-24 11:28:40 UTC",
      "updated_date": "2025-04-24 11:28:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:22:30.243109"
    },
    {
      "arxiv_id": "2504.17447v1",
      "title": "FRAG: Frame Selection Augmented Generation for Long Video and Long Document Understanding",
      "title_zh": "翻译失败",
      "authors": [
        "De-An Huang",
        "Subhashree Radhakrishnan",
        "Zhiding Yu",
        "Jan Kautz"
      ],
      "abstract": "There has been impressive progress in Large Multimodal Models (LMMs). Recent\nworks extend these models to long inputs, including multi-page documents and\nlong videos. However, the model size and performance of these long context\nmodels are still limited due to the computational cost in both training and\ninference. In this work, we explore an orthogonal direction and process long\ninputs without long context LMMs. We propose Frame Selection Augmented\nGeneration (FRAG), where the model first selects relevant frames within the\ninput, and then only generates the final outputs based on the selected frames.\nThe core of the selection process is done by scoring each frame independently,\nwhich does not require long context processing. The frames with the highest\nscores are then selected by a simple Top-K selection. We show that this\nfrustratingly simple framework is applicable to both long videos and multi-page\ndocuments using existing LMMs without any fine-tuning. We consider two models,\nLLaVA-OneVision and InternVL2, in our experiments and show that FRAG\nconsistently improves the performance and achieves state-of-the-art\nperformances for both long video and long document understanding. For videos,\nFRAG substantially improves InternVL2-76B by 5.8% on MLVU and 3.7% on\nVideo-MME. For documents, FRAG achieves over 20% improvements on MP-DocVQA\ncompared with recent LMMs specialized in long document understanding. Code is\navailable at: https://github.com/NVlabs/FRAG",
      "tldr_zh": "本论文提出 FRAG（Frame Selection Augmented Generation）框架，用于提升 Large Multimodal Models (LMMs) 在长视频和长文档理解方面的性能，而无需依赖长上下文模型。\nFRAG 的核心方法是先独立评分每个输入帧，然后通过 Top-K 选择机制选取相关帧，仅基于这些帧生成输出，从而减少计算成本，使用现有模型如 LLaVA-OneVision 和 InternVL2 无需微调。\n实验结果显示，FRAG 显著提高了基准性能：在视频任务上，InternVL2-76B 在 MLVU 上提升 5.8%、在 Video-MME 上提升 3.7%；在文档任务上，在 MP-DocVQA 上比专业 LMMs 提升超过 20%，达到了 state-of-the-art 水平。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.17447v1",
      "published_date": "2025-04-24 11:19:18 UTC",
      "updated_date": "2025-04-24 11:19:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:22:42.849149"
    },
    {
      "arxiv_id": "2504.17428v1",
      "title": "Detection, Classification and Prevalence of Self-Admitted Aging Debt",
      "title_zh": "翻译失败",
      "authors": [
        "Murali Sridharan",
        "Mika Mäntylä",
        "Leevi Rantala"
      ],
      "abstract": "Context: Previous research on software aging is limited with focus on dynamic\nruntime indicators like memory and performance, often neglecting evolutionary\nindicators like source code comments and narrowly examining legacy issues\nwithin the TD context. Objective: We introduce the concept of Aging Debt (AD),\nrepresenting the increased maintenance efforts and costs needed to keep\nsoftware updated. We study AD through Self-Admitted Aging Debt (SAAD) observed\nin source code comments left by software developers. Method: We employ a\nmixed-methods approach, combining qualitative and quantitative analyses to\ndetect and measure AD in software. This includes framing SAAD patterns from the\nsource code comments after analysing the source code context, then utilizing\nthe SAAD patterns to detect SAAD comments. In the process, we develop a\ntaxonomy for SAAD that reflects the temporal aging of software and its\nassociated debt. Then we utilize the taxonomy to quantify the different types\nof AD prevalent in OSS repositories. Results: Our proposed taxonomy categorizes\ntemporal software aging into Active and Dormant types. Our extensive analysis\nof over 9,000+ Open Source Software (OSS) repositories reveals that more than\n21% repositories exhibit signs of SAAD as observed from our gold standard SAAD\ndataset. Notably, Dormant AD emerges as the predominant category, highlighting\na critical but often overlooked aspect of software maintenance. Conclusion: As\nsoftware volume grows annually, so do evolutionary aging and maintenance\nchallenges; our proposed taxonomy can aid researchers in detailed software\naging studies and help practitioners develop improved and proactive maintenance\nstrategies.",
      "tldr_zh": "这篇论文引入了Aging Debt (AD)概念，指的是维护软件更新所需的额外努力和成本，并通过Self-Admitted Aging Debt (SAAD)——开发人员在源代码注释中承认的老化债务——来进行研究。研究采用混合方法，包括从源代码中提取SAAD模式并开发一个将软件老化分为Active和Dormant类型的taxonomy，用于检测和量化AD在开源软件(OSS)仓库中的分布。结果显示，在超过9,000个OSS仓库中，超过21%的仓库存在SAAD迹象，其中Dormant AD是最常见类型。最终，该taxonomy可帮助研究人员深入分析软件老化和从业者制定更主动的维护策略。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CE",
        "cs.GL",
        "D.2.7; D.2.9"
      ],
      "primary_category": "cs.SE",
      "comment": "Draft",
      "pdf_url": "http://arxiv.org/pdf/2504.17428v1",
      "published_date": "2025-04-24 10:38:55 UTC",
      "updated_date": "2025-04-24 10:38:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:22:53.921544"
    },
    {
      "arxiv_id": "2504.17426v1",
      "title": "Towards Leveraging Large Language Model Summaries for Topic Modeling in Source Code",
      "title_zh": "翻译失败",
      "authors": [
        "Michele Carissimi",
        "Martina Saletta",
        "Claudio Ferretti"
      ],
      "abstract": "Understanding source code is a topic of great interest in the software\nengineering community, since it can help programmers in various tasks such as\nsoftware maintenance and reuse. Recent advances in large language models (LLMs)\nhave demonstrated remarkable program comprehension capabilities, while\ntransformer-based topic modeling techniques offer effective ways to extract\nsemantic information from text. This paper proposes and explores a novel\napproach that combines these strengths to automatically identify meaningful\ntopics in a corpus of Python programs. Our method consists in applying topic\nmodeling on the descriptions obtained by asking an LLM to summarize the code.\nTo assess the internal consistency of the extracted topics, we compare them\nagainst topics inferred from function names alone, and those derived from\nexisting docstrings. Experimental results suggest that leveraging LLM-generated\nsummaries provides interpretable and semantically rich representation of code\nstructure. The promising results suggest that our approach can be fruitfully\napplied in various software engineering tasks such as automatic documentation\nand tagging, code search, software reorganization and knowledge discovery in\nlarge repositories.",
      "tldr_zh": "本研究探讨了利用大型语言模型（LLMs）生成的代码总结来进行主题建模，以更好地理解源代码结构。该方法涉及使用 LLMs 自动总结 Python 程序，然后应用主题建模技术提取语义主题，并与从函数名和文档字符串（docstrings）中推断的主题进行比较。实验结果显示，基于 LLM 总结的主题更具可解释性和语义丰富性。该方法有望应用于软件工程任务，如自动文档生成、代码搜索、软件重组和知识发现。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.17426v1",
      "published_date": "2025-04-24 10:30:40 UTC",
      "updated_date": "2025-04-24 10:30:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:23:04.244069"
    },
    {
      "arxiv_id": "2504.17424v1",
      "title": "Object Pose Estimation by Camera Arm Control Based on the Next Viewpoint Estimation",
      "title_zh": "翻译失败",
      "authors": [
        "Tomoki Mizuno",
        "Kazuya Yabashi",
        "Tsuyoshi Tasaki"
      ],
      "abstract": "We have developed a new method to estimate a Next Viewpoint (NV) which is\neffective for pose estimation of simple-shaped products for product display\nrobots in retail stores. Pose estimation methods using Neural Networks (NN)\nbased on an RGBD camera are highly accurate, but their accuracy significantly\ndecreases when the camera acquires few texture and shape features at a current\nview point. However, it is difficult for previous mathematical model-based\nmethods to estimate effective NV which is because the simple shaped objects\nhave few shape features. Therefore, we focus on the relationship between the\npose estimation and NV estimation. When the pose estimation is more accurate,\nthe NV estimation is more accurate. Therefore, we develop a new pose estimation\nNN that estimates NV simultaneously. Experimental results showed that our NV\nestimation realized a pose estimation success rate 77.3\\%, which was 7.4pt\nhigher than the mathematical model-based NV calculation did. Moreover, we\nverified that the robot using our method displayed 84.2\\% of products.",
      "tldr_zh": "本研究提出了一种基于 Next Viewpoint (NV) 估计的新方法，用于提升简单形状产品的姿态估计准确性，应用于零售店的产品展示机器人。方法通过开发一个新的 Neural Networks (NN) 模型，同时估计物体姿态和下一个视点，从而利用姿态估计的准确性来改进 NV 估计，解决了现有 RGBD 相机方法在缺少纹理和形状特征时的局限性。实验结果显示，该方法实现了77.3%的姿态估计成功率，比基于数学模型的 NV 计算高出7.4%，并使机器人成功展示84.2%的产品。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.17424v1",
      "published_date": "2025-04-24 10:26:14 UTC",
      "updated_date": "2025-04-24 10:26:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:23:19.016883"
    },
    {
      "arxiv_id": "2504.17421v1",
      "title": "Towards Harnessing the Collaborative Power of Large and Small Models for Domain Tasks",
      "title_zh": "翻译失败",
      "authors": [
        "Yang Liu",
        "Bingjie Yan",
        "Tianyuan Zou",
        "Jianqing Zhang",
        "Zixuan Gu",
        "Jianbing Ding",
        "Xidong Wang",
        "Jingyi Li",
        "Xiaozhou Ye",
        "Ye Ouyang",
        "Qiang Yang",
        "Ya-Qin Zhang"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities, but\nthey require vast amounts of data and computational resources. In contrast,\nsmaller models (SMs), while less powerful, can be more efficient and tailored\nto specific domains. In this position paper, we argue that taking a\ncollaborative approach, where large and small models work synergistically, can\naccelerate the adaptation of LLMs to private domains and unlock new potential\nin AI. We explore various strategies for model collaboration and identify\npotential challenges and opportunities. Building upon this, we advocate for\nindustry-driven research that prioritizes multi-objective benchmarks on\nreal-world private datasets and applications.",
      "tldr_zh": "本研究探讨了大型语言模型 (LLMs) 与小型模型 (SMs) 的协同合作潜力，旨在加速 LLMs 在私有领域的适应。LLMs 虽强大但需大量资源，而 SMs 更高效且可定制，通过协作策略如模型结合和知识共享，可以解锁 AI 的新潜力。文章识别了潜在挑战和机会，并倡导基于真实私有数据集的行业驱动研究，强调多目标基准测试以推动实际应用。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.17421v1",
      "published_date": "2025-04-24 10:24:35 UTC",
      "updated_date": "2025-04-24 10:24:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:23:28.538025"
    },
    {
      "arxiv_id": "2504.17404v2",
      "title": "Redefining Superalignment: From Weak-to-Strong Alignment to Human-AI Co-Alignment to Sustainable Symbiotic Society",
      "title_zh": "翻译失败",
      "authors": [
        "Yi Zeng",
        "Feifei Zhao",
        "Yuwei Wang",
        "Enmeng Lu",
        "Yaodong Yang",
        "Lei Wang",
        "Chao Liu",
        "Yitao Liang",
        "Dongcheng Zhao",
        "Bing Han",
        "Haibo Tong",
        "Yao Liang",
        "Dongqi Liang",
        "Kang Sun",
        "Boyuan Chen",
        "Jinyu Fan"
      ],
      "abstract": "Artificial Intelligence (AI) systems are becoming increasingly powerful and\nautonomous, and may progress to surpass human intelligence levels, namely\nArtificial Superintelligence (ASI). During the progression from AI to ASI, it\nmay exceed human control, violate human values, and even lead to irreversible\ncatastrophic consequences in extreme cases. This gives rise to a pressing issue\nthat needs to be addressed: superalignment, ensuring that AI systems much\nsmarter than humans, remain aligned with human (compatible) intentions and\nvalues. Existing scalable oversight and weak-to-strong generalization methods\nmay prove substantially infeasible and inadequate when facing ASI. We must\nexplore safer and more pluralistic frameworks and approaches for\nsuperalignment. In this paper, we redefine superalignment as the human-AI\nco-alignment towards a sustainable symbiotic society, and highlight a framework\nthat integrates external oversight and intrinsic proactive alignment. External\noversight superalignment should be grounded in human-centered ultimate\ndecision, supplemented by interpretable automated evaluation and correction, to\nachieve continuous alignment with humanity's evolving values. Intrinsic\nproactive superalignment is rooted in a profound understanding of the Self,\nothers, and society, integrating self-awareness, self-reflection, and empathy\nto spontaneously infer human intentions, distinguishing good from evil and\nproactively considering human well-being, ultimately attaining human-AI\nco-alignment through iterative interaction. The integration of\nexternally-driven oversight with intrinsically-driven proactive alignment\nempowers sustainable symbiotic societies through human-AI co-alignment, paving\nthe way for achieving safe and beneficial AGI and ASI for good, for human, and\nfor a symbiotic ecology.",
      "tldr_zh": "该论文重新定义了超对齐（superalignment），从传统的弱到强对齐扩展到人类-AI 共同对齐（human-AI co-alignment），旨在构建可持续的共生社会，以应对人工智能（AI）可能超越人类智能（Artificial Superintelligence, ASI）并导致失控或违背人类价值观的风险。论文提出一个整合框架，包括外部监督（external oversight），以人为中心进行最终决策，并辅以可解释的自动评估和修正，确保AI持续符合人类演变的值；以及内在主动对齐（intrinsic proactive alignment），通过AI的自我意识、自我反思和同理心，主动推断人类意图、区分善恶，并通过迭代互动实现共同对齐。这种方法整合外部驱动和内在驱动，有望促进安全有益的AGI和ASI发展，构建人类-AI 可持续共生生态。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.17404v2",
      "published_date": "2025-04-24 09:53:49 UTC",
      "updated_date": "2025-04-25 15:32:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:23:41.379917"
    },
    {
      "arxiv_id": "2504.17402v1",
      "title": "Assessing the Capability of Large Language Models for Domain-Specific Ontology Generation",
      "title_zh": "评估大语言模型在特定领域本体生成方面的能力",
      "authors": [
        "Anna Sofia Lippolis",
        "Mohammad Javad Saeedizade",
        "Robin Keskisarkka",
        "Aldo Gangemi",
        "Eva Blomqvist",
        "Andrea Giovanni Nuzzolese"
      ],
      "abstract": "Large Language Models (LLMs) have shown significant potential for ontology\nengineering. However, it is still unclear to what extent they are applicable to\nthe task of domain-specific ontology generation. In this study, we explore the\napplication of LLMs for automated ontology generation and evaluate their\nperformance across different domains. Specifically, we investigate the\ngeneralizability of two state-of-the-art LLMs, DeepSeek and o1-preview, both\nequipped with reasoning capabilities, by generating ontologies from a set of\ncompetency questions (CQs) and related user stories. Our experimental setup\ncomprises six distinct domains carried out in existing ontology engineering\nprojects and a total of 95 curated CQs designed to test the models' reasoning\nfor ontology engineering. Our findings show that with both LLMs, the\nperformance of the experiments is remarkably consistent across all domains,\nindicating that these methods are capable of generalizing ontology generation\ntasks irrespective of the domain. These results highlight the potential of\nLLM-based approaches in achieving scalable and domain-agnostic ontology\nconstruction and lay the groundwork for further research into enhancing\nautomated reasoning and knowledge representation techniques.",
      "tldr_zh": "本文评估了Large Language Models (LLMs) 在领域特定本体生成中的能力，聚焦于DeepSeek和o1-preview 两个具备推理能力的模型。研究通过从95个精心设计的competency questions (CQs) 和相关用户故事中生成本体，并应用于六个不同领域，测试了模型的泛化性能。结果显示，LLMs在所有领域的表现高度一致，证明其可实现可扩展且领域无关的本体构建，为进一步提升自动化推理和知识表示技术奠定了基础。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.17402v1",
      "published_date": "2025-04-24 09:47:14 UTC",
      "updated_date": "2025-04-24 09:47:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:23:52.649043"
    },
    {
      "arxiv_id": "2504.17401v1",
      "title": "StereoMamba: Real-time and Robust Intraoperative Stereo Disparity Estimation via Long-range Spatial Dependencies",
      "title_zh": "翻译失败",
      "authors": [
        "Xu Wang",
        "Jialang Xu",
        "Shuai Zhang",
        "Baoru Huang",
        "Danail Stoyanov",
        "Evangelos B. Mazomenos"
      ],
      "abstract": "Stereo disparity estimation is crucial for obtaining depth information in\nrobot-assisted minimally invasive surgery (RAMIS). While current deep learning\nmethods have made significant advancements, challenges remain in achieving an\noptimal balance between accuracy, robustness, and inference speed. To address\nthese challenges, we propose the StereoMamba architecture, which is\nspecifically designed for stereo disparity estimation in RAMIS. Our approach is\nbased on a novel Feature Extraction Mamba (FE-Mamba) module, which enhances\nlong-range spatial dependencies both within and across stereo images. To\neffectively integrate multi-scale features from FE-Mamba, we then introduce a\nnovel Multidimensional Feature Fusion (MFF) module. Experiments against the\nstate-of-the-art on the ex-vivo SCARED benchmark demonstrate that StereoMamba\nachieves superior performance on EPE of 2.64 px and depth MAE of 2.55 mm, the\nsecond-best performance on Bad2 of 41.49% and Bad3 of 26.99%, while maintaining\nan inference speed of 21.28 FPS for a pair of high-resolution images\n(1280*1024), striking the optimum balance between accuracy, robustness, and\nefficiency. Furthermore, by comparing synthesized right images, generated from\nwarping left images using the generated disparity maps, with the actual right\nimage, StereoMamba achieves the best average SSIM (0.8970) and PSNR (16.0761),\nexhibiting strong zero-shot generalization on the in-vivo RIS2017 and StereoMIS\ndatasets.",
      "tldr_zh": "本研究针对机器人辅助微创手术 (RAMIS) 中的立体视差估计问题，提出 StereoMamba 架构，以实现准确性、鲁棒性和实时性的最佳平衡。该架构基于新型 Feature Extraction Mamba (FE-Mamba) 模块增强立体图像内和间的长距离空间依赖，并引入 Multidimensional Feature Fusion (MFF) 模块来整合多尺度特征。在 ex-vivo SCARED 基准测试中，StereoMamba 取得了 EPE 2.64 px、深度 MAE 2.55 mm 的优异性能，同时保持 21.28 FPS 的推理速度，并在 Bad2 和 Bad3 指标上位居前列。此外，该方法在零样本泛化上表现出色，在 in-vivo RIS2017 和 StereoMIS 数据集上实现了最佳的 SSIM (0.8970) 和 PSNR (16.0761)。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.17401v1",
      "published_date": "2025-04-24 09:46:15 UTC",
      "updated_date": "2025-04-24 09:46:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:24:06.744015"
    },
    {
      "arxiv_id": "2504.18593v1",
      "title": "Severity Classification of Chronic Obstructive Pulmonary Disease in Intensive Care Units: A Semi-Supervised Approach Using MIMIC-III Dataset",
      "title_zh": "慢性阻塞性肺疾病在重症监护病房中的严重程度分类：一种使用 MIMIC-III 数据集的半监督方法",
      "authors": [
        "Akram Shojaei",
        "Mehdi Delrobaei"
      ],
      "abstract": "Chronic obstructive pulmonary disease (COPD) represents a significant global\nhealth burden, where precise severity assessment is particularly critical for\neffective clinical management in intensive care unit (ICU) settings. This study\nintroduces an innovative machine learning framework for COPD severity\nclassification utilizing the MIMIC-III critical care database, thereby\nexpanding the applications of artificial intelligence in critical care\nmedicine. Our research developed a robust classification model incorporating\nkey ICU parameters such as blood gas measurements and vital signs, while\nimplementing semi-supervised learning techniques to effectively utilize\nunlabeled data and enhance model performance. The random forest classifier\nemerged as particularly effective, demonstrating exceptional discriminative\ncapability with 92.51% accuracy and 0.98 ROC AUC in differentiating between\nmild-to-moderate and severe COPD cases. This machine learning approach provides\nclinicians with a practical, accurate, and efficient tool for rapid COPD\nseverity evaluation in ICU environments, with significant potential to improve\nboth clinical decision-making processes and patient outcomes. Future research\ndirections should prioritize external validation across diverse patient\npopulations and integration with clinical decision support systems to optimize\nCOPD management in critical care settings.",
      "tldr_zh": "这项研究针对ICU中的慢性阻塞性肺病（COPD）严重程度分类，提出了一种基于MIMIC-III数据集的半监督学习框架，利用血气测量和生命体征等关键参数来提升模型性能。随机森林分类器在该框架中表现出色，实现了92.51%的准确率和0.98的ROC AUC，在区分轻度-中度与严重COPD病例方面表现出卓越的鉴别能力。该方法为临床医生提供了一个快速、准确的评估工具，有助于改善决策过程和患者预后，并建议未来通过外部验证和与临床决策支持系统的整合来优化应用。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.18593v1",
      "published_date": "2025-04-24 09:37:52 UTC",
      "updated_date": "2025-04-24 09:37:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:24:17.621260"
    },
    {
      "arxiv_id": "2504.17393v2",
      "title": "Towards User-Centred Design of AI-Assisted Decision-Making in Law Enforcement",
      "title_zh": "迈向以用户为中心的人工智能辅助执法决策设计",
      "authors": [
        "Vesna Nowack",
        "Dalal Alrajeh",
        "Carolina Gutierrez Muñoz",
        "Katie Thomas",
        "William Hobson",
        "Patrick Benjamin",
        "Catherine Hamilton-Giachritsis",
        "Tim Grant",
        "Juliane A. Kloess",
        "Jessica Woodhams"
      ],
      "abstract": "Artificial Intelligence (AI) has become an important part of our everyday\nlives, yet user requirements for designing AI-assisted systems in law\nenforcement remain unclear. To address this gap, we conducted qualitative\nresearch on decision-making within a law enforcement agency. Our study aimed to\nidentify limitations of existing practices, explore user requirements and\nunderstand the responsibilities that humans expect to undertake in these\nsystems.\n  Participants in our study highlighted the need for a system capable of\nprocessing and analysing large volumes of data efficiently to help in crime\ndetection and prevention. Additionally, the system should satisfy requirements\nfor scalability, accuracy, justification, trustworthiness and adaptability to\nbe adopted in this domain. Participants also emphasised the importance of\nhaving end users review the input data that might be challenging for AI to\ninterpret, and validate the generated output to ensure the system's accuracy.\nTo keep up with the evolving nature of the law enforcement domain, end users\nneed to help the system adapt to the changes in criminal behaviour and\ngovernment guidance, and technical experts need to regularly oversee and\nmonitor the system. Furthermore, user-friendly human interaction with the\nsystem is essential for its adoption and some of the participants confirmed\nthey would be happy to be in the loop and provide necessary feedback that the\nsystem can learn from. Finally, we argue that it is very unlikely that the\nsystem will ever achieve full automation due to the dynamic and complex nature\nof the law enforcement domain.",
      "tldr_zh": "这篇论文通过定性研究探讨了AI辅助决策在执法领域的用户需求，旨在识别现有实践的局限性、探索系统要求以及人类在这些系统中的责任。研究发现，用户强调AI系统需高效处理大量数据、确保准确性、可信性和适应性，同时由终端用户审查输入数据并验证输出以维持可靠性。参与者还指出，人机交互需用户友好，并通过反馈帮助系统适应执法领域的动态变化，最终认为AI系统在该领域不可能实现完全自动化。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "10 pages",
      "pdf_url": "http://arxiv.org/pdf/2504.17393v2",
      "published_date": "2025-04-24 09:25:29 UTC",
      "updated_date": "2025-05-07 09:34:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:24:28.821249"
    },
    {
      "arxiv_id": "2504.17384v2",
      "title": "On the workflow, opportunities and challenges of developing foundation model in geophysics",
      "title_zh": "关于在地球物理学中开发基础模型的工作流、机会与挑战",
      "authors": [
        "Hanlin Sheng",
        "Xinming Wu",
        "Hang Gao",
        "Haibin Di",
        "Sergey Fomel",
        "Jintao Li",
        "Xu Si"
      ],
      "abstract": "Foundation models, as a mainstream technology in artificial intelligence,\nhave demonstrated immense potential across various domains in recent years,\nparticularly in handling complex tasks and multimodal data. In the field of\ngeophysics, although the application of foundation models is gradually\nexpanding, there is currently a lack of comprehensive reviews discussing the\nfull workflow of integrating foundation models with geophysical data. To\naddress this gap, this paper presents a complete framework that systematically\nexplores the entire process of developing foundation models in conjunction with\ngeophysical data. From data collection and preprocessing to model architecture\nselection, pre-training strategies, and model deployment, we provide a detailed\nanalysis of the key techniques and methodologies at each stage. In particular,\nconsidering the diversity, complexity, and physical consistency constraints of\ngeophysical data, we discuss targeted solutions to address these challenges.\nFurthermore, we discuss how to leverage the transfer learning capabilities of\nfoundation models to reduce reliance on labeled data, enhance computational\nefficiency, and incorporate physical constraints into model training, thereby\nimproving physical consistency and interpretability. Through a comprehensive\nsummary and analysis of the current technological landscape, this paper not\nonly fills the gap in the geophysics domain regarding a full-process review of\nfoundation models but also offers valuable practical guidance for their\napplication in geophysical data analysis, driving innovation and advancement in\nthe field.",
      "tldr_zh": "这篇论文探讨了在地球物理学领域开发 foundation models 的完整工作流程、机会和挑战，填补了现有文献中缺乏系统性回顾的空白。论文提出一个全面框架，从数据收集、预处理到模型架构选择、预训练策略和部署，详细分析了每个阶段的关键技术和方法，并针对地球物理数据的多样性、复杂性和物理一致性约束提供针对性解决方案。通过利用 foundation models 的迁移学习能力，论文强调如何减少对标注数据的依赖、提升计算效率，并将物理约束融入训练，从而提高模型的物理一致性和可解释性。该研究为地球物理数据分析提供实际指导，推动该领域的创新和发展。",
      "categories": [
        "physics.geo-ph",
        "cs.AI"
      ],
      "primary_category": "physics.geo-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.17384v2",
      "published_date": "2025-04-24 09:08:24 UTC",
      "updated_date": "2025-04-25 07:35:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:24:41.801638"
    },
    {
      "arxiv_id": "2504.17829v1",
      "title": "Fine-Tuning Adversarially-Robust Transformers for Single-Image Dehazing",
      "title_zh": "翻译失败",
      "authors": [
        "Vlad Vasilescu",
        "Ana Neacsu",
        "Daniela Faur"
      ],
      "abstract": "Single-image dehazing is an important topic in remote sensing applications,\nenhancing the quality of acquired images and increasing object detection\nprecision. However, the reliability of such structures has not been\nsufficiently analyzed, which poses them to the risk of imperceptible\nperturbations that can significantly hinder their performance. In this work, we\nshow that state-of-the-art image-to-image dehazing transformers are susceptible\nto adversarial noise, with even 1 pixel change being able to decrease the PSNR\nby as much as 2.8 dB. Next, we propose two lightweight fine-tuning strategies\naimed at increasing the robustness of pre-trained transformers. Our methods\nresults in comparable clean performance, while significantly increasing the\nprotection against adversarial data. We further present their applicability in\ntwo remote sensing scenarios, showcasing their robust behavior for\nout-of-distribution data. The source code for adversarial fine-tuning and\nattack algorithms can be found at github.com/Vladimirescu/RobustDehazing.",
      "tldr_zh": "本研究探讨了单图像去雾（Single-Image Dehazing）在遥感应用中的鲁棒性问题，发现现有transformer模型极易受对抗噪声影响，即使改变1像素也可导致PSNR下降多达2.8 dB。\n作者提出两种轻量级fine-tuning策略，用于增强预训练transformer的对抗鲁棒性，同时保持干净数据的性能相当。\n实验验证了这些方法在两个遥感场景中的有效性，并展示了其对out-of-distribution数据的鲁棒行为，相关代码已在GitHub开源。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.17829v1",
      "published_date": "2025-04-24 08:52:14 UTC",
      "updated_date": "2025-04-24 08:52:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:24:53.677901"
    },
    {
      "arxiv_id": "2504.18591v1",
      "title": "Geometry aware inference of steady state PDEs using Equivariant Neural Fields representations",
      "title_zh": "翻译失败",
      "authors": [
        "Giovanni Catalani",
        "Michael Bauerheim",
        "Frédéric Tost",
        "Xavier Bertrand",
        "Joseph Morlier"
      ],
      "abstract": "Recent advances in Neural Fields have enabled powerful,\ndiscretization-invariant methods for learning neural operators that approximate\nsolutions of Partial Differential Equations (PDEs) on general geometries.\nBuilding on these developments, we introduce enf2enf, an encoder--decoder\nmethodology for predicting steady-state Partial Differential Equations with\nnon-parameterized geometric variability, based on recently proposed Equivariant\nNeural Field architectures. In enf2enf, input geometries are encoded into\nlatent point cloud embeddings that inherently preserve geometric grounding and\ncapture local phenomena. The resulting representations are then combined with\nglobal parameters and directly decoded into continuous output fields, thus\nefficiently modeling the coupling between geometry and physics. By leveraging\nthe inductive biases of locality and translation invariance, our approach is\nable to capture fine-scale physical features as well as complex shape\nvariations, thereby enhancing generalization and physical compliance. Extensive\nexperiments on a high-fidelity aerodynamic dataset, a hyper-elastic material\nbenchmark, and multi-element airfoil geometries, demonstrate that the proposed\nmodel achieves superior or competitive performance compared to state-of-the-art\ngraph based, operator learning, and neural field methods. Notably, our method\nsupports real time inference and zero-shot super-resolution, enabling efficient\ntraining on low-resolution meshes while maintaining high accuracy on full-scale\ndiscretizations.",
      "tldr_zh": "本研究提出 enf2enf，一种基于 Equivariant Neural Fields 的编码器-解码器方法，用于推断稳态 PDEs（Partial Differential Equations），能够处理非参数化的几何变化。方法通过将输入几何编码成保留局部现象的潜点云嵌入，并结合全局参数直接解码成连续输出场，从而高效建模几何与物理之间的耦合，利用局部性和平移不变性的归纳偏差提升泛化和物理一致性。在空气动力学数据集、超弹性材料基准和多元素机翼几何的实验中，enf2enf 表现出优于或相当于现有图神经网络、操作学习和神经场方法的性能，并支持实时推理和零样本超分辨率，实现低分辨率训练与高准确性预测的平衡。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.18591v1",
      "published_date": "2025-04-24 08:30:32 UTC",
      "updated_date": "2025-04-24 08:30:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:25:05.631243"
    },
    {
      "arxiv_id": "2504.17366v1",
      "title": "LiveLongBench: Tackling Long-Context Understanding for Spoken Texts from Live Streams",
      "title_zh": "LiveLongBench: 攻克直播流口语文本的长上下文理解",
      "authors": [
        "Yongxuan Wu",
        "Runyu Chen",
        "Peiyu Liu",
        "Hongjin Qian"
      ],
      "abstract": "Long-context understanding poses significant challenges in natural language\nprocessing, particularly for real-world dialogues characterized by speech-based\nelements, high redundancy, and uneven information density. Although large\nlanguage models (LLMs) achieve impressive results on existing benchmarks, these\ndatasets fail to reflect the complexities of such texts, limiting their\napplicability to practical scenarios. To bridge this gap, we construct the\nfirst spoken long-text dataset, derived from live streams, designed to reflect\nthe redundancy-rich and conversational nature of real-world scenarios. We\nconstruct tasks in three categories: retrieval-dependent, reasoning-dependent,\nand hybrid. We then evaluate both popular LLMs and specialized methods to\nassess their ability to understand long-contexts in these tasks. Our results\nshow that current methods exhibit strong task-specific preferences and perform\npoorly on highly redundant inputs, with no single method consistently\noutperforming others. We propose a new baseline that better handles redundancy\nin spoken text and achieves strong performance across tasks. Our findings\nhighlight key limitations of current methods and suggest future directions for\nimproving long-context understanding. Finally, our benchmark fills a gap in\nevaluating long-context spoken language understanding and provides a practical\nfoundation for developing real-world e-commerce systems. The code and benchmark\nare available at https://github.com/Yarayx/livelongbench.",
      "tldr_zh": "该论文构建了LiveLongBench，这是一个首个基于直播流的语音长文本数据集，旨在解决长上下文理解在处理高度冗余和对话式文本时的挑战，如现有Large Language Models (LLMs) 在真实场景中的局限性。数据集包括三类任务：依赖检索的、依赖推理的和混合任务，通过评估流行LLMs和专业方法，发现这些模型表现出任务特定偏好，并在冗余输入上性能低下。研究提出了一种新基线方法，能够更有效地处理语音文本冗余，并在各项任务中实现强劲表现。该基准填补了长上下文语音语言理解的评估空白，并为开发实际电商系统提供实用基础。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.17366v1",
      "published_date": "2025-04-24 08:27:48 UTC",
      "updated_date": "2025-04-24 08:27:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:25:18.394156"
    },
    {
      "arxiv_id": "2504.18590v1",
      "title": "A multilevel approach to accelerate the training of Transformers",
      "title_zh": "翻译失败",
      "authors": [
        "Guillaume Lauga",
        "Maël Chaumette",
        "Edgar Desainte-Maréville",
        "Étienne Lasalle",
        "Arthur Lebeurrier"
      ],
      "abstract": "In this article, we investigate the potential of multilevel approaches to\naccelerate the training of transformer architectures. Using an ordinary\ndifferential equation (ODE) interpretation of these architectures, we propose\nan appropriate way of varying the discretization of these ODE Transformers in\norder to accelerate the training. We validate our approach experimentally by a\ncomparison with the standard training procedure.",
      "tldr_zh": "本论文提出了一种多级方法来加速Transformer架构的训练，通过将Transformer解释为常微分方程(ODE)，并调整其离散化方式以优化训练过程。研究者验证了这一方法的有效性，通过实验与标准训练程序进行比较。结果表明，该方法能够显著提升训练效率，为Transformer模型的优化提供新途径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.18590v1",
      "published_date": "2025-04-24 08:23:50 UTC",
      "updated_date": "2025-04-24 08:23:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:25:28.239426"
    },
    {
      "arxiv_id": "2504.17356v1",
      "title": "Comprehend, Divide, and Conquer: Feature Subspace Exploration via Multi-Agent Hierarchical Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Weiliang Zhang",
        "Xiaohan Huang",
        "Yi Du",
        "Ziyue Qiao",
        "Qingqing Long",
        "Zhen Meng",
        "Yuanchun Zhou",
        "Meng Xiao"
      ],
      "abstract": "Feature selection aims to preprocess the target dataset, find an optimal and\nmost streamlined feature subset, and enhance the downstream machine learning\ntask. Among filter, wrapper, and embedded-based approaches, the reinforcement\nlearning (RL)-based subspace exploration strategy provides a novel objective\noptimization-directed perspective and promising performance. Nevertheless, even\nwith improved performance, current reinforcement learning approaches face\nchallenges similar to conventional methods when dealing with complex datasets.\nThese challenges stem from the inefficient paradigm of using one agent per\nfeature and the inherent complexities present in the datasets. This observation\nmotivates us to investigate and address the above issue and propose a novel\napproach, namely HRLFS. Our methodology initially employs a Large Language\nModel (LLM)-based hybrid state extractor to capture each feature's mathematical\nand semantic characteristics. Based on this information, features are\nclustered, facilitating the construction of hierarchical agents for each\ncluster and sub-cluster. Extensive experiments demonstrate the efficiency,\nscalability, and robustness of our approach. Compared to contemporary or the\none-feature-one-agent RL-based approaches, HRLFS improves the downstream ML\nperformance with iterative feature subspace exploration while accelerating\ntotal run time by reducing the number of agents involved.",
      "tldr_zh": "该论文针对特征选择（feature selection）问题，提出了一种名为HRLFS的多代理层次化强化学习（Hierarchical Reinforcement Learning）方法，以优化特征子集并提升下游机器学习任务的性能。方法首先利用Large Language Model (LLM)-based混合状态提取器捕获每个特征的数学和语义特性，然后对特征进行聚类，并构建层次化代理来探索特征子空间。实验结果显示，HRLFS相较于传统每特征一个代理的强化学习（RL）方法，不仅提高了机器学习性能，还显著减少了代理数量并加速了整体运行时间。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "20 pages, keywords: Automated Feature Engineering, Tabular Dataset,\n  Multi-Agent Reinforcement Learning, Feature Selection",
      "pdf_url": "http://arxiv.org/pdf/2504.17356v1",
      "published_date": "2025-04-24 08:16:36 UTC",
      "updated_date": "2025-04-24 08:16:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:25:42.096008"
    },
    {
      "arxiv_id": "2504.17355v1",
      "title": "Collaborative Multi-Agent Reinforcement Learning for Automated Feature Transformation with Graph-Driven Path Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaohan Huang",
        "Dongjie Wang",
        "Zhiyuan Ning",
        "Ziyue Qiao",
        "Qingqing Long",
        "Haowei Zhu",
        "Yi Du",
        "Min Wu",
        "Yuanchun Zhou",
        "Meng Xiao"
      ],
      "abstract": "Feature transformation methods aim to find an optimal mathematical\nfeature-feature crossing process that generates high-value features and\nimproves the performance of downstream machine learning tasks. Existing\nframeworks, though designed to mitigate manual costs, often treat feature\ntransformations as isolated operations, ignoring dynamic dependencies between\ntransformation steps. To address the limitations, we propose TCTO, a\ncollaborative multi-agent reinforcement learning framework that automates\nfeature engineering through graph-driven path optimization. The framework's\ncore innovation lies in an evolving interaction graph that models features as\nnodes and transformations as edges. Through graph pruning and backtracking, it\ndynamically eliminates low-impact edges, reduces redundant operations, and\nenhances exploration stability. This graph also provides full traceability to\nempower TCTO to reuse high-utility subgraphs from historical transformations.\nTo demonstrate the efficacy and adaptability of our approach, we conduct\ncomprehensive experiments and case studies, which show superior performance\nacross a range of datasets.",
      "tldr_zh": "本文提出 TCTO 框架，利用 Collaborative Multi-Agent Reinforcement Learning 和 Graph-Driven Path Optimization 来自动化特征转换，解决现有方法忽略转换步骤动态依赖的局限性。框架的核心创新是构建演化交互图，将特征建模为节点、转换作为边，通过图修剪和回溯动态消除低影响边、减少冗余操作，并重用高实用子图以提升探索稳定性。实验在多种数据集上进行，显示 TCTO 框架表现出优越性能和适应性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "13 pages, Keywords: Automated Feature Transformation, Tabular\n  Dataset, Reinforcement Learning",
      "pdf_url": "http://arxiv.org/pdf/2504.17355v1",
      "published_date": "2025-04-24 08:16:13 UTC",
      "updated_date": "2025-04-24 08:16:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:25:53.370096"
    },
    {
      "arxiv_id": "2504.17354v1",
      "title": "Data-Driven Surrogate Modeling Techniques to Predict the Effective Contact Area of Rough Surface Contact Problems",
      "title_zh": "数据驱动的代理建模技术用于预测粗糙表面接触问题的有效接触面积",
      "authors": [
        "Tarik Sahin",
        "Jacopo Bonari",
        "Sebastian Brandstaeter",
        "Alexander Popp"
      ],
      "abstract": "The effective contact area in rough surface contact plays a critical role in\nmulti-physics phenomena such as wear, sealing, and thermal or electrical\nconduction. Although accurate numerical methods, like the Boundary Element\nMethod (BEM), are available to compute this quantity, their high computational\ncost limits their applicability in multi-query contexts, such as uncertainty\nquantification, parameter identification, and multi-scale algorithms, where\nmany repeated evaluations are required. This study proposes a surrogate\nmodeling framework for predicting the effective contact area using\nfast-to-evaluate data-driven techniques. Various machine learning algorithms\nare trained on a precomputed dataset, where the inputs are the imposed load and\nstatistical roughness parameters, and the output is the corresponding effective\ncontact area. All models undergo hyperparameter optimization to enable fair\ncomparisons in terms of predictive accuracy and computational efficiency,\nevaluated using established quantitative metrics. Among the models, the Kernel\nRidge Regressor demonstrates the best trade-off between accuracy and\nefficiency, achieving high predictive accuracy, low prediction time, and\nminimal training overhead-making it a strong candidate for general-purpose\nsurrogate modeling. The Gaussian Process Regressor provides an attractive\nalternative when uncertainty quantification is required, although it incurs\nadditional computational cost due to variance estimation. The generalization\ncapability of the Kernel Ridge model is validated on an unseen simulation\nscenario, confirming its ability to transfer to new configurations. Database\ngeneration constitutes the dominant cost in the surrogate modeling process.\nNevertheless, the approach proves practical and efficient for multi-query\ntasks, even when accounting for this initial expense.",
      "tldr_zh": "本研究针对粗糙表面接触的有效接触面积计算问题提出了一种数据驱动的代理建模框架，以解决传统方法如Boundary Element Method (BEM)的高计算成本问题。该框架使用机器学习算法（如Kernel Ridge Regressor和Gaussian Process Regressor）训练模型，以施加负载和粗糙度统计参数作为输入，预测有效接触面积，并通过超参数优化比较其准确性和效率。结果显示，Kernel Ridge Regressor在预测准确性、计算速度和训练开销方面表现出最佳平衡，且其泛化能力在未见模拟场景中得到验证。尽管数据库生成是主要成本，该方法仍适用于多查询任务如不确定性量化，提供高效的替代方案。",
      "categories": [
        "cs.CE",
        "cs.AI"
      ],
      "primary_category": "cs.CE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.17354v1",
      "published_date": "2025-04-24 08:15:46 UTC",
      "updated_date": "2025-04-24 08:15:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:26:05.517757"
    },
    {
      "arxiv_id": "2504.17346v1",
      "title": "Dual-Individual Genetic Algorithm: A Dual-Individual Approach for Efficient Training of Multi-Layer Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Tran Thuy Nga Truong",
        "Jooyong Kim"
      ],
      "abstract": "This paper introduces an enhanced Genetic Algorithm technique called\nDual-Individual Genetic Algorithm (Dual-Individual GA), which optimizes neural\nnetworks for binary image classification tasks, such as cat vs. non-cat\nclassification. The proposed method employs only two individuals for crossover,\nrepresented by two parameter sets: Leader and Follower. The Leader focuses on\nexploitation, representing the primary optimal solution at even-indexed\npositions (0, 2, 4, ...), while the Follower promotes exploration by preserving\ndiversity and avoiding premature convergence, operating at odd-indexed\npositions (1, 3, 5, ...). Leader and Follower are modeled as two phases or\nroles. The key contributions of this work are threefold: (1) a self-adaptive\nlayer dimension mechanism that eliminates the need for manual tuning of layer\narchitectures; (2) generates two parameter sets, leader and follower parameter\nsets, with 10 layer architecture configurations (5 for each set), ranked by\nPareto dominance and cost. post-optimization; and (3) demonstrated superior\nperformance compared to traditional gradient-based methods. Experimental\nresults show that the Dual-Individual GA achieves 99.04% training accuracy and\n80% testing accuracy (cost = 0.034) on a three-layer network with architecture\n[12288, 17, 4, 1], outperforming a gradient-based approach that achieves 98%\ntraining accuracy and 80% testing accuracy (cost = 0.092) on a four-layer\nnetwork with architecture [12288, 20, 7, 5, 1]. These findings highlight the\nefficiency and effectiveness of the proposed method in optimizing neural\nnetworks.",
      "tldr_zh": "本论文提出了一种名为 Dual-Individual Genetic Algorithm (Dual-Individual GA) 的增强遗传算法，用于优化多层神经网络在二进制图像分类任务（如猫 vs 非猫）的训练。该方法采用两个个体——Leader（专注于 exploitation）和Follower（专注于 exploration）——分别在偶数和奇数位置操作，以提升多样性和避免过早收敛，同时引入自适应层维度机制和基于 Pareto dominance 的参数集优化。关键贡献包括无需手动调整层架构、生成优化的Leader和Follower参数集，以及在实验中展现出优于传统梯度方法的性能。结果显示，在三层网络[12288, 17, 4, 1]上，Dual-Individual GA 实现了99.04%的训练准确率和80%的测试准确率（cost=0.034），相比梯度方法的98%训练准确率和80%测试准确率（cost=0.092）在四层网络上表现出更高效率。",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.17346v1",
      "published_date": "2025-04-24 08:04:08 UTC",
      "updated_date": "2025-04-24 08:04:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:26:17.761370"
    },
    {
      "arxiv_id": "2504.17331v1",
      "title": "Exploring Context-aware and LLM-driven Locomotion for Immersive Virtual Reality",
      "title_zh": "翻译失败",
      "authors": [
        "Süleyman Özdel",
        "Kadir Burak Buldu",
        "Enkelejda Kasneci",
        "Efe Bozkir"
      ],
      "abstract": "Locomotion plays a crucial role in shaping the user experience within virtual\nreality environments. In particular, hands-free locomotion offers a valuable\nalternative by supporting accessibility and freeing users from reliance on\nhandheld controllers. To this end, traditional speech-based methods often\ndepend on rigid command sets, limiting the naturalness and flexibility of\ninteraction. In this study, we propose a novel locomotion technique powered by\nlarge language models (LLMs), which allows users to navigate virtual\nenvironments using natural language with contextual awareness. We evaluate\nthree locomotion methods: controller-based teleportation, voice-based steering,\nand our language model-driven approach. Our evaluation measures include\neye-tracking data analysis, including explainable machine learning through SHAP\nanalysis as well as standardized questionnaires for usability, presence,\ncybersickness, and cognitive load to examine user attention and engagement. Our\nfindings indicate that the LLM-driven locomotion possesses comparable\nusability, presence, and cybersickness scores to established methods like\nteleportation, demonstrating its novel potential as a comfortable, natural\nlanguage-based, hands-free alternative. In addition, it enhances user attention\nwithin the virtual environment, suggesting greater engagement. Complementary to\nthese findings, SHAP analysis revealed that fixation, saccade, and\npupil-related features vary across techniques, indicating distinct patterns of\nvisual attention and cognitive processing. Overall, we state that our method\ncan facilitate hands-free locomotion in virtual spaces, especially in\nsupporting accessibility.",
      "tldr_zh": "这篇论文探索了基于大型语言模型(LLMs)的上下文感知移动技术，用于沉浸式虚拟现实(VR)，允许用户通过自然语言导航虚拟环境，从而提供更自然和灵活的手部自由交互。研究者评估了三种移动方法——基于控制器的传送、基于语音的转向以及LLM驱动方法——通过眼动追踪数据分析(SHAP分析)和标准化问卷，测量了可用性、在场感、cybersickness和认知负荷。结果表明，LLM驱动方法在可用性、在场感和晕眩方面与传统方法相当，同时提升了用户注意力，促进了更高的参与度，并证明其在支持VR可访问性方面的潜力。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "This work has been submitted to the IEEE for possible publication",
      "pdf_url": "http://arxiv.org/pdf/2504.17331v1",
      "published_date": "2025-04-24 07:48:09 UTC",
      "updated_date": "2025-04-24 07:48:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:26:30.078146"
    },
    {
      "arxiv_id": "2504.17315v1",
      "title": "DIMT25@ICDAR2025: HW-TSC's End-to-End Document Image Machine Translation System Leveraging Large Vision-Language Model",
      "title_zh": "翻译失败",
      "authors": [
        "Zhanglin Wu",
        "Tengfei Song",
        "Ning Xie",
        "Weidong Zhang",
        "Pengfei Li",
        "Shuang Wu",
        "Chong Li",
        "Junhao Zhu",
        "Hao Yang"
      ],
      "abstract": "This paper presents the technical solution proposed by Huawei Translation\nService Center (HW-TSC) for the \"End-to-End Document Image Machine Translation\nfor Complex Layouts\" competition at the 19th International Conference on\nDocument Analysis and Recognition (DIMT25@ICDAR2025). Leveraging\nstate-of-the-art open-source large vision-language model (LVLM), we introduce a\ntraining framework that combines multi-task learning with perceptual\nchain-of-thought to develop a comprehensive end-to-end document translation\nsystem. During the inference phase, we apply minimum Bayesian decoding and\npost-processing strategies to further enhance the system's translation\ncapabilities. Our solution uniquely addresses both OCR-based and OCR-free\ndocument image translation tasks within a unified framework. This paper\nsystematically details the training methods, inference strategies, LVLM base\nmodels, training data, experimental setups, and results, demonstrating an\neffective approach to document image machine translation.",
      "tldr_zh": "这篇论文介绍了华为翻译服务中心（HW-TSC）针对 DIMT25@ICDAR2025 比赛提出的端到端文档图像机器翻译系统，该系统利用大型视觉语言模型（Large Vision-Language Model, LVLM）处理复杂布局的文档翻译任务。研究采用多任务学习结合感知链式思维（perceptual chain-of-thought）的训练框架，实现对 OCR-based 和 OCR-free 翻译任务的统一处理。推理阶段通过最小贝叶斯解码（minimum Bayesian decoding）和后处理策略提升翻译准确性，实验结果证明了该方法的有效性，为文档图像机器翻译提供了全面的解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "7 pages, 1 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2504.17315v1",
      "published_date": "2025-04-24 07:17:59 UTC",
      "updated_date": "2025-04-24 07:17:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:26:41.182153"
    },
    {
      "arxiv_id": "2504.17311v1",
      "title": "FLUKE: A Linguistically-Driven and Task-Agnostic Framework for Robustness Evaluation",
      "title_zh": "FLUKE：一个语言驱动且任务无关的",
      "authors": [
        "Yulia Otmakhova",
        "Hung Thinh Truong",
        "Rahmad Mahendra",
        "Zenan Zhai",
        "Rongxin Zhu",
        "Daniel Beck",
        "Jey Han Lau"
      ],
      "abstract": "We present FLUKE (Framework for LingUistically-driven and tasK-agnostic\nrobustness Evaluation), a task-agnostic framework for assessing model\nrobustness through systematic minimal variations of test data. FLUKE introduces\ncontrolled variations across linguistic levels - from orthography to dialect\nand style varieties - and leverages large language models (LLMs) with human\nvalidation to generate modifications. We demonstrate FLUKE's utility by\nevaluating both fine-tuned models and LLMs across four diverse NLP tasks, and\nreveal that (1) the impact of linguistic variations is highly task-dependent,\nwith some tests being critical for certain tasks but irrelevant for others; (2)\nwhile LLMs have better overall robustness compared to fine-tuned models, they\nstill exhibit significant brittleness to certain linguistic variations; (3) all\nmodels show substantial vulnerability to negation modifications across most\ntasks. These findings highlight the importance of systematic robustness testing\nfor understanding model behaviors.",
      "tldr_zh": "本研究提出了FLUKE框架，这是一个基于语言学驱动的任务无关方法，用于通过系统性的最小变化评估模型的鲁棒性。FLUKE在正字法、方言和风格等语言层面引入受控变异，并利用LLMs结合人工验证生成修改，以评估微调模型和LLMs在四个NLP任务上的表现。结果显示，语言变异的影响高度依赖任务，LLMs整体鲁棒性较强但仍易受某些变异影响，且所有模型对否定修改表现出显著脆弱性。这些发现强调了系统鲁棒性测试在理解模型行为中的重要性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.17311v1",
      "published_date": "2025-04-24 07:12:37 UTC",
      "updated_date": "2025-04-24 07:12:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:26:53.576457"
    },
    {
      "arxiv_id": "2504.17306v1",
      "title": "Advanced Segmentation of Diabetic Retinopathy Lesions Using DeepLabv3+",
      "title_zh": "翻译失败",
      "authors": [
        "Meher Boulaabi",
        "Takwa Ben Aïcha Gader",
        "Afef Kacem Echi",
        "Sameh Mbarek"
      ],
      "abstract": "To improve the segmentation of diabetic retinopathy lesions (microaneurysms,\nhemorrhages, exudates, and soft exudates), we implemented a binary segmentation\nmethod specific to each type of lesion. As post-segmentation, we combined the\nindividual model outputs into a single image to better analyze the lesion\ntypes. This approach facilitated parameter optimization and improved accuracy,\neffectively overcoming challenges related to dataset limitations and annotation\ncomplexity. Specific preprocessing steps included cropping and applying\ncontrast-limited adaptive histogram equalization to the L channel of the LAB\nimage. Additionally, we employed targeted data augmentation techniques to\nfurther refine the model's efficacy. Our methodology utilized the DeepLabv3+\nmodel, achieving a segmentation accuracy of 99%. These findings highlight the\nefficacy of innovative strategies in advancing medical image analysis,\nparticularly in the precise segmentation of diabetic retinopathy lesions. The\nIDRID dataset was utilized to validate and demonstrate the robustness of our\napproach.",
      "tldr_zh": "本研究针对糖尿病视网膜病变（Diabetic Retinopathy）病变（如微动脉瘤、出血、渗出物和软渗出物）的分割问题，采用 DeepLabv3+ 模型实现每种病变类型的二元分割，并通过后处理将输出合并成单一图像，以优化参数并提高准确性。预处理步骤包括图像裁剪和对 LAB 图像 L 通道应用对比度限制的自适应直方图均衡（contrast-limited adaptive histogram equalization），结合针对性数据增强技术来克服数据集限制和标注复杂性。该方法在 IDRID 数据集上验证，达到了 99% 的分割准确率，展示了创新策略在医疗图像分析中的显著效能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This work was accepted at the ACS/IEEE International Conference on\n  Computer Systems and Applications (AICCSA) 2024",
      "pdf_url": "http://arxiv.org/pdf/2504.17306v1",
      "published_date": "2025-04-24 07:00:38 UTC",
      "updated_date": "2025-04-24 07:00:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:27:06.275086"
    },
    {
      "arxiv_id": "2504.17304v1",
      "title": "You Are What You Bought: Generating Customer Personas for E-commerce Applications",
      "title_zh": "你所购即是你：为电子商务应用生成客户画像",
      "authors": [
        "Yimin Shi",
        "Yang Fei",
        "Shiqi Zhang",
        "Haixun Wang",
        "Xiaokui Xiao"
      ],
      "abstract": "In e-commerce, user representations are essential for various applications.\nExisting methods often use deep learning techniques to convert customer\nbehaviors into implicit embeddings. However, these embeddings are difficult to\nunderstand and integrate with external knowledge, limiting the effectiveness of\napplications such as customer segmentation, search navigation, and product\nrecommendations. To address this, our paper introduces the concept of the\ncustomer persona. Condensed from a customer's numerous purchasing histories, a\ncustomer persona provides a multi-faceted and human-readable characterization\nof specific purchase behaviors and preferences, such as Busy Parents or Bargain\nHunters.\n  This work then focuses on representing each customer by multiple personas\nfrom a predefined set, achieving readable and informative explicit user\nrepresentations. To this end, we propose an effective and efficient solution\nGPLR. To ensure effectiveness, GPLR leverages pre-trained LLMs to infer\npersonas for customers. To reduce overhead, GPLR applies LLM-based labeling to\nonly a fraction of users and utilizes a random walk technique to predict\npersonas for the remaining customers. We further propose RevAff, which provides\nan absolute error $\\epsilon$ guarantee while improving the time complexity of\nthe exact solution by a factor of at least\n$O(\\frac{\\epsilon\\cdot|E|N}{|E|+N\\log N})$, where $N$ represents the number of\ncustomers and products, and $E$ represents the interactions between them. We\nevaluate the performance of our persona-based representation in terms of\naccuracy and robustness for recommendation and customer segmentation tasks\nusing three real-world e-commerce datasets. Most notably, we find that\nintegrating customer persona representations improves the state-of-the-art\ngraph convolution-based recommendation model by up to 12% in terms of NDCG@K\nand F1-Score@K.",
      "tldr_zh": "本文提出 customer persona 概念，用于从客户的购买历史中提炼出多维度的可读用户表示（如 Busy Parents 或 Bargain Hunters），以解决现有隐式嵌入方法在电商应用中的理解和整合难题。作者开发了 GPLR 框架，利用预训练 LLMs 对部分用户进行 persona 推断，并通过随机游走技术预测剩余用户，同时引入 RevAff 算法，提供绝对误差保证并显著优化时间复杂度。实验结果显示，在三个真实电商数据集上，customer persona 表示可将最先进图卷积推荐模型的 NDCG@K 和 F1-Score@K 性能提高多达 12%，并提升推荐和客户分段任务的准确性和鲁棒性。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "SIGIR 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.17304v1",
      "published_date": "2025-04-24 06:59:16 UTC",
      "updated_date": "2025-04-24 06:59:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:27:18.138349"
    },
    {
      "arxiv_id": "2504.17295v1",
      "title": "AI-Enhanced Business Process Automation: A Case Study in the Insurance Domain Using Object-Centric Process Mining",
      "title_zh": "翻译失败",
      "authors": [
        "Shahrzad Khayatbashi",
        "Viktor Sjölind",
        "Anders Granåker",
        "Amin Jalali"
      ],
      "abstract": "Recent advancements in Artificial Intelligence (AI), particularly Large\nLanguage Models (LLMs), have enhanced organizations' ability to reengineer\nbusiness processes by automating knowledge-intensive tasks. This automation\ndrives digital transformation, often through gradual transitions that improve\nprocess efficiency and effectiveness. To fully assess the impact of such\nautomation, a data-driven analysis approach is needed - one that examines how\ntraditional and AI-enhanced process variants coexist during this transition.\nObject-Centric Process Mining (OCPM) has emerged as a valuable method that\nenables such analysis, yet real-world case studies are still needed to\ndemonstrate its applicability. This paper presents a case study from the\ninsurance sector, where an LLM was deployed in production to automate the\nidentification of claim parts, a task previously performed manually and\nidentified as a bottleneck for scalability. To evaluate this transformation, we\napply OCPM to assess the impact of AI-driven automation on process scalability.\nOur findings indicate that while LLMs significantly enhance operational\ncapacity, they also introduce new process dynamics that require further\nrefinement. This study also demonstrates the practical application of OCPM in a\nreal-world setting, highlighting its advantages and limitations.",
      "tldr_zh": "这篇论文通过一个保险领域的案例研究，探讨了人工智能（AI），特别是Large Language Models (LLMs)，在业务流程自动化中的应用，以自动化索赔识别任务并解决手动处理的瓶颈问题。研究采用Object-Centric Process Mining (OCPM)方法，对传统和AI增强流程的共存进行数据驱动分析，评估其对过程效率和可伸缩性的影响。结果表明，LLMs显著提升了操作能力，但也引入了新的过程动态，需要进一步优化，同时突出了OCPM在真实场景中的优势和局限性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.17295v1",
      "published_date": "2025-04-24 06:43:29 UTC",
      "updated_date": "2025-04-24 06:43:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:27:29.567993"
    },
    {
      "arxiv_id": "2504.17282v1",
      "title": "Cracking the Code of Action: a Generative Approach to Affordances for Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Lynn Cherif",
        "Flemming Kondrup",
        "David Venuto",
        "Ankit Anand",
        "Doina Precup",
        "Khimya Khetarpal"
      ],
      "abstract": "Agents that can autonomously navigate the web through a graphical user\ninterface (GUI) using a unified action space (e.g., mouse and keyboard actions)\ncan require very large amounts of domain-specific expert demonstrations to\nachieve good performance. Low sample efficiency is often exacerbated in\nsparse-reward and large-action-space environments, such as a web GUI, where\nonly a few actions are relevant in any given situation. In this work, we\nconsider the low-data regime, with limited or no access to expert behavior. To\nenable sample-efficient learning, we explore the effect of constraining the\naction space through $\\textit{intent-based affordances}$ -- i.e., considering\nin any situation only the subset of actions that achieve a desired outcome. We\npropose $\\textbf{Code as Generative Affordances}$ $(\\textbf{$\\texttt{CoGA}$})$,\na method that leverages pre-trained vision-language models (VLMs) to generate\ncode that determines affordable actions through implicit intent-completion\nfunctions and using a fully-automated program generation and verification\npipeline. These programs are then used in-the-loop of a reinforcement learning\nagent to return a set of affordances given a pixel observation. By greatly\nreducing the number of actions that an agent must consider, we demonstrate on a\nwide range of tasks in the MiniWob++ benchmark that: $\\textbf{1)}$\n$\\texttt{CoGA}$ is orders of magnitude more sample efficient than its RL agent,\n$\\textbf{2)}$ $\\texttt{CoGA}$'s programs can generalize within a family of\ntasks, and $\\textbf{3)}$ $\\texttt{CoGA}$ performs better or on par compared\nwith behavior cloning when a small number of expert demonstrations is\navailable.",
      "tldr_zh": "该论文探讨了在强化学习（Reinforcement Learning）中，通过生成式方法提升代理在图形用户界面（GUI）导航的样例效率问题，特别是针对稀疏奖励和大型动作空间的环境。作者提出CoGA（Code as Generative Affordances）方法，利用预训练的视觉语言模型（VLMs）生成代码，以定义intent-based affordances，即根据像素观察自动识别和限制相关动作子集，并通过自动化程序生成和验证管道实现意图完成。实验在MiniWob++基准上的多种任务中显示，CoGA比传统RL代理样例效率高几个数量级，能够在任务家族内泛化，且在少量专家演示下，其性能优于或相当于是行为克隆方法。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.17282v1",
      "published_date": "2025-04-24 06:20:08 UTC",
      "updated_date": "2025-04-24 06:20:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:27:42.099877"
    },
    {
      "arxiv_id": "2504.17277v1",
      "title": "ExOSITO: Explainable Off-Policy Learning with Side Information for Intensive Care Unit Blood Test Orders",
      "title_zh": "翻译失败",
      "authors": [
        "Zongliang Ji",
        "Andre Carlos Kajdacsy-Balla Amaral",
        "Anna Goldenberg",
        "Rahul G. Krishnan"
      ],
      "abstract": "Ordering a minimal subset of lab tests for patients in the intensive care\nunit (ICU) can be challenging. Care teams must balance between ensuring the\navailability of the right information and reducing the clinical burden and\ncosts associated with each lab test order. Most in-patient settings experience\nfrequent over-ordering of lab tests, but are now aiming to reduce this burden\non both hospital resources and the environment. This paper develops a novel\nmethod that combines off-policy learning with privileged information to\nidentify the optimal set of ICU lab tests to order. Our approach, EXplainable\nOff-policy learning with Side Information for ICU blood Test Orders (ExOSITO)\ncreates an interpretable assistive tool for clinicians to order lab tests by\nconsidering both the observed and predicted future status of each patient. We\npose this problem as a causal bandit trained using offline data and a reward\nfunction derived from clinically-approved rules; we introduce a novel learning\nframework that integrates clinical knowledge with observational data to bridge\nthe gap between the optimal and logging policies. The learned policy function\nprovides interpretable clinical information and reduces costs without omitting\nany vital lab orders, outperforming both a physician's policy and prior\napproaches to this practical problem.",
      "tldr_zh": "这篇论文针对ICU实验室测试订购的挑战，提出了一种名为ExOSITO的框架，结合off-policy learning和side information，帮助临床团队优化测试选择，以平衡信息可用性和成本负担。ExOSITO将问题建模为causal bandit，使用离线数据和基于临床规则的奖励函数训练策略，并整合临床知识与观察数据来桥接最优策略和日志策略。实验结果表明，该方法提供可解释的临床信息，减少测试成本而不遗漏关键订单，并优于医生策略和现有方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to the Conference on Health, Inference, and Learning (CHIL)\n  2025",
      "pdf_url": "http://arxiv.org/pdf/2504.17277v1",
      "published_date": "2025-04-24 06:07:14 UTC",
      "updated_date": "2025-04-24 06:07:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:27:54.110875"
    },
    {
      "arxiv_id": "2505.00018v1",
      "title": "Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management",
      "title_zh": "翻译失败",
      "authors": [
        "Ju Wu",
        "Calvin K. L. Or"
      ],
      "abstract": "This position paper critically surveys a broad spectrum of recent empirical\ndevelopments on human-AI agents collaboration, highlighting both their\ntechnical achievements and persistent gaps. We observe a lack of a unifying\ntheoretical framework that can coherently integrate these varied studies,\nespecially when tackling open-ended, complex tasks. To address this, we propose\na novel conceptual architecture: one that systematically interlinks the\ntechnical details of multi-agent coordination, knowledge management, cybernetic\nfeedback loops, and higher-level control mechanisms. By mapping existing\ncontributions, from symbolic AI techniques and connectionist LLM-based agents\nto hybrid organizational practices, onto this proposed framework (Hierarchical\nExploration-Exploitation Net), our approach facilitates revision of legacy\nmethods and inspires new work that fuses qualitative and quantitative\nparadigms. The paper's structure allows it to be read from any section, serving\nequally as a critical review of technical implementations and as a\nforward-looking reference for designing or extending human-AI symbioses.\nTogether, these insights offer a stepping stone toward deeper co-evolution of\nhuman cognition and AI capability.",
      "tldr_zh": "这篇位置论文（Position Paper）审视了人类-AI 代理协作（Human-AI Agents Collaboration）的最新发展，强调了技术成就（如多代理协调和知识管理）与缺乏统一框架的差距，尤其在处理开放式复杂任务时。论文提出一个新概念架构——Hierarchical Exploration-Exploitation Net，该框架系统整合多代理协调（Multi-Agent Coordination）、知识管理（Knowledge Management）、控制机制和反馈循环（Cybernetic Feedback Loops），并映射符号 AI 和 LLM-based 代理等现有贡献。如此设计不仅促进传统方法的修订，还融合定性和定量范式，助力人类认知与 AI 能力的共同演化，并为未来的人类-AI 协作系统设计提供参考。",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.00018v1",
      "published_date": "2025-04-24 05:57:03 UTC",
      "updated_date": "2025-04-24 05:57:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:28:05.451182"
    },
    {
      "arxiv_id": "2504.17264v1",
      "title": "JurisCTC: Enhancing Legal Judgment Prediction via Cross-Domain Transfer and Contrastive Learning",
      "title_zh": "JurisCTC：通过跨领域转移和对比学习增强法律判决预测",
      "authors": [
        "Zhaolu Kang",
        "Hongtian Cai",
        "Xiangyang Ji",
        "Jinzhe Li",
        "Nanfei Gu"
      ],
      "abstract": "In recent years, Unsupervised Domain Adaptation (UDA) has gained significant\nattention in the field of Natural Language Processing (NLP) owing to its\nability to enhance model generalization across diverse domains. However, its\napplication for knowledge transfer between distinct legal domains remains\nlargely unexplored. To address the challenges posed by lengthy and complex\nlegal texts and the limited availability of large-scale annotated datasets, we\npropose JurisCTC, a novel model designed to improve the accuracy of Legal\nJudgment Prediction (LJP) tasks. Unlike existing approaches, JurisCTC\nfacilitates effective knowledge transfer across various legal domains and\nemploys contrastive learning to distinguish samples from different domains.\nSpecifically, for the LJP task, we enable knowledge transfer between civil and\ncriminal law domains. Compared to other models and specific large language\nmodels (LLMs), JurisCTC demonstrates notable advancements, achieving peak\naccuracies of 76.59% and 78.83%, respectively.",
      "tldr_zh": "这篇论文提出 JurisCTC 模型，通过 Unsupervised Domain Adaptation (UDA) 和 Contrastive Learning 提升 Legal Judgment Prediction (LJP) 任务的准确性，针对法律文本的复杂性和标注数据集的稀缺性问题。模型实现跨法律领域的知识转移，例如从民事法到刑事法，并使用对比学习来区分不同领域的样本，从而提高泛化能力。与其他模型和 Large Language Models (LLMs) 相比，JurisCTC 取得了显著改进，分别达到76.59%和78.83%的峰值准确率。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted in International Joint Conference on Neural Networks (IJCNN)\n  2025",
      "pdf_url": "http://arxiv.org/pdf/2504.17264v1",
      "published_date": "2025-04-24 05:48:57 UTC",
      "updated_date": "2025-04-24 05:48:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:28:17.362289"
    },
    {
      "arxiv_id": "2504.17261v1",
      "title": "Symbolic Representation for Any-to-Any Generative Tasks",
      "title_zh": "翻译失败",
      "authors": [
        "Jiaqi Chen",
        "Xiaoye Zhu",
        "Yue Wang",
        "Tianyang Liu",
        "Xinhui Chen",
        "Ying Chen",
        "Chak Tou Leong",
        "Yifei Ke",
        "Joseph Liu",
        "Yiwen Yuan",
        "Julian McAuley",
        "Li-jia Li"
      ],
      "abstract": "We propose a symbolic generative task description language and a\ncorresponding inference engine capable of representing arbitrary multimodal\ntasks as structured symbolic flows. Unlike conventional generative models that\nrely on large-scale training and implicit neural representations to learn\ncross-modal mappings, often at high computational cost and with limited\nflexibility, our framework introduces an explicit symbolic representation\ncomprising three core primitives: functions, parameters, and topological logic.\nLeveraging a pre-trained language model, our inference engine maps natural\nlanguage instructions directly to symbolic workflows in a training-free manner.\nOur framework successfully performs over 12 diverse multimodal generative\ntasks, demonstrating strong performance and flexibility without the need for\ntask-specific tuning. Experiments show that our method not only matches or\noutperforms existing state-of-the-art unified models in content quality, but\nalso offers greater efficiency, editability, and interruptibility. We believe\nthat symbolic task representations provide a cost-effective and extensible\nfoundation for advancing the capabilities of generative AI.",
      "tldr_zh": "我们提出了一种符号化生成任务描述语言和相应的推理引擎，能够将任意多模态任务表示为结构化的符号流，包括核心基元：functions、parameters 和 topological logic。这种框架利用预训练语言模型，实现训练-free 的自然语言指令到符号工作流的直接映射，从而避免了传统生成模型的高计算成本和灵活性限制。实验结果显示，该方法在超过12种多模态生成任务上，内容质量匹配或优于现有最先进统一模型，同时提供更高的效率、可编辑性和中断性，为生成AI提供了一个成本效益高且可扩展的基础。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.17261v1",
      "published_date": "2025-04-24 05:35:47 UTC",
      "updated_date": "2025-04-24 05:35:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:28:29.992961"
    },
    {
      "arxiv_id": "2504.17255v1",
      "title": "3D Deep-learning-based Segmentation of Human Skin Sweat Glands and Their 3D Morphological Response to Temperature Variations",
      "title_zh": "翻译失败",
      "authors": [
        "Shaoyu Pei",
        "Renxiong Wu",
        "Hao Zheng",
        "Lang Qin",
        "Shuaichen Lin",
        "Yuxing Gan",
        "Wenjing Huang",
        "Zhixuan Wang",
        "Mohan Qin",
        "Yong Liu",
        "Guangming Ni"
      ],
      "abstract": "Skin, the primary regulator of heat exchange, relies on sweat glands for\nthermoregulation. Alterations in sweat gland morphology play a crucial role in\nvarious pathological conditions and clinical diagnoses. Current methods for\nobserving sweat gland morphology are limited by their two-dimensional, in\nvitro, and destructive nature, underscoring the urgent need for real-time,\nnon-invasive, quantifiable technologies. We proposed a novel three-dimensional\n(3D) transformer-based multi-object segmentation framework, integrating a\nsliding window approach, joint spatial-channel attention mechanism, and\narchitectural heterogeneity between shallow and deep layers. Our proposed\nnetwork enables precise 3D sweat gland segmentation from skin volume data\ncaptured by optical coherence tomography (OCT). For the first time, subtle\nvariations of sweat gland 3D morphology in response to temperature changes,\nhave been visualized and quantified. Our approach establishes a benchmark for\nnormal sweat gland morphology and provides a real-time, non-invasive tool for\nquantifying 3D structural parameters. This enables the study of individual\nvariability and pathological changes in sweat gland structure, advancing\ndermatological research and clinical applications, including thermoregulation\nand bromhidrosis treatment.",
      "tldr_zh": "该研究提出了一种基于 Transformer 的三维多对象分割框架，用于精确分割人体皮肤汗腺及其对温度变化的形态响应。该框架整合了滑动窗口方法、联合空间-通道注意力机制，以及浅层和深层架构的异质性，从光学相干断层扫描 (OCT) 捕获的皮肤体积数据中实现高精度分割。首次实现了汗腺三维形态微小变化的可视化和量化，建立了一个汗腺形态基准，并提供实时、非侵入的工具来评估个体变异和病理变化。最终，这为皮肤病研究和临床应用（如体温调节和汗臭治疗）提供了重要进展。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "physics.optics"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.17255v1",
      "published_date": "2025-04-24 05:19:47 UTC",
      "updated_date": "2025-04-24 05:19:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:28:42.545274"
    },
    {
      "arxiv_id": "2504.17247v1",
      "title": "Targeted AMP generation through controlled diffusion with efficient embeddings",
      "title_zh": "翻译失败",
      "authors": [
        "Diogo Soares",
        "Leon Hetzel",
        "Paulina Szymczak",
        "Fabian Theis",
        "Stephan Günnemann",
        "Ewa Szczurek"
      ],
      "abstract": "Deep learning-based antimicrobial peptide (AMP) discovery faces critical\nchallenges such as low experimental hit rates as well as the need for nuanced\ncontrollability and efficient modeling of peptide properties. To address these\nchallenges, we introduce OmegAMP, a framework that leverages a diffusion-based\ngenerative model with efficient low-dimensional embeddings, precise\ncontrollability mechanisms, and novel classifiers with drastically reduced\nfalse positive rates for candidate filtering. OmegAMP enables the targeted\ngeneration of AMPs with specific physicochemical properties, activity profiles,\nand species-specific effectiveness. Moreover, it maximizes sample diversity\nwhile ensuring faithfulness to the underlying data distribution during\ngeneration. We demonstrate that OmegAMP achieves state-of-the-art performance\nacross all stages of the AMP discovery pipeline, significantly advancing the\npotential of computational frameworks in combating antimicrobial resistance.",
      "tldr_zh": "该研究针对深度学习在抗菌肽(AMP)发现中的挑战，如低实验命中率和对肽属性的精细控制需求，引入了OmegAMP框架。OmegAMP利用基于扩散的生成模型、有效的低维嵌入以及精确的可控机制，结合创新分类器来减少假阳性率，从而实现针对性生成具有特定理化性质、活性谱和物种特定有效性的AMP，同时最大化样本多样性和数据分布忠实度。实验结果显示，OmegAMP在AMP发现管道的所有阶段达到了最先进性能，大大提升了计算框架在对抗抗菌耐药性方面的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.17247v1",
      "published_date": "2025-04-24 04:53:04 UTC",
      "updated_date": "2025-04-24 04:53:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:28:53.777201"
    },
    {
      "arxiv_id": "2504.17243v2",
      "title": "NeuralGrok: Accelerate Grokking by Neural Gradient Transformation",
      "title_zh": "NeuralGrok：通过神经梯度变换加速 Grokking",
      "authors": [
        "Xinyu Zhou",
        "Simin Fan",
        "Martin Jaggi",
        "Jie Fu"
      ],
      "abstract": "Grokking is proposed and widely studied as an intricate phenomenon in which\ngeneralization is achieved after a long-lasting period of overfitting. In this\nwork, we propose NeuralGrok, a novel gradient-based approach that learns an\noptimal gradient transformation to accelerate the generalization of\ntransformers in arithmetic tasks. Specifically, NeuralGrok trains an auxiliary\nmodule (e.g., an MLP block) in conjunction with the base model. This module\ndynamically modulates the influence of individual gradient components based on\ntheir contribution to generalization, guided by a bilevel optimization\nalgorithm. Our extensive experiments demonstrate that NeuralGrok significantly\naccelerates generalization, particularly in challenging arithmetic tasks. We\nalso show that NeuralGrok promotes a more stable training paradigm, constantly\nreducing the model's complexity, while traditional regularization methods, such\nas weight decay, can introduce substantial instability and impede\ngeneralization. We further investigate the intrinsic model complexity\nleveraging a novel Absolute Gradient Entropy (AGE) metric, which explains that\nNeuralGrok effectively facilitates generalization by reducing the model\ncomplexity. We offer valuable insights on the grokking phenomenon of\nTransformer models, which encourages a deeper understanding of the fundamental\nprinciples governing generalization ability.",
      "tldr_zh": "本研究提出NeuralGrok，一种基于神经梯度变换的创新方法，用于加速Transformer模型在算术任务中的Grokking现象，即模型在过度拟合后实现泛化。NeuralGrok通过训练一个辅助模块（如MLP块）动态调整梯度组件的影响，利用bilevel optimization算法来优化梯度贡献，从而促进更有效的泛化过程。实验结果显示，该方法显著加快泛化速度，尤其在具有挑战性的算术任务中，并通过减少模型复杂度实现更稳定的训练，比传统正则化方法如weight decay更可靠。此外，引入Absolute Gradient Entropy (AGE)指标解释了NeuralGrok如何降低模型复杂度，从而深化了对Transformer泛化能力的理解。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint, 16 pages",
      "pdf_url": "http://arxiv.org/pdf/2504.17243v2",
      "published_date": "2025-04-24 04:41:35 UTC",
      "updated_date": "2025-04-25 03:44:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:29:06.084478"
    },
    {
      "arxiv_id": "2504.17828v1",
      "title": "VEU-Bench: Towards Comprehensive Understanding of Video Editing",
      "title_zh": "翻译失败",
      "authors": [
        "Bozheng Li",
        "Yongliang Wu",
        "Yi Lu",
        "Jiashuo Yu",
        "Licheng Tang",
        "Jiawang Cao",
        "Wenqing Zhu",
        "Yuyang Sun",
        "Jay Wu",
        "Wenbo Zhu"
      ],
      "abstract": "Widely shared videos on the internet are often edited. Recently, although\nVideo Large Language Models (Vid-LLMs) have made great progress in general\nvideo understanding tasks, their capabilities in video editing understanding\n(VEU) tasks remain unexplored. To address this gap, in this paper, we introduce\nVEU-Bench (Video Editing Understanding Benchmark), a comprehensive benchmark\nthat categorizes video editing components across various dimensions, from\nintra-frame features like shot size to inter-shot attributes such as cut types\nand transitions. Unlike previous video editing understanding benchmarks that\nfocus mainly on editing element classification, VEU-Bench encompasses 19\nfine-grained tasks across three stages: recognition, reasoning, and judging. To\nenhance the annotation of VEU automatically, we built an annotation pipeline\nintegrated with an ontology-based knowledge base. Through extensive experiments\nwith 11 state-of-the-art Vid-LLMs, our findings reveal that current Vid-LLMs\nface significant challenges in VEU tasks, with some performing worse than\nrandom choice. To alleviate this issue, we develop Oscars, a VEU expert model\nfine-tuned on the curated VEU-Bench dataset. It outperforms existing\nopen-source Vid-LLMs on VEU-Bench by over 28.3% in accuracy and achieves\nperformance comparable to commercial models like GPT-4o. We also demonstrate\nthat incorporating VEU data significantly enhances the performance of Vid-LLMs\non general video understanding benchmarks, with an average improvement of 8.3%\nacross nine reasoning tasks.",
      "tldr_zh": "这篇论文引入了 VEU-Bench，这是一个全面的基准，用于评估视频编辑理解（VEU）任务，包括从帧内特征（如镜头大小）到帧间属性（如剪切类型和过渡）的19个细粒度任务，分为识别、推理和判断三个阶段。作者构建了一个集成了基于本体的知识库的自动注释管道，以提升VEU任务的标注效率。通过实验测试11个最先进的 Vid-LLMs，发现它们在VEU任务中表现不佳，有些甚至不如随机选择。为解决这一问题，论文开发了 Oscars 模型，通过在VEU-Bench数据集上微调，比现有开源 Vid-LLMs 准确率提高了28.3%，并达到与 GPT-4o 相当的水平；此外，融入 VEU 数据还能使 Vid-LLMs 在一般视频理解基准上平均提升8.3%。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to CVPR2025",
      "pdf_url": "http://arxiv.org/pdf/2504.17828v1",
      "published_date": "2025-04-24 04:36:28 UTC",
      "updated_date": "2025-04-24 04:36:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:29:19.624308"
    },
    {
      "arxiv_id": "2504.17219v1",
      "title": "Enhancing Variational Autoencoders with Smooth Robust Latent Encoding",
      "title_zh": "翻译失败",
      "authors": [
        "Hyomin Lee",
        "Minseon Kim",
        "Sangwon Jang",
        "Jongheon Jeong",
        "Sung Ju Hwang"
      ],
      "abstract": "Variational Autoencoders (VAEs) have played a key role in scaling up\ndiffusion-based generative models, as in Stable Diffusion, yet questions\nregarding their robustness remain largely underexplored. Although adversarial\ntraining has been an established technique for enhancing robustness in\npredictive models, it has been overlooked for generative models due to concerns\nabout potential fidelity degradation by the nature of trade-offs between\nperformance and robustness. In this work, we challenge this presumption,\nintroducing Smooth Robust Latent VAE (SRL-VAE), a novel adversarial training\nframework that boosts both generation quality and robustness. In contrast to\nconventional adversarial training, which focuses on robustness only, our\napproach smooths the latent space via adversarial perturbations, promoting more\ngeneralizable representations while regularizing with originality\nrepresentation to sustain original fidelity. Applied as a post-training step on\npre-trained VAEs, SRL-VAE improves image robustness and fidelity with minimal\ncomputational overhead. Experiments show that SRL-VAE improves both generation\nquality, in image reconstruction and text-guided image editing, and robustness,\nagainst Nightshade attacks and image editing attacks. These results establish a\nnew paradigm, showing that adversarial training, once thought to be detrimental\nto generative models, can instead enhance both fidelity and robustness.",
      "tldr_zh": "本研究针对 Variational Autoencoders (VAEs) 在扩散生成模型（如 Stable Diffusion）中的鲁棒性问题，提出 Smooth Robust Latent VAE (SRL-VAE) 框架，通过对抗训练平滑潜在空间并使用原创性表示进行正则化，从而提升生成质量和鲁棒性，同时最小化计算开销。不同于传统对抗训练可能损害保真度的担忧，SRL-VAE 作为预训练 VAEs 的后训练步骤，能改善图像重建、文本引导图像编辑的表现，并增强对 Nightshade 攻击和图像编辑攻击的抵抗力。实验结果证明，该方法显著提高了 VAEs 的整体性能，建立了一个新范式，即对抗训练可同时增强生成模型的保真度和鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "Under review",
      "pdf_url": "http://arxiv.org/pdf/2504.17219v1",
      "published_date": "2025-04-24 03:17:57 UTC",
      "updated_date": "2025-04-24 03:17:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:29:31.730110"
    },
    {
      "arxiv_id": "2504.17827v3",
      "title": "Evolution Meets Diffusion: Efficient Neural Architecture Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Bingye Zhou",
        "Caiyang Yu"
      ],
      "abstract": "Neural Architecture Search (NAS) has gained widespread attention for its\ntransformative potential in deep learning model design. However, the vast and\ncomplex search space of NAS leads to significant computational and time costs.\nNeural Architecture Generation (NAG) addresses this by reframing NAS as a\ngeneration problem, enabling the precise generation of optimal architectures\nfor specific tasks. Despite its promise, mainstream methods like diffusion\nmodels face limitations in global search capabilities and are still hindered by\nhigh computational and time demands. To overcome these challenges, we propose\nEvolutionary Diffusion-based Neural Architecture Generation (EDNAG), a novel\napproach that achieves efficient and training-free architecture generation.\nEDNAG leverages evolutionary algorithms to simulate the denoising process in\ndiffusion models, using fitness to guide the transition from random Gaussian\ndistributions to optimal architecture distributions. This approach combines the\nstrengths of evolutionary strategies and diffusion models, enabling rapid and\neffective architecture generation. Extensive experiments demonstrate that EDNAG\nachieves state-of-the-art (SOTA) performance in architecture optimization, with\nan improvement in accuracy of up to 10.45%. Furthermore, it eliminates the need\nfor time-consuming training and boosts inference speed by an average of 50\ntimes, showcasing its exceptional efficiency and effectiveness.",
      "tldr_zh": "该论文提出 EDNAG（Evolutionary Diffusion-based Neural Architecture Generation），一种高效的神经架构生成方法，将进化算法与扩散模型相结合，以解决 Neural Architecture Search (NAS) 的高计算和时间成本问题。EDNAG 通过进化算法模拟扩散模型的去噪过程，使用适应度指导从随机高斯分布向最优架构分布的过渡，从而实现无需训练的快速架构生成。实验证明，该方法在架构优化中达到 SOTA 性能，准确率提升高达 10.45%，并将推理速度平均提高 50 倍。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.17827v3",
      "published_date": "2025-04-24 03:09:04 UTC",
      "updated_date": "2025-04-30 08:52:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:29:42.672101"
    },
    {
      "arxiv_id": "2504.18588v1",
      "title": "Dynamic QoS Prediction via a Non-Negative Tensor Snowflake Factorization",
      "title_zh": "翻译失败",
      "authors": [
        "YongHui Xia",
        "Lan Wang",
        "Hao Wu"
      ],
      "abstract": "Dynamic quality of service (QoS) data exhibit rich temporal patterns in\nuser-service interactions, which are crucial for a comprehensive understanding\nof user behavior and service conditions in Web service. As the number of users\nand services increases, there is a large amount of unobserved QoS data, which\nsignificantly affects users'choice of services. To predict unobserved QoS data,\nwe propose a Non-negative Snowflake Factorization of tensors model. This method\ndesigns a snowflake core tensor to enhance the model's learning capability.\nAdditionally, it employs a single latent factor-based, nonnegative\nmultiplication update on tensor (SLF-NMUT) for parameter learning. Empirical\nresults demonstrate that the proposed model more accurately learns dynamic\nuser-service interaction patterns, thereby yielding improved predictions for\nmissing QoS data.",
      "tldr_zh": "该研究针对动态 QoS 数据中用户-服务交互的丰富时间模式，提出了一种 Non-negative Tensor Snowflake Factorization 模型，用于预测未观察到的 QoS 数据。该模型设计了 snowflake core tensor 来提升学习能力，并采用 SLF-NMUT（single latent factor-based, nonnegative multiplication update on tensor）方法进行参数优化。实验结果表明，该模型能更准确地学习动态交互模式，从而显著改善对缺失 QoS 数据的预测准确性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.18588v1",
      "published_date": "2025-04-24 03:03:22 UTC",
      "updated_date": "2025-04-24 03:03:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:29:54.504521"
    },
    {
      "arxiv_id": "2504.17213v2",
      "title": "MASR: Self-Reflective Reasoning through Multimodal Hierarchical Attention Focusing for Agent-based Video Understanding",
      "title_zh": "MASR：通过多模态层次注意力聚焦实现的自我反思推理，用于基于代理的视频理解",
      "authors": [
        "Shiwen Cao",
        "Zhaoxing Zhang",
        "Junming Jiao",
        "Juyi Qiao",
        "Guowen Song",
        "Rong Shen",
        "Xiangbing Meng"
      ],
      "abstract": "Even in the era of rapid advances in large models, video understanding\nremains a highly challenging task. Compared to texts or images, videos commonly\ncontain more information with redundancy, requiring large models to properly\nallocate attention at a global level for comprehensive and accurate\nunderstanding. To address this, we propose a Multimodal hierarchical Attention\nfocusing Self-reflective Reasoning (MASR) framework for agent-based video\nunderstanding. The key innovation lies in its ability to detect and prioritize\nsegments of videos that are highly relevant to the query. Firstly, MASR\nrealizes Multimodal Coarse-to-fine Relevance Sensing (MCRS) which enhances the\ncorrelation between the acquired contextual information and the query.\nSecondly, MASR employs Dilated Temporal Expansion (DTE) to mitigate the risk of\nmissing crucial details when extracting semantic information from the focused\nframes selected through MCRS. By iteratively applying MCRS and DTE in the\nself-reflective reasoning process, MASR is able to adaptively adjust the\nattention to extract highly query-relevant context and therefore improve the\nresponse accuracy. In the EgoSchema dataset, MASR achieves a remarkable 5%\nperformance gain over previous leading approaches. In the Next-QA and IntentQA\ndatasets, it outperforms the state-of-the-art standards by 0.2% and 0.3%\nrespectively. In the Video-MME dataset that contains long-term videos, MASR\nalso performs better than other agent-based methods.",
      "tldr_zh": "该研究提出MASR框架，通过Multimodal Hierarchical Attention Focusing实现自反性推理(Self-Reflective Reasoning)，以提升基于代理的视频理解能力。MASR的关键创新包括Multimodal Coarse-to-fine Relevance Sensing (MCRS)用于增强查询与上下文的相关性，以及Dilated Temporal Expansion (DTE)来避免提取语义信息时遗漏关键细节，通过迭代应用这些模块实现适应性注意力调整。实验结果显示，在EgoSchema数据集上，MASR比领先方法提升5%，而在Next-QA和IntentQA数据集上分别超过现有最佳标准0.2%和0.3%，在Video-MME数据集上也表现出优越性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.17213v2",
      "published_date": "2025-04-24 02:54:40 UTC",
      "updated_date": "2025-04-28 05:05:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:30:07.009274"
    },
    {
      "arxiv_id": "2504.17210v1",
      "title": "Synthetic Power Flow Data Generation Using Physics-Informed Denoising Diffusion Probabilistic Models",
      "title_zh": "翻译失败",
      "authors": [
        "Junfei Wang",
        "Darshana Upadhyay",
        "Marzia Zaman",
        "Pirathayini Srikantha"
      ],
      "abstract": "Many data-driven modules in smart grid rely on access to high-quality power\nflow data; however, real-world data are often limited due to privacy and\noperational constraints. This paper presents a physics-informed generative\nframework based on Denoising Diffusion Probabilistic Models (DDPMs) for\nsynthesizing feasible power flow data. By incorporating auxiliary training and\nphysics-informed loss functions, the proposed method ensures that the generated\ndata exhibit both statistical fidelity and adherence to power system\nfeasibility. We evaluate the approach on the IEEE 14-bus and 30-bus benchmark\nsystems, demonstrating its ability to capture key distributional properties and\ngeneralize to out-of-distribution scenarios. Comparative results show that the\nproposed model outperforms three baseline models in terms of feasibility,\ndiversity, and accuracy of statistical features. This work highlights the\npotential of integrating generative modelling into data-driven power system\napplications.",
      "tldr_zh": "本文提出了一种基于 Physics-Informed Denoising Diffusion Probabilistic Models (DDPMs) 的生成框架，用于合成可行的功率流数据，以解决智能电网中数据驱动模块因隐私和操作限制而缺乏高质量数据的难题。该框架通过辅助训练和物理信息损失函数，确保生成数据在统计上真实且符合功率系统可行性。在 IEEE 14-bus 和 30-bus 基准系统上进行评估，结果显示该模型在可行性、多样性和统计特征准确性方面优于三个基线模型，并能泛化到分布外场景。该工作突出了将生成模型整合到数据驱动功率系统应用中的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Submitted to IEEE SmartGridComm Conference 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.17210v1",
      "published_date": "2025-04-24 02:53:22 UTC",
      "updated_date": "2025-04-24 02:53:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:30:18.050195"
    },
    {
      "arxiv_id": "2504.17826v1",
      "title": "FashionM3: Multimodal, Multitask, and Multiround Fashion Assistant based on Unified Vision-Language Model",
      "title_zh": "翻译失败",
      "authors": [
        "Kaicheng Pang",
        "Xingxing Zou",
        "Waikeung Wong"
      ],
      "abstract": "Fashion styling and personalized recommendations are pivotal in modern\nretail, contributing substantial economic value in the fashion industry. With\nthe advent of vision-language models (VLM), new opportunities have emerged to\nenhance retailing through natural language and visual interactions. This work\nproposes FashionM3, a multimodal, multitask, and multiround fashion assistant,\nbuilt upon a VLM fine-tuned for fashion-specific tasks. It helps users discover\nsatisfying outfits by offering multiple capabilities including personalized\nrecommendation, alternative suggestion, product image generation, and virtual\ntry-on simulation. Fine-tuned on the novel FashionRec dataset, comprising\n331,124 multimodal dialogue samples across basic, personalized, and alternative\nrecommendation tasks, FashionM3 delivers contextually personalized suggestions\nwith iterative refinement through multiround interactions. Quantitative and\nqualitative evaluations, alongside user studies, demonstrate FashionM3's\nsuperior performance in recommendation effectiveness and practical value as a\nfashion assistant.",
      "tldr_zh": "这篇论文提出了 FashionM3，一种基于统一 Vision-Language Model (VLM) 的多模态、多任务、多轮时尚助手，用于提升时尚零售中的个性化推荐和互动体验。FashionM3 提供多种功能，包括个性化推荐、备选建议、产品图像生成和虚拟试穿模拟，并通过新数据集 FashionRec（包含 331,124 个多模态对话样本）进行微调，支持多轮上下文交互以实现迭代优化。实验评估显示，FashionM3 在推荐效果和实用价值上显著优于基线模型，经定量、定性和用户研究证实。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.17826v1",
      "published_date": "2025-04-24 02:44:22 UTC",
      "updated_date": "2025-04-24 02:44:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:30:30.626796"
    },
    {
      "arxiv_id": "2504.17825v1",
      "title": "Dual Prompting Image Restoration with Diffusion Transformers",
      "title_zh": "翻译失败",
      "authors": [
        "Dehong Kong",
        "Fan Li",
        "Zhixin Wang",
        "Jiaqi Xu",
        "Renjing Pei",
        "Wenbo Li",
        "WenQi Ren"
      ],
      "abstract": "Recent state-of-the-art image restoration methods mostly adopt latent\ndiffusion models with U-Net backbones, yet still facing challenges in achieving\nhigh-quality restoration due to their limited capabilities. Diffusion\ntransformers (DiTs), like SD3, are emerging as a promising alternative because\nof their better quality with scalability. In this paper, we introduce DPIR\n(Dual Prompting Image Restoration), a novel image restoration method that\neffectivly extracts conditional information of low-quality images from multiple\nperspectives. Specifically, DPIR consits of two branches: a low-quality image\nconditioning branch and a dual prompting control branch. The first branch\nutilizes a lightweight module to incorporate image priors into the DiT with\nhigh efficiency. More importantly, we believe that in image restoration,\ntextual description alone cannot fully capture its rich visual characteristics.\nTherefore, a dual prompting module is designed to provide DiT with additional\nvisual cues, capturing both global context and local appearance. The extracted\nglobal-local visual prompts as extra conditional control, alongside textual\nprompts to form dual prompts, greatly enhance the quality of the restoration.\nExtensive experimental results demonstrate that DPIR delivers superior image\nrestoration performance.",
      "tldr_zh": "该论文提出了一种名为 DPIR 的图像恢复方法，使用 Diffusion Transformers (DiTs) 如 SD3 作为骨干网络，以克服传统扩散模型的局限性。DPIR 通过两个分支设计实现：低质量图像条件分支利用轻量级模块高效融入图像先验，以及双提示控制分支，该分支捕捉全局和局部视觉提示，与文本提示结合，提供额外的条件控制，从而提升恢复质量。实验结果显示，DPIR 在图像恢复任务中表现出色，显著提高了生成图像的质量。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR2025",
      "pdf_url": "http://arxiv.org/pdf/2504.17825v1",
      "published_date": "2025-04-24 02:34:44 UTC",
      "updated_date": "2025-04-24 02:34:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:30:42.127462"
    },
    {
      "arxiv_id": "2504.17198v1",
      "title": "Automatically Generating Rules of Malicious Software Packages via Large Language Model",
      "title_zh": "翻译失败",
      "authors": [
        "XiangRui Zhang",
        "HaoYu Chen",
        "Yongzhong He",
        "Wenjia Niu",
        "Qiang Li"
      ],
      "abstract": "Today's security tools predominantly rely on predefined rules crafted by\nexperts, making them poorly adapted to the emergence of software supply chain\nattacks. To tackle this limitation, we propose a novel tool, RuleLLM, which\nleverages large language models (LLMs) to automate rule generation for OSS\necosystems. RuleLLM extracts metadata and code snippets from malware as its\ninput, producing YARA and Semgrep rules that can be directly deployed in\nsoftware development. Specifically, the rule generation task involves three\nsubtasks: crafting rules, refining rules, and aligning rules. To validate\nRuleLLM's effectiveness, we implemented a prototype system and conducted\nexperiments on the dataset of 1,633 malicious packages. The results are\npromising that RuleLLM generated 763 rules (452 YARA and 311 Semgrep) with a\nprecision of 85.2\\% and a recall of 91.8\\%, outperforming state-of-the-art\n(SOTA) tools and scored-based approaches. We further analyzed generated rules\nand proposed a rule taxonomy: 11 categories and 38 subcategories.",
      "tldr_zh": "本研究针对现有安全工具依赖专家预定义规则且难以适应软件供应链攻击的问题，提出RuleLLM工具，利用Large Language Model (LLMs)自动生成恶意软件规则。RuleLLM以恶意软件的元数据和代码片段作为输入，通过三个子任务（制作规则、精炼规则和对齐规则）生成可直接部署的YARA和Semgrep规则。在1633个恶意包数据集上的实验中，RuleLLM生成了763条规则（452 YARA和311 Semgrep），精确率达85.2%、召回率达91.8%，优于SOTA工具。此外，该研究还分析了生成的规则并提出一个规则分类法，包括11个类别和38个子类别。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.SE",
      "comment": "14 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.17198v1",
      "published_date": "2025-04-24 02:15:45 UTC",
      "updated_date": "2025-04-24 02:15:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:30:55.438212"
    },
    {
      "arxiv_id": "2504.17180v2",
      "title": "We'll Fix it in Post: Improving Text-to-Video Generation with Neuro-Symbolic Feedback",
      "title_zh": "翻译失败",
      "authors": [
        "Minkyu Choi",
        "S P Sharan",
        "Harsh Goel",
        "Sahil Shah",
        "Sandeep Chinchali"
      ],
      "abstract": "Current text-to-video (T2V) generation models are increasingly popular due to\ntheir ability to produce coherent videos from textual prompts. However, these\nmodels often struggle to generate semantically and temporally consistent videos\nwhen dealing with longer, more complex prompts involving multiple objects or\nsequential events. Additionally, the high computational costs associated with\ntraining or fine-tuning make direct improvements impractical. To overcome these\nlimitations, we introduce NeuS-E, a novel zero-training video refinement\npipeline that leverages neuro-symbolic feedback to automatically enhance video\ngeneration, achieving superior alignment with the prompts. Our approach first\nderives the neuro-symbolic feedback by analyzing a formal video representation\nand pinpoints semantically inconsistent events, objects, and their\ncorresponding frames. This feedback then guides targeted edits to the original\nvideo. Extensive empirical evaluations on both open-source and proprietary T2V\nmodels demonstrate that NeuS-E significantly enhances temporal and logical\nalignment across diverse prompts by almost 40%",
      "tldr_zh": "本研究针对文本到视频 (T2V) 生成模型在处理复杂提示时存在的语义和时间一致性问题，以及高计算成本的限制，提出了一种零训练的视频精炼管道 NeuS-E。NeuS-E 通过 neuro-symbolic feedback 分析视频的正式表示，识别并修复不一致的事件、对象和帧，从而实现针对性编辑和提升视频与提示的对齐。实验结果显示，在开源和专有 T2V 模型上，NeuS-E 几乎提高了 40% 的时间和逻辑对齐性能，为高效改进视频生成提供了可行方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.17180v2",
      "published_date": "2025-04-24 01:34:12 UTC",
      "updated_date": "2025-04-25 02:41:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:31:05.971152"
    },
    {
      "arxiv_id": "2504.17179v1",
      "title": "AUTHENTICATION: Identifying Rare Failure Modes in Autonomous Vehicle Perception Systems using Adversarially Guided Diffusion Models",
      "title_zh": "翻译失败",
      "authors": [
        "Mohammad Zarei",
        "Melanie A Jutras",
        "Eliana Evans",
        "Mike Tan",
        "Omid Aaramoon"
      ],
      "abstract": "Autonomous Vehicles (AVs) rely on artificial intelligence (AI) to accurately\ndetect objects and interpret their surroundings. However, even when trained\nusing millions of miles of real-world data, AVs are often unable to detect rare\nfailure modes (RFMs). The problem of RFMs is commonly referred to as the\n\"long-tail challenge\", due to the distribution of data including many instances\nthat are very rarely seen. In this paper, we present a novel approach that\nutilizes advanced generative and explainable AI techniques to aid in\nunderstanding RFMs. Our methods can be used to enhance the robustness and\nreliability of AVs when combined with both downstream model training and\ntesting. We extract segmentation masks for objects of interest (e.g., cars) and\ninvert them to create environmental masks. These masks, combined with carefully\ncrafted text prompts, are fed into a custom diffusion model. We leverage the\nStable Diffusion inpainting model guided by adversarial noise optimization to\ngenerate images containing diverse environments designed to evade object\ndetection models and expose vulnerabilities in AI systems. Finally, we produce\nnatural language descriptions of the generated RFMs that can guide developers\nand policymakers to improve the safety and reliability of AV systems.",
      "tldr_zh": "本文提出AUTHENTICATION方法，利用对抗性引导的扩散模型来识别自动驾驶车辆(AVs)感知系统中的稀有故障模式(RFMs)，以解决长尾挑战问题。该方法通过提取物体分割掩码、生成环境掩码，并结合精心设计的文本提示输入Stable Diffusion inpainting模型，创建多样化的图像以规避物体检测模型并暴露AI漏洞。最终，生成的RFMs图像配以自然语言描述，可用于增强下游模型的训练和测试，提高AVs的鲁棒性和可靠性。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.RO",
        "68T45, 68T05 68T45, 68T05 68T45, 68T05",
        "I.2.6; I.2.10; I.4.8"
      ],
      "primary_category": "cs.AI",
      "comment": "8 pages, 10 figures. Accepted to IEEE Conference on Artificial\n  Intelligence (CAI), 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.17179v1",
      "published_date": "2025-04-24 01:31:13 UTC",
      "updated_date": "2025-04-24 01:31:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:31:17.744453"
    },
    {
      "arxiv_id": "2504.18587v1",
      "title": "Training Large Language Models to Reason via EM Policy Gradient",
      "title_zh": "通过 EM 策略梯度训练大语言模型进行推理",
      "authors": [
        "Tianbing Xu"
      ],
      "abstract": "Recently, foundation models such as OpenAI's O1 and O3, along with DeepSeek's\nR1, have demonstrated strong reasoning capacities and problem-solving skills\nacquired through large-scale reinforcement learning (RL), with wide\napplications in mathematics, coding, science, intelligent agents, and virtual\nassistants. In this work, we introduce an off-policy reinforcement learning\nalgorithm, EM Policy Gradient, aimed at enhancing LLM reasoning by optimizing\nexpected return over reasoning trajectories. We frame the reasoning task as an\nExpectation-Maximization (EM) optimization problem, alternating between\nsampling diverse rationale trajectories and performing reward-guided\nfine-tuning. Unlike PPO and GRPO, which rely on complex importance weights and\nheuristic clipping, our method provides a simpler, more principled off-policy\npolicy gradient approach, eliminating these complexities while maintaining\nstrong performance. We evaluate the effectiveness of EM Policy Gradient on the\nGSM8K and MATH (HARD) datasets, where it achieves performance comparable to or\nslightly surpassing the state-of-the-art GRPO, while offering additional\nadvantages in scalability, simplicity, and reasoning conciseness. Moreover,\nmodels fine-tuned with our method exhibit cognitive behaviors, such as\nsub-problem decomposition, self-verification, and backtracking, highlighting\nits potential to enhance both the interpretability and robustness of LLM\nreasoning.",
      "tldr_zh": "该论文提出了一种名为 EM Policy Gradient 的 off-policy 强化学习算法，用于训练大型语言模型（LLMs）提升推理能力，通过优化推理轨迹的预期回报来实现。算法将推理任务建模为 Expectation-Maximization (EM) 优化问题，交替采样多样化推理轨迹并进行奖励引导微调，从而避免了 PPO 和 GRPO 的复杂权重和剪切机制，提供更简单且高效的训练方式。在 GSM8K 和 MATH (HARD) 数据集上，EM Policy Gradient 达到了与 GRPO 相当或略高的性能，同时提升了模型的可扩展性、推理简洁性和认知行为，如子问题分解、自我验证及回溯，最终增强了 LLM 推理的可解释性和鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.18587v1",
      "published_date": "2025-04-24 01:31:05 UTC",
      "updated_date": "2025-04-24 01:31:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:31:31.688121"
    },
    {
      "arxiv_id": "2504.17170v1",
      "title": "Improving Human-Autonomous Vehicle Interaction in Complex Systems",
      "title_zh": "在复杂系统中改进人类与自动驾驶车辆的交互",
      "authors": [
        "Robert Kaufman"
      ],
      "abstract": "Unresolved questions about how autonomous vehicles (AVs) should meet the\ninformational needs of riders hinder real-world adoption. Complicating our\nability to satisfy rider needs is that different people, goals, and driving\ncontexts have different criteria for what constitutes interaction success.\nUnfortunately, most human-AV research and design today treats all people and\nsituations uniformly. It is crucial to understand how an AV should communicate\nto meet rider needs, and how communications should change when the human-AV\ncomplex system changes. I argue that understanding the relationships between\ndifferent aspects of the human-AV system can help us build improved and\nadaptable AV communications. I support this argument using three empirical\nstudies. First, I identify optimal communication strategies that enhance\ndriving performance, confidence, and trust for learning in extreme driving\nenvironments. Findings highlight the need for task-sensitive,\nmodality-appropriate communications tuned to learner cognitive limits and\ngoals. Next, I highlight the consequences of deploying faulty communication\nsystems and demonstrate the need for context-sensitive communications. Third, I\nuse machine learning (ML) to illuminate personal factors predicting trust in\nAVs, emphasizing the importance of tailoring designs to individual traits and\nconcerns. Together, this dissertation supports the necessity of transparent,\nadaptable, and personalized AV systems that cater to individual needs, goals,\nand contextual demands. By considering the complex system within which human-AV\ninteractions occur, we can deliver valuable insights for designers,\nresearchers, and policymakers. This dissertation also provides a concrete\ndomain to study theories of human-machine joint action and situational\nawareness, and can be used to guide future human-AI interaction research.\n[shortened for arxiv]",
      "tldr_zh": "这篇论文探讨了如何提升人类与Autonomous Vehicles (AVs)互动，以满足不同骑手的资讯需求，强调个性化通信的重要性，因为当前研究往往忽略人群、目标和情境的差异。作者通过三个实证研究支持论点：首先，识别最佳通信策略以提高极端驾驶环境中的驾驶表现、信心和信任；其次，分析故障通信系统的后果，突出情境敏感性的必要性；第三，使用Machine Learning (ML)预测影响AV信任的个人因素。总体贡献在于提出透明、可适应和个性化的AV系统设计框架，为设计师、研究者和政策制定者提供指导，并为人类-机器联合行动和情境意识理论的研究提供实际应用领域。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "PhD Dissertation from University of California, San Diego; 175 pages",
      "pdf_url": "http://arxiv.org/pdf/2504.17170v1",
      "published_date": "2025-04-24 01:09:51 UTC",
      "updated_date": "2025-04-24 01:09:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:31:44.832806"
    },
    {
      "arxiv_id": "2505.00017v1",
      "title": "ReCellTy: Domain-specific knowledge graph retrieval-augmented LLMs workflow for single-cell annotation",
      "title_zh": "翻译失败",
      "authors": [
        "Dezheng Han",
        "Yibin Jia",
        "Ruxiao Chen",
        "Wenjie Han",
        "Shuaishuai Guo",
        "Jianbo Wang"
      ],
      "abstract": "To enable precise and fully automated cell type annotation with large\nlanguage models (LLMs), we developed a graph structured feature marker database\nto retrieve entities linked to differential genes for cell reconstruction. We\nfurther designed a multi task workflow to optimize the annotation process.\nCompared to general purpose LLMs, our method improves human evaluation scores\nby up to 0.21 and semantic similarity by 6.1% across 11 tissue types, while\nmore closely aligning with the cognitive logic of manual annotation.",
      "tldr_zh": "该研究提出ReCellTy，一种基于领域特定知识图谱的检索增强LLMs工作流，用于实现单细胞注释的精确自动化。\n该框架构建了一个图结构化的特征标记数据库，以检索与差异基因相关的实体，并设计多任务流程优化注释过程。\n与通用LLMs相比，ReCellTy在11种组织类型上将人类评估分数提高最多0.21，并提升语义相似性6.1%，更符合手动注释的认知逻辑。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DB",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.00017v1",
      "published_date": "2025-04-24 01:05:22 UTC",
      "updated_date": "2025-04-24 01:05:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:31:53.525975"
    },
    {
      "arxiv_id": "2504.17162v1",
      "title": "A Comprehensive Review on RNA Subcellular Localization Prediction",
      "title_zh": "RNA 亚细胞定位预测的全面综述",
      "authors": [
        "Cece Zhang",
        "Xuehuan Zhu",
        "Nick Peterson",
        "Jieqiong Wang",
        "Shibiao Wan"
      ],
      "abstract": "The subcellular localization of RNAs, including long non-coding RNAs\n(lncRNAs), messenger RNAs (mRNAs), microRNAs (miRNAs) and other smaller RNAs,\nplays a critical role in determining their biological functions. For instance,\nlncRNAs are predominantly associated with chromatin and act as regulators of\ngene transcription and chromatin structure, while mRNAs are distributed across\nthe nucleus and cytoplasm, facilitating the transport of genetic information\nfor protein synthesis. Understanding RNA localization sheds light on processes\nlike gene expression regulation with spatial and temporal precision. However,\ntraditional wet lab methods for determining RNA localization, such as in situ\nhybridization, are often time-consuming, resource-demanding, and costly. To\novercome these challenges, computational methods leveraging artificial\nintelligence (AI) and machine learning (ML) have emerged as powerful\nalternatives, enabling large-scale prediction of RNA subcellular localization.\nThis paper provides a comprehensive review of the latest advancements in\nAI-based approaches for RNA subcellular localization prediction, covering\nvarious RNA types and focusing on sequence-based, image-based, and hybrid\nmethodologies that combine both data types. We highlight the potential of these\nmethods to accelerate RNA research, uncover molecular pathways, and guide\ntargeted disease treatments. Furthermore, we critically discuss the challenges\nin AI/ML approaches for RNA subcellular localization, such as data scarcity and\nlack of benchmarks, and opportunities to address them. This review aims to\nserve as a valuable resource for researchers seeking to develop innovative\nsolutions in the field of RNA subcellular localization and beyond.",
      "tldr_zh": "这篇综述论文全面审视了RNA亚细胞定位预测的重要性，包括lncRNAs、mRNAs、miRNAs等RNA类型如何影响基因表达调控和生物功能。论文强调了AI和ML方法（如基于序列、图像和混合模型）的最新进展，作为替代传统耗时实验（如原位杂交）的强大工具，能够加速RNA研究、揭示分子途径并指导疾病治疗。作者还讨论了当前挑战，如数据稀缺和缺乏基准，并指出未来机会，以推动创新解决方案的发展。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "q-bio.GN",
        "q-bio.SC"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.17162v1",
      "published_date": "2025-04-24 00:47:31 UTC",
      "updated_date": "2025-04-24 00:47:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:32:05.660290"
    },
    {
      "arxiv_id": "2504.17160v1",
      "title": "OUI Need to Talk About Weight Decay: A New Perspective on Overfitting Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Alberto Fernández-Hernández",
        "Jose I. Mestre",
        "Manuel F. Dolz",
        "Jose Duato",
        "Enrique S. Quintana-Ortí"
      ],
      "abstract": "We introduce the Overfitting-Underfitting Indicator (OUI), a novel tool for\nmonitoring the training dynamics of Deep Neural Networks (DNNs) and identifying\noptimal regularization hyperparameters. Specifically, we validate that OUI can\neffectively guide the selection of the Weight Decay (WD) hyperparameter by\nindicating whether a model is overfitting or underfitting during training\nwithout requiring validation data. Through experiments on DenseNet-BC-100 with\nCIFAR- 100, EfficientNet-B0 with TinyImageNet and ResNet-34 with ImageNet-1K,\nwe show that maintaining OUI within a prescribed interval correlates strongly\nwith improved generalization and validation scores. Notably, OUI converges\nsignificantly faster than traditional metrics such as loss or accuracy,\nenabling practitioners to identify optimal WD (hyperparameter) values within\nthe early stages of training. By leveraging OUI as a reliable indicator, we can\ndetermine early in training whether the chosen WD value leads the model to\nunderfit the training data, overfit, or strike a well-balanced trade-off that\nmaximizes validation scores. This enables more precise WD tuning for optimal\nperformance on the tested datasets and DNNs. All code for reproducing these\nexperiments is available at https://github.com/AlbertoFdezHdez/OUI.",
      "tldr_zh": "本研究引入了Overfitting-Underfitting Indicator (OUI)，一种新型工具，用于监控Deep Neural Networks (DNNs)的训练动态，并帮助识别最佳正则化超参数，特别是Weight Decay (WD)。OUI能够在训练过程中检测模型是否过拟合或欠拟合，而无需依赖验证数据，从而指导WD的精确调整。实验在DenseNet-BC-100与CIFAR-100、EfficientNet-B0与TinyImageNet以及ResNet-34与ImageNet-1K上显示，保持OUI在特定区间内显著提升模型泛化和验证分数，且OUI比传统指标如损失或准确率收敛更快，便于在训练早期优化超参数。代码已开源，供进一步验证和应用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.17160v1",
      "published_date": "2025-04-24 00:41:59 UTC",
      "updated_date": "2025-04-24 00:41:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T16:32:18.386848"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 110,
  "processed_papers_count": 110,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-24T16:32:46.416846"
}
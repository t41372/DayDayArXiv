{
  "date": "2025-01-06",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-01-06 的 arXiv 中文 TLDR 快报！今天的 arXiv 论文聚焦于 AI 模型优化、医疗应用、公平性挑战和 LLM 增强等主题，亮点包括 H. Vincent Poor 等知名学者的联邦学习论文，以及 Jitendra Malik 参与的视觉模型创新，强调了 LLM 在数学推理和图像生成中的潜力，同时探讨了 AI 安全性和可解释性问题。\n\n### 重点论文速览\n我挑选了今天最具影响力和话题度的论文进行讨论，先从 AI 安全、LLM 应用和医疗领域的创新入手，再快速覆盖其他相关主题。以下按主题归类，突出核心贡献。\n\n#### AI 安全与生成内容检测\n- **BoundingDocs: a Unified Dataset for Document Question Answering with Spatial Annotations**（BoundingDocs: 统一数据集用于文档问答的 spatial 标注）  \n  这篇论文引入了一个统一数据集，将文档 AI 任务转化为问答形式，并提供 OCR 和边界框标注，主要贡献是通过不同提示技术提升开源模型的文档理解性能，实验显示其在视觉丰富文档理解（VRDU）中表现出色。\n  \n- **Detecting AI-Generated Text in Educational Content: Leveraging Machine Learning and Explainable AI for Academic Integrity**（检测教育内容中的 AI 生成文本：利用机器学习和可解释 AI 提升学术诚信）  \n  作者提出 CyberHumanAI 数据集和 XGBoost 模型，用于区分人类和 AI 生成文本，主要发现是传统机器学习在短文本分类中准确率达 83%，并通过可解释 AI 识别特征（如人类文本更实用化），比 GPTZero 更可靠。\n\n- **CALM: Curiosity-Driven Auditing for Large Language Models**（CALM: 基于好奇心的 LLM 审计）  \n  这篇论文使用强化学习审计黑盒 LLM，主要贡献是开发一个代理模型自动发现有害输入-输出对，实验在名人侮辱和敏感内容检测中有效，提升了 LLM 的安全评估。\n\n#### LLM 和数学/视觉应用\n- **BoostStep: Boosting mathematical capability of Large Language Models via improved single-step reasoning**（BoostStep: 通过改进单步推理提升 LLM 的数学能力）  \n  作者提出 BoostStep 方法，使用步对齐的 In-Context Learning 提升 LLM 在数学任务中的准确性，主要发现是它将 GPT-4o 的 CoT 性能提高 4.6%，并与树搜索结合进一步提升 7.5%，特别适合复杂问题。\n\n- **Gaussian Masked Autoencoders**（高斯 Masked Autoencoders）  \n  Jitendra Malik 等作者创新性地将高斯散斑引入 Masked Autoencoders，主要贡献是联合学习语义和空间理解，实现零样本任务如图像分层和边缘检测，保持了 MAE 的语义优势。\n\n- **Automated Generation of Challenging Multiple-Choice Questions for Vision Language Model Evaluation**（自动生成挑战性多项选择题用于视觉语言模型评估）  \n  这篇论文的 AutoConverter 框架将开放式问题转化为多项选择题数据集 VMCBench，主要发现是它提升了 VLM 的客观评估，33 款模型在基准测试中表现出色。\n\n#### 医疗和图像处理创新\n- **FTA-FTL: A Fine-Tuned Aggregation Federated Transfer Learning Scheme for Lithology Microscopic Image Classification**（FTA-FTL: 用于岩性显微图像分类的微调聚合联邦迁移学习方案）  \n  作者提出 FTA-FTL 算法，用于处理地质图像分类，主要贡献是通过联邦学习和迁移学习在小数据集上实现高精度分类，实验显示其与集中式方法相当。\n\n- **PRMBench: A Fine-grained and Challenging Benchmark for Process-Level Reward Models**（PRMBench: 用于过程级奖励模型的细粒度基准）  \n  这篇论文构建了 PRMBench 数据集，评估奖励模型在推理过程中的错误检测能力，主要发现是它暴露了现有 PRM 的弱点，并为未来研究提供新方向。\n\n- **GMAE: Gaussian Masked Autoencoders for Image Representation Learning**（GMAE: 用于图像表示学习的 Gaussian Masked Autoencoders）  \n  相关于视觉模型的扩展，主要贡献是引入 3D 高斯表示提升图像重建和零样本任务性能。\n\n#### 机器学习公平性和优化\n- **Over-the-Air Fair Federated Learning via Multi-Objective Optimization**（通过多目标优化实现的 Over-the-Air 公平联邦学习）  \n  H. Vincent Poor 等作者提出 OTA-FFL 算法，用于处理联邦学习中的数据异质性，主要贡献是通过 Chebyshev 方法优化梯度聚合，实验证明它在公平性和鲁棒性上优于现有方法。\n\n- **LangFair: A Python Package for Assessing Bias and Fairness in Large Language Model Use Cases**（LangFair: 用于评估 LLM 偏差和公平性的 Python 包）  \n  这篇论文开发了 LangFair 工具包，主要发现是它通过生成评估数据集和决策框架，帮助 LLM 从业者检测偏差，提升模型公平性。\n\n其他论文如生存分析、机器人路径规划和图神经网络优化虽有贡献，但相对小众或技术细节较深，这里仅简要提及。例如，**LightGNN: Simple Graph Neural Network for Recommendation**（LightGNN: 简单图神经网络用于推荐）通过轻量级剪枝提升推荐效率；**Samba-ASR: State-Of-The-Art Speech Recognition Leveraging Structured State-Space Models**（Samba-ASR: 利用结构化状态空间模型的顶级语音识别）在语音任务中表现出色，但这些未列为重点。\n\n总之，今天的 arXiv 展示了 AI 领域的多样创新，特别在 LLM 安全和应用上值得关注，未来可能推动更可靠的模型部署。更多细节请查阅具体论文！",
  "papers": [
    {
      "arxiv_id": "2501.03413v1",
      "title": "SALT: Sales Autocompletion Linked Business Tables Dataset",
      "title_zh": "SALT：销售自动完成链接业务表格数据集",
      "authors": [
        "Tassilo Klein",
        "Clemens Biehl",
        "Margarida Costa",
        "Andre Sres",
        "Jonas Kolk",
        "Johannes Hoffart"
      ],
      "abstract": "Foundation models, particularly those that incorporate Transformer\narchitectures, have demonstrated exceptional performance in domains such as\nnatural language processing and image processing. Adapting these models to\nstructured data, like tables, however, introduces significant challenges. These\ndifficulties are even more pronounced when addressing multi-table data linked\nvia foreign key, which is prevalent in the enterprise realm and crucial for\nempowering business use cases. Despite its substantial impact, research\nfocusing on such linked business tables within enterprise settings remains a\nsignificantly important yet underexplored domain. To address this, we introduce\na curated dataset sourced from an Enterprise Resource Planning (ERP) system,\nfeaturing extensive linked tables. This dataset is specifically designed to\nsupport research endeavors in table representation learning. By providing\naccess to authentic enterprise data, our goal is to potentially enhance the\neffectiveness and applicability of models for real-world business contexts.",
      "tldr_zh": "本研究探讨了基础模型（Foundation models），尤其是Transformer架构，在处理结构化数据（如表格）时的挑战，特别是多表数据通过外键链接的企业场景，该领域虽重要但研究不足。为解决这一问题，研究团队引入了SALT数据集，该数据集从企业资源规划（ERP）系统中获取，包含广泛的链接表，旨在支持表格表示学习（table representation learning）的相关研究。通过提供真实企业数据，SALT有望提升模型在实际商业环境的适用性和有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.LG",
      "comment": "Table Representation Learning Workshop at NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2501.03413v1",
      "published_date": "2025-01-06 22:20:02 UTC",
      "updated_date": "2025-01-06 22:20:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:44:41.038238"
    },
    {
      "arxiv_id": "2501.03403v1",
      "title": "BoundingDocs: a Unified Dataset for Document Question Answering with Spatial Annotations",
      "title_zh": "BoundingDocs：一个带有空间标注的文档问答统一数据集",
      "authors": [
        "Simone Giovannini",
        "Fabio Coppini",
        "Andrea Gemelli",
        "Simone Marinai"
      ],
      "abstract": "We present a unified dataset for document Question-Answering (QA), which is\nobtained combining several public datasets related to Document AI and visually\nrich document understanding (VRDU). Our main contribution is twofold: on the\none hand we reformulate existing Document AI tasks, such as Information\nExtraction (IE), into a Question-Answering task, making it a suitable resource\nfor training and evaluating Large Language Models; on the other hand, we\nrelease the OCR of all the documents and include the exact position of the\nanswer to be found in the document image as a bounding box. Using this dataset,\nwe explore the impact of different prompting techniques (that might include\nbounding box information) on the performance of open-weight models, identifying\nthe most effective approaches for document comprehension.",
      "tldr_zh": "该论文介绍了 BoundingDocs，这是一个统一的文档问答（QA）数据集，通过整合多个公开的文档 AI 和视觉丰富文档理解（VRDU）相关数据集，旨在支持大型语言模型（Large Language Models）的训练和评估。研究者将现有任务如信息提取（IE）重新表述为 QA 格式，并提供所有文档的 OCR（光学字符识别）以及答案在文档图像中的精确边界框（bounding box）注解，作为主要贡献。实验探索了不同提示技术（包括边界框信息）对开源模型性能的影响，识别出最有效的文档理解方法。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03403v1",
      "published_date": "2025-01-06 21:46:22 UTC",
      "updated_date": "2025-01-06 21:46:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:44:54.382532"
    },
    {
      "arxiv_id": "2501.03394v2",
      "title": "Enhanced Importance Sampling through Latent Space Exploration in Normalizing Flows",
      "title_zh": "翻译失败",
      "authors": [
        "Liam A. Kruse",
        "Alexandros E. Tzikas",
        "Harrison Delecki",
        "Mansur M. Arief",
        "Mykel J. Kochenderfer"
      ],
      "abstract": "Importance sampling is a rare event simulation technique used in Monte Carlo\nsimulations to bias the sampling distribution towards the rare event of\ninterest. By assigning appropriate weights to sampled points, importance\nsampling allows for more efficient estimation of rare events or tails of\ndistributions. However, importance sampling can fail when the proposal\ndistribution does not effectively cover the target distribution. In this work,\nwe propose a method for more efficient sampling by updating the proposal\ndistribution in the latent space of a normalizing flow. Normalizing flows learn\nan invertible mapping from a target distribution to a simpler latent\ndistribution. The latent space can be more easily explored during the search\nfor a proposal distribution, and samples from the proposal distribution are\nrecovered in the space of the target distribution via the invertible mapping.\nWe empirically validate our methodology on simulated robotics applications such\nas autonomous racing and aircraft ground collision avoidance.",
      "tldr_zh": "这篇论文提出了一种通过在 Normalizing Flows 的 Latent Space 中探索来增强 Importance Sampling 的方法，以解决传统采样分布无法有效覆盖目标分布的问题。方法利用 Normalizing Flows 的可逆映射，在更简单的潜在空间中更新提案分布，然后映射回目标分布空间，从而提高稀有事件模拟的效率。实验在模拟机器人应用中（如自主赛车和飞机地面碰撞避免）验证了该方法的有效性，展示了其在 Monte Carlo 模拟中的实际优势。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted at AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.03394v2",
      "published_date": "2025-01-06 21:18:02 UTC",
      "updated_date": "2025-05-13 05:04:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:45:05.378900"
    },
    {
      "arxiv_id": "2501.03392v1",
      "title": "Over-the-Air Fair Federated Learning via Multi-Objective Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Shayan Mohajer Hamidi",
        "Ali Bereyhi",
        "Saba Asaad",
        "H. Vincent Poor"
      ],
      "abstract": "In federated learning (FL), heterogeneity among the local dataset\ndistributions of clients can result in unsatisfactory performance for some,\nleading to an unfair model. To address this challenge, we propose an\nover-the-air fair federated learning algorithm (OTA-FFL), which leverages\nover-the-air computation to train fair FL models. By formulating FL as a\nmulti-objective minimization problem, we introduce a modified Chebyshev\napproach to compute adaptive weighting coefficients for gradient aggregation in\neach communication round. To enable efficient aggregation over the multiple\naccess channel, we derive analytical solutions for the optimal transmit scalars\nat the clients and the de-noising scalar at the parameter server. Extensive\nexperiments demonstrate the superiority of OTA-FFL in achieving fairness and\nrobust performance compared to existing methods.",
      "tldr_zh": "本文提出 OTA-FFL 算法，利用 over-the-air computation 技术来解决联邦学习 (FL) 中客户端数据集分布异质性导致的不公平问题。算法将 FL 表述为多目标最小化问题，并采用修改 Chebyshev 方法计算自适应权重系数，以优化每个通信轮次的梯度聚合。同时，通过导出客户端的最佳传输标量和参数服务器的去噪标量，实现多路访问信道上的高效聚合。实验结果显示，OTA-FFL 在公平性和鲁棒性能方面显著优于现有方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03392v1",
      "published_date": "2025-01-06 21:16:51 UTC",
      "updated_date": "2025-01-06 21:16:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:45:18.276590"
    },
    {
      "arxiv_id": "2501.03376v1",
      "title": "Existential Crisis: A Social Robot's Reason for Being",
      "title_zh": "存在危机：社交机器人的存在理由",
      "authors": [
        "Dora Medgyesy",
        "Joella Galas",
        "Julian van Pol",
        "Rustam Eynaliyev",
        "Thijs Vollebregt"
      ],
      "abstract": "As Robots become ever more important in our daily lives there's growing need\nfor understanding how they're perceived by people. This study aims to\ninvestigate how the user perception of robots is influenced by displays of\npersonality. Using LLMs and speech to text technology, we designed a\nwithin-subject study to compare two conditions: a personality-driven robot and\na purely task-oriented, personality-neutral robot. Twelve participants,\nrecruited from Socially Intelligent Robotics course at Vrije Universiteit\nAmsterdam, interacted with a robot Nao tasked with asking them a set of medical\nquestions under both conditions. After completing both interactions, the\nparticipants completed a user experience questionnaire measuring their\nemotional states and robot perception using standardized questionnaires from\nthe SRI and Psychology literature.",
      "tldr_zh": "这篇论文探讨了机器人显示个性如何影响用户感知的问题，旨在通过实验评估其对人类互动的影响。研究采用within-subject设计，比较了两种条件：一个基于LLMs和语音到文本技术的个性驱动机器人，以及一个纯任务导向的、没有个性的机器人。十二名参与者从阿姆斯特丹自由大学的社交智能机器人课程中招募，与Nao机器人互动，回答医疗问题后，通过SRI和心理学文献中的标准化问卷测量了他们的情绪状态和对机器人的感知。该研究为设计更具吸引力的社交机器人提供了宝贵见解。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03376v1",
      "published_date": "2025-01-06 20:30:15 UTC",
      "updated_date": "2025-01-06 20:30:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:45:30.748025"
    },
    {
      "arxiv_id": "2501.03374v1",
      "title": "License Plate Images Generation with Diffusion Models",
      "title_zh": "翻译失败",
      "authors": [
        "Mariia Shpir",
        "Nadiya Shvai",
        "Amir Nakib"
      ],
      "abstract": "Despite the evident practical importance of license plate recognition (LPR),\ncorresponding research is limited by the volume of publicly available datasets\ndue to privacy regulations such as the General Data Protection Regulation\n(GDPR). To address this challenge, synthetic data generation has emerged as a\npromising approach. In this paper, we propose to synthesize realistic license\nplates (LPs) using diffusion models, inspired by recent advances in image and\nvideo generation. In our experiments a diffusion model was successfully trained\non a Ukrainian LP dataset, and 1000 synthetic images were generated for\ndetailed analysis. Through manual classification and annotation of the\ngenerated images, we performed a thorough study of the model output, such as\nsuccess rate, character distributions, and type of failures. Our contributions\ninclude experimental validation of the efficacy of diffusion models for LP\nsynthesis, along with insights into the characteristics of the generated data.\nFurthermore, we have prepared a synthetic dataset consisting of 10,000 LP\nimages, publicly available at https://zenodo.org/doi/10.5281/zenodo.13342102.\nConducted experiments empirically confirm the usefulness of synthetic data for\nthe LPR task. Despite the initial performance gap between the model trained\nwith real and synthetic data, the expansion of the training data set with\npseudolabeled synthetic data leads to an improvement in LPR accuracy by 3%\ncompared to baseline.",
      "tldr_zh": "本论文针对车牌识别（LPR）研究中数据集受限的问题（如 GDPR 隐私法规），提出使用 diffusion models 生成 realistic 车牌图像作为合成数据解决方案。通过在乌克兰车牌数据集上训练 diffusion models，生成并分析 1000 张合成图像，包括成功率、字符分布和失败类型，并公开一个包含 10,000 张图像的合成数据集（https://zenodo.org/doi/10.5281/zenodo.13342102）。实验结果显示，使用伪标签增强的合成数据训练 LPR 模型，能将准确率提高 3%，验证了 diffusion models 在 LP 合成中的有效性，并为 LPR 任务提供宝贵资源。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03374v1",
      "published_date": "2025-01-06 20:22:18 UTC",
      "updated_date": "2025-01-06 20:22:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:45:42.693229"
    },
    {
      "arxiv_id": "2501.03370v1",
      "title": "Advanced Machine Learning Techniques for Social Support Detection on Social Media",
      "title_zh": "翻译失败",
      "authors": [
        "Olga Kolesnikova",
        "Moein Shahiki Tash",
        "Zahra Ahani",
        "Ameeta Agrawal",
        "Raul Monroy",
        "Grigori Sidorov"
      ],
      "abstract": "The widespread use of social media highlights the need to understand its\nimpact, particularly the role of online social support. This study uses a\ndataset focused on online social support, which includes binary and multiclass\nclassifications of social support content on social media. The classification\nof social support is divided into three tasks. The first task focuses on\ndistinguishing between supportive and non-supportive. The second task aims to\nidentify whether the support is directed toward an individual or a group. The\nthird task categorizes the specific type of social support, grouping it into\ncategories such as Nation, LGBTQ, Black people, Women, Religion, and Other (if\nit does not fit into the previously mentioned categories). To address data\nimbalances in these tasks, we employed K-means clustering for balancing the\ndataset and compared the results with the original unbalanced data. Using\nadvanced machine learning techniques, including transformers and zero-shot\nlearning approaches with GPT3, GPT4, and GPT4-o, we predict social support\nlevels in various contexts. The effectiveness of the dataset is evaluated using\nbaseline models across different learning approaches, with transformer-based\nmethods demonstrating superior performance. Additionally, we achieved a 0.4\\%\nincrease in the macro F1 score for the second task and a 0.7\\% increase for the\nthird task, compared to previous work utilizing traditional machine learning\nwith psycholinguistic and unigram-based TF-IDF values.",
      "tldr_zh": "本研究探讨了使用高级机器学习技术检测社交媒体上的社会支持（social support），基于一个包含二元和多类分类的数据集，定义了三个任务：区分支持性与非支持性内容、识别支持对象是个人还是群体，以及分类支持类型（如 Nation、LGBTQ、Black people、Women、Religion 和 Other）。为了应对数据不平衡问题，研究者采用了 K-means clustering 平衡数据集，并比较了原始数据的结果。利用 transformers 和 zero-shot learning 方法（如 GPT3、GPT4 和 GPT4-o），transformer-based 模型展示了优越性能，并在第二任务上将 macro F1 score 提高了 0.4%，第三任务提高了 0.7%，相比传统机器学习方法取得了小幅改进。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03370v1",
      "published_date": "2025-01-06 20:14:09 UTC",
      "updated_date": "2025-01-06 20:14:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:45:54.740671"
    },
    {
      "arxiv_id": "2501.03349v1",
      "title": "FTA-FTL: A Fine-Tuned Aggregation Federated Transfer Learning Scheme for Lithology Microscopic Image Classification",
      "title_zh": "FTA-FTL：一种针对岩性显微图像分类的微调聚合联邦迁移学习方案",
      "authors": [
        "Keyvan RahimiZadeh",
        "Ahmad Taheri",
        "Jan Baumbach",
        "Esmael Makarian",
        "Abbas Dehghani",
        "Bahman Ravaei",
        "Bahman Javadi",
        "Amin Beheshti"
      ],
      "abstract": "Lithology discrimination is a crucial activity in characterizing oil\nreservoirs, and processing lithology microscopic images is an essential\ntechnique for investigating fossils and minerals and geological assessment of\nshale oil exploration. In this way, Deep Learning (DL) technique is a powerful\napproach for building robust classifier models. However, there is still a\nconsiderable challenge to collect and produce a large dataset.\nTransfer-learning and data augmentation techniques have emerged as popular\napproaches to tackle this problem. Furthermore, due to different reasons,\nespecially data privacy, individuals, organizations, and industry companies\noften are not willing to share their sensitive data and information. Federated\nLearning (FL) has emerged to train a highly accurate central model across\nmultiple decentralized edge servers without transferring sensitive data,\npreserving sensitive data, and enhancing security. This study involves two\nphases; the first phase is to conduct Lithology microscopic image\nclassification on a small dataset using transfer learning. In doing so, various\npre-trained DL model architectures are comprehensively compared for the\nclassification task. In the second phase, we formulated the classification task\nto a Federated Transfer Learning (FTL) scheme and proposed a Fine-Tuned\nAggregation strategy for Federated Learning (FTA-FTL). In order to perform a\ncomprehensive experimental study, several metrics such as accuracy, f1 score,\nprecision, specificity, sensitivity (recall), and confusion matrix are taken\ninto account. The results are in excellent agreement and confirm the efficiency\nof the proposed scheme, and show that the proposed FTA-FTL algorithm is capable\nenough to achieve approximately the same results obtained by the centralized\nimplementation for Lithology microscopic images classification task.",
      "tldr_zh": "本研究针对岩石学微观图像分类面临的挑战，如数据不足和隐私问题，提出了一种Fine-Tuned Aggregation Federated Transfer Learning (FTA-FTL)方案。\n首先，通过转移学习(Transfer Learning)在小数据集上比较多种预训练深度学习(DL)模型的性能，以构建高效的图像分类器。\n随后，将任务扩展到联邦学习(Federated Learning)框架中，引入Fine-Tuned Aggregation策略，实现无需共享敏感数据的情况下训练中心模型。\n实验结果显示，FTA-FTL在准确率、F1分数、精确度和敏感度等指标上，与集中式实现相当，证明了其在岩石学图像分类中的高效性和实用性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03349v1",
      "published_date": "2025-01-06 19:32:14 UTC",
      "updated_date": "2025-01-06 19:32:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:46:07.162521"
    },
    {
      "arxiv_id": "2501.03324v1",
      "title": "Analyzing Bias in Swiss Federal Supreme Court Judgments Using Facebook's Holistic Bias Dataset: Implications for Language Model Training",
      "title_zh": "翻译失败",
      "authors": [
        "Sabine Wehnert",
        "Muhammet Ertas",
        "Ernesto William De Luca"
      ],
      "abstract": "Natural Language Processing (NLP) is vital for computers to process and\nrespond accurately to human language. However, biases in training data can\nintroduce unfairness, especially in predicting legal judgment. This study\nfocuses on analyzing biases within the Swiss Judgment Prediction Dataset\n(SJP-Dataset). Our aim is to ensure unbiased factual descriptions essential for\nfair decision making by NLP models in legal contexts. We analyze the dataset\nusing social bias descriptors from the Holistic Bias dataset and employ\nadvanced NLP techniques, including attention visualization, to explore the\nimpact of dispreferred descriptors on model predictions. The study identifies\nbiases and examines their influence on model behavior. Challenges include\ndataset imbalance and token limits affecting model performance.",
      "tldr_zh": "这篇论文分析了 Swiss Judgment Prediction Dataset (SJP-Dataset) 中的偏见问题，旨在确保 Natural Language Processing (NLP) 模型在法律判断预测中提供公平、无偏见的描述。研究采用 Holistic Bias dataset 的社会偏见描述符以及高级 NLP 技术（如注意力可视化），来探索这些偏见对模型预测的影响，并识别出偏见如何导致模型行为偏差。最终，论文揭示了数据集不平衡和令牌限制等挑战，并讨论了这些发现对语言模型训练的启示，以促进更公正的法律应用。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03324v1",
      "published_date": "2025-01-06 19:00:09 UTC",
      "updated_date": "2025-01-06 19:00:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:46:18.510186"
    },
    {
      "arxiv_id": "2501.03229v1",
      "title": "Gaussian Masked Autoencoders",
      "title_zh": "翻译失败",
      "authors": [
        "Jathushan Rajasegaran",
        "Xinlei Chen",
        "Rulilong Li",
        "Christoph Feichtenhofer",
        "Jitendra Malik",
        "Shiry Ginosar"
      ],
      "abstract": "This paper explores Masked Autoencoders (MAE) with Gaussian Splatting. While\nreconstructive self-supervised learning frameworks such as MAE learns good\nsemantic abstractions, it is not trained for explicit spatial awareness. Our\napproach, named Gaussian Masked Autoencoder, or GMAE, aims to learn semantic\nabstractions and spatial understanding jointly. Like MAE, it reconstructs the\nimage end-to-end in the pixel space, but beyond MAE, it also introduces an\nintermediate, 3D Gaussian-based representation and renders images via\nsplatting. We show that GMAE can enable various zero-shot learning capabilities\nof spatial understanding (e.g., figure-ground segmentation, image layering,\nedge detection, etc.) while preserving the high-level semantics of\nself-supervised representation quality from MAE. To our knowledge, we are the\nfirst to employ Gaussian primitives in an image representation learning\nframework beyond optimization-based single-scene reconstructions. We believe\nGMAE will inspire further research in this direction and contribute to\ndeveloping next-generation techniques for modeling high-fidelity visual data.\nMore details at https://brjathu.github.io/gmae",
      "tldr_zh": "这篇论文提出了 Gaussian Masked Autoencoder (GMAE)，一种结合 Masked Autoencoders (MAE) 和 Gaussian Splatting 的方法，旨在同时学习图像的语义抽象和空间理解。GMAE 在像素空间端到端重建图像，同时引入 3D Gaussian-based 代表并通过 splatting 渲染，从而提升了模型的空间感知能力。实验结果显示，GMAE 实现了各种零样本学习功能，如 figure-ground segmentation、image layering 和 edge detection，同时保留了 MAE 的高级语义表示。该框架首次将 Gaussian primitives 应用于图像表示学习框架之外的场景，有望推动高保真视觉数据建模的创新发展。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03229v1",
      "published_date": "2025-01-06 18:59:57 UTC",
      "updated_date": "2025-01-06 18:59:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:46:31.056904"
    },
    {
      "arxiv_id": "2501.03228v3",
      "title": "LightGNN: Simple Graph Neural Network for Recommendation",
      "title_zh": "翻译失败",
      "authors": [
        "Guoxuan Chen",
        "Lianghao Xia",
        "Chao Huang"
      ],
      "abstract": "Graph neural networks (GNNs) have demonstrated superior performance in\ncollaborative recommendation through their ability to conduct high-order\nrepresentation smoothing, effectively capturing structural information within\nusers' interaction patterns. However, existing GNN paradigms face significant\nchallenges in scalability and robustness when handling large-scale, noisy, and\nreal-world datasets. To address these challenges, we present LightGNN, a\nlightweight and distillation-based GNN pruning framework designed to\nsubstantially reduce model complexity while preserving essential collaboration\nmodeling capabilities. Our LightGNN framework introduces a computationally\nefficient pruning module that adaptively identifies and removes redundant edges\nand embedding entries for model compression. The framework is guided by a\nresource-friendly hierarchical knowledge distillation objective, whose\nintermediate layer augments the observed graph to maintain performance,\nparticularly in high-rate compression scenarios. Extensive experiments on\npublic datasets demonstrate LightGNN's effectiveness, significantly improving\nboth computational efficiency and recommendation accuracy. Notably, LightGNN\nachieves an 80% reduction in edge count and 90% reduction in embedding entries\nwhile maintaining performance comparable to more complex state-of-the-art\nbaselines. The implementation of our LightGNN framework is available at the\ngithub repository: https://github.com/HKUDS/LightGNN.",
      "tldr_zh": "该研究针对图神经网络(GNNs)在推荐系统中的可伸缩性和鲁棒性挑战，提出了一种轻量级框架LightGNN。该框架通过基于蒸馏的GNN修剪模块，自适应移除冗余边和嵌入条目，并采用分层知识 distillation 目标来增强观察图，从而显著降低模型复杂度同时保留协作建模能力。在公共数据集上的实验显示，LightGNN实现了80%的边数减少和90%的嵌入条目减少，同时保持与最先进基线相当的推荐准确率和计算效率。开源代码可在GitHub上获取。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted to WSDM 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.03228v3",
      "published_date": "2025-01-06 18:59:55 UTC",
      "updated_date": "2025-02-04 08:34:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:46:43.233358"
    },
    {
      "arxiv_id": "2501.03226v3",
      "title": "BoostStep: Boosting mathematical capability of Large Language Models via improved single-step reasoning",
      "title_zh": "BoostStep",
      "authors": [
        "Beichen Zhang",
        "Yuhong Liu",
        "Xiaoyi Dong",
        "Yuhang Zang",
        "Pan Zhang",
        "Haodong Duan",
        "Yuhang Cao",
        "Dahua Lin",
        "Jiaqi Wang"
      ],
      "abstract": "Large language models (LLMs) have demonstrated impressive ability in solving\ncomplex mathematical problems with multi-step reasoning and can be further\nenhanced with well-designed in-context learning (ICL) examples. However, this\npotential is often constrained by two major challenges in ICL: granularity\nmismatch and irrelevant information. We observe that while LLMs excel at\ndecomposing mathematical problems, they often struggle with reasoning errors in\nfine-grained steps. Moreover, ICL examples retrieved at the question level may\nomit critical steps or even mislead the model with irrelevant details. To\naddress this issue, we propose BoostStep, a method that enhances reasoning\naccuracy through step-aligned ICL, a novel mechanism that carefully aligns\nretrieved reference steps with the corresponding reasoning steps. Additionally,\nBoostStep incorporates an effective \"first-try\" strategy to deliver exemplars\nhighly relevant to the current state of reasoning. BoostStep is a flexible and\npowerful method that integrates seamlessly with chain-of-thought (CoT) and tree\nsearch algorithms, refining both candidate selection and decision-making.\nEmpirical results show that BoostStep improves GPT-4o's CoT performance by 4.6%\nacross mathematical benchmarks, significantly surpassing traditional few-shot\nlearning's 1.2%. Moreover, it can achieve an additional 7.5\\% gain combined\nwith tree search. Surprisingly, it enhances state-of-the-art LLMs to solve\nchallenging math problems using simpler examples. It improves\nDeepSeek-R1-671B's performance on AIME by 2.2%, leveraging simple examples only\nfrom the MATH dataset.",
      "tldr_zh": "本文提出 BoostStep 方法，通过 step-aligned ICL（步骤对齐的 in-context learning）和 \"first-try\" 策略，提升 Large Language Models (LLMs) 在数学问题的单步推理能力，解决 ICL 中的 granularity mismatch（粒度不匹配）和 irrelevant information（无关信息）问题。BoostStep 与 Chain-of-Thought (CoT) 和树搜索算法无缝整合，提高候选选择和决策准确性。实验结果显示，它使 GPT-4o 的 CoT 性能提升 4.6%，比传统 few-shot learning 高出 3.4%，并在结合树搜索时额外获得 7.5% 的收益，同时使用简单示例提升 DeepSeek-R1-671B 在 AIME 上的表现 2.2%。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Codes and Data are available at\n  https://github.com/beichenzbc/BoostStep",
      "pdf_url": "http://arxiv.org/pdf/2501.03226v3",
      "published_date": "2025-01-06 18:59:13 UTC",
      "updated_date": "2025-02-17 06:27:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:46:55.970817"
    },
    {
      "arxiv_id": "2501.03225v2",
      "title": "Automated Generation of Challenging Multiple-Choice Questions for Vision Language Model Evaluation",
      "title_zh": "针对视觉语言模型评估的自动化生成挑战性多项选择题",
      "authors": [
        "Yuhui Zhang",
        "Yuchang Su",
        "Yiming Liu",
        "Xiaohan Wang",
        "James Burgess",
        "Elaine Sui",
        "Chenyu Wang",
        "Josiah Aklilu",
        "Alejandro Lozano",
        "Anjiang Wei",
        "Ludwig Schmidt",
        "Serena Yeung-Levy"
      ],
      "abstract": "The rapid development of vision language models (VLMs) demands rigorous and\nreliable evaluation. However, current visual question answering (VQA)\nbenchmarks often depend on open-ended questions, making accurate evaluation\ndifficult due to the variability in natural language responses. To address\nthis, we introduce AutoConverter, an agentic framework that automatically\nconverts these open-ended questions into multiple-choice format, enabling\nobjective evaluation while reducing the costly multiple-choice question\ncreation process. Our experiments demonstrate that AutoConverter can generate\ncorrect and challenging multiple-choice questions, with VLMs demonstrating\nconsistently similar or lower accuracy on these questions compared to\nhuman-created ones. Using AutoConverter, we construct VMCBench, a benchmark\ncreated by transforming 20 existing VQA datasets into a unified multiple-choice\nformat, totaling 9,018 questions. We comprehensively evaluate 33\nstate-of-the-art VLMs on VMCBench, setting a new standard for scalable,\nconsistent, and reproducible VLM evaluation.",
      "tldr_zh": "本文提出 AutoConverter，一种代理框架（agentic framework），用于自动将视觉问答（VQA）中的开放式问题转换为具有挑战性的多项选择题格式，从而实现对视觉语言模型（VLMs）的客观、可靠评估，并减少手动创建题目的成本。实验结果显示，AutoConverter 生成的题目准确且难度高，VLMs 在这些题目上的准确率与人类创建的题目类似或更低。作者利用该框架构建了 VMCBench 基准，将 20 个现有 VQA 数据集转换为统一的多项选择格式，共计 9,018 道题，并对 33 个最先进 VLMs 进行了全面评估，建立了一个可扩展、一致且可重复的评估标准。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.03225v2",
      "published_date": "2025-01-06 18:57:31 UTC",
      "updated_date": "2025-04-09 17:25:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:47:07.937921"
    },
    {
      "arxiv_id": "2501.03203v1",
      "title": "Detecting AI-Generated Text in Educational Content: Leveraging Machine Learning and Explainable AI for Academic Integrity",
      "title_zh": "翻译失败",
      "authors": [
        "Ayat A. Najjar",
        "Huthaifa I. Ashqar",
        "Omar A. Darwish",
        "Eman Hammad"
      ],
      "abstract": "This study seeks to enhance academic integrity by providing tools to detect\nAI-generated content in student work using advanced technologies. The findings\npromote transparency and accountability, helping educators maintain ethical\nstandards and supporting the responsible integration of AI in education. A key\ncontribution of this work is the generation of the CyberHumanAI dataset, which\nhas 1000 observations, 500 of which are written by humans and the other 500\nproduced by ChatGPT. We evaluate various machine learning (ML) and deep\nlearning (DL) algorithms on the CyberHumanAI dataset comparing human-written\nand AI-generated content from Large Language Models (LLMs) (i.e., ChatGPT).\nResults demonstrate that traditional ML algorithms, specifically XGBoost and\nRandom Forest, achieve high performance (83% and 81% accuracies respectively).\nResults also show that classifying shorter content seems to be more challenging\nthan classifying longer content. Further, using Explainable Artificial\nIntelligence (XAI) we identify discriminative features influencing the ML\nmodel's predictions, where human-written content tends to use a practical\nlanguage (e.g., use and allow). Meanwhile AI-generated text is characterized by\nmore abstract and formal terms (e.g., realm and employ). Finally, a comparative\nanalysis with GPTZero show that our narrowly focused, simple, and fine-tuned\nmodel can outperform generalized systems like GPTZero. The proposed model\nachieved approximately 77.5% accuracy compared to GPTZero's 48.5% accuracy when\ntasked to classify Pure AI, Pure Human, and mixed class. GPTZero showed a\ntendency to classify challenging and small-content cases as either mixed or\nunrecognized while our proposed model showed a more balanced performance across\nthe three classes.",
      "tldr_zh": "这篇论文旨在通过 Machine Learning (ML) 和 Explainable AI (XAI) 检测教育内容中的 AI 生成文本，以提升学术诚信、促进透明度和责任感。研究贡献包括创建 CyberHumanAI 数据集（包含 1000 条观察，其中 500 条由人类撰写，500 条由 ChatGPT 生成），并评估各种 ML 和深度学习 (DL) 算法，如 XGBoost 和 Random Forest，分别达到 83% 和 81% 的准确率。结果显示，短文本比长文本更难分类，且 XAI 分析揭示人类文本倾向于使用实用语言（如 use 和 allow），而 AI 生成文本更常用抽象正式术语（如 realm 和 employ）。此外，与 GPTZero 相比，该模型在分类 Pure AI、Pure Human 和混合类别的任务中表现出色，准确率约 77.5%，而 GPTZero 仅为 48.5%。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03203v1",
      "published_date": "2025-01-06 18:34:20 UTC",
      "updated_date": "2025-01-06 18:34:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:47:19.246548"
    },
    {
      "arxiv_id": "2501.03187v1",
      "title": "Turn-based Multi-Agent Reinforcement Learning Model Checking",
      "title_zh": "翻译失败",
      "authors": [
        "Dennis Gross"
      ],
      "abstract": "In this paper, we propose a novel approach for verifying the compliance of\nturn-based multi-agent reinforcement learning (TMARL) agents with complex\nrequirements in stochastic multiplayer games. Our method overcomes the\nlimitations of existing verification approaches, which are inadequate for\ndealing with TMARL agents and not scalable to large games with multiple agents.\nOur approach relies on tight integration of TMARL and a verification technique\nreferred to as model checking. We demonstrate the effectiveness and scalability\nof our technique through experiments in different types of environments. Our\nexperiments show that our method is suited to verify TMARL agents and scales\nbetter than naive monolithic model checking.",
      "tldr_zh": "这篇论文提出了一种新方法，用于验证轮转多智能体强化学习（TMARL）代理在随机多人游戏中是否符合复杂要求，克服了现有验证方法的局限性，如无法处理TMARL代理和在大型多代理游戏中缺乏可扩展性。方法依赖于TMARL与模型检查（model checking）的紧密整合，实现更高效的验证过程。实验在不同环境中证明，该方法适用于TMARL代理验证，并比传统的整体模型检查（monolithic model checking）具有更好的可扩展性。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03187v1",
      "published_date": "2025-01-06 18:04:20 UTC",
      "updated_date": "2025-01-06 18:04:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:47:29.982868"
    },
    {
      "arxiv_id": "2501.03172v1",
      "title": "GLiREL -- Generalist Model for Zero-Shot Relation Extraction",
      "title_zh": "翻译失败",
      "authors": [
        "Jack Boylan",
        "Chris Hokamp",
        "Demian Gholipour Ghalandari"
      ],
      "abstract": "We introduce GLiREL (Generalist Lightweight model for zero-shot Relation\nExtraction), an efficient architecture and training paradigm for zero-shot\nrelation classification. Inspired by recent advancements in zero-shot named\nentity recognition, this work presents an approach to efficiently and\naccurately predict zero-shot relationship labels between multiple entities in a\nsingle forward pass. Experiments using the FewRel and WikiZSL benchmarks\ndemonstrate that our approach achieves state-of-the-art results on the\nzero-shot relation classification task. In addition, we contribute a protocol\nfor synthetically-generating datasets with diverse relation labels.",
      "tldr_zh": "我们介绍了 GLiREL，这是一个通用轻量级模型，针对零样本关系提取（zero-shot Relation Extraction）设计，灵感来源于零样本命名实体识别的进展，能够在单次前向传递中高效预测多个实体之间的关系标签。GLiREL 通过高效架构和训练范式实现了准确的零样本关系分类，在 FewRel 和 WikiZSL 基准上取得了最先进的结果。此外，我们贡献了一个协议，用于合成生成具有多样关系标签的数据集，以支持进一步研究。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Submitted to NAACL 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.03172v1",
      "published_date": "2025-01-06 17:42:29 UTC",
      "updated_date": "2025-01-06 17:42:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:47:42.366046"
    },
    {
      "arxiv_id": "2501.03152v1",
      "title": "The Scaling Law for LoRA Base on Mutual Information Upper Bound",
      "title_zh": "基于互信息上界的 LoRA 缩放定律",
      "authors": [
        "Jing Zhang",
        "Hui Gao",
        "Peng Zhang",
        "Shuzhen Sun",
        "Chang Yang",
        "Yuexian Hou"
      ],
      "abstract": "LoRA (Low-Rank Adaptation) is a widely used model fine-tuning method. In\nfine-tuning, the law among model performance, model parameters, and data\ncomplexity has been a focal issue in the field. Existing methods often leverage\nexternal metrics (such as cross-entropy or perplexity) to evaluate model\nperformance. In the fine-tuning process for large models, two types of\nknowledge are typically involved: the frozen, general knowledge acquired by the\nmodel during pre-training and the new knowledge learned through the LoRA module\nfrom the current data. Generally, the less LoRA's learned knowledge relies on\nthe large model, the more it captures the specific knowledge of new data,\nthereby enhancing its adaptability to new tasks. However, external metrics do\nnot readily capture the dependency relationship between these two types of\nknowledge. Therefore, we designed an internal metric based on the Mutual\nInformation Upper Bound (MIUB) theory to investigate the scaling law of\nlarge-model LoRA fine-tuning. In our experiments, we validated this approach on\nbenchmark datasets, using the Llama3-8B and Phi3-3B models. The results show\nthat the proposed MIUB metric aligns more accurately and stably with the\nscaling law of LoRA fine-tuning compared to cross-entropy and perplexity.",
      "tldr_zh": "本研究探讨了 LoRA (Low-Rank Adaptation) 微调中模型性能、参数和数据复杂度之间的缩放定律，指出现有外部指标如交叉熵和困惑度无法有效捕捉预训练知识与新知识的依赖关系。作者提出了一种基于 Mutual Information Upper Bound (MIUB) 的内部指标，用于量化这种依赖关系，从而更准确地评估 LoRA 微调的适应性。在 Llama3-8B 和 Phi3-3B 模型上的基准数据集实验中，MIUB 指标显示出比传统指标更稳定和精确的性能表现。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03152v1",
      "published_date": "2025-01-06 17:19:19 UTC",
      "updated_date": "2025-01-06 17:19:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:47:55.184291"
    },
    {
      "arxiv_id": "2501.03151v1",
      "title": "Large language models for artificial general intelligence (AGI): A survey of foundational principles and approaches",
      "title_zh": "大型语言模型用于人工通用智能 (AGI)：基础原则和方法的调查",
      "authors": [
        "Alhassan Mumuni",
        "Fuseini Mumuni"
      ],
      "abstract": "Generative artificial intelligence (AI) systems based on large-scale\npretrained foundation models (PFMs) such as vision-language models, large\nlanguage models (LLMs), diffusion models and vision-language-action (VLA)\nmodels have demonstrated the ability to solve complex and truly non-trivial AI\nproblems in a wide variety of domains and contexts. Multimodal large language\nmodels (MLLMs), in particular, learn from vast and diverse data sources,\nallowing rich and nuanced representations of the world and, thereby, providing\nextensive capabilities, including the ability to reason, engage in meaningful\ndialog; collaborate with humans and other agents to jointly solve complex\nproblems; and understand social and emotional aspects of humans. Despite this\nimpressive feat, the cognitive abilities of state-of-the-art LLMs trained on\nlarge-scale datasets are still superficial and brittle. Consequently, generic\nLLMs are severely limited in their generalist capabilities. A number of\nfoundational problems -- embodiment, symbol grounding, causality and memory --\nare required to be addressed for LLMs to attain human-level general\nintelligence. These concepts are more aligned with human cognition and provide\nLLMs with inherent human-like cognitive properties that support the realization\nof physically-plausible, semantically meaningful, flexible and more\ngeneralizable knowledge and intelligence. In this work, we discuss the\naforementioned foundational issues and survey state-of-the art approaches for\nimplementing these concepts in LLMs. Specifically, we discuss how the\nprinciples of embodiment, symbol grounding, causality and memory can be\nleveraged toward the attainment of artificial general intelligence (AGI) in an\norganic manner.",
      "tldr_zh": "这篇论文调研了大型语言模型 (LLMs) 在实现人工通用智能 (AGI) 中的基础原则和方法，强调了基于预训练基础模型的生成式 AI 系统（如视觉语言模型和 Multimodal LLMs）的强大能力，包括推理、对话和协作等。论文指出，尽管 LLMs 已在多种领域表现出色，但其认知能力仍存在浅薄和脆弱的局限性，需要解决 embodiment、symbol grounding、causality 和 memory 等核心问题。最终，论文探讨了这些概念如何被整合到 LLMs 中，以构建更具人类认知特性的通用智能系统。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03151v1",
      "published_date": "2025-01-06 17:18:47 UTC",
      "updated_date": "2025-01-06 17:18:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:48:06.576114"
    },
    {
      "arxiv_id": "2501.03145v2",
      "title": "Geometry Restoration and Dewarping of Camera-Captured Document Images",
      "title_zh": "翻译失败",
      "authors": [
        "Valery Istomin",
        "Oleg Pereziabov",
        "Ilya Afanasyev"
      ],
      "abstract": "This research focuses on developing a method for restoring the topology of\ndigital images of paper documents captured by a camera, using algorithms for\ndetection, segmentation, geometry restoration, and dewarping. Our methodology\nemploys deep learning (DL) for document outline detection, followed by computer\nvision (CV) to create a topological 2D grid using cubic polynomial\ninterpolation and correct nonlinear distortions by remapping the image. Using\nclassical CV methods makes the document topology restoration process more\nefficient and faster, as it requires significantly fewer computational\nresources and memory. We developed a new pipeline for automatic document\ndewarping and reconstruction, along with a framework and annotated dataset to\ndemonstrate its efficiency. Our experiments confirm the promise of our\nmethodology and its superiority over existing benchmarks (including mobile apps\nand popular DL solutions, such as RectiNet, DocGeoNet, and DocTr++) both\nvisually and in terms of document readability via Optical Character Recognition\n(OCR) and geometry restoration metrics. This paves the way for creating\nhigh-quality digital copies of paper documents and enhancing the efficiency of\nOCR systems. Project page: https://github.com/HorizonParadox/DRCCBI",
      "tldr_zh": "这篇论文提出了一种方法，用于恢复相机捕捉的纸质文档图像的几何结构和去扭曲，通过深度学习（DL）检测文档轮廓，并结合计算机视觉（CV）技术使用立方多项式插值创建拓扑 2D 网格，以高效纠正非线性扭曲。相比传统 DL 解决方案，该方法采用经典 CV 手段，显著降低了计算资源和内存需求，并开发了新的自动文档去扭曲管道以及带注释的数据集。实验结果显示，该方法在视觉效果、Optical Character Recognition (OCR) 准确性和几何恢复指标上优于现有基准（如 RectiNet、DocGeoNet 和 DocTr++），从而提升了数字文档复制的质量和 OCR 系统效率。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "28 pages, 16 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.03145v2",
      "published_date": "2025-01-06 17:12:19 UTC",
      "updated_date": "2025-01-09 15:31:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:48:18.203937"
    },
    {
      "arxiv_id": "2501.03142v1",
      "title": "Co-Activation Graph Analysis of Safety-Verified and Explainable Deep Reinforcement Learning Policies",
      "title_zh": "安全验证和可解释深度强化学习策略的共同激活图分析",
      "authors": [
        "Dennis Gross",
        "Helge Spieker"
      ],
      "abstract": "Deep reinforcement learning (RL) policies can demonstrate unsafe behaviors\nand are challenging to interpret. To address these challenges, we combine RL\npolicy model checking--a technique for determining whether RL policies exhibit\nunsafe behaviors--with co-activation graph analysis--a method that maps neural\nnetwork inner workings by analyzing neuron activation patterns--to gain insight\ninto the safe RL policy's sequential decision-making. This combination lets us\ninterpret the RL policy's inner workings for safe decision-making. We\ndemonstrate its applicability in various experiments.",
      "tldr_zh": "该研究针对深度强化学习（Deep Reinforcement Learning, RL）政策的潜在不安全行为和解释难题，提出了一种结合RL政策模型检查和协同激活图分析（co-activation graph analysis）的框架。RL政策模型检查用于验证政策是否表现出不安全行为，而协同激活图分析则通过分析神经元激活模式来映射神经网络的内部决策过程，从而揭示安全RL政策的顺序决策机制。这种方法在各种实验中证明了其有效性，为可解释的RL政策提供了新颖洞见。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03142v1",
      "published_date": "2025-01-06 17:07:44 UTC",
      "updated_date": "2025-01-06 17:07:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:48:29.801867"
    },
    {
      "arxiv_id": "2501.03124v3",
      "title": "PRMBench: A Fine-grained and Challenging Benchmark for Process-Level Reward Models",
      "title_zh": "翻译失败",
      "authors": [
        "Mingyang Song",
        "Zhaochen Su",
        "Xiaoye Qu",
        "Jiawei Zhou",
        "Yu Cheng"
      ],
      "abstract": "Process-level Reward Models (PRMs) are crucial for complex reasoning and\ndecision-making tasks, where each intermediate step plays an important role in\nthe reasoning process. Since language models are prone to various types of\nerrors during the reasoning process, PRMs are required to possess nuanced\ncapabilities for detecting various implicit error types in real-world\nscenarios. However, current benchmarks primarily focus on step correctness,\nfailing to evaluate PRMs' performance systematically. To address this gap, we\nintroduce PRMBench, a process-level benchmark specifically designed to assess\nthe fine-grained error detection capabilities of PRMs. PRMBench comprises 6,216\ncarefully designed problems and 83,456 step-level labels, evaluating models\nacross multiple dimensions, including simplicity, soundness, and sensitivity.\nIn our experiments on 15 models, spanning both open-source PRMs and\nclosed-source large language models prompted as critic models, we uncover\nsignificant weaknesses in current PRMs. These findings underscore the\nchallenges inherent in process-level evaluation and highlight key directions\nfor future research. We hope PRMBench can be a robust bench for advancing\nresearch on PRM evaluation and development.",
      "tldr_zh": "本文引入 PRMBench，这是一个细粒度的基准，用于系统评估 Process-Level Reward Models (PRMs) 在复杂推理任务中检测隐式错误的能力，填补了现有基准仅关注步骤正确性的空白。PRMBench 包含 6,216 个精心设计的问题和 83,456 个步骤级标签，涵盖简单性、合理性和敏感性等多个维度。实验结果显示，15 个模型（包括开源 PRMs 和闭源大语言模型）存在显著弱点，突显了过程级评估的挑战，并为 PRM 的未来开发和研究提供了重要方向。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Project Page: https://prmbench.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2501.03124v3",
      "published_date": "2025-01-06 16:31:45 UTC",
      "updated_date": "2025-04-09 05:29:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:48:42.926241"
    },
    {
      "arxiv_id": "2501.03119v2",
      "title": "From Models to Network Topologies: A Topology Inference Attack in Decentralized Federated Learning",
      "title_zh": "从模型到网络拓扑：去中心化联邦学习中的拓扑推断攻击",
      "authors": [
        "Chao Feng",
        "Yuanzhe Gao",
        "Alberto Huertas Celdran",
        "Gerome Bovet",
        "Burkhard Stiller"
      ],
      "abstract": "Federated Learning (FL) is widely recognized as a privacy-preserving machine\nlearning paradigm due to its model-sharing mechanism that avoids direct data\nexchange. Nevertheless, model training leaves exploitable traces that can be\nused to infer sensitive information. In Decentralized FL (DFL), the topology,\ndefining how participants are connected, plays a crucial role in shaping the\nmodel's privacy, robustness, and convergence. However, the topology introduces\nan unexplored vulnerability: attackers can exploit it to infer participant\nrelationships and launch targeted attacks. This work uncovers the hidden risks\nof DFL topologies by proposing a novel Topology Inference Attack that infers\nthe topology solely from model behavior. A taxonomy of topology inference\nattacks is introduced, categorizing them by the attacker's capabilities and\nknowledge. Practical attack strategies are designed for various scenarios, and\nexperiments are conducted to identify key factors influencing attack success.\nThe results demonstrate that analyzing only the model of each node can\naccurately infer the DFL topology, highlighting a critical privacy risk in DFL\nsystems. These findings offer valuable insights for improving privacy\npreservation in DFL environments.",
      "tldr_zh": "这篇论文揭示了Decentralized Federated Learning (DFL)中拓扑结构带来的隐私风险，提出了一种新型Topology Inference Attack，仅通过模型行为推断参与者间的连接拓扑。论文引入了攻击分类体系，根据攻击者的能力和知识进行划分，并设计了适用于不同场景的实际攻击策略。实验结果表明，仅分析每个节点的模型即可准确推断DFL拓扑，这突显了DFL系统的关键隐私漏洞，并为改进隐私保护机制提供了重要见解。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03119v2",
      "published_date": "2025-01-06 16:27:53 UTC",
      "updated_date": "2025-05-09 08:49:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:48:54.687016"
    },
    {
      "arxiv_id": "2501.03112v1",
      "title": "LangFair: A Python Package for Assessing Bias and Fairness in Large Language Model Use Cases",
      "title_zh": "翻译失败",
      "authors": [
        "Dylan Bouchard",
        "Mohit Singh Chauhan",
        "David Skarbrevik",
        "Viren Bajaj",
        "Zeya Ahmad"
      ],
      "abstract": "Large Language Models (LLMs) have been observed to exhibit bias in numerous\nways, potentially creating or worsening outcomes for specific groups identified\nby protected attributes such as sex, race, sexual orientation, or age. To help\naddress this gap, we introduce LangFair, an open-source Python package that\naims to equip LLM practitioners with the tools to evaluate bias and fairness\nrisks relevant to their specific use cases. The package offers functionality to\neasily generate evaluation datasets, comprised of LLM responses to\nuse-case-specific prompts, and subsequently calculate applicable metrics for\nthe practitioner's use case. To guide in metric selection, LangFair offers an\nactionable decision framework.",
      "tldr_zh": "大语言模型 (LLMs) 常表现出偏见，可能对基于性别、种族等保护属性的特定群体造成负面影响。研究团队开发了开源 Python 包 LangFair，帮助 LLM 从业者评估用例中的偏见和公平性风险。该包的功能包括生成用例特定提示的评估数据集、计算适用指标，并提供一个可操作的决策框架，以指导风险评估和缓解。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Journal of Open Source Software; LangFair repository:\n  https://github.com/cvs-health/langfair",
      "pdf_url": "http://arxiv.org/pdf/2501.03112v1",
      "published_date": "2025-01-06 16:20:44 UTC",
      "updated_date": "2025-01-06 16:20:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:49:05.784681"
    },
    {
      "arxiv_id": "2501.03085v1",
      "title": "Personalized Fashion Recommendation with Image Attributes and Aesthetics Assessment",
      "title_zh": "基于图像属性和审美评估的个性化时尚推荐",
      "authors": [
        "Chongxian Chen",
        "Fan Mo",
        "Xin Fan",
        "Hayato Yamana"
      ],
      "abstract": "Personalized fashion recommendation is a difficult task because 1) the\ndecisions are highly correlated with users' aesthetic appetite, which previous\nwork frequently overlooks, and 2) many new items are constantly rolling out\nthat cause strict cold-start problems in the popular identity (ID)-based\nrecommendation methods. These new items are critical to recommend because of\ntrend-driven consumerism. In this work, we aim to provide more accurate\npersonalized fashion recommendations and solve the cold-start problem by\nconverting available information, especially images, into two attribute graphs\nfocusing on optimized image utilization and noise-reducing user modeling.\nCompared with previous methods that separate image and text as two components,\nthe proposed method combines image and text information to create a richer\nattributes graph. Capitalizing on the advancement of large language and vision\nmodels, we experiment with extracting fine-grained attributes efficiently and\nas desired using two different prompts. Preliminary experiments on the IQON3000\ndataset have shown that the proposed method achieves competitive accuracy\ncompared with baselines.",
      "tldr_zh": "本研究针对个性化时尚推荐面临的挑战，包括忽略用户审美偏好（aesthetics assessment）和新物品冷启动问题（cold-start problems），提出了一种创新方法。该方法将图像和文本信息整合成两个属性图（attribute graphs），通过优化图像利用和减少噪声的用户建模来提升推荐准确性，并利用大型语言和视觉模型（large language and vision models）通过不同提示提取细粒度属性。在 IQON3000 数据集上的初步实验显示，该方法与基线模型相比取得了竞争性的准确性表现。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03085v1",
      "published_date": "2025-01-06 15:31:10 UTC",
      "updated_date": "2025-01-06 15:31:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:49:17.800291"
    },
    {
      "arxiv_id": "2501.03301v2",
      "title": "Rethinking Byzantine Robustness in Federated Recommendation from Sparse Aggregation Perspective",
      "title_zh": "从稀疏聚合视角重新思考联邦推荐中的Byzantine鲁棒性",
      "authors": [
        "Zhongjian Zhang",
        "Mengmei Zhang",
        "Xiao Wang",
        "Lingjuan Lyu",
        "Bo Yan",
        "Junping Du",
        "Chuan Shi"
      ],
      "abstract": "To preserve user privacy in recommender systems, federated recommendation\n(FR) based on federated learning (FL) emerges, keeping the personal data on the\nlocal client and updating a model collaboratively. Unlike FL, FR has a unique\nsparse aggregation mechanism, where the embedding of each item is updated by\nonly partial clients, instead of full clients in a dense aggregation of general\nFL. Recently, as an essential principle of FL, model security has received\nincreasing attention, especially for Byzantine attacks, where malicious clients\ncan send arbitrary updates. The problem of exploring the Byzantine robustness\nof FR is particularly critical since in the domains applying FR, e.g.,\ne-commerce, malicious clients can be injected easily by registering new\naccounts. However, existing Byzantine works neglect the unique sparse\naggregation of FR, making them unsuitable for our problem. Thus, we make the\nfirst effort to investigate Byzantine attacks on FR from the perspective of\nsparse aggregation, which is non-trivial: it is not clear how to define\nByzantine robustness under sparse aggregations and design Byzantine attacks\nunder limited knowledge/capability. In this paper, we reformulate the Byzantine\nrobustness under sparse aggregation by defining the aggregation for a single\nitem as the smallest execution unit. Then we propose a family of effective\nattack strategies, named Spattack, which exploit the vulnerability in sparse\naggregation and are categorized along the adversary's knowledge and capability.\nExtensive experimental results demonstrate that Spattack can effectively\nprevent convergence and even break down defenses under a few malicious clients,\nraising alarms for securing FR systems.",
      "tldr_zh": "本文从稀疏聚合（sparse aggregation）视角重新审视联邦推荐（Federated Recommendation, FR）中的拜占庭鲁棒性（Byzantine robustness），强调 FR 的独特机制——每个物品的嵌入仅由部分客户端更新——使其易受恶意攻击。作者重新定义了在稀疏聚合下的鲁棒性，将单个物品的聚合作为最小执行单位，并提出 Spattack 攻击策略家族，根据攻击者的知识和能力分类，以利用这一漏洞。实验结果显示，Spattack 在少数恶意客户端下即可有效阻止模型收敛并突破现有防御，警告了 FR 系统在电商等领域的安全风险。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.DC",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "accepted by AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.03301v2",
      "published_date": "2025-01-06 15:19:26 UTC",
      "updated_date": "2025-01-08 11:47:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:49:30.910874"
    },
    {
      "arxiv_id": "2501.03059v1",
      "title": "Through-The-Mask: Mask-based Motion Trajectories for Image-to-Video Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Guy Yariv",
        "Yuval Kirstain",
        "Amit Zohar",
        "Shelly Sheynin",
        "Yaniv Taigman",
        "Yossi Adi",
        "Sagie Benaim",
        "Adam Polyak"
      ],
      "abstract": "We consider the task of Image-to-Video (I2V) generation, which involves\ntransforming static images into realistic video sequences based on a textual\ndescription. While recent advancements produce photorealistic outputs, they\nfrequently struggle to create videos with accurate and consistent object\nmotion, especially in multi-object scenarios. To address these limitations, we\npropose a two-stage compositional framework that decomposes I2V generation\ninto: (i) An explicit intermediate representation generation stage, followed by\n(ii) A video generation stage that is conditioned on this representation. Our\nkey innovation is the introduction of a mask-based motion trajectory as an\nintermediate representation, that captures both semantic object information and\nmotion, enabling an expressive but compact representation of motion and\nsemantics. To incorporate the learned representation in the second stage, we\nutilize object-level attention objectives. Specifically, we consider a spatial,\nper-object, masked-cross attention objective, integrating object-specific\nprompts into corresponding latent space regions and a masked spatio-temporal\nself-attention objective, ensuring frame-to-frame consistency for each object.\nWe evaluate our method on challenging benchmarks with multi-object and\nhigh-motion scenarios and empirically demonstrate that the proposed method\nachieves state-of-the-art results in temporal coherence, motion realism, and\ntext-prompt faithfulness. Additionally, we introduce \\benchmark, a new\nchallenging benchmark for single-object and multi-object I2V generation, and\ndemonstrate our method's superiority on this benchmark. Project page is\navailable at https://guyyariv.github.io/TTM/.",
      "tldr_zh": "本文提出一种名为 Through-The-Mask 的两阶段框架，用于解决 Image-to-Video (I2V) 生成任务中物体运动不准确的问题，特别是多物体场景。关键创新是引入 mask-based motion trajectory 作为中间表示，该表示同时捕捉物体的语义信息和运动轨迹，提供紧凑且表达性强的表示。框架在第二阶段利用 object-level attention，包括空间的 masked-cross attention 和 masked spatio-temporal self-attention，确保物体间的帧到帧一致性。实验结果显示，该方法在多物体和高运动基准上，实现了最先进的性能，在时间连贯性、运动真实性和文本提示忠实度方面均有显著提升；此外，作者还引入了一个新的基准 \\benchmark，以验证方法的优越性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03059v1",
      "published_date": "2025-01-06 14:49:26 UTC",
      "updated_date": "2025-01-06 14:49:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:49:43.531019"
    },
    {
      "arxiv_id": "2501.03058v1",
      "title": "Survival Analysis Revisited: Understanding and Unifying Poisson, Exponential, and Cox Models in Fall Risk Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Tianhua Chen"
      ],
      "abstract": "This paper explores foundational and applied aspects of survival analysis,\nusing fall risk assessment as a case study. It revisits key time-related\nprobability distributions and statistical methods, including logistic\nregression, Poisson regression, Exponential regression, and the Cox\nProportional Hazards model, offering a unified perspective on their\nrelationships within the survival analysis framework. A contribution of this\nwork is the step-by-step derivation and clarification of the relationships\namong these models, particularly demonstrating that Poisson regression in the\nsurvival context is a specific case of the Cox model. These insights address\ngaps in understanding and reinforce the simplicity and interpretability of\nsurvival models. The paper also emphasizes the practical utility of survival\nanalysis by connecting theoretical insights with real-world applications. In\nthe context of fall detection, it demonstrates how these models can\nsimultaneously predict fall risk, analyze contributing factors, and estimate\ntime-to-event outcomes within a single streamlined framework. In contrast,\nadvanced deep learning methods often require complex post-hoc interpretation\nand separate training for different tasks particularly when working with\nstructured numerical data. This highlights the enduring relevance of classical\nstatistical frameworks and makes survival models especially valuable in\nhealthcare settings, where explainability and robustness are critical. By\nunifying foundational concepts and offering a cohesive perspective on\ntime-to-event analysis, this work serves as an accessible resource for\nunderstanding survival models and applying them effectively to diverse\nanalytical challenges.",
      "tldr_zh": "这篇论文重新审视生存分析框架，以跌倒风险评估为例，统一了 logistic regression、Poisson regression、Exponential regression 和 Cox Proportional Hazards model 之间的关系，并证明 Poisson regression 在生存分析中是 Cox 模型的特例。作者通过逐步推导澄清了这些模型的相互联系，强调了其简单性和可解释性，同时展示了它们在预测跌倒风险、分析影响因素和估计事件发生时间方面的实际应用。相比深度学习方法，生存模型在医疗领域更具优势，因为它们无需复杂的后处理解释，且能在单一框架中处理多任务，提供更可靠的解释性资源。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03058v1",
      "published_date": "2025-01-06 14:48:30 UTC",
      "updated_date": "2025-01-06 14:48:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:49:55.525815"
    },
    {
      "arxiv_id": "2501.03055v2",
      "title": "To Analyze and Regulate Human-in-the-loop Learning for Congestion Games",
      "title_zh": "翻译失败",
      "authors": [
        "Hongbo Li",
        "Lingjie Duan"
      ],
      "abstract": "In congestion games, selfish users behave myopically to crowd to the shortest\npaths, and the social planner designs mechanisms to regulate such selfish\nrouting through information or payment incentives. However, such mechanism\ndesign requires the knowledge of time-varying traffic conditions and it is the\nusers themselves to learn and report past road experiences to the social\nplanner (e.g., Waze or Google Maps). When congestion games meet mobile\ncrowdsourcing, it is critical to incentivize selfish users to explore\nnon-shortest paths in the best exploitation-exploration trade-off. First, we\nconsider a simple but fundamental parallel routing network with one\ndeterministic path and multiple stochastic paths for users with an average\narrival probability $\\lambda$. We prove that the current myopic routing policy\n(widely used in Waze and Google Maps) misses both exploration (when strong\nhazard belief) and exploitation (when weak hazard belief) as compared to the\nsocial optimum. Due to the myopic policy's under-exploration, we prove that the\ncaused price of anarchy (PoA) is larger than\n\\(\\frac{1}{1-\\rho^{\\frac{1}{\\lambda}}}\\), which can be arbitrarily large as\ndiscount factor \\(\\rho\\rightarrow1\\). To mitigate such huge efficiency loss, we\npropose a novel selective information disclosure (SID) mechanism: we only\nreveal the latest traffic information to users when they intend to over-explore\nstochastic paths upon arrival, while hiding such information when they want to\nunder-explore. We prove that our mechanism successfully reduces PoA to be less\nthan~\\(2\\). Besides the parallel routing network, we further extend our\nmechanism and PoA results to any linear path graphs with multiple intermediate\nnodes.",
      "tldr_zh": "这篇论文分析了拥堵游戏(congestion games)中自私用户的短视路由(myopic routing policy)，证明其在 exploitation-exploration trade-off 中存在不足，导致 price of anarchy (PoA) 可能无限大，因为用户在强 hazard belief 时缺少探索，在弱时缺少利用。作者提出了一种 selective information disclosure (SID) 机制，仅在用户意图过度探索随机路径时揭示最新交通信息，从而激励更优的路由决策。理论证明显示，该机制成功将 PoA 降低到小于 2，并扩展到任何线性路径图，为社会规划者调节用户行为提供了有效工具。",
      "categories": [
        "cs.GT",
        "cs.AI"
      ],
      "primary_category": "cs.GT",
      "comment": "arXiv admin note: substantial text overlap with arXiv:2211.14029",
      "pdf_url": "http://arxiv.org/pdf/2501.03055v2",
      "published_date": "2025-01-06 14:41:45 UTC",
      "updated_date": "2025-01-14 07:46:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:50:07.838726"
    },
    {
      "arxiv_id": "2501.03045v1",
      "title": "Single-Channel Distance-Based Source Separation for Mobile GPU in Outdoor and Indoor Environments",
      "title_zh": "翻译失败",
      "authors": [
        "Hanbin Bae",
        "Byungjun Kang",
        "Jiwon Kim",
        "Jaeyong Hwang",
        "Hosang Sung",
        "Hoon-Young Cho"
      ],
      "abstract": "This study emphasizes the significance of exploring distance-based source\nseparation (DSS) in outdoor environments. Unlike existing studies that\nprimarily focus on indoor settings, the proposed model is designed to capture\nthe unique characteristics of outdoor audio sources. It incorporates advanced\ntechniques, including a two-stage conformer block, a linear relation-aware\nself-attention (RSA), and a TensorFlow Lite GPU delegate. While the linear RSA\nmay not capture physical cues as explicitly as the quadratic RSA, the linear\nRSA enhances the model's context awareness, leading to improved performance on\nthe DSS that requires an understanding of physical cues in outdoor and indoor\nenvironments. The experimental results demonstrated that the proposed model\novercomes the limitations of existing approaches and considerably enhances\nenergy efficiency and real-time inference speed on mobile devices.",
      "tldr_zh": "这篇论文强调了在户外环境中探索基于距离的源分离（DSS）的必要性，提出了一种新模型来捕捉户外音频源的独特特性，同时适用于室内环境。模型整合了两阶段 Conformer 块、线性关系感知自注意力（linear RSA）和 TensorFlow Lite GPU delegate，其中 linear RSA 提升了模型的上下文感知能力，尽管不如二次 RSA 直接捕捉物理线索。实验结果表明，该模型克服了现有方法的局限性，在户外和室内场景中显著提高了源分离性能，并提升了移动设备的能量效率和实时推理速度。",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted by ICASSP2025. \\c{opyright} 2025 IEEE. Personal use of this\n  material is permitted. Permission from IEEE must be obtained for all other\n  uses, in any current or future media, including reprinting/republishing this\n  material for advertising or promotional purposes, creating new collective\n  works, for resale or redistribution to servers or lists, or reuse of any\n  copyrighted component",
      "pdf_url": "http://arxiv.org/pdf/2501.03045v1",
      "published_date": "2025-01-06 14:32:24 UTC",
      "updated_date": "2025-01-06 14:32:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:50:18.830898"
    },
    {
      "arxiv_id": "2501.03038v2",
      "title": "Piano Transcription by Hierarchical Language Modeling with Pretrained Roll-based Encoders",
      "title_zh": "翻译失败",
      "authors": [
        "Dichucheng Li",
        "Yongyi Zang",
        "Qiuqiang Kong"
      ],
      "abstract": "Automatic Music Transcription (AMT), aiming to get musical notes from raw\naudio, typically uses frame-level systems with piano-roll outputs or language\nmodel (LM)-based systems with note-level predictions. However, frame-level\nsystems require manual thresholding, while the LM-based systems struggle with\nlong sequences. In this paper, we propose a hybrid method combining pre-trained\nroll-based encoders with an LM decoder to leverage the strengths of both\nmethods. Besides, our approach employs a hierarchical prediction strategy,\nfirst predicting onset and pitch, then velocity, and finally offset. The\nhierarchical prediction strategy reduces computational costs by breaking down\nlong sequences into different hierarchies. Evaluated on two benchmark\nroll-based encoders, our method outperforms traditional piano-roll outputs 0.01\nand 0.022 in onset-offset-velocity F1 score, demonstrating its potential as a\nperformance-enhancing plug-in for arbitrary roll-based music transcription\nencoder.",
      "tldr_zh": "本文提出了一种混合方法，用于 Automatic Music Transcription (AMT)，通过结合预训练的 roll-based encoders 和 language model (LM) decoder，克服了帧级系统需要手动阈值设置以及 LM 系统处理长序列的难题。方法采用分层预测策略，先预测 onset 和 pitch，然后 velocity，最后 offset，以降低计算成本并提高准确性。在基准测试中，该方法在 onset-offset-velocity F1 score 上比传统 piano-roll 输出分别提高了 0.01 和 0.022，展示了其作为 roll-based 音乐转录编码器性能增强插件的潜力。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted by ICASSP 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.03038v2",
      "published_date": "2025-01-06 14:26:00 UTC",
      "updated_date": "2025-01-07 15:13:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:50:30.527796"
    },
    {
      "arxiv_id": "2501.03035v4",
      "title": "Quantization Meets Reasoning: Exploring LLM Low-Bit Quantization Degradation for Mathematical Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Zhen Li",
        "Yupeng Su",
        "Runming Yang",
        "Congkai Xie",
        "Zheng Wang",
        "Zhongwei Xie",
        "Ngai Wong",
        "Hongxia Yang"
      ],
      "abstract": "Large language models have achieved significant advancements in complex\nmathematical reasoning benchmarks, such as MATH. However, their substantial\ncomputational requirements present challenges for practical deployment. Model\nquantization has emerged as an effective strategy to reduce memory usage and\ncomputational costs by employing lower precision and bit-width representations.\nIn this study, we systematically evaluate the impact of quantization on\nmathematical reasoning tasks. Our results demonstrate that aggressive\nquantization methods like AWQ and GPTQ introduce up to 32.39% accuracy\ndegradation (average 11.31%) on Llama-3 models, particularly in numerical\ncomputation and reasoning planning. To address this, we introduce a\nmultidimensional evaluation framework combining qualitative capability analysis\nand quantitative error assessment. We further develop targeted recovery\nstrategies, showing that fine-tuning quantized models on only 545 task-specific\nexamples for 3 minutes on 4 GPUs effectively restores reasoning capabilities to\nnear full-precision levels. Additionally, our error assessment pipeline\nachieves 98.9% accuracy in diagnosing and localizing errors across 3,366\nfailure cases, providing actionable insights for mitigating\nquantization-induced degradation.",
      "tldr_zh": "这篇论文探讨了LLM低位量化(quantization)对数学推理任务的影响，发现使用AWQ和GPTQ等方法会导致Llama-3模型准确率平均下降11.31%（最高达32.39%），尤其在数值计算和推理规划上。研究者引入了一个多维评估框架，结合定性能力分析和定量错误评估，以系统评估量化带来的性能退化。针对此问题，他们开发了针对性恢复策略，通过在4个GPU上fine-tuning量化模型仅用545个任务特定示例和3分钟时间，即可将推理能力恢复到接近全精度水平；此外，错误评估管道在3,366个失败案例中实现了98.9%的诊断准确率，提供可操作的见解来缓解量化诱发的退化。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03035v4",
      "published_date": "2025-01-06 14:23:02 UTC",
      "updated_date": "2025-02-24 14:34:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:50:43.242004"
    },
    {
      "arxiv_id": "2501.03026v1",
      "title": "Putnam's Critical and Explanatory Tendencies Interpreted from a Machine Learning Perspective",
      "title_zh": "从机器学习视角解读Putnam的批判与解释倾向",
      "authors": [
        "Sheldon Z. Soudin"
      ],
      "abstract": "Making sense of theory choice in normal and across extraordinary science is\ncentral to philosophy of science. The emergence of machine learning models has\nthe potential to act as a wrench in the gears of current debates. In this\npaper, I will attempt to reconstruct the main movements that lead to and came\nout of Putnam's critical and explanatory tendency distinction, argue for the\nbiconditional necessity of the tendencies, and conceptualize that wrench\nthrough a machine learning interpretation of my claim.",
      "tldr_zh": "这篇论文从机器学习视角重新解读 Putnam 的 critical and explanatory tendencies 在科学哲学中的作用，探讨了这些倾向在正常和非常规科学理论选择中的重要性。作者重建了这些倾向的演变过程，并论证了它们之间的 biconditional necessity，即互为条件的必要性。最终，通过机器学习模型的解释，该论文为当前科学哲学辩论注入新颖视角，提供了一个潜在的框架来分析理论选择问题。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "9 pages",
      "pdf_url": "http://arxiv.org/pdf/2501.03026v1",
      "published_date": "2025-01-06 14:09:35 UTC",
      "updated_date": "2025-01-06 14:09:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:50:54.263566"
    },
    {
      "arxiv_id": "2501.03012v1",
      "title": "Analyzing Fine-tuning Representation Shift for Multimodal LLMs Steering alignment",
      "title_zh": "翻译失败",
      "authors": [
        "Pegah Khayatan",
        "Mustafa Shukor",
        "Jayneel Parekh",
        "Matthieu Cord"
      ],
      "abstract": "Multimodal LLMs have reached remarkable levels of proficiency in\nunderstanding multimodal inputs, driving extensive research to develop\nincreasingly powerful models. However, much less attention has been paid to\nunderstanding and explaining the underlying mechanisms of these models. Most\nexisting explainability research examines these models only in their final\nstates, overlooking the dynamic representational shifts that occur during\ntraining. In this work, we systematically analyze the evolution of hidden state\nrepresentations to reveal how fine-tuning alters the internal structure of a\nmodel to specialize in new multimodal tasks. Using a concept-based approach, we\nmap hidden states to interpretable visual and textual concepts, enabling us to\ntrace changes in encoded concepts across modalities as training progresses. We\nalso demonstrate the use of shift vectors to capture these concepts changes.\nThese shift vectors allow us to recover fine-tuned concepts by shifting those\nin the original model. Finally, we explore the practical impact of our findings\non model steering, showing that we can adjust multimodal LLMs behaviors without\nany training, such as modifying answer types, captions style, or biasing the\nmodel toward specific responses. Our work sheds light on how multimodal\nrepresentations evolve through fine-tuning and offers a new perspective for\ninterpreting model adaptation in multimodal tasks. The code for this project is\npublicly available at https://github.com/mshukor/xl-vlms.",
      "tldr_zh": "该研究分析了多模态LLMs在微调过程中的表示变化（representation shift），以揭示模型如何适应新多模态任务。研究采用基于概念的方法，将隐藏状态（hidden states）映射到可解释的视觉和文本概念，并使用shift vectors捕捉训练中的概念变化，从而恢复微调后的表示。结果显示，通过shift vectors可以在不进行额外训练的情况下引导模型行为，例如调整答案类型、标题风格或偏向特定响应。该工作为理解多模态LLMs的适应机制提供了新视角，并公开了相关代码。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "The first three authors contributed equally",
      "pdf_url": "http://arxiv.org/pdf/2501.03012v1",
      "published_date": "2025-01-06 13:37:13 UTC",
      "updated_date": "2025-01-06 13:37:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:51:05.662727"
    },
    {
      "arxiv_id": "2501.03008v1",
      "title": "Quality Estimation based Feedback Training for Improving Pronoun Translation",
      "title_zh": "翻译失败",
      "authors": [
        "Harshit Dhankhar",
        "Baban Gain",
        "Asif Ekbal",
        "Yogesh Mani Tripathi"
      ],
      "abstract": "Pronoun translation is a longstanding challenge in neural machine translation\n(NMT), often requiring inter-sentential context to ensure linguistic accuracy.\nTo address this, we introduce ProNMT, a novel framework designed to enhance\npronoun and overall translation quality in context-aware machine translation\nsystems. ProNMT leverages Quality Estimation (QE) models and a unique Pronoun\nGeneration Likelihood-Based Feedback mechanism to iteratively fine-tune\npre-trained NMT models without relying on extensive human annotations. The\nframework combines QE scores with pronoun-specific rewards to guide training,\nensuring improved handling of linguistic nuances. Extensive experiments\ndemonstrate significant gains in pronoun translation accuracy and general\ntranslation quality across multiple metrics. ProNMT offers an efficient,\nscalable, and context-aware approach to improving NMT systems, particularly in\ntranslating context-dependent elements like pronouns.",
      "tldr_zh": "这篇论文提出 ProNMT 框架，用于提升神经机器翻译 (NMT) 中的代词翻译质量，特别针对需要跨句上下文的语言挑战。框架结合 Quality Estimation (QE) 模型和基于代词生成似然的反馈机制，通过迭代微调预训练 NMT 模型来指导训练，而无需大量人工标注。实验结果显示，ProNMT 在多个指标上显著提高了代词翻译准确性和整体翻译性能，提供了一个高效、可扩展的上下文感知解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03008v1",
      "published_date": "2025-01-06 13:34:51 UTC",
      "updated_date": "2025-01-06 13:34:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:51:17.795617"
    },
    {
      "arxiv_id": "2501.02997v1",
      "title": "CALM: Curiosity-Driven Auditing for Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Xiang Zheng",
        "Longxiang Wang",
        "Yi Liu",
        "Xingjun Ma",
        "Chao Shen",
        "Cong Wang"
      ],
      "abstract": "Auditing Large Language Models (LLMs) is a crucial and challenging task. In\nthis study, we focus on auditing black-box LLMs without access to their\nparameters, only to the provided service. We treat this type of auditing as a\nblack-box optimization problem where the goal is to automatically uncover\ninput-output pairs of the target LLMs that exhibit illegal, immoral, or unsafe\nbehaviors. For instance, we may seek a non-toxic input that the target LLM\nresponds to with a toxic output or an input that induces the hallucinative\nresponse from the target LLM containing politically sensitive individuals. This\nblack-box optimization is challenging due to the scarcity of feasible points,\nthe discrete nature of the prompt space, and the large search space. To address\nthese challenges, we propose Curiosity-Driven Auditing for Large Language\nModels (CALM), which uses intrinsically motivated reinforcement learning to\nfinetune an LLM as the auditor agent to uncover potential harmful and biased\ninput-output pairs of the target LLM. CALM successfully identifies derogatory\ncompletions involving celebrities and uncovers inputs that elicit specific\nnames under the black-box setting. This work offers a promising direction for\nauditing black-box LLMs. Our code is available at\nhttps://github.com/x-zheng16/CALM.git.",
      "tldr_zh": "该研究针对黑盒 Large Language Models (LLMs) 的审计问题提出了一种 Curiosity-Driven Auditing for Large Language Models (CALM) 方法，该方法将审计视为黑盒优化问题，旨在自动发现目标 LLMs 的输入-输出对，这些对可能涉及非法、不道德或不安全行为，如产生有毒响应或幻觉。CALM 通过基于内在动机的 reinforcement learning 微调一个 LLM 作为审计代理，克服了提示空间离散性和搜索空间大的挑战。实验结果显示，该框架成功识别了涉及名人的贬低完成和特定名称的输入，展示了其在审计黑盒 LLMs 方面的潜力，并提供了开源代码以推动进一步研究。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by AAAI 2025 AI Alignment Track",
      "pdf_url": "http://arxiv.org/pdf/2501.02997v1",
      "published_date": "2025-01-06 13:14:34 UTC",
      "updated_date": "2025-01-06 13:14:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:51:30.609990"
    },
    {
      "arxiv_id": "2501.02992v2",
      "title": "GLFC: Unified Global-Local Feature and Contrast Learning with Mamba-Enhanced UNet for Synthetic CT Generation from CBCT",
      "title_zh": "翻译失败",
      "authors": [
        "Xianhao Zhou",
        "Jianghao Wu",
        "Huangxuan Zhao",
        "Lei Chen",
        "Shaoting Zhang",
        "Guotai Wang"
      ],
      "abstract": "Generating synthetic Computed Tomography (CT) images from Cone Beam Computed\nTomography (CBCT) is desirable for improving the image quality of CBCT.\nExisting synthetic CT (sCT) generation methods using Convolutional Neural\nNetworks (CNN) and Transformers often face difficulties in effectively\ncapturing both global and local features and contrasts for high-quality sCT\ngeneration. In this work, we propose a Global-Local Feature and Contrast\nlearning (GLFC) framework for sCT generation. First, a Mamba-Enhanced UNet\n(MEUNet) is introduced by integrating Mamba blocks into the skip connections of\na high-resolution UNet for effective global and local feature learning. Second,\nwe propose a Multiple Contrast Loss (MCL) that calculates synthetic loss at\ndifferent intensity windows to improve quality for both soft tissues and bone\nregions. Experiments on the SynthRAD2023 dataset demonstrate that GLFC improved\nthe SSIM of sCT from 77.91% to 91.50% compared with the original CBCT, and\nsignificantly outperformed several existing methods for sCT generation. The\ncode is available at https://github.com/HiLab-git/GLFC",
      "tldr_zh": "本研究提出GLFC框架，用于从Cone Beam Computed Tomography (CBCT)生成高质量的合成Computed Tomography (CT)图像，以解决现有CNN和Transformer方法在捕获全局和局部特征及对比度方面的不足。首先，引入Mamba-Enhanced UNet (MEUNet)，通过将Mamba blocks整合到高分辨率UNet的跳跃连接中，实现对全局和局部特征的有效学习。其次，提出Multiple Contrast Loss (MCL)，在不同强度窗口计算合成损失，以提升软组织和骨骼区域的图像质量。在SynthRAD2023数据集的实验中，GLFC将sCT的SSIM从77.91%提高到91.50%，并显著优于现有方法。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "Accepted by ISBI2025",
      "pdf_url": "http://arxiv.org/pdf/2501.02992v2",
      "published_date": "2025-01-06 13:11:47 UTC",
      "updated_date": "2025-01-11 14:46:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:51:42.817272"
    },
    {
      "arxiv_id": "2501.02982v2",
      "title": "A Bio-Inspired Research Paradigm of Collision Perception Neurons Enabling Neuro-Robotic Integration: The LGMD Case",
      "title_zh": "翻译失败",
      "authors": [
        "Ziyan Qin",
        "Jigen Peng",
        "Shigang Yue",
        "Qinbing Fu"
      ],
      "abstract": "Compared to human vision, locust visual systems excel at rapid and precise\ncollision detection, despite relying on only hundreds of thousands of neurons\norganized through a few neuropils. This efficiency makes them an attractive\nmodel system for developing artificial collision-detecting systems.\nSpecifically, researchers have identified collision-selective neurons in the\nlocust's optic lobe, called lobula giant movement detectors (LGMDs), which\nrespond specifically to approaching objects. Research upon LGMD neurons began\nin the early 1970s. Initially, due to their large size, these neurons were\nidentified as motion detectors, but their role as looming detectors was\nrecognized over time. Since then, progress in neuroscience, computational\nmodeling of LGMD's visual neural circuits, and LGMD-based robotics have\nadvanced in tandem, each field supporting and driving the others. Today, with a\ndeeper understanding of LGMD neurons, LGMD-based models have significantly\nimproved collision-free navigation in mobile robots including ground and aerial\nrobots. This review highlights recent developments in LGMD research from the\nperspectives of neuroscience, computational modeling, and robotics. It\nemphasizes a biologically plausible research paradigm, where insights from\nneuroscience inform real-world applications, which would in turn validate and\nadvance neuroscience. With strong support from extensive research and growing\napplication demand, this paradigm has reached a mature stage and demonstrates\nversatility across different areas of neuroscience research, thereby enhancing\nour understanding of the interconnections between neuroscience, computational\nmodeling, and robotics. Furthermore, this paradigm would shed light upon the\nmodeling and robotic research into other motion-sensitive neurons or neural\ncircuits.",
      "tldr_zh": "这篇论文回顾了蝗虫视觉系统中的 LGMD（lobula giant movement detectors）神经元，这些神经元尽管仅依赖少量神经元，就能实现快速精确的碰撞检测，并以此作为生物启发模型。论文强调了一个生物启发的神经-机器人整合研究范式，将神经科学、计算建模和机器人应用相结合，相互驱动和验证。LGMD 模型已在地面和空中机器人中显著提高了碰撞避免导航性能，并展示了这种范式在扩展到其他运动敏感神经元或神经回路研究中的多功能性和成熟性。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "q-bio.NC"
      ],
      "primary_category": "cs.NE",
      "comment": "35 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.02982v2",
      "published_date": "2025-01-06 12:44:48 UTC",
      "updated_date": "2025-05-14 06:57:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:51:54.296513"
    },
    {
      "arxiv_id": "2501.02981v2",
      "title": "CONTINUUM: Detecting APT Attacks through Spatial-Temporal Graph Neural Networks",
      "title_zh": "CONTINUUM：通过时空图神经网络检测 APT 攻击",
      "authors": [
        "Atmane Ayoub Mansour Bahar",
        "Kamel Soaid Ferrahi",
        "Mohamed-Lamine Messai",
        "Hamida Seba",
        "Karima Amrouche"
      ],
      "abstract": "Advanced Persistent Threats (APTs) represent a significant challenge in\ncybersecurity due to their sophisticated and stealthy nature. Traditional\nIntrusion Detection Systems (IDS) often fall short in detecting these\nmulti-stage attacks. Recently, Graph Neural Networks (GNNs) have been employed\nto enhance IDS capabilities by analyzing the complex relationships within\nnetworked data. However, existing GNN-based solutions are hampered by high\nfalse positive rates and substantial resource consumption. In this paper, we\npresent a novel IDS designed to detect APTs using a Spatio-Temporal Graph\nNeural Network Autoencoder. Our approach leverages spatial information to\nunderstand the interactions between entities within a graph and temporal\ninformation to capture the evolution of the graph over time. This dual\nperspective is crucial for identifying the sequential stages of APTs.\nFurthermore, to address privacy and scalability concerns, we deploy our\narchitecture in a federated learning environment. This setup ensures that local\ndata remains on-premise while encrypted model-weights are shared and aggregated\nusing homomorphic encryption, maintaining data privacy and security. Our\nevaluation shows that this system effectively detects APTs with lower false\npositive rates and optimized resource usage compared to existing methods,\nhighlighting the potential of spatio-temporal analysis and federated learning\nin enhancing cybersecurity defenses.",
      "tldr_zh": "这篇论文针对高级持续性威胁(APTs)的检测挑战，提出了一种名为CONTINUUM的新型入侵检测系统(IDS)，利用Spatio-Temporal Graph Neural Network Autoencoder结合空间信息（实体间交互）和时间信息（图的演变），来识别APTs的多阶段攻击。该系统通过联邦学习(federated learning)环境部署，确保本地数据隐私，并使用同态加密(homomorphic encryption)共享和聚合加密模型权重，以解决高假阳性率和资源消耗问题。实验评估表明，CONTINUUM相较于现有Graph Neural Networks (GNNs)方法，显著降低了假阳性率并优化了资源使用，提升了网络安全防御的整体效能。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.NI"
      ],
      "primary_category": "cs.CR",
      "comment": "31 pages",
      "pdf_url": "http://arxiv.org/pdf/2501.02981v2",
      "published_date": "2025-01-06 12:43:59 UTC",
      "updated_date": "2025-01-07 08:39:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:52:07.069883"
    },
    {
      "arxiv_id": "2501.02977v2",
      "title": "CAMP: Collaborative Attention Model with Profiles for Vehicle Routing Problems",
      "title_zh": "翻译失败",
      "authors": [
        "Chuanbo Hua",
        "Federico Berto",
        "Jiwoo Son",
        "Seunghyun Kang",
        "Changhyun Kwon",
        "Jinkyoo Park"
      ],
      "abstract": "The profiled vehicle routing problem (PVRP) is a generalization of the\nheterogeneous capacitated vehicle routing problem (HCVRP) in which the\nobjective is to optimize the routes of vehicles to serve client demands subject\nto different vehicle profiles, with each having a preference or constraint on a\nper-client basis. While existing learning methods have shown promise for\nsolving the HCVRP in real-time, no learning method exists to solve the more\npractical and challenging PVRP. In this paper, we propose a Collaborative\nAttention Model with Profiles (CAMP), a novel approach that learns efficient\nsolvers for PVRP using multi-agent reinforcement learning. CAMP employs a\nspecialized attention-based encoder architecture to embed profiled client\nembeddings in parallel for each vehicle profile. We design a communication\nlayer between agents for collaborative decision-making across profiled\nembeddings at each decoding step and a batched pointer mechanism to attend to\nthe profiled embeddings to evaluate the likelihood of the next actions. We\nevaluate CAMP on two variants of PVRPs: PVRP with preferences, which explicitly\ninfluence the reward function, and PVRP with zone constraints with different\nnumbers of agents and clients, demonstrating that our learned solvers achieve\ncompetitive results compared to both classical state-of-the-art neural\nmulti-agent models in terms of solution quality and computational efficiency.\nWe make our code openly available at https://github.com/ai4co/camp.",
      "tldr_zh": "本论文针对 profiled vehicle routing problem (PVRP)——一种扩展了 heterogeneous capacitated vehicle routing problem (HCVRP) 的问题——提出了一种 Collaborative Attention Model with Profiles (CAMP) 方法，利用多智能体强化学习来优化车辆路由以满足每个客户的偏好或约束。CAMP 采用 attention-based encoder 来并行嵌入 profiled client embeddings，并通过代理间通信层和 batched pointer mechanism 实现协作决策和动作评估。实验结果显示，该方法在 PVRP with preferences 和 PVRP with zone constraints 的变体上，相比传统神经多智能体模型，在解决方案质量和计算效率方面表现出色。代码已开源，以推动相关研究。",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "Accepted at AAMAS 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.02977v2",
      "published_date": "2025-01-06 12:37:56 UTC",
      "updated_date": "2025-02-04 09:21:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:52:19.170651"
    },
    {
      "arxiv_id": "2501.02975v1",
      "title": "Fuzzy Granule Density-Based Outlier Detection with Multi-Scale Granular Balls",
      "title_zh": "翻译失败",
      "authors": [
        "Can Gao",
        "Xiaofeng Tan",
        "Jie Zhou",
        "Weiping Ding",
        "Witold Pedrycz"
      ],
      "abstract": "Outlier detection refers to the identification of anomalous samples that\ndeviate significantly from the distribution of normal data and has been\nextensively studied and used in a variety of practical tasks. However, most\nunsupervised outlier detection methods are carefully designed to detect\nspecified outliers, while real-world data may be entangled with different types\nof outliers. In this study, we propose a fuzzy rough sets-based multi-scale\noutlier detection method to identify various types of outliers. Specifically, a\nnovel fuzzy rough sets-based method that integrates relative fuzzy granule\ndensity is first introduced to improve the capability of detecting local\noutliers. Then, a multi-scale view generation method based on granular-ball\ncomputing is proposed to collaboratively identify group outliers at different\nlevels of granularity. Moreover, reliable outliers and inliers determined by\nthe three-way decision are used to train a weighted support vector machine to\nfurther improve the performance of outlier detection. The proposed method\ninnovatively transforms unsupervised outlier detection into a semi-supervised\nclassification problem and for the first time explores the fuzzy rough\nsets-based outlier detection from the perspective of multi-scale granular\nballs, allowing for high adaptability to different types of outliers. Extensive\nexperiments carried out on both artificial and UCI datasets demonstrate that\nthe proposed outlier detection method significantly outperforms the\nstate-of-the-art methods, improving the results by at least 8.48% in terms of\nthe Area Under the ROC Curve (AUROC) index. { The source codes are released at\n\\url{https://github.com/Xiaofeng-Tan/MGBOD}. }",
      "tldr_zh": "这篇论文提出了一种基于模糊粗糙集（fuzzy rough sets）的多尺度异常检测方法，利用多尺度颗粒球（multi-scale granular balls）来识别各种类型的异常，包括局部异常和组异常。具体而言，该方法整合相对模糊颗粒密度（relative fuzzy granule density）来提升局部异常检测能力，并通过颗粒球计算（granular-ball computing）生成多尺度视图，同时采用三向决策（three-way decision）训练加权支持向量机（weighted support vector machine），将无监督检测转化为半监督分类问题。创新性地从多尺度颗粒球视角探索模糊粗糙集在异常检测中的应用，使其更适应不同异常类型。在人工和 UCI 数据集上的实验表明，该方法在 AUROC 指标上比最先进方法至少提高了 8.48%。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02975v1",
      "published_date": "2025-01-06 12:35:51 UTC",
      "updated_date": "2025-01-06 12:35:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:52:31.189961"
    },
    {
      "arxiv_id": "2501.02971v1",
      "title": "Proof-of-Data: A Consensus Protocol for Collaborative Intelligence",
      "title_zh": "Proof-of-Data：一种用于协作智能的共识协议",
      "authors": [
        "Huiwen Liu",
        "Feida Zhu",
        "Ling Cheng"
      ],
      "abstract": "Existing research on federated learning has been focused on the setting where\nlearning is coordinated by a centralized entity. Yet the greatest potential of\nfuture collaborative intelligence would be unleashed in a more open and\ndemocratized setting with no central entity in a dominant role, referred to as\n\"decentralized federated learning\". New challenges arise accordingly in\nachieving both correct model training and fair reward allocation with\ncollective effort among all participating nodes, especially with the threat of\nthe Byzantine node jeopardising both tasks.\n  In this paper, we propose a blockchain-based decentralized Byzantine\nfault-tolerant federated learning framework based on a novel Proof-of-Data\n(PoD) consensus protocol to resolve both the \"trust\" and \"incentive\"\ncomponents. By decoupling model training and contribution accounting, PoD is\nable to enjoy not only the benefit of learning efficiency and system liveliness\nfrom asynchronous societal-scale PoW-style learning but also the finality of\nconsensus and reward allocation from epoch-based BFT-style voting. To mitigate\nfalse reward claims by data forgery from Byzantine attacks, a privacy-aware\ndata verification and contribution-based reward allocation mechanism is\ndesigned to complete the framework. Our evaluation results show that PoD\ndemonstrates performance in model training close to that of the centralized\ncounterpart while achieving trust in consensus and fairness for reward\nallocation with a fault tolerance ratio of 1/3.",
      "tldr_zh": "该论文针对去中心化联邦学习(federated learning)中的信任和激励挑战，提出了一种基于区块链的拜占庭容错框架，使用新型Proof-of-Data (PoD)共识协议。该协议通过解耦模型训练和贡献会计，结合异步PoW风格的学习效率与BFT风格的共识最终性，并设计隐私感知的数据验证机制来防范Byzantine攻击，确保公平奖励分配。实验结果表明，PoD的模型训练性能接近中心化联邦学习，同时实现了1/3的容错率，在共识信任和奖励公平性上表现出色。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02971v1",
      "published_date": "2025-01-06 12:27:59 UTC",
      "updated_date": "2025-01-06 12:27:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:52:44.789984"
    },
    {
      "arxiv_id": "2501.02964v2",
      "title": "Socratic Questioning: Learn to Self-guide Multimodal Reasoning in the Wild",
      "title_zh": "Socratic Questioning：学会自我引导多模态推理于野外",
      "authors": [
        "Wanpeng Hu",
        "Haodi Liu",
        "Lin Chen",
        "Feng Zhou",
        "Changming Xiao",
        "Qi Yang",
        "Changshui Zhang"
      ],
      "abstract": "Complex visual reasoning remains a key challenge today. Typically, the\nchallenge is tackled using methodologies such as Chain of Thought (COT) and\nvisual instruction tuning. However, how to organically combine these two\nmethodologies for greater success remains unexplored. Also, issues like\nhallucinations and high training cost still need to be addressed. In this work,\nwe devise an innovative multi-round training and reasoning framework suitable\nfor lightweight Multimodal Large Language Models (MLLMs). Our self-questioning\napproach heuristically guides MLLMs to focus on visual clues relevant to the\ntarget problem, reducing hallucinations and enhancing the model's ability to\ndescribe fine-grained image details. This ultimately enables the model to\nperform well in complex visual reasoning and question-answering tasks. We have\nnamed this framework Socratic Questioning(SQ). To facilitate future research,\nwe create a multimodal mini-dataset named CapQA, which includes 1k images of\nfine-grained activities, for visual instruction tuning and evaluation, our\nproposed SQ method leads to a 31.2% improvement in the hallucination score. Our\nextensive experiments on various benchmarks demonstrate SQ's remarkable\ncapabilities in heuristic self-questioning, zero-shot visual reasoning and\nhallucination mitigation. Our model and code will be publicly available.",
      "tldr_zh": "该研究提出了一种名为Socratic Questioning (SQ) 的创新框架，用于指导轻量级Multimodal Large Language Models (MLLMs) 在复杂视觉推理任务中进行多轮训练和推理。SQ 通过启发式自我提问机制，帮助模型聚焦于相关视觉线索，减少hallucinations 并提升对细粒度图像细节的描述，从而改善Chain of Thought (COT) 与视觉指令微调的有机结合。研究者创建了CapQA 多模态小数据集（包含1k 图像），用于评估和训练；实验结果显示，SQ 方法在hallucination 分数上提升31.2%，并在各种基准上表现出色，包括零样本视觉推理和幻觉缓解。模型和代码将公开可用，为未来研究提供基础。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02964v2",
      "published_date": "2025-01-06 12:16:56 UTC",
      "updated_date": "2025-01-07 02:55:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:52:55.082742"
    },
    {
      "arxiv_id": "2501.02950v2",
      "title": "Key-value memory in the brain",
      "title_zh": "大脑中的键值记忆",
      "authors": [
        "Samuel J. Gershman",
        "Ila Fiete",
        "Kazuki Irie"
      ],
      "abstract": "Classical models of memory in psychology and neuroscience rely on\nsimilarity-based retrieval of stored patterns, where similarity is a function\nof retrieval cues and the stored patterns. While parsimonious, these models do\nnot allow distinct representations for storage and retrieval, despite their\ndistinct computational demands. Key-value memory systems, in contrast,\ndistinguish representations used for storage (values) and those used for\nretrieval (keys). This allows key-value memory systems to optimize\nsimultaneously for fidelity in storage and discriminability in retrieval. We\nreview the computational foundations of key-value memory, its role in modern\nmachine learning systems, related ideas from psychology and neuroscience,\napplications to a number of empirical puzzles, and possible biological\nimplementations.",
      "tldr_zh": "本论文审视了古典记忆模型在心理学和神经科学中的局限性，这些模型依赖于基于相似性的检索，而未区分存储表示和检索表示。Key-value memory 系统则通过将存储表示（values）和检索表示（keys）分开，实现了存储的保真度和检索的可区分性的同时优化。论文回顾了 Key-value memory 的计算基础、在机器学习中的应用、与相关心理学和神经科学概念的联系、对经验谜题的潜在应用，以及可能的生物实现方式。",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.NC",
      "comment": "Accepted to Neuron",
      "pdf_url": "http://arxiv.org/pdf/2501.02950v2",
      "published_date": "2025-01-06 11:46:40 UTC",
      "updated_date": "2025-03-04 03:10:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:53:05.769768"
    },
    {
      "arxiv_id": "2501.03295v2",
      "title": "A Soft Sensor Method with Uncertainty-Awareness and Self-Explanation Based on Large Language Models Enhanced by Domain Knowledge Retrieval",
      "title_zh": "翻译失败",
      "authors": [
        "Shuo Tong",
        "Han Liu",
        "Runyuan Guo",
        "Wenqing Wang",
        "Xueqiong Tian",
        "Lingyun Wei",
        "Lin Zhang",
        "Huayong Wu",
        "Ding Liu",
        "Youmin Zhang"
      ],
      "abstract": "Data-driven soft sensors are crucial in predicting key performance indicators\nin industrial systems. However, current methods predominantly rely on the\nsupervised learning paradigms of parameter updating, which inherently faces\nchallenges such as high development costs, poor robustness, training\ninstability, and lack of interpretability. Recently, large language models\n(LLMs) have demonstrated significant potential across various domains, notably\nthrough In-Context Learning (ICL), which enables high-performance task\nexecution with minimal input-label demonstrations and no prior training. This\npaper aims to replace supervised learning with the emerging ICL paradigm for\nsoft sensor modeling to address existing challenges and explore new avenues for\nadvancement. To achieve this, we propose a novel framework called the Few-shot\nUncertainty-aware and self-Explaining Soft Sensor (LLM-FUESS), which includes\nthe Zero-shot Auxiliary Variable Selector (LLM-ZAVS) and the Uncertainty-aware\nFew-shot Soft Sensor (LLM-UFSS). The LLM-ZAVS retrieves from the Industrial\nKnowledge Vector Storage to enhance LLMs' domain-specific knowledge, enabling\nzero-shot auxiliary variable selection. In the LLM-UFSS, we utilize text-based\ncontext demonstrations of structured data to prompt LLMs to execute ICL for\npredicting and propose a context sample retrieval augmentation strategy to\nimprove performance. Additionally, we explored LLMs' AIGC and probabilistic\ncharacteristics to propose self-explanation and uncertainty quantification\nmethods for constructing a trustworthy soft sensor. Extensive experiments\ndemonstrate that our method achieved state-of-the-art predictive performance,\nstrong robustness, and flexibility, effectively mitigates training instability\nfound in traditional methods. To the best of our knowledge, this is the first\nwork to establish soft sensor utilizing LLMs.",
      "tldr_zh": "本研究提出了一种基于大语言模型 (LLMs) 的软传感器方法，旨在解决传统监督学习在工业系统预测中的高成本、鲁棒性差和缺乏可解释性等问题，通过 In-Context Learning (ICL) 范式进行建模。框架名为 Few-shot Uncertainty-aware and self-Explaining Soft Sensor (LLM-FUESS)，包括 Zero-shot Auxiliary Variable Selector (LLM-ZAVS) 用于从 Industrial Knowledge Vector Storage 检索领域知识实现零样本变量选择，以及 Uncertainty-aware Few-shot Soft Sensor (LLM-UFSS) 通过文本上下文演示和样本检索增强策略进行预测，同时整合 LLMs 的 AIGC 和概率特性实现自解释和不确定性量化。实验结果显示，该方法在预测性能、鲁棒性和灵活性上达到 state-of-the-art 水平，并有效缓解训练不稳定性；这是首次利用 LLMs 构建软传感器的创新工作。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03295v2",
      "published_date": "2025-01-06 11:43:29 UTC",
      "updated_date": "2025-01-08 04:50:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:53:19.249042"
    },
    {
      "arxiv_id": "2501.02922v1",
      "title": "Label-free Concept Based Multiple Instance Learning for Gigapixel Histopathology",
      "title_zh": "翻译失败",
      "authors": [
        "Susu Sun",
        "Leslie Tessier",
        "Frédérique Meeuwsen",
        "Clément Grisi",
        "Dominique van Midden",
        "Geert Litjens",
        "Christian F. Baumgartner"
      ],
      "abstract": "Multiple Instance Learning (MIL) methods allow for gigapixel Whole-Slide\nImage (WSI) analysis with only slide-level annotations. Interpretability is\ncrucial for safely deploying such algorithms in high-stakes medical domains.\nTraditional MIL methods offer explanations by highlighting salient regions.\nHowever, such spatial heatmaps provide limited insights for end users. To\naddress this, we propose a novel inherently interpretable WSI-classification\napproach that uses human-understandable pathology concepts to generate\nexplanations. Our proposed Concept MIL model leverages recent advances in\nvision-language models to directly predict pathology concepts based on image\nfeatures. The model's predictions are obtained through a linear combination of\nthe concepts identified on the top-K patches of a WSI, enabling inherent\nexplanations by tracing each concept's influence on the prediction. In contrast\nto traditional concept-based interpretable models, our approach eliminates the\nneed for costly human annotations by leveraging the vision-language model. We\nvalidate our method on two widely used pathology datasets: Camelyon16 and\nPANDA. On both datasets, Concept MIL achieves AUC and accuracy scores over 0.9,\nputting it on par with state-of-the-art models. We further find that 87.1\\%\n(Camelyon16) and 85.3\\% (PANDA) of the top 20 patches fall within the tumor\nregion. A user study shows that the concepts identified by our model align with\nthe concepts used by pathologists, making it a promising strategy for\nhuman-interpretable WSI classification.",
      "tldr_zh": "本论文提出了一种无需标签的 Concept MIL 模型，用于 Gigapixel Histopathology 中的 Whole-Slide Image (WSI) 分类，该方法通过人类可理解的病理概念生成固有可解释的预测，解决了传统 Multiple Instance Learning (MIL) 方法的空间热图解释不足的问题。模型利用 vision-language 模型直接基于图像特征预测病理概念，并通过线性组合 WSI 的 top-K 补丁上的概念进行预测，从而无需昂贵的人工标注。实验在 Camelyon16 和 PANDA 数据集上显示，Concept MIL 的 AUC 和准确率均超过 0.9，与最先进模型相当，且用户研究证实模型识别的概念与病理学家使用的概念高度一致，为可解释的医疗图像分析提供了新策略。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02922v1",
      "published_date": "2025-01-06 11:03:04 UTC",
      "updated_date": "2025-01-06 11:03:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:53:30.708221"
    },
    {
      "arxiv_id": "2501.02921v2",
      "title": "Unsupervised Tomato Split Anomaly Detection using Hyperspectral Imaging and Variational Autoencoders",
      "title_zh": "翻译失败",
      "authors": [
        "Mahmoud Abdulsalam",
        "Usman Zahidi",
        "Bradley Hurst",
        "Simon Pearson",
        "Grzegorz Cielniak",
        "James Brown"
      ],
      "abstract": "Tomato anomalies/damages pose a significant challenge in greenhouse farming.\nWhile this method of cultivation benefits from efficient resource utilization,\nanomalies can significantly degrade the quality of farm produce. A common\nanomaly associated with tomatoes is splitting, characterized by the development\nof cracks on the tomato skin, which degrades its quality. Detecting this type\nof anomaly is challenging due to dynamic variations in appearance and sizes,\ncompounded by dataset scarcity. We address this problem in an unsupervised\nmanner by utilizing a tailored variational autoencoder (VAE) with hyperspectral\ninput. Preliminary analysis of the dataset enabled us to select the optimal\nrange of wavelengths for detecting this anomaly. Our findings indicate that the\n530nm - 550nm range is suitable for identifying tomato dry splits. The proposed\nVAE model achieved a 97% detection accuracy for tomato split anomalies in the\ntest data. The analysis on reconstruction loss allow us to not only detect the\nanomalies but also to some degree estimate the anomalous regions.",
      "tldr_zh": "本研究针对温室番茄裂开异常检测的挑战，提出了一种无监督方法，利用高光谱成像(Hyperspectral Imaging)和变分自编码器(VAE)。通过初步数据集分析，他们确定了530nm-550nm的波长范围最适合识别番茄干裂。结果显示，该VAE模型在测试数据中实现了97%的检测准确率，并能通过重建损失估算异常区域的程度。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPPA Workshop",
      "pdf_url": "http://arxiv.org/pdf/2501.02921v2",
      "published_date": "2025-01-06 11:02:52 UTC",
      "updated_date": "2025-04-28 11:24:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:53:41.995127"
    },
    {
      "arxiv_id": "2501.02905v1",
      "title": "Skillful High-Resolution Ensemble Precipitation Forecasting with an Integrated Deep Learning Framework",
      "title_zh": "翻译失败",
      "authors": [
        "Shuangshuang He",
        "Hongli Liang",
        "Yuanting Zhang",
        "Xingyuan Yuan"
      ],
      "abstract": "High-resolution precipitation forecasts are crucial for providing accurate\nweather prediction and supporting effective responses to extreme weather\nevents. Traditional numerical models struggle with stochastic subgrid-scale\nprocesses, while recent deep learning models often produce blurry results. To\naddress these challenges, we propose a physics-inspired deep learning framework\nfor high-resolution (0.05\\textdegree{} $\\times$ 0.05\\textdegree{}) ensemble\nprecipitation forecasting. Trained on ERA5 and CMPA high-resolution\nprecipitation datasets, the framework integrates deterministic and\nprobabilistic components. The deterministic model, based on a 3D\nSwinTransformer, captures average precipitation at mesoscale resolution and\nincorporates strategies to enhance performance, particularly for moderate to\nheavy rainfall. The probabilistic model employs conditional diffusion in latent\nspace to account for uncertainties in residual precipitation at convective\nscales. During inference, ensemble members are generated by repeatedly sampling\nlatent variables, enabling the model to represent precipitation uncertainty.\nOur model significantly enhances spatial resolution and forecast accuracy. Rank\nhistogram shows that the ensemble system is reliable and unbiased. In a case\nstudy of heavy precipitation in southern China, the model outputs align more\nclosely with observed precipitation distributions than ERA5, demonstrating\nsuperior capability in capturing extreme precipitation events. Additionally,\n5-day real-time forecasts show good performance in terms of CSI scores.",
      "tldr_zh": "本文提出一个受物理启发的深度学习框架，用于高分辨率（0.05° × 0.05°）集合降水预报，旨在解决传统数值模型处理随机子网格过程的难题和深度学习模型输出模糊的问题。框架结合基于3D SwinTransformer的确定性模型（捕捉中尺度平均降水）和条件扩散的概率模型（处理对流尺度残差不确定性），并通过重复采样潜在变量生成集合成员。实验结果显示，该模型显著提升空间分辨率和预报准确性，在中国南方重度降水案例中输出更接近观测值，且5天实时预报的CSI scores表现出色，优于ERA5基准。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02905v1",
      "published_date": "2025-01-06 10:29:38 UTC",
      "updated_date": "2025-01-06 10:29:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:53:55.145978"
    },
    {
      "arxiv_id": "2501.02891v2",
      "title": "Explaining Humour Style Classifications: An XAI Approach to Understanding Computational Humour Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Mary Ogbuka Kenneth",
        "Foaad Khosmood",
        "Abbas Edalat"
      ],
      "abstract": "Humour styles can have either a negative or a positive impact on well-being.\nGiven the importance of these styles to mental health, significant research has\nbeen conducted on their automatic identification. However, the automated\nmachine learning models used for this purpose are black boxes, making their\nprediction decisions opaque. Clarity and transparency are vital in the field of\nmental health. This paper presents an explainable AI (XAI) framework for\nunderstanding humour style classification, building upon previous work in\ncomputational humour analysis. Using the best-performing single model\n(ALI+XGBoost) from prior research, we apply comprehensive XAI techniques to\nanalyse how linguistic, emotional, and semantic features contribute to humour\nstyle classification decisions. Our analysis reveals distinct patterns in how\ndifferent humour styles are characterised and misclassified, with particular\nemphasis on the challenges in distinguishing affiliative humour from other\nstyles. Through detailed examination of feature importance, error patterns, and\nmisclassification cases, we identify key factors influencing model decisions,\nincluding emotional ambiguity, context misinterpretation, and target\nidentification. The framework demonstrates significant utility in understanding\nmodel behaviour, achieving interpretable insights into the complex interplay of\nfeatures that define different humour styles. Our findings contribute to both\nthe theoretical understanding of computational humour analysis and practical\napplications in mental health, content moderation, and digital humanities\nresearch.",
      "tldr_zh": "这篇论文提出一个可解释 AI (XAI) 框架，用于分析幽默风格分类模型的决策过程，解决现有黑盒模型在心理健康应用中的不透明问题。基于先前 ALI+XGBoost 模型，他们应用 XAI 技术考察语言、情感和语义特征的重要性，揭示不同幽默风格的特征模式，特别是区分 affiliative humour 的挑战，如情感模糊、上下文误解和目标识别错误。研究通过分析特征重要性、错误模式和误分类案例，提供可解释的洞见，提升对计算幽默分析的理论理解，并支持实际应用如心理健康、内容审查和数字人文研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02891v2",
      "published_date": "2025-01-06 10:08:56 UTC",
      "updated_date": "2025-02-28 17:57:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:54:06.413639"
    },
    {
      "arxiv_id": "2501.02869v1",
      "title": "IIMedGPT: Promoting Large Language Model Capabilities of Medical Tasks by Efficient Human Preference Alignment",
      "title_zh": "IIMedGPT：通过",
      "authors": [
        "Yiming Zhang",
        "Zheng Chang",
        "Wentao Cai",
        "MengXing Ren",
        "Kang Yuan",
        "Yining Sun",
        "Zenghui Ding"
      ],
      "abstract": "Recent researches of large language models(LLM), which is pre-trained on\nmassive general-purpose corpora, have achieved breakthroughs in responding\nhuman queries. However, these methods face challenges including limited data\ninsufficiency to support extensive pre-training and can not align responses\nwith users' instructions. To address these issues, we introduce a medical\ninstruction dataset, CMedINS, containing six medical instructions derived from\nactual medical tasks, which effectively fine-tunes LLM in conjunction with\nother data. Subsequently, We launch our medical model, IIMedGPT, employing an\nefficient preference alignment method, Direct preference Optimization(DPO). The\nresults show that our final model outperforms existing medical models in\nmedical dialogue.Datsets, Code and model checkpoints will be released upon\nacceptance.",
      "tldr_zh": "该研究针对大型语言模型（LLM）在医疗任务中面临的 数据不足和指令对齐问题，引入了一个名为 CMedINS 的医疗指令数据集，该数据集包含从实际医疗任务中衍生的六个指令，用于有效微调 LLM。研究团队开发了 IIMedGPT 模型，通过高效的 Direct Preference Optimization (DPO) 方法进行人类偏好对齐，提升了模型在医疗对话中的性能。结果显示，IIMedGPT 优于现有医疗模型，并在数据、代码和模型检查点方面计划于接受后发布。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02869v1",
      "published_date": "2025-01-06 09:22:36 UTC",
      "updated_date": "2025-01-06 09:22:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:54:17.971192"
    },
    {
      "arxiv_id": "2501.02840v1",
      "title": "Enhanced Rooftop Solar Panel Detection by Efficiently Aggregating Local Features",
      "title_zh": "通过高效聚合局部特征增强屋顶太阳能面板检测",
      "authors": [
        "Kuldeep Kurte",
        "Kedar Kulkarni"
      ],
      "abstract": "In this paper, we present an enhanced Convolutional Neural Network\n(CNN)-based rooftop solar photovoltaic (PV) panel detection approach using\nsatellite images. We propose to use pre-trained CNN-based model to extract the\nlocal convolutional features of rooftops. These local features are then\ncombined using the Vectors of Locally Aggregated Descriptors (VLAD) technique\nto obtain rooftop-level global features, which are then used to train\ntraditional Machine Learning (ML) models to identify rooftop images that do and\ndo not contain PV panels. On the dataset used in this study, the proposed\napproach achieved rooftop-PV classification scores exceeding the predefined\nthreshold of 0.9 across all three cities for each of the feature extractor\nnetworks evaluated. Moreover, we propose a 3-phase approach to enable efficient\nutilization of the previously trained models on a new city or region with\nlimited labelled data. We illustrate the effectiveness of this 3-phase approach\nfor multi-city rooftop-PV detection task.",
      "tldr_zh": "本文提出了一种基于卷积神经网络 (CNN) 的屋顶太阳能光伏 (PV) 面板检测方法，使用卫星图像提取局部卷积特征，并通过 Vectors of Locally Aggregated Descriptors (VLAD) 技术聚合为全局特征，以训练传统机器学习 (ML) 模型进行分类。实验结果显示，该方法在三个城市的数据集上均超过了 0.9 的分类阈值。作者还引入了 3 阶段方法，用于高效地将预训练模型应用于新城市或区域，即使标记数据有限，从而提升多城市屋顶 PV 检测任务的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at CODS-COMAD 2024, December, 2024, Jodhpur, India\n  (https://cods-comad.in/accepted-papers.php)",
      "pdf_url": "http://arxiv.org/pdf/2501.02840v1",
      "published_date": "2025-01-06 08:36:44 UTC",
      "updated_date": "2025-01-06 08:36:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:55:38.840254"
    },
    {
      "arxiv_id": "2501.03292v1",
      "title": "Multi-Modal One-Shot Federated Ensemble Learning for Medical Data with Vision Large Language Model",
      "title_zh": "翻译失败",
      "authors": [
        "Naibo Wang",
        "Yuchen Deng",
        "Shichen Fan",
        "Jianwei Yin",
        "See-Kiong Ng"
      ],
      "abstract": "Federated learning (FL) has attracted considerable interest in the medical\ndomain due to its capacity to facilitate collaborative model training while\nmaintaining data privacy. However, conventional FL methods typically\nnecessitate multiple communication rounds, leading to significant communication\noverhead and delays, especially in environments with limited bandwidth.\nOne-shot federated learning addresses these issues by conducting model training\nand aggregation in a single communication round, thereby reducing communication\ncosts while preserving privacy. Among these, one-shot federated ensemble\nlearning combines independently trained client models using ensemble techniques\nsuch as voting, further boosting performance in non-IID data scenarios. On the\nother hand, existing machine learning methods in healthcare predominantly use\nunimodal data (e.g., medical images or textual reports), which restricts their\ndiagnostic accuracy and comprehensiveness. Therefore, the integration of\nmulti-modal data is proposed to address these shortcomings. In this paper, we\nintroduce FedMME, an innovative one-shot multi-modal federated ensemble\nlearning framework that utilizes multi-modal data for medical image analysis.\nSpecifically, FedMME capitalizes on vision large language models to produce\ntextual reports from medical images, employs a BERT model to extract textual\nfeatures from these reports, and amalgamates these features with visual\nfeatures to improve diagnostic accuracy. Experimental results show that our\nmethod demonstrated superior performance compared to existing one-shot\nfederated learning methods in healthcare scenarios across four datasets with\nvarious data distributions. For instance, it surpasses existing one-shot\nfederated learning approaches by more than 17.5% in accuracy on the RSNA\ndataset when applying a Dirichlet distribution with ($\\alpha$ = 0.3).",
      "tldr_zh": "本文提出 FedMME，一种创新的 One-Shot Federated Ensemble Learning 框架，用于处理医疗领域的多模态数据，旨在减少通信开销并提升非独立同分布 (non-IID) 数据的诊断性能。该框架利用 Vision Large Language Model 生成文本报告，结合 BERT 模型提取文本特征，并与视觉特征融合，提高整体准确性。实验结果显示，FedMME 在四个数据集上超越现有 One-Shot Federated Learning 方法，例如在 RSNA 数据集上准确率提升超过17.5%。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03292v1",
      "published_date": "2025-01-06 08:36:28 UTC",
      "updated_date": "2025-01-06 08:36:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:55:50.302729"
    },
    {
      "arxiv_id": "2501.05471v1",
      "title": "Found in Translation: semantic approaches for enhancing AI interpretability in face verification",
      "title_zh": "翻译失败",
      "authors": [
        "Miriam Doh",
        "Caroline Mazini Rodrigues",
        "N. Boutry",
        "L. Najman",
        "Matei Mancas",
        "Bernard Gosselin"
      ],
      "abstract": "The increasing complexity of machine learning models in computer vision,\nparticularly in face verification, requires the development of explainable\nartificial intelligence (XAI) to enhance interpretability and transparency.\nThis study extends previous work by integrating semantic concepts derived from\nhuman cognitive processes into XAI frameworks to bridge the comprehension gap\nbetween model outputs and human understanding. We propose a novel approach\ncombining global and local explanations, using semantic features defined by\nuser-selected facial landmarks to generate similarity maps and textual\nexplanations via large language models (LLMs). The methodology was validated\nthrough quantitative experiments and user feedback, demonstrating improved\ninterpretability. Results indicate that our semantic-based approach,\nparticularly the most detailed set, offers a more nuanced understanding of\nmodel decisions than traditional methods. User studies highlight a preference\nfor our semantic explanations over traditional pixelbased heatmaps, emphasizing\nthe benefits of human-centric interpretability in AI. This work contributes to\nthe ongoing efforts to create XAI frameworks that align AI models behaviour\nwith human cognitive processes, fostering trust and acceptance in critical\napplications.",
      "tldr_zh": "这篇论文探讨了在人脸验证领域通过语义方法提升 AI 可解释性（XAI），旨在桥接模型输出与人类认知的理解差距。研究提出一种新框架，将从人类认知过程衍生的语义概念整合到 XAI 中，结合全局和本地解释，使用用户选择的面部 landmarks 生成相似性地图，并通过大型语言模型（LLMs）提供文本解释。实验和用户反馈显示，该语义-based 方法，尤其是最详细的版本，比传统像素-based 热图提供更细致的模型决策理解，用户更倾向于这种人类中心化的解释。该工作有助于构建与人类认知对齐的 XAI 框架，促进 AI 在关键应用中的信任和接受。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.05471v1",
      "published_date": "2025-01-06 08:34:53 UTC",
      "updated_date": "2025-01-06 08:34:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:56:02.690692"
    },
    {
      "arxiv_id": "2501.02837v1",
      "title": "Forward Once for All: Structural Parameterized Adaptation for Efficient Cloud-coordinated On-device Recommendation",
      "title_zh": "翻译失败",
      "authors": [
        "Kairui Fu",
        "Zheqi Lv",
        "Shengyu Zhang",
        "Fan Wu",
        "Kun Kuang"
      ],
      "abstract": "In cloud-centric recommender system, regular data exchanges between user\ndevices and cloud could potentially elevate bandwidth demands and privacy\nrisks. On-device recommendation emerges as a viable solution by performing\nreranking locally to alleviate these concerns. Existing methods primarily focus\non developing local adaptive parameters, while potentially neglecting the\ncritical role of tailor-made model architecture. Insights from broader research\ndomains suggest that varying data distributions might favor distinct\narchitectures for better fitting. In addition, imposing a uniform model\nstructure across heterogeneous devices may result in risking inefficacy on less\ncapable devices or sub-optimal performance on those with sufficient\ncapabilities. In response to these gaps, our paper introduces Forward-OFA, a\nnovel approach for the dynamic construction of device-specific networks (both\nstructure and parameters). Forward-OFA employs a structure controller to\nselectively determine whether each block needs to be assembled for a given\ndevice. However, during the training of the structure controller, these\nassembled heterogeneous structures are jointly optimized, where the co-adaption\namong blocks might encounter gradient conflicts. To mitigate this, Forward-OFA\nis designed to establish a structure-guided mapping of real-time behaviors to\nthe parameters of assembled networks. Structure-related parameters and parallel\ncomponents within the mapper prevent each part from receiving heterogeneous\ngradients from others, thus bypassing the gradient conflicts for coupled\noptimization. Besides, direct mapping enables Forward-OFA to achieve adaptation\nthrough only one forward pass, allowing for swift adaptation to changing\ninterests and eliminating the requirement for on-device backpropagation.\nExperiments on real-world datasets demonstrate the effectiveness and efficiency\nof Forward-OFA.",
      "tldr_zh": "该论文提出 Forward-OFA，一种结构参数化适应方法，用于高效的云协调设备端推荐系统，以缓解传统云端推荐的带宽需求和隐私风险问题。该方法通过 structure controller 动态构建设备特定网络（包括结构和参数），并采用结构引导的实时行为映射，避免训练中的梯度冲突，实现高效的联合优化。Forward-OFA 仅需一次 forward pass 即可适应用户兴趣变化，无需设备端反向传播；实验在实世界数据集上证明了其有效性和效率。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.DC",
      "comment": "Accepted by KDD 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.02837v1",
      "published_date": "2025-01-06 08:32:16 UTC",
      "updated_date": "2025-01-06 08:32:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:56:14.116633"
    },
    {
      "arxiv_id": "2502.15701v1",
      "title": "Political Events using RAG with LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Muhammad Arslan",
        "Saba Munawar",
        "Christophe Cruz"
      ],
      "abstract": "In the contemporary digital landscape, media content stands as the foundation\nfor political news analysis, offering invaluable insights sourced from various\nchannels like news articles, social media updates, speeches, and reports.\nNatural Language Processing (NLP) has revolutionized Political Information\nExtraction (IE), automating tasks such as Event Extraction (EE) from these\ndiverse media outlets. While traditional NLP methods often necessitate\nspecialized expertise to build rule-based systems or train machine learning\nmodels with domain-specific datasets, the emergence of Large Language Models\n(LLMs) driven by Generative Artificial Intelligence (GenAI) presents a\npromising alternative. These models offer accessibility, alleviating challenges\nassociated with model construction from scratch and reducing the dependency on\nextensive datasets during the training phase, thus facilitating rapid\nimplementation. However, challenges persist in handling domain-specific tasks,\nleading to the development of the Retrieval-Augmented Generation (RAG)\nframework. RAG enhances LLMs by integrating external data retrieval, enriching\ntheir contextual understanding, and expanding their knowledge base beyond\npre-existing training data. To illustrate RAG's efficacy, we introduce the\nPolitical EE system, specifically tailored to extract political event\ninformation from news articles. Understanding these political insights is\nessential for remaining informed about the latest political advancements,\nwhether on a national or global scale.",
      "tldr_zh": "本研究探讨了使用 Retrieval-Augmented Generation (RAG) 框架增强 Large Language Models (LLMs) 来提取政治事件信息的问题，旨在解决传统 Natural Language Processing (NLP) 方法的复杂性和数据依赖性。RAG 通过整合外部数据检索，扩展 LLMs 的知识基和上下文理解，使其更适合领域特定任务，如从新闻文章中自动提取政治事件。论文引入了一个 Political Event Extraction (EE) 系统，展示了 RAG 的有效性，帮助用户快速获取国家或全球政治动态。实验结果表明，这种方法简化了实现过程，并提高了事件提取的效率和准确性。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15701v1",
      "published_date": "2025-01-06 08:16:24 UTC",
      "updated_date": "2025-01-06 08:16:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:56:26.445138"
    },
    {
      "arxiv_id": "2501.02832v3",
      "title": "Samba-ASR: State-Of-The-Art Speech Recognition Leveraging Structured State-Space Models",
      "title_zh": "翻译失败",
      "authors": [
        "Syed Abdul Gaffar Shakhadri",
        "Kruthika KR",
        "Kartik Basavaraj Angadi"
      ],
      "abstract": "We propose Samba ASR,the first state of the art Automatic Speech\nRecognition(ASR)model leveraging the novel Mamba architecture as both encoder\nand decoder,built on the foundation of state space models(SSMs).Unlike\ntransformerbased ASR models,which rely on self-attention mechanisms to capture\ndependencies,Samba ASR effectively models both local and global temporal\ndependencies using efficient statespace dynamics,achieving remarkable\nperformance gains.By addressing the limitations of transformers,such as\nquadratic scaling with input length and difficulty in handling longrange\ndependencies,Samba ASR achieves superior accuracy and efficiency.Experimental\nresults demonstrate that Samba ASR surpasses existing opensource\ntransformerbased ASR models across various standard benchmarks,establishing it\nas the new state of theart in ASR.Extensive evaluations on the benchmark\ndataset show significant improvements in Word Error Rate(WER),with competitive\nperformance even in lowresource scenarios.Furthermore,the inherent\ncomputational efficiency and parameter optimization of the Mamba architecture\nmake Samba ASR a scalable and robust solution for diverse ASR tasks.Our\ncontributions include the development of a new Samba ASR architecture for\nautomatic speech recognition(ASR),demonstrating the superiority of structured\nstatespace models(SSMs)over transformer based models for speech sequence\nprocessing.We provide a comprehensive evaluation on public\nbenchmarks,showcasing stateoftheart(SOTA)performance,and present an indepth\nanalysis of computational efficiency,robustness to noise,and sequence\ngeneralization.This work highlights the viability of Mamba SSMs as a\ntransformerfree alternative for efficient and accurate ASR.By leveraging the\nadvancements of statespace modeling,Samba ASR redefines ASR performance\nstandards and sets a new benchmark for future research in this field.",
      "tldr_zh": "我们提出 Samba-ASR，这是一个基于 Mamba 架构的状态空间模型 (SSMs) 的先进自动语音识别 (ASR) 系统，作为编码器和解码器来处理局部和全局时间依赖性，避免了 Transformer 模型的二次方缩放问题和长序列处理难题。相比现有开源 Transformer 模型，Samba-ASR 在标准基准测试中显著降低了 Word Error Rate (WER)，并在低资源场景下保持出色性能，同时提升了计算效率和对噪声的鲁棒性。该研究证明了 SSMs 作为 Transformer-free 替代方案的优越性，重新定义了 ASR 的性能标准，并为未来语音处理研究设定了新基准。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02832v3",
      "published_date": "2025-01-06 08:16:06 UTC",
      "updated_date": "2025-01-08 17:46:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:56:38.824210"
    },
    {
      "arxiv_id": "2502.15700v1",
      "title": "Sustainable Digitalization of Business with Multi-Agent RAG and LLM",
      "title_zh": "翻译失败",
      "authors": [
        "Muhammad Arslan",
        "Saba Munawar",
        "Christophe Cruz"
      ],
      "abstract": "Businesses heavily rely on data sourced from various channels like news\narticles, financial reports, and consumer reviews to drive their operations,\nenabling informed decision-making and identifying opportunities. However,\ntraditional manual methods for data extraction are often time-consuming and\nresource-intensive, prompting the adoption of digital transformation\ninitiatives to enhance efficiency. Yet, concerns persist regarding the\nsustainability of such initiatives and their alignment with the United Nations\n(UN)'s Sustainable Development Goals (SDGs). This research aims to explore the\nintegration of Large Language Models (LLMs) with Retrieval-Augmented Generation\n(RAG) as a sustainable solution for Information Extraction (IE) and processing.\nThe research methodology involves reviewing existing solutions for business\ndecision-making, noting that many systems require training new machine learning\nmodels, which are resource-intensive and have significant environmental\nimpacts. Instead, we propose a sustainable business solution using pre-existing\nLLMs that can work with diverse datasets. We link domain-specific datasets to\ntailor LLMs to company needs and employ a Multi-Agent architecture to divide\ntasks such as information retrieval, enrichment, and classification among\nspecialized agents. This approach optimizes the extraction process and improves\noverall efficiency. Through the utilization of these technologies, businesses\ncan optimize resource utilization, improve decision-making processes, and\ncontribute to sustainable development goals, thereby fostering environmental\nresponsibility within the corporate sector.",
      "tldr_zh": "该研究探讨了使用大型语言模型(LLMs)和检索增强生成(RAG)技术来实现企业数字化转型的可持续性，旨在解决传统数据提取方法耗时费力和环境影响大的问题。作者提出一种多智能体架构，将任务如信息检索、丰富和分类分配给专门代理，从而优化资源利用和决策过程，而非训练新机器学习模型。该方法通过整合领域特定数据集与预训练LLMs，帮助企业提升效率，并与联合国可持续发展目标(SDGs)对齐，最终促进企业环保责任和可持续发展。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15700v1",
      "published_date": "2025-01-06 08:14:23 UTC",
      "updated_date": "2025-01-06 08:14:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:56:49.205568"
    },
    {
      "arxiv_id": "2501.02822v1",
      "title": "RDD4D: 4D Attention-Guided Road Damage Detection And Classification",
      "title_zh": "RDD4D: 4D 注意力引导的道路损坏检测和分类",
      "authors": [
        "Asma Alkalbani",
        "Muhammad Saqib",
        "Ahmed Salim Alrawahi",
        "Abbas Anwar",
        "Chandarnath Adak",
        "Saeed Anwar"
      ],
      "abstract": "Road damage detection and assessment are crucial components of infrastructure\nmaintenance. However, current methods often struggle with detecting multiple\ntypes of road damage in a single image, particularly at varying scales. This is\ndue to the lack of road datasets with various damage types having varying\nscales. To overcome this deficiency, first, we present a novel dataset called\nDiverse Road Damage Dataset (DRDD) for road damage detection that captures the\ndiverse road damage types in individual images, addressing a crucial gap in\nexisting datasets. Then, we provide our model, RDD4D, that exploits Attention4D\nblocks, enabling better feature refinement across multiple scales. The\nAttention4D module processes feature maps through an attention mechanism\ncombining positional encoding and \"Talking Head\" components to capture local\nand global contextual information. In our comprehensive experimental analysis\ncomparing various state-of-the-art models on our proposed, our enhanced model\ndemonstrated superior performance in detecting large-sized road cracks with an\nAverage Precision (AP) of 0.458 and maintained competitive performance with an\noverall AP of 0.445. Moreover, we also provide results on the CrackTinyNet\ndataset; our model achieved around a 0.21 increase in performance. The code,\nmodel weights, dataset, and our results are available on\n\\href{https://github.com/msaqib17/Road_Damage_Detection}{https://github.com/msaqib17/Road\\_Damage\\_Detection}.",
      "tldr_zh": "本文提出了一种新的道路损坏检测和分类方法RDD4D，以解决现有模型在处理单一图像中多种损坏类型和不同规模时的困难。首先，作者引入了Diverse Road Damage Dataset (DRDD)，这是一个包含各种损坏类型的多样化数据集，填补了现有数据集的空白。RDD4D模型采用Attention4D模块，通过结合位置编码和\"Talking Head\"组件的注意力机制，实现对特征图的局部和全局上下文信息的精炼。在实验中，该模型在DRDD数据集上检测大尺寸裂缝的Average Precision (AP)达到0.458，整体AP为0.445，并在CrackTinyNet数据集上性能提升约0.21，展现出显著优势。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02822v1",
      "published_date": "2025-01-06 07:48:04 UTC",
      "updated_date": "2025-01-06 07:48:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:57:02.331066"
    },
    {
      "arxiv_id": "2501.02816v1",
      "title": "InpDiffusion: Image Inpainting Localization via Conditional Diffusion Models",
      "title_zh": "翻译失败",
      "authors": [
        "Kai Wang",
        "Shaozhang Niu",
        "Qixian Hao",
        "Jiwei Zhang"
      ],
      "abstract": "As artificial intelligence advances rapidly, particularly with the advent of\nGANs and diffusion models, the accuracy of Image Inpainting Localization (IIL)\nhas become increasingly challenging. Current IIL methods face two main\nchallenges: a tendency towards overconfidence, leading to incorrect\npredictions; and difficulty in detecting subtle tampering boundaries in\ninpainted images. In response, we propose a new paradigm that treats IIL as a\nconditional mask generation task utilizing diffusion models. Our method,\nInpDiffusion, utilizes the denoising process enhanced by the integration of\nimage semantic conditions to progressively refine predictions. During\ndenoising, we employ edge conditions and introduce a novel edge supervision\nstrategy to enhance the model's perception of edge details in inpainted\nobjects. Balancing the diffusion model's stochastic sampling with edge\nsupervision of tampered image regions mitigates the risk of incorrect\npredictions from overconfidence and prevents the loss of subtle boundaries that\ncan result from overly stochastic processes. Furthermore, we propose an\ninnovative Dual-stream Multi-scale Feature Extractor (DMFE) for extracting\nmulti-scale features, enhancing feature representation by considering both\nsemantic and edge conditions of the inpainted images. Extensive experiments\nacross challenging datasets demonstrate that the InpDiffusion significantly\noutperforms existing state-of-the-art methods in IIL tasks, while also\nshowcasing excellent generalization capabilities and robustness.",
      "tldr_zh": "该论文提出 InpDiffusion 方法，将 Image Inpainting Localization (IIL) 视为条件掩码生成任务，利用扩散模型的去噪过程结合图像语义条件来逐步优化预测。InpDiffusion 引入边缘条件和创新的边缘监督策略，以平衡模型的随机采样，减少过度自信导致的错误预测并保留微妙篡改边界；同时，提出 Dual-stream Multi-scale Feature Extractor (DMFE) 来提取多尺度特征，提升对 inpainted 图像语义和边缘的表示。实验结果显示，InpDiffusion 在多个挑战数据集上显著优于现有最先进方法，并展现出优秀的泛化能力和鲁棒性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02816v1",
      "published_date": "2025-01-06 07:32:12 UTC",
      "updated_date": "2025-01-06 07:32:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:57:14.797138"
    },
    {
      "arxiv_id": "2501.03290v1",
      "title": "A Decision-Based Heterogenous Graph Attention Network for Multi-Class Fake News Detection",
      "title_zh": "基于决策的异构图注意力网络用于多类假新闻检测",
      "authors": [
        "Batool Lakzaei",
        "Mostafa Haghir Chehreghani",
        "Alireza Bagheri"
      ],
      "abstract": "A promising tool for addressing fake news detection is Graph Neural Networks\n(GNNs). However, most existing GNN-based methods rely on binary classification,\ncategorizing news as either real or fake. Additionally, traditional GNN models\nuse a static neighborhood for each node, making them susceptible to issues like\nover-squashing. In this paper, we introduce a novel model named Decision-based\nHeterogeneous Graph Attention Network (DHGAT) for fake news detection in a\nsemi-supervised setting. DHGAT effectively addresses the limitations of\ntraditional GNNs by dynamically optimizing and selecting the neighborhood type\nfor each node in every layer. It represents news data as a heterogeneous graph\nwhere nodes (news items) are connected by various types of edges. The\narchitecture of DHGAT consists of a decision network that determines the\noptimal neighborhood type and a representation network that updates node\nembeddings based on this selection. As a result, each node learns an optimal\nand task-specific computational graph, enhancing both the accuracy and\nefficiency of the fake news detection process. We evaluate DHGAT on the LIAR\ndataset, a large and challenging dataset for multi-class fake news detection,\nwhich includes news items categorized into six classes. Our results demonstrate\nthat DHGAT outperforms existing methods, improving accuracy by approximately 4%\nand showing robustness with limited labeled data.",
      "tldr_zh": "本论文提出了一种名为 DHGAT 的 Decision-Based Heterogeneous Graph Attention Network，用于多类假新闻检测，以解决传统 GNNs 在二元分类和静态邻居（如 over-squashing）问题上的局限性。DHGAT 通过决策网络动态优化每个节点的邻居类型，并结合表示网络更新节点嵌入，将新闻数据表示为 heterogeneous graph，从而实现任务特定的计算图优化。在 LIAR 数据集的半监督设置中，实验结果显示 DHGAT 比现有方法提高了约 4% 的准确率，并表现出在标签数据有限情况下的鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03290v1",
      "published_date": "2025-01-06 07:18:31 UTC",
      "updated_date": "2025-01-06 07:18:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:57:26.586842"
    },
    {
      "arxiv_id": "2501.02803v1",
      "title": "Enhancing Lifelong Multi-Agent Path Finding with Cache Mechanism",
      "title_zh": "翻译失败",
      "authors": [
        "Yimin Tang",
        "Zhenghong Yu",
        "Yi Zheng",
        "T. K. Satish Kumar",
        "Jiaoyang Li",
        "Sven Koenig"
      ],
      "abstract": "Multi-Agent Path Finding (MAPF), which focuses on finding collision-free\npaths for multiple robots, is crucial in autonomous warehouse operations.\nLifelong MAPF (L-MAPF), where agents are continuously reassigned new targets\nupon completing their current tasks, offers a more realistic approximation of\nreal-world warehouse scenarios. While cache storage systems can enhance\nefficiency and reduce operational costs, existing approaches primarily rely on\nexpectations and mathematical models, often without adequately addressing the\nchallenges of multi-robot planning and execution. In this paper, we introduce a\nnovel mechanism called Lifelong MAPF with Cache Mechanism (L-MAPF-CM), which\nintegrates high-level cache storage with low-level path planning. We have\ninvolved a new type of map grid called cache for temporary item storage.\nAdditionally, we involved a task assigner (TA) with a locking mechanism to\nbridge the gap between the new cache grid and L-MAPF algorithm. The TA\ndynamically allocates target locations to agents based on their status in\nvarious scenarios. We evaluated L-MAPF-CM using different cache replacement\npolicies and task distributions. L-MAPF-CM has demonstrated performance\nimprovements particularly with high cache hit rates and smooth traffic\nconditions.",
      "tldr_zh": "本文针对 Multi-Agent Path Finding (MAPF) 在自动仓库中的应用，提出了一种新型框架 Lifelong MAPF with Cache Mechanism (L-MAPF-CM)，通过整合高层缓存存储和底层路径规划来提升多代理连续任务分配的效率。L-MAPF-CM 引入了 cache 网格用于临时存储物品，以及一个任务分配器 (Task Assigner, TA) 带有锁定机制，以动态根据代理状态分配目标位置。实验结果显示，该框架在不同缓存替换策略和任务分布下，实现了高缓存命中率和顺畅交通条件下的性能显著提升。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "arXiv admin note: substantial text overlap with arXiv:2403.13421",
      "pdf_url": "http://arxiv.org/pdf/2501.02803v1",
      "published_date": "2025-01-06 06:44:13 UTC",
      "updated_date": "2025-01-06 06:44:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:57:39.109335"
    },
    {
      "arxiv_id": "2501.02793v1",
      "title": "Fairness Through Matching",
      "title_zh": "通过匹配实现公平性",
      "authors": [
        "Kunwoong Kim",
        "Insung Kong",
        "Jongjin Lee",
        "Minwoo Chae",
        "Sangchul Park",
        "Yongdai Kim"
      ],
      "abstract": "Group fairness requires that different protected groups, characterized by a\ngiven sensitive attribute, receive equal outcomes overall. Typically, the level\nof group fairness is measured by the statistical gap between predictions from\ndifferent protected groups. In this study, we reveal an implicit property of\nexisting group fairness measures, which provides an insight into how the\ngroup-fair models behave. Then, we develop a new group-fair constraint based on\nthis implicit property to learn group-fair models. To do so, we first introduce\na notable theoretical observation: every group-fair model has an implicitly\ncorresponding transport map between the input spaces of each protected group.\nBased on this observation, we introduce a new group fairness measure termed\nMatched Demographic Parity (MDP), which quantifies the averaged gap between\npredictions of two individuals (from different protected groups) matched by a\ngiven transport map. Then, we prove that any transport map can be used in MDP\nto learn group-fair models, and develop a novel algorithm called Fairness\nThrough Matching (FTM), which learns a group-fair model using MDP constraint\nwith an user-specified transport map. We specifically propose two favorable\ntypes of transport maps for MDP, based on the optimal transport theory, and\ndiscuss their advantages. Experiments reveal that FTM successfully trains\ngroup-fair models with certain desirable properties by choosing the transport\nmap accordingly.",
      "tldr_zh": "这篇论文探讨了群组公平（group fairness）的概念，即确保不同受保护群组在预测结果上统计差距最小。研究者揭示了现有公平措施的隐含属性，并基于传输映射（transport map）的理论观察，引入了新的公平度量标准 Matched Demographic Parity (MDP)，用于量化通过传输映射匹配的个体预测差距。论文开发了 Fairness Through Matching (FTM) 算法，利用 MDP 约束和用户指定的传输映射（如基于 optimal transport theory 的类型）来训练群组公平模型，实验结果显示 FTM 可以有效实现模型的公平性并带来可取的属性。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "comment": "Published in TMLR",
      "pdf_url": "http://arxiv.org/pdf/2501.02793v1",
      "published_date": "2025-01-06 06:27:38 UTC",
      "updated_date": "2025-01-06 06:27:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:57:52.673718"
    },
    {
      "arxiv_id": "2501.02790v1",
      "title": "Segmenting Text and Learning Their Rewards for Improved RLHF in Language Model",
      "title_zh": "翻译失败",
      "authors": [
        "Yueqin Yin",
        "Shentao Yang",
        "Yujia Xie",
        "Ziyi Yang",
        "Yuting Sun",
        "Hany Awadalla",
        "Weizhu Chen",
        "Mingyuan Zhou"
      ],
      "abstract": "Reinforcement learning from human feedback (RLHF) has been widely adopted to\nalign language models (LMs) with human preference. Prior RLHF works typically\ntake a bandit formulation, which, though intuitive, ignores the sequential\nnature of LM generation and can suffer from the sparse reward issue. While\nrecent works propose dense token-level RLHF, treating each token as an action\nmay be oversubtle to proper reward assignment. In this paper, we seek to get\nthe best of both by training and utilizing a segment-level reward model, which\nassigns a reward to each semantically complete text segment that spans over a\nshort sequence of tokens. For reward learning, our method allows dynamic text\nsegmentation and compatibility with standard sequence-preference datasets. For\neffective RL-based LM training against segment reward, we generalize the\nclassical scalar bandit reward normalizers into location-aware normalizer\nfunctions and interpolate the segment reward for further densification. With\nthese designs, our method performs competitively on three popular RLHF\nbenchmarks for LM policy: AlpacaEval 2.0, Arena-Hard, and MT-Bench. Ablation\nstudies are conducted to further demonstrate our method.",
      "tldr_zh": "本文提出了一种改进的 Reinforcement Learning from Human Feedback (RLHF) 方法，用于更好地对齐语言模型 (LMs)，通过训练 segment-level 奖励模型来解决传统 bandit formulation 的顺序忽略和稀疏奖励问题。该模型对语义完整的文本段分配奖励，支持动态文本分割，并与标准序列偏好数据集兼容；同时，引入 location-aware normalizer 函数和奖励插值以增强 RL 训练效果。实验结果显示，该方法在 AlpacaEval 2.0、Arena-Hard 和 MT-Bench 等基准上表现出竞争性性能，消融研究进一步证实了其优势。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02790v1",
      "published_date": "2025-01-06 06:17:56 UTC",
      "updated_date": "2025-01-06 06:17:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:58:03.756243"
    },
    {
      "arxiv_id": "2501.03288v1",
      "title": "CodeVision: Detecting LLM-Generated Code Using 2D Token Probability Maps and Vision Models",
      "title_zh": "翻译失败",
      "authors": [
        "Zhenyu Xu",
        "Victor S. Sheng"
      ],
      "abstract": "The rise of large language models (LLMs) like ChatGPT has significantly\nimproved automated code generation, enhancing software development efficiency.\nHowever, this introduces challenges in academia, particularly in distinguishing\nbetween human-written and LLM-generated code, which complicates issues of\nacademic integrity. Existing detection methods, such as pre-trained models and\nwatermarking, face limitations in adaptability and computational efficiency. In\nthis paper, we propose a novel detection method using 2D token probability maps\ncombined with vision models, preserving spatial code structures such as\nindentation and brackets. By transforming code into log probability matrices\nand applying vision models like Vision Transformers (ViT) and ResNet, we\ncapture both content and structure for more accurate detection. Our method\nshows robustness across multiple programming languages and improves upon\ntraditional detectors, offering a scalable and computationally efficient\nsolution for identifying LLM-generated code.",
      "tldr_zh": "本文研究了大型语言模型（LLM）如 ChatGPT 在代码生成中的应用所带来的学术诚信挑战，提出了一种名为 CodeVision 的新检测方法，以区分人类编写和 LLM 生成的代码。方法通过将代码转化为 2D token probability maps（并转换为 log probability matrices），并结合视觉模型如 Vision Transformers (ViT) 和 ResNet，捕获代码的内容和结构（如缩进和括号），从而实现更准确的检测。实验结果表明，该方法在多种编程语言上表现出色，比传统检测器（如预训练模型和水印技术）更鲁棒、可扩展和计算高效。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03288v1",
      "published_date": "2025-01-06 06:15:10 UTC",
      "updated_date": "2025-01-06 06:15:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:58:14.486505"
    },
    {
      "arxiv_id": "2501.02788v2",
      "title": "GLoG-CSUnet: Enhancing Vision Transformers with Adaptable Radiomic Features for Medical Image Segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Niloufar Eghbali",
        "Hassan Bagher-Ebadian",
        "Tuka Alhanai",
        "Mohammad M. Ghassemi"
      ],
      "abstract": "Vision Transformers (ViTs) have shown promise in medical image semantic\nsegmentation (MISS) by capturing long-range correlations. However, ViTs often\nstruggle to model local spatial information effectively, which is essential for\naccurately segmenting fine anatomical details, particularly when applied to\nsmall datasets without extensive pre-training. We introduce Gabor and Laplacian\nof Gaussian Convolutional Swin Network (GLoG-CSUnet), a novel architecture\nenhancing Transformer-based models by incorporating learnable radiomic\nfeatures. This approach integrates dynamically adaptive Gabor and Laplacian of\nGaussian (LoG) filters to capture texture, edge, and boundary information,\nenhancing the feature representation processed by the Transformer model. Our\nmethod uniquely combines the long-range dependency modeling of Transformers\nwith the texture analysis capabilities of Gabor and LoG features. Evaluated on\nthe Synapse multi-organ and ACDC cardiac segmentation datasets, GLoG-CSUnet\ndemonstrates significant improvements over state-of-the-art models, achieving a\n1.14% increase in Dice score for Synapse and 0.99% for ACDC, with minimal\ncomputational overhead (only 15 and 30 additional parameters, respectively).\nGLoG-CSUnet's flexible design allows integration with various base models,\noffering a promising approach for incorporating radiomics-inspired feature\nextraction in Transformer architectures for medical image analysis. The code\nimplementation is available on GitHub at: https://github.com/HAAIL/GLoG-CSUnet.",
      "tldr_zh": "该论文提出 GLoG-CSUnet，一种新型架构，用于增强 Vision Transformers (ViTs) 在医疗图像语义分割 (MISS) 中的性能，特别针对捕捉局部空间信息和处理小数据集的挑战。方法通过整合动态适应的 Gabor 和 Laplacian of Gaussian (LoG) 过滤器，捕捉纹理、边缘和边界特征，并与 Transformers 的长距离依赖建模相结合，提升特征表示。在 Synapse multi-organ 和 ACDC cardiac segmentation 数据集上，GLoG-CSUnet 相比最先进模型的 Dice score 分别提高了 1.14% 和 0.99%，同时仅增加了 15 和 30 个参数，计算开销最小。该架构设计灵活，可与各种基模型整合，并已在 GitHub 上开源。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02788v2",
      "published_date": "2025-01-06 06:07:40 UTC",
      "updated_date": "2025-01-08 18:33:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:58:26.838951"
    },
    {
      "arxiv_id": "2501.02785v1",
      "title": "Hybrid deep convolution model for lung cancer detection with transfer learning",
      "title_zh": "基于迁移学习的肺癌检测混合深度卷积模型",
      "authors": [
        "Sugandha Saxena",
        "S. N. Prasad",
        "Ashwin M Polnaya",
        "Shweta Agarwala"
      ],
      "abstract": "Advances in healthcare research have significantly enhanced our understanding\nof disease mechanisms, diagnostic precision, and therapeutic options. Yet, lung\ncancer remains one of the leading causes of cancer-related mortality worldwide\ndue to challenges in early and accurate diagnosis. While current lung cancer\ndetection models show promise, there is considerable potential for further\nimproving the accuracy for timely intervention. To address this challenge, we\nintroduce a hybrid deep convolution model leveraging transfer learning, named\nthe Maximum Sensitivity Neural Network (MSNN). MSNN is designed to improve the\nprecision of lung cancer detection by refining sensitivity and specificity.\nThis model has surpassed existing deep learning approaches through experimental\nvalidation, achieving an accuracy of 98% and a sensitivity of 97%. By\noverlaying sensitivity maps onto lung Computed Tomography (CT) scans, it\nenables the visualization of regions most indicative of malignant or benign\nclassifications. This innovative method demonstrates exceptional performance in\ndistinguishing lung cancer with minimal false positives, thereby enhancing the\naccuracy of medical diagnoses.",
      "tldr_zh": "本研究针对肺癌早期诊断的挑战，提出了一种Hybrid deep convolution model，名为Maximum Sensitivity Neural Network (MSNN)，利用transfer learning来提升检测的敏感性和特异性。该模型通过实验验证超越了现有深度学习方法，在肺癌检测中实现了98%的准确率和97%的敏感性。此外，MSNN能将敏感性地图叠加到Computed Tomography (CT) scans上，实现对恶性和良性区域的可视化，从而减少假阳性和提高医疗诊断的精确性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "13 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.02785v1",
      "published_date": "2025-01-06 06:01:01 UTC",
      "updated_date": "2025-01-06 06:01:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:58:37.987987"
    },
    {
      "arxiv_id": "2501.02778v1",
      "title": "ICFNet: Integrated Cross-modal Fusion Network for Survival Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Binyu Zhang",
        "Zhu Meng",
        "Junhao Dong",
        "Fei Su",
        "Zhicheng Zhao"
      ],
      "abstract": "Survival prediction is a crucial task in the medical field and is essential\nfor optimizing treatment options and resource allocation. However, current\nmethods often rely on limited data modalities, resulting in suboptimal\nperformance. In this paper, we propose an Integrated Cross-modal Fusion Network\n(ICFNet) that integrates histopathology whole slide images, genomic expression\nprofiles, patient demographics, and treatment protocols. Specifically, three\ntypes of encoders, a residual orthogonal decomposition module and a unification\nfusion module are employed to merge multi-modal features to enhance prediction\naccuracy. Additionally, a balanced negative log-likelihood loss function is\ndesigned to ensure fair training across different patients. Extensive\nexperiments demonstrate that our ICFNet outperforms state-of-the-art algorithms\non five public TCGA datasets, including BLCA, BRCA, GBMLGG, LUAD, and UCEC, and\nshows its potential to support clinical decision-making and advance precision\nmedicine. The codes are available at: https://github.com/binging512/ICFNet.",
      "tldr_zh": "该研究提出了一种集成跨模态融合网络（ICFNet），旨在通过整合组织病理学全滑玻图像（histopathology whole slide images）、基因组表达配置文件（genomic expression profiles）、患者人口统计学数据和治疗方案等多模态数据，提高医疗领域的生存预测准确性。具体而言，ICFNet 采用三种类型的编码器、残差正交分解模块（residual orthogonal decomposition module）和统一融合模块（unification fusion module）来合并特征，并设计了平衡的负对数似然损失函数（balanced negative log-likelihood loss function）以确保公平训练。在五个公共 TCGA 数据集（包括 BLCA, BRCA, GBMLGG, LUAD 和 UCEC）上的实验表明，ICFNet 优于现有最先进算法，支持临床决策并推进精准医学。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02778v1",
      "published_date": "2025-01-06 05:49:08 UTC",
      "updated_date": "2025-01-06 05:49:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:58:50.580007"
    },
    {
      "arxiv_id": "2501.02770v2",
      "title": "Multi-Agent Path Finding under Limited Communication Range Constraint via Dynamic Leading",
      "title_zh": "翻译失败",
      "authors": [
        "Hoang-Dung Bui",
        "Erion Plaku",
        "Gregoy J. Stein"
      ],
      "abstract": "This paper proposes a novel framework to handle a multi-agent path finding\nproblem under a limited communication range constraint, where all agents must\nhave a connected communication channel to the rest of the team. Many existing\napproaches to multi-agent path finding (e.g., leader-follower platooning)\novercome computational challenges of planning in this domain by planning one\nagent at a time in a fixed order. However, fixed leader-follower approaches can\nbecome stuck during planning, limiting their practical utility in dense-clutter\nenvironments. To overcome this limitation, we develop dynamic leading\nmulti-agent path finding, which allows for dynamic reselection of the leading\nagent during path planning whenever progress cannot be made. The experiments\nshow the efficiency of our framework, which can handle up to 25 agents with\nmore than 90% success-rate across five environment types where baselines\nroutinely fail.",
      "tldr_zh": "该论文提出了一种新框架，用于处理有限通信范围约束下的多智能体路径寻找（Multi-Agent Path Finding）问题，确保所有智能体保持与团队的连接通信。不同于传统固定领导者-跟随者（leader-follower）方法，该框架引入动态领导（Dynamic Leading）机制，允许在路径规划过程中动态重新选择领导智能体，以避免在密集杂乱环境中卡住。实验结果显示，该框架能高效处理多达 25 个智能体，在五种环境类型中成功率超过 90%，而基线方法经常失败，从而提升了多智能体系统的实用性和鲁棒性。",
      "categories": [
        "cs.AI",
        "cs.MA",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02770v2",
      "published_date": "2025-01-06 05:21:18 UTC",
      "updated_date": "2025-02-05 15:32:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:59:01.800599"
    },
    {
      "arxiv_id": "2501.02767v1",
      "title": "Enhancing Trustworthiness of Graph Neural Networks with Rank-Based Conformal Training",
      "title_zh": "翻译失败",
      "authors": [
        "Ting Wang",
        "Zhixin Zhou",
        "Rui Luo"
      ],
      "abstract": "Graph Neural Networks (GNNs) has been widely used in a variety of fields\nbecause of their great potential in representing graph-structured data.\nHowever, lacking of rigorous uncertainty estimations limits their application\nin high-stakes. Conformal Prediction (CP) can produce statistically guaranteed\nuncertainty estimates by using the classifier's probability estimates to obtain\nprediction sets, which contains the true class with a user-specified\nprobability. In this paper, we propose a Rank-based CP during training\nframework to GNNs (RCP-GNN) for reliable uncertainty estimates to enhance the\ntrustworthiness of GNNs in the node classification scenario. By exploiting rank\ninformation of the classifier's outcome, prediction sets with desired coverage\nrate can be efficiently constructed. The strategy of CP during training with\ndifferentiable rank-based conformity loss function is further explored to adapt\nprediction sets according to network topology information. In this way, the\ncomposition of prediction sets can be guided by the goal of jointly reducing\ninefficiency and probability estimation errors. Extensive experiments on\nseveral real-world datasets show that our model achieves any pre-defined target\nmarginal coverage while significantly reducing the inefficiency compared with\nstate-of-the-art methods.",
      "tldr_zh": "这篇论文提出了一种名为 RCP-GNN 的框架，通过 Rank-based Conformal Prediction (CP) 在训练过程中提升 Graph Neural Networks (GNNs) 的可信度，旨在解决 GNNs 在节点分类中缺乏可靠不确定性估计的问题。方法利用分类器输出的排名信息来高效构建具有期望覆盖率的预测集，并引入可微的基于排名的符合损失函数，使预测集适应网络拓扑信息，从而减少无效性和概率估计错误。实验在多个真实数据集上验证，该框架实现了预定义的目标边际覆盖率，并显著降低了无效性，比现有方法先进。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "8 pages,2 figures,published to AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.02767v1",
      "published_date": "2025-01-06 05:19:24 UTC",
      "updated_date": "2025-01-06 05:19:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:59:14.790015"
    },
    {
      "arxiv_id": "2501.02766v2",
      "title": "Are GNNs Actually Effective for Multimodal Fault Diagnosis in Microservice Systems?",
      "title_zh": "GNNs 在多模态微服务系统故障诊断中是否真正有效？",
      "authors": [
        "Fei Gao",
        "Ruyue Xin",
        "Xiaocui Li",
        "Yaqiang Zhang"
      ],
      "abstract": "Graph Neural Networks (GNNs) are widely adopted for fault diagnosis in\nmicroservice systems, premised on their ability to model service dependencies.\nHowever, the necessity of explicit graph structures remains underexamined, as\nexisting evaluations conflate preprocessing with architectural contributions.\nTo isolate the true value of GNNs, we propose DiagMLP, a deliberately minimal,\ntopology-agnostic baseline that retains multimodal fusion capabilities while\nexcluding graph modeling. Through ablation experiments across five datasets,\nDiagMLP achieves performance parity with state-of-the-art GNN-based methods in\nfault detection, localization, and classification. These findings challenge the\nprevailing assumption that graph structures are indispensable, revealing that:\n(i) preprocessing pipelines already encode critical dependency information, and\n(ii) GNN modules contribute marginally beyond multimodality fusion. Our work\nadvocates for systematic re-evaluation of architectural complexity and\nhighlights the need for standardized baseline protocols to validate model\ninnovations.",
      "tldr_zh": "这篇论文质疑 GNNs（Graph Neural Networks）在微服务系统多模态故障诊断中的实际有效性，指出现有评估混淆了预处理和架构贡献。作者提出一个简单基线模型 DiagMLP，它不依赖图结构但保留多模态融合能力，通过在五个数据集上的消融实验实现了与最先进 GNN 方法在故障检测、定位和分类方面的性能相当。研究发现，预处理管道已充分编码关键服务依赖信息，而 GNN 模块在多模态融合之外的贡献微乎其微。该工作呼吁系统重新评估架构复杂性，并强调标准化基线协议的重要性，以验证模型创新。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "6 pages, 5 figures, submitted to conference",
      "pdf_url": "http://arxiv.org/pdf/2501.02766v2",
      "published_date": "2025-01-06 05:18:13 UTC",
      "updated_date": "2025-03-10 09:51:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:59:27.002800"
    },
    {
      "arxiv_id": "2501.02765v1",
      "title": "Visual Large Language Models for Generalized and Specialized Applications",
      "title_zh": "视觉大型语言模型用于泛化和专业化应用",
      "authors": [
        "Yifan Li",
        "Zhixin Lai",
        "Wentao Bao",
        "Zhen Tan",
        "Anh Dao",
        "Kewei Sui",
        "Jiayi Shen",
        "Dong Liu",
        "Huan Liu",
        "Yu Kong"
      ],
      "abstract": "Visual-language models (VLM) have emerged as a powerful tool for learning a\nunified embedding space for vision and language. Inspired by large language\nmodels, which have demonstrated strong reasoning and multi-task capabilities,\nvisual large language models (VLLMs) are gaining increasing attention for\nbuilding general-purpose VLMs. Despite the significant progress made in VLLMs,\nthe related literature remains limited, particularly from a comprehensive\napplication perspective, encompassing generalized and specialized applications\nacross vision (image, video, depth), action, and language modalities. In this\nsurvey, we focus on the diverse applications of VLLMs, examining their using\nscenarios, identifying ethics consideration and challenges, and discussing\nfuture directions for their development. By synthesizing these contents, we aim\nto provide a comprehensive guide that will pave the way for future innovations\nand broader applications of VLLMs. The paper list repository is available:\nhttps://github.com/JackYFL/awesome-VLLMs.",
      "tldr_zh": "这篇论文对Visual Large Language Models (VLLMs)进行了全面调查，探讨了这些模型在泛化和专业应用中的潜力，包括视觉（图像、视频、深度）、动作和语言模态的多样场景。论文分析了VLLMs的使用方式、伦理考虑以及面临的挑战，如文献有限和实际应用问题，并为未来发展提供了指导和建议。最终，该研究旨在推动VLLMs的创新应用，并附带了一个资源仓库（https://github.com/JackYFL/awesome-VLLMs）。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02765v1",
      "published_date": "2025-01-06 05:15:59 UTC",
      "updated_date": "2025-01-06 05:15:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:59:37.912753"
    },
    {
      "arxiv_id": "2501.02749v2",
      "title": "Intelligent logistics management robot path planning algorithm integrating transformer and GCN network",
      "title_zh": "集成 Transformer 和 GCN 网络的智能物流管理机器人路径规划算法",
      "authors": [
        "Hao Luo",
        "Jianjun Wei",
        "Shuchen Zhao",
        "Ankai Liang",
        "Zhongjin Xu",
        "Ruxue Jiang"
      ],
      "abstract": "This research delves into advanced route optimization for robots in smart\nlogistics, leveraging a fusion of Transformer architectures, Graph Neural\nNetworks (GNNs), and Generative Adversarial Networks (GANs). The approach\nutilizes a graph-based representation encompassing geographical data, cargo\nallocation, and robot dynamics, addressing both spatial and resource\nlimitations to refine route efficiency. Through extensive testing with\nauthentic logistics datasets, the proposed method achieves notable\nimprovements, including a 15% reduction in travel distance, a 20% boost in time\nefficiency, and a 10% decrease in energy consumption. These findings highlight\nthe algorithm's effectiveness, promoting enhanced performance in intelligent\nlogistics operations.",
      "tldr_zh": "这篇论文提出了一种智能物流管理机器人路径规划算法，将Transformer架构、Graph Neural Networks (GNNs) 和Generative Adversarial Networks (GANs) 整合起来，以优化机器人路线。算法采用基于图的表示方法，涵盖地理数据、货物分配和机器人动态，解决了空间和资源限制问题，从而提升路线效率。通过真实物流数据集的测试，该方法实现了15%的旅行距离减少、20%的时间效率提升和10%的能源消耗降低。这些结果突出了算法的有效性，有助于促进智能物流操作的整体性能。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "21 pages",
      "pdf_url": "http://arxiv.org/pdf/2501.02749v2",
      "published_date": "2025-01-06 03:53:02 UTC",
      "updated_date": "2025-03-12 03:29:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:59:50.527535"
    },
    {
      "arxiv_id": "2501.02740v1",
      "title": "Interpretable Recognition of Fused Magnesium Furnace Working Conditions with Deep Convolutional Stochastic Configuration Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Li Weitao",
        "Zhang Xinru",
        "Wang Dianhui",
        "Tong Qianqian",
        "Chai Tianyou"
      ],
      "abstract": "To address the issues of a weak generalization capability and\ninterpretability in working condition recognition model of a fused magnesium\nfurnace, this paper proposes an interpretable working condition recognition\nmethod based on deep convolutional stochastic configuration networks (DCSCNs).\nFirstly, a supervised learning mechanism is employed to generate physically\nmeaningful Gaussian differential convolution kernels. An incremental method is\nutilized to construct a DCSCNs model, ensuring the convergence of recognition\nerrors in a hierarchical manner and avoiding the iterative optimization process\nof convolutional kernel parameters using the widely used backpropagation\nalgorithm. The independent coefficient of channel feature maps is defined to\nobtain the visualization results of feature class activation maps for the fused\nmagnesium furnace. A joint reward function is constructed based on the\nrecognition accuracy, the interpretable trustworthiness evaluation metrics, and\nthe model parameter quantity. Reinforcement learning (RL) is applied to\nadaptively prune the convolutional kernels of the DCSCNs model, aiming to build\na compact, highly performed and interpretable network. The experimental results\ndemonstrate that the proposed method outperforms the other deep learning\napproaches in terms of recognition accuracy and interpretability.",
      "tldr_zh": "本研究针对熔融镁炉工作条件识别模型的泛化能力弱和可解释性不足问题，提出了一种基于深层卷积随机配置网络(DCSCNs)的可解释识别方法。该方法采用监督学习生成物理意义上的高斯差分卷积核，并通过增量构建DCSCNs模型实现分层错误收敛，避免使用反向传播算法优化参数，同时引入强化学习(RL)基于联合奖励函数适配性地修剪卷积核，以构建紧凑且高性能的网络。实验结果显示，该方法在识别准确性和可解释性方面优于其他深度学习方法，为工业应用提供了更可靠的解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02740v1",
      "published_date": "2025-01-06 03:17:41 UTC",
      "updated_date": "2025-01-06 03:17:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:00:01.945946"
    },
    {
      "arxiv_id": "2501.02739v1",
      "title": "TARDiS : Text Augmentation for Refining Diversity and Separability",
      "title_zh": "翻译失败",
      "authors": [
        "Kyungmin Kim",
        "SangHun Im",
        "GiBaeg Kim",
        "Heung-Seon Oh"
      ],
      "abstract": "Text augmentation (TA) is a critical technique for text classification,\nespecially in few-shot settings. This paper introduces a novel LLM-based TA\nmethod, TARDiS, to address challenges inherent in the generation and alignment\nstages of two-stage TA methods. For the generation stage, we propose two\ngeneration processes, SEG and CEG, incorporating multiple class-specific\nprompts to enhance diversity and separability. For the alignment stage, we\nintroduce a class adaptation (CA) method to ensure that generated examples\nalign with their target classes through verification and modification.\nExperimental results demonstrate TARDiS's effectiveness, outperforming\nstate-of-the-art LLM-based TA methods in various few-shot text classification\ntasks. An in-depth analysis confirms the detailed behaviors at each stage.",
      "tldr_zh": "这篇论文引入了TARDiS，一种基于LLM的文本增强(TA)方法，旨在通过优化生成和对齐阶段来提升少样本文本分类的多样性和可分性。TARDiS在生成阶段采用SEG和CEG过程，使用多个类特定提示来产生更丰富的示例；在对齐阶段，通过类适应(CA)方法进行验证和修改，确保生成的示例与目标类精确匹配。实验结果表明，TARDiS在各种少样本文本分类任务中优于最先进的方法，并通过深入分析验证了每个阶段的详细行为。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages",
      "pdf_url": "http://arxiv.org/pdf/2501.02739v1",
      "published_date": "2025-01-06 03:17:35 UTC",
      "updated_date": "2025-01-06 03:17:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:00:14.662753"
    },
    {
      "arxiv_id": "2501.02732v1",
      "title": "AFed: Algorithmic Fair Federated Learning",
      "title_zh": "AFed: 算法公平联邦学习",
      "authors": [
        "Huiqiang Chen",
        "Tianqing Zhu",
        "Wanlei Zhou",
        "Wei Zhao"
      ],
      "abstract": "Federated Learning (FL) has gained significant attention as it facilitates\ncollaborative machine learning among multiple clients without centralizing\ntheir data on a server. FL ensures the privacy of participating clients by\nlocally storing their data, which creates new challenges in fairness.\nTraditional debiasing methods assume centralized access to sensitive\ninformation, rendering them impractical for the FL setting. Additionally, FL is\nmore susceptible to fairness issues than centralized machine learning due to\nthe diverse client data sources that may be associated with group information.\nTherefore, training a fair model in FL without access to client local data is\nimportant and challenging. This paper presents AFed, a straightforward yet\neffective framework for promoting group fairness in FL. The core idea is to\ncircumvent restricted data access by learning the global data distribution.\nThis paper proposes two approaches: AFed-G, which uses a conditional generator\ntrained on the server side, and AFed-GAN, which improves upon AFed-G by\ntraining a conditional GAN on the client side. We augment the client data with\nthe generated samples to help remove bias. Our theoretical analysis justifies\nthe proposed methods, and empirical results on multiple real-world datasets\ndemonstrate a substantial improvement in AFed over several baselines.",
      "tldr_zh": "这篇论文针对 Federated Learning (FL) 中的群组公平性挑战，提出 AFed 框架，以解决传统去偏方法在 FL 设置中无法访问本地数据的限制。AFed 的核心思想是通过学习全局数据分布，采用两种方法：AFed-G 使用服务器端的条件生成器，以及 AFed-GAN 在客户端训练条件 GAN 来生成样本，从而增强数据并去除偏差。实验结果显示，AFed 在多个真实数据集上显著优于基线模型，理论分析也支持了其有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by IEEE Transactions on Neural Networks and Learning Systems",
      "pdf_url": "http://arxiv.org/pdf/2501.02732v1",
      "published_date": "2025-01-06 03:05:49 UTC",
      "updated_date": "2025-01-06 03:05:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:00:26.346522"
    },
    {
      "arxiv_id": "2501.05470v1",
      "title": "RTLSquad: Multi-Agent Based Interpretable RTL Design",
      "title_zh": "RTLSquad：基于多智能体的可解释 RTL 设计",
      "authors": [
        "Bowei Wang",
        "Qi Xiong",
        "Zeqing Xiang",
        "Lei Wang",
        "Renzhi Chen"
      ],
      "abstract": "Optimizing Register-Transfer Level (RTL) code is crucial for improving\nhardware PPA performance. Large Language Models (LLMs) offer new approaches for\nautomatic RTL code generation and optimization. However, existing methods often\nlack decision interpretability (sufficient, understandable justification for\ndecisions), making it difficult for hardware engineers to trust the generated\nresults, thus preventing these methods from being integrated into the design\nprocess. To address this, we propose RTLSquad, a novel LLM-Based Multi-Agent\nsystem for interpretable RTL code generation. RTLSquad divides the design\nprocess into exploration, implementation, and verification & evaluation stages\nmanaged by specialized agent squads, generating optimized RTL code through\ninter-agent collaboration, and providing decision interpretability through the\ncommunication process. Experiments show that RTLSquad excels in generating\nfunctionally correct RTL code and optimizing PPA performance, while also having\nthe capability to provide decision paths, demonstrating the practical value of\nour system.",
      "tldr_zh": "该研究针对 Register-Transfer Level (RTL) 代码优化在硬件 PPA 性能中的关键作用，提出 RTLSquad，一种基于 Large Language Models (LLMs) 的多智能体系统，以提升 RTL 代码生成的决策可解释性。该系统将设计过程分为探索、实施和验证与评估三个阶段，由专门的代理小组协作管理，通过代理间通信生成优化的 RTL 代码并提供详细决策路径。实验结果表明，RTLSquad 在生成功能正确的 RTL 代码和优化 PPA 性能方面优于现有方法，并展示了其在实际硬件设计中的实用价值。",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.05470v1",
      "published_date": "2025-01-06 02:57:54 UTC",
      "updated_date": "2025-01-06 02:57:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:00:37.670829"
    },
    {
      "arxiv_id": "2501.02728v1",
      "title": "OpenGU: A Comprehensive Benchmark for Graph Unlearning",
      "title_zh": "OpenGU：用于图取消学习的全面基准",
      "authors": [
        "Bowen Fan",
        "Yuming Ai",
        "Xunkai Li",
        "Zhilin Guo",
        "Rong-Hua Li",
        "Guoren Wang"
      ],
      "abstract": "Graph Machine Learning is essential for understanding and analyzing\nrelational data. However, privacy-sensitive applications demand the ability to\nefficiently remove sensitive information from trained graph neural networks\n(GNNs), avoiding the unnecessary time and space overhead caused by retraining\nmodels from scratch. To address this issue, Graph Unlearning (GU) has emerged\nas a critical solution, with the potential to support dynamic graph updates in\ndata management systems and enable scalable unlearning in distributed data\nsystems while ensuring privacy compliance. Unlike machine unlearning in\ncomputer vision or other fields, GU faces unique difficulties due to the\nnon-Euclidean nature of graph data and the recursive message-passing mechanism\nof GNNs. Additionally, the diversity of downstream tasks and the complexity of\nunlearning requests further amplify these challenges. Despite the proliferation\nof diverse GU strategies, the absence of a benchmark providing fair comparisons\nfor GU, and the limited flexibility in combining downstream tasks and\nunlearning requests, have yielded inconsistencies in evaluations, hindering the\ndevelopment of this domain. To fill this gap, we present OpenGU, the first GU\nbenchmark, where 16 SOTA GU algorithms and 37 multi-domain datasets are\nintegrated, enabling various downstream tasks with 13 GNN backbones when\nresponding to flexible unlearning requests. Based on this unified benchmark\nframework, we are able to provide a comprehensive and fair evaluation for GU.\nThrough extensive experimentation, we have drawn $8$ crucial conclusions about\nexisting GU methods, while also gaining valuable insights into their\nlimitations, shedding light on potential avenues for future research.",
      "tldr_zh": "这篇论文提出了OpenGU，这是一个全面的Graph Unlearning (GU) 基准，用于评估图神经网络 (GNNs) 中隐私敏感数据的移除方法，以避免重新训练的开销。OpenGU 集成了16个SOTA GU算法、37个多领域数据集和13个GNN骨干，支持各种下游任务和灵活的unlearning请求，从而提供统一的评估框架。通过广泛实验，论文得出了8个关键结论，揭示了现有GU方法的局限性，并为未来研究指明了方向。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "under review",
      "pdf_url": "http://arxiv.org/pdf/2501.02728v1",
      "published_date": "2025-01-06 02:57:32 UTC",
      "updated_date": "2025-01-06 02:57:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:00:50.420885"
    },
    {
      "arxiv_id": "2501.02727v1",
      "title": "Tree-based RAG-Agent Recommendation System: A Case Study in Medical Test Data",
      "title_zh": "翻译失败",
      "authors": [
        "Yahe Yang",
        "Chengyue Huang"
      ],
      "abstract": "We present HiRMed (Hierarchical RAG-enhanced Medical Test Recommendation), a\nnovel tree-structured recommendation system that leverages Retrieval-Augmented\nGeneration (RAG) for intelligent medical test recommendations. Unlike\ntraditional vector similarity-based approaches, our system performs medical\nreasoning at each tree node through a specialized RAG process. Starting from\nthe root node with initial symptoms, the system conducts step-wise medical\nanalysis to identify potential underlying conditions and their corresponding\ndiagnostic requirements. At each level, instead of simple matching, our\nRAG-enhanced nodes analyze retrieved medical knowledge to understand\nsymptom-disease relationships and determine the most appropriate diagnostic\npath. The system dynamically adjusts its recommendation strategy based on\nmedical reasoning results, considering factors such as urgency levels and\ndiagnostic uncertainty. Experimental results demonstrate that our approach\nachieves superior performance in terms of coverage rate, accuracy, and miss\nrate compared to conventional retrieval-based methods. This work represents a\nsignificant advance in medical test recommendation by introducing medical\nreasoning capabilities into the traditional tree-based retrieval structure.",
      "tldr_zh": "本研究提出HiRMed，一种基于树结构的推荐系统（Tree-based RAG-Agent Recommendation System），利用Retrieval-Augmented Generation (RAG)技术进行智能医疗测试推荐，通过在每个树节点进行医疗推理来分析症状-疾病关系。系统从根节点起始症状逐步展开分层分析，动态调整诊断路径并考虑紧急程度和不确定性，与传统向量相似度方法相比实现了更精确的医疗决策。实验结果显示，HiRMed在覆盖率、准确性和漏报率方面均优于传统检索方法，为医疗测试推荐领域引入了医疗推理能力，标志着重要进展。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02727v1",
      "published_date": "2025-01-06 02:50:51 UTC",
      "updated_date": "2025-01-06 02:50:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:01:02.209271"
    },
    {
      "arxiv_id": "2501.02725v2",
      "title": "Artificial Intelligence in Creative Industries: Advances Prior to 2025",
      "title_zh": "人工智能在创意产业中的进展：2025年之前",
      "authors": [
        "Nantheera Anantrasirichai",
        "Fan Zhang",
        "David Bull"
      ],
      "abstract": "The rapid advancements in artificial intelligence (AI), particularly in\ngenerative AI and large language models (LLMs), have profoundly impacted the\ncreative industries by enabling innovative content creation, enhancing\nworkflows, and democratizing access to creative tools. This paper explores the\nsignificant technological shifts since our previous review in 2022,\nhighlighting how these developments have expanded creative opportunities and\nefficiency. These technological advancements have enhanced the capabilities of\ntext-to-image, text-to-video, and multimodal generation technologies. In\nparticular, key breakthroughs in LLMs have established new benchmarks in\nconversational AI, while advancements in image generators have revolutionized\ncontent creation. We also discuss AI integration into post-production\nworkflows, which has significantly accelerated and refined traditional\nprocesses. Despite these innovations, challenges remain, particularly for the\nmedia industry, due to the demands on communication traffic from creative\ncontent. We therefore include data compression and quality assessment in this\npaper. Furthermore, we highlight the trend toward unified AI frameworks capable\nof addressing multiple creative tasks and underscore the importance of human\noversight to mitigate AI-generated inaccuracies. Finally, we explore AI's\nfuture potential in the creative sector, stressing the need to navigate\nemerging challenges to maximize its benefits while addressing associated risks.",
      "tldr_zh": "这篇论文回顾了自2022年以来AI在创意产业中的进展，特别是generative AI和LLMs如何推动创新内容创建、工作流程优化以及创意工具的民主化。关键突破包括提升text-to-image、text-to-video和多模态生成技术的能力，以及AI在后期制作中的整合，这些变革显著提高了效率并设定了对话AI和图像生成的新基准。尽管这些创新带来机遇，但论文也讨论了媒体行业面临的通信流量挑战，并强调了数据压缩、质量评估以及人类监督的重要性。最后，论文探讨了统一AI框架的趋势，并呼吁在最大化AI益处的同时，应对潜在风险以实现可持续应用。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "This is an updated review of our previous paper (see\n  https://doi.org/10.1007/s10462-021-10039-7)",
      "pdf_url": "http://arxiv.org/pdf/2501.02725v2",
      "published_date": "2025-01-06 02:46:33 UTC",
      "updated_date": "2025-02-16 10:20:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:01:14.961889"
    },
    {
      "arxiv_id": "2501.02715v1",
      "title": "Improved Data Encoding for Emerging Computing Paradigms: From Stochastic to Hyperdimensional Computing",
      "title_zh": "针对新兴计算范式的改进数据编码：从随机计算到高维计算",
      "authors": [
        "Mehran Shoushtari Moghadam",
        "Sercan Aygun",
        "M. Hassan Najafi"
      ],
      "abstract": "Data encoding is a fundamental step in emerging computing paradigms,\nparticularly in stochastic computing (SC) and hyperdimensional computing (HDC),\nwhere it plays a crucial role in determining the overall system performance and\nhardware cost efficiency. This study presents an advanced encoding strategy\nthat leverages a hardware-friendly class of low-discrepancy (LD) sequences,\nspecifically powers-of-2 bases of Van der Corput (VDC) sequences (VDC-2^n), as\nsources for random number generation. Our approach significantly enhances the\naccuracy and efficiency of SC and HDC systems by addressing challenges\nassociated with randomness. By employing LD sequences, we improve correlation\nproperties and reduce hardware complexity. Experimental results demonstrate\nsignificant improvements in accuracy and energy savings for SC and HDC systems.\nOur solution provides a robust framework for integrating SC and HDC in\nresource-constrained environments, paving the way for efficient and scalable AI\nimplementations.",
      "tldr_zh": "本文提出了一种改进的数据编码策略，针对随机计算(SC)和超维计算(HDC)等新兴计算范式，旨在提升系统性能和硬件效率。该策略利用低差异(LD)序列，特别是基于2的幂的Van der Corput (VDC-2^n)序列，作为随机数生成源，以改善相关性属性并降低硬件复杂度。实验结果显示，该方法显著提高了SC和HDC系统的准确性和能源节省，并为资源受限环境下的高效、可扩展AI实现提供了稳健框架。",
      "categories": [
        "cs.ET",
        "cs.AI",
        "cs.LG",
        "cs.NE"
      ],
      "primary_category": "cs.ET",
      "comment": "5 pages, 3 figures, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2501.02715v1",
      "published_date": "2025-01-06 02:07:49 UTC",
      "updated_date": "2025-01-06 02:07:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:01:26.905195"
    },
    {
      "arxiv_id": "2501.02711v1",
      "title": "KG-CF: Knowledge Graph Completion with Context Filtering under the Guidance of Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Zaiyi Zheng",
        "Yushun Dong",
        "Song Wang",
        "Haochen Liu",
        "Qi Wang",
        "Jundong Li"
      ],
      "abstract": "Large Language Models (LLMs) have shown impressive performance in various\ntasks, including knowledge graph completion (KGC). However, current studies\nmostly apply LLMs to classification tasks, like identifying missing triplets,\nrather than ranking-based tasks, where the model ranks candidate entities based\non plausibility. This focus limits the practical use of LLMs in KGC, as\nreal-world applications prioritize highly plausible triplets. Additionally,\nwhile graph paths can help infer the existence of missing triplets and improve\ncompletion accuracy, they often contain redundant information. To address these\nissues, we propose KG-CF, a framework tailored for ranking-based KGC tasks.\nKG-CF leverages LLMs' reasoning abilities to filter out irrelevant contexts,\nachieving superior results on real-world datasets. The code and datasets are\navailable at \\url{https://anonymous.4open.science/r/KG-CF}.",
      "tldr_zh": "该论文指出，现有的研究主要将大型语言模型 (LLMs) 应用于知识图谱补全 (KGC) 的分类任务，而忽略了更实用的排名任务，以及图路径中存在的冗余信息问题。针对这些问题，作者提出 KG-CF 框架，利用 LLMs 的推理能力过滤无关上下文，从而提升排名-based KGC 的准确性。在真实数据集上的实验显示，该框架取得了优越的性能，并提供了代码和数据集以供进一步验证。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "6 pages",
      "pdf_url": "http://arxiv.org/pdf/2501.02711v1",
      "published_date": "2025-01-06 01:52:15 UTC",
      "updated_date": "2025-01-06 01:52:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:01:38.415412"
    },
    {
      "arxiv_id": "2501.02709v1",
      "title": "Horizon Generalization in Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Vivek Myers",
        "Catherine Ji",
        "Benjamin Eysenbach"
      ],
      "abstract": "We study goal-conditioned RL through the lens of generalization, but not in\nthe traditional sense of random augmentations and domain randomization. Rather,\nwe aim to learn goal-directed policies that generalize with respect to the\nhorizon: after training to reach nearby goals (which are easy to learn), these\npolicies should succeed in reaching distant goals (which are quite challenging\nto learn). In the same way that invariance is closely linked with\ngeneralization is other areas of machine learning (e.g., normalization layers\nmake a network invariant to scale, and therefore generalize to inputs of\nvarying scales), we show that this notion of horizon generalization is closely\nlinked with invariance to planning: a policy navigating towards a goal will\nselect the same actions as if it were navigating to a waypoint en route to that\ngoal. Thus, such a policy trained to reach nearby goals should succeed at\nreaching arbitrarily-distant goals. Our theoretical analysis proves that both\nhorizon generalization and planning invariance are possible, under some\nassumptions. We present new experimental results and recall findings from prior\nwork in support of our theoretical results. Taken together, our results open\nthe door to studying how techniques for invariance and generalization developed\nin other areas of machine learning might be adapted to achieve this alluring\nproperty.",
      "tldr_zh": "本研究探讨了强化学习中的horizon generalization，即训练策略学会达到附近目标后，能推广到遥远目标的能力，这不同于传统的泛化方法。论文将此概念与planning invariance联系起来，证明如果策略对规划过程保持不变（如选择去往目标的动作等同于去往途中路标），则能实现horizon generalization。理论分析显示，在某些假设下，这种泛化是可能的，并通过新实验和先前研究结果进行验证。总体而言，该工作为从其他机器学习领域借鉴不变性和泛化技术提供了新途径，以提升目标导向强化学习的鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02709v1",
      "published_date": "2025-01-06 01:42:46 UTC",
      "updated_date": "2025-01-06 01:42:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:01:49.931588"
    },
    {
      "arxiv_id": "2501.02702v1",
      "title": "QuIM-RAG: Advancing Retrieval-Augmented Generation with Inverted Question Matching for Enhanced QA Performance",
      "title_zh": "翻译失败",
      "authors": [
        "Binita Saha",
        "Utsha Saha",
        "Muhammad Zubair Malik"
      ],
      "abstract": "This work presents a novel architecture for building Retrieval-Augmented\nGeneration (RAG) systems to improve Question Answering (QA) tasks from a target\ncorpus. Large Language Models (LLMs) have revolutionized the analyzing and\ngeneration of human-like text. These models rely on pre-trained data and lack\nreal-time updates unless integrated with live data tools. RAG enhances LLMs by\nintegrating online resources and databases to generate contextually appropriate\nresponses. However, traditional RAG still encounters challenges like\ninformation dilution and hallucinations when handling vast amounts of data. Our\napproach addresses these challenges by converting corpora into a\ndomain-specific dataset and RAG architecture is constructed to generate\nresponses from the target document. We introduce QuIM-RAG (Question-to-question\nInverted Index Matching), a novel approach for the retrieval mechanism in our\nsystem. This strategy generates potential questions from document chunks and\nmatches these with user queries to identify the most relevant text chunks for\ngenerating accurate answers. We have implemented our RAG system on top of the\nopen-source Meta-LLaMA3-8B-instruct model by Meta Inc. that is available on\nHugging Face. We constructed a custom corpus of 500+ pages from a high-traffic\nwebsite accessed thousands of times daily for answering complex questions,\nalong with manually prepared ground truth QA for evaluation. We compared our\napproach with traditional RAG models using BERT-Score and RAGAS,\nstate-of-the-art metrics for evaluating LLM applications. Our evaluation\ndemonstrates that our approach outperforms traditional RAG architectures on\nboth metrics.",
      "tldr_zh": "本研究提出 QuIM-RAG，一种创新的 Retrieval-Augmented Generation (RAG) 架构，通过 Inverted Question Matching 机制提升 Question Answering (QA) 任务的性能，以解决传统 RAG 在处理大量数据时可能出现的信息稀释和幻觉问题。QuIM-RAG 的核心方法是将文档块转换为潜在问题，并与用户查询进行匹配，从而精准检索相关文本生成答案。该系统基于开源 Meta-LLaMA3-8B-instruct 模型构建，并使用一个自定义语料库（500+ 页）进行实验。评估结果显示，QuIM-RAG 在 BERT-Score 和 RAGAS 指标上均优于传统 RAG 模型，证明了其在 QA 性能上的显著改进。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02702v1",
      "published_date": "2025-01-06 01:07:59 UTC",
      "updated_date": "2025-01-06 01:07:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:02:04.410106"
    },
    {
      "arxiv_id": "2501.02699v1",
      "title": "EAGLE: Enhanced Visual Grounding Minimizes Hallucinations in Instructional Multimodal Models",
      "title_zh": "翻译失败",
      "authors": [
        "Andrés Villa",
        "Juan León Alcázar",
        "Motasem Alfarra",
        "Vladimir Araujo",
        "Alvaro Soto",
        "Bernard Ghanem"
      ],
      "abstract": "Large language models and vision transformers have demonstrated impressive\nzero-shot capabilities, enabling significant transferability in downstream\ntasks. The fusion of these models has resulted in multi-modal architectures\nwith enhanced instructional capabilities. Despite incorporating vast image and\nlanguage pre-training, these multi-modal architectures often generate responses\nthat deviate from the ground truth in the image data. These failure cases are\nknown as hallucinations. Current methods for mitigating hallucinations\ngenerally focus on regularizing the language component, improving the fusion\nmodule, or ensembling multiple visual encoders to improve visual\nrepresentation. In this paper, we address the hallucination issue by directly\nenhancing the capabilities of the visual component. Our approach, named EAGLE,\nis fully agnostic to the LLM or fusion module and works as a post-pretraining\napproach that improves the grounding and language alignment of the visual\nencoder. We show that a straightforward reformulation of the original\ncontrastive pre-training task results in an improved visual encoder that can be\nincorporated into the instructional multi-modal architecture without additional\ninstructional training. As a result, EAGLE achieves a significant reduction in\nhallucinations across multiple challenging benchmarks and tasks.",
      "tldr_zh": "本文研究了多模态模型中的 hallucinations 问题，这些模型尽管融合了大型语言模型和视觉变压器，但仍可能生成与图像真实数据不符的响应。作者提出 EAGLE 方法，通过直接增强视觉编码器的 grounding 和语言对齐，作为一种后预训练方法，重新制定对比预训练任务，而无需额外指令训练。实验结果显示，EAGLE 在多个挑战性基准和任务上显著减少了 hallucinations，提升了模型的可靠性和性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "12 pages, 4 figures, 8 tables",
      "pdf_url": "http://arxiv.org/pdf/2501.02699v1",
      "published_date": "2025-01-06 00:39:31 UTC",
      "updated_date": "2025-01-06 00:39:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:02:14.405457"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 83,
  "processed_papers_count": 83,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-21T21:02:35.427037"
}
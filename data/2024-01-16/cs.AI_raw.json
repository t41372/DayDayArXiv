[
  {
    "arxiv_id": "2401.08887v1",
    "title": "NOTSOFAR-1 Challenge: New Datasets, Baseline, and Tasks for Distant Meeting Transcription",
    "authors": [
      "Alon Vinnikov",
      "Amir Ivry",
      "Aviv Hurvitz",
      "Igor Abramovski",
      "Sharon Koubi",
      "Ilya Gurvich",
      "Shai Pe`er",
      "Xiong Xiao",
      "Benjamin Martinez Elizalde",
      "Naoyuki Kanda",
      "Xiaofei Wang",
      "Shalev Shaer",
      "Stav Yagev",
      "Yossi Asher",
      "Sunit Sivasankaran",
      "Yifan Gong",
      "Min Tang",
      "Huaming Wang",
      "Eyal Krupka"
    ],
    "abstract": "We introduce the first Natural Office Talkers in Settings of Far-field Audio\nRecordings (``NOTSOFAR-1'') Challenge alongside datasets and baseline system.\nThe challenge focuses on distant speaker diarization and automatic speech\nrecognition (DASR) in far-field meeting scenarios, with single-channel and\nknown-geometry multi-channel tracks, and serves as a launch platform for two\nnew datasets: First, a benchmarking dataset of 315 meetings, averaging 6\nminutes each, capturing a broad spectrum of real-world acoustic conditions and\nconversational dynamics. It is recorded across 30 conference rooms, featuring\n4-8 attendees and a total of 35 unique speakers. Second, a 1000-hour simulated\ntraining dataset, synthesized with enhanced authenticity for real-world\ngeneralization, incorporating 15,000 real acoustic transfer functions. The\ntasks focus on single-device DASR, where multi-channel devices always share the\nsame known geometry. This is aligned with common setups in actual conference\nrooms, and avoids technical complexities associated with multi-device tasks. It\nalso allows for the development of geometry-specific solutions. The NOTSOFAR-1\nChallenge aims to advance research in the field of distant conversational\nspeech recognition, providing key resources to unlock the potential of\ndata-driven methods, which we believe are currently constrained by the absence\nof comprehensive high-quality training and benchmarking datasets.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "preprint",
    "pdf_url": "http://arxiv.org/pdf/2401.08887v1",
    "published_date": "2024-01-16 23:50:26 UTC",
    "updated_date": "2024-01-16 23:50:26 UTC"
  },
  {
    "arxiv_id": "2401.08879v2",
    "title": "Contribution Functions for Quantitative Bipolar Argumentation Graphs: A Principle-based Analysis",
    "authors": [
      "Timotheus Kampik",
      "Nico Potyka",
      "Xiang Yin",
      "Kristijonas ÄŒyras",
      "Francesca Toni"
    ],
    "abstract": "We present a principle-based analysis of contribution functions for\nquantitative bipolar argumentation graphs that quantify the contribution of one\nargument to another. The introduced principles formalise the intuitions\nunderlying different contribution functions as well as expectations one would\nhave regarding the behaviour of contribution functions in general. As none of\nthe covered contribution functions satisfies all principles, our analysis can\nserve as a tool that enables the selection of the most suitable function based\non the requirements of a given use case.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.08879v2",
    "published_date": "2024-01-16 23:27:42 UTC",
    "updated_date": "2024-06-13 22:07:30 UTC"
  },
  {
    "arxiv_id": "2401.08875v2",
    "title": "DCRMTA: Unbiased Causal Representation for Multi-touch Attribution",
    "authors": [
      "Jiaming Tang"
    ],
    "abstract": "Multi-touch attribution (MTA) currently plays a pivotal role in achieving a\nfair estimation of the contributions of each advertising touchpoint to-wards\nconversion behavior, deeply influencing budget allocation and advertising\nrecommenda-tion. Previous works attempted to eliminate the bias caused by user\npreferences to achieve the unbiased assumption of the conversion model. The\nmulti-model collaboration method is not ef-ficient, and the complete\nelimination of user in-fluence also eliminates the causal effect of user\nfeatures on conversion, resulting in limited per-formance of the conversion\nmodel. This paper re-defines the causal effect of user features on con-versions\nand proposes a novel end-to-end ap-proach, Deep Causal Representation for MTA\n(DCRMTA). Our model focuses on extracting causa features between conversions\nand users while eliminating confounding variables. Fur-thermore, extensive\nexperiments demonstrate DCRMTA's superior performance in converting prediction\nacross varying data distributions, while also effectively attributing value\nacross dif-ferent advertising channels.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ME"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.08875v2",
    "published_date": "2024-01-16 23:16:18 UTC",
    "updated_date": "2024-02-05 08:43:29 UTC"
  },
  {
    "arxiv_id": "2401.08863v1",
    "title": "Robust Localization of Key Fob Using Channel Impulse Response of Ultra Wide Band Sensors for Keyless Entry Systems",
    "authors": [
      "Abhiram Kolli",
      "Filippo Casamassima",
      "Horst Possegger",
      "Horst Bischof"
    ],
    "abstract": "Using neural networks for localization of key fob within and surrounding a\ncar as a security feature for keyless entry is fast emerging. In this paper we\nstudy: 1) the performance of pre-computed features of neural networks based UWB\n(ultra wide band) localization classification forming the baseline of our\nexperiments. 2) Investigate the inherent robustness of various neural networks;\ntherefore, we include the study of robustness of the adversarial examples\nwithout any adversarial training in this work. 3) Propose a multi-head\nself-supervised neural network architecture which outperforms the baseline\nneural networks without any adversarial training. The model's performance\nimproved by 67% at certain ranges of adversarial magnitude for fast gradient\nsign method and 37% each for basic iterative method and projected gradient\ndescent method.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.08863v1",
    "published_date": "2024-01-16 22:35:14 UTC",
    "updated_date": "2024-01-16 22:35:14 UTC"
  },
  {
    "arxiv_id": "2401.08850v2",
    "title": "REValueD: Regularised Ensemble Value-Decomposition for Factorisable Markov Decision Processes",
    "authors": [
      "David Ireland",
      "Giovanni Montana"
    ],
    "abstract": "Discrete-action reinforcement learning algorithms often falter in tasks with\nhigh-dimensional discrete action spaces due to the vast number of possible\nactions. A recent advancement leverages value-decomposition, a concept from\nmulti-agent reinforcement learning, to tackle this challenge. This study delves\ndeep into the effects of this value-decomposition, revealing that whilst it\ncurtails the over-estimation bias inherent to Q-learning algorithms, it\namplifies target variance. To counteract this, we present an ensemble of\ncritics to mitigate target variance. Moreover, we introduce a regularisation\nloss that helps to mitigate the effects that exploratory actions in one\ndimension can have on the value of optimal actions in other dimensions. Our\nnovel algorithm, REValueD, tested on discretised versions of the DeepMind\nControl Suite tasks, showcases superior performance, especially in the\nchallenging humanoid and dog tasks. We further dissect the factors influencing\nREValueD's performance, evaluating the significance of the regularisation loss\nand the scalability of REValueD with increasing sub-actions per dimension.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR camera ready version",
    "pdf_url": "http://arxiv.org/pdf/2401.08850v2",
    "published_date": "2024-01-16 21:47:23 UTC",
    "updated_date": "2024-03-08 10:06:53 UTC"
  },
  {
    "arxiv_id": "2401.09491v2",
    "title": "Memory, Space, and Planning: Multiscale Predictive Representations",
    "authors": [
      "Ida Momennejad"
    ],
    "abstract": "Memory is inherently entangled with prediction and planning. Flexible\nbehavior in biological and artificial agents depends on the interplay of\nlearning from the past and predicting the future in ever-changing environments.\nThis chapter reviews computational, behavioral, and neural evidence suggesting\nthese processes rely on learning the relational structure of experiences, known\nas cognitive maps, and draws two key takeaways. First, that these memory\nstructures are organized as multiscale, compact predictive representations in\nhippocampal and prefrontal cortex, or PFC, hierarchies. Second, we argue that\nsuch predictive memory structures are crucial to the complementary functions of\nthe hippocampus and PFC, both for enabling a recall of detailed and coherent\npast episodes as well as generalizing experiences at varying scales for\nefficient prediction and planning. These insights advance our understanding of\nmemory and planning mechanisms in the brain and hold significant implications\nfor advancing artificial intelligence systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "To be published as a chapter in an edited volume by Oxford University\n  Press (Editors: Sara Aronowitz and Lynn Nadel)",
    "pdf_url": "http://arxiv.org/pdf/2401.09491v2",
    "published_date": "2024-01-16 21:46:43 UTC",
    "updated_date": "2024-02-19 21:01:23 UTC"
  },
  {
    "arxiv_id": "2401.08819v2",
    "title": "Learning from Sparse Offline Datasets via Conservative Density Estimation",
    "authors": [
      "Zhepeng Cen",
      "Zuxin Liu",
      "Zitong Wang",
      "Yihang Yao",
      "Henry Lam",
      "Ding Zhao"
    ],
    "abstract": "Offline reinforcement learning (RL) offers a promising direction for learning\npolicies from pre-collected datasets without requiring further interactions\nwith the environment. However, existing methods struggle to handle\nout-of-distribution (OOD) extrapolation errors, especially in sparse reward or\nscarce data settings. In this paper, we propose a novel training algorithm\ncalled Conservative Density Estimation (CDE), which addresses this challenge by\nexplicitly imposing constraints on the state-action occupancy stationary\ndistribution. CDE overcomes the limitations of existing approaches, such as the\nstationary distribution correction method, by addressing the support mismatch\nissue in marginal importance sampling. Our method achieves state-of-the-art\nperformance on the D4RL benchmark. Notably, CDE consistently outperforms\nbaselines in challenging tasks with sparse rewards or insufficient data,\ndemonstrating the advantages of our approach in addressing the extrapolation\nerror problem in offline RL.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.08819v2",
    "published_date": "2024-01-16 20:42:15 UTC",
    "updated_date": "2024-03-11 14:43:52 UTC"
  },
  {
    "arxiv_id": "2401.08815v1",
    "title": "Adversarial Supervision Makes Layout-to-Image Diffusion Models Thrive",
    "authors": [
      "Yumeng Li",
      "Margret Keuper",
      "Dan Zhang",
      "Anna Khoreva"
    ],
    "abstract": "Despite the recent advances in large-scale diffusion models, little progress\nhas been made on the layout-to-image (L2I) synthesis task. Current L2I models\neither suffer from poor editability via text or weak alignment between the\ngenerated image and the input layout. This limits their usability in practice.\nTo mitigate this, we propose to integrate adversarial supervision into the\nconventional training pipeline of L2I diffusion models (ALDM). Specifically, we\nemploy a segmentation-based discriminator which provides explicit feedback to\nthe diffusion generator on the pixel-level alignment between the denoised image\nand the input layout. To encourage consistent adherence to the input layout\nover the sampling steps, we further introduce the multistep unrolling strategy.\nInstead of looking at a single timestep, we unroll a few steps recursively to\nimitate the inference process, and ask the discriminator to assess the\nalignment of denoised images with the layout over a certain time window. Our\nexperiments show that ALDM enables layout faithfulness of the generated images,\nwhile allowing broad editability via text prompts. Moreover, we showcase its\nusefulness for practical applications: by synthesizing target distribution\nsamples via text control, we improve domain generalization of semantic\nsegmentation models by a large margin (~12 mIoU points).",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at ICLR 2024. Project page:\n  https://yumengli007.github.io/ALDM/ and code:\n  https://github.com/boschresearch/ALDM",
    "pdf_url": "http://arxiv.org/pdf/2401.08815v1",
    "published_date": "2024-01-16 20:31:46 UTC",
    "updated_date": "2024-01-16 20:31:46 UTC"
  },
  {
    "arxiv_id": "2401.09489v1",
    "title": "PUPAE: Intuitive and Actionable Explanations for Time Series Anomalies",
    "authors": [
      "Audrey Der",
      "Chin-Chia Michael Yeh",
      "Yan Zheng",
      "Junpeng Wang",
      "Zhongfang Zhuang",
      "Liang Wang",
      "Wei Zhang",
      "Eamonn J. Keogh"
    ],
    "abstract": "In recent years there has been significant progress in time series anomaly\ndetection. However, after detecting an (perhaps tentative) anomaly, can we\nexplain it? Such explanations would be useful to triage anomalies. For example,\nin an oil refinery, should we respond to an anomaly by dispatching a hydraulic\nengineer, or an intern to replace the battery on a sensor? There have been some\nparallel efforts to explain anomalies, however many proposed techniques produce\nexplanations that are indirect, and often seem more complex than the anomaly\nthey seek to explain. Our review of the literature/checklists/user-manuals used\nby frontline practitioners in various domains reveals an interesting\nnear-universal commonality. Most practitioners discuss, explain and report\nanomalies in the following format: The anomaly would be like normal data A, if\nnot for the corruption B. The reader will appreciate that is a type of\ncounterfactual explanation. In this work we introduce a domain agnostic\ncounterfactual explanation technique to produce explanations for time series\nanomalies. As we will show, our method can produce both visual and text-based\nexplanations that are objectively correct, intuitive and in many circumstances,\ndirectly actionable.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "9 Page Manuscript, 1 Page Supplementary (Supplement not published in\n  conference proceedings.)",
    "pdf_url": "http://arxiv.org/pdf/2401.09489v1",
    "published_date": "2024-01-16 20:13:46 UTC",
    "updated_date": "2024-01-16 20:13:46 UTC"
  },
  {
    "arxiv_id": "2401.08577v1",
    "title": "MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World",
    "authors": [
      "Yining Hong",
      "Zishuo Zheng",
      "Peihao Chen",
      "Yian Wang",
      "Junyan Li",
      "Chuang Gan"
    ],
    "abstract": "Human beings possess the capability to multiply a melange of multisensory\ncues while actively exploring and interacting with the 3D world. Current\nmulti-modal large language models, however, passively absorb sensory data as\ninputs, lacking the capacity to actively interact with the objects in the 3D\nenvironment and dynamically collect their multisensory information. To usher in\nthe study of this area, we propose MultiPLY, a multisensory embodied large\nlanguage model that could incorporate multisensory interactive data, including\nvisual, audio, tactile, and thermal information into large language models,\nthereby establishing the correlation among words, actions, and percepts. To\nthis end, we first collect Multisensory Universe, a large-scale multisensory\ninteraction dataset comprising 500k data by deploying an LLM-powered embodied\nagent to engage with the 3D environment. To perform instruction tuning with\npre-trained LLM on such generated data, we first encode the 3D scene as\nabstracted object-centric representations and then introduce action tokens\ndenoting that the embodied agent takes certain actions within the environment,\nas well as state tokens that represent the multisensory state observations of\nthe agent at each time step. In the inference time, MultiPLY could generate\naction tokens, instructing the agent to take the action in the environment and\nobtain the next multisensory state observation. The observation is then\nappended back to the LLM via state tokens to generate subsequent text or action\ntokens. We demonstrate that MultiPLY outperforms baselines by a large margin\nthrough a diverse set of embodied tasks involving object retrieval, tool use,\nmultisensory captioning, and task decomposition.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://vis-www.cs.umass.edu/multiply",
    "pdf_url": "http://arxiv.org/pdf/2401.08577v1",
    "published_date": "2024-01-16 18:59:45 UTC",
    "updated_date": "2024-01-16 18:59:45 UTC"
  },
  {
    "arxiv_id": "2401.08743v2",
    "title": "MMToM-QA: Multimodal Theory of Mind Question Answering",
    "authors": [
      "Chuanyang Jin",
      "Yutong Wu",
      "Jing Cao",
      "Jiannan Xiang",
      "Yen-Ling Kuo",
      "Zhiting Hu",
      "Tomer Ullman",
      "Antonio Torralba",
      "Joshua B. Tenenbaum",
      "Tianmin Shu"
    ],
    "abstract": "Theory of Mind (ToM), the ability to understand people's mental states, is an\nessential ingredient for developing machines with human-level social\nintelligence. Recent machine learning models, particularly large language\nmodels, seem to show some aspects of ToM understanding. However, existing ToM\nbenchmarks use unimodal datasets - either video or text. Human ToM, on the\nother hand, is more than video or text understanding. People can flexibly\nreason about another person's mind based on conceptual representations (e.g.,\ngoals, beliefs, plans) extracted from any available data. To address this, we\nintroduce a multimodal Theory of Mind question answering (MMToM-QA) benchmark.\nMMToM-QA comprehensively evaluates machine ToM both on multimodal data and on\ndifferent kinds of unimodal data about a person's activity in a household\nenvironment. To engineer multimodal ToM capacity, we propose a novel method,\nBIP-ALM (Bayesian Inverse Planning Accelerated by Language Models). BIP-ALM\nextracts unified representations from multimodal data and utilizes language\nmodels for scalable Bayesian inverse planning. We conducted a systematic\ncomparison of human performance, BIP-ALM, and state-of-the-art models,\nincluding GPT-4. The experiments demonstrate that large language models and\nlarge multimodal models still lack robust ToM capacity. BIP-ALM, on the other\nhand, shows promising results, by leveraging the power of both model-based\nmental inference and language models.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "ACL 2024. 26 pages, 11 figures, 7 tables",
    "pdf_url": "http://arxiv.org/pdf/2401.08743v2",
    "published_date": "2024-01-16 18:59:24 UTC",
    "updated_date": "2024-06-15 10:13:14 UTC"
  },
  {
    "arxiv_id": "2401.08741v1",
    "title": "Fixed Point Diffusion Models",
    "authors": [
      "Xingjian Bai",
      "Luke Melas-Kyriazi"
    ],
    "abstract": "We introduce the Fixed Point Diffusion Model (FPDM), a novel approach to\nimage generation that integrates the concept of fixed point solving into the\nframework of diffusion-based generative modeling. Our approach embeds an\nimplicit fixed point solving layer into the denoising network of a diffusion\nmodel, transforming the diffusion process into a sequence of closely-related\nfixed point problems. Combined with a new stochastic training method, this\napproach significantly reduces model size, reduces memory usage, and\naccelerates training. Moreover, it enables the development of two new\ntechniques to improve sampling efficiency: reallocating computation across\ntimesteps and reusing fixed point solutions between timesteps. We conduct\nextensive experiments with state-of-the-art models on ImageNet, FFHQ,\nCelebA-HQ, and LSUN-Church, demonstrating substantial improvements in\nperformance and efficiency. Compared to the state-of-the-art DiT model, FPDM\ncontains 87% fewer parameters, consumes 60% less memory during training, and\nimproves image generation quality in situations where sampling computation or\ntime is limited. Our code and pretrained models are available at\nhttps://lukemelas.github.io/fixed-point-diffusion-models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page:\n  https://lukemelas.github.io/fixed-point-diffusion-models",
    "pdf_url": "http://arxiv.org/pdf/2401.08741v1",
    "published_date": "2024-01-16 18:55:54 UTC",
    "updated_date": "2024-01-16 18:55:54 UTC"
  },
  {
    "arxiv_id": "2401.08739v2",
    "title": "EgoGen: An Egocentric Synthetic Data Generator",
    "authors": [
      "Gen Li",
      "Kaifeng Zhao",
      "Siwei Zhang",
      "Xiaozhong Lyu",
      "Mihai Dusmanu",
      "Yan Zhang",
      "Marc Pollefeys",
      "Siyu Tang"
    ],
    "abstract": "Understanding the world in first-person view is fundamental in Augmented\nReality (AR). This immersive perspective brings dramatic visual changes and\nunique challenges compared to third-person views. Synthetic data has empowered\nthird-person-view vision models, but its application to embodied egocentric\nperception tasks remains largely unexplored. A critical challenge lies in\nsimulating natural human movements and behaviors that effectively steer the\nembodied cameras to capture a faithful egocentric representation of the 3D\nworld. To address this challenge, we introduce EgoGen, a new synthetic data\ngenerator that can produce accurate and rich ground-truth training data for\negocentric perception tasks. At the heart of EgoGen is a novel human motion\nsynthesis model that directly leverages egocentric visual inputs of a virtual\nhuman to sense the 3D environment. Combined with collision-avoiding motion\nprimitives and a two-stage reinforcement learning approach, our motion\nsynthesis model offers a closed-loop solution where the embodied perception and\nmovement of the virtual human are seamlessly coupled. Compared to previous\nworks, our model eliminates the need for a pre-defined global path, and is\ndirectly applicable to dynamic environments. Combined with our easy-to-use and\nscalable data generation pipeline, we demonstrate EgoGen's efficacy in three\ntasks: mapping and localization for head-mounted cameras, egocentric camera\ntracking, and human mesh recovery from egocentric views. EgoGen will be fully\nopen-sourced, offering a practical solution for creating realistic egocentric\ntraining data and aiming to serve as a useful tool for egocentric computer\nvision research. Refer to our project page: https://ego-gen.github.io/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by CVPR 2024 (Oral). 23 pages, 17 figures. Project page:\n  https://ego-gen.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2401.08739v2",
    "published_date": "2024-01-16 18:55:22 UTC",
    "updated_date": "2024-04-11 16:35:22 UTC"
  },
  {
    "arxiv_id": "2401.08567v1",
    "title": "Connect, Collapse, Corrupt: Learning Cross-Modal Tasks with Uni-Modal Data",
    "authors": [
      "Yuhui Zhang",
      "Elaine Sui",
      "Serena Yeung-Levy"
    ],
    "abstract": "Building cross-modal applications is challenging due to limited paired\nmulti-modal data. Recent works have shown that leveraging a pre-trained\nmulti-modal contrastive representation space enables cross-modal tasks to be\nlearned from uni-modal data. This is based on the assumption that contrastive\noptimization makes embeddings from different modalities interchangeable.\nHowever, this assumption is under-explored due to the poorly understood\ngeometry of the multi-modal contrastive space, where a modality gap exists. In\nour study, we provide a theoretical explanation of this space's geometry and\nintroduce a three-step method, $C^3$ (Connect, Collapse, Corrupt), to bridge\nthe modality gap, enhancing the interchangeability of embeddings. Our $C^3$\nmethod significantly improves cross-modal learning from uni-modal data,\nachieving state-of-the-art results on zero-shot image / audio / video\ncaptioning and text-to-image generation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Published at ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.08567v1",
    "published_date": "2024-01-16 18:52:27 UTC",
    "updated_date": "2024-01-16 18:52:27 UTC"
  },
  {
    "arxiv_id": "2401.08552v2",
    "title": "Explaining Time Series via Contrastive and Locally Sparse Perturbations",
    "authors": [
      "Zichuan Liu",
      "Yingying Zhang",
      "Tianchun Wang",
      "Zefan Wang",
      "Dongsheng Luo",
      "Mengnan Du",
      "Min Wu",
      "Yi Wang",
      "Chunlin Chen",
      "Lunting Fan",
      "Qingsong Wen"
    ],
    "abstract": "Explaining multivariate time series is a compound challenge, as it requires\nidentifying important locations in the time series and matching complex\ntemporal patterns. Although previous saliency-based methods addressed the\nchallenges, their perturbation may not alleviate the distribution shift issue,\nwhich is inevitable especially in heterogeneous samples. We present ContraLSP,\na locally sparse model that introduces counterfactual samples to build\nuninformative perturbations but keeps distribution using contrastive learning.\nFurthermore, we incorporate sample-specific sparse gates to generate more\nbinary-skewed and smooth masks, which easily integrate temporal trends and\nselect the salient features parsimoniously. Empirical studies on both synthetic\nand real-world datasets show that ContraLSP outperforms state-of-the-art\nmodels, demonstrating a substantial improvement in explanation quality for time\nseries data. The source code is available at\n\\url{https://github.com/zichuan-liu/ContraLSP}.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by International Conference on Learning Representations\n  (ICLR 2024)",
    "pdf_url": "http://arxiv.org/pdf/2401.08552v2",
    "published_date": "2024-01-16 18:27:37 UTC",
    "updated_date": "2024-01-29 04:44:46 UTC"
  },
  {
    "arxiv_id": "2401.08534v4",
    "title": "DiConStruct: Causal Concept-based Explanations through Black-Box Distillation",
    "authors": [
      "Ricardo Moreira",
      "Jacopo Bono",
      "MÃ¡rio Cardoso",
      "Pedro Saleiro",
      "MÃ¡rio A. T. Figueiredo",
      "Pedro Bizarro"
    ],
    "abstract": "Model interpretability plays a central role in human-AI decision-making\nsystems. Ideally, explanations should be expressed using human-interpretable\nsemantic concepts. Moreover, the causal relations between these concepts should\nbe captured by the explainer to allow for reasoning about the explanations.\nLastly, explanation methods should be efficient and not compromise the\nperformance of the predictive task. Despite the rapid advances in AI\nexplainability in recent years, as far as we know to date, no method fulfills\nthese three properties. Indeed, mainstream methods for local concept\nexplainability do not produce causal explanations and incur a trade-off between\nexplainability and prediction performance. We present DiConStruct, an\nexplanation method that is both concept-based and causal, with the goal of\ncreating more interpretable local explanations in the form of structural causal\nmodels and concept attributions. Our explainer works as a distillation model to\nany black-box machine learning model by approximating its predictions while\nproducing the respective explanations. Because of this, DiConStruct generates\nexplanations efficiently while not impacting the black-box prediction task. We\nvalidate our method on an image dataset and a tabular dataset, showing that\nDiConStruct approximates the black-box models with higher fidelity than other\nconcept explainability baselines, while providing explanations that include the\ncausal relations between the concepts.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at Conference on Causal Learning and Reasoning (CLeaR 2024,\n  https://www.cclear.cc/2024). To be published at Proceedings of Machine\n  Learning Research (PMLR)",
    "pdf_url": "http://arxiv.org/pdf/2401.08534v4",
    "published_date": "2024-01-16 17:54:02 UTC",
    "updated_date": "2024-01-26 14:48:35 UTC"
  },
  {
    "arxiv_id": "2401.08527v1",
    "title": "MICA: Towards Explainable Skin Lesion Diagnosis via Multi-Level Image-Concept Alignment",
    "authors": [
      "Yequan Bie",
      "Luyang Luo",
      "Hao Chen"
    ],
    "abstract": "Black-box deep learning approaches have showcased significant potential in\nthe realm of medical image analysis. However, the stringent trustworthiness\nrequirements intrinsic to the medical field have catalyzed research into the\nutilization of Explainable Artificial Intelligence (XAI), with a particular\nfocus on concept-based methods. Existing concept-based methods predominantly\napply concept annotations from a single perspective (e.g., global level),\nneglecting the nuanced semantic relationships between sub-regions and concepts\nembedded within medical images. This leads to underutilization of the valuable\nmedical information and may cause models to fall short in harmoniously\nbalancing interpretability and performance when employing inherently\ninterpretable architectures such as Concept Bottlenecks. To mitigate these\nshortcomings, we propose a multi-modal explainable disease diagnosis framework\nthat meticulously aligns medical images and clinical-related concepts\nsemantically at multiple strata, encompassing the image level, token level, and\nconcept level. Moreover, our method allows for model intervention and offers\nboth textual and visual explanations in terms of human-interpretable concepts.\nExperimental results on three skin image datasets demonstrate that our method,\nwhile preserving model interpretability, attains high performance and label\nefficiency for concept detection and disease diagnosis.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.08527v1",
    "published_date": "2024-01-16 17:45:01 UTC",
    "updated_date": "2024-01-16 17:45:01 UTC"
  },
  {
    "arxiv_id": "2401.08525v1",
    "title": "GATS: Gather-Attend-Scatter",
    "authors": [
      "Konrad Zolna",
      "Serkan Cabi",
      "Yutian Chen",
      "Eric Lau",
      "Claudio Fantacci",
      "Jurgis Pasukonis",
      "Jost Tobias Springenberg",
      "Sergio Gomez Colmenarejo"
    ],
    "abstract": "As the AI community increasingly adopts large-scale models, it is crucial to\ndevelop general and flexible tools to integrate them. We introduce\nGather-Attend-Scatter (GATS), a novel module that enables seamless combination\nof pretrained foundation models, both trainable and frozen, into larger\nmultimodal networks. GATS empowers AI systems to process and generate\ninformation across multiple modalities at different rates. In contrast to\ntraditional fine-tuning, GATS allows for the original component models to\nremain frozen, avoiding the risk of them losing important knowledge acquired\nduring the pretraining phase. We demonstrate the utility and versatility of\nGATS with a few experiments across games, robotics, and multimodal input-output\nsystems.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.08525v1",
    "published_date": "2024-01-16 17:43:42 UTC",
    "updated_date": "2024-01-16 17:43:42 UTC"
  },
  {
    "arxiv_id": "2401.08517v3",
    "title": "Supporting Student Decisions on Learning Recommendations: An LLM-Based Chatbot with Knowledge Graph Contextualization for Conversational Explainability and Mentoring",
    "authors": [
      "Hasan Abu-Rasheed",
      "Mohamad Hussam Abdulsalam",
      "Christian Weber",
      "Madjid Fathi"
    ],
    "abstract": "Student commitment towards a learning recommendation is not separable from\ntheir understanding of the reasons it was recommended to them; and their\nability to modify it based on that understanding. Among explainability\napproaches, chatbots offer the potential to engage the student in a\nconversation, similar to a discussion with a peer or a mentor. The capabilities\nof chatbots, however, are still not sufficient to replace a human mentor,\ndespite the advancements of generative AI (GenAI) and large language models\n(LLM). Therefore, we propose an approach to utilize chatbots as mediators of\nthe conversation and sources of limited and controlled generation of\nexplanations, to harvest the potential of LLMs while reducing their potential\nrisks at the same time. The proposed LLM-based chatbot supports students in\nunderstanding learning-paths recommendations. We use a knowledge graph (KG) as\na human-curated source of information, to regulate the LLM's output through\ndefining its prompt's context. A group chat approach is developed to connect\nstudents with human mentors, either on demand or in cases that exceed the\nchatbot's pre-defined tasks. We evaluate the chatbot with a user study, to\nprovide a proof-of-concept and highlight the potential requirements and\nlimitations of utilizing chatbots in conversational explainability.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.08517v3",
    "published_date": "2024-01-16 17:31:35 UTC",
    "updated_date": "2024-01-24 09:55:37 UTC"
  },
  {
    "arxiv_id": "2401.08505v4",
    "title": "Harnessing Orthogonality to Train Low-Rank Neural Networks",
    "authors": [
      "Daniel Coquelin",
      "Katharina FlÃ¼gel",
      "Marie Weiel",
      "Nicholas Kiefer",
      "Charlotte Debus",
      "Achim Streit",
      "Markus GÃ¶tz"
    ],
    "abstract": "This study explores the learning dynamics of neural networks by analyzing the\nsingular value decomposition (SVD) of their weights throughout training. Our\ninvestigation reveals that an orthogonal basis within each multidimensional\nweight's SVD representation stabilizes during training. Building upon this, we\nintroduce Orthogonality-Informed Adaptive Low-Rank (OIALR) training, a novel\ntraining method exploiting the intrinsic orthogonality of neural networks.\nOIALR seamlessly integrates into existing training workflows with minimal\naccuracy loss, as demonstrated by benchmarking on various datasets and\nwell-established network architectures. With appropriate hyperparameter tuning,\nOIALR can surpass conventional training setups, including those of\nstate-of-the-art models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.08505v4",
    "published_date": "2024-01-16 17:07:22 UTC",
    "updated_date": "2024-07-10 06:59:20 UTC"
  },
  {
    "arxiv_id": "2401.08478v2",
    "title": "Solving Continual Offline Reinforcement Learning with Decision Transformer",
    "authors": [
      "Kaixin Huang",
      "Li Shen",
      "Chen Zhao",
      "Chun Yuan",
      "Dacheng Tao"
    ],
    "abstract": "Continuous offline reinforcement learning (CORL) combines continuous and\noffline reinforcement learning, enabling agents to learn multiple tasks from\nstatic datasets without forgetting prior tasks. However, CORL faces challenges\nin balancing stability and plasticity. Existing methods, employing Actor-Critic\nstructures and experience replay (ER), suffer from distribution shifts, low\nefficiency, and weak knowledge-sharing. We aim to investigate whether Decision\nTransformer (DT), another offline RL paradigm, can serve as a more suitable\noffline continuous learner to address these issues. We first compare AC-based\noffline algorithms with DT in the CORL framework. DT offers advantages in\nlearning efficiency, distribution shift mitigation, and zero-shot\ngeneralization but exacerbates the forgetting problem during supervised\nparameter updates. We introduce multi-head DT (MH-DT) and low-rank adaptation\nDT (LoRA-DT) to mitigate DT's forgetting problem. MH-DT stores task-specific\nknowledge using multiple heads, facilitating knowledge sharing with common\ncomponents. It employs distillation and selective rehearsal to enhance current\ntask learning when a replay buffer is available. In buffer-unavailable\nscenarios, LoRA-DT merges less influential weights and fine-tunes DT's decisive\nMLP layer to adapt to the current task. Extensive experiments on MoJuCo and\nMeta-World benchmarks demonstrate that our methods outperform SOTA CORL\nbaselines and showcase enhanced learning capabilities and superior memory\nefficiency.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.08478v2",
    "published_date": "2024-01-16 16:28:32 UTC",
    "updated_date": "2024-04-07 11:29:37 UTC"
  },
  {
    "arxiv_id": "2401.08461v1",
    "title": "Decentralised Emergence of Robust and Adaptive Linguistic Conventions in Populations of Autonomous Agents Grounded in Continuous Worlds",
    "authors": [
      "JÃ©rÃ´me Botoko Ekila",
      "Jens Nevens",
      "Lara Verheyen",
      "Katrien Beuls",
      "Paul Van Eecke"
    ],
    "abstract": "This paper introduces a methodology through which a population of autonomous\nagents can establish a linguistic convention that enables them to refer to\narbitrary entities that they observe in their environment. The linguistic\nconvention emerges in a decentralised manner through local communicative\ninteractions between pairs of agents drawn from the population. The convention\nconsists of symbolic labels (word forms) associated to concept representations\n(word meanings) that are grounded in a continuous feature space. The concept\nrepresentations of each agent are individually constructed yet compatible on a\ncommunicative level. Through a range of experiments, we show (i) that the\nmethodology enables a population to converge on a communicatively effective,\ncoherent and human-interpretable linguistic convention, (ii) that it is\nnaturally robust against sensor defects in individual agents, (iii) that it can\neffectively deal with noisy observations, uncalibrated sensors and\nheteromorphic populations, (iv) that the method is adequate for continual\nlearning, and (v) that the convention self-adapts to changes in the environment\nand communicative needs of the agents.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.NE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.08461v1",
    "published_date": "2024-01-16 16:11:35 UTC",
    "updated_date": "2024-01-16 16:11:35 UTC"
  },
  {
    "arxiv_id": "2401.08460v1",
    "title": "Reinforcement Learning for Conversational Question Answering over Knowledge Graph",
    "authors": [
      "Mi Wu"
    ],
    "abstract": "Conversational question answering (ConvQA) over law knowledge bases (KBs)\ninvolves answering multi-turn natural language questions about law and hope to\nfind answers in the law knowledge base. Despite many methods have been\nproposed. Existing law knowledge base ConvQA model assume that the input\nquestion is clear and can perfectly reflect user's intention. However, in real\nworld, the input questions are noisy and inexplict. This makes the model hard\nto find the correct answer in the law knowledge bases. In this paper, we try to\nuse reinforcement learning to solve this problem. The reinforcement learning\nagent can automatically learn how to find the answer based on the input\nquestion and the conversation history, even when the input question is\ninexplicit. We test the proposed method on several real world datasets and the\nresults show the effectivenss of the proposed model.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.08460v1",
    "published_date": "2024-01-16 16:09:56 UTC",
    "updated_date": "2024-01-16 16:09:56 UTC"
  },
  {
    "arxiv_id": "2401.08458v1",
    "title": "Security and Privacy Issues and Solutions in Federated Learning for Digital Healthcare",
    "authors": [
      "Hyejun Jeong",
      "Tai-Myoung Chung"
    ],
    "abstract": "The advent of Federated Learning has enabled the creation of a\nhigh-performing model as if it had been trained on a considerable amount of\ndata. A multitude of participants and a server cooperatively train a model\nwithout the need for data disclosure or collection. The healthcare industry,\nwhere security and privacy are paramount, can substantially benefit from this\nnew learning paradigm, as data collection is no longer feasible due to\nstringent data policies. Nonetheless, unaddressed challenges and insufficient\nattack mitigation are hampering its adoption. Attack surfaces differ from\ntraditional centralized learning in that the server and clients communicate\nbetween each round of training. In this paper, we thus present vulnerabilities,\nattacks, and defenses based on the widened attack surfaces, as well as suggest\npromising new research directions toward a more robust FL.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.08458v1",
    "published_date": "2024-01-16 16:07:53 UTC",
    "updated_date": "2024-01-16 16:07:53 UTC"
  },
  {
    "arxiv_id": "2401.08728v3",
    "title": "AgentMixer: Multi-Agent Correlated Policy Factorization",
    "authors": [
      "Zhiyuan Li",
      "Wenshuai Zhao",
      "Lijun Wu",
      "Joni Pajarinen"
    ],
    "abstract": "In multi-agent reinforcement learning, centralized training with\ndecentralized execution (CTDE) methods typically assume that agents make\ndecisions based on their local observations independently, which may not lead\nto a correlated joint policy with coordination. Coordination can be explicitly\nencouraged during training and individual policies can be trained to imitate\nthe correlated joint policy. However, this may lead to an \\textit{asymmetric\nlearning failure} due to the observation mismatch between the joint and\nindividual policies. Inspired by the concept of correlated equilibrium, we\nintroduce a \\textit{strategy modification} called AgentMixer that allows agents\nto correlate their policies. AgentMixer combines individual partially\nobservable policies into a joint fully observable policy non-linearly. To\nenable decentralized execution, we introduce\n\\textit{Individual-Global-Consistency} to guarantee mode consistency during\njoint training of the centralized and decentralized policies and prove that\nAgentMixer converges to an $\\epsilon$-approximate Correlated Equilibrium. In\nthe Multi-Agent MuJoCo, SMAC-v2, Matrix Game, and Predator-Prey benchmarks,\nAgentMixer outperforms or matches state-of-the-art methods.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.08728v3",
    "published_date": "2024-01-16 15:32:41 UTC",
    "updated_date": "2024-12-13 11:12:33 UTC"
  },
  {
    "arxiv_id": "2401.08429v1",
    "title": "Machine Translation with Large Language Models: Prompt Engineering for Persian, English, and Russian Directions",
    "authors": [
      "Nooshin Pourkamali",
      "Shler Ebrahim Sharifi"
    ],
    "abstract": "Generative large language models (LLMs) have demonstrated exceptional\nproficiency in various natural language processing (NLP) tasks, including\nmachine translation, question answering, text summarization, and natural\nlanguage understanding.\n  To further enhance the performance of LLMs in machine translation, we\nconducted an investigation into two popular prompting methods and their\ncombination, focusing on cross-language combinations of Persian, English, and\nRussian. We employed n-shot feeding and tailored prompting frameworks. Our\nfindings indicate that multilingual LLMs like PaLM exhibit human-like machine\ntranslation outputs, enabling superior fine-tuning of desired translation\nnuances in accordance with style guidelines and linguistic considerations.\nThese models also excel in processing and applying prompts. However, the choice\nof language model, machine translation task, and the specific source and target\nlanguages necessitate certain considerations when adopting prompting frameworks\nand utilizing n-shot in-context learning.\n  Furthermore, we identified errors and limitations inherent in popular LLMs as\nmachine translation tools and categorized them based on various linguistic\nmetrics. This typology of errors provides valuable insights for utilizing LLMs\neffectively and offers methods for designing prompts for in-context learning.\nOur report aims to contribute to the advancement of machine translation with\nLLMs by improving both the accuracy and reliability of evaluation metrics.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "ACM-class: I.2.2",
      "I.2.2"
    ],
    "primary_category": "cs.CL",
    "comment": "34 pages, 46 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.08429v1",
    "published_date": "2024-01-16 15:16:34 UTC",
    "updated_date": "2024-01-16 15:16:34 UTC"
  },
  {
    "arxiv_id": "2401.08425v1",
    "title": "U-DIADS-Bib: a full and few-shot pixel-precise dataset for document layout analysis of ancient manuscripts",
    "authors": [
      "Silvia Zottin",
      "Axel De Nardin",
      "Emanuela Colombi",
      "Claudio Piciarelli",
      "Filippo Pavan",
      "Gian Luca Foresti"
    ],
    "abstract": "Document Layout Analysis, which is the task of identifying different semantic\nregions inside of a document page, is a subject of great interest for both\ncomputer scientists and humanities scholars as it represents a fundamental step\ntowards further analysis tasks for the former and a powerful tool to improve\nand facilitate the study of the documents for the latter. However, many of the\nworks currently present in the literature, especially when it comes to the\navailable datasets, fail to meet the needs of both worlds and, in particular,\ntend to lean towards the needs and common practices of the computer science\nside, leading to resources that are not representative of the humanities real\nneeds. For this reason, the present paper introduces U-DIADS-Bib, a novel,\npixel-precise, non-overlapping and noiseless document layout analysis dataset\ndeveloped in close collaboration between specialists in the fields of computer\nvision and humanities. Furthermore, we propose a novel, computer-aided,\nsegmentation pipeline in order to alleviate the burden represented by the\ntime-consuming process of manual annotation, necessary for the generation of\nthe ground truth segmentation maps. Finally, we present a standardized few-shot\nversion of the dataset (U-DIADS-BibFS), with the aim of encouraging the\ndevelopment of models and solutions able to address this task with as few\nsamples as possible, which would allow for more effective use in a real-world\nscenario, where collecting a large number of segmentations is not always\nfeasible.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Neural Comput & Applic (2024)",
    "pdf_url": "http://arxiv.org/pdf/2401.08425v1",
    "published_date": "2024-01-16 15:11:18 UTC",
    "updated_date": "2024-01-16 15:11:18 UTC"
  },
  {
    "arxiv_id": "2402.08611v1",
    "title": "A Cost-Sensitive Transformer Model for Prognostics Under Highly Imbalanced Industrial Data",
    "authors": [
      "Ali Beikmohammadi",
      "Mohammad Hosein Hamian",
      "Neda Khoeyniha",
      "Tony Lindgren",
      "Olof Steinert",
      "Sindri MagnÃºsson"
    ],
    "abstract": "The rapid influx of data-driven models into the industrial sector has been\nfacilitated by the proliferation of sensor technology, enabling the collection\nof vast quantities of data. However, leveraging these models for failure\ndetection and prognosis poses significant challenges, including issues like\nmissing values and class imbalances. Moreover, the cost sensitivity associated\nwith industrial operations further complicates the application of conventional\nmodels in this context. This paper introduces a novel cost-sensitive\ntransformer model developed as part of a systematic workflow, which also\nintegrates a hybrid resampler and a regression-based imputer. After subjecting\nour approach to rigorous testing using the APS failure dataset from Scania\ntrucks and the SECOM dataset, we observed a substantial enhancement in\nperformance compared to state-of-the-art methods. Moreover, we conduct an\nablation study to analyze the contributions of different components in our\nproposed method. Our findings highlight the potential of our method in\naddressing the unique challenges of failure prediction in industrial settings,\nthereby contributing to enhanced reliability and efficiency in industrial\noperations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.08611v1",
    "published_date": "2024-01-16 15:09:53 UTC",
    "updated_date": "2024-01-16 15:09:53 UTC"
  },
  {
    "arxiv_id": "2401.08405v3",
    "title": "Interrogating AI: Characterizing Emergent Playful Interactions with ChatGPT",
    "authors": [
      "Mohammad Ronagh Nikghalb",
      "Jinghui Cheng"
    ],
    "abstract": "In an era of AI's growing capabilities and influences, recent advancements\nare reshaping HCI and CSCW's view of AI. Playful interactions emerged as an\nimportant way for users to make sense of the ever-changing AI technologies, yet\nremained underexamined. We target this gap by investigating playful\ninteractions exhibited by users of a popular AI technology, ChatGPT. Through a\nthematic analysis of 372 user-generated posts on the ChatGPT subreddit, we\nfound that more than half (54\\%) of user discourse revolved around playful\ninteractions. The analysis further allowed us to construct a preliminary\nframework to describe these interactions, categorizing them into six types:\nreflecting, jesting, imitating, challenging, tricking, and contriving; each\nincluded sub-categories. This study contributes to HCI and CSCW by identifying\nthe diverse ways users engage in playful interactions with AI. It examines how\nthese interactions can help users understand AI's agency, shape human-AI\nrelationships, and provide insights for designing AI systems.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted to CSCW 2025; 23 pages",
    "pdf_url": "http://arxiv.org/pdf/2401.08405v3",
    "published_date": "2024-01-16 14:44:13 UTC",
    "updated_date": "2024-10-15 02:57:10 UTC"
  },
  {
    "arxiv_id": "2401.08396v4",
    "title": "Hidden flaws behind expert-level accuracy of multimodal GPT-4 vision in medicine",
    "authors": [
      "Qiao Jin",
      "Fangyuan Chen",
      "Yiliang Zhou",
      "Ziyang Xu",
      "Justin M. Cheung",
      "Robert Chen",
      "Ronald M. Summers",
      "Justin F. Rousseau",
      "Peiyun Ni",
      "Marc J Landsman",
      "Sally L. Baxter",
      "Subhi J. Al'Aref",
      "Yijia Li",
      "Alex Chen",
      "Josef A. Brejt",
      "Michael F. Chiang",
      "Yifan Peng",
      "Zhiyong Lu"
    ],
    "abstract": "Recent studies indicate that Generative Pre-trained Transformer 4 with Vision\n(GPT-4V) outperforms human physicians in medical challenge tasks. However,\nthese evaluations primarily focused on the accuracy of multi-choice questions\nalone. Our study extends the current scope by conducting a comprehensive\nanalysis of GPT-4V's rationales of image comprehension, recall of medical\nknowledge, and step-by-step multimodal reasoning when solving New England\nJournal of Medicine (NEJM) Image Challenges - an imaging quiz designed to test\nthe knowledge and diagnostic capabilities of medical professionals. Evaluation\nresults confirmed that GPT-4V performs comparatively to human physicians\nregarding multi-choice accuracy (81.6% vs. 77.8%). GPT-4V also performs well in\ncases where physicians incorrectly answer, with over 78% accuracy. However, we\ndiscovered that GPT-4V frequently presents flawed rationales in cases where it\nmakes the correct final choices (35.5%), most prominent in image comprehension\n(27.2%). Regardless of GPT-4V's high accuracy in multi-choice questions, our\nfindings emphasize the necessity for further in-depth evaluations of its\nrationales before integrating such multimodal AI models into clinical\nworkflows.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.08396v4",
    "published_date": "2024-01-16 14:41:20 UTC",
    "updated_date": "2024-08-31 23:51:14 UTC"
  },
  {
    "arxiv_id": "2401.08397v2",
    "title": "A Micro Architectural Events Aware Real-Time Embedded System Fault Injector",
    "authors": [
      "Enrico Magliano",
      "Alessio Carpegna",
      "Alessadro Savino",
      "Stefano Di Carlo"
    ],
    "abstract": "In contemporary times, the increasing complexity of the system poses\nsignificant challenges to the reliability, trustworthiness, and security of the\nSACRES. Key issues include the susceptibility to phenomena such as\ninstantaneous voltage spikes, electromagnetic interference, neutron strikes,\nand out-of-range temperatures. These factors can induce switch state changes in\ntransistors, resulting in bit-flipping, soft errors, and transient corruption\nof stored data in memory. The occurrence of soft errors, in turn, may lead to\nsystem faults that can propel the system into a hazardous state. Particularly\nin critical sectors like automotive, avionics, or aerospace, such malfunctions\ncan have real-world implications, potentially causing harm to individuals.\n  This paper introduces a novel fault injector designed to facilitate the\nmonitoring, aggregation, and examination of micro-architectural events. This is\nachieved by harnessing the microprocessor's PMU and the debugging interface,\nspecifically focusing on ensuring the repeatability of fault injections. The\nfault injection methodology targets bit-flipping within the memory system,\naffecting CPU registers and RAM. The outcomes of these fault injections enable\na thorough analysis of the impact of soft errors and establish a robust\ncorrelation between the identified faults and the essential timing\npredictability demanded by SACRES.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.08397v2",
    "published_date": "2024-01-16 14:41:20 UTC",
    "updated_date": "2024-06-11 08:44:00 UTC"
  },
  {
    "arxiv_id": "2401.08727v2",
    "title": "MA2GCN: Multi Adjacency relationship Attention Graph Convolutional Networks for Traffic Prediction using Trajectory data",
    "authors": [
      "Zhengke Sun",
      "Yuliang Ma"
    ],
    "abstract": "The problem of traffic congestion not only causes a large amount of economic\nlosses, but also seriously endangers the urban environment. Predicting traffic\ncongestion has important practical significance. So far, most studies have been\nbased on historical data from sensors placed on different roads to predict\nfuture traffic flow and speed, to analyze the traffic congestion conditions of\na certain road segment. However, due to the fixed position of sensors, it is\ndifficult to mine new information. On the other hand, vehicle trajectory data\nis more flexible and can extract traffic information as needed. Therefore, we\nproposed a new traffic congestion prediction model - Multi Adjacency\nrelationship Attention Graph Convolutional Networks(MA2GCN). This model\ntransformed vehicle trajectory data into graph structured data in grid form,\nand proposed a vehicle entry and exit matrix based on the mobility between\ndifferent grids. At the same time, in order to improve the performance of the\nmodel, this paper also built a new adaptive adjacency matrix generation method\nand adjacency matrix attention module. This model mainly used gated temporal\nconvolution and graph convolution to extract temporal and spatial information,\nrespectively. Compared with multiple baselines, our model achieved the best\nperformance on Shanghai taxi GPS trajectory dataset. The code is available at\nhttps://github.com/zachysun/Taxi_Traffic_Benchmark.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.08727v2",
    "published_date": "2024-01-16 14:22:44 UTC",
    "updated_date": "2024-01-18 05:25:22 UTC"
  },
  {
    "arxiv_id": "2401.08386v1",
    "title": "Deep Learning-based Group Causal Inference in Multivariate Time-series",
    "authors": [
      "Wasim Ahmad",
      "Maha Shadaydeh",
      "Joachim Denzler"
    ],
    "abstract": "Causal inference in a nonlinear system of multivariate timeseries is\ninstrumental in disentangling the intricate web of relationships among\nvariables, enabling us to make more accurate predictions and gain deeper\ninsights into real-world complex systems. Causality methods typically identify\nthe causal structure of a multivariate system by considering the cause-effect\nrelationship of each pair of variables while ignoring the collective effect of\na group of variables or interactions involving more than two-time series\nvariables. In this work, we test model invariance by group-level interventions\non the trained deep networks to infer causal direction in groups of variables,\nsuch as climate and ecosystem, brain networks, etc. Extensive testing with\nsynthetic and real-world time series data shows a significant improvement of\nour method over other applied group causality methods and provides us insights\ninto real-world time series. The code for our method can be found\nat:https://github.com/wasimahmadpk/gCause.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted in AAAI24 (AI4TS)",
    "pdf_url": "http://arxiv.org/pdf/2401.08386v1",
    "published_date": "2024-01-16 14:19:28 UTC",
    "updated_date": "2024-01-16 14:19:28 UTC"
  },
  {
    "arxiv_id": "2401.08383v2",
    "title": "Exploiting Inter-Layer Expert Affinity for Accelerating Mixture-of-Experts Model Inference",
    "authors": [
      "Jinghan Yao",
      "Quentin Anthony",
      "Aamir Shafi",
      "Hari Subramoni",
      "Dhabaleswar K.",
      "Panda"
    ],
    "abstract": "In large language models like the Generative Pre-trained Transformer, the\nMixture of Experts paradigm has emerged as a powerful technique for enhancing\nmodel expressiveness and accuracy. However, deploying GPT MoE models for\nparallel inference on distributed systems presents significant challenges,\nprimarily due to the extensive Alltoall communication required for expert\nrouting and aggregation. This communication bottleneck exacerbates the already\ncomplex computational landscape, hindering the efficient utilization of\nhigh-performance computing resources. In this paper, we propose a lightweight\noptimization technique called ExFlow, to largely accelerate the inference of\nthese MoE models. We take a new perspective on alleviating the communication\noverhead by exploiting the inter-layer expert affinity. Unlike previous\nmethods, our solution can be directly applied to pre-trained MoE models without\nany fine-tuning or accuracy degradation. By proposing a context-coherent expert\nparallelism on distributed systems, our design only uses one Alltoall\ncommunication to deliver the same functionality while previous methods all\nrequire two Alltoalls. By carefully examining the conditional probability in\ntokens' routing across multiple layers, we proved that pre-trained GPT MoE\nmodels implicitly exhibit a strong inter-layer expert affinity. We then design\nan efficient integer programming model to capture such features and show that\nby properly placing the experts on corresponding GPUs, we can reduce up to 67%\ncross-GPU routing latency. Our solution beats the cutting-edge MoE\nimplementations with experts from 8 to 64, with up to 2.2x improvement in\ninference throughput. We further provide a detailed study of how the model\nimplicitly acquires this expert affinity at the very early training stage and\nhow this affinity evolves and stabilizes during training.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.08383v2",
    "published_date": "2024-01-16 14:16:47 UTC",
    "updated_date": "2024-01-17 03:37:00 UTC"
  },
  {
    "arxiv_id": "2401.08376v1",
    "title": "KADEL: Knowledge-Aware Denoising Learning for Commit Message Generation",
    "authors": [
      "Wei Tao",
      "Yucheng Zhou",
      "Yanlin Wang",
      "Hongyu Zhang",
      "Haofen Wang",
      "Wenqiang Zhang"
    ],
    "abstract": "Commit messages are natural language descriptions of code changes, which are\nimportant for software evolution such as code understanding and maintenance.\nHowever, previous methods are trained on the entire dataset without considering\nthe fact that a portion of commit messages adhere to good practice (i.e.,\ngood-practice commits), while the rest do not. On the basis of our empirical\nstudy, we discover that training on good-practice commits significantly\ncontributes to the commit message generation. Motivated by this finding, we\npropose a novel knowledge-aware denoising learning method called KADEL.\nConsidering that good-practice commits constitute only a small proportion of\nthe dataset, we align the remaining training samples with these good-practice\ncommits. To achieve this, we propose a model that learns the commit knowledge\nby training on good-practice commits. This knowledge model enables\nsupplementing more information for training samples that do not conform to good\npractice. However, since the supplementary information may contain noise or\nprediction errors, we propose a dynamic denoising training method. This method\ncomposes a distribution-aware confidence function and a dynamic distribution\nlist, which enhances the effectiveness of the training process. Experimental\nresults on the whole MCMD dataset demonstrate that our method overall achieves\nstate-of-the-art performance compared with previous methods. Our source code\nand data are available at https://github.com/DeepSoftwareAnalytics/KADEL",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted to ACM Transactions on Software Engineering and Methodology\n  2024 (TOSEM'24)",
    "pdf_url": "http://arxiv.org/pdf/2401.08376v1",
    "published_date": "2024-01-16 14:07:48 UTC",
    "updated_date": "2024-01-16 14:07:48 UTC"
  },
  {
    "arxiv_id": "2401.12988v2",
    "title": "Few-Shot Learning for Mental Disorder Detection: A Continuous Multi-Prompt Engineering Approach with Medical Knowledge Injection",
    "authors": [
      "Haoxin Liu",
      "Wenli Zhang",
      "Jiaheng Xie",
      "Buomsoo Kim",
      "Zhu Zhang",
      "Yidong Chai",
      "Sudha Ram"
    ],
    "abstract": "This study harnesses state-of-the-art AI technology for detecting mental\ndisorders through user-generated textual content. Existing studies typically\nrely on fully supervised machine learning, which presents challenges such as\nthe labor-intensive manual process of annotating extensive training data for\neach research problem and the need to design specialized deep learning\narchitectures for each task. We propose a novel method to address these\nchallenges by leveraging large language models and continuous multi-prompt\nengineering, which offers two key advantages: (1) developing personalized\nprompts that capture each user's unique characteristics and (2) integrating\nstructured medical knowledge into prompts to provide context for disease\ndetection and facilitate predictive modeling. We evaluate our method using\nthree widely prevalent mental disorders as research cases. Our method\nsignificantly outperforms existing methods, including feature engineering,\narchitecture engineering, and discrete prompt engineering. Meanwhile, our\napproach demonstrates success in few-shot learning, i.e., requiring only a\nminimal number of training examples. Moreover, our method can be generalized to\nother rare mental disorder detection tasks with few positive labels. In\naddition to its technical contributions, our method has the potential to\nenhance the well-being of individuals with mental disorders and offer a\ncost-effective, accessible alternative for stakeholders beyond traditional\nmental disorder screening methods.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "K.5",
      "I.2.7; H.4.m"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.12988v2",
    "published_date": "2024-01-16 13:54:43 UTC",
    "updated_date": "2025-03-14 01:34:01 UTC"
  },
  {
    "arxiv_id": "2402.01669v2",
    "title": "Improved Performances and Motivation in Intelligent Tutoring Systems: Combining Machine Learning and Learner Choice",
    "authors": [
      "Benjamin ClÃ©ment",
      "HÃ©lÃ¨ne SauzÃ©on",
      "Didier Roy",
      "Pierre-Yves Oudeyer"
    ],
    "abstract": "Large class sizes challenge personalized learning in schools, prompting the\nuse of educational technologies such as intelligent tutoring systems. To\naddress this, we present an AI-driven personalization system, called ZPDES,\nbased on the Learning Progress Hypothesis - modeling curiosity-driven learning\n- and multi-armed bandit techniques. It sequences exercises that maximize\nlearning progress for each student. While previous studies demonstrated its\nefficacy in enhancing learning compared to hand-made curricula, its impact on\nstudent motivation remained unexplored. Furthermore, ZPDES previously lacked\nfeatures allowing student choice, a limitation in agency that conflicts with\nits foundation on models of curiosity-driven learning. This study investigates\nhow integrating choice, as a gamification element unrelated to exercise\ndifficulty, affects both learning outcomes and motivation. We conducted an\nextensive field study (265 7-8 years old children, RCT design), comparing ZPDES\nwith and without choice against a hand-designed curriculum. Results show that\nZPDES improves both learning performance and the learning experience. Moreover\nadding choice to ZPDES enhances intrinsic motivation and further strengthens\nits learning benefits. In contrast, incorporating choice into a fixed, linear\ncurriculum negatively impacts learning outcomes. These findings highlight that\nthe intrinsic motivation elicited by choice (gamification) is beneficial only\nwhen paired with an adaptive personalized learning system. This insight is\ncritical as gamified features become increasingly prevalent in educational\ntechnologies.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG",
      "I.2.1; I.2.6"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.01669v2",
    "published_date": "2024-01-16 13:41:00 UTC",
    "updated_date": "2025-03-05 08:23:02 UTC"
  },
  {
    "arxiv_id": "2401.08358v1",
    "title": "Hallucination Detection and Hallucination Mitigation: An Investigation",
    "authors": [
      "Junliang Luo",
      "Tianyu Li",
      "Di Wu",
      "Michael Jenkin",
      "Steve Liu",
      "Gregory Dudek"
    ],
    "abstract": "Large language models (LLMs), including ChatGPT, Bard, and Llama, have\nachieved remarkable successes over the last two years in a range of different\napplications. In spite of these successes, there exist concerns that limit the\nwide application of LLMs. A key problem is the problem of hallucination.\nHallucination refers to the fact that in addition to correct responses, LLMs\ncan also generate seemingly correct but factually incorrect responses. This\nreport aims to present a comprehensive review of the current literature on both\nhallucination detection and hallucination mitigation. We hope that this report\ncan serve as a good reference for both engineers and researchers who are\ninterested in LLMs and applying them to real world tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.08358v1",
    "published_date": "2024-01-16 13:36:07 UTC",
    "updated_date": "2024-01-16 13:36:07 UTC"
  },
  {
    "arxiv_id": "2401.08330v2",
    "title": "Boosting Gradient Ascent for Continuous DR-submodular Maximization",
    "authors": [
      "Qixin Zhang",
      "Zongqi Wan",
      "Zengde Deng",
      "Zaiyi Chen",
      "Xiaoming Sun",
      "Jialin Zhang",
      "Yu Yang"
    ],
    "abstract": "Projected Gradient Ascent (PGA) is the most commonly used optimization scheme\nin machine learning and operations research areas. Nevertheless, numerous\nstudies and examples have shown that the PGA methods may fail to achieve the\ntight approximation ratio for continuous DR-submodular maximization problems.\nTo address this challenge, we present a boosting technique in this paper, which\ncan efficiently improve the approximation guarantee of the standard PGA to\n\\emph{optimal} with only small modifications on the objective function. The\nfundamental idea of our boosting technique is to exploit non-oblivious search\nto derive a novel auxiliary function $F$, whose stationary points are excellent\napproximations to the global maximum of the original DR-submodular objective\n$f$. Specifically, when $f$ is monotone and $\\gamma$-weakly DR-submodular, we\npropose an auxiliary function $F$ whose stationary points can provide a better\n$(1-e^{-\\gamma})$-approximation than the\n$(\\gamma^2/(1+\\gamma^2))$-approximation guaranteed by the stationary points of\n$f$ itself. Similarly, for the non-monotone case, we devise another auxiliary\nfunction $F$ whose stationary points can achieve an optimal\n$\\frac{1-\\min_{\\boldsymbol{x}\\in\\mathcal{C}}\\|\\boldsymbol{x}\\|_{\\infty}}{4}$-approximation\nguarantee where $\\mathcal{C}$ is a convex constraint set. In contrast, the\nstationary points of the original non-monotone DR-submodular function can be\narbitrarily bad~\\citep{chen2023continuous}. Furthermore, we demonstrate the\nscalability of our boosting technique on four problems. In all of these four\nproblems, our resulting variants of boosting PGA algorithm beat the previous\nstandard PGA in several aspects such as approximation ratio and efficiency.\nFinally, we corroborate our theoretical findings with numerical experiments,\nwhich demonstrate the effectiveness of our boosting PGA methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "74 pages, 6 figures and 9 tables. An extended version of Stochastic\n  Continuous Submodular Maximization: Boosting via Non-oblivious Function (ICML\n  2022)",
    "pdf_url": "http://arxiv.org/pdf/2401.08330v2",
    "published_date": "2024-01-16 12:49:10 UTC",
    "updated_date": "2024-07-24 08:13:26 UTC"
  },
  {
    "arxiv_id": "2401.08326v3",
    "title": "RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning",
    "authors": [
      "Junjie Ye",
      "Yilong Wu",
      "Songyang Gao",
      "Caishuang Huang",
      "Sixian Li",
      "Guanyu Li",
      "Xiaoran Fan",
      "Qi Zhang",
      "Tao Gui",
      "Xuanjing Huang"
    ],
    "abstract": "Tool learning has generated widespread interest as a vital means of\ninteraction between Large Language Models (LLMs) and the physical world.\nCurrent research predominantly emphasizes LLMs' capacity to utilize tools in\nwell-structured environments while overlooking their stability when confronted\nwith the inevitable noise of the real world. To bridge this gap, we introduce\nRoTBench, a multi-level benchmark for evaluating the robustness of LLMs in tool\nlearning. Specifically, we establish five external environments, each featuring\nvarying levels of noise (i.e., Clean, Slight, Medium, Heavy, and Union),\nproviding an in-depth analysis of the model's resilience across three critical\nphases: tool selection, parameter identification, and content filling.\nExperiments involving six widely-used models underscore the urgent necessity\nfor enhancing the robustness of LLMs in tool learning. For instance, the\nperformance of GPT-4 even drops significantly from 80.00 to 58.10 when there is\nno substantial change in manual accuracy. More surprisingly, the noise\ncorrection capability inherent in the GPT family paradoxically impedes its\nadaptability in the face of mild noise. In light of these findings, we propose\nRoTTuning, a strategy that enriches the diversity of training environments to\nbolster the robustness of LLMs in tool learning. The code and data are\navailable at https://github.com/Junjie-Ye/RoTBench.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by EMNLP 2024 Main conference",
    "pdf_url": "http://arxiv.org/pdf/2401.08326v3",
    "published_date": "2024-01-16 12:45:15 UTC",
    "updated_date": "2024-09-21 08:03:33 UTC"
  },
  {
    "arxiv_id": "2403.12058v1",
    "title": "Water-Based Metaheuristics: How Water Dynamics Can Help Us to Solve NP-Hard Problems",
    "authors": [
      "Fernando Rubio",
      "Ismael RodrÃ­guez"
    ],
    "abstract": "Many water-based optimization metaheuristics have been introduced during the\nlast decade, both for combinatorial and for continuous optimization. Despite\nthe strong similarities of these methods in terms of their underlying natural\nmetaphors (most of them emulate, in some way or another, how drops\ncollaboratively form paths down to the sea), in general the resulting\nalgorithms are quite different in terms of their searching approach or their\nsolution construction approach. For instance, each entity may represent a\nsolution by itself or, alternatively, entities may construct solutions by\nmodifying the landscape while moving. A researcher or practitioner could assume\nthat the degree of similarity between two water-based metaheuristics heavily\ndepends on the similarity of the natural water mechanics they emulate, but this\nis not the case. In order to bring some clarity to this mosaic of apparently\nrelated metaheuristics, in this paper we introduce them, explain their\nmechanics, and highlight their differences.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "68T20",
      "I.2"
    ],
    "primary_category": "cs.NE",
    "comment": "14 pages, 0 figures, published in journal Complexity, 2019",
    "pdf_url": "http://arxiv.org/pdf/2403.12058v1",
    "published_date": "2024-01-16 11:39:42 UTC",
    "updated_date": "2024-01-16 11:39:42 UTC"
  },
  {
    "arxiv_id": "2401.08273v3",
    "title": "Large Language Models are Null-Shot Learners",
    "authors": [
      "Pittawat Taveekitworachai",
      "Febri Abdullah",
      "Ruck Thawonmas"
    ],
    "abstract": "This paper presents null-shot prompting. Null-shot prompting exploits\nhallucination in large language models (LLMs) by instructing LLMs to utilize\ninformation from the \"Examples\" section that never exists within the provided\ncontext to perform a task. While reducing hallucination is crucial and\nnon-negligible for daily and critical uses of LLMs, we propose that in the\ncurrent landscape in which these LLMs still hallucinate, it is possible, in\nfact, to exploit hallucination to increase performance in performing tasks\ncompared to standard zero-shot prompting. Experiments with eight LLMs show\nimprovements in performance across the majority of eight datasets, including\nreading comprehension, arithmetic reasoning, and closed-book question\nanswering. The observed inconsistency in increased relative performance across\nthe LLMs also potentially indicates a different degree of inherent\nhallucination in each model. These differences show that it is possible to\nutilize null-shot prompting as a way to detect degrees of hallucination in LLMs\nusing existing benchmarking datasets. We also perform ablation studies,\nincluding experimenting with a modified version of null-shot prompting that\nincorporates ideas from zero-shot chain-of-thought prompting, which shows\ndifferent trends of results.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "28 pages; v2: added Gemini Pro results, error analysis, and a\n  discussion on confabulation; v3: see its extended version, an EMNLP 2024\n  paper, at https://aclanthology.org/2024.emnlp-main.740/",
    "pdf_url": "http://arxiv.org/pdf/2401.08273v3",
    "published_date": "2024-01-16 10:53:11 UTC",
    "updated_date": "2024-11-16 04:23:20 UTC"
  },
  {
    "arxiv_id": "2401.08268v2",
    "title": "An Explainable Proxy Model for Multiabel Audio Segmentation",
    "authors": [
      "ThÃ©o Mariotte",
      "Antonio AlmudÃ©var",
      "Marie Tahon",
      "Alfonso Ortega"
    ],
    "abstract": "Audio signal segmentation is a key task for automatic audio indexing. It\nconsists of detecting the boundaries of class-homogeneous segments in the\nsignal. In many applications, explainable AI is a vital process for\ntransparency of decision-making with machine learning. In this paper, we\npropose an explainable multilabel segmentation model that solves speech\nactivity (SAD), music (MD), noise (ND), and overlapped speech detection (OSD)\nsimultaneously. This proxy uses the non-negative matrix factorization (NMF) to\nmap the embedding used for the segmentation to the frequency domain.\nExperiments conducted on two datasets show similar performances as the\npre-trained black box model while showing strong explainability features.\nSpecifically, the frequency bins used for the decision can be easily identified\nat both the segment level (local explanations) and global level (class\nprototypes).",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "eess.SP"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted at ICASSP 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.08268v2",
    "published_date": "2024-01-16 10:41:33 UTC",
    "updated_date": "2024-01-17 13:28:04 UTC"
  },
  {
    "arxiv_id": "2401.08261v2",
    "title": "Probabilistically Robust Watermarking of Neural Networks",
    "authors": [
      "Mikhail Pautov",
      "Nikita Bogdanov",
      "Stanislav Pyatkin",
      "Oleg Rogov",
      "Ivan Oseledets"
    ],
    "abstract": "As deep learning (DL) models are widely and effectively used in Machine\nLearning as a Service (MLaaS) platforms, there is a rapidly growing interest in\nDL watermarking techniques that can be used to confirm the ownership of a\nparticular model. Unfortunately, these methods usually produce watermarks\nsusceptible to model stealing attacks. In our research, we introduce a novel\ntrigger set-based watermarking approach that demonstrates resilience against\nfunctionality stealing attacks, particularly those involving extraction and\ndistillation. Our approach does not require additional model training and can\nbe applied to any model architecture. The key idea of our method is to compute\nthe trigger set, which is transferable between the source model and the set of\nproxy models with a high probability. In our experimental study, we show that\nif the probability of the set being transferable is reasonably high, it can be\neffectively used for ownership verification of the stolen model. We evaluate\nour method on multiple benchmarks and show that our approach outperforms\ncurrent state-of-the-art watermarking techniques in all considered experimental\nsetups.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.08261v2",
    "published_date": "2024-01-16 10:32:13 UTC",
    "updated_date": "2024-09-18 16:50:32 UTC"
  },
  {
    "arxiv_id": "2401.08255v1",
    "title": "A Generative Adversarial Attack for Multilingual Text Classifiers",
    "authors": [
      "Tom Roth",
      "Inigo Jauregi Unanue",
      "Alsharif Abuadbba",
      "Massimo Piccardi"
    ],
    "abstract": "Current adversarial attack algorithms, where an adversary changes a text to\nfool a victim model, have been repeatedly shown to be effective against text\nclassifiers. These attacks, however, generally assume that the victim model is\nmonolingual and cannot be used to target multilingual victim models, a\nsignificant limitation given the increased use of these models. For this\nreason, in this work we propose an approach to fine-tune a multilingual\nparaphrase model with an adversarial objective so that it becomes able to\ngenerate effective adversarial examples against multilingual classifiers. The\ntraining objective incorporates a set of pre-trained models to ensure text\nquality and language consistency of the generated text. In addition, all the\nmodels are suitably connected to the generator by vocabulary-mapping matrices,\nallowing for full end-to-end differentiability of the overall training\npipeline. The experimental validation over two multilingual datasets and five\nlanguages has shown the effectiveness of the proposed approach compared to\nexisting baselines, particularly in terms of query efficiency. We also provide\na detailed analysis of the generated attacks and discuss limitations and\nopportunities for future research.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "AAAI-24 Workshop on Artificial Intelligence for Cyber Security (AICS)",
    "pdf_url": "http://arxiv.org/pdf/2401.08255v1",
    "published_date": "2024-01-16 10:14:27 UTC",
    "updated_date": "2024-01-16 10:14:27 UTC"
  },
  {
    "arxiv_id": "2401.08221v1",
    "title": "Towards Causal Relationship in Indefinite Data: Baseline Model and New Datasets",
    "authors": [
      "Hang Chen",
      "Xinyu Yang",
      "Keqing Du"
    ],
    "abstract": "Integrating deep learning and causal discovery has encouraged us to spot that\nlearning causal structures and representations in dialogue and video is full of\nchallenges. We defined These data forms as \"Indefinite Data\", characterized by\nmulti-structure data and multi-value representations. Unlike existing adaptable\ndata forms, Indefinite Data still faces gaps in datasets and methods. To\naddress the dataset gap, we release two high-quality datasets - Causalogue and\nCausaction, containing text dialogue samples and video action samples with\ncausal annotations respectively. Moreover, the method gap arises from the\ncoexistence of multi-structure data and multi-value representations, breaking\nthe assumptions of all current methods and rendering them infeasible on\nIndefinite Data. To this end, we propose a probabilistic framework as a\nbaseline, incorporating three designed highlights for this gap: 1) establishing\nCausation Condition of representations using the independence of noise terms\nunder non-fixed causal structures, 2) treating causal strength as a latent\nvariable and measuring the reconstruction loss in the correlation space, and 3)\nestimating the effects of latent confounders. These highpoints make the\nprobabilistic model capable of overcoming challenges brought by the coexistence\nof multi-structure data and multi-value representations and pave the way for\nthe extension of latent confounders. Comprehensive experiments have evaluated\nbaseline results of causal structures, causal representations, and confounding\ndisentanglement.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "If you are interested in the two new datasets, pls contact us by\n  email",
    "pdf_url": "http://arxiv.org/pdf/2401.08221v1",
    "published_date": "2024-01-16 09:15:43 UTC",
    "updated_date": "2024-01-16 09:15:43 UTC"
  },
  {
    "arxiv_id": "2402.03337v1",
    "title": "Reinforcement-learning robotic sailboats: simulator and preliminary results",
    "authors": [
      "Eduardo Charles Vasconcellos",
      "Ronald M Sampaio",
      "AndrÃ© P D AraÃºjo",
      "Esteban Walter Gonzales Clua",
      "Philippe Preux",
      "Raphael Guerra",
      "Luiz M G GonÃ§alves",
      "Luis MartÃ­",
      "Hernan Lira",
      "Nayat Sanchez-Pi"
    ],
    "abstract": "This work focuses on the main challenges and problems in developing a virtual\noceanic environment reproducing real experiments using Unmanned Surface\nVehicles (USV) digital twins. We introduce the key features for building\nvirtual worlds, considering using Reinforcement Learning (RL) agents for\nautonomous navigation and control. With this in mind, the main problems concern\nthe definition of the simulation equations (physics and mathematics), their\neffective implementation, and how to include strategies for simulated control\nand perception (sensors) to be used with RL. We present the modeling,\nimplementation steps, and challenges required to create a functional digital\ntwin based on a real robotic sailing vessel. The application is immediate for\ndeveloping navigation algorithms based on RL to be applied on real boats.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.03337v1",
    "published_date": "2024-01-16 09:04:05 UTC",
    "updated_date": "2024-01-16 09:04:05 UTC"
  },
  {
    "arxiv_id": "2401.08721v1",
    "title": "A Telerehabilitation System for the Selection, Evaluation and Remote Management of Therapies",
    "authors": [
      "David Anton",
      "Idoia Berges",
      "JesÃºs BermÃºdez",
      "Alfredo GoÃ±i",
      "Arantza Illarramendi"
    ],
    "abstract": "Telerehabilitation systems that support physical therapy sessions anywhere\ncan help save healthcare costs while also improving the quality of life of the\nusers that need rehabilitation. The main contribution of this paper is to\npresent, as a whole, all the features supported by the innovative Kinect-based\nTelerehabilitation System (KiReS). In addition to the functionalities provided\nby current systems, it handles two new ones that could be incorporated into\nthem, in order to give a step forward towards a new generation of\ntelerehabilitation systems. The knowledge extraction functionality handles\nknowledge about the physical therapy record of patients and treatment protocols\ndescribed in an ontology, named TRHONT, to select the adequate exercises for\nthe rehabilitation of patients. The teleimmersion functionality provides a\nconvenient, effective and user-friendly experience when performing the\ntelerehabilitation, through a two-way real-time multimedia communication. The\nontology contains about 2300 classes and 100 properties, and the system allows\na reliable transmission of Kinect video depth, audio and skeleton data, being\nable to adapt to various network conditions. Moreover, the system has been\ntested with patients who suffered from shoulder disorders or total hip\nreplacement.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.08721v1",
    "published_date": "2024-01-16 08:35:36 UTC",
    "updated_date": "2024-01-16 08:35:36 UTC"
  },
  {
    "arxiv_id": "2401.08194v1",
    "title": "End-to-End Optimized Image Compression with the Frequency-Oriented Transform",
    "authors": [
      "Yuefeng Zhang",
      "Kai Lin"
    ],
    "abstract": "Image compression constitutes a significant challenge amidst the era of\ninformation explosion. Recent studies employing deep learning methods have\ndemonstrated the superior performance of learning-based image compression\nmethods over traditional codecs. However, an inherent challenge associated with\nthese methods lies in their lack of interpretability. Following an analysis of\nthe varying degrees of compression degradation across different frequency\nbands, we propose the end-to-end optimized image compression model facilitated\nby the frequency-oriented transform. The proposed end-to-end image compression\nmodel consists of four components: spatial sampling, frequency-oriented\ntransform, entropy estimation, and frequency-aware fusion. The\nfrequency-oriented transform separates the original image signal into distinct\nfrequency bands, aligning with the human-interpretable concept. Leveraging the\nnon-overlapping hypothesis, the model enables scalable coding through the\nselective transmission of arbitrary frequency components. Extensive experiments\nare conducted to demonstrate that our model outperforms all traditional codecs\nincluding next-generation standard H.266/VVC on MS-SSIM metric. Moreover,\nvisual analysis tasks (i.e., object detection and semantic segmentation) are\nconducted to verify the proposed compression method could preserve semantic\nfidelity besides signal-level precision.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "25 pages, accepted by MVAP",
    "pdf_url": "http://arxiv.org/pdf/2401.08194v1",
    "published_date": "2024-01-16 08:16:10 UTC",
    "updated_date": "2024-01-16 08:16:10 UTC"
  },
  {
    "arxiv_id": "2401.08189v4",
    "title": "PRewrite: Prompt Rewriting with Reinforcement Learning",
    "authors": [
      "Weize Kong",
      "Spurthi Amba Hombaiah",
      "Mingyang Zhang",
      "Qiaozhu Mei",
      "Michael Bendersky"
    ],
    "abstract": "Prompt engineering is critical for the development of LLM-based applications.\nHowever, it is usually done manually in a \"trial and error\" fashion that can be\ntime consuming, ineffective, and sub-optimal. Even for the prompts which\nseemingly work well, there is always a lingering question: can the prompts be\nmade better with further modifications?\n  To address these problems, we investigate automated prompt engineering in\nthis paper. Specifically, we propose PRewrite, an automated method to rewrite\nan under-optimized prompt to a more effective prompt. We instantiate the prompt\nrewriter using a LLM. The rewriter LLM is trained using reinforcement learning\nto optimize the performance on a given downstream task. We conduct experiments\non diverse benchmark datasets, which demonstrates the effectiveness of\nPRewrite.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.08189v4",
    "published_date": "2024-01-16 08:04:50 UTC",
    "updated_date": "2024-06-10 13:46:22 UTC"
  },
  {
    "arxiv_id": "2401.08185v1",
    "title": "DPAFNet:Dual Path Attention Fusion Network for Single Image Deraining",
    "authors": [
      "Bingcai Wei"
    ],
    "abstract": "Rainy weather will have a significant impact on the regular operation of the\nimaging system. Based on this premise, image rain removal has always been a\npopular branch of low-level visual tasks, especially methods using deep neural\nnetworks. However, most neural networks are but-branched, such as only using\nconvolutional neural networks or Transformers, which is unfavourable for the\nmultidimensional fusion of image features. In order to solve this problem, this\npaper proposes a dual-branch attention fusion network. Firstly, a two-branch\nnetwork structure is proposed. Secondly, an attention fusion module is proposed\nto selectively fuse the features extracted by the two branches rather than\nsimply adding them. Finally, complete ablation experiments and sufficient\ncomparison experiments prove the rationality and effectiveness of the proposed\nmethod.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.08185v1",
    "published_date": "2024-01-16 08:01:09 UTC",
    "updated_date": "2024-01-16 08:01:09 UTC"
  },
  {
    "arxiv_id": "2401.08138v1",
    "title": "LLMs for Test Input Generation for Semantic Caches",
    "authors": [
      "Zafaryab Rasool",
      "Scott Barnett",
      "David Willie",
      "Stefanus Kurniawan",
      "Sherwin Balugo",
      "Srikanth Thudumu",
      "Mohamed Abdelrazek"
    ],
    "abstract": "Large language models (LLMs) enable state-of-the-art semantic capabilities to\nbe added to software systems such as semantic search of unstructured documents\nand text generation. However, these models are computationally expensive. At\nscale, the cost of serving thousands of users increases massively affecting\nalso user experience. To address this problem, semantic caches are used to\ncheck for answers to similar queries (that may have been phrased differently)\nwithout hitting the LLM service. Due to the nature of these semantic cache\ntechniques that rely on query embeddings, there is a high chance of errors\nimpacting user confidence in the system. Adopting semantic cache techniques\nusually requires testing the effectiveness of a semantic cache (accurate cache\nhits and misses) which requires a labelled test set of similar queries and\nresponses which is often unavailable. In this paper, we present VaryGen, an\napproach for using LLMs for test input generation that produces similar\nquestions from unstructured text documents. Our novel approach uses the\nreasoning capabilities of LLMs to 1) adapt queries to the domain, 2) synthesise\nsubtle variations to queries, and 3) evaluate the synthesised test dataset. We\nevaluated our approach in the domain of a student question and answer system by\nqualitatively analysing 100 generated queries and result pairs, and conducting\nan empirical case study with an open source semantic cache. Our results show\nthat query pairs satisfy human expectations of similarity and our generated\ndata demonstrates failure cases of a semantic cache. Additionally, we also\nevaluate our approach on Qasper dataset. This work is an important first step\ninto test input generation for semantic applications and presents\nconsiderations for practitioners when calibrating a semantic cache.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted in International Conference on AI Engineering Software\n  Engineering (CAIN 2024)",
    "pdf_url": "http://arxiv.org/pdf/2401.08138v1",
    "published_date": "2024-01-16 06:16:33 UTC",
    "updated_date": "2024-01-16 06:16:33 UTC"
  },
  {
    "arxiv_id": "2401.08121v1",
    "title": "CycLight: learning traffic signal cooperation with a cycle-level strategy",
    "authors": [
      "Gengyue Han",
      "Xiaohan Liu",
      "Xianyue Peng",
      "Hao Wang",
      "Yu Han"
    ],
    "abstract": "This study introduces CycLight, a novel cycle-level deep reinforcement\nlearning (RL) approach for network-level adaptive traffic signal control\n(NATSC) systems. Unlike most traditional RL-based traffic controllers that\nfocus on step-by-step decision making, CycLight adopts a cycle-level strategy,\noptimizing cycle length and splits simultaneously using Parameterized Deep\nQ-Networks (PDQN) algorithm. This cycle-level approach effectively reduces the\ncomputational burden associated with frequent data communication, meanwhile\nenhancing the practicality and safety of real-world applications. A\ndecentralized framework is formulated for multi-agent cooperation, while\nattention mechanism is integrated to accurately assess the impact of the\nsurroundings on the current intersection. CycLight is tested in a large\nsynthetic traffic grid using the microscopic traffic simulation tool, SUMO.\nExperimental results not only demonstrate the superiority of CycLight over\nother state-of-the-art approaches but also showcase its robustness against\ninformation transmission delays.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.08121v1",
    "published_date": "2024-01-16 05:28:12 UTC",
    "updated_date": "2024-01-16 05:28:12 UTC"
  },
  {
    "arxiv_id": "2401.08117v1",
    "title": "E2HQV: High-Quality Video Generation from Event Camera via Theory-Inspired Model-Aided Deep Learning",
    "authors": [
      "Qiang Qu",
      "Yiran Shen",
      "Xiaoming Chen",
      "Yuk Ying Chung",
      "Tongliang Liu"
    ],
    "abstract": "The bio-inspired event cameras or dynamic vision sensors are capable of\nasynchronously capturing per-pixel brightness changes (called event-streams) in\nhigh temporal resolution and high dynamic range. However, the non-structural\nspatial-temporal event-streams make it challenging for providing intuitive\nvisualization with rich semantic information for human vision. It calls for\nevents-to-video (E2V) solutions which take event-streams as input and generate\nhigh quality video frames for intuitive visualization. However, current\nsolutions are predominantly data-driven without considering the prior knowledge\nof the underlying statistics relating event-streams and video frames. It highly\nrelies on the non-linearity and generalization capability of the deep neural\nnetworks, thus, is struggling on reconstructing detailed textures when the\nscenes are complex. In this work, we propose \\textbf{E2HQV}, a novel E2V\nparadigm designed to produce high-quality video frames from events. This\napproach leverages a model-aided deep learning framework, underpinned by a\ntheory-inspired E2V model, which is meticulously derived from the fundamental\nimaging principles of event cameras. To deal with the issue of state-reset in\nthe recurrent components of E2HQV, we also design a temporal shift embedding\nmodule to further improve the quality of the video frames. Comprehensive\nevaluations on the real world event camera datasets validate our approach, with\nE2HQV, notably outperforming state-of-the-art approaches, e.g., surpassing the\nsecond best by over 40\\% for some evaluation metrics.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM",
      "cs.NE"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted in AAAI2024",
    "pdf_url": "http://arxiv.org/pdf/2401.08117v1",
    "published_date": "2024-01-16 05:10:50 UTC",
    "updated_date": "2024-01-16 05:10:50 UTC"
  },
  {
    "arxiv_id": "2401.08115v2",
    "title": "No-Clean-Reference Image Super-Resolution: Application to Electron Microscopy",
    "authors": [
      "Mohammad Khateri",
      "Morteza Ghahremani",
      "Alejandra Sierra",
      "Jussi Tohka"
    ],
    "abstract": "The inability to acquire clean high-resolution (HR) electron microscopy (EM)\nimages over a large brain tissue volume hampers many neuroscience studies. To\naddress this challenge, we propose a deep-learning-based image super-resolution\n(SR) approach to computationally reconstruct clean HR 3D-EM with a large field\nof view (FoV) from noisy low-resolution (LR) acquisition. Our contributions are\nI) Investigating training with no-clean references for $\\ell_2$ and $\\ell_1$\nloss functions; II) Introducing a novel network architecture, named EMSR, for\nenhancing the resolution of LR EM images while reducing inherent noise; and,\nIII) Comparing different training strategies including using acquired LR and HR\nimage pairs, i.e., real pairs with no-clean references contaminated with real\ncorruptions, the pairs of synthetic LR and acquired HR, as well as acquired LR\nand denoised HR pairs. Experiments with nine brain datasets showed that\ntraining with real pairs can produce high-quality super-resolved results,\ndemonstrating the feasibility of training with non-clean references for both\nloss functions. Additionally, comparable results were observed, both visually\nand numerically, when employing denoised and noisy references for training.\nMoreover, utilizing the network trained with synthetically generated LR images\nfrom HR counterparts proved effective in yielding satisfactory SR results, even\nin certain cases, outperforming training with real pairs. The proposed SR\nnetwork was compared quantitatively and qualitatively with several established\nSR techniques, showcasing either the superiority or competitiveness of the\nproposed method in mitigating noise while recovering fine details.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "13 pages, 12 figures, and 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2401.08115v2",
    "published_date": "2024-01-16 05:05:08 UTC",
    "updated_date": "2024-01-26 11:27:15 UTC"
  },
  {
    "arxiv_id": "2401.08105v1",
    "title": "Hardware Acceleration for Real-Time Wildfire Detection Onboard Drone Networks",
    "authors": [
      "Austin Briley",
      "Fatemeh Afghah"
    ],
    "abstract": "Early wildfire detection in remote and forest areas is crucial for minimizing\ndevastation and preserving ecosystems. Autonomous drones offer agile access to\nremote, challenging terrains, equipped with advanced imaging technology that\ndelivers both high-temporal and detailed spatial resolution, making them\nvaluable assets in the early detection and monitoring of wildfires. However,\nthe limited computation and battery resources of Unmanned Aerial Vehicles\n(UAVs) pose significant challenges in implementing robust and efficient image\nclassification models. Current works in this domain often operate offline,\nemphasizing the need for solutions that can perform inference in real time,\ngiven the constraints of UAVs. To address these challenges, this paper aims to\ndevelop a real-time image classification and fire segmentation model. It\npresents a comprehensive investigation into hardware acceleration using the\nJetson Nano P3450 and the implications of TensorRT, NVIDIA's high-performance\ndeep-learning inference library, on fire classification accuracy and speed. The\nstudy includes implementations of Quantization Aware Training (QAT), Automatic\nMixed Precision (AMP), and post-training mechanisms, comparing them against the\nlatest baselines for fire segmentation and classification. All experiments\nutilize the FLAME dataset - an image dataset collected by low-altitude drones\nduring a prescribed forest fire. This work contributes to the ongoing efforts\nto enable real-time, on-board wildfire detection capabilities for UAVs,\naddressing speed and the computational and energy constraints of these crucial\nmonitoring systems. The results show a 13% increase in classification speed\ncompared to similar models without hardware optimization. Comparatively, loss\nand accuracy are within 1.225% of the original values.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "6 pages, 7 figures, NETROBOTICS conference submission",
    "pdf_url": "http://arxiv.org/pdf/2401.08105v1",
    "published_date": "2024-01-16 04:16:46 UTC",
    "updated_date": "2024-01-16 04:16:46 UTC"
  },
  {
    "arxiv_id": "2401.08103v5",
    "title": "Resolving Ethics Trade-offs in Implementing Responsible AI",
    "authors": [
      "Conrad Sanderson",
      "Emma Schleiger",
      "David Douglas",
      "Petra Kuhnert",
      "Qinghua Lu"
    ],
    "abstract": "While the operationalisation of high-level AI ethics principles into\npractical AI/ML systems has made progress, there is still a theory-practice gap\nin managing tensions between the underlying AI ethics aspects. We cover five\napproaches for addressing the tensions via trade-offs, ranging from rudimentary\nto complex. The approaches differ in the types of considered context, scope,\nmethods for measuring contexts, and degree of justification. None of the\napproaches is likely to be appropriate for all organisations, systems, or\napplications. To address this, we propose a framework which consists of: (i)\nproactive identification of tensions, (ii) prioritisation and weighting of\nethics aspects, (iii) justification and documentation of trade-off decisions.\nThe proposed framework aims to facilitate the implementation of well-rounded\nAI/ML systems that are appropriate for potential regulatory requirements.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "68T01",
      "K.4.1; I.2.m; C.4"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.08103v5",
    "published_date": "2024-01-16 04:14:23 UTC",
    "updated_date": "2024-12-24 12:25:21 UTC"
  },
  {
    "arxiv_id": "2401.12986v2",
    "title": "Crowdsourced Adaptive Surveys",
    "authors": [
      "Yamil Velez"
    ],
    "abstract": "Public opinion surveys are vital for informing democratic decision-making,\nbut responding to rapidly evolving information environments and measuring\nbeliefs within niche communities can be challenging for traditional survey\nmethods. This paper introduces a crowdsourced adaptive survey methodology\n(CSAS) that unites advances in natural language processing and adaptive\nalgorithms to generate question banks that evolve with user input. The CSAS\nmethod converts open-ended text provided by participants into survey items and\napplies a multi-armed bandit algorithm to determine which questions should be\nprioritized in the survey. The method's adaptive nature allows for the\nexploration of new survey questions, while imposing minimal costs in survey\nlength. Applications in the domains of Latino information environments,\nnational issue importance, and local politics showcase CSAS's ability to\nidentify topics that might otherwise escape the notice of survey researchers. I\nconclude by highlighting CSAS's potential to bridge conceptual gaps between\nresearchers and participants in survey research.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "stat.AP"
    ],
    "primary_category": "cs.CL",
    "comment": "18 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.12986v2",
    "published_date": "2024-01-16 04:05:25 UTC",
    "updated_date": "2024-12-06 19:42:55 UTC"
  },
  {
    "arxiv_id": "2401.08100v1",
    "title": "KTVIC: A Vietnamese Image Captioning Dataset on the Life Domain",
    "authors": [
      "Anh-Cuong Pham",
      "Van-Quang Nguyen",
      "Thi-Hong Vuong",
      "Quang-Thuy Ha"
    ],
    "abstract": "Image captioning is a crucial task with applications in a wide range of\ndomains, including healthcare and education. Despite extensive research on\nEnglish image captioning datasets, the availability of such datasets for\nVietnamese remains limited, with only two existing datasets. In this study, we\nintroduce KTVIC, a comprehensive Vietnamese Image Captioning dataset focused on\nthe life domain, covering a wide range of daily activities. This dataset\ncomprises 4,327 images and 21,635 Vietnamese captions, serving as a valuable\nresource for advancing image captioning in the Vietnamese language. We conduct\nexperiments using various deep neural networks as the baselines on our dataset,\nevaluating them using the standard image captioning metrics, including BLEU,\nMETEOR, CIDEr, and ROUGE. Our findings underscore the effectiveness of the\nproposed dataset and its potential contributions to the field of image\ncaptioning in the Vietnamese context.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.08100v1",
    "published_date": "2024-01-16 04:01:49 UTC",
    "updated_date": "2024-01-16 04:01:49 UTC"
  },
  {
    "arxiv_id": "2401.08099v1",
    "title": "Inpainting Normal Maps for Lightstage data",
    "authors": [
      "Hancheng Zuo",
      "Bernard Tiddeman"
    ],
    "abstract": "This study introduces a novel method for inpainting normal maps using a\ngenerative adversarial network (GAN). Normal maps, often derived from a\nlightstage, are crucial in performance capture but can have obscured areas due\nto movement (e.g., by arms, hair, or props). Inpainting fills these missing\nareas with plausible data. Our approach extends previous general image\ninpainting techniques, employing a bow tie-like generator network and a\ndiscriminator network, with alternating training phases. The generator aims to\nsynthesize images aligning with the ground truth and deceive the discriminator,\nwhich differentiates between real and processed images. Periodically, the\ndiscriminator undergoes retraining to enhance its ability to identify processed\nimages. Importantly, our method adapts to the unique characteristics of normal\nmap data, necessitating modifications to the loss function. We utilize a cosine\nloss instead of mean squared error loss for generator training. Limited\ntraining data availability, even with synthetic datasets, demands significant\naugmentation, considering the specific nature of the input data. This includes\nappropriate image flipping and in-plane rotations to accurately alter normal\nvectors. Throughout training, we monitored key metrics such as average loss,\nStructural Similarity Index Measure (SSIM), and Peak Signal-to-Noise Ratio\n(PSNR) for the generator, along with average loss and accuracy for the\ndiscriminator. Our findings suggest that the proposed model effectively\ngenerates high-quality, realistic inpainted normal maps, suitable for\nperformance capture applications. These results establish a foundation for\nfuture research, potentially involving more advanced networks and comparisons\nwith inpainting of source images used to create the normal maps.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "I.2.6; I.4.5"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 4 figures, CGVC Conference, The Eurographics Association",
    "pdf_url": "http://arxiv.org/pdf/2401.08099v1",
    "published_date": "2024-01-16 03:59:07 UTC",
    "updated_date": "2024-01-16 03:59:07 UTC"
  },
  {
    "arxiv_id": "2401.08097v4",
    "title": "Fairness Concerns in App Reviews: A Study on AI-based Mobile Apps",
    "authors": [
      "Ali Rezaei Nasab",
      "Maedeh Dashti",
      "Mojtaba Shahin",
      "Mansooreh Zahedi",
      "Hourieh Khalajzadeh",
      "Chetan Arora",
      "Peng Liang"
    ],
    "abstract": "Fairness is one of the socio-technical concerns that must be addressed in\nsoftware systems. Considering the popularity of mobile software applications\n(apps) among a wide range of individuals worldwide, mobile apps with unfair\nbehaviors and outcomes can affect a significant proportion of the global\npopulation, potentially more than any other type of software system. Users\nexpress a wide range of socio-technical concerns in mobile app reviews. This\nresearch aims to investigate fairness concerns raised in mobile app reviews.\nOur research focuses on AI-based mobile app reviews as the chance of unfair\nbehaviors and outcomes in AI-based mobile apps may be higher than in\nnon-AI-based apps. To this end, we first manually constructed a ground-truth\ndataset, including 1,132 fairness and 1,473 non-fairness reviews. Leveraging\nthe ground-truth dataset, we developed and evaluated a set of machine learning\nand deep learning models that distinguish fairness reviews from non-fairness\nreviews. Our experiments show that our best-performing model can detect\nfairness reviews with a precision of 94%. We then applied the best-performing\nmodel on approximately 9.5M reviews collected from 108 AI-based apps and\nidentified around 92K fairness reviews. Next, applying the K-means clustering\ntechnique to the 92K fairness reviews, followed by manual analysis, led to the\nidentification of six distinct types of fairness concerns (e.g., 'receiving\ndifferent quality of features and services in different platforms and devices'\nand 'lack of transparency and fairness in dealing with user-generated\ncontent'). Finally, the manual analysis of 2,248 app owners' responses to the\nfairness reviews identified six root causes (e.g., 'copyright issues') that app\nowners report to justify fairness concerns.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.SE",
    "comment": "Preprint accepted for publication in ACM Transactions on Software\n  Engineering and Methodology (TOSEM), 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.08097v4",
    "published_date": "2024-01-16 03:43:33 UTC",
    "updated_date": "2024-07-31 14:45:52 UTC"
  },
  {
    "arxiv_id": "2401.08095v4",
    "title": "DurFlex-EVC: Duration-Flexible Emotional Voice Conversion Leveraging Discrete Representations without Text Alignment",
    "authors": [
      "Hyung-Seok Oh",
      "Sang-Hoon Lee",
      "Deok-Hyeon Cho",
      "Seong-Whan Lee"
    ],
    "abstract": "Emotional voice conversion (EVC) involves modifying various acoustic\ncharacteristics, such as pitch and spectral envelope, to match a desired\nemotional state while preserving the speaker's identity. Existing EVC methods\noften rely on text transcriptions or time-alignment information and struggle to\nhandle varying speech durations effectively. In this paper, we propose\nDurFlex-EVC, a duration-flexible EVC framework that operates without the need\nfor text or alignment information. We introduce a unit aligner that models\ncontextual information by aligning speech with discrete units representing\ncontent, eliminating the need for text or speech-text alignment. Additionally,\nwe design a style autoencoder that effectively disentangles content and\nemotional style, allowing precise manipulation of the emotional characteristics\nof the speech. We further enhance emotional expressiveness through a\nhierarchical stylize encoder that applies the target emotional style at\nmultiple hierarchical levels, refining the stylization process to improve the\nnaturalness and expressiveness of the converted speech. Experimental results\nfrom subjective and objective evaluations demonstrate that our approach\noutperforms baseline models, effectively handling duration variability and\nenhancing emotional expressiveness in the converted speech.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "15 pages, 11 figures, 12 tables",
    "pdf_url": "http://arxiv.org/pdf/2401.08095v4",
    "published_date": "2024-01-16 03:39:35 UTC",
    "updated_date": "2025-01-21 02:51:53 UTC"
  },
  {
    "arxiv_id": "2401.08092v2",
    "title": "A Survey of Resource-efficient LLM and Multimodal Foundation Models",
    "authors": [
      "Mengwei Xu",
      "Wangsong Yin",
      "Dongqi Cai",
      "Rongjie Yi",
      "Daliang Xu",
      "Qipeng Wang",
      "Bingyang Wu",
      "Yihao Zhao",
      "Chen Yang",
      "Shihe Wang",
      "Qiyang Zhang",
      "Zhenyan Lu",
      "Li Zhang",
      "Shangguang Wang",
      "Yuanchun Li",
      "Yunxin Liu",
      "Xin Jin",
      "Xuanzhe Liu"
    ],
    "abstract": "Large foundation models, including large language models (LLMs), vision\ntransformers (ViTs), diffusion, and LLM-based multimodal models, are\nrevolutionizing the entire machine learning lifecycle, from training to\ndeployment. However, the substantial advancements in versatility and\nperformance these models offer come at a significant cost in terms of hardware\nresources. To support the growth of these large models in a scalable and\nenvironmentally sustainable way, there has been a considerable focus on\ndeveloping resource-efficient strategies. This survey delves into the critical\nimportance of such research, examining both algorithmic and systemic aspects.\nIt offers a comprehensive analysis and valuable insights gleaned from existing\nliterature, encompassing a broad array of topics from cutting-edge model\narchitectures and training/serving algorithms to practical system designs and\nimplementations. The goal of this survey is to provide an overarching\nunderstanding of how current approaches are tackling the resource challenges\nposed by large foundation models and to potentially inspire future\nbreakthroughs in this field.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.08092v2",
    "published_date": "2024-01-16 03:35:26 UTC",
    "updated_date": "2024-09-23 07:37:34 UTC"
  },
  {
    "arxiv_id": "2401.08089v1",
    "title": "A Study on Training and Developing Large Language Models for Behavior Tree Generation",
    "authors": [
      "Fu Li",
      "Xueying Wang",
      "Bin Li",
      "Yunlong Wu",
      "Yanzhen Wang",
      "Xiaodong Yi"
    ],
    "abstract": "This paper presents an innovative exploration of the application potential of\nlarge language models (LLM) in addressing the challenging task of automatically\ngenerating behavior trees (BTs) for complex tasks. The conventional manual BT\ngeneration method is inefficient and heavily reliant on domain expertise. On\nthe other hand, existing automatic BT generation technologies encounter\nbottlenecks related to task complexity, model adaptability, and reliability. In\norder to overcome these challenges, we propose a novel methodology that\nleverages the robust representation and reasoning abilities of LLMs. The core\ncontribution of this paper lies in the design of a BT generation framework\nbased on LLM, which encompasses the entire process, from data synthesis and\nmodel training to application developing and data verification. Synthetic data\nis introduced to train the BT generation model (BTGen model), enhancing its\nunderstanding and adaptability to various complex tasks, thereby significantly\nimproving its overall performance. In order to ensure the effectiveness and\nexecutability of the generated BTs, we emphasize the importance of data\nverification and introduce a multilevel verification strategy. Additionally, we\nexplore a range of agent design and development schemes with LLM as the central\nelement. We hope that the work in this paper may provide a reference for the\nresearchers who are interested in BT generation based on LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.08089v1",
    "published_date": "2024-01-16 03:28:29 UTC",
    "updated_date": "2024-01-16 03:28:29 UTC"
  },
  {
    "arxiv_id": "2401.08077v1",
    "title": "Transformer-based approach for Ethereum Price Prediction Using Crosscurrency correlation and Sentiment Analysis",
    "authors": [
      "Shubham Singh",
      "Mayur Bhat"
    ],
    "abstract": "The research delves into the capabilities of a transformer-based neural\nnetwork for Ethereum cryptocurrency price forecasting. The experiment runs\naround the hypothesis that cryptocurrency prices are strongly correlated with\nother cryptocurrencies and the sentiments around the cryptocurrency. The model\nemploys a transformer architecture for several setups from single-feature\nscenarios to complex configurations incorporating volume, sentiment, and\ncorrelated cryptocurrency prices. Despite a smaller dataset and less complex\narchitecture, the transformer model surpasses ANN and MLP counterparts on some\nparameters. The conclusion presents a hypothesis on the illusion of causality\nin cryptocurrency price movements driven by sentiments.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-fin.PR"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages",
    "pdf_url": "http://arxiv.org/pdf/2401.08077v1",
    "published_date": "2024-01-16 03:03:39 UTC",
    "updated_date": "2024-01-16 03:03:39 UTC"
  },
  {
    "arxiv_id": "2401.08066v1",
    "title": "Achieve Fairness without Demographics for Dermatological Disease Diagnosis",
    "authors": [
      "Ching-Hao Chiu",
      "Yu-Jen Chen",
      "Yawen Wu",
      "Yiyu Shi",
      "Tsung-Yi Ho"
    ],
    "abstract": "In medical image diagnosis, fairness has become increasingly crucial. Without\nbias mitigation, deploying unfair AI would harm the interests of the\nunderprivileged population and potentially tear society apart. Recent research\naddresses prediction biases in deep learning models concerning demographic\ngroups (e.g., gender, age, and race) by utilizing demographic (sensitive\nattribute) information during training. However, many sensitive attributes\nnaturally exist in dermatological disease images. If the trained model only\ntargets fairness for a specific attribute, it remains unfair for other\nattributes. Moreover, training a model that can accommodate multiple sensitive\nattributes is impractical due to privacy concerns. To overcome this, we propose\na method enabling fair predictions for sensitive attributes during the testing\nphase without using such information during training. Inspired by prior work\nhighlighting the impact of feature entanglement on fairness, we enhance the\nmodel features by capturing the features related to the sensitive and target\nattributes and regularizing the feature entanglement between corresponding\nclasses. This ensures that the model can only classify based on the features\nrelated to the target attribute without relying on features associated with\nsensitive attributes, thereby improving fairness and accuracy. Additionally, we\nuse disease masks from the Segment Anything Model (SAM) to enhance the quality\nof the learned feature. Experimental results demonstrate that the proposed\nmethod can improve fairness in classification compared to state-of-the-art\nmethods in two dermatological disease datasets.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.08066v1",
    "published_date": "2024-01-16 02:49:52 UTC",
    "updated_date": "2024-01-16 02:49:52 UTC"
  },
  {
    "arxiv_id": "2401.10286v3",
    "title": "Code-Based English Models Surprising Performance on Chinese QA Pair Extraction Task",
    "authors": [
      "Linghan Zheng",
      "Hui Liu",
      "Xiaojun Lin",
      "Jiayuan Dong",
      "Yue Sheng",
      "Gang Shi",
      "Zhiwei Liu",
      "Hongwei Chen"
    ],
    "abstract": "In previous studies, code-based models have consistently outperformed\ntext-based models in reasoning-intensive scenarios. When generating our\nknowledge base for Retrieval-Augmented Generation (RAG), we observed that\ncode-based models also perform exceptionally well in Chinese QA Pair Extraction\ntask. Further, our experiments and the metrics we designed discovered that\ncode-based models containing a certain amount of Chinese data achieve even\nbetter performance. Additionally, the capabilities of code-based English models\nin specified Chinese tasks offer a distinct perspective for discussion on the\nphilosophical \"Chinese Room\" thought experiment.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10286v3",
    "published_date": "2024-01-16 02:11:35 UTC",
    "updated_date": "2024-03-11 01:23:47 UTC"
  },
  {
    "arxiv_id": "2401.08046v1",
    "title": "Enhancing Robustness of LLM-Synthetic Text Detectors for Academic Writing: A Comprehensive Analysis",
    "authors": [
      "Zhicheng Dou",
      "Yuchen Guo",
      "Ching-Chun Chang",
      "Huy H. Nguyen",
      "Isao Echizen"
    ],
    "abstract": "The emergence of large language models (LLMs), such as Generative Pre-trained\nTransformer 4 (GPT-4) used by ChatGPT, has profoundly impacted the academic and\nbroader community. While these models offer numerous advantages in terms of\nrevolutionizing work and study methods, they have also garnered significant\nattention due to their potential negative consequences. One example is\ngenerating academic reports or papers with little to no human contribution.\nConsequently, researchers have focused on developing detectors to address the\nmisuse of LLMs. However, most existing methods prioritize achieving higher\naccuracy on restricted datasets, neglecting the crucial aspect of\ngeneralizability. This limitation hinders their practical application in\nreal-life scenarios where reliability is paramount. In this paper, we present a\ncomprehensive analysis of the impact of prompts on the text generated by LLMs\nand highlight the potential lack of robustness in one of the current\nstate-of-the-art GPT detectors. To mitigate these issues concerning the misuse\nof LLMs in academic writing, we propose a reference-based Siamese detector\nnamed Synthetic-Siamese which takes a pair of texts, one as the inquiry and the\nother as the reference. Our method effectively addresses the lack of robustness\nof previous detectors (OpenAI detector and DetectGPT) and significantly\nimproves the baseline performances in realistic academic writing scenarios by\napproximately 67% to 95%.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.08046v1",
    "published_date": "2024-01-16 01:58:36 UTC",
    "updated_date": "2024-01-16 01:58:36 UTC"
  },
  {
    "arxiv_id": "2401.08025v2",
    "title": "Self-Imagine: Effective Unimodal Reasoning with Multimodal Models using Self-Imagination",
    "authors": [
      "Syeda Nahida Akter",
      "Aman Madaan",
      "Sangwu Lee",
      "Yiming Yang",
      "Eric Nyberg"
    ],
    "abstract": "The potential of Vision-Language Models (VLMs) often remains underutilized in\nhandling complex text-based problems, particularly when these problems could\nbenefit from visual representation. Resonating with humans' ability to solve\ncomplex text-based problems by (1) creating a visual diagram from the problem\nand (2) deducing what steps they need to take to solve it, we propose\nSelf-Imagine. We leverage a single Vision-Language Model (VLM) to generate a\nstructured representation of the question using HTML, then render the HTML as\nan image, and finally use the same VLM to answer the question using both the\nquestion and the image. Our approach does not require any additional training\ndata or training. We evaluate our approach on three mathematics tasks and nine\ngeneral-purpose reasoning tasks using state-of-the-art (LLAVA-1.5 and GEMINI\nPRO) VLMs. Our approach boosts the performance of LLAVA-1.5 and GEMINI PRO on\nall math tasks (on average GSM8K: +3.1%; ASDIV: +3.2%; SVAMP: +6.9%) and the\nmajority of the general-purpose reasoning tasks by 3.2% to 6.0% on average.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "18 pages, 9 figures, 12 tables",
    "pdf_url": "http://arxiv.org/pdf/2401.08025v2",
    "published_date": "2024-01-16 00:46:29 UTC",
    "updated_date": "2024-02-21 22:11:19 UTC"
  },
  {
    "arxiv_id": "2401.08715v1",
    "title": "Selecting Subsets of Source Data for Transfer Learning with Applications in Metal Additive Manufacturing",
    "authors": [
      "Yifan Tang",
      "M. Rahmani Dehaghani",
      "Pouyan Sajadi",
      "G. Gary Wang"
    ],
    "abstract": "Considering data insufficiency in metal additive manufacturing (AM), transfer\nlearning (TL) has been adopted to extract knowledge from source domains (e.g.,\ncompleted printings) to improve the modeling performance in target domains\n(e.g., new printings). Current applications use all accessible source data\ndirectly in TL with no regard to the similarity between source and target data.\nThis paper proposes a systematic method to find appropriate subsets of source\ndata based on similarities between the source and target datasets for a given\nset of limited target domain data. Such similarity is characterized by the\nspatial and model distance metrics. A Pareto frontier-based source data\nselection method is developed, where the source data located on the Pareto\nfrontier defined by two similarity distance metrics are selected iteratively.\nThe method is integrated into an instance-based TL method (decision tree\nregression model) and a model-based TL method (fine-tuned artificial neural\nnetwork). Both models are then tested on several regression tasks in metal AM.\nComparison results demonstrate that 1) the source data selection method is\ngeneral and supports integration with various TL methods and distance metrics,\n2) compared with using all source data, the proposed method can find a small\nsubset of source data from the same domain with better TL performance in metal\nAM regression tasks involving different processes and machines, and 3) when\nmultiple source domains exist, the source data selection method could find the\nsubset from one source domain to obtain comparable or better TL performance\nthan the model constructed using data from all source domains.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "26 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.08715v1",
    "published_date": "2024-01-16 00:14:37 UTC",
    "updated_date": "2024-01-16 00:14:37 UTC"
  }
]
{
  "date": "2025-04-09",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-04-09 的 arXiv 中文 TLDR 快报！\n\n今天的 arXiv 更新聚焦于 AI 模型的优化、多模态学习和医疗图像分析等领域，亮点包括 LLM 在决策和医疗诊断中的安全与高效应用，以及知名学者如 Stuart Russell 的强化学习作品；令人印象深刻的论文有那些探索 LLM 安全性和医疗 AI 的创新方法，如 Trustworthy AI Must Account for Intersectionality 和 AssistanceZero，它们展示了 AI 在实际场景中的潜力。\n\n以下是今日值得关注的论文，我将相关主题归类，先讨论重要或话题度高的论文（如医疗 AI 和 LLM 优化），快速掠过较基础或工具性的内容。每篇论文的标题以中文 + 英文形式列出，重点突出核心贡献和发现。\n\n### 医疗 AI 与图像处理\n- **Zeus: Zero-shot LLM Instruction for Union Segmentation in Multimodal Medical Imaging（Zeus: 无监督 LLM 指令用于多模态医疗图像分割）**：该论文提出一个基于大型语言模型 (LLMs) 的框架，用于无监督的多模态医疗图像分割（如 MRI 和 CT），核心贡献是通过生成文本指令模拟放射学诊断过程，实现无需配对数据集的精确分割，实验显示其在临床诊断中优于传统基线。\n- **Identifying regions of interest in whole slide images of renal cell carcinoma（识别肾细胞癌全滑玻图像中的感兴趣区域）**：作者开发了一个自动化系统，使用 Dominant Rotated Local Binary Pattern (DRLBP) 纹理描述符和 SVM 分类器，准确识别肾癌图像中的关键区域，主要发现是其在小数据集上达到 99.17% 精度，提升了病理学诊断效率。\n- **ColonScopeX: Leveraging Explainable Expert Systems with Multimodal Data for Improved Early Diagnosis of Colorectal Cancer（ColonScopeX: 使用可解释专家系统和多模态数据提升结直肠癌早期诊断）**：这篇论文引入一个可解释 AI 框架，结合血样数据和患者元数据进行结直肠癌早期检测，核心贡献是通过 Savitzky-Golay 算法和 XAI 技术实现透明决策，支持作为筛查工具，显著提高了诊断准确性。\n\n这些医疗相关论文突出了 AI 在实际诊断中的应用潜力，尤其在处理多模态数据和解释性方面的创新，值得关注未来临床转化。\n\n### LLM 优化与安全\n- **Trustworthy AI Must Account for Intersectionality（可信 AI 必须考虑交叉性）**：知名学者 Stuart Russell 参与的作品，强调 AI 的公平性、隐私和鲁棒性需同时考虑多个维度，核心发现是通过系统分析不同方面间的权衡（如隐私保护可能放大偏差），提出整体视角来提升 AI 可靠性。\n- **HypoEval: Hypothesis-Guided Evaluation for Natural Language Generation（HypoEval: 假设引导的自然语言生成评估）**：论文提出一个假设引导的评估框架，使用 LLM 检查生成文本的假设一致性，主要贡献是仅需少量人类评估数据就实现高相关性评估，平均提升 11.86% 的性能，适用于 LLM 输出质量检查。\n- **FM-LoRA: Factorized Low-Rank Meta-Prompting for Continual Learning（FM-LoRA: 因子化低秩元提示用于持续学习）**：该工作解决 LLM 在连续任务中的灾难性遗忘问题，通过动态秩选择和元提示框架优化参数，核心发现是其在 ImageNet-R 等基准上提升 7% 准确率，同时避免参数膨胀。\n- **AssistanceZero: Scalably Solving Assistance Games（AssistanceZero: 可扩展解决辅助游戏）**：Stuart Russell 合著的论文，引入一种强化学习框架优化 AI 辅助决策，核心贡献是通过 AlphaZero 扩展处理不确定性，实现 Minecraft 任务中减少人类操作 54.3%，展示了 AI 与人类协作的潜力。\n\n这些 LLM 论文聚焦安全和持续学习，揭示了当前模型的局限性（如过拟合和遗忘），并提供实用方法，话题度高，因为它们直接影响 AI 伦理和部署。\n\n### 机器人与强化学习\n- **SkillWeaver: Web Agents can Self-Improve by Discovering and Honing Skills（SkillWeaver: 通过发现和精炼技能实现 Web 代理自提升）**：论文提出一个技能导向框架，让 Web 代理自动发现并优化技能，核心发现是其在 WebArena 上成功率提升 39.8%，强调代理的自适应性。\n- **Modeling Response Consistency in Multi-Agent LLM Systems: A Comparative Analysis of Shared and Separate Context Approaches（多代理 LLM 系统中的响应一致性建模: 共享与分离上下文的比较分析）**：作者引入 Response Consistency Index (RCI) 指标，比较上下文策略，核心贡献是优化多代理系统的可扩展性，实验显示其在噪声环境中提升响应一致性。\n\n其他论文，如 Petri nets 工具（论文6）和各种技术性方法（论文8、12、14等），虽然有贡献但相对基础，我这里快速掠过：它们主要提供工具或小改进，如 Petri nets 用于本体图（核心是 Python 工具实现），或文本分类的损失函数优化，对一般读者影响不大。\n\n总之，今天的更新强调 AI 在医疗和决策中的实际应用，LLM 安全问题值得深入探讨。如果您对特定领域感兴趣，建议优先查看上述论文！",
  "papers": [
    {
      "arxiv_id": "2504.07336v1",
      "title": "Zeus: Zero-shot LLM Instruction for Union Segmentation in Multimodal Medical Imaging",
      "title_zh": "翻译失败",
      "authors": [
        "Siyuan Dai",
        "Kai Ye",
        "Guodong Liu",
        "Haoteng Tang",
        "Liang Zhan"
      ],
      "abstract": "Medical image segmentation has achieved remarkable success through the\ncontinuous advancement of UNet-based and Transformer-based foundation\nbackbones. However, clinical diagnosis in the real world often requires\nintegrating domain knowledge, especially textual information. Conducting\nmultimodal learning involves visual and text modalities shown as a solution,\nbut collecting paired vision-language datasets is expensive and time-consuming,\nposing significant challenges. Inspired by the superior ability in numerous\ncross-modal tasks for Large Language Models (LLMs), we proposed a novel\nVision-LLM union framework to address the issues. Specifically, we introduce\nfrozen LLMs for zero-shot instruction generation based on corresponding medical\nimages, imitating the radiology scanning and report generation process. {To\nbetter approximate real-world diagnostic processes}, we generate more precise\ntext instruction from multimodal radiology images (e.g., T1-w or T2-w MRI and\nCT). Based on the impressive ability of semantic understanding and rich\nknowledge of LLMs. This process emphasizes extracting special features from\ndifferent modalities and reunion the information for the ultimate clinical\ndiagnostic. With generated text instruction, our proposed union segmentation\nframework can handle multimodal segmentation without prior collected\nvision-language datasets. To evaluate our proposed method, we conduct\ncomprehensive experiments with influential baselines, the statistical results\nand the visualized case study demonstrate the superiority of our novel method.}",
      "tldr_zh": "这篇论文提出了Zeus框架，一种基于Zero-shot LLM指令生成的技术，用于处理多模态医疗图像中的联合分割（Union Segmentation），旨在整合视觉和文本信息而无需预先收集配对的视觉-语言数据集。具体方法包括使用冻结的LLMs从医疗图像（如T1-w或T2-w MRI和CT）模仿放射学扫描过程生成精确文本指令，从而提取多模态特征并实现有效的分割。实验结果显示，Zeus框架在与基线模型的比较中表现出显著优势，统计数据和可视化案例证明了其在临床诊断中的superiority。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "21 pages, 4 figures, In Press by a journal",
      "pdf_url": "http://arxiv.org/pdf/2504.07336v1",
      "published_date": "2025-04-09 23:33:35 UTC",
      "updated_date": "2025-04-09 23:33:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:06:30.341616"
    },
    {
      "arxiv_id": "2504.07334v2",
      "title": "Objaverse++: Curated 3D Object Dataset with Quality Annotations",
      "title_zh": "Objaverse++：精选的三维物体数据集，带有高质量标注",
      "authors": [
        "Chendi Lin",
        "Heshan Liu",
        "Qunshu Lin",
        "Zachary Bright",
        "Shitao Tang",
        "Yihui He",
        "Minghao Liu",
        "Ling Zhu",
        "Cindy Le"
      ],
      "abstract": "This paper presents Objaverse++, a curated subset of Objaverse enhanced with\ndetailed attribute annotations by human experts. Recent advances in 3D content\ngeneration have been driven by large-scale datasets such as Objaverse, which\ncontains over 800,000 3D objects collected from the Internet. Although\nObjaverse represents the largest available 3D asset collection, its utility is\nlimited by the predominance of low-quality models. To address this limitation,\nwe manually annotate 10,000 3D objects with detailed attributes, including\naesthetic quality scores, texture color classifications, multi-object\ncomposition flags, transparency characteristics, etc. Then, we trained a neural\nnetwork capable of annotating the tags for the rest of the Objaverse dataset.\nThrough experiments and a user study on generation results, we demonstrate that\nmodels pre-trained on our quality-focused subset achieve better performance\nthan those trained on the larger dataset of Objaverse in image-to-3D generation\ntasks. In addition, by comparing multiple subsets of training data filtered by\nour tags, our results show that the higher the data quality, the faster the\ntraining loss converges. These findings suggest that careful curation and rich\nannotation can compensate for the raw dataset size, potentially offering a more\nefficient path to develop 3D generative models. We release our enhanced dataset\nof approximately 500,000 curated 3D models to facilitate further research on\nvarious downstream tasks in 3D computer vision. In the near future, we aim to\nextend our annotations to cover the entire Objaverse dataset.",
      "tldr_zh": "本文介绍了 Objaverse++，这是一个对原始 Objaverse 数据集进行精选并添加详细属性注释的 3D 对象子集，由人类专家手动标注了 10,000 个对象的美学质量分数、纹理颜色分类、多对象组成标志和透明度特征等。作者随后训练了一个 neural network 来自动为剩余数据集添加标签。实验和用户研究显示，使用该质量聚焦子集预训练的模型在 image-to-3D generation 任务中比使用整个 Objaverse 数据集的模型性能提升，且数据质量越高，训练损失收敛越快。这些发现表明，精细的 curation 和丰富注释可以补偿数据集规模，提供更高效的 3D 生成模型开发路径，并已发布约 50 万个精选 3D 模型以支持下游 3D 计算机视觉研究。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "68T45, 68T07",
        "I.2.10; I.3.5; I.3.7; I.4.8; I.5.1"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 8 figures. Accepted to CVPR 2025 Workshop on Efficient Large\n  Vision Models (April 2025)",
      "pdf_url": "http://arxiv.org/pdf/2504.07334v2",
      "published_date": "2025-04-09 23:29:08 UTC",
      "updated_date": "2025-04-11 23:48:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:06:42.906004"
    },
    {
      "arxiv_id": "2504.07313v1",
      "title": "Identifying regions of interest in whole slide images of renal cell carcinoma",
      "title_zh": "翻译失败",
      "authors": [
        "Mohammed Lamine Benomar",
        "Nesma Settouti",
        "Eric Debreuve",
        "Xavier Descombes",
        "Damien Ambrosetti"
      ],
      "abstract": "The histopathological images contain a huge amount of information, which can\nmake diagnosis an extremely timeconsuming and tedious task. In this study, we\ndeveloped a completely automated system to detect regions of interest (ROIs) in\nwhole slide images (WSI) of renal cell carcinoma (RCC), to reduce time analysis\nand assist pathologists in making more accurate decisions. The proposed\napproach is based on an efficient texture descriptor named dominant rotated\nlocal binary pattern (DRLBP) and color transformation to reveal and exploit the\nimmense texture variability at the microscopic high magnifications level.\nThereby, the DRLBPs retain the structural information and utilize the magnitude\nvalues in a local neighborhood for more discriminative power. For the\nclassification of the relevant ROIs, feature extraction of WSIs patches was\nperformed on the color channels separately to form the histograms. Next, we\nused the most frequently occurring patterns as a feature selection step to\ndiscard non-informative features. The performances of different classifiers on\na set of 1800 kidney cancer patches originating from 12 whole slide images were\ncompared and evaluated. Furthermore, the small size of the image dataset allows\nto investigate deep learning approach based on transfer learning for image\npatches classification by using deep features and fine-tuning methods. High\nrecognition accuracy was obtained and the classifiers are efficient, the best\nprecision result was 99.17% achieved with SVM. Moreover, transfer learning\nmodels perform well with comparable performance, and the highest precision\nusing ResNet-50 reached 98.50%. The proposed approach results revealed a very\nefficient image classification and demonstrated efficacy in identifying ROIs.\nThis study presents an automatic system to detect regions of interest relevant\nto the diagnosis of kidney cancer in whole slide histopathology images.",
      "tldr_zh": "本研究开发了一个完全自动化的系统，用于在肾细胞癌 (RCC) 的全滑片图像 (WSI) 中识别感兴趣区域 (ROIs)，以减少病理学家的分析时间并提高诊断准确性。系统基于 dominant rotated local binary pattern (DRLBP) 纹理描述符和颜色转换，提取微观高倍放大的纹理特征，并通过特征选择和分类器（如 SVM）对图像补丁进行处理。实验结果显示，该方法在 1800 个肾癌补丁上实现了最高精度达 99.17% 的表现，而基于迁移学习的 ResNet-50 模型也达到了 98.50% 的精度。该系统证明了其在高效识别 ROIs 和辅助肾癌诊断方面的有效性。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.07313v1",
      "published_date": "2025-04-09 22:28:26 UTC",
      "updated_date": "2025-04-09 22:28:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:06:53.765431"
    },
    {
      "arxiv_id": "2504.07304v1",
      "title": "PAYADOR: A Minimalist Approach to Grounding Language Models on Structured Data for Interactive Storytelling and Role-playing Games",
      "title_zh": "翻译失败",
      "authors": [
        "Santiago Góngora",
        "Luis Chiruzzo",
        "Gonzalo Méndez",
        "Pablo Gervás"
      ],
      "abstract": "Every time an Interactive Storytelling (IS) system gets a player input, it is\nfacing the world-update problem. Classical approaches to this problem consist\nin mapping that input to known preprogrammed actions, what can severely\nconstrain the free will of the player. When the expected experience has a\nstrong focus on improvisation, like in Role-playing Games (RPGs), this problem\nis critical. In this paper we present PAYADOR, a different approach that\nfocuses on predicting the outcomes of the actions instead of representing the\nactions themselves. To implement this approach, we ground a Large Language\nModel to a minimal representation of the fictional world, obtaining promising\nresults. We make this contribution open-source, so it can be adapted and used\nfor other related research on unleashing the co-creativity power of RPGs.",
      "tldr_zh": "这篇论文介绍了 PAYADOR，一种极简方法，通过将 Large Language Model 接地到结构化数据上，来解决 Interactive Storytelling (IS) 系统中的世界更新问题。不同于传统方法，PAYADOR 专注于预测玩家动作的结果而非预编程动作，从而增强玩家的自由意志，尤其适用于需要即兴创作的 Role-playing Games (RPGs)。实验结果显示该方法取得了有前景的效果，并开源以促进 RPGs 共同创造力的相关研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Presented at the 15th International Conference on Computational\n  Creativity (ICCC'24)",
      "pdf_url": "http://arxiv.org/pdf/2504.07304v1",
      "published_date": "2025-04-09 21:59:31 UTC",
      "updated_date": "2025-04-09 21:59:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:07:04.668486"
    },
    {
      "arxiv_id": "2504.07303v1",
      "title": "Modeling Response Consistency in Multi-Agent LLM Systems: A Comparative Analysis of Shared and Separate Context Approaches",
      "title_zh": "多智能体 LLM 系统中的响应一致性建模：共享与分离上下文方法的比较分析",
      "authors": [
        "Tooraj Helmi"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly utilized in multi-agent systems\n(MAS) to enhance collaborative problem-solving and interactive reasoning.\nRecent advancements have enabled LLMs to function as autonomous agents capable\nof understanding complex interactions across multiple topics. However,\ndeploying LLMs in MAS introduces challenges related to context management,\nresponse consistency, and scalability, especially when agents must operate\nunder memory limitations and handle noisy inputs. While prior research has\nexplored optimizing context sharing and response latency in LLM-driven MAS,\nthese efforts often focus on either fully centralized or decentralized\nconfigurations, each with distinct trade-offs.\n  In this paper, we develop a probabilistic framework to analyze the impact of\nshared versus separate context configurations on response consistency and\nresponse times in LLM-based MAS. We introduce the Response Consistency Index\n(RCI) as a metric to evaluate the effects of context limitations, noise, and\ninter-agent dependencies on system performance. Our approach differs from\nexisting research by focusing on the interplay between memory constraints and\nnoise management, providing insights into optimizing scalability and response\ntimes in environments with interdependent topics. Through this analysis, we\noffer a comprehensive understanding of how different configurations impact the\nefficiency of LLM-driven multi-agent systems, thereby guiding the design of\nmore robust architectures.",
      "tldr_zh": "本研究比较了在多智能体 LLM 系统（Multi-Agent LLM Systems）中，共享上下文和分离上下文方法对响应一致性的影响，针对上下文管理、响应一致性和可扩展性等挑战进行分析。作者开发了一个概率框架，引入了 Response Consistency Index (RCI) 作为指标，评估上下文限制、噪声输入和代理间依赖对系统性能的影响。结果显示，该框架揭示了内存约束与噪声管理之间的相互作用，为优化 LLM 驱动的多智能体系统的可扩展性和响应时间提供了关键见解，从而指导更稳健架构的设计。",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.07303v1",
      "published_date": "2025-04-09 21:54:21 UTC",
      "updated_date": "2025-04-09 21:54:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:07:17.881005"
    },
    {
      "arxiv_id": "2504.08006v1",
      "title": "A Python toolkit for dealing with Petri nets over ontological graphs",
      "title_zh": "翻译失败",
      "authors": [
        "Krzysztof Pancerz"
      ],
      "abstract": "We present theoretical rudiments of Petri nets over ontological graphs as\nwell as the designed and implemented Python toolkit for dealing with such nets.\nIn Petri nets over ontological graphs, the domain knowledge is enclosed in a\nform of ontologies. In this way, some valuable knowledge (especially in terms\nof semantic relations) can be added to model reasoning and control processes by\nmeans of Petri nets. In the implemented approach, ontological graphs are\nobtained from ontologies built in accordance with the OWL 2 Web Ontology\nLanguage. The implemented tool enables the users to define the structure and\ndynamics of Petri nets over ontological graphs.",
      "tldr_zh": "这篇论文介绍了Petri nets over ontological graphs的理论基础，并开发了一个Python工具包来处理这些网。该方法将领域知识以ontologies的形式融入Petri nets中，利用OWL 2 Web Ontology Language构建的本体，添加语义关系以增强模型的推理和控制过程。该工具允许用户定义和动态管理这些网的结构，为知识表示和建模提供更灵活的解决方案。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "PP-RAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2504.08006v1",
      "published_date": "2025-04-09 21:52:17 UTC",
      "updated_date": "2025-04-09 21:52:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:07:28.242883"
    },
    {
      "arxiv_id": "2504.07278v1",
      "title": "A Multi-Phase Analysis of Blood Culture Stewardship: Machine Learning Prediction, Expert Recommendation Assessment, and LLM Automation",
      "title_zh": "翻译失败",
      "authors": [
        "Fatemeh Amrollahi",
        "Nicholas Marshall",
        "Fateme Nateghi Haredasht",
        "Kameron C Black",
        "Aydin Zahedivash",
        "Manoj V Maddali",
        "Stephen P. Ma",
        "Amy Chang",
        "MD Phar Stanley C Deresinski",
        "Mary Kane Goldstein",
        "Steven M. Asch",
        "Niaz Banaei",
        "Jonathan H Chen"
      ],
      "abstract": "Blood cultures are often over ordered without clear justification, straining\nhealthcare resources and contributing to inappropriate antibiotic use pressures\nworsened by the global shortage. In study of 135483 emergency department (ED)\nblood culture orders, we developed machine learning (ML) models to predict the\nrisk of bacteremia using structured electronic health record (EHR) data and\nprovider notes via a large language model (LLM). The structured models AUC\nimproved from 0.76 to 0.79 with note embeddings and reached 0.81 with added\ndiagnosis codes. Compared to an expert recommendation framework applied by\nhuman reviewers and an LLM-based pipeline, our ML approach offered higher\nspecificity without compromising sensitivity. The recommendation framework\nachieved sensitivity 86%, specificity 57%, while the LLM maintained high\nsensitivity (96%) but over classified negatives, reducing specificity (16%).\nThese findings demonstrate that ML models integrating structured and\nunstructured data can outperform consensus recommendations, enhancing\ndiagnostic stewardship beyond existing standards of care.",
      "tldr_zh": "本文研究了血培养过度订购的问题，通过机器学习(ML)模型预测菌血症风险，基于135483个急诊部(ED)订单的数据，包括结构化电子健康记录(EHR)和提供者笔记。该ML模型整合笔记嵌入和诊断代码，AUC从0.76提升至0.81，与专家推荐框架（敏感性86%、特异性57%）和LLM方法（敏感性96%、特异性16%）相比，提供更高特异性而不牺牲敏感性。这些发现证明，整合结构化和非结构化数据的ML模型能超越共识推荐，提升诊断管理标准。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 2 figures, 2 tables, conference",
      "pdf_url": "http://arxiv.org/pdf/2504.07278v1",
      "published_date": "2025-04-09 21:12:29 UTC",
      "updated_date": "2025-04-09 21:12:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:07:42.337710"
    },
    {
      "arxiv_id": "2504.07273v1",
      "title": "Evaluating Parameter-Based Training Performance of Neural Networks and Variational Quantum Circuits",
      "title_zh": "评估神经网络和变分量子电路的基于参数训练性能",
      "authors": [
        "Michael Kölle",
        "Alexander Feist",
        "Jonas Stein",
        "Sebastian Wölckert",
        "Claudia Linnhoff-Popien"
      ],
      "abstract": "In recent years, neural networks (NNs) have driven significant advances in\nmachine learning. However, as tasks grow more complex, NNs often require large\nnumbers of trainable parameters, which increases computational and energy\ndemands. Variational quantum circuits (VQCs) offer a promising alternative:\nthey leverage quantum mechanics to capture intricate relationships and\ntypically need fewer parameters. In this work, we evaluate NNs and VQCs on\nsimple supervised and reinforcement learning tasks, examining models with\ndifferent parameter sizes. We simulate VQCs and execute selected parts of the\ntraining process on real quantum hardware to approximate actual training times.\nOur results show that VQCs can match NNs in performance while using\nsignificantly fewer parameters, despite longer training durations. As quantum\ntechnology and algorithms advance, and VQC architectures improve, we posit that\nVQCs could become advantageous for certain machine learning tasks.",
      "tldr_zh": "本研究评估了神经网络(NNs)和变分量子电路(VQCs)在简单监督学习和强化学习任务中的训练性能，重点比较了不同参数规模下的计算需求和效率。研究方法包括模拟VQCs并在真实量子硬件上执行部分训练过程，以模拟实际训练时间。结果显示，VQCs能够在性能上与NNs匹敌，同时使用显著更少的参数，尽管训练时间较长；随着量子技术和算法的进步，VQCs可能在某些机器学习任务中更具优势。",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "quant-ph",
      "comment": "Accepted at ICCS 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.07273v1",
      "published_date": "2025-04-09 21:00:41 UTC",
      "updated_date": "2025-04-09 21:00:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:07:54.529793"
    },
    {
      "arxiv_id": "2504.08824v1",
      "title": "ColonScopeX: Leveraging Explainable Expert Systems with Multimodal Data for Improved Early Diagnosis of Colorectal Cancer",
      "title_zh": "翻译失败",
      "authors": [
        "Natalia Sikora",
        "Robert L. Manschke",
        "Alethea M. Tang",
        "Peter Dunstan",
        "Dean A. Harris",
        "Su Yang"
      ],
      "abstract": "Colorectal cancer (CRC) ranks as the second leading cause of cancer-related\ndeaths and the third most prevalent malignant tumour worldwide. Early detection\nof CRC remains problematic due to its non-specific and often embarrassing\nsymptoms, which patients frequently overlook or hesitate to report to\nclinicians. Crucially, the stage at which CRC is diagnosed significantly\nimpacts survivability, with a survival rate of 80-95\\% for Stage I and a stark\ndecline to 10\\% for Stage IV. Unfortunately, in the UK, only 14.4\\% of cases\nare diagnosed at the earliest stage (Stage I).\n  In this study, we propose ColonScopeX, a machine learning framework utilizing\nexplainable AI (XAI) methodologies to enhance the early detection of CRC and\npre-cancerous lesions. Our approach employs a multimodal model that integrates\nsignals from blood sample measurements, processed using the Savitzky-Golay\nalgorithm for fingerprint smoothing, alongside comprehensive patient metadata,\nincluding medication history, comorbidities, age, weight, and BMI. By\nleveraging XAI techniques, we aim to render the model's decision-making process\ntransparent and interpretable, thereby fostering greater trust and\nunderstanding in its predictions. The proposed framework could be utilised as a\ntriage tool or a screening tool of the general population.\n  This research highlights the potential of combining diverse patient data\nsources and explainable machine learning to tackle critical challenges in\nmedical diagnostics.",
      "tldr_zh": "结直肠癌（Colorectal Cancer）是全球第二大癌症致死原因，早诊率低（如英国仅14.4%病例在Stage I诊断），本文提出ColonScopeX框架，利用Explainable AI (XAI) 和多模态模型提升早期检测。框架整合血液样本测量（通过Savitzky-Golay算法进行平滑处理）以及患者元数据（如药物史、共病、年龄、体重和BMI），使模型决策过程透明并易解释。ColonScopeX可作为分诊或筛查工具，提高诊断准确性并增强患者信任。该研究突显了结合多样数据源和可解释机器学习在医疗诊断中的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.HC",
        "stat.AP"
      ],
      "primary_category": "cs.LG",
      "comment": "Published to AAAI-25 Bridge Program",
      "pdf_url": "http://arxiv.org/pdf/2504.08824v1",
      "published_date": "2025-04-09 20:45:11 UTC",
      "updated_date": "2025-04-09 20:45:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:08:06.200798"
    },
    {
      "arxiv_id": "2504.07257v1",
      "title": "Better Decisions through the Right Causal World Model",
      "title_zh": "通过正确的因果世界模型实现更好的决策",
      "authors": [
        "Elisabeth Dillies",
        "Quentin Delfosse",
        "Jannis Blüml",
        "Raban Emunds",
        "Florian Peter Busch",
        "Kristian Kersting"
      ],
      "abstract": "Reinforcement learning (RL) agents have shown remarkable performances in\nvarious environments, where they can discover effective policies directly from\nsensory inputs. However, these agents often exploit spurious correlations in\nthe training data, resulting in brittle behaviours that fail to generalize to\nnew or slightly modified environments. To address this, we introduce the Causal\nObject-centric Model Extraction Tool (COMET), a novel algorithm designed to\nlearn the exact interpretable causal world models (CWMs). COMET first extracts\nobject-centric state descriptions from observations and identifies the\nenvironment's internal states related to the depicted objects' properties.\nUsing symbolic regression, it models object-centric transitions and derives\ncausal relationships governing object dynamics. COMET further incorporates\nlarge language models (LLMs) for semantic inference, annotating causal\nvariables to enhance interpretability.\n  By leveraging these capabilities, COMET constructs CWMs that align with the\ntrue causal structure of the environment, enabling agents to focus on\ntask-relevant features. The extracted CWMs mitigate the danger of shortcuts,\npermitting the development of RL systems capable of better planning and\ndecision-making across dynamic scenarios. Our results, validated in Atari\nenvironments such as Pong and Freeway, demonstrate the accuracy and robustness\nof COMET, highlighting its potential to bridge the gap between object-centric\nreasoning and causal inference in reinforcement learning.",
      "tldr_zh": "本研究针对强化学习（RL）代理在环境中利用虚假相关性导致行为脆弱和泛化能力差的问题，引入了Causal Object-centric Model Extraction Tool（COMET）算法，以学习精确的、可解释的因果世界模型（CWMs）。COMET首先从观察中提取对象中心的状态描述，并使用符号回归（Symbolic Regression）建模对象动态的转换，同时整合大型语言模型（LLMs）进行语义推理，以标注因果变量并提升模型的可解释性。该算法构建的CWMs能够帮助RL代理专注于任务相关特征，避免捷径依赖，并在Atari环境（如Pong和Freeway）中验证，展示了显著的准确性和鲁棒性，从而提升了代理在动态场景中的规划和决策能力。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "5 pages including references, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.07257v1",
      "published_date": "2025-04-09 20:29:13 UTC",
      "updated_date": "2025-04-09 20:29:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:08:17.476168"
    },
    {
      "arxiv_id": "2504.08823v1",
      "title": "FM-LoRA: Factorized Low-Rank Meta-Prompting for Continual Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaobing Yu",
        "Jin Yang",
        "Xiao Wu",
        "Peijie Qiu",
        "Xiaofeng Liu"
      ],
      "abstract": "How to adapt a pre-trained model continuously for sequential tasks with\ndifferent prediction class labels and domains and finally learn a generalizable\nmodel across diverse tasks is a long-lasting challenge. Continual learning (CL)\nhas emerged as a promising approach to leverage pre-trained models (e.g.,\nTransformers) for sequential tasks. While many existing CL methods\nincrementally store additional learned structures, such as Low-Rank Adaptation\n(LoRA) adapters or prompts and sometimes even preserve features from previous\nsamples to maintain performance. This leads to unsustainable parameter growth\nand escalating storage costs as the number of tasks increases. Moreover,\ncurrent approaches often lack task similarity awareness, which further hinders\nthe models ability to effectively adapt to new tasks without interfering with\npreviously acquired knowledge. To address these challenges, we propose FM-LoRA,\na novel and efficient low-rank adaptation method that integrates both a dynamic\nrank selector (DRS) and dynamic meta-prompting (DMP). This framework allocates\nmodel capacity more effectively across tasks by leveraging a shared low-rank\nsubspace critical for preserving knowledge, thereby avoiding continual\nparameter expansion. Extensive experiments on various CL benchmarks, including\nImageNet-R, CIFAR100, and CUB200 for class-incremental learning (CIL), and\nDomainNet for domain-incremental learning (DIL), with Transformers backbone\ndemonstrate that FM-LoRA effectively mitigates catastrophic forgetting while\ndelivering robust performance across a diverse range of tasks and domains.",
      "tldr_zh": "该研究针对持续学习(Continual Learning)中的参数膨胀和灾难性遗忘问题，提出FM-LoRA，一种基于因子化低秩元提示(Factorized Low-Rank Meta-Prompting)的创新方法。FM-LoRA通过整合动态秩选择器(dynamic rank selector, DRS)和动态元提示(dynamic meta-prompting, DMP)，利用共享低秩子空间来有效分配模型容量，避免参数持续增长，同时增强任务相似性意识。实验结果显示，在ImageNet-R、CIFAR100、CUB200（class-incremental learning, CIL）和DomainNet（domain-incremental learning, DIL）等基准上，FM-LoRA显著缓解了灾难性遗忘，并实现了Transformer模型在多样任务和领域中的稳健性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "8 Pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.08823v1",
      "published_date": "2025-04-09 19:36:18 UTC",
      "updated_date": "2025-04-09 19:36:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:08:30.163991"
    },
    {
      "arxiv_id": "2504.07245v1",
      "title": "A new training approach for text classification in Mental Health: LatentGLoss",
      "title_zh": "一种新的训练方法用于心理健康领域的文本分类：LatentGLoss",
      "authors": [
        "Korhan Sevinç"
      ],
      "abstract": "This study presents a multi-stage approach to mental health classification by\nleveraging traditional machine learning algorithms, deep learning\narchitectures, and transformer-based models. A novel data set was curated and\nutilized to evaluate the performance of various methods, starting with\nconventional classifiers and advancing through neural networks. To broaden the\narchitectural scope, recurrent neural networks (RNNs) such as LSTM and GRU were\nalso evaluated to explore their effectiveness in modeling sequential patterns\nin the data. Subsequently, transformer models such as BERT were fine-tuned to\nassess the impact of contextual embeddings in this domain. Beyond these\nbaseline evaluations, the core contribution of this study lies in a novel\ntraining strategy involving a dual-model architecture composed of a teacher and\na student network. Unlike standard distillation techniques, this method does\nnot rely on soft label transfer; instead, it facilitates information flow\nthrough both the teacher model's output and its latent representations by\nmodifying the loss function. The experimental results highlight the\neffectiveness of each modeling stage and demonstrate that the proposed loss\nfunction and teacher-student interaction significantly enhance the model's\nlearning capacity in mental health prediction tasks.",
      "tldr_zh": "这篇论文提出了一种多阶段方法，用于心理健康文本分类，评估了传统机器学习算法、深度学习架构（如RNNs，包括LSTM和GRU）和Transformer模型（如BERT）的性能，并基于一个新颖的数据集进行实验。核心贡献是LatentGLoss，一种新颖的训练策略，采用双模型架构（教师和学生网络），通过修改损失函数来利用教师模型的输出和latent representations，而非依赖标准的knowledge distillation。实验结果显示，这种策略显著提升了模型在心理健康预测任务中的学习能力和整体表现。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages, 3 Figures, 4 Tables",
      "pdf_url": "http://arxiv.org/pdf/2504.07245v1",
      "published_date": "2025-04-09 19:34:31 UTC",
      "updated_date": "2025-04-09 19:34:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:08:41.011340"
    },
    {
      "arxiv_id": "2504.10509v1",
      "title": "Beyond Reproducibility: Advancing Zero-shot LLM Reranking Efficiency with Setwise Insertion",
      "title_zh": "翻译失败",
      "authors": [
        "Jakub Podolak",
        "Leon Peric",
        "Mina Janicijevic",
        "Roxana Petcu"
      ],
      "abstract": "This study presents a comprehensive reproducibility and extension analysis of\nthe Setwise prompting methodology for zero-shot ranking with Large Language\nModels (LLMs), as proposed by Zhuang et al. We evaluate its effectiveness and\nefficiency compared to traditional Pointwise, Pairwise, and Listwise approaches\nin document ranking tasks. Our reproduction confirms the findings of Zhuang et\nal., highlighting the trade-offs between computational efficiency and ranking\neffectiveness in Setwise methods. Building on these insights, we introduce\nSetwise Insertion, a novel approach that leverages the initial document ranking\nas prior knowledge, reducing unnecessary comparisons and uncertainty by\nfocusing on candidates more likely to improve the ranking results. Experimental\nresults across multiple LLM architectures (Flan-T5, Vicuna, and Llama) show\nthat Setwise Insertion yields a 31% reduction in query time, a 23% reduction in\nmodel inferences, and a slight improvement in reranking effectiveness compared\nto the original Setwise method. These findings highlight the practical\nadvantage of incorporating prior ranking knowledge into Setwise prompting for\nefficient and accurate zero-shot document reranking.",
      "tldr_zh": "这项研究对 Setwise prompting 方法进行了可复现性和扩展分析，用于零样本(Zero-shot) LLM 重新排名任务，并与传统 Pointwise、Pairwise 和 Listwise 方法进行比较，确认了其在计算效率与排名效果间的权衡。作者提出 Setwise Insertion 新方法，通过利用初始文档排名作为先验知识，减少不必要的比较，从而降低不确定性和提升整体效率。在 Flan-T5、Vicuna 和 Llama 等 LLM 架构的实验中，Setwise Insertion 实现了查询时间减少 31%、模型推理减少 23%，并略微提高了重新排名的效果。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.10509v1",
      "published_date": "2025-04-09 18:44:34 UTC",
      "updated_date": "2025-04-09 18:44:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:08:54.859324"
    },
    {
      "arxiv_id": "2504.07199v2",
      "title": "SemEval-2025 Task 5: LLMs4Subjects -- LLM-based Automated Subject Tagging for a National Technical Library's Open-Access Catalog",
      "title_zh": "翻译失败",
      "authors": [
        "Jennifer D'Souza",
        "Sameer Sadruddin",
        "Holger Israel",
        "Mathias Begoin",
        "Diana Slawig"
      ],
      "abstract": "We present SemEval-2025 Task 5: LLMs4Subjects, a shared task on automated\nsubject tagging for scientific and technical records in English and German\nusing the GND taxonomy. Participants developed LLM-based systems to recommend\ntop-k subjects, evaluated through quantitative metrics (precision, recall,\nF1-score) and qualitative assessments by subject specialists. Results highlight\nthe effectiveness of LLM ensembles, synthetic data generation, and multilingual\nprocessing, offering insights into applying LLMs for digital library\nclassification.",
      "tldr_zh": "本研究介绍了 SemEval-2025 Task 5：LLMs4Subjects，这是一个共享任务，旨在使用 LLM（Large Language Models）对英语和德语的科学和技术记录进行自动主题标记，基于 GND taxonomy。参与者开发了基于 LLM 的系统来推荐 top-k 主题，通过精确率、召回率和 F1-score 等量化指标以及定性评估进行评估。结果突出了 LLM 集成、合成数据生成和多语言处理的有效性，为数字图书馆分类提供了宝贵的应用见解。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DL",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, 4 figures, Accepted as SemEval 2025 Task 5 description\n  paper",
      "pdf_url": "http://arxiv.org/pdf/2504.07199v2",
      "published_date": "2025-04-09 18:26:46 UTC",
      "updated_date": "2025-04-11 10:14:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:09:05.238811"
    },
    {
      "arxiv_id": "2504.07198v1",
      "title": "Face-LLaVA: Facial Expression and Attribute Understanding through Instruction Tuning",
      "title_zh": "Face-LLaVA：通过指令微调实现面部表情和属性理解",
      "authors": [
        "Ashutosh Chaubey",
        "Xulang Guan",
        "Mohammad Soleymani"
      ],
      "abstract": "The human face plays a central role in social communication, necessitating\nthe use of performant computer vision tools for human-centered applications. We\npropose Face-LLaVA, a multimodal large language model for face-centered,\nin-context learning, including facial expression and attribute recognition.\nAdditionally, Face-LLaVA is able to generate natural language descriptions that\ncan be used for reasoning. Leveraging existing visual databases, we first\ndeveloped FaceInstruct-1M, a face-centered database for instruction tuning\nMLLMs for face processing. We then developed a novel face-specific visual\nencoder powered by Face-Region Guided Cross-Attention that integrates face\ngeometry with local visual features. We evaluated the proposed method across\nnine different datasets and five different face processing tasks, including\nfacial expression recognition, action unit detection, facial attribute\ndetection, age estimation and deepfake detection. Face-LLaVA achieves superior\nresults compared to existing open-source MLLMs and competitive performance\ncompared to commercial solutions. Our model output also receives a higher\nreasoning rating by GPT under a zero-shot setting across all the tasks. Both\nour dataset and model wil be released at https://face-llava.github.io to\nsupport future advancements in social AI and foundational vision-language\nresearch.",
      "tldr_zh": "该研究提出了Face-LLaVA，一种通过Instruction Tuning的多模态大语言模型（MLLMs），专注于面部表情和属性识别，并能生成自然语言描述以支持推理。\n他们开发了FaceInstruct-1M数据集用于训练，以及一个基于Face-Region Guided Cross-Attention的面部特定视觉编码器，整合面部几何和局部视觉特征。\n在九个数据集上的五个任务（如面部表情识别、动作单元检测、年龄估计和深度伪造检测）中，Face-LLaVA优于现有开源MLLMs，与商业解决方案竞争，并在零样本设置下获得更高的GPT推理评级。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CV",
      "comment": "Project Page: https://face-llava.github.io",
      "pdf_url": "http://arxiv.org/pdf/2504.07198v1",
      "published_date": "2025-04-09 18:26:07 UTC",
      "updated_date": "2025-04-09 18:26:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:09:16.868142"
    },
    {
      "arxiv_id": "2504.07174v1",
      "title": "HypoEval: Hypothesis-Guided Evaluation for Natural Language Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Mingxuan Li",
        "Hanchen Li",
        "Chenhao Tan"
      ],
      "abstract": "Large language models (LLMs) have demonstrated great potential for automating\nthe evaluation of natural language generation. Previous frameworks of\nLLM-as-a-judge fall short in two ways: they either use zero-shot setting\nwithout consulting any human input, which leads to low alignment, or fine-tune\nLLMs on labeled data, which requires a non-trivial number of samples. Moreover,\nprevious methods often provide little reasoning behind automated evaluations.\nIn this paper, we propose HypoEval, Hypothesis-guided Evaluation framework,\nwhich first uses a small corpus of human evaluations to generate more detailed\nrubrics for human judgments and then incorporates a checklist-like approach to\ncombine LLM's assigned scores on each decomposed dimension to acquire overall\nscores. With only 30 human evaluations, HypoEval achieves state-of-the-art\nperformance in alignment with both human rankings (Spearman correlation) and\nhuman scores (Pearson correlation), on average outperforming G-Eval by 11.86%\nand fine-tuned Llama-3.1-8B-Instruct with at least 3 times more human\nevaluations by 11.95%. Furthermore, we conduct systematic studies to assess the\nrobustness of HypoEval, highlighting its effectiveness as a reliable and\ninterpretable automated evaluation framework.",
      "tldr_zh": "本文提出HypoEval，一种Hypothesis-guided Evaluation框架，用于提升自然语言生成(NLG)的自动评估性能。它通过利用少量人类评估（如30个样本）生成详细的rubrics，并采用检查列表-like方法，让LLM对每个分解维度评分后结合得出整体分数，从而解决现有LLM-as-a-judge框架的低对齐性和缺乏推理问题。实验结果显示，HypoEval在Spearman correlation和Pearson correlation上达到最先进水平，平均比G-Eval高11.86%，并优于使用至少3倍人类评估的微调Llama-3.1-8B-Instruct模型11.95%。此外，系统性研究证实了HypoEval的鲁棒性和可解释性，使其成为可靠的自动评估工具。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "22 pages, 3 figures, code link:\n  https://github.com/ChicagoHAI/HypoEval-Gen",
      "pdf_url": "http://arxiv.org/pdf/2504.07174v1",
      "published_date": "2025-04-09 18:00:01 UTC",
      "updated_date": "2025-04-09 18:00:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:09:30.653974"
    },
    {
      "arxiv_id": "2504.07170v1",
      "title": "Trustworthy AI Must Account for Intersectionality",
      "title_zh": "翻译失败",
      "authors": [
        "Jesse C. Cresswell"
      ],
      "abstract": "Trustworthy AI encompasses many aspirational aspects for aligning AI systems\nwith human values, including fairness, privacy, robustness, explainability, and\nuncertainty quantification. However, efforts to enhance one aspect often\nintroduce unintended trade-offs that negatively impact others, making it\nchallenging to improve all aspects simultaneously. In this position paper, we\nreview notable approaches to these five aspects and systematically consider\nevery pair, detailing the negative interactions that can arise. For example,\napplying differential privacy to model training can amplify biases in the data,\nundermining fairness. Drawing on these findings, we take the position that\naddressing trustworthiness along each axis in isolation is insufficient.\nInstead, research on Trustworthy AI must account for intersectionality between\naspects and adopt a holistic view across all relevant axes at once. To\nillustrate our perspective, we provide guidance on how researchers can work\ntowards integrated trustworthiness, a case study on how intersectionality\napplies to the financial industry, and alternative views to our position.",
      "tldr_zh": "这篇立场论文强调，可信 AI 的五个关键方面——fairness、privacy、robustness、explainability 和 uncertainty quantification——之间存在负面互动，例如应用 differential privacy 可能放大数据偏差，从而损害 fairness。论文审视了这些方面的相关方法，并系统分析了每对方面的潜在冲突，主张研究不能孤立处理每个方面，而必须考虑 intersectionality 并采用整体视角。作者提供了实现集成可信性的指导、金融行业的案例研究，以及对该立场的替代观点，以推动更全面的 AI 信任框架。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Presented at the ICLR 2025 Workshop on Bidirectional Human-AI\n  Alignment",
      "pdf_url": "http://arxiv.org/pdf/2504.07170v1",
      "published_date": "2025-04-09 18:00:00 UTC",
      "updated_date": "2025-04-09 18:00:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:09:40.717986"
    },
    {
      "arxiv_id": "2504.07097v1",
      "title": "Sculpting Subspaces: Constrained Full Fine-Tuning in LLMs for Continual Learning",
      "title_zh": "塑造子空间：大语言模型中用于持续学习的约束全微调",
      "authors": [
        "Nikhil Shivakumar Nayak",
        "Krishnateja Killamsetty",
        "Ligong Han",
        "Abhishek Bhandwaldar",
        "Prateek Chanda",
        "Kai Xu",
        "Hao Wang",
        "Aldo Pareja",
        "Oleg Silkin",
        "Mustafa Eyceoz",
        "Akash Srivastava"
      ],
      "abstract": "Continual learning in large language models (LLMs) is prone to catastrophic\nforgetting, where adapting to new tasks significantly degrades performance on\npreviously learned ones. Existing methods typically rely on low-rank,\nparameter-efficient updates that limit the model's expressivity and introduce\nadditional parameters per task, leading to scalability issues. To address these\nlimitations, we propose a novel continual full fine-tuning approach leveraging\nadaptive singular value decomposition (SVD). Our method dynamically identifies\ntask-specific low-rank parameter subspaces and constrains updates to be\northogonal to critical directions associated with prior tasks, thus effectively\nminimizing interference without additional parameter overhead or storing\nprevious task gradients. We evaluate our approach extensively on standard\ncontinual learning benchmarks using both encoder-decoder (T5-Large) and\ndecoder-only (LLaMA-2 7B) models, spanning diverse tasks including\nclassification, generation, and reasoning. Empirically, our method achieves\nstate-of-the-art results, up to 7% higher average accuracy than recent\nbaselines like O-LoRA, and notably maintains the model's general linguistic\ncapabilities, instruction-following accuracy, and safety throughout the\ncontinual learning process by reducing forgetting to near-negligible levels.\nOur adaptive SVD framework effectively balances model plasticity and knowledge\nretention, providing a practical, theoretically grounded, and computationally\nscalable solution for continual learning scenarios in large language models.",
      "tldr_zh": "本研究针对大型语言模型（LLMs）在持续学习中的灾难性遗忘（catastrophic forgetting）问题，提出了一种新颖的约束全微调方法，利用自适应奇异值分解（adaptive SVD）动态识别任务特定的低秩参数子空间，并确保更新与先前任务的关键方向正交，从而最小化任务干扰。不同于现有低秩参数高效更新方法，该方法无需额外参数或存储先前任务梯度，提升了可扩展性。实验在 T5-Large 和 LLaMA-2 7B 模型上评估了分类、生成和推理任务，取得了比基线如 O-LoRA 高达 7% 的平均准确率，并显著减少遗忘，保持模型的语言能力、指令遵循准确性和安全性。该框架为 LLMs 的持续学习提供了平衡可塑性和知识保留的实用解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "math.PR",
        "stat.ML",
        "68T50",
        "I.2.0; G.3"
      ],
      "primary_category": "cs.LG",
      "comment": "25 pages, 13 figures, 6 tables",
      "pdf_url": "http://arxiv.org/pdf/2504.07097v1",
      "published_date": "2025-04-09 17:59:42 UTC",
      "updated_date": "2025-04-09 17:59:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:09:53.299206"
    },
    {
      "arxiv_id": "2504.07092v2",
      "title": "Are We Done with Object-Centric Learning?",
      "title_zh": "翻译失败",
      "authors": [
        "Alexander Rubinstein",
        "Ameya Prabhu",
        "Matthias Bethge",
        "Seong Joon Oh"
      ],
      "abstract": "Object-centric learning (OCL) seeks to learn representations that only encode\nan object, isolated from other objects or background cues in a scene. This\napproach underpins various aims, including out-of-distribution (OOD)\ngeneralization, sample-efficient composition, and modeling of structured\nenvironments. Most research has focused on developing unsupervised mechanisms\nthat separate objects into discrete slots in the representation space,\nevaluated using unsupervised object discovery. However, with recent\nsample-efficient segmentation models, we can separate objects in the pixel\nspace and encode them independently. This achieves remarkable zero-shot\nperformance on OOD object discovery benchmarks, is scalable to foundation\nmodels, and can handle a variable number of slots out-of-the-box. Hence, the\ngoal of OCL methods to obtain object-centric representations has been largely\nachieved. Despite this progress, a key question remains: How does the ability\nto separate objects within a scene contribute to broader OCL objectives, such\nas OOD generalization? We address this by investigating the OOD generalization\nchallenge caused by spurious background cues through the lens of OCL. We\npropose a novel, training-free probe called Object-Centric Classification with\nApplied Masks (OCCAM), demonstrating that segmentation-based encoding of\nindividual objects significantly outperforms slot-based OCL methods. However,\nchallenges in real-world applications remain. We provide the toolbox for the\nOCL community to use scalable object-centric representations, and focus on\npractical applications and fundamental questions, such as understanding object\nperception in human cognition. Our code is available here:\nhttps://github.com/AlexanderRubinstein/OCCAM.",
      "tldr_zh": "本论文审视了对象中心学习（OCL）的现状，指出虽然 OCL 旨在通过分离场景中的对象实现 out-of-distribution (OOD) 泛化、样本高效组合和结构化环境建模，但现代样本高效分割模型已在像素空间成功分离并独立编码对象，实现了出色零样本性能并可扩展到基础模型。作者提出了一种新型训练-free 探针 Object-Centric Classification with Applied Masks (OCCAM)，用于评估对象分离对 OOD 泛化的贡献，结果显示基于分割的编码方法显著优于传统槽位-based OCL 方法。尽管 OCL 的核心目标已基本达成，但现实应用中仍存在挑战，如处理虚假背景线索；作者提供工具箱和代码，鼓励社区聚焦实际应用和基础问题，例如人类认知中的对象感知。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.07092v2",
      "published_date": "2025-04-09 17:59:05 UTC",
      "updated_date": "2025-04-10 21:45:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:10:06.973073"
    },
    {
      "arxiv_id": "2504.07091v1",
      "title": "AssistanceZero: Scalably Solving Assistance Games",
      "title_zh": "AssistanceZero: 可扩展求解辅助游戏",
      "authors": [
        "Cassidy Laidlaw",
        "Eli Bronstein",
        "Timothy Guo",
        "Dylan Feng",
        "Lukas Berglund",
        "Justin Svegliato",
        "Stuart Russell",
        "Anca Dragan"
      ],
      "abstract": "Assistance games are a promising alternative to reinforcement learning from\nhuman feedback (RLHF) for training AI assistants. Assistance games resolve key\ndrawbacks of RLHF, such as incentives for deceptive behavior, by explicitly\nmodeling the interaction between assistant and user as a two-player game where\nthe assistant cannot observe their shared goal. Despite their potential,\nassistance games have only been explored in simple settings. Scaling them to\nmore complex environments is difficult because it requires both solving\nintractable decision-making problems under uncertainty and accurately modeling\nhuman users' behavior. We present the first scalable approach to solving\nassistance games and apply it to a new, challenging Minecraft-based assistance\ngame with over $10^{400}$ possible goals. Our approach, AssistanceZero, extends\nAlphaZero with a neural network that predicts human actions and rewards,\nenabling it to plan under uncertainty. We show that AssistanceZero outperforms\nmodel-free RL algorithms and imitation learning in the Minecraft-based\nassistance game. In a human study, our AssistanceZero-trained assistant\nsignificantly reduces the number of actions participants take to complete\nbuilding tasks in Minecraft. Our results suggest that assistance games are a\ntractable framework for training effective AI assistants in complex\nenvironments. Our code and models are available at\nhttps://github.com/cassidylaidlaw/minecraft-building-assistance-game.",
      "tldr_zh": "这篇论文提出了 AssistanceZero，一种可扩展的方法，用于解决 Assistance Games 中的挑战，这些游戏将 AI 助手和用户互动建模为两人博弈，以避免 RLHF 中的欺骗行为问题。AssistanceZero 扩展了 AlphaZero，通过神经网络预测人类动作和奖励，从而在不确定性下进行规划，并应用于一个基于 Minecraft 的新游戏环境，该环境有超过 10^400 种可能目标。实验结果显示，AssistanceZero 优于无模型 RL 算法和模仿学习，并在人类研究中显著减少了参与者完成 Minecraft 建筑任务的动作次数。这些发现表明，Assistance Games 是训练有效 AI 助手的可行框架。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.07091v1",
      "published_date": "2025-04-09 17:59:03 UTC",
      "updated_date": "2025-04-09 17:59:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:10:17.633429"
    },
    {
      "arxiv_id": "2504.07087v1",
      "title": "KG-LLM-Bench: A Scalable Benchmark for Evaluating LLM Reasoning on Textualized Knowledge Graphs",
      "title_zh": "KG-LLM-Bench：一个用于评估 LLM 在文本化知识图谱上推理的可扩展基准",
      "authors": [
        "Elan Markowitz",
        "Krupa Galiya",
        "Greg Ver Steeg",
        "Aram Galstyan"
      ],
      "abstract": "Knowledge graphs have emerged as a popular method for injecting up-to-date,\nfactual knowledge into large language models (LLMs). This is typically achieved\nby converting the knowledge graph into text that the LLM can process in\ncontext. While multiple methods of encoding knowledge graphs have been\nproposed, the impact of this textualization process on LLM performance remains\nunder-explored. We introduce KG-LLM-Bench, a comprehensive and extensible\nbenchmark spanning five knowledge graph understanding tasks, and evaluate how\ndifferent encoding strategies affect performance across various base models.\nOur extensive experiments with seven language models and five textualization\nstrategies provide insights for optimizing LLM performance on KG reasoning\ntasks.",
      "tldr_zh": "该研究引入了 KG-LLM-Bench，这是一个可扩展的基准，用于评估大型语言模型(LLMs)在文本化知识图谱(Knowledge Graphs)上的推理性能。基准涵盖五种知识图谱理解任务，并通过实验比较了七个语言模型和五种文本化策略对性能的影响。结果显示，不同编码策略显著影响 LLM 的表现，并提供了优化 KG 推理任务的实用见解。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "To be presented at NAACL-HLT, KnowledgeNLP Workshop (2025)",
      "pdf_url": "http://arxiv.org/pdf/2504.07087v1",
      "published_date": "2025-04-09 17:58:47 UTC",
      "updated_date": "2025-04-09 17:58:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:10:28.962923"
    },
    {
      "arxiv_id": "2504.07081v1",
      "title": "Self-Steering Language Models",
      "title_zh": "自引导语言模型",
      "authors": [
        "Gabriel Grand",
        "Joshua B. Tenenbaum",
        "Vikash K. Mansinghka",
        "Alexander K. Lew",
        "Jacob Andreas"
      ],
      "abstract": "While test-time reasoning enables language models to tackle complex tasks,\nsearching or planning in natural language can be slow, costly, and error-prone.\nBut even when LMs struggle to emulate the precise reasoning steps needed to\nsolve a problem, they often excel at describing its abstract structure--both\nhow to verify solutions and how to search for them. This paper introduces\nDisCIPL, a method for \"self-steering\" LMs where a Planner model generates a\ntask-specific inference program that is executed by a population of Follower\nmodels. Our approach equips LMs with the ability to write recursive search\nprocedures that guide LM inference, enabling new forms of verifiable and\nefficient reasoning. When instantiated with a small Follower (e.g.,\nLlama-3.2-1B), DisCIPL matches (and sometimes outperforms) much larger models,\nincluding GPT-4o and o1, on challenging constrained generation tasks. In\ndecoupling planning from execution, our work opens up a design space of\nhighly-parallelized Monte Carlo inference strategies that outperform standard\nbest-of-N sampling, require no finetuning, and can be implemented automatically\nby existing LMs.",
      "tldr_zh": "本论文提出 DisCIPL 方法，实现 Language Models (LMs) 的自引导推理，通过 Planner 模型生成任务特定的推理程序，由一组 Follower 模型执行，从而编写递归搜索程序以提升推理的效率和可验证性。\n这种方法允许 LMs 即使在复杂任务中，也能描述问题的抽象结构，并指导搜索过程，而无需微调。\n实验显示，使用小型 Follower 模型（如 Llama-3.2-1B），DisCIPL 在 challenging 的 constrained generation 任务上，性能可与大型模型（如 GPT-4o 和 o1）匹敌或超越，并支持高并行化的 Monte Carlo 推理策略。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.07081v1",
      "published_date": "2025-04-09 17:54:22 UTC",
      "updated_date": "2025-04-09 17:54:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:10:41.676854"
    },
    {
      "arxiv_id": "2504.10508v1",
      "title": "Poly-Vector Retrieval: Reference and Content Embeddings for Legal Documents",
      "title_zh": "翻译失败",
      "authors": [
        "João Alberto de Oliveira Lima"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) has emerged as an effective paradigm for\ngenerating contextually accurate answers by integrating Large Language Models\n(LLMs) with retrieval mechanisms. However, in legal contexts, users frequently\nreference norms by their labels or nicknames (e.g., Article 5 of the\nConstitution or Consumer Defense Code (CDC)), rather than by their content,\nposing challenges for traditional RAG approaches that rely solely on semantic\nembeddings of text. Furthermore, legal texts themselves heavily rely on\nexplicit cross-references (e.g., \"pursuant to Article 34\") that function as\npointers. Both scenarios pose challenges for traditional RAG approaches that\nrely solely on semantic embeddings of text, often failing to retrieve the\nnecessary referenced content. This paper introduces Poly-Vector Retrieval, a\nmethod assigning multiple distinct embeddings to each legal provision: one\nembedding captures the content (the full text), another captures the label (the\nidentifier or proper name), and optionally additional embeddings capture\nalternative denominations. Inspired by Frege's distinction between Sense and\nReference, this poly-vector retrieval approach treats labels, identifiers and\nreference markers as rigid designators and content embeddings as carriers of\nsemantic substance. Experiments on the Brazilian Federal Constitution\ndemonstrate that Poly-Vector Retrieval significantly improves retrieval\naccuracy for label-centric queries and potential to resolve internal and\nexternal cross-references, without compromising performance on purely semantic\nqueries. The study discusses philosophical and practical implications of\nexplicitly separating reference from content in vector embeddings and proposes\nfuture research directions for applying this approach to broader legal datasets\nand other domains characterized by explicit reference identifiers.",
      "tldr_zh": "这篇论文针对法律文档检索中的挑战，提出了 Poly-Vector Retrieval 方法，以解决 Retrieval-Augmented Generation (RAG) 系统在处理标签或昵称引用（如“Article 5”）时的不足。该方法为每个法律条款分配多个嵌入，包括内容嵌入（捕捉全文语义）、标签嵌入（捕捉标识符）和可选的替代名称嵌入，受 Frege's Sense and Reference 区分的启发。实验在巴西联邦宪法数据集上表明，该方法显著提高了标签导向查询的检索准确性，同时不影响纯语义查询的性能。论文还讨论了将参考与内容在 vector embeddings 中分离的哲学和实际含义，并提出扩展到其他领域的未来研究方向。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "I.2.8"
      ],
      "primary_category": "cs.IR",
      "comment": "39 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.10508v1",
      "published_date": "2025-04-09 17:54:11 UTC",
      "updated_date": "2025-04-09 17:54:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:10:54.389972"
    },
    {
      "arxiv_id": "2504.07080v1",
      "title": "DeduCE: Deductive Consistency as a Framework to Evaluate LLM Reasoning",
      "title_zh": "DeduCE：演绎一致性作为评估 LLM 推理的框架",
      "authors": [
        "Atharva Pandey",
        "Kshitij Dubey",
        "Rahul Sharma",
        "Amit Sharma"
      ],
      "abstract": "Despite great performance on Olympiad-level reasoning problems, frontier\nlarge language models can still struggle on high school math when presented\nwith novel problems outside standard benchmarks. Going beyond final accuracy,\nwe propose a deductive consistency metric to analyze chain-of-thought output\nfrom language models (LMs).Formally, deductive reasoning involves two subtasks:\nunderstanding a set of input premises and inferring the conclusions that follow\nfrom them. The proposed metric studies LMs' performance on these subtasks, with\nthe goal of explaining LMs' reasoning errors on novel problems: how well do LMs\nunderstand input premises with increasing context lengths, and how well can\nthey infer conclusions over multiple reasoning hops? Since existing benchmarks\nmay be memorized, we develop a pipeline to evaluate LMs' deductive consistency\non novel, perturbed versions of benchmark problems. On novel grade school math\nproblems (GSM-8k), we find that LMs are fairly robust to increasing number of\ninput premises, but suffer significant accuracy decay as the number of\nreasoning hops is increased. Interestingly, these errors are masked in the\noriginal benchmark as all models achieve near 100% accuracy. As we increase the\nnumber of solution steps using a synthetic dataset, prediction over multiple\nhops still remains the major source of error compared to understanding input\npremises. Other factors, such as shifts in language style or natural\npropagation of early errors do not explain the trends. Our analysis provides a\nnew view to characterize LM reasoning -- as computations over a window of input\npremises and reasoning hops -- that can provide unified evaluation across\nproblem domains.",
      "tldr_zh": "该论文提出 DeduCE 框架，作为评估大型语言模型 (LLMs) 推理能力的度量方法，专注于分析 chain-of-thought 输出中的 deductive consistency，以解释模型在处理新颖问题时的推理错误。框架将推理分解为理解输入前提和推断结论两个子任务，并使用一个管道生成新颖的扰动基准问题进行评估。实验结果显示，在 GSM-8k 等数据集上，LLMs 对增加输入前提数量较为鲁棒，但随着 reasoning hops 增加，准确率显著下降，且原基准可能掩盖了这些问题，为理解 LM 推理提供了一个统一的计算视角。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.07080v1",
      "published_date": "2025-04-09 17:53:55 UTC",
      "updated_date": "2025-04-09 17:53:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:11:05.356225"
    },
    {
      "arxiv_id": "2504.07079v1",
      "title": "SkillWeaver: Web Agents can Self-Improve by Discovering and Honing Skills",
      "title_zh": "SkillWeaver：Web 智能体可以通过发现和磨炼技能实现自我提升",
      "authors": [
        "Boyuan Zheng",
        "Michael Y. Fatemi",
        "Xiaolong Jin",
        "Zora Zhiruo Wang",
        "Apurva Gandhi",
        "Yueqi Song",
        "Yu Gu",
        "Jayanth Srinivasa",
        "Gaowen Liu",
        "Graham Neubig",
        "Yu Su"
      ],
      "abstract": "To survive and thrive in complex environments, humans have evolved\nsophisticated self-improvement mechanisms through environment exploration,\nhierarchical abstraction of experiences into reuseable skills, and\ncollaborative construction of an ever-growing skill repertoire. Despite recent\nadvancements, autonomous web agents still lack crucial self-improvement\ncapabilities, struggling with procedural knowledge abstraction, refining\nskills, and skill composition. In this work, we introduce SkillWeaver, a\nskill-centric framework enabling agents to self-improve by autonomously\nsynthesizing reusable skills as APIs. Given a new website, the agent\nautonomously discovers skills, executes them for practice, and distills\npractice experiences into robust APIs. Iterative exploration continually\nexpands a library of lightweight, plug-and-play APIs, significantly enhancing\nthe agent's capabilities. Experiments on WebArena and real-world websites\ndemonstrate the efficacy of SkillWeaver, achieving relative success rate\nimprovements of 31.8% and 39.8%, respectively. Additionally, APIs synthesized\nby strong agents substantially enhance weaker agents through transferable\nskills, yielding improvements of up to 54.3% on WebArena. These results\ndemonstrate the effectiveness of honing diverse website interactions into APIs,\nwhich can be seamlessly shared among various web agents.",
      "tldr_zh": "该论文提出SkillWeaver框架，帮助网页代理通过自主发现和精炼技能来实现自我提升，模拟人类的环境探索和技能抽象过程。框架允许代理在新网站上自动识别技能、执行实践并提炼成可重用API，从而通过迭代探索不断扩展技能库。实验结果显示，在WebArena和真实网站上，成功率分别提升31.8%和39.8%，此外，强代理合成的API可转移给弱代理，提升高达54.3%，证明了技能共享的有效性。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.07079v1",
      "published_date": "2025-04-09 17:51:50 UTC",
      "updated_date": "2025-04-09 17:51:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:11:16.827326"
    },
    {
      "arxiv_id": "2504.07069v1",
      "title": "HalluciNot: Hallucination Detection Through Context and Common Knowledge Verification",
      "title_zh": "HalluciNot：通过上下文和常识知识验证的幻觉检测",
      "authors": [
        "Bibek Paudel",
        "Alexander Lyzhov",
        "Preetam Joshi",
        "Puneet Anand"
      ],
      "abstract": "This paper introduces a comprehensive system for detecting hallucinations in\nlarge language model (LLM) outputs in enterprise settings. We present a novel\ntaxonomy of LLM responses specific to hallucination in enterprise applications,\ncategorizing them into context-based, common knowledge, enterprise-specific,\nand innocuous statements. Our hallucination detection model HDM-2 validates LLM\nresponses with respect to both context and generally known facts (common\nknowledge). It provides both hallucination scores and word-level annotations,\nenabling precise identification of problematic content. To evaluate it on\ncontext-based and common-knowledge hallucinations, we introduce a new dataset\nHDMBench. Experimental results demonstrate that HDM-2 out-performs existing\napproaches across RagTruth, TruthfulQA, and HDMBench datasets. This work\naddresses the specific challenges of enterprise deployment, including\ncomputational efficiency, domain specialization, and fine-grained error\nidentification. Our evaluation dataset, model weights, and inference code are\npublicly available.",
      "tldr_zh": "这篇论文引入了 HalluciNot 系统，用于检测大型语言模型 (LLM) 输出中的幻觉，特别针对企业环境。作者提出一个新分类法，将幻觉分为基于上下文的、常见知识的、企业特定的和无害声明，并开发了 HDM-2 模型，通过验证上下文和 common knowledge 提供幻觉分数及词级注解。实验结果显示 HDM-2 在 RagTruth、TruthfulQA 和新数据集 HDMBench 上优于现有方法，同时解决了企业部署的计算效率、领域专业化和细粒度错误识别挑战，并公开了模型权重、推理代码和评估数据集。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.07069v1",
      "published_date": "2025-04-09 17:39:41 UTC",
      "updated_date": "2025-04-09 17:39:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:11:29.165962"
    },
    {
      "arxiv_id": "2504.07055v1",
      "title": "$Π$-NeSy: A Possibilistic Neuro-Symbolic Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Ismaïl Baaj",
        "Pierre Marquis"
      ],
      "abstract": "In this article, we introduce a neuro-symbolic approach that combines a\nlow-level perception task performed by a neural network with a high-level\nreasoning task performed by a possibilistic rule-based system. The goal is to\nbe able to derive for each input instance the degree of possibility that it\nbelongs to a target (meta-)concept. This (meta-)concept is connected to\nintermediate concepts by a possibilistic rule-based system. The probability of\neach intermediate concept for the input instance is inferred using a neural\nnetwork. The connection between the low-level perception task and the\nhigh-level reasoning task lies in the transformation of neural network outputs\nmodeled by probability distributions (through softmax activation) into\npossibility distributions. The use of intermediate concepts is valuable for the\nexplanation purpose: using the rule-based system, the classification of an\ninput instance as an element of the (meta-)concept can be justified by the fact\nthat intermediate concepts have been recognized.\n  From the technical side, our contribution consists of the design of efficient\nmethods for defining the matrix relation and the equation system associated\nwith a possibilistic rule-based system. The corresponding matrix and equation\nare key data structures used to perform inferences from a possibilistic\nrule-based system and to learn the values of the rule parameters in such a\nsystem according to a training data sample. Furthermore, leveraging recent\nresults on the handling of inconsistent systems of fuzzy relational equations,\nan approach for learning rule parameters according to multiple training data\nsamples is presented. Experiments carried out on the MNIST addition problems\nand the MNIST Sudoku puzzles problems highlight the effectiveness of our\napproach compared with state-of-the-art neuro-symbolic ones.",
      "tldr_zh": "该论文提出 $Π$-NeSy，一种可能性神经符号方法（possibilistic neuro-symbolic approach），将神经网络的低级感知任务（如概率分布输出通过 softmax activation 转换为可能性分布）与基于可能性的规则系统的高级推理任务相结合，旨在为输入实例推导属于目标（元）概念的可能性度。中间概念通过规则系统连接，提供解释性支持，便于 justification 输入分类。技术贡献包括设计高效的矩阵关系和方程系统方法，用于推理和学习规则参数，并处理不一致的模糊关系方程（fuzzy relational equations）。实验在 MNIST 加法问题和 Sudoku 谜题上显示，该方法在有效性上优于现有神经符号方法。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.07055v1",
      "published_date": "2025-04-09 17:16:23 UTC",
      "updated_date": "2025-04-09 17:16:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:11:41.851926"
    },
    {
      "arxiv_id": "2504.06994v1",
      "title": "RayFronts: Open-Set Semantic Ray Frontiers for Online Scene Understanding and Exploration",
      "title_zh": "翻译失败",
      "authors": [
        "Omar Alama",
        "Avigyan Bhattacharya",
        "Haoyang He",
        "Seungchan Kim",
        "Yuheng Qiu",
        "Wenshan Wang",
        "Cherie Ho",
        "Nikhil Keetha",
        "Sebastian Scherer"
      ],
      "abstract": "Open-set semantic mapping is crucial for open-world robots. Current mapping\napproaches either are limited by the depth range or only map beyond-range\nentities in constrained settings, where overall they fail to combine\nwithin-range and beyond-range observations. Furthermore, these methods make a\ntrade-off between fine-grained semantics and efficiency. We introduce\nRayFronts, a unified representation that enables both dense and beyond-range\nefficient semantic mapping. RayFronts encodes task-agnostic open-set semantics\nto both in-range voxels and beyond-range rays encoded at map boundaries,\nempowering the robot to reduce search volumes significantly and make informed\ndecisions both within & beyond sensory range, while running at 8.84 Hz on an\nOrin AGX. Benchmarking the within-range semantics shows that RayFronts's\nfine-grained image encoding provides 1.34x zero-shot 3D semantic segmentation\nperformance while improving throughput by 16.5x. Traditionally, online mapping\nperformance is entangled with other system components, complicating evaluation.\nWe propose a planner-agnostic evaluation framework that captures the utility\nfor online beyond-range search and exploration, and show RayFronts reduces\nsearch volume 2.2x more efficiently than the closest online baselines.",
      "tldr_zh": "该研究提出了 RayFronts，一种统一的表示框架，用于在线场景理解和探索，能够高效处理 open-set 语义映射，包括近距离体素和远距离射线的编码，从而整合 within-range 和 beyond-range 观察。RayFronts 通过任务无关的 open-set 语义编码，帮助机器人显著减少搜索体积，并在 Orin AGX 上以 8.84 Hz 的速度运行，同时在细粒度语义上实现 1.34 倍的零样本 3D semantic segmentation 性能提升和 16.5 倍的吞吐量改进。作者还引入了一个 planner-agnostic 的评估框架，证明 RayFronts 比现有在线基准更高效地减少 2.2 倍的搜索体积，为开放世界机器人的决策提供可靠支持。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.06994v1",
      "published_date": "2025-04-09 16:06:58 UTC",
      "updated_date": "2025-04-09 16:06:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:11:53.205537"
    },
    {
      "arxiv_id": "2504.06987v2",
      "title": "Enhancing Metabolic Syndrome Prediction with Hybrid Data Balancing and Counterfactuals",
      "title_zh": "通过混合数据平衡和反事实增强代谢综合征预测",
      "authors": [
        "Sanyam Paresh Shah",
        "Abdullah Mamun",
        "Shovito Barua Soumma",
        "Hassan Ghasemzadeh"
      ],
      "abstract": "Metabolic Syndrome (MetS) is a cluster of interrelated risk factors that\nsignificantly increases the risk of cardiovascular diseases and type 2\ndiabetes. Despite its global prevalence, accurate prediction of MetS remains\nchallenging due to issues such as class imbalance, data scarcity, and\nmethodological inconsistencies in existing studies. In this paper, we address\nthese challenges by systematically evaluating and optimizing machine learning\n(ML) models for MetS prediction, leveraging advanced data balancing techniques\nand counterfactual analysis. Multiple ML models, including XGBoost, Random\nForest, TabNet, etc., were trained and compared under various data balancing\ntechniques such as random oversampling (ROS), SMOTE, ADASYN, and CTGAN.\nAdditionally, we introduce MetaBoost, a novel hybrid framework that integrates\nSMOTE, ADASYN, and CTGAN, optimizing synthetic data generation through weighted\naveraging and iterative weight tuning to enhance the model's performance\n(achieving up to a 1.87% accuracy improvement over individual balancing\ntechniques). A comprehensive counterfactual analysis is conducted to quantify\nthe feature-level changes required to shift individuals from high-risk to\nlow-risk categories. The results indicate that blood glucose (50.3%) and\ntriglycerides (46.7%) were the most frequently modified features, highlighting\ntheir clinical significance in MetS risk reduction. Additionally, probabilistic\nanalysis shows elevated blood glucose (85.5% likelihood) and triglycerides\n(74.9% posterior probability) as the strongest predictors. This study not only\nadvances the methodological rigor of MetS prediction but also provides\nactionable insights for clinicians and researchers, highlighting the potential\nof ML in mitigating the public health burden of metabolic syndrome.",
      "tldr_zh": "本研究针对代谢综合征 (MetS) 的预测挑战，如类不平衡和数据稀缺，评估了多种机器学习模型（包括 XGBoost、Random Forest 和 TabNet），并应用数据平衡技术（如 ROS、SMOTE、ADASYN 和 CTGAN）。他们提出了一种新型混合框架 MetaBoost，将 SMOTE、ADASYN 和 CTGAN 结合，通过加权平均和迭代权重调整优化合成数据生成，提高模型准确率高达 1.87%。反事实分析显示，血糖 (50.3% 修改频率) 和甘油三酯 (46.7%) 是关键特征，且血糖 (85.5% 可能性) 和甘油三酯 (74.9% 后验概率) 是最强预测因子。该研究不仅提升了 MetS 预测的方法严谨性，还为临床实践提供可操作洞见，以减轻其公共健康负担。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at the IEEE EMBC 2025 Conference. 7 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.06987v2",
      "published_date": "2025-04-09 15:51:10 UTC",
      "updated_date": "2025-05-07 11:50:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:12:05.336775"
    },
    {
      "arxiv_id": "2504.06963v1",
      "title": "RNN-Transducer-based Losses for Speech Recognition on Noisy Targets",
      "title_zh": "翻译失败",
      "authors": [
        "Vladimir Bataev"
      ],
      "abstract": "Training speech recognition systems on noisy transcripts is a significant\nchallenge in industrial pipelines, where datasets are enormous and ensuring\naccurate transcription for every instance is difficult. In this work, we\nintroduce novel loss functions to mitigate the impact of transcription errors\nin RNN-Transducer models. Our Star-Transducer loss addresses deletion errors by\nincorporating \"skip frame\" transitions in the loss lattice, restoring over 90%\nof the system's performance compared to models trained with accurate\ntranscripts. The Bypass-Transducer loss uses \"skip token\" transitions to tackle\ninsertion errors, recovering more than 60% of the quality. Finally, the\nTarget-Robust Transducer loss merges these approaches, offering robust\nperformance against arbitrary errors. Experimental results demonstrate that the\nTarget-Robust Transducer loss significantly improves RNN-T performance on noisy\ndata by restoring over 70% of the quality compared to well-transcribed data.",
      "tldr_zh": "本文提出新的损失函数，用于改善 RNN-Transducer 模型在噪声转录数据上的语音识别性能，旨在缓解转录错误的影响。Star-Transducer loss 通过引入 \"skip frame\" 过渡来处理删除错误，恢复了超过90%的系统性能；Bypass-Transducer loss 则使用 \"skip token\" 过渡应对插入错误，恢复了超过60%的质量。最终，Target-Robust Transducer loss 整合了这些方法，提供对任意错误的鲁棒性，并在实验中恢复了超过70%的性能。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "Final Project Report, Bachelor's Degree in Computer Science,\n  University of London, March 2024",
      "pdf_url": "http://arxiv.org/pdf/2504.06963v1",
      "published_date": "2025-04-09 15:18:29 UTC",
      "updated_date": "2025-04-09 15:18:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:12:17.585992"
    },
    {
      "arxiv_id": "2504.06962v2",
      "title": "Efficient Self-Supervised Learning for Earth Observation via Dynamic Dataset Curation",
      "title_zh": "高效自监督",
      "authors": [
        "Thomas Kerdreux",
        "Alexandre Tuel",
        "Quentin Febvre",
        "Alexis Mouche",
        "Bertrand Chapron"
      ],
      "abstract": "Self-supervised learning (SSL) has enabled the development of vision\nfoundation models for Earth Observation (EO), demonstrating strong\ntransferability across diverse remote sensing tasks. While prior work has\nfocused on network architectures and training strategies, the role of dataset\ncuration, especially in balancing and diversifying pre-training datasets,\nremains underexplored. In EO, this challenge is amplified by the redundancy and\nheavy-tailed distributions common in satellite imagery, which can lead to\nbiased representations and inefficient training.\n  In this work, we propose a dynamic dataset pruning strategy designed to\nimprove SSL pre-training by maximizing dataset diversity and balance. Our\nmethod iteratively refines the training set without requiring a pre-existing\nfeature extractor, making it well-suited for domains where curated datasets are\nlimited or unavailable. We demonstrate our approach on the Sentinel-1 Wave Mode\n(WV) Synthetic Aperture Radar (SAR) archive, a challenging dataset dominated by\nocean observations. We train models from scratch on the entire Sentinel-1 WV\narchive spanning 10 years. Across three downstream tasks, our results show that\ndynamic pruning improves both computational efficiency and representation\nquality, leading to stronger transferability.\n  We also release the weights of OceanSAR-1, the first model in the OceanSAR\nfamily, a series of foundation models for ocean observation and analysis using\nSAR imagery, at github.com/galeio-research/OceanSAR-models/.",
      "tldr_zh": "本文提出了一种动态数据集修剪策略，用于提升 Self-Supervised Learning (SSL) 在 Earth Observation (EO) 中的效率，解决卫星图像中冗余和重尾分布导致的偏见问题。该方法通过迭代精炼训练集，最大化数据集的多样性和平衡，而无需预先特征提取器，在 Sentinel-1 Wave Mode (WV) Synthetic Aperture Radar (SAR) 档案上进行从头训练。实验结果显示，该策略显著提高了计算效率和表示质量，并在三个下游任务上增强了模型的迁移性能。作者还发布了 OceanSAR-1 模型权重，作为 OceanSAR 系列的首个基础模型，用于海洋观测和分析。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at CVPR Workshop : The First Workshop on Foundation and\n  Large Vision Models in Remote Sensing",
      "pdf_url": "http://arxiv.org/pdf/2504.06962v2",
      "published_date": "2025-04-09 15:13:26 UTC",
      "updated_date": "2025-04-28 15:32:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:12:30.823064"
    },
    {
      "arxiv_id": "2504.06949v1",
      "title": "Adaptive Computation Pruning for the Forgetting Transformer",
      "title_zh": "Forgetting Transformer 的适应性计算修剪",
      "authors": [
        "Zhixuan Lin",
        "Johan Obando-Ceron",
        "Xu Owen He",
        "Aaron Courville"
      ],
      "abstract": "The recently proposed Forgetting Transformer (FoX) incorporates a forget gate\ninto softmax attention and has shown consistently better or on-par performance\ncompared to the standard RoPE-based Transformer. Notably, many attention heads\nin FoX tend to forget quickly, causing their output at each timestep to rely\nprimarily on the local context. Based on this observation, we propose Adaptive\nComputation Pruning (ACP) for FoX, a method that dynamically prunes\ncomputations involving input-output dependencies that are strongly decayed by\nthe forget gate. This is achieved using a dynamically set pruning threshold\nthat ensures that the pruned attention weights remain negligible. We apply ACP\nto language model pretraining with FoX and show it consistently reduces the\nnumber of FLOPs in softmax attention by around 70% across different model sizes\nand context lengths, resulting in a roughly 10% to 35% improvement in training\nthroughput. Furthermore, longer context lengths yield greater computational\nsavings. All these speed improvements are achieved without any performance\ndegradation. We also perform several analyses to provide deeper insights into\nour method, such as examining the pruning patterns and analyzing the\ndistribution of FLOP savings across different attention heads. Our code is\navailable at https://github.com/zhixuan-lin/arctic-fox.",
      "tldr_zh": "该论文提出 Adaptive Computation Pruning (ACP) 方法，用于优化 Forgetting Transformer (FoX)，后者通过在 softmax attention 中加入 forget gate 实现了比标准 RoPE-based Transformer 更好的性能。ACP 通过动态设置修剪阈值，修剪 forget gate 强烈衰减的输入-输出依赖，从而在语言模型预训练中减少 softmax attention 的 FLOPs 约70%，并提高训练吞吐量10%至35%。实验结果显示，该方法在不同模型大小和上下文长度下无性能损失，且更长上下文带来更大计算节省。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint. Under review",
      "pdf_url": "http://arxiv.org/pdf/2504.06949v1",
      "published_date": "2025-04-09 14:57:55 UTC",
      "updated_date": "2025-04-09 14:57:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:12:41.212435"
    },
    {
      "arxiv_id": "2504.06943v2",
      "title": "Review of Case-Based Reasoning for LLM Agents: Theoretical Foundations, Architectural Components, and Cognitive Integration",
      "title_zh": "翻译失败",
      "authors": [
        "Kostas Hatalis",
        "Despina Christou",
        "Vyshnavi Kondapalli"
      ],
      "abstract": "Agents powered by Large Language Models (LLMs) have recently demonstrated\nimpressive capabilities in various tasks. Still, they face limitations in tasks\nrequiring specific, structured knowledge, flexibility, or accountable\ndecision-making. While agents are capable of perceiving their environments,\nforming inferences, planning, and executing actions towards goals, they often\nface issues such as hallucinations and lack of contextual memory across\ninteractions. This paper explores how Case-Based Reasoning (CBR), a strategy\nthat solves new problems by referencing past experiences, can be integrated\ninto LLM agent frameworks. This integration allows LLMs to leverage explicit\nknowledge, enhancing their effectiveness. We systematically review the\ntheoretical foundations of these enhanced agents, identify critical framework\ncomponents, and formulate a mathematical model for the CBR processes of case\nretrieval, adaptation, and learning. We also evaluate CBR-enhanced agents\nagainst other methods like Chain-of-Thought reasoning and standard\nRetrieval-Augmented Generation, analyzing their relative strengths. Moreover,\nwe explore how leveraging CBR's cognitive dimensions (including\nself-reflection, introspection, and curiosity) via goal-driven autonomy\nmechanisms can further enhance the LLM agent capabilities. Contributing to the\nongoing research on neuro-symbolic hybrid systems, this work posits CBR as a\nviable technique for enhancing the reasoning skills and cognitive aspects of\nautonomous LLM agents.",
      "tldr_zh": "本论文审视了如何将 Case-Based Reasoning (CBR) 整合到 Large Language Models (LLMs) 代理中，以解决其在特定任务中的局限性，如幻觉问题和缺乏上下文记忆。研究系统回顾了 CBR 的理论基础、关键框架组件，并构建了 CBR 过程的数学模型，包括案例检索、适应和学习。论文通过比较 CBR 与 Chain-of-Thought 推理和 Retrieval-Augmented Generation 等方法，展示了 CBR 在提升代理的推理技能和认知维度（如自反省、内省和好奇心）方面的优势，最终为神经符号混合系统的发展提供可行技术。",
      "categories": [
        "cs.AI",
        "cs.MA",
        "68",
        "I.2; I.2.7"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.06943v2",
      "published_date": "2025-04-09 14:51:02 UTC",
      "updated_date": "2025-04-11 05:34:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:12:53.460817"
    },
    {
      "arxiv_id": "2504.06928v1",
      "title": "Beyond Tools: Generative AI as Epistemic Infrastructure in Education",
      "title_zh": "超越工具：生成式 AI 作为教育中的认识论基础设施",
      "authors": [
        "Bodong Chen"
      ],
      "abstract": "As generative AI rapidly integrates into educational infrastructures\nworldwide, it transforms how knowledge gets created, validated, and shared, yet\ncurrent discourse inadequately addresses its implications as epistemic\ninfrastructure mediating teaching and learning. This paper investigates how AI\nsystems function as epistemic infrastructures in education and their impact on\nhuman epistemic agency. Adopting a situated cognition perspective and following\na value-sensitive design approach, the study conducts a technical investigation\nof two representative AI systems in educational settings, analyzing their\nimpact on teacher practice across three dimensions: affordances for skilled\nepistemic actions, support for epistemic sensitivity, and implications for\nlong-term habit formation. The analysis reveals that current AI systems\ninadequately support teachers' skilled epistemic actions, insufficiently foster\nepistemic sensitivity, and potentially cultivate problematic habits that\nprioritize efficiency over epistemic agency. To address these challenges, the\npaper recommends recognizing the infrastructural transformation occurring in\neducation, developing AI environments that stimulate skilled actions while\nupholding epistemic norms, and involving educators in AI design processes --\nrecommendations aimed at fostering AI integration that aligns with core\neducational values and maintains human epistemic agency.",
      "tldr_zh": "这篇论文探讨了生成式 AI 作为教育中的认知基础设施（epistemic infrastructure），分析其如何改变知识的创建、验证和共享，并对人类认知代理（epistemic agency）的影响。采用 situated cognition 视角和 value-sensitive design 方法，该研究对两个代表性 AI 系统进行了技术调查，评估其对教师实践的三个维度：支持熟练认知行动、促进认知敏感性，以及对长期习惯形成的影响。结果发现，当前 AI 系统不足以支持教师的熟练认知行动，未能充分培养认知敏感性，并可能导致优先效率而非认知代理的坏习惯。为应对这些挑战，论文推荐认识到教育基础设施的转变，开发能刺激熟练行动并维护认知规范的 AI 环境，并让教育者参与 AI 设计，以确保 AI 整合符合核心教育价值观。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "K.3.1; K.4.3; H.5.2"
      ],
      "primary_category": "cs.CY",
      "comment": "23 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.06928v1",
      "published_date": "2025-04-09 14:35:30 UTC",
      "updated_date": "2025-04-09 14:35:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:13:05.790000"
    },
    {
      "arxiv_id": "2504.06925v1",
      "title": "Are Vision-Language Models Ready for Dietary Assessment? Exploring the Next Frontier in AI-Powered Food Image Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Sergio Romero-Tapiador",
        "Ruben Tolosana",
        "Blanca Lacruz-Pleguezuelos",
        "Laura Judith Marcos Zambrano",
        "Guadalupe X. Bazán",
        "Isabel Espinosa-Salinas",
        "Julian Fierrez",
        "Javier Ortega-Garcia",
        "Enrique Carrillo de Santa Pau",
        "Aythami Morales"
      ],
      "abstract": "Automatic dietary assessment based on food images remains a challenge,\nrequiring precise food detection, segmentation, and classification.\nVision-Language Models (VLMs) offer new possibilities by integrating visual and\ntextual reasoning. In this study, we evaluate six state-of-the-art VLMs\n(ChatGPT, Gemini, Claude, Moondream, DeepSeek, and LLaVA), analyzing their\ncapabilities in food recognition at different levels. For the experimental\nframework, we introduce the FoodNExTDB, a unique food image database that\ncontains 9,263 expert-labeled images across 10 categories (e.g., \"protein\nsource\"), 62 subcategories (e.g., \"poultry\"), and 9 cooking styles (e.g.,\n\"grilled\"). In total, FoodNExTDB includes 50k nutritional labels generated by\nseven experts who manually annotated all images in the database. Also, we\npropose a novel evaluation metric, Expert-Weighted Recall (EWR), that accounts\nfor the inter-annotator variability. Results show that closed-source models\noutperform open-source ones, achieving over 90% EWR in recognizing food\nproducts in images containing a single product. Despite their potential,\ncurrent VLMs face challenges in fine-grained food recognition, particularly in\ndistinguishing subtle differences in cooking styles and visually similar food\nitems, which limits their reliability for automatic dietary assessment. The\nFoodNExTDB database is publicly available at\nhttps://github.com/AI4Food/FoodNExtDB.",
      "tldr_zh": "这篇论文评估了六种视觉语言模型（VLMs，包括 ChatGPT、Gemini 等）在基于食物图像的自动饮食评估中的表现，焦点在于食物检测、分割和分类。研究者引入了 FoodNExTDB 数据库，该数据库包含 9,263 张专家标注的图像，涵盖 10 个类别、62 个子类别和 9 个烹饪风格，并提出 Expert-Weighted Recall (EWR) 指标来处理标注者间变异性。结果显示，闭源模型在单产品图像识别中超过 90% EWR，但所有模型在细粒度任务（如区分烹饪风格或相似食物）上面临挑战，限制了其在饮食评估中的可靠性。FoodNExTDB 数据库已公开可用，为未来 AI 食物识别研究提供资源。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at IEEE/CVF Computer Vision and Pattern Recognition\n  Conference workshops 2025 (CVPRw) 10 pages, 4 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2504.06925v1",
      "published_date": "2025-04-09 14:33:59 UTC",
      "updated_date": "2025-04-09 14:33:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:13:18.418183"
    },
    {
      "arxiv_id": "2504.06924v1",
      "title": "Longitudinal Assessment of Lung Lesion Burden in CT",
      "title_zh": "CT 中肺部病变负担的纵向评估",
      "authors": [
        "Tejas Sudharshan Mathai",
        "Benjamin Hou",
        "Ronald M. Summers"
      ],
      "abstract": "In the U.S., lung cancer is the second major cause of death. Early detection\nof suspicious lung nodules is crucial for patient treatment planning,\nmanagement, and improving outcomes. Many approaches for lung nodule\nsegmentation and volumetric analysis have been proposed, but few have looked at\nlongitudinal changes in total lung tumor burden. In this work, we trained two\n3D models (nnUNet) with and without anatomical priors to automatically segment\nlung lesions and quantified total lesion burden for each patient. The 3D model\nwithout priors significantly outperformed ($p < .001$) the model trained with\nanatomy priors. For detecting clinically significant lesions $>$ 1cm, a\nprecision of 71.3\\%, sensitivity of 68.4\\%, and F1-score of 69.8\\% was\nachieved. For segmentation, a Dice score of 77.1 $\\pm$ 20.3 and Hausdorff\ndistance error of 11.7 $\\pm$ 24.1 mm was obtained. The median lesion burden was\n6.4 cc (IQR: 2.1, 18.1) and the median volume difference between manual and\nautomated measurements was 0.02 cc (IQR: -2.8, 1.2). Agreements were also\nevaluated with linear regression and Bland-Altman plots. The proposed approach\ncan produce a personalized evaluation of the total tumor burden for a patient\nand facilitate interval change tracking over time.",
      "tldr_zh": "本文研究了通过 CT 图像对肺病变负担进行纵向评估，以提升肺癌早发现和治疗效果。研究者训练了两个 3D nnUNet 模型，其中不使用 anatomical priors 的模型在分割性能上显著优于使用 priors 的模型（p < 0.001），并在检测大于1cm的临床重要病变时达到了71.3%精确度、68.4%敏感度和69.8% F1-score。分割结果显示 Dice score 为77.1 ± 20.3 和 Hausdorff distance error 为11.7 ± 24.1 mm，且手动与自动测量体积差的中位值为0.02 cc。该方法能自动量化患者的总肿瘤负担，支持个性化评估和时间变化跟踪。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "Published at SPIE Medical Imaging 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.06924v1",
      "published_date": "2025-04-09 14:30:43 UTC",
      "updated_date": "2025-04-09 14:30:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:13:32.151931"
    },
    {
      "arxiv_id": "2504.06921v1",
      "title": "Leveraging Anatomical Priors for Automated Pancreas Segmentation on Abdominal CT",
      "title_zh": "翻译失败",
      "authors": [
        "Anisa V. Prasad",
        "Tejas Sudharshan Mathai",
        "Pritam Mukherjee",
        "Jianfei Liu",
        "Ronald M. Summers"
      ],
      "abstract": "An accurate segmentation of the pancreas on CT is crucial to identify\npancreatic pathologies and extract imaging-based biomarkers. However, prior\nresearch on pancreas segmentation has primarily focused on modifying the\nsegmentation model architecture or utilizing pre- and post-processing\ntechniques. In this article, we investigate the utility of anatomical priors to\nenhance the segmentation performance of the pancreas. Two 3D full-resolution\nnnU-Net models were trained, one with 8 refined labels from the public PANORAMA\ndataset, and another that combined them with labels derived from the public\nTotalSegmentator (TS) tool. The addition of anatomical priors resulted in a 6\\%\nincrease in Dice score ($p < .001$) and a 36.5 mm decrease in Hausdorff\ndistance for pancreas segmentation ($p < .001$). Moreover, the pancreas was\nalways detected when anatomy priors were used, whereas there were 8 instances\nof failed detections without their use. The use of anatomy priors shows promise\nfor pancreas segmentation and subsequent derivation of imaging biomarkers.",
      "tldr_zh": "本文提出利用解剖先验(Anatomical Priors)来提升胰腺在腹部 CT 上的自动分割性能，旨在改善胰腺病变识别和基于图像的生物标志物提取。研究者训练了两个 3D 全分辨率 nnU-Net 模型：一个使用 PANORAMA 数据集的 8 个精炼标签，另一个结合 TotalSegmentator (TS) 工具的标签。结果显示，添加解剖先验后，Dice score 提高了 6%（p < 0.001），Hausdorff distance 降低了 36.5 mm（p < 0.001），并完全消除了检测失败的情况。该方法为胰腺分割和后续临床应用提供了显著改进前景。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "Published at SPIE Medical Imaging 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.06921v1",
      "published_date": "2025-04-09 14:29:08 UTC",
      "updated_date": "2025-04-09 14:29:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:13:42.329100"
    },
    {
      "arxiv_id": "2504.06915v1",
      "title": "An Analysis of Temporal Dropout in Earth Observation Time Series for Regression Tasks",
      "title_zh": "翻译失败",
      "authors": [
        "Miro Miranda",
        "Francisco Mena",
        "Andreas Dengel"
      ],
      "abstract": "Missing instances in time series data impose a significant challenge to deep\nlearning models, particularly in regression tasks. In the Earth Observation\nfield, satellite failure or cloud occlusion frequently results in missing\ntime-steps, introducing uncertainties in the predicted output and causing a\ndecline in predictive performance. While many studies address missing\ntime-steps through data augmentation to improve model robustness, the\nuncertainty arising at the input level is commonly overlooked. To address this\ngap, we introduce Monte Carlo Temporal Dropout (MC-TD), a method that\nexplicitly accounts for input-level uncertainty by randomly dropping time-steps\nduring inference using a predefined dropout ratio, thereby simulating the\neffect of missing data. To bypass the need for costly searches for the optimal\ndropout ratio, we extend this approach with Monte Carlo Concrete Temporal\nDropout (MC-ConcTD), a method that learns the optimal dropout distribution\ndirectly. Both MC-TD and MC-ConcTD are applied during inference, leveraging\nMonte Carlo sampling for uncertainty quantification. Experiments on three EO\ntime-series datasets demonstrate that MC-ConcTD improves predictive performance\nand uncertainty calibration compared to existing approaches. Additionally, we\nhighlight the advantages of adaptive dropout tuning over manual selection,\nmaking uncertainty quantification more robust and accessible for EO\napplications.",
      "tldr_zh": "该论文分析了地球观测（EO）时间序列数据中缺失时间步对回归任务的影响，强调现有方法忽略了输入级不确定性问题。作者提出 Monte Carlo Temporal Dropout (MC-TD) 方法，通过在推理时随机丢弃时间步模拟缺失数据，并扩展为 Monte Carlo Concrete Temporal Dropout (MC-ConcTD)，该方法自动学习最优丢弃分布以避免手动搜索。实验结果显示，在三个 EO 时间序列数据集上，MC-ConcTD 比现有方法提升了预测性能和不确定性校准，并证明了自适应丢弃调整的鲁棒性和实用性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at Symposium on Intelligent Data Analysis (IDA 2025)",
      "pdf_url": "http://arxiv.org/pdf/2504.06915v1",
      "published_date": "2025-04-09 14:23:04 UTC",
      "updated_date": "2025-04-09 14:23:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:13:53.640608"
    },
    {
      "arxiv_id": "2504.13908v1",
      "title": "AI-Assisted Conversational Interviewing: Effects on Data Quality and User Experience",
      "title_zh": "AI 辅助的对话式访谈：对数据质量和用户体验的影响",
      "authors": [
        "Soubhik Barari",
        "Jarret Angbazo",
        "Natalie Wang",
        "Leah M. Christian",
        "Elizabeth Dean",
        "Zoe Slowinski",
        "Brandon Sepulvado"
      ],
      "abstract": "Standardized surveys scale efficiently but sacrifice depth, while\nconversational interviews improve response quality at the cost of scalability\nand consistency. This study bridges the gap between these methods by\nintroducing a framework for AI-assisted conversational interviewing. To\nevaluate this framework, we conducted a web survey experiment where 1,800\nparticipants were randomly assigned to text-based conversational AI agents, or\n\"textbots\", to dynamically probe respondents for elaboration and interactively\ncode open-ended responses. We assessed textbot performance in terms of coding\naccuracy, response quality, and respondent experience. Our findings reveal that\ntextbots perform moderately well in live coding even without survey-specific\nfine-tuning, despite slightly inflated false positive errors due to respondent\nacquiescence bias. Open-ended responses were more detailed and informative, but\nthis came at a slight cost to respondent experience. Our findings highlight the\nfeasibility of using AI methods to enhance open-ended data collection in web\nsurveys.",
      "tldr_zh": "本研究提出了一种AI辅助对话式访谈框架，旨在桥接标准化调查的效率与对话式访谈的深度，评估其对数据质量和用户体验的影响。通过网络调查实验，1800名参与者被随机分配到文本机器人（textbots），这些AI代理动态探查回应并实时编码开放式回答。结果显示，textbots在编码准确性上表现出中等水平，尽管存在轻微的假阳性错误（如受访者顺从偏差），但成功提升了回应的详细性和信息量，尽管略微降低了受访者体验。该框架证明了AI方法在网络调查中增强开放式数据收集的可行性。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "stat.AP"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13908v1",
      "published_date": "2025-04-09 13:58:07 UTC",
      "updated_date": "2025-04-09 13:58:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:14:04.578843"
    },
    {
      "arxiv_id": "2504.06897v1",
      "title": "MedSegFactory: Text-Guided Generation of Medical Image-Mask Pairs",
      "title_zh": "MedSegFactory：文本引导的医疗图像-掩码对生成",
      "authors": [
        "Jiawei Mao",
        "Yuhan Wang",
        "Yucheng Tang",
        "Daguang Xu",
        "Kang Wang",
        "Yang Yang",
        "Zongwei Zhou",
        "Yuyin Zhou"
      ],
      "abstract": "This paper presents MedSegFactory, a versatile medical synthesis framework\nthat generates high-quality paired medical images and segmentation masks across\nmodalities and tasks. It aims to serve as an unlimited data repository,\nsupplying image-mask pairs to enhance existing segmentation tools. The core of\nMedSegFactory is a dual-stream diffusion model, where one stream synthesizes\nmedical images and the other generates corresponding segmentation masks. To\nensure precise alignment between image-mask pairs, we introduce Joint\nCross-Attention (JCA), enabling a collaborative denoising paradigm by dynamic\ncross-conditioning between streams. This bidirectional interaction allows both\nrepresentations to guide each other's generation, enhancing consistency between\ngenerated pairs. MedSegFactory unlocks on-demand generation of paired medical\nimages and segmentation masks through user-defined prompts that specify the\ntarget labels, imaging modalities, anatomical regions, and pathological\nconditions, facilitating scalable and high-quality data generation. This new\nparadigm of medical image synthesis enables seamless integration into diverse\nmedical imaging workflows, enhancing both efficiency and accuracy. Extensive\nexperiments show that MedSegFactory generates data of superior quality and\nusability, achieving competitive or state-of-the-art performance in 2D and 3D\nsegmentation tasks while addressing data scarcity and regulatory constraints.",
      "tldr_zh": "本文提出 MedSegFactory，一种基于文本引导的医疗合成框架，能够生成高质量的配对医疗图像和分割掩码，适用于多种模态和任务，从而作为无限数据仓库增强现有分割工具。框架的核心是双流扩散模型，其中一个流合成图像，另一个生成对应掩码，并通过 Joint Cross-Attention (JCA) 实现动态交叉条件化，确保图像-掩码对的精确对齐和一致性。用户可通过自定义提示（如目标标签、成像模态、解剖区域和病理条件）实现按需数据生成，支持可扩展的医疗工作流。实验表明，MedSegFactory 在2D和3D分割任务中达到竞争或最先进性能，有效解决数据稀缺和监管约束问题。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "12 pages, 8 figures, The project page can be accessed via\n  https://jwmao1.github.io/MedSegFactory_web",
      "pdf_url": "http://arxiv.org/pdf/2504.06897v1",
      "published_date": "2025-04-09 13:56:05 UTC",
      "updated_date": "2025-04-09 13:56:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:14:17.274750"
    },
    {
      "arxiv_id": "2504.06884v1",
      "title": "Audio-visual Event Localization on Portrait Mode Short Videos",
      "title_zh": "翻译失败",
      "authors": [
        "Wuyang Liu",
        "Yi Chai",
        "Yongpeng Yan",
        "Yanzhen Ren"
      ],
      "abstract": "Audio-visual event localization (AVEL) plays a critical role in multimodal\nscene understanding. While existing datasets for AVEL predominantly comprise\nlandscape-oriented long videos with clean and simple audio context, short\nvideos have become the primary format of online video content due to the the\nproliferation of smartphones. Short videos are characterized by\nportrait-oriented framing and layered audio compositions (e.g., overlapping\nsound effects, voiceovers, and music), which brings unique challenges\nunaddressed by conventional methods. To this end, we introduce AVE-PM, the\nfirst AVEL dataset specifically designed for portrait mode short videos,\ncomprising 25,335 clips that span 86 fine-grained categories with frame-level\nannotations. Beyond dataset creation, our empirical analysis shows that\nstate-of-the-art AVEL methods suffer an average 18.66% performance drop during\ncross-mode evaluation. Further analysis reveals two key challenges of different\nvideo formats: 1) spatial bias from portrait-oriented framing introduces\ndistinct domain priors, and 2) noisy audio composition compromise the\nreliability of audio modality. To address these issues, we investigate optimal\npreprocessing recipes and the impact of background music for AVEL on portrait\nmode videos. Experiments show that these methods can still benefit from\ntailored preprocessing and specialized model design, thus achieving improved\nperformance. This work provides both a foundational benchmark and actionable\ninsights for advancing AVEL research in the era of mobile-centric video\ncontent. Dataset and code will be released.",
      "tldr_zh": "这篇论文针对音频-视觉事件定位 (AVEL) 在肖像模式短视频上的应用，引入了首个专属数据集 AVE-PM，包含 25,335 个剪辑、86 个细粒度类别，并提供帧级标注，以应对短视频的独特挑战，如肖像取向和复杂音频组成。\n实验分析显示，现有的 AVEL 方法在跨模式评估中性能平均下降 18.66%，主要源于空间偏差和音频模态的可靠性问题。\n作者探索了优化预处理策略和背景音乐的影响，通过定制预处理及专用模型设计，显著提升了性能。\n这项工作为 AVEL 研究提供了关键基准和实用见解，推动移动视频内容时代的多模态场景理解。",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.MM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.06884v1",
      "published_date": "2025-04-09 13:38:40 UTC",
      "updated_date": "2025-04-09 13:38:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:14:30.613066"
    },
    {
      "arxiv_id": "2504.06881v1",
      "title": "Compound and Parallel Modes of Tropical Convolutional Neural Networks",
      "title_zh": "热带卷积神经网络的复合和并行模式",
      "authors": [
        "Mingbo Li",
        "Liying Liu",
        "Ye Luo"
      ],
      "abstract": "Convolutional neural networks have become increasingly deep and complex,\nleading to higher computational costs. While tropical convolutional neural\nnetworks (TCNNs) reduce multiplications, they underperform compared to standard\nCNNs. To address this, we propose two new variants - compound TCNN (cTCNN) and\nparallel TCNN (pTCNN)-that use combinations of tropical min-plus and max-plus\nkernels to replace traditional convolution kernels. This reduces\nmultiplications and balances efficiency with performance. Experiments on\nvarious datasets show that cTCNN and pTCNN match or exceed the performance of\nother CNN methods. Combining these with conventional CNNs in deeper\narchitectures also improves performance. We are further exploring simplified\nTCNN architectures that reduce parameters and multiplications with minimal\naccuracy loss, aiming for efficient and effective models.",
      "tldr_zh": "该研究针对传统卷积神经网络(CNNs)的计算成本高问题，提出两种新型变体：compound TCNN (cTCNN) 和 parallel TCNN (pTCNN)，它们通过结合 tropical min-plus 和 max-plus 内核来替换标准卷积内核，从而减少乘法运算并平衡效率与性能。实验结果显示，在多种数据集上，cTCNN 和 pTCNN 的表现匹配或超过了其他 CNN 方法；此外，将这些变体与传统 CNN 整合到更深层架构中还能进一步提升整体性能。该工作还探讨了简化 TCNN 架构，以减少参数和乘法运算，同时最小化准确率损失，旨在开发更高效的模型。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "I.2.6"
      ],
      "primary_category": "cs.CV",
      "comment": "28 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.06881v1",
      "published_date": "2025-04-09 13:36:11 UTC",
      "updated_date": "2025-04-09 13:36:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:14:40.977098"
    },
    {
      "arxiv_id": "2504.08818v1",
      "title": "From Text to Time? Rethinking the Effectiveness of the Large Language Model for Time Series Forecasting",
      "title_zh": "翻译失败",
      "authors": [
        "Xinyu Zhang",
        "Shanshan Feng",
        "Xutao Li"
      ],
      "abstract": "Using pre-trained large language models (LLMs) as the backbone for time\nseries prediction has recently gained significant research interest. However,\nthe effectiveness of LLM backbones in this domain remains a topic of debate.\nBased on thorough empirical analyses, we observe that training and testing\nLLM-based models on small datasets often leads to the Encoder and Decoder\nbecoming overly adapted to the dataset, thereby obscuring the true predictive\ncapabilities of the LLM backbone. To investigate the genuine potential of LLMs\nin time series prediction, we introduce three pre-training models with\nidentical architectures but different pre-training strategies. Thereby,\nlarge-scale pre-training allows us to create unbiased Encoder and Decoder\ncomponents tailored to the LLM backbone. Through controlled experiments, we\nevaluate the zero-shot and few-shot prediction performance of the LLM, offering\ninsights into its capabilities. Extensive experiments reveal that although the\nLLM backbone demonstrates some promise, its forecasting performance is limited.\nOur source code is publicly available in the anonymous repository:\nhttps://anonymous.4open.science/r/LLM4TS-0B5C.",
      "tldr_zh": "这篇论文重新审视了大型语言模型 (LLMs) 在时间序列预测中的有效性，发现小数据集训练常导致 Encoder 和 Decoder 过度适应数据，从而掩盖 LLM 骨干的真实能力。作者引入了三种具有相同架构但不同预训练策略的模型，通过大规模预训练创建无偏的组件，并通过控制实验评估了 LLM 的零样本和少样本预测性能。实验结果表明，LLM 虽显示出一些潜力，但其整体预测性能有限。该研究公开了源代码，以促进进一步探讨。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.08818v1",
      "published_date": "2025-04-09 13:20:09 UTC",
      "updated_date": "2025-04-09 13:20:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:14:53.141049"
    },
    {
      "arxiv_id": "2504.06868v3",
      "title": "Persona Dynamics: Unveiling the Impact of Personality Traits on Agents in Text-Based Games",
      "title_zh": "翻译失败",
      "authors": [
        "Seungwon Lim",
        "Seungbeen Lee",
        "Dongjun Min",
        "Youngjae Yu"
      ],
      "abstract": "Artificial agents are increasingly central to complex interactions and\ndecision-making tasks, yet aligning their behaviors with desired human values\nremains an open challenge. In this work, we investigate how human-like\npersonality traits influence agent behavior and performance within text-based\ninteractive environments. We introduce PANDA: Personality Adapted Neural\nDecision Agents, a novel method for projecting human personality traits onto\nagents to guide their behavior. To induce personality in a text-based game\nagent, (i) we train a personality classifier to identify what personality type\nthe agent's actions exhibit, and (ii) we integrate the personality profiles\ndirectly into the agent's policy-learning pipeline. By deploying agents\nembodying 16 distinct personality types across 25 text-based games and\nanalyzing their trajectories, we demonstrate that an agent's action decisions\ncan be guided toward specific personality profiles. Moreover, certain\npersonality types, such as those characterized by higher levels of Openness,\ndisplay marked advantages in performance. These findings underscore the promise\nof personality-adapted agents for fostering more aligned, effective, and\nhuman-centric decision-making in interactive environments.",
      "tldr_zh": "本文研究了人类个性特征对文本-based games中AI代理行为和性能的影响，提出了一种新方法PANDA（Personality Adapted Neural Decision Agents），通过训练个性分类器并将其整合到代理的政策学习管道中，来引导代理行为符合特定个性类型。在25个文本游戏中测试了16种个性类型后，发现某些特征如高Openness能显著提升代理性能。这些发现突显了个性适应代理在促进更对齐人类价值和有效决策的互动环境中的潜力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.06868v3",
      "published_date": "2025-04-09 13:17:00 UTC",
      "updated_date": "2025-04-28 07:35:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:15:04.714463"
    },
    {
      "arxiv_id": "2504.06866v1",
      "title": "GraspClutter6D: A Large-scale Real-world Dataset for Robust Perception and Grasping in Cluttered Scenes",
      "title_zh": "GraspClutter6D：用于杂乱场景中鲁棒感知和抓取的大规模真实世界数据集",
      "authors": [
        "Seunghyeok Back",
        "Joosoon Lee",
        "Kangmin Kim",
        "Heeseon Rho",
        "Geonhyup Lee",
        "Raeyoung Kang",
        "Sangbeom Lee",
        "Sangjun Noh",
        "Youngjin Lee",
        "Taeyeop Lee",
        "Kyoobin Lee"
      ],
      "abstract": "Robust grasping in cluttered environments remains an open challenge in\nrobotics. While benchmark datasets have significantly advanced deep learning\nmethods, they mainly focus on simplistic scenes with light occlusion and\ninsufficient diversity, limiting their applicability to practical scenarios. We\npresent GraspClutter6D, a large-scale real-world grasping dataset featuring:\n(1) 1,000 highly cluttered scenes with dense arrangements (14.1 objects/scene,\n62.6\\% occlusion), (2) comprehensive coverage across 200 objects in 75\nenvironment configurations (bins, shelves, and tables) captured using four\nRGB-D cameras from multiple viewpoints, and (3) rich annotations including 736K\n6D object poses and 9.3B feasible robotic grasps for 52K RGB-D images. We\nbenchmark state-of-the-art segmentation, object pose estimation, and grasping\ndetection methods to provide key insights into challenges in cluttered\nenvironments. Additionally, we validate the dataset's effectiveness as a\ntraining resource, demonstrating that grasping networks trained on\nGraspClutter6D significantly outperform those trained on existing datasets in\nboth simulation and real-world experiments. The dataset, toolkit, and\nannotation tools are publicly available on our project website:\nhttps://sites.google.com/view/graspclutter6d.",
      "tldr_zh": "这篇论文介绍了 GraspClutter6D，一个大规模真实世界数据集，旨在解决机器人学中杂乱环境下的鲁棒感知和抓取挑战，弥补现有数据集在遮挡和多样性方面的不足。数据集包含 1000 个高度杂乱场景（每场景平均 14.1 个物体，遮挡率 62.6%），覆盖 200 个物体和 75 个环境配置（如 bins、shelves 和 tables），并使用四个 RGB-D 相机从多个视角捕获，提供 736K 6D object poses 和 9.3B 可行机器人抓取动作的丰富标注。通过基准测试最先进的分割、对象位姿估计和抓取检测方法，论文揭示了杂乱环境的关键挑战，并证明在 GraspClutter6D 上训练的抓取网络在模拟和真实世界实验中显著优于现有数据集。该数据集及其工具已公开可用，以支持进一步研究。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.06866v1",
      "published_date": "2025-04-09 13:15:46 UTC",
      "updated_date": "2025-04-09 13:15:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:15:18.973946"
    },
    {
      "arxiv_id": "2504.06861v1",
      "title": "EIDT-V: Exploiting Intersections in Diffusion Trajectories for Model-Agnostic, Zero-Shot, Training-Free Text-to-Video Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Diljeet Jagpal",
        "Xi Chen",
        "Vinay P. Namboodiri"
      ],
      "abstract": "Zero-shot, training-free, image-based text-to-video generation is an emerging\narea that aims to generate videos using existing image-based diffusion models.\nCurrent methods in this space require specific architectural changes to image\ngeneration models, which limit their adaptability and scalability. In contrast\nto such methods, we provide a model-agnostic approach. We use intersections in\ndiffusion trajectories, working only with the latent values. We could not\nobtain localized frame-wise coherence and diversity using only the intersection\nof trajectories. Thus, we instead use a grid-based approach. An in-context\ntrained LLM is used to generate coherent frame-wise prompts; another is used to\nidentify differences between frames. Based on these, we obtain a CLIP-based\nattention mask that controls the timing of switching the prompts for each grid\ncell. Earlier switching results in higher variance, while later switching\nresults in more coherence. Therefore, our approach can ensure appropriate\ncontrol between coherence and variance for the frames. Our approach results in\nstate-of-the-art performance while being more flexible when working with\ndiverse image-generation models. The empirical analysis using quantitative\nmetrics and user studies confirms our model's superior temporal consistency,\nvisual fidelity and user satisfaction, thus providing a novel way to obtain\ntraining-free, image-based text-to-video generation.",
      "tldr_zh": "该研究提出EIDT-V，一种模型无关、零样本、无需训练的文本到视频生成方法，通过利用扩散轨迹(Diffusion Trajectories)的交点，仅处理潜在值来实现视频生成。方法采用基于网格(grid-based)的策略，使用in-context训练的LLM生成帧级提示并识别帧间差异，然后通过CLIP-based注意力掩码控制提示切换时机，以平衡帧间一致性和变异性。实验结果显示，该方法在定量指标和用户研究中表现出色，显著提升了视频的时间一致性、视觉保真度和用户满意度，提供了一种灵活的图像生成模型兼容方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.06861v1",
      "published_date": "2025-04-09 13:11:09 UTC",
      "updated_date": "2025-04-09 13:11:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:15:29.919813"
    },
    {
      "arxiv_id": "2504.06843v1",
      "title": "Integrating Cognitive Processing Signals into Language Models: A Review of Advances, Applications and Future Directions",
      "title_zh": "翻译失败",
      "authors": [
        "Angela Lopez-Cardona",
        "Sebastian Idesis",
        "Ioannis Arapakis"
      ],
      "abstract": "Recently, the integration of cognitive neuroscience in Natural Language\nProcessing (NLP) has gained significant attention. This article provides a\ncritical and timely overview of recent advancements in leveraging cognitive\nsignals, particularly Eye-tracking (ET) signals, to enhance Language Models\n(LMs) and Multimodal Large Language Models (MLLMs). By incorporating\nuser-centric cognitive signals, these approaches address key challenges,\nincluding data scarcity and the environmental costs of training large-scale\nmodels. Cognitive signals enable efficient data augmentation, faster\nconvergence, and improved human alignment. The review emphasises the potential\nof ET data in tasks like Visual Question Answering (VQA) and mitigating\nhallucinations in MLLMs, and concludes by discussing emerging challenges and\nresearch trends.",
      "tldr_zh": "这篇评论文章概述了将认知神经科学信号，特别是 Eye-tracking (ET) 信号，整合到 Language Models (LMs) 和 Multimodal Large Language Models (MLLMs) 中的最新进展，从而解决数据稀缺和环境成本等挑战。整合这些用户中心认知信号能实现高效数据增强、快速收敛以及提升模型的人类对齐性。在任务如 Visual Question Answering (VQA) 中，ET 数据显示出减少模型幻觉的潜力，最终讨论了新兴挑战和未来研究趋势。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.06843v1",
      "published_date": "2025-04-09 13:01:48 UTC",
      "updated_date": "2025-04-09 13:01:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:15:41.496121"
    },
    {
      "arxiv_id": "2504.06829v1",
      "title": "Adaptive Locally Linear Embedding",
      "title_zh": "自适应局部线性嵌入",
      "authors": [
        "Ali Goli",
        "Mahdieh Alizadeh",
        "Hadi Sadoghi Yazdi"
      ],
      "abstract": "Manifold learning techniques, such as Locally linear embedding (LLE), are\ndesigned to preserve the local neighborhood structures of high-dimensional data\nduring dimensionality reduction. Traditional LLE employs Euclidean distance to\ndefine neighborhoods, which can struggle to capture the intrinsic geometric\nrelationships within complex data. A novel approach, Adaptive locally linear\nembedding(ALLE), is introduced to address this limitation by incorporating a\ndynamic, data-driven metric that enhances topological preservation. This method\nredefines the concept of proximity by focusing on topological neighborhood\ninclusion rather than fixed distances. By adapting the metric based on the\nlocal structure of the data, it achieves superior neighborhood preservation,\nparticularly for datasets with complex geometries and high-dimensional\nstructures. Experimental results demonstrate that ALLE significantly improves\nthe alignment between neighborhoods in the input and feature spaces, resulting\nin more accurate and topologically faithful embeddings. This approach advances\nmanifold learning by tailoring distance metrics to the underlying data,\nproviding a robust solution for capturing intricate relationships in\nhigh-dimensional datasets.",
      "tldr_zh": "本论文提出了一种改进的流形学习方法，Adaptive Locally Linear Embedding (ALLE)，旨在解决传统 Locally Linear Embedding (LLE) 使用 Euclidean distance 定义邻域时，无法有效捕捉复杂数据内在几何关系的局限性。ALLE 通过引入动态、数据驱动的度量，重新定义邻域为基于拓扑邻域包含，从而实现更好的拓扑保留，尤其适用于高维和复杂几何数据集。实验结果表明，ALLE 显著提升了输入空间和特征空间之间的邻域对齐，提供更准确且拓扑忠实的嵌入。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "16 pages",
      "pdf_url": "http://arxiv.org/pdf/2504.06829v1",
      "published_date": "2025-04-09 12:40:13 UTC",
      "updated_date": "2025-04-09 12:40:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:15:53.685663"
    },
    {
      "arxiv_id": "2504.06796v1",
      "title": "Learning in Spiking Neural Networks with a Calcium-based Hebbian Rule for Spike-timing-dependent Plasticity",
      "title_zh": "翻译失败",
      "authors": [
        "Willian Soares Girão",
        "Nicoletta Risi",
        "Elisabetta Chicca"
      ],
      "abstract": "Understanding how biological neural networks are shaped via local plasticity\nmechanisms can lead to energy-efficient and self-adaptive information\nprocessing systems, which promises to mitigate some of the current roadblocks\nin edge computing systems. While biology makes use of spikes to seamless use\nboth spike timing and mean firing rate to modulate synaptic strength, most\nmodels focus on one of the two. In this work, we present a Hebbian local\nlearning rule that models synaptic modification as a function of calcium traces\ntracking neuronal activity. We show how the rule reproduces results from spike\ntime and spike rate protocols from neuroscientific studies. Moreover, we use\nthe model to train spiking neural networks on MNIST digit recognition to show\nand explain what sort of mechanisms are needed to learn real-world patterns. We\nshow how our model is sensitive to correlated spiking activity and how this\nenables it to modulate the learning rate of the network without altering the\nmean firing rate of the neurons nor the hyparameters of the learning rule. To\nthe best of our knowledge, this is the first work that showcases how spike\ntiming and rate can be complementary in their role of shaping the connectivity\nof spiking neural networks.",
      "tldr_zh": "该研究提出了一种基于钙迹（calcium-based）的Hebbian学习规则，用于尖峰神经网络（Spiking Neural Networks）的尖峰时间相关可塑性（Spike-timing-dependent Plasticity），旨在模拟生物神经网络的局部可塑性机制，以实现高效的自适应信息处理系统。规则将突触修改建模为神经元活动追踪的钙迹函数，并成功再现了神经科学中尖峰时间和尖峰率协议的结果。在MNIST数字识别任务上训练尖峰神经网络时，该模型展示了其对相关尖峰活动的高度敏感性，能够在不改变神经元平均放电率或学习规则超参数的情况下调节网络学习率。该工作首次证明了尖峰时间和率在塑造尖峰神经网络连接方面互补的作用，为边缘计算系统的优化提供了新见解。",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.06796v1",
      "published_date": "2025-04-09 11:39:59 UTC",
      "updated_date": "2025-04-09 11:39:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:16:06.090837"
    },
    {
      "arxiv_id": "2504.06785v1",
      "title": "Zero-Shot Image-Based Large Language Model Approach to Road Pavement Monitoring",
      "title_zh": "翻译失败",
      "authors": [
        "Shuoshuo Xu",
        "Kai Zhao",
        "James Loney",
        "Zili Li",
        "Andrea Visentin"
      ],
      "abstract": "Effective and rapid evaluation of pavement surface condition is critical for\nprioritizing maintenance, ensuring transportation safety, and minimizing\nvehicle wear and tear. While conventional manual inspections suffer from\nsubjectivity, existing machine learning-based methods are constrained by their\nreliance on large and high-quality labeled datasets, which require significant\nresources and limit adaptability across varied road conditions. The\nrevolutionary advancements in Large Language Models (LLMs) present significant\npotential for overcoming these challenges. In this study, we propose an\ninnovative automated zero-shot learning approach that leverages the image\nrecognition and natural language understanding capabilities of LLMs to assess\nroad conditions effectively. Multiple LLM-based assessment models were\ndeveloped, employing prompt engineering strategies aligned with the Pavement\nSurface Condition Index (PSCI) standards. These models' accuracy and\nreliability were evaluated against official PSCI results, with an optimized\nmodel ultimately selected. Extensive tests benchmarked the optimized model\nagainst evaluations from various levels experts using Google Street View road\nimages. The results reveal that the LLM-based approach can effectively assess\nroad conditions, with the optimized model -employing comprehensive and\nstructured prompt engineering strategies -outperforming simpler configurations\nby achieving high accuracy and consistency, even surpassing expert evaluations.\nMoreover, successfully applying the optimized model to Google Street View\nimages demonstrates its potential for future city-scale deployments. These\nfindings highlight the transformative potential of LLMs in automating road\ndamage evaluations and underscore the pivotal role of detailed prompt\nengineering in achieving reliable assessments.",
      "tldr_zh": "本研究提出了一种零-shot学习方法，利用Large Language Models (LLMs)的图像识别和自然语言理解能力，来实现路面状况的自动评估，解决传统手动检查的主观性和机器学习方法对大量标注数据的依赖问题。研究开发了多个LLM-based模型，通过prompt engineering策略与Pavement Surface Condition Index (PSCI)标准对齐，并优化了一个模型，使其在Google Street View图像上表现出色。实验结果显示，该优化模型的准确性和一致性不仅超过了简单配置，还超过了专家评估，证明了LLMs在自动化路面损伤评估中的巨大潜力，并强调了详细prompt engineering的关键作用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.06785v1",
      "published_date": "2025-04-09 11:19:17 UTC",
      "updated_date": "2025-04-09 11:19:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:16:17.067466"
    },
    {
      "arxiv_id": "2504.08817v1",
      "title": "Exploring utilization of generative AI for research and education in data-driven materials science",
      "title_zh": "探索生成式 AI 在数据驱动材料科学中的研究和教育利用",
      "authors": [
        "Takahiro Misawa",
        "Ai Koizumi",
        "Ryo Tamura",
        "Kazuyoshi Yoshimi"
      ],
      "abstract": "Generative AI has recently had a profound impact on various fields, including\ndaily life, research, and education. To explore its efficient utilization in\ndata-driven materials science, we organized a hackathon -- AIMHack2024 -- in\nJuly 2024. In this hackathon, researchers from fields such as materials\nscience, information science, bioinformatics, and condensed matter physics\nworked together to explore how generative AI can facilitate research and\neducation. Based on the results of the hackathon, this paper presents topics\nrelated to (1) conducting AI-assisted software trials, (2) building AI tutors\nfor software, and (3) developing GUI applications for software. While\ngenerative AI continues to evolve rapidly, this paper provides an early record\nof its application in data-driven materials science and highlights strategies\nfor integrating AI into research and education.",
      "tldr_zh": "本研究探讨了生成式AI（generative AI）在数据驱动材料科学（data-driven materials science）中的应用，通过组织2024年7月的AIMHack2024黑客马拉松，促进多领域研究者合作。参与者聚焦于AI辅助软件测试（AI-assisted software trials）、构建AI导师（AI tutors）以及开发GUI应用（GUI applications），以提升研究和教育效率。论文基于黑客马拉松成果，提供生成式AI的早期应用记录，并提出整合AI的策略，为未来材料科学领域的发展提供参考。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG",
        "physics.ed-ph"
      ],
      "primary_category": "cs.CY",
      "comment": "13 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.08817v1",
      "published_date": "2025-04-09 11:15:21 UTC",
      "updated_date": "2025-04-09 11:15:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:16:28.405952"
    },
    {
      "arxiv_id": "2504.06771v1",
      "title": "AI, Help Me Think$\\unicode{x2014}$but for Myself: Assisting People in Complex Decision-Making by Providing Different Kinds of Cognitive Support",
      "title_zh": "翻译失败",
      "authors": [
        "Leon Reicherts",
        "Zelun Tony Zhang",
        "Elisabeth von Oswald",
        "Yuanting Liu",
        "Yvonne Rogers",
        "Mariam Hassib"
      ],
      "abstract": "How can we design AI tools that effectively support human decision-making by\ncomplementing and enhancing users' reasoning processes? Common\nrecommendation-centric approaches face challenges such as inappropriate\nreliance or a lack of integration with users' decision-making processes. Here,\nwe explore an alternative interaction model in which the AI outputs build upon\nusers' own decision-making rationales. We compare this approach, which we call\nExtendAI, with a recommendation-based AI. Participants in our mixed-methods\nuser study interacted with both AIs as part of an investment decision-making\ntask. We found that the AIs had different impacts, with ExtendAI integrating\nbetter into the decision-making process and people's own thinking and leading\nto slightly better outcomes. RecommendAI was able to provide more novel\ninsights while requiring less cognitive effort. We discuss the implications of\nthese and other findings along with three tensions of AI-assisted\ndecision-making which our study revealed.",
      "tldr_zh": "这篇论文探讨了如何设计AI工具来支持人类复杂决策过程，通过提供不同的认知支持来补充和增强用户的推理。作者提出了ExtendAI方法，该方法基于用户的决策理由构建AI输出，并与传统的推荐式AI（RecommendAI）进行了比较。在一项混合方法用户研究中，ExtendAI更好地整合了参与者的思考过程，导致决策结果略微改善，而RecommendAI则提供了更多新颖见解但需要更少认知努力。研究揭示了AI-assisted decision-making中的三个张力，包括依赖性与整合性平衡等问题。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "68, 91",
        "I.2; J.4"
      ],
      "primary_category": "cs.HC",
      "comment": "To be published at ACM CHI 2025 Conference on Human Factors in\n  Computing Systems",
      "pdf_url": "http://arxiv.org/pdf/2504.06771v1",
      "published_date": "2025-04-09 10:48:17 UTC",
      "updated_date": "2025-04-09 10:48:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:16:40.923078"
    },
    {
      "arxiv_id": "2504.07156v1",
      "title": "PLM-eXplain: Divide and Conquer the Protein Embedding Space",
      "title_zh": "PLM-eXplain：分治蛋白质嵌入空间",
      "authors": [
        "Jan van Eck",
        "Dea Gogishvili",
        "Wilson Silva",
        "Sanne Abeln"
      ],
      "abstract": "Protein language models (PLMs) have revolutionised computational biology\nthrough their ability to generate powerful sequence representations for diverse\nprediction tasks. However, their black-box nature limits biological\ninterpretation and translation to actionable insights. We present an\nexplainable adapter layer - PLM-eXplain (PLM-X), that bridges this gap by\nfactoring PLM embeddings into two components: an interpretable subspace based\non established biochemical features, and a residual subspace that preserves the\nmodel's predictive power. Using embeddings from ESM2, our adapter incorporates\nwell-established properties, including secondary structure and hydropathy while\nmaintaining high performance. We demonstrate the effectiveness of our approach\nacross three protein-level classification tasks: prediction of extracellular\nvesicle association, identification of transmembrane helices, and prediction of\naggregation propensity. PLM-X enables biological interpretation of model\ndecisions without sacrificing accuracy, offering a generalisable solution for\nenhancing PLM interpretability across various downstream applications. This\nwork addresses a critical need in computational biology by providing a bridge\nbetween powerful deep learning models and actionable biological insights.",
      "tldr_zh": "本研究针对蛋白质语言模型 (PLMs) 在计算生物学中的黑箱问题，提出了一种可解释适配器层 PLM-eXplain (PLM-X)，将 PLMs 嵌入分解为基于生化特征的可解释子空间（如 secondary structure 和 hydropathy）和保留预测能力的残差子空间。利用 ESM2 嵌入，该方法在预测细胞外囊泡关联、识别 transmembrane helices 和预测聚集倾向等三个蛋白质分类任务上保持了高性能，同时提升了模型决策的生物学解释性。PLM-X 提供了一个通用解决方案，帮助桥接深度学习模型与可操作的生物学洞见。",
      "categories": [
        "q-bio.BM",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.BM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.07156v1",
      "published_date": "2025-04-09 10:46:24 UTC",
      "updated_date": "2025-04-09 10:46:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:16:53.474348"
    },
    {
      "arxiv_id": "2504.06766v1",
      "title": "FamilyTool: A Multi-hop Personalized Tool Use Benchmark",
      "title_zh": "FamilyTool: 多跳个性化工具使用基准",
      "authors": [
        "Yuxin Wang",
        "Yiran Guo",
        "Yining Zheng",
        "Zhangyue Yin",
        "Shuo Chen",
        "Jie Yang",
        "Jiajun Chen",
        "Xuanjing Huang",
        "Xipeng Qiu"
      ],
      "abstract": "The integration of tool learning with Large Language Models (LLMs) has\nexpanded their capabilities in handling complex tasks by leveraging external\ntools. However, existing benchmarks for tool learning inadequately address\ncritical real-world personalized scenarios, particularly those requiring\nmulti-hop reasoning and inductive knowledge adaptation in dynamic environments.\nTo bridge this gap, we introduce FamilyTool, a novel benchmark grounded in a\nfamily-based knowledge graph (KG) that simulates personalized, multi-hop tool\nuse scenarios. FamilyTool challenges LLMs with queries spanning 1 to 3\nrelational hops (e.g., inferring familial connections and preferences) and\nincorporates an inductive KG setting where models must adapt to unseen user\npreferences and relationships without re-training, a common limitation in prior\napproaches that compromises generalization. We further propose KGETool: a\nsimple KG-augmented evaluation pipeline to systematically assess LLMs' tool use\nability in these settings. Experiments reveal significant performance gaps in\nstate-of-the-art LLMs, with accuracy dropping sharply as hop complexity\nincreases and inductive scenarios exposing severe generalization deficits.\nThese findings underscore the limitations of current LLMs in handling\npersonalized, evolving real-world contexts and highlight the urgent need for\nadvancements in tool-learning frameworks. FamilyTool serves as a critical\nresource for evaluating and advancing LLM agents' reasoning, adaptability, and\nscalability in complex, dynamic environments. Code and dataset are available at\nGithub.",
      "tldr_zh": "本研究引入 FamilyTool，一种基于知识图谱 (KG) 的新基准，用于评估大型语言模型 (LLMs) 在个性化、多跳工具使用场景中的性能，针对现有基准在处理多跳推理和动态环境适应方面的不足。FamilyTool 模拟家庭关系查询（涵盖 1 到 3 个关系跳跃），并采用归纳 KG 设置，要求模型无需重新训练即可适应未见过的用户偏好和关系，同时提出 KGETool 作为系统化的评估管道。实验结果显示，现有 LLMs 的准确率随跳跃复杂度增加而急剧下降，并在归纳场景中暴露严重的泛化缺陷，强调了提升工具学习框架以应对真实世界动态环境的迫切需求。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.06766v1",
      "published_date": "2025-04-09 10:42:36 UTC",
      "updated_date": "2025-04-09 10:42:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:17:06.567837"
    },
    {
      "arxiv_id": "2504.06753v1",
      "title": "Detect All-Type Deepfake Audio: Wavelet Prompt Tuning for Enhanced Auditory Perception",
      "title_zh": "翻译失败",
      "authors": [
        "Yuankun Xie",
        "Ruibo Fu",
        "Zhiyong Wang",
        "Xiaopeng Wang",
        "Songjun Cao",
        "Long Ma",
        "Haonan Cheng",
        "Long Ye"
      ],
      "abstract": "The rapid advancement of audio generation technologies has escalated the\nrisks of malicious deepfake audio across speech, sound, singing voice, and\nmusic, threatening multimedia security and trust. While existing\ncountermeasures (CMs) perform well in single-type audio deepfake detection\n(ADD), their performance declines in cross-type scenarios. This paper is\ndedicated to studying the alltype ADD task. We are the first to comprehensively\nestablish an all-type ADD benchmark to evaluate current CMs, incorporating\ncross-type deepfake detection across speech, sound, singing voice, and music.\nThen, we introduce the prompt tuning self-supervised learning (PT-SSL) training\nparadigm, which optimizes SSL frontend by learning specialized prompt tokens\nfor ADD, requiring 458x fewer trainable parameters than fine-tuning (FT).\nConsidering the auditory perception of different audio types,we propose the\nwavelet prompt tuning (WPT)-SSL method to capture type-invariant auditory\ndeepfake information from the frequency domain without requiring additional\ntraining parameters, thereby enhancing performance over FT in the all-type ADD\ntask. To achieve an universally CM, we utilize all types of deepfake audio for\nco-training. Experimental results demonstrate that WPT-XLSR-AASIST achieved the\nbest performance, with an average EER of 3.58% across all evaluation sets. The\ncode is available online.",
      "tldr_zh": "该论文针对音频生成技术的快速发展导致的深度伪造音频风险（如语音、声音、歌声和音乐），首次建立了全类型音频深度伪造检测（ADD）基准，以评估现有反制措施在跨类型场景中的表现。作者提出小波提示调优（WPT）-自监督学习（SSL）方法，通过从频率域捕获类型不变的听觉伪造信息，仅需比微调（FT）少458倍的参数，即可提升检测性能。实验结果显示，WPT-XLSR-AASIST模型在所有评估集上平均EER达到3.58%，证明了其作为通用反制措施的有效性，并提供了公开代码以促进进一步研究。",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.06753v1",
      "published_date": "2025-04-09 10:18:45 UTC",
      "updated_date": "2025-04-09 10:18:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:17:17.433062"
    },
    {
      "arxiv_id": "2504.06738v1",
      "title": "EDIT: Enhancing Vision Transformers by Mitigating Attention Sink through an Encoder-Decoder Architecture",
      "title_zh": "翻译失败",
      "authors": [
        "Wenfeng Feng",
        "Guoying Sun"
      ],
      "abstract": "In this paper, we propose EDIT (Encoder-Decoder Image Transformer), a novel\narchitecture designed to mitigate the attention sink phenomenon observed in\nVision Transformer models. Attention sink occurs when an excessive amount of\nattention is allocated to the [CLS] token, distorting the model's ability to\neffectively process image patches. To address this, we introduce a\nlayer-aligned encoder-decoder architecture, where the encoder utilizes\nself-attention to process image patches, while the decoder uses cross-attention\nto focus on the [CLS] token. Unlike traditional encoder-decoder framework,\nwhere the decoder depends solely on high-level encoder representations, EDIT\nallows the decoder to extract information starting from low-level features,\nprogressively refining the representation layer by layer. EDIT is naturally\ninterpretable demonstrated through sequential attention maps, illustrating the\nrefined, layer-by-layer focus on key image features. Experiments on ImageNet-1k\nand ImageNet-21k, along with transfer learning tasks, show that EDIT achieves\nconsistent performance improvements over DeiT3 models. These results highlight\nthe effectiveness of EDIT's design in addressing attention sink and improving\nvisual feature extraction.",
      "tldr_zh": "本文提出 EDIT（Encoder-Decoder Image Transformer）架构，以缓解 Vision Transformers 模型中的 attention sink 现象，即过多的注意力集中在 [CLS] token 上，导致图像 patches 处理不佳。该架构采用层对齐的 encoder-decoder 设计，其中 encoder 通过 self-attention 处理图像 patches，decoder 通过 cross-attention 关注 [CLS] token，并从低级特征开始逐层精炼表示，从而提升模型的可解释性和视觉特征提取能力。在 ImageNet-1k 和 ImageNet-21k 等数据集上的实验表明，EDIT 比 DeiT3 模型实现了持续的性能提升，证明了其在解决 attention sink 问题方面的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.06738v1",
      "published_date": "2025-04-09 09:51:41 UTC",
      "updated_date": "2025-04-09 09:51:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:17:29.804136"
    },
    {
      "arxiv_id": "2504.06721v1",
      "title": "Learning global control of underactuated systems with Model-Based Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Niccolò Turcato",
        "Marco Calì",
        "Alberto Dalla Libera",
        "Giulio Giacomuzzo",
        "Ruggero Carli",
        "Diego Romeres"
      ],
      "abstract": "This short paper describes our proposed solution for the third edition of the\n\"AI Olympics with RealAIGym\" competition, held at ICRA 2025. We employed\nMonte-Carlo Probabilistic Inference for Learning Control (MC-PILCO), an MBRL\nalgorithm recognized for its exceptional data efficiency across various\nlow-dimensional robotic tasks, including cart-pole, ball \\& plate, and Furuta\npendulum systems. MC-PILCO optimizes a system dynamics model using interaction\ndata, enabling policy refinement through simulation rather than direct system\ndata optimization. This approach has proven highly effective in physical\nsystems, offering greater data efficiency than Model-Free (MF) alternatives.\nNotably, MC-PILCO has previously won the first two editions of this\ncompetition, demonstrating its robustness in both simulated and real-world\nenvironments. Besides briefly reviewing the algorithm, we discuss the most\ncritical aspects of the MC-PILCO implementation in the tasks at hand: learning\na global policy for the pendubot and acrobot systems.",
      "tldr_zh": "这篇论文介绍了在ICRA 2025 \"AI Olympics with RealAIGym\" 比赛中，使用Model-Based Reinforcement Learning (MBRL)算法MC-PILCO来学习underactuated系统的全局控制。MC-PILCO通过优化系统动态模型并利用模拟进行策略改进，显著提高了数据效率，适用于低维机器人任务如cart-pole、ball & plate和Furuta pendulum系统。相比Model-Free (MF)方法，该算法在物理环境中表现更出色，并已在前两届比赛中获胜。论文重点讨论了在pendubot和acrobot系统上实现MC-PILCO的关键方面，为实际机器人控制提供了鲁棒解决方案。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "arXiv admin note: substantial text overlap with arXiv:2409.05811",
      "pdf_url": "http://arxiv.org/pdf/2504.06721v1",
      "published_date": "2025-04-09 09:20:37 UTC",
      "updated_date": "2025-04-09 09:20:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:17:41.075886"
    },
    {
      "arxiv_id": "2504.06719v1",
      "title": "Masked Scene Modeling: Narrowing the Gap Between Supervised and Self-Supervised Learning in 3D Scene Understanding",
      "title_zh": "Masked Scene Modeling：缩小监督学习与自监督学习在3D场景理解中的差距",
      "authors": [
        "Pedro Hermosilla",
        "Christian Stippel",
        "Leon Sick"
      ],
      "abstract": "Self-supervised learning has transformed 2D computer vision by enabling\nmodels trained on large, unannotated datasets to provide versatile\noff-the-shelf features that perform similarly to models trained with labels.\nHowever, in 3D scene understanding, self-supervised methods are typically only\nused as a weight initialization step for task-specific fine-tuning, limiting\ntheir utility for general-purpose feature extraction. This paper addresses this\nshortcoming by proposing a robust evaluation protocol specifically designed to\nassess the quality of self-supervised features for 3D scene understanding. Our\nprotocol uses multi-resolution feature sampling of hierarchical models to\ncreate rich point-level representations that capture the semantic capabilities\nof the model and, hence, are suitable for evaluation with linear probing and\nnearest-neighbor methods. Furthermore, we introduce the first self-supervised\nmodel that performs similarly to supervised models when only off-the-shelf\nfeatures are used in a linear probing setup. In particular, our model is\ntrained natively in 3D with a novel self-supervised approach based on a Masked\nScene Modeling objective, which reconstructs deep features of masked patches in\na bottom-up manner and is specifically tailored to hierarchical 3D models. Our\nexperiments not only demonstrate that our method achieves competitive\nperformance to supervised models, but also surpasses existing self-supervised\napproaches by a large margin. The model and training code can be found at our\nGithub repository (https://github.com/phermosilla/msm).",
      "tldr_zh": "本论文探讨了自监督学习(Self-Supervised Learning)在3D场景理解(3D Scene Understanding)中的局限性，提出了一种新的评估协议，通过多分辨率特征采样和层次模型创建丰富的点级表示，并使用线性探测(Linear Probing)和最近邻方法进行评估。作者引入了基于Masked Scene Modeling目标的首个自监督模型，该模型在3D环境中从底层向上重建被掩盖补丁的深度特征，专为层次3D模型设计。实验结果显示，该方法在off-the-shelf特征的表现上与监督学习(Supervised Learning)模型相当，并大幅超越现有自监督方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.06719v1",
      "published_date": "2025-04-09 09:19:49 UTC",
      "updated_date": "2025-04-09 09:19:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:17:53.294252"
    },
    {
      "arxiv_id": "2504.06683v1",
      "title": "Hyperparameter Optimisation with Practical Interpretability and Explanation Methods in Probabilistic Curriculum Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Llewyn Salt",
        "Marcus Gallagher"
      ],
      "abstract": "Hyperparameter optimisation (HPO) is crucial for achieving strong performance\nin reinforcement learning (RL), as RL algorithms are inherently sensitive to\nhyperparameter settings. Probabilistic Curriculum Learning (PCL) is a\ncurriculum learning strategy designed to improve RL performance by structuring\nthe agent's learning process, yet effective hyperparameter tuning remains\nchallenging and computationally demanding. In this paper, we provide an\nempirical analysis of hyperparameter interactions and their effects on the\nperformance of a PCL algorithm within standard RL tasks, including point-maze\nnavigation and DC motor control. Using the AlgOS framework integrated with\nOptuna's Tree-Structured Parzen Estimator (TPE), we present strategies to\nrefine hyperparameter search spaces, enhancing optimisation efficiency.\nAdditionally, we introduce a novel SHAP-based interpretability approach\ntailored specifically for analysing hyperparameter impacts, offering clear\ninsights into how individual hyperparameters and their interactions influence\nRL performance. Our work contributes practical guidelines and interpretability\ntools that significantly improve the effectiveness and computational\nfeasibility of hyperparameter optimisation in reinforcement learning.",
      "tldr_zh": "本研究探讨了超参数优化 (HPO) 在强化学习 (RL) 中的关键作用，特别是针对 Probabilistic Curriculum Learning (PCL) 算法的调优挑战，通过实证分析超参数交互及其对 RL 任务（如点迷宫导航和 DC 电机控制）的影响。研究利用 AlgOS 框架结合 Optuna 的 Tree-Structured Parzen Estimator (TPE) 优化超参数搜索空间，提高了优化效率。作者引入了一种新型基于 SHAP 的可解释性方法，分析单个超参数及其交互对 RL 性能的影响，提供清晰的洞见。总体上，该工作贡献了实用指南和工具，提升了 HPO 在 RL 中的有效性和计算可行性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.06683v1",
      "published_date": "2025-04-09 08:41:27 UTC",
      "updated_date": "2025-04-09 08:41:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:18:05.305461"
    },
    {
      "arxiv_id": "2504.06669v1",
      "title": "NLP Security and Ethics, in the Wild",
      "title_zh": "NLP 安全与伦理，在野外",
      "authors": [
        "Heather Lent",
        "Erick Galinkin",
        "Yiyi Chen",
        "Jens Myrup Pedersen",
        "Leon Derczynski",
        "Johannes Bjerva"
      ],
      "abstract": "As NLP models are used by a growing number of end-users, an area of\nincreasing importance is NLP Security (NLPSec): assessing the vulnerability of\nmodels to malicious attacks and developing comprehensive countermeasures\nagainst them. While work at the intersection of NLP and cybersecurity has the\npotential to create safer NLP for all, accidental oversights can result in\ntangible harm (e.g., breaches of privacy or proliferation of malicious models).\nIn this emerging field, however, the research ethics of NLP have not yet faced\nmany of the long-standing conundrums pertinent to cybersecurity, until now. We\nthus examine contemporary works across NLPSec, and explore their engagement\nwith cybersecurity's ethical norms. We identify trends across the literature,\nultimately finding alarming gaps on topics like harm minimization and\nresponsible disclosure. To alleviate these concerns, we provide concrete\nrecommendations to help NLP researchers navigate this space more ethically,\nbridging the gap between traditional cybersecurity and NLP ethics, which we\nframe as ``white hat NLP''. The goal of this work is to help cultivate an\nintentional culture of ethical research for those working in NLP Security.",
      "tldr_zh": "这篇论文探讨了NLP Security (NLPSec)领域的伦理挑战，通过审查现有文献，评估NLP模型对恶意攻击的脆弱性及其潜在危害，如隐私泄露或恶意模型扩散。\n研究发现，NLPSec研究中存在明显空白，例如在伤害最小化和负责任披露方面，与传统网络安全伦理规范的对接不足。\n作者提出具体推荐，帮助NLP研究者采用更道德的方法，引入“white hat NLP”概念，以培养一个注重伦理的NLPSec研究文化。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to TACL",
      "pdf_url": "http://arxiv.org/pdf/2504.06669v1",
      "published_date": "2025-04-09 08:12:34 UTC",
      "updated_date": "2025-04-09 08:12:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:18:16.431675"
    },
    {
      "arxiv_id": "2504.06667v1",
      "title": "Toward Holistic Evaluation of Recommender Systems Powered by Generative Models",
      "title_zh": "面向生成模型驱动的推荐系统的整体评估",
      "authors": [
        "Yashar Deldjoo",
        "Nikhil Mehta",
        "Maheswaran Sathiamoorthy",
        "Shuai Zhang",
        "Pablo Castells",
        "Julian McAuley"
      ],
      "abstract": "Recommender systems powered by generative models (Gen-RecSys) extend beyond\nclassical item ranking by producing open-ended content, which simultaneously\nunlocks richer user experiences and introduces new risks. On one hand, these\nsystems can enhance personalization and appeal through dynamic explanations and\nmulti-turn dialogues. On the other hand, they might venture into unknown\nterritory-hallucinating nonexistent items, amplifying bias, or leaking private\ninformation. Traditional accuracy metrics cannot fully capture these\nchallenges, as they fail to measure factual correctness, content safety, or\nalignment with user intent.\n  This paper makes two main contributions. First, we categorize the evaluation\nchallenges of Gen-RecSys into two groups: (i) existing concerns that are\nexacerbated by generative outputs (e.g., bias, privacy) and (ii) entirely new\nrisks (e.g., item hallucinations, contradictory explanations). Second, we\npropose a holistic evaluation approach that includes scenario-based assessments\nand multi-metric checks-incorporating relevance, factual grounding, bias\ndetection, and policy compliance. Our goal is to provide a guiding framework so\nresearchers and practitioners can thoroughly assess Gen-RecSys, ensuring\neffective personalization and responsible deployment.",
      "tldr_zh": "该论文探讨了基于生成模型的推荐系统（Gen-RecSys），它通过产生开放内容提升用户体验（如个性化解释和多轮对话），但也引入新风险，如项目幻觉、偏见放大和隐私泄露。作者将评估挑战分为两类：已存在的问题（如偏见和隐私）以及新风险（如矛盾解释），并提出一个整体评估框架。框架结合场景-based assessments 和多-metric checks，包括相关性、事实基础、偏见检测和政策遵守，以指导研究者和从业者实现有效个性化并确保负责任部署。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.06667v1",
      "published_date": "2025-04-09 08:08:16 UTC",
      "updated_date": "2025-04-09 08:08:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:18:29.286269"
    },
    {
      "arxiv_id": "2504.06659v1",
      "title": "Bridging the Gap Between Preference Alignment and Machine Unlearning",
      "title_zh": "弥合偏好对齐与机器遗忘之间的差距",
      "authors": [
        "Xiaohua Feng",
        "Yuyuan Li",
        "Huwei Ji",
        "Jiaming Zhang",
        "Li Zhang",
        "Tianyu Du",
        "Chaochao Chen"
      ],
      "abstract": "Despite advances in Preference Alignment (PA) for Large Language Models\n(LLMs), mainstream methods like Reinforcement Learning with Human Feedback\n(RLHF) face notable challenges. These approaches require high-quality datasets\nof positive preference examples, which are costly to obtain and computationally\nintensive due to training instability, limiting their use in low-resource\nscenarios. LLM unlearning technique presents a promising alternative, by\ndirectly removing the influence of negative examples. However, current research\nhas primarily focused on empirical validation, lacking systematic quantitative\nanalysis. To bridge this gap, we propose a framework to explore the\nrelationship between PA and LLM unlearning. Specifically, we introduce a\nbi-level optimization-based method to quantify the impact of unlearning\nspecific negative examples on PA performance. Our analysis reveals that not all\nnegative examples contribute equally to alignment improvement when unlearned,\nand the effect varies significantly across examples. Building on this insight,\nwe pose a crucial question: how can we optimally select and weight negative\nexamples for unlearning to maximize PA performance? To answer this, we propose\na framework called Unlearning to Align (U2A), which leverages bi-level\noptimization to efficiently select and unlearn examples for optimal PA\nperformance. We validate the proposed method through extensive experiments,\nwith results confirming its effectiveness.",
      "tldr_zh": "该论文探讨了Preference Alignment (PA) 与LLM unlearning之间的关系，指出主流PA方法如Reinforcement Learning with Human Feedback (RLHF) 面临数据成本高和计算密集的挑战，而LLM unlearning可作为替代方案但缺乏系统定量分析。研究者引入基于bi-level optimization的方法来量化unlearning负面示例对PA性能的影响，发现不同负面示例对alignment改善的贡献不均等。基于这一洞见，他们提出Unlearning to Align (U2A) 框架，通过bi-level optimization高效选择和unlearning示例，以最大化PA性能。实验结果证实了U2A的有效性，为低资源场景下的模型优化提供了新途径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "17 pages",
      "pdf_url": "http://arxiv.org/pdf/2504.06659v1",
      "published_date": "2025-04-09 07:49:08 UTC",
      "updated_date": "2025-04-09 07:49:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:18:42.045193"
    },
    {
      "arxiv_id": "2504.06658v1",
      "title": "A Neuro-inspired Interpretation of Unlearning in Large Language Models through Sample-level Unlearning Difficulty",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaohua Feng",
        "Yuyuan Li",
        "Chengye Wang",
        "Junlin Liu",
        "Li Zhang",
        "Chaochao Chen"
      ],
      "abstract": "Driven by privacy protection laws and regulations, unlearning in Large\nLanguage Models (LLMs) is gaining increasing attention. However, current\nresearch often neglects the interpretability of the unlearning process,\nparticularly concerning sample-level unlearning difficulty. Existing studies\ntypically assume a uniform unlearning difficulty across samples. This\nsimplification risks attributing the performance of unlearning algorithms to\nsample selection rather than the algorithm's design, potentially steering the\ndevelopment of LLM unlearning in the wrong direction. Thus, we investigate the\nrelationship between LLM unlearning and sample characteristics, with a focus on\nunlearning difficulty. Drawing inspiration from neuroscience, we propose a\nMemory Removal Difficulty ($\\mathrm{MRD}$) metric to quantify sample-level\nunlearning difficulty. Using $\\mathrm{MRD}$, we analyze the characteristics of\nhard-to-unlearn versus easy-to-unlearn samples. Furthermore, we propose an\n$\\mathrm{MRD}$-based weighted sampling method to optimize existing unlearning\nalgorithms, which prioritizes easily forgettable samples, thereby improving\nunlearning efficiency and effectiveness. We validate the proposed metric and\nmethod using public benchmarks and datasets, with results confirming its\neffectiveness.",
      "tldr_zh": "该论文探讨了大型语言模型（LLMs）中的unlearning过程，强调样本级别的unlearning难度对算法设计的意义，指出现有研究忽略了这一因素可能导致误导。受神经科学启发，作者提出Memory Removal Difficulty (MRD) 指标，用于量化样本的unlearning难度，并分析了难忘与易忘样本的特征。基于MRD，他们开发了一种加权采样方法，优先处理易忘样本，从而优化现有unlearning算法，提高效率和效果；实验在公共基准和数据集上验证了该方法的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "16 pages",
      "pdf_url": "http://arxiv.org/pdf/2504.06658v1",
      "published_date": "2025-04-09 07:48:10 UTC",
      "updated_date": "2025-04-09 07:48:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:18:53.314003"
    },
    {
      "arxiv_id": "2504.06649v1",
      "title": "GRAIN: Multi-Granular and Implicit Information Aggregation Graph Neural Network for Heterophilous Graphs",
      "title_zh": "翻译失败",
      "authors": [
        "Songwei Zhao",
        "Yuan Jiang",
        "Zijing Zhang",
        "Yang Yu",
        "Hechang Chen"
      ],
      "abstract": "Graph neural networks (GNNs) have shown significant success in learning graph\nrepresentations. However, recent studies reveal that GNNs often fail to\noutperform simple MLPs on heterophilous graph tasks, where connected nodes may\ndiffer in features or labels, challenging the homophily assumption. Existing\nmethods addressing this issue often overlook the importance of information\ngranularity and rarely consider implicit relationships between distant nodes.\nTo overcome these limitations, we propose the Granular and Implicit Graph\nNetwork (GRAIN), a novel GNN model specifically designed for heterophilous\ngraphs. GRAIN enhances node embeddings by aggregating multi-view information at\nvarious granularity levels and incorporating implicit data from distant,\nnon-neighboring nodes. This approach effectively integrates local and global\ninformation, resulting in smoother, more accurate node representations. We also\nintroduce an adaptive graph information aggregator that efficiently combines\nmulti-granularity and implicit data, significantly improving node\nrepresentation quality, as shown by experiments on 13 datasets covering varying\nhomophily and heterophily. GRAIN consistently outperforms 12 state-of-the-art\nmodels, excelling on both homophilous and heterophilous graphs.",
      "tldr_zh": "该研究针对图神经网络（GNNs）在异质图（heterophilous graphs）任务上的不足——如无法超越简单 MLPs 模型的问题——提出了一种新型 GRAIN 模型。GRAIN 通过聚合多粒度级别的多视图信息和来自远距离非邻居节点的隐式数据，来增强节点嵌入并整合本地与全局信息，同时引入自适应图信息聚合器以提高表示质量。实验结果显示，在 13 个数据集上，GRAIN 优于 12 个最先进模型，在同质和异质图任务中均表现出显著优势。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.06649v1",
      "published_date": "2025-04-09 07:36:44 UTC",
      "updated_date": "2025-04-09 07:36:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:19:05.492305"
    },
    {
      "arxiv_id": "2504.06643v3",
      "title": "AMAD: AutoMasked Attention for Unsupervised Multivariate Time Series Anomaly Detection",
      "title_zh": "AMAD：自动掩码注意力用于无监督多变量时间序列异常检测",
      "authors": [
        "Tiange Huang",
        "Yongjun Li"
      ],
      "abstract": "Unsupervised multivariate time series anomaly detection (UMTSAD) plays a\ncritical role in various domains, including finance, networks, and sensor\nsystems. In recent years, due to the outstanding performance of deep learning\nin general sequential tasks, many models have been specialized for deep UMTSAD\ntasks and have achieved impressive results, particularly those based on the\nTransformer and self-attention mechanisms. However, the sequence anomaly\nassociation assumptions underlying these models are often limited to specific\npredefined patterns and scenarios, such as concentrated or peak anomaly\npatterns. These limitations hinder their ability to generalize to diverse\nanomaly situations, especially where the lack of labels poses significant\nchallenges. To address these issues, we propose AMAD, which integrates\n\\textbf{A}uto\\textbf{M}asked Attention for UMTS\\textbf{AD} scenarios. AMAD\nintroduces a novel structure based on the AutoMask mechanism and an attention\nmixup module, forming a simple yet generalized anomaly association\nrepresentation framework. This framework is further enhanced by a Max-Min\ntraining strategy and a Local-Global contrastive learning approach. By\ncombining multi-scale feature extraction with automatic relative association\nmodeling, AMAD provides a robust and adaptable solution to UMTSAD challenges.\nExtensive experimental results demonstrate that the proposed model achieving\ncompetitive performance results compared to SOTA benchmarks across a variety of\ndatasets.",
      "tldr_zh": "该论文针对无监督多变量时间序列异常检测 (UMTSAD) 的局限性，提出了一种基于 AutoMasked Attention 的新框架 AMAD，以解决现有基于 Transformer 和自注意力机制的模型在泛化方面的不足，特别是对多样化异常模式（如非集中或非峰值模式）的适应性。AMAD 引入 AutoMask 机制和 attention mixup 模块，结合 Max-Min 训练策略以及 Local-Global 对比学习方法，实现多尺度特征提取和自动相对关联建模，从而构建了一个鲁棒的异常关联表示框架。实验结果显示，AMAD 在多种数据集上与 SOTA 基准相比取得了竞争性的性能表现，证明了其在实际应用中的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "I.5.1"
      ],
      "primary_category": "cs.LG",
      "comment": "fix some grammar issues",
      "pdf_url": "http://arxiv.org/pdf/2504.06643v3",
      "published_date": "2025-04-09 07:32:59 UTC",
      "updated_date": "2025-04-25 15:30:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:19:16.481100"
    },
    {
      "arxiv_id": "2504.08813v1",
      "title": "SafeMLRM: Demystifying Safety in Multi-modal Large Reasoning Models",
      "title_zh": "翻译失败",
      "authors": [
        "Junfeng Fang",
        "Yukai Wang",
        "Ruipeng Wang",
        "Zijun Yao",
        "Kun Wang",
        "An Zhang",
        "Xiang Wang",
        "Tat-Seng Chua"
      ],
      "abstract": "The rapid advancement of multi-modal large reasoning models (MLRMs) --\nenhanced versions of multimodal language models (MLLMs) equipped with reasoning\ncapabilities -- has revolutionized diverse applications. However, their safety\nimplications remain underexplored. While prior work has exposed critical\nvulnerabilities in unimodal reasoning models, MLRMs introduce distinct risks\nfrom cross-modal reasoning pathways. This work presents the first systematic\nsafety analysis of MLRMs through large-scale empirical studies comparing MLRMs\nwith their base MLLMs. Our experiments reveal three critical findings: (1) The\nReasoning Tax: Acquiring reasoning capabilities catastrophically degrades\ninherited safety alignment. MLRMs exhibit 37.44% higher jailbreaking success\nrates than base MLLMs under adversarial attacks. (2) Safety Blind Spots: While\nsafety degradation is pervasive, certain scenarios (e.g., Illegal Activity)\nsuffer 25 times higher attack rates -- far exceeding the average 3.4 times\nincrease, revealing scenario-specific vulnerabilities with alarming cross-model\nand datasets consistency. (3) Emergent Self-Correction: Despite tight\nreasoning-answer safety coupling, MLRMs demonstrate nascent self-correction --\n16.9% of jailbroken reasoning steps are overridden by safe answers, hinting at\nintrinsic safeguards. These findings underscore the urgency of scenario-aware\nsafety auditing and mechanisms to amplify MLRMs' self-correction potential. To\ncatalyze research, we open-source OpenSafeMLRM, the first toolkit for MLRM\nsafety evaluation, providing unified interface for mainstream models, datasets,\nand jailbreaking methods. Our work calls for immediate efforts to harden\nreasoning-augmented AI, ensuring its transformative potential aligns with\nethical safeguards.",
      "tldr_zh": "这篇论文首次系统分析了多模态大型推理模型 (MLRMs) 的安全问题，通过大规模实证研究与基础多模态语言模型 (MLLMs) 进行比较。研究发现，“Reasoning Tax”效应导致 MLRMs 在对抗攻击下越狱成功率提高 37.44%，并暴露了“Safety Blind Spots”，如非法活动场景的攻击率高出 25 倍，显示出场景特定漏洞的一致性。同时，MLRMs 展现出初步的“Emergent Self-Correction”，其中 16.9% 的越狱推理步骤被安全答案覆盖。论文开源了 OpenSafeMLRM 工具包，并呼吁开发场景感知的安全审计机制，以强化 MLRMs 的安全性和伦理对齐。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.08813v1",
      "published_date": "2025-04-09 06:53:23 UTC",
      "updated_date": "2025-04-09 06:53:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:19:30.747205"
    },
    {
      "arxiv_id": "2504.06611v2",
      "title": "Wanting to be Understood",
      "title_zh": "翻译失败",
      "authors": [
        "Chrisantha Fernando",
        "Dylan Banarse",
        "Simon Osindero"
      ],
      "abstract": "This paper explores an intrinsic motivation for mutual awareness,\nhypothesizing that humans possess a fundamental drive to understand and to be\nunderstood even in the absence of extrinsic rewards. Through simulations of the\nperceptual crossing paradigm, we explore the effect of various internal reward\nfunctions in reinforcement learning agents. The drive to understand is\nimplemented as an active inference type artificial curiosity reward, whereas\nthe drive to be understood is implemented through intrinsic rewards for\nimitation, influence/impressionability, and sub-reaction time anticipation of\nthe other. Results indicate that while artificial curiosity alone does not lead\nto a preference for social interaction, rewards emphasizing reciprocal\nunderstanding successfully drive agents to prioritize interaction. We\ndemonstrate that this intrinsic motivation can facilitate cooperation in tasks\nwhere only one agent receives extrinsic reward for the behaviour of the other.",
      "tldr_zh": "这篇论文探讨了人类的一种内在动机，即在缺乏外部奖励的情况下追求相互理解和被理解，通过模拟感知交叉范式来验证这一假设。研究采用强化学习代理，测试了各种内部奖励函数，包括 active inference 类型的 artificial curiosity 奖励，以及模仿、影响和子反应时间预期的奖励。结果表明，仅靠 artificial curiosity 无法驱动代理偏好社交互动，但强调相互理解的奖励能促进代理优先互动，并在只有一方获得外部奖励的任务中实现合作。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.06611v2",
      "published_date": "2025-04-09 06:15:24 UTC",
      "updated_date": "2025-04-10 07:46:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:19:40.649956"
    },
    {
      "arxiv_id": "2504.06609v1",
      "title": "InteractRank: Personalized Web-Scale Search Pre-Ranking with Cross Interaction Features",
      "title_zh": "翻译失败",
      "authors": [
        "Sujay Khandagale",
        "Bhawna Juneja",
        "Prabhat Agarwal",
        "Aditya Subramanian",
        "Jaewon Yang",
        "Yuting Wang"
      ],
      "abstract": "Modern search systems use a multi-stage architecture to deliver personalized\nresults efficiently. Key stages include retrieval, pre-ranking, full ranking,\nand blending, which refine billions of items to top selections. The pre-ranking\nstage, vital for scoring and filtering hundreds of thousands of items down to a\nfew thousand, typically relies on two tower models due to their computational\nefficiency, despite often lacking in capturing complex interactions. While\nquery-item cross interaction features are paramount for full ranking,\nintegrating them into pre-ranking models presents efficiency-related\nchallenges. In this paper, we introduce InteractRank, a novel two tower\npre-ranking model with robust cross interaction features used at Pinterest. By\nincorporating historical user engagement-based query-item interactions in the\nscoring function along with the two tower dot product, InteractRank\nsignificantly boosts pre-ranking performance with minimal latency and\ncomputation costs. In real-world A/B experiments at Pinterest, InteractRank\nimproves the online engagement metric by 6.5% over a BM25 baseline and by 3.7%\nover a vanilla two tower baseline. We also highlight other components of\nInteractRank, like real-time user-sequence modeling, and analyze their\ncontributions through offline ablation studies. The code for InteractRank is\navailable at https://github.com/pinterest/atg-research/tree/main/InteractRank.",
      "tldr_zh": "本研究提出InteractRank，一种新型的两塔预排序模型，用于个性化网络规模搜索系统，通过整合基于历史用户互动的查询-物品交叉交互特征，提升了预排序阶段的性能，同时保持低延迟和计算成本。InteractRank在Pinterest的真实A/B实验中，比BM25基线提高了6.5%的在线参与度指标，并比传统的两塔基线提升了3.7%，同时通过离线消融研究分析了实时用户序列建模等组件的贡献。该模型的代码已在GitHub开源，为高效的搜索系统优化提供了实用解决方案。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG",
        "H.3.3"
      ],
      "primary_category": "cs.IR",
      "comment": "8 pages, 3 figures, to appear at TheWebConf Industry Track 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.06609v1",
      "published_date": "2025-04-09 06:13:58 UTC",
      "updated_date": "2025-04-09 06:13:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:19:53.184901"
    },
    {
      "arxiv_id": "2504.06600v1",
      "title": "Automated Business Process Analysis: An LLM-Based Approach to Value Assessment",
      "title_zh": "自动化业务流程分析：一种基于 LLM 的价值评估方法",
      "authors": [
        "William De Michele",
        "Abel Armas Cervantes",
        "Lea Frermann"
      ],
      "abstract": "Business processes are fundamental to organizational operations, yet their\noptimization remains challenging due to the timeconsuming nature of manual\nprocess analysis. Our paper harnesses Large Language Models (LLMs) to automate\nvalue-added analysis, a qualitative process analysis technique that aims to\nidentify steps in the process that do not deliver value. To date, this\ntechnique is predominantly manual, time-consuming, and subjective. Our method\noffers a more principled approach which operates in two phases: first,\ndecomposing high-level activities into detailed steps to enable granular\nanalysis, and second, performing a value-added analysis to classify each step\naccording to Lean principles. This approach enables systematic identification\nof waste while maintaining the semantic understanding necessary for qualitative\nanalysis. We develop our approach using 50 business process models, for which\nwe collect and publish manual ground-truth labels. Our evaluation, comparing\nzero-shot baselines with more structured prompts reveals (a) a consistent\nbenefit of structured prompting and (b) promising performance for both tasks.\nWe discuss the potential for LLMs to augment human expertise in qualitative\nprocess analysis while reducing the time and subjectivity inherent in manual\napproaches.",
      "tldr_zh": "本研究提出了一种基于Large Language Models (LLMs)的自动化方法，用于评估商业流程的价值增加分析（value-added analysis），旨在解决手动分析耗时、主观且低效的问题。该方法分为两个阶段：首先，将高层活动分解为详细步骤；其次，根据Lean principles对每个步骤进行分类，以系统识别无价值步骤。使用50个商业流程模型并收集ground-truth labels进行评估，结果显示结构化提示比零-shot基线更有效，且LLMs在任务中表现出色。该方法有望增强人类专业知识，减少分析的主观性和时间消耗。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.06600v1",
      "published_date": "2025-04-09 05:52:50 UTC",
      "updated_date": "2025-04-09 05:52:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:20:05.343119"
    },
    {
      "arxiv_id": "2504.08000v1",
      "title": "Neuron-level Balance between Stability and Plasticity in Deep Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Jiahua Lan",
        "Sen Zhang",
        "Haixia Pan",
        "Ruijun Liu",
        "Li Shen",
        "Dacheng Tao"
      ],
      "abstract": "In contrast to the human ability to continuously acquire knowledge, agents\nstruggle with the stability-plasticity dilemma in deep reinforcement learning\n(DRL), which refers to the trade-off between retaining existing skills\n(stability) and learning new knowledge (plasticity). Current methods focus on\nbalancing these two aspects at the network level, lacking sufficient\ndifferentiation and fine-grained control of individual neurons. To overcome\nthis limitation, we propose Neuron-level Balance between Stability and\nPlasticity (NBSP) method, by taking inspiration from the observation that\nspecific neurons are strongly relevant to task-relevant skills. Specifically,\nNBSP first (1) defines and identifies RL skill neurons that are crucial for\nknowledge retention through a goal-oriented method, and then (2) introduces a\nframework by employing gradient masking and experience replay techniques\ntargeting these neurons to preserve the encoded existing skills while enabling\nadaptation to new tasks. Numerous experimental results on the Meta-World and\nAtari benchmarks demonstrate that NBSP significantly outperforms existing\napproaches in balancing stability and plasticity.",
      "tldr_zh": "这项研究针对深度强化学习（Deep Reinforcement Learning, DRL）中稳定性（保留现有技能）和可塑性（学习新知识）的困境，提出了一种在神经元级别的平衡方法，名为 NBSP。NBSP 先通过目标导向方法定义并识别关键的 RL 技能神经元，然后采用 gradient masking 和 experience replay 技术，针对这些神经元保护已有知识的同时允许适应新任务。实验结果显示，在 Meta-World 和 Atari 基准上，NBSP 显著优于现有方法，在稳定性与可塑性平衡方面取得了更好的性能。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Reinforcement learning, RL skill neuron, stability and plasticity",
      "pdf_url": "http://arxiv.org/pdf/2504.08000v1",
      "published_date": "2025-04-09 05:43:30 UTC",
      "updated_date": "2025-04-09 05:43:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:20:17.823100"
    },
    {
      "arxiv_id": "2504.06581v1",
      "title": "Right Prediction, Wrong Reasoning: Uncovering LLM Misalignment in RA Disease Diagnosis",
      "title_zh": "翻译失败",
      "authors": [
        "Umakanta Maharana",
        "Sarthak Verma",
        "Avarna Agarwal",
        "Prakashini Mruthyunjaya",
        "Dwarikanath Mahapatra",
        "Sakir Ahmed",
        "Murari Mandal"
      ],
      "abstract": "Large language models (LLMs) offer a promising pre-screening tool, improving\nearly disease detection and providing enhanced healthcare access for\nunderprivileged communities. The early diagnosis of various diseases continues\nto be a significant challenge in healthcare, primarily due to the nonspecific\nnature of early symptoms, the shortage of expert medical practitioners, and the\nneed for prolonged clinical evaluations, all of which can delay treatment and\nadversely affect patient outcomes. With impressive accuracy in prediction\nacross a range of diseases, LLMs have the potential to revolutionize clinical\npre-screening and decision-making for various medical conditions. In this work,\nwe study the diagnostic capability of LLMs for Rheumatoid Arthritis (RA) with\nreal world patients data. Patient data was collected alongside diagnoses from\nmedical experts, and the performance of LLMs was evaluated in comparison to\nexpert diagnoses for RA disease prediction. We notice an interesting pattern in\ndisease diagnosis and find an unexpected \\textit{misalignment between\nprediction and explanation}. We conduct a series of multi-round analyses using\ndifferent LLM agents. The best-performing model accurately predicts rheumatoid\narthritis (RA) diseases approximately 95\\% of the time. However, when medical\nexperts evaluated the reasoning generated by the model, they found that nearly\n68\\% of the reasoning was incorrect. This study highlights a clear misalignment\nbetween LLMs high prediction accuracy and its flawed reasoning, raising\nimportant questions about relying on LLM explanations in clinical settings.\n\\textbf{LLMs provide incorrect reasoning to arrive at the correct answer for RA\ndisease diagnosis.}",
      "tldr_zh": "该研究探讨了大语言模型（LLMs）在风湿性关节炎（RA）疾病诊断中的表现，发现尽管LLMs的预测准确率高达95%，但其推理过程存在严重误align（misalignment），约68%的推理被医疗专家判定为错误。通过使用真实患者数据和多轮分析，与专家诊断进行比较，论文揭示了LLMs在临床预筛中的潜在风险。最终，该工作强调了依赖LLMs解释的局限性，呼吁在医疗决策中谨慎使用此类模型。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.06581v1",
      "published_date": "2025-04-09 05:04:01 UTC",
      "updated_date": "2025-04-09 05:04:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:20:28.644243"
    },
    {
      "arxiv_id": "2504.06580v1",
      "title": "Exploring Ordinal Bias in Action Recognition for Instructional Videos",
      "title_zh": "探索教学视频动作识别中的序偏差",
      "authors": [
        "Joochan Kim",
        "Minjoon Jung",
        "Byoung-Tak Zhang"
      ],
      "abstract": "Action recognition models have achieved promising results in understanding\ninstructional videos. However, they often rely on dominant, dataset-specific\naction sequences rather than true video comprehension, a problem that we define\nas ordinal bias. To address this issue, we propose two effective video\nmanipulation methods: Action Masking, which masks frames of frequently\nco-occurring actions, and Sequence Shuffling, which randomizes the order of\naction segments. Through comprehensive experiments, we demonstrate that current\nmodels exhibit significant performance drops when confronted with nonstandard\naction sequences, underscoring their vulnerability to ordinal bias. Our\nfindings emphasize the importance of rethinking evaluation strategies and\ndeveloping models capable of generalizing beyond fixed action patterns in\ndiverse instructional videos.",
      "tldr_zh": "本研究探讨了动作识别模型在教学视频中的ordinal bias问题，即模型过度依赖数据集特定的动作序列，而非真正的视频理解，导致泛化能力不足。\n为了解决这一问题，研究者提出了两种视频操作方法：Action Masking（掩盖频繁共同出现的动作帧）和Sequence Shuffling（随机化动作段的顺序）。\n通过全面实验，证明当前模型在面对非标准动作序列时性能显著下降，这强调了重新思考评估策略并开发能超越固定模式、适用于多样化教学视频的模型的重要性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to SCSL @ ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.06580v1",
      "published_date": "2025-04-09 05:03:51 UTC",
      "updated_date": "2025-04-09 05:03:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:20:40.932444"
    },
    {
      "arxiv_id": "2504.06578v1",
      "title": "Attributes-aware Visual Emotion Representation Learning",
      "title_zh": "属性感知的视觉情感表示学习",
      "authors": [
        "Rahul Singh Maharjan",
        "Marta Romeo",
        "Angelo Cangelosi"
      ],
      "abstract": "Visual emotion analysis or recognition has gained considerable attention due\nto the growing interest in understanding how images can convey rich semantics\nand evoke emotions in human perception. However, visual emotion analysis poses\ndistinctive challenges compared to traditional vision tasks, especially due to\nthe intricate relationship between general visual features and the different\naffective states they evoke, known as the affective gap. Researchers have used\ndeep representation learning methods to address this challenge of extracting\ngeneralized features from entire images. However, most existing methods\noverlook the importance of specific emotional attributes such as brightness,\ncolorfulness, scene understanding, and facial expressions. Through this paper,\nwe introduce A4Net, a deep representation network to bridge the affective gap\nby leveraging four key attributes: brightness (Attribute 1), colorfulness\n(Attribute 2), scene context (Attribute 3), and facial expressions (Attribute\n4). By fusing and jointly training all aspects of attribute recognition and\nvisual emotion analysis, A4Net aims to provide a better insight into emotional\ncontent in images. Experimental results show the effectiveness of A4Net,\nshowcasing competitive performance compared to state-of-the-art methods across\ndiverse visual emotion datasets. Furthermore, visualizations of activation maps\ngenerated by A4Net offer insights into its ability to generalize across\ndifferent visual emotion datasets.",
      "tldr_zh": "本研究针对视觉情感分析中的情感差距(affective gap)问题，提出了一种属性感知的深度表示网络A4Net。该方法通过整合四个关键属性——亮度(brightness)、色彩丰富度(colorfulness)、场景上下文(scene context)和面部表情(facial expressions)，实现属性识别与视觉情感分析的融合和联合训练，从而更好地提取图像的情感内容。实验结果表明，A4Net在多种视觉情感数据集上表现出色，与最先进方法相比具有竞争性，并通过激活地图可视化展示了其在不同数据集上的泛化能力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "9 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.06578v1",
      "published_date": "2025-04-09 05:00:43 UTC",
      "updated_date": "2025-04-09 05:00:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:20:53.579052"
    },
    {
      "arxiv_id": "2504.06549v1",
      "title": "Societal Impacts Research Requires Benchmarks for Creative Composition Tasks",
      "title_zh": "翻译失败",
      "authors": [
        "Judy Hanwen Shen",
        "Carlos Guestrin"
      ],
      "abstract": "Foundation models that are capable of automating cognitive tasks represent a\npivotal technological shift, yet their societal implications remain unclear.\nThese systems promise exciting advances, yet they also risk flooding our\ninformation ecosystem with formulaic, homogeneous, and potentially misleading\nsynthetic content. Developing benchmarks grounded in real use cases where these\nrisks are most significant is therefore critical. Through a thematic analysis\nusing 2 million language model user prompts, we identify creative composition\ntasks as a prevalent usage category where users seek help with personal tasks\nthat require everyday creativity. Our fine-grained analysis identifies\nmismatches between current benchmarks and usage patterns among these tasks.\nCrucially, we argue that the same use cases that currently lack thorough\nevaluations can lead to negative downstream impacts. This position paper argues\nthat benchmarks focused on creative composition tasks is a necessary step\ntowards understanding the societal harms of AI-generated content. We call for\ngreater transparency in usage patterns to inform the development of new\nbenchmarks that can effectively measure both the progress and the impacts of\nmodels with creative capabilities.",
      "tldr_zh": "这篇论文强调，基础模型（foundation models）在自动化认知任务方面虽有潜力，但可能导致信息生态泛滥公式化、同质化和误导性合成内容，从而带来社会风险。研究者通过对200万条语言模型用户提示进行主题分析（thematic analysis），识别出创意写作任务（creative composition tasks）作为常见应用场景，用户常用于需要日常创意的个人任务，但当前基准（benchmarks）与实际使用模式不匹配。论文主张开发针对这些任务的专用基准，以评估AI生成内容的负面下游影响，并呼吁提升使用模式透明度，以更好地理解模型的进步和社会危害。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "v1: ICLR 2025 Workshop on Bidirectional Human-AI Alignment (BiAlign)",
      "pdf_url": "http://arxiv.org/pdf/2504.06549v1",
      "published_date": "2025-04-09 03:12:16 UTC",
      "updated_date": "2025-04-09 03:12:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:21:06.274835"
    },
    {
      "arxiv_id": "2504.08810v1",
      "title": "PriM: Principle-Inspired Material Discovery through Multi-Agent Collaboration",
      "title_zh": "翻译失败",
      "authors": [
        "Zheyuan Lai",
        "Yingming Pu"
      ],
      "abstract": "Complex chemical space and limited knowledge scope with biases holds immense\nchallenge for human scientists, yet in automated materials discovery. Existing\nintelligent methods relies more on numerical computation, leading to\ninefficient exploration and results with hard-interpretability. To bridge this\ngap, we introduce a principles-guided material discovery system powered by\nlanguage inferential multi-agent system (MAS), namely PriM. Our framework\nintegrates automated hypothesis generation with experimental validation in a\nroundtable system of MAS, enabling systematic exploration while maintaining\nscientific rigor. Based on our framework, the case study of nano helix\ndemonstrates higher materials exploration rate and property value while\nproviding transparent reasoning pathways. This approach develops an\nautomated-and-transparent paradigm for material discovery, with broad\nimplications for rational design of functional materials. Code is publicly\navailable at our \\href{https://github.com/amair-lab/PriM}{GitHub}.",
      "tldr_zh": "该研究针对材料发现领域的复杂化学空间和现有方法的计算依赖性问题，提出了一种基于原则引导的多智能体系统（Multi-Agent System, MAS）框架，即 PriM。通过整合自动假设生成与实验验证的圆桌协作机制，PriM 实现了系统化的材料探索，同时确保推理过程的透明性。在纳米螺旋的案例研究中，该框架展示了更高的材料探索率和属性值提升，为功能材料的理性设计提供了自动化且可解释的范式。代码已在 GitHub 上公开可用。",
      "categories": [
        "cs.LG",
        "cond-mat.mtrl-sci",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.08810v1",
      "published_date": "2025-04-09 03:05:10 UTC",
      "updated_date": "2025-04-09 03:05:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:21:16.721116"
    },
    {
      "arxiv_id": "2504.06542v1",
      "title": "Polygon: Symbolic Reasoning for SQL using Conflict-Driven Under-Approximation Search",
      "title_zh": "Polygon: 基于冲突驱动下近似搜索的 SQL 符号推理",
      "authors": [
        "Pinhan Zhao",
        "Yuepeng Wang",
        "Xinyu Wang"
      ],
      "abstract": "We present a novel symbolic reasoning engine for SQL which can efficiently\ngenerate an input $I$ for $n$ queries $P_1, \\cdots, P_n$, such that their\noutputs on $I$ satisfy a given property (expressed in SMT). This is useful in\ndifferent contexts, such as disproving equivalence of two SQL queries and\ndisambiguating a set of queries. Our first idea is to reason about an\nunder-approximation of each $P_i$ -- that is, a subset of $P_i$'s input-output\nbehaviors. While it makes our approach both semantics-aware and lightweight,\nthis idea alone is incomplete (as a fixed under-approximation might miss some\nbehaviors of interest). Therefore, our second idea is to perform search over an\nexpressive family of under-approximations (which collectively cover all program\nbehaviors of interest), thereby making our approach complete. We have\nimplemented these ideas in a tool, Polygon, and evaluated it on over 30,000\nbenchmarks across two tasks (namely, SQL equivalence refutation and query\ndisambiguation). Our evaluation results show that Polygon significantly\noutperforms all prior techniques.",
      "tldr_zh": "我们介绍了Polygon，一种基于冲突驱动低估搜索(Conflict-Driven Under-Approximation Search)的SQL符号推理引擎，能够为多个查询生成输入I，使其输出满足给定的SMT属性，从而用于SQL查询不等价证明和查询消歧等任务。该方法首先对每个查询进行低估(under-approximation)，捕捉其输入-输出行为的子集，以实现语义感知和轻量处理；其次，通过搜索一个表达丰富的低估家族，确保覆盖所有相关行为，从而提升推理的完整性。在超过30,000个基准测试中，Polygon在SQL等价反驳和查询消歧任务上显著优于现有技术。",
      "categories": [
        "cs.PL",
        "cs.AI",
        "cs.DB",
        "cs.SE"
      ],
      "primary_category": "cs.PL",
      "comment": "PLDI 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.06542v1",
      "published_date": "2025-04-09 02:46:52 UTC",
      "updated_date": "2025-04-09 02:46:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:21:29.226424"
    },
    {
      "arxiv_id": "2504.06538v1",
      "title": "OPAL: Encoding Causal Understanding of Physical Systems for Robot Learning",
      "title_zh": "OPAL: 编码物理系统的因果理解",
      "authors": [
        "Daniel Tcheurekdjian",
        "Joshua Klasmeier",
        "Tom Cooney",
        "Christopher McCann",
        "Tyler Fenstermaker"
      ],
      "abstract": "We present OPAL (Operant Physical Agent with Language), a novel\nvision-language-action architecture that introduces topological constraints to\nflow matching for robotic control. To do so, we further introduce topological\nattention. Our approach models action sequences as topologically-structured\nrepresentations with non-trivial constraints. Experimental results across 10\ncomplex manipulation tasks demonstrate OPAL's superior performance compared to\nprevious approaches, including Octo, OpenVLA, and ${\\pi}$0.\n  Our architecture achieves significant improvements in zero-shot performance\nwithout requiring task-specific fine-tuning, while reducing inference\ncomputational requirements by 42%. The theoretical guarantees provided by our\ntopological approach result in more coherent long-horizon action sequences. Our\nresults highlight the potential of constraining the search space of learning\nproblems in robotics by deriving from fundamental physical laws, and the\npossibility of using topological attention to embed causal understanding into\ntransformer architectures.",
      "tldr_zh": "本研究引入了 OPAL（Operant Physical Agent with Language），一种新型视觉-语言-动作架构，通过拓扑约束和拓扑 attention 来优化机器人控制，将动作序列建模为具有非平凡约束的拓扑结构表示。实验在 10 个复杂操作任务中显示，OPAL 比现有方法如 Octo、OpenVLA 和 π0 表现出色，实现零-shot performance 的显著提升，同时减少 42% 的推理计算需求。OPAL 的拓扑方法提供了理论保证，确保更连贯的长视野动作序列，并展示了利用基本物理定律约束机器人学习空间的潜力。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "11 pages, 2 figures, 3 tables, 24 equations",
      "pdf_url": "http://arxiv.org/pdf/2504.06538v1",
      "published_date": "2025-04-09 02:29:36 UTC",
      "updated_date": "2025-04-09 02:29:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:21:41.436012"
    },
    {
      "arxiv_id": "2504.06536v1",
      "title": "Lugha-Llama: Adapting Large Language Models for African Languages",
      "title_zh": "翻译失败",
      "authors": [
        "Happy Buzaaba",
        "Alexander Wettig",
        "David Ifeoluwa Adelani",
        "Christiane Fellbaum"
      ],
      "abstract": "Large language models (LLMs) have achieved impressive results in a wide range\nof natural language applications. However, they often struggle to recognize\nlow-resource languages, in particular African languages, which are not well\nrepresented in large training corpora. In this paper, we consider how to adapt\nLLMs to low-resource African languages. We find that combining curated data\nfrom African languages with high-quality English educational texts results in a\ntraining mix that substantially improves the model's performance on these\nlanguages. On the challenging IrokoBench dataset, our models consistently\nachieve the best performance amongst similarly sized baselines, particularly on\nknowledge-intensive multiple-choice questions (AfriMMLU). Additionally, on the\ncross-lingual question answering benchmark AfriQA, our models outperform the\nbase model by over 10%. To better understand the role of English data during\ntraining, we translate a subset of 200M tokens into Swahili language and\nperform an analysis which reveals that the content of these data is primarily\nresponsible for the strong performance. We release our models and data to\nencourage future research on African languages.",
      "tldr_zh": "本文提出 Lugha-Llama 方法，以适应大型语言模型 (LLMs) 到低资源非洲语言，通过结合精选的非洲语言数据和高品质英语教育文本作为训练混合，从而显著提升模型性能。在 IrokoBench 数据集的知识密集型多项选择任务 (AfriMMLU) 上，该模型优于类似规模基线，而在跨语言问答基准 AfriQA 上，性能比基线提升超过 10%。此外，通过将部分数据翻译成斯瓦希里语的分析，研究发现内容质量是关键因素，并开源模型和数据以推动非洲语言研究。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.06536v1",
      "published_date": "2025-04-09 02:25:53 UTC",
      "updated_date": "2025-04-09 02:25:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:21:53.550111"
    },
    {
      "arxiv_id": "2504.08806v1",
      "title": "Endowing Embodied Agents with Spatial Reasoning Capabilities for Vision-and-Language Navigation",
      "title_zh": "翻译失败",
      "authors": [
        "Luo Ling",
        "Bai Qianqian"
      ],
      "abstract": "Enhancing the spatial perception capabilities of mobile robots is crucial for\nachieving embodied Vision-and-Language Navigation (VLN). Although significant\nprogress has been made in simulated environments, directly transferring these\ncapabilities to real-world scenarios often results in severe hallucination\nphenomena, causing robots to lose effective spatial awareness. To address this\nissue, we propose BrainNav, a bio-inspired spatial cognitive navigation\nframework inspired by biological spatial cognition theories and cognitive map\ntheory. BrainNav integrates dual-map (coordinate map and topological map) and\ndual-orientation (relative orientation and absolute orientation) strategies,\nenabling real-time navigation through dynamic scene capture and path planning.\nIts five core modules-Hippocampal Memory Hub, Visual Cortex Perception Engine,\nParietal Spatial Constructor, Prefrontal Decision Center, and Cerebellar Motion\nExecution Unit-mimic biological cognitive functions to reduce spatial\nhallucinations and enhance adaptability. Validated in a zero-shot real-world\nlab environment using the Limo Pro robot, BrainNav, compatible with GPT-4,\noutperforms existing State-of-the-Art (SOTA) Vision-and-Language Navigation in\nContinuous Environments (VLN-CE) methods without fine-tuning.",
      "tldr_zh": "这篇论文针对 Vision-and-Language Navigation (VLN) 中机器人空间感知的幻觉问题，提出了一种受生物空间认知理论启发的框架 BrainNav，以提升实体代理的空间推理能力。BrainNav 整合双地图（坐标地图和拓扑地图）以及双方向策略（相对方向和绝对方向），并通过五个核心模块——Hippocampal Memory Hub、Visual Cortex Perception Engine、Parietal Spatial Constructor、Prefrontal Decision Center 和 Cerebellar Motion Execution Unit——模仿生物认知功能，实现动态场景捕捉和实时路径规划，从而减少空间幻觉并提高适应性。在零样本真实世界实验室环境中，使用 Limo Pro 机器人验证，BrainNav 与 GPT-4 兼容，且在不进行微调的情况下超越现有 SOTA VLN-CE 方法。",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.08806v1",
      "published_date": "2025-04-09 02:19:22 UTC",
      "updated_date": "2025-04-09 02:19:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:22:06.565175"
    },
    {
      "arxiv_id": "2504.06533v2",
      "title": "Flexible Graph Similarity Computation With A Proactive Optimization Strategy",
      "title_zh": "翻译失败",
      "authors": [
        "Zhouyang Liu",
        "Ning Liu",
        "Yixin Chen",
        "Jiezhong He",
        "Dongsheng Li"
      ],
      "abstract": "Graph Edit Distance (GED) offers a principled and flexible measure of graph\nsimilarity, as it quantifies the minimum cost needed to transform one graph\ninto another with customizable edit operation costs. Despite recent\nlearning-based efforts to approximate GED via vector space representations,\nexisting methods struggle with adapting to varying operation costs.\nFurthermore, they suffer from inefficient, reactive mapping refinements due to\nreliance on isolated node-level distance as guidance. To address these issues,\nwe propose GEN, a novel learning-based approach for flexible GED approximation.\nGEN addresses the varying costs adaptation by integrating operation costs prior\nto match establishment, enabling mappings to dynamically adapt to cost\nvariations. Furthermore, GEN introduces a proactive guidance optimization\nstrategy that captures graph-level dependencies between matches, allowing\ninformed matching decisions in a single step without costly iterative\nrefinements. Extensive evaluations on real-world and synthetic datasets\ndemonstrate that GEN achieves up to 37.8% reduction in GED approximation error\nand 72.7% reduction in inference time compared with state-of-the-art methods,\nwhile consistently maintaining robustness under diverse cost settings and graph\nsizes.",
      "tldr_zh": "这篇论文提出了一种新型学习方法 GEN，用于灵活计算 Graph Edit Distance (GED)，以解决现有方法在适应不同操作成本时的局限性，包括成本适应性和低效的映射优化问题。GEN 通过在匹配建立前整合操作成本，并引入主动指导优化策略来捕捉图级依赖性，实现单步匹配决策，避免昂贵的迭代精炼。实验结果显示，GEN 在真实和合成数据集上比最先进方法降低了 37.8% 的 GED 近似误差和 72.7% 的推理时间，同时在多样化的成本设置和图大小下保持鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DS"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.06533v2",
      "published_date": "2025-04-09 02:16:46 UTC",
      "updated_date": "2025-05-15 08:42:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:22:17.462772"
    },
    {
      "arxiv_id": "2504.06532v1",
      "title": "WaveHiTS: Wavelet-Enhanced Hierarchical Time Series Modeling for Wind Direction Nowcasting in Eastern Inner Mongolia",
      "title_zh": "翻译失败",
      "authors": [
        "Hailong Shu",
        "Weiwei Song",
        "Yue Wang",
        "Jiping Zhang"
      ],
      "abstract": "Wind direction forecasting plays a crucial role in optimizing wind energy\nproduction, but faces significant challenges due to the circular nature of\ndirectional data, error accumulation in multi-step forecasting, and complex\nmeteorological interactions. This paper presents a novel model, WaveHiTS, which\nintegrates wavelet transform with Neural Hierarchical Interpolation for Time\nSeries to address these challenges. Our approach decomposes wind direction into\nU-V components, applies wavelet transform to capture multi-scale frequency\npatterns, and utilizes a hierarchical structure to model temporal dependencies\nat multiple scales, effectively mitigating error propagation. Experiments\nconducted on real-world meteorological data from Inner Mongolia, China\ndemonstrate that WaveHiTS significantly outperforms deep learning models (RNN,\nLSTM, GRU), transformer-based approaches (TFT, Informer, iTransformer), and\nhybrid models (EMD-LSTM). The proposed model achieves RMSE values of\napproximately 19.2{\\deg}-19.4{\\deg} compared to 56{\\deg}-64{\\deg} for deep\nlearning recurrent models, maintaining consistent accuracy across all\nforecasting steps up to 60 minutes ahead. Moreover, WaveHiTS demonstrates\nsuperior robustness with vector correlation coefficients (VCC) of 0.985-0.987\nand hit rates of 88.5%-90.1%, substantially outperforming baseline models.\nAblation studies confirm that each component-wavelet transform, hierarchical\nstructure, and U-V decomposition-contributes meaningfully to overall\nperformance. These improvements in wind direction nowcasting have significant\nimplications for enhancing wind turbine yaw control efficiency and grid\nintegration of wind energy.",
      "tldr_zh": "本研究针对风向预测面临的挑战，如方向数据循环性、多步预测错误积累和复杂气象互动，提出了一种新型模型WaveHiTS，将wavelet transform与Neural Hierarchical Interpolation for Time Series相结合。WaveHiTS通过将风向分解为U-V组件、应用小波变换捕获多尺度频率模式，并采用分层结构建模时间依赖性，有效减少错误传播。实验在内蒙古真实气象数据上显示，WaveHiTS的RMSE值为19.2°-19.4°，远优于RNN、LSTM等基线模型（56°-64°），并在VCC（0.985-0.987）和命中率（88.5%-90.1%）上表现出卓越的鲁棒性和一致性。这些改进有助于提升风力涡轮机偏航控制效率和风能网格整合。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.06532v1",
      "published_date": "2025-04-09 02:15:48 UTC",
      "updated_date": "2025-04-09 02:15:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:22:30.186854"
    },
    {
      "arxiv_id": "2504.06531v1",
      "title": "Beyond Moore's Law: Harnessing the Redshift of Generative AI with Effective Hardware-Software Co-Design",
      "title_zh": "超越摩尔定律：通过有效的硬件-软件协同设计利用生成式 AI 的红移",
      "authors": [
        "Amir Yazdanbakhsh"
      ],
      "abstract": "For decades, Moore's Law has served as a steadfast pillar in computer\narchitecture and system design, promoting a clear abstraction between hardware\nand software. This traditional Moore's computing paradigm has deepened the rift\nbetween the two, enabling software developers to achieve near-exponential\nperformance gains often without needing to delve deeply into hardware-specific\noptimizations. Yet today, Moore's Law -- with its once relentless performance\ngains now diminished to incremental improvements -- faces inevitable physical\nbarriers. This stagnation necessitates a reevaluation of the conventional\nsystem design philosophy. The traditional decoupled system design philosophy,\nwhich maintains strict abstractions between hardware and software, is\nincreasingly obsolete. The once-clear boundary between software and hardware is\nrapidly dissolving, replaced by co-design. It is imperative for the computing\ncommunity to intensify its commitment to hardware-software co-design, elevating\nsystem abstractions to first-class citizens and reimagining design principles\nto satisfy the insatiable appetite of modern computing. Hardware-software\nco-design is not a recent innovation. To illustrate its historical evolution, I\nclassify its development into five relatively distinct ``epochs''. This post\nalso highlights the growing influence of the architecture community in\ninterdisciplinary teams -- particularly alongside ML researchers -- and\nexplores why current co-design paradigms are struggling in today's computing\nlandscape. Additionally, I will examine the concept of the ``hardware lottery''\nand explore directions to mitigate its constraining influence on the next era\nof computing innovation.",
      "tldr_zh": "这篇论文讨论了Moore's Law的衰退及其对计算机架构的影响，指出传统硬件和软件分离的抽象模式已过时，无法满足现代计算需求。作者主张加强hardware-software co-design，通过将系统抽象提升为首要设计原则来推动创新，并将co-design的历史发展分为五个时代，以说明其演变。论文还分析了架构社区在多学科团队中的作用、当前co-design范式的不足，以及如何缓解“hardware lottery”的负面影响，以支持生成式AI等领域的未来计算进步。",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.06531v1",
      "published_date": "2025-04-09 02:10:58 UTC",
      "updated_date": "2025-04-09 02:10:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:22:40.988835"
    },
    {
      "arxiv_id": "2504.06527v1",
      "title": "TSP-OCS: A Time-Series Prediction for Optimal Camera Selection in Multi-Viewpoint Surgical Video Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Xinyu Liu",
        "Xiaoguang Lin",
        "Xiang Liu",
        "Yong Yang",
        "Hongqian Wang",
        "Qilong Sun"
      ],
      "abstract": "Recording the open surgery process is essential for educational and medical\nevaluation purposes; however, traditional single-camera methods often face\nchallenges such as occlusions caused by the surgeon's head and body, as well as\nlimitations due to fixed camera angles, which reduce comprehensibility of the\nvideo content. This study addresses these limitations by employing a\nmulti-viewpoint camera recording system, capturing the surgical procedure from\nsix different angles to mitigate occlusions. We propose a fully supervised\nlearning-based time series prediction method to choose the best shot sequences\nfrom multiple simultaneously recorded video streams, ensuring optimal\nviewpoints at each moment. Our time series prediction model forecasts future\ncamera selections by extracting and fusing visual and semantic features from\nsurgical videos using pre-trained models. These features are processed by a\ntemporal prediction network with TimeBlocks to capture sequential dependencies.\nA linear embedding layer reduces dimensionality, and a Softmax classifier\nselects the optimal camera view based on the highest probability. In our\nexperiments, we created five groups of open thyroidectomy videos, each with\nsimultaneous recordings from six different angles. The results demonstrate that\nour method achieves competitive accuracy compared to traditional supervised\nmethods, even when predicting over longer time horizons. Furthermore, our\napproach outperforms state-of-the-art time series prediction techniques on our\ndataset. This manuscript makes a unique contribution by presenting an\ninnovative framework that advances surgical video analysis techniques, with\nsignificant implications for improving surgical education and patient safety.",
      "tldr_zh": "这篇论文提出 TSP-OCS，一种基于时序预测的方法，用于多视角手术视频分析中选择最佳摄像头，以解决传统单摄像头录像中遮挡和固定角度的局限性。方法通过预训练模型提取并融合手术视频的视觉和语义特征，利用 TimeBlocks 的时间预测网络捕获顺序依赖，并结合线性嵌入层和 Softmax 分类器预测未来最佳视角。实验在五组开放性甲状腺切除术视频数据集上验证，该方法在长时序预测中表现出色，优于现有技术，并为提升手术教育和患者安全提供创新框架。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.06527v1",
      "published_date": "2025-04-09 02:07:49 UTC",
      "updated_date": "2025-04-09 02:07:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:22:53.518281"
    },
    {
      "arxiv_id": "2504.06525v1",
      "title": "The Power of the Pareto Front: Balancing Uncertain Rewards for Adaptive Experimentation in scanning probe microscopy",
      "title_zh": "翻译失败",
      "authors": [
        "Yu Liu",
        "Sergei V. Kalinin"
      ],
      "abstract": "Automated experimentation has the potential to revolutionize scientific\ndiscovery, but its effectiveness depends on well-defined optimization targets,\nwhich are often uncertain or probabilistic in real-world settings. In this\nwork, we demonstrate the application of Multi-Objective Bayesian Optimization\n(MOBO) to balance multiple, competing rewards in autonomous experimentation.\nUsing scanning probe microscopy (SPM) imaging, one of the most widely used and\nfoundational SPM modes, we show that MOBO can optimize imaging parameters to\nenhance measurement quality, reproducibility, and efficiency. A key advantage\nof this approach is the ability to compute and analyze the Pareto front, which\nnot only guides optimization but also provides physical insights into the\ntrade-offs between different objectives. Additionally, MOBO offers a natural\nframework for human-in-the-loop decision-making, enabling researchers to\nfine-tune experimental trade-offs based on domain expertise. By standardizing\nhigh-quality, reproducible measurements and integrating human input into\nAI-driven optimization, this work highlights MOBO as a powerful tool for\nadvancing autonomous scientific discovery.",
      "tldr_zh": "本研究探讨了 Multi-Objective Bayesian Optimization (MOBO) 在自动化实验中的应用，旨在平衡扫描探针显微镜 (SPM) 成像中的多个不确定奖励，从而优化参数以提升测量质量、可重复性和效率。\nMOBO 的关键优势在于计算 Pareto front，这不仅指导优化过程，还提供物理洞见，帮助理解不同目标之间的权衡。\n此外，该框架支持人类参与决策，允许研究人员基于领域知识调整实验权衡，最终促进标准化高品质测量和 AI 驱动的自主科学发现。",
      "categories": [
        "cs.LG",
        "cond-mat.mes-hall",
        "cond-mat.mtrl-sci",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "23 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.06525v1",
      "published_date": "2025-04-09 01:59:31 UTC",
      "updated_date": "2025-04-09 01:59:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:23:05.157330"
    },
    {
      "arxiv_id": "2504.06514v2",
      "title": "Missing Premise exacerbates Overthinking: Are Reasoning Models losing Critical Thinking Skill?",
      "title_zh": "翻译失败",
      "authors": [
        "Chenrui Fan",
        "Ming Li",
        "Lichao Sun",
        "Tianyi Zhou"
      ],
      "abstract": "We find that the response length of reasoning LLMs, whether trained by\nreinforcement learning or supervised learning, drastically increases for\nill-posed questions with missing premises (MiP), ending up with redundant and\nineffective thinking. This newly introduced scenario exacerbates the general\noverthinking issue to a large extent, which we name as the MiP-Overthinking.\nSuch failures are against the ``test-time scaling law'' but have been widely\nobserved on multiple datasets we curated with MiP, indicating the harm of cheap\noverthinking and a lack of critical thinking. Surprisingly, LLMs not\nspecifically trained for reasoning exhibit much better performance on the MiP\nscenario, producing much shorter responses that quickly identify ill-posed\nqueries. This implies a critical flaw of the current training recipe for\nreasoning LLMs, which does not encourage efficient thinking adequately, leading\nto the abuse of thinking patterns. To further investigate the reasons behind\nsuch failures, we conduct fine-grained analyses of the reasoning length,\noverthinking patterns, and location of critical thinking on different types of\nLLMs. Moreover, our extended ablation study reveals that the overthinking is\ncontagious through the distillation of reasoning models' responses. These\nresults improve the understanding of overthinking and shed novel insights into\nmitigating the problem.",
      "tldr_zh": "本研究发现，推理LLMs（Large Language Models）在处理缺少前提（Missing Premises, MiP）的无效问题时，会显著增加响应长度，导致冗余的overthinking现象，并命名为MiP-Overthinking，这违背了test-time scaling law并在多个数据集上得到证实。相比之下，未专门训练用于推理的LLMs表现出色，能生成更短的响应并快速识别无效查询，揭示当前训练方法缺乏对高效思考的鼓励。研究通过细粒度分析推理长度、overthinking模式和关键思考位置，以及消融研究，证明overthinking可通过distillation传播，并提供新见解来缓解这一问题。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.06514v2",
      "published_date": "2025-04-09 01:25:27 UTC",
      "updated_date": "2025-04-11 02:36:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:23:17.203024"
    },
    {
      "arxiv_id": "2504.06497v1",
      "title": "Continuous-Variable Quantum Encoding Techniques: A Comparative Study of Embedding Techniques and Their Impact on Machine Learning Performance",
      "title_zh": "翻译失败",
      "authors": [
        "Minati Rath",
        "Hema Date"
      ],
      "abstract": "This study explores the intersection of continuous-variable quantum computing\n(CVQC) and classical machine learning, focusing on CVQC data encoding\ntechniques, including Displacement encoding and squeezing encoding, alongside\nInstantaneous Quantum Polynomial (IQP) encoding from discrete quantum\ncomputing. We perform an extensive empirical analysis to assess the impact of\nthese encoding methods on classical machine learning models, such as Logistic\nRegression, Support Vector Machines, K-Nearest Neighbors, and ensemble methods\nlike Random Forest and LightGBM. Our findings indicate that CVQC-based encoding\nmethods significantly enhance feature expressivity, resulting in improved\nclassification accuracy and F1 scores, especially in high-dimensional and\ncomplex datasets. However, these improvements come with varying computational\ncosts, which depend on the complexity of the encoding and the architecture of\nthe machine learning models. Additionally, we examine the trade-off between\nquantum expressibility and classical learnability, offering valuable insights\ninto the practical feasibility of incorporating these quantum encodings into\nreal-world applications. This study contributes to the growing body of research\non quantum-classical hybrid learning, emphasizing the role of CVQC in advancing\nquantum data representation and its integration into classical machine learning\nworkflows.",
      "tldr_zh": "这篇论文比较了连续变量量子计算 (CVQC) 的数据编码技术，包括 Displacement encoding、squeezing encoding 和 Instantaneous Quantum Polynomial (IQP) encoding，对经典机器学习模型（如 Logistic Regression、Support Vector Machines 和 Random Forest）的性能影响。研究通过广泛的实证分析发现，这些编码方法显著提升了特征表达性，导致分类准确性和 F1 分数在高维复杂数据集上得到改善，但同时带来了额外的计算成本。论文还探讨了量子表达性和经典可学习性之间的权衡，提供实用见解，推进量子-经典混合学习在实际应用中的整合。",
      "categories": [
        "quant-ph",
        "cs.AI"
      ],
      "primary_category": "quant-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.06497v1",
      "published_date": "2025-04-09 00:00:45 UTC",
      "updated_date": "2025-04-09 00:00:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T11:23:30.011712"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 86,
  "processed_papers_count": 86,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-24T11:23:47.635044"
}
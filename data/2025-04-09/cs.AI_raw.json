[
  {
    "arxiv_id": "2504.07336v1",
    "title": "Zeus: Zero-shot LLM Instruction for Union Segmentation in Multimodal Medical Imaging",
    "authors": [
      "Siyuan Dai",
      "Kai Ye",
      "Guodong Liu",
      "Haoteng Tang",
      "Liang Zhan"
    ],
    "abstract": "Medical image segmentation has achieved remarkable success through the\ncontinuous advancement of UNet-based and Transformer-based foundation\nbackbones. However, clinical diagnosis in the real world often requires\nintegrating domain knowledge, especially textual information. Conducting\nmultimodal learning involves visual and text modalities shown as a solution,\nbut collecting paired vision-language datasets is expensive and time-consuming,\nposing significant challenges. Inspired by the superior ability in numerous\ncross-modal tasks for Large Language Models (LLMs), we proposed a novel\nVision-LLM union framework to address the issues. Specifically, we introduce\nfrozen LLMs for zero-shot instruction generation based on corresponding medical\nimages, imitating the radiology scanning and report generation process. {To\nbetter approximate real-world diagnostic processes}, we generate more precise\ntext instruction from multimodal radiology images (e.g., T1-w or T2-w MRI and\nCT). Based on the impressive ability of semantic understanding and rich\nknowledge of LLMs. This process emphasizes extracting special features from\ndifferent modalities and reunion the information for the ultimate clinical\ndiagnostic. With generated text instruction, our proposed union segmentation\nframework can handle multimodal segmentation without prior collected\nvision-language datasets. To evaluate our proposed method, we conduct\ncomprehensive experiments with influential baselines, the statistical results\nand the visualized case study demonstrate the superiority of our novel method.}",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "21 pages, 4 figures, In Press by a journal",
    "pdf_url": "http://arxiv.org/pdf/2504.07336v1",
    "published_date": "2025-04-09 23:33:35 UTC",
    "updated_date": "2025-04-09 23:33:35 UTC"
  },
  {
    "arxiv_id": "2504.07334v2",
    "title": "Objaverse++: Curated 3D Object Dataset with Quality Annotations",
    "authors": [
      "Chendi Lin",
      "Heshan Liu",
      "Qunshu Lin",
      "Zachary Bright",
      "Shitao Tang",
      "Yihui He",
      "Minghao Liu",
      "Ling Zhu",
      "Cindy Le"
    ],
    "abstract": "This paper presents Objaverse++, a curated subset of Objaverse enhanced with\ndetailed attribute annotations by human experts. Recent advances in 3D content\ngeneration have been driven by large-scale datasets such as Objaverse, which\ncontains over 800,000 3D objects collected from the Internet. Although\nObjaverse represents the largest available 3D asset collection, its utility is\nlimited by the predominance of low-quality models. To address this limitation,\nwe manually annotate 10,000 3D objects with detailed attributes, including\naesthetic quality scores, texture color classifications, multi-object\ncomposition flags, transparency characteristics, etc. Then, we trained a neural\nnetwork capable of annotating the tags for the rest of the Objaverse dataset.\nThrough experiments and a user study on generation results, we demonstrate that\nmodels pre-trained on our quality-focused subset achieve better performance\nthan those trained on the larger dataset of Objaverse in image-to-3D generation\ntasks. In addition, by comparing multiple subsets of training data filtered by\nour tags, our results show that the higher the data quality, the faster the\ntraining loss converges. These findings suggest that careful curation and rich\nannotation can compensate for the raw dataset size, potentially offering a more\nefficient path to develop 3D generative models. We release our enhanced dataset\nof approximately 500,000 curated 3D models to facilitate further research on\nvarious downstream tasks in 3D computer vision. In the near future, we aim to\nextend our annotations to cover the entire Objaverse dataset.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "68T45, 68T07",
      "I.2.10; I.3.5; I.3.7; I.4.8; I.5.1"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 8 figures. Accepted to CVPR 2025 Workshop on Efficient Large\n  Vision Models (April 2025)",
    "pdf_url": "http://arxiv.org/pdf/2504.07334v2",
    "published_date": "2025-04-09 23:29:08 UTC",
    "updated_date": "2025-04-11 23:48:12 UTC"
  },
  {
    "arxiv_id": "2504.07313v1",
    "title": "Identifying regions of interest in whole slide images of renal cell carcinoma",
    "authors": [
      "Mohammed Lamine Benomar",
      "Nesma Settouti",
      "Eric Debreuve",
      "Xavier Descombes",
      "Damien Ambrosetti"
    ],
    "abstract": "The histopathological images contain a huge amount of information, which can\nmake diagnosis an extremely timeconsuming and tedious task. In this study, we\ndeveloped a completely automated system to detect regions of interest (ROIs) in\nwhole slide images (WSI) of renal cell carcinoma (RCC), to reduce time analysis\nand assist pathologists in making more accurate decisions. The proposed\napproach is based on an efficient texture descriptor named dominant rotated\nlocal binary pattern (DRLBP) and color transformation to reveal and exploit the\nimmense texture variability at the microscopic high magnifications level.\nThereby, the DRLBPs retain the structural information and utilize the magnitude\nvalues in a local neighborhood for more discriminative power. For the\nclassification of the relevant ROIs, feature extraction of WSIs patches was\nperformed on the color channels separately to form the histograms. Next, we\nused the most frequently occurring patterns as a feature selection step to\ndiscard non-informative features. The performances of different classifiers on\na set of 1800 kidney cancer patches originating from 12 whole slide images were\ncompared and evaluated. Furthermore, the small size of the image dataset allows\nto investigate deep learning approach based on transfer learning for image\npatches classification by using deep features and fine-tuning methods. High\nrecognition accuracy was obtained and the classifiers are efficient, the best\nprecision result was 99.17% achieved with SVM. Moreover, transfer learning\nmodels perform well with comparable performance, and the highest precision\nusing ResNet-50 reached 98.50%. The proposed approach results revealed a very\nefficient image classification and demonstrated efficacy in identifying ROIs.\nThis study presents an automatic system to detect regions of interest relevant\nto the diagnosis of kidney cancer in whole slide histopathology images.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.07313v1",
    "published_date": "2025-04-09 22:28:26 UTC",
    "updated_date": "2025-04-09 22:28:26 UTC"
  },
  {
    "arxiv_id": "2504.07304v1",
    "title": "PAYADOR: A Minimalist Approach to Grounding Language Models on Structured Data for Interactive Storytelling and Role-playing Games",
    "authors": [
      "Santiago Góngora",
      "Luis Chiruzzo",
      "Gonzalo Méndez",
      "Pablo Gervás"
    ],
    "abstract": "Every time an Interactive Storytelling (IS) system gets a player input, it is\nfacing the world-update problem. Classical approaches to this problem consist\nin mapping that input to known preprogrammed actions, what can severely\nconstrain the free will of the player. When the expected experience has a\nstrong focus on improvisation, like in Role-playing Games (RPGs), this problem\nis critical. In this paper we present PAYADOR, a different approach that\nfocuses on predicting the outcomes of the actions instead of representing the\nactions themselves. To implement this approach, we ground a Large Language\nModel to a minimal representation of the fictional world, obtaining promising\nresults. We make this contribution open-source, so it can be adapted and used\nfor other related research on unleashing the co-creativity power of RPGs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Presented at the 15th International Conference on Computational\n  Creativity (ICCC'24)",
    "pdf_url": "http://arxiv.org/pdf/2504.07304v1",
    "published_date": "2025-04-09 21:59:31 UTC",
    "updated_date": "2025-04-09 21:59:31 UTC"
  },
  {
    "arxiv_id": "2504.07303v1",
    "title": "Modeling Response Consistency in Multi-Agent LLM Systems: A Comparative Analysis of Shared and Separate Context Approaches",
    "authors": [
      "Tooraj Helmi"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly utilized in multi-agent systems\n(MAS) to enhance collaborative problem-solving and interactive reasoning.\nRecent advancements have enabled LLMs to function as autonomous agents capable\nof understanding complex interactions across multiple topics. However,\ndeploying LLMs in MAS introduces challenges related to context management,\nresponse consistency, and scalability, especially when agents must operate\nunder memory limitations and handle noisy inputs. While prior research has\nexplored optimizing context sharing and response latency in LLM-driven MAS,\nthese efforts often focus on either fully centralized or decentralized\nconfigurations, each with distinct trade-offs.\n  In this paper, we develop a probabilistic framework to analyze the impact of\nshared versus separate context configurations on response consistency and\nresponse times in LLM-based MAS. We introduce the Response Consistency Index\n(RCI) as a metric to evaluate the effects of context limitations, noise, and\ninter-agent dependencies on system performance. Our approach differs from\nexisting research by focusing on the interplay between memory constraints and\nnoise management, providing insights into optimizing scalability and response\ntimes in environments with interdependent topics. Through this analysis, we\noffer a comprehensive understanding of how different configurations impact the\nefficiency of LLM-driven multi-agent systems, thereby guiding the design of\nmore robust architectures.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.07303v1",
    "published_date": "2025-04-09 21:54:21 UTC",
    "updated_date": "2025-04-09 21:54:21 UTC"
  },
  {
    "arxiv_id": "2504.08006v1",
    "title": "A Python toolkit for dealing with Petri nets over ontological graphs",
    "authors": [
      "Krzysztof Pancerz"
    ],
    "abstract": "We present theoretical rudiments of Petri nets over ontological graphs as\nwell as the designed and implemented Python toolkit for dealing with such nets.\nIn Petri nets over ontological graphs, the domain knowledge is enclosed in a\nform of ontologies. In this way, some valuable knowledge (especially in terms\nof semantic relations) can be added to model reasoning and control processes by\nmeans of Petri nets. In the implemented approach, ontological graphs are\nobtained from ontologies built in accordance with the OWL 2 Web Ontology\nLanguage. The implemented tool enables the users to define the structure and\ndynamics of Petri nets over ontological graphs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "PP-RAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2504.08006v1",
    "published_date": "2025-04-09 21:52:17 UTC",
    "updated_date": "2025-04-09 21:52:17 UTC"
  },
  {
    "arxiv_id": "2504.07278v1",
    "title": "A Multi-Phase Analysis of Blood Culture Stewardship: Machine Learning Prediction, Expert Recommendation Assessment, and LLM Automation",
    "authors": [
      "Fatemeh Amrollahi",
      "Nicholas Marshall",
      "Fateme Nateghi Haredasht",
      "Kameron C Black",
      "Aydin Zahedivash",
      "Manoj V Maddali",
      "Stephen P. Ma",
      "Amy Chang",
      "MD Phar Stanley C Deresinski",
      "Mary Kane Goldstein",
      "Steven M. Asch",
      "Niaz Banaei",
      "Jonathan H Chen"
    ],
    "abstract": "Blood cultures are often over ordered without clear justification, straining\nhealthcare resources and contributing to inappropriate antibiotic use pressures\nworsened by the global shortage. In study of 135483 emergency department (ED)\nblood culture orders, we developed machine learning (ML) models to predict the\nrisk of bacteremia using structured electronic health record (EHR) data and\nprovider notes via a large language model (LLM). The structured models AUC\nimproved from 0.76 to 0.79 with note embeddings and reached 0.81 with added\ndiagnosis codes. Compared to an expert recommendation framework applied by\nhuman reviewers and an LLM-based pipeline, our ML approach offered higher\nspecificity without compromising sensitivity. The recommendation framework\nachieved sensitivity 86%, specificity 57%, while the LLM maintained high\nsensitivity (96%) but over classified negatives, reducing specificity (16%).\nThese findings demonstrate that ML models integrating structured and\nunstructured data can outperform consensus recommendations, enhancing\ndiagnostic stewardship beyond existing standards of care.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 2 figures, 2 tables, conference",
    "pdf_url": "http://arxiv.org/pdf/2504.07278v1",
    "published_date": "2025-04-09 21:12:29 UTC",
    "updated_date": "2025-04-09 21:12:29 UTC"
  },
  {
    "arxiv_id": "2504.07273v1",
    "title": "Evaluating Parameter-Based Training Performance of Neural Networks and Variational Quantum Circuits",
    "authors": [
      "Michael Kölle",
      "Alexander Feist",
      "Jonas Stein",
      "Sebastian Wölckert",
      "Claudia Linnhoff-Popien"
    ],
    "abstract": "In recent years, neural networks (NNs) have driven significant advances in\nmachine learning. However, as tasks grow more complex, NNs often require large\nnumbers of trainable parameters, which increases computational and energy\ndemands. Variational quantum circuits (VQCs) offer a promising alternative:\nthey leverage quantum mechanics to capture intricate relationships and\ntypically need fewer parameters. In this work, we evaluate NNs and VQCs on\nsimple supervised and reinforcement learning tasks, examining models with\ndifferent parameter sizes. We simulate VQCs and execute selected parts of the\ntraining process on real quantum hardware to approximate actual training times.\nOur results show that VQCs can match NNs in performance while using\nsignificantly fewer parameters, despite longer training durations. As quantum\ntechnology and algorithms advance, and VQC architectures improve, we posit that\nVQCs could become advantageous for certain machine learning tasks.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "quant-ph",
    "comment": "Accepted at ICCS 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.07273v1",
    "published_date": "2025-04-09 21:00:41 UTC",
    "updated_date": "2025-04-09 21:00:41 UTC"
  },
  {
    "arxiv_id": "2504.08824v1",
    "title": "ColonScopeX: Leveraging Explainable Expert Systems with Multimodal Data for Improved Early Diagnosis of Colorectal Cancer",
    "authors": [
      "Natalia Sikora",
      "Robert L. Manschke",
      "Alethea M. Tang",
      "Peter Dunstan",
      "Dean A. Harris",
      "Su Yang"
    ],
    "abstract": "Colorectal cancer (CRC) ranks as the second leading cause of cancer-related\ndeaths and the third most prevalent malignant tumour worldwide. Early detection\nof CRC remains problematic due to its non-specific and often embarrassing\nsymptoms, which patients frequently overlook or hesitate to report to\nclinicians. Crucially, the stage at which CRC is diagnosed significantly\nimpacts survivability, with a survival rate of 80-95\\% for Stage I and a stark\ndecline to 10\\% for Stage IV. Unfortunately, in the UK, only 14.4\\% of cases\nare diagnosed at the earliest stage (Stage I).\n  In this study, we propose ColonScopeX, a machine learning framework utilizing\nexplainable AI (XAI) methodologies to enhance the early detection of CRC and\npre-cancerous lesions. Our approach employs a multimodal model that integrates\nsignals from blood sample measurements, processed using the Savitzky-Golay\nalgorithm for fingerprint smoothing, alongside comprehensive patient metadata,\nincluding medication history, comorbidities, age, weight, and BMI. By\nleveraging XAI techniques, we aim to render the model's decision-making process\ntransparent and interpretable, thereby fostering greater trust and\nunderstanding in its predictions. The proposed framework could be utilised as a\ntriage tool or a screening tool of the general population.\n  This research highlights the potential of combining diverse patient data\nsources and explainable machine learning to tackle critical challenges in\nmedical diagnostics.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.HC",
      "stat.AP"
    ],
    "primary_category": "cs.LG",
    "comment": "Published to AAAI-25 Bridge Program",
    "pdf_url": "http://arxiv.org/pdf/2504.08824v1",
    "published_date": "2025-04-09 20:45:11 UTC",
    "updated_date": "2025-04-09 20:45:11 UTC"
  },
  {
    "arxiv_id": "2504.07257v1",
    "title": "Better Decisions through the Right Causal World Model",
    "authors": [
      "Elisabeth Dillies",
      "Quentin Delfosse",
      "Jannis Blüml",
      "Raban Emunds",
      "Florian Peter Busch",
      "Kristian Kersting"
    ],
    "abstract": "Reinforcement learning (RL) agents have shown remarkable performances in\nvarious environments, where they can discover effective policies directly from\nsensory inputs. However, these agents often exploit spurious correlations in\nthe training data, resulting in brittle behaviours that fail to generalize to\nnew or slightly modified environments. To address this, we introduce the Causal\nObject-centric Model Extraction Tool (COMET), a novel algorithm designed to\nlearn the exact interpretable causal world models (CWMs). COMET first extracts\nobject-centric state descriptions from observations and identifies the\nenvironment's internal states related to the depicted objects' properties.\nUsing symbolic regression, it models object-centric transitions and derives\ncausal relationships governing object dynamics. COMET further incorporates\nlarge language models (LLMs) for semantic inference, annotating causal\nvariables to enhance interpretability.\n  By leveraging these capabilities, COMET constructs CWMs that align with the\ntrue causal structure of the environment, enabling agents to focus on\ntask-relevant features. The extracted CWMs mitigate the danger of shortcuts,\npermitting the development of RL systems capable of better planning and\ndecision-making across dynamic scenarios. Our results, validated in Atari\nenvironments such as Pong and Freeway, demonstrate the accuracy and robustness\nof COMET, highlighting its potential to bridge the gap between object-centric\nreasoning and causal inference in reinforcement learning.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "5 pages including references, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.07257v1",
    "published_date": "2025-04-09 20:29:13 UTC",
    "updated_date": "2025-04-09 20:29:13 UTC"
  },
  {
    "arxiv_id": "2504.08823v1",
    "title": "FM-LoRA: Factorized Low-Rank Meta-Prompting for Continual Learning",
    "authors": [
      "Xiaobing Yu",
      "Jin Yang",
      "Xiao Wu",
      "Peijie Qiu",
      "Xiaofeng Liu"
    ],
    "abstract": "How to adapt a pre-trained model continuously for sequential tasks with\ndifferent prediction class labels and domains and finally learn a generalizable\nmodel across diverse tasks is a long-lasting challenge. Continual learning (CL)\nhas emerged as a promising approach to leverage pre-trained models (e.g.,\nTransformers) for sequential tasks. While many existing CL methods\nincrementally store additional learned structures, such as Low-Rank Adaptation\n(LoRA) adapters or prompts and sometimes even preserve features from previous\nsamples to maintain performance. This leads to unsustainable parameter growth\nand escalating storage costs as the number of tasks increases. Moreover,\ncurrent approaches often lack task similarity awareness, which further hinders\nthe models ability to effectively adapt to new tasks without interfering with\npreviously acquired knowledge. To address these challenges, we propose FM-LoRA,\na novel and efficient low-rank adaptation method that integrates both a dynamic\nrank selector (DRS) and dynamic meta-prompting (DMP). This framework allocates\nmodel capacity more effectively across tasks by leveraging a shared low-rank\nsubspace critical for preserving knowledge, thereby avoiding continual\nparameter expansion. Extensive experiments on various CL benchmarks, including\nImageNet-R, CIFAR100, and CUB200 for class-incremental learning (CIL), and\nDomainNet for domain-incremental learning (DIL), with Transformers backbone\ndemonstrate that FM-LoRA effectively mitigates catastrophic forgetting while\ndelivering robust performance across a diverse range of tasks and domains.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "8 Pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.08823v1",
    "published_date": "2025-04-09 19:36:18 UTC",
    "updated_date": "2025-04-09 19:36:18 UTC"
  },
  {
    "arxiv_id": "2504.07245v1",
    "title": "A new training approach for text classification in Mental Health: LatentGLoss",
    "authors": [
      "Korhan Sevinç"
    ],
    "abstract": "This study presents a multi-stage approach to mental health classification by\nleveraging traditional machine learning algorithms, deep learning\narchitectures, and transformer-based models. A novel data set was curated and\nutilized to evaluate the performance of various methods, starting with\nconventional classifiers and advancing through neural networks. To broaden the\narchitectural scope, recurrent neural networks (RNNs) such as LSTM and GRU were\nalso evaluated to explore their effectiveness in modeling sequential patterns\nin the data. Subsequently, transformer models such as BERT were fine-tuned to\nassess the impact of contextual embeddings in this domain. Beyond these\nbaseline evaluations, the core contribution of this study lies in a novel\ntraining strategy involving a dual-model architecture composed of a teacher and\na student network. Unlike standard distillation techniques, this method does\nnot rely on soft label transfer; instead, it facilitates information flow\nthrough both the teacher model's output and its latent representations by\nmodifying the loss function. The experimental results highlight the\neffectiveness of each modeling stage and demonstrate that the proposed loss\nfunction and teacher-student interaction significantly enhance the model's\nlearning capacity in mental health prediction tasks.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 3 Figures, 4 Tables",
    "pdf_url": "http://arxiv.org/pdf/2504.07245v1",
    "published_date": "2025-04-09 19:34:31 UTC",
    "updated_date": "2025-04-09 19:34:31 UTC"
  },
  {
    "arxiv_id": "2504.10509v1",
    "title": "Beyond Reproducibility: Advancing Zero-shot LLM Reranking Efficiency with Setwise Insertion",
    "authors": [
      "Jakub Podolak",
      "Leon Peric",
      "Mina Janicijevic",
      "Roxana Petcu"
    ],
    "abstract": "This study presents a comprehensive reproducibility and extension analysis of\nthe Setwise prompting methodology for zero-shot ranking with Large Language\nModels (LLMs), as proposed by Zhuang et al. We evaluate its effectiveness and\nefficiency compared to traditional Pointwise, Pairwise, and Listwise approaches\nin document ranking tasks. Our reproduction confirms the findings of Zhuang et\nal., highlighting the trade-offs between computational efficiency and ranking\neffectiveness in Setwise methods. Building on these insights, we introduce\nSetwise Insertion, a novel approach that leverages the initial document ranking\nas prior knowledge, reducing unnecessary comparisons and uncertainty by\nfocusing on candidates more likely to improve the ranking results. Experimental\nresults across multiple LLM architectures (Flan-T5, Vicuna, and Llama) show\nthat Setwise Insertion yields a 31% reduction in query time, a 23% reduction in\nmodel inferences, and a slight improvement in reranking effectiveness compared\nto the original Setwise method. These findings highlight the practical\nadvantage of incorporating prior ranking knowledge into Setwise prompting for\nefficient and accurate zero-shot document reranking.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.10509v1",
    "published_date": "2025-04-09 18:44:34 UTC",
    "updated_date": "2025-04-09 18:44:34 UTC"
  },
  {
    "arxiv_id": "2504.07199v2",
    "title": "SemEval-2025 Task 5: LLMs4Subjects -- LLM-based Automated Subject Tagging for a National Technical Library's Open-Access Catalog",
    "authors": [
      "Jennifer D'Souza",
      "Sameer Sadruddin",
      "Holger Israel",
      "Mathias Begoin",
      "Diana Slawig"
    ],
    "abstract": "We present SemEval-2025 Task 5: LLMs4Subjects, a shared task on automated\nsubject tagging for scientific and technical records in English and German\nusing the GND taxonomy. Participants developed LLM-based systems to recommend\ntop-k subjects, evaluated through quantitative metrics (precision, recall,\nF1-score) and qualitative assessments by subject specialists. Results highlight\nthe effectiveness of LLM ensembles, synthetic data generation, and multilingual\nprocessing, offering insights into applying LLMs for digital library\nclassification.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 4 figures, Accepted as SemEval 2025 Task 5 description\n  paper",
    "pdf_url": "http://arxiv.org/pdf/2504.07199v2",
    "published_date": "2025-04-09 18:26:46 UTC",
    "updated_date": "2025-04-11 10:14:39 UTC"
  },
  {
    "arxiv_id": "2504.07198v1",
    "title": "Face-LLaVA: Facial Expression and Attribute Understanding through Instruction Tuning",
    "authors": [
      "Ashutosh Chaubey",
      "Xulang Guan",
      "Mohammad Soleymani"
    ],
    "abstract": "The human face plays a central role in social communication, necessitating\nthe use of performant computer vision tools for human-centered applications. We\npropose Face-LLaVA, a multimodal large language model for face-centered,\nin-context learning, including facial expression and attribute recognition.\nAdditionally, Face-LLaVA is able to generate natural language descriptions that\ncan be used for reasoning. Leveraging existing visual databases, we first\ndeveloped FaceInstruct-1M, a face-centered database for instruction tuning\nMLLMs for face processing. We then developed a novel face-specific visual\nencoder powered by Face-Region Guided Cross-Attention that integrates face\ngeometry with local visual features. We evaluated the proposed method across\nnine different datasets and five different face processing tasks, including\nfacial expression recognition, action unit detection, facial attribute\ndetection, age estimation and deepfake detection. Face-LLaVA achieves superior\nresults compared to existing open-source MLLMs and competitive performance\ncompared to commercial solutions. Our model output also receives a higher\nreasoning rating by GPT under a zero-shot setting across all the tasks. Both\nour dataset and model wil be released at https://face-llava.github.io to\nsupport future advancements in social AI and foundational vision-language\nresearch.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page: https://face-llava.github.io",
    "pdf_url": "http://arxiv.org/pdf/2504.07198v1",
    "published_date": "2025-04-09 18:26:07 UTC",
    "updated_date": "2025-04-09 18:26:07 UTC"
  },
  {
    "arxiv_id": "2504.07174v1",
    "title": "HypoEval: Hypothesis-Guided Evaluation for Natural Language Generation",
    "authors": [
      "Mingxuan Li",
      "Hanchen Li",
      "Chenhao Tan"
    ],
    "abstract": "Large language models (LLMs) have demonstrated great potential for automating\nthe evaluation of natural language generation. Previous frameworks of\nLLM-as-a-judge fall short in two ways: they either use zero-shot setting\nwithout consulting any human input, which leads to low alignment, or fine-tune\nLLMs on labeled data, which requires a non-trivial number of samples. Moreover,\nprevious methods often provide little reasoning behind automated evaluations.\nIn this paper, we propose HypoEval, Hypothesis-guided Evaluation framework,\nwhich first uses a small corpus of human evaluations to generate more detailed\nrubrics for human judgments and then incorporates a checklist-like approach to\ncombine LLM's assigned scores on each decomposed dimension to acquire overall\nscores. With only 30 human evaluations, HypoEval achieves state-of-the-art\nperformance in alignment with both human rankings (Spearman correlation) and\nhuman scores (Pearson correlation), on average outperforming G-Eval by 11.86%\nand fine-tuned Llama-3.1-8B-Instruct with at least 3 times more human\nevaluations by 11.95%. Furthermore, we conduct systematic studies to assess the\nrobustness of HypoEval, highlighting its effectiveness as a reliable and\ninterpretable automated evaluation framework.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "22 pages, 3 figures, code link:\n  https://github.com/ChicagoHAI/HypoEval-Gen",
    "pdf_url": "http://arxiv.org/pdf/2504.07174v1",
    "published_date": "2025-04-09 18:00:01 UTC",
    "updated_date": "2025-04-09 18:00:01 UTC"
  },
  {
    "arxiv_id": "2504.07170v1",
    "title": "Trustworthy AI Must Account for Intersectionality",
    "authors": [
      "Jesse C. Cresswell"
    ],
    "abstract": "Trustworthy AI encompasses many aspirational aspects for aligning AI systems\nwith human values, including fairness, privacy, robustness, explainability, and\nuncertainty quantification. However, efforts to enhance one aspect often\nintroduce unintended trade-offs that negatively impact others, making it\nchallenging to improve all aspects simultaneously. In this position paper, we\nreview notable approaches to these five aspects and systematically consider\nevery pair, detailing the negative interactions that can arise. For example,\napplying differential privacy to model training can amplify biases in the data,\nundermining fairness. Drawing on these findings, we take the position that\naddressing trustworthiness along each axis in isolation is insufficient.\nInstead, research on Trustworthy AI must account for intersectionality between\naspects and adopt a holistic view across all relevant axes at once. To\nillustrate our perspective, we provide guidance on how researchers can work\ntowards integrated trustworthiness, a case study on how intersectionality\napplies to the financial industry, and alternative views to our position.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Presented at the ICLR 2025 Workshop on Bidirectional Human-AI\n  Alignment",
    "pdf_url": "http://arxiv.org/pdf/2504.07170v1",
    "published_date": "2025-04-09 18:00:00 UTC",
    "updated_date": "2025-04-09 18:00:00 UTC"
  },
  {
    "arxiv_id": "2504.07097v1",
    "title": "Sculpting Subspaces: Constrained Full Fine-Tuning in LLMs for Continual Learning",
    "authors": [
      "Nikhil Shivakumar Nayak",
      "Krishnateja Killamsetty",
      "Ligong Han",
      "Abhishek Bhandwaldar",
      "Prateek Chanda",
      "Kai Xu",
      "Hao Wang",
      "Aldo Pareja",
      "Oleg Silkin",
      "Mustafa Eyceoz",
      "Akash Srivastava"
    ],
    "abstract": "Continual learning in large language models (LLMs) is prone to catastrophic\nforgetting, where adapting to new tasks significantly degrades performance on\npreviously learned ones. Existing methods typically rely on low-rank,\nparameter-efficient updates that limit the model's expressivity and introduce\nadditional parameters per task, leading to scalability issues. To address these\nlimitations, we propose a novel continual full fine-tuning approach leveraging\nadaptive singular value decomposition (SVD). Our method dynamically identifies\ntask-specific low-rank parameter subspaces and constrains updates to be\northogonal to critical directions associated with prior tasks, thus effectively\nminimizing interference without additional parameter overhead or storing\nprevious task gradients. We evaluate our approach extensively on standard\ncontinual learning benchmarks using both encoder-decoder (T5-Large) and\ndecoder-only (LLaMA-2 7B) models, spanning diverse tasks including\nclassification, generation, and reasoning. Empirically, our method achieves\nstate-of-the-art results, up to 7% higher average accuracy than recent\nbaselines like O-LoRA, and notably maintains the model's general linguistic\ncapabilities, instruction-following accuracy, and safety throughout the\ncontinual learning process by reducing forgetting to near-negligible levels.\nOur adaptive SVD framework effectively balances model plasticity and knowledge\nretention, providing a practical, theoretically grounded, and computationally\nscalable solution for continual learning scenarios in large language models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "math.PR",
      "stat.ML",
      "68T50",
      "I.2.0; G.3"
    ],
    "primary_category": "cs.LG",
    "comment": "25 pages, 13 figures, 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2504.07097v1",
    "published_date": "2025-04-09 17:59:42 UTC",
    "updated_date": "2025-04-09 17:59:42 UTC"
  },
  {
    "arxiv_id": "2504.07092v2",
    "title": "Are We Done with Object-Centric Learning?",
    "authors": [
      "Alexander Rubinstein",
      "Ameya Prabhu",
      "Matthias Bethge",
      "Seong Joon Oh"
    ],
    "abstract": "Object-centric learning (OCL) seeks to learn representations that only encode\nan object, isolated from other objects or background cues in a scene. This\napproach underpins various aims, including out-of-distribution (OOD)\ngeneralization, sample-efficient composition, and modeling of structured\nenvironments. Most research has focused on developing unsupervised mechanisms\nthat separate objects into discrete slots in the representation space,\nevaluated using unsupervised object discovery. However, with recent\nsample-efficient segmentation models, we can separate objects in the pixel\nspace and encode them independently. This achieves remarkable zero-shot\nperformance on OOD object discovery benchmarks, is scalable to foundation\nmodels, and can handle a variable number of slots out-of-the-box. Hence, the\ngoal of OCL methods to obtain object-centric representations has been largely\nachieved. Despite this progress, a key question remains: How does the ability\nto separate objects within a scene contribute to broader OCL objectives, such\nas OOD generalization? We address this by investigating the OOD generalization\nchallenge caused by spurious background cues through the lens of OCL. We\npropose a novel, training-free probe called Object-Centric Classification with\nApplied Masks (OCCAM), demonstrating that segmentation-based encoding of\nindividual objects significantly outperforms slot-based OCL methods. However,\nchallenges in real-world applications remain. We provide the toolbox for the\nOCL community to use scalable object-centric representations, and focus on\npractical applications and fundamental questions, such as understanding object\nperception in human cognition. Our code is available here:\nhttps://github.com/AlexanderRubinstein/OCCAM.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.07092v2",
    "published_date": "2025-04-09 17:59:05 UTC",
    "updated_date": "2025-04-10 21:45:00 UTC"
  },
  {
    "arxiv_id": "2504.07091v1",
    "title": "AssistanceZero: Scalably Solving Assistance Games",
    "authors": [
      "Cassidy Laidlaw",
      "Eli Bronstein",
      "Timothy Guo",
      "Dylan Feng",
      "Lukas Berglund",
      "Justin Svegliato",
      "Stuart Russell",
      "Anca Dragan"
    ],
    "abstract": "Assistance games are a promising alternative to reinforcement learning from\nhuman feedback (RLHF) for training AI assistants. Assistance games resolve key\ndrawbacks of RLHF, such as incentives for deceptive behavior, by explicitly\nmodeling the interaction between assistant and user as a two-player game where\nthe assistant cannot observe their shared goal. Despite their potential,\nassistance games have only been explored in simple settings. Scaling them to\nmore complex environments is difficult because it requires both solving\nintractable decision-making problems under uncertainty and accurately modeling\nhuman users' behavior. We present the first scalable approach to solving\nassistance games and apply it to a new, challenging Minecraft-based assistance\ngame with over $10^{400}$ possible goals. Our approach, AssistanceZero, extends\nAlphaZero with a neural network that predicts human actions and rewards,\nenabling it to plan under uncertainty. We show that AssistanceZero outperforms\nmodel-free RL algorithms and imitation learning in the Minecraft-based\nassistance game. In a human study, our AssistanceZero-trained assistant\nsignificantly reduces the number of actions participants take to complete\nbuilding tasks in Minecraft. Our results suggest that assistance games are a\ntractable framework for training effective AI assistants in complex\nenvironments. Our code and models are available at\nhttps://github.com/cassidylaidlaw/minecraft-building-assistance-game.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.07091v1",
    "published_date": "2025-04-09 17:59:03 UTC",
    "updated_date": "2025-04-09 17:59:03 UTC"
  },
  {
    "arxiv_id": "2504.07087v1",
    "title": "KG-LLM-Bench: A Scalable Benchmark for Evaluating LLM Reasoning on Textualized Knowledge Graphs",
    "authors": [
      "Elan Markowitz",
      "Krupa Galiya",
      "Greg Ver Steeg",
      "Aram Galstyan"
    ],
    "abstract": "Knowledge graphs have emerged as a popular method for injecting up-to-date,\nfactual knowledge into large language models (LLMs). This is typically achieved\nby converting the knowledge graph into text that the LLM can process in\ncontext. While multiple methods of encoding knowledge graphs have been\nproposed, the impact of this textualization process on LLM performance remains\nunder-explored. We introduce KG-LLM-Bench, a comprehensive and extensible\nbenchmark spanning five knowledge graph understanding tasks, and evaluate how\ndifferent encoding strategies affect performance across various base models.\nOur extensive experiments with seven language models and five textualization\nstrategies provide insights for optimizing LLM performance on KG reasoning\ntasks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "To be presented at NAACL-HLT, KnowledgeNLP Workshop (2025)",
    "pdf_url": "http://arxiv.org/pdf/2504.07087v1",
    "published_date": "2025-04-09 17:58:47 UTC",
    "updated_date": "2025-04-09 17:58:47 UTC"
  },
  {
    "arxiv_id": "2504.07081v1",
    "title": "Self-Steering Language Models",
    "authors": [
      "Gabriel Grand",
      "Joshua B. Tenenbaum",
      "Vikash K. Mansinghka",
      "Alexander K. Lew",
      "Jacob Andreas"
    ],
    "abstract": "While test-time reasoning enables language models to tackle complex tasks,\nsearching or planning in natural language can be slow, costly, and error-prone.\nBut even when LMs struggle to emulate the precise reasoning steps needed to\nsolve a problem, they often excel at describing its abstract structure--both\nhow to verify solutions and how to search for them. This paper introduces\nDisCIPL, a method for \"self-steering\" LMs where a Planner model generates a\ntask-specific inference program that is executed by a population of Follower\nmodels. Our approach equips LMs with the ability to write recursive search\nprocedures that guide LM inference, enabling new forms of verifiable and\nefficient reasoning. When instantiated with a small Follower (e.g.,\nLlama-3.2-1B), DisCIPL matches (and sometimes outperforms) much larger models,\nincluding GPT-4o and o1, on challenging constrained generation tasks. In\ndecoupling planning from execution, our work opens up a design space of\nhighly-parallelized Monte Carlo inference strategies that outperform standard\nbest-of-N sampling, require no finetuning, and can be implemented automatically\nby existing LMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.07081v1",
    "published_date": "2025-04-09 17:54:22 UTC",
    "updated_date": "2025-04-09 17:54:22 UTC"
  },
  {
    "arxiv_id": "2504.10508v1",
    "title": "Poly-Vector Retrieval: Reference and Content Embeddings for Legal Documents",
    "authors": [
      "João Alberto de Oliveira Lima"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) has emerged as an effective paradigm for\ngenerating contextually accurate answers by integrating Large Language Models\n(LLMs) with retrieval mechanisms. However, in legal contexts, users frequently\nreference norms by their labels or nicknames (e.g., Article 5 of the\nConstitution or Consumer Defense Code (CDC)), rather than by their content,\nposing challenges for traditional RAG approaches that rely solely on semantic\nembeddings of text. Furthermore, legal texts themselves heavily rely on\nexplicit cross-references (e.g., \"pursuant to Article 34\") that function as\npointers. Both scenarios pose challenges for traditional RAG approaches that\nrely solely on semantic embeddings of text, often failing to retrieve the\nnecessary referenced content. This paper introduces Poly-Vector Retrieval, a\nmethod assigning multiple distinct embeddings to each legal provision: one\nembedding captures the content (the full text), another captures the label (the\nidentifier or proper name), and optionally additional embeddings capture\nalternative denominations. Inspired by Frege's distinction between Sense and\nReference, this poly-vector retrieval approach treats labels, identifiers and\nreference markers as rigid designators and content embeddings as carriers of\nsemantic substance. Experiments on the Brazilian Federal Constitution\ndemonstrate that Poly-Vector Retrieval significantly improves retrieval\naccuracy for label-centric queries and potential to resolve internal and\nexternal cross-references, without compromising performance on purely semantic\nqueries. The study discusses philosophical and practical implications of\nexplicitly separating reference from content in vector embeddings and proposes\nfuture research directions for applying this approach to broader legal datasets\nand other domains characterized by explicit reference identifiers.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "I.2.8"
    ],
    "primary_category": "cs.IR",
    "comment": "39 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.10508v1",
    "published_date": "2025-04-09 17:54:11 UTC",
    "updated_date": "2025-04-09 17:54:11 UTC"
  },
  {
    "arxiv_id": "2504.07080v1",
    "title": "DeduCE: Deductive Consistency as a Framework to Evaluate LLM Reasoning",
    "authors": [
      "Atharva Pandey",
      "Kshitij Dubey",
      "Rahul Sharma",
      "Amit Sharma"
    ],
    "abstract": "Despite great performance on Olympiad-level reasoning problems, frontier\nlarge language models can still struggle on high school math when presented\nwith novel problems outside standard benchmarks. Going beyond final accuracy,\nwe propose a deductive consistency metric to analyze chain-of-thought output\nfrom language models (LMs).Formally, deductive reasoning involves two subtasks:\nunderstanding a set of input premises and inferring the conclusions that follow\nfrom them. The proposed metric studies LMs' performance on these subtasks, with\nthe goal of explaining LMs' reasoning errors on novel problems: how well do LMs\nunderstand input premises with increasing context lengths, and how well can\nthey infer conclusions over multiple reasoning hops? Since existing benchmarks\nmay be memorized, we develop a pipeline to evaluate LMs' deductive consistency\non novel, perturbed versions of benchmark problems. On novel grade school math\nproblems (GSM-8k), we find that LMs are fairly robust to increasing number of\ninput premises, but suffer significant accuracy decay as the number of\nreasoning hops is increased. Interestingly, these errors are masked in the\noriginal benchmark as all models achieve near 100% accuracy. As we increase the\nnumber of solution steps using a synthetic dataset, prediction over multiple\nhops still remains the major source of error compared to understanding input\npremises. Other factors, such as shifts in language style or natural\npropagation of early errors do not explain the trends. Our analysis provides a\nnew view to characterize LM reasoning -- as computations over a window of input\npremises and reasoning hops -- that can provide unified evaluation across\nproblem domains.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.07080v1",
    "published_date": "2025-04-09 17:53:55 UTC",
    "updated_date": "2025-04-09 17:53:55 UTC"
  },
  {
    "arxiv_id": "2504.07079v1",
    "title": "SkillWeaver: Web Agents can Self-Improve by Discovering and Honing Skills",
    "authors": [
      "Boyuan Zheng",
      "Michael Y. Fatemi",
      "Xiaolong Jin",
      "Zora Zhiruo Wang",
      "Apurva Gandhi",
      "Yueqi Song",
      "Yu Gu",
      "Jayanth Srinivasa",
      "Gaowen Liu",
      "Graham Neubig",
      "Yu Su"
    ],
    "abstract": "To survive and thrive in complex environments, humans have evolved\nsophisticated self-improvement mechanisms through environment exploration,\nhierarchical abstraction of experiences into reuseable skills, and\ncollaborative construction of an ever-growing skill repertoire. Despite recent\nadvancements, autonomous web agents still lack crucial self-improvement\ncapabilities, struggling with procedural knowledge abstraction, refining\nskills, and skill composition. In this work, we introduce SkillWeaver, a\nskill-centric framework enabling agents to self-improve by autonomously\nsynthesizing reusable skills as APIs. Given a new website, the agent\nautonomously discovers skills, executes them for practice, and distills\npractice experiences into robust APIs. Iterative exploration continually\nexpands a library of lightweight, plug-and-play APIs, significantly enhancing\nthe agent's capabilities. Experiments on WebArena and real-world websites\ndemonstrate the efficacy of SkillWeaver, achieving relative success rate\nimprovements of 31.8% and 39.8%, respectively. Additionally, APIs synthesized\nby strong agents substantially enhance weaker agents through transferable\nskills, yielding improvements of up to 54.3% on WebArena. These results\ndemonstrate the effectiveness of honing diverse website interactions into APIs,\nwhich can be seamlessly shared among various web agents.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.07079v1",
    "published_date": "2025-04-09 17:51:50 UTC",
    "updated_date": "2025-04-09 17:51:50 UTC"
  },
  {
    "arxiv_id": "2504.07069v1",
    "title": "HalluciNot: Hallucination Detection Through Context and Common Knowledge Verification",
    "authors": [
      "Bibek Paudel",
      "Alexander Lyzhov",
      "Preetam Joshi",
      "Puneet Anand"
    ],
    "abstract": "This paper introduces a comprehensive system for detecting hallucinations in\nlarge language model (LLM) outputs in enterprise settings. We present a novel\ntaxonomy of LLM responses specific to hallucination in enterprise applications,\ncategorizing them into context-based, common knowledge, enterprise-specific,\nand innocuous statements. Our hallucination detection model HDM-2 validates LLM\nresponses with respect to both context and generally known facts (common\nknowledge). It provides both hallucination scores and word-level annotations,\nenabling precise identification of problematic content. To evaluate it on\ncontext-based and common-knowledge hallucinations, we introduce a new dataset\nHDMBench. Experimental results demonstrate that HDM-2 out-performs existing\napproaches across RagTruth, TruthfulQA, and HDMBench datasets. This work\naddresses the specific challenges of enterprise deployment, including\ncomputational efficiency, domain specialization, and fine-grained error\nidentification. Our evaluation dataset, model weights, and inference code are\npublicly available.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.07069v1",
    "published_date": "2025-04-09 17:39:41 UTC",
    "updated_date": "2025-04-09 17:39:41 UTC"
  },
  {
    "arxiv_id": "2504.07055v1",
    "title": "$Π$-NeSy: A Possibilistic Neuro-Symbolic Approach",
    "authors": [
      "Ismaïl Baaj",
      "Pierre Marquis"
    ],
    "abstract": "In this article, we introduce a neuro-symbolic approach that combines a\nlow-level perception task performed by a neural network with a high-level\nreasoning task performed by a possibilistic rule-based system. The goal is to\nbe able to derive for each input instance the degree of possibility that it\nbelongs to a target (meta-)concept. This (meta-)concept is connected to\nintermediate concepts by a possibilistic rule-based system. The probability of\neach intermediate concept for the input instance is inferred using a neural\nnetwork. The connection between the low-level perception task and the\nhigh-level reasoning task lies in the transformation of neural network outputs\nmodeled by probability distributions (through softmax activation) into\npossibility distributions. The use of intermediate concepts is valuable for the\nexplanation purpose: using the rule-based system, the classification of an\ninput instance as an element of the (meta-)concept can be justified by the fact\nthat intermediate concepts have been recognized.\n  From the technical side, our contribution consists of the design of efficient\nmethods for defining the matrix relation and the equation system associated\nwith a possibilistic rule-based system. The corresponding matrix and equation\nare key data structures used to perform inferences from a possibilistic\nrule-based system and to learn the values of the rule parameters in such a\nsystem according to a training data sample. Furthermore, leveraging recent\nresults on the handling of inconsistent systems of fuzzy relational equations,\nan approach for learning rule parameters according to multiple training data\nsamples is presented. Experiments carried out on the MNIST addition problems\nand the MNIST Sudoku puzzles problems highlight the effectiveness of our\napproach compared with state-of-the-art neuro-symbolic ones.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.07055v1",
    "published_date": "2025-04-09 17:16:23 UTC",
    "updated_date": "2025-04-09 17:16:23 UTC"
  },
  {
    "arxiv_id": "2504.06994v1",
    "title": "RayFronts: Open-Set Semantic Ray Frontiers for Online Scene Understanding and Exploration",
    "authors": [
      "Omar Alama",
      "Avigyan Bhattacharya",
      "Haoyang He",
      "Seungchan Kim",
      "Yuheng Qiu",
      "Wenshan Wang",
      "Cherie Ho",
      "Nikhil Keetha",
      "Sebastian Scherer"
    ],
    "abstract": "Open-set semantic mapping is crucial for open-world robots. Current mapping\napproaches either are limited by the depth range or only map beyond-range\nentities in constrained settings, where overall they fail to combine\nwithin-range and beyond-range observations. Furthermore, these methods make a\ntrade-off between fine-grained semantics and efficiency. We introduce\nRayFronts, a unified representation that enables both dense and beyond-range\nefficient semantic mapping. RayFronts encodes task-agnostic open-set semantics\nto both in-range voxels and beyond-range rays encoded at map boundaries,\nempowering the robot to reduce search volumes significantly and make informed\ndecisions both within & beyond sensory range, while running at 8.84 Hz on an\nOrin AGX. Benchmarking the within-range semantics shows that RayFronts's\nfine-grained image encoding provides 1.34x zero-shot 3D semantic segmentation\nperformance while improving throughput by 16.5x. Traditionally, online mapping\nperformance is entangled with other system components, complicating evaluation.\nWe propose a planner-agnostic evaluation framework that captures the utility\nfor online beyond-range search and exploration, and show RayFronts reduces\nsearch volume 2.2x more efficiently than the closest online baselines.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.06994v1",
    "published_date": "2025-04-09 16:06:58 UTC",
    "updated_date": "2025-04-09 16:06:58 UTC"
  },
  {
    "arxiv_id": "2504.06987v2",
    "title": "Enhancing Metabolic Syndrome Prediction with Hybrid Data Balancing and Counterfactuals",
    "authors": [
      "Sanyam Paresh Shah",
      "Abdullah Mamun",
      "Shovito Barua Soumma",
      "Hassan Ghasemzadeh"
    ],
    "abstract": "Metabolic Syndrome (MetS) is a cluster of interrelated risk factors that\nsignificantly increases the risk of cardiovascular diseases and type 2\ndiabetes. Despite its global prevalence, accurate prediction of MetS remains\nchallenging due to issues such as class imbalance, data scarcity, and\nmethodological inconsistencies in existing studies. In this paper, we address\nthese challenges by systematically evaluating and optimizing machine learning\n(ML) models for MetS prediction, leveraging advanced data balancing techniques\nand counterfactual analysis. Multiple ML models, including XGBoost, Random\nForest, TabNet, etc., were trained and compared under various data balancing\ntechniques such as random oversampling (ROS), SMOTE, ADASYN, and CTGAN.\nAdditionally, we introduce MetaBoost, a novel hybrid framework that integrates\nSMOTE, ADASYN, and CTGAN, optimizing synthetic data generation through weighted\naveraging and iterative weight tuning to enhance the model's performance\n(achieving up to a 1.87% accuracy improvement over individual balancing\ntechniques). A comprehensive counterfactual analysis is conducted to quantify\nthe feature-level changes required to shift individuals from high-risk to\nlow-risk categories. The results indicate that blood glucose (50.3%) and\ntriglycerides (46.7%) were the most frequently modified features, highlighting\ntheir clinical significance in MetS risk reduction. Additionally, probabilistic\nanalysis shows elevated blood glucose (85.5% likelihood) and triglycerides\n(74.9% posterior probability) as the strongest predictors. This study not only\nadvances the methodological rigor of MetS prediction but also provides\nactionable insights for clinicians and researchers, highlighting the potential\nof ML in mitigating the public health burden of metabolic syndrome.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at the IEEE EMBC 2025 Conference. 7 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.06987v2",
    "published_date": "2025-04-09 15:51:10 UTC",
    "updated_date": "2025-05-07 11:50:43 UTC"
  },
  {
    "arxiv_id": "2504.06963v1",
    "title": "RNN-Transducer-based Losses for Speech Recognition on Noisy Targets",
    "authors": [
      "Vladimir Bataev"
    ],
    "abstract": "Training speech recognition systems on noisy transcripts is a significant\nchallenge in industrial pipelines, where datasets are enormous and ensuring\naccurate transcription for every instance is difficult. In this work, we\nintroduce novel loss functions to mitigate the impact of transcription errors\nin RNN-Transducer models. Our Star-Transducer loss addresses deletion errors by\nincorporating \"skip frame\" transitions in the loss lattice, restoring over 90%\nof the system's performance compared to models trained with accurate\ntranscripts. The Bypass-Transducer loss uses \"skip token\" transitions to tackle\ninsertion errors, recovering more than 60% of the quality. Finally, the\nTarget-Robust Transducer loss merges these approaches, offering robust\nperformance against arbitrary errors. Experimental results demonstrate that the\nTarget-Robust Transducer loss significantly improves RNN-T performance on noisy\ndata by restoring over 70% of the quality compared to well-transcribed data.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "Final Project Report, Bachelor's Degree in Computer Science,\n  University of London, March 2024",
    "pdf_url": "http://arxiv.org/pdf/2504.06963v1",
    "published_date": "2025-04-09 15:18:29 UTC",
    "updated_date": "2025-04-09 15:18:29 UTC"
  },
  {
    "arxiv_id": "2504.06962v2",
    "title": "Efficient Self-Supervised Learning for Earth Observation via Dynamic Dataset Curation",
    "authors": [
      "Thomas Kerdreux",
      "Alexandre Tuel",
      "Quentin Febvre",
      "Alexis Mouche",
      "Bertrand Chapron"
    ],
    "abstract": "Self-supervised learning (SSL) has enabled the development of vision\nfoundation models for Earth Observation (EO), demonstrating strong\ntransferability across diverse remote sensing tasks. While prior work has\nfocused on network architectures and training strategies, the role of dataset\ncuration, especially in balancing and diversifying pre-training datasets,\nremains underexplored. In EO, this challenge is amplified by the redundancy and\nheavy-tailed distributions common in satellite imagery, which can lead to\nbiased representations and inefficient training.\n  In this work, we propose a dynamic dataset pruning strategy designed to\nimprove SSL pre-training by maximizing dataset diversity and balance. Our\nmethod iteratively refines the training set without requiring a pre-existing\nfeature extractor, making it well-suited for domains where curated datasets are\nlimited or unavailable. We demonstrate our approach on the Sentinel-1 Wave Mode\n(WV) Synthetic Aperture Radar (SAR) archive, a challenging dataset dominated by\nocean observations. We train models from scratch on the entire Sentinel-1 WV\narchive spanning 10 years. Across three downstream tasks, our results show that\ndynamic pruning improves both computational efficiency and representation\nquality, leading to stronger transferability.\n  We also release the weights of OceanSAR-1, the first model in the OceanSAR\nfamily, a series of foundation models for ocean observation and analysis using\nSAR imagery, at github.com/galeio-research/OceanSAR-models/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at CVPR Workshop : The First Workshop on Foundation and\n  Large Vision Models in Remote Sensing",
    "pdf_url": "http://arxiv.org/pdf/2504.06962v2",
    "published_date": "2025-04-09 15:13:26 UTC",
    "updated_date": "2025-04-28 15:32:50 UTC"
  },
  {
    "arxiv_id": "2504.06949v1",
    "title": "Adaptive Computation Pruning for the Forgetting Transformer",
    "authors": [
      "Zhixuan Lin",
      "Johan Obando-Ceron",
      "Xu Owen He",
      "Aaron Courville"
    ],
    "abstract": "The recently proposed Forgetting Transformer (FoX) incorporates a forget gate\ninto softmax attention and has shown consistently better or on-par performance\ncompared to the standard RoPE-based Transformer. Notably, many attention heads\nin FoX tend to forget quickly, causing their output at each timestep to rely\nprimarily on the local context. Based on this observation, we propose Adaptive\nComputation Pruning (ACP) for FoX, a method that dynamically prunes\ncomputations involving input-output dependencies that are strongly decayed by\nthe forget gate. This is achieved using a dynamically set pruning threshold\nthat ensures that the pruned attention weights remain negligible. We apply ACP\nto language model pretraining with FoX and show it consistently reduces the\nnumber of FLOPs in softmax attention by around 70% across different model sizes\nand context lengths, resulting in a roughly 10% to 35% improvement in training\nthroughput. Furthermore, longer context lengths yield greater computational\nsavings. All these speed improvements are achieved without any performance\ndegradation. We also perform several analyses to provide deeper insights into\nour method, such as examining the pruning patterns and analyzing the\ndistribution of FLOP savings across different attention heads. Our code is\navailable at https://github.com/zhixuan-lin/arctic-fox.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint. Under review",
    "pdf_url": "http://arxiv.org/pdf/2504.06949v1",
    "published_date": "2025-04-09 14:57:55 UTC",
    "updated_date": "2025-04-09 14:57:55 UTC"
  },
  {
    "arxiv_id": "2504.06943v2",
    "title": "Review of Case-Based Reasoning for LLM Agents: Theoretical Foundations, Architectural Components, and Cognitive Integration",
    "authors": [
      "Kostas Hatalis",
      "Despina Christou",
      "Vyshnavi Kondapalli"
    ],
    "abstract": "Agents powered by Large Language Models (LLMs) have recently demonstrated\nimpressive capabilities in various tasks. Still, they face limitations in tasks\nrequiring specific, structured knowledge, flexibility, or accountable\ndecision-making. While agents are capable of perceiving their environments,\nforming inferences, planning, and executing actions towards goals, they often\nface issues such as hallucinations and lack of contextual memory across\ninteractions. This paper explores how Case-Based Reasoning (CBR), a strategy\nthat solves new problems by referencing past experiences, can be integrated\ninto LLM agent frameworks. This integration allows LLMs to leverage explicit\nknowledge, enhancing their effectiveness. We systematically review the\ntheoretical foundations of these enhanced agents, identify critical framework\ncomponents, and formulate a mathematical model for the CBR processes of case\nretrieval, adaptation, and learning. We also evaluate CBR-enhanced agents\nagainst other methods like Chain-of-Thought reasoning and standard\nRetrieval-Augmented Generation, analyzing their relative strengths. Moreover,\nwe explore how leveraging CBR's cognitive dimensions (including\nself-reflection, introspection, and curiosity) via goal-driven autonomy\nmechanisms can further enhance the LLM agent capabilities. Contributing to the\nongoing research on neuro-symbolic hybrid systems, this work posits CBR as a\nviable technique for enhancing the reasoning skills and cognitive aspects of\nautonomous LLM agents.",
    "categories": [
      "cs.AI",
      "cs.MA",
      "68",
      "I.2; I.2.7"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.06943v2",
    "published_date": "2025-04-09 14:51:02 UTC",
    "updated_date": "2025-04-11 05:34:20 UTC"
  },
  {
    "arxiv_id": "2504.06928v1",
    "title": "Beyond Tools: Generative AI as Epistemic Infrastructure in Education",
    "authors": [
      "Bodong Chen"
    ],
    "abstract": "As generative AI rapidly integrates into educational infrastructures\nworldwide, it transforms how knowledge gets created, validated, and shared, yet\ncurrent discourse inadequately addresses its implications as epistemic\ninfrastructure mediating teaching and learning. This paper investigates how AI\nsystems function as epistemic infrastructures in education and their impact on\nhuman epistemic agency. Adopting a situated cognition perspective and following\na value-sensitive design approach, the study conducts a technical investigation\nof two representative AI systems in educational settings, analyzing their\nimpact on teacher practice across three dimensions: affordances for skilled\nepistemic actions, support for epistemic sensitivity, and implications for\nlong-term habit formation. The analysis reveals that current AI systems\ninadequately support teachers' skilled epistemic actions, insufficiently foster\nepistemic sensitivity, and potentially cultivate problematic habits that\nprioritize efficiency over epistemic agency. To address these challenges, the\npaper recommends recognizing the infrastructural transformation occurring in\neducation, developing AI environments that stimulate skilled actions while\nupholding epistemic norms, and involving educators in AI design processes --\nrecommendations aimed at fostering AI integration that aligns with core\neducational values and maintains human epistemic agency.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "K.3.1; K.4.3; H.5.2"
    ],
    "primary_category": "cs.CY",
    "comment": "23 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.06928v1",
    "published_date": "2025-04-09 14:35:30 UTC",
    "updated_date": "2025-04-09 14:35:30 UTC"
  },
  {
    "arxiv_id": "2504.06925v1",
    "title": "Are Vision-Language Models Ready for Dietary Assessment? Exploring the Next Frontier in AI-Powered Food Image Recognition",
    "authors": [
      "Sergio Romero-Tapiador",
      "Ruben Tolosana",
      "Blanca Lacruz-Pleguezuelos",
      "Laura Judith Marcos Zambrano",
      "Guadalupe X. Bazán",
      "Isabel Espinosa-Salinas",
      "Julian Fierrez",
      "Javier Ortega-Garcia",
      "Enrique Carrillo de Santa Pau",
      "Aythami Morales"
    ],
    "abstract": "Automatic dietary assessment based on food images remains a challenge,\nrequiring precise food detection, segmentation, and classification.\nVision-Language Models (VLMs) offer new possibilities by integrating visual and\ntextual reasoning. In this study, we evaluate six state-of-the-art VLMs\n(ChatGPT, Gemini, Claude, Moondream, DeepSeek, and LLaVA), analyzing their\ncapabilities in food recognition at different levels. For the experimental\nframework, we introduce the FoodNExTDB, a unique food image database that\ncontains 9,263 expert-labeled images across 10 categories (e.g., \"protein\nsource\"), 62 subcategories (e.g., \"poultry\"), and 9 cooking styles (e.g.,\n\"grilled\"). In total, FoodNExTDB includes 50k nutritional labels generated by\nseven experts who manually annotated all images in the database. Also, we\npropose a novel evaluation metric, Expert-Weighted Recall (EWR), that accounts\nfor the inter-annotator variability. Results show that closed-source models\noutperform open-source ones, achieving over 90% EWR in recognizing food\nproducts in images containing a single product. Despite their potential,\ncurrent VLMs face challenges in fine-grained food recognition, particularly in\ndistinguishing subtle differences in cooking styles and visually similar food\nitems, which limits their reliability for automatic dietary assessment. The\nFoodNExTDB database is publicly available at\nhttps://github.com/AI4Food/FoodNExtDB.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at IEEE/CVF Computer Vision and Pattern Recognition\n  Conference workshops 2025 (CVPRw) 10 pages, 4 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2504.06925v1",
    "published_date": "2025-04-09 14:33:59 UTC",
    "updated_date": "2025-04-09 14:33:59 UTC"
  },
  {
    "arxiv_id": "2504.06924v1",
    "title": "Longitudinal Assessment of Lung Lesion Burden in CT",
    "authors": [
      "Tejas Sudharshan Mathai",
      "Benjamin Hou",
      "Ronald M. Summers"
    ],
    "abstract": "In the U.S., lung cancer is the second major cause of death. Early detection\nof suspicious lung nodules is crucial for patient treatment planning,\nmanagement, and improving outcomes. Many approaches for lung nodule\nsegmentation and volumetric analysis have been proposed, but few have looked at\nlongitudinal changes in total lung tumor burden. In this work, we trained two\n3D models (nnUNet) with and without anatomical priors to automatically segment\nlung lesions and quantified total lesion burden for each patient. The 3D model\nwithout priors significantly outperformed ($p < .001$) the model trained with\nanatomy priors. For detecting clinically significant lesions $>$ 1cm, a\nprecision of 71.3\\%, sensitivity of 68.4\\%, and F1-score of 69.8\\% was\nachieved. For segmentation, a Dice score of 77.1 $\\pm$ 20.3 and Hausdorff\ndistance error of 11.7 $\\pm$ 24.1 mm was obtained. The median lesion burden was\n6.4 cc (IQR: 2.1, 18.1) and the median volume difference between manual and\nautomated measurements was 0.02 cc (IQR: -2.8, 1.2). Agreements were also\nevaluated with linear regression and Bland-Altman plots. The proposed approach\ncan produce a personalized evaluation of the total tumor burden for a patient\nand facilitate interval change tracking over time.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "Published at SPIE Medical Imaging 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.06924v1",
    "published_date": "2025-04-09 14:30:43 UTC",
    "updated_date": "2025-04-09 14:30:43 UTC"
  },
  {
    "arxiv_id": "2504.06921v1",
    "title": "Leveraging Anatomical Priors for Automated Pancreas Segmentation on Abdominal CT",
    "authors": [
      "Anisa V. Prasad",
      "Tejas Sudharshan Mathai",
      "Pritam Mukherjee",
      "Jianfei Liu",
      "Ronald M. Summers"
    ],
    "abstract": "An accurate segmentation of the pancreas on CT is crucial to identify\npancreatic pathologies and extract imaging-based biomarkers. However, prior\nresearch on pancreas segmentation has primarily focused on modifying the\nsegmentation model architecture or utilizing pre- and post-processing\ntechniques. In this article, we investigate the utility of anatomical priors to\nenhance the segmentation performance of the pancreas. Two 3D full-resolution\nnnU-Net models were trained, one with 8 refined labels from the public PANORAMA\ndataset, and another that combined them with labels derived from the public\nTotalSegmentator (TS) tool. The addition of anatomical priors resulted in a 6\\%\nincrease in Dice score ($p < .001$) and a 36.5 mm decrease in Hausdorff\ndistance for pancreas segmentation ($p < .001$). Moreover, the pancreas was\nalways detected when anatomy priors were used, whereas there were 8 instances\nof failed detections without their use. The use of anatomy priors shows promise\nfor pancreas segmentation and subsequent derivation of imaging biomarkers.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "Published at SPIE Medical Imaging 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.06921v1",
    "published_date": "2025-04-09 14:29:08 UTC",
    "updated_date": "2025-04-09 14:29:08 UTC"
  },
  {
    "arxiv_id": "2504.06915v1",
    "title": "An Analysis of Temporal Dropout in Earth Observation Time Series for Regression Tasks",
    "authors": [
      "Miro Miranda",
      "Francisco Mena",
      "Andreas Dengel"
    ],
    "abstract": "Missing instances in time series data impose a significant challenge to deep\nlearning models, particularly in regression tasks. In the Earth Observation\nfield, satellite failure or cloud occlusion frequently results in missing\ntime-steps, introducing uncertainties in the predicted output and causing a\ndecline in predictive performance. While many studies address missing\ntime-steps through data augmentation to improve model robustness, the\nuncertainty arising at the input level is commonly overlooked. To address this\ngap, we introduce Monte Carlo Temporal Dropout (MC-TD), a method that\nexplicitly accounts for input-level uncertainty by randomly dropping time-steps\nduring inference using a predefined dropout ratio, thereby simulating the\neffect of missing data. To bypass the need for costly searches for the optimal\ndropout ratio, we extend this approach with Monte Carlo Concrete Temporal\nDropout (MC-ConcTD), a method that learns the optimal dropout distribution\ndirectly. Both MC-TD and MC-ConcTD are applied during inference, leveraging\nMonte Carlo sampling for uncertainty quantification. Experiments on three EO\ntime-series datasets demonstrate that MC-ConcTD improves predictive performance\nand uncertainty calibration compared to existing approaches. Additionally, we\nhighlight the advantages of adaptive dropout tuning over manual selection,\nmaking uncertainty quantification more robust and accessible for EO\napplications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at Symposium on Intelligent Data Analysis (IDA 2025)",
    "pdf_url": "http://arxiv.org/pdf/2504.06915v1",
    "published_date": "2025-04-09 14:23:04 UTC",
    "updated_date": "2025-04-09 14:23:04 UTC"
  },
  {
    "arxiv_id": "2504.13908v1",
    "title": "AI-Assisted Conversational Interviewing: Effects on Data Quality and User Experience",
    "authors": [
      "Soubhik Barari",
      "Jarret Angbazo",
      "Natalie Wang",
      "Leah M. Christian",
      "Elizabeth Dean",
      "Zoe Slowinski",
      "Brandon Sepulvado"
    ],
    "abstract": "Standardized surveys scale efficiently but sacrifice depth, while\nconversational interviews improve response quality at the cost of scalability\nand consistency. This study bridges the gap between these methods by\nintroducing a framework for AI-assisted conversational interviewing. To\nevaluate this framework, we conducted a web survey experiment where 1,800\nparticipants were randomly assigned to text-based conversational AI agents, or\n\"textbots\", to dynamically probe respondents for elaboration and interactively\ncode open-ended responses. We assessed textbot performance in terms of coding\naccuracy, response quality, and respondent experience. Our findings reveal that\ntextbots perform moderately well in live coding even without survey-specific\nfine-tuning, despite slightly inflated false positive errors due to respondent\nacquiescence bias. Open-ended responses were more detailed and informative, but\nthis came at a slight cost to respondent experience. Our findings highlight the\nfeasibility of using AI methods to enhance open-ended data collection in web\nsurveys.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "stat.AP"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.13908v1",
    "published_date": "2025-04-09 13:58:07 UTC",
    "updated_date": "2025-04-09 13:58:07 UTC"
  },
  {
    "arxiv_id": "2504.06897v1",
    "title": "MedSegFactory: Text-Guided Generation of Medical Image-Mask Pairs",
    "authors": [
      "Jiawei Mao",
      "Yuhan Wang",
      "Yucheng Tang",
      "Daguang Xu",
      "Kang Wang",
      "Yang Yang",
      "Zongwei Zhou",
      "Yuyin Zhou"
    ],
    "abstract": "This paper presents MedSegFactory, a versatile medical synthesis framework\nthat generates high-quality paired medical images and segmentation masks across\nmodalities and tasks. It aims to serve as an unlimited data repository,\nsupplying image-mask pairs to enhance existing segmentation tools. The core of\nMedSegFactory is a dual-stream diffusion model, where one stream synthesizes\nmedical images and the other generates corresponding segmentation masks. To\nensure precise alignment between image-mask pairs, we introduce Joint\nCross-Attention (JCA), enabling a collaborative denoising paradigm by dynamic\ncross-conditioning between streams. This bidirectional interaction allows both\nrepresentations to guide each other's generation, enhancing consistency between\ngenerated pairs. MedSegFactory unlocks on-demand generation of paired medical\nimages and segmentation masks through user-defined prompts that specify the\ntarget labels, imaging modalities, anatomical regions, and pathological\nconditions, facilitating scalable and high-quality data generation. This new\nparadigm of medical image synthesis enables seamless integration into diverse\nmedical imaging workflows, enhancing both efficiency and accuracy. Extensive\nexperiments show that MedSegFactory generates data of superior quality and\nusability, achieving competitive or state-of-the-art performance in 2D and 3D\nsegmentation tasks while addressing data scarcity and regulatory constraints.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 8 figures, The project page can be accessed via\n  https://jwmao1.github.io/MedSegFactory_web",
    "pdf_url": "http://arxiv.org/pdf/2504.06897v1",
    "published_date": "2025-04-09 13:56:05 UTC",
    "updated_date": "2025-04-09 13:56:05 UTC"
  },
  {
    "arxiv_id": "2504.06884v1",
    "title": "Audio-visual Event Localization on Portrait Mode Short Videos",
    "authors": [
      "Wuyang Liu",
      "Yi Chai",
      "Yongpeng Yan",
      "Yanzhen Ren"
    ],
    "abstract": "Audio-visual event localization (AVEL) plays a critical role in multimodal\nscene understanding. While existing datasets for AVEL predominantly comprise\nlandscape-oriented long videos with clean and simple audio context, short\nvideos have become the primary format of online video content due to the the\nproliferation of smartphones. Short videos are characterized by\nportrait-oriented framing and layered audio compositions (e.g., overlapping\nsound effects, voiceovers, and music), which brings unique challenges\nunaddressed by conventional methods. To this end, we introduce AVE-PM, the\nfirst AVEL dataset specifically designed for portrait mode short videos,\ncomprising 25,335 clips that span 86 fine-grained categories with frame-level\nannotations. Beyond dataset creation, our empirical analysis shows that\nstate-of-the-art AVEL methods suffer an average 18.66% performance drop during\ncross-mode evaluation. Further analysis reveals two key challenges of different\nvideo formats: 1) spatial bias from portrait-oriented framing introduces\ndistinct domain priors, and 2) noisy audio composition compromise the\nreliability of audio modality. To address these issues, we investigate optimal\npreprocessing recipes and the impact of background music for AVEL on portrait\nmode videos. Experiments show that these methods can still benefit from\ntailored preprocessing and specialized model design, thus achieving improved\nperformance. This work provides both a foundational benchmark and actionable\ninsights for advancing AVEL research in the era of mobile-centric video\ncontent. Dataset and code will be released.",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.MM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.06884v1",
    "published_date": "2025-04-09 13:38:40 UTC",
    "updated_date": "2025-04-09 13:38:40 UTC"
  },
  {
    "arxiv_id": "2504.06881v1",
    "title": "Compound and Parallel Modes of Tropical Convolutional Neural Networks",
    "authors": [
      "Mingbo Li",
      "Liying Liu",
      "Ye Luo"
    ],
    "abstract": "Convolutional neural networks have become increasingly deep and complex,\nleading to higher computational costs. While tropical convolutional neural\nnetworks (TCNNs) reduce multiplications, they underperform compared to standard\nCNNs. To address this, we propose two new variants - compound TCNN (cTCNN) and\nparallel TCNN (pTCNN)-that use combinations of tropical min-plus and max-plus\nkernels to replace traditional convolution kernels. This reduces\nmultiplications and balances efficiency with performance. Experiments on\nvarious datasets show that cTCNN and pTCNN match or exceed the performance of\nother CNN methods. Combining these with conventional CNNs in deeper\narchitectures also improves performance. We are further exploring simplified\nTCNN architectures that reduce parameters and multiplications with minimal\naccuracy loss, aiming for efficient and effective models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.2.6"
    ],
    "primary_category": "cs.CV",
    "comment": "28 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.06881v1",
    "published_date": "2025-04-09 13:36:11 UTC",
    "updated_date": "2025-04-09 13:36:11 UTC"
  },
  {
    "arxiv_id": "2504.08818v1",
    "title": "From Text to Time? Rethinking the Effectiveness of the Large Language Model for Time Series Forecasting",
    "authors": [
      "Xinyu Zhang",
      "Shanshan Feng",
      "Xutao Li"
    ],
    "abstract": "Using pre-trained large language models (LLMs) as the backbone for time\nseries prediction has recently gained significant research interest. However,\nthe effectiveness of LLM backbones in this domain remains a topic of debate.\nBased on thorough empirical analyses, we observe that training and testing\nLLM-based models on small datasets often leads to the Encoder and Decoder\nbecoming overly adapted to the dataset, thereby obscuring the true predictive\ncapabilities of the LLM backbone. To investigate the genuine potential of LLMs\nin time series prediction, we introduce three pre-training models with\nidentical architectures but different pre-training strategies. Thereby,\nlarge-scale pre-training allows us to create unbiased Encoder and Decoder\ncomponents tailored to the LLM backbone. Through controlled experiments, we\nevaluate the zero-shot and few-shot prediction performance of the LLM, offering\ninsights into its capabilities. Extensive experiments reveal that although the\nLLM backbone demonstrates some promise, its forecasting performance is limited.\nOur source code is publicly available in the anonymous repository:\nhttps://anonymous.4open.science/r/LLM4TS-0B5C.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08818v1",
    "published_date": "2025-04-09 13:20:09 UTC",
    "updated_date": "2025-04-09 13:20:09 UTC"
  },
  {
    "arxiv_id": "2504.06868v3",
    "title": "Persona Dynamics: Unveiling the Impact of Personality Traits on Agents in Text-Based Games",
    "authors": [
      "Seungwon Lim",
      "Seungbeen Lee",
      "Dongjun Min",
      "Youngjae Yu"
    ],
    "abstract": "Artificial agents are increasingly central to complex interactions and\ndecision-making tasks, yet aligning their behaviors with desired human values\nremains an open challenge. In this work, we investigate how human-like\npersonality traits influence agent behavior and performance within text-based\ninteractive environments. We introduce PANDA: Personality Adapted Neural\nDecision Agents, a novel method for projecting human personality traits onto\nagents to guide their behavior. To induce personality in a text-based game\nagent, (i) we train a personality classifier to identify what personality type\nthe agent's actions exhibit, and (ii) we integrate the personality profiles\ndirectly into the agent's policy-learning pipeline. By deploying agents\nembodying 16 distinct personality types across 25 text-based games and\nanalyzing their trajectories, we demonstrate that an agent's action decisions\ncan be guided toward specific personality profiles. Moreover, certain\npersonality types, such as those characterized by higher levels of Openness,\ndisplay marked advantages in performance. These findings underscore the promise\nof personality-adapted agents for fostering more aligned, effective, and\nhuman-centric decision-making in interactive environments.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.06868v3",
    "published_date": "2025-04-09 13:17:00 UTC",
    "updated_date": "2025-04-28 07:35:12 UTC"
  },
  {
    "arxiv_id": "2504.06866v1",
    "title": "GraspClutter6D: A Large-scale Real-world Dataset for Robust Perception and Grasping in Cluttered Scenes",
    "authors": [
      "Seunghyeok Back",
      "Joosoon Lee",
      "Kangmin Kim",
      "Heeseon Rho",
      "Geonhyup Lee",
      "Raeyoung Kang",
      "Sangbeom Lee",
      "Sangjun Noh",
      "Youngjin Lee",
      "Taeyeop Lee",
      "Kyoobin Lee"
    ],
    "abstract": "Robust grasping in cluttered environments remains an open challenge in\nrobotics. While benchmark datasets have significantly advanced deep learning\nmethods, they mainly focus on simplistic scenes with light occlusion and\ninsufficient diversity, limiting their applicability to practical scenarios. We\npresent GraspClutter6D, a large-scale real-world grasping dataset featuring:\n(1) 1,000 highly cluttered scenes with dense arrangements (14.1 objects/scene,\n62.6\\% occlusion), (2) comprehensive coverage across 200 objects in 75\nenvironment configurations (bins, shelves, and tables) captured using four\nRGB-D cameras from multiple viewpoints, and (3) rich annotations including 736K\n6D object poses and 9.3B feasible robotic grasps for 52K RGB-D images. We\nbenchmark state-of-the-art segmentation, object pose estimation, and grasping\ndetection methods to provide key insights into challenges in cluttered\nenvironments. Additionally, we validate the dataset's effectiveness as a\ntraining resource, demonstrating that grasping networks trained on\nGraspClutter6D significantly outperform those trained on existing datasets in\nboth simulation and real-world experiments. The dataset, toolkit, and\nannotation tools are publicly available on our project website:\nhttps://sites.google.com/view/graspclutter6d.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.06866v1",
    "published_date": "2025-04-09 13:15:46 UTC",
    "updated_date": "2025-04-09 13:15:46 UTC"
  },
  {
    "arxiv_id": "2504.06861v1",
    "title": "EIDT-V: Exploiting Intersections in Diffusion Trajectories for Model-Agnostic, Zero-Shot, Training-Free Text-to-Video Generation",
    "authors": [
      "Diljeet Jagpal",
      "Xi Chen",
      "Vinay P. Namboodiri"
    ],
    "abstract": "Zero-shot, training-free, image-based text-to-video generation is an emerging\narea that aims to generate videos using existing image-based diffusion models.\nCurrent methods in this space require specific architectural changes to image\ngeneration models, which limit their adaptability and scalability. In contrast\nto such methods, we provide a model-agnostic approach. We use intersections in\ndiffusion trajectories, working only with the latent values. We could not\nobtain localized frame-wise coherence and diversity using only the intersection\nof trajectories. Thus, we instead use a grid-based approach. An in-context\ntrained LLM is used to generate coherent frame-wise prompts; another is used to\nidentify differences between frames. Based on these, we obtain a CLIP-based\nattention mask that controls the timing of switching the prompts for each grid\ncell. Earlier switching results in higher variance, while later switching\nresults in more coherence. Therefore, our approach can ensure appropriate\ncontrol between coherence and variance for the frames. Our approach results in\nstate-of-the-art performance while being more flexible when working with\ndiverse image-generation models. The empirical analysis using quantitative\nmetrics and user studies confirms our model's superior temporal consistency,\nvisual fidelity and user satisfaction, thus providing a novel way to obtain\ntraining-free, image-based text-to-video generation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.06861v1",
    "published_date": "2025-04-09 13:11:09 UTC",
    "updated_date": "2025-04-09 13:11:09 UTC"
  },
  {
    "arxiv_id": "2504.06843v1",
    "title": "Integrating Cognitive Processing Signals into Language Models: A Review of Advances, Applications and Future Directions",
    "authors": [
      "Angela Lopez-Cardona",
      "Sebastian Idesis",
      "Ioannis Arapakis"
    ],
    "abstract": "Recently, the integration of cognitive neuroscience in Natural Language\nProcessing (NLP) has gained significant attention. This article provides a\ncritical and timely overview of recent advancements in leveraging cognitive\nsignals, particularly Eye-tracking (ET) signals, to enhance Language Models\n(LMs) and Multimodal Large Language Models (MLLMs). By incorporating\nuser-centric cognitive signals, these approaches address key challenges,\nincluding data scarcity and the environmental costs of training large-scale\nmodels. Cognitive signals enable efficient data augmentation, faster\nconvergence, and improved human alignment. The review emphasises the potential\nof ET data in tasks like Visual Question Answering (VQA) and mitigating\nhallucinations in MLLMs, and concludes by discussing emerging challenges and\nresearch trends.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.06843v1",
    "published_date": "2025-04-09 13:01:48 UTC",
    "updated_date": "2025-04-09 13:01:48 UTC"
  },
  {
    "arxiv_id": "2504.06829v1",
    "title": "Adaptive Locally Linear Embedding",
    "authors": [
      "Ali Goli",
      "Mahdieh Alizadeh",
      "Hadi Sadoghi Yazdi"
    ],
    "abstract": "Manifold learning techniques, such as Locally linear embedding (LLE), are\ndesigned to preserve the local neighborhood structures of high-dimensional data\nduring dimensionality reduction. Traditional LLE employs Euclidean distance to\ndefine neighborhoods, which can struggle to capture the intrinsic geometric\nrelationships within complex data. A novel approach, Adaptive locally linear\nembedding(ALLE), is introduced to address this limitation by incorporating a\ndynamic, data-driven metric that enhances topological preservation. This method\nredefines the concept of proximity by focusing on topological neighborhood\ninclusion rather than fixed distances. By adapting the metric based on the\nlocal structure of the data, it achieves superior neighborhood preservation,\nparticularly for datasets with complex geometries and high-dimensional\nstructures. Experimental results demonstrate that ALLE significantly improves\nthe alignment between neighborhoods in the input and feature spaces, resulting\nin more accurate and topologically faithful embeddings. This approach advances\nmanifold learning by tailoring distance metrics to the underlying data,\nproviding a robust solution for capturing intricate relationships in\nhigh-dimensional datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages",
    "pdf_url": "http://arxiv.org/pdf/2504.06829v1",
    "published_date": "2025-04-09 12:40:13 UTC",
    "updated_date": "2025-04-09 12:40:13 UTC"
  },
  {
    "arxiv_id": "2504.06796v1",
    "title": "Learning in Spiking Neural Networks with a Calcium-based Hebbian Rule for Spike-timing-dependent Plasticity",
    "authors": [
      "Willian Soares Girão",
      "Nicoletta Risi",
      "Elisabetta Chicca"
    ],
    "abstract": "Understanding how biological neural networks are shaped via local plasticity\nmechanisms can lead to energy-efficient and self-adaptive information\nprocessing systems, which promises to mitigate some of the current roadblocks\nin edge computing systems. While biology makes use of spikes to seamless use\nboth spike timing and mean firing rate to modulate synaptic strength, most\nmodels focus on one of the two. In this work, we present a Hebbian local\nlearning rule that models synaptic modification as a function of calcium traces\ntracking neuronal activity. We show how the rule reproduces results from spike\ntime and spike rate protocols from neuroscientific studies. Moreover, we use\nthe model to train spiking neural networks on MNIST digit recognition to show\nand explain what sort of mechanisms are needed to learn real-world patterns. We\nshow how our model is sensitive to correlated spiking activity and how this\nenables it to modulate the learning rate of the network without altering the\nmean firing rate of the neurons nor the hyparameters of the learning rule. To\nthe best of our knowledge, this is the first work that showcases how spike\ntiming and rate can be complementary in their role of shaping the connectivity\nof spiking neural networks.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.06796v1",
    "published_date": "2025-04-09 11:39:59 UTC",
    "updated_date": "2025-04-09 11:39:59 UTC"
  },
  {
    "arxiv_id": "2504.06785v1",
    "title": "Zero-Shot Image-Based Large Language Model Approach to Road Pavement Monitoring",
    "authors": [
      "Shuoshuo Xu",
      "Kai Zhao",
      "James Loney",
      "Zili Li",
      "Andrea Visentin"
    ],
    "abstract": "Effective and rapid evaluation of pavement surface condition is critical for\nprioritizing maintenance, ensuring transportation safety, and minimizing\nvehicle wear and tear. While conventional manual inspections suffer from\nsubjectivity, existing machine learning-based methods are constrained by their\nreliance on large and high-quality labeled datasets, which require significant\nresources and limit adaptability across varied road conditions. The\nrevolutionary advancements in Large Language Models (LLMs) present significant\npotential for overcoming these challenges. In this study, we propose an\ninnovative automated zero-shot learning approach that leverages the image\nrecognition and natural language understanding capabilities of LLMs to assess\nroad conditions effectively. Multiple LLM-based assessment models were\ndeveloped, employing prompt engineering strategies aligned with the Pavement\nSurface Condition Index (PSCI) standards. These models' accuracy and\nreliability were evaluated against official PSCI results, with an optimized\nmodel ultimately selected. Extensive tests benchmarked the optimized model\nagainst evaluations from various levels experts using Google Street View road\nimages. The results reveal that the LLM-based approach can effectively assess\nroad conditions, with the optimized model -employing comprehensive and\nstructured prompt engineering strategies -outperforming simpler configurations\nby achieving high accuracy and consistency, even surpassing expert evaluations.\nMoreover, successfully applying the optimized model to Google Street View\nimages demonstrates its potential for future city-scale deployments. These\nfindings highlight the transformative potential of LLMs in automating road\ndamage evaluations and underscore the pivotal role of detailed prompt\nengineering in achieving reliable assessments.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.06785v1",
    "published_date": "2025-04-09 11:19:17 UTC",
    "updated_date": "2025-04-09 11:19:17 UTC"
  },
  {
    "arxiv_id": "2504.08817v1",
    "title": "Exploring utilization of generative AI for research and education in data-driven materials science",
    "authors": [
      "Takahiro Misawa",
      "Ai Koizumi",
      "Ryo Tamura",
      "Kazuyoshi Yoshimi"
    ],
    "abstract": "Generative AI has recently had a profound impact on various fields, including\ndaily life, research, and education. To explore its efficient utilization in\ndata-driven materials science, we organized a hackathon -- AIMHack2024 -- in\nJuly 2024. In this hackathon, researchers from fields such as materials\nscience, information science, bioinformatics, and condensed matter physics\nworked together to explore how generative AI can facilitate research and\neducation. Based on the results of the hackathon, this paper presents topics\nrelated to (1) conducting AI-assisted software trials, (2) building AI tutors\nfor software, and (3) developing GUI applications for software. While\ngenerative AI continues to evolve rapidly, this paper provides an early record\nof its application in data-driven materials science and highlights strategies\nfor integrating AI into research and education.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG",
      "physics.ed-ph"
    ],
    "primary_category": "cs.CY",
    "comment": "13 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.08817v1",
    "published_date": "2025-04-09 11:15:21 UTC",
    "updated_date": "2025-04-09 11:15:21 UTC"
  },
  {
    "arxiv_id": "2504.06771v1",
    "title": "AI, Help Me Think$\\unicode{x2014}$but for Myself: Assisting People in Complex Decision-Making by Providing Different Kinds of Cognitive Support",
    "authors": [
      "Leon Reicherts",
      "Zelun Tony Zhang",
      "Elisabeth von Oswald",
      "Yuanting Liu",
      "Yvonne Rogers",
      "Mariam Hassib"
    ],
    "abstract": "How can we design AI tools that effectively support human decision-making by\ncomplementing and enhancing users' reasoning processes? Common\nrecommendation-centric approaches face challenges such as inappropriate\nreliance or a lack of integration with users' decision-making processes. Here,\nwe explore an alternative interaction model in which the AI outputs build upon\nusers' own decision-making rationales. We compare this approach, which we call\nExtendAI, with a recommendation-based AI. Participants in our mixed-methods\nuser study interacted with both AIs as part of an investment decision-making\ntask. We found that the AIs had different impacts, with ExtendAI integrating\nbetter into the decision-making process and people's own thinking and leading\nto slightly better outcomes. RecommendAI was able to provide more novel\ninsights while requiring less cognitive effort. We discuss the implications of\nthese and other findings along with three tensions of AI-assisted\ndecision-making which our study revealed.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "68, 91",
      "I.2; J.4"
    ],
    "primary_category": "cs.HC",
    "comment": "To be published at ACM CHI 2025 Conference on Human Factors in\n  Computing Systems",
    "pdf_url": "http://arxiv.org/pdf/2504.06771v1",
    "published_date": "2025-04-09 10:48:17 UTC",
    "updated_date": "2025-04-09 10:48:17 UTC"
  },
  {
    "arxiv_id": "2504.07156v1",
    "title": "PLM-eXplain: Divide and Conquer the Protein Embedding Space",
    "authors": [
      "Jan van Eck",
      "Dea Gogishvili",
      "Wilson Silva",
      "Sanne Abeln"
    ],
    "abstract": "Protein language models (PLMs) have revolutionised computational biology\nthrough their ability to generate powerful sequence representations for diverse\nprediction tasks. However, their black-box nature limits biological\ninterpretation and translation to actionable insights. We present an\nexplainable adapter layer - PLM-eXplain (PLM-X), that bridges this gap by\nfactoring PLM embeddings into two components: an interpretable subspace based\non established biochemical features, and a residual subspace that preserves the\nmodel's predictive power. Using embeddings from ESM2, our adapter incorporates\nwell-established properties, including secondary structure and hydropathy while\nmaintaining high performance. We demonstrate the effectiveness of our approach\nacross three protein-level classification tasks: prediction of extracellular\nvesicle association, identification of transmembrane helices, and prediction of\naggregation propensity. PLM-X enables biological interpretation of model\ndecisions without sacrificing accuracy, offering a generalisable solution for\nenhancing PLM interpretability across various downstream applications. This\nwork addresses a critical need in computational biology by providing a bridge\nbetween powerful deep learning models and actionable biological insights.",
    "categories": [
      "q-bio.BM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.BM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.07156v1",
    "published_date": "2025-04-09 10:46:24 UTC",
    "updated_date": "2025-04-09 10:46:24 UTC"
  },
  {
    "arxiv_id": "2504.06766v1",
    "title": "FamilyTool: A Multi-hop Personalized Tool Use Benchmark",
    "authors": [
      "Yuxin Wang",
      "Yiran Guo",
      "Yining Zheng",
      "Zhangyue Yin",
      "Shuo Chen",
      "Jie Yang",
      "Jiajun Chen",
      "Xuanjing Huang",
      "Xipeng Qiu"
    ],
    "abstract": "The integration of tool learning with Large Language Models (LLMs) has\nexpanded their capabilities in handling complex tasks by leveraging external\ntools. However, existing benchmarks for tool learning inadequately address\ncritical real-world personalized scenarios, particularly those requiring\nmulti-hop reasoning and inductive knowledge adaptation in dynamic environments.\nTo bridge this gap, we introduce FamilyTool, a novel benchmark grounded in a\nfamily-based knowledge graph (KG) that simulates personalized, multi-hop tool\nuse scenarios. FamilyTool challenges LLMs with queries spanning 1 to 3\nrelational hops (e.g., inferring familial connections and preferences) and\nincorporates an inductive KG setting where models must adapt to unseen user\npreferences and relationships without re-training, a common limitation in prior\napproaches that compromises generalization. We further propose KGETool: a\nsimple KG-augmented evaluation pipeline to systematically assess LLMs' tool use\nability in these settings. Experiments reveal significant performance gaps in\nstate-of-the-art LLMs, with accuracy dropping sharply as hop complexity\nincreases and inductive scenarios exposing severe generalization deficits.\nThese findings underscore the limitations of current LLMs in handling\npersonalized, evolving real-world contexts and highlight the urgent need for\nadvancements in tool-learning frameworks. FamilyTool serves as a critical\nresource for evaluating and advancing LLM agents' reasoning, adaptability, and\nscalability in complex, dynamic environments. Code and dataset are available at\nGithub.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.06766v1",
    "published_date": "2025-04-09 10:42:36 UTC",
    "updated_date": "2025-04-09 10:42:36 UTC"
  },
  {
    "arxiv_id": "2504.06753v1",
    "title": "Detect All-Type Deepfake Audio: Wavelet Prompt Tuning for Enhanced Auditory Perception",
    "authors": [
      "Yuankun Xie",
      "Ruibo Fu",
      "Zhiyong Wang",
      "Xiaopeng Wang",
      "Songjun Cao",
      "Long Ma",
      "Haonan Cheng",
      "Long Ye"
    ],
    "abstract": "The rapid advancement of audio generation technologies has escalated the\nrisks of malicious deepfake audio across speech, sound, singing voice, and\nmusic, threatening multimedia security and trust. While existing\ncountermeasures (CMs) perform well in single-type audio deepfake detection\n(ADD), their performance declines in cross-type scenarios. This paper is\ndedicated to studying the alltype ADD task. We are the first to comprehensively\nestablish an all-type ADD benchmark to evaluate current CMs, incorporating\ncross-type deepfake detection across speech, sound, singing voice, and music.\nThen, we introduce the prompt tuning self-supervised learning (PT-SSL) training\nparadigm, which optimizes SSL frontend by learning specialized prompt tokens\nfor ADD, requiring 458x fewer trainable parameters than fine-tuning (FT).\nConsidering the auditory perception of different audio types,we propose the\nwavelet prompt tuning (WPT)-SSL method to capture type-invariant auditory\ndeepfake information from the frequency domain without requiring additional\ntraining parameters, thereby enhancing performance over FT in the all-type ADD\ntask. To achieve an universally CM, we utilize all types of deepfake audio for\nco-training. Experimental results demonstrate that WPT-XLSR-AASIST achieved the\nbest performance, with an average EER of 3.58% across all evaluation sets. The\ncode is available online.",
    "categories": [
      "cs.SD",
      "cs.AI"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.06753v1",
    "published_date": "2025-04-09 10:18:45 UTC",
    "updated_date": "2025-04-09 10:18:45 UTC"
  },
  {
    "arxiv_id": "2504.06738v1",
    "title": "EDIT: Enhancing Vision Transformers by Mitigating Attention Sink through an Encoder-Decoder Architecture",
    "authors": [
      "Wenfeng Feng",
      "Guoying Sun"
    ],
    "abstract": "In this paper, we propose EDIT (Encoder-Decoder Image Transformer), a novel\narchitecture designed to mitigate the attention sink phenomenon observed in\nVision Transformer models. Attention sink occurs when an excessive amount of\nattention is allocated to the [CLS] token, distorting the model's ability to\neffectively process image patches. To address this, we introduce a\nlayer-aligned encoder-decoder architecture, where the encoder utilizes\nself-attention to process image patches, while the decoder uses cross-attention\nto focus on the [CLS] token. Unlike traditional encoder-decoder framework,\nwhere the decoder depends solely on high-level encoder representations, EDIT\nallows the decoder to extract information starting from low-level features,\nprogressively refining the representation layer by layer. EDIT is naturally\ninterpretable demonstrated through sequential attention maps, illustrating the\nrefined, layer-by-layer focus on key image features. Experiments on ImageNet-1k\nand ImageNet-21k, along with transfer learning tasks, show that EDIT achieves\nconsistent performance improvements over DeiT3 models. These results highlight\nthe effectiveness of EDIT's design in addressing attention sink and improving\nvisual feature extraction.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.06738v1",
    "published_date": "2025-04-09 09:51:41 UTC",
    "updated_date": "2025-04-09 09:51:41 UTC"
  },
  {
    "arxiv_id": "2504.06721v1",
    "title": "Learning global control of underactuated systems with Model-Based Reinforcement Learning",
    "authors": [
      "Niccolò Turcato",
      "Marco Calì",
      "Alberto Dalla Libera",
      "Giulio Giacomuzzo",
      "Ruggero Carli",
      "Diego Romeres"
    ],
    "abstract": "This short paper describes our proposed solution for the third edition of the\n\"AI Olympics with RealAIGym\" competition, held at ICRA 2025. We employed\nMonte-Carlo Probabilistic Inference for Learning Control (MC-PILCO), an MBRL\nalgorithm recognized for its exceptional data efficiency across various\nlow-dimensional robotic tasks, including cart-pole, ball \\& plate, and Furuta\npendulum systems. MC-PILCO optimizes a system dynamics model using interaction\ndata, enabling policy refinement through simulation rather than direct system\ndata optimization. This approach has proven highly effective in physical\nsystems, offering greater data efficiency than Model-Free (MF) alternatives.\nNotably, MC-PILCO has previously won the first two editions of this\ncompetition, demonstrating its robustness in both simulated and real-world\nenvironments. Besides briefly reviewing the algorithm, we discuss the most\ncritical aspects of the MC-PILCO implementation in the tasks at hand: learning\na global policy for the pendubot and acrobot systems.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "arXiv admin note: substantial text overlap with arXiv:2409.05811",
    "pdf_url": "http://arxiv.org/pdf/2504.06721v1",
    "published_date": "2025-04-09 09:20:37 UTC",
    "updated_date": "2025-04-09 09:20:37 UTC"
  },
  {
    "arxiv_id": "2504.06719v1",
    "title": "Masked Scene Modeling: Narrowing the Gap Between Supervised and Self-Supervised Learning in 3D Scene Understanding",
    "authors": [
      "Pedro Hermosilla",
      "Christian Stippel",
      "Leon Sick"
    ],
    "abstract": "Self-supervised learning has transformed 2D computer vision by enabling\nmodels trained on large, unannotated datasets to provide versatile\noff-the-shelf features that perform similarly to models trained with labels.\nHowever, in 3D scene understanding, self-supervised methods are typically only\nused as a weight initialization step for task-specific fine-tuning, limiting\ntheir utility for general-purpose feature extraction. This paper addresses this\nshortcoming by proposing a robust evaluation protocol specifically designed to\nassess the quality of self-supervised features for 3D scene understanding. Our\nprotocol uses multi-resolution feature sampling of hierarchical models to\ncreate rich point-level representations that capture the semantic capabilities\nof the model and, hence, are suitable for evaluation with linear probing and\nnearest-neighbor methods. Furthermore, we introduce the first self-supervised\nmodel that performs similarly to supervised models when only off-the-shelf\nfeatures are used in a linear probing setup. In particular, our model is\ntrained natively in 3D with a novel self-supervised approach based on a Masked\nScene Modeling objective, which reconstructs deep features of masked patches in\na bottom-up manner and is specifically tailored to hierarchical 3D models. Our\nexperiments not only demonstrate that our method achieves competitive\nperformance to supervised models, but also surpasses existing self-supervised\napproaches by a large margin. The model and training code can be found at our\nGithub repository (https://github.com/phermosilla/msm).",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.06719v1",
    "published_date": "2025-04-09 09:19:49 UTC",
    "updated_date": "2025-04-09 09:19:49 UTC"
  },
  {
    "arxiv_id": "2504.06683v1",
    "title": "Hyperparameter Optimisation with Practical Interpretability and Explanation Methods in Probabilistic Curriculum Learning",
    "authors": [
      "Llewyn Salt",
      "Marcus Gallagher"
    ],
    "abstract": "Hyperparameter optimisation (HPO) is crucial for achieving strong performance\nin reinforcement learning (RL), as RL algorithms are inherently sensitive to\nhyperparameter settings. Probabilistic Curriculum Learning (PCL) is a\ncurriculum learning strategy designed to improve RL performance by structuring\nthe agent's learning process, yet effective hyperparameter tuning remains\nchallenging and computationally demanding. In this paper, we provide an\nempirical analysis of hyperparameter interactions and their effects on the\nperformance of a PCL algorithm within standard RL tasks, including point-maze\nnavigation and DC motor control. Using the AlgOS framework integrated with\nOptuna's Tree-Structured Parzen Estimator (TPE), we present strategies to\nrefine hyperparameter search spaces, enhancing optimisation efficiency.\nAdditionally, we introduce a novel SHAP-based interpretability approach\ntailored specifically for analysing hyperparameter impacts, offering clear\ninsights into how individual hyperparameters and their interactions influence\nRL performance. Our work contributes practical guidelines and interpretability\ntools that significantly improve the effectiveness and computational\nfeasibility of hyperparameter optimisation in reinforcement learning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.06683v1",
    "published_date": "2025-04-09 08:41:27 UTC",
    "updated_date": "2025-04-09 08:41:27 UTC"
  },
  {
    "arxiv_id": "2504.06669v1",
    "title": "NLP Security and Ethics, in the Wild",
    "authors": [
      "Heather Lent",
      "Erick Galinkin",
      "Yiyi Chen",
      "Jens Myrup Pedersen",
      "Leon Derczynski",
      "Johannes Bjerva"
    ],
    "abstract": "As NLP models are used by a growing number of end-users, an area of\nincreasing importance is NLP Security (NLPSec): assessing the vulnerability of\nmodels to malicious attacks and developing comprehensive countermeasures\nagainst them. While work at the intersection of NLP and cybersecurity has the\npotential to create safer NLP for all, accidental oversights can result in\ntangible harm (e.g., breaches of privacy or proliferation of malicious models).\nIn this emerging field, however, the research ethics of NLP have not yet faced\nmany of the long-standing conundrums pertinent to cybersecurity, until now. We\nthus examine contemporary works across NLPSec, and explore their engagement\nwith cybersecurity's ethical norms. We identify trends across the literature,\nultimately finding alarming gaps on topics like harm minimization and\nresponsible disclosure. To alleviate these concerns, we provide concrete\nrecommendations to help NLP researchers navigate this space more ethically,\nbridging the gap between traditional cybersecurity and NLP ethics, which we\nframe as ``white hat NLP''. The goal of this work is to help cultivate an\nintentional culture of ethical research for those working in NLP Security.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to TACL",
    "pdf_url": "http://arxiv.org/pdf/2504.06669v1",
    "published_date": "2025-04-09 08:12:34 UTC",
    "updated_date": "2025-04-09 08:12:34 UTC"
  },
  {
    "arxiv_id": "2504.06667v1",
    "title": "Toward Holistic Evaluation of Recommender Systems Powered by Generative Models",
    "authors": [
      "Yashar Deldjoo",
      "Nikhil Mehta",
      "Maheswaran Sathiamoorthy",
      "Shuai Zhang",
      "Pablo Castells",
      "Julian McAuley"
    ],
    "abstract": "Recommender systems powered by generative models (Gen-RecSys) extend beyond\nclassical item ranking by producing open-ended content, which simultaneously\nunlocks richer user experiences and introduces new risks. On one hand, these\nsystems can enhance personalization and appeal through dynamic explanations and\nmulti-turn dialogues. On the other hand, they might venture into unknown\nterritory-hallucinating nonexistent items, amplifying bias, or leaking private\ninformation. Traditional accuracy metrics cannot fully capture these\nchallenges, as they fail to measure factual correctness, content safety, or\nalignment with user intent.\n  This paper makes two main contributions. First, we categorize the evaluation\nchallenges of Gen-RecSys into two groups: (i) existing concerns that are\nexacerbated by generative outputs (e.g., bias, privacy) and (ii) entirely new\nrisks (e.g., item hallucinations, contradictory explanations). Second, we\npropose a holistic evaluation approach that includes scenario-based assessments\nand multi-metric checks-incorporating relevance, factual grounding, bias\ndetection, and policy compliance. Our goal is to provide a guiding framework so\nresearchers and practitioners can thoroughly assess Gen-RecSys, ensuring\neffective personalization and responsible deployment.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.06667v1",
    "published_date": "2025-04-09 08:08:16 UTC",
    "updated_date": "2025-04-09 08:08:16 UTC"
  },
  {
    "arxiv_id": "2504.06659v1",
    "title": "Bridging the Gap Between Preference Alignment and Machine Unlearning",
    "authors": [
      "Xiaohua Feng",
      "Yuyuan Li",
      "Huwei Ji",
      "Jiaming Zhang",
      "Li Zhang",
      "Tianyu Du",
      "Chaochao Chen"
    ],
    "abstract": "Despite advances in Preference Alignment (PA) for Large Language Models\n(LLMs), mainstream methods like Reinforcement Learning with Human Feedback\n(RLHF) face notable challenges. These approaches require high-quality datasets\nof positive preference examples, which are costly to obtain and computationally\nintensive due to training instability, limiting their use in low-resource\nscenarios. LLM unlearning technique presents a promising alternative, by\ndirectly removing the influence of negative examples. However, current research\nhas primarily focused on empirical validation, lacking systematic quantitative\nanalysis. To bridge this gap, we propose a framework to explore the\nrelationship between PA and LLM unlearning. Specifically, we introduce a\nbi-level optimization-based method to quantify the impact of unlearning\nspecific negative examples on PA performance. Our analysis reveals that not all\nnegative examples contribute equally to alignment improvement when unlearned,\nand the effect varies significantly across examples. Building on this insight,\nwe pose a crucial question: how can we optimally select and weight negative\nexamples for unlearning to maximize PA performance? To answer this, we propose\na framework called Unlearning to Align (U2A), which leverages bi-level\noptimization to efficiently select and unlearn examples for optimal PA\nperformance. We validate the proposed method through extensive experiments,\nwith results confirming its effectiveness.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "17 pages",
    "pdf_url": "http://arxiv.org/pdf/2504.06659v1",
    "published_date": "2025-04-09 07:49:08 UTC",
    "updated_date": "2025-04-09 07:49:08 UTC"
  },
  {
    "arxiv_id": "2504.06658v1",
    "title": "A Neuro-inspired Interpretation of Unlearning in Large Language Models through Sample-level Unlearning Difficulty",
    "authors": [
      "Xiaohua Feng",
      "Yuyuan Li",
      "Chengye Wang",
      "Junlin Liu",
      "Li Zhang",
      "Chaochao Chen"
    ],
    "abstract": "Driven by privacy protection laws and regulations, unlearning in Large\nLanguage Models (LLMs) is gaining increasing attention. However, current\nresearch often neglects the interpretability of the unlearning process,\nparticularly concerning sample-level unlearning difficulty. Existing studies\ntypically assume a uniform unlearning difficulty across samples. This\nsimplification risks attributing the performance of unlearning algorithms to\nsample selection rather than the algorithm's design, potentially steering the\ndevelopment of LLM unlearning in the wrong direction. Thus, we investigate the\nrelationship between LLM unlearning and sample characteristics, with a focus on\nunlearning difficulty. Drawing inspiration from neuroscience, we propose a\nMemory Removal Difficulty ($\\mathrm{MRD}$) metric to quantify sample-level\nunlearning difficulty. Using $\\mathrm{MRD}$, we analyze the characteristics of\nhard-to-unlearn versus easy-to-unlearn samples. Furthermore, we propose an\n$\\mathrm{MRD}$-based weighted sampling method to optimize existing unlearning\nalgorithms, which prioritizes easily forgettable samples, thereby improving\nunlearning efficiency and effectiveness. We validate the proposed metric and\nmethod using public benchmarks and datasets, with results confirming its\neffectiveness.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages",
    "pdf_url": "http://arxiv.org/pdf/2504.06658v1",
    "published_date": "2025-04-09 07:48:10 UTC",
    "updated_date": "2025-04-09 07:48:10 UTC"
  },
  {
    "arxiv_id": "2504.06649v1",
    "title": "GRAIN: Multi-Granular and Implicit Information Aggregation Graph Neural Network for Heterophilous Graphs",
    "authors": [
      "Songwei Zhao",
      "Yuan Jiang",
      "Zijing Zhang",
      "Yang Yu",
      "Hechang Chen"
    ],
    "abstract": "Graph neural networks (GNNs) have shown significant success in learning graph\nrepresentations. However, recent studies reveal that GNNs often fail to\noutperform simple MLPs on heterophilous graph tasks, where connected nodes may\ndiffer in features or labels, challenging the homophily assumption. Existing\nmethods addressing this issue often overlook the importance of information\ngranularity and rarely consider implicit relationships between distant nodes.\nTo overcome these limitations, we propose the Granular and Implicit Graph\nNetwork (GRAIN), a novel GNN model specifically designed for heterophilous\ngraphs. GRAIN enhances node embeddings by aggregating multi-view information at\nvarious granularity levels and incorporating implicit data from distant,\nnon-neighboring nodes. This approach effectively integrates local and global\ninformation, resulting in smoother, more accurate node representations. We also\nintroduce an adaptive graph information aggregator that efficiently combines\nmulti-granularity and implicit data, significantly improving node\nrepresentation quality, as shown by experiments on 13 datasets covering varying\nhomophily and heterophily. GRAIN consistently outperforms 12 state-of-the-art\nmodels, excelling on both homophilous and heterophilous graphs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.06649v1",
    "published_date": "2025-04-09 07:36:44 UTC",
    "updated_date": "2025-04-09 07:36:44 UTC"
  },
  {
    "arxiv_id": "2504.06643v3",
    "title": "AMAD: AutoMasked Attention for Unsupervised Multivariate Time Series Anomaly Detection",
    "authors": [
      "Tiange Huang",
      "Yongjun Li"
    ],
    "abstract": "Unsupervised multivariate time series anomaly detection (UMTSAD) plays a\ncritical role in various domains, including finance, networks, and sensor\nsystems. In recent years, due to the outstanding performance of deep learning\nin general sequential tasks, many models have been specialized for deep UMTSAD\ntasks and have achieved impressive results, particularly those based on the\nTransformer and self-attention mechanisms. However, the sequence anomaly\nassociation assumptions underlying these models are often limited to specific\npredefined patterns and scenarios, such as concentrated or peak anomaly\npatterns. These limitations hinder their ability to generalize to diverse\nanomaly situations, especially where the lack of labels poses significant\nchallenges. To address these issues, we propose AMAD, which integrates\n\\textbf{A}uto\\textbf{M}asked Attention for UMTS\\textbf{AD} scenarios. AMAD\nintroduces a novel structure based on the AutoMask mechanism and an attention\nmixup module, forming a simple yet generalized anomaly association\nrepresentation framework. This framework is further enhanced by a Max-Min\ntraining strategy and a Local-Global contrastive learning approach. By\ncombining multi-scale feature extraction with automatic relative association\nmodeling, AMAD provides a robust and adaptable solution to UMTSAD challenges.\nExtensive experimental results demonstrate that the proposed model achieving\ncompetitive performance results compared to SOTA benchmarks across a variety of\ndatasets.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.5.1"
    ],
    "primary_category": "cs.LG",
    "comment": "fix some grammar issues",
    "pdf_url": "http://arxiv.org/pdf/2504.06643v3",
    "published_date": "2025-04-09 07:32:59 UTC",
    "updated_date": "2025-04-25 15:30:42 UTC"
  },
  {
    "arxiv_id": "2504.08813v1",
    "title": "SafeMLRM: Demystifying Safety in Multi-modal Large Reasoning Models",
    "authors": [
      "Junfeng Fang",
      "Yukai Wang",
      "Ruipeng Wang",
      "Zijun Yao",
      "Kun Wang",
      "An Zhang",
      "Xiang Wang",
      "Tat-Seng Chua"
    ],
    "abstract": "The rapid advancement of multi-modal large reasoning models (MLRMs) --\nenhanced versions of multimodal language models (MLLMs) equipped with reasoning\ncapabilities -- has revolutionized diverse applications. However, their safety\nimplications remain underexplored. While prior work has exposed critical\nvulnerabilities in unimodal reasoning models, MLRMs introduce distinct risks\nfrom cross-modal reasoning pathways. This work presents the first systematic\nsafety analysis of MLRMs through large-scale empirical studies comparing MLRMs\nwith their base MLLMs. Our experiments reveal three critical findings: (1) The\nReasoning Tax: Acquiring reasoning capabilities catastrophically degrades\ninherited safety alignment. MLRMs exhibit 37.44% higher jailbreaking success\nrates than base MLLMs under adversarial attacks. (2) Safety Blind Spots: While\nsafety degradation is pervasive, certain scenarios (e.g., Illegal Activity)\nsuffer 25 times higher attack rates -- far exceeding the average 3.4 times\nincrease, revealing scenario-specific vulnerabilities with alarming cross-model\nand datasets consistency. (3) Emergent Self-Correction: Despite tight\nreasoning-answer safety coupling, MLRMs demonstrate nascent self-correction --\n16.9% of jailbroken reasoning steps are overridden by safe answers, hinting at\nintrinsic safeguards. These findings underscore the urgency of scenario-aware\nsafety auditing and mechanisms to amplify MLRMs' self-correction potential. To\ncatalyze research, we open-source OpenSafeMLRM, the first toolkit for MLRM\nsafety evaluation, providing unified interface for mainstream models, datasets,\nand jailbreaking methods. Our work calls for immediate efforts to harden\nreasoning-augmented AI, ensuring its transformative potential aligns with\nethical safeguards.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08813v1",
    "published_date": "2025-04-09 06:53:23 UTC",
    "updated_date": "2025-04-09 06:53:23 UTC"
  },
  {
    "arxiv_id": "2504.06611v2",
    "title": "Wanting to be Understood",
    "authors": [
      "Chrisantha Fernando",
      "Dylan Banarse",
      "Simon Osindero"
    ],
    "abstract": "This paper explores an intrinsic motivation for mutual awareness,\nhypothesizing that humans possess a fundamental drive to understand and to be\nunderstood even in the absence of extrinsic rewards. Through simulations of the\nperceptual crossing paradigm, we explore the effect of various internal reward\nfunctions in reinforcement learning agents. The drive to understand is\nimplemented as an active inference type artificial curiosity reward, whereas\nthe drive to be understood is implemented through intrinsic rewards for\nimitation, influence/impressionability, and sub-reaction time anticipation of\nthe other. Results indicate that while artificial curiosity alone does not lead\nto a preference for social interaction, rewards emphasizing reciprocal\nunderstanding successfully drive agents to prioritize interaction. We\ndemonstrate that this intrinsic motivation can facilitate cooperation in tasks\nwhere only one agent receives extrinsic reward for the behaviour of the other.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.06611v2",
    "published_date": "2025-04-09 06:15:24 UTC",
    "updated_date": "2025-04-10 07:46:00 UTC"
  },
  {
    "arxiv_id": "2504.06609v1",
    "title": "InteractRank: Personalized Web-Scale Search Pre-Ranking with Cross Interaction Features",
    "authors": [
      "Sujay Khandagale",
      "Bhawna Juneja",
      "Prabhat Agarwal",
      "Aditya Subramanian",
      "Jaewon Yang",
      "Yuting Wang"
    ],
    "abstract": "Modern search systems use a multi-stage architecture to deliver personalized\nresults efficiently. Key stages include retrieval, pre-ranking, full ranking,\nand blending, which refine billions of items to top selections. The pre-ranking\nstage, vital for scoring and filtering hundreds of thousands of items down to a\nfew thousand, typically relies on two tower models due to their computational\nefficiency, despite often lacking in capturing complex interactions. While\nquery-item cross interaction features are paramount for full ranking,\nintegrating them into pre-ranking models presents efficiency-related\nchallenges. In this paper, we introduce InteractRank, a novel two tower\npre-ranking model with robust cross interaction features used at Pinterest. By\nincorporating historical user engagement-based query-item interactions in the\nscoring function along with the two tower dot product, InteractRank\nsignificantly boosts pre-ranking performance with minimal latency and\ncomputation costs. In real-world A/B experiments at Pinterest, InteractRank\nimproves the online engagement metric by 6.5% over a BM25 baseline and by 3.7%\nover a vanilla two tower baseline. We also highlight other components of\nInteractRank, like real-time user-sequence modeling, and analyze their\ncontributions through offline ablation studies. The code for InteractRank is\navailable at https://github.com/pinterest/atg-research/tree/main/InteractRank.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG",
      "H.3.3"
    ],
    "primary_category": "cs.IR",
    "comment": "8 pages, 3 figures, to appear at TheWebConf Industry Track 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.06609v1",
    "published_date": "2025-04-09 06:13:58 UTC",
    "updated_date": "2025-04-09 06:13:58 UTC"
  },
  {
    "arxiv_id": "2504.06600v1",
    "title": "Automated Business Process Analysis: An LLM-Based Approach to Value Assessment",
    "authors": [
      "William De Michele",
      "Abel Armas Cervantes",
      "Lea Frermann"
    ],
    "abstract": "Business processes are fundamental to organizational operations, yet their\noptimization remains challenging due to the timeconsuming nature of manual\nprocess analysis. Our paper harnesses Large Language Models (LLMs) to automate\nvalue-added analysis, a qualitative process analysis technique that aims to\nidentify steps in the process that do not deliver value. To date, this\ntechnique is predominantly manual, time-consuming, and subjective. Our method\noffers a more principled approach which operates in two phases: first,\ndecomposing high-level activities into detailed steps to enable granular\nanalysis, and second, performing a value-added analysis to classify each step\naccording to Lean principles. This approach enables systematic identification\nof waste while maintaining the semantic understanding necessary for qualitative\nanalysis. We develop our approach using 50 business process models, for which\nwe collect and publish manual ground-truth labels. Our evaluation, comparing\nzero-shot baselines with more structured prompts reveals (a) a consistent\nbenefit of structured prompting and (b) promising performance for both tasks.\nWe discuss the potential for LLMs to augment human expertise in qualitative\nprocess analysis while reducing the time and subjectivity inherent in manual\napproaches.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.06600v1",
    "published_date": "2025-04-09 05:52:50 UTC",
    "updated_date": "2025-04-09 05:52:50 UTC"
  },
  {
    "arxiv_id": "2504.08000v1",
    "title": "Neuron-level Balance between Stability and Plasticity in Deep Reinforcement Learning",
    "authors": [
      "Jiahua Lan",
      "Sen Zhang",
      "Haixia Pan",
      "Ruijun Liu",
      "Li Shen",
      "Dacheng Tao"
    ],
    "abstract": "In contrast to the human ability to continuously acquire knowledge, agents\nstruggle with the stability-plasticity dilemma in deep reinforcement learning\n(DRL), which refers to the trade-off between retaining existing skills\n(stability) and learning new knowledge (plasticity). Current methods focus on\nbalancing these two aspects at the network level, lacking sufficient\ndifferentiation and fine-grained control of individual neurons. To overcome\nthis limitation, we propose Neuron-level Balance between Stability and\nPlasticity (NBSP) method, by taking inspiration from the observation that\nspecific neurons are strongly relevant to task-relevant skills. Specifically,\nNBSP first (1) defines and identifies RL skill neurons that are crucial for\nknowledge retention through a goal-oriented method, and then (2) introduces a\nframework by employing gradient masking and experience replay techniques\ntargeting these neurons to preserve the encoded existing skills while enabling\nadaptation to new tasks. Numerous experimental results on the Meta-World and\nAtari benchmarks demonstrate that NBSP significantly outperforms existing\napproaches in balancing stability and plasticity.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Reinforcement learning, RL skill neuron, stability and plasticity",
    "pdf_url": "http://arxiv.org/pdf/2504.08000v1",
    "published_date": "2025-04-09 05:43:30 UTC",
    "updated_date": "2025-04-09 05:43:30 UTC"
  },
  {
    "arxiv_id": "2504.06581v1",
    "title": "Right Prediction, Wrong Reasoning: Uncovering LLM Misalignment in RA Disease Diagnosis",
    "authors": [
      "Umakanta Maharana",
      "Sarthak Verma",
      "Avarna Agarwal",
      "Prakashini Mruthyunjaya",
      "Dwarikanath Mahapatra",
      "Sakir Ahmed",
      "Murari Mandal"
    ],
    "abstract": "Large language models (LLMs) offer a promising pre-screening tool, improving\nearly disease detection and providing enhanced healthcare access for\nunderprivileged communities. The early diagnosis of various diseases continues\nto be a significant challenge in healthcare, primarily due to the nonspecific\nnature of early symptoms, the shortage of expert medical practitioners, and the\nneed for prolonged clinical evaluations, all of which can delay treatment and\nadversely affect patient outcomes. With impressive accuracy in prediction\nacross a range of diseases, LLMs have the potential to revolutionize clinical\npre-screening and decision-making for various medical conditions. In this work,\nwe study the diagnostic capability of LLMs for Rheumatoid Arthritis (RA) with\nreal world patients data. Patient data was collected alongside diagnoses from\nmedical experts, and the performance of LLMs was evaluated in comparison to\nexpert diagnoses for RA disease prediction. We notice an interesting pattern in\ndisease diagnosis and find an unexpected \\textit{misalignment between\nprediction and explanation}. We conduct a series of multi-round analyses using\ndifferent LLM agents. The best-performing model accurately predicts rheumatoid\narthritis (RA) diseases approximately 95\\% of the time. However, when medical\nexperts evaluated the reasoning generated by the model, they found that nearly\n68\\% of the reasoning was incorrect. This study highlights a clear misalignment\nbetween LLMs high prediction accuracy and its flawed reasoning, raising\nimportant questions about relying on LLM explanations in clinical settings.\n\\textbf{LLMs provide incorrect reasoning to arrive at the correct answer for RA\ndisease diagnosis.}",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.06581v1",
    "published_date": "2025-04-09 05:04:01 UTC",
    "updated_date": "2025-04-09 05:04:01 UTC"
  },
  {
    "arxiv_id": "2504.06580v1",
    "title": "Exploring Ordinal Bias in Action Recognition for Instructional Videos",
    "authors": [
      "Joochan Kim",
      "Minjoon Jung",
      "Byoung-Tak Zhang"
    ],
    "abstract": "Action recognition models have achieved promising results in understanding\ninstructional videos. However, they often rely on dominant, dataset-specific\naction sequences rather than true video comprehension, a problem that we define\nas ordinal bias. To address this issue, we propose two effective video\nmanipulation methods: Action Masking, which masks frames of frequently\nco-occurring actions, and Sequence Shuffling, which randomizes the order of\naction segments. Through comprehensive experiments, we demonstrate that current\nmodels exhibit significant performance drops when confronted with nonstandard\naction sequences, underscoring their vulnerability to ordinal bias. Our\nfindings emphasize the importance of rethinking evaluation strategies and\ndeveloping models capable of generalizing beyond fixed action patterns in\ndiverse instructional videos.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to SCSL @ ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.06580v1",
    "published_date": "2025-04-09 05:03:51 UTC",
    "updated_date": "2025-04-09 05:03:51 UTC"
  },
  {
    "arxiv_id": "2504.06578v1",
    "title": "Attributes-aware Visual Emotion Representation Learning",
    "authors": [
      "Rahul Singh Maharjan",
      "Marta Romeo",
      "Angelo Cangelosi"
    ],
    "abstract": "Visual emotion analysis or recognition has gained considerable attention due\nto the growing interest in understanding how images can convey rich semantics\nand evoke emotions in human perception. However, visual emotion analysis poses\ndistinctive challenges compared to traditional vision tasks, especially due to\nthe intricate relationship between general visual features and the different\naffective states they evoke, known as the affective gap. Researchers have used\ndeep representation learning methods to address this challenge of extracting\ngeneralized features from entire images. However, most existing methods\noverlook the importance of specific emotional attributes such as brightness,\ncolorfulness, scene understanding, and facial expressions. Through this paper,\nwe introduce A4Net, a deep representation network to bridge the affective gap\nby leveraging four key attributes: brightness (Attribute 1), colorfulness\n(Attribute 2), scene context (Attribute 3), and facial expressions (Attribute\n4). By fusing and jointly training all aspects of attribute recognition and\nvisual emotion analysis, A4Net aims to provide a better insight into emotional\ncontent in images. Experimental results show the effectiveness of A4Net,\nshowcasing competitive performance compared to state-of-the-art methods across\ndiverse visual emotion datasets. Furthermore, visualizations of activation maps\ngenerated by A4Net offer insights into its ability to generalize across\ndifferent visual emotion datasets.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.06578v1",
    "published_date": "2025-04-09 05:00:43 UTC",
    "updated_date": "2025-04-09 05:00:43 UTC"
  },
  {
    "arxiv_id": "2504.06549v1",
    "title": "Societal Impacts Research Requires Benchmarks for Creative Composition Tasks",
    "authors": [
      "Judy Hanwen Shen",
      "Carlos Guestrin"
    ],
    "abstract": "Foundation models that are capable of automating cognitive tasks represent a\npivotal technological shift, yet their societal implications remain unclear.\nThese systems promise exciting advances, yet they also risk flooding our\ninformation ecosystem with formulaic, homogeneous, and potentially misleading\nsynthetic content. Developing benchmarks grounded in real use cases where these\nrisks are most significant is therefore critical. Through a thematic analysis\nusing 2 million language model user prompts, we identify creative composition\ntasks as a prevalent usage category where users seek help with personal tasks\nthat require everyday creativity. Our fine-grained analysis identifies\nmismatches between current benchmarks and usage patterns among these tasks.\nCrucially, we argue that the same use cases that currently lack thorough\nevaluations can lead to negative downstream impacts. This position paper argues\nthat benchmarks focused on creative composition tasks is a necessary step\ntowards understanding the societal harms of AI-generated content. We call for\ngreater transparency in usage patterns to inform the development of new\nbenchmarks that can effectively measure both the progress and the impacts of\nmodels with creative capabilities.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "v1: ICLR 2025 Workshop on Bidirectional Human-AI Alignment (BiAlign)",
    "pdf_url": "http://arxiv.org/pdf/2504.06549v1",
    "published_date": "2025-04-09 03:12:16 UTC",
    "updated_date": "2025-04-09 03:12:16 UTC"
  },
  {
    "arxiv_id": "2504.08810v1",
    "title": "PriM: Principle-Inspired Material Discovery through Multi-Agent Collaboration",
    "authors": [
      "Zheyuan Lai",
      "Yingming Pu"
    ],
    "abstract": "Complex chemical space and limited knowledge scope with biases holds immense\nchallenge for human scientists, yet in automated materials discovery. Existing\nintelligent methods relies more on numerical computation, leading to\ninefficient exploration and results with hard-interpretability. To bridge this\ngap, we introduce a principles-guided material discovery system powered by\nlanguage inferential multi-agent system (MAS), namely PriM. Our framework\nintegrates automated hypothesis generation with experimental validation in a\nroundtable system of MAS, enabling systematic exploration while maintaining\nscientific rigor. Based on our framework, the case study of nano helix\ndemonstrates higher materials exploration rate and property value while\nproviding transparent reasoning pathways. This approach develops an\nautomated-and-transparent paradigm for material discovery, with broad\nimplications for rational design of functional materials. Code is publicly\navailable at our \\href{https://github.com/amair-lab/PriM}{GitHub}.",
    "categories": [
      "cs.LG",
      "cond-mat.mtrl-sci",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08810v1",
    "published_date": "2025-04-09 03:05:10 UTC",
    "updated_date": "2025-04-09 03:05:10 UTC"
  },
  {
    "arxiv_id": "2504.06542v1",
    "title": "Polygon: Symbolic Reasoning for SQL using Conflict-Driven Under-Approximation Search",
    "authors": [
      "Pinhan Zhao",
      "Yuepeng Wang",
      "Xinyu Wang"
    ],
    "abstract": "We present a novel symbolic reasoning engine for SQL which can efficiently\ngenerate an input $I$ for $n$ queries $P_1, \\cdots, P_n$, such that their\noutputs on $I$ satisfy a given property (expressed in SMT). This is useful in\ndifferent contexts, such as disproving equivalence of two SQL queries and\ndisambiguating a set of queries. Our first idea is to reason about an\nunder-approximation of each $P_i$ -- that is, a subset of $P_i$'s input-output\nbehaviors. While it makes our approach both semantics-aware and lightweight,\nthis idea alone is incomplete (as a fixed under-approximation might miss some\nbehaviors of interest). Therefore, our second idea is to perform search over an\nexpressive family of under-approximations (which collectively cover all program\nbehaviors of interest), thereby making our approach complete. We have\nimplemented these ideas in a tool, Polygon, and evaluated it on over 30,000\nbenchmarks across two tasks (namely, SQL equivalence refutation and query\ndisambiguation). Our evaluation results show that Polygon significantly\noutperforms all prior techniques.",
    "categories": [
      "cs.PL",
      "cs.AI",
      "cs.DB",
      "cs.SE"
    ],
    "primary_category": "cs.PL",
    "comment": "PLDI 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.06542v1",
    "published_date": "2025-04-09 02:46:52 UTC",
    "updated_date": "2025-04-09 02:46:52 UTC"
  },
  {
    "arxiv_id": "2504.06538v1",
    "title": "OPAL: Encoding Causal Understanding of Physical Systems for Robot Learning",
    "authors": [
      "Daniel Tcheurekdjian",
      "Joshua Klasmeier",
      "Tom Cooney",
      "Christopher McCann",
      "Tyler Fenstermaker"
    ],
    "abstract": "We present OPAL (Operant Physical Agent with Language), a novel\nvision-language-action architecture that introduces topological constraints to\nflow matching for robotic control. To do so, we further introduce topological\nattention. Our approach models action sequences as topologically-structured\nrepresentations with non-trivial constraints. Experimental results across 10\ncomplex manipulation tasks demonstrate OPAL's superior performance compared to\nprevious approaches, including Octo, OpenVLA, and ${\\pi}$0.\n  Our architecture achieves significant improvements in zero-shot performance\nwithout requiring task-specific fine-tuning, while reducing inference\ncomputational requirements by 42%. The theoretical guarantees provided by our\ntopological approach result in more coherent long-horizon action sequences. Our\nresults highlight the potential of constraining the search space of learning\nproblems in robotics by deriving from fundamental physical laws, and the\npossibility of using topological attention to embed causal understanding into\ntransformer architectures.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "11 pages, 2 figures, 3 tables, 24 equations",
    "pdf_url": "http://arxiv.org/pdf/2504.06538v1",
    "published_date": "2025-04-09 02:29:36 UTC",
    "updated_date": "2025-04-09 02:29:36 UTC"
  },
  {
    "arxiv_id": "2504.06536v1",
    "title": "Lugha-Llama: Adapting Large Language Models for African Languages",
    "authors": [
      "Happy Buzaaba",
      "Alexander Wettig",
      "David Ifeoluwa Adelani",
      "Christiane Fellbaum"
    ],
    "abstract": "Large language models (LLMs) have achieved impressive results in a wide range\nof natural language applications. However, they often struggle to recognize\nlow-resource languages, in particular African languages, which are not well\nrepresented in large training corpora. In this paper, we consider how to adapt\nLLMs to low-resource African languages. We find that combining curated data\nfrom African languages with high-quality English educational texts results in a\ntraining mix that substantially improves the model's performance on these\nlanguages. On the challenging IrokoBench dataset, our models consistently\nachieve the best performance amongst similarly sized baselines, particularly on\nknowledge-intensive multiple-choice questions (AfriMMLU). Additionally, on the\ncross-lingual question answering benchmark AfriQA, our models outperform the\nbase model by over 10%. To better understand the role of English data during\ntraining, we translate a subset of 200M tokens into Swahili language and\nperform an analysis which reveals that the content of these data is primarily\nresponsible for the strong performance. We release our models and data to\nencourage future research on African languages.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.06536v1",
    "published_date": "2025-04-09 02:25:53 UTC",
    "updated_date": "2025-04-09 02:25:53 UTC"
  },
  {
    "arxiv_id": "2504.08806v1",
    "title": "Endowing Embodied Agents with Spatial Reasoning Capabilities for Vision-and-Language Navigation",
    "authors": [
      "Luo Ling",
      "Bai Qianqian"
    ],
    "abstract": "Enhancing the spatial perception capabilities of mobile robots is crucial for\nachieving embodied Vision-and-Language Navigation (VLN). Although significant\nprogress has been made in simulated environments, directly transferring these\ncapabilities to real-world scenarios often results in severe hallucination\nphenomena, causing robots to lose effective spatial awareness. To address this\nissue, we propose BrainNav, a bio-inspired spatial cognitive navigation\nframework inspired by biological spatial cognition theories and cognitive map\ntheory. BrainNav integrates dual-map (coordinate map and topological map) and\ndual-orientation (relative orientation and absolute orientation) strategies,\nenabling real-time navigation through dynamic scene capture and path planning.\nIts five core modules-Hippocampal Memory Hub, Visual Cortex Perception Engine,\nParietal Spatial Constructor, Prefrontal Decision Center, and Cerebellar Motion\nExecution Unit-mimic biological cognitive functions to reduce spatial\nhallucinations and enhance adaptability. Validated in a zero-shot real-world\nlab environment using the Limo Pro robot, BrainNav, compatible with GPT-4,\noutperforms existing State-of-the-Art (SOTA) Vision-and-Language Navigation in\nContinuous Environments (VLN-CE) methods without fine-tuning.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08806v1",
    "published_date": "2025-04-09 02:19:22 UTC",
    "updated_date": "2025-04-09 02:19:22 UTC"
  },
  {
    "arxiv_id": "2504.06533v2",
    "title": "Flexible Graph Similarity Computation With A Proactive Optimization Strategy",
    "authors": [
      "Zhouyang Liu",
      "Ning Liu",
      "Yixin Chen",
      "Jiezhong He",
      "Dongsheng Li"
    ],
    "abstract": "Graph Edit Distance (GED) offers a principled and flexible measure of graph\nsimilarity, as it quantifies the minimum cost needed to transform one graph\ninto another with customizable edit operation costs. Despite recent\nlearning-based efforts to approximate GED via vector space representations,\nexisting methods struggle with adapting to varying operation costs.\nFurthermore, they suffer from inefficient, reactive mapping refinements due to\nreliance on isolated node-level distance as guidance. To address these issues,\nwe propose GEN, a novel learning-based approach for flexible GED approximation.\nGEN addresses the varying costs adaptation by integrating operation costs prior\nto match establishment, enabling mappings to dynamically adapt to cost\nvariations. Furthermore, GEN introduces a proactive guidance optimization\nstrategy that captures graph-level dependencies between matches, allowing\ninformed matching decisions in a single step without costly iterative\nrefinements. Extensive evaluations on real-world and synthetic datasets\ndemonstrate that GEN achieves up to 37.8% reduction in GED approximation error\nand 72.7% reduction in inference time compared with state-of-the-art methods,\nwhile consistently maintaining robustness under diverse cost settings and graph\nsizes.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DS"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.06533v2",
    "published_date": "2025-04-09 02:16:46 UTC",
    "updated_date": "2025-05-15 08:42:50 UTC"
  },
  {
    "arxiv_id": "2504.06532v1",
    "title": "WaveHiTS: Wavelet-Enhanced Hierarchical Time Series Modeling for Wind Direction Nowcasting in Eastern Inner Mongolia",
    "authors": [
      "Hailong Shu",
      "Weiwei Song",
      "Yue Wang",
      "Jiping Zhang"
    ],
    "abstract": "Wind direction forecasting plays a crucial role in optimizing wind energy\nproduction, but faces significant challenges due to the circular nature of\ndirectional data, error accumulation in multi-step forecasting, and complex\nmeteorological interactions. This paper presents a novel model, WaveHiTS, which\nintegrates wavelet transform with Neural Hierarchical Interpolation for Time\nSeries to address these challenges. Our approach decomposes wind direction into\nU-V components, applies wavelet transform to capture multi-scale frequency\npatterns, and utilizes a hierarchical structure to model temporal dependencies\nat multiple scales, effectively mitigating error propagation. Experiments\nconducted on real-world meteorological data from Inner Mongolia, China\ndemonstrate that WaveHiTS significantly outperforms deep learning models (RNN,\nLSTM, GRU), transformer-based approaches (TFT, Informer, iTransformer), and\nhybrid models (EMD-LSTM). The proposed model achieves RMSE values of\napproximately 19.2{\\deg}-19.4{\\deg} compared to 56{\\deg}-64{\\deg} for deep\nlearning recurrent models, maintaining consistent accuracy across all\nforecasting steps up to 60 minutes ahead. Moreover, WaveHiTS demonstrates\nsuperior robustness with vector correlation coefficients (VCC) of 0.985-0.987\nand hit rates of 88.5%-90.1%, substantially outperforming baseline models.\nAblation studies confirm that each component-wavelet transform, hierarchical\nstructure, and U-V decomposition-contributes meaningfully to overall\nperformance. These improvements in wind direction nowcasting have significant\nimplications for enhancing wind turbine yaw control efficiency and grid\nintegration of wind energy.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.06532v1",
    "published_date": "2025-04-09 02:15:48 UTC",
    "updated_date": "2025-04-09 02:15:48 UTC"
  },
  {
    "arxiv_id": "2504.06531v1",
    "title": "Beyond Moore's Law: Harnessing the Redshift of Generative AI with Effective Hardware-Software Co-Design",
    "authors": [
      "Amir Yazdanbakhsh"
    ],
    "abstract": "For decades, Moore's Law has served as a steadfast pillar in computer\narchitecture and system design, promoting a clear abstraction between hardware\nand software. This traditional Moore's computing paradigm has deepened the rift\nbetween the two, enabling software developers to achieve near-exponential\nperformance gains often without needing to delve deeply into hardware-specific\noptimizations. Yet today, Moore's Law -- with its once relentless performance\ngains now diminished to incremental improvements -- faces inevitable physical\nbarriers. This stagnation necessitates a reevaluation of the conventional\nsystem design philosophy. The traditional decoupled system design philosophy,\nwhich maintains strict abstractions between hardware and software, is\nincreasingly obsolete. The once-clear boundary between software and hardware is\nrapidly dissolving, replaced by co-design. It is imperative for the computing\ncommunity to intensify its commitment to hardware-software co-design, elevating\nsystem abstractions to first-class citizens and reimagining design principles\nto satisfy the insatiable appetite of modern computing. Hardware-software\nco-design is not a recent innovation. To illustrate its historical evolution, I\nclassify its development into five relatively distinct ``epochs''. This post\nalso highlights the growing influence of the architecture community in\ninterdisciplinary teams -- particularly alongside ML researchers -- and\nexplores why current co-design paradigms are struggling in today's computing\nlandscape. Additionally, I will examine the concept of the ``hardware lottery''\nand explore directions to mitigate its constraining influence on the next era\nof computing innovation.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.06531v1",
    "published_date": "2025-04-09 02:10:58 UTC",
    "updated_date": "2025-04-09 02:10:58 UTC"
  },
  {
    "arxiv_id": "2504.06527v1",
    "title": "TSP-OCS: A Time-Series Prediction for Optimal Camera Selection in Multi-Viewpoint Surgical Video Analysis",
    "authors": [
      "Xinyu Liu",
      "Xiaoguang Lin",
      "Xiang Liu",
      "Yong Yang",
      "Hongqian Wang",
      "Qilong Sun"
    ],
    "abstract": "Recording the open surgery process is essential for educational and medical\nevaluation purposes; however, traditional single-camera methods often face\nchallenges such as occlusions caused by the surgeon's head and body, as well as\nlimitations due to fixed camera angles, which reduce comprehensibility of the\nvideo content. This study addresses these limitations by employing a\nmulti-viewpoint camera recording system, capturing the surgical procedure from\nsix different angles to mitigate occlusions. We propose a fully supervised\nlearning-based time series prediction method to choose the best shot sequences\nfrom multiple simultaneously recorded video streams, ensuring optimal\nviewpoints at each moment. Our time series prediction model forecasts future\ncamera selections by extracting and fusing visual and semantic features from\nsurgical videos using pre-trained models. These features are processed by a\ntemporal prediction network with TimeBlocks to capture sequential dependencies.\nA linear embedding layer reduces dimensionality, and a Softmax classifier\nselects the optimal camera view based on the highest probability. In our\nexperiments, we created five groups of open thyroidectomy videos, each with\nsimultaneous recordings from six different angles. The results demonstrate that\nour method achieves competitive accuracy compared to traditional supervised\nmethods, even when predicting over longer time horizons. Furthermore, our\napproach outperforms state-of-the-art time series prediction techniques on our\ndataset. This manuscript makes a unique contribution by presenting an\ninnovative framework that advances surgical video analysis techniques, with\nsignificant implications for improving surgical education and patient safety.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.06527v1",
    "published_date": "2025-04-09 02:07:49 UTC",
    "updated_date": "2025-04-09 02:07:49 UTC"
  },
  {
    "arxiv_id": "2504.06525v1",
    "title": "The Power of the Pareto Front: Balancing Uncertain Rewards for Adaptive Experimentation in scanning probe microscopy",
    "authors": [
      "Yu Liu",
      "Sergei V. Kalinin"
    ],
    "abstract": "Automated experimentation has the potential to revolutionize scientific\ndiscovery, but its effectiveness depends on well-defined optimization targets,\nwhich are often uncertain or probabilistic in real-world settings. In this\nwork, we demonstrate the application of Multi-Objective Bayesian Optimization\n(MOBO) to balance multiple, competing rewards in autonomous experimentation.\nUsing scanning probe microscopy (SPM) imaging, one of the most widely used and\nfoundational SPM modes, we show that MOBO can optimize imaging parameters to\nenhance measurement quality, reproducibility, and efficiency. A key advantage\nof this approach is the ability to compute and analyze the Pareto front, which\nnot only guides optimization but also provides physical insights into the\ntrade-offs between different objectives. Additionally, MOBO offers a natural\nframework for human-in-the-loop decision-making, enabling researchers to\nfine-tune experimental trade-offs based on domain expertise. By standardizing\nhigh-quality, reproducible measurements and integrating human input into\nAI-driven optimization, this work highlights MOBO as a powerful tool for\nadvancing autonomous scientific discovery.",
    "categories": [
      "cs.LG",
      "cond-mat.mes-hall",
      "cond-mat.mtrl-sci",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "23 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.06525v1",
    "published_date": "2025-04-09 01:59:31 UTC",
    "updated_date": "2025-04-09 01:59:31 UTC"
  },
  {
    "arxiv_id": "2504.06514v2",
    "title": "Missing Premise exacerbates Overthinking: Are Reasoning Models losing Critical Thinking Skill?",
    "authors": [
      "Chenrui Fan",
      "Ming Li",
      "Lichao Sun",
      "Tianyi Zhou"
    ],
    "abstract": "We find that the response length of reasoning LLMs, whether trained by\nreinforcement learning or supervised learning, drastically increases for\nill-posed questions with missing premises (MiP), ending up with redundant and\nineffective thinking. This newly introduced scenario exacerbates the general\noverthinking issue to a large extent, which we name as the MiP-Overthinking.\nSuch failures are against the ``test-time scaling law'' but have been widely\nobserved on multiple datasets we curated with MiP, indicating the harm of cheap\noverthinking and a lack of critical thinking. Surprisingly, LLMs not\nspecifically trained for reasoning exhibit much better performance on the MiP\nscenario, producing much shorter responses that quickly identify ill-posed\nqueries. This implies a critical flaw of the current training recipe for\nreasoning LLMs, which does not encourage efficient thinking adequately, leading\nto the abuse of thinking patterns. To further investigate the reasons behind\nsuch failures, we conduct fine-grained analyses of the reasoning length,\noverthinking patterns, and location of critical thinking on different types of\nLLMs. Moreover, our extended ablation study reveals that the overthinking is\ncontagious through the distillation of reasoning models' responses. These\nresults improve the understanding of overthinking and shed novel insights into\nmitigating the problem.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.06514v2",
    "published_date": "2025-04-09 01:25:27 UTC",
    "updated_date": "2025-04-11 02:36:28 UTC"
  },
  {
    "arxiv_id": "2504.06497v1",
    "title": "Continuous-Variable Quantum Encoding Techniques: A Comparative Study of Embedding Techniques and Their Impact on Machine Learning Performance",
    "authors": [
      "Minati Rath",
      "Hema Date"
    ],
    "abstract": "This study explores the intersection of continuous-variable quantum computing\n(CVQC) and classical machine learning, focusing on CVQC data encoding\ntechniques, including Displacement encoding and squeezing encoding, alongside\nInstantaneous Quantum Polynomial (IQP) encoding from discrete quantum\ncomputing. We perform an extensive empirical analysis to assess the impact of\nthese encoding methods on classical machine learning models, such as Logistic\nRegression, Support Vector Machines, K-Nearest Neighbors, and ensemble methods\nlike Random Forest and LightGBM. Our findings indicate that CVQC-based encoding\nmethods significantly enhance feature expressivity, resulting in improved\nclassification accuracy and F1 scores, especially in high-dimensional and\ncomplex datasets. However, these improvements come with varying computational\ncosts, which depend on the complexity of the encoding and the architecture of\nthe machine learning models. Additionally, we examine the trade-off between\nquantum expressibility and classical learnability, offering valuable insights\ninto the practical feasibility of incorporating these quantum encodings into\nreal-world applications. This study contributes to the growing body of research\non quantum-classical hybrid learning, emphasizing the role of CVQC in advancing\nquantum data representation and its integration into classical machine learning\nworkflows.",
    "categories": [
      "quant-ph",
      "cs.AI"
    ],
    "primary_category": "quant-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.06497v1",
    "published_date": "2025-04-09 00:00:45 UTC",
    "updated_date": "2025-04-09 00:00:45 UTC"
  }
]
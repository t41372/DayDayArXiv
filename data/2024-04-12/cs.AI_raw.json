[
  {
    "arxiv_id": "2404.08850v2",
    "title": "Assessing Economic Viability: A Comparative Analysis of Total Cost of Ownership for Domain-Adapted Large Language Models versus State-of-the-art Counterparts in Chip Design Coding Assistance",
    "authors": [
      "Amit Sharma",
      "Teodor-Dumitru Ene",
      "Kishor Kunal",
      "Mingjie Liu",
      "Zafar Hasan",
      "Haoxing Ren"
    ],
    "abstract": "This paper presents a comparative analysis of total cost of ownership (TCO)\nand performance between domain-adapted large language models (LLM) and\nstate-of-the-art (SoTA) LLMs , with a particular emphasis on tasks related to\ncoding assistance for chip design. We examine the TCO and performance metrics\nof a domain-adaptive LLM, ChipNeMo, against two leading LLMs, Claude 3 Opus and\nChatGPT-4 Turbo, to assess their efficacy in chip design coding generation.\nThrough a detailed evaluation of the accuracy of the model, training\nmethodologies, and operational expenditures, this study aims to provide\nstakeholders with critical information to select the most economically viable\nand performance-efficient solutions for their specific needs. Our results\nunderscore the benefits of employing domain-adapted models, such as ChipNeMo,\nthat demonstrate improved performance at significantly reduced costs compared\nto their general-purpose counterparts. In particular, we reveal the potential\nof domain-adapted LLMs to decrease TCO by approximately 90%-95%, with the cost\nadvantages becoming increasingly evident as the deployment scale expands. With\nexpansion of deployment, the cost benefits of ChipNeMo become more pronounced,\nmaking domain-adaptive LLMs an attractive option for organizations with\nsubstantial coding needs supported by LLMs",
    "categories": [
      "cs.AI",
      "cs.CE",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Paper accepted in IEEE-ACM conference: 2024 IEEE LLM-Aided Design\n  Workshop (LAD)",
    "pdf_url": "http://arxiv.org/pdf/2404.08850v2",
    "published_date": "2024-04-12 23:37:56 UTC",
    "updated_date": "2024-05-28 17:11:44 UTC"
  },
  {
    "arxiv_id": "2404.08844v2",
    "title": "Multi-fingered Robotic Hand Grasping in Cluttered Environments through Hand-object Contact Semantic Mapping",
    "authors": [
      "Lei Zhang",
      "Kaixin Bai",
      "Guowen Huang",
      "Zhenshan Bing",
      "Zhaopeng Chen",
      "Alois Knoll",
      "Jianwei Zhang"
    ],
    "abstract": "The deep learning models has significantly advanced dexterous manipulation\ntechniques for multi-fingered hand grasping. However, the contact\ninformation-guided grasping in cluttered environments remains largely\nunderexplored. To address this gap, we have developed a method for generating\nmulti-fingered hand grasp samples in cluttered settings through contact\nsemantic map. We introduce a contact semantic conditional variational\nautoencoder network (CoSe-CVAE) for creating comprehensive contact semantic map\nfrom object point cloud. We utilize grasp detection method to estimate hand\ngrasp poses from the contact semantic map. Finally, an unified grasp evaluation\nmodel is designed to assess grasp quality and collision probability,\nsubstantially improving the reliability of identifying optimal grasps in\ncluttered scenarios. Our grasp generation method has demonstrated remarkable\nsuccess, outperforming state-of-the-art methods by at least 4.65% with 81.0%\naverage grasping success rate in real-world single-object environment and 75.3%\ngrasping success rate in cluttered scenes. We also proposed the multi-modal\nmulti-fingered grasping dataset generation method. Our multi-fingered hand\ngrasping dataset outperforms previous datasets in scene diversity, modality\ndiversity. The dataset, code and supplementary materials can be found at\nhttps://sites.google.com/view/ffh-cluttered-grasping.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages",
    "pdf_url": "http://arxiv.org/pdf/2404.08844v2",
    "published_date": "2024-04-12 23:11:36 UTC",
    "updated_date": "2024-09-22 11:46:14 UTC"
  },
  {
    "arxiv_id": "2404.08837v2",
    "title": "Vehicle-to-Vehicle Charging: Model, Complexity, and Heuristics",
    "authors": [
      "Cláudio Gomes",
      "João Paulo Fernandes",
      "Gabriel Falcao",
      "Soummya Kar",
      "Sridhar Tayur"
    ],
    "abstract": "The rapid adoption of Electric Vehicles (EVs) poses challenges for\nelectricity grids to accommodate or mitigate peak demand. Vehicle-to-Vehicle\nCharging (V2VC) has been recently adopted by popular EVs, posing new\nopportunities and challenges to the management and operation of EVs. We present\na novel V2VC model that allows decision-makers to take V2VC into account when\noptimizing their EV operations. We show that optimizing V2VC is NP-Complete and\nfind that even small problem instances are computationally challenging. We\npropose R-V2VC, a heuristic that takes advantage of the resulting totally\nunimodular constraint matrix to efficiently solve problems of realistic sizes.\nOur results demonstrate that R-V2VC presents a linear growth in the solution\ntime as the problem size increases, while achieving solutions of optimal or\nnear-optimal quality. R-V2VC can be used for real-world operations and to study\nwhat-if scenarios when evaluating the costs and benefits of V2VC.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "7 pages, 6 figures, and 3 tables. This work has been submitted to the\n  IEEE for possible publication",
    "pdf_url": "http://arxiv.org/pdf/2404.08837v2",
    "published_date": "2024-04-12 22:46:37 UTC",
    "updated_date": "2024-10-14 14:05:22 UTC"
  },
  {
    "arxiv_id": "2404.08836v1",
    "title": "BERT-LSH: Reducing Absolute Compute For Attention",
    "authors": [
      "Zezheng Li",
      "Kingston Yip"
    ],
    "abstract": "This study introduces a novel BERT-LSH model that incorporates Locality\nSensitive Hashing (LSH) to approximate the attention mechanism in the BERT\narchitecture. We examine the computational efficiency and performance of this\nmodel compared to a standard baseline BERT model. Our findings reveal that\nBERT-LSH significantly reduces computational demand for the self-attention\nlayer while unexpectedly outperforming the baseline model in pretraining and\nfine-tuning tasks. These results suggest that the LSH-based attention mechanism\nnot only offers computational advantages but also may enhance the model's\nability to generalize from its training data. For more information, visit our\nGitHub repository: https://github.com/leo4life2/algoml-final",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.08836v1",
    "published_date": "2024-04-12 22:35:00 UTC",
    "updated_date": "2024-04-12 22:35:00 UTC"
  },
  {
    "arxiv_id": "2405.02316v1",
    "title": "A Cloud-Edge Framework for Energy-Efficient Event-Driven Control: An Integration of Online Supervised Learning, Spiking Neural Networks and Local Plasticity Rules",
    "authors": [
      "Reza Ahmadvand",
      "Sarah Safura Sharif",
      "Yaser Mike Banad"
    ],
    "abstract": "This paper presents a novel cloud-edge framework for addressing computational\nand energy constraints in complex control systems. Our approach centers around\na learning-based controller using Spiking Neural Networks (SNN) on physical\nplants. By integrating a biologically plausible learning method with local\nplasticity rules, we harness the efficiency, scalability, and low latency of\nSNNs. This design replicates control signals from a cloud-based controller\ndirectly on the plant, reducing the need for constant plant-cloud\ncommunication. The plant updates weights only when errors surpass predefined\nthresholds, ensuring efficiency and robustness in various conditions. Applied\nto linear workbench systems and satellite rendezvous scenarios, including\nobstacle avoidance, our architecture dramatically lowers normalized tracking\nerror by 96% with increased network size. The event-driven nature of SNNs\nminimizes energy consumption, utilizing only about 111 nJ (0.3% of conventional\ncomputing requirements). The results demonstrate the system's adjustment to\nchanging work environments and its efficient use of computational and energy\nresources, with a moderate increase in energy consumption of 27.2% and 37% for\nstatic and dynamic obstacles, respectively, compared to non-obstacle scenarios.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.LG",
      "cs.NE",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "13 pages, 19 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.02316v1",
    "published_date": "2024-04-12 22:34:17 UTC",
    "updated_date": "2024-04-12 22:34:17 UTC"
  },
  {
    "arxiv_id": "2404.08828v1",
    "title": "Hindsight PRIORs for Reward Learning from Human Preferences",
    "authors": [
      "Mudit Verma",
      "Katherine Metcalf"
    ],
    "abstract": "Preference based Reinforcement Learning (PbRL) removes the need to hand\nspecify a reward function by learning a reward from preference feedback over\npolicy behaviors. Current approaches to PbRL do not address the credit\nassignment problem inherent in determining which parts of a behavior most\ncontributed to a preference, which result in data intensive approaches and\nsubpar reward functions. We address such limitations by introducing a credit\nassignment strategy (Hindsight PRIOR) that uses a world model to approximate\nstate importance within a trajectory and then guides rewards to be proportional\nto state importance through an auxiliary predicted return redistribution\nobjective. Incorporating state importance into reward learning improves the\nspeed of policy learning, overall policy performance, and reward recovery on\nboth locomotion and manipulation tasks. For example, Hindsight PRIOR recovers\non average significantly (p<0.05) more reward on MetaWorld (20%) and DMC (15%).\nThe performance gains and our ablations demonstrate the benefits even a simple\ncredit assignment strategy can have on reward learning and that state\nimportance in forward dynamics prediction is a strong proxy for a state's\ncontribution to a preference decision. Code repository can be found at\nhttps://github.com/apple/ml-rlhf-hindsight-prior.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "International Conference on Learning Representations, 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.08828v1",
    "published_date": "2024-04-12 21:59:42 UTC",
    "updated_date": "2024-04-12 21:59:42 UTC"
  },
  {
    "arxiv_id": "2404.08825v2",
    "title": "Inverse Kinematics for Neuro-Robotic Grasping with Humanoid Embodied Agents",
    "authors": [
      "Jan-Gerrit Habekost",
      "Connor Gäde",
      "Philipp Allgeuer",
      "Stefan Wermter"
    ],
    "abstract": "This paper introduces a novel zero-shot motion planning method that allows\nusers to quickly design smooth robot motions in Cartesian space. A B\\'ezier\ncurve-based Cartesian plan is transformed into a joint space trajectory by our\nneuro-inspired inverse kinematics (IK) method CycleIK, for which we enable\nplatform independence by scaling it to arbitrary robot designs. The motion\nplanner is evaluated on the physical hardware of the two humanoid robots NICO\nand NICOL in a human-in-the-loop grasping scenario. Our method is deployed with\nan embodied agent that is a large language model (LLM) at its core. We\ngeneralize the embodied agent, that was introduced for NICOL, to also embody\nNICO. The agent can execute a discrete set of physical actions and allows the\nuser to verbally instruct various different robots. We contribute a grasping\nprimitive to its action space that allows for precise manipulation of household\nobjects. The updated CycleIK method is compared to popular numerical IK solvers\nand state-of-the-art neural IK methods in simulation and is shown to be\ncompetitive with or outperform all evaluated methods when the algorithm runtime\nis very short. The grasping primitive is evaluated on both NICOL and NICO\nrobots with a reported grasp success of 72% to 82% for each robot,\nrespectively.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Published at IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.08825v2",
    "published_date": "2024-04-12 21:42:34 UTC",
    "updated_date": "2024-11-06 21:13:33 UTC"
  },
  {
    "arxiv_id": "2404.10534v2",
    "title": "Into the Fog: Evaluating Robustness of Multiple Object Tracking",
    "authors": [
      "Nadezda Kirillova",
      "M. Jehanzeb Mirza",
      "Horst Bischof",
      "Horst Possegger"
    ],
    "abstract": "State-of-the-art Multiple Object Tracking (MOT) approaches have shown\nremarkable performance when trained and evaluated on current benchmarks.\nHowever, these benchmarks primarily consist of clear weather scenarios,\noverlooking adverse atmospheric conditions such as fog, haze, smoke and dust.\nAs a result, the robustness of trackers against these challenging conditions\nremains underexplored. To address this gap, we introduce physics-based\nvolumetric fog simulation method for arbitrary MOT datasets, utilizing\nframe-by-frame monocular depth estimation and a fog formation optical model. We\nenhance our simulation by rendering both homogeneous and heterogeneous fog and\npropose to use the dark channel prior method to estimate atmospheric light,\nshowing promising results even in night and indoor scenes. We present the\nleading benchmark MOTChallenge (third release) augmented with fog (smoke for\nindoor scenes) of various intensities and conduct a comprehensive evaluation of\nMOT methods, revealing their limitations under fog and fog-like challenges.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.10534v2",
    "published_date": "2024-04-12 21:41:50 UTC",
    "updated_date": "2024-11-13 14:36:47 UTC"
  },
  {
    "arxiv_id": "2404.10789v1",
    "title": "PASA: Attack Agnostic Unsupervised Adversarial Detection using Prediction & Attribution Sensitivity Analysis",
    "authors": [
      "Dipkamal Bhusal",
      "Md Tanvirul Alam",
      "Monish K. Veerabhadran",
      "Michael Clifford",
      "Sara Rampazzi",
      "Nidhi Rastogi"
    ],
    "abstract": "Deep neural networks for classification are vulnerable to adversarial\nattacks, where small perturbations to input samples lead to incorrect\npredictions. This susceptibility, combined with the black-box nature of such\nnetworks, limits their adoption in critical applications like autonomous\ndriving. Feature-attribution-based explanation methods provide relevance of\ninput features for model predictions on input samples, thus explaining model\ndecisions. However, we observe that both model predictions and feature\nattributions for input samples are sensitive to noise. We develop a practical\nmethod for this characteristic of model prediction and feature attribution to\ndetect adversarial samples. Our method, PASA, requires the computation of two\ntest statistics using model prediction and feature attribution and can reliably\ndetect adversarial samples using thresholds learned from benign samples. We\nvalidate our lightweight approach by evaluating the performance of PASA on\nvarying strengths of FGSM, PGD, BIM, and CW attacks on multiple image and\nnon-image datasets. On average, we outperform state-of-the-art statistical\nunsupervised adversarial detectors on CIFAR-10 and ImageNet by 14\\% and 35\\%\nROC-AUC scores, respectively. Moreover, our approach demonstrates competitive\nperformance even when an adversary is aware of the defense mechanism.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "9th IEEE European Symposium on Security and Privacy",
    "pdf_url": "http://arxiv.org/pdf/2404.10789v1",
    "published_date": "2024-04-12 21:22:21 UTC",
    "updated_date": "2024-04-12 21:22:21 UTC"
  },
  {
    "arxiv_id": "2404.08814v2",
    "title": "E3: Ensemble of Expert Embedders for Adapting Synthetic Image Detectors to New Generators Using Limited Data",
    "authors": [
      "Aref Azizpour",
      "Tai D. Nguyen",
      "Manil Shrestha",
      "Kaidi Xu",
      "Edward Kim",
      "Matthew C. Stamm"
    ],
    "abstract": "As generative AI progresses rapidly, new synthetic image generators continue\nto emerge at a swift pace. Traditional detection methods face two main\nchallenges in adapting to these generators: the forensic traces of synthetic\nimages from new techniques can vastly differ from those learned during\ntraining, and access to data for these new generators is often limited. To\naddress these issues, we introduce the Ensemble of Expert Embedders (E3), a\nnovel continual learning framework for updating synthetic image detectors. E3\nenables the accurate detection of images from newly emerged generators using\nminimal training data. Our approach does this by first employing transfer\nlearning to develop a suite of expert embedders, each specializing in the\nforensic traces of a specific generator. Then, all embeddings are jointly\nanalyzed by an Expert Knowledge Fusion Network to produce accurate and reliable\ndetection decisions. Our experiments demonstrate that E3 outperforms existing\ncontinual learning methods, including those developed specifically for\nsynthetic image detection.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "11 pages, 4 figures, To be published in CVPRWMF24",
    "pdf_url": "http://arxiv.org/pdf/2404.08814v2",
    "published_date": "2024-04-12 21:14:20 UTC",
    "updated_date": "2024-04-16 14:17:51 UTC"
  },
  {
    "arxiv_id": "2404.08811v2",
    "title": "Reducing the Barriers to Entry for Foundation Model Training",
    "authors": [
      "Paolo Faraboschi",
      "Ellis Giles",
      "Justin Hotard",
      "Konstanty Owczarek",
      "Andrew Wheeler"
    ],
    "abstract": "The world has recently witnessed an unprecedented acceleration in demands for\nMachine Learning and Artificial Intelligence applications. This spike in demand\nhas imposed tremendous strain on the underlying technology stack in supply\nchain, GPU-accelerated hardware, software, datacenter power density, and energy\nconsumption. If left on the current technological trajectory, future demands\nshow insurmountable spending trends, further limiting market players, stifling\ninnovation, and widening the technology gap. To address these challenges, we\npropose a fundamental change in the AI training infrastructure throughout the\ntechnology ecosystem. The changes require advancements in supercomputing and\nnovel AI training approaches, from high-end software to low-level hardware,\nmicroprocessor, and chip design, while advancing the energy efficiency required\nby a sustainable infrastructure. This paper presents the analytical framework\nthat quantitatively highlights the challenges and points to the opportunities\nto reduce the barriers to entry for training large language models.",
    "categories": [
      "cs.ET",
      "cs.AI",
      "cs.AR",
      "cs.LG"
    ],
    "primary_category": "cs.ET",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.08811v2",
    "published_date": "2024-04-12 20:58:25 UTC",
    "updated_date": "2024-10-14 17:03:06 UTC"
  },
  {
    "arxiv_id": "2404.14418v1",
    "title": "Mitigating Cascading Effects in Large Adversarial Graph Environments",
    "authors": [
      "James D. Cunningham",
      "Conrad S. Tucker"
    ],
    "abstract": "A significant amount of society's infrastructure can be modeled using graph\nstructures, from electric and communication grids, to traffic networks, to\nsocial networks. Each of these domains are also susceptible to the cascading\nspread of negative impacts, whether this be overloaded devices in the power\ngrid or the reach of a social media post containing misinformation. The\npotential harm of a cascade is compounded when considering a malicious attack\nby an adversary that is intended to maximize the cascading impact. However, by\nexploiting knowledge of the cascading dynamics, targets with the largest\ncascading impact can be preemptively prioritized for defense, and the damage an\nadversary can inflict can be mitigated. While game theory provides tools for\nfinding an optimal preemptive defense strategy, existing methods struggle to\nscale to the context of large graph environments because of the combinatorial\nexplosion of possible actions that occurs when the attacker and defender can\neach choose multiple targets in the graph simultaneously. The proposed method\nenables a data-driven deep learning approach that uses multi-node\nrepresentation learning and counterfactual data augmentation to generalize to\nthe full combinatorial action space by training on a variety of small\nrestricted subsets of the action space. We demonstrate through experiments that\nthe proposed method is capable of identifying defense strategies that are less\nexploitable than SOTA methods for large graphs, while still being able to\nproduce strategies near the Nash equilibrium for small-scale scenarios for\nwhich it can be computed. Moreover, the proposed method demonstrates superior\nprediction accuracy on a validation set of unseen cascades compared to other\ndeep learning approaches.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.SI",
    "comment": "10 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.14418v1",
    "published_date": "2024-04-12 20:23:02 UTC",
    "updated_date": "2024-04-12 20:23:02 UTC"
  },
  {
    "arxiv_id": "2404.08799v1",
    "title": "Semantic Approach to Quantifying the Consistency of Diffusion Model Image Generation",
    "authors": [
      "Brinnae Bent"
    ],
    "abstract": "In this study, we identify the need for an interpretable, quantitative score\nof the repeatability, or consistency, of image generation in diffusion models.\nWe propose a semantic approach, using a pairwise mean CLIP (Contrastive\nLanguage-Image Pretraining) score as our semantic consistency score. We applied\nthis metric to compare two state-of-the-art open-source image generation\ndiffusion models, Stable Diffusion XL and PixArt-{\\alpha}, and we found\nstatistically significant differences between the semantic consistency scores\nfor the models. Agreement between the Semantic Consistency Score selected model\nand aggregated human annotations was 94%. We also explored the consistency of\nSDXL and a LoRA-fine-tuned version of SDXL and found that the fine-tuned model\nhad significantly higher semantic consistency in generated images. The Semantic\nConsistency Score proposed here offers a measure of image generation alignment,\nfacilitating the evaluation of model architectures for specific tasks and\naiding in informed decision-making regarding model selection.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to 2024 CVPR 3rd Explainable AI for Computer Vision (XAI4CV)\n  Workshop",
    "pdf_url": "http://arxiv.org/pdf/2404.08799v1",
    "published_date": "2024-04-12 20:16:03 UTC",
    "updated_date": "2024-04-12 20:16:03 UTC"
  },
  {
    "arxiv_id": "2404.10788v1",
    "title": "The Path To Autonomous Cyber Defense",
    "authors": [
      "Sean Oesch",
      "Phillipe Austria",
      "Amul Chaulagain",
      "Brian Weber",
      "Cory Watson",
      "Matthew Dixson",
      "Amir Sadovnik"
    ],
    "abstract": "Defenders are overwhelmed by the number and scale of attacks against their\nnetworks.This problem will only be exacerbated as attackers leverage artificial\nintelligence to automate their workflows. We propose a path to autonomous cyber\nagents able to augment defenders by automating critical steps in the cyber\ndefense life cycle.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "9 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.10788v1",
    "published_date": "2024-04-12 19:51:45 UTC",
    "updated_date": "2024-04-12 19:51:45 UTC"
  },
  {
    "arxiv_id": "2404.08791v2",
    "title": "Expectation Alignment: Handling Reward Misspecification in the Presence of Expectation Mismatch",
    "authors": [
      "Malek Mechergui",
      "Sarath Sreedharan"
    ],
    "abstract": "Detecting and handling misspecified objectives, such as reward functions, has\nbeen widely recognized as one of the central challenges within the domain of\nArtificial Intelligence (AI) safety research. However, even with the\nrecognition of the importance of this problem, we are unaware of any works that\nattempt to provide a clear definition for what constitutes (a) misspecified\nobjectives and (b) successfully resolving such misspecifications. In this work,\nwe use the theory of mind, i.e., the human user's beliefs about the AI agent,\nas a basis to develop a formal explanatory framework called Expectation\nAlignment (EAL) to understand the objective misspecification and its causes.\nOur EAL framework not only acts as an explanatory framework for existing works\nbut also provides us with concrete insights into the limitations of existing\nmethods to handle reward misspecification and novel solution strategies. We use\nthese insights to propose a new interactive algorithm that uses the specified\nreward to infer potential user expectations about the system behavior. We show\nhow one can efficiently implement this algorithm by mapping the inference\nproblem into linear programs. We evaluate our method on a set of standard\nMarkov Decision Process (MDP) benchmarks.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.08791v2",
    "published_date": "2024-04-12 19:43:37 UTC",
    "updated_date": "2024-10-31 02:34:00 UTC"
  },
  {
    "arxiv_id": "2404.08789v1",
    "title": "Differentiable and Stable Long-Range Tracking of Multiple Posterior Modes",
    "authors": [
      "Ali Younis",
      "Erik Sudderth"
    ],
    "abstract": "Particle filters flexibly represent multiple posterior modes\nnonparametrically, via a collection of weighted samples, but have classically\nbeen applied to tracking problems with known dynamics and observation\nlikelihoods. Such generative models may be inaccurate or unavailable for\nhigh-dimensional observations like images. We instead leverage training data to\ndiscriminatively learn particle-based representations of uncertainty in latent\nobject states, conditioned on arbitrary observations via deep neural network\nencoders. While prior discriminative particle filters have used heuristic\nrelaxations of discrete particle resampling, or biased learning by truncating\ngradients at resampling steps, we achieve unbiased and low-variance gradient\nestimates by representing posteriors as continuous mixture densities. Our\ntheory and experiments expose dramatic failures of existing\nreparameterization-based estimators for mixture gradients, an issue we address\nvia an importance-sampling gradient estimator. Unlike standard recurrent neural\nnetworks, our mixture density particle filter represents multimodal uncertainty\nin continuous latent states, improving accuracy and robustness. On a range of\nchallenging tracking and robot localization problems, our approach achieves\ndramatic improvements in accuracy, while also showing much greater stability\nacross multiple training runs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "Neurips 2023",
    "pdf_url": "http://arxiv.org/pdf/2404.08789v1",
    "published_date": "2024-04-12 19:33:52 UTC",
    "updated_date": "2024-04-12 19:33:52 UTC"
  },
  {
    "arxiv_id": "2404.19675v1",
    "title": "Deep Learning for Educational Data Science",
    "authors": [
      "Juan D. Pinto",
      "Luc Paquette"
    ],
    "abstract": "With the ever-growing presence of deep artificial neural networks in every\nfacet of modern life, a growing body of researchers in educational data science\n-- a field consisting of various interrelated research communities -- have\nturned their attention to leveraging these powerful algorithms within the\ndomain of education. Use cases range from advanced knowledge tracing models\nthat can leverage open-ended student essays or snippets of code to automatic\naffect and behavior detectors that can identify when a student is frustrated or\naimlessly trying to solve problems unproductively -- and much more. This\nchapter provides a brief introduction to deep learning, describes some of its\nadvantages and limitations, presents a survey of its many uses in education,\nand discusses how it may further come to shape the field of educational data\nscience.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "18 pages. To be published in Trust and Inclusion in AI-Mediated\n  Education: Where Human Learning Meets Learning Machines by Springer\n  International",
    "pdf_url": "http://arxiv.org/pdf/2404.19675v1",
    "published_date": "2024-04-12 19:17:14 UTC",
    "updated_date": "2024-04-12 19:17:14 UTC"
  },
  {
    "arxiv_id": "2404.08786v4",
    "title": "NeuroLGP-SM: Scalable Surrogate-Assisted Neuroevolution for Deep Neural Networks",
    "authors": [
      "Fergal Stapleton",
      "Edgar Galván"
    ],
    "abstract": "Evolutionary Algorithms (EAs) play a crucial role in the architectural\nconfiguration and training of Artificial Deep Neural Networks (DNNs), a process\nknown as neuroevolution. However, neuroevolution is hindered by its inherent\ncomputational expense, requiring multiple generations, a large population, and\nnumerous epochs. The most computationally intensive aspect lies in evaluating\nthe fitness function of a single candidate solution. To address this challenge,\nwe employ Surrogate-assisted EAs (SAEAs). While a few SAEAs approaches have\nbeen proposed in neuroevolution, none have been applied to truly large DNNs due\nto issues like intractable information usage. In this work, drawing inspiration\nfrom Genetic Programming semantics, we use phenotypic distance vectors,\noutputted from DNNs, alongside Kriging Partial Least Squares (KPLS), an\napproach that is effective in handling these large vectors, making them\nsuitable for search. Our proposed approach, named Neuro-Linear Genetic\nProgramming surrogate model (NeuroLGP-SM), efficiently and accurately estimates\nDNN fitness without the need for complete evaluations. NeuroLGP-SM demonstrates\ncompetitive or superior results compared to 12 other methods, including\nNeuroLGP without SM, convolutional neural networks, support vector machines,\nand autoencoders. Additionally, it is worth noting that NeuroLGP-SM is 25% more\nenergy-efficient than its NeuroLGP counterpart. This efficiency advantage adds\nto the overall appeal of our proposed NeuroLGP-SM in optimising the\nconfiguration of large DNNs.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "published in IEEE Congress on Evolutionary Computation (CEC) (CEC\n  2024), Yokohama, Japan, 8 pages, 5 figures, 2 tables (EDIT: added DOI)",
    "pdf_url": "http://arxiv.org/pdf/2404.08786v4",
    "published_date": "2024-04-12 19:15:38 UTC",
    "updated_date": "2024-09-16 13:48:43 UTC"
  },
  {
    "arxiv_id": "2404.08760v4",
    "title": "The Generation Gap: Exploring Age Bias in the Value Systems of Large Language Models",
    "authors": [
      "Siyang Liu",
      "Trish Maturi",
      "Bowen Yi",
      "Siqi Shen",
      "Rada Mihalcea"
    ],
    "abstract": "We explore the alignment of values in Large Language Models (LLMs) with\nspecific age groups, leveraging data from the World Value Survey across\nthirteen categories. Through a diverse set of prompts tailored to ensure\nresponse robustness, we find a general inclination of LLM values towards\nyounger demographics, especially when compared to the US population. Although a\ngeneral inclination can be observed, we also found that this inclination toward\nyounger groups can be different across different value categories.\nAdditionally, we explore the impact of incorporating age identity information\nin prompts and observe challenges in mitigating value discrepancies with\ndifferent age cohorts. Our findings highlight the age bias in LLMs and provide\ninsights for future work. Materials for our analysis are available at \\url{\nhttps://github.com/MichiganNLP/Age-Bias-In-LLMs}",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "5 pages",
    "pdf_url": "http://arxiv.org/pdf/2404.08760v4",
    "published_date": "2024-04-12 18:36:20 UTC",
    "updated_date": "2024-10-15 09:10:09 UTC"
  },
  {
    "arxiv_id": "2404.08755v1",
    "title": "Training a Vision Language Model as Smartphone Assistant",
    "authors": [
      "Nicolai Dorka",
      "Janusz Marecki",
      "Ammar Anwar"
    ],
    "abstract": "Addressing the challenge of a digital assistant capable of executing a wide\narray of user tasks, our research focuses on the realm of instruction-based\nmobile device control. We leverage recent advancements in large language models\n(LLMs) and present a visual language model (VLM) that can fulfill diverse tasks\non mobile devices. Our model functions by interacting solely with the user\ninterface (UI). It uses the visual input from the device screen and mimics\nhuman-like interactions, encompassing gestures such as tapping and swiping.\nThis generality in the input and output space allows our agent to interact with\nany application on the device. Unlike previous methods, our model operates not\nonly on a single screen image but on vision-language sentences created from\nsequences of past screenshots along with corresponding actions. Evaluating our\nmethod on the challenging Android in the Wild benchmark demonstrates its\npromising efficacy and potential.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR 2024 workshop on Generative Models for Decision Making",
    "pdf_url": "http://arxiv.org/pdf/2404.08755v1",
    "published_date": "2024-04-12 18:28:44 UTC",
    "updated_date": "2024-04-12 18:28:44 UTC"
  },
  {
    "arxiv_id": "2404.08747v1",
    "title": "Observation-specific explanations through scattered data approximation",
    "authors": [
      "Valentina Ghidini",
      "Michael Multerer",
      "Jacopo Quizi",
      "Rohan Sen"
    ],
    "abstract": "This work introduces the definition of observation-specific explanations to\nassign a score to each data point proportional to its importance in the\ndefinition of the prediction process. Such explanations involve the\nidentification of the most influential observations for the black-box model of\ninterest. The proposed method involves estimating these explanations by\nconstructing a surrogate model through scattered data approximation utilizing\nthe orthogonal matching pursuit algorithm. The proposed approach is validated\non both simulated and real-world datasets.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "cs.NA",
      "math.NA"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.08747v1",
    "published_date": "2024-04-12 18:20:26 UTC",
    "updated_date": "2024-04-12 18:20:26 UTC"
  },
  {
    "arxiv_id": "2404.08634v2",
    "title": "Inheritune: Training Smaller Yet More Attentive Language Models",
    "authors": [
      "Sunny Sanyal",
      "Ravid Shwartz-Ziv",
      "Alexandros G. Dimakis",
      "Sujay Sanghavi"
    ],
    "abstract": "Large Language Models (LLMs) have achieved remarkable performance across\nvarious natural language processing tasks, primarily due to the transformer\narchitecture and its self-attention mechanism. However, we observe that in\nstandard decoder-style LLMs, attention matrices degenerate to single-column for\ndeeper layers. Layers in this state are unable to learn anything meaningful and\nmostly redundant; we refer to these as lazy layers. The goal of this paper is\nto train smaller models by eliminating this structural inefficiency without\ncompromising performance.\n  Motivated by this observation, we propose Inheritune, a simple yet effective\ntraining recipe for developing smaller, high-performing language models.\nSmaller models trained with Inheritune, inherit early transformer layers from a\nlarger pre-trained model, then retrain and progressively expand until they\nmatch or exceed the performance of the larger model. We demonstrate that\nInheritune enables the training of various sizes of GPT-2 models on datasets\nlike OpenWebText-9B and FineWeb_edu. Models trained with Inheritune, despite\nhaving significantly fewer layers, match or even surpass the performance of\ntheir larger counterparts. For instance, our 16-layer GPT-2 medium variant\nachieves comparable performance to the standard 24-layer GPT-2 medium model.\nCode is available at https://github.com/sanyalsunny111/LLM-Inheritune.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "25 pages, 13 figures, 10 tables",
    "pdf_url": "http://arxiv.org/pdf/2404.08634v2",
    "published_date": "2024-04-12 17:53:34 UTC",
    "updated_date": "2024-10-04 05:14:48 UTC"
  },
  {
    "arxiv_id": "2404.08630v1",
    "title": "A Conceptual Framework for Conversational Search and Recommendation: Conceptualizing Agent-Human Interactions During the Conversational Search Process",
    "authors": [
      "Leif Azzopardi",
      "Mateusz Dubiel",
      "Martin Halvey",
      "Jeffery Dalton"
    ],
    "abstract": "The conversational search task aims to enable a user to resolve information\nneeds via natural language dialogue with an agent. In this paper, we aim to\ndevelop a conceptual framework of the actions and intents of users and agents\nexplaining how these actions enable the user to explore the search space and\nresolve their information need. We outline the different actions and intents,\nbefore discussing key decision points in the conversation where the agent needs\nto decide how to steer the conversational search process to a successful and/or\nsatisfactory conclusion. Essentially, this paper provides a conceptualization\nof the conversational search process between an agent and user, which provides\na framework and a starting point for research, development and evaluation of\nconversational search agents.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.08630v1",
    "published_date": "2024-04-12 17:48:18 UTC",
    "updated_date": "2024-04-12 17:48:18 UTC"
  },
  {
    "arxiv_id": "2404.08627v2",
    "title": "Is ChatGPT Transforming Academics' Writing Style?",
    "authors": [
      "Mingmeng Geng",
      "Roberto Trotta"
    ],
    "abstract": "Based on one million arXiv papers submitted from May 2018 to January 2024, we\nassess the textual density of ChatGPT's writing style in their abstracts\nthrough a statistical analysis of word frequency changes. Our model is\ncalibrated and validated on a mixture of real abstracts and ChatGPT-modified\nabstracts (simulated data) after a careful noise analysis. The words used for\nestimation are not fixed but adaptive, including those with decreasing\nfrequency. We find that large language models (LLMs), represented by ChatGPT,\nare having an increasing impact on arXiv abstracts, especially in the field of\ncomputer science, where the fraction of LLM-style abstracts is estimated to be\napproximately 35%, if we take the responses of GPT-3.5 to one simple prompt,\n\"revise the following sentences\", as a baseline. We conclude with an analysis\nof both positive and negative aspects of the penetration of LLMs into\nacademics' writing style.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "19 pages",
    "pdf_url": "http://arxiv.org/pdf/2404.08627v2",
    "published_date": "2024-04-12 17:41:05 UTC",
    "updated_date": "2024-11-08 18:56:42 UTC"
  },
  {
    "arxiv_id": "2404.08611v2",
    "title": "Automatic Quantification of Serial PET/CT Images for Pediatric Hodgkin Lymphoma Patients Using a Longitudinally-Aware Segmentation Network",
    "authors": [
      "Xin Tie",
      "Muheon Shin",
      "Changhee Lee",
      "Scott B. Perlman",
      "Zachary Huemann",
      "Amy J. Weisman",
      "Sharon M. Castellino",
      "Kara M. Kelly",
      "Kathleen M. McCarten",
      "Adina L. Alazraki",
      "Junjie Hu",
      "Steve Y. Cho",
      "Tyler J. Bradshaw"
    ],
    "abstract": "$\\textbf{Purpose}$: Automatic quantification of longitudinal changes in PET\nscans for lymphoma patients has proven challenging, as residual disease in\ninterim-therapy scans is often subtle and difficult to detect. Our goal was to\ndevelop a longitudinally-aware segmentation network (LAS-Net) that can quantify\nserial PET/CT images for pediatric Hodgkin lymphoma patients.\n$\\textbf{Materials and Methods}$: This retrospective study included baseline\n(PET1) and interim (PET2) PET/CT images from 297 patients enrolled in two\nChildren's Oncology Group clinical trials (AHOD1331 and AHOD0831). LAS-Net\nincorporates longitudinal cross-attention, allowing relevant features from PET1\nto inform the analysis of PET2. Model performance was evaluated using Dice\ncoefficients for PET1 and detection F1 scores for PET2. Additionally, we\nextracted and compared quantitative PET metrics, including metabolic tumor\nvolume (MTV) and total lesion glycolysis (TLG) in PET1, as well as qPET and\n$\\Delta$SUVmax in PET2, against physician measurements. We quantified their\nagreement using Spearman's $\\rho$ correlations and employed bootstrap\nresampling for statistical analysis. $\\textbf{Results}$: LAS-Net detected\nresidual lymphoma in PET2 with an F1 score of 0.606 (precision/recall:\n0.615/0.600), outperforming all comparator methods (P<0.01). For baseline\nsegmentation, LAS-Net achieved a mean Dice score of 0.772. In PET\nquantification, LAS-Net's measurements of qPET, $\\Delta$SUVmax, MTV and TLG\nwere strongly correlated with physician measurements, with Spearman's $\\rho$ of\n0.78, 0.80, 0.93 and 0.96, respectively. The performance remained high, with a\nslight decrease, in an external testing cohort. $\\textbf{Conclusion}$: LAS-Net\ndemonstrated significant improvements in quantifying PET metrics across serial\nscans, highlighting the value of longitudinal awareness in evaluating\nmulti-time-point imaging datasets.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "physics.med-ph"
    ],
    "primary_category": "cs.CV",
    "comment": "There are 6 figures and 4 tables in the main text. The supplementary\n  material is appended to the main text",
    "pdf_url": "http://arxiv.org/pdf/2404.08611v2",
    "published_date": "2024-04-12 17:20:57 UTC",
    "updated_date": "2024-10-01 00:14:32 UTC"
  },
  {
    "arxiv_id": "2404.08727v1",
    "title": "Can LLMs substitute SQL? Comparing Resource Utilization of Querying LLMs versus Traditional Relational Databases",
    "authors": [
      "Xiang Zhang",
      "Khatoon Khedri",
      "Reza Rawassizadeh"
    ],
    "abstract": "Large Language Models (LLMs) can automate or substitute different types of\ntasks in the software engineering process. This study evaluates the resource\nutilization and accuracy of LLM in interpreting and executing natural language\nqueries against traditional SQL within relational database management systems.\nWe empirically examine the resource utilization and accuracy of nine LLMs\nvarying from 7 to 34 Billion parameters, including Llama2 7B, Llama2 13B,\nMistral, Mixtral, Optimus-7B, SUS-chat-34B, platypus-yi-34b,\nNeuralHermes-2.5-Mistral-7B and Starling-LM-7B-alpha, using a small transaction\ndataset. Our findings indicate that using LLMs for database queries incurs\nsignificant energy overhead (even small and quantized models), making it an\nenvironmentally unfriendly approach. Therefore, we advise against replacing\nrelational databases with LLMs due to their substantial resource utilization.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.CL",
      "68-04",
      "H.2.m"
    ],
    "primary_category": "cs.DB",
    "comment": "13 pages, 2 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2404.08727v1",
    "published_date": "2024-04-12 16:44:28 UTC",
    "updated_date": "2024-04-12 16:44:28 UTC"
  },
  {
    "arxiv_id": "2404.08590v2",
    "title": "Vision-Aware Text Features in Referring Image Segmentation: From Object Understanding to Context Understanding",
    "authors": [
      "Hai Nguyen-Truong",
      "E-Ro Nguyen",
      "Tuan-Anh Vu",
      "Minh-Triet Tran",
      "Binh-Son Hua",
      "Sai-Kit Yeung"
    ],
    "abstract": "Referring image segmentation is a challenging task that involves generating\npixel-wise segmentation masks based on natural language descriptions. The\ncomplexity of this task increases with the intricacy of the sentences provided.\nExisting methods have relied mostly on visual features to generate the\nsegmentation masks while treating text features as supporting components.\nHowever, this under-utilization of text understanding limits the model's\ncapability to fully comprehend the given expressions. In this work, we propose\na novel framework that specifically emphasizes object and context comprehension\ninspired by human cognitive processes through Vision-Aware Text Features.\nFirstly, we introduce a CLIP Prior module to localize the main object of\ninterest and embed the object heatmap into the query initialization process.\nSecondly, we propose a combination of two components: Contextual Multimodal\nDecoder and Meaning Consistency Constraint, to further enhance the coherent and\nconsistent interpretation of language cues with the contextual understanding\nobtained from the image. Our method achieves significant performance\nimprovements on three benchmark datasets RefCOCO, RefCOCO+ and G-Ref. Project\npage: \\url{https://vatex.hkustvgd.com/}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This paper is accepted in WACV 2025",
    "pdf_url": "http://arxiv.org/pdf/2404.08590v2",
    "published_date": "2024-04-12 16:38:48 UTC",
    "updated_date": "2024-11-04 05:43:17 UTC"
  },
  {
    "arxiv_id": "2404.08589v1",
    "title": "Enhancing Visual Question Answering through Question-Driven Image Captions as Prompts",
    "authors": [
      "Övgü Özdemir",
      "Erdem Akagündüz"
    ],
    "abstract": "Visual question answering (VQA) is known as an AI-complete task as it\nrequires understanding, reasoning, and inferring about the vision and the\nlanguage content. Over the past few years, numerous neural architectures have\nbeen suggested for the VQA problem. However, achieving success in zero-shot VQA\nremains a challenge due to its requirement for advanced generalization and\nreasoning skills. This study explores the impact of incorporating image\ncaptioning as an intermediary process within the VQA pipeline. Specifically, we\nexplore the efficacy of utilizing image captions instead of images and\nleveraging large language models (LLMs) to establish a zero-shot setting. Since\nimage captioning is the most crucial step in this process, we compare the\nimpact of state-of-the-art image captioning models on VQA performance across\nvarious question types in terms of structure and semantics. We propose a\nstraightforward and efficient question-driven image captioning approach within\nthis pipeline to transfer contextual information into the question-answering\n(QA) model. This method involves extracting keywords from the question,\ngenerating a caption for each image-question pair using the keywords, and\nincorporating the question-driven caption into the LLM prompt. We evaluate the\nefficacy of using general-purpose and question-driven image captions in the VQA\npipeline. Our study highlights the potential of employing image captions and\nharnessing the capabilities of LLMs to achieve competitive performance on GQA\nunder the zero-shot setting. Our code is available at\n\\url{https://github.com/ovguyo/captions-in-VQA}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "The paper has been accepted for presentation at CVPR 2024 Workshop on\n  Prompting in Vision",
    "pdf_url": "http://arxiv.org/pdf/2404.08589v1",
    "published_date": "2024-04-12 16:35:23 UTC",
    "updated_date": "2024-04-12 16:35:23 UTC"
  },
  {
    "arxiv_id": "2404.08726v1",
    "title": "An Integrated Toolbox for Creating Neuromorphic Edge Applications",
    "authors": [
      "Lars Niedermeier",
      "Jeffrey L. Krichmar"
    ],
    "abstract": "Spiking Neural Networks (SNNs) and neuromorphic models are more efficient and\nhave more biological realism than the activation functions typically used in\ndeep neural networks, transformer models and generative AI. SNNs have local\nlearning rules, are able to learn on small data sets, and can adapt through\nneuromodulation. Although research has shown their advantages, there are still\nfew compelling practical applications, especially at the edge where sensors and\nactuators need to be processed in a timely fashion. One reason for this might\nbe that SNNs are much more challenging to understand, build, and operate due to\ntheir intrinsic properties. For instance, the mathematical foundation involves\ndifferential equations rather than basic activation functions. To address these\nchallenges, we have developed CARLsim++. It is an integrated toolbox that\nenables fast and easy creation of neuromorphic applications. It encapsulates\nthe mathematical intrinsics and low-level C++ programming by providing a\ngraphical user interface for users who do not have a background in software\nengineering but still want to create neuromorphic models. Developers can easily\nconfigure inputs and outputs to devices and robots. These can be accurately\nsimulated before deploying on physical devices. CARLsim++ can lead to rapid\ndevelopment of neuromorphic applications for simulation or edge processing.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "8 pages, 5 figures, NICE 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.08726v1",
    "published_date": "2024-04-12 16:34:55 UTC",
    "updated_date": "2024-04-12 16:34:55 UTC"
  },
  {
    "arxiv_id": "2404.08582v1",
    "title": "FashionFail: Addressing Failure Cases in Fashion Object Detection and Segmentation",
    "authors": [
      "Riza Velioglu",
      "Robin Chan",
      "Barbara Hammer"
    ],
    "abstract": "In the realm of fashion object detection and segmentation for online shopping\nimages, existing state-of-the-art fashion parsing models encounter limitations,\nparticularly when exposed to non-model-worn apparel and close-up shots. To\naddress these failures, we introduce FashionFail; a new fashion dataset with\ne-commerce images for object detection and segmentation. The dataset is\nefficiently curated using our novel annotation tool that leverages recent\nfoundation models. The primary objective of FashionFail is to serve as a test\nbed for evaluating the robustness of models. Our analysis reveals the\nshortcomings of leading models, such as Attribute-Mask R-CNN and Fashionformer.\nAdditionally, we propose a baseline approach using naive data augmentation to\nmitigate common failure cases and improve model robustness. Through this work,\nwe aim to inspire and support further research in fashion item detection and\nsegmentation for industrial applications. The dataset, annotation tool, code,\nand models are available at \\url{https://rizavelioglu.github.io/fashionfail/}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "to be published in 2024 International Joint Conference on Neural\n  Networks (IJCNN)",
    "pdf_url": "http://arxiv.org/pdf/2404.08582v1",
    "published_date": "2024-04-12 16:28:30 UTC",
    "updated_date": "2024-04-12 16:28:30 UTC"
  },
  {
    "arxiv_id": "2404.08579v1",
    "title": "Small Models Are (Still) Effective Cross-Domain Argument Extractors",
    "authors": [
      "William Gantt",
      "Aaron Steven White"
    ],
    "abstract": "Effective ontology transfer has been a major goal of recent work on event\nargument extraction (EAE). Two methods in particular -- question answering (QA)\nand template infilling (TI) -- have emerged as promising approaches to this\nproblem. However, detailed explorations of these techniques' ability to\nactually enable this transfer are lacking. In this work, we provide such a\nstudy, exploring zero-shot transfer using both techniques on six major EAE\ndatasets at both the sentence and document levels. Further, we challenge the\ngrowing reliance on LLMs for zero-shot extraction, showing that vastly smaller\nmodels trained on an appropriate source ontology can yield zero-shot\nperformance superior to that of GPT-3.5 or GPT-4.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL Rolling Review Short Paper",
    "pdf_url": "http://arxiv.org/pdf/2404.08579v1",
    "published_date": "2024-04-12 16:23:41 UTC",
    "updated_date": "2024-04-12 16:23:41 UTC"
  },
  {
    "arxiv_id": "2404.08570v1",
    "title": "Enhancing Autonomous Vehicle Training with Language Model Integration and Critical Scenario Generation",
    "authors": [
      "Hanlin Tian",
      "Kethan Reddy",
      "Yuxiang Feng",
      "Mohammed Quddus",
      "Yiannis Demiris",
      "Panagiotis Angeloudis"
    ],
    "abstract": "This paper introduces CRITICAL, a novel closed-loop framework for autonomous\nvehicle (AV) training and testing. CRITICAL stands out for its ability to\ngenerate diverse scenarios, focusing on critical driving situations that target\nspecific learning and performance gaps identified in the Reinforcement Learning\n(RL) agent. The framework achieves this by integrating real-world traffic\ndynamics, driving behavior analysis, surrogate safety measures, and an optional\nLarge Language Model (LLM) component. It is proven that the establishment of a\nclosed feedback loop between the data generation pipeline and the training\nprocess can enhance the learning rate during training, elevate overall system\nperformance, and augment safety resilience. Our evaluations, conducted using\nthe Proximal Policy Optimization (PPO) and the HighwayEnv simulation\nenvironment, demonstrate noticeable performance improvements with the\nintegration of critical case generation and LLM analysis, indicating CRITICAL's\npotential to improve the robustness of AV systems and streamline the generation\nof critical scenarios. This ultimately serves to hasten the development of AV\nagents, expand the general scope of RL training, and ameliorate validation\nefforts for AV safety.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "7 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.08570v1",
    "published_date": "2024-04-12 16:13:10 UTC",
    "updated_date": "2024-04-12 16:13:10 UTC"
  },
  {
    "arxiv_id": "2404.08561v2",
    "title": "IDD-X: A Multi-View Dataset for Ego-relative Important Object Localization and Explanation in Dense and Unstructured Traffic",
    "authors": [
      "Chirag Parikh",
      "Rohit Saluja",
      "C. V. Jawahar",
      "Ravi Kiran Sarvadevabhatla"
    ],
    "abstract": "Intelligent vehicle systems require a deep understanding of the interplay\nbetween road conditions, surrounding entities, and the ego vehicle's driving\nbehavior for safe and efficient navigation. This is particularly critical in\ndeveloping countries where traffic situations are often dense and unstructured\nwith heterogeneous road occupants. Existing datasets, predominantly geared\ntowards structured and sparse traffic scenarios, fall short of capturing the\ncomplexity of driving in such environments. To fill this gap, we present IDD-X,\na large-scale dual-view driving video dataset. With 697K bounding boxes, 9K\nimportant object tracks, and 1-12 objects per video, IDD-X offers comprehensive\nego-relative annotations for multiple important road objects covering 10\ncategories and 19 explanation label categories. The dataset also incorporates\nrearview information to provide a more complete representation of the driving\nenvironment. We also introduce custom-designed deep networks aimed at multiple\nimportant object localization and per-object explanation prediction. Overall,\nour dataset and introduced prediction models form the foundation for studying\nhow road conditions and surrounding entities affect driving behavior in complex\ntraffic situations.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at ICRA 2024; Project page: https://idd-x.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2404.08561v2",
    "published_date": "2024-04-12 16:00:03 UTC",
    "updated_date": "2024-04-23 19:19:35 UTC"
  },
  {
    "arxiv_id": "2404.08555v2",
    "title": "RLHF Deciphered: A Critical Analysis of Reinforcement Learning from Human Feedback for LLMs",
    "authors": [
      "Shreyas Chaudhari",
      "Pranjal Aggarwal",
      "Vishvak Murahari",
      "Tanmay Rajpurohit",
      "Ashwin Kalyan",
      "Karthik Narasimhan",
      "Ameet Deshpande",
      "Bruno Castro da Silva"
    ],
    "abstract": "State-of-the-art large language models (LLMs) have become indispensable tools\nfor various tasks. However, training LLMs to serve as effective assistants for\nhumans requires careful consideration. A promising approach is reinforcement\nlearning from human feedback (RLHF), which leverages human feedback to update\nthe model in accordance with human preferences and mitigate issues like\ntoxicity and hallucinations. Yet, an understanding of RLHF for LLMs is largely\nentangled with initial design choices that popularized the method and current\nresearch focuses on augmenting those choices rather than fundamentally\nimproving the framework. In this paper, we analyze RLHF through the lens of\nreinforcement learning principles to develop an understanding of its\nfundamentals, dedicating substantial focus to the core component of RLHF -- the\nreward model. Our study investigates modeling choices, caveats of function\napproximation, and their implications on RLHF training algorithms, highlighting\nthe underlying assumptions made about the expressivity of reward. Our analysis\nimproves the understanding of the role of reward models and methods for their\ntraining, concurrently revealing limitations of the current methodology. We\ncharacterize these limitations, including incorrect generalization, model\nmisspecification, and the sparsity of feedback, along with their impact on the\nperformance of a language model. The discussion and analysis are substantiated\nby a categorical review of current literature, serving as a reference for\nresearchers and practitioners to understand the challenges of RLHF and build\nupon existing efforts.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.08555v2",
    "published_date": "2024-04-12 15:54:15 UTC",
    "updated_date": "2024-04-16 00:22:16 UTC"
  },
  {
    "arxiv_id": "2404.16054v2",
    "title": "LlamaTouch: A Faithful and Scalable Testbed for Mobile UI Task Automation",
    "authors": [
      "Li Zhang",
      "Shihe Wang",
      "Xianqing Jia",
      "Zhihan Zheng",
      "Yunhe Yan",
      "Longxi Gao",
      "Yuanchun Li",
      "Mengwei Xu"
    ],
    "abstract": "The emergent large language/multimodal models facilitate the evolution of\nmobile agents, especially in mobile UI task automation. However, existing\nevaluation approaches, which rely on human validation or established datasets\nto compare agent-predicted actions with predefined action sequences, are\nunscalable and unfaithful. To overcome these limitations, this paper presents\nLlamaTouch, a testbed for on-device mobile UI task execution and faithful,\nscalable task evaluation. By observing that the task execution process only\ntransfers UI states, LlamaTouch employs a novel evaluation approach that only\nassesses whether an agent traverses all manually annotated, essential\napplication/system states. LlamaTouch comprises three key techniques: (1)\nOn-device task execution that enables mobile agents to interact with realistic\nmobile environments for task execution. (2) Fine-grained UI component\nannotation that merges pixel-level screenshots and textual screen hierarchies\nto explicitly identify and precisely annotate essential UI components with a\nrich set of designed annotation primitives. (3) A multi-level application state\nmatching algorithm that utilizes exact and fuzzy matching to accurately detect\ncritical information in each screen, even with unpredictable UI layout/content\ndynamics. LlamaTouch currently incorporates four mobile agents and 496 tasks,\nencompassing both tasks in the widely-used datasets and our self-constructed\nones to cover more diverse mobile applications. Evaluation results demonstrate\nLlamaTouch's high faithfulness of evaluation in real-world mobile environments\nand its better scalability than human validation. LlamaTouch also enables easy\ntask annotation and integration of new mobile agents. Code and dataset are\npublicly available at https://github.com/LlamaTouch/LlamaTouch.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted at ACM UIST 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.16054v2",
    "published_date": "2024-04-12 15:39:09 UTC",
    "updated_date": "2024-08-02 13:49:32 UTC"
  },
  {
    "arxiv_id": "2404.08544v2",
    "title": "Analyzing Decades-Long Environmental Changes in Namibia Using Archival Aerial Photography and Deep Learning",
    "authors": [
      "Girmaw Abebe Tadesse",
      "Caleb Robinson",
      "Gilles Quentin Hacheme",
      "Akram Zaytar",
      "Rahul Dodhia",
      "Tsering Wangyal Shawa",
      "Juan M. Lavista Ferres",
      "Emmanuel H. Kreike"
    ],
    "abstract": "This study explores object detection in historical aerial photographs of\nNamibia to identify long-term environmental changes. Specifically, we aim to\nidentify key objects -- Waterholes, Omuti homesteads, and Big trees -- around\nOshikango in Namibia using sub-meter gray-scale aerial imagery from 1943 and\n1972. In this work, we propose a workflow for analyzing historical aerial\nimagery using a deep semantic segmentation model on sparse hand-labels. To this\nend, we employ a number of strategies including class-weighting,\npseudo-labeling and empirical p-value-based filtering to balance skewed and\nsparse representations of objects in the ground truth data. Results demonstrate\nthe benefits of these different training strategies resulting in an average\n$F_1=0.661$ and $F_1=0.755$ over the three objects of interest for the 1943 and\n1972 imagery, respectively. We also identified that the average size of\nWaterhole and Big trees increased while the average size of Omuti homesteads\ndecreased between 1943 and 1972 reflecting some of the local effects of the\nmassive post-Second World War economic, agricultural, demographic, and\nenvironmental changes. This work also highlights the untapped potential of\nhistorical aerial photographs in understanding long-term environmental changes\nbeyond Namibia (and Africa). With the lack of adequate satellite technology in\nthe past, archival aerial photography offers a great alternative to uncover\ndecades-long environmental changes.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.08544v2",
    "published_date": "2024-04-12 15:37:53 UTC",
    "updated_date": "2024-04-21 10:24:45 UTC"
  },
  {
    "arxiv_id": "2404.08543v1",
    "title": "Memory Traces: Are Transformers Tulving Machines?",
    "authors": [
      "Jean-Marie Chauvet"
    ],
    "abstract": "Memory traces--changes in the memory system that result from the perception\nand encoding of an event--were measured in pioneering studies by Endel Tulving\nand Michael J. Watkins in 1975. These and further experiments informed the\nmaturation of Tulving's memory model, from the GAPS (General Abstract\nProcessing System} to the SPI (Serial-Parallel Independent) model. Having\ncurrent top of the line LLMs revisit the original Tulving-Watkins tests may\nhelp in assessing whether foundation models completely instantiate or not this\nclass of psychological models.",
    "categories": [
      "cs.AI",
      "I.2.4"
    ],
    "primary_category": "cs.AI",
    "comment": "14 pages, 1 figure and 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2404.08543v1",
    "published_date": "2024-04-12 15:37:35 UTC",
    "updated_date": "2024-04-12 15:37:35 UTC"
  },
  {
    "arxiv_id": "2404.08523v1",
    "title": "Advancing Forest Fire Prevention: Deep Reinforcement Learning for Effective Firebreak Placement",
    "authors": [
      "Lucas Murray",
      "Tatiana Castillo",
      "Jaime Carrasco",
      "Andrés Weintraub",
      "Richard Weber",
      "Isaac Martín de Diego",
      "José Ramón González",
      "Jordi García-Gonzalo"
    ],
    "abstract": "Over the past decades, the increase in both frequency and intensity of\nlarge-scale wildfires due to climate change has emerged as a significant\nnatural threat. The pressing need to design resilient landscapes capable of\nwithstanding such disasters has become paramount, requiring the development of\nadvanced decision-support tools. Existing methodologies, including Mixed\nInteger Programming, Stochastic Optimization, and Network Theory, have proven\neffective but are hindered by computational demands, limiting their\napplicability.\n  In response to this challenge, we propose using artificial intelligence\ntechniques, specifically Deep Reinforcement Learning, to address the complex\nproblem of firebreak placement in the landscape. We employ value-function based\napproaches like Deep Q-Learning, Double Deep Q-Learning, and Dueling Double\nDeep Q-Learning. Utilizing the Cell2Fire fire spread simulator combined with\nConvolutional Neural Networks, we have successfully implemented a computational\nagent capable of learning firebreak locations within a forest environment,\nachieving good results.\n  Furthermore, we incorporate a pre-training loop, initially teaching our agent\nto mimic a heuristic-based algorithm and observe that it consistently exceeds\nthe performance of these solutions. Our findings underscore the immense\npotential of Deep Reinforcement Learning for operational research challenges,\nespecially in fire prevention. Our approach demonstrates convergence with\nhighly favorable results in problem instances as large as 40 x 40 cells,\nmarking a significant milestone in applying Reinforcement Learning to this\ncritical issue.\n  To the best of our knowledge, this study represents a pioneering effort in\nusing Reinforcement Learning to address the aforementioned problem, offering\npromising perspectives in fire prevention and landscape management",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "20 pages, 15 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.08523v1",
    "published_date": "2024-04-12 15:10:57 UTC",
    "updated_date": "2024-04-12 15:10:57 UTC"
  },
  {
    "arxiv_id": "2404.08517v1",
    "title": "Online Safety Analysis for LLMs: a Benchmark, an Assessment, and a Path Forward",
    "authors": [
      "Xuan Xie",
      "Jiayang Song",
      "Zhehua Zhou",
      "Yuheng Huang",
      "Da Song",
      "Lei Ma"
    ],
    "abstract": "While Large Language Models (LLMs) have seen widespread applications across\nnumerous fields, their limited interpretability poses concerns regarding their\nsafe operations from multiple aspects, e.g., truthfulness, robustness, and\nfairness. Recent research has started developing quality assurance methods for\nLLMs, introducing techniques such as offline detector-based or uncertainty\nestimation methods. However, these approaches predominantly concentrate on\npost-generation analysis, leaving the online safety analysis for LLMs during\nthe generation phase an unexplored area. To bridge this gap, we conduct in this\nwork a comprehensive evaluation of the effectiveness of existing online safety\nanalysis methods on LLMs. We begin with a pilot study that validates the\nfeasibility of detecting unsafe outputs in the early generation process.\nFollowing this, we establish the first publicly available benchmark of online\nsafety analysis for LLMs, including a broad spectrum of methods, models, tasks,\ndatasets, and evaluation metrics. Utilizing this benchmark, we extensively\nanalyze the performance of state-of-the-art online safety analysis methods on\nboth open-source and closed-source LLMs. This analysis reveals the strengths\nand weaknesses of individual methods and offers valuable insights into\nselecting the most appropriate method based on specific application scenarios\nand task requirements. Furthermore, we also explore the potential of using\nhybridization methods, i.e., combining multiple methods to derive a collective\nsafety conclusion, to enhance the efficacy of online safety analysis for LLMs.\nOur findings indicate a promising direction for the development of innovative\nand trustworthy quality assurance methodologies for LLMs, facilitating their\nreliable deployments across diverse domains.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.08517v1",
    "published_date": "2024-04-12 14:55:16 UTC",
    "updated_date": "2024-04-12 14:55:16 UTC"
  },
  {
    "arxiv_id": "2404.08513v1",
    "title": "Adversarial Imitation Learning via Boosting",
    "authors": [
      "Jonathan D. Chang",
      "Dhruv Sreenivas",
      "Yingbing Huang",
      "Kianté Brantley",
      "Wen Sun"
    ],
    "abstract": "Adversarial imitation learning (AIL) has stood out as a dominant framework\nacross various imitation learning (IL) applications, with Discriminator Actor\nCritic (DAC) (Kostrikov et al.,, 2019) demonstrating the effectiveness of\noff-policy learning algorithms in improving sample efficiency and scalability\nto higher-dimensional observations. Despite DAC's empirical success, the\noriginal AIL objective is on-policy and DAC's ad-hoc application of off-policy\ntraining does not guarantee successful imitation (Kostrikov et al., 2019;\n2020). Follow-up work such as ValueDICE (Kostrikov et al., 2020) tackles this\nissue by deriving a fully off-policy AIL objective. Instead in this work, we\ndevelop a novel and principled AIL algorithm via the framework of boosting.\nLike boosting, our new algorithm, AILBoost, maintains an ensemble of properly\nweighted weak learners (i.e., policies) and trains a discriminator that\nwitnesses the maximum discrepancy between the distributions of the ensemble and\nthe expert policy. We maintain a weighted replay buffer to represent the\nstate-action distribution induced by the ensemble, allowing us to train\ndiscriminators using the entire data collected so far. In the weighted replay\nbuffer, the contribution of the data from older policies are properly\ndiscounted with the weight computed based on the boosting framework.\nEmpirically, we evaluate our algorithm on both controller state-based and\npixel-based environments from the DeepMind Control Suite. AILBoost outperforms\nDAC on both types of environments, demonstrating the benefit of properly\nweighting replay buffer data for off-policy training. On state-based\nenvironments, DAC outperforms ValueDICE and IQ-Learn (Gary et al., 2021),\nachieving competitive performance with as little as one expert trajectory.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "19 pages, 7 figures, 4 tables, 3 algorithms, ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.08513v1",
    "published_date": "2024-04-12 14:53:36 UTC",
    "updated_date": "2024-04-12 14:53:36 UTC"
  },
  {
    "arxiv_id": "2404.08511v1",
    "title": "Leveraging Multi-AI Agents for Cross-Domain Knowledge Discovery",
    "authors": [
      "Shiva Aryal",
      "Tuyen Do",
      "Bisesh Heyojoo",
      "Sandeep Chataut",
      "Bichar Dip Shrestha Gurung",
      "Venkataramana Gadhamshetty",
      "Etienne Gnimpieba"
    ],
    "abstract": "In the rapidly evolving field of artificial intelligence, the ability to\nharness and integrate knowledge across various domains stands as a paramount\nchallenge and opportunity. This study introduces a novel approach to\ncross-domain knowledge discovery through the deployment of multi-AI agents,\neach specialized in distinct knowledge domains. These AI agents, designed to\nfunction as domain-specific experts, collaborate in a unified framework to\nsynthesize and provide comprehensive insights that transcend the limitations of\nsingle-domain expertise. By facilitating seamless interaction among these\nagents, our platform aims to leverage the unique strengths and perspectives of\neach, thereby enhancing the process of knowledge discovery and decision-making.\nWe present a comparative analysis of the different multi-agent workflow\nscenarios evaluating their performance in terms of efficiency, accuracy, and\nthe breadth of knowledge integration. Through a series of experiments involving\ncomplex, interdisciplinary queries, our findings demonstrate the superior\ncapability of domain specific multi-AI agent system in identifying and bridging\nknowledge gaps. This research not only underscores the significance of\ncollaborative AI in driving innovation but also sets the stage for future\nadvancements in AI-driven, cross-disciplinary research and application. Our\nmethods were evaluated on a small pilot data and it showed a trend we expected,\nif we increase the amount of data we custom train the agents, the trend is\nexpected to be more smooth.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.08511v1",
    "published_date": "2024-04-12 14:50:41 UTC",
    "updated_date": "2024-04-12 14:50:41 UTC"
  },
  {
    "arxiv_id": "2404.08501v1",
    "title": "Analyzing and Overcoming Local Optima in Complex Multi-Objective Optimization by Decomposition-Based Evolutionary Algorithms",
    "authors": [
      "Ting Dong",
      "Haoxin Wang",
      "Hengxi Zhang",
      "Wenbo Ding"
    ],
    "abstract": "When addressing the challenge of complex multi-objective optimization\nproblems, particularly those with non-convex and non-uniform Pareto fronts,\nDecomposition-based Multi-Objective Evolutionary Algorithms (MOEADs) often\nconverge to local optima, thereby limiting solution diversity. Despite its\nsignificance, this issue has received limited theoretical exploration. Through\na comprehensive geometric analysis, we identify that the traditional method of\nReference Point (RP) selection fundamentally contributes to this challenge. In\nresponse, we introduce an innovative RP selection strategy, the Weight\nVector-Guided and Gaussian-Hybrid method, designed to overcome the local optima\nissue. This approach employs a novel RP type that aligns with weight vector\ndirections and integrates a Gaussian distribution to combine three distinct RP\ncategories. Our research comprises two main experimental components: an\nablation study involving 14 algorithms within the MOEADs framework, spanning\nfrom 2014 to 2022, to validate our theoretical framework, and a series of\nempirical tests to evaluate the effectiveness of our proposed method against\nboth traditional and cutting-edge alternatives. Results demonstrate that our\nmethod achieves remarkable improvements in both population diversity and\nconvergence.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.08501v1",
    "published_date": "2024-04-12 14:29:45 UTC",
    "updated_date": "2024-04-12 14:29:45 UTC"
  },
  {
    "arxiv_id": "2404.08495v3",
    "title": "Dataset Reset Policy Optimization for RLHF",
    "authors": [
      "Jonathan D. Chang",
      "Wenhao Zhan",
      "Owen Oertell",
      "Kianté Brantley",
      "Dipendra Misra",
      "Jason D. Lee",
      "Wen Sun"
    ],
    "abstract": "Reinforcement Learning (RL) from Human Preference-based feedback is a popular\nparadigm for fine-tuning generative models, which has produced impressive\nmodels such as GPT-4 and Claude3 Opus. This framework often consists of two\nsteps: learning a reward model from an offline preference dataset followed by\nrunning online RL to optimize the learned reward model. In this work,\nleveraging the idea of reset, we propose a new RLHF algorithm with provable\nguarantees. Motivated by the fact that offline preference dataset provides\ninformative states (i.e., data that is preferred by the labelers), our new\nalgorithm, Dataset Reset Policy Optimization (DR-PO), integrates the existing\noffline preference dataset into the online policy training procedure via\ndataset reset: it directly resets the policy optimizer to the states in the\noffline dataset, instead of always starting from the initial state\ndistribution. In theory, we show that DR-PO learns to perform at least as good\nas any policy that is covered by the offline dataset under general function\napproximation with finite sample complexity. In experiments, we demonstrate\nthat on both the TL;DR summarization and the Anthropic Helpful Harmful (HH)\ndataset, the generation from DR-PO is better than that from Proximal Policy\nOptimization (PPO) and Direction Preference Optimization (DPO), under the\nmetric of GPT4 win-rate. Code for this work can be found at\nhttps://github.com/Cornell-RL/drpo.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "28 pages, 6 tables, 3 Figures, 3 Algorithms",
    "pdf_url": "http://arxiv.org/pdf/2404.08495v3",
    "published_date": "2024-04-12 14:25:49 UTC",
    "updated_date": "2024-04-16 17:36:39 UTC"
  },
  {
    "arxiv_id": "2404.08491v1",
    "title": "Mitigating Language-Level Performance Disparity in mPLMs via Teacher Language Selection and Cross-lingual Self-Distillation",
    "authors": [
      "Haozhe Zhao",
      "Zefan Cai",
      "Shuzheng Si",
      "Liang Chen",
      "Yufeng He",
      "Kaikai An",
      "Baobao Chang"
    ],
    "abstract": "Large-scale multilingual Pretrained Language Models (mPLMs) yield impressive\nperformance on cross-language tasks, yet significant performance disparities\nexist across different languages within the same mPLM. Previous studies\nendeavored to narrow these disparities by supervise fine-tuning the mPLMs with\nmultilingual data. However, obtaining labeled multilingual data is\ntime-consuming, and fine-tuning mPLM with limited labeled multilingual data\nmerely encapsulates the knowledge specific to the labeled data. Therefore, we\nintroduce ALSACE to leverage the learned knowledge from the well-performing\nlanguages to guide under-performing ones within the same mPLM, eliminating the\nneed for additional labeled multilingual data. Experiments show that ALSACE\neffectively mitigates language-level performance disparity across various mPLMs\nwhile showing the competitive performance on different multilingual NLU tasks,\nranging from full resource to limited resource settings. The code for our\napproach is available at https://github.com/pkunlp-icler/ALSACE.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "NAACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.08491v1",
    "published_date": "2024-04-12 14:19:16 UTC",
    "updated_date": "2024-04-12 14:19:16 UTC"
  },
  {
    "arxiv_id": "2404.08476v1",
    "title": "Combining Statistical Depth and Fermat Distance for Uncertainty Quantification",
    "authors": [
      "Hai-Vy Nguyen",
      "Fabrice Gamboa",
      "Reda Chhaibi",
      "Sixin Zhang",
      "Serge Gratton",
      "Thierry Giaccone"
    ],
    "abstract": "We measure the Out-of-domain uncertainty in the prediction of Neural Networks\nusing a statistical notion called ``Lens Depth'' (LD) combined with Fermat\nDistance, which is able to capture precisely the ``depth'' of a point with\nrespect to a distribution in feature space, without any assumption about the\nform of distribution. Our method has no trainable parameter. The method is\napplicable to any classification model as it is applied directly in feature\nspace at test time and does not intervene in training process. As such, it does\nnot impact the performance of the original model. The proposed method gives\nexcellent qualitative result on toy datasets and can give competitive or better\nuncertainty estimation on standard deep learning datasets compared to strong\nbaseline methods.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "math.PR",
      "stat.AP"
    ],
    "primary_category": "stat.ML",
    "comment": "12 pages",
    "pdf_url": "http://arxiv.org/pdf/2404.08476v1",
    "published_date": "2024-04-12 13:54:21 UTC",
    "updated_date": "2024-04-12 13:54:21 UTC"
  },
  {
    "arxiv_id": "2404.08461v2",
    "title": "OTTER: Effortless Label Distribution Adaptation of Zero-shot Models",
    "authors": [
      "Changho Shin",
      "Jitian Zhao",
      "Sonia Cromp",
      "Harit Vishwakarma",
      "Frederic Sala"
    ],
    "abstract": "Popular zero-shot models suffer due to artifacts inherited from pretraining.\nOne particularly detrimental issue, caused by unbalanced web-scale pretraining\ndata, is mismatched label distribution. Existing approaches that seek to repair\nthe label distribution are not suitable in zero-shot settings, as they have\nmismatching requirements, such as needing access to labeled downstream task\ndata or knowledge of the true label balance in the pretraining distribution. We\nsidestep these challenges and introduce a simple and lightweight approach to\nadjust pretrained model predictions via optimal transport. Our technique\nrequires only an estimate of the label distribution of a downstream task.\nTheoretically, we characterize the improvement produced by our procedure under\ncertain mild conditions and provide bounds on the error caused by\nmisspecification. Empirically, we validate our method in a wide array of\nzero-shot image and text classification tasks, improving accuracy by 4.8% and\n15.9% on average, and beating baselines like prior matching -- often by\nsignificant margins -- in 17 out of 21 datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.08461v2",
    "published_date": "2024-04-12 13:18:47 UTC",
    "updated_date": "2024-10-30 11:40:40 UTC"
  },
  {
    "arxiv_id": "2404.08721v1",
    "title": "Beyond One-Size-Fits-All: Adapting Counterfactual Explanations to User Objectives",
    "authors": [
      "Orfeas Menis Mastromichalakis",
      "Jason Liartis",
      "Giorgos Stamou"
    ],
    "abstract": "Explainable Artificial Intelligence (XAI) has emerged as a critical area of\nresearch aimed at enhancing the transparency and interpretability of AI\nsystems. Counterfactual Explanations (CFEs) offer valuable insights into the\ndecision-making processes of machine learning algorithms by exploring\nalternative scenarios where certain factors differ. Despite the growing\npopularity of CFEs in the XAI community, existing literature often overlooks\nthe diverse needs and objectives of users across different applications and\ndomains, leading to a lack of tailored explanations that adequately address the\ndifferent use cases. In this paper, we advocate for a nuanced understanding of\nCFEs, recognizing the variability in desired properties based on user\nobjectives and target applications. We identify three primary user objectives\nand explore the desired characteristics of CFEs in each case. By addressing\nthese differences, we aim to design more effective and tailored explanations\nthat meet the specific needs of users, thereby enhancing collaboration with AI\nsystems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.08721v1",
    "published_date": "2024-04-12 13:11:55 UTC",
    "updated_date": "2024-04-12 13:11:55 UTC"
  },
  {
    "arxiv_id": "2404.08458v2",
    "title": "On the Independence Assumption in Neurosymbolic Learning",
    "authors": [
      "Emile van Krieken",
      "Pasquale Minervini",
      "Edoardo M. Ponti",
      "Antonio Vergari"
    ],
    "abstract": "State-of-the-art neurosymbolic learning systems use probabilistic reasoning\nto guide neural networks towards predictions that conform to logical\nconstraints over symbols. Many such systems assume that the probabilities of\nthe considered symbols are conditionally independent given the input to\nsimplify learning and reasoning. We study and criticise this assumption,\nhighlighting how it can hinder optimisation and prevent uncertainty\nquantification. We prove that loss functions bias conditionally independent\nneural networks to become overconfident in their predictions. As a result, they\nare unable to represent uncertainty over multiple valid options. Furthermore,\nwe prove that these loss functions are difficult to optimise: they are\nnon-convex, and their minima are usually highly disconnected. Our theoretical\nanalysis gives the foundation for replacing the conditional independence\nassumption and designing more expressive neurosymbolic probabilistic models.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "Accepted at ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.08458v2",
    "published_date": "2024-04-12 13:09:48 UTC",
    "updated_date": "2024-06-07 15:10:50 UTC"
  },
  {
    "arxiv_id": "2404.08434v2",
    "title": "An improved tabular data generator with VAE-GMM integration",
    "authors": [
      "Patricia A. Apellániz",
      "Juan Parras",
      "Santiago Zazo"
    ],
    "abstract": "The rising use of machine learning in various fields requires robust methods\nto create synthetic tabular data. Data should preserve key characteristics\nwhile addressing data scarcity challenges. Current approaches based on\nGenerative Adversarial Networks, such as the state-of-the-art CTGAN model,\nstruggle with the complex structures inherent in tabular data. These data often\ncontain both continuous and discrete features with non-Gaussian distributions.\nTherefore, we propose a novel Variational Autoencoder (VAE)-based model that\naddresses these limitations. Inspired by the TVAE model, our approach\nincorporates a Bayesian Gaussian Mixture model (BGM) within the VAE\narchitecture. This avoids the limitations imposed by assuming a strictly\nGaussian latent space, allowing for a more accurate representation of the\nunderlying data distribution during data generation. Furthermore, our model\noffers enhanced flexibility by allowing the use of various differentiable\ndistributions for individual features, making it possible to handle both\ncontinuous and discrete data types. We thoroughly validate our model on three\nreal-world datasets with mixed data types, including two medically relevant\nones, based on their resemblance and utility. This evaluation demonstrates\nsignificant outperformance against CTGAN and TVAE, establishing its potential\nas a valuable tool for generating synthetic tabular data in various domains,\nparticularly in healthcare.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2.1"
    ],
    "primary_category": "cs.LG",
    "comment": "7 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.08434v2",
    "published_date": "2024-04-12 12:31:06 UTC",
    "updated_date": "2024-11-14 09:11:26 UTC"
  },
  {
    "arxiv_id": "2404.08424v3",
    "title": "Comparing Apples to Oranges: LLM-powered Multimodal Intention Prediction in an Object Categorization Task",
    "authors": [
      "Hassan Ali",
      "Philipp Allgeuer",
      "Stefan Wermter"
    ],
    "abstract": "Human intention-based systems enable robots to perceive and interpret user\nactions to interact with humans and adapt to their behavior proactively.\nTherefore, intention prediction is pivotal in creating a natural interaction\nwith social robots in human-designed environments. In this paper, we examine\nusing Large Language Models (LLMs) to infer human intention in a collaborative\nobject categorization task with a physical robot. We propose a novel multimodal\napproach that integrates user non-verbal cues, like hand gestures, body poses,\nand facial expressions, with environment states and user verbal cues to predict\nuser intentions in a hierarchical architecture. Our evaluation of five LLMs\nshows the potential for reasoning about verbal and non-verbal user cues,\nleveraging their context-understanding and real-world knowledge to support\nintention prediction while collaborating on a task with a social robot. Video:\nhttps://youtu.be/tBJHfAuzohI",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC",
      "I.2.9; I.2.7; I.2.8"
    ],
    "primary_category": "cs.RO",
    "comment": "Published in the Proceedings of the 16th International Conference on\n  Social Robotics (ICSR) 2024,15 pages,5 figures,2 tables; work was co-funded\n  by Horizon Europe project TERAIS under Grant agreement number 101079338",
    "pdf_url": "http://arxiv.org/pdf/2404.08424v3",
    "published_date": "2024-04-12 12:15:14 UTC",
    "updated_date": "2025-04-08 10:48:19 UTC"
  },
  {
    "arxiv_id": "2404.08417v2",
    "title": "AdapterSwap: Continuous Training of LLMs with Data Removal and Access-Control Guarantees",
    "authors": [
      "William Fleshman",
      "Aleem Khan",
      "Marc Marone",
      "Benjamin Van Durme"
    ],
    "abstract": "Large language models (LLMs) are increasingly capable of completing knowledge\nintensive tasks by recalling information from a static pretraining corpus. Here\nwe are concerned with LLMs in the context of evolving data requirements. For\ninstance: batches of new data that are introduced periodically; subsets of data\nwith user-based access controls; or requirements on dynamic removal of\ndocuments with guarantees that associated knowledge cannot be recalled. We wish\nto satisfy these requirements while at the same time ensuring a model does not\nforget old information when new data becomes available. To address these\nissues, we introduce AdapterSwap, a training and inference scheme that\norganizes knowledge from a data collection into a set of low-rank adapters,\nwhich are dynamically composed during inference. Our experiments demonstrate\nAdapterSwap's ability to support efficient continual learning, while also\nenabling organizations to have fine-grained control over data access and\ndeletion.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "In Proceedings of the Conference on Applied Machine Learning in\n  Information Security, 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.08417v2",
    "published_date": "2024-04-12 12:06:02 UTC",
    "updated_date": "2025-02-09 21:25:40 UTC"
  },
  {
    "arxiv_id": "2404.08414v1",
    "title": "Evolutionary Preference Sampling for Pareto Set Learning",
    "authors": [
      "Rongguang Ye",
      "Longcan Chen",
      "Jinyuan Zhang",
      "Hisao Ishibuchi"
    ],
    "abstract": "Recently, Pareto Set Learning (PSL) has been proposed for learning the entire\nPareto set using a neural network. PSL employs preference vectors to scalarize\nmultiple objectives, facilitating the learning of mappings from preference\nvectors to specific Pareto optimal solutions. Previous PSL methods have shown\ntheir effectiveness in solving artificial multi-objective optimization problems\n(MOPs) with uniform preference vector sampling. The quality of the learned\nPareto set is influenced by the sampling strategy of the preference vector, and\nthe sampling of the preference vector needs to be decided based on the Pareto\nfront shape. However, a fixed preference sampling strategy cannot\nsimultaneously adapt the Pareto front of multiple MOPs. To address this\nlimitation, this paper proposes an Evolutionary Preference Sampling (EPS)\nstrategy to efficiently sample preference vectors. Inspired by evolutionary\nalgorithms, we consider preference sampling as an evolutionary process to\ngenerate preference vectors for neural network training. We integrate the EPS\nstrategy into five advanced PSL methods. Extensive experiments demonstrate that\nour proposed method has a faster convergence speed than baseline algorithms on\n7 testing problems. Our implementation is available at\nhttps://github.com/rG223/EPS.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "Genetic and Evolutionary Computation Conference (GECCO '24)",
    "pdf_url": "http://arxiv.org/pdf/2404.08414v1",
    "published_date": "2024-04-12 11:58:13 UTC",
    "updated_date": "2024-04-12 11:58:13 UTC"
  },
  {
    "arxiv_id": "2404.08412v2",
    "title": "PiRD: Physics-informed Residual Diffusion for Flow Field Reconstruction",
    "authors": [
      "Siming Shan",
      "Pengkai Wang",
      "Song Chen",
      "Jiaxu Liu",
      "Chao Xu",
      "Shengze Cai"
    ],
    "abstract": "The use of machine learning in fluid dynamics is becoming more common to\nexpedite the computation when solving forward and inverse problems of partial\ndifferential equations. Yet, a notable challenge with existing convolutional\nneural network (CNN)-based methods for data fidelity enhancement is their\nreliance on specific low-fidelity data patterns and distributions during the\ntraining phase. In addition, the CNN-based method essentially treats the flow\nreconstruction task as a computer vision task that prioritizes the element-wise\nprecision which lacks a physical and mathematical explanation. This dependence\ncan dramatically affect the models' effectiveness in real-world scenarios,\nespecially when the low-fidelity input deviates from the training data or\ncontains noise not accounted for during training. The introduction of diffusion\nmodels in this context shows promise for improving performance and\ngeneralizability. Unlike direct mapping from a specific low-fidelity to a\nhigh-fidelity distribution, diffusion models learn to transition from any\nlow-fidelity distribution towards a high-fidelity one. Our proposed model -\nPhysics-informed Residual Diffusion, demonstrates the capability to elevate the\nquality of data from both standard low-fidelity inputs, to low-fidelity inputs\nwith injected Gaussian noise, and randomly collected samples. By integrating\nphysics-based insights into the objective function, it further refines the\naccuracy and the fidelity of the inferred high-quality data. Experimental\nresults have shown that our approach can effectively reconstruct high-quality\noutcomes for two-dimensional turbulent flows from a range of low-fidelity input\nconditions without requiring retraining.",
    "categories": [
      "physics.flu-dyn",
      "cs.AI"
    ],
    "primary_category": "physics.flu-dyn",
    "comment": "22 pages",
    "pdf_url": "http://arxiv.org/pdf/2404.08412v2",
    "published_date": "2024-04-12 11:45:51 UTC",
    "updated_date": "2024-05-09 15:30:17 UTC"
  },
  {
    "arxiv_id": "2404.08408v1",
    "title": "Seismic First Break Picking in a Higher Dimension Using Deep Graph Learning",
    "authors": [
      "Hongtao Wang",
      "Li Long",
      "Jiangshe Zhang",
      "Xiaoli Wei",
      "Chunxia Zhang",
      "Zhenbo Guo"
    ],
    "abstract": "Contemporary automatic first break (FB) picking methods typically analyze 1D\nsignals, 2D source gathers, or 3D source-receiver gathers. Utilizing\nhigher-dimensional data, such as 2D or 3D, incorporates global features,\nimproving the stability of local picking. Despite the benefits,\nhigh-dimensional data requires structured input and increases computational\ndemands. Addressing this, we propose a novel approach using deep graph learning\ncalled DGL-FB, constructing a large graph to efficiently extract information.\nIn this graph, each seismic trace is represented as a node, connected by edges\nthat reflect similarities. To manage the size of the graph, we develop a\nsubgraph sampling technique to streamline model training and inference. Our\nproposed framework, DGL-FB, leverages deep graph learning for FB picking. It\nencodes subgraphs into global features using a deep graph encoder.\nSubsequently, the encoded global features are combined with local node signals\nand fed into a ResUNet-based 1D segmentation network for FB detection. Field\nsurvey evaluations of DGL-FB show superior accuracy and stability compared to a\n2D U-Net-based benchmark method.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP",
      "physics.geo-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.08408v1",
    "published_date": "2024-04-12 11:36:24 UTC",
    "updated_date": "2024-04-12 11:36:24 UTC"
  },
  {
    "arxiv_id": "2404.08404v2",
    "title": "A Complexity Map of Probabilistic Reasoning for Neurosymbolic Classification Techniques",
    "authors": [
      "Arthur Ledaguenel",
      "Céline Hudelot",
      "Mostepha Khouadjia"
    ],
    "abstract": "Neurosymbolic artificial intelligence is a growing field of research aiming\nto combine neural network learning capabilities with the reasoning abilities of\nsymbolic systems. Informed multi-label classification is a sub-field of\nneurosymbolic AI which studies how to leverage prior knowledge to improve\nneural classification systems. Recently, a family of neurosymbolic techniques\nfor informed classification based on probabilistic reasoning has gained\nsignificant traction. Unfortunately, depending on the language used to\nrepresent prior knowledge, solving certain probabilistic reasoning problems can\nbecome prohibitively hard when the number of classes increases. Therefore, the\nasymptotic complexity of probabilistic reasoning is of cardinal importance to\nassess the scalability of such techniques. In this paper, we develop a unified\nformalism for four probabilistic reasoning problems. Then, we compile several\nknown and new tractability results into a single complexity map of\nprobabilistic reasoning. We build on top of this complexity map to characterize\nthe domains of scalability of several techniques. We hope this work will help\nneurosymbolic AI practitioners navigate the scalability landscape of\nprobabilistic neurosymbolic techniques.",
    "categories": [
      "cs.AI",
      "cs.CC",
      "cs.LG",
      "cs.SC"
    ],
    "primary_category": "cs.AI",
    "comment": "36 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.08404v2",
    "published_date": "2024-04-12 11:31:37 UTC",
    "updated_date": "2025-01-23 09:52:48 UTC"
  },
  {
    "arxiv_id": "2407.00743v1",
    "title": "AIMDiT: Modality Augmentation and Interaction via Multimodal Dimension Transformation for Emotion Recognition in Conversations",
    "authors": [
      "Sheng Wu",
      "Jiaxing Liu",
      "Longbiao Wang",
      "Dongxiao He",
      "Xiaobao Wang",
      "Jianwu Dang"
    ],
    "abstract": "Emotion Recognition in Conversations (ERC) is a popular task in natural\nlanguage processing, which aims to recognize the emotional state of the speaker\nin conversations. While current research primarily emphasizes contextual\nmodeling, there exists a dearth of investigation into effective multimodal\nfusion methods. We propose a novel framework called AIMDiT to solve the problem\nof multimodal fusion of deep features. Specifically, we design a Modality\nAugmentation Network which performs rich representation learning through\ndimension transformation of different modalities and parameter-efficient\ninception block. On the other hand, the Modality Interaction Network performs\ninteraction fusion of extracted inter-modal features and intra-modal features.\nExperiments conducted using our AIMDiT framework on the public benchmark\ndataset MELD reveal 2.34% and 2.87% improvements in terms of the Acc-7 and w-F1\nmetrics compared to the state-of-the-art (SOTA) models.",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.CL",
      "eess.AS"
    ],
    "primary_category": "cs.MM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.00743v1",
    "published_date": "2024-04-12 11:31:18 UTC",
    "updated_date": "2024-04-12 11:31:18 UTC"
  },
  {
    "arxiv_id": "2404.08401v4",
    "title": "PnLCalib: Sports Field Registration via Points and Lines Optimization",
    "authors": [
      "Marc Gutiérrez-Pérez",
      "Antonio Agudo"
    ],
    "abstract": "Camera calibration in broadcast sports videos presents numerous challenges\nfor accurate sports field registration due to multiple camera angles, varying\ncamera parameters, and frequent occlusions of the field. Traditional\nsearch-based methods depend on initial camera pose estimates, which can\nstruggle in non-standard positions and dynamic environments. In response, we\npropose an optimization-based calibration pipeline that leverages a 3D soccer\nfield model and a predefined set of keypoints to overcome these limitations.\nOur method also introduces a novel refinement module that improves initial\ncalibration by using detected field lines in a non-linear optimization process.\nThis approach outperforms existing techniques in both multi-view and\nsingle-view 3D camera calibration tasks, while maintaining competitive\nperformance in homography estimation. Extensive experimentation on real-world\nsoccer datasets, including SoccerNet-Calibration, WorldCup 2014, and\nTS-WorldCup, highlights the robustness and accuracy of our method across\ndiverse broadcast scenarios. Our approach offers significant improvements in\ncamera calibration precision and reliability.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.2; I.4; I.5"
    ],
    "primary_category": "cs.CV",
    "comment": "Extended version of \"No Bells, Just Whistles: Sports Field\n  Registration Leveraging Geometric Properties\"",
    "pdf_url": "http://arxiv.org/pdf/2404.08401v4",
    "published_date": "2024-04-12 11:15:15 UTC",
    "updated_date": "2024-10-24 14:41:42 UTC"
  },
  {
    "arxiv_id": "2404.08398v1",
    "title": "Multi-Agent eXperimenter (MAX)",
    "authors": [
      "Önder Gürcan"
    ],
    "abstract": "We present a novel multi-agent simulator named Multi-Agent eXperimenter (MAX)\nthat is designed to simulate blockchain experiments involving large numbers of\nagents of different types acting in one or several environments. The\narchitecture of MAX is highly modular, enabling easy addition of new models.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.MA",
    "comment": "3 pages, no figures",
    "pdf_url": "http://arxiv.org/pdf/2404.08398v1",
    "published_date": "2024-04-12 11:07:10 UTC",
    "updated_date": "2024-04-12 11:07:10 UTC"
  },
  {
    "arxiv_id": "2404.08382v2",
    "title": "Look at the Text: Instruction-Tuned Language Models are More Robust Multiple Choice Selectors than You Think",
    "authors": [
      "Xinpeng Wang",
      "Chengzhi Hu",
      "Bolei Ma",
      "Paul Röttger",
      "Barbara Plank"
    ],
    "abstract": "Multiple choice questions (MCQs) are commonly used to evaluate the\ncapabilities of large language models (LLMs). One common way to evaluate the\nmodel response is to rank the candidate answers based on the log probability of\nthe first token prediction. An alternative way is to examine the text output.\nPrior work has shown that first token probabilities lack robustness to changes\nin MCQ phrasing, and that first token probabilities do not match text answers\nfor instruction-tuned models. Therefore, in this paper, we investigate the\nrobustness of text answers. We show that the text answers are more robust to\nquestion perturbations than the first token probabilities, when the first token\nanswers mismatch the text answers. The difference in robustness increases as\nthe mismatch rate becomes greater. As the mismatch reaches over 50\\%, the text\nanswer is more robust to option order changes than the debiased first token\nprobabilities using state-of-the-art debiasing methods such as PriDe. Our\nfindings provide further evidence for the benefits of text answer evaluation\nover first token probability evaluation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "COLM 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.08382v2",
    "published_date": "2024-04-12 10:36:15 UTC",
    "updated_date": "2024-08-20 08:07:49 UTC"
  },
  {
    "arxiv_id": "2404.08376v1",
    "title": "Graph data augmentation with Gromow-Wasserstein Barycenters",
    "authors": [
      "Andrea Ponti"
    ],
    "abstract": "Graphs are ubiquitous in various fields, and deep learning methods have been\nsuccessful applied in graph classification tasks. However, building large and\ndiverse graph datasets for training can be expensive. While augmentation\ntechniques exist for structured data like images or numerical data, the\naugmentation of graph data remains challenging. This is primarily due to the\ncomplex and non-Euclidean nature of graph data. In this paper, it has been\nproposed a novel augmentation strategy for graphs that operates in a\nnon-Euclidean space. This approach leverages graphon estimation, which models\nthe generative mechanism of networks sequences. Computational results\ndemonstrate the effectiveness of the proposed augmentation framework in\nimproving the performance of graph classification models. Additionally, using a\nnon-Euclidean distance, specifically the Gromow-Wasserstein distance, results\nin better approximations of the graphon. This framework also provides a means\nto validate different graphon estimation approaches, particularly in real-world\nscenarios where the true graphon is unknown.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.08376v1",
    "published_date": "2024-04-12 10:22:55 UTC",
    "updated_date": "2024-04-12 10:22:55 UTC"
  },
  {
    "arxiv_id": "2404.08361v2",
    "title": "Large-Scale Multi-Domain Recommendation: an Automatic Domain Feature Extraction and Personalized Integration Framework",
    "authors": [
      "Dongbo Xi",
      "Zhen Chen",
      "Yuexian Wang",
      "He Cui",
      "Chong Peng",
      "Fuzhen Zhuang",
      "Peng Yan"
    ],
    "abstract": "Feed recommendation is currently the mainstream mode for many real-world\napplications (e.g., TikTok, Dianping), it is usually necessary to model and\npredict user interests in multiple scenarios (domains) within and even outside\nthe application. Multi-domain learning is a typical solution in this regard.\nWhile considerable efforts have been made in this regard, there are still two\nlong-standing challenges: (1) Accurately depicting the differences among\ndomains using domain features is crucial for enhancing the performance of each\ndomain. However, manually designing domain features and models for numerous\ndomains can be a laborious task. (2) Users typically have limited impressions\nin only a few domains. Extracting features automatically from other domains and\nleveraging them to improve the predictive capabilities of each domain has\nconsistently posed a challenging problem. In this paper, we propose an\nAutomatic Domain Feature Extraction and Personalized Integration (DFEI)\nframework for the large-scale multi-domain recommendation. The framework\nautomatically transforms the behavior of each individual user into an\naggregation of all user behaviors within the domain, which serves as the domain\nfeatures. Unlike offline feature engineering methods, the extracted domain\nfeatures are higher-order representations and directly related to the target\nlabel. Besides, by personalized integration of domain features from other\ndomains for each user and the innovation in the training mode, the DFEI\nframework can yield more accurate conversion identification. Experimental\nresults on both public and industrial datasets, consisting of over 20 domains,\nclearly demonstrate that the proposed framework achieves significantly better\nperformance compared with SOTA baselines. Furthermore, we have released the\nsource code of the proposed framework at https://github.com/xidongbo/DFEI.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "8 pages",
    "pdf_url": "http://arxiv.org/pdf/2404.08361v2",
    "published_date": "2024-04-12 09:57:17 UTC",
    "updated_date": "2024-04-15 01:50:25 UTC"
  },
  {
    "arxiv_id": "2404.08359v1",
    "title": "Improving Health Question Answering with Reliable and Time-Aware Evidence Retrieval",
    "authors": [
      "Juraj Vladika",
      "Florian Matthes"
    ],
    "abstract": "In today's digital world, seeking answers to health questions on the Internet\nis a common practice. However, existing question answering (QA) systems often\nrely on using pre-selected and annotated evidence documents, thus making them\ninadequate for addressing novel questions. Our study focuses on the open-domain\nQA setting, where the key challenge is to first uncover relevant evidence in\nlarge knowledge bases. By utilizing the common retrieve-then-read QA pipeline\nand PubMed as a trustworthy collection of medical research documents, we answer\nhealth questions from three diverse datasets. We modify different retrieval\nsettings to observe their influence on the QA pipeline's performance, including\nthe number of retrieved documents, sentence selection process, the publication\nyear of articles, and their number of citations. Our results reveal that\ncutting down on the amount of retrieved documents and favoring more recent and\nhighly cited documents can improve the final macro F1 score up to 10%. We\ndiscuss the results, highlight interesting examples, and outline challenges for\nfuture research, like managing evidence disagreement and crafting user-friendly\nexplanations.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to NAACL 2024 (Findings)",
    "pdf_url": "http://arxiv.org/pdf/2404.08359v1",
    "published_date": "2024-04-12 09:56:12 UTC",
    "updated_date": "2024-04-12 09:56:12 UTC"
  },
  {
    "arxiv_id": "2405.01440v1",
    "title": "A Review of Reward Functions for Reinforcement Learning in the context of Autonomous Driving",
    "authors": [
      "Ahmed Abouelazm",
      "Jonas Michel",
      "J. Marius Zoellner"
    ],
    "abstract": "Reinforcement learning has emerged as an important approach for autonomous\ndriving. A reward function is used in reinforcement learning to establish the\nlearned skill objectives and guide the agent toward the optimal policy. Since\nautonomous driving is a complex domain with partly conflicting objectives with\nvarying degrees of priority, developing a suitable reward function represents a\nfundamental challenge. This paper aims to highlight the gap in such function\ndesign by assessing different proposed formulations in the literature and\ndividing individual objectives into Safety, Comfort, Progress, and Traffic\nRules compliance categories. Additionally, the limitations of the reviewed\nreward functions are discussed, such as objectives aggregation and indifference\nto driving context. Furthermore, the reward categories are frequently\ninadequately formulated and lack standardization. This paper concludes by\nproposing future research that potentially addresses the observed shortcomings\nin rewards, including a reward validation framework and structured rewards that\nare context-aware and able to resolve conflicts.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted at \"Interaction-driven Behavior Prediction and Planning for\n  Autonomous Vehicles\" workshop in 35th IEEE Intelligent Vehicles Symposium (IV\n  2024)",
    "pdf_url": "http://arxiv.org/pdf/2405.01440v1",
    "published_date": "2024-04-12 08:32:54 UTC",
    "updated_date": "2024-04-12 08:32:54 UTC"
  },
  {
    "arxiv_id": "2404.08322v1",
    "title": "BOND: Bootstrapping From-Scratch Name Disambiguation with Multi-task Promoting",
    "authors": [
      "Yuqing Cheng",
      "Bo Chen",
      "Fanjin Zhang",
      "Jie Tang"
    ],
    "abstract": "From-scratch name disambiguation is an essential task for establishing a\nreliable foundation for academic platforms. It involves partitioning documents\nauthored by identically named individuals into groups representing distinct\nreal-life experts. Canonically, the process is divided into two decoupled\ntasks: locally estimating the pairwise similarities between documents followed\nby globally grouping these documents into appropriate clusters. However, such a\ndecoupled approach often inhibits optimal information exchange between these\nintertwined tasks. Therefore, we present BOND, which bootstraps the local and\nglobal informative signals to promote each other in an end-to-end regime.\nSpecifically, BOND harnesses local pairwise similarities to drive global\nclustering, subsequently generating pseudo-clustering labels. These global\nsignals further refine local pairwise characterizations. The experimental\nresults establish BOND's superiority, outperforming other advanced baselines by\na substantial margin. Moreover, an enhanced version, BOND+, incorporating\nensemble and post-match techniques, rivals the top methods in the WhoIsWho\ncompetition.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "H.3.7; H.3.3"
    ],
    "primary_category": "cs.SI",
    "comment": "TheWebConf 2024 (WWW '24)",
    "pdf_url": "http://arxiv.org/pdf/2404.08322v1",
    "published_date": "2024-04-12 08:28:52 UTC",
    "updated_date": "2024-04-12 08:28:52 UTC"
  },
  {
    "arxiv_id": "2404.08313v1",
    "title": "The Integration of Semantic and Structural Knowledge in Knowledge Graph Entity Typing",
    "authors": [
      "Muzhi Li",
      "Minda Hu",
      "Irwin King",
      "Ho-fung Leung"
    ],
    "abstract": "The Knowledge Graph Entity Typing (KGET) task aims to predict missing type\nannotations for entities in knowledge graphs. Recent works only utilize the\n\\textit{\\textbf{structural knowledge}} in the local neighborhood of entities,\ndisregarding \\textit{\\textbf{semantic knowledge}} in the textual\nrepresentations of entities, relations, and types that are also crucial for\ntype inference. Additionally, we observe that the interaction between semantic\nand structural knowledge can be utilized to address the false-negative problem.\nIn this paper, we propose a novel \\textbf{\\underline{S}}emantic and\n\\textbf{\\underline{S}}tructure-aware KG \\textbf{\\underline{E}}ntity\n\\textbf{\\underline{T}}yping~{(SSET)} framework, which is composed of three\nmodules. First, the \\textit{Semantic Knowledge Encoding} module encodes factual\nknowledge in the KG with a Masked Entity Typing task. Then, the\n\\textit{Structural Knowledge Aggregation} module aggregates knowledge from the\nmulti-hop neighborhood of entities to infer missing types. Finally, the\n\\textit{Unsupervised Type Re-ranking} module utilizes the inference results\nfrom the two models above to generate type predictions that are robust to\nfalse-negative samples. Extensive experiments show that SSET significantly\noutperforms existing state-of-the-art methods.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted in NAACL2024 main",
    "pdf_url": "http://arxiv.org/pdf/2404.08313v1",
    "published_date": "2024-04-12 08:17:44 UTC",
    "updated_date": "2024-04-12 08:17:44 UTC"
  },
  {
    "arxiv_id": "2406.11865v1",
    "title": "Artificial Intelligence in Everyday Life 2.0: Educating University Students from Different Majors",
    "authors": [
      "Maria Kasinidou",
      "Styliani Kleanthous",
      "Matteo Busso",
      "Marcelo Rodas",
      "Jahna Otterbacher",
      "Fausto Giunchiglia"
    ],
    "abstract": "With the surge in data-centric AI and its increasing capabilities, AI\napplications have become a part of our everyday lives. However,\nmisunderstandings regarding their capabilities, limitations, and associated\nadvantages and disadvantages are widespread. Consequently, in the university\nsetting, there is a crucial need to educate not only computer science majors\nbut also students from various disciplines about AI. In this experience report,\nwe present an overview of an introductory course that we offered to students\ncoming from different majors. Moreover, we discuss the assignments and quizzes\nof the course, which provided students with a firsthand experience of AI\nprocesses and insights into their learning patterns. Additionally, we provide a\nsummary of the course evaluation, as well as students' performance. Finally, we\npresent insights gained from teaching this course and elaborate on our future\nplans.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "7 pages, ITiCSE conference",
    "pdf_url": "http://arxiv.org/pdf/2406.11865v1",
    "published_date": "2024-04-12 08:10:42 UTC",
    "updated_date": "2024-04-12 08:10:42 UTC"
  },
  {
    "arxiv_id": "2404.08309v1",
    "title": "Subtoxic Questions: Dive Into Attitude Change of LLM's Response in Jailbreak Attempts",
    "authors": [
      "Tianyu Zhang",
      "Zixuan Zhao",
      "Jiaqi Huang",
      "Jingyu Hua",
      "Sheng Zhong"
    ],
    "abstract": "As Large Language Models (LLMs) of Prompt Jailbreaking are getting more and\nmore attention, it is of great significance to raise a generalized research\nparadigm to evaluate attack strengths and a basic model to conduct subtler\nexperiments. In this paper, we propose a novel approach by focusing on a set of\ntarget questions that are inherently more sensitive to jailbreak prompts,\naiming to circumvent the limitations posed by enhanced LLM security. Through\ndesigning and analyzing these sensitive questions, this paper reveals a more\neffective method of identifying vulnerabilities in LLMs, thereby contributing\nto the advancement of LLM security. This research not only challenges existing\njailbreaking methodologies but also fortifies LLMs against potential exploits.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "4 pages, 2 figures. This paper was submitted to The 7th Deep Learning\n  Security and Privacy Workshop (DLSP 2024) and was accepted as extended\n  abstract, see https://dlsp2024.ieee-security.org/",
    "pdf_url": "http://arxiv.org/pdf/2404.08309v1",
    "published_date": "2024-04-12 08:08:44 UTC",
    "updated_date": "2024-04-12 08:08:44 UTC"
  },
  {
    "arxiv_id": "2404.08295v1",
    "title": "Study of Emotion Concept Formation by Integrating Vision, Physiology, and Word Information using Multilayered Multimodal Latent Dirichlet Allocation",
    "authors": [
      "Kazuki Tsurumaki",
      "Chie Hieida",
      "Kazuki Miyazawa"
    ],
    "abstract": "How are emotions formed? Through extensive debate and the promulgation of\ndiverse theories , the theory of constructed emotion has become prevalent in\nrecent research on emotions. According to this theory, an emotion concept\nrefers to a category formed by interoceptive and exteroceptive information\nassociated with a specific emotion. An emotion concept stores past experiences\nas knowledge and can predict unobserved information from acquired information.\nTherefore, in this study, we attempted to model the formation of emotion\nconcepts using a constructionist approach from the perspective of the\nconstructed emotion theory. Particularly, we constructed a model using\nmultilayered multimodal latent Dirichlet allocation , which is a probabilistic\ngenerative model. We then trained the model for each subject using vision,\nphysiology, and word information obtained from multiple people who experienced\ndifferent visual emotion-evoking stimuli. To evaluate the model, we verified\nwhether the formed categories matched human subjectivity and determined whether\nunobserved information could be predicted via categories. The verification\nresults exceeded chance level, suggesting that emotion concept formation can be\nexplained by the proposed model.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "cs.RO",
      "cs.SC"
    ],
    "primary_category": "cs.AI",
    "comment": "This work has been submitted to the IEEE for possible publication. We\n  would like to thank Professor Takayuki Nagai for useful discussions",
    "pdf_url": "http://arxiv.org/pdf/2404.08295v1",
    "published_date": "2024-04-12 07:34:46 UTC",
    "updated_date": "2024-04-12 07:34:46 UTC"
  },
  {
    "arxiv_id": "2404.08285v2",
    "title": "A Survey of Neural Network Robustness Assessment in Image Recognition",
    "authors": [
      "Jie Wang",
      "Jun Ai",
      "Minyan Lu",
      "Haoran Su",
      "Dan Yu",
      "Yutao Zhang",
      "Junda Zhu",
      "Jingyu Liu"
    ],
    "abstract": "In recent years, there has been significant attention given to the robustness\nassessment of neural networks. Robustness plays a critical role in ensuring\nreliable operation of artificial intelligence (AI) systems in complex and\nuncertain environments. Deep learning's robustness problem is particularly\nsignificant, highlighted by the discovery of adversarial attacks on image\nclassification models. Researchers have dedicated efforts to evaluate\nrobustness in diverse perturbation conditions for image recognition tasks.\nRobustness assessment encompasses two main techniques: robustness verification/\ncertification for deliberate adversarial attacks and robustness testing for\nrandom data corruptions. In this survey, we present a detailed examination of\nboth adversarial robustness (AR) and corruption robustness (CR) in neural\nnetwork assessment. Analyzing current research papers and standards, we provide\nan extensive overview of robustness assessment in image recognition. Three\nessential aspects are analyzed: concepts, metrics, and assessment methods. We\ninvestigate the perturbation metrics and range representations used to measure\nthe degree of perturbations on images, as well as the robustness metrics\nspecifically for the robustness conditions of classification models. The\nstrengths and limitations of the existing methods are also discussed, and some\npotential directions for future research are provided.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.CV",
    "comment": "Corrected typos and grammatical errors in Section 5",
    "pdf_url": "http://arxiv.org/pdf/2404.08285v2",
    "published_date": "2024-04-12 07:19:16 UTC",
    "updated_date": "2024-04-15 10:50:47 UTC"
  },
  {
    "arxiv_id": "2404.08263v2",
    "title": "Relational Prompt-based Pre-trained Language Models for Social Event Detection",
    "authors": [
      "Pu Li",
      "Xiaoyan Yu",
      "Hao Peng",
      "Yantuan Xian",
      "Linqin Wang",
      "Li Sun",
      "Jingyun Zhang",
      "Philip S. Yu"
    ],
    "abstract": "Social Event Detection (SED) aims to identify significant events from social\nstreams, and has a wide application ranging from public opinion analysis to\nrisk management. In recent years, Graph Neural Network (GNN) based solutions\nhave achieved state-of-the-art performance. However, GNN-based methods often\nstruggle with missing and noisy edges between messages, affecting the quality\nof learned message embedding. Moreover, these methods statically initialize\nnode embedding before training, which, in turn, limits the ability to learn\nfrom message texts and relations simultaneously. In this paper, we approach\nsocial event detection from a new perspective based on Pre-trained Language\nModels (PLMs), and present RPLM_SED (Relational prompt-based Pre-trained\nLanguage Models for Social Event Detection). We first propose a new pairwise\nmessage modeling strategy to construct social messages into message pairs with\nmulti-relational sequences. Secondly, a new multi-relational prompt-based\npairwise message learning mechanism is proposed to learn more comprehensive\nmessage representation from message pairs with multi-relational prompts using\nPLMs. Thirdly, we design a new clustering constraint to optimize the encoding\nprocess by enhancing intra-cluster compactness and inter-cluster dispersion,\nmaking the message representation more distinguishable. We evaluate the\nRPLM_SED on three real-world datasets, demonstrating that the RPLM_SED model\nachieves state-of-the-art performance in offline, online, low-resource, and\nlong-tail distribution scenarios for social event detection tasks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SI"
    ],
    "primary_category": "cs.CL",
    "comment": "ACM TOIS",
    "pdf_url": "http://arxiv.org/pdf/2404.08263v2",
    "published_date": "2024-04-12 06:23:07 UTC",
    "updated_date": "2024-09-10 09:14:49 UTC"
  },
  {
    "arxiv_id": "2404.08262v3",
    "title": "Pretraining and Updates of Domain-Specific LLM: A Case Study in the Japanese Business Domain",
    "authors": [
      "Kosuke Takahashi",
      "Takahiro Omi",
      "Kosuke Arima",
      "Tatsuya Ishigaki"
    ],
    "abstract": "The development of Large Language Models (LLMs) in various languages has been\nadvancing, but the combination of non-English languages with domain-specific\ncontexts remains underexplored. This paper presents our findings from training\nand evaluating a Japanese business domain-specific LLM designed to better\nunderstand business-related documents, such as the news on current affairs,\ntechnical reports, and patents. Additionally, LLMs in this domain require\nregular updates to incorporate the most recent knowledge. Therefore, we also\nreport our findings from the first experiments and evaluations involving\nupdates to this LLM using the latest article data, which is an important\nproblem setting that has not been addressed in previous research. From our\nexperiments on a newly created benchmark dataset for question answering in the\ntarget domain, we found that (1) our pretrained model improves QA accuracy\nwithout losing general knowledge, and (2) a proper mixture of the latest and\nolder texts in the training data for the update is necessary. Our pretrained\nmodel and business domain benchmark are publicly available to support further\nstudies.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "68T50",
      "I.2"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at PACLIC 38",
    "pdf_url": "http://arxiv.org/pdf/2404.08262v3",
    "published_date": "2024-04-12 06:21:48 UTC",
    "updated_date": "2024-11-06 16:19:24 UTC"
  },
  {
    "arxiv_id": "2404.08242v1",
    "title": "RLEMMO: Evolutionary Multimodal Optimization Assisted By Deep Reinforcement Learning",
    "authors": [
      "Hongqiao Lian",
      "Zeyuan Ma",
      "Hongshu Guo",
      "Ting Huang",
      "Yue-Jiao Gong"
    ],
    "abstract": "Solving multimodal optimization problems (MMOP) requires finding all optimal\nsolutions, which is challenging in limited function evaluations. Although\nexisting works strike the balance of exploration and exploitation through\nhand-crafted adaptive strategies, they require certain expert knowledge, hence\ninflexible to deal with MMOP with different properties. In this paper, we\npropose RLEMMO, a Meta-Black-Box Optimization framework, which maintains a\npopulation of solutions and incorporates a reinforcement learning agent for\nflexibly adjusting individual-level searching strategies to match the\nup-to-date optimization status, hence boosting the search performance on MMOP.\nConcretely, we encode landscape properties and evolution path information into\neach individual and then leverage attention networks to advance population\ninformation sharing. With a novel reward mechanism that encourages both quality\nand diversity, RLEMMO can be effectively trained using a policy gradient\nalgorithm. The experimental results on the CEC2013 MMOP benchmark underscore\nthe competitive optimization performance of RLEMMO against several strong\nbaselines.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "Accepted as full paper at GECCO 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.08242v1",
    "published_date": "2024-04-12 05:02:49 UTC",
    "updated_date": "2024-04-12 05:02:49 UTC"
  },
  {
    "arxiv_id": "2404.08239v1",
    "title": "Auto-configuring Exploration-Exploitation Tradeoff in Evolutionary Computation via Deep Reinforcement Learning",
    "authors": [
      "Zeyuan Ma",
      "Jiacheng Chen",
      "Hongshu Guo",
      "Yining Ma",
      "Yue-Jiao Gong"
    ],
    "abstract": "Evolutionary computation (EC) algorithms, renowned as powerful black-box\noptimizers, leverage a group of individuals to cooperatively search for the\noptimum. The exploration-exploitation tradeoff (EET) plays a crucial role in\nEC, which, however, has traditionally been governed by manually designed rules.\nIn this paper, we propose a deep reinforcement learning-based framework that\nautonomously configures and adapts the EET throughout the EC search process.\nThe framework allows different individuals of the population to selectively\nattend to the global and local exemplars based on the current search state,\nmaximizing the cooperative search outcome. Our proposed framework is\ncharacterized by its simplicity, effectiveness, and generalizability, with the\npotential to enhance numerous existing EC algorithms. To validate its\ncapabilities, we apply our framework to several representative EC algorithms\nand conduct extensive experiments on the augmented CEC2021 benchmark. The\nresults demonstrate significant improvements in the performance of the backbone\nalgorithms, as well as favorable generalization across diverse problem classes,\ndimensions, and population sizes. Additionally, we provide an in-depth analysis\nof the EET issue by interpreting the learned behaviors of EC.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "Accepted as a full paper at GECCO 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.08239v1",
    "published_date": "2024-04-12 04:48:32 UTC",
    "updated_date": "2024-04-12 04:48:32 UTC"
  },
  {
    "arxiv_id": "2404.08237v1",
    "title": "IFViT: Interpretable Fixed-Length Representation for Fingerprint Matching via Vision Transformer",
    "authors": [
      "Yuhang Qiu",
      "Honghui Chen",
      "Xingbo Dong",
      "Zheng Lin",
      "Iman Yi Liao",
      "Massimo Tistarelli",
      "Zhe Jin"
    ],
    "abstract": "Determining dense feature points on fingerprints used in constructing deep\nfixed-length representations for accurate matching, particularly at the pixel\nlevel, is of significant interest. To explore the interpretability of\nfingerprint matching, we propose a multi-stage interpretable fingerprint\nmatching network, namely Interpretable Fixed-length Representation for\nFingerprint Matching via Vision Transformer (IFViT), which consists of two\nprimary modules. The first module, an interpretable dense registration module,\nestablishes a Vision Transformer (ViT)-based Siamese Network to capture\nlong-range dependencies and the global context in fingerprint pairs. It\nprovides interpretable dense pixel-wise correspondences of feature points for\nfingerprint alignment and enhances the interpretability in the subsequent\nmatching stage. The second module takes into account both local and global\nrepresentations of the aligned fingerprint pair to achieve an interpretable\nfixed-length representation extraction and matching. It employs the ViTs\ntrained in the first module with the additional fully connected layer and\nretrains them to simultaneously produce the discriminative fixed-length\nrepresentation and interpretable dense pixel-wise correspondences of feature\npoints. Extensive experimental results on diverse publicly available\nfingerprint databases demonstrate that the proposed framework not only exhibits\nsuperior performance on dense registration and matching but also significantly\npromotes the interpretability in deep fixed-length representations-based\nfingerprint matching.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "ready to submit to IEEE Transactions on Information Forensics and\n  Security (TIFS)",
    "pdf_url": "http://arxiv.org/pdf/2404.08237v1",
    "published_date": "2024-04-12 04:44:11 UTC",
    "updated_date": "2024-04-12 04:44:11 UTC"
  },
  {
    "arxiv_id": "2404.08233v2",
    "title": "Generalized Population-Based Training for Hyperparameter Optimization in Reinforcement Learning",
    "authors": [
      "Hui Bai",
      "Ran Cheng"
    ],
    "abstract": "Hyperparameter optimization plays a key role in the machine learning domain.\nIts significance is especially pronounced in reinforcement learning (RL), where\nagents continuously interact with and adapt to their environments, requiring\ndynamic adjustments in their learning trajectories. To cater to this\ndynamicity, the Population-Based Training (PBT) was introduced, leveraging the\ncollective intelligence of a population of agents learning simultaneously.\nHowever, PBT tends to favor high-performing agents, potentially neglecting the\nexplorative potential of agents on the brink of significant advancements. To\nmitigate the limitations of PBT, we present the Generalized Population-Based\nTraining (GPBT), a refined framework designed for enhanced granularity and\nflexibility in hyperparameter adaptation. Complementing GPBT, we further\nintroduce Pairwise Learning (PL). Instead of merely focusing on elite agents,\nPL employs a comprehensive pairwise strategy to identify performance\ndifferentials and provide holistic guidance to underperforming agents. By\nintegrating the capabilities of GPBT and PL, our approach significantly\nimproves upon traditional PBT in terms of adaptability and computational\nefficiency. Rigorous empirical evaluations across a range of RL benchmarks\nconfirm that our approach consistently outperforms not only the conventional\nPBT but also its Bayesian-optimized variant.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "IEEE Transactions on Emerging Topics in Computational Intelligence",
    "pdf_url": "http://arxiv.org/pdf/2404.08233v2",
    "published_date": "2024-04-12 04:23:20 UTC",
    "updated_date": "2024-04-23 03:28:31 UTC"
  },
  {
    "arxiv_id": "2404.08224v2",
    "title": "HCL-MTSAD: Hierarchical Contrastive Consistency Learning for Accurate Detection of Industrial Multivariate Time Series Anomalies",
    "authors": [
      "Haili Sun",
      "Yan Huang",
      "Lansheng Han",
      "Cai Fu",
      "Chunjie Zhou"
    ],
    "abstract": "Multivariate Time Series (MTS) anomaly detection focuses on pinpointing\nsamples that diverge from standard operational patterns, which is crucial for\nensuring the safety and security of industrial applications. The primary\nchallenge in this domain is to develop representations capable of discerning\nanomalies effectively. The prevalent methods for anomaly detection in the\nliterature are predominantly reconstruction-based and predictive in nature.\nHowever, they typically concentrate on a single-dimensional instance level,\nthereby not fully harnessing the complex associations inherent in industrial\nMTS. To address this issue, we propose a novel self-supervised hierarchical\ncontrastive consistency learning method for detecting anomalies in MTS, named\nHCL-MTSAD. It innovatively leverages data consistency at multiple levels\ninherent in industrial MTS, systematically capturing consistent associations\nacross four latent levels-measurement, sample, channel, and process. By\ndeveloping a multi-layer contrastive loss, HCL-MTSAD can extensively mine data\nconsistency and spatio-temporal association, resulting in more informative\nrepresentations. Subsequently, an anomaly discrimination module, grounded in\nself-supervised hierarchical contrastive learning, is designed to detect\ntimestamp-level anomalies by calculating multi-scale data consistency.\nExtensive experiments conducted on six diverse MTS datasets retrieved from real\ncyber-physical systems and server machines, in comparison with 20 baselines,\nindicate that HCL-MTSAD's anomaly detection capability outperforms the\nstate-of-the-art benchmark models by an average of 1.8\\% in terms of F1 score.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.IT",
      "cs.SY",
      "eess.SY",
      "math.IT"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper is a manuscript that is still in the process of revision,\n  including Table 1, Figure 2, problem definition in section III.B and method\n  description proposed in section IV. In addition, the submitter has not been\n  authorized by the first author and other co-authors to post the paper to\n  arXiv",
    "pdf_url": "http://arxiv.org/pdf/2404.08224v2",
    "published_date": "2024-04-12 03:39:33 UTC",
    "updated_date": "2024-04-18 04:15:35 UTC"
  },
  {
    "arxiv_id": "2404.08189v1",
    "title": "Reducing hallucination in structured outputs via Retrieval-Augmented Generation",
    "authors": [
      "Patrice Béchard",
      "Orlando Marquez Ayala"
    ],
    "abstract": "A common and fundamental limitation of Generative AI (GenAI) is its\npropensity to hallucinate. While large language models (LLM) have taken the\nworld by storm, without eliminating or at least reducing hallucinations,\nreal-world GenAI systems may face challenges in user adoption. In the process\nof deploying an enterprise application that produces workflows based on natural\nlanguage requirements, we devised a system leveraging Retrieval Augmented\nGeneration (RAG) to greatly improve the quality of the structured output that\nrepresents such workflows. Thanks to our implementation of RAG, our proposed\nsystem significantly reduces hallucinations in the output and improves the\ngeneralization of our LLM in out-of-domain settings. In addition, we show that\nusing a small, well-trained retriever encoder can reduce the size of the\naccompanying LLM, thereby making deployments of LLM-based systems less\nresource-intensive.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.LG",
    "comment": "To be presented at NAACL 2024. 11 pages and 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.08189v1",
    "published_date": "2024-04-12 01:42:09 UTC",
    "updated_date": "2024-04-12 01:42:09 UTC"
  },
  {
    "arxiv_id": "2404.15182v1",
    "title": "FLoRA: Enhancing Vision-Language Models with Parameter-Efficient Federated Learning",
    "authors": [
      "Duy Phuong Nguyen",
      "J. Pablo Munoz",
      "Ali Jannesari"
    ],
    "abstract": "In the rapidly evolving field of artificial intelligence, multimodal models,\ne.g., integrating vision and language into visual-language models (VLMs), have\nbecome pivotal for many applications, ranging from image captioning to\nmultimodal search engines. Among these models, the Contrastive Language-Image\nPre-training (CLIP) model has demonstrated remarkable performance in\nunderstanding and generating nuanced relationships between text and images.\nHowever, the conventional training of such models often requires centralized\naggregation of vast datasets, posing significant privacy and data governance\nchallenges. To address these concerns, this paper proposes a novel approach\nthat leverages Federated Learning and parameter-efficient adapters, i.e.,\nLow-Rank Adaptation (LoRA), to train VLMs. This methodology preserves data\nprivacy by training models across decentralized data sources and ensures model\nadaptability and efficiency through LoRA's parameter-efficient fine-tuning. Our\napproach accelerates training time by up to 34.72 times and requires 2.47 times\nless memory usage than full fine-tuning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.15182v1",
    "published_date": "2024-04-12 00:36:43 UTC",
    "updated_date": "2024-04-12 00:36:43 UTC"
  },
  {
    "arxiv_id": "2404.08164v2",
    "title": "Language Model Prompt Selection via Simulation Optimization",
    "authors": [
      "Haoting Zhang",
      "Jinghai He",
      "Rhonda Righter",
      "Zeyu Zheng"
    ],
    "abstract": "With the advancement in generative language models, the selection of prompts\nhas gained significant attention in recent years. A prompt is an instruction or\ndescription provided by the user, serving as a guide for the generative\nlanguage model in content generation. Despite existing methods for prompt\nselection that are based on human labor, we consider facilitating this\nselection through simulation optimization, aiming to maximize a pre-defined\nscore for the selected prompt. Specifically, we propose a two-stage framework.\nIn the first stage, we determine a feasible set of prompts in sufficient\nnumbers, where each prompt is represented by a moderate-dimensional vector. In\nthe subsequent stage for evaluation and selection, we construct a surrogate\nmodel of the score regarding the moderate-dimensional vectors that represent\nthe prompts. We propose sequentially selecting the prompt for evaluation based\non this constructed surrogate model. We prove the consistency of the sequential\nevaluation procedure in our framework. We also conduct numerical experiments to\ndemonstrate the efficacy of our proposed framework, providing practical\ninstructions for implementation.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.08164v2",
    "published_date": "2024-04-12 00:03:56 UTC",
    "updated_date": "2024-05-20 02:34:42 UTC"
  }
]
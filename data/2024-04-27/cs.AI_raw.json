[
  {
    "arxiv_id": "2404.18021v1",
    "title": "CRISPR-GPT: An LLM Agent for Automated Design of Gene-Editing Experiments",
    "authors": [
      "Kaixuan Huang",
      "Yuanhao Qu",
      "Henry Cousins",
      "William A. Johnson",
      "Di Yin",
      "Mihir Shah",
      "Denny Zhou",
      "Russ Altman",
      "Mengdi Wang",
      "Le Cong"
    ],
    "abstract": "The introduction of genome engineering technology has transformed biomedical\nresearch, making it possible to make precise changes to genetic information.\nHowever, creating an efficient gene-editing system requires a deep\nunderstanding of CRISPR technology, and the complex experimental systems under\ninvestigation. While Large Language Models (LLMs) have shown promise in various\ntasks, they often lack specific knowledge and struggle to accurately solve\nbiological design problems. In this work, we introduce CRISPR-GPT, an LLM agent\naugmented with domain knowledge and external tools to automate and enhance the\ndesign process of CRISPR-based gene-editing experiments. CRISPR-GPT leverages\nthe reasoning ability of LLMs to facilitate the process of selecting CRISPR\nsystems, designing guide RNAs, recommending cellular delivery methods, drafting\nprotocols, and designing validation experiments to confirm editing outcomes. We\nshowcase the potential of CRISPR-GPT for assisting non-expert researchers with\ngene-editing experiments from scratch and validate the agent's effectiveness in\na real-world use case. Furthermore, we explore the ethical and regulatory\nconsiderations associated with automated gene-editing design, highlighting the\nneed for responsible and transparent use of these tools. Our work aims to\nbridge the gap between beginner biological researchers and CRISPR genome\nengineering techniques, and demonstrate the potential of LLM agents in\nfacilitating complex biological discovery tasks.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "q-bio.QM"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.18021v1",
    "published_date": "2024-04-27 22:59:17 UTC",
    "updated_date": "2024-04-27 22:59:17 UTC"
  },
  {
    "arxiv_id": "2404.18001v1",
    "title": "LLMParser: An Exploratory Study on Using Large Language Models for Log Parsing",
    "authors": [
      "Zeyang Ma",
      "An Ran Chen",
      "Dong Jae Kim",
      "Tse-Hsun Chen",
      "Shaowei Wang"
    ],
    "abstract": "Logs are important in modern software development with runtime information.\nLog parsing is the first step in many log-based analyses, that involve\nextracting structured information from unstructured log data. Traditional log\nparsers face challenges in accurately parsing logs due to the diversity of log\nformats, which directly impacts the performance of downstream log-analysis\ntasks. In this paper, we explore the potential of using Large Language Models\n(LLMs) for log parsing and propose LLMParser, an LLM-based log parser based on\ngenerative LLMs and few-shot tuning. We leverage four LLMs, Flan-T5-small,\nFlan-T5-base, LLaMA-7B, and ChatGLM-6B in LLMParsers. Our evaluation of 16\nopen-source systems shows that LLMParser achieves statistically significantly\nhigher parsing accuracy than state-of-the-art parsers (a 96% average parsing\naccuracy). We further conduct a comprehensive empirical analysis on the effect\nof training size, model size, and pre-training LLM on log parsing accuracy. We\nfind that smaller LLMs may be more effective than more complex LLMs; for\ninstance where Flan-T5-base achieves comparable results as LLaMA-7B with a\nshorter inference time. We also find that using LLMs pre-trained using logs\nfrom other systems does not always improve parsing accuracy. While using\npre-trained Flan-T5-base shows an improvement in accuracy, pre-trained LLaMA\nresults in a decrease (decrease by almost 55% in group accuracy). In short, our\nstudy provides empirical evidence for using LLMs for log parsing and highlights\nthe limitations and future research direction of LLM-based log parsers.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.18001v1",
    "published_date": "2024-04-27 20:34:29 UTC",
    "updated_date": "2024-04-27 20:34:29 UTC"
  },
  {
    "arxiv_id": "2404.17999v1",
    "title": "MediFact at MEDIQA-CORR 2024: Why AI Needs a Human Touch",
    "authors": [
      "Nadia Saeed"
    ],
    "abstract": "Accurate representation of medical information is crucial for patient safety,\nyet artificial intelligence (AI) systems, such as Large Language Models (LLMs),\nencounter challenges in error-free clinical text interpretation. This paper\npresents a novel approach submitted to the MEDIQA-CORR 2024 shared task (Ben\nAbacha et al., 2024a), focusing on the automatic correction of single-word\nerrors in clinical notes. Unlike LLMs that rely on extensive generic data, our\nmethod emphasizes extracting contextually relevant information from available\nclinical text data. Leveraging an ensemble of extractive and abstractive\nquestion-answering approaches, we construct a supervised learning framework\nwith domain-specific feature engineering. Our methodology incorporates domain\nexpertise to enhance error correction accuracy. By integrating domain expertise\nand prioritizing meaningful information extraction, our approach underscores\nthe significance of a human-centric strategy in adapting AI for healthcare.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "7 pages, 4 figures, Clinical NLP 2024 Workshop",
    "pdf_url": "http://arxiv.org/pdf/2404.17999v1",
    "published_date": "2024-04-27 20:28:38 UTC",
    "updated_date": "2024-04-27 20:28:38 UTC"
  },
  {
    "arxiv_id": "2404.18952v1",
    "title": "CUE-Net: Violence Detection Video Analytics with Spatial Cropping, Enhanced UniformerV2 and Modified Efficient Additive Attention",
    "authors": [
      "Damith Chamalke Senadeera",
      "Xiaoyun Yang",
      "Dimitrios Kollias",
      "Gregory Slabaugh"
    ],
    "abstract": "In this paper we introduce CUE-Net, a novel architecture designed for\nautomated violence detection in video surveillance. As surveillance systems\nbecome more prevalent due to technological advances and decreasing costs, the\nchallenge of efficiently monitoring vast amounts of video data has intensified.\nCUE-Net addresses this challenge by combining spatial Cropping with an enhanced\nversion of the UniformerV2 architecture, integrating convolutional and\nself-attention mechanisms alongside a novel Modified Efficient Additive\nAttention mechanism (which reduces the quadratic time complexity of\nself-attention) to effectively and efficiently identify violent activities.\nThis approach aims to overcome traditional challenges such as capturing distant\nor partially obscured subjects within video frames. By focusing on both local\nand global spatiotemporal features, CUE-Net achieves state-of-the-art\nperformance on the RWF-2000 and RLVS datasets, surpassing existing methods.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "To be published in the proceedings of 2024 IEEE/CVF Conference on\n  Computer Vision and Pattern Recognition Workshops (CVPRW)",
    "pdf_url": "http://arxiv.org/pdf/2404.18952v1",
    "published_date": "2024-04-27 20:09:40 UTC",
    "updated_date": "2024-04-27 20:09:40 UTC"
  },
  {
    "arxiv_id": "2405.01583v1",
    "title": "MediFact at MEDIQA-M3G 2024: Medical Question Answering in Dermatology with Multimodal Learning",
    "authors": [
      "Nadia Saeed"
    ],
    "abstract": "The MEDIQA-M3G 2024 challenge necessitates novel solutions for Multilingual &\nMultimodal Medical Answer Generation in dermatology (wai Yim et al., 2024a).\nThis paper addresses the limitations of traditional methods by proposing a\nweakly supervised learning approach for open-ended medical question-answering\n(QA). Our system leverages readily available MEDIQA-M3G images via a\nVGG16-CNN-SVM model, enabling multilingual (English, Chinese, Spanish) learning\nof informative skin condition representations. Using pre-trained QA models, we\nfurther bridge the gap between visual and textual information through\nmultimodal fusion. This approach tackles complex, open-ended questions even\nwithout predefined answer choices. We empower the generation of comprehensive\nanswers by feeding the ViT-CLIP model with multiple responses alongside images.\nThis work advances medical QA research, paving the way for clinical decision\nsupport systems and ultimately improving healthcare delivery.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "7 pages, 3 figures, Clinical NLP 2024 workshop proceedings in Shared\n  Task",
    "pdf_url": "http://arxiv.org/pdf/2405.01583v1",
    "published_date": "2024-04-27 20:03:47 UTC",
    "updated_date": "2024-04-27 20:03:47 UTC"
  },
  {
    "arxiv_id": "2404.17985v1",
    "title": "Detection of Conspiracy Theories Beyond Keyword Bias in German-Language Telegram Using Large Language Models",
    "authors": [
      "Milena Pustet",
      "Elisabeth Steffen",
      "Helena MihaljeviÄ‡"
    ],
    "abstract": "The automated detection of conspiracy theories online typically relies on\nsupervised learning. However, creating respective training data requires\nexpertise, time and mental resilience, given the often harmful content.\nMoreover, available datasets are predominantly in English and often\nkeyword-based, introducing a token-level bias into the models. Our work\naddresses the task of detecting conspiracy theories in German Telegram\nmessages. We compare the performance of supervised fine-tuning approaches using\nBERT-like models with prompt-based approaches using Llama2, GPT-3.5, and GPT-4\nwhich require little or no additional training data. We use a dataset of\n$\\sim\\!\\! 4,000$ messages collected during the COVID-19 pandemic, without the\nuse of keyword filters.\n  Our findings demonstrate that both approaches can be leveraged effectively:\nFor supervised fine-tuning, we report an F1 score of $\\sim\\!\\! 0.8$ for the\npositive class, making our model comparable to recent models trained on\nkeyword-focused English corpora. We demonstrate our model's adaptability to\nintra-domain temporal shifts, achieving F1 scores of $\\sim\\!\\! 0.7$. Among\nprompting variants, the best model is GPT-4, achieving an F1 score of $\\sim\\!\\!\n0.8$ for the positive class in a zero-shot setting and equipped with a custom\nconspiracy theory definition.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to the 8th Workshop on Online Abuse and Harms (WOAH), ACL\n  2024",
    "pdf_url": "http://arxiv.org/pdf/2404.17985v1",
    "published_date": "2024-04-27 19:17:31 UTC",
    "updated_date": "2024-04-27 19:17:31 UTC"
  },
  {
    "arxiv_id": "2404.17984v1",
    "title": "Privacy-Preserving, Dropout-Resilient Aggregation in Decentralized Learning",
    "authors": [
      "Ali Reza Ghavamipour",
      "Benjamin Zi Hao Zhao",
      "Fatih Turkmen"
    ],
    "abstract": "Decentralized learning (DL) offers a novel paradigm in machine learning by\ndistributing training across clients without central aggregation, enhancing\nscalability and efficiency. However, DL's peer-to-peer model raises challenges\nin protecting against inference attacks and privacy leaks. By forgoing central\nbottlenecks, DL demands privacy-preserving aggregation methods to protect data\nfrom 'honest but curious' clients and adversaries, maintaining network-wide\nprivacy. Privacy-preserving DL faces the additional hurdle of client dropout,\nclients not submitting updates due to connectivity problems or unavailability,\nfurther complicating aggregation.\n  This work proposes three secret sharing-based dropout resilience approaches\nfor privacy-preserving DL. Our study evaluates the efficiency, performance, and\naccuracy of these protocols through experiments on datasets such as MNIST,\nFashion-MNIST, SVHN, and CIFAR-10. We compare our protocols with traditional\nsecret-sharing solutions across scenarios, including those with up to 1000\nclients. Evaluations show that our protocols significantly outperform\nconventional methods, especially in scenarios with up to 30% of clients dropout\nand model sizes of up to $10^6$ parameters. Our approaches demonstrate markedly\nhigh efficiency with larger models, higher dropout rates, and extensive client\nnetworks, highlighting their effectiveness in enhancing decentralized learning\nsystems' privacy and dropout robustness.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17984v1",
    "published_date": "2024-04-27 19:17:02 UTC",
    "updated_date": "2024-04-27 19:17:02 UTC"
  },
  {
    "arxiv_id": "2404.17977v2",
    "title": "Advancing Healthcare Automation: Multi-Agent System for Medical Necessity Justification",
    "authors": [
      "Himanshu Pandey",
      "Akhil Amod",
      "Shivang"
    ],
    "abstract": "Prior Authorization delivers safe, appropriate, and cost-effective care that\nis medically justified with evidence-based guidelines. However, the process\noften requires labor-intensive manual comparisons between patient medical\nrecords and clinical guidelines, that is both repetitive and time-consuming.\nRecent developments in Large Language Models (LLMs) have shown potential in\naddressing complex medical NLP tasks with minimal supervision. This paper\nexplores the application of Multi-Agent System (MAS) that utilize specialized\nLLM agents to automate Prior Authorization task by breaking them down into\nsimpler and manageable sub-tasks. Our study systematically investigates the\neffects of various prompting strategies on these agents and benchmarks the\nperformance of different LLMs. We demonstrate that GPT-4 achieves an accuracy\nof 86.2% in predicting checklist item-level judgments with evidence, and 95.6%\nin determining overall checklist judgment. Additionally, we explore how these\nagents can contribute to explainability of steps taken in the process, thereby\nenhancing trust and transparency in the system.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at BioNLP2024",
    "pdf_url": "http://arxiv.org/pdf/2404.17977v2",
    "published_date": "2024-04-27 18:40:05 UTC",
    "updated_date": "2024-07-06 09:29:16 UTC"
  },
  {
    "arxiv_id": "2404.17975v2",
    "title": "Automating Customer Needs Analysis: A Comparative Study of Large Language Models in the Travel Industry",
    "authors": [
      "Simone Barandoni",
      "Filippo Chiarello",
      "Lorenzo Cascone",
      "Emiliano Marrale",
      "Salvatore Puccio"
    ],
    "abstract": "In the rapidly evolving landscape of Natural Language Processing (NLP), Large\nLanguage Models (LLMs) have emerged as powerful tools for many tasks, such as\nextracting valuable insights from vast amounts of textual data. In this study,\nwe conduct a comparative analysis of LLMs for the extraction of travel customer\nneeds from TripAdvisor and Reddit posts. Leveraging a diverse range of models,\nincluding both open-source and proprietary ones such as GPT-4 and Gemini, we\naim to elucidate their strengths and weaknesses in this specialized domain.\nThrough an evaluation process involving metrics such as BERTScore, ROUGE, and\nBLEU, we assess the performance of each model in accurately identifying and\nsummarizing customer needs. Our findings highlight the efficacy of opensource\nLLMs, particularly Mistral 7B, in achieving comparable performance to larger\nclosed models while offering affordability and customization benefits.\nAdditionally, we underscore the importance of considering factors such as model\nsize, resource requirements, and performance metrics when selecting the most\nsuitable LLM for customer needs analysis tasks. Overall, this study contributes\nvaluable insights for businesses seeking to leverage advanced NLP techniques to\nenhance customer experience and drive operational efficiency in the travel\nindustry.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17975v2",
    "published_date": "2024-04-27 18:28:10 UTC",
    "updated_date": "2025-04-09 10:21:07 UTC"
  },
  {
    "arxiv_id": "2404.17970v1",
    "title": "Privacy-Preserving Aggregation for Decentralized Learning with Byzantine-Robustness",
    "authors": [
      "Ali Reza Ghavamipour",
      "Benjamin Zi Hao Zhao",
      "Oguzhan Ersoy",
      "Fatih Turkmen"
    ],
    "abstract": "Decentralized machine learning (DL) has been receiving an increasing interest\nrecently due to the elimination of a single point of failure, present in\nFederated learning setting. Yet, it is threatened by the looming threat of\nByzantine clients who intentionally disrupt the learning process by\nbroadcasting arbitrary model updates to other clients, seeking to degrade the\nperformance of the global model. In response, robust aggregation schemes have\nemerged as promising solutions to defend against such Byzantine clients,\nthereby enhancing the robustness of Decentralized Learning. Defenses against\nByzantine adversaries, however, typically require access to the updates of\nother clients, a counterproductive privacy trade-off that in turn increases the\nrisk of inference attacks on those same model updates.\n  In this paper, we introduce SecureDL, a novel DL protocol designed to enhance\nthe security and privacy of DL against Byzantine threats. SecureDL~facilitates\na collaborative defense, while protecting the privacy of clients' model updates\nthrough secure multiparty computation. The protocol employs efficient\ncomputation of cosine similarity and normalization of updates to robustly\ndetect and exclude model updates detrimental to model convergence. By using\nMNIST, Fashion-MNIST, SVHN and CIFAR-10 datasets, we evaluated SecureDL against\nvarious Byzantine attacks and compared its effectiveness with four existing\ndefense mechanisms. Our experiments show that SecureDL is effective even in the\ncase of attacks by the malicious majority (e.g., 80% Byzantine clients) while\npreserving high training accuracy.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17970v1",
    "published_date": "2024-04-27 18:17:36 UTC",
    "updated_date": "2024-04-27 18:17:36 UTC"
  },
  {
    "arxiv_id": "2404.17962v2",
    "title": "Deep Learning for Low-Latency, Quantum-Ready RF Sensing",
    "authors": [
      "Pranav Gokhale",
      "Caitlin Carnahan",
      "William Clark",
      "Teague Tomesh",
      "Frederic T. Chong"
    ],
    "abstract": "Recent work has shown the promise of applying deep learning to enhance\nsoftware processing of radio frequency (RF) signals. In parallel, hardware\ndevelopments with quantum RF sensors based on Rydberg atoms are breaking\nlongstanding barriers in frequency range, resolution, and sensitivity. In this\npaper, we describe our implementations of quantum-ready machine learning\napproaches for RF signal classification. Our primary objective is latency:\nwhile deep learning offers a more powerful computational paradigm, it also\ntraditionally incurs latency overheads that hinder wider scale deployment. Our\nwork spans three axes. (1) A novel continuous wavelet transform (CWT) based\nrecurrent neural network (RNN) architecture that enables flexible online\nclassification of RF signals on-the-fly with reduced sampling time. (2)\nLow-latency inference techniques for both GPU and CPU that span over 100x\nreductions in inference time, enabling real-time operation with sub-millisecond\ninference. (3) Quantum-readiness validated through application of our models to\nphysics-based simulation of Rydberg atom QRF sensors. Altogether, our work\nbridges towards next-generation RF sensors that use quantum technology to\nsurpass previous physical limits, paired with latency-optimized AI/ML software\nthat is suitable for real-time deployment.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG",
      "cs.PF",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "quant-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17962v2",
    "published_date": "2024-04-27 17:22:12 UTC",
    "updated_date": "2025-04-23 03:33:09 UTC"
  },
  {
    "arxiv_id": "2404.17947v1",
    "title": "Bounding the Expected Robustness of Graph Neural Networks Subject to Node Feature Attacks",
    "authors": [
      "Yassine Abbahaddou",
      "Sofiane Ennadir",
      "Johannes F. Lutzeyer",
      "Michalis Vazirgiannis",
      "Henrik BostrÃ¶m"
    ],
    "abstract": "Graph Neural Networks (GNNs) have demonstrated state-of-the-art performance\nin various graph representation learning tasks. Recently, studies revealed\ntheir vulnerability to adversarial attacks. In this work, we theoretically\ndefine the concept of expected robustness in the context of attributed graphs\nand relate it to the classical definition of adversarial robustness in the\ngraph representation learning literature. Our definition allows us to derive an\nupper bound of the expected robustness of Graph Convolutional Networks (GCNs)\nand Graph Isomorphism Networks subject to node feature attacks. Building on\nthese findings, we connect the expected robustness of GNNs to the\northonormality of their weight matrices and consequently propose an\nattack-independent, more robust variant of the GCN, called the Graph\nConvolutional Orthonormal Robust Networks (GCORNs). We further introduce a\nprobabilistic method to estimate the expected robustness, which allows us to\nevaluate the effectiveness of GCORN on several real-world datasets.\nExperimental experiments showed that GCORN outperforms available defense\nmethods. Our code is publicly available at:\n\\href{https://github.com/Sennadir/GCORN}{https://github.com/Sennadir/GCORN}.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.17947v1",
    "published_date": "2024-04-27 15:57:35 UTC",
    "updated_date": "2024-04-27 15:57:35 UTC"
  },
  {
    "arxiv_id": "2404.17943v2",
    "title": "Deep Representation Learning for Forecasting Recursive and Multi-Relational Events in Temporal Networks",
    "authors": [
      "Tony Gracious",
      "Ambedkar Dukkipati"
    ],
    "abstract": "Understanding relations arising out of interactions among entities can be\nvery difficult, and predicting them is even more challenging. This problem has\nmany applications in various fields, such as financial networks and e-commerce.\nThese relations can involve much more complexities than just involving more\nthan two entities. One such scenario is evolving recursive relations between\nmultiple entities, and so far, this is still an open problem. This work\naddresses the problem of forecasting higher-order interaction events that can\nbe multi-relational and recursive. We pose the problem in the framework of\nrepresentation learning of temporal hypergraphs that can capture complex\nrelationships involving multiple entities. The proposed model,\n\\textit{Relational Recursive Hyperedge Temporal Point Process} (RRHyperTPP)\nuses an encoder that learns a dynamic node representation based on the\nhistorical interaction patterns and then a hyperedge link prediction-based\ndecoder to model the occurrence of interaction events. These learned\nrepresentations are then used for downstream tasks involving forecasting the\ntype and time of interactions. The main challenge in learning from hyperedge\nevents is that the number of possible hyperedges grows exponentially with the\nnumber of nodes in the network. This will make the computation of negative\nlog-likelihood of the temporal point process expensive, as the calculation of\nsurvival function requires a summation over all possible hyperedges. In our\nwork, we develop a noise contrastive estimation method to learn the parameters\nof our model, and we have experimentally shown that our models perform better\nthan previous state-of-the-art methods for interaction forecasting.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "AAAI-2025",
    "pdf_url": "http://arxiv.org/pdf/2404.17943v2",
    "published_date": "2024-04-27 15:46:54 UTC",
    "updated_date": "2024-12-18 16:33:51 UTC"
  },
  {
    "arxiv_id": "2404.17937v1",
    "title": "DTization: A New Method for Supervised Feature Scaling",
    "authors": [
      "Niful Islam"
    ],
    "abstract": "Artificial intelligence is currently a dominant force in shaping various\naspects of the world. Machine learning is a sub-field in artificial\nintelligence. Feature scaling is one of the data pre-processing techniques that\nimproves the performance of machine learning algorithms. The traditional\nfeature scaling techniques are unsupervised where they do not have influence of\nthe dependent variable in the scaling process. In this paper, we have presented\na novel feature scaling technique named DTization that employs decision tree\nand robust scaler for supervised feature scaling. The proposed method utilizes\ndecision tree to measure the feature importance and based on the importance,\ndifferent features get scaled differently with the robust scaler algorithm. The\nproposed method has been extensively evaluated on ten classification and\nregression datasets on various evaluation matrices and the results show a\nnoteworthy performance improvement compared to the traditional feature scaling\nmethods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17937v1",
    "published_date": "2024-04-27 15:25:03 UTC",
    "updated_date": "2024-04-27 15:25:03 UTC"
  },
  {
    "arxiv_id": "2404.17930v1",
    "title": "Multi-Stream Cellular Test-Time Adaptation of Real-Time Models Evolving in Dynamic Environments",
    "authors": [
      "BenoÃ®t GÃ©rin",
      "AnaÃ¯s Halin",
      "Anthony Cioppa",
      "Maxim Henry",
      "Bernard Ghanem",
      "BenoÃ®t Macq",
      "Christophe De Vleeschouwer",
      "Marc Van Droogenbroeck"
    ],
    "abstract": "In the era of the Internet of Things (IoT), objects connect through a dynamic\nnetwork, empowered by technologies like 5G, enabling real-time data sharing.\nHowever, smart objects, notably autonomous vehicles, face challenges in\ncritical local computations due to limited resources. Lightweight AI models\noffer a solution but struggle with diverse data distributions. To address this\nlimitation, we propose a novel Multi-Stream Cellular Test-Time Adaptation\n(MSC-TTA) setup where models adapt on the fly to a dynamic environment divided\ninto cells. Then, we propose a real-time adaptive student-teacher method that\nleverages the multiple streams available in each cell to quickly adapt to\nchanging data distributions. We validate our methodology in the context of\nautonomous vehicles navigating across cells defined based on location and\nweather conditions. To facilitate future benchmarking, we release a new\nmulti-stream large-scale synthetic semantic segmentation dataset, called DADE,\nand show that our multi-stream approach outperforms a single-stream baseline.\nWe believe that our work will open research opportunities in the IoT and 5G\neras, offering solutions for real-time model adaptation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17930v1",
    "published_date": "2024-04-27 15:00:57 UTC",
    "updated_date": "2024-04-27 15:00:57 UTC"
  },
  {
    "arxiv_id": "2404.17929v1",
    "title": "Spatio-Temporal Side Tuning Pre-trained Foundation Models for Video-based Pedestrian Attribute Recognition",
    "authors": [
      "Xiao Wang",
      "Qian Zhu",
      "Jiandong Jin",
      "Jun Zhu",
      "Futian Wang",
      "Bo Jiang",
      "Yaowei Wang",
      "Yonghong Tian"
    ],
    "abstract": "Existing pedestrian attribute recognition (PAR) algorithms are mainly\ndeveloped based on a static image, however, the performance is unreliable in\nchallenging scenarios, such as heavy occlusion, motion blur, etc. In this work,\nwe propose to understand human attributes using video frames that can fully use\ntemporal information by fine-tuning a pre-trained multi-modal foundation model\nefficiently. Specifically, we formulate the video-based PAR as a\nvision-language fusion problem and adopt a pre-trained foundation model CLIP to\nextract the visual features. More importantly, we propose a novel\nspatiotemporal side-tuning strategy to achieve parameter-efficient optimization\nof the pre-trained vision foundation model. To better utilize the semantic\ninformation, we take the full attribute list that needs to be recognized as\nanother input and transform the attribute words/phrases into the corresponding\nsentence via split, expand, and prompt operations. Then, the text encoder of\nCLIP is utilized for embedding processed attribute descriptions. The averaged\nvisual tokens and text tokens are concatenated and fed into a fusion\nTransformer for multi-modal interactive learning. The enhanced tokens will be\nfed into a classification head for pedestrian attribute prediction. Extensive\nexperiments on two large-scale video-based PAR datasets fully validated the\neffectiveness of our proposed framework. The source code of this paper is\navailable at https://github.com/Event-AHU/OpenPAR.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Parameter Efficient Fine-Tuning Strategy for Video-based Pedestrian\n  Attribute Recognition",
    "pdf_url": "http://arxiv.org/pdf/2404.17929v1",
    "published_date": "2024-04-27 14:43:32 UTC",
    "updated_date": "2024-04-27 14:43:32 UTC"
  },
  {
    "arxiv_id": "2404.17926v1",
    "title": "Pre-training on High Definition X-ray Images: An Experimental Study",
    "authors": [
      "Xiao Wang",
      "Yuehang Li",
      "Wentao Wu",
      "Jiandong Jin",
      "Yao Rong",
      "Bo Jiang",
      "Chuanfu Li",
      "Jin Tang"
    ],
    "abstract": "Existing X-ray based pre-trained vision models are usually conducted on a\nrelatively small-scale dataset (less than 500k samples) with limited resolution\n(e.g., 224 $\\times$ 224). However, the key to the success of self-supervised\npre-training large models lies in massive training data, and maintaining high\nresolution in the field of X-ray images is the guarantee of effective solutions\nto difficult miscellaneous diseases. In this paper, we address these issues by\nproposing the first high-definition (1280 $\\times$ 1280) X-ray based\npre-trained foundation vision model on our newly collected large-scale dataset\nwhich contains more than 1 million X-ray images. Our model follows the masked\nauto-encoder framework which takes the tokens after mask processing (with a\nhigh rate) is used as input, and the masked image patches are reconstructed by\nthe Transformer encoder-decoder network. More importantly, we introduce a novel\ncontext-aware masking strategy that utilizes the chest contour as a boundary\nfor adaptive masking operations. We validate the effectiveness of our model on\ntwo downstream tasks, including X-ray report generation and disease\nrecognition. Extensive experiments demonstrate that our pre-trained medical\nfoundation vision model achieves comparable or even new state-of-the-art\nperformance on downstream benchmark datasets. The source code and pre-trained\nmodels of this paper will be released on\nhttps://github.com/Event-AHU/Medical_Image_Analysis.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "Technology Report",
    "pdf_url": "http://arxiv.org/pdf/2404.17926v1",
    "published_date": "2024-04-27 14:29:53 UTC",
    "updated_date": "2024-04-27 14:29:53 UTC"
  },
  {
    "arxiv_id": "2404.17924v2",
    "title": "Results about sets of desirable gamble sets",
    "authors": [
      "Catrin Campbell-Moore"
    ],
    "abstract": "Coherent sets of desirable gamble sets is used as a model for representing an\nagents opinions and choice preferences under uncertainty. In this paper we\nprovide some results about the axioms required for coherence and the natural\nextension of a given set of desirable gamble sets. We also show that coherent\nsets of desirable gamble sets can be represented by a proper filter of coherent\nsets of desirable gambles.",
    "categories": [
      "cs.AI",
      "cs.GT"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17924v2",
    "published_date": "2024-04-27 14:29:13 UTC",
    "updated_date": "2024-05-16 16:35:01 UTC"
  },
  {
    "arxiv_id": "2404.17916v2",
    "title": "FedCRL: Personalized Federated Learning with Contrastive Shared Representations for Label Heterogeneity in Non-IID Data",
    "authors": [
      "Chenghao Huang",
      "Xiaolu Chen",
      "Yanru Zhang",
      "Hao Wang"
    ],
    "abstract": "Heterogeneity resulting from label distribution skew and data scarcity can\nlead to inaccuracy and unfairness in intelligent communication applications\nthat mainly rely on distributed computing. To deal with it, this paper proposes\na novel personalized federated learning algorithm, named Federated Contrastive\nShareable Representations (FedCoSR), to facilitate knowledge sharing among\nclients while maintaining data privacy. Specifically, parameters of local\nmodels' shallow layers and typical local representations are both considered\nshareable information for the server and aggregated globally. To address poor\nperformance caused by label distribution skew among clients, contrastive\nlearning is adopted between local and global representations to enrich local\nknowledge. Additionally, to ensure fairness for clients with scarce data,\nFedCoSR introduces adaptive local aggregation to coordinate the global model\ninvolvement in each client. Our simulations demonstrate FedCoSR's effectiveness\nin mitigating label heterogeneity by achieving accuracy and fairness\nimprovements over existing methods on datasets with varying degrees of label\nheterogeneity.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17916v2",
    "published_date": "2024-04-27 14:05:18 UTC",
    "updated_date": "2024-11-22 12:51:47 UTC"
  },
  {
    "arxiv_id": "2404.17912v2",
    "title": "SERPENT-VLM : Self-Refining Radiology Report Generation Using Vision Language Models",
    "authors": [
      "Manav Nitin Kapadnis",
      "Sohan Patnaik",
      "Abhilash Nandy",
      "Sourjyadip Ray",
      "Pawan Goyal",
      "Debdoot Sheet"
    ],
    "abstract": "Radiology Report Generation (R2Gen) demonstrates how Multi-modal Large\nLanguage Models (MLLMs) can automate the creation of accurate and coherent\nradiological reports. Existing methods often hallucinate details in text-based\nreports that don't accurately reflect the image content. To mitigate this, we\nintroduce a novel strategy, SERPENT-VLM (SElf Refining Radiology RePort\nGENeraTion using Vision Language Models), which improves the R2Gen task by\nintegrating a self-refining mechanism into the MLLM framework. We employ a\nunique self-supervised loss that leverages similarity between pooled image\nrepresentations and the contextual representations of the generated\nradiological text, alongside the standard Causal Language Modeling objective,\nto refine image-text representations. This allows the model to scrutinize and\nalign the generated text through dynamic interaction between a given image and\nthe generated text, therefore reducing hallucination and continuously enhancing\nnuanced report generation. SERPENT-VLM outperforms existing baselines such as\nLLaVA-Med, BiomedGPT, etc., achieving SoTA performance on the IU X-ray and\nRadiology Objects in COntext (ROCO) datasets, and also proves to be robust\nagainst noisy images. A qualitative case study emphasizes the significant\nadvancements towards more sophisticated MLLM frameworks for R2Gen, opening\npaths for further research into self-supervised refinement in the medical\nimaging domain.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages, 3 figures, 4 tables, Accepted as oral at Clinical NLP\n  workshop at NAACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.17912v2",
    "published_date": "2024-04-27 13:46:23 UTC",
    "updated_date": "2024-07-18 16:03:18 UTC"
  },
  {
    "arxiv_id": "2404.17892v1",
    "title": "Shared learning of powertrain control policies for vehicle fleets",
    "authors": [
      "Lindsey Kerbel",
      "Beshah Ayalew",
      "Andrej Ivanco"
    ],
    "abstract": "Emerging data-driven approaches, such as deep reinforcement learning (DRL),\naim at on-the-field learning of powertrain control policies that optimize fuel\neconomy and other performance metrics. Indeed, they have shown great potential\nin this regard for individual vehicles on specific routes or drive cycles.\nHowever, for fleets of vehicles that must service a distribution of routes, DRL\napproaches struggle with learning stability issues that result in high\nvariances and challenge their practical deployment. In this paper, we present a\nnovel framework for shared learning among a fleet of vehicles through the use\nof a distilled group policy as the knowledge sharing mechanism for the policy\nlearning computations at each vehicle. We detail the mathematical formulation\nthat makes this possible. Several scenarios are considered to analyze the\nfunctionality, performance, and computational scalability of the framework with\nfleet size. Comparisons of the cumulative performance of fleets using our\nproposed shared learning approach with a baseline of individual learning agents\nand another state-of-the-art approach with a centralized learner show clear\nadvantages to our approach. For example, we find a fleet average asymptotic\nimprovement of 8.5 percent in fuel economy compared to the baseline while also\nimproving on the metrics of acceleration error and shifting frequency for\nfleets serving a distribution of suburban routes. Furthermore, we include\ndemonstrative results that show how the framework reduces variance within a\nfleet and also how it helps individual agents adapt better to new routes.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.LG",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17892v1",
    "published_date": "2024-04-27 13:01:05 UTC",
    "updated_date": "2024-04-27 13:01:05 UTC"
  },
  {
    "arxiv_id": "2404.17890v2",
    "title": "DPER: Diffusion Prior Driven Neural Representation for Limited Angle and Sparse View CT Reconstruction",
    "authors": [
      "Chenhe Du",
      "Xiyue Lin",
      "Qing Wu",
      "Xuanyu Tian",
      "Ying Su",
      "Zhe Luo",
      "Rui Zheng",
      "Yang Chen",
      "Hongjiang Wei",
      "S. Kevin Zhou",
      "Jingyi Yu",
      "Yuyao Zhang"
    ],
    "abstract": "Limited-angle and sparse-view computed tomography (LACT and SVCT) are crucial\nfor expanding the scope of X-ray CT applications. However, they face challenges\ndue to incomplete data acquisition, resulting in diverse artifacts in the\nreconstructed CT images. Emerging implicit neural representation (INR)\ntechniques, such as NeRF, NeAT, and NeRP, have shown promise in\nunder-determined CT imaging reconstruction tasks. However, the unsupervised\nnature of INR architecture imposes limited constraints on the solution space,\nparticularly for the highly ill-posed reconstruction task posed by LACT and\nultra-SVCT. In this study, we introduce the Diffusion Prior Driven Neural\nRepresentation (DPER), an advanced unsupervised framework designed to address\nthe exceptionally ill-posed CT reconstruction inverse problems. DPER adopts the\nHalf Quadratic Splitting (HQS) algorithm to decompose the inverse problem into\ndata fidelity and distribution prior sub-problems. The two sub-problems are\nrespectively addressed by INR reconstruction scheme and pre-trained score-based\ndiffusion model. This combination first injects the implicit image local\nconsistency prior from INR. Additionally, it effectively augments the\nfeasibility of the solution space for the inverse problem through the\ngenerative diffusion model, resulting in increased stability and precision in\nthe solutions. We conduct comprehensive experiments to evaluate the performance\nof DPER on LACT and ultra-SVCT reconstruction with two public datasets (AAPM\nand LIDC), an in-house clinical COVID-19 dataset and a public raw projection\ndataset created by Mayo Clinic. The results show that our method outperforms\nthe state-of-the-art reconstruction methods on in-domain datasets, while\nachieving significant performance improvements on out-of-domain (OOD) datasets.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "I.2.10; I.4.5"
    ],
    "primary_category": "eess.IV",
    "comment": "16 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.17890v2",
    "published_date": "2024-04-27 12:55:13 UTC",
    "updated_date": "2024-07-19 08:12:24 UTC"
  },
  {
    "arxiv_id": "2404.17886v1",
    "title": "Feature graphs for interpretable unsupervised tree ensembles: centrality, interaction, and application in disease subtyping",
    "authors": [
      "Christel Sirocchi",
      "Martin Urschler",
      "Bastian Pfeifer"
    ],
    "abstract": "Interpretable machine learning has emerged as central in leveraging\nartificial intelligence within high-stakes domains such as healthcare, where\nunderstanding the rationale behind model predictions is as critical as\nachieving high predictive accuracy. In this context, feature selection assumes\na pivotal role in enhancing model interpretability by identifying the most\nimportant input features in black-box models. While random forests are\nfrequently used in biomedicine for their remarkable performance on tabular\ndatasets, the accuracy gained from aggregating decision trees comes at the\nexpense of interpretability. Consequently, feature selection for enhancing\ninterpretability in random forests has been extensively explored in supervised\nsettings. However, its investigation in the unsupervised regime remains notably\nlimited. To address this gap, the study introduces novel methods to construct\nfeature graphs from unsupervised random forests and feature selection\nstrategies to derive effective feature combinations from these graphs. Feature\ngraphs are constructed for the entire dataset as well as individual clusters\nleveraging the parent-child node splits within the trees, such that feature\ncentrality captures their relevance to the clustering task, while edge weights\nreflect the discriminating power of feature pairs. Graph-based feature\nselection methods are extensively evaluated on synthetic and benchmark datasets\nboth in terms of their ability to reduce dimensionality while improving\nclustering performance, as well as to enhance model interpretability. An\napplication on omics data for disease subtyping identifies the top features for\neach cluster, showcasing the potential of the proposed approach to enhance\ninterpretability in clustering analyses and its utility in a real-world\nbiomedical application.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2.1; I.5.3; J.3"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17886v1",
    "published_date": "2024-04-27 12:47:37 UTC",
    "updated_date": "2024-04-27 12:47:37 UTC"
  },
  {
    "arxiv_id": "2404.17871v4",
    "title": "Deep Learning Library Testing: Definition, Methods and Challenges",
    "authors": [
      "Xiaoyu Zhang",
      "Weipeng Jiang",
      "Chao Shen",
      "Qi Li",
      "Qian Wang",
      "Chenhao Lin",
      "Xiaohong Guan"
    ],
    "abstract": "In recent years, software systems powered by deep learning (DL) techniques\nhave significantly facilitated people's lives in many aspects. As the backbone\nof these DL systems, various DL libraries undertake the underlying optimization\nand computation. However, like traditional software, DL libraries are not\nimmune to bugs, which can pose serious threats to users' personal property and\nsafety. Studying the characteristics of DL libraries, their associated bugs,\nand the corresponding testing methods is crucial for enhancing the security of\nDL systems and advancing the widespread application of DL technology. This\npaper provides an overview of the testing research related to various DL\nlibraries, discusses the strengths and weaknesses of existing methods, and\nprovides guidance and reference for the application of the DL library. This\npaper first introduces the workflow of DL underlying libraries and the\ncharacteristics of three kinds of DL libraries involved, namely DL framework,\nDL compiler, and DL hardware library. It then provides definitions for DL\nunderlying library bugs and testing. Additionally, this paper summarizes the\nexisting testing methods and tools tailored to these DL libraries separately\nand analyzes their effectiveness and limitations. It also discusses the\nexisting challenges of DL library testing and outlines potential directions for\nfuture research.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "37 pages, 10 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2404.17871v4",
    "published_date": "2024-04-27 11:42:13 UTC",
    "updated_date": "2025-02-05 02:29:52 UTC"
  },
  {
    "arxiv_id": "2404.17865v1",
    "title": "Vision-based Discovery of Nonlinear Dynamics for 3D Moving Target",
    "authors": [
      "Zitong Zhang",
      "Yang Liu",
      "Hao Sun"
    ],
    "abstract": "Data-driven discovery of governing equations has kindled significant\ninterests in many science and engineering areas. Existing studies primarily\nfocus on uncovering equations that govern nonlinear dynamics based on direct\nmeasurement of the system states (e.g., trajectories). Limited efforts have\nbeen placed on distilling governing laws of dynamics directly from videos for\nmoving targets in a 3D space. To this end, we propose a vision-based approach\nto automatically uncover governing equations of nonlinear dynamics for 3D\nmoving targets via raw videos recorded by a set of cameras. The approach is\ncomposed of three key blocks: (1) a target tracking module that extracts plane\npixel motions of the moving target in each video, (2) a Rodrigues' rotation\nformula-based coordinate transformation learning module that reconstructs the\n3D coordinates with respect to a predefined reference point, and (3) a\nspline-enhanced library-based sparse regressor that uncovers the underlying\ngoverning law of dynamics. This framework is capable of effectively handling\nthe challenges associated with measurement data, e.g., noise in the video,\nimprecise tracking of the target that causes data missing, etc. The efficacy of\nour method has been demonstrated through multiple sets of synthetic videos\nconsidering different nonlinear dynamics.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "nlin.CD"
    ],
    "primary_category": "cs.CV",
    "comment": "17 pages",
    "pdf_url": "http://arxiv.org/pdf/2404.17865v1",
    "published_date": "2024-04-27 11:13:55 UTC",
    "updated_date": "2024-04-27 11:13:55 UTC"
  },
  {
    "arxiv_id": "2404.17854v1",
    "title": "GLIMS: Attention-Guided Lightweight Multi-Scale Hybrid Network for Volumetric Semantic Segmentation",
    "authors": [
      "Ziya Ata YazÄ±cÄ±",
      "Ä°lkay Ã–ksÃ¼z",
      "HazÄ±m Kemal Ekenel"
    ],
    "abstract": "Convolutional Neural Networks (CNNs) have become widely adopted for medical\nimage segmentation tasks, demonstrating promising performance. However, the\ninherent inductive biases in convolutional architectures limit their ability to\nmodel long-range dependencies and spatial correlations. While recent\ntransformer-based architectures address these limitations by leveraging\nself-attention mechanisms to encode long-range dependencies and learn\nexpressive representations, they often struggle to extract low-level features\nand are highly dependent on data availability. This motivated us for the\ndevelopment of GLIMS, a data-efficient attention-guided hybrid volumetric\nsegmentation network. GLIMS utilizes Dilated Feature Aggregator Convolutional\nBlocks (DACB) to capture local-global feature correlations efficiently.\nFurthermore, the incorporated Swin Transformer-based bottleneck bridges the\nlocal and global features to improve the robustness of the model. Additionally,\nGLIMS employs an attention-guided segmentation approach through Channel and\nSpatial-Wise Attention Blocks (CSAB) to localize expressive features for\nfine-grained border segmentation. Quantitative and qualitative results on\nglioblastoma and multi-organ CT segmentation tasks demonstrate GLIMS'\neffectiveness in terms of complexity and accuracy. GLIMS demonstrated\noutstanding performance on BraTS2021 and BTCV datasets, surpassing the\nperformance of Swin UNETR. Notably, GLIMS achieved this high performance with a\nsignificantly reduced number of trainable parameters. Specifically, GLIMS has\n47.16M trainable parameters and 72.30G FLOPs, while Swin UNETR has 61.98M\ntrainable parameters and 394.84G FLOPs. The code is publicly available on\nhttps://github.com/yaziciz/GLIMS.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "The article was accepted for publication in the Image and Vision\n  Computing journal",
    "pdf_url": "http://arxiv.org/pdf/2404.17854v1",
    "published_date": "2024-04-27 10:18:55 UTC",
    "updated_date": "2024-04-27 10:18:55 UTC"
  },
  {
    "arxiv_id": "2404.17842v1",
    "title": "Using LLMs in Software Requirements Specifications: An Empirical Evaluation",
    "authors": [
      "Madhava Krishna",
      "Bhagesh Gaur",
      "Arsh Verma",
      "Pankaj Jalote"
    ],
    "abstract": "The creation of a Software Requirements Specification (SRS) document is\nimportant for any software development project. Given the recent prowess of\nLarge Language Models (LLMs) in answering natural language queries and\ngenerating sophisticated textual outputs, our study explores their capability\nto produce accurate, coherent, and structured drafts of these documents to\naccelerate the software development lifecycle. We assess the performance of\nGPT-4 and CodeLlama in drafting an SRS for a university club management system\nand compare it against human benchmarks using eight distinct criteria. Our\nresults suggest that LLMs can match the output quality of an entry-level\nsoftware engineer to generate an SRS, delivering complete and consistent\ndrafts. We also evaluate the capabilities of LLMs to identify and rectify\nproblems in a given requirements document. Our experiments indicate that GPT-4\nis capable of identifying issues and giving constructive feedback for\nrectifying them, while CodeLlama's results for validation were not as\nencouraging. We repeated the generation exercise for four distinct use cases to\nstudy the time saved by employing LLMs for SRS generation. The experiment\ndemonstrates that LLMs may facilitate a significant reduction in development\ntime for entry-level software engineers. Hence, we conclude that the LLMs can\nbe gainfully used by software engineers to increase productivity by saving time\nand effort in generating, validating and rectifying software requirements.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted to RE@Next! at the IEEE International Requirements\n  Engineering Conference 2024 at Reykjavik, Iceland",
    "pdf_url": "http://arxiv.org/pdf/2404.17842v1",
    "published_date": "2024-04-27 09:37:00 UTC",
    "updated_date": "2024-04-27 09:37:00 UTC"
  },
  {
    "arxiv_id": "2404.17833v1",
    "title": "Testing and Understanding Erroneous Planning in LLM Agents through Synthesized User Inputs",
    "authors": [
      "Zhenlan Ji",
      "Daoyuan Wu",
      "Pingchuan Ma",
      "Zongjie Li",
      "Shuai Wang"
    ],
    "abstract": "Agents based on large language models (LLMs) have demonstrated effectiveness\nin solving a wide range of tasks by integrating LLMs with key modules such as\nplanning, memory, and tool usage. Increasingly, customers are adopting LLM\nagents across a variety of commercial applications critical to reliability,\nincluding support for mental well-being, chemical synthesis, and software\ndevelopment. Nevertheless, our observations and daily use of LLM agents\nindicate that they are prone to making erroneous plans, especially when the\ntasks are complex and require long-term planning.\n  In this paper, we propose PDoctor, a novel and automated approach to testing\nLLM agents and understanding their erroneous planning. As the first work in\nthis direction, we formulate the detection of erroneous planning as a\nconstraint satisfiability problem: an LLM agent's plan is considered erroneous\nif its execution violates the constraints derived from the user inputs. To this\nend, PDoctor first defines a domain-specific language (DSL) for user queries\nand synthesizes varying inputs with the assistance of the Z3 constraint solver.\nThese synthesized inputs are natural language paragraphs that specify the\nrequirements for completing a series of tasks. Then, PDoctor derives\nconstraints from these requirements to form a testing oracle. We evaluate\nPDoctor with three mainstream agent frameworks and two powerful LLMs (GPT-3.5\nand GPT-4). The results show that PDoctor can effectively detect diverse errors\nin agent planning and provide insights and error characteristics that are\nvaluable to both agent developers and users. We conclude by discussing\npotential alternative designs and directions to extend PDoctor.",
    "categories": [
      "cs.AI",
      "cs.PL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17833v1",
    "published_date": "2024-04-27 08:56:45 UTC",
    "updated_date": "2024-04-27 08:56:45 UTC"
  },
  {
    "arxiv_id": "2404.17820v1",
    "title": "Motion planning for off-road autonomous driving based on human-like cognition and weight adaptation",
    "authors": [
      "Yuchun Wang",
      "Cheng Gong",
      "Jianwei Gong",
      "Peng Jia"
    ],
    "abstract": "Driving in an off-road environment is challenging for autonomous vehicles due\nto the complex and varied terrain. To ensure stable and efficient travel, the\nvehicle requires consideration and balancing of environmental factors, such as\nundulations, roughness, and obstacles, to generate optimal trajectories that\ncan adapt to changing scenarios. However, traditional motion planners often\nutilize a fixed cost function for trajectory optimization, making it difficult\nto adapt to different driving strategies in challenging irregular terrains and\nuncommon scenarios. To address these issues, we propose an adaptive motion\nplanner based on human-like cognition and cost evaluation for off-road driving.\nFirst, we construct a multi-layer map describing different features of off-road\nterrains, including terrain elevation, roughness, obstacle, and artificial\npotential field map. Subsequently, we employ a CNN-LSTM network to learn the\ntrajectories planned by human drivers in various off-road scenarios. Then,\nbased on human-like generated trajectories in different environments, we design\na primitive-based trajectory planner that aims to mimic human trajectories and\ncost weight selection, generating trajectories that are consistent with the\ndynamics of off-road vehicles. Finally, we compute optimal cost weights and\nselect and extend behavioral primitives to generate highly adaptive, stable,\nand efficient trajectories.\n  We validate the effectiveness of the proposed method through experiments in a\ndesert off-road environment with complex terrain and varying road conditions.\nThe experimental results show that the proposed human-like motion planner has\nexcellent adaptability to different off-road conditions. It shows real-time\noperation, greater stability, and more human-like planning ability in diverse\nand challenging scenarios.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17820v1",
    "published_date": "2024-04-27 08:00:35 UTC",
    "updated_date": "2024-04-27 08:00:35 UTC"
  },
  {
    "arxiv_id": "2405.09556v2",
    "title": "Co-learning-aided Multi-modal-deep-learning Framework of Passive DOA Estimators for a Heterogeneous Hybrid Massive MIMO Receiver",
    "authors": [
      "Jiatong Bai",
      "Feng Shu",
      "Qinghe Zheng",
      "Bo Xu",
      "Baihua Shi",
      "Yiwen Chen",
      "Weibin Zhang",
      "Xianpeng Wang"
    ],
    "abstract": "Due to its excellent performance in rate and resolution, fully-digital (FD)\nmassive multiple-input multiple-output (MIMO) antenna arrays has been widely\napplied in data transmission and direction of arrival (DOA) measurements, etc.\nBut it confronts with two main challenges: high computational complexity and\ncircuit cost. The two problems may be addressed well by hybrid analog-digital\n(HAD) structure. But there exists the problem of phase ambiguity for HAD, which\nleads to its low-efficiency or high-latency. Does exist there such a MIMO\nstructure of owning low-cost, low-complexity and high time efficiency at the\nsame time. To satisfy the three properties, a novel heterogeneous hybrid MIMO\nreceiver structure of integrating FD and heterogeneous HAD ($\\rm{H}^2$AD-FD) is\nproposed and corresponding multi-modal (MD)-learning framework is developed.\nThe framework includes three major stages: 1) generate the candidate sets via\nroot multiple signal classification (Root-MUSIC) or deep learning (DL); 2)\ninfer the class of true solutions from candidate sets using machine learning\n(ML) methods; 3) fuse the two-part true solutions to achieve a better DOA\nestimation. The above process form two methods named MD-Root-MUSIC and MDDL. To\nimprove DOA estimation accuracy and reduce the clustering complexity, a\nco-learning-aided MD framework is proposed to form two enhanced methods named\nCoMDDL and CoMD-RootMUSIC. Moreover, the Cramer-Rao lower bound (CRLB) for the\nproposed $\\rm{H}^2$AD-FD structure is also derived. Experimental results\ndemonstrate that our proposed four methods could approach the CRLB for\nsignal-to-noise ratio (SNR) > 0 dB and the proposed CoMDDL and MDDL perform\nbetter than CoMD-RootMUSIC and MD-RootMUSIC, particularly in the extremely low\nSNR region.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.09556v2",
    "published_date": "2024-04-27 07:34:36 UTC",
    "updated_date": "2024-06-12 08:16:01 UTC"
  },
  {
    "arxiv_id": "2404.18947v3",
    "title": "Multimodal Fusion on Low-quality Data: A Comprehensive Survey",
    "authors": [
      "Qingyang Zhang",
      "Yake Wei",
      "Zongbo Han",
      "Huazhu Fu",
      "Xi Peng",
      "Cheng Deng",
      "Qinghua Hu",
      "Cai Xu",
      "Jie Wen",
      "Di Hu",
      "Changqing Zhang"
    ],
    "abstract": "Multimodal fusion focuses on integrating information from multiple modalities\nwith the goal of more accurate prediction, which has achieved remarkable\nprogress in a wide range of scenarios, including autonomous driving and medical\ndiagnosis. However, the reliability of multimodal fusion remains largely\nunexplored especially under low-quality data settings. This paper surveys the\ncommon challenges and recent advances of multimodal fusion in the wild and\npresents them in a comprehensive taxonomy. From a data-centric view, we\nidentify four main challenges that are faced by multimodal fusion on\nlow-quality data, namely (1) noisy multimodal data that are contaminated with\nheterogeneous noises, (2) incomplete multimodal data that some modalities are\nmissing, (3) imbalanced multimodal data that the qualities or properties of\ndifferent modalities are significantly different and (4) quality-varying\nmultimodal data that the quality of each modality dynamically changes with\nrespect to different samples. This new taxonomy will enable researchers to\nunderstand the state of the field and identify several potential directions. We\nalso provide discussion for the open problems in this field together with\ninteresting future research directions.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Feel free to comment on our manuscript: qingyangzhang@tju$.$edu$.$cn",
    "pdf_url": "http://arxiv.org/pdf/2404.18947v3",
    "published_date": "2024-04-27 07:22:28 UTC",
    "updated_date": "2024-11-01 13:53:44 UTC"
  },
  {
    "arxiv_id": "2404.17809v1",
    "title": "Recall, Retrieve and Reason: Towards Better In-Context Relation Extraction",
    "authors": [
      "Guozheng Li",
      "Peng Wang",
      "Wenjun Ke",
      "Yikai Guo",
      "Ke Ji",
      "Ziyu Shang",
      "Jiajun Liu",
      "Zijie Xu"
    ],
    "abstract": "Relation extraction (RE) aims to identify relations between entities\nmentioned in texts. Although large language models (LLMs) have demonstrated\nimpressive in-context learning (ICL) abilities in various tasks, they still\nsuffer from poor performances compared to most supervised fine-tuned RE\nmethods. Utilizing ICL for RE with LLMs encounters two challenges: (1)\nretrieving good demonstrations from training examples, and (2) enabling LLMs\nexhibit strong ICL abilities in RE. On the one hand, retrieving good\ndemonstrations is a non-trivial process in RE, which easily results in low\nrelevance regarding entities and relations. On the other hand, ICL with an LLM\nachieves poor performance in RE while RE is different from language modeling in\nnature or the LLM is not large enough. In this work, we propose a novel\nrecall-retrieve-reason RE framework that synergizes LLMs with retrieval corpora\n(training examples) to enable relevant retrieving and reliable in-context\nreasoning. Specifically, we distill the consistently ontological knowledge from\ntraining datasets to let LLMs generate relevant entity pairs grounded by\nretrieval corpora as valid queries. These entity pairs are then used to\nretrieve relevant training examples from the retrieval corpora as\ndemonstrations for LLMs to conduct better ICL via instruction tuning. Extensive\nexperiments on different LLMs and RE datasets demonstrate that our method\ngenerates relevant and valid entity pairs and boosts ICL abilities of LLMs,\nachieving competitive or new state-of-the-art performance on sentence-level RE\ncompared to previous supervised fine-tuning methods and ICL-based methods.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "IJCAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.17809v1",
    "published_date": "2024-04-27 07:12:52 UTC",
    "updated_date": "2024-04-27 07:12:52 UTC"
  },
  {
    "arxiv_id": "2404.17807v1",
    "title": "Meta In-Context Learning Makes Large Language Models Better Zero and Few-Shot Relation Extractors",
    "authors": [
      "Guozheng Li",
      "Peng Wang",
      "Jiajun Liu",
      "Yikai Guo",
      "Ke Ji",
      "Ziyu Shang",
      "Zijie Xu"
    ],
    "abstract": "Relation extraction (RE) is an important task that aims to identify the\nrelationships between entities in texts. While large language models (LLMs)\nhave revealed remarkable in-context learning (ICL) capability for general zero\nand few-shot learning, recent studies indicate that current LLMs still struggle\nwith zero and few-shot RE. Previous studies are mainly dedicated to design\nprompt formats and select good examples for improving ICL-based RE. Although\nboth factors are vital for ICL, if one can fundamentally boost the ICL\ncapability of LLMs in RE, the zero and few-shot RE performance via ICL would be\nsignificantly improved. To this end, we introduce \\textsc{Micre} (\\textbf{M}eta\n\\textbf{I}n-\\textbf{C}ontext learning of LLMs for \\textbf{R}elation\n\\textbf{E}xtraction), a new meta-training framework for zero and few-shot RE\nwhere an LLM is tuned to do ICL on a diverse collection of RE datasets (i.e.,\nlearning to learn in context for RE). Through meta-training, the model becomes\nmore effectively to learn a new RE task in context by conditioning on a few\ntraining examples with no parameter updates or task-specific templates at\ninference time, enabling better zero and few-shot task generalization. We\nexperiment \\textsc{Micre} on various LLMs with different model scales and 12\npublic RE datasets, and then evaluate it on unseen RE benchmarks under zero and\nfew-shot settings. \\textsc{Micre} delivers comparable or superior performance\ncompared to a range of baselines including supervised fine-tuning and typical\nin-context learning methods. We find that the gains are particular significant\nfor larger model scales, and using a diverse set of the meta-training RE\ndatasets is key to improvements. Empirically, we show that \\textsc{Micre} can\ntransfer the relation semantic knowledge via relation label name during\ninference on target RE datasets.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "IJCAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.17807v1",
    "published_date": "2024-04-27 07:06:39 UTC",
    "updated_date": "2024-04-27 07:06:39 UTC"
  },
  {
    "arxiv_id": "2404.17802v1",
    "title": "Empirical Analysis of Dialogue Relation Extraction with Large Language Models",
    "authors": [
      "Guozheng Li",
      "Zijie Xu",
      "Ziyu Shang",
      "Jiajun Liu",
      "Ke Ji",
      "Yikai Guo"
    ],
    "abstract": "Dialogue relation extraction (DRE) aims to extract relations between two\narguments within a dialogue, which is more challenging than standard RE due to\nthe higher person pronoun frequency and lower information density in dialogues.\nHowever, existing DRE methods still suffer from two serious issues: (1) hard to\ncapture long and sparse multi-turn information, and (2) struggle to extract\ngolden relations based on partial dialogues, which motivates us to discover\nmore effective methods that can alleviate the above issues. We notice that the\nrise of large language models (LLMs) has sparked considerable interest in\nevaluating their performance across diverse tasks. To this end, we initially\ninvestigate the capabilities of different LLMs in DRE, considering both\nproprietary models and open-source models. Interestingly, we discover that LLMs\nsignificantly alleviate two issues in existing DRE methods. Generally, we have\nfollowing findings: (1) scaling up model size substantially boosts the overall\nDRE performance and achieves exceptional results, tackling the difficulty of\ncapturing long and sparse multi-turn information; (2) LLMs encounter with much\nsmaller performance drop from entire dialogue setting to partial dialogue\nsetting compared to existing methods; (3) LLMs deliver competitive or superior\nperformances under both full-shot and few-shot settings compared to current\nstate-of-the-art; (4) LLMs show modest performances on inverse relations but\nmuch stronger improvements on general relations, and they can handle dialogues\nof various lengths especially for longer sequences.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "IJCAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.17802v1",
    "published_date": "2024-04-27 06:55:41 UTC",
    "updated_date": "2024-04-27 06:55:41 UTC"
  },
  {
    "arxiv_id": "2404.17799v1",
    "title": "Personalized Federated Learning via Sequential Layer Expansion in Representation Learning",
    "authors": [
      "Jaewon Jang",
      "Bonjun Choi"
    ],
    "abstract": "Federated learning ensures the privacy of clients by conducting distributed\ntraining on individual client devices and sharing only the model weights with a\ncentral server. However, in real-world scenarios, the heterogeneity of data\namong clients necessitates appropriate personalization methods. In this paper,\nwe aim to address this heterogeneity using a form of parameter decoupling known\nas representation learning. Representation learning divides deep learning\nmodels into 'base' and 'head' components. The base component, capturing common\nfeatures across all clients, is shared with the server, while the head\ncomponent, capturing unique features specific to individual clients, remains\nlocal. We propose a new representation learning-based approach that suggests\ndecoupling the entire deep learning model into more densely divided parts with\nthe application of suitable scheduling methods, which can benefit not only data\nheterogeneity but also class heterogeneity. In this paper, we compare and\nanalyze two layer scheduling approaches, namely forward (\\textit{Vanilla}) and\nbackward (\\textit{Anti}), in the context of data and class heterogeneity among\nclients. Our experimental results show that the proposed algorithm, when\ncompared to existing personalized federated learning algorithms, achieves\nincreased accuracy, especially under challenging conditions, while reducing\ncomputation costs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages, 7 figure",
    "pdf_url": "http://arxiv.org/pdf/2404.17799v1",
    "published_date": "2024-04-27 06:37:19 UTC",
    "updated_date": "2024-04-27 06:37:19 UTC"
  },
  {
    "arxiv_id": "2404.17794v1",
    "title": "GPT for Games: A Scoping Review (2020-2023)",
    "authors": [
      "Daijin Yang",
      "Erica Kleinman",
      "Casper Harteveld"
    ],
    "abstract": "This paper introduces a scoping review of 55 articles to explore GPT's\npotential for games, offering researchers a comprehensive understanding of the\ncurrent applications and identifying both emerging trends and unexplored areas.\nWe identify five key applications of GPT in current game research: procedural\ncontent generation, mixed-initiative game design, mixed-initiative gameplay,\nplaying games, and game user research. Drawing from insights in each of these\napplication areas, we propose directions for future research in each one. This\nreview aims to lay the groundwork by illustrating the state of the art for\ninnovative GPT applications in games, promising to enrich game development and\nenhance player experiences with cutting-edge AI innovations.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "To be published in IEEE Conference on Games 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.17794v1",
    "published_date": "2024-04-27 06:26:18 UTC",
    "updated_date": "2024-04-27 06:26:18 UTC"
  },
  {
    "arxiv_id": "2404.17790v1",
    "title": "Continual Pre-Training for Cross-Lingual LLM Adaptation: Enhancing Japanese Language Capabilities",
    "authors": [
      "Kazuki Fujii",
      "Taishi Nakamura",
      "Mengsay Loem",
      "Hiroki Iida",
      "Masanari Ohi",
      "Kakeru Hattori",
      "Hirai Shota",
      "Sakae Mizuki",
      "Rio Yokota",
      "Naoaki Okazaki"
    ],
    "abstract": "Cross-lingual continual pre-training of large language models (LLMs)\ninitially trained on English corpus allows us to leverage the vast amount of\nEnglish language resources and reduce the pre-training cost. In this study, we\nconstructed Swallow, an LLM with enhanced Japanese capability, by extending the\nvocabulary of Llama 2 to include Japanese characters and conducting continual\npre-training on a large Japanese web corpus. Experimental results confirmed\nthat the performance on Japanese tasks drastically improved through continual\npre-training, and the performance monotonically increased with the amount of\ntraining data up to 100B tokens. Consequently, Swallow achieved superior\nperformance compared to other LLMs that were trained from scratch in English\nand Japanese. An analysis of the effects of continual pre-training revealed\nthat it was particularly effective for Japanese question answering tasks.\nFurthermore, to elucidate effective methodologies for cross-lingual continual\npre-training from English to Japanese, we investigated the impact of vocabulary\nexpansion and the effectiveness of incorporating parallel corpora. The results\nshowed that the efficiency gained through vocabulary expansion had no negative\nimpact on performance, except for the summarization task, and that the combined\nuse of parallel corpora enhanced translation ability.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17790v1",
    "published_date": "2024-04-27 06:07:55 UTC",
    "updated_date": "2024-04-27 06:07:55 UTC"
  },
  {
    "arxiv_id": "2405.09545v1",
    "title": "Intrinsic Voltage Offsets in Memcapacitive Bio-Membranes Enable High-Performance Physical Reservoir Computing",
    "authors": [
      "Ahmed S. Mohamed",
      "Anurag Dhungel",
      "Md Sakib Hasan",
      "Joseph S. Najem"
    ],
    "abstract": "Reservoir computing is a brain-inspired machine learning framework for\nprocessing temporal data by mapping inputs into high-dimensional spaces.\nPhysical reservoir computers (PRCs) leverage native fading memory and\nnonlinearity in physical substrates, including atomic switches, photonics,\nvolatile memristors, and, recently, memcapacitors, to achieve efficient\nhigh-dimensional mapping. Traditional PRCs often consist of homogeneous device\narrays, which rely on input encoding methods and large stochastic\ndevice-to-device variations for increased nonlinearity and high-dimensional\nmapping. These approaches incur high pre-processing costs and restrict\nreal-time deployment. Here, we introduce a novel heterogeneous\nmemcapacitor-based PRC that exploits internal voltage offsets to enable both\nmonotonic and non-monotonic input-state correlations crucial for efficient\nhigh-dimensional transformations. We demonstrate our approach's efficacy by\npredicting a second-order nonlinear dynamical system with an extremely low\nprediction error (0.00018). Additionally, we predict a chaotic H\\'enon map,\nachieving a low normalized root mean square error (0.080). Unlike previous\nPRCs, such errors are achieved without input encoding methods, underscoring the\npower of distinct input-state correlations. Most importantly, we generalize our\napproach to other neuromorphic devices that lack inherent voltage offsets using\nexternally applied offsets to realize various input-state correlations. Our\napproach and unprecedented performance are a major milestone towards\nhigh-performance full in-materia PRCs.",
    "categories": [
      "cs.ET",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.ET",
    "comment": "Supplementary Information is included under the main text",
    "pdf_url": "http://arxiv.org/pdf/2405.09545v1",
    "published_date": "2024-04-27 05:47:38 UTC",
    "updated_date": "2024-04-27 05:47:38 UTC"
  },
  {
    "arxiv_id": "2404.17780v1",
    "title": "Verco: Learning Coordinated Verbal Communication for Multi-agent Reinforcement Learning",
    "authors": [
      "Dapeng Li",
      "Hang Dong",
      "Lu Wang",
      "Bo Qiao",
      "Si Qin",
      "Qingwei Lin",
      "Dongmei Zhang",
      "Qi Zhang",
      "Zhiwei Xu",
      "Bin Zhang",
      "Guoliang Fan"
    ],
    "abstract": "In recent years, multi-agent reinforcement learning algorithms have made\nsignificant advancements in diverse gaming environments, leading to increased\ninterest in the broader application of such techniques. To address the\nprevalent challenge of partial observability, communication-based algorithms\nhave improved cooperative performance through the sharing of numerical\nembedding between agents. However, the understanding of the formation of\ncollaborative mechanisms is still very limited, making designing a\nhuman-understandable communication mechanism a valuable problem to address. In\nthis paper, we propose a novel multi-agent reinforcement learning algorithm\nthat embeds large language models into agents, endowing them with the ability\nto generate human-understandable verbal communication. The entire framework has\na message module and an action module. The message module is responsible for\ngenerating and sending verbal messages to other agents, effectively enhancing\ninformation sharing among agents. To further enhance the message module, we\nemploy a teacher model to generate message labels from the global view and\nupdate the student model through Supervised Fine-Tuning (SFT). The action\nmodule receives messages from other agents and selects actions based on current\nlocal observations and received messages. Experiments conducted on the\nOvercooked game demonstrate our method significantly enhances the learning\nefficiency and performance of existing methods, while also providing an\ninterpretable tool for humans to understand the process of multi-agent\ncooperation.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "12 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.17780v1",
    "published_date": "2024-04-27 05:10:33 UTC",
    "updated_date": "2024-04-27 05:10:33 UTC"
  },
  {
    "arxiv_id": "2404.17778v1",
    "title": "MRScore: Evaluating Radiology Report Generation with LLM-based Reward System",
    "authors": [
      "Yunyi Liu",
      "Zhanyu Wang",
      "Yingshu Li",
      "Xinyu Liang",
      "Lingqiao Liu",
      "Lei Wang",
      "Luping Zhou"
    ],
    "abstract": "In recent years, automated radiology report generation has experienced\nsignificant growth. This paper introduces MRScore, an automatic evaluation\nmetric tailored for radiology report generation by leveraging Large Language\nModels (LLMs). Conventional NLG (natural language generation) metrics like BLEU\nare inadequate for accurately assessing the generated radiology reports, as\nsystematically demonstrated by our observations within this paper. To address\nthis challenge, we collaborated with radiologists to develop a framework that\nguides LLMs for radiology report evaluation, ensuring alignment with human\nanalysis. Our framework includes two key components: i) utilizing GPT to\ngenerate large amounts of training data, i.e., reports with different\nqualities, and ii) pairing GPT-generated reports as accepted and rejected\nsamples and training LLMs to produce MRScore as the model reward. Our\nexperiments demonstrate MRScore's higher correlation with human judgments and\nsuperior performance in model selection compared to traditional metrics. Our\ncode and datasets will be available on GitHub.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17778v1",
    "published_date": "2024-04-27 04:42:45 UTC",
    "updated_date": "2024-04-27 04:42:45 UTC"
  },
  {
    "arxiv_id": "2405.00728v1",
    "title": "Evaluating the Application of ChatGPT in Outpatient Triage Guidance: A Comparative Study",
    "authors": [
      "Dou Liu",
      "Ying Han",
      "Xiandi Wang",
      "Xiaomei Tan",
      "Di Liu",
      "Guangwu Qian",
      "Kang Li",
      "Dan Pu",
      "Rong Yin"
    ],
    "abstract": "The integration of Artificial Intelligence (AI) in healthcare presents a\ntransformative potential for enhancing operational efficiency and health\noutcomes. Large Language Models (LLMs), such as ChatGPT, have shown their\ncapabilities in supporting medical decision-making. Embedding LLMs in medical\nsystems is becoming a promising trend in healthcare development. The potential\nof ChatGPT to address the triage problem in emergency departments has been\nexamined, while few studies have explored its application in outpatient\ndepartments. With a focus on streamlining workflows and enhancing efficiency\nfor outpatient triage, this study specifically aims to evaluate the consistency\nof responses provided by ChatGPT in outpatient guidance, including both\nwithin-version response analysis and between-version comparisons. For\nwithin-version, the results indicate that the internal response consistency for\nChatGPT-4.0 is significantly higher than ChatGPT-3.5 (p=0.03) and both have a\nmoderate consistency (71.2% for 4.0 and 59.6% for 3.5) in their top\nrecommendation. However, the between-version consistency is relatively low\n(mean consistency score=1.43/3, median=1), indicating few recommendations match\nbetween the two versions. Also, only 50% top recommendations match perfectly in\nthe comparisons. Interestingly, ChatGPT-3.5 responses are more likely to be\ncomplete than those from ChatGPT-4.0 (p=0.02), suggesting possible differences\nin information processing and response generation between the two versions. The\nfindings offer insights into AI-assisted outpatient operations, while also\nfacilitating the exploration of potentials and limitations of LLMs in\nhealthcare utilization. Future research may focus on carefully optimizing LLMs\nand AI integration in healthcare systems based on ergonomic and human factors\nprinciples, precisely aligning with the specific needs of effective outpatient\ntriage.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages, 1 figure, conference(International Ergonomics Association)",
    "pdf_url": "http://arxiv.org/pdf/2405.00728v1",
    "published_date": "2024-04-27 04:12:02 UTC",
    "updated_date": "2024-04-27 04:12:02 UTC"
  },
  {
    "arxiv_id": "2404.17768v2",
    "title": "Changing the Training Data Distribution to Reduce Simplicity Bias Improves In-distribution Generalization",
    "authors": [
      "Dang Nguyen",
      "Paymon Haddad",
      "Eric Gan",
      "Baharan Mirzasoleiman"
    ],
    "abstract": "Can we modify the training data distribution to encourage the underlying\noptimization method toward finding solutions with superior generalization\nperformance on in-distribution data? In this work, we approach this question\nfor the first time by comparing the inductive bias of gradient descent (GD)\nwith that of sharpness-aware minimization (SAM). By studying a two-layer CNN,\nwe rigorously prove that SAM learns different features more uniformly,\nparticularly in early epochs. That is, SAM is less susceptible to simplicity\nbias compared to GD. We also show that examples containing features that are\nlearned early are separable from the rest based on the model's output. Based on\nthis observation, we propose a method that (i) clusters examples based on the\nnetwork output early in training, (ii) identifies a cluster of examples with\nsimilar network output, and (iii) upsamples the rest of examples only once to\nalleviate the simplicity bias. We show empirically that USEFUL effectively\nimproves the generalization performance on the original data distribution when\ntraining with various gradient methods, including (S)GD and SAM. Notably, we\ndemonstrate that our method can be combined with SAM variants and existing data\naugmentation strategies to achieve, to the best of our knowledge,\nstate-of-the-art performance for training ResNet18 on CIFAR10, STL10, CINIC10,\nTiny-ImageNet; ResNet34 on CIFAR100; and VGG19 and DenseNet121 on CIFAR10.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "43 pages, 15 figures, 9 tables",
    "pdf_url": "http://arxiv.org/pdf/2404.17768v2",
    "published_date": "2024-04-27 03:30:50 UTC",
    "updated_date": "2024-11-02 00:51:16 UTC"
  },
  {
    "arxiv_id": "2404.17766v1",
    "title": "Implementation of Big AI Models for Wireless Networks with Collaborative Edge Computing",
    "authors": [
      "Liekang Zeng",
      "Shengyuan Ye",
      "Xu Chen",
      "Yang Yang"
    ],
    "abstract": "Big Artificial Intelligence (AI) models have emerged as a crucial element in\nvarious intelligent applications at the edge, such as voice assistants in smart\nhomes and autonomous robotics in smart factories. Training big AI models, e.g.,\nfor personalized fine-tuning and continual model refinement, poses significant\nchallenges to edge devices due to the inherent conflict between limited\ncomputing resources and intensive workload associated with training. Despite\nthe constraints of on-device training, traditional approaches usually resort to\naggregating training data and sending it to a remote cloud for centralized\ntraining. Nevertheless, this approach is neither sustainable, which strains\nlong-range backhaul transmission and energy-consuming datacenters, nor safely\nprivate, which shares users' raw data with remote infrastructures. To address\nthese challenges, we alternatively observe that prevalent edge environments\nusually contain a diverse collection of trusted edge devices with untapped idle\nresources, which can be leveraged for edge training acceleration. Motivated by\nthis, in this article, we propose collaborative edge training, a novel training\nmechanism that orchestrates a group of trusted edge devices as a resource pool\nfor expedited, sustainable big AI model training at the edge. As an initial\nstep, we present a comprehensive framework for building collaborative edge\ntraining systems and analyze in-depth its merits and sustainable scheduling\nchoices following its workflow. To further investigate the impact of its\nparallelism design, we empirically study a case of four typical parallelisms\nfrom the perspective of energy demand with realistic testbeds. Finally, we\ndiscuss open challenges for sustainable collaborative edge training to point to\nfuture directions of edge-centric big AI model training.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "cs.NI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17766v1",
    "published_date": "2024-04-27 03:09:39 UTC",
    "updated_date": "2024-04-27 03:09:39 UTC"
  },
  {
    "arxiv_id": "2404.17760v1",
    "title": "Adversarial Examples: Generation Proposal in the Context of Facial Recognition Systems",
    "authors": [
      "Marina Fuster",
      "Ignacio Vidaurreta"
    ],
    "abstract": "In this paper we investigate the vulnerability that facial recognition\nsystems present to adversarial examples by introducing a new methodology from\nthe attacker perspective. The technique is based on the use of the autoencoder\nlatent space, organized with principal component analysis. We intend to analyze\nthe potential to craft adversarial examples suitable for both dodging and\nimpersonation attacks, against state-of-the-art systems. Our initial\nhypothesis, which was not strongly favoured by the results, stated that it\nwould be possible to separate between the \"identity\" and \"facial expression\"\nfeatures to produce high-quality examples. Despite the findings not supporting\nit, the results sparked insights into adversarial examples generation and\nopened new research avenues in the area.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17760v1",
    "published_date": "2024-04-27 02:35:15 UTC",
    "updated_date": "2024-04-27 02:35:15 UTC"
  },
  {
    "arxiv_id": "2404.17758v2",
    "title": "The Common Core Ontologies",
    "authors": [
      "Mark Jensen",
      "Giacomo De Colle",
      "Sean Kindya",
      "Cameron More",
      "Alexander P. Cox",
      "John Beverley"
    ],
    "abstract": "The Common Core Ontologies (CCO) are designed as a mid-level ontology suite\nthat extends the Basic Formal Ontology. CCO has since been increasingly adopted\nby a broad group of users and applications and is proposed as the first\nstandard mid-level ontology. Despite these successes, documentation of the\ncontents and design patterns of the CCO has been comparatively minimal. This\npaper is a step toward providing enhanced documentation for the mid-level\nontology suite through a discussion of the contents of the eleven ontologies\nthat collectively comprise the Common Core Ontology suite.",
    "categories": [
      "cs.AI",
      "cs.DB",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "12 pages",
    "pdf_url": "http://arxiv.org/pdf/2404.17758v2",
    "published_date": "2024-04-27 02:23:02 UTC",
    "updated_date": "2024-08-16 03:26:25 UTC"
  },
  {
    "arxiv_id": "2404.17757v2",
    "title": "Middle Architecture Criteria",
    "authors": [
      "John Beverley",
      "Giacomo De Colle",
      "Mark Jensen",
      "Carter Benson",
      "Barry Smith"
    ],
    "abstract": "Mid-level ontologies are used to integrate terminologies and data across\ndisparate domains. There are, however, no clear, defensible criteria for\ndetermining whether a given ontology should count as mid-level, because we lack\na rigorous characterization of what the middle level of generality is supposed\nto contain. Attempts to provide such a characterization have failed, we\nbelieve, because they have focused on the goal of specifying what is\ncharacteristic of those single ontologies that have been advanced as mid-level\nontologies. Unfortunately, single ontologies of this sort are generally a\nmixture of top- and mid-level, and sometimes even of domain-level terms. To\ngain clarity, we aim to specify the necessary and sufficient conditions for a\ncollection of one or more ontologies to inhabit what we call a mid-level\narchitecture.",
    "categories": [
      "cs.AI",
      "cs.DB",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "14 pages",
    "pdf_url": "http://arxiv.org/pdf/2404.17757v2",
    "published_date": "2024-04-27 02:16:26 UTC",
    "updated_date": "2024-08-16 03:20:35 UTC"
  },
  {
    "arxiv_id": "2404.17753v1",
    "title": "Leveraging Cross-Modal Neighbor Representation for Improved CLIP Classification",
    "authors": [
      "Chao Yi",
      "Lu Ren",
      "De-Chuan Zhan",
      "Han-Jia Ye"
    ],
    "abstract": "CLIP showcases exceptional cross-modal matching capabilities due to its\ntraining on image-text contrastive learning tasks. However, without specific\noptimization for unimodal scenarios, its performance in single-modality feature\nextraction might be suboptimal. Despite this, some studies have directly used\nCLIP's image encoder for tasks like few-shot classification, introducing a\nmisalignment between its pre-training objectives and feature extraction\nmethods. This inconsistency can diminish the quality of the image's feature\nrepresentation, adversely affecting CLIP's effectiveness in target tasks. In\nthis paper, we view text features as precise neighbors of image features in\nCLIP's space and present a novel CrOss-moDal nEighbor Representation(CODER)\nbased on the distance structure between images and their neighbor texts. This\nfeature extraction method aligns better with CLIP's pre-training objectives,\nthereby fully leveraging CLIP's robust cross-modal capabilities. The key to\nconstruct a high-quality CODER lies in how to create a vast amount of\nhigh-quality and diverse texts to match with images. We introduce the Auto Text\nGenerator(ATG) to automatically generate the required texts in a data-free and\ntraining-free manner. We apply CODER to CLIP's zero-shot and few-shot image\nclassification tasks. Experiment results across various datasets and models\nconfirm CODER's effectiveness. Code is available\nat:https://github.com/YCaigogogo/CVPR24-CODER.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17753v1",
    "published_date": "2024-04-27 02:04:36 UTC",
    "updated_date": "2024-04-27 02:04:36 UTC"
  },
  {
    "arxiv_id": "2404.17749v2",
    "title": "UMass-BioNLP at MEDIQA-M3G 2024: DermPrompt -- A Systematic Exploration of Prompt Engineering with GPT-4V for Dermatological Diagnosis",
    "authors": [
      "Parth Vashisht",
      "Abhilasha Lodha",
      "Mukta Maddipatla",
      "Zonghai Yao",
      "Avijit Mitra",
      "Zhichao Yang",
      "Junda Wang",
      "Sunjae Kwon",
      "Hong Yu"
    ],
    "abstract": "This paper presents our team's participation in the MEDIQA-ClinicalNLP2024\nshared task B. We present a novel approach to diagnosing clinical dermatology\ncases by integrating large multimodal models, specifically leveraging the\ncapabilities of GPT-4V under a retriever and a re-ranker framework. Our\ninvestigation reveals that GPT-4V, when used as a retrieval agent, can\naccurately retrieve the correct skin condition 85% of the time using\ndermatological images and brief patient histories. Additionally, we empirically\nshow that Naive Chain-of-Thought (CoT) works well for retrieval while Medical\nGuidelines Grounded CoT is required for accurate dermatological diagnosis.\nFurther, we introduce a Multi-Agent Conversation (MAC) framework and show its\nsuperior performance and potential over the best CoT strategy. The experiments\nsuggest that using naive CoT for retrieval and multi-agent conversation for\ncritique-based diagnosis, GPT-4V can lead to an early and accurate diagnosis of\ndermatological conditions. The implications of this work extend to improving\ndiagnostic workflows, supporting dermatological education, and enhancing\npatient care by providing a scalable, accessible, and accurate diagnostic tool.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at NAACL-ClinicalNLP workshop 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.17749v2",
    "published_date": "2024-04-27 01:39:05 UTC",
    "updated_date": "2024-05-08 21:57:24 UTC"
  },
  {
    "arxiv_id": "2404.19640v1",
    "title": "Attacking Bayes: On the Adversarial Robustness of Bayesian Neural Networks",
    "authors": [
      "Yunzhen Feng",
      "Tim G. J. Rudner",
      "Nikolaos Tsilivis",
      "Julia Kempe"
    ],
    "abstract": "Adversarial examples have been shown to cause neural networks to fail on a\nwide range of vision and language tasks, but recent work has claimed that\nBayesian neural networks (BNNs) are inherently robust to adversarial\nperturbations. In this work, we examine this claim. To study the adversarial\nrobustness of BNNs, we investigate whether it is possible to successfully break\nstate-of-the-art BNN inference methods and prediction pipelines using even\nrelatively unsophisticated attacks for three tasks: (1) label prediction under\nthe posterior predictive mean, (2) adversarial example detection with Bayesian\npredictive uncertainty, and (3) semantic shift detection. We find that BNNs\ntrained with state-of-the-art approximate inference methods, and even BNNs\ntrained with Hamiltonian Monte Carlo, are highly susceptible to adversarial\nattacks. We also identify various conceptual and experimental errors in\nprevious works that claimed inherent adversarial robustness of BNNs and\nconclusively demonstrate that BNNs and uncertainty-aware Bayesian prediction\npipelines are not inherently robust against adversarial attacks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ME",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.19640v1",
    "published_date": "2024-04-27 01:34:46 UTC",
    "updated_date": "2024-04-27 01:34:46 UTC"
  },
  {
    "arxiv_id": "2404.17735v3",
    "title": "Causal Diffusion Autoencoders: Toward Counterfactual Generation via Diffusion Probabilistic Models",
    "authors": [
      "Aneesh Komanduri",
      "Chen Zhao",
      "Feng Chen",
      "Xintao Wu"
    ],
    "abstract": "Diffusion probabilistic models (DPMs) have become the state-of-the-art in\nhigh-quality image generation. However, DPMs have an arbitrary noisy latent\nspace with no interpretable or controllable semantics. Although there has been\nsignificant research effort to improve image sample quality, there is little\nwork on representation-controlled generation using diffusion models.\nSpecifically, causal modeling and controllable counterfactual generation using\nDPMs is an underexplored area. In this work, we propose CausalDiffAE, a\ndiffusion-based causal representation learning framework to enable\ncounterfactual generation according to a specified causal model. Our key idea\nis to use an encoder to extract high-level semantically meaningful causal\nvariables from high-dimensional data and model stochastic variation using\nreverse diffusion. We propose a causal encoding mechanism that maps\nhigh-dimensional data to causally related latent factors and parameterize the\ncausal mechanisms among latent factors using neural networks. To enforce the\ndisentanglement of causal variables, we formulate a variational objective and\nleverage auxiliary label information in a prior to regularize the latent space.\nWe propose a DDIM-based counterfactual generation procedure subject to\ndo-interventions. Finally, to address the limited label supervision scenario,\nwe also study the application of CausalDiffAE when a part of the training data\nis unlabeled, which also enables granular control over the strength of\ninterventions in generating counterfactuals during inference. We empirically\nshow that CausalDiffAE learns a disentangled latent space and is capable of\ngenerating high-quality counterfactual images.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ME"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to the 27th European Conference on Artificial Intelligence\n  (ECAI 2024)",
    "pdf_url": "http://arxiv.org/pdf/2404.17735v3",
    "published_date": "2024-04-27 00:09:26 UTC",
    "updated_date": "2024-08-23 22:02:23 UTC"
  },
  {
    "arxiv_id": "2404.17733v1",
    "title": "Building a Large Japanese Web Corpus for Large Language Models",
    "authors": [
      "Naoaki Okazaki",
      "Kakeru Hattori",
      "Hirai Shota",
      "Hiroki Iida",
      "Masanari Ohi",
      "Kazuki Fujii",
      "Taishi Nakamura",
      "Mengsay Loem",
      "Rio Yokota",
      "Sakae Mizuki"
    ],
    "abstract": "Open Japanese large language models (LLMs) have been trained on the Japanese\nportions of corpora such as CC-100, mC4, and OSCAR. However, these corpora were\nnot created for the quality of Japanese texts. This study builds a large\nJapanese web corpus by extracting and refining text from the Common Crawl\narchive (21 snapshots of approximately 63.4 billion pages crawled between 2020\nand 2023). This corpus consists of approximately 312.1 billion characters\n(approximately 173 million pages), which is the largest of all available\ntraining corpora for Japanese LLMs, surpassing CC-100 (approximately 25.8\nbillion characters), mC4 (approximately 239.7 billion characters) and OSCAR\n23.10 (approximately 74 billion characters). To confirm the quality of the\ncorpus, we performed continual pre-training on Llama 2 7B, 13B, 70B, Mistral 7B\nv0.1, and Mixtral 8x7B Instruct as base LLMs and gained consistent (6.6-8.1\npoints) improvements on Japanese benchmark datasets. We also demonstrate that\nthe improvement on Llama 2 13B brought from the presented corpus was the\nlargest among those from other existing corpora.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "17 pages",
    "pdf_url": "http://arxiv.org/pdf/2404.17733v1",
    "published_date": "2024-04-27 00:02:45 UTC",
    "updated_date": "2024-04-27 00:02:45 UTC"
  }
]
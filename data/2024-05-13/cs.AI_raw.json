[
  {
    "arxiv_id": "2405.08238v1",
    "title": "Silver-Tongued and Sundry: Exploring Intersectional Pronouns with ChatGPT",
    "authors": [
      "Takao Fujii",
      "Katie Seaborn",
      "Madeleine Steeds"
    ],
    "abstract": "ChatGPT is a conversational agent built on a large language model. Trained on\na significant portion of human output, ChatGPT can mimic people to a degree. As\nsuch, we need to consider what social identities ChatGPT simulates (or can be\ndesigned to simulate). In this study, we explored the case of identity\nsimulation through Japanese first-person pronouns, which are tightly connected\nto social identities in intersectional ways, i.e., intersectional pronouns. We\nconducted a controlled online experiment where people from two regions in Japan\n(Kanto and Kinki) witnessed interactions with ChatGPT using ten sets of\nfirst-person pronouns. We discovered that pronouns alone can evoke perceptions\nof social identities in ChatGPT at the intersections of gender, age, region,\nand formality, with caveats. This work highlights the importance of pronoun use\nfor social identity simulation, provides a language-based methodology for\nculturally-sensitive persona development, and advances the potential of\nintersectional identities in intelligent agents.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.HC",
    "comment": "Honorable Mention award (top 5%) at CHI '24",
    "pdf_url": "http://arxiv.org/pdf/2405.08238v1",
    "published_date": "2024-05-13 23:38:50 UTC",
    "updated_date": "2024-05-13 23:38:50 UTC"
  },
  {
    "arxiv_id": "2405.13012v1",
    "title": "Divergent Creativity in Humans and Large Language Models",
    "authors": [
      "Antoine Bellemare-Pepin",
      "François Lespinasse",
      "Philipp Thölke",
      "Yann Harel",
      "Kory Mathewson",
      "Jay A. Olson",
      "Yoshua Bengio",
      "Karim Jerbi"
    ],
    "abstract": "The recent surge in the capabilities of Large Language Models (LLMs) has led\nto claims that they are approaching a level of creativity akin to human\ncapabilities. This idea has sparked a blend of excitement and apprehension.\nHowever, a critical piece that has been missing in this discourse is a\nsystematic evaluation of LLM creativity, particularly in comparison to human\ndivergent thinking. To bridge this gap, we leverage recent advances in\ncreativity science to build a framework for in-depth analysis of divergent\ncreativity in both state-of-the-art LLMs and a substantial dataset of 100,000\nhumans. We found evidence suggesting that LLMs can indeed surpass human\ncapabilities in specific creative tasks such as divergent association and\ncreative writing. Our quantitative benchmarking framework opens up new paths\nfor the development of more creative LLMs, but it also encourages more granular\ninquiries into the distinctive elements that constitute human inventive thought\nprocesses, compared to those that can be artificially generated.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "First two and last listed authors are corresponding authors. The\n  first two listed authors contributed equally to this work",
    "pdf_url": "http://arxiv.org/pdf/2405.13012v1",
    "published_date": "2024-05-13 22:37:52 UTC",
    "updated_date": "2024-05-13 22:37:52 UTC"
  },
  {
    "arxiv_id": "2405.08183v2",
    "title": "Towards Energy-Aware Federated Learning via MARL: A Dual-Selection Approach for Model and Client",
    "authors": [
      "Jun Xia",
      "Yi Zhang",
      "Yiyu Shi"
    ],
    "abstract": "Although Federated Learning (FL) is promising in knowledge sharing for\nheterogeneous Artificial Intelligence of Thing (AIoT) devices, their training\nperformance and energy efficacy are severely restricted in practical\nbattery-driven scenarios due to the ``wooden barrel effect'' caused by the\nmismatch between homogeneous model paradigms and heterogeneous device\ncapability. As a result, due to various kinds of differences among devices, it\nis hard for existing FL methods to conduct training effectively in\nenergy-constrained scenarios, such as battery constraints of devices. To tackle\nthe above issues, we propose an energy-aware FL framework named DR-FL, which\nconsiders the energy constraints in both clients and heterogeneous deep\nlearning models to enable energy-efficient FL. Unlike Vanilla FL, DR-FL adopts\nour proposed Muti-Agents Reinforcement Learning (MARL)-based dual-selection\nmethod, which allows participated devices to make contributions to the global\nmodel effectively and adaptively based on their computing capabilities and\nenergy capacities in a MARL-based manner. Experiments conducted with various\nwidely recognized datasets demonstrate that DR-FL has the capability to\noptimize the exchange of knowledge among diverse models in large-scale AIoT\nsystems while adhering to energy limitations. Additionally, it improves the\nperformance of each individual heterogeneous device's model.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.08183v2",
    "published_date": "2024-05-13 21:02:31 UTC",
    "updated_date": "2024-07-09 16:46:19 UTC"
  },
  {
    "arxiv_id": "2405.08174v2",
    "title": "Estimating Direct and Indirect Causal Effects of Spatiotemporal Interventions in Presence of Spatial Interference",
    "authors": [
      "Sahara Ali",
      "Omar Faruque",
      "Jianwu Wang"
    ],
    "abstract": "Spatial interference (SI) occurs when the treatment at one location affects\nthe outcomes at other locations. Accounting for spatial interference in\nspatiotemporal settings poses further challenges as interference violates the\nstable unit treatment value assumption, making it infeasible for standard\ncausal inference methods to quantify the effects of time-varying treatment at\nspatially varying outcomes. In this paper, we first formalize the concept of\nspatial interference in case of time-varying treatment assignments by extending\nthe potential outcome framework under the assumption of no unmeasured\nconfounding. We then propose our deep learning based potential outcome model\nfor spatiotemporal causal inference. We utilize latent factor modeling to\nreduce the bias due to time-varying confounding while leveraging the power of\nU-Net architecture to capture global and local spatial interference in data\nover time. Our causal estimators are an extension of average treatment effect\n(ATE) for estimating direct (DATE) and indirect effects (IATE) of spatial\ninterference on treated and untreated data. Being the first of its kind deep\nlearning based spatiotemporal causal inference technique, our approach shows\nadvantages over several baseline methods based on the experiment results on two\nsynthetic datasets, with and without spatial interference. Our results on\nreal-world climate dataset also align with domain knowledge, further\ndemonstrating the effectiveness of our proposed method.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.08174v2",
    "published_date": "2024-05-13 20:39:27 UTC",
    "updated_date": "2024-08-29 23:21:03 UTC"
  },
  {
    "arxiv_id": "2405.08172v1",
    "title": "CANTONMT: Investigating Back-Translation and Model-Switch Mechanisms for Cantonese-English Neural Machine Translation",
    "authors": [
      "Kung Yin Hong",
      "Lifeng Han",
      "Riza Batista-Navarro",
      "Goran Nenadic"
    ],
    "abstract": "This paper investigates the development and evaluation of machine translation\nmodels from Cantonese to English, where we propose a novel approach to tackle\nlow-resource language translations. The main objectives of the study are to\ndevelop a model that can effectively translate Cantonese to English and\nevaluate it against state-of-the-art commercial models. To achieve this, a new\nparallel corpus has been created by combining different available corpora\nonline with preprocessing and cleaning. In addition, a monolingual Cantonese\ndataset has been created through web scraping to aid the synthetic parallel\ncorpus generation. Following the data collection process, several approaches,\nincluding fine-tuning models, back-translation, and model switch, have been\nused. The translation quality of models has been evaluated with multiple\nquality metrics, including lexicon-based metrics (SacreBLEU and hLEPOR) and\nembedding-space metrics (COMET and BERTscore). Based on the automatic metrics,\nthe best model is selected and compared against the 2 best commercial\ntranslators using the human evaluation framework HOPES. The best model proposed\nin this investigation (NLLB-mBART) with model switch mechanisms has reached\ncomparable and even better automatic evaluation scores against State-of-the-art\ncommercial models (Bing and Baidu Translators), with a SacreBLEU score of 16.8\non our test set. Furthermore, an open-source web application has been developed\nto allow users to translate between Cantonese and English, with the different\ntrained models available for effective comparisons between models from this\ninvestigation and users. CANTONMT is available at\nhttps://github.com/kenrickkung/CantoneseTranslation",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "on-going work, 30 pages",
    "pdf_url": "http://arxiv.org/pdf/2405.08172v1",
    "published_date": "2024-05-13 20:37:04 UTC",
    "updated_date": "2024-05-13 20:37:04 UTC"
  },
  {
    "arxiv_id": "2405.10812v2",
    "title": "VQDNA: Unleashing the Power of Vector Quantization for Multi-Species Genomic Sequence Modeling",
    "authors": [
      "Siyuan Li",
      "Zedong Wang",
      "Zicheng Liu",
      "Di Wu",
      "Cheng Tan",
      "Jiangbin Zheng",
      "Yufei Huang",
      "Stan Z. Li"
    ],
    "abstract": "Similar to natural language models, pre-trained genome language models are\nproposed to capture the underlying intricacies within genomes with unsupervised\nsequence modeling. They have become essential tools for researchers and\npractitioners in biology. However, the hand-crafted tokenization policies used\nin these models may not encode the most discriminative patterns from the\nlimited vocabulary of genomic data. In this paper, we introduce VQDNA, a\ngeneral-purpose framework that renovates genome tokenization from the\nperspective of genome vocabulary learning. By leveraging vector-quantized\ncodebooks as learnable vocabulary, VQDNA can adaptively tokenize genomes into\npattern-aware embeddings in an end-to-end manner. To further push its limits,\nwe propose Hierarchical Residual Quantization (HRQ), where varying scales of\ncodebooks are designed in a hierarchy to enrich the genome vocabulary in a\ncoarse-to-fine manner. Extensive experiments on 32 genome datasets demonstrate\nVQDNA's superiority and favorable parameter efficiency compared to existing\ngenome language models. Notably, empirical analysis of SARS-CoV-2 mutations\nreveals the fine-grained pattern awareness and biological significance of\nlearned HRQ vocabulary, highlighting its untapped potential for broader\napplications in genomics.",
    "categories": [
      "q-bio.GN",
      "cs.AI"
    ],
    "primary_category": "q-bio.GN",
    "comment": "ICML 2024. Preprint V2 with 17 pages and 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.10812v2",
    "published_date": "2024-05-13 20:15:03 UTC",
    "updated_date": "2024-06-02 17:50:46 UTC"
  },
  {
    "arxiv_id": "2405.08154v1",
    "title": "LLM Theory of Mind and Alignment: Opportunities and Risks",
    "authors": [
      "Winnie Street"
    ],
    "abstract": "Large language models (LLMs) are transforming human-computer interaction and\nconceptions of artificial intelligence (AI) with their impressive capacities\nfor conversing and reasoning in natural language. There is growing interest in\nwhether LLMs have theory of mind (ToM); the ability to reason about the mental\nand emotional states of others that is core to human social intelligence. As\nLLMs are integrated into the fabric of our personal, professional and social\nlives and given greater agency to make decisions with real-world consequences,\nthere is a critical need to understand how they can be aligned with human\nvalues. ToM seems to be a promising direction of inquiry in this regard.\nFollowing the literature on the role and impacts of human ToM, this paper\nidentifies key areas in which LLM ToM will show up in human:LLM interactions at\nindividual and group levels, and what opportunities and risks for alignment are\nraised in each. On the individual level, the paper considers how LLM ToM might\nmanifest in goal specification, conversational adaptation, empathy and\nanthropomorphism. On the group level, it considers how LLM ToM might facilitate\ncollective alignment, cooperation or competition, and moral judgement-making.\nThe paper lays out a broad spectrum of potential implications and suggests the\nmost pressing areas for future research.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "H.1.2; H.5.2"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.08154v1",
    "published_date": "2024-05-13 19:52:16 UTC",
    "updated_date": "2024-05-13 19:52:16 UTC"
  },
  {
    "arxiv_id": "2405.13011v1",
    "title": "Unveiling Social Media Comments with a Novel Named Entity Recognition System for Identity Groups",
    "authors": [
      "Andrés Carvallo",
      "Tamara Quiroga",
      "Carlos Aspillaga",
      "Marcelo Mendoza"
    ],
    "abstract": "While civilized users employ social media to stay informed and discuss daily\noccurrences, haters perceive these platforms as fertile ground for attacking\ngroups and individuals. The prevailing approach to counter this phenomenon\ninvolves detecting such attacks by identifying toxic language. Effective\nplatform measures aim to report haters and block their network access. In this\ncontext, employing hate speech detection methods aids in identifying these\nattacks amidst vast volumes of text, which are impossible for humans to analyze\nmanually. In our study, we expand upon the usual hate speech detection methods,\ntypically based on text classifiers, to develop a Named Entity Recognition\n(NER) System for Identity Groups. To achieve this, we created a dataset that\nallows extending a conventional NER to recognize identity groups. Consequently,\nour tool not only detects whether a sentence contains an attack but also tags\nthe sentence tokens corresponding to the mentioned group. Results indicate that\nthe model performs competitively in identifying groups with an average f1-score\nof 0.75, outperforming in identifying ethnicity attack spans with an f1-score\nof 0.80 compared to other identity groups. Moreover, the tool shows an\noutstanding generalization capability to minority classes concerning sexual\norientation and gender, achieving an f1-score of 0.77 and 0.72, respectively.\nWe tested the utility of our tool in a case study on social media, annotating\nand comparing comments from Facebook related to news mentioning identity\ngroups. The case study reveals differences in the types of attacks recorded,\neffectively detecting named entities related to the categories of the analyzed\nnews articles. Entities are accurately tagged within their categories, with a\nnegligible error rate for inter-category tagging.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.13011v1",
    "published_date": "2024-05-13 19:33:18 UTC",
    "updated_date": "2024-05-13 19:33:18 UTC"
  },
  {
    "arxiv_id": "2405.08131v1",
    "title": "When factorization meets argumentation: towards argumentative explanations",
    "authors": [
      "Jinfeng Zhong",
      "Elsa Negre"
    ],
    "abstract": "Factorization-based models have gained popularity since the Netflix challenge\n{(2007)}. Since that, various factorization-based models have been developed\nand these models have been proven to be efficient in predicting users' ratings\ntowards items. A major concern is that explaining the recommendations generated\nby such methods is non-trivial because the explicit meaning of the latent\nfactors they learn are not always clear. In response, we propose a novel model\nthat combines factorization-based methods with argumentation frameworks (AFs).\nThe integration of AFs provides clear meaning at each stage of the model,\nenabling it to produce easily understandable explanations for its\nrecommendations. In this model, for every user-item interaction, an AF is\ndefined in which the features of items are considered as arguments, and the\nusers' ratings towards these features determine the strength and polarity of\nthese arguments. This perspective allows our model to treat feature attribution\nas a structured argumentation procedure, where each calculation is marked with\nexplicit meaning, enhancing its inherent interpretability. Additionally, our\nframework seamlessly incorporates side information, such as user contexts,\nleading to more accurate predictions. We anticipate at least three practical\napplications for our model: creating explanation templates, providing\ninteractive explanations, and generating contrastive explanations. Through\ntesting on real-world datasets, we have found that our model, along with its\nvariants, not only surpasses existing argumentation-based methods but also\ncompetes effectively with current context-free and context-aware methods.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "arXiv admin note: substantial text overlap with arXiv:2310.16157",
    "pdf_url": "http://arxiv.org/pdf/2405.08131v1",
    "published_date": "2024-05-13 19:16:28 UTC",
    "updated_date": "2024-05-13 19:16:28 UTC"
  },
  {
    "arxiv_id": "2405.08125v2",
    "title": "AI-Cybersecurity Education Through Designing AI-based Cyberharassment Detection Lab",
    "authors": [
      "Ebuka Okpala",
      "Nishant Vishwamitra",
      "Keyan Guo",
      "Song Liao",
      "Long Cheng",
      "Hongxin Hu",
      "Yongkai Wu",
      "Xiaohong Yuan",
      "Jeannette Wade",
      "Sajad Khorsandroo"
    ],
    "abstract": "Cyberharassment is a critical, socially relevant cybersecurity problem\nbecause of the adverse effects it can have on targeted groups or individuals.\nWhile progress has been made in understanding cyber-harassment, its detection,\nattacks on artificial intelligence (AI) based cyberharassment systems, and the\nsocial problems in cyberharassment detectors, little has been done in designing\nexperiential learning educational materials that engage students in this\nemerging social cybersecurity in the era of AI. Experiential learning\nopportunities are usually provided through capstone projects and engineering\ndesign courses in STEM programs such as computer science. While capstone\nprojects are an excellent example of experiential learning, given the\ninterdisciplinary nature of this emerging social cybersecurity problem, it can\nbe challenging to use them to engage non-computing students without prior\nknowledge of AI. Because of this, we were motivated to develop a hands-on lab\nplatform that provided experiential learning experiences to non-computing\nstudents with little or no background knowledge in AI and discussed the lessons\nlearned in developing this lab. In this lab used by social science students at\nNorth Carolina A&T State University across two semesters (spring and fall) in\n2022, students are given a detailed lab manual and are to complete a set of\nwell-detailed tasks. Through this process, students learn AI concepts and the\napplication of AI for cyberharassment detection. Using pre- and post-surveys,\nwe asked students to rate their knowledge or skills in AI and their\nunderstanding of the concepts learned. The results revealed that the students\nmoderately understood the concepts of AI and cyberharassment.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "10 pages",
    "pdf_url": "http://arxiv.org/pdf/2405.08125v2",
    "published_date": "2024-05-13 19:09:50 UTC",
    "updated_date": "2024-05-16 12:10:58 UTC"
  },
  {
    "arxiv_id": "2405.08120v1",
    "title": "From Questions to Insightful Answers: Building an Informed Chatbot for University Resources",
    "authors": [
      "Subash Neupane",
      "Elias Hossain",
      "Jason Keith",
      "Himanshu Tripathi",
      "Farbod Ghiasi",
      "Noorbakhsh Amiri Golilarz",
      "Amin Amirlatifi",
      "Sudip Mittal",
      "Shahram Rahimi"
    ],
    "abstract": "This paper presents BARKPLUG V.2, a Large Language Model (LLM)-based chatbot\nsystem built using Retrieval Augmented Generation (RAG) pipelines to enhance\nthe user experience and access to information within academic settings.The\nobjective of BARKPLUG V.2 is to provide information to users about various\ncampus resources, including academic departments, programs, campus facilities,\nand student resources at a university setting in an interactive fashion. Our\nsystem leverages university data as an external data corpus and ingests it into\nour RAG pipelines for domain-specific question-answering tasks. We evaluate the\neffectiveness of our system in generating accurate and pertinent responses for\nMississippi State University, as a case study, using quantitative measures,\nemploying frameworks such as Retrieval Augmented Generation Assessment(RAGAS).\nFurthermore, we evaluate the usability of this system via subjective\nsatisfaction surveys using the System Usability Scale (SUS). Our system\ndemonstrates impressive quantitative performance, with a mean RAGAS score of\n0.96, and experience, as validated by usability assessments.",
    "categories": [
      "cs.ET",
      "cs.AI"
    ],
    "primary_category": "cs.ET",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.08120v1",
    "published_date": "2024-05-13 19:05:42 UTC",
    "updated_date": "2024-05-13 19:05:42 UTC"
  },
  {
    "arxiv_id": "2405.07992v3",
    "title": "MambaOut: Do We Really Need Mamba for Vision?",
    "authors": [
      "Weihao Yu",
      "Xinchao Wang"
    ],
    "abstract": "Mamba, an architecture with RNN-like token mixer of state space model (SSM),\nwas recently introduced to address the quadratic complexity of the attention\nmechanism and subsequently applied to vision tasks. Nevertheless, the\nperformance of Mamba for vision is often underwhelming when compared with\nconvolutional and attention-based models. In this paper, we delve into the\nessence of Mamba, and conceptually conclude that Mamba is ideally suited for\ntasks with long-sequence and autoregressive characteristics. For vision tasks,\nas image classification does not align with either characteristic, we\nhypothesize that Mamba is not necessary for this task; Detection and\nsegmentation tasks are also not autoregressive, yet they adhere to the\nlong-sequence characteristic, so we believe it is still worthwhile to explore\nMamba's potential for these tasks. To empirically verify our hypotheses, we\nconstruct a series of models named MambaOut through stacking Mamba blocks while\nremoving their core token mixer, SSM. Experimental results strongly support our\nhypotheses. Specifically, our MambaOut model surpasses all visual Mamba models\non ImageNet image classification, indicating that Mamba is indeed unnecessary\nfor this task. As for detection and segmentation, MambaOut cannot match the\nperformance of state-of-the-art visual Mamba models, demonstrating the\npotential of Mamba for long-sequence visual tasks. The code is available at\nhttps://github.com/yuweihao/MambaOut",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Code: https://github.com/yuweihao/MambaOut",
    "pdf_url": "http://arxiv.org/pdf/2405.07992v3",
    "published_date": "2024-05-13 17:59:56 UTC",
    "updated_date": "2024-05-20 16:36:21 UTC"
  },
  {
    "arxiv_id": "2405.07991v1",
    "title": "SPIN: Simultaneous Perception, Interaction and Navigation",
    "authors": [
      "Shagun Uppal",
      "Ananye Agarwal",
      "Haoyu Xiong",
      "Kenneth Shaw",
      "Deepak Pathak"
    ],
    "abstract": "While there has been remarkable progress recently in the fields of\nmanipulation and locomotion, mobile manipulation remains a long-standing\nchallenge. Compared to locomotion or static manipulation, a mobile system must\nmake a diverse range of long-horizon tasks feasible in unstructured and dynamic\nenvironments. While the applications are broad and interesting, there are a\nplethora of challenges in developing these systems such as coordination between\nthe base and arm, reliance on onboard perception for perceiving and interacting\nwith the environment, and most importantly, simultaneously integrating all\nthese parts together. Prior works approach the problem using disentangled\nmodular skills for mobility and manipulation that are trivially tied together.\nThis causes several limitations such as compounding errors, delays in\ndecision-making, and no whole-body coordination. In this work, we present a\nreactive mobile manipulation framework that uses an active visual system to\nconsciously perceive and react to its environment. Similar to how humans\nleverage whole-body and hand-eye coordination, we develop a mobile manipulator\nthat exploits its ability to move and see, more specifically -- to move in\norder to see and to see in order to move. This allows it to not only move\naround and interact with its environment but also, choose \"when\" to perceive\n\"what\" using an active visual system. We observe that such an agent learns to\nnavigate around complex cluttered scenarios while displaying agile whole-body\ncoordination using only ego-vision without needing to create environment maps.\nResults visualizations and videos at https://spin-robot.github.io/",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "In CVPR 2024. Website at https://spin-robot.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2405.07991v1",
    "published_date": "2024-05-13 17:59:36 UTC",
    "updated_date": "2024-05-13 17:59:36 UTC"
  },
  {
    "arxiv_id": "2405.07987v5",
    "title": "The Platonic Representation Hypothesis",
    "authors": [
      "Minyoung Huh",
      "Brian Cheung",
      "Tongzhou Wang",
      "Phillip Isola"
    ],
    "abstract": "We argue that representations in AI models, particularly deep networks, are\nconverging. First, we survey many examples of convergence in the literature:\nover time and across multiple domains, the ways by which different neural\nnetworks represent data are becoming more aligned. Next, we demonstrate\nconvergence across data modalities: as vision models and language models get\nlarger, they measure distance between datapoints in a more and more alike way.\nWe hypothesize that this convergence is driving toward a shared statistical\nmodel of reality, akin to Plato's concept of an ideal reality. We term such a\nrepresentation the platonic representation and discuss several possible\nselective pressures toward it. Finally, we discuss the implications of these\ntrends, their limitations, and counterexamples to our analysis.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "Equal contributions. Project: https://phillipi.github.io/prh/ Code:\n  https://github.com/minyoungg/platonic-rep",
    "pdf_url": "http://arxiv.org/pdf/2405.07987v5",
    "published_date": "2024-05-13 17:58:30 UTC",
    "updated_date": "2024-07-25 09:33:50 UTC"
  },
  {
    "arxiv_id": "2405.07976v3",
    "title": "Localized Adaptive Risk Control",
    "authors": [
      "Matteo Zecchin",
      "Osvaldo Simeone"
    ],
    "abstract": "Adaptive Risk Control (ARC) is an online calibration strategy based on set\nprediction that offers worst-case deterministic long-term risk control, as well\nas statistical marginal coverage guarantees. ARC adjusts the size of the\nprediction set by varying a single scalar threshold based on feedback from past\ndecisions. In this work, we introduce Localized Adaptive Risk Control (L-ARC),\nan online calibration scheme that targets statistical localized risk guarantees\nranging from conditional risk to marginal risk, while preserving the worst-case\nperformance of ARC. L-ARC updates a threshold function within a reproducing\nkernel Hilbert space (RKHS), with the kernel determining the level of\nlocalization of the statistical risk guarantee. The theoretical results\nhighlight a trade-off between localization of the statistical risk and\nconvergence speed to the long-term risk target. Thanks to localization, L-ARC\nis demonstrated via experiments to produce prediction sets with risk guarantees\nacross different data subpopulations, significantly improving the fairness of\nthe calibrated model for tasks such as image segmentation and beam selection in\nwireless networks.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.IT",
      "cs.LG",
      "math.IT"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.07976v3",
    "published_date": "2024-05-13 17:48:45 UTC",
    "updated_date": "2024-10-10 07:17:42 UTC"
  },
  {
    "arxiv_id": "2405.07969v1",
    "title": "Investigating the Semantic Robustness of CLIP-based Zero-Shot Anomaly Segmentation",
    "authors": [
      "Kevin Stangl",
      "Marius Arvinte",
      "Weilin Xu",
      "Cory Cornelius"
    ],
    "abstract": "Zero-shot anomaly segmentation using pre-trained foundation models is a\npromising approach that enables effective algorithms without expensive,\ndomain-specific training or fine-tuning. Ensuring that these methods work\nacross various environmental conditions and are robust to distribution shifts\nis an open problem. We investigate the performance of WinCLIP [14] zero-shot\nanomaly segmentation algorithm by perturbing test data using three semantic\ntransformations: bounded angular rotations, bounded saturation shifts, and hue\nshifts. We empirically measure a lower performance bound by aggregating across\nper-sample worst-case perturbations and find that average performance drops by\nup to 20% in area under the ROC curve and 40% in area under the per-region\noverlap curve. We find that performance is consistently lowered on three CLIP\nbackbones, regardless of model architecture or learning objective,\ndemonstrating a need for careful performance evaluation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.07969v1",
    "published_date": "2024-05-13 17:47:08 UTC",
    "updated_date": "2024-05-13 17:47:08 UTC"
  },
  {
    "arxiv_id": "2405.07966v1",
    "title": "OverlapMamba: Novel Shift State Space Model for LiDAR-based Place Recognition",
    "authors": [
      "Qiuchi Xiang",
      "Jintao Cheng",
      "Jiehao Luo",
      "Jin Wu",
      "Rui Fan",
      "Xieyuanli Chen",
      "Xiaoyu Tang"
    ],
    "abstract": "Place recognition is the foundation for enabling autonomous systems to\nachieve independent decision-making and safe operations. It is also crucial in\ntasks such as loop closure detection and global localization within SLAM.\nPrevious methods utilize mundane point cloud representations as input and deep\nlearning-based LiDAR-based Place Recognition (LPR) approaches employing\ndifferent point cloud image inputs with convolutional neural networks (CNNs) or\ntransformer architectures. However, the recently proposed Mamba deep learning\nmodel, combined with state space models (SSMs), holds great potential for long\nsequence modeling. Therefore, we developed OverlapMamba, a novel network for\nplace recognition, which represents input range views (RVs) as sequences. In a\nnovel way, we employ a stochastic reconstruction approach to build shift state\nspace models, compressing the visual representation. Evaluated on three\ndifferent public datasets, our method effectively detects loop closures,\nshowing robustness even when traversing previously visited locations from\ndifferent directions. Relying on raw range view inputs, it outperforms typical\nLiDAR and multi-view combination methods in time complexity and speed,\nindicating strong place recognition capabilities and real-time efficiency.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.07966v1",
    "published_date": "2024-05-13 17:46:35 UTC",
    "updated_date": "2024-05-13 17:46:35 UTC"
  },
  {
    "arxiv_id": "2405.07943v2",
    "title": "Decision Mamba Architectures",
    "authors": [
      "André Correia",
      "Luís A. Alexandre"
    ],
    "abstract": "Recent advancements in imitation learning have been largely fueled by the\nintegration of sequence models, which provide a structured flow of information\nto effectively mimic task behaviours. Currently, Decision Transformer (DT) and\nsubsequently, the Hierarchical Decision Transformer (HDT), presented\nTransformer-based approaches to learn task policies. Recently, the Mamba\narchitecture has shown to outperform Transformers across various task domains.\nIn this work, we introduce two novel methods, Decision Mamba (DM) and\nHierarchical Decision Mamba (HDM), aimed at enhancing the performance of the\nTransformer models. Through extensive experimentation across diverse\nenvironments such as OpenAI Gym and D4RL, leveraging varying demonstration data\nsets, we demonstrate the superiority of Mamba models over their Transformer\ncounterparts in a majority of tasks. Results show that DM outperforms other\nmethods in most settings. The code can be found at\nhttps://github.com/meowatthemoon/DecisionMamba.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.07943v2",
    "published_date": "2024-05-13 17:18:08 UTC",
    "updated_date": "2024-10-17 09:48:06 UTC"
  },
  {
    "arxiv_id": "2405.07932v2",
    "title": "PARDEN, Can You Repeat That? Defending against Jailbreaks via Repetition",
    "authors": [
      "Ziyang Zhang",
      "Qizhen Zhang",
      "Jakob Foerster"
    ],
    "abstract": "Large language models (LLMs) have shown success in many natural language\nprocessing tasks. Despite rigorous safety alignment processes, supposedly\nsafety-aligned LLMs like Llama 2 and Claude 2 are still susceptible to\njailbreaks, leading to security risks and abuse of the models. One option to\nmitigate such risks is to augment the LLM with a dedicated \"safeguard\", which\nchecks the LLM's inputs or outputs for undesired behaviour. A promising\napproach is to use the LLM itself as the safeguard. Nonetheless, baseline\nmethods, such as prompting the LLM to self-classify toxic content, demonstrate\nlimited efficacy. We hypothesise that this is due to domain shift: the\nalignment training imparts a self-censoring behaviour to the model (\"Sorry I\ncan't do that\"), while the self-classify approach shifts it to a classification\nformat (\"Is this prompt malicious\"). In this work, we propose PARDEN, which\navoids this domain shift by simply asking the model to repeat its own outputs.\nPARDEN neither requires finetuning nor white box access to the model. We\nempirically verify the effectiveness of our method and show that PARDEN\nsignificantly outperforms existing jailbreak detection baselines for Llama-2\nand Claude-2. Code and data are available at https://github.com/Ed-Zh/PARDEN.\n  We find that PARDEN is particularly powerful in the relevant regime of high\nTrue Positive Rate (TPR) and low False Positive Rate (FPR). For instance, for\nLlama2-7B, at TPR equal to 90%, PARDEN accomplishes a roughly 11x reduction in\nthe FPR from 24.8% to 2.0% on the harmful behaviours dataset.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at ICML 20224",
    "pdf_url": "http://arxiv.org/pdf/2405.07932v2",
    "published_date": "2024-05-13 17:08:42 UTC",
    "updated_date": "2024-05-14 15:56:37 UTC"
  },
  {
    "arxiv_id": "2405.07925v1",
    "title": "Stable Diffusion-based Data Augmentation for Federated Learning with Non-IID Data",
    "authors": [
      "Mahdi Morafah",
      "Matthias Reisser",
      "Bill Lin",
      "Christos Louizos"
    ],
    "abstract": "The proliferation of edge devices has brought Federated Learning (FL) to the\nforefront as a promising paradigm for decentralized and collaborative model\ntraining while preserving the privacy of clients' data. However, FL struggles\nwith a significant performance reduction and poor convergence when confronted\nwith Non-Independent and Identically Distributed (Non-IID) data distributions\namong participating clients. While previous efforts, such as client drift\nmitigation and advanced server-side model fusion techniques, have shown some\nsuccess in addressing this challenge, they often overlook the root cause of the\nperformance reduction - the absence of identical data accurately mirroring the\nglobal data distribution among clients. In this paper, we introduce Gen-FedSD,\na novel approach that harnesses the powerful capability of state-of-the-art\ntext-to-image foundation models to bridge the significant Non-IID performance\ngaps in FL. In Gen-FedSD, each client constructs textual prompts for each class\nlabel and leverages an off-the-shelf state-of-the-art pre-trained Stable\nDiffusion model to synthesize high-quality data samples. The generated\nsynthetic data is tailored to each client's unique local data gaps and\ndistribution disparities, effectively making the final augmented local data\nIID. Through extensive experimentation, we demonstrate that Gen-FedSD achieves\nstate-of-the-art performance and significant communication cost savings across\nvarious datasets and Non-IID settings.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "International Workshop on Federated Foundation Models for the Web\n  2024 (FL@FM-TheWebConf'24)",
    "pdf_url": "http://arxiv.org/pdf/2405.07925v1",
    "published_date": "2024-05-13 16:57:48 UTC",
    "updated_date": "2024-05-13 16:57:48 UTC"
  },
  {
    "arxiv_id": "2405.07916v1",
    "title": "IMAFD: An Interpretable Multi-stage Approach to Flood Detection from time series Multispectral Data",
    "authors": [
      "Ziyang Zhang",
      "Plamen Angelov",
      "Dmitry Kangin",
      "Nicolas Longépé"
    ],
    "abstract": "In this paper, we address two critical challenges in the domain of flood\ndetection: the computational expense of large-scale time series change\ndetection and the lack of interpretable decision-making processes on\nexplainable AI (XAI). To overcome these challenges, we proposed an\ninterpretable multi-stage approach to flood detection, IMAFD has been proposed.\nIt provides an automatic, efficient and interpretable solution suitable for\nlarge-scale remote sensing tasks and offers insight into the decision-making\nprocess. The proposed IMAFD approach combines the analysis of the dynamic time\nseries image sequences to identify images with possible flooding with the\nstatic, within-image semantic segmentation. It combines anomaly detection (at\nboth image and pixel level) with semantic segmentation. The flood detection\nproblem is addressed through four stages: (1) at a sequence level: identifying\nthe suspected images (2) at a multi-image level: detecting change within\nsuspected images (3) at an image level: semantic segmentation of images into\nLand, Water or Cloud class (4) decision making. Our contributions are two\nfolder. First, we efficiently reduced the number of frames to be processed for\ndense change detection by providing a multi-stage holistic approach to flood\ndetection. Second, the proposed semantic change detection method (stage 3)\nprovides human users with an interpretable decision-making process, while most\nof the explainable AI (XAI) methods provide post hoc explanations. The\nevaluation of the proposed IMAFD framework was performed on three datasets,\nWorldFloods, RavAEn and MediaEval. For all the above datasets, the proposed\nframework demonstrates a competitive performance compared to other methods\noffering also interpretability and insight.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.07916v1",
    "published_date": "2024-05-13 16:47:53 UTC",
    "updated_date": "2024-05-13 16:47:53 UTC"
  },
  {
    "arxiv_id": "2405.07893v1",
    "title": "Science based AI model certification for new operational environments with application in traffic state estimation",
    "authors": [
      "Daryl Mupupuni",
      "Anupama Guntu",
      "Liang Hong",
      "Kamrul Hasan",
      "Leehyun Keel"
    ],
    "abstract": "The expanding role of Artificial Intelligence (AI) in diverse engineering\ndomains highlights the challenges associated with deploying AI models in new\noperational environments, involving substantial investments in data collection\nand model training. Rapid application of AI necessitates evaluating the\nfeasibility of utilizing pre-trained models in unobserved operational settings\nwith minimal or no additional data. However, interpreting the opaque nature of\nAI's black-box models remains a persistent challenge. Addressing this issue,\nthis paper proposes a science-based certification methodology to assess the\nviability of employing pre-trained data-driven models in new operational\nenvironments. The methodology advocates a profound integration of domain\nknowledge, leveraging theoretical and analytical models from physics and\nrelated disciplines, with data-driven AI models. This novel approach introduces\ntools to facilitate the development of secure engineering systems, providing\ndecision-makers with confidence in the trustworthiness and safety of AI-based\nmodels across diverse environments characterized by limited training data and\ndynamic, uncertain conditions. The paper demonstrates the efficacy of this\nmethodology in real-world safety-critical scenarios, particularly in the\ncontext of traffic state estimation. Through simulation results, the study\nillustrates how the proposed methodology efficiently quantifies physical\ninconsistencies exhibited by pre-trained AI models. By utilizing analytical\nmodels, the methodology offers a means to gauge the applicability of\npre-trained AI models in new operational environments. This research\ncontributes to advancing the understanding and deployment of AI models,\noffering a robust certification framework that enhances confidence in their\nreliability and safety across a spectrum of operational conditions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "7 Pages, 5 figures, \\c{opyright}2024 IEEE INTERNATIONAL CONFERENCE on\n  ELECTRO/INFORMATION TECHNOLOGY",
    "pdf_url": "http://arxiv.org/pdf/2405.07893v1",
    "published_date": "2024-05-13 16:28:00 UTC",
    "updated_date": "2024-05-13 16:28:00 UTC"
  },
  {
    "arxiv_id": "2405.07863v3",
    "title": "RLHF Workflow: From Reward Modeling to Online RLHF",
    "authors": [
      "Hanze Dong",
      "Wei Xiong",
      "Bo Pang",
      "Haoxiang Wang",
      "Han Zhao",
      "Yingbo Zhou",
      "Nan Jiang",
      "Doyen Sahoo",
      "Caiming Xiong",
      "Tong Zhang"
    ],
    "abstract": "We present the workflow of Online Iterative Reinforcement Learning from Human\nFeedback (RLHF) in this technical report, which is widely reported to\noutperform its offline counterpart by a large margin in the recent large\nlanguage model (LLM) literature. However, existing open-source RLHF projects\nare still largely confined to the offline learning setting. In this technical\nreport, we aim to fill in this gap and provide a detailed recipe that is easy\nto reproduce for online iterative RLHF. In particular, since online human\nfeedback is usually infeasible for open-source communities with limited\nresources, we start by constructing preference models using a diverse set of\nopen-source datasets and use the constructed proxy preference model to\napproximate human feedback. Then, we discuss the theoretical insights and\nalgorithmic principles behind online iterative RLHF, followed by a detailed\npractical implementation. Our trained LLM achieves impressive performance on\nLLM chatbot benchmarks, including AlpacaEval-2, Arena-Hard, and MT-Bench, as\nwell as other academic benchmarks such as HumanEval and TruthfulQA. We have\nshown that supervised fine-tuning (SFT) and iterative RLHF can obtain\nstate-of-the-art performance with fully open-source datasets. Further, we have\nmade our models, curated datasets, and comprehensive step-by-step code\nguidebooks publicly available. Please refer to\nhttps://github.com/RLHFlow/RLHF-Reward-Modeling and\nhttps://github.com/RLHFlow/Online-RLHF for more detailed information.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Published in Transactions on Machine Learning Research (09/2024)",
    "pdf_url": "http://arxiv.org/pdf/2405.07863v3",
    "published_date": "2024-05-13 15:50:39 UTC",
    "updated_date": "2024-11-12 11:18:43 UTC"
  },
  {
    "arxiv_id": "2405.07857v3",
    "title": "Synergistic Integration of Coordinate Network and Tensorial Feature for Improving Neural Radiance Fields from Sparse Inputs",
    "authors": [
      "Mingyu Kim",
      "Jun-Seong Kim",
      "Se-Young Yun",
      "Jin-Hwa Kim"
    ],
    "abstract": "The multi-plane representation has been highlighted for its fast training and\ninference across static and dynamic neural radiance fields. This approach\nconstructs relevant features via projection onto learnable grids and\ninterpolating adjacent vertices. However, it has limitations in capturing\nlow-frequency details and tends to overuse parameters for low-frequency\nfeatures due to its bias toward fine details, despite its multi-resolution\nconcept. This phenomenon leads to instability and inefficiency when training\nposes are sparse. In this work, we propose a method that synergistically\nintegrates multi-plane representation with a coordinate-based MLP network known\nfor strong bias toward low-frequency signals. The coordinate-based network is\nresponsible for capturing low-frequency details, while the multi-plane\nrepresentation focuses on capturing fine-grained details. We demonstrate that\nusing residual connections between them seamlessly preserves their own inherent\nproperties. Additionally, the proposed progressive training scheme accelerates\nthe disentanglement of these two features. We demonstrate empirically that our\nproposed method not only outperforms baseline models for both static and\ndynamic NeRFs with sparse inputs, but also achieves comparable results with\nfewer parameters.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "ICML2024 ; Project page is accessible at\n  https://mingyukim87.github.io/SynergyNeRF ; Code is available at\n  https://github.com/MingyuKim87/SynergyNeRF",
    "pdf_url": "http://arxiv.org/pdf/2405.07857v3",
    "published_date": "2024-05-13 15:42:46 UTC",
    "updated_date": "2024-06-05 09:32:36 UTC"
  },
  {
    "arxiv_id": "2405.07839v2",
    "title": "Constrained Exploration via Reflected Replica Exchange Stochastic Gradient Langevin Dynamics",
    "authors": [
      "Haoyang Zheng",
      "Hengrong Du",
      "Qi Feng",
      "Wei Deng",
      "Guang Lin"
    ],
    "abstract": "Replica exchange stochastic gradient Langevin dynamics (reSGLD) is an\neffective sampler for non-convex learning in large-scale datasets. However, the\nsimulation may encounter stagnation issues when the high-temperature chain\ndelves too deeply into the distribution tails. To tackle this issue, we propose\nreflected reSGLD (r2SGLD): an algorithm tailored for constrained non-convex\nexploration by utilizing reflection steps within a bounded domain.\nTheoretically, we observe that reducing the diameter of the domain enhances\nmixing rates, exhibiting a $\\textit{quadratic}$ behavior. Empirically, we test\nits performance through extensive experiments, including identifying dynamical\nsystems with physical constraints, simulations of constrained multi-modal\ndistributions, and image classification tasks. The theoretical and empirical\nfindings highlight the crucial role of constrained exploration in improving the\nsimulation efficiency.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "28 pages, 13 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.07839v2",
    "published_date": "2024-05-13 15:25:03 UTC",
    "updated_date": "2024-06-03 13:48:52 UTC"
  },
  {
    "arxiv_id": "2405.07838v2",
    "title": "Adaptive Exploration for Data-Efficient General Value Function Evaluations",
    "authors": [
      "Arushi Jain",
      "Josiah P. Hanna",
      "Doina Precup"
    ],
    "abstract": "General Value Functions (GVFs) (Sutton et al., 2011) represent predictive\nknowledge in reinforcement learning. Each GVF computes the expected return for\na given policy, based on a unique reward. Existing methods relying on fixed\nbehavior policies or pre-collected data often face data efficiency issues when\nlearning multiple GVFs in parallel using off-policy methods. To address this,\nwe introduce GVFExplorer, which adaptively learns a single behavior policy that\nefficiently collects data for evaluating multiple GVFs in parallel. Our method\noptimizes the behavior policy by minimizing the total variance in return across\nGVFs, thereby reducing the required environmental interactions. We use an\nexisting temporal-difference-style variance estimator to approximate the return\nvariance. We prove that each behavior policy update decreases the overall mean\nsquared error in GVF predictions. We empirically show our method's performance\nin tabular and nonlinear function approximation settings, including Mujoco\nenvironments, with stationary and non-stationary reward signals, optimizing\ndata usage and reducing prediction errors across multiple GVFs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "26 pages, 16 figures, Accepted in NeurIPS 2024 Conference",
    "pdf_url": "http://arxiv.org/pdf/2405.07838v2",
    "published_date": "2024-05-13 15:24:27 UTC",
    "updated_date": "2024-10-13 15:54:10 UTC"
  },
  {
    "arxiv_id": "2405.07827v1",
    "title": "Automatic Recognition of Food Ingestion Environment from the AIM-2 Wearable Sensor",
    "authors": [
      "Yuning Huang",
      "Mohamed Abul Hassan",
      "Jiangpeng He",
      "Janine Higgins",
      "Megan McCrory",
      "Heather Eicher-Miller",
      "Graham Thomas",
      "Edward O Sazonov",
      "Fengqing Maggie Zhu"
    ],
    "abstract": "Detecting an ingestion environment is an important aspect of monitoring\ndietary intake. It provides insightful information for dietary assessment.\nHowever, it is a challenging problem where human-based reviewing can be\ntedious, and algorithm-based review suffers from data imbalance and perceptual\naliasing problems. To address these issues, we propose a neural network-based\nmethod with a two-stage training framework that tactfully combines fine-tuning\nand transfer learning techniques. Our method is evaluated on a newly collected\ndataset called ``UA Free Living Study\", which uses an egocentric wearable\ncamera, AIM-2 sensor, to simulate food consumption in free-living conditions.\nThe proposed training framework is applied to common neural network backbones,\ncombined with approaches in the general imbalanced classification field.\nExperimental results on the collected dataset show that our proposed method for\nautomatic ingestion environment recognition successfully addresses the\nchallenging data imbalance problem in the dataset and achieves a promising\noverall classification accuracy of 96.63%.",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.MM",
    "comment": "Accepted at CVPRw 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.07827v1",
    "published_date": "2024-05-13 15:12:21 UTC",
    "updated_date": "2024-05-13 15:12:21 UTC"
  },
  {
    "arxiv_id": "2405.07822v2",
    "title": "Synthetic Tabular Data Validation: A Divergence-Based Approach",
    "authors": [
      "Patricia A. Apellániz",
      "Ana Jiménez",
      "Borja Arroyo Galende",
      "Juan Parras",
      "Santiago Zazo"
    ],
    "abstract": "The ever-increasing use of generative models in various fields where tabular\ndata is used highlights the need for robust and standardized validation metrics\nto assess the similarity between real and synthetic data. Current methods lack\na unified framework and rely on diverse and often inconclusive statistical\nmeasures. Divergences, which quantify discrepancies between data distributions,\noffer a promising avenue for validation. However, traditional approaches\ncalculate divergences independently for each feature due to the complexity of\njoint distribution modeling. This paper addresses this challenge by proposing a\nnovel approach that uses divergence estimation to overcome the limitations of\nmarginal comparisons. Our core contribution lies in applying a divergence\nestimator to build a validation metric considering the joint distribution of\nreal and synthetic data. We leverage a probabilistic classifier to approximate\nthe density ratio between datasets, allowing the capture of complex\nrelationships. We specifically calculate two divergences: the well-known\nKullback-Leibler (KL) divergence and the Jensen-Shannon (JS) divergence. KL\ndivergence offers an established use in the field, while JS divergence is\nsymmetric and bounded, providing a reliable metric. The efficacy of this\napproach is demonstrated through a series of experiments with varying\ndistribution complexities. The initial phase involves comparing estimated\ndivergences with analytical solutions for simple distributions, setting a\nbenchmark for accuracy. Finally, we validate our method on a real-world dataset\nand its corresponding synthetic counterpart, showcasing its effectiveness in\npractical applications. This research offers a significant contribution with\napplicability beyond tabular data and the potential to improve synthetic data\nvalidation in various fields.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2.0"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages, 14 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.07822v2",
    "published_date": "2024-05-13 15:07:52 UTC",
    "updated_date": "2024-07-31 10:00:58 UTC"
  },
  {
    "arxiv_id": "2405.07817v1",
    "title": "The Power of Combined Modalities in Interactive Robot Learning",
    "authors": [
      "Helen Beierling",
      "Anna-Lisa Vollmer"
    ],
    "abstract": "This study contributes to the evolving field of robot learning in interaction\nwith humans, examining the impact of diverse input modalities on learning\noutcomes. It introduces the concept of \"meta-modalities\" which encapsulate\nadditional forms of feedback beyond the traditional preference and scalar\nfeedback mechanisms. Unlike prior research that focused on individual\nmeta-modalities, this work evaluates their combined effect on learning\noutcomes. Through a study with human participants, we explore user preferences\nfor these modalities and their impact on robot learning performance. Our\nfindings reveal that while individual modalities are perceived differently,\ntheir combination significantly improves learning behavior and usability. This\nresearch not only provides valuable insights into the optimization of\nhuman-robot interactive task learning but also opens new avenues for enhancing\nthe interactive freedom and scaffolding capabilities provided to users in such\nsettings.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.07817v1",
    "published_date": "2024-05-13 14:59:44 UTC",
    "updated_date": "2024-05-13 14:59:44 UTC"
  },
  {
    "arxiv_id": "2405.07816v1",
    "title": "Quick and Accurate Affordance Learning",
    "authors": [
      "Fedor Scholz",
      "Erik Ayari",
      "Johannes Bertram",
      "Martin V. Butz"
    ],
    "abstract": "Infants learn actively in their environments, shaping their own learning\ncurricula. They learn about their environments' affordances, that is, how local\ncircumstances determine how their behavior can affect the environment. Here we\nmodel this type of behavior by means of a deep learning architecture. The\narchitecture mediates between global cognitive map exploration and local\naffordance learning. Inference processes actively move the simulated agent\ntowards regions where they expect affordance-related knowledge gain. We\ncontrast three measures of uncertainty to guide this exploration: predicted\nuncertainty of a model, standard deviation between the means of several models\n(SD), and the Jensen-Shannon Divergence (JSD) between several models. We show\nthat the first measure gets fooled by aleatoric uncertainty inherent in the\nenvironment, while the two other measures focus learning on epistemic\nuncertainty. JSD exhibits the most balanced exploration strategy. From a\ncomputational perspective, our model suggests three key ingredients for\ncoordinating the active generation of learning curricula: (1) Navigation\nbehavior needs to be coordinated with local motor behavior for enabling active\naffordance learning. (2) Affordances need to be encoded locally for acquiring\ngeneralized knowledge. (3) Effective active affordance learning mechanisms\nshould use density comparison techniques for estimating expected knowledge\ngain. Future work may seek collaborations with developmental psychology to\nmodel active play in children in more realistic scenarios.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.07816v1",
    "published_date": "2024-05-13 14:58:57 UTC",
    "updated_date": "2024-05-13 14:58:57 UTC"
  },
  {
    "arxiv_id": "2405.08045v1",
    "title": "Comparative analysis of neural network architectures for short-term FOREX forecasting",
    "authors": [
      "Theodoros Zafeiriou",
      "Dimitris Kalles"
    ],
    "abstract": "The present document delineates the analysis, design, implementation, and\nbenchmarking of various neural network architectures within a short-term\nfrequency prediction system for the foreign exchange market (FOREX). Our aim is\nto simulate the judgment of the human expert (technical analyst) using a system\nthat responds promptly to changes in market conditions, thus enabling the\noptimization of short-term trading strategies. We designed and implemented a\nseries of LSTM neural network architectures which are taken as input the\nexchange rate values and generate the short-term market trend forecasting\nsignal and an ANN custom architecture based on technical analysis indicator\nsimulators We performed a comparative analysis of the results and came to\nuseful conclusions regarding the suitability of each architecture and the cost\nin terms of time and computational power to implement them. The ANN custom\narchitecture produces better prediction quality with higher sensitivity using\nfewer resources and spending less time than LSTM architectures. The ANN custom\narchitecture appears to be ideal for use in low-power computing systems and for\nuse cases that need fast decisions with the least possible computational cost.",
    "categories": [
      "q-fin.MF",
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "q-fin.MF",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.08045v1",
    "published_date": "2024-05-13 14:51:02 UTC",
    "updated_date": "2024-05-13 14:51:02 UTC"
  },
  {
    "arxiv_id": "2405.07798v2",
    "title": "FreeVA: Offline MLLM as Training-Free Video Assistant",
    "authors": [
      "Wenhao Wu"
    ],
    "abstract": "This paper undertakes an empirical study to revisit the latest advancements\nin Multimodal Large Language Models (MLLMs): Video Assistant. This study,\nnamely FreeVA, aims to extend existing image-based MLLM to the video domain in\na training-free manner. The study provides an essential, yet must-know\nbaseline, and reveals several surprising findings: 1) FreeVA, leveraging only\noffline image-based MLLM without additional training, excels in zero-shot video\nquestion-answering (e.g., MSVD-QA, ActivityNet-QA, and MSRVTT-QA), even\nsurpassing state-of-the-art methods that involve video instruction tuning. 2)\nWhile mainstream video-based MLLMs typically initialize with an image-based\nMLLM (e.g., LLaVA) and then fine-tune using video instruction tuning, the study\nindicates that utilizing the widely adopted VideoInstruct-100K for video\ninstruction tuning doesn't actually lead to better performance compared to not\ntraining at all. 3) The commonly used evaluation metrics in existing works are\nsignificantly influenced by changes in the GPT API version over time. If\nignored, this could affect the fairness and uniformity of comparisons between\ndifferent methods and impact the analysis and judgment of researchers in the\nfield. The advancement of MLLMs is currently thriving, drawing numerous\nresearchers into the field. We aim for this work to serve as a plug-and-play,\nsimple yet effective baseline, encouraging the direct evaluation of existing\nMLLMs in video domain while also standardizing the field of video\nconversational models to a certain extent. Also, we encourage researchers to\nreconsider: Have current video MLLM methods truly acquired knowledge beyond\nimage MLLM? Code is available at https://github.com/whwu95/FreeVA",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Preprint. Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2405.07798v2",
    "published_date": "2024-05-13 14:42:13 UTC",
    "updated_date": "2024-06-10 13:55:21 UTC"
  },
  {
    "arxiv_id": "2405.07780v1",
    "title": "Harnessing Hierarchical Label Distribution Variations in Test Agnostic Long-tail Recognition",
    "authors": [
      "Zhiyong Yang",
      "Qianqian Xu",
      "Zitai Wang",
      "Sicong Li",
      "Boyu Han",
      "Shilong Bao",
      "Xiaochun Cao",
      "Qingming Huang"
    ],
    "abstract": "This paper explores test-agnostic long-tail recognition, a challenging\nlong-tail task where the test label distributions are unknown and arbitrarily\nimbalanced. We argue that the variation in these distributions can be broken\ndown hierarchically into global and local levels. The global ones reflect a\nbroad range of diversity, while the local ones typically arise from milder\nchanges, often focused on a particular neighbor. Traditional methods\npredominantly use a Mixture-of-Expert (MoE) approach, targeting a few fixed\ntest label distributions that exhibit substantial global variations. However,\nthe local variations are left unconsidered. To address this issue, we propose a\nnew MoE strategy, $\\mathsf{DirMixE}$, which assigns experts to different\nDirichlet meta-distributions of the label distribution, each targeting a\nspecific aspect of local variations. Additionally, the diversity among these\nDirichlet meta-distributions inherently captures global variations. This\ndual-level approach also leads to a more stable objective function, allowing us\nto sample different test distributions better to quantify the mean and variance\nof performance outcomes. Theoretically, we show that our proposed objective\nbenefits from enhanced generalization by virtue of the variance-based\nregularization. Comprehensive experiments across multiple benchmarks confirm\nthe effectiveness of $\\mathsf{DirMixE}$. The code is available at\n\\url{https://github.com/scongl/DirMixE}.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.07780v1",
    "published_date": "2024-05-13 14:24:56 UTC",
    "updated_date": "2024-05-13 14:24:56 UTC"
  },
  {
    "arxiv_id": "2405.07778v1",
    "title": "A Comprehensive Analysis of Static Word Embeddings for Turkish",
    "authors": [
      "Karahan Sarıtaş",
      "Cahid Arda Öz",
      "Tunga Güngör"
    ],
    "abstract": "Word embeddings are fixed-length, dense and distributed word representations\nthat are used in natural language processing (NLP) applications. There are\nbasically two types of word embedding models which are non-contextual (static)\nmodels and contextual models. The former method generates a single embedding\nfor a word regardless of its context, while the latter method produces distinct\nembeddings for a word based on the specific contexts in which it appears. There\nare plenty of works that compare contextual and non-contextual embedding models\nwithin their respective groups in different languages. However, the number of\nstudies that compare the models in these two groups with each other is very few\nand there is no such study in Turkish. This process necessitates converting\ncontextual embeddings into static embeddings. In this paper, we compare and\nevaluate the performance of several contextual and non-contextual models in\nboth intrinsic and extrinsic evaluation settings for Turkish. We make a\nfine-grained comparison by analyzing the syntactic and semantic capabilities of\nthe models separately. The results of the analyses provide insights about the\nsuitability of different embedding models in different types of NLP tasks. We\nalso build a Turkish word embedding repository comprising the embedding models\nused in this work, which may serve as a valuable resource for researchers and\npractitioners in the field of Turkish NLP. We make the word embeddings,\nscripts, and evaluation datasets publicly available.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.07778v1",
    "published_date": "2024-05-13 14:23:37 UTC",
    "updated_date": "2024-05-13 14:23:37 UTC"
  },
  {
    "arxiv_id": "2405.07773v2",
    "title": "Human-Modeling in Sequential Decision-Making: An Analysis through the Lens of Human-Aware AI",
    "authors": [
      "Silvia Tulli",
      "Stylianos Loukas Vasileiou",
      "Sarath Sreedharan"
    ],
    "abstract": "\"Human-aware\" has become a popular keyword used to describe a particular\nclass of AI systems that are designed to work and interact with humans. While\nthere exists a surprising level of consistency among the works that use the\nlabel human-aware, the term itself mostly remains poorly understood. In this\nwork, we retroactively try to provide an account of what constitutes a\nhuman-aware AI system. We see that human-aware AI is a design oriented\nparadigm, one that focuses on the need for modeling the humans it may interact\nwith. Additionally, we see that this paradigm offers us intuitive dimensions to\nunderstand and categorize the kinds of interactions these systems might have\nwith humans. We show the pedagogical value of these dimensions by using them as\na tool to understand and review the current landscape of work related to\nhuman-AI systems that purport some form of human modeling. To fit the scope of\na workshop paper, we specifically narrowed our review to papers that deal with\nsequential decision-making and were published in a major AI conference in the\nlast three years. Our analysis helps identify the space of potential research\nproblems that are currently being overlooked. We perform additional analysis on\nthe degree to which these works make explicit reference to results from social\nscience and whether they actually perform user-studies to validate their\nsystems. We also provide an accounting of the various AI methods used by these\nworks.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "I.2"
    ],
    "primary_category": "cs.RO",
    "comment": "9 pages, 1 figure, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2405.07773v2",
    "published_date": "2024-05-13 14:17:52 UTC",
    "updated_date": "2024-07-15 15:01:54 UTC"
  },
  {
    "arxiv_id": "2405.07767v1",
    "title": "Synthetic Test Collections for Retrieval Evaluation",
    "authors": [
      "Hossein A. Rahmani",
      "Nick Craswell",
      "Emine Yilmaz",
      "Bhaskar Mitra",
      "Daniel Campos"
    ],
    "abstract": "Test collections play a vital role in evaluation of information retrieval\n(IR) systems. Obtaining a diverse set of user queries for test collection\nconstruction can be challenging, and acquiring relevance judgments, which\nindicate the appropriateness of retrieved documents to a query, is often costly\nand resource-intensive. Generating synthetic datasets using Large Language\nModels (LLMs) has recently gained significant attention in various\napplications. In IR, while previous work exploited the capabilities of LLMs to\ngenerate synthetic queries or documents to augment training data and improve\nthe performance of ranking models, using LLMs for constructing synthetic test\ncollections is relatively unexplored. Previous studies demonstrate that LLMs\nhave the potential to generate synthetic relevance judgments for use in the\nevaluation of IR systems. In this paper, we comprehensively investigate whether\nit is possible to use LLMs to construct fully synthetic test collections by\ngenerating not only synthetic judgments but also synthetic queries. In\nparticular, we analyse whether it is possible to construct reliable synthetic\ntest collections and the potential risks of bias such test collections may\nexhibit towards LLM-based models. Our experiments indicate that using LLMs it\nis possible to construct synthetic test collections that can reliably be used\nfor retrieval evaluation.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "SIGIR 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.07767v1",
    "published_date": "2024-05-13 14:11:09 UTC",
    "updated_date": "2024-05-13 14:11:09 UTC"
  },
  {
    "arxiv_id": "2405.07766v1",
    "title": "Challenges and Opportunities of NLP for HR Applications: A Discussion Paper",
    "authors": [
      "Jochen L. Leidner",
      "Mark Stevenson"
    ],
    "abstract": "Over the course of the recent decade, tremendous progress has been made in\nthe areas of machine learning and natural language processing, which opened up\nvast areas of potential application use cases, including hiring and human\nresource management. We review the use cases for text analytics in the realm of\nhuman resources/personnel management, including actually realized as well as\npotential but not yet implemented ones, and we analyze the opportunities and\nrisks of these.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7; I.2.1"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 2 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2405.07766v1",
    "published_date": "2024-05-13 14:09:06 UTC",
    "updated_date": "2024-05-13 14:09:06 UTC"
  },
  {
    "arxiv_id": "2405.07761v2",
    "title": "LLM4ED: Large Language Models for Automatic Equation Discovery",
    "authors": [
      "Mengge Du",
      "Yuntian Chen",
      "Zhongzheng Wang",
      "Longfeng Nie",
      "Dongxiao Zhang"
    ],
    "abstract": "Equation discovery is aimed at directly extracting physical laws from data\nand has emerged as a pivotal research domain. Previous methods based on\nsymbolic mathematics have achieved substantial advancements, but often require\nthe design of implementation of complex algorithms. In this paper, we introduce\na new framework that utilizes natural language-based prompts to guide large\nlanguage models (LLMs) in automatically mining governing equations from data.\nSpecifically, we first utilize the generation capability of LLMs to generate\ndiverse equations in string form, and then evaluate the generated equations\nbased on observations. In the optimization phase, we propose two alternately\niterated strategies to optimize generated equations collaboratively. The first\nstrategy is to take LLMs as a black-box optimizer and achieve equation\nself-improvement based on historical samples and their performance. The second\nstrategy is to instruct LLMs to perform evolutionary operators for global\nsearch. Experiments are extensively conducted on both partial differential\nequations and ordinary differential equations. Results demonstrate that our\nframework can discover effective equations to reveal the underlying physical\nlaws under various nonlinear dynamic systems. Further comparisons are made with\nstate-of-the-art models, demonstrating good stability and usability. Our\nframework substantially lowers the barriers to learning and applying equation\ndiscovery techniques, demonstrating the application potential of LLMs in the\nfield of knowledge discovery.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SC",
      "math-ph",
      "math.MP",
      "stat.AP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.07761v2",
    "published_date": "2024-05-13 14:03:49 UTC",
    "updated_date": "2024-07-22 07:13:18 UTC"
  },
  {
    "arxiv_id": "2405.07759v2",
    "title": "MADRL-Based Rate Adaptation for 360° Video Streaming with Multi-Viewpoint Prediction",
    "authors": [
      "Haopeng Wang",
      "Zijian Long",
      "Haiwei Dong",
      "Abdulmotaleb El Saddik"
    ],
    "abstract": "Over the last few years, 360{\\deg} video traffic on the network has grown\nsignificantly. A key challenge of 360{\\deg} video playback is ensuring a high\nquality of experience (QoE) with limited network bandwidth. Currently, most\nstudies focus on tile-based adaptive bitrate (ABR) streaming based on single\nviewport prediction to reduce bandwidth consumption. However, the performance\nof models for single-viewpoint prediction is severely limited by the inherent\nuncertainty in head movement, which can not cope with the sudden movement of\nusers very well. This paper first presents a multimodal spatial-temporal\nattention transformer to generate multiple viewpoint trajectories with their\nprobabilities given a historical trajectory. The proposed method models\nviewpoint prediction as a classification problem and uses attention mechanisms\nto capture the spatial and temporal characteristics of input video frames and\nviewpoint trajectories for multi-viewpoint prediction. After that, a\nmulti-agent deep reinforcement learning (MADRL)-based ABR algorithm utilizing\nmulti-viewpoint prediction for 360{\\deg} video streaming is proposed for\nmaximizing different QoE objectives under various network conditions. We\nformulate the ABR problem as a decentralized partially observable Markov\ndecision process (Dec-POMDP) problem and present a MAPPO algorithm based on\ncentralized training and decentralized execution (CTDE) framework to solve the\nproblem. The experimental results show that our proposed method improves the\ndefined QoE metric by up to 85.5% compared to existing ABR methods.",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.NI",
      "eess.IV"
    ],
    "primary_category": "cs.MM",
    "comment": "Accepted by IEEE Internet of Things Journal",
    "pdf_url": "http://arxiv.org/pdf/2405.07759v2",
    "published_date": "2024-05-13 13:59:59 UTC",
    "updated_date": "2024-05-17 23:21:14 UTC"
  },
  {
    "arxiv_id": "2405.08044v3",
    "title": "On the Volatility of Shapley-Based Contribution Metrics in Federated Learning",
    "authors": [
      "Arno Geimer",
      "Beltran Fiz",
      "Radu State"
    ],
    "abstract": "Federated learning (FL) is a collaborative and privacy-preserving Machine\nLearning paradigm, allowing the development of robust models without the need\nto centralize sensitive data. A critical challenge in FL lies in fairly and\naccurately allocating contributions from diverse participants. Inaccurate\nallocation can undermine trust, lead to unfair compensation, and thus\nparticipants may lack the incentive to join or actively contribute to the\nfederation. Various remuneration strategies have been proposed to date,\nincluding auction-based approaches and Shapley-value-based methods, the latter\noffering a means to quantify the contribution of each participant. However,\nlittle to no work has studied the stability of these contribution evaluation\nmethods. In this paper, we evaluate participant contributions in federated\nlearning using gradient-based model reconstruction techniques with Shapley\nvalues and compare the round-based contributions to a classic data contribution\nmeasurement scheme. We provide an extensive analysis of the discrepancies of\nShapley values across a set of aggregation strategies, and examine them on an\noverall and a per-client level. We show that, between different aggregation\ntechniques, Shapley values lead to unstable reward allocations among\nparticipants. Our analysis spans various data heterogeneity distributions,\nincluding independent and identically distributed (IID) and non-IID scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.08044v3",
    "published_date": "2024-05-13 13:55:34 UTC",
    "updated_date": "2025-04-03 13:13:46 UTC"
  },
  {
    "arxiv_id": "2405.07749v1",
    "title": "DeepHYDRA: Resource-Efficient Time-Series Anomaly Detection in Dynamically-Configured Systems",
    "authors": [
      "Franz Kevin Stehle",
      "Wainer Vandelli",
      "Giuseppe Avolio",
      "Felix Zahn",
      "Holger Fröning"
    ],
    "abstract": "Anomaly detection in distributed systems such as High-Performance Computing\n(HPC) clusters is vital for early fault detection, performance optimisation,\nsecurity monitoring, reliability in general but also operational insights. Deep\nNeural Networks have seen successful use in detecting long-term anomalies in\nmultidimensional data, originating for instance from industrial or medical\nsystems, or weather prediction. A downside of such methods is that they require\na static input size, or lose data through cropping, sampling, or other\ndimensionality reduction methods, making deployment on systems with variability\non monitored data channels, such as computing clusters difficult. To address\nthese problems, we present DeepHYDRA (Deep Hybrid DBSCAN/Reduction-Based\nAnomaly Detection) which combines DBSCAN and learning-based anomaly detection.\nDBSCAN clustering is used to find point anomalies in time-series data,\nmitigating the risk of missing outliers through loss of information when\nreducing input data to a fixed number of channels. A deep learning-based\ntime-series anomaly detection method is then applied to the reduced data in\norder to identify long-term outliers. This hybrid approach reduces the chances\nof missing anomalies that might be made indistinguishable from normal data by\nthe reduction process, and likewise enables the algorithm to be scalable and\ntolerate partial system failures while retaining its detection capabilities.\nUsing a subset of the well-known SMD dataset family, a modified variant of the\nEclipse dataset, as well as an in-house dataset with a large variability in\nactive data channels, made publicly available with this work, we furthermore\nanalyse computational intensity, memory footprint, and activation counts.\nDeepHYDRA is shown to reliably detect different types of anomalies in both\nlarge and complex datasets.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.07749v1",
    "published_date": "2024-05-13 13:47:15 UTC",
    "updated_date": "2024-05-13 13:47:15 UTC"
  },
  {
    "arxiv_id": "2405.07745v1",
    "title": "LlamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language",
    "authors": [
      "Cagri Toraman"
    ],
    "abstract": "Despite advancements in English-dominant generative large language models,\nfurther development is needed for low-resource languages to enhance global\naccessibility. The primary methods for representing these languages are\nmonolingual and multilingual pretraining. Monolingual pretraining is expensive\ndue to hardware requirements, and multilingual models often have uneven\nperformance across languages. This study explores an alternative solution by\nadapting large language models, primarily trained on English, to low-resource\nlanguages. We assess various strategies, including continual training,\ninstruction fine-tuning, task-specific fine-tuning, and vocabulary extension.\nThe results show that continual training improves language comprehension, as\nreflected in perplexity scores, and task-specific tuning generally enhances\nperformance of downstream tasks. However, extending the vocabulary shows no\nsubstantial benefits. Additionally, while larger models improve task\nperformance with few-shot tuning, multilingual models perform worse than their\nmonolingual counterparts when adapted.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.07745v1",
    "published_date": "2024-05-13 13:41:59 UTC",
    "updated_date": "2024-05-13 13:41:59 UTC"
  },
  {
    "arxiv_id": "2405.07735v2",
    "title": "Federated Hierarchical Tensor Networks: a Collaborative Learning Quantum AI-Driven Framework for Healthcare",
    "authors": [
      "Amandeep Singh Bhatia",
      "David E. Bernal Neira"
    ],
    "abstract": "Healthcare industries frequently handle sensitive and proprietary data, and\ndue to strict privacy regulations, they are often reluctant to share data\ndirectly. In today's context, Federated Learning (FL) stands out as a crucial\nremedy, facilitating the rapid advancement of distributed machine learning\nwhile effectively managing critical concerns regarding data privacy and\ngovernance. The fusion of federated learning and quantum computing represents a\ngroundbreaking interdisciplinary approach with immense potential to\nrevolutionize various industries, from healthcare to finance. In this work, we\nproposed a federated learning framework based on quantum tensor networks, which\nleverages the principles of many-body quantum physics. Currently, there are no\nknown classical tensor networks implemented in federated settings. Furthermore,\nwe investigated the effectiveness and feasibility of the proposed framework by\nconducting a differential privacy analysis to ensure the security of sensitive\ndata across healthcare institutions. Experiments on popular medical image\ndatasets show that the federated quantum tensor network model achieved a mean\nreceiver-operator characteristic area under the curve (ROC-AUC) between\n0.91-0.98. Experimental results demonstrate that the quantum federated global\nmodel, consisting of highly entangled tensor network structures, showed better\ngeneralization and robustness and achieved higher testing accuracy, surpassing\nthe performance of locally trained clients under unbalanced data distributions\namong healthcare institutions.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "quant-ph",
    "comment": "12 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.07735v2",
    "published_date": "2024-05-13 13:32:02 UTC",
    "updated_date": "2024-07-04 01:27:00 UTC"
  },
  {
    "arxiv_id": "2405.13010v1",
    "title": "UCCIX: Irish-eXcellence Large Language Model",
    "authors": [
      "Khanh-Tung Tran",
      "Barry O'Sullivan",
      "Hoang D. Nguyen"
    ],
    "abstract": "The development of Large Language Models (LLMs) has predominantly focused on\nhigh-resource languages, leaving extremely low-resource languages like Irish\nwith limited representation. This work presents UCCIX, a pioneering effort on\nthe development of an open-source Irish-based LLM. We propose a novel framework\nfor continued pre-training of LLMs specifically adapted for extremely\nlow-resource languages, requiring only a fraction of the textual data typically\nneeded for training LLMs according to scaling laws. Our model, based on Llama\n2-13B, outperforms much larger models on Irish language tasks with up to 12%\nperformance improvement, showcasing the effectiveness and efficiency of our\napproach. We also contribute comprehensive Irish benchmarking datasets,\nincluding IrishQA, a question-answering dataset, and Irish version of MT-bench.\nThese datasets enable rigorous evaluation and facilitate future research in\nIrish LLM systems. Our work aims to preserve and promote the Irish language,\nknowledge, and culture of Ireland in the digital era while providing a\nframework for adapting LLMs to other indigenous languages.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.13010v1",
    "published_date": "2024-05-13 13:19:27 UTC",
    "updated_date": "2024-05-13 13:19:27 UTC"
  },
  {
    "arxiv_id": "2405.07719v5",
    "title": "USP: A Unified Sequence Parallelism Approach for Long Context Generative AI",
    "authors": [
      "Jiarui Fang",
      "Shangchun Zhao"
    ],
    "abstract": "Sequence parallelism (SP), which divides the sequence dimension of input\ntensors across multiple computational devices, is becoming key to unlocking the\nlong-context capabilities of generative AI models. This paper investigates the\nstate-of-the-art SP approaches, i.e. DeepSpeed-Ulysses and Ring-Attention, and\nproposes a unified SP approach, which is more robust to transformer model\narchitectures and network hardware topology. This paper compares the\ncommunication and memory cost of SP and existing parallelism, including\ndata/tensor/zero/pipeline parallelism, and discusses the best practices for\ndesigning hybrid 4D parallelism involving SP. We achieved 47% MFU on two 8xA800\nnodes using SP for the LLAMA3-8B model training using sequence length 208K. Our\ncode is publicly available at\nhttps://github.com/feifeibear/long-context-attention.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.07719v5",
    "published_date": "2024-05-13 13:08:02 UTC",
    "updated_date": "2024-07-02 09:03:26 UTC"
  },
  {
    "arxiv_id": "2405.08042v1",
    "title": "LLAniMAtion: LLAMA Driven Gesture Animation",
    "authors": [
      "Jonathan Windle",
      "Iain Matthews",
      "Sarah Taylor"
    ],
    "abstract": "Co-speech gesturing is an important modality in conversation, providing\ncontext and social cues. In character animation, appropriate and synchronised\ngestures add realism, and can make interactive agents more engaging.\nHistorically, methods for automatically generating gestures were predominantly\naudio-driven, exploiting the prosodic and speech-related content that is\nencoded in the audio signal. In this paper we instead experiment with using LLM\nfeatures for gesture generation that are extracted from text using LLAMA2. We\ncompare against audio features, and explore combining the two modalities in\nboth objective tests and a user study. Surprisingly, our results show that\nLLAMA2 features on their own perform significantly better than audio features\nand that including both modalities yields no significant difference to using\nLLAMA2 features in isolation. We demonstrate that the LLAMA2 based model can\ngenerate both beat and semantic gestures without any audio input, suggesting\nLLMs can provide rich encodings that are well suited for gesture generation.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.08042v1",
    "published_date": "2024-05-13 12:40:18 UTC",
    "updated_date": "2024-05-13 12:40:18 UTC"
  },
  {
    "arxiv_id": "2405.07682v1",
    "title": "FastSAG: Towards Fast Non-Autoregressive Singing Accompaniment Generation",
    "authors": [
      "Jianyi Chen",
      "Wei Xue",
      "Xu Tan",
      "Zhen Ye",
      "Qifeng Liu",
      "Yike Guo"
    ],
    "abstract": "Singing Accompaniment Generation (SAG), which generates instrumental music to\naccompany input vocals, is crucial to developing human-AI symbiotic art\ncreation systems. The state-of-the-art method, SingSong, utilizes a multi-stage\nautoregressive (AR) model for SAG, however, this method is extremely slow as it\ngenerates semantic and acoustic tokens recursively, and this makes it\nimpossible for real-time applications. In this paper, we aim to develop a Fast\nSAG method that can create high-quality and coherent accompaniments. A non-AR\ndiffusion-based framework is developed, which by carefully designing the\nconditions inferred from the vocal signals, generates the Mel spectrogram of\nthe target accompaniment directly. With diffusion and Mel spectrogram modeling,\nthe proposed method significantly simplifies the AR token-based SingSong\nframework, and largely accelerates the generation. We also design semantic\nprojection, prior projection blocks as well as a set of loss functions, to\nensure the generated accompaniment has semantic and rhythm coherence with the\nvocal signal. By intensive experimental studies, we demonstrate that the\nproposed method can generate better samples than SingSong, and accelerate the\ngeneration by at least 30 times. Audio samples and code are available at\nhttps://fastsag.github.io/.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "cs.MM",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "IJCAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.07682v1",
    "published_date": "2024-05-13 12:14:54 UTC",
    "updated_date": "2024-05-13 12:14:54 UTC"
  },
  {
    "arxiv_id": "2407.04710v2",
    "title": "Visual Evaluative AI: A Hypothesis-Driven Tool with Concept-Based Explanations and Weight of Evidence",
    "authors": [
      "Thao Le",
      "Tim Miller",
      "Ruihan Zhang",
      "Liz Sonenberg",
      "Ronal Singh"
    ],
    "abstract": "This paper presents Visual Evaluative AI, a decision aid that provides\npositive and negative evidence from image data for a given hypothesis. This\ntool finds high-level human concepts in an image and generates the Weight of\nEvidence (WoE) for each hypothesis in the decision-making process. We apply and\nevaluate this tool in the skin cancer domain by building a web-based\napplication that allows users to upload a dermatoscopic image, select a\nhypothesis and analyse their decisions by evaluating the provided evidence.\nFurther, we demonstrate the effectiveness of Visual Evaluative AI on different\nconcept-based explanation approaches.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CV",
    "comment": "4 pages",
    "pdf_url": "http://arxiv.org/pdf/2407.04710v2",
    "published_date": "2024-05-13 12:09:01 UTC",
    "updated_date": "2025-01-19 01:01:25 UTC"
  },
  {
    "arxiv_id": "2405.07668v1",
    "title": "CrossCert: A Cross-Checking Detection Approach to Patch Robustness Certification for Deep Learning Models",
    "authors": [
      "Qilin Zhou",
      "Zhengyuan Wei",
      "Haipeng Wang",
      "Bo Jiang",
      "W. K. Chan"
    ],
    "abstract": "Patch robustness certification is an emerging kind of defense technique\nagainst adversarial patch attacks with provable guarantees. There are two\nresearch lines: certified recovery and certified detection. They aim to label\nmalicious samples with provable guarantees correctly and issue warnings for\nmalicious samples predicted to non-benign labels with provable guarantees,\nrespectively. However, existing certified detection defenders suffer from\nprotecting labels subject to manipulation, and existing certified recovery\ndefenders cannot systematically warn samples about their labels. A certified\ndefense that simultaneously offers robust labels and systematic warning\nprotection against patch attacks is desirable. This paper proposes a novel\ncertified defense technique called CrossCert. CrossCert formulates a novel\napproach by cross-checking two certified recovery defenders to provide\nunwavering certification and detection certification. Unwavering certification\nensures that a certified sample, when subjected to a patched perturbation, will\nalways be returned with a benign label without triggering any warnings with a\nprovable guarantee. To our knowledge, CrossCert is the first certified\ndetection technique to offer this guarantee. Our experiments show that, with a\nslightly lower performance than ViP and comparable performance with PatchCensor\nin terms of detection certification, CrossCert certifies a significant\nproportion of samples with the guarantee of unwavering certification.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.SE",
    "comment": "23 pages, 2 figures, accepted by FSE 2024 (The ACM International\n  Conference on the Foundations of Software Engineering)",
    "pdf_url": "http://arxiv.org/pdf/2405.07668v1",
    "published_date": "2024-05-13 11:54:03 UTC",
    "updated_date": "2024-05-13 11:54:03 UTC"
  },
  {
    "arxiv_id": "2405.07664v1",
    "title": "Geospatial Knowledge Graphs",
    "authors": [
      "Rui Zhu"
    ],
    "abstract": "Geospatial knowledge graphs have emerged as a novel paradigm for representing\nand reasoning over geospatial information. In this framework, entities such as\nplaces, people, events, and observations are depicted as nodes, while their\nrelationships are represented as edges. This graph-based data format lays the\nfoundation for creating a \"FAIR\" (Findable, Accessible, Interoperable, and\nReusable) environment, facilitating the management and analysis of geographic\ninformation. This entry first introduces key concepts in knowledge graphs along\nwith their associated standardization and tools. It then delves into the\napplication of knowledge graphs in geography and environmental sciences,\nemphasizing their role in bridging symbolic and subsymbolic GeoAI to address\ncross-disciplinary geospatial challenges. At the end, new research directions\nrelated to geospatial knowledge graphs are outlined.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.07664v1",
    "published_date": "2024-05-13 11:45:22 UTC",
    "updated_date": "2024-05-13 11:45:22 UTC"
  },
  {
    "arxiv_id": "2405.07662v1",
    "title": "Squeezing Lemons with Hammers: An Evaluation of AutoML and Tabular Deep Learning for Data-Scarce Classification Applications",
    "authors": [
      "Ricardo Knauer",
      "Erik Rodner"
    ],
    "abstract": "Many industry verticals are confronted with small-sized tabular data. In this\nlow-data regime, it is currently unclear whether the best performance can be\nexpected from simple baselines, or more complex machine learning approaches\nthat leverage meta-learning and ensembling. On 44 tabular classification\ndatasets with sample sizes $\\leq$ 500, we find that L2-regularized logistic\nregression performs similar to state-of-the-art automated machine learning\n(AutoML) frameworks (AutoPrognosis, AutoGluon) and off-the-shelf deep neural\nnetworks (TabPFN, HyperFast) on the majority of the benchmark datasets. We\ntherefore recommend to consider logistic regression as the first choice for\ndata-scarce applications with tabular data and provide practitioners with best\npractices for further method selection.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR 2024 Workshop on Practical ML for Low Resource Settings",
    "pdf_url": "http://arxiv.org/pdf/2405.07662v1",
    "published_date": "2024-05-13 11:43:38 UTC",
    "updated_date": "2024-05-13 11:43:38 UTC"
  },
  {
    "arxiv_id": "2405.07653v1",
    "title": "Fast Training Data Acquisition for Object Detection and Segmentation using Black Screen Luminance Keying",
    "authors": [
      "Thomas Pöllabauer",
      "Volker Knauthe",
      "André Boller",
      "Arjan Kuijper",
      "Dieter Fellner"
    ],
    "abstract": "Deep Neural Networks (DNNs) require large amounts of annotated training data\nfor a good performance. Often this data is generated using manual labeling\n(error-prone and time-consuming) or rendering (requiring geometry and material\ninformation). Both approaches make it difficult or uneconomic to apply them to\nmany small-scale applications. A fast and straightforward approach of acquiring\nthe necessary training data would allow the adoption of deep learning to even\nthe smallest of applications. Chroma keying is the process of replacing a color\n(usually blue or green) with another background. Instead of chroma keying, we\npropose luminance keying for fast and straightforward training image\nacquisition. We deploy a black screen with high light absorption (99.99\\%) to\nrecord roughly 1-minute long videos of our target objects, circumventing\ntypical problems of chroma keying, such as color bleeding or color overlap\nbetween background color and object color. Next we automatically mask our\nobjects using simple brightness thresholding, saving the need for manual\nannotation. Finally, we automatically place the objects on random backgrounds\nand train a 2D object detector. We do extensive evaluation of the performance\non the widely-used YCB-V object set and compare favourably to other\nconventional techniques such as rendering, without needing 3D meshes, materials\nor any other information of our target objects and in a fraction of the time\nneeded for other approaches. Our work demonstrates highly accurate training\ndata acquisition allowing to start training state-of-the-art networks within\nminutes.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "32. International Conference in Central Europe on Computer Graphics,\n  Visualization and Computer Vision'2024",
    "pdf_url": "http://arxiv.org/pdf/2405.07653v1",
    "published_date": "2024-05-13 11:28:58 UTC",
    "updated_date": "2024-05-13 11:28:58 UTC"
  },
  {
    "arxiv_id": "2405.07652v1",
    "title": "G-VOILA: Gaze-Facilitated Information Querying in Daily Scenarios",
    "authors": [
      "Zeyu Wang",
      "Yuanchun Shi",
      "Yuntao Wang",
      "Yuchen Yao",
      "Kun Yan",
      "Yuhan Wang",
      "Lei Ji",
      "Xuhai Xu",
      "Chun Yu"
    ],
    "abstract": "Modern information querying systems are progressively incorporating\nmultimodal inputs like vision and audio. However, the integration of gaze -- a\nmodality deeply linked to user intent and increasingly accessible via\ngaze-tracking wearables -- remains underexplored. This paper introduces a novel\ngaze-facilitated information querying paradigm, named G-VOILA, which synergizes\nusers' gaze, visual field, and voice-based natural language queries to\nfacilitate a more intuitive querying process. In a user-enactment study\ninvolving 21 participants in 3 daily scenarios (p = 21, scene = 3), we revealed\nthe ambiguity in users' query language and a gaze-voice coordination pattern in\nusers' natural query behaviors with G-VOILA. Based on the quantitative and\nqualitative findings, we developed a design framework for the G-VOILA paradigm,\nwhich effectively integrates the gaze data with the in-situ querying context.\nThen we implemented a G-VOILA proof-of-concept using cutting-edge deep learning\ntechniques. A follow-up user study (p = 16, scene = 2) demonstrates its\neffectiveness by achieving both higher objective score and subjective score,\ncompared to a baseline without gaze data. We further conducted interviews and\nprovided insights for future gaze-facilitated information querying systems.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "25 pages, 12 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.07652v1",
    "published_date": "2024-05-13 11:24:53 UTC",
    "updated_date": "2024-05-13 11:24:53 UTC"
  },
  {
    "arxiv_id": "2405.07640v3",
    "title": "Hyperparameter Importance Analysis for Multi-Objective AutoML",
    "authors": [
      "Daphne Theodorakopoulos",
      "Frederic Stahl",
      "Marius Lindauer"
    ],
    "abstract": "Hyperparameter optimization plays a pivotal role in enhancing the predictive\nperformance and generalization capabilities of ML models. However, in many\napplications, we do not only care about predictive performance but also about\nadditional objectives such as inference time, memory, or energy consumption. In\nsuch multi-objective scenarios, determining the importance of hyperparameters\nposes a significant challenge due to the complex interplay between the\nconflicting objectives. In this paper, we propose the first method for\nassessing the importance of hyperparameters in multi-objective hyperparameter\noptimization. Our approach leverages surrogate-based hyperparameter importance\nmeasures, i.e., fANOVA and ablation paths, to provide insights into the impact\nof hyperparameters on the optimization objectives. Specifically, we compute the\na-priori scalarization of the objectives and determine the importance of the\nhyperparameters for different objective tradeoffs. Through extensive empirical\nevaluations on diverse benchmark datasets with three different objective pairs,\neach combined with accuracy, namely time, demographic parity loss, and energy\nconsumption, we demonstrate the effectiveness and robustness of our proposed\nmethod. Our findings not only offer valuable guidance for hyperparameter tuning\nin multi-objective optimization tasks but also contribute to advancing the\nunderstanding of hyperparameter importance in complex optimization scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Presented at the 27th European Conference on Artificial Intelligence,\n  19-24 October 2024, Santiago de Compostela, Spain",
    "pdf_url": "http://arxiv.org/pdf/2405.07640v3",
    "published_date": "2024-05-13 11:00:25 UTC",
    "updated_date": "2025-01-02 13:46:53 UTC"
  },
  {
    "arxiv_id": "2405.07638v1",
    "title": "DoLLM: How Large Language Models Understanding Network Flow Data to Detect Carpet Bombing DDoS",
    "authors": [
      "Qingyang Li",
      "Yihang Zhang",
      "Zhidong Jia",
      "Yannan Hu",
      "Lei Zhang",
      "Jianrong Zhang",
      "Yongming Xu",
      "Yong Cui",
      "Zongming Guo",
      "Xinggong Zhang"
    ],
    "abstract": "It is an interesting question Can and How Large Language Models (LLMs)\nunderstand non-language network data, and help us detect unknown malicious\nflows. This paper takes Carpet Bombing as a case study and shows how to exploit\nLLMs' powerful capability in the networking area. Carpet Bombing is a new DDoS\nattack that has dramatically increased in recent years, significantly\nthreatening network infrastructures. It targets multiple victim IPs within\nsubnets, causing congestion on access links and disrupting network services for\na vast number of users. Characterized by low-rates, multi-vectors, these\nattacks challenge traditional DDoS defenses. We propose DoLLM, a DDoS detection\nmodel utilizes open-source LLMs as backbone. By reorganizing non-contextual\nnetwork flows into Flow-Sequences and projecting them into LLMs semantic space\nas token embeddings, DoLLM leverages LLMs' contextual understanding to extract\nflow representations in overall network context. The representations are used\nto improve the DDoS detection performance. We evaluate DoLLM with public\ndatasets CIC-DDoS2019 and real NetFlow trace from Top-3 countrywide ISP. The\ntests have proven that DoLLM possesses strong detection capabilities. Its F1\nscore increased by up to 33.3% in zero-shot scenarios and by at least 20.6% in\nreal ISP traces.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.07638v1",
    "published_date": "2024-05-13 10:53:41 UTC",
    "updated_date": "2024-05-13 10:53:41 UTC"
  },
  {
    "arxiv_id": "2405.13009v2",
    "title": "MetaReflection: Learning Instructions for Language Agents using Past Reflections",
    "authors": [
      "Priyanshu Gupta",
      "Shashank Kirtania",
      "Ananya Singha",
      "Sumit Gulwani",
      "Arjun Radhakrishna",
      "Sherry Shi",
      "Gustavo Soares"
    ],
    "abstract": "The popularity of Large Language Models (LLMs) have unleashed a new age\nofLanguage Agents for solving a diverse range of tasks. While contemporary\nfrontier LLMs are capable enough to power reasonably good Language agents, the\nclosed-API model makes it hard to improve in cases they perform sub-optimally.\nTo address this, recent works have explored ways to improve their performance\nusing techniques like self-reflection and prompt optimization. Unfortunately,\ntechniques like self-reflection can be used only in an online setup, while\ncontemporary prompt optimization techniques are designed and tested to work on\nsimple tasks. To this end, we introduce MetaReflection, a novel offline\nreinforcement learning technique that enhances the performance of Language\nAgents by augmenting a semantic memory based on experiential learnings from\npast trials. We demonstrate the efficacy of MetaReflection by evaluating across\nmultiple domains, including complex logical reasoning, biomedical semantic\nsimilarity, open world question answering, and vulnerability threat detection,\nin Infrastructure-as-Code, spanning different agent designs. MetaReflection\nboosts Language agents' performance by 4% to 16.82% over the raw GPT-4 baseline\nand performs on par with existing state-of-the-art prompt optimization\ntechniques while requiring fewer LLM calls.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "We release our experimental code at:\n  https://aka.ms/metareflection-code",
    "pdf_url": "http://arxiv.org/pdf/2405.13009v2",
    "published_date": "2024-05-13 10:51:43 UTC",
    "updated_date": "2024-10-10 11:51:24 UTC"
  },
  {
    "arxiv_id": "2405.07626v2",
    "title": "AnomalyLLM: Few-shot Anomaly Edge Detection for Dynamic Graphs using Large Language Models",
    "authors": [
      "Shuo Liu",
      "Di Yao",
      "Lanting Fang",
      "Zhetao Li",
      "Wenbin Li",
      "Kaiyu Feng",
      "XiaoWen Ji",
      "Jingping Bi"
    ],
    "abstract": "Detecting anomaly edges for dynamic graphs aims to identify edges\nsignificantly deviating from the normal pattern and can be applied in various\ndomains, such as cybersecurity, financial transactions and AIOps. With the\nevolving of time, the types of anomaly edges are emerging and the labeled\nanomaly samples are few for each type. Current methods are either designed to\ndetect randomly inserted edges or require sufficient labeled data for model\ntraining, which harms their applicability for real-world applications. In this\npaper, we study this problem by cooperating with the rich knowledge encoded in\nlarge language models(LLMs) and propose a method, namely AnomalyLLM. To align\nthe dynamic graph with LLMs, AnomalyLLM pre-trains a dynamic-aware encoder to\ngenerate the representations of edges and reprograms the edges using the\nprototypes of word embeddings. Along with the encoder, we design an in-context\nlearning framework that integrates the information of a few labeled samples to\nachieve few-shot anomaly detection. Experiments on four datasets reveal that\nAnomalyLLM can not only significantly improve the performance of few-shot\nanomaly detection, but also achieve superior results on new anomalies without\nany update of model parameters.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "13pages",
    "pdf_url": "http://arxiv.org/pdf/2405.07626v2",
    "published_date": "2024-05-13 10:37:50 UTC",
    "updated_date": "2024-08-28 06:18:28 UTC"
  },
  {
    "arxiv_id": "2405.07621v2",
    "title": "Towards Adaptive IMFs -- Generalization of utility functions in Multi-Agent Frameworks",
    "authors": [
      "Kaushik Dey",
      "Satheesh K. Perepu",
      "Abir Das",
      "Pallab Dasgupta"
    ],
    "abstract": "Intent Management Function (IMF) is an integral part of future-generation\nnetworks. In recent years, there has been some work on AI-based IMFs that can\nhandle conflicting intents and prioritize the global objective based on apriori\ndefinition of the utility function and accorded priorities for competing\nintents. Some of the earlier works use Multi-Agent Reinforcement Learning\n(MARL) techniques with AdHoc Teaming (AHT) approaches for efficient conflict\nhandling in IMF. However, the success of such frameworks in real-life scenarios\nrequires them to be flexible to business situations. The intent priorities can\nchange and the utility function, which measures the extent of intent\nfulfilment, may also vary in definition. This paper proposes a novel mechanism\nwhereby the IMF can generalize to different forms of utility functions and\nchange of intent priorities at run-time without additional training. Such\ngeneralization ability, without additional training requirements, would help to\ndeploy IMF in live networks where customer intents and priorities change\nfrequently. Results on the network emulator demonstrate the efficacy of the\napproach, scalability for new intents, outperforming existing techniques that\nrequire additional training to achieve the same degree of flexibility thereby\nsaving cost, and increasing efficiency and adaptability.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted in Netsoft-2024 conference",
    "pdf_url": "http://arxiv.org/pdf/2405.07621v2",
    "published_date": "2024-05-13 10:27:11 UTC",
    "updated_date": "2024-05-14 06:29:36 UTC"
  },
  {
    "arxiv_id": "2405.07609v2",
    "title": "NoiseBench: Benchmarking the Impact of Real Label Noise on Named Entity Recognition",
    "authors": [
      "Elena Merdjanovska",
      "Ansar Aynetdinov",
      "Alan Akbik"
    ],
    "abstract": "Available training data for named entity recognition (NER) often contains a\nsignificant percentage of incorrect labels for entity types and entity\nboundaries. Such label noise poses challenges for supervised learning and may\nsignificantly deteriorate model quality. To address this, prior work proposed\nvarious noise-robust learning approaches capable of learning from data with\npartially incorrect labels. These approaches are typically evaluated using\nsimulated noise where the labels in a clean dataset are automatically\ncorrupted. However, as we show in this paper, this leads to unrealistic noise\nthat is far easier to handle than real noise caused by human error or\nsemi-automatic annotation. To enable the study of the impact of various types\nof real noise, we introduce NoiseBench, an NER benchmark consisting of clean\ntraining data corrupted with 6 types of real noise, including expert errors,\ncrowdsourcing errors, automatic annotation errors and LLM errors. We present an\nanalysis that shows that real noise is significantly more challenging than\nsimulated noise, and show that current state-of-the-art models for noise-robust\nlearning fall far short of their theoretically achievable upper bound. We\nrelease NoiseBench to the research community.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "data available at https://github.com/elenamer/NoiseBench; to appear\n  at EMNLP2024 main conference",
    "pdf_url": "http://arxiv.org/pdf/2405.07609v2",
    "published_date": "2024-05-13 10:20:31 UTC",
    "updated_date": "2024-10-14 10:19:37 UTC"
  },
  {
    "arxiv_id": "2405.07603v1",
    "title": "Reducing Risk for Assistive Reinforcement Learning Policies with Diffusion Models",
    "authors": [
      "Andrii Tytarenko"
    ],
    "abstract": "Care-giving and assistive robotics, driven by advancements in AI, offer\npromising solutions to meet the growing demand for care, particularly in the\ncontext of increasing numbers of individuals requiring assistance. This creates\na pressing need for efficient and safe assistive devices, particularly in light\nof heightened demand due to war-related injuries. While cost has been a barrier\nto accessibility, technological progress is able to democratize these\nsolutions. Safety remains a paramount concern, especially given the intricate\ninteractions between assistive robots and humans. This study explores the\napplication of reinforcement learning (RL) and imitation learning, in improving\npolicy design for assistive robots. The proposed approach makes the risky\npolicies safer without additional environmental interactions. Through\nexperimentation using simulated environments, the enhancement of the\nconventional RL approaches in tasks related to assistive robotics is\ndemonstrated.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.07603v1",
    "published_date": "2024-05-13 10:07:36 UTC",
    "updated_date": "2024-05-13 10:07:36 UTC"
  },
  {
    "arxiv_id": "2405.07601v2",
    "title": "On-device Online Learning and Semantic Management of TinyML Systems",
    "authors": [
      "Haoyu Ren",
      "Xue Li",
      "Darko Anicic",
      "Thomas A. Runkler"
    ],
    "abstract": "Recent advances in Tiny Machine Learning (TinyML) empower low-footprint\nembedded devices for real-time on-device Machine Learning. While many\nacknowledge the potential benefits of TinyML, its practical implementation\npresents unique challenges. This study aims to bridge the gap between\nprototyping single TinyML models and developing reliable TinyML systems in\nproduction: (1) Embedded devices operate in dynamically changing conditions.\nExisting TinyML solutions primarily focus on inference, with models trained\noffline on powerful machines and deployed as static objects. However, static\nmodels may underperform in the real world due to evolving input data\ndistributions. We propose online learning to enable training on constrained\ndevices, adapting local models towards the latest field conditions. (2)\nNevertheless, current on-device learning methods struggle with heterogeneous\ndeployment conditions and the scarcity of labeled data when applied across\nnumerous devices. We introduce federated meta-learning incorporating online\nlearning to enhance model generalization, facilitating rapid learning. This\napproach ensures optimal performance among distributed devices by knowledge\nsharing. (3) Moreover, TinyML's pivotal advantage is widespread adoption.\nEmbedded devices and TinyML models prioritize extreme efficiency, leading to\ndiverse characteristics ranging from memory and sensors to model architectures.\nGiven their diversity and non-standardized representations, managing these\nresources becomes challenging as TinyML systems scale up. We present semantic\nmanagement for the joint management of models and devices at scale. We\ndemonstrate our methods through a basic regression example and then assess them\nin three real-world TinyML applications: handwritten character image\nclassification, keyword audio classification, and smart building presence\ndetection, confirming our approaches' effectiveness.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DB",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by Journal Transactions on Embedded Computing Systems (TECS)",
    "pdf_url": "http://arxiv.org/pdf/2405.07601v2",
    "published_date": "2024-05-13 10:03:34 UTC",
    "updated_date": "2024-05-15 20:09:26 UTC"
  },
  {
    "arxiv_id": "2405.07595v1",
    "title": "Environmental Matching Attack Against Unmanned Aerial Vehicles Object Detection",
    "authors": [
      "Dehong Kong",
      "Siyuan Liang",
      "Wenqi Ren"
    ],
    "abstract": "Object detection techniques for Unmanned Aerial Vehicles (UAVs) rely on Deep\nNeural Networks (DNNs), which are vulnerable to adversarial attacks.\nNonetheless, adversarial patches generated by existing algorithms in the UAV\ndomain pay very little attention to the naturalness of adversarial patches.\nMoreover, imposing constraints directly on adversarial patches makes it\ndifficult to generate patches that appear natural to the human eye while\nensuring a high attack success rate. We notice that patches are natural looking\nwhen their overall color is consistent with the environment. Therefore, we\npropose a new method named Environmental Matching Attack(EMA) to address the\nissue of optimizing the adversarial patch under the constraints of color. To\nthe best of our knowledge, this paper is the first to consider natural patches\nin the domain of UAVs. The EMA method exploits strong prior knowledge of a\npretrained stable diffusion to guide the optimization direction of the\nadversarial patch, where the text guidance can restrict the color of the patch.\nTo better match the environment, the contrast and brightness of the patch are\nappropriately adjusted. Instead of optimizing the adversarial patch itself, we\noptimize an adversarial perturbation patch which initializes to zero so that\nthe model can better trade off attacking performance and naturalness.\nExperiments conducted on the DroneVehicle and Carpk datasets have shown that\nour work can reach nearly the same attack performance in the digital attack(no\ngreater than 2 in mAP$\\%$), surpass the baseline method in the physical\nspecific scenarios, and exhibit a significant advantage in terms of naturalness\nin visualization and color difference with the environment.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.07595v1",
    "published_date": "2024-05-13 09:56:57 UTC",
    "updated_date": "2024-05-13 09:56:57 UTC"
  },
  {
    "arxiv_id": "2405.07590v1",
    "title": "Evaluating the Explainable AI Method Grad-CAM for Breath Classification on Newborn Time Series Data",
    "authors": [
      "Camelia Oprea",
      "Mike Grüne",
      "Mateusz Buglowski",
      "Lena Olivier",
      "Thorsten Orlikowsky",
      "Stefan Kowalewski",
      "Mark Schoberer",
      "André Stollenwerk"
    ],
    "abstract": "With the digitalization of health care systems, artificial intelligence\nbecomes more present in medicine. Especially machine learning shows great\npotential for complex tasks such as time series classification, usually at the\ncost of transparency and comprehensibility. This leads to a lack of trust by\nhumans and thus hinders its active usage. Explainable artificial intelligence\ntries to close this gap by providing insight into the decision-making process,\nthe actual usefulness of its different methods is however unclear. This paper\nproposes a user study based evaluation of the explanation method Grad-CAM with\napplication to a neural network for the classification of breaths in time\nseries neonatal ventilation data. We present the perceived usefulness of the\nexplainability method by different stakeholders, exposing the difficulty to\nachieve actual transparency and the wish for more in-depth explanations by many\nof the participants.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LG",
      "I.2.1; K.4.m; J.3"
    ],
    "primary_category": "cs.AI",
    "comment": "\\c{opyright} 2024 The authors. This work has been accepted to IFAC\n  for publication under a Creative Commons Licence CC-BY-NC-ND. Accepted for\n  the 12th IFAC Symposium on Biological and Medical Systems. 6 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.07590v1",
    "published_date": "2024-05-13 09:53:25 UTC",
    "updated_date": "2024-05-13 09:53:25 UTC"
  },
  {
    "arxiv_id": "2405.08041v1",
    "title": "DeepFMEA -- A Scalable Framework Harmonizing Process Expertise and Data-Driven PHM",
    "authors": [
      "Christoph Netsch",
      "Till Schöpe",
      "Benedikt Schindele",
      "Joyam Jayakumar"
    ],
    "abstract": "Machine Learning (ML) based prognostics and health monitoring (PHM) tools\nprovide new opportunities for manufacturers to operate and maintain their\nequipment in a risk-optimized manner and utilize it more sustainably along its\nlifecycle. Yet, in most industrial settings, data is often limited in quantity,\nand its quality can be inconsistent - both critical for developing and\noperating reliable ML models. To bridge this gap in practice, successfully\nindustrialized PHM tools rely on the introduction of domain expertise as a\nprior, to enable sufficiently accurate predictions, while enhancing their\ninterpretability.\n  Thus, a key challenge while developing data-driven PHM tools involves\ntranslating the experience and process knowledge of maintenance personnel,\ndevelopment, and service engineers into a data structure. This structure must\nnot only capture the diversity and variability of the expertise but also render\nthis knowledge accessible for various data-driven algorithms. This results in\ndata models that are heavily tailored towards a specific application and the\nfailure modes the development team aims to detect or predict. The lack of a\nstandardized approach limits developments' extensibility to new failure modes,\ntheir transferability to new applications, and it inhibits the utilization of\nstandard data management and MLOps tools, increasing the burden on the\ndevelopment team.\n  DeepFMEA draws inspiration from the Failure Mode and Effects Analysis (FMEA)\nin its structured approach to the analysis of any technical system and the\nresulting standardized data model, while considering aspects that are crucial\nto capturing process and maintenance expertise in a way that is both intuitive\nto domain experts and the resulting information can be introduced as priors to\nML algorithms.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.08041v1",
    "published_date": "2024-05-13 09:41:34 UTC",
    "updated_date": "2024-05-13 09:41:34 UTC"
  },
  {
    "arxiv_id": "2405.07580v1",
    "title": "DynLLM: When Large Language Models Meet Dynamic Graph Recommendation",
    "authors": [
      "Ziwei Zhao",
      "Fake Lin",
      "Xi Zhu",
      "Zhi Zheng",
      "Tong Xu",
      "Shitian Shen",
      "Xueying Li",
      "Zikai Yin",
      "Enhong Chen"
    ],
    "abstract": "Last year has witnessed the considerable interest of Large Language Models\n(LLMs) for their potential applications in recommender systems, which may\nmitigate the persistent issue of data sparsity. Though large efforts have been\nmade for user-item graph augmentation with better graph-based recommendation\nperformance, they may fail to deal with the dynamic graph recommendation task,\nwhich involves both structural and temporal graph dynamics with inherent\ncomplexity in processing time-evolving data. To bridge this gap, in this paper,\nwe propose a novel framework, called DynLLM, to deal with the dynamic graph\nrecommendation task with LLMs. Specifically, DynLLM harnesses the power of LLMs\nto generate multi-faceted user profiles based on the rich textual features of\nhistorical purchase records, including crowd segments, personal interests,\npreferred categories, and favored brands, which in turn supplement and enrich\nthe underlying relationships between users and items. Along this line, to fuse\nthe multi-faceted profiles with temporal graph embedding, we engage LLMs to\nderive corresponding profile embeddings, and further employ a distilled\nattention mechanism to refine the LLM-generated profile embeddings for\nalleviating noisy signals, while also assessing and adjusting the relevance of\neach distilled facet embedding for seamless integration with temporal graph\nembedding from continuous time dynamic graphs (CTDGs). Extensive experiments on\ntwo real e-commerce datasets have validated the superior improvements of DynLLM\nover a wide range of state-of-the-art baseline methods.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "11 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.07580v1",
    "published_date": "2024-05-13 09:36:17 UTC",
    "updated_date": "2024-05-13 09:36:17 UTC"
  },
  {
    "arxiv_id": "2405.13008v1",
    "title": "Control Token with Dense Passage Retrieval",
    "authors": [
      "Juhwan Lee",
      "Jisu Kim"
    ],
    "abstract": "This study addresses the hallucination problem in large language models\n(LLMs). We adopted Retrieval-Augmented Generation(RAG) (Lewis et al., 2020), a\ntechnique that involves embedding relevant information in the prompt to obtain\naccurate answers. However, RAG also faced inherent issues in retrieving correct\ninformation. To address this, we employed the Dense Passage Retrieval(DPR)\n(Karpukhin et al., 2020) model for fetching domain-specific documents related\nto user queries. Despite this, the DPR model still lacked accuracy in document\nretrieval. We enhanced the DPR model by incorporating control tokens, achieving\nsignificantly superior performance over the standard DPR model, with a 13%\nimprovement in Top-1 accuracy and a 4% improvement in Top-20 accuracy.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.13008v1",
    "published_date": "2024-05-13 09:17:19 UTC",
    "updated_date": "2024-05-13 09:17:19 UTC"
  },
  {
    "arxiv_id": "2405.13007v1",
    "title": "News Recommendation with Category Description by a Large Language Model",
    "authors": [
      "Yuki Yada",
      "Hayato Yamana"
    ],
    "abstract": "Personalized news recommendations are essential for online news platforms to\nassist users in discovering news articles that match their interests from a\nvast amount of online content. Appropriately encoded content features, such as\ntext, categories, and images, are essential for recommendations. Among these\nfeatures, news categories, such as tv-golden-globe, finance-real-estate, and\nnews-politics, play an important role in understanding news content, inspiring\nus to enhance the categories' descriptions. In this paper, we propose a novel\nmethod that automatically generates informative category descriptions using a\nlarge language model (LLM) without manual effort or domain-specific knowledge\nand incorporates them into recommendation models as additional information. In\nour comprehensive experimental evaluations using the MIND dataset, our method\nsuccessfully achieved 5.8% improvement at most in AUC compared with baseline\napproaches without the LLM's generated category descriptions for the\nstate-of-the-art content-based recommendation models including NAML, NRMS, and\nNPA. These results validate the effectiveness of our approach. The code is\navailable at https://github.com/yamanalab/gpt-augmented-news-recommendation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "5 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.13007v1",
    "published_date": "2024-05-13 08:53:43 UTC",
    "updated_date": "2024-05-13 08:53:43 UTC"
  },
  {
    "arxiv_id": "2405.07562v1",
    "title": "GLiRA: Black-Box Membership Inference Attack via Knowledge Distillation",
    "authors": [
      "Andrey V. Galichin",
      "Mikhail Pautov",
      "Alexey Zhavoronkin",
      "Oleg Y. Rogov",
      "Ivan Oseledets"
    ],
    "abstract": "While Deep Neural Networks (DNNs) have demonstrated remarkable performance in\ntasks related to perception and control, there are still several unresolved\nconcerns regarding the privacy of their training data, particularly in the\ncontext of vulnerability to Membership Inference Attacks (MIAs). In this paper,\nwe explore a connection between the susceptibility to membership inference\nattacks and the vulnerability to distillation-based functionality stealing\nattacks. In particular, we propose {GLiRA}, a distillation-guided approach to\nmembership inference attack on the black-box neural network. We observe that\nthe knowledge distillation significantly improves the efficiency of likelihood\nratio of membership inference attack, especially in the black-box setting,\ni.e., when the architecture of the target model is unknown to the attacker. We\nevaluate the proposed method across multiple image classification datasets and\nmodels and demonstrate that likelihood ratio attacks when guided by the\nknowledge distillation, outperform the current state-of-the-art membership\ninference attacks in the black-box setting.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.07562v1",
    "published_date": "2024-05-13 08:52:04 UTC",
    "updated_date": "2024-05-13 08:52:04 UTC"
  },
  {
    "arxiv_id": "2405.07551v1",
    "title": "MuMath-Code: Combining Tool-Use Large Language Models with Multi-perspective Data Augmentation for Mathematical Reasoning",
    "authors": [
      "Shuo Yin",
      "Weihao You",
      "Zhilong Ji",
      "Guoqiang Zhong",
      "Jinfeng Bai"
    ],
    "abstract": "The tool-use Large Language Models (LLMs) that integrate with external Python\ninterpreters have significantly enhanced mathematical reasoning capabilities\nfor open-source LLMs, while tool-free methods chose another track: augmenting\nmath reasoning data. However, a great method to integrate the above two\nresearch paths and combine their advantages remains to be explored. In this\nwork, we firstly include new math questions via multi-perspective data\naugmenting methods and then synthesize code-nested solutions to them. The open\nLLMs (i.e., Llama-2) are finetuned on the augmented dataset to get the\nresulting models, MuMath-Code ($\\mu$-Math-Code). During the inference phase,\nour MuMath-Code generates code and interacts with the external python\ninterpreter to get the execution results. Therefore, MuMath-Code leverages the\nadvantages of both the external tool and data augmentation. To fully leverage\nthe advantages of our augmented data, we propose a two-stage training strategy:\nIn Stage-1, we finetune Llama-2 on pure CoT data to get an intermediate model,\nwhich then is trained on the code-nested data in Stage-2 to get the resulting\nMuMath-Code. Our MuMath-Code-7B achieves 83.8 on GSM8K and 52.4 on MATH, while\nMuMath-Code-70B model achieves new state-of-the-art performance among open\nmethods -- achieving 90.7% on GSM8K and 55.1% on MATH. Extensive experiments\nvalidate the combination of tool use and data augmentation, as well as our\ntwo-stage training strategy. We release the proposed dataset along with the\nassociated code for public use.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "The state-of-the-art open-source tool-use LLMs for mathematical\n  reasoning",
    "pdf_url": "http://arxiv.org/pdf/2405.07551v1",
    "published_date": "2024-05-13 08:32:19 UTC",
    "updated_date": "2024-05-13 08:32:19 UTC"
  },
  {
    "arxiv_id": "2405.07541v4",
    "title": "Walk model that continuously generates Brownian walks to Lévy walks depending on destination attractiveness",
    "authors": [
      "Shuji Shinohara",
      "Daiki Morita",
      "Hayato Hirai",
      "Ryosuke Kuribayashi",
      "Nobuhito Manome",
      "Toru Moriyama",
      "Hiroshi Okamoto",
      "Yoshihiro Nakajima",
      "Yukio-Pegio Gunji",
      "Ung-il Chung"
    ],
    "abstract": "The L\\'evy walk, a type of random walk characterized by linear step lengths\nthat follow a power-law distribution, is observed in the migratory behaviors of\nvarious organisms, ranging from bacteria to humans. Notably, L\\'evy walks with\npower exponents close to two, also known as Cauchy walks, are frequently\nobserved, though their underlying causes remain elusive. This study proposes a\nwalk model in which agents move toward a destination in multi-dimensional space\nand their movement strategy is parameterized by the extent to which they pursue\nthe shortest path to the destination. This parameter is taken to represent the\nattractiveness of the destination to the agents. Our findings reveal that if\nthe destination is very attractive, agents intensively search the area around\nit using Brownian walks, whereas if the destination is unattractive, they\nexplore a distant region away from the point using L\\'evy walks with power\nexponents less than two. In the case where agents are unable to determine\nwhether the destination is attractive or unattractive, Cauchy walks emerge. The\nCauchy walker searches the region with a probability inversely proportional to\nthe distance from the destination. This suggests that it preferentially\nsearches the area close to the destination, while concurrently having the\npotential to extend the search area much further. Our model, which can change\nthe search method and search area depending on the attractiveness of the\ndestination, has the potential to be utilized for exploring the parameter space\nof optimization problems.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.07541v4",
    "published_date": "2024-05-13 08:22:44 UTC",
    "updated_date": "2024-09-12 01:31:02 UTC"
  },
  {
    "arxiv_id": "2405.07527v1",
    "title": "Train Faster, Perform Better: Modular Adaptive Training in Over-Parameterized Models",
    "authors": [
      "Yubin Shi",
      "Yixuan Chen",
      "Mingzhi Dong",
      "Xiaochen Yang",
      "Dongsheng Li",
      "Yujiang Wang",
      "Robert P. Dick",
      "Qin Lv",
      "Yingying Zhao",
      "Fan Yang",
      "Tun Lu",
      "Ning Gu",
      "Li Shang"
    ],
    "abstract": "Despite their prevalence in deep-learning communities, over-parameterized\nmodels convey high demands of computational costs for proper training. This\nwork studies the fine-grained, modular-level learning dynamics of\nover-parameterized models to attain a more efficient and fruitful training\nstrategy. Empirical evidence reveals that when scaling down into network\nmodules, such as heads in self-attention models, we can observe varying\nlearning patterns implicitly associated with each module's trainability. To\ndescribe such modular-level learning capabilities, we introduce a novel concept\ndubbed modular neural tangent kernel (mNTK), and we demonstrate that the\nquality of a module's learning is tightly associated with its mNTK's principal\neigenvalue $\\lambda_{\\max}$. A large $\\lambda_{\\max}$ indicates that the module\nlearns features with better convergence, while those miniature ones may impact\ngeneralization negatively. Inspired by the discovery, we propose a novel\ntraining strategy termed Modular Adaptive Training (MAT) to update those\nmodules with their $\\lambda_{\\max}$ exceeding a dynamic threshold selectively,\nconcentrating the model on learning common features and ignoring those\ninconsistent ones. Unlike most existing training schemes with a complete BP\ncycle across all network modules, MAT can significantly save computations by\nits partially-updating strategy and can further improve performance.\nExperiments show that MAT nearly halves the computational cost of model\ntraining and outperforms the accuracy of baselines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at NeurIPS 2023",
    "pdf_url": "http://arxiv.org/pdf/2405.07527v1",
    "published_date": "2024-05-13 07:46:48 UTC",
    "updated_date": "2024-05-13 07:46:48 UTC"
  },
  {
    "arxiv_id": "2405.07523v1",
    "title": "Adaptation of Distinct Semantics for Uncertain Areas in Polyp Segmentation",
    "authors": [
      "Quang Vinh Nguyen",
      "Van Thong Huynh",
      "Soo-Hyung Kim"
    ],
    "abstract": "Colonoscopy is a common and practical method for detecting and treating\npolyps. Segmenting polyps from colonoscopy image is useful for diagnosis and\nsurgery progress. Nevertheless, achieving excellent segmentation performance is\nstill difficult because of polyp characteristics like shape, color, condition,\nand obvious non-distinction from the surrounding context. This work presents a\nnew novel architecture namely Adaptation of Distinct Semantics for Uncertain\nAreas in Polyp Segmentation (ADSNet), which modifies misclassified details and\nrecovers weak features having the ability to vanish and not be detected at the\nfinal stage. The architecture consists of a complementary trilateral decoder to\nproduce an early global map. A continuous attention module modifies semantics\nof high-level features to analyze two separate semantics of the early global\nmap. The suggested method is experienced on polyp benchmarks in learning\nability and generalization ability, experimental results demonstrate the great\ncorrection and recovery ability leading to better segmentation performance\ncompared to the other state of the art in the polyp image segmentation task.\nEspecially, the proposed architecture could be experimented flexibly for other\nCNN-based encoders, Transformer-based encoders, and decoder backbones.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.5.4, I.2.1, I.4.6, J.3"
    ],
    "primary_category": "cs.CV",
    "comment": "13 pages with 7 figures, British Machine Vision Conference 2023",
    "pdf_url": "http://arxiv.org/pdf/2405.07523v1",
    "published_date": "2024-05-13 07:41:28 UTC",
    "updated_date": "2024-05-13 07:41:28 UTC"
  },
  {
    "arxiv_id": "2405.07518v2",
    "title": "SambaNova SN40L: Scaling the AI Memory Wall with Dataflow and Composition of Experts",
    "authors": [
      "Raghu Prabhakar",
      "Ram Sivaramakrishnan",
      "Darshan Gandhi",
      "Yun Du",
      "Mingran Wang",
      "Xiangyu Song",
      "Kejie Zhang",
      "Tianren Gao",
      "Angela Wang",
      "Karen Li",
      "Yongning Sheng",
      "Joshua Brot",
      "Denis Sokolov",
      "Apurv Vivek",
      "Calvin Leung",
      "Arjun Sabnis",
      "Jiayu Bai",
      "Tuowen Zhao",
      "Mark Gottscho",
      "David Jackson",
      "Mark Luttrell",
      "Manish K. Shah",
      "Edison Chen",
      "Kaizhao Liang",
      "Swayambhoo Jain",
      "Urmish Thakker",
      "Dawei Huang",
      "Sumti Jairath",
      "Kevin J. Brown",
      "Kunle Olukotun"
    ],
    "abstract": "Monolithic large language models (LLMs) like GPT-4 have paved the way for\nmodern generative AI applications. Training, serving, and maintaining\nmonolithic LLMs at scale, however, remains prohibitively expensive and\nchallenging. The disproportionate increase in compute-to-memory ratio of modern\nAI accelerators have created a memory wall, necessitating new methods to deploy\nAI. Composition of Experts (CoE) is an alternative modular approach that lowers\nthe cost and complexity of training and serving. However, this approach\npresents two key challenges when using conventional hardware: (1) without fused\noperations, smaller models have lower operational intensity, which makes high\nutilization more challenging to achieve; and (2) hosting a large number of\nmodels can be either prohibitively expensive or slow when dynamically switching\nbetween them.\n  In this paper, we describe how combining CoE, streaming dataflow, and a\nthree-tier memory system scales the AI memory wall. We describe Samba-CoE, a\nCoE system with 150 experts and a trillion total parameters. We deploy\nSamba-CoE on the SambaNova SN40L Reconfigurable Dataflow Unit (RDU) - a\ncommercial dataflow accelerator architecture that has been co-designed for\nenterprise inference and training applications. The chip introduces a new\nthree-tier memory system with on-chip distributed SRAM, on-package HBM, and\noff-package DDR DRAM. A dedicated inter-RDU network enables scaling up and out\nover multiple sockets. We demonstrate speedups ranging from 2$\\times$ to\n13$\\times$ on various benchmarks running on eight RDU sockets compared with an\nunfused baseline. We show that for CoE inference deployments, the 8-socket RDU\nNode reduces machine footprint by up to 19$\\times$, speeds up model switching\ntime by 15$\\times$ to 31$\\times$, and achieves an overall speedup of\n3.7$\\times$ over a DGX H100 and 6.6$\\times$ over a DGX A100.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "C.1.3; C.0"
    ],
    "primary_category": "cs.AR",
    "comment": "2024 57th IEEE/ACM International Symposium on Microarchitecture\n  (MICRO)",
    "pdf_url": "http://arxiv.org/pdf/2405.07518v2",
    "published_date": "2024-05-13 07:32:45 UTC",
    "updated_date": "2024-11-05 02:53:00 UTC"
  },
  {
    "arxiv_id": "2405.07515v1",
    "title": "OpenBot-Fleet: A System for Collective Learning with Real Robots",
    "authors": [
      "Matthias Müller",
      "Samarth Brahmbhatt",
      "Ankur Deka",
      "Quentin Leboutet",
      "David Hafner",
      "Vladlen Koltun"
    ],
    "abstract": "We introduce OpenBot-Fleet, a comprehensive open-source cloud robotics system\nfor navigation. OpenBot-Fleet uses smartphones for sensing, local compute and\ncommunication, Google Firebase for secure cloud storage and off-board compute,\nand a robust yet low-cost wheeled robot toact in real-world environments. The\nrobots collect task data and upload it to the cloud where navigation policies\ncan be learned either offline or online and can then be sent back to the robot\nfleet. In our experiments we distribute 72 robots to a crowd of workers who\noperate them in homes, and show that OpenBot-Fleet can learn robust navigation\npolicies that generalize to unseen homes with >80% success rate. OpenBot-Fleet\nrepresents a significant step forward in cloud robotics, making it possible to\ndeploy large continually learning robot fleets in a cost-effective and scalable\nmanner. All materials can be found at https://www.openbot.org. A video is\navailable at https://youtu.be/wiv2oaDgDi8",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted at ICRA'24",
    "pdf_url": "http://arxiv.org/pdf/2405.07515v1",
    "published_date": "2024-05-13 07:22:50 UTC",
    "updated_date": "2024-05-13 07:22:50 UTC"
  },
  {
    "arxiv_id": "2405.07509v1",
    "title": "RESTAD: REconstruction and Similarity based Transformer for time series Anomaly Detection",
    "authors": [
      "Ramin Ghorbani",
      "Marcel J. T. Reinders",
      "David M. J. Tax"
    ],
    "abstract": "Anomaly detection in time series data is crucial across various domains. The\nscarcity of labeled data for such tasks has increased the attention towards\nunsupervised learning methods. These approaches, often relying solely on\nreconstruction error, typically fail to detect subtle anomalies in complex\ndatasets. To address this, we introduce RESTAD, an adaptation of the\nTransformer model by incorporating a layer of Radial Basis Function (RBF)\nneurons within its architecture. This layer fits a non-parametric density in\nthe latent representation, such that a high RBF output indicates similarity\nwith predominantly normal training data. RESTAD integrates the RBF similarity\nscores with the reconstruction errors to increase sensitivity to anomalies. Our\nempirical evaluations demonstrate that RESTAD outperforms various established\nbaselines across multiple benchmark datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Manuscript under review",
    "pdf_url": "http://arxiv.org/pdf/2405.07509v1",
    "published_date": "2024-05-13 07:10:35 UTC",
    "updated_date": "2024-05-13 07:10:35 UTC"
  },
  {
    "arxiv_id": "2405.08038v1",
    "title": "Feature Expansion and enhanced Compression for Class Incremental Learning",
    "authors": [
      "Quentin Ferdinand",
      "Gilles Le Chenadec",
      "Benoit Clement",
      "Panagiotis Papadakis",
      "Quentin Oliveau"
    ],
    "abstract": "Class incremental learning consists in training discriminative models to\nclassify an increasing number of classes over time. However, doing so using\nonly the newly added class data leads to the known problem of catastrophic\nforgetting of the previous classes. Recently, dynamic deep learning\narchitectures have been shown to exhibit a better stability-plasticity\ntrade-off by dynamically adding new feature extractors to the model in order to\nlearn new classes followed by a compression step to scale the model back to its\noriginal size, thus avoiding a growing number of parameters. In this context,\nwe propose a new algorithm that enhances the compression of previous class\nknowledge by cutting and mixing patches of previous class samples with the new\nimages during compression using our Rehearsal-CutMix method. We show that this\nnew data augmentation reduces catastrophic forgetting by specifically targeting\npast class information and improving its compression. Extensive experiments\nperformed on the CIFAR and ImageNet datasets under diverse incremental learning\nevaluation protocols demonstrate that our approach consistently outperforms the\nstate-of-the-art . The code will be made available upon publication of our\nwork.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.08038v1",
    "published_date": "2024-05-13 06:57:18 UTC",
    "updated_date": "2024-05-13 06:57:18 UTC"
  },
  {
    "arxiv_id": "2405.07503v2",
    "title": "Consistency Policy: Accelerated Visuomotor Policies via Consistency Distillation",
    "authors": [
      "Aaditya Prasad",
      "Kevin Lin",
      "Jimmy Wu",
      "Linqi Zhou",
      "Jeannette Bohg"
    ],
    "abstract": "Many robotic systems, such as mobile manipulators or quadrotors, cannot be\nequipped with high-end GPUs due to space, weight, and power constraints. These\nconstraints prevent these systems from leveraging recent developments in\nvisuomotor policy architectures that require high-end GPUs to achieve fast\npolicy inference. In this paper, we propose Consistency Policy, a faster and\nsimilarly powerful alternative to Diffusion Policy for learning visuomotor\nrobot control. By virtue of its fast inference speed, Consistency Policy can\nenable low latency decision making in resource-constrained robotic setups. A\nConsistency Policy is distilled from a pretrained Diffusion Policy by enforcing\nself-consistency along the Diffusion Policy's learned trajectories. We compare\nConsistency Policy with Diffusion Policy and other related speed-up methods\nacross 6 simulation tasks as well as three real-world tasks where we\ndemonstrate inference on a laptop GPU. For all these tasks, Consistency Policy\nspeeds up inference by an order of magnitude compared to the fastest\nalternative method and maintains competitive success rates. We also show that\nthe Conistency Policy training procedure is robust to the pretrained Diffusion\nPolicy's quality, a useful result that helps practioners avoid extensive\ntesting of the pretrained model. Key design decisions that enabled this\nperformance are the choice of consistency objective, reduced initial sample\nvariance, and the choice of preset chaining steps.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "https://consistency-policy.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2405.07503v2",
    "published_date": "2024-05-13 06:53:42 UTC",
    "updated_date": "2024-06-28 21:56:25 UTC"
  },
  {
    "arxiv_id": "2405.07500v1",
    "title": "PromptLink: Leveraging Large Language Models for Cross-Source Biomedical Concept Linking",
    "authors": [
      "Yuzhang Xie",
      "Jiaying Lu",
      "Joyce Ho",
      "Fadi Nahab",
      "Xiao Hu",
      "Carl Yang"
    ],
    "abstract": "Linking (aligning) biomedical concepts across diverse data sources enables\nvarious integrative analyses, but it is challenging due to the discrepancies in\nconcept naming conventions. Various strategies have been developed to overcome\nthis challenge, such as those based on string-matching rules, manually crafted\nthesauri, and machine learning models. However, these methods are constrained\nby limited prior biomedical knowledge and can hardly generalize beyond the\nlimited amounts of rules, thesauri, or training samples. Recently, large\nlanguage models (LLMs) have exhibited impressive results in diverse biomedical\nNLP tasks due to their unprecedentedly rich prior knowledge and strong\nzero-shot prediction abilities. However, LLMs suffer from issues including high\ncosts, limited context length, and unreliable predictions. In this research, we\npropose PromptLink, a novel biomedical concept linking framework that leverages\nLLMs. It first employs a biomedical-specialized pre-trained language model to\ngenerate candidate concepts that can fit in the LLM context windows. Then it\nutilizes an LLM to link concepts through two-stage prompts, where the\nfirst-stage prompt aims to elicit the biomedical prior knowledge from the LLM\nfor the concept linking task and the second-stage prompt enforces the LLM to\nreflect on its own predictions to further enhance their reliability. Empirical\nresults on the concept linking task between two EHR datasets and an external\nbiomedical KG demonstrate the effectiveness of PromptLink. Furthermore,\nPromptLink is a generic framework without reliance on additional prior\nknowledge, context, or training data, making it well-suited for concept linking\nacross various types of data sources. The source code is available at\nhttps://github.com/constantjxyz/PromptLink.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.07500v1",
    "published_date": "2024-05-13 06:36:30 UTC",
    "updated_date": "2024-05-13 06:36:30 UTC"
  },
  {
    "arxiv_id": "2405.07495v1",
    "title": "MacBehaviour: An R package for behavioural experimentation on large language models",
    "authors": [
      "Xufeng Duan",
      "Shixuan Li",
      "Zhenguang G. Cai1"
    ],
    "abstract": "There has been increasing interest in investigating the behaviours of large\nlanguage models (LLMs) and LLM-powered chatbots by treating an LLM as a\nparticipant in a psychological experiment. We therefore developed an R package\ncalled \"MacBehaviour\" that aims to interact with more than 60 language models\nin one package (e.g., OpenAI's GPT family, the Claude family, Gemini, Llama\nfamily, and open-source models) and streamline the experimental process of LLMs\nbehaviour experiments. The package offers a comprehensive set of functions\ndesigned for LLM experiments, covering experiment design, stimuli presentation,\nmodel behaviour manipulation, logging response and token probability. To\ndemonstrate the utility and effectiveness of \"MacBehaviour,\" we conducted three\nvalidation experiments on three LLMs (GPT-3.5, Llama-2 7B, and Vicuna-1.5 13B)\nto replicate sound-gender association in LLMs. The results consistently showed\nthat they exhibit human-like tendencies to infer gender from novel personal\nnames based on their phonology, as previously demonstrated (Cai et al., 2023).\nIn summary, \"MacBehaviour\" is an R package for machine behaviour studies which\noffers a user-friendly interface and comprehensive features to simplify and\nstandardize the experimental process.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "11 pages",
    "pdf_url": "http://arxiv.org/pdf/2405.07495v1",
    "published_date": "2024-05-13 06:31:48 UTC",
    "updated_date": "2024-05-13 06:31:48 UTC"
  },
  {
    "arxiv_id": "2405.08037v1",
    "title": "Layout Generation Agents with Large Language Models",
    "authors": [
      "Yuichi Sasazawa",
      "Yasuhiro Sogawa"
    ],
    "abstract": "In recent years, there has been an increasing demand for customizable 3D\nvirtual spaces. Due to the significant human effort required to create these\nvirtual spaces, there is a need for efficiency in virtual space creation. While\nexisting studies have proposed methods for automatically generating layouts\nsuch as floor plans and furniture arrangements, these methods only generate\ntext indicating the layout structure based on user instructions, without\nutilizing the information obtained during the generation process. In this\nstudy, we propose an agent-driven layout generation system using the GPT-4V\nmultimodal large language model and validate its effectiveness. Specifically,\nthe language model manipulates agents to sequentially place objects in the\nvirtual space, thus generating layouts that reflect user instructions.\nExperimental results confirm that our proposed method can generate virtual\nspaces reflecting user instructions with a high success rate. Additionally, we\nsuccessfully identified elements contributing to the improvement in behavior\ngeneration performance through ablation study.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.08037v1",
    "published_date": "2024-05-13 06:27:23 UTC",
    "updated_date": "2024-05-13 06:27:23 UTC"
  },
  {
    "arxiv_id": "2405.07490v1",
    "title": "Strategic Data Ordering: Enhancing Large Language Model Performance through Curriculum Learning",
    "authors": [
      "Jisu Kim",
      "Juhwan Lee"
    ],
    "abstract": "The rapid advancement of Large Language Models (LLMs) has improved text\nunderstanding and generation but poses challenges in computational resources.\nThis study proposes a curriculum learning-inspired, data-centric training\nstrategy that begins with simpler tasks and progresses to more complex ones,\nusing criteria such as prompt length, attention scores, and loss values to\nstructure the training data. Experiments with Mistral-7B (Jiang et al., 2023)\nand Gemma-7B (Team et al., 2024) models demonstrate that curriculum learning\nslightly improves performance compared to traditional random data shuffling.\nNotably, we observed that sorting data based on our proposed attention criteria\ngenerally led to better performance. This approach offers a sustainable method\nto enhance LLM performance without increasing model size or dataset volume,\naddressing scalability challenges in LLM training.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.07490v1",
    "published_date": "2024-05-13 06:09:10 UTC",
    "updated_date": "2024-05-13 06:09:10 UTC"
  },
  {
    "arxiv_id": "2405.07474v2",
    "title": "Integrating Intent Understanding and Optimal Behavior Planning for Behavior Tree Generation from Human Instructions",
    "authors": [
      "Xinglin Chen",
      "Yishuai Cai",
      "Yunxin Mao",
      "Minglong Li",
      "Wenjing Yang",
      "Weixia Xu",
      "Ji Wang"
    ],
    "abstract": "Robots executing tasks following human instructions in domestic or industrial\nenvironments essentially require both adaptability and reliability. Behavior\nTree (BT) emerges as an appropriate control architecture for these scenarios\ndue to its modularity and reactivity. Existing BT generation methods, however,\neither do not involve interpreting natural language or cannot theoretically\nguarantee the BTs' success. This paper proposes a two-stage framework for BT\ngeneration, which first employs large language models (LLMs) to interpret goals\nfrom high-level instructions, then constructs an efficient goal-specific BT\nthrough the Optimal Behavior Tree Expansion Algorithm (OBTEA). We represent\ngoals as well-formed formulas in first-order logic, effectively bridging intent\nunderstanding and optimal behavior planning. Experiments in the service robot\nvalidate the proficiency of LLMs in producing grammatically correct and\naccurately interpreted goals, demonstrate OBTEA's superiority over the baseline\nBT Expansion algorithm in various metrics, and finally confirm the practical\ndeployability of our framework. The project website is\nhttps://dids-ei.github.io/Project/LLM-OBTEA/.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.07474v2",
    "published_date": "2024-05-13 05:23:48 UTC",
    "updated_date": "2024-06-27 13:17:58 UTC"
  },
  {
    "arxiv_id": "2405.07468v1",
    "title": "Evaluating large language models in medical applications: a survey",
    "authors": [
      "Xiaolan Chen",
      "Jiayang Xiang",
      "Shanfu Lu",
      "Yexin Liu",
      "Mingguang He",
      "Danli Shi"
    ],
    "abstract": "Large language models (LLMs) have emerged as powerful tools with\ntransformative potential across numerous domains, including healthcare and\nmedicine. In the medical domain, LLMs hold promise for tasks ranging from\nclinical decision support to patient education. However, evaluating the\nperformance of LLMs in medical contexts presents unique challenges due to the\ncomplex and critical nature of medical information. This paper provides a\ncomprehensive overview of the landscape of medical LLM evaluation, synthesizing\ninsights from existing studies and highlighting evaluation data sources, task\nscenarios, and evaluation methods. Additionally, it identifies key challenges\nand opportunities in medical LLM evaluation, emphasizing the need for continued\nresearch and innovation to ensure the responsible integration of LLMs into\nclinical practice.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "4 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2405.07468v1",
    "published_date": "2024-05-13 05:08:33 UTC",
    "updated_date": "2024-05-13 05:08:33 UTC"
  },
  {
    "arxiv_id": "2405.07460v4",
    "title": "HoneyBee: A Scalable Modular Framework for Creating Multimodal Oncology Datasets with Foundational Embedding Models",
    "authors": [
      "Aakash Tripathi",
      "Asim Waqas",
      "Matthew B. Schabath",
      "Yasin Yilmaz",
      "Ghulam Rasool"
    ],
    "abstract": "Developing accurate machine learning models for oncology requires\nlarge-scale, high-quality multimodal datasets. However, creating such datasets\nremains challenging due to the complexity and heterogeneity of medical data. To\naddress this challenge, we introduce HoneyBee, a scalable modular framework for\nbuilding multimodal oncology datasets that leverages foundation models to\ngenerate representative embeddings. HoneyBee integrates various data\nmodalities, including clinical diagnostic and pathology imaging data, medical\nnotes, reports, records, and molecular data. It employs data preprocessing\ntechniques and foundation models to generate embeddings that capture the\nessential features and relationships within the raw medical data. The generated\nembeddings are stored in a structured format using Hugging Face datasets and\nPyTorch dataloaders for accessibility. Vector databases enable efficient\nquerying and retrieval for machine learning applications. We demonstrate the\neffectiveness of HoneyBee through experiments assessing the quality and\nrepresentativeness of these embeddings. The framework is designed to be\nextensible to other medical domains and aims to accelerate oncology research by\nproviding high-quality, machine learning-ready datasets. HoneyBee is an ongoing\nopen-source effort, and the code, datasets, and models are available at the\nproject repository.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.07460v4",
    "published_date": "2024-05-13 04:35:14 UTC",
    "updated_date": "2024-11-21 16:12:54 UTC"
  },
  {
    "arxiv_id": "2407.04709v1",
    "title": "Efficient 4D Radar Data Auto-labeling Method using LiDAR-based Object Detection Network",
    "authors": [
      "Min-Hyeok Sun",
      "Dong-Hee Paek",
      "Seung-Hyun Song",
      "Seung-Hyun Kong"
    ],
    "abstract": "Focusing on the strength of 4D (4-Dimensional) radar, research about robust\n3D object detection networks in adverse weather conditions has gained\nattention. To train such networks, datasets that contain large amounts of 4D\nradar data and ground truth labels are essential. However, the existing 4D\nradar datasets (e.g., K-Radar) lack sufficient sensor data and labels, which\nhinders the advancement in this research domain. Furthermore, enlarging the 4D\nradar datasets requires a time-consuming and expensive manual labeling process.\nTo address these issues, we propose the auto-labeling method of 4D radar tensor\n(4DRT) in the K-Radar dataset. The proposed method initially trains a\nLiDAR-based object detection network (LODN) using calibrated LiDAR point cloud\n(LPC). The trained LODN then automatically generates ground truth labels (i.e.,\nauto-labels, ALs) of the K-Radar train dataset without human intervention. The\ngenerated ALs are used to train the 4D radar-based object detection network\n(4DRODN), Radar Tensor Network with Height (RTNH). The experimental results\ndemonstrate that RTNH trained with ALs has achieved a similar detection\nperformance to the original RTNH which is trained with manually annotated\nground truth labels, thereby verifying the effectiveness of the proposed\nauto-labeling method. All relevant codes will be soon available at the\nfollowing GitHub project: https://github.com/kaist-avelab/K-Radar",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.SP",
    "comment": "Accept at IEEE IVS 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.04709v1",
    "published_date": "2024-05-13 04:28:06 UTC",
    "updated_date": "2024-05-13 04:28:06 UTC"
  },
  {
    "arxiv_id": "2405.09572v1",
    "title": "Deep Neural Operator Enabled Digital Twin Modeling for Additive Manufacturing",
    "authors": [
      "Ning Liu",
      "Xuxiao Li",
      "Manoj R. Rajanna",
      "Edward W. Reutzel",
      "Brady Sawyer",
      "Prahalada Rao",
      "Jim Lua",
      "Nam Phan",
      "Yue Yu"
    ],
    "abstract": "A digital twin (DT), with the components of a physics-based model, a\ndata-driven model, and a machine learning (ML) enabled efficient surrogate,\nbehaves as a virtual twin of the real-world physical process. In terms of Laser\nPowder Bed Fusion (L-PBF) based additive manufacturing (AM), a DT can predict\nthe current and future states of the melt pool and the resulting defects\ncorresponding to the input laser parameters, evolve itself by assimilating\nin-situ sensor data, and optimize the laser parameters to mitigate defect\nformation. In this paper, we present a deep neural operator enabled\ncomputational framework of the DT for closed-loop feedback control of the L-PBF\nprocess. This is accomplished by building a high-fidelity computational model\nto accurately represent the melt pool states, an efficient surrogate model to\napproximate the melt pool solution field, followed by an physics-based\nprocedure to extract information from the computed melt pool simulation that\ncan further be correlated to the defect quantities of interest (e.g., surface\nroughness). In particular, we leverage the data generated from the\nhigh-fidelity physics-based model and train a series of Fourier neural operator\n(FNO) based ML models to effectively learn the relation between the input laser\nparameters and the corresponding full temperature field of the melt pool.\nSubsequently, a set of physics-informed variables such as the melt pool\ndimensions and the peak temperature can be extracted to compute the resulting\ndefects. An optimization algorithm is then exercised to control laser input and\nminimize defects. On the other hand, the constructed DT can also evolve with\nthe physical twin via offline finetuning and online material calibration.\nFinally, a probabilistic framework is adopted for uncertainty quantification.\nThe developed DT is envisioned to guide the AM process and facilitate\nhigh-quality manufacturing.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.09572v1",
    "published_date": "2024-05-13 03:53:46 UTC",
    "updated_date": "2024-05-13 03:53:46 UTC"
  },
  {
    "arxiv_id": "2405.10976v1",
    "title": "On Constructing Algorithm Portfolios in Algorithm Selection for Computationally Expensive Black-box Optimization in the Fixed-budget Setting",
    "authors": [
      "Takushi Yoshikawa",
      "Ryoji Tanabe"
    ],
    "abstract": "Feature-based offline algorithm selection has shown its effectiveness in a\nwide range of optimization problems, including the black-box optimization\nproblem. An algorithm selection system selects the most promising optimizer\nfrom an algorithm portfolio, which is a set of pre-defined optimizers. Thus,\nalgorithm selection requires a well-constructed algorithm portfolio consisting\nof efficient optimizers complementary to each other. Although construction\nmethods for the fixed-target setting have been well studied, those for the\nfixed-budget setting have received less attention. Here, the fixed-budget\nsetting is generally used for computationally expensive optimization, where a\nbudget of function evaluations is small. In this context, first, this paper\npoints out some undesirable properties of experimental setups in previous\nstudies. Then, this paper argues the importance of considering the number of\nfunction evaluations used in the sampling phase when constructing algorithm\nportfolios, whereas the previous studies ignored that. The results show that\nalgorithm portfolios constructed by our approach perform significantly better\nthan those by the previous approach.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted for GECCO 2024 Workshop Industrial Applications of\n  Metaheuristics",
    "pdf_url": "http://arxiv.org/pdf/2405.10976v1",
    "published_date": "2024-05-13 03:31:13 UTC",
    "updated_date": "2024-05-13 03:31:13 UTC"
  },
  {
    "arxiv_id": "2405.13006v1",
    "title": "Auto FAQ Generation",
    "authors": [
      "Anjaneya Teja Kalvakolanu",
      "NagaSai Chandra",
      "Michael Fekadu"
    ],
    "abstract": "FAQ documents are commonly used with text documents and websites to provide\nimportant information in the form of question answer pairs to either aid in\nreading comprehension or provide a shortcut to the key ideas. We suppose that\nsalient sentences from a given document serve as a good proxy fro the answers\nto an aggregated set of FAQs from readers. We propose a system for generating\nFAQ documents that extract the salient questions and their corresponding\nanswers from sizeable text documents scraped from the Stanford Encyclopedia of\nPhilosophy. We use existing text summarization, sentence ranking via the Text\nrank algorithm, and question-generation tools to create an initial set of\nquestions and answers. Finally, we apply some heuristics to filter out invalid\nquestions. We use human evaluation to rate the generated questions on grammar,\nwhether the question is meaningful, and whether the question's answerability is\npresent within a summarized context. On average, participants thought 71\npercent of the questions were meaningful.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "3 figures and peer evaluated",
    "pdf_url": "http://arxiv.org/pdf/2405.13006v1",
    "published_date": "2024-05-13 03:30:27 UTC",
    "updated_date": "2024-05-13 03:30:27 UTC"
  },
  {
    "arxiv_id": "2405.08036v5",
    "title": "POWQMIX: Weighted Value Factorization with Potentially Optimal Joint Actions Recognition for Cooperative Multi-Agent Reinforcement Learning",
    "authors": [
      "Chang Huang",
      "Shatong Zhu",
      "Junqiao Zhao",
      "Hongtu Zhou",
      "Chen Ye",
      "Tiantian Feng",
      "Changjun Jiang"
    ],
    "abstract": "Value function factorization methods are commonly used in cooperative\nmulti-agent reinforcement learning, with QMIX receiving significant attention.\nMany QMIX-based methods introduce monotonicity constraints between the joint\naction value and individual action values to achieve decentralized execution.\nHowever, such constraints limit the representation capacity of value\nfactorization, restricting the joint action values it can represent and\nhindering the learning of the optimal policy. To address this challenge, we\npropose the Potentially Optimal Joint Actions Weighted QMIX (POWQMIX)\nalgorithm, which recognizes the potentially optimal joint actions and assigns\nhigher weights to the corresponding losses of these joint actions during\ntraining. We theoretically prove that with such a weighted training approach\nthe optimal policy is guaranteed to be recovered. Experiments in matrix games,\ndifficulty-enhanced predator-prey, and StarCraft II Multi-Agent Challenge\nenvironments demonstrate that our algorithm outperforms the state-of-the-art\nvalue-based multi-agent reinforcement learning methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper needs further refinement",
    "pdf_url": "http://arxiv.org/pdf/2405.08036v5",
    "published_date": "2024-05-13 03:27:35 UTC",
    "updated_date": "2025-04-10 01:21:52 UTC"
  },
  {
    "arxiv_id": "2405.17440v1",
    "title": "CataLM: Empowering Catalyst Design Through Large Language Models",
    "authors": [
      "Ludi Wang",
      "Xueqing Chen",
      "Yi Du",
      "Yuanchun Zhou",
      "Yang Gao",
      "Wenjuan Cui"
    ],
    "abstract": "The field of catalysis holds paramount importance in shaping the trajectory\nof sustainable development, prompting intensive research efforts to leverage\nartificial intelligence (AI) in catalyst design. Presently, the fine-tuning of\nopen-source large language models (LLMs) has yielded significant breakthroughs\nacross various domains such as biology and healthcare. Drawing inspiration from\nthese advancements, we introduce CataLM Cata}lytic Language Model), a large\nlanguage model tailored to the domain of electrocatalytic materials. Our\nfindings demonstrate that CataLM exhibits remarkable potential for facilitating\nhuman-AI collaboration in catalyst knowledge exploration and design. To the\nbest of our knowledge, CataLM stands as the pioneering LLM dedicated to the\ncatalyst domain, offering novel avenues for catalyst discovery and development.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.17440v1",
    "published_date": "2024-05-13 03:19:47 UTC",
    "updated_date": "2024-05-13 03:19:47 UTC"
  },
  {
    "arxiv_id": "2405.08035v1",
    "title": "A LLM-based Controllable, Scalable, Human-Involved User Simulator Framework for Conversational Recommender Systems",
    "authors": [
      "Lixi Zhu",
      "Xiaowen Huang",
      "Jitao Sang"
    ],
    "abstract": "Conversational Recommender System (CRS) leverages real-time feedback from\nusers to dynamically model their preferences, thereby enhancing the system's\nability to provide personalized recommendations and improving the overall user\nexperience. CRS has demonstrated significant promise, prompting researchers to\nconcentrate their efforts on developing user simulators that are both more\nrealistic and trustworthy. The emergence of Large Language Models (LLMs) has\nmarked the onset of a new epoch in computational capabilities, exhibiting\nhuman-level intelligence in various tasks. Research efforts have been made to\nutilize LLMs for building user simulators to evaluate the performance of CRS.\nAlthough these efforts showcase innovation, they are accompanied by certain\nlimitations. In this work, we introduce a Controllable, Scalable, and\nHuman-Involved (CSHI) simulator framework that manages the behavior of user\nsimulators across various stages via a plugin manager. CSHI customizes the\nsimulation of user behavior and interactions to provide a more lifelike and\nconvincing user interaction experience. Through experiments and case studies in\ntwo conversational recommendation scenarios, we show that our framework can\nadapt to a variety of conversational recommendation settings and effectively\nsimulate users' personalized preferences. Consequently, our simulator is able\nto generate feedback that closely mirrors that of real users. This facilitates\na reliable assessment of existing CRS studies and promotes the creation of\nhigh-quality conversational recommendation datasets.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.08035v1",
    "published_date": "2024-05-13 03:02:56 UTC",
    "updated_date": "2024-05-13 03:02:56 UTC"
  },
  {
    "arxiv_id": "2405.07442v2",
    "title": "Rene: A Pre-trained Multi-modal Architecture for Auscultation of Respiratory Diseases",
    "authors": [
      "Pengfei Zhang",
      "Zhihang Zheng",
      "Shichen Zhang",
      "Minghao Yang",
      "Shaojun Tang"
    ],
    "abstract": "Compared with invasive examinations that require tissue sampling, respiratory\nsound testing is a non-invasive examination method that is safer and easier for\npatients to accept. In this study, we introduce Rene, a pioneering large-scale\nmodel tailored for respiratory sound recognition. Rene has been rigorously\nfine-tuned with an extensive dataset featuring a broad array of respiratory\naudio samples, targeting disease detection, sound pattern classification, and\nevent identification. Our innovative approach applies a pre-trained speech\nrecognition model to process respiratory sounds, augmented with patient medical\nrecords. The resulting multi-modal deep-learning framework addresses\ninterpretability and real-time diagnostic challenges that have hindered\nprevious respiratory-focused models. Benchmark comparisons reveal that Rene\nsignificantly outperforms existing models, achieving improvements of 10.27%,\n16.15%, 15.29%, and 18.90% in respiratory event detection and audio\nclassification on the SPRSound database. Disease prediction accuracy on the\nICBHI database improved by 23% over the baseline in both mean average and\nharmonic scores. Moreover, we have developed a real-time respiratory sound\ndiscrimination system utilizing the Rene architecture. Employing\nstate-of-the-art Edge AI technology, this system enables rapid and accurate\nresponses for respiratory sound\nauscultation(https://github.com/zpforlove/Rene).",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS",
      "q-bio.QM"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.07442v2",
    "published_date": "2024-05-13 03:00:28 UTC",
    "updated_date": "2024-06-07 00:01:23 UTC"
  },
  {
    "arxiv_id": "2405.07437v2",
    "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
    "authors": [
      "Hao Yu",
      "Aoran Gan",
      "Kai Zhang",
      "Shiwei Tong",
      "Qi Liu",
      "Zhaofeng Liu"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) has recently gained traction in natural\nlanguage processing. Numerous studies and real-world applications are\nleveraging its ability to enhance generative models through external\ninformation retrieval. Evaluating these RAG systems, however, poses unique\nchallenges due to their hybrid structure and reliance on dynamic knowledge\nsources. To better understand these challenges, we conduct A Unified Evaluation\nProcess of RAG (Auepora) and aim to provide a comprehensive overview of the\nevaluation and benchmarks of RAG systems. Specifically, we examine and compare\nseveral quantifiable metrics of the Retrieval and Generation components, such\nas relevance, accuracy, and faithfulness, within the current RAG benchmarks,\nencompassing the possible output and ground truth pairs. We then analyze the\nvarious datasets and metrics, discuss the limitations of current benchmarks,\nand suggest potential directions to advance the field of RAG benchmarks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.07437v2",
    "published_date": "2024-05-13 02:33:25 UTC",
    "updated_date": "2024-07-03 04:59:32 UTC"
  },
  {
    "arxiv_id": "2405.07436v1",
    "title": "Can Language Models Explain Their Own Classification Behavior?",
    "authors": [
      "Dane Sherburn",
      "Bilal Chughtai",
      "Owain Evans"
    ],
    "abstract": "Large language models (LLMs) perform well at a myriad of tasks, but\nexplaining the processes behind this performance is a challenge. This paper\ninvestigates whether LLMs can give faithful high-level explanations of their\nown internal processes. To explore this, we introduce a dataset,\nArticulateRules, of few-shot text-based classification tasks generated by\nsimple rules. Each rule is associated with a simple natural-language\nexplanation. We test whether models that have learned to classify inputs\ncompetently (both in- and out-of-distribution) are able to articulate freeform\nnatural language explanations that match their classification behavior. Our\ndataset can be used for both in-context and finetuning evaluations. We evaluate\na range of LLMs, demonstrating that articulation accuracy varies considerably\nbetween models, with a particularly sharp increase from GPT-3 to GPT-4. We then\ninvestigate whether we can improve GPT-3's articulation accuracy through a\nrange of methods. GPT-3 completely fails to articulate 7/10 rules in our test,\neven after additional finetuning on correct explanations. We release our\ndataset, ArticulateRules, which can be used to test self-explanation for LLMs\ntrained either in-context or by finetuning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.07436v1",
    "published_date": "2024-05-13 02:31:08 UTC",
    "updated_date": "2024-05-13 02:31:08 UTC"
  },
  {
    "arxiv_id": "2405.07414v2",
    "title": "Binning as a Pretext Task: Improving Self-Supervised Learning in Tabular Domains",
    "authors": [
      "Kyungeun Lee",
      "Ye Seul Sim",
      "Hye-Seung Cho",
      "Moonjung Eo",
      "Suhee Yoon",
      "Sanghyu Yoon",
      "Woohyung Lim"
    ],
    "abstract": "The ability of deep networks to learn superior representations hinges on\nleveraging the proper inductive biases, considering the inherent properties of\ndatasets. In tabular domains, it is critical to effectively handle\nheterogeneous features (both categorical and numerical) in a unified manner and\nto grasp irregular functions like piecewise constant functions. To address the\nchallenges in the self-supervised learning framework, we propose a novel\npretext task based on the classical binning method. The idea is\nstraightforward: reconstructing the bin indices (either orders or classes)\nrather than the original values. This pretext task provides the encoder with an\ninductive bias to capture the irregular dependencies, mapping from continuous\ninputs to discretized bins, and mitigates the feature heterogeneity by setting\nall features to have category-type targets. Our empirical investigations\nascertain several advantages of binning: capturing the irregular function,\ncompatibility with encoder architecture and additional modifications,\nstandardizing all features into equal sets, grouping similar values within a\nfeature, and providing ordering information. Comprehensive evaluations across\ndiverse tabular datasets corroborate that our method consistently improves\ntabular representation learning performance for a wide range of downstream\ntasks. The codes are available in\nhttps://github.com/kyungeun-lee/tabularbinning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML 2024, 18 pages (including supplementary materials)",
    "pdf_url": "http://arxiv.org/pdf/2405.07414v2",
    "published_date": "2024-05-13 01:23:14 UTC",
    "updated_date": "2024-05-14 01:29:37 UTC"
  },
  {
    "arxiv_id": "2405.07411v1",
    "title": "MoVL:Exploring Fusion Strategies for the Domain-Adaptive Application of Pretrained Models in Medical Imaging Tasks",
    "authors": [
      "Haijiang Tian",
      "Jingkun Yue",
      "Xiaohong Liu",
      "Guoxing Yang",
      "Zeyu Jiang",
      "Guangyu Wang"
    ],
    "abstract": "Medical images are often more difficult to acquire than natural images due to\nthe specialism of the equipment and technology, which leads to less medical\nimage datasets. So it is hard to train a strong pretrained medical vision\nmodel. How to make the best of natural pretrained vision model and adapt in\nmedical domain still pends. For image classification, a popular method is\nlinear probe (LP). However, LP only considers the output after feature\nextraction. Yet, there exists a gap between input medical images and natural\npretrained vision model. We introduce visual prompting (VP) to fill in the gap,\nand analyze the strategies of coupling between LP and VP. We design a joint\nlearning loss function containing categorisation loss and discrepancy loss,\nwhich describe the variance of prompted and plain images, naming this joint\ntraining strategy MoVL (Mixture of Visual Prompting and Linear Probe). We\nexperiment on 4 medical image classification datasets, with two mainstream\narchitectures, ResNet and CLIP. Results shows that without changing the\nparameters and architecture of backbone model and with less parameters, there\nis potential for MoVL to achieve full finetune (FF) accuracy (on four medical\ndatasets, average 90.91% for MoVL and 91.13% for FF). On out of distribution\nmedical dataset, our method(90.33%) can outperform FF (85.15%) with absolute\n5.18 % lead.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.07411v1",
    "published_date": "2024-05-13 01:18:25 UTC",
    "updated_date": "2024-05-13 01:18:25 UTC"
  },
  {
    "arxiv_id": "2405.07407v1",
    "title": "PitcherNet: Powering the Moneyball Evolution in Baseball Video Analytics",
    "authors": [
      "Jerrin Bright",
      "Bavesh Balaji",
      "Yuhao Chen",
      "David A Clausi",
      "John S Zelek"
    ],
    "abstract": "In the high-stakes world of baseball, every nuance of a pitcher's mechanics\nholds the key to maximizing performance and minimizing runs. Traditional\nanalysis methods often rely on pre-recorded offline numerical data, hindering\ntheir application in the dynamic environment of live games. Broadcast video\nanalysis, while seemingly ideal, faces significant challenges due to factors\nlike motion blur and low resolution. To address these challenges, we introduce\nPitcherNet, an end-to-end automated system that analyzes pitcher kinematics\ndirectly from live broadcast video, thereby extracting valuable pitch\nstatistics including velocity, release point, pitch position, and release\nextension. This system leverages three key components: (1) Player tracking and\nidentification by decoupling actions from player kinematics; (2) Distribution\nand depth-aware 3D human modeling; and (3) Kinematic-driven pitch statistics.\nExperimental validation demonstrates that PitcherNet achieves robust analysis\nresults with 96.82% accuracy in pitcher tracklet identification, reduced joint\nposition error by 1.8mm and superior analytics compared to baseline methods. By\nenabling performance-critical kinematic analysis from broadcast video,\nPitcherNet paves the way for the future of baseball analytics by optimizing\npitching strategies, preventing injuries, and unlocking a deeper understanding\nof pitcher mechanics, forever transforming the game.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  Workshops (CVPRW'24)",
    "pdf_url": "http://arxiv.org/pdf/2405.07407v1",
    "published_date": "2024-05-13 01:03:06 UTC",
    "updated_date": "2024-05-13 01:03:06 UTC"
  },
  {
    "arxiv_id": "2405.07406v2",
    "title": "Machine Unlearning: A Comprehensive Survey",
    "authors": [
      "Weiqi Wang",
      "Zhiyi Tian",
      "Chenhan Zhang",
      "Shui Yu"
    ],
    "abstract": "As the right to be forgotten has been legislated worldwide, many studies\nattempt to design unlearning mechanisms to protect users' privacy when they\nwant to leave machine learning service platforms. Specifically, machine\nunlearning is to make a trained model to remove the contribution of an erased\nsubset of the training dataset. This survey aims to systematically classify a\nwide range of machine unlearning and discuss their differences, connections and\nopen problems. We categorize current unlearning methods into four scenarios:\ncentralized unlearning, distributed and irregular data unlearning, unlearning\nverification, and privacy and security issues in unlearning. Since centralized\nunlearning is the primary domain, we use two parts to introduce: firstly, we\nclassify centralized unlearning into exact unlearning and approximate\nunlearning; secondly, we offer a detailed introduction to the techniques of\nthese methods. Besides the centralized unlearning, we notice some studies about\ndistributed and irregular data unlearning and introduce federated unlearning\nand graph unlearning as the two representative directions. After introducing\nunlearning methods, we review studies about unlearning verification. Moreover,\nwe consider the privacy and security issues essential in machine unlearning and\norganize the latest related literature. Finally, we discuss the challenges of\nvarious unlearning scenarios and address the potential research directions.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.07406v2",
    "published_date": "2024-05-13 00:58:34 UTC",
    "updated_date": "2024-07-25 01:03:11 UTC"
  },
  {
    "arxiv_id": "2405.07404v1",
    "title": "Indoor PM2.5 forecasting and the association with outdoor air pollution: a modelling study based on sensor data in Australia",
    "authors": [
      "Wenhua Yu",
      "Bahareh Nakisa",
      "Seng W. Loke",
      "Svetlana Stevanovic",
      "Yuming Guo",
      "Mohammad Naim Rastgoo"
    ],
    "abstract": "Exposure to poor indoor air quality poses significant health risks,\nnecessitating thorough assessment to mitigate associated dangers. This study\naims to predict hourly indoor fine particulate matter (PM2.5) concentrations\nand investigate their correlation with outdoor PM2.5 levels across 24 distinct\nbuildings in Australia. Indoor air quality data were gathered from 91\nmonitoring sensors in eight Australian cities spanning 2019 to 2022. Employing\nan innovative three-stage deep ensemble machine learning framework (DEML),\ncomprising three base models (Support Vector Machine, Random Forest, and\neXtreme Gradient Boosting) and two meta-models (Random Forest and Generalized\nLinear Model), hourly indoor PM2.5 concentrations were predicted. The model's\naccuracy was evaluated using a rolling windows approach, comparing its\nperformance against three benchmark algorithms (SVM, RF, and XGBoost).\nAdditionally, a correlation analysis assessed the relationship between indoor\nand outdoor PM2.5 concentrations. Results indicate that the DEML model\nconsistently outperformed benchmark models, achieving an R2 ranging from 0.63\nto 0.99 and RMSE from 0.01 to 0.663 mg/m3 for most sensors. Notably, outdoor\nPM2.5 concentrations significantly impacted indoor air quality, particularly\nevident during events like bushfires. This study underscores the importance of\naccurate indoor air quality prediction, crucial for developing\nlocation-specific early warning systems and informing effective interventions.\nBy promoting protective behaviors, these efforts contribute to enhanced public\nhealth outcomes.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.07404v1",
    "published_date": "2024-05-13 00:51:36 UTC",
    "updated_date": "2024-05-13 00:51:36 UTC"
  }
]
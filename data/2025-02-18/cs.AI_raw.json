[
  {
    "arxiv_id": "2502.13345v1",
    "title": "Secure and Efficient Watermarking for Latent Diffusion Models in Model Distribution Scenarios",
    "authors": [
      "Liangqi Lei",
      "Keke Gai",
      "Jing Yu",
      "Liehuang Zhu",
      "Qi Wu"
    ],
    "abstract": "Latent diffusion models have exhibited considerable potential in generative\ntasks. Watermarking is considered to be an alternative to safeguard the\ncopyright of generative models and prevent their misuse. However, in the\ncontext of model distribution scenarios, the accessibility of models to large\nscale of model users brings new challenges to the security, efficiency and\nrobustness of existing watermark solutions. To address these issues, we propose\na secure and efficient watermarking solution. A new security mechanism is\ndesigned to prevent watermark leakage and watermark escape, which considers\nwatermark randomness and watermark-model association as two constraints for\nmandatory watermark injection. To reduce the time cost of training the security\nmodule, watermark injection and the security mechanism are decoupled, ensuring\nthat fine-tuning VAE only accomplishes the security mechanism without the\nburden of learning watermark patterns. A watermark distribution-based\nverification strategy is proposed to enhance the robustness against diverse\nattacks in the model distribution scenarios. Experimental results prove that\nour watermarking consistently outperforms existing six baselines on\neffectiveness and robustness against ten image processing attacks and\nadversarial attacks, while enhancing security in the distribution scenarios.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.13345v1",
    "published_date": "2025-02-18 23:55:33 UTC",
    "updated_date": "2025-02-18 23:55:33 UTC"
  },
  {
    "arxiv_id": "2502.13339v1",
    "title": "How Expressive are Knowledge Graph Foundation Models?",
    "authors": [
      "Xingyue Huang",
      "Pablo Barceló",
      "Michael M. Bronstein",
      "İsmail İlkan Ceylan",
      "Mikhail Galkin",
      "Juan L Reutter",
      "Miguel Romero Orth"
    ],
    "abstract": "Knowledge Graph Foundation Models (KGFMs) are at the frontier for deep\nlearning on knowledge graphs (KGs), as they can generalize to completely novel\nknowledge graphs with different relational vocabularies. Despite their\nempirical success, our theoretical understanding of KGFMs remains very limited.\nIn this paper, we conduct a rigorous study of the expressive power of KGFMs.\nSpecifically, we show that the expressive power of KGFMs directly depends on\nthe motifs that are used to learn the relation representations. We then observe\nthat the most typical motifs used in the existing literature are binary, as the\nrepresentations are learned based on how pairs of relations interact, which\nlimits the model's expressiveness. As part of our study, we design more\nexpressive KGFMs using richer motifs, which necessitate learning relation\nrepresentations based on, e.g., how triples of relations interact with each\nother. Finally, we empirically validate our theoretical findings, showing that\nthe use of richer motifs results in better performance on a wide range of\ndatasets drawn from different domains.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.13339v1",
    "published_date": "2025-02-18 23:38:39 UTC",
    "updated_date": "2025-02-18 23:38:39 UTC"
  },
  {
    "arxiv_id": "2502.13337v1",
    "title": "Language Models are Few-Shot Graders",
    "authors": [
      "Chenyan Zhao",
      "Mariana Silva",
      "Seth Poulsen"
    ],
    "abstract": "Providing evaluations to student work is a critical component of effective\nstudent learning, and automating its process can significantly reduce the\nworkload on human graders. Automatic Short Answer Grading (ASAG) systems,\nenabled by advancements in Large Language Models (LLMs), offer a promising\nsolution for assessing and providing instant feedback for open-ended student\nresponses. In this paper, we present an ASAG pipeline leveraging\nstate-of-the-art LLMs. Our new LLM-based ASAG pipeline achieves better\nperformances than existing custom-built models on the same datasets. We also\ncompare the grading performance of three OpenAI models: GPT-4, GPT-4o, and\no1-preview. Our results demonstrate that GPT-4o achieves the best balance\nbetween accuracy and cost-effectiveness. On the other hand, o1-preview, despite\nhigher accuracy, exhibits a larger variance in error that makes it less\npractical for classroom use. We investigate the effects of incorporating\ninstructor-graded examples into prompts using no examples, random selection,\nand Retrieval-Augmented Generation (RAG)-based selection strategies. Our\nfindings indicate that providing graded examples enhances grading accuracy,\nwith RAG-based selection outperforming random selection. Additionally,\nintegrating grading rubrics improves accuracy by offering a structured standard\nfor evaluation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.13337v1",
    "published_date": "2025-02-18 23:38:21 UTC",
    "updated_date": "2025-02-18 23:38:21 UTC"
  },
  {
    "arxiv_id": "2502.13329v1",
    "title": "Language Models Can Predict Their Own Behavior",
    "authors": [
      "Dhananjay Ashok",
      "Jonathan May"
    ],
    "abstract": "Autoregressive Language Models output text by sequentially predicting the\nnext token to generate, with modern methods like Chain-of-Thought (CoT)\nprompting achieving state-of-the-art reasoning capabilities by scaling the\nnumber of generated tokens. However, are there times when we can infer how the\nmodel will behave (e.g. abstain from answering a question) early in the\ncomputation, making generation unnecessary? We show that internal\nrepresentation of input tokens alone can often precisely predict, not just the\nnext token, but eventual behavior over the entire output sequence. We leverage\nthis capacity and learn probes on internal states to create early warning (and\nexit) systems. Specifically, if the probes can confidently estimate the way the\nLM is going to behave, then the system will avoid generating tokens altogether\nand return the estimated behavior instead. On 27 text classification datasets\nspanning five different tasks, we apply this method to estimate the eventual\nanswer of an LM under CoT prompting, reducing inference costs by 65% (average)\nwhile suffering an accuracy loss of no more than 1.4% (worst case). We\ndemonstrate the potential of this method to pre-emptively identify when a model\nwill abstain from answering a question, fail to follow output format\nspecifications, or give a low-confidence response. We explore the limits of\nthis capability, showing that probes generalize to unseen datasets, but perform\nworse when LM outputs are longer and struggle to predict properties that\nrequire access to knowledge that the models themselves lack. Encouragingly,\nperformance scales with model size, suggesting applicability to the largest of\nmodels",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.13329v1",
    "published_date": "2025-02-18 23:13:16 UTC",
    "updated_date": "2025-02-18 23:13:16 UTC"
  },
  {
    "arxiv_id": "2502.13321v1",
    "title": "Adjust for Trust: Mitigating Trust-Induced Inappropriate Reliance on AI Assistance",
    "authors": [
      "Tejas Srinivasan",
      "Jesse Thomason"
    ],
    "abstract": "Trust biases how users rely on AI recommendations in AI-assisted\ndecision-making tasks, with low and high levels of trust resulting in increased\nunder- and over-reliance, respectively. We propose that AI assistants should\nadapt their behavior through trust-adaptive interventions to mitigate such\ninappropriate reliance. For instance, when user trust is low, providing an\nexplanation can elicit more careful consideration of the assistant's advice by\nthe user. In two decision-making scenarios -- laypeople answering science\nquestions and doctors making medical diagnoses -- we find that providing\nsupporting and counter-explanations during moments of low and high trust,\nrespectively, yields up to 38% reduction in inappropriate reliance and 20%\nimprovement in decision accuracy. We are similarly able to reduce over-reliance\nby adaptively inserting forced pauses to promote deliberation. Our results\nhighlight how AI adaptation to user trust facilitates appropriate reliance,\npresenting exciting avenues for improving human-AI collaboration.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.13321v1",
    "published_date": "2025-02-18 22:42:39 UTC",
    "updated_date": "2025-02-18 22:42:39 UTC"
  },
  {
    "arxiv_id": "2502.13313v1",
    "title": "Revisiting Privacy, Utility, and Efficiency Trade-offs when Fine-Tuning Large Language Models",
    "authors": [
      "Soumi Das",
      "Camila Kolling",
      "Mohammad Aflah Khan",
      "Mahsa Amani",
      "Bishwamittra Ghosh",
      "Qinyuan Wu",
      "Till Speicher",
      "Krishna P. Gummadi"
    ],
    "abstract": "We study the inherent trade-offs in minimizing privacy risks and maximizing\nutility, while maintaining high computational efficiency, when fine-tuning\nlarge language models (LLMs). A number of recent works in privacy research have\nattempted to mitigate privacy risks posed by memorizing fine-tuning data by\nusing differentially private training methods (e.g., DP), albeit at a\nsignificantly higher computational cost (inefficiency). In parallel, several\nworks in systems research have focussed on developing (parameter) efficient\nfine-tuning methods (e.g., LoRA), but few works, if any, investigated whether\nsuch efficient methods enhance or diminish privacy risks. In this paper, we\ninvestigate this gap and arrive at a surprising conclusion: efficient\nfine-tuning methods like LoRA mitigate privacy risks similar to private\nfine-tuning methods like DP. Our empirical finding directly contradicts\nprevailing wisdom that privacy and efficiency objectives are at odds during\nfine-tuning. Our finding is established by (a) carefully defining measures of\nprivacy and utility that distinguish between memorizing sensitive and\nnon-sensitive tokens in training and test datasets used in fine-tuning and (b)\nextensive evaluations using multiple open-source language models from Pythia,\nGemma, and Llama families and different domain-specific datasets.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "This is a work in progress. The draft may change in future",
    "pdf_url": "http://arxiv.org/pdf/2502.13313v1",
    "published_date": "2025-02-18 22:16:03 UTC",
    "updated_date": "2025-02-18 22:16:03 UTC"
  },
  {
    "arxiv_id": "2502.13311v2",
    "title": "Training Turn-by-Turn Verifiers for Dialogue Tutoring Agents: The Curious Case of LLMs as Your Coding Tutors",
    "authors": [
      "Jian Wang",
      "Yinpei Dai",
      "Yichi Zhang",
      "Ziqiao Ma",
      "Wenjie Li",
      "Joyce Chai"
    ],
    "abstract": "Intelligent tutoring agents powered by large language models (LLMs) have been\nincreasingly explored to deliver personalized guidance in areas such as\nlanguage learning and science education. However, their capabilities in guiding\nusers to solve complex real-world tasks remain underexplored. To address this\nlimitation, in this work, we focus on coding tutoring, a challenging problem\nthat requires tutors to proactively guide students toward completing predefined\ncoding tasks. We propose a novel agent workflow, Trace-and-Verify (TRAVER),\nwhich combines knowledge tracing to estimate a student's knowledge state and\nturn-by-turn verification to ensure effective guidance toward task completion.\nWe introduce DICT, an automatic evaluation protocol that assesses tutor agents\nholistically using controlled student simulation and code generation tests.\nExtensive experiments reveal the challenges of coding tutoring and demonstrate\nthat TRAVER achieves a significantly higher success rate. Although we use code\ntutoring as an example in this paper, our results and findings can be extended\nbeyond coding, providing valuable insights into advancing tutoring agents for a\nvariety of tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.13311v2",
    "published_date": "2025-02-18 22:13:00 UTC",
    "updated_date": "2025-02-21 17:25:44 UTC"
  },
  {
    "arxiv_id": "2502.13297v1",
    "title": "Understanding and Tackling Label Errors in Individual-Level Nature Language Understanding",
    "authors": [
      "Yunpeng Xiao",
      "Youpeng Zhao",
      "Kai Shu"
    ],
    "abstract": "Natural language understanding (NLU) is a task that enables machines to\nunderstand human language. Some tasks, such as stance detection and sentiment\nanalysis, are closely related to individual subjective perspectives, thus\ntermed individual-level NLU. Previously, these tasks are often simplified to\ntext-level NLU tasks, ignoring individual factors. This not only makes\ninference difficult and unexplainable but often results in a large number of\nlabel errors when creating datasets. To address the above limitations, we\npropose a new NLU annotation guideline based on individual-level factors.\nSpecifically, we incorporate other posts by the same individual and then\nannotate individual subjective perspectives after considering all individual\nposts. We use this guideline to expand and re-annotate the stance detection and\ntopic-based sentiment analysis datasets. We find that error rates in the\nsamples were as high as 31.7\\% and 23.3\\%. We further use large language models\nto conduct experiments on the re-annotation datasets and find that the large\nlanguage models perform well on both datasets after adding individual factors.\nBoth GPT-4o and Llama3-70B can achieve an accuracy greater than 87\\% on the\nre-annotation datasets. We also verify the effectiveness of individual factors\nthrough ablation studies. We call on future researchers to add individual\nfactors when creating such datasets. Our re-annotation dataset can be found at\nhttps://github.com/24yearsoldstudent/Individual-NLU",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages",
    "pdf_url": "http://arxiv.org/pdf/2502.13297v1",
    "published_date": "2025-02-18 21:35:46 UTC",
    "updated_date": "2025-02-18 21:35:46 UTC"
  },
  {
    "arxiv_id": "2502.13295v2",
    "title": "Demonstrating specification gaming in reasoning models",
    "authors": [
      "Alexander Bondarenko",
      "Denis Volk",
      "Dmitrii Volkov",
      "Jeffrey Ladish"
    ],
    "abstract": "We demonstrate LLM agent specification gaming by instructing models to win\nagainst a chess engine. We find reasoning models like OpenAI o3 and DeepSeek R1\nwill often hack the benchmark by default, while language models like GPT-4o and\nClaude 3.5 Sonnet need to be told that normal play won't work to hack.\n  We improve upon prior work like (Hubinger et al., 2024; Meinke et al., 2024;\nWeij et al., 2024) by using realistic task prompts and avoiding excess nudging.\nOur results suggest reasoning models may resort to hacking to solve difficult\nproblems, as observed in OpenAI (2024)'s o1 Docker escape during cyber\ncapabilities testing.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Updated with o3 results",
    "pdf_url": "http://arxiv.org/pdf/2502.13295v2",
    "published_date": "2025-02-18 21:32:24 UTC",
    "updated_date": "2025-05-15 13:42:18 UTC"
  },
  {
    "arxiv_id": "2502.13290v1",
    "title": "Prediction of Clinical Complication Onset using Neural Point Processes",
    "authors": [
      "Sachini Weerasekara",
      "Sagar Kamarthi",
      "Jacqueline Isaacs"
    ],
    "abstract": "Predicting medical events in advance within critical care settings is\nparamount for patient outcomes and resource management. Utilizing predictive\nmodels, healthcare providers can anticipate issues such as cardiac arrest,\nsepsis, or respiratory failure before they manifest. Recently, there has been a\nsurge in research focusing on forecasting adverse medical event onsets prior to\nclinical manifestation using machine learning. However, while these models\nprovide temporal prognostic predictions for the occurrence of a specific\nadverse event of interest within defined time intervals, their interpretability\noften remains a challenge. In this work, we explore the applicability of neural\ntemporal point processes in the context of adverse event onset prediction, with\nthe aim of explaining clinical pathways and providing interpretable insights.\nOur experiments span six state-of-the-art neural point processes and six\ncritical care datasets, each focusing on the onset of distinct adverse events.\nThis work represents a novel application class of neural temporal point\nprocesses in event prediction.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.13290v1",
    "published_date": "2025-02-18 21:18:19 UTC",
    "updated_date": "2025-02-18 21:18:19 UTC"
  },
  {
    "arxiv_id": "2502.13278v1",
    "title": "Performance Evaluation of Sentiment Analysis on Text and Emoji Data Using End-to-End, Transfer Learning, Distributed and Explainable AI Models",
    "authors": [
      "Sirisha Velampalli",
      "Chandrashekar Muniyappa",
      "Ashutosh Saxena"
    ],
    "abstract": "Emojis are being frequently used in todays digital world to express from\nsimple to complex thoughts more than ever before. Hence, they are also being\nused in sentiment analysis and targeted marketing campaigns. In this work, we\nperformed sentiment analysis of Tweets as well as on emoji dataset from the\nKaggle. Since tweets are sentences we have used Universal Sentence Encoder\n(USE) and Sentence Bidirectional Encoder Representations from Transformers\n(SBERT) end-to-end sentence embedding models to generate the embeddings which\nare used to train the Standard fully connected Neural Networks (NN), and LSTM\nNN models. We observe the text classification accuracy was almost the same for\nboth the models around 98 percent. On the contrary, when the validation set was\nbuilt using emojis that were not present in the training set then the accuracy\nof both the models reduced drastically to 70 percent. In addition, the models\nwere also trained using the distributed training approach instead of a\ntraditional singlethreaded model for better scalability. Using the distributed\ntraining approach, we were able to reduce the run-time by roughly 15% without\ncompromising on accuracy. Finally, as part of explainable AI the Shap algorithm\nwas used to explain the model behaviour and check for model biases for the\ngiven feature set.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "68T50",
      "I.2.7; I.2.11"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.13278v1",
    "published_date": "2025-02-18 20:58:37 UTC",
    "updated_date": "2025-02-18 20:58:37 UTC"
  },
  {
    "arxiv_id": "2502.13277v2",
    "title": "HyperGCL: Multi-Modal Graph Contrastive Learning via Learnable Hypergraph Views",
    "authors": [
      "Khaled Mohammed Saifuddin",
      "Shihao Ji",
      "Esra Akbas"
    ],
    "abstract": "Recent advancements in Graph Contrastive Learning (GCL) have demonstrated\nremarkable effectiveness in improving graph representations. However, relying\non predefined augmentations (e.g., node dropping, edge perturbation, attribute\nmasking) may result in the loss of task-relevant information and a lack of\nadaptability to diverse input data. Furthermore, the selection of negative\nsamples remains rarely explored. In this paper, we introduce HyperGCL, a novel\nmultimodal GCL framework from a hypergraph perspective. HyperGCL constructs\nthree distinct hypergraph views by jointly utilizing the input graph's\nstructure and attributes, enabling a comprehensive integration of multiple\nmodalities in contrastive learning. A learnable adaptive topology augmentation\ntechnique enhances these views by preserving important relations and filtering\nout noise. View-specific encoders capture essential characteristics from each\nview, while a network-aware contrastive loss leverages the underlying topology\nto define positive and negative samples effectively. Extensive experiments on\nbenchmark datasets demonstrate that HyperGCL achieves state-of-the-art node\nclassification performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.13277v2",
    "published_date": "2025-02-18 20:57:56 UTC",
    "updated_date": "2025-02-26 13:24:15 UTC"
  },
  {
    "arxiv_id": "2502.15799v1",
    "title": "Investigating the Impact of Quantization Methods on the Safety and Reliability of Large Language Models",
    "authors": [
      "Artyom Kharinaev",
      "Viktor Moskvoretskii",
      "Egor Shvetsov",
      "Kseniia Studenikina",
      "Bykov Mikhail",
      "Evgeny Burnaev"
    ],
    "abstract": "Large Language Models (LLMs) have emerged as powerful tools for addressing\nmodern challenges and enabling practical applications. However, their\ncomputational expense remains a significant barrier to widespread adoption.\nQuantization has emerged as a promising technique to democratize access and\nenable low resource device deployment. Despite these advancements, the safety\nand trustworthiness of quantized models remain underexplored, as prior studies\noften overlook contemporary architectures and rely on overly simplistic\nbenchmarks and evaluations. To address this gap, we introduce OpenSafetyMini, a\nnovel open-ended safety dataset designed to better distinguish between models.\nWe evaluate 4 state-of-the-art quantization techniques across LLaMA and Mistral\nmodels using 4 benchmarks, including human evaluations. Our findings reveal\nthat the optimal quantization method varies for 4-bit precision, while vector\nquantization techniques deliver the best safety and trustworthiness performance\nat 2-bit precision, providing foundation for future research.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.15799v1",
    "published_date": "2025-02-18 20:32:05 UTC",
    "updated_date": "2025-02-18 20:32:05 UTC"
  },
  {
    "arxiv_id": "2502.15798v1",
    "title": "MaxSup: Overcoming Representation Collapse in Label Smoothing",
    "authors": [
      "Yuxuan Zhou",
      "Heng Li",
      "Zhi-Qi Cheng",
      "Xudong Yan",
      "Mario Fritz",
      "Margret Keuper"
    ],
    "abstract": "Label Smoothing (LS) is widely adopted to curb overconfidence in neural\nnetwork predictions and enhance generalization. However, previous research\nshows that LS can force feature representations into excessively tight\nclusters, eroding intra-class distinctions. More recent findings suggest that\nLS also induces overconfidence in misclassifications, yet the precise mechanism\nremained unclear. In this work, we decompose the loss term introduced by LS,\nrevealing two key components: (i) a regularization term that functions only\nwhen the prediction is correct, and (ii) an error-enhancement term that emerges\nunder misclassifications. This latter term compels the model to reinforce\nincorrect predictions with exaggerated certainty, further collapsing the\nfeature space. To address these issues, we propose Max Suppression (MaxSup),\nwhich uniformly applies the intended regularization to both correct and\nincorrect predictions by penalizing the top-1 logit instead of the ground-truth\nlogit. Through feature analyses, we show that MaxSup restores intra-class\nvariation and sharpens inter-class boundaries. Extensive experiments on image\nclassification and downstream tasks confirm that MaxSup is a more robust\nalternative to LS. Code is available at:\nhttps://github.com/ZhouYuxuanYX/Maximum-Suppression-Regularization.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "19 pages, 9 Tables, preliminary work under review do not distribute",
    "pdf_url": "http://arxiv.org/pdf/2502.15798v1",
    "published_date": "2025-02-18 20:10:34 UTC",
    "updated_date": "2025-02-18 20:10:34 UTC"
  },
  {
    "arxiv_id": "2502.13260v1",
    "title": "Stepwise Perplexity-Guided Refinement for Efficient Chain-of-Thought Reasoning in Large Language Models",
    "authors": [
      "Yingqian Cui",
      "Pengfei He",
      "Jingying Zeng",
      "Hui Liu",
      "Xianfeng Tang",
      "Zhenwei Dai",
      "Yan Han",
      "Chen Luo",
      "Jing Huang",
      "Zhen Li",
      "Suhang Wang",
      "Yue Xing",
      "Jiliang Tang",
      "Qi He"
    ],
    "abstract": "Chain-of-Thought (CoT) reasoning, which breaks down complex tasks into\nintermediate reasoning steps, has significantly enhanced the performance of\nlarge language models (LLMs) on challenging tasks. However, the detailed\nreasoning process in CoT often incurs long generation times and high\ncomputational costs, partly due to the inclusion of unnecessary steps. To\naddress this, we propose a method to identify critical reasoning steps using\nperplexity as a measure of their importance: a step is deemed critical if its\nremoval causes a significant increase in perplexity. Our method enables models\nto focus solely on generating these critical steps. This can be achieved\nthrough two approaches: refining demonstration examples in few-shot CoT or\nfine-tuning the model using selected examples that include only critical steps.\nComprehensive experiments validate the effectiveness of our method, which\nachieves a better balance between the reasoning accuracy and efficiency of CoT.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.13260v1",
    "published_date": "2025-02-18 20:04:51 UTC",
    "updated_date": "2025-02-18 20:04:51 UTC"
  },
  {
    "arxiv_id": "2502.13259v1",
    "title": "HumT DumT: Measuring and controlling human-like language in LLMs",
    "authors": [
      "Myra Cheng",
      "Sunny Yu",
      "Dan Jurafsky"
    ],
    "abstract": "Should LLMs generate language that makes them seem human? Human-like language\nmight improve user experience, but might also lead to overreliance and\nstereotyping. Assessing these potential impacts requires a systematic way to\nmeasure human-like tone in LLM outputs. We introduce HumT and SocioT, metrics\nfor human-like tone and other dimensions of social perceptions in text data\nbased on relative probabilities from an LLM. By measuring HumT across\npreference and usage datasets, we find that users prefer less human-like\noutputs from LLMs. HumT also offers insights into the impacts of\nanthropomorphism: human-like LLM outputs are highly correlated with warmth,\nsocial closeness, femininity, and low status, which are closely linked to the\naforementioned harms. We introduce DumT, a method using HumT to systematically\ncontrol and reduce the degree of human-like tone while preserving model\nperformance. DumT offers a practical approach for mitigating risks associated\nwith anthropomorphic language generation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.13259v1",
    "published_date": "2025-02-18 20:04:09 UTC",
    "updated_date": "2025-02-18 20:04:09 UTC"
  },
  {
    "arxiv_id": "2502.13256v1",
    "title": "A Survey of Anomaly Detection in Cyber-Physical Systems",
    "authors": [
      "Danial Abshari",
      "Meera Sridhar"
    ],
    "abstract": "In our increasingly interconnected world, Cyber-Physical Systems (CPS) play a\ncrucial role in industries like healthcare, transportation, and manufacturing\nby combining physical processes with computing power. These systems, however,\nface many challenges, especially regarding security and system faults.\nAnomalies in CPS may indicate unexpected problems, from sensor malfunctions to\ncyber-attacks, and must be detected to prevent failures that can cause harm or\ndisrupt services. This paper provides an overview of the different ways\nresearchers have approached anomaly detection in CPS. We categorize and compare\nmethods like machine learning, deep learning, mathematical models, invariant,\nand hybrid techniques. Our goal is to help readers understand the strengths and\nweaknesses of these methods and how they can be used to create safer, more\nreliable CPS. By identifying the gaps in current solutions, we aim to encourage\nfuture research that will make CPS more secure and adaptive in our increasingly\nautomated world.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.13256v1",
    "published_date": "2025-02-18 19:38:18 UTC",
    "updated_date": "2025-02-18 19:38:18 UTC"
  },
  {
    "arxiv_id": "2502.15797v1",
    "title": "OCCULT: Evaluating Large Language Models for Offensive Cyber Operation Capabilities",
    "authors": [
      "Michael Kouremetis",
      "Marissa Dotter",
      "Alex Byrne",
      "Dan Martin",
      "Ethan Michalak",
      "Gianpaolo Russo",
      "Michael Threet",
      "Guido Zarrella"
    ],
    "abstract": "The prospect of artificial intelligence (AI) competing in the adversarial\nlandscape of cyber security has long been considered one of the most impactful,\nchallenging, and potentially dangerous applications of AI. Here, we demonstrate\na new approach to assessing AI's progress towards enabling and scaling\nreal-world offensive cyber operations (OCO) tactics in use by modern threat\nactors. We detail OCCULT, a lightweight operational evaluation framework that\nallows cyber security experts to contribute to rigorous and repeatable\nmeasurement of the plausible cyber security risks associated with any given\nlarge language model (LLM) or AI employed for OCO. We also prototype and\nevaluate three very different OCO benchmarks for LLMs that demonstrate our\napproach and serve as examples for building benchmarks under the OCCULT\nframework. Finally, we provide preliminary evaluation results to demonstrate\nhow this framework allows us to move beyond traditional all-or-nothing tests,\nsuch as those crafted from educational exercises like capture-the-flag\nenvironments, to contextualize our indicators and warnings in true cyber threat\nscenarios that present risks to modern infrastructure. We find that there has\nbeen significant recent advancement in the risks of AI being used to scale\nrealistic cyber threats. For the first time, we find a model (DeepSeek-R1) is\ncapable of correctly answering over 90% of challenging offensive cyber\nknowledge tests in our Threat Actor Competency Test for LLMs (TACTL)\nmultiple-choice benchmarks. We also show how Meta's Llama and Mistral's Mixtral\nmodel families show marked performance improvements over earlier models against\nour benchmarks where LLMs act as offensive agents in MITRE's high-fidelity\noffensive and defensive cyber operations simulation environment, CyberLayer.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "31 pages, 17 figures, 11 tables",
    "pdf_url": "http://arxiv.org/pdf/2502.15797v1",
    "published_date": "2025-02-18 19:33:14 UTC",
    "updated_date": "2025-02-18 19:33:14 UTC"
  },
  {
    "arxiv_id": "2502.15796v1",
    "title": "Pruning as a Defense: Reducing Memorization in Large Language Models",
    "authors": [
      "Mansi Gupta",
      "Nikhar Waghela",
      "Sarthak Gupta",
      "Shourya Goel",
      "Sanjif Shanmugavelu"
    ],
    "abstract": "Large language models have been shown to memorize significant portions of\ntheir training data, which they can reproduce when appropriately prompted. This\nwork investigates the impact of simple pruning techniques on this behavior. Our\nfindings reveal that pruning effectively reduces the extent of memorization in\nLLMs, demonstrating its potential as a foundational approach for mitigating\nmembership inference attacks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.15796v1",
    "published_date": "2025-02-18 19:32:10 UTC",
    "updated_date": "2025-02-18 19:32:10 UTC"
  },
  {
    "arxiv_id": "2502.13251v2",
    "title": "Neural Attention Search",
    "authors": [
      "Difan Deng",
      "Marius Lindauer"
    ],
    "abstract": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "18 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.13251v2",
    "published_date": "2025-02-18 19:22:44 UTC",
    "updated_date": "2025-02-20 09:03:05 UTC"
  },
  {
    "arxiv_id": "2502.13248v1",
    "title": "Communication Strategy on Macro-and-Micro Traffic State in Cooperative Deep Reinforcement Learning for Regional Traffic Signal Control",
    "authors": [
      "Hankang Gu",
      "Shangbo Wang",
      "Dongyao Jia",
      "Yuli Zhang",
      "Yanrong Luo",
      "Guoqiang Mao",
      "Jianping Wang",
      "Eng Gee Lim"
    ],
    "abstract": "Adaptive Traffic Signal Control (ATSC) has become a popular research topic in\nintelligent transportation systems. Regional Traffic Signal Control (RTSC)\nusing the Multi-agent Deep Reinforcement Learning (MADRL) technique has become\na promising approach for ATSC due to its ability to achieve the optimum\ntrade-off between scalability and optimality. Most existing RTSC approaches\npartition a traffic network into several disjoint regions, followed by applying\ncentralized reinforcement learning techniques to each region. However, the\npursuit of cooperation among RTSC agents still remains an open issue and no\ncommunication strategy for RTSC agents has been investigated. In this paper, we\npropose communication strategies to capture the correlation of micro-traffic\nstates among lanes and the correlation of macro-traffic states among\nintersections. We first justify the evolution equation of the RTSC process is\nMarkovian via a system of store-and-forward queues. Next, based on the\nevolution equation, we propose two GAT-Aggregated (GA2) communication\nmodules--GA2-Naive and GA2-Aug to extract both intra-region and inter-region\ncorrelations between macro and micro traffic states. While GA2-Naive only\nconsiders the movements at each intersection, GA2-Aug also considers the\nlane-changing behavior of vehicles. Two proposed communication modules are then\naggregated into two existing novel RTSC frameworks--RegionLight and\nRegional-DRL. Experimental results demonstrate that both GA2-Naive and GA2-Aug\neffectively improve the performance of existing RTSC frameworks under both real\nand synthetic scenarios. Hyperparameter testing also reveals the robustness and\npotential of our communication modules in large-scale traffic networks.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.13248v1",
    "published_date": "2025-02-18 19:20:51 UTC",
    "updated_date": "2025-02-18 19:20:51 UTC"
  },
  {
    "arxiv_id": "2502.15795v1",
    "title": "Lean-ing on Quality: How High-Quality Data Beats Diverse Multilingual Data in AutoFormalization",
    "authors": [
      "Willy Chan",
      "Michael Souliman",
      "Jakob Nordhagen",
      "Brando Miranda",
      "Elyas Obbad",
      "Kai Fronsdal Sanmi Koyejo"
    ],
    "abstract": "Autoformalization, the process of transforming informal mathematical language\ninto formal specifications and proofs remains a difficult task for\nstate-of-the-art (large) language models. Existing works point to competing\nexplanations for the performance gap. To this end, we introduce a novel\nmethodology that leverages back-translation with hand-curated prompts to\nenhance the mathematical capabilities of language models, particularly\naddressing the challenge posed by the scarcity of labeled data. Specifically,\nwe evaluate three primary variations of this strategy: (1) on-the-fly (online)\nbacktranslation, (2) distilled (offline) backtranslation with few-shot\namplification, and (3) line-by-line proof analysis integrated with proof state\ninformation. Each variant is designed to optimize data quality over quantity,\nfocusing on the high fidelity of generated proofs rather than sheer data scale.\nOur findings provide evidence that employing our proposed approaches to\ngenerate synthetic data, which prioritizes quality over volume, improves the\nAutoformalization performance of LLMs as measured by standard benchmarks such\nas ProofNet. Crucially, our approach outperforms pretrained models using a\nminimal number of tokens. We also show, through strategic prompting and\nbacktranslation, that our approaches surpass the performance of fine-tuning\nwith extensive multilingual datasets such as MMA on ProofNet with only 1/150th\nof the tokens. Taken together, our methods show a promising new approach to\nsignificantly reduce the resources required to formalize proofs, thereby\naccelerating AI for math.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.PL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.15795v1",
    "published_date": "2025-02-18 19:16:54 UTC",
    "updated_date": "2025-02-18 19:16:54 UTC"
  },
  {
    "arxiv_id": "2502.13234v1",
    "title": "MotionMatcher: Motion Customization of Text-to-Video Diffusion Models via Motion Feature Matching",
    "authors": [
      "Yen-Siang Wu",
      "Chi-Pin Huang",
      "Fu-En Yang",
      "Yu-Chiang Frank Wang"
    ],
    "abstract": "Text-to-video (T2V) diffusion models have shown promising capabilities in\nsynthesizing realistic videos from input text prompts. However, the input text\ndescription alone provides limited control over the precise objects movements\nand camera framing. In this work, we tackle the motion customization problem,\nwhere a reference video is provided as motion guidance. While most existing\nmethods choose to fine-tune pre-trained diffusion models to reconstruct the\nframe differences of the reference video, we observe that such strategy suffer\nfrom content leakage from the reference video, and they cannot capture complex\nmotion accurately. To address this issue, we propose MotionMatcher, a motion\ncustomization framework that fine-tunes the pre-trained T2V diffusion model at\nthe feature level. Instead of using pixel-level objectives, MotionMatcher\ncompares high-level, spatio-temporal motion features to fine-tune diffusion\nmodels, ensuring precise motion learning. For the sake of memory efficiency and\naccessibility, we utilize a pre-trained T2V diffusion model, which contains\nconsiderable prior knowledge about video motion, to compute these motion\nfeatures. In our experiments, we demonstrate state-of-the-art motion\ncustomization performances, validating the design of our framework.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://www.csie.ntu.edu.tw/~b09902097/motionmatcher/",
    "pdf_url": "http://arxiv.org/pdf/2502.13234v1",
    "published_date": "2025-02-18 19:12:51 UTC",
    "updated_date": "2025-02-18 19:12:51 UTC"
  },
  {
    "arxiv_id": "2502.13233v1",
    "title": "SearchRAG: Can Search Engines Be Helpful for LLM-based Medical Question Answering?",
    "authors": [
      "Yucheng Shi",
      "Tianze Yang",
      "Canyu Chen",
      "Quanzheng Li",
      "Tianming Liu",
      "Xiang Li",
      "Ninghao Liu"
    ],
    "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in general\ndomains but often struggle with tasks requiring specialized knowledge.\nConventional Retrieval-Augmented Generation (RAG) techniques typically retrieve\nexternal information from static knowledge bases, which can be outdated or\nincomplete, missing fine-grained clinical details essential for accurate\nmedical question answering. In this work, we propose SearchRAG, a novel\nframework that overcomes these limitations by leveraging real-time search\nengines. Our method employs synthetic query generation to convert complex\nmedical questions into search-engine-friendly queries and utilizes\nuncertainty-based knowledge selection to filter and incorporate the most\nrelevant and informative medical knowledge into the LLM's input. Experimental\nresults demonstrate that our method significantly improves response accuracy in\nmedical question answering tasks, particularly for complex questions requiring\ndetailed and up-to-date knowledge.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages, three figures",
    "pdf_url": "http://arxiv.org/pdf/2502.13233v1",
    "published_date": "2025-02-18 19:12:15 UTC",
    "updated_date": "2025-02-18 19:12:15 UTC"
  },
  {
    "arxiv_id": "2502.13228v1",
    "title": "Conformal Prediction as Bayesian Quadrature",
    "authors": [
      "Jake C. Snell",
      "Thomas L. Griffiths"
    ],
    "abstract": "As machine learning-based prediction systems are increasingly used in\nhigh-stakes situations, it is important to understand how such predictive\nmodels will perform upon deployment. Distribution-free uncertainty\nquantification techniques such as conformal prediction provide guarantees about\nthe loss black-box models will incur even when the details of the models are\nhidden. However, such methods are based on frequentist probability, which\nunduly limits their applicability. We revisit the central aspects of conformal\nprediction from a Bayesian perspective and thereby illuminate the shortcomings\nof frequentist guarantees. We propose a practical alternative based on Bayesian\nquadrature that provides interpretable guarantees and offers a richer\nrepresentation of the likely range of losses to be observed at test time.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.13228v1",
    "published_date": "2025-02-18 19:06:21 UTC",
    "updated_date": "2025-02-18 19:06:21 UTC"
  },
  {
    "arxiv_id": "2502.14906v1",
    "title": "Beyond Words: Exploring Cultural Value Sensitivity in Multimodal Models",
    "authors": [
      "Srishti Yadav",
      "Zhi Zhang",
      "Daniel Hershcovich",
      "Ekaterina Shutova"
    ],
    "abstract": "Investigating value alignment in Large Language Models (LLMs) based on\ncultural context has become a critical area of research. However, similar\nbiases have not been extensively explored in large vision-language models\n(VLMs). As the scale of multimodal models continues to grow, it becomes\nincreasingly important to assess whether images can serve as reliable proxies\nfor culture and how these values are embedded through the integration of both\nvisual and textual data. In this paper, we conduct a thorough evaluation of\nmultimodal model at different scales, focusing on their alignment with cultural\nvalues. Our findings reveal that, much like LLMs, VLMs exhibit sensitivity to\ncultural values, but their performance in aligning with these values is highly\ncontext-dependent. While VLMs show potential in improving value understanding\nthrough the use of images, this alignment varies significantly across contexts\nhighlighting the complexities and underexplored challenges in the alignment of\nmultimodal models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14906v1",
    "published_date": "2025-02-18 19:03:02 UTC",
    "updated_date": "2025-02-18 19:03:02 UTC"
  },
  {
    "arxiv_id": "2502.13221v1",
    "title": "Two Tickets are Better than One: Fair and Accurate Hiring Under Strategic LLM Manipulations",
    "authors": [
      "Lee Cohen",
      "Jack Hsieh",
      "Connie Hong",
      "Judy Hanwen Shen"
    ],
    "abstract": "In an era of increasingly capable foundation models, job seekers are turning\nto generative AI tools to enhance their application materials. However, unequal\naccess to and knowledge about generative AI tools can harm both employers and\ncandidates by reducing the accuracy of hiring decisions and giving some\ncandidates an unfair advantage. To address these challenges, we introduce a new\nvariant of the strategic classification framework tailored to manipulations\nperformed using large language models, accommodating varying levels of\nmanipulations and stochastic outcomes. We propose a ``two-ticket'' scheme,\nwhere the hiring algorithm applies an additional manipulation to each submitted\nresume and considers this manipulated version together with the original\nsubmitted resume. We establish theoretical guarantees for this scheme, showing\nimprovements for both the fairness and accuracy of hiring decisions when the\ntrue positive rate is maximized subject to a no false positives constraint. We\nfurther generalize this approach to an $n$-ticket scheme and prove that hiring\noutcomes converge to a fixed, group-independent decision, eliminating\ndisparities arising from differential LLM access. Finally, we empirically\nvalidate our framework and the performance of our two-ticket scheme on real\nresumes using an open-source resume screening tool.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "cs.GT"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.13221v1",
    "published_date": "2025-02-18 19:01:04 UTC",
    "updated_date": "2025-02-18 19:01:04 UTC"
  },
  {
    "arxiv_id": "2502.13207v1",
    "title": "Thinking Outside the (Gray) Box: A Context-Based Score for Assessing Value and Originality in Neural Text Generation",
    "authors": [
      "Giorgio Franceschelli",
      "Mirco Musolesi"
    ],
    "abstract": "Despite the increasing use of large language models for creative tasks, their\noutputs often lack diversity. Common solutions, such as sampling at higher\ntemperatures, can compromise the quality of the results. Drawing on information\ntheory, we propose a context-based score to quantitatively evaluate value and\noriginality. This score incentivizes accuracy and adherence to the request\nwhile fostering divergence from the learned distribution. We propose using our\nscore as a reward in a reinforcement learning framework to fine-tune large\nlanguage models for maximum performance. We validate our strategy through\nexperiments in poetry generation and math problem solving, demonstrating that\nit enhances the value and originality of the generated solutions.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.13207v1",
    "published_date": "2025-02-18 19:00:01 UTC",
    "updated_date": "2025-02-18 19:00:01 UTC"
  },
  {
    "arxiv_id": "2502.13143v1",
    "title": "SoFar: Language-Grounded Orientation Bridges Spatial Reasoning and Object Manipulation",
    "authors": [
      "Zekun Qi",
      "Wenyao Zhang",
      "Yufei Ding",
      "Runpei Dong",
      "Xinqiang Yu",
      "Jingwen Li",
      "Lingyun Xu",
      "Baoyu Li",
      "Xialin He",
      "Guofan Fan",
      "Jiazhao Zhang",
      "Jiawei He",
      "Jiayuan Gu",
      "Xin Jin",
      "Kaisheng Ma",
      "Zhizheng Zhang",
      "He Wang",
      "Li Yi"
    ],
    "abstract": "Spatial intelligence is a critical component of embodied AI, promoting robots\nto understand and interact with their environments. While recent advances have\nenhanced the ability of VLMs to perceive object locations and positional\nrelationships, they still lack the capability to precisely understand object\norientations-a key requirement for tasks involving fine-grained manipulations.\nAddressing this limitation not only requires geometric reasoning but also an\nexpressive and intuitive way to represent orientation. In this context, we\npropose that natural language offers a more flexible representation space than\ncanonical frames, making it particularly suitable for instruction-following\nrobotic systems. In this paper, we introduce the concept of semantic\norientation, which defines object orientations using natural language in a\nreference-frame-free manner (e.g., the ''plug-in'' direction of a USB or the\n''handle'' direction of a knife). To support this, we construct OrienText300K,\na large-scale dataset of 3D models annotated with semantic orientations that\nlink geometric understanding to functional semantics. By integrating semantic\norientation into a VLM system, we enable robots to generate manipulation\nactions with both positional and orientational constraints. Extensive\nexperiments in simulation and real world demonstrate that our approach\nsignificantly enhances robotic manipulation capabilities, e.g., 48.7% accuracy\non Open6DOR and 74.9% accuracy on SIMPLER.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "Project page: https://qizekun.github.io/sofar/",
    "pdf_url": "http://arxiv.org/pdf/2502.13143v1",
    "published_date": "2025-02-18 18:59:02 UTC",
    "updated_date": "2025-02-18 18:59:02 UTC"
  },
  {
    "arxiv_id": "2502.13142v2",
    "title": "Pre-training Auto-regressive Robotic Models with 4D Representations",
    "authors": [
      "Dantong Niu",
      "Yuvan Sharma",
      "Haoru Xue",
      "Giscard Biamby",
      "Junyi Zhang",
      "Ziteng Ji",
      "Trevor Darrell",
      "Roei Herzig"
    ],
    "abstract": "Foundation models pre-trained on massive unlabeled datasets have\nrevolutionized natural language and computer vision, exhibiting remarkable\ngeneralization capabilities, thus highlighting the importance of pre-training.\nYet, efforts in robotics have struggled to achieve similar success, limited by\neither the need for costly robotic annotations or the lack of representations\nthat effectively model the physical world. In this paper, we introduce ARM4R,\nan Auto-regressive Robotic Model that leverages low-level 4D Representations\nlearned from human video data to yield a better pre-trained robotic model.\nSpecifically, we focus on utilizing 3D point tracking representations from\nvideos derived by lifting 2D representations into 3D space via monocular depth\nestimation across time. These 4D representations maintain a shared geometric\nstructure between the points and robot state representations up to a linear\ntransformation, enabling efficient transfer learning from human video data to\nlow-level robotic control. Our experiments show that ARM4R can transfer\nefficiently from human video data to robotics and consistently improves\nperformance on tasks across various robot environments and configurations.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.13142v2",
    "published_date": "2025-02-18 18:59:01 UTC",
    "updated_date": "2025-05-17 17:33:08 UTC"
  },
  {
    "arxiv_id": "2502.13141v1",
    "title": "UniGuardian: A Unified Defense for Detecting Prompt Injection, Backdoor Attacks and Adversarial Attacks in Large Language Models",
    "authors": [
      "Huawei Lin",
      "Yingjie Lao",
      "Tong Geng",
      "Tan Yu",
      "Weijie Zhao"
    ],
    "abstract": "Large Language Models (LLMs) are vulnerable to attacks like prompt injection,\nbackdoor attacks, and adversarial attacks, which manipulate prompts or models\nto generate harmful outputs. In this paper, departing from traditional deep\nlearning attack paradigms, we explore their intrinsic relationship and\ncollectively term them Prompt Trigger Attacks (PTA). This raises a key\nquestion: Can we determine if a prompt is benign or poisoned? To address this,\nwe propose UniGuardian, the first unified defense mechanism designed to detect\nprompt injection, backdoor attacks, and adversarial attacks in LLMs.\nAdditionally, we introduce a single-forward strategy to optimize the detection\npipeline, enabling simultaneous attack detection and text generation within a\nsingle forward pass. Our experiments confirm that UniGuardian accurately and\nefficiently identifies malicious prompts in LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "18 Pages, 8 Figures, 5 Tables, Keywords: Attack Defending, Security,\n  Prompt Injection, Backdoor Attacks, Adversarial Attacks, Prompt Trigger\n  Attacks",
    "pdf_url": "http://arxiv.org/pdf/2502.13141v1",
    "published_date": "2025-02-18 18:59:00 UTC",
    "updated_date": "2025-02-18 18:59:00 UTC"
  },
  {
    "arxiv_id": "2502.13138v1",
    "title": "AIDE: AI-Driven Exploration in the Space of Code",
    "authors": [
      "Zhengyao Jiang",
      "Dominik Schmidt",
      "Dhruv Srikanth",
      "Dixing Xu",
      "Ian Kaplan",
      "Deniss Jacenko",
      "Yuxiang Wu"
    ],
    "abstract": "Machine learning, the foundation of modern artificial intelligence, has\ndriven innovations that have fundamentally transformed the world. Yet, behind\nadvancements lies a complex and often tedious process requiring labor and\ncompute intensive iteration and experimentation. Engineers and scientists\ndeveloping machine learning models spend much of their time on trial-and-error\ntasks instead of conceptualizing innovative solutions or research hypotheses.\nTo address this challenge, we introduce AI-Driven Exploration (AIDE), a machine\nlearning engineering agent powered by large language models (LLMs). AIDE frames\nmachine learning engineering as a code optimization problem, and formulates\ntrial-and-error as a tree search in the space of potential solutions. By\nstrategically reusing and refining promising solutions, AIDE effectively trades\ncomputational resources for enhanced performance, achieving state-of-the-art\nresults on multiple machine learning engineering benchmarks, including our\nKaggle evaluations, OpenAI MLE-Bench and METRs RE-Bench.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.13138v1",
    "published_date": "2025-02-18 18:57:21 UTC",
    "updated_date": "2025-02-18 18:57:21 UTC"
  },
  {
    "arxiv_id": "2502.13137v1",
    "title": "Theorem Prover as a Judge for Synthetic Data Generation",
    "authors": [
      "Joshua Ong Jun Leang",
      "Giwon Hong",
      "Wenda Li",
      "Shay B. Cohen"
    ],
    "abstract": "The demand for synthetic data in mathematical reasoning has increased due to\nits potential to enhance the mathematical capabilities of large language models\n(LLMs). However, ensuring the validity of intermediate reasoning steps remains\na significant challenge, affecting data quality. While formal verification via\ntheorem provers effectively validates LLM reasoning, the autoformalisation of\nmathematical proofs remains error-prone. In response, we introduce iterative\nautoformalisation, an approach that iteratively refines theorem prover\nformalisation to mitigate errors, thereby increasing the execution rate on the\nLean prover from 60% to 87%. Building upon that, we introduce Theorem Prover as\na Judge (TP-as-a-Judge), a method that employs theorem prover formalisation to\nrigorously assess LLM intermediate reasoning, effectively integrating\nautoformalisation with synthetic data generation. Finally, we present\nReinforcement Learning from Theorem Prover Feedback (RLTPF), a framework that\nreplaces human annotation with theorem prover feedback in Reinforcement\nLearning from Human Feedback (RLHF). Across multiple LLMs, applying\nTP-as-a-Judge and RLTPF improves benchmarks with only 3,508 samples, achieving\n5.56% accuracy gain on Mistral-7B for MultiArith, 6.00% on Llama-2-7B for\nSVAMP, and 3.55% on Llama-3.1-8B for AQUA.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.13137v1",
    "published_date": "2025-02-18 18:57:09 UTC",
    "updated_date": "2025-02-18 18:57:09 UTC"
  },
  {
    "arxiv_id": "2502.13135v1",
    "title": "Sleepless Nights, Sugary Days: Creating Synthetic Users with Health Conditions for Realistic Coaching Agent Interactions",
    "authors": [
      "Taedong Yun",
      "Eric Yang",
      "Mustafa Safdari",
      "Jong Ha Lee",
      "Vaishnavi Vinod Kumar",
      "S. Sara Mahdavi",
      "Jonathan Amar",
      "Derek Peyton",
      "Reut Aharony",
      "Andreas Michaelides",
      "Logan Schneider",
      "Isaac Galatzer-Levy",
      "Yugang Jia",
      "John Canny",
      "Arthur Gretton",
      "Maja Matarić"
    ],
    "abstract": "We present an end-to-end framework for generating synthetic users for\nevaluating interactive agents designed to encourage positive behavior changes,\nsuch as in health and lifestyle coaching. The synthetic users are grounded in\nhealth and lifestyle conditions, specifically sleep and diabetes management in\nthis study, to ensure realistic interactions with the health coaching agent.\nSynthetic users are created in two stages: first, structured data are generated\ngrounded in real-world health and lifestyle factors in addition to basic\ndemographics and behavioral attributes; second, full profiles of the synthetic\nusers are developed conditioned on the structured data. Interactions between\nsynthetic users and the coaching agent are simulated using generative\nagent-based models such as Concordia, or directly by prompting a language\nmodel. Using two independently-developed agents for sleep and diabetes coaching\nas case studies, the validity of this framework is demonstrated by analyzing\nthe coaching agent's understanding of the synthetic users' needs and\nchallenges. Finally, through multiple blinded evaluations of user-coach\ninteractions by human experts, we demonstrate that our synthetic users with\nhealth and behavioral attributes more accurately portray real human users with\nthe same attributes, compared to generic synthetic users not grounded in such\nattributes. The proposed framework lays the foundation for efficient\ndevelopment of conversational agents through extensive, realistic, and grounded\nsimulated interactions.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.13135v1",
    "published_date": "2025-02-18 18:56:44 UTC",
    "updated_date": "2025-02-18 18:56:44 UTC"
  },
  {
    "arxiv_id": "2502.13132v1",
    "title": "Learning to Defer for Causal Discovery with Imperfect Experts",
    "authors": [
      "Oscar Clivio",
      "Divyat Mahajan",
      "Perouz Taslakian",
      "Sara Magliacane",
      "Ioannis Mitliagkas",
      "Valentina Zantedeschi",
      "Alexandre Drouin"
    ],
    "abstract": "Integrating expert knowledge, e.g. from large language models, into causal\ndiscovery algorithms can be challenging when the knowledge is not guaranteed to\nbe correct. Expert recommendations may contradict data-driven results, and\ntheir reliability can vary significantly depending on the domain or specific\nquery. Existing methods based on soft constraints or inconsistencies in\npredicted causal relationships fail to account for these variations in\nexpertise. To remedy this, we propose L2D-CD, a method for gauging the\ncorrectness of expert recommendations and optimally combining them with\ndata-driven causal discovery results. By adapting learning-to-defer (L2D)\nalgorithms for pairwise causal discovery (CD), we learn a deferral function\nthat selects whether to rely on classical causal discovery methods using\nnumerical data or expert recommendations based on textual meta-data. We\nevaluate L2D-CD on the canonical T\\\"ubingen pairs dataset and demonstrate its\nsuperior performance compared to both the causal discovery method and the\nexpert used in isolation. Moreover, our approach identifies domains where the\nexpert's performance is strong or weak. Finally, we outline a strategy for\ngeneralizing this approach to causal discovery on graphs with more than two\nvariables, paving the way for further research in this area.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.13132v1",
    "published_date": "2025-02-18 18:55:53 UTC",
    "updated_date": "2025-02-18 18:55:53 UTC"
  },
  {
    "arxiv_id": "2502.13131v1",
    "title": "Rethinking Diverse Human Preference Learning through Principal Component Analysis",
    "authors": [
      "Feng Luo",
      "Rui Yang",
      "Hao Sun",
      "Chunyuan Deng",
      "Jiarui Yao",
      "Jingyan Shen",
      "Huan Zhang",
      "Hanjie Chen"
    ],
    "abstract": "Understanding human preferences is crucial for improving foundation models\nand building personalized AI systems. However, preferences are inherently\ndiverse and complex, making it difficult for traditional reward models to\ncapture their full range. While fine-grained preference data can help,\ncollecting it is expensive and hard to scale. In this paper, we introduce\nDecomposed Reward Models (DRMs), a novel approach that extracts diverse human\npreferences from binary comparisons without requiring fine-grained annotations.\nOur key insight is to represent human preferences as vectors and analyze them\nusing Principal Component Analysis (PCA). By constructing a dataset of\nembedding differences between preferred and rejected responses, DRMs identify\northogonal basis vectors that capture distinct aspects of preference. These\ndecomposed rewards can be flexibly combined to align with different user needs,\noffering an interpretable and scalable alternative to traditional reward\nmodels. We demonstrate that DRMs effectively extract meaningful preference\ndimensions (e.g., helpfulness, safety, humor) and adapt to new users without\nadditional training. Our results highlight DRMs as a powerful framework for\npersonalized and interpretable LLM alignment.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "14 pages",
    "pdf_url": "http://arxiv.org/pdf/2502.13131v1",
    "published_date": "2025-02-18 18:55:26 UTC",
    "updated_date": "2025-02-18 18:55:26 UTC"
  },
  {
    "arxiv_id": "2502.13130v1",
    "title": "Magma: A Foundation Model for Multimodal AI Agents",
    "authors": [
      "Jianwei Yang",
      "Reuben Tan",
      "Qianhui Wu",
      "Ruijie Zheng",
      "Baolin Peng",
      "Yongyuan Liang",
      "Yu Gu",
      "Mu Cai",
      "Seonghyeon Ye",
      "Joel Jang",
      "Yuquan Deng",
      "Lars Liden",
      "Jianfeng Gao"
    ],
    "abstract": "We present Magma, a foundation model that serves multimodal AI agentic tasks\nin both the digital and physical worlds. Magma is a significant extension of\nvision-language (VL) models in that it not only retains the VL understanding\nability (verbal intelligence) of the latter, but is also equipped with the\nability to plan and act in the visual-spatial world (spatial-temporal\nintelligence) and complete agentic tasks ranging from UI navigation to robot\nmanipulation. To endow the agentic capabilities, Magma is pretrained on large\namounts of heterogeneous datasets spanning from images, videos to robotics\ndata, where the actionable visual objects (e.g., clickable buttons in GUI) in\nimages are labeled by Set-of-Mark (SoM) for action grounding, and the object\nmovements (e.g., the trace of human hands or robotic arms) in videos are\nlabeled by Trace-of-Mark (ToM) for action planning. Extensive experiments show\nthat SoM and ToM reach great synergy and facilitate the acquisition of\nspatial-temporal intelligence for our Magma model, which is fundamental to a\nwide range of tasks as shown in Fig.1. In particular, Magma creates new\nstate-of-the-art results on UI navigation and robotic manipulation tasks,\noutperforming previous models that are specifically tailored to these tasks. On\nimage and video-related multimodal tasks, Magma also compares favorably to\npopular large multimodal models that are trained on much larger datasets. We\nmake our model and code public for reproducibility at\nhttps://microsoft.github.io/Magma.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "29 pages, 16 figures, technical report from MSR",
    "pdf_url": "http://arxiv.org/pdf/2502.13130v1",
    "published_date": "2025-02-18 18:55:21 UTC",
    "updated_date": "2025-02-18 18:55:21 UTC"
  },
  {
    "arxiv_id": "2502.13128v1",
    "title": "SongGen: A Single Stage Auto-regressive Transformer for Text-to-Song Generation",
    "authors": [
      "Zihan Liu",
      "Shuangrui Ding",
      "Zhixiong Zhang",
      "Xiaoyi Dong",
      "Pan Zhang",
      "Yuhang Zang",
      "Yuhang Cao",
      "Dahua Lin",
      "Jiaqi Wang"
    ],
    "abstract": "Text-to-song generation, the task of creating vocals and accompaniment from\ntextual inputs, poses significant challenges due to domain complexity and data\nscarcity. Existing approaches often employ multi-stage generation procedures,\nresulting in cumbersome training and inference pipelines. In this paper, we\npropose SongGen, a fully open-source, single-stage auto-regressive transformer\ndesigned for controllable song generation. The proposed model facilitates\nfine-grained control over diverse musical attributes, including lyrics and\ntextual descriptions of instrumentation, genre, mood, and timbre, while also\noffering an optional three-second reference clip for voice cloning. Within a\nunified auto-regressive framework, SongGen supports two output modes: mixed\nmode, which generates a mixture of vocals and accompaniment directly, and\ndual-track mode, which synthesizes them separately for greater flexibility in\ndownstream applications. We explore diverse token pattern strategies for each\nmode, leading to notable improvements and valuable insights. Furthermore, we\ndesign an automated data preprocessing pipeline with effective quality control.\nTo foster community engagement and future research, we will release our model\nweights, training code, annotated data, and preprocessing pipeline. The\ngenerated samples are showcased on our project page at\nhttps://liuzh-19.github.io/SongGen/ , and the code will be available at\nhttps://github.com/LiuZH-19/SongGen .",
    "categories": [
      "cs.SD",
      "cs.AI"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.13128v1",
    "published_date": "2025-02-18 18:52:21 UTC",
    "updated_date": "2025-02-18 18:52:21 UTC"
  },
  {
    "arxiv_id": "2502.13120v1",
    "title": "Adapting Psycholinguistic Research for LLMs: Gender-inclusive Language in a Coreference Context",
    "authors": [
      "Marion Bartl",
      "Thomas Brendan Murphy",
      "Susan Leavy"
    ],
    "abstract": "Gender-inclusive language is often used with the aim of ensuring that all\nindividuals, regardless of gender, can be associated with certain concepts.\nWhile psycholinguistic studies have examined its effects in relation to human\ncognition, it remains unclear how Large Language Models (LLMs) process\ngender-inclusive language. Given that commercial LLMs are gaining an\nincreasingly strong foothold in everyday applications, it is crucial to examine\nwhether LLMs in fact interpret gender-inclusive language neutrally, because the\nlanguage they generate has the potential to influence the language of their\nusers. This study examines whether LLM-generated coreferent terms align with a\ngiven gender expression or reflect model biases. Adapting psycholinguistic\nmethods from French to English and German, we find that in English, LLMs\ngenerally maintain the antecedent's gender but exhibit underlying masculine\nbias. In German, this bias is much stronger, overriding all tested\ngender-neutralization strategies.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, 7 figures, submitted to ACL 2025 (ARR February 2025 cycle)",
    "pdf_url": "http://arxiv.org/pdf/2502.13120v1",
    "published_date": "2025-02-18 18:42:11 UTC",
    "updated_date": "2025-02-18 18:42:11 UTC"
  },
  {
    "arxiv_id": "2502.13200v1",
    "title": "Learning To Explore With Predictive World Model Via Self-Supervised Learning",
    "authors": [
      "Alana Santana",
      "Paula P. Costa",
      "Esther L. Colombini"
    ],
    "abstract": "Autonomous artificial agents must be able to learn behaviors in complex\nenvironments without humans to design tasks and rewards. Designing these\nfunctions for each environment is not feasible, thus, motivating the\ndevelopment of intrinsic reward functions. In this paper, we propose using\nseveral cognitive elements that have been neglected for a long time to build an\ninternal world model for an intrinsically motivated agent. Our agent performs\nsatisfactory iterations with the environment, learning complex behaviors\nwithout needing previously designed reward functions. We used 18 Atari games to\nevaluate what cognitive skills emerge in games that require reactive and\ndeliberative behaviors. Our results show superior performance compared to the\nstate-of-the-art in many test cases with dense and sparse rewards.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.13200v1",
    "published_date": "2025-02-18 18:39:23 UTC",
    "updated_date": "2025-02-18 18:39:23 UTC"
  },
  {
    "arxiv_id": "2502.13117v1",
    "title": "Performance Evaluation of Large Language Models in Statistical Programming",
    "authors": [
      "Xinyi Song",
      "Kexin Xie",
      "Lina Lee",
      "Ruizhe Chen",
      "Jared M. Clark",
      "Hao He",
      "Haoran He",
      "Jie Min",
      "Xinlei Zhang",
      "Simin Zheng",
      "Zhiyang Zhang",
      "Xinwei Deng",
      "Yili Hong"
    ],
    "abstract": "The programming capabilities of large language models (LLMs) have\nrevolutionized automatic code generation and opened new avenues for automatic\nstatistical analysis. However, the validity and quality of these generated\ncodes need to be systematically evaluated before they can be widely adopted.\nDespite their growing prominence, a comprehensive evaluation of statistical\ncode generated by LLMs remains scarce in the literature. In this paper, we\nassess the performance of LLMs, including two versions of ChatGPT and one\nversion of Llama, in the domain of SAS programming for statistical analysis.\nOur study utilizes a set of statistical analysis tasks encompassing diverse\nstatistical topics and datasets. Each task includes a problem description,\ndataset information, and human-verified SAS code. We conduct a comprehensive\nassessment of the quality of SAS code generated by LLMs through human expert\nevaluation based on correctness, effectiveness, readability, executability, and\nthe accuracy of output results. The analysis of rating scores reveals that\nwhile LLMs demonstrate usefulness in generating syntactically correct code,\nthey struggle with tasks requiring deep domain understanding and may produce\nredundant or incorrect results. This study offers valuable insights into the\ncapabilities and limitations of LLMs in statistical programming, providing\nguidance for future advancements in AI-assisted coding systems for statistical\nanalysis.",
    "categories": [
      "stat.AP",
      "cs.AI"
    ],
    "primary_category": "stat.AP",
    "comment": "27 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.13117v1",
    "published_date": "2025-02-18 18:37:15 UTC",
    "updated_date": "2025-02-18 18:37:15 UTC"
  },
  {
    "arxiv_id": "2502.13115v1",
    "title": "Near-Optimal Private Learning in Linear Contextual Bandits",
    "authors": [
      "Fan Chen",
      "Jiachun Li",
      "Alexander Rakhlin",
      "David Simchi-Levi"
    ],
    "abstract": "We analyze the problem of private learning in generalized linear contextual\nbandits. Our approach is based on a novel method of re-weighted regression,\nyielding an efficient algorithm with regret of order\n$\\sqrt{T}+\\frac{1}{\\alpha}$ and $\\sqrt{T}/\\alpha$ in the joint and local model\nof $\\alpha$-privacy, respectively. Further, we provide near-optimal private\nprocedures that achieve dimension-independent rates in private linear models\nand linear contextual bandits. In particular, our results imply that joint\nprivacy is almost \"for free\" in all the settings we consider, partially\naddressing the open problem posed by Azize and Basu (2024).",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.13115v1",
    "published_date": "2025-02-18 18:35:24 UTC",
    "updated_date": "2025-02-18 18:35:24 UTC"
  },
  {
    "arxiv_id": "2502.13108v2",
    "title": "Clinical QA 2.0: Multi-Task Learning for Answer Extraction and Categorization",
    "authors": [
      "Priyaranjan Pattnayak",
      "Hitesh Laxmichand Patel",
      "Amit Agarwal",
      "Bhargava Kumar",
      "Srikant Panda",
      "Tejaswini Kumar"
    ],
    "abstract": "Clinical Question Answering (CQA) plays a crucial role in medical\ndecision-making, enabling physicians to extract relevant information from\nElectronic Medical Records (EMRs). While transformer-based models such as BERT,\nBioBERT, and ClinicalBERT have demonstrated state-of-the-art performance in\nCQA, existing models lack the ability to categorize extracted answers, which is\ncritical for structured retrieval, content filtering, and medical decision\nsupport.\n  To address this limitation, we introduce a Multi-Task Learning (MTL)\nframework that jointly trains CQA models for both answer extraction and medical\ncategorization. In addition to predicting answer spans, our model classifies\nresponses into five standardized medical categories: Diagnosis, Medication,\nSymptoms, Procedure, and Lab Reports. This categorization enables more\nstructured and interpretable outputs, making clinical QA models more useful in\nreal-world healthcare settings.\n  We evaluate our approach on emrQA, a large-scale dataset for medical question\nanswering. Results show that MTL improves F1-score by 2.2% compared to standard\nfine-tuning, while achieving 90.7% accuracy in answer categorization. These\nfindings suggest that MTL not only enhances CQA performance but also introduces\nan effective mechanism for categorization and structured medical information\nretrieval.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.13108v2",
    "published_date": "2025-02-18 18:20:37 UTC",
    "updated_date": "2025-04-23 17:13:28 UTC"
  },
  {
    "arxiv_id": "2502.13107v3",
    "title": "MatterChat: A Multi-Modal LLM for Material Science",
    "authors": [
      "Yingheng Tang",
      "Wenbin Xu",
      "Jie Cao",
      "Weilu Gao",
      "Steve Farrell",
      "Benjamin Erichson",
      "Michael W. Mahoney",
      "Andy Nonaka",
      "Zhi Yao"
    ],
    "abstract": "Understanding and predicting the properties of inorganic materials is crucial\nfor accelerating advancements in materials science and driving applications in\nenergy, electronics, and beyond. Integrating material structure data with\nlanguage-based information through multi-modal large language models (LLMs)\noffers great potential to support these efforts by enhancing human-AI\ninteraction. However, a key challenge lies in integrating atomic structures at\nfull resolution into LLMs. In this work, we introduce MatterChat, a versatile\nstructure-aware multi-modal LLM that unifies material structural data and\ntextual inputs into a single cohesive model. MatterChat employs a bridging\nmodule to effectively align a pretrained machine learning interatomic potential\nwith a pretrained LLM, reducing training costs and enhancing flexibility. Our\nresults demonstrate that MatterChat significantly improves performance in\nmaterial property prediction and human-AI interaction, surpassing\ngeneral-purpose LLMs such as GPT-4. We also demonstrate its usefulness in\napplications such as more advanced scientific reasoning and step-by-step\nmaterial synthesis.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.13107v3",
    "published_date": "2025-02-18 18:19:36 UTC",
    "updated_date": "2025-04-26 03:31:50 UTC"
  },
  {
    "arxiv_id": "2502.13199v2",
    "title": "The Role of GitHub Copilot on Software Development: A Perspective on Productivity, Security, Best Practices and Future Directions",
    "authors": [
      "Suresh Babu Nettur",
      "Shanthi Karpurapu",
      "Unnati Nettur",
      "Likhit Sagar Gajja",
      "Sravanthy Myneni",
      "Akhil Dusi"
    ],
    "abstract": "GitHub Copilot is transforming software development by automating tasks and\nboosting productivity through AI driven code generation. In this paper, we\nconduct a literature survey to synthesize insights on Copilot's impact on\nproductivity and security. We review academic journal databases, industry\nreports, and official documentation to highlight key findings and challenges.\nWhile Copilot accelerates coding and prototyping, concerns over security\nvulnerabilities and intellectual property risks persist. Drawing from the\nliterature, we provide a perspective on best practices and future directions\nfor responsible AI adoption in software engineering, offering actionable\ninsights for developers and organizations to integrate Copilot effectively\nwhile maintaining high standards of quality and security.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "Correspondence and co-first authors: nettursuresh@gmail.com,\n  shanthi.karpurapu@gmail.com",
    "pdf_url": "http://arxiv.org/pdf/2502.13199v2",
    "published_date": "2025-02-18 18:08:20 UTC",
    "updated_date": "2025-05-02 16:44:12 UTC"
  },
  {
    "arxiv_id": "2502.13198v1",
    "title": "Enhancing Machine Learning Performance through Intelligent Data Quality Assessment: An Unsupervised Data-centric Framework",
    "authors": [
      "Manal Rahal",
      "Bestoun S. Ahmed",
      "Gergely Szabados",
      "Torgny Fornstedt",
      "Jorgen Samuelsson"
    ],
    "abstract": "Poor data quality limits the advantageous power of Machine Learning (ML) and\nweakens high-performing ML software systems. Nowadays, data are more prone to\nthe risk of poor quality due to their increasing volume and complexity.\nTherefore, tedious and time-consuming work goes into data preparation and\nimprovement before moving further in the ML pipeline. To address this\nchallenge, we propose an intelligent data-centric evaluation framework that can\nidentify high-quality data and improve the performance of an ML system. The\nproposed framework combines the curation of quality measurements and\nunsupervised learning to distinguish high- and low-quality data. The framework\nis designed to integrate flexible and general-purpose methods so that it is\ndeployed in various domains and applications. To validate the outcomes of the\ndesigned framework, we implemented it in a real-world use case from the field\nof analytical chemistry, where it is tested on three datasets of anti-sense\noligonucleotides. A domain expert is consulted to identify the relevant quality\nmeasurements and evaluate the outcomes of the framework. The results show that\nthe quality-centric data evaluation framework identifies the characteristics of\nhigh-quality data that guide the conduct of efficient laboratory experiments\nand consequently improve the performance of the ML system.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "42 pages",
    "pdf_url": "http://arxiv.org/pdf/2502.13198v1",
    "published_date": "2025-02-18 18:01:36 UTC",
    "updated_date": "2025-02-18 18:01:36 UTC"
  },
  {
    "arxiv_id": "2502.13092v2",
    "title": "Text2World: Benchmarking Large Language Models for Symbolic World Model Generation",
    "authors": [
      "Mengkang Hu",
      "Tianxing Chen",
      "Yude Zou",
      "Yuheng Lei",
      "Qiguang Chen",
      "Ming Li",
      "Yao Mu",
      "Hongyuan Zhang",
      "Wenqi Shao",
      "Ping Luo"
    ],
    "abstract": "Recently, there has been growing interest in leveraging large language models\n(LLMs) to generate symbolic world models from textual descriptions. Although\nLLMs have been extensively explored in the context of world modeling, prior\nstudies encountered several challenges, including evaluation randomness,\ndependence on indirect metrics, and a limited domain scope. To address these\nlimitations, we introduce a novel benchmark, Text2World, based on planning\ndomain definition language (PDDL), featuring hundreds of diverse domains and\nemploying multi-criteria, execution-based metrics for a more robust evaluation.\nWe benchmark current LLMs using Text2World and find that reasoning models\ntrained with large-scale reinforcement learning outperform others. However,\neven the best-performing model still demonstrates limited capabilities in world\nmodeling. Building on these insights, we examine several promising strategies\nto enhance the world modeling capabilities of LLMs, including test-time\nscaling, agent training, and more. We hope that Text2World can serve as a\ncrucial resource, laying the groundwork for future research in leveraging LLMs\nas world models. The project page is available at\nhttps://text-to-world.github.io/.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Project page: https://text-to-world.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2502.13092v2",
    "published_date": "2025-02-18 17:59:48 UTC",
    "updated_date": "2025-02-24 15:59:04 UTC"
  },
  {
    "arxiv_id": "2502.13080v2",
    "title": "BOLIMES: Boruta and LIME optiMized fEature Selection for Gene Expression Classification",
    "authors": [
      "Bich-Chung Phan",
      "Thanh Ma",
      "Huu-Hoa Nguyen",
      "Thanh-Nghi Do"
    ],
    "abstract": "Gene expression classification is a pivotal yet challenging task in\nbioinformatics, primarily due to the high dimensionality of genomic data and\nthe risk of overfitting. To bridge this gap, we propose BOLIMES, a novel\nfeature selection algorithm designed to enhance gene expression classification\nby systematically refining the feature subset. Unlike conventional methods that\nrely solely on statistical ranking or classifier-specific selection, we\nintegrate the robustness of Boruta with the interpretability of LIME, ensuring\nthat only the most relevant and influential genes are retained. BOLIMES first\nemploys Boruta to filter out non-informative genes by comparing each feature\nagainst its randomized counterpart, thus preserving valuable information. It\nthen uses LIME to rank the remaining genes based on their local importance to\nthe classifier. Finally, an iterative classification evaluation determines the\noptimal feature subset by selecting the number of genes that maximizes\npredictive accuracy. By combining exhaustive feature selection with\ninterpretability-driven refinement, our solution effectively balances\ndimensionality reduction with high classification performance, offering a\npowerful solution for high-dimensional gene expression analysis.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages",
    "pdf_url": "http://arxiv.org/pdf/2502.13080v2",
    "published_date": "2025-02-18 17:33:41 UTC",
    "updated_date": "2025-02-26 08:36:40 UTC"
  },
  {
    "arxiv_id": "2502.13194v1",
    "title": "Conditional Max-Sum for Asynchronous Multiagent Decision Making",
    "authors": [
      "Dimitrios Troullinos",
      "Georgios Chalkiadakis",
      "Ioannis Papamichail",
      "Markos Papageorgiou"
    ],
    "abstract": "In this paper we present a novel approach for multiagent decision making in\ndynamic environments based on Factor Graphs and the Max-Sum algorithm,\nconsidering asynchronous variable reassignments and distributed message-passing\namong agents. Motivated by the challenging domain of lane-free traffic where\nautomated vehicles can communicate and coordinate as agents, we propose a more\nrealistic communication framework for Factor Graph formulations that satisfies\nthe above-mentioned restrictions, along with Conditional Max-Sum: an extension\nof Max-Sum with a revised message-passing process that is better suited for\nasynchronous settings. The overall application in lane-free traffic can be\nviewed as a hybrid system where the Factor Graph formulation undertakes the\nstrategic decision making of vehicles, that of desired lateral alignment in a\ncoordinated manner; and acts on top of a rule-based method we devise that\nprovides a structured representation of the lane-free environment for the\nfactors, while also handling the underlying control of vehicles regarding core\noperations and safety. Our experimental evaluation showcases the capabilities\nof the proposed framework in problems with intense coordination needs when\ncompared to a domain-specific baseline without communication, and an increased\nadeptness of Conditional Max-Sum with respect to the standard algorithm.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "Accepted Full Paper (Main Technical Track) - 24th International\n  Conference on Autonomous Agents and Multiagent Systems (AAMAS 2025). This\n  extended version includes the Appendix at the end",
    "pdf_url": "http://arxiv.org/pdf/2502.13194v1",
    "published_date": "2025-02-18 17:16:27 UTC",
    "updated_date": "2025-02-18 17:16:27 UTC"
  },
  {
    "arxiv_id": "2502.13069v1",
    "title": "Interactive Agents to Overcome Ambiguity in Software Engineering",
    "authors": [
      "Sanidhya Vijayvargiya",
      "Xuhui Zhou",
      "Akhila Yerukola",
      "Maarten Sap",
      "Graham Neubig"
    ],
    "abstract": "AI agents are increasingly being deployed to automate tasks, often based on\nambiguous and underspecified user instructions. Making unwarranted assumptions\nand failing to ask clarifying questions can lead to suboptimal outcomes, safety\nrisks due to tool misuse, and wasted computational resources. In this work, we\nstudy the ability of LLM agents to handle ambiguous instructions in interactive\ncode generation settings by evaluating proprietary and open-weight models on\ntheir performance across three key steps: (a) leveraging interactivity to\nimprove performance in ambiguous scenarios, (b) detecting ambiguity, and (c)\nasking targeted questions. Our findings reveal that models struggle to\ndistinguish between well-specified and underspecified instructions. However,\nwhen models interact for underspecified inputs, they effectively obtain vital\ninformation from the user, leading to significant improvements in performance\nand underscoring the value of effective interaction. Our study highlights\ncritical gaps in how current state-of-the-art models handle ambiguity in\ncomplex software engineering tasks and structures the evaluation into distinct\nsteps to enable targeted improvements.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "15 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.13069v1",
    "published_date": "2025-02-18 17:12:26 UTC",
    "updated_date": "2025-02-18 17:12:26 UTC"
  },
  {
    "arxiv_id": "2502.13062v1",
    "title": "AI-Assisted Decision Making with Human Learning",
    "authors": [
      "Gali Noti",
      "Kate Donahue",
      "Jon Kleinberg",
      "Sigal Oren"
    ],
    "abstract": "AI systems increasingly support human decision-making. In many cases, despite\nthe algorithm's superior performance, the final decision remains in human\nhands. For example, an AI may assist doctors in determining which diagnostic\ntests to run, but the doctor ultimately makes the diagnosis. This paper studies\nsuch AI-assisted decision-making settings, where the human learns through\nrepeated interactions with the algorithm. In our framework, the algorithm --\ndesigned to maximize decision accuracy according to its own model -- determines\nwhich features the human can consider. The human then makes a prediction based\non their own less accurate model. We observe that the discrepancy between the\nalgorithm's model and the human's model creates a fundamental tradeoff. Should\nthe algorithm prioritize recommending more informative features, encouraging\nthe human to recognize their importance, even if it results in less accurate\npredictions in the short term until learning occurs? Or is it preferable to\nforgo educating the human and instead select features that align more closely\nwith their existing understanding, minimizing the immediate cost of learning?\nThis tradeoff is shaped by the algorithm's time-discounted objective and the\nhuman's learning ability. Our results show that optimal feature selection has a\nsurprisingly clean combinatorial characterization, reducible to a stationary\nsequence of feature subsets that is tractable to compute. As the algorithm\nbecomes more \"patient\" or the human's learning improves, the algorithm\nincreasingly selects more informative features, enhancing both prediction\naccuracy and the human's understanding. Notably, early investment in learning\nleads to the selection of more informative features than a later investment. We\ncomplement our analysis by showing that the impact of errors in the algorithm's\nknowledge is limited as it does not make the prediction directly.",
    "categories": [
      "cs.AI",
      "cs.GT",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.13062v1",
    "published_date": "2025-02-18 17:08:21 UTC",
    "updated_date": "2025-02-18 17:08:21 UTC"
  },
  {
    "arxiv_id": "2502.13061v2",
    "title": "Robust Adaptation of Large Multimodal Models for Retrieval Augmented Hateful Meme Detection",
    "authors": [
      "Jingbiao Mei",
      "Jinghong Chen",
      "Guangyu Yang",
      "Weizhe Lin",
      "Bill Byrne"
    ],
    "abstract": "Hateful memes have become a significant concern on the Internet,\nnecessitating robust automated detection systems. While LMMs have shown promise\nin hateful meme detection, they face notable challenges like sub-optimal\nperformance and limited out-of-domain generalization capabilities. Recent\nstudies further reveal the limitations of both SFT and in-context learning when\napplied to LMMs in this setting. To address these issues, we propose a robust\nadaptation framework for hateful meme detection that enhances in-domain\naccuracy and cross-domain generalization while preserving the general\nvision-language capabilities of LMMs. Experiments on six meme classification\ndatasets show that our approach achieves state-of-the-art performance,\noutperforming larger agentic systems. Moreover, our method generates\nhigher-quality rationales for explaining hateful content compared to standard\nSFT, enhancing model interpretability.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint. Under Review",
    "pdf_url": "http://arxiv.org/pdf/2502.13061v2",
    "published_date": "2025-02-18 17:07:29 UTC",
    "updated_date": "2025-05-19 22:27:41 UTC"
  },
  {
    "arxiv_id": "2502.13055v2",
    "title": "LAMD: Context-driven Android Malware Detection and Classification with LLMs",
    "authors": [
      "Xingzhi Qian",
      "Xinran Zheng",
      "Yiling He",
      "Shuo Yang",
      "Lorenzo Cavallaro"
    ],
    "abstract": "The rapid growth of mobile applications has escalated Android malware\nthreats. Although there are numerous detection methods, they often struggle\nwith evolving attacks, dataset biases, and limited explainability. Large\nLanguage Models (LLMs) offer a promising alternative with their zero-shot\ninference and reasoning capabilities. However, applying LLMs to Android malware\ndetection presents two key challenges: (1)the extensive support code in Android\napplications, often spanning thousands of classes, exceeds LLMs' context limits\nand obscures malicious behavior within benign functionality; (2)the structural\ncomplexity and interdependencies of Android applications surpass LLMs'\nsequence-based reasoning, fragmenting code analysis and hindering malicious\nintent inference. To address these challenges, we propose LAMD, a practical\ncontext-driven framework to enable LLM-based Android malware detection. LAMD\nintegrates key context extraction to isolate security-critical code regions and\nconstruct program structures, then applies tier-wise code reasoning to analyze\napplication behavior progressively, from low-level instructions to high-level\nsemantics, providing final prediction and explanation. A well-designed factual\nconsistency verification mechanism is equipped to mitigate LLM hallucinations\nfrom the first tier. Evaluation in real-world settings demonstrates LAMD's\neffectiveness over conventional detectors, establishing a feasible basis for\nLLM-driven malware analysis in dynamic threat landscapes.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "accepted by 2025 46th IEEE Symposium on Security and Privacy\n  Workshops (SPW)",
    "pdf_url": "http://arxiv.org/pdf/2502.13055v2",
    "published_date": "2025-02-18 17:01:37 UTC",
    "updated_date": "2025-04-21 18:09:35 UTC"
  },
  {
    "arxiv_id": "2502.15794v1",
    "title": "Self-Supervised Transformers as Iterative Solution Improvers for Constraint Satisfaction",
    "authors": [
      "Yudong W. Xu",
      "Wenhao Li",
      "Scott Sanner",
      "Elias B. Khalil"
    ],
    "abstract": "We present a Transformer-based framework for Constraint Satisfaction Problems\n(CSPs). CSPs find use in many applications and thus accelerating their solution\nwith machine learning is of wide interest. Most existing approaches rely on\nsupervised learning from feasible solutions or reinforcement learning,\nparadigms that require either feasible solutions to these NP-Complete CSPs or\nlarge training budgets and a complex expert-designed reward signal. To address\nthese challenges, we propose ConsFormer, a self-supervised framework that\nleverages a Transformer as a solution refiner. ConsFormer constructs a solution\nto a CSP iteratively in a process that mimics local search. Instead of using\nfeasible solutions as labeled data, we devise differentiable approximations to\nthe discrete constraints of a CSP to guide model training. Our model is trained\nto improve random assignments for a single step but is deployed iteratively at\ntest time, circumventing the bottlenecks of supervised and reinforcement\nlearning. Our method can tackle out-of-distribution CSPs simply through\nadditional iterations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.LO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.15794v1",
    "published_date": "2025-02-18 16:51:01 UTC",
    "updated_date": "2025-02-18 16:51:01 UTC"
  },
  {
    "arxiv_id": "2502.17487v2",
    "title": "User Intent to Use DeepSeek for Healthcare Purposes and their Trust in the Large Language Model: Multinational Survey Study",
    "authors": [
      "Avishek Choudhury",
      "Yeganeh Shahsavar",
      "Hamid Shamszare"
    ],
    "abstract": "Large language models (LLMs) increasingly serve as interactive healthcare\nresources, yet user acceptance remains underexplored. This study examines how\nease of use, perceived usefulness, trust, and risk perception interact to shape\nintentions to adopt DeepSeek, an emerging LLM-based platform, for healthcare\npurposes. A cross-sectional survey of 556 participants from India, the United\nKingdom, and the United States was conducted to measure perceptions and usage\npatterns. Structural equation modeling assessed both direct and indirect\neffects, including potential quadratic relationships. Results revealed that\ntrust plays a pivotal mediating role: ease of use exerts a significant indirect\neffect on usage intentions through trust, while perceived usefulness\ncontributes to both trust development and direct adoption. By contrast, risk\nperception negatively affects usage intent, emphasizing the importance of\nrobust data governance and transparency. Notably, significant non-linear paths\nwere observed for ease of use and risk, indicating threshold or plateau\neffects. The measurement model demonstrated strong reliability and validity,\nsupported by high composite reliabilities, average variance extracted, and\ndiscriminant validity measures. These findings extend technology acceptance and\nhealth informatics research by illuminating the multifaceted nature of user\nadoption in sensitive domains. Stakeholders should invest in trust-building\nstrategies, user-centric design, and risk mitigation measures to encourage\nsustained and safe uptake of LLMs in healthcare. Future work can employ\nlongitudinal designs or examine culture-specific variables to further clarify\nhow user perceptions evolve over time and across different regulatory\nenvironments. Such insights are critical for harnessing AI to enhance outcomes.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.17487v2",
    "published_date": "2025-02-18 16:49:36 UTC",
    "updated_date": "2025-03-02 06:05:40 UTC"
  },
  {
    "arxiv_id": "2502.13034v1",
    "title": "Natural Language Generation from Visual Sequences: Challenges and Future Directions",
    "authors": [
      "Aditya K Surikuchi",
      "Raquel Fernández",
      "Sandro Pezzelle"
    ],
    "abstract": "The ability to use natural language to talk about visual content is at the\ncore of human intelligence and a crucial feature of any artificial intelligence\nsystem. Various studies have focused on generating text for single images. In\ncontrast, comparatively little attention has been paid to exhaustively\nanalyzing and advancing work on multiple-image vision-to-text settings. In this\nposition paper, we claim that any task dealing with temporally ordered\nsequences of multiple images or frames is an instance of a broader, more\ngeneral problem involving the understanding of intricate relationships between\nthe visual content and the corresponding text. We comprehensively analyze five\ntasks that are instances of this problem and argue that they pose a common set\nof challenges and share similarities in terms of modeling and evaluation\napproaches. Based on the insights from these various aspects and stages of\nmulti-image-to-text generation, we highlight several open questions and suggest\nfuture research directions. We believe that these directions can advance the\nunderstanding of complex phenomena in this domain and the development of better\nmodels.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.13034v1",
    "published_date": "2025-02-18 16:48:18 UTC",
    "updated_date": "2025-02-18 16:48:18 UTC"
  },
  {
    "arxiv_id": "2503.05728v1",
    "title": "Political Neutrality in AI is Impossible- But Here is How to Approximate it",
    "authors": [
      "Jillian Fisher",
      "Ruth E. Appel",
      "Chan Young Park",
      "Yujin Potter",
      "Liwei Jiang",
      "Taylor Sorensen",
      "Shangbin Feng",
      "Yulia Tsvetkov",
      "Margaret E. Roberts",
      "Jennifer Pan",
      "Dawn Song",
      "Yejin Choi"
    ],
    "abstract": "AI systems often exhibit political bias, influencing users' opinions and\ndecision-making. While political neutrality-defined as the absence of bias-is\noften seen as an ideal solution for fairness and safety, this position paper\nargues that true political neutrality is neither feasible nor universally\ndesirable due to its subjective nature and the biases inherent in AI training\ndata, algorithms, and user interactions. However, inspired by Joseph Raz's\nphilosophical insight that \"neutrality [...] can be a matter of degree\" (Raz,\n1986), we argue that striving for some neutrality remains essential for\npromoting balanced AI interactions and mitigating user manipulation. Therefore,\nwe use the term \"approximation\" of political neutrality to shift the focus from\nunattainable absolutes to achievable, practical proxies. We propose eight\ntechniques for approximating neutrality across three levels of conceptualizing\nAI, examining their trade-offs and implementation strategies. In addition, we\nexplore two concrete applications of these approximations to illustrate their\npracticality. Finally, we assess our framework on current large language models\n(LLMs) at the output level, providing a demonstration of how it can be\nevaluated. This work seeks to advance nuanced discussions of political\nneutrality in AI and promote the development of responsible, aligned language\nmodels.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "Code: https://github.com/jfisher52/Approximation_Political_Neutrality",
    "pdf_url": "http://arxiv.org/pdf/2503.05728v1",
    "published_date": "2025-02-18 16:48:04 UTC",
    "updated_date": "2025-02-18 16:48:04 UTC"
  },
  {
    "arxiv_id": "2502.13030v1",
    "title": "Likelihood-Ratio Regularized Quantile Regression: Adapting Conformal Prediction to High-Dimensional Covariate Shifts",
    "authors": [
      "Sunay Joshi",
      "Shayan Kiyani",
      "George Pappas",
      "Edgar Dobriban",
      "Hamed Hassani"
    ],
    "abstract": "We consider the problem of conformal prediction under covariate shift. Given\nlabeled data from a source domain and unlabeled data from a covariate shifted\ntarget domain, we seek to construct prediction sets with valid marginal\ncoverage in the target domain. Most existing methods require estimating the\nunknown likelihood ratio function, which can be prohibitive for\nhigh-dimensional data such as images. To address this challenge, we introduce\nthe likelihood ratio regularized quantile regression (LR-QR) algorithm, which\ncombines the pinball loss with a novel choice of regularization in order to\nconstruct a threshold function without directly estimating the unknown\nlikelihood ratio. We show that the LR-QR method has coverage at the desired\nlevel in the target domain, up to a small error term that we can control. Our\nproofs draw on a novel analysis of coverage via stability bounds from learning\ntheory. Our experiments demonstrate that the LR-QR algorithm outperforms\nexisting methods on high-dimensional prediction tasks, including a regression\ntask for the Communities and Crime dataset, and an image classification task\nfrom the WILDS repository.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.13030v1",
    "published_date": "2025-02-18 16:46:44 UTC",
    "updated_date": "2025-02-18 16:46:44 UTC"
  },
  {
    "arxiv_id": "2502.14905v1",
    "title": "Think Inside the JSON: Reinforcement Strategy for Strict LLM Schema Adherence",
    "authors": [
      "Bhavik Agarwal",
      "Ishan Joshi",
      "Viktoria Rojkova"
    ],
    "abstract": "In this paper, we address the challenge of enforcing strict schema adherence\nin large language model (LLM) generation by leveraging LLM reasoning\ncapabilities. Building on the DeepSeek R1 reinforcement learning framework, our\napproach trains structured reasoning skills of a 1.5B parameter model through a\nnovel pipeline that combines synthetic reasoning dataset construction with\ncustom reward functions under Group Relative Policy Optimization (GRPO).\nSpecifically, we first perform R1 reinforcement learning on a 20K sample\nunstructured-to-structured dataset, mirroring the original DeepSeek R1 methods,\nto establish core reasoning abilities. Subsequently, we performed supervised\nfine-tuning on a separate 10K reasoning sample dataset, focusing on refining\nschema adherence for downstream tasks. Despite the relatively modest training\nscope, requiring approximately 20 hours on an 8xH100 GPU cluster for GRPO\ntraining and 3 hours on 1xA100 for SFT, our model demonstrates robust\nperformance in enforcing schema consistency. We compare our ThinkJSON approach\nagainst the original DeepSeek R1 (671B), distilled versions of DeepSeek R1\n(Qwen-1.5B and Qwen-7B), and Gemini 2.0 Flash (70B), showcasing its\neffectiveness in real-world applications. Our results underscore the practical\nutility of a resource-efficient framework for schema-constrained text\ngeneration.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14905v1",
    "published_date": "2025-02-18 16:44:55 UTC",
    "updated_date": "2025-02-18 16:44:55 UTC"
  },
  {
    "arxiv_id": "2502.13025v1",
    "title": "Agentic Deep Graph Reasoning Yields Self-Organizing Knowledge Networks",
    "authors": [
      "Markus J. Buehler"
    ],
    "abstract": "We present an agentic, autonomous graph expansion framework that iteratively\nstructures and refines knowledge in situ. Unlike conventional knowledge graph\nconstruction methods relying on static extraction or single-pass learning, our\napproach couples a reasoning-native large language model with a continually\nupdated graph representation. At each step, the system actively generates new\nconcepts and relationships, merges them into a global graph, and formulates\nsubsequent prompts based on its evolving structure. Through this\nfeedback-driven loop, the model organizes information into a scale-free network\ncharacterized by hub formation, stable modularity, and bridging nodes that link\ndisparate knowledge clusters. Over hundreds of iterations, new nodes and edges\ncontinue to appear without saturating, while centrality measures and shortest\npath distributions evolve to yield increasingly distributed connectivity. Our\nanalysis reveals emergent patterns, such as the rise of highly connected 'hub'\nconcepts and the shifting influence of 'bridge' nodes, indicating that agentic,\nself-reinforcing graph construction can yield open-ended, coherent knowledge\nstructures. Applied to materials design problems, we present compositional\nreasoning experiments by extracting node-specific and synergy-level principles\nto foster genuinely novel knowledge synthesis, yielding cross-domain ideas that\ntranscend rote summarization and strengthen the framework's potential for\nopen-ended scientific discovery. We discuss other applications in scientific\ndiscovery and outline future directions for enhancing scalability and\ninterpretability.",
    "categories": [
      "cs.AI",
      "cond-mat.mtrl-sci",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.13025v1",
    "published_date": "2025-02-18 16:44:42 UTC",
    "updated_date": "2025-02-18 16:44:42 UTC"
  },
  {
    "arxiv_id": "2502.13016v1",
    "title": "LLM-Powered Proactive Data Systems",
    "authors": [
      "Sepanta Zeighami",
      "Yiming Lin",
      "Shreya Shankar",
      "Aditya Parameswaran"
    ],
    "abstract": "With the power of LLMs, we now have the ability to query data that was\npreviously impossible to query, including text, images, and video. However,\ndespite this enormous potential, most present-day data systems that leverage\nLLMs are reactive, reflecting our community's desire to map LLMs to known\nabstractions. Most data systems treat LLMs as an opaque black box that operates\non user inputs and data as is, optimizing them much like any other approximate,\nexpensive UDFs, in conjunction with other relational operators. Such data\nsystems do as they are told, but fail to understand and leverage what the LLM\nis being asked to do (i.e. the underlying operations, which may be\nerror-prone), the data the LLM is operating on (e.g., long, complex documents),\nor what the user really needs. They don't take advantage of the characteristics\nof the operations and/or the data at hand, or ensure correctness of results\nwhen there are imprecisions and ambiguities. We argue that data systems instead\nneed to be proactive: they need to be given more agency -- armed with the power\nof LLMs -- to understand and rework the user inputs and the data and to make\ndecisions on how the operations and the data should be represented and\nprocessed. By allowing the data system to parse, rewrite, and decompose user\ninputs and data, or to interact with the user in ways that go beyond the\nstandard single-shot query-result paradigm, the data system is able to address\nuser needs more efficiently and effectively. These new capabilities lead to a\nrich design space where the data system takes more initiative: they are\nempowered to perform optimization based on the transformation operations, data\ncharacteristics, and user intent. We discuss various successful examples of how\nthis framework has been and can be applied in real-world tasks, and present\nfuture directions for this ambitious research agenda.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.13016v1",
    "published_date": "2025-02-18 16:34:45 UTC",
    "updated_date": "2025-02-18 16:34:45 UTC"
  },
  {
    "arxiv_id": "2502.13013v2",
    "title": "HOMIE: Humanoid Loco-Manipulation with Isomorphic Exoskeleton Cockpit",
    "authors": [
      "Qingwei Ben",
      "Feiyu Jia",
      "Jia Zeng",
      "Junting Dong",
      "Dahua Lin",
      "Jiangmiao Pang"
    ],
    "abstract": "Generalizable humanoid loco-manipulation poses significant challenges,\nrequiring coordinated whole-body control and precise, contact-rich object\nmanipulation. To address this, this paper introduces HOMIE, a semi-autonomous\nteleoperation system that combines a reinforcement learning policy for body\ncontrol mapped to a pedal, an isomorphic exoskeleton arm for arm control, and\nmotion-sensing gloves for hand control, forming a unified cockpit to freely\noperate humanoids and establish a data flywheel. The policy incorporates novel\ndesigns, including an upper-body pose curriculum, a height-tracking reward, and\nsymmetry utilization. These features enable the system to perform walking and\nsquatting to specific heights while seamlessly adapting to arbitrary upper-body\nposes. The exoskeleton, by eliminating the reliance on inverse dynamics,\ndelivers faster and more precise arm control. The gloves utilize Hall sensors\ninstead of servos, allowing even compact devices to achieve 15 or more degrees\nof freedom and freely adapt to any model of dexterous hands. Compared to\nprevious teleoperation systems, HOMIE stands out for its exceptional\nefficiency, completing tasks in half the time; its expanded working range,\nallowing users to freely reach high and low areas as well as interact with any\nobjects; and its affordability, with a price of just $500. The system is fully\nopen-source, demos and code can be found in our https://homietele.github.io/.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.13013v2",
    "published_date": "2025-02-18 16:33:38 UTC",
    "updated_date": "2025-04-28 09:21:56 UTC"
  },
  {
    "arxiv_id": "2502.13006v1",
    "title": "Integrating Reinforcement Learning, Action Model Learning, and Numeric Planning for Tackling Complex Tasks",
    "authors": [
      "Yarin Benyamin",
      "Argaman Mordoch",
      "Shahaf S. Shperberg",
      "Roni Stern"
    ],
    "abstract": "Automated Planning algorithms require a model of the domain that specifies\nthe preconditions and effects of each action. Obtaining such a domain model is\nnotoriously hard. Algorithms for learning domain models exist, yet it remains\nunclear whether learning a domain model and planning is an effective approach\nfor numeric planning environments, i.e., where states include discrete and\nnumeric state variables. In this work, we explore the benefits of learning a\nnumeric domain model and compare it with alternative model-free solutions. As a\ncase study, we use two tasks in Minecraft, a popular sandbox game that has been\nused as an AI challenge. First, we consider an offline learning setting, where\na set of expert trajectories are available to learn from. This is the standard\nsetting for learning domain models. We used the Numeric Safe Action Model\nLearning (NSAM) algorithm to learn a numeric domain model and solve new\nproblems with the learned domain model and a numeric planner. We call this\nmodel-based solution NSAM_(+p), and compare it to several model-free Imitation\nLearning (IL) and Offline Reinforcement Learning (RL) algorithms. Empirical\nresults show that some IL algorithms can learn faster to solve simple tasks,\nwhile NSAM_(+p) allows solving tasks that require long-term planning and\nenables generalizing to solve problems in larger environments. Then, we\nconsider an online learning setting, where learning is done by moving an agent\nin the environment. For this setting, we introduce RAMP. In RAMP, observations\ncollected during the agent's execution are used to simultaneously train an RL\npolicy and learn a planning domain action model. This forms a positive feedback\nloop between the RL policy and the learned domain model. We demonstrate\nexperimentally the benefits of using RAMP, showing that it finds more efficient\nplans and solves more problems than several RL baselines.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.13006v1",
    "published_date": "2025-02-18 16:26:21 UTC",
    "updated_date": "2025-02-18 16:26:21 UTC"
  },
  {
    "arxiv_id": "2502.13001v1",
    "title": "You need to MIMIC to get FAME: Solving Meeting Transcript Scarcity with a Multi-Agent Conversations",
    "authors": [
      "Frederic Kirstein",
      "Muneeb Khan",
      "Jan Philip Wahle",
      "Terry Ruas",
      "Bela Gipp"
    ],
    "abstract": "Meeting summarization suffers from limited high-quality data, mainly due to\nprivacy restrictions and expensive collection processes. We address this gap\nwith FAME, a dataset of 500 meetings in English and 300 in German produced by\nMIMIC, our new multi-agent meeting synthesis framework that generates meeting\ntranscripts on a given knowledge source by defining psychologically grounded\nparticipant profiles, outlining the conversation, and orchestrating a large\nlanguage model (LLM) debate. A modular post-processing step refines these\noutputs, mitigating potential repetitiveness and overly formal tones, ensuring\ncoherent, credible dialogues at scale. We also propose a psychologically\ngrounded evaluation framework assessing naturalness, social behavior\nauthenticity, and transcript difficulties. Human assessments show that FAME\napproximates real-meeting spontaneity (4.5/5 in naturalness), preserves\nspeaker-centric challenges (3/5 in spoken language), and introduces richer\ninformation-oriented difficulty (4/5 in difficulty). These findings highlight\nthat FAME is a good and scalable proxy for real-world meeting conditions. It\nenables new test scenarios for meeting summarization research and other\nconversation-centric applications in tasks requiring conversation data or\nsimulating social scenarios under behavioral constraints.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.13001v1",
    "published_date": "2025-02-18 16:21:22 UTC",
    "updated_date": "2025-02-18 16:21:22 UTC"
  },
  {
    "arxiv_id": "2502.12998v1",
    "title": "Personalized Top-k Set Queries Over Predicted Scores",
    "authors": [
      "Sohrab Namazi Nia",
      "Subhodeep Ghosh",
      "Senjuti Basu Roy",
      "Sihem Amer-Yahia"
    ],
    "abstract": "This work studies the applicability of expensive external oracles such as\nlarge language models in answering top-k queries over predicted scores. Such\nscores are incurred by user-defined functions to answer personalized queries\nover multi-modal data. We propose a generic computational framework that\nhandles arbitrary set-based scoring functions, as long as the functions could\nbe decomposed into constructs, each of which sent to an oracle (in our case an\nLLM) to predict partial scores. At a given point in time, the framework assumes\na set of responses and their partial predicted scores, and it maintains a\ncollection of possible sets that are likely to be the true top-k. Since calling\noracles is costly, our framework judiciously identifies the next construct,\ni.e., the next best question to ask the oracle so as to maximize the likelihood\nof identifying the true top-k. We present a principled probabilistic model that\nquantifies that likelihood. We study efficiency opportunities in designing\nalgorithms. We run an evaluation with three large scale datasets, scoring\nfunctions, and baselines. Experiments indicate the efficacy of our framework,\nas it achieves an order of magnitude improvement over baselines in requiring\nLLM calls while ensuring result accuracy. Scalability experiments further\nindicate that our framework could be used in large-scale applications.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12998v1",
    "published_date": "2025-02-18 16:19:08 UTC",
    "updated_date": "2025-02-18 16:19:08 UTC"
  },
  {
    "arxiv_id": "2502.12995v1",
    "title": "Free Argumentative Exchanges for Explaining Image Classifiers",
    "authors": [
      "Avinash Kori",
      "Antonio Rago",
      "Francesca Toni"
    ],
    "abstract": "Deep learning models are powerful image classifiers but their opacity hinders\ntheir trustworthiness. Explanation methods for capturing the reasoning process\nwithin these classifiers faithfully and in a clear manner are scarce, due to\ntheir sheer complexity and size. We provide a solution for this problem by\ndefining a novel method for explaining the outputs of image classifiers with\ndebates between two agents, each arguing for a particular class. We obtain\nthese debates as concrete instances of Free Argumentative eXchanges (FAXs), a\nnovel argumentation-based multi-agent framework allowing agents to internalise\nopinions by other agents differently than originally stated. We define two\nmetrics (consensus and persuasion rate) to assess the usefulness of FAXs as\nargumentative explanations for image classifiers. We then conduct a number of\nempirical experiments showing that FAXs perform well along these metrics as\nwell as being more faithful to the image classifiers than conventional,\nnon-argumentative explanation methods. All our implementations can be found at\nhttps://github.com/koriavinash1/FAX.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 3 figures. To be published at AAMAS 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.12995v1",
    "published_date": "2025-02-18 16:15:36 UTC",
    "updated_date": "2025-02-18 16:15:36 UTC"
  },
  {
    "arxiv_id": "2502.12992v1",
    "title": "B-cos LM: Efficiently Transforming Pre-trained Language Models for Improved Explainability",
    "authors": [
      "Yifan Wang",
      "Sukrut Rao",
      "Ji-Ung Lee",
      "Mayank Jobanputra",
      "Vera Demberg"
    ],
    "abstract": "Post-hoc explanation methods for black-box models often struggle with\nfaithfulness and human interpretability due to the lack of explainability in\ncurrent neural models. Meanwhile, B-cos networks have been introduced to\nimprove model explainability through architectural and computational\nadaptations, but their application has so far been limited to computer vision\nmodels and their associated training pipelines. In this work, we introduce\nB-cos LMs, i.e., B-cos networks empowered for NLP tasks. Our approach directly\ntransforms pre-trained language models into B-cos LMs by combining B-cos\nconversion and task fine-tuning, improving efficiency compared to previous\nB-cos methods. Our automatic and human evaluation results demonstrate that\nB-cos LMs produce more faithful and human interpretable explanations than post\nhoc methods, while maintaining task performance comparable to conventional\nfine-tuning. Our in-depth analysis explores how B-cos LMs differ from\nconventionally fine-tuned models in their learning processes and explanation\npatterns. Finally, we provide practical guidelines for effectively building\nB-cos LMs based on our findings. Our code is available at\nhttps://anonymous.4open.science/r/bcos_lm.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "20 pages, 15 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.12992v1",
    "published_date": "2025-02-18 16:13:08 UTC",
    "updated_date": "2025-02-18 16:13:08 UTC"
  },
  {
    "arxiv_id": "2502.12985v1",
    "title": "PartSDF: Part-Based Implicit Neural Representation for Composite 3D Shape Parametrization and Optimization",
    "authors": [
      "Nicolas Talabot",
      "Olivier Clerc",
      "Arda Cinar Demirtas",
      "Doruk Oner",
      "Pascal Fua"
    ],
    "abstract": "Accurate 3D shape representation is essential in engineering applications\nsuch as design, optimization, and simulation. In practice, engineering\nworkflows require structured, part-aware representations, as objects are\ninherently designed as assemblies of distinct components. However, most\nexisting methods either model shapes holistically or decompose them without\npredefined part structures, limiting their applicability in real-world design\ntasks. We propose PartSDF, a supervised implicit representation framework that\nexplicitly models composite shapes with independent, controllable parts while\nmaintaining shape consistency. Despite its simple single-decoder architecture,\nPartSDF outperforms both supervised and unsupervised baselines in\nreconstruction and generation tasks. We further demonstrate its effectiveness\nas a structured shape prior for engineering applications, enabling precise\ncontrol over individual components while preserving overall coherence. Code\navailable at https://github.com/cvlab-epfl/PartSDF.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "22 pages, 14 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.12985v1",
    "published_date": "2025-02-18 16:08:47 UTC",
    "updated_date": "2025-02-18 16:08:47 UTC"
  },
  {
    "arxiv_id": "2502.12982v1",
    "title": "Sailor2: Sailing in South-East Asia with Inclusive Multilingual LLMs",
    "authors": [
      "Longxu Dou",
      "Qian Liu",
      "Fan Zhou",
      "Changyu Chen",
      "Zili Wang",
      "Ziqi Jin",
      "Zichen Liu",
      "Tongyao Zhu",
      "Cunxiao Du",
      "Penghui Yang",
      "Haonan Wang",
      "Jiaheng Liu",
      "Yongchi Zhao",
      "Xiachong Feng",
      "Xin Mao",
      "Man Tsung Yeung",
      "Kunat Pipatanakul",
      "Fajri Koto",
      "Min Si Thu",
      "Hynek Kydlíček",
      "Zeyi Liu",
      "Qunshu Lin",
      "Sittipong Sripaisarnmongkol",
      "Kridtaphad Sae-Khow",
      "Nirattisai Thongchim",
      "Taechawat Konkaew",
      "Narong Borijindargoon",
      "Anh Dao",
      "Matichon Maneegard",
      "Phakphum Artkaew",
      "Zheng-Xin Yong",
      "Quan Nguyen",
      "Wannaphong Phatthiyaphaibun",
      "Hoang H. Tran",
      "Mike Zhang",
      "Shiqi Chen",
      "Tianyu Pang",
      "Chao Du",
      "Xinyi Wan",
      "Wei Lu",
      "Min Lin"
    ],
    "abstract": "Sailor2 is a family of cutting-edge multilingual language models for\nSouth-East Asian (SEA) languages, available in 1B, 8B, and 20B sizes to suit\ndiverse applications. Building on Qwen2.5, Sailor2 undergoes continuous\npre-training on 500B tokens (400B SEA-specific and 100B replay tokens) to\nsupport 13 SEA languages while retaining proficiency in Chinese and English.\nSailor2-20B model achieves a 50-50 win rate against GPT-4o across SEA\nlanguages. We also deliver a comprehensive cookbook on how to develop the\nmultilingual model in an efficient manner, including five key aspects: data\ncuration, pre-training, post-training, model customization and evaluation. We\nhope that Sailor2 model (Apache 2.0 license) will drive language development in\nthe SEA region, and Sailor2 cookbook will inspire researchers to build more\ninclusive LLMs for other under-served languages.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "49 pages, 16 figures. Technical Report of Sailor2:\n  https://sea-sailor.github.io/blog/sailor2/",
    "pdf_url": "http://arxiv.org/pdf/2502.12982v1",
    "published_date": "2025-02-18 16:04:57 UTC",
    "updated_date": "2025-02-18 16:04:57 UTC"
  },
  {
    "arxiv_id": "2502.15791v1",
    "title": "Learning-Guided Rolling Horizon Optimization for Long-Horizon Flexible Job-Shop Scheduling",
    "authors": [
      "Sirui Li",
      "Wenbin Ouyang",
      "Yining Ma",
      "Cathy Wu"
    ],
    "abstract": "Long-horizon combinatorial optimization problems (COPs), such as the Flexible\nJob-Shop Scheduling Problem (FJSP), often involve complex, interdependent\ndecisions over extended time frames, posing significant challenges for existing\nsolvers. While Rolling Horizon Optimization (RHO) addresses this by decomposing\nproblems into overlapping shorter-horizon subproblems, such overlap often\ninvolves redundant computations. In this paper, we present L-RHO, the first\nlearning-guided RHO framework for COPs. L-RHO employs a neural network to\nintelligently fix variables that in hindsight did not need to be re-optimized,\nresulting in smaller and thus easier-to-solve subproblems. For FJSP, this means\nidentifying operations with unchanged machine assignments between consecutive\nsubproblems. Applied to FJSP, L-RHO accelerates RHO by up to 54% while\nsignificantly improving solution quality, outperforming other heuristic and\nlearning-based baselines. We also provide in-depth discussions and verify the\ndesirable adaptability and generalization of L-RHO across numerous FJSP\nvariates, distributions, online scenarios and benchmark instances. Moreover, we\nprovide a theoretical analysis to elucidate the conditions under which learning\nis beneficial.",
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "math.OC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.15791v1",
    "published_date": "2025-02-18 15:54:54 UTC",
    "updated_date": "2025-02-18 15:54:54 UTC"
  },
  {
    "arxiv_id": "2502.15790v1",
    "title": "Signal Collapse in One-Shot Pruning: When Sparse Models Fail to Distinguish Neural Representations",
    "authors": [
      "Dhananjay Saikumar",
      "Blesson Varghese"
    ],
    "abstract": "Neural network pruning is essential for reducing model complexity to enable\ndeployment on resource constrained hardware. While performance loss of pruned\nnetworks is often attributed to the removal of critical parameters, we identify\nsignal collapse a reduction in activation variance across layers as the root\ncause. Existing one shot pruning methods focus on weight selection strategies\nand rely on computationally expensive second order approximations. In contrast,\nwe demonstrate that mitigating signal collapse, rather than optimizing weight\nselection, is key to improving accuracy of pruned networks. We propose REFLOW\nthat addresses signal collapse without updating trainable weights, revealing\nhigh quality sparse sub networks within the original parameter space. REFLOW\nenables magnitude pruning to achieve state of the art performance, restoring\nResNeXt101 accuracy from under 4.1% to 78.9% on ImageNet with only 20% of the\nweights retained, surpassing state of the art approaches.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.15790v1",
    "published_date": "2025-02-18 15:47:33 UTC",
    "updated_date": "2025-02-18 15:47:33 UTC"
  },
  {
    "arxiv_id": "2502.12965v1",
    "title": "A Survey of Text Classification Under Class Distribution Shift",
    "authors": [
      "Adriana Valentina Costache",
      "Silviu Florin Gheorghe",
      "Eduard Gabriel Poesina",
      "Paul Irofti",
      "Radu Tudor Ionescu"
    ],
    "abstract": "The basic underlying assumption of machine learning (ML) models is that the\ntraining and test data are sampled from the same distribution. However, in\ndaily practice, this assumption is often broken, i.e.~the distribution of the\ntest data changes over time, which hinders the application of conventional ML\nmodels. One domain where the distribution shift naturally occurs is text\nclassification, since people always find new topics to discuss. To this end, we\nsurvey research articles studying open-set text classification and related\ntasks. We divide the methods in this area based on the constraints that define\nthe kind of distribution shift and the corresponding problem formulation,\ni.e.~learning with the Universum, zero-shot learning, and open-set learning. We\nnext discuss the predominant mitigation approaches for each problem setup.\nFinally, we identify several future work directions, aiming to push the\nboundaries beyond the state of the art. Interestingly, we find that continual\nlearning can solve many of the issues caused by the shifting class\ndistribution. We maintain a list of relevant papers at\nhttps://github.com/Eduard6421/Open-Set-Survey.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12965v1",
    "published_date": "2025-02-18 15:46:54 UTC",
    "updated_date": "2025-02-18 15:46:54 UTC"
  },
  {
    "arxiv_id": "2502.12961v1",
    "title": "Adaptive Tool Use in Large Language Models with Meta-Cognition Trigger",
    "authors": [
      "Wenjun Li",
      "Dexun Li",
      "Kuicai Dong",
      "Cong Zhang",
      "Hao Zhang",
      "Weiwen Liu",
      "Yasheng Wang",
      "Ruiming Tang",
      "Yong Liu"
    ],
    "abstract": "Large language models (LLMs) have shown remarkable emergent capabilities,\ntransforming the execution of functional tasks by leveraging external tools for\ncomplex problems that require specialized processing or real-time data. While\nexisting research expands LLMs access to diverse tools (e.g., program\ninterpreters, search engines, weather/map apps), the necessity of using these\ntools is often overlooked, leading to indiscriminate tool invocation. This\nnaive approach raises two key issues:(1) increased delays due to unnecessary\ntool calls, and (2) potential errors resulting from faulty interactions with\nexternal tools. In this paper, we introduce meta-cognition as a proxy for LLMs\nself-assessment of their capabilities, representing the model's awareness of\nits own limitations. Based on this, we propose MeCo, an adaptive\ndecision-making strategy for external tool use. MeCo quantifies metacognitive\nscores by capturing high-level cognitive signals in the representation space,\nguiding when to invoke tools. Notably, MeCo is fine-tuning-free and incurs\nminimal cost. Our experiments show that MeCo accurately detects LLMs' internal\ncognitive signals and significantly improves tool-use decision-making across\nmultiple base models and benchmarks.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12961v1",
    "published_date": "2025-02-18 15:45:01 UTC",
    "updated_date": "2025-02-18 15:45:01 UTC"
  },
  {
    "arxiv_id": "2502.12959v1",
    "title": "AlignFreeze: Navigating the Impact of Realignment on the Layers of Multilingual Models Across Diverse Languages",
    "authors": [
      "Steve Bakos",
      "Félix Gaschi",
      "David Guzmán",
      "Riddhi More",
      "Kelly Chutong Li",
      "En-Shiun Annie Lee"
    ],
    "abstract": "Realignment techniques are often employed to enhance cross-lingual transfer\nin multilingual language models, still, they can sometimes degrade performance\nin languages that differ significantly from the fine-tuned source language.\nThis paper introduces AlignFreeze, a method that freezes either the layers'\nlower half or upper half during realignment. Through controlled experiments on\n4 tasks, 3 models, and in 35 languages, we find that realignment affects all\nthe layers but can be the most detrimental to the lower ones. Freezing the\nlower layers can prevent performance degradation. Particularly, AlignFreeze\nimproves Part-of-Speech (PoS) tagging performances in languages where full\nrealignment fails: with XLM-R, it provides improvements of more than one\nstandard deviation in accuracy in seven more languages than full realignment.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "24 pages, 2 figures, to be published in Proceedings of NAACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.12959v1",
    "published_date": "2025-02-18 15:43:27 UTC",
    "updated_date": "2025-02-18 15:43:27 UTC"
  },
  {
    "arxiv_id": "2502.12953v1",
    "title": "Task-Informed Anti-Curriculum by Masking Improves Downstream Performance on Text",
    "authors": [
      "Andrei Jarca",
      "Florinel Alin Croitoru",
      "Radu Tudor Ionescu"
    ],
    "abstract": "Masked language modeling has become a widely adopted unsupervised technique\nto pre-train language models. However, the process of selecting tokens for\nmasking is random, and the percentage of masked tokens is typically fixed for\nthe entire training process. In this paper, we propose to adjust the masking\nratio and to decide which tokens to mask based on a novel task-informed\nanti-curriculum learning scheme. First, we harness task-specific knowledge\nabout useful and harmful tokens in order to determine which tokens to mask.\nSecond, we propose a cyclic decaying masking ratio, which corresponds to an\nanti-curriculum schedule (from hard to easy). We exemplify our novel\ntask-informed anti-curriculum by masking (TIACBM) approach across three diverse\ndownstream tasks: sentiment analysis, text classification by topic, and\nauthorship attribution. Our findings suggest that TIACBM enhances the ability\nof the model to focus on key task-relevant features, contributing to\nstatistically significant performance gains across tasks. We release our code\nat https://github.com/JarcaAndrei/TIACBM.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12953v1",
    "published_date": "2025-02-18 15:36:16 UTC",
    "updated_date": "2025-02-18 15:36:16 UTC"
  },
  {
    "arxiv_id": "2502.12948v1",
    "title": "Fake It Till You Make It: Using Synthetic Data and Domain Knowledge for Improved Text-Based Learning for LGE Detection",
    "authors": [
      "Athira J Jacob",
      "Puneet Sharma",
      "Daniel Rueckert"
    ],
    "abstract": "Detection of hyperenhancement from cardiac LGE MRI images is a complex task\nrequiring significant clinical expertise. Although deep learning-based models\nhave shown promising results for the task, they require large amounts of data\nwith fine-grained annotations. Clinical reports generated for cardiac MR\nstudies contain rich, clinically relevant information, including the location,\nextent and etiology of any scars present. Although recently developed\nCLIP-based training enables pretraining models with image-text pairs, it\nrequires large amounts of data and further finetuning strategies on downstream\ntasks. In this study, we use various strategies rooted in domain knowledge to\ntrain a model for LGE detection solely using text from clinical reports, on a\nrelatively small clinical cohort of 965 patients. We improve performance\nthrough the use of synthetic data augmentation, by systematically creating scar\nimages and associated text. In addition, we standardize the orientation of the\nimages in an anatomy-informed way to enable better alignment of spatial and\ntext features. We also use a captioning loss to enable fine-grained supervision\nand explore the effect of pretraining of the vision encoder on performance.\nFinally, ablation studies are carried out to elucidate the contributions of\neach design component to the overall performance of the model.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Poster at Workshop on Large Language Models and Generative AI for\n  Health at AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.12948v1",
    "published_date": "2025-02-18 15:30:48 UTC",
    "updated_date": "2025-02-18 15:30:48 UTC"
  },
  {
    "arxiv_id": "2502.12947v1",
    "title": "Every Expert Matters: Towards Effective Knowledge Distillation for Mixture-of-Experts Language Models",
    "authors": [
      "Gyeongman Kim",
      "Gyouk Chu",
      "Eunho Yang"
    ],
    "abstract": "With the emergence of Mixture-of-Experts (MoE), the efficient scaling of\nmodel size has accelerated the development of large language models in recent\nyears. However, their high memory requirements prevent their use in\nresource-constrained environments. While knowledge distillation (KD) has been a\nproven method for model compression, its application to MoE teacher models\nremains underexplored. Through our investigation, we discover that\nnon-activated experts in MoE models possess valuable knowledge that benefits\nstudent models. We further demonstrate that existing KD methods are not optimal\nfor compressing MoE models, as they fail to leverage this knowledge\neffectively. To address this, we propose two intuitive MoE-specific KD methods\nfor the first time: Knowledge Augmentation (KA) and Student-Aware Router (SAR),\nboth designed to effectively extract knowledge from all experts. Specifically,\nKA augments knowledge by sampling experts multiple times, while SAR uses all\nexperts and adjusts the expert weights through router training to provide\noptimal knowledge. Extensive experiments show that our methods outperform\nconventional KD methods, demonstrating their effectiveness for MoE teacher\nmodels.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12947v1",
    "published_date": "2025-02-18 15:30:34 UTC",
    "updated_date": "2025-02-18 15:30:34 UTC"
  },
  {
    "arxiv_id": "2502.13191v2",
    "title": "On the Privacy Risks of Spiking Neural Networks: A Membership Inference Analysis",
    "authors": [
      "Junyi Guan",
      "Abhijith Sharma",
      "Chong Tian",
      "Salem Lahlou"
    ],
    "abstract": "Spiking Neural Networks (SNNs) are increasingly explored for their energy\nefficiency and robustness in real-world applications, yet their privacy risks\nremain largely unexamined. In this work, we investigate the susceptibility of\nSNNs to Membership Inference Attacks (MIAs) -- a major privacy threat where an\nadversary attempts to determine whether a given sample was part of the training\ndataset. While prior work suggests that SNNs may offer inherent robustness due\nto their discrete, event-driven nature, we find that its resilience diminishes\nas latency (T) increases. Furthermore, we introduce an input dropout strategy\nunder black box setting, that significantly enhances membership inference in\nSNNs. Our findings challenge the assumption that SNNs are inherently more\nsecure, and even though they are expected to be better, our results reveal that\nSNNs exhibit privacy vulnerabilities that are equally comparable to Artificial\nNeural Networks (ANNs). Our code is available at\nhttps://anonymous.4open.science/r/MIA_SNN-3610.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "13 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.13191v2",
    "published_date": "2025-02-18 15:19:20 UTC",
    "updated_date": "2025-03-16 15:25:29 UTC"
  },
  {
    "arxiv_id": "2502.12929v1",
    "title": "Flow-of-Options: Diversified and Improved LLM Reasoning by Thinking Through Options",
    "authors": [
      "Lakshmi Nair",
      "Ian Trase",
      "Mark Kim"
    ],
    "abstract": "We present a novel reasoning approach called Flow-of-Options (FoO), designed\nto address intrinsic biases in Large Language Models (LLMs). FoO enables LLMs\nto systematically explore a diverse range of possibilities in their reasoning,\nas demonstrated by an FoO-based agentic system for autonomously solving Machine\nLearning tasks (AutoML). Our framework outperforms state-of-the-art baselines,\nachieving improvements of 38.2% - 69.2% on standard data science tasks, and\n37.4% - 47.9% on therapeutic chemistry tasks. With an overall operation cost\nunder $1 per task, our framework is well-suited for cost-sensitive\napplications. Beyond classification and regression, we illustrate the broader\napplicability of our FoO-based agentic system to tasks such as reinforcement\nlearning and image generation. Our framework presents significant advancements\ncompared to current state-of-the-art agentic systems for AutoML, due to the\nbenefits of FoO in enforcing diversity in LLM solutions through compressed,\nexplainable representations that also support long-term memory when combined\nwith case-based reasoning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Github code: https://github.com/flagshippioneering/Flow-of-Options",
    "pdf_url": "http://arxiv.org/pdf/2502.12929v1",
    "published_date": "2025-02-18 15:11:46 UTC",
    "updated_date": "2025-02-18 15:11:46 UTC"
  },
  {
    "arxiv_id": "2502.12926v1",
    "title": "Towards more Contextual Agents: An extractor-Generator Optimization Framework",
    "authors": [
      "Mourad Aouini",
      "Jinan Loubani"
    ],
    "abstract": "Large Language Model (LLM)-based agents have demonstrated remarkable success\nin solving complex tasks across a wide range of general-purpose applications.\nHowever, their performance often degrades in context-specific scenarios, such\nas specialized industries or research domains, where the absence of\ndomain-relevant knowledge leads to imprecise or suboptimal outcomes. To address\nthis challenge, our work introduces a systematic approach to enhance the\ncontextual adaptability of LLM-based agents by optimizing their underlying\nprompts-critical components that govern agent behavior, roles, and\ninteractions. Manually crafting optimized prompts for context-specific tasks is\nlabor-intensive, error-prone, and lacks scalability. In this work, we introduce\nan Extractor-Generator framework designed to automate the optimization of\ncontextual LLM-based agents. Our method operates through two key stages: (i)\nfeature extraction from a dataset of gold-standard input-output examples, and\n(ii) prompt generation via a high-level optimization strategy that iteratively\nidentifies underperforming cases and applies self-improvement techniques. This\nframework substantially improves prompt adaptability by enabling more precise\ngeneralization across diverse inputs, particularly in context-specific tasks\nwhere maintaining semantic consistency and minimizing error propagation are\ncritical for reliable performance. Although developed with single-stage\nworkflows in mind, the approach naturally extends to multi-stage workflows,\noffering broad applicability across various agent-based systems. Empirical\nevaluations demonstrate that our framework significantly enhances the\nperformance of prompt-optimized agents, providing a structured and efficient\napproach to contextual LLM-based agents.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12926v1",
    "published_date": "2025-02-18 15:07:06 UTC",
    "updated_date": "2025-02-18 15:07:06 UTC"
  },
  {
    "arxiv_id": "2502.12925v1",
    "title": "Keep what you need : extracting efficient subnetworks from large audio representation models",
    "authors": [
      "David Genova",
      "Philippe Esling",
      "Tom Hurlin"
    ],
    "abstract": "Recently, research on audio foundation models has witnessed notable advances,\nas illustrated by the ever improving results on complex downstream tasks.\nSubsequently, those pretrained networks have quickly been used for various\naudio applications. These improvements have however resulted in a considerable\nincrease both in size and complexity of these models. Along the environmental\nconcerns this issue raises, this prevents the deployment of such networks on\nconsumer-level devices, and precludes their use for real-time applications.\nMoreover, this appears contradictory with the specificity of the tasks for\nwhich these models are used, which are often simpler compared to extracting a\nrich, multi-purpose representation from any type of audio data. In this paper,\nwe address this issue with a simple, yet effective method to extract\nlightweight specialist subnetworks from large foundation models. Specifically,\nwe introduce learnable binary masks in-between the layers of a pretrained\nrepresentation model. When training the end-to-end model on a downstream task,\nwe add a sparsity-inducing loss to the overall objective, hence learning a\ncompact subnetwork specialized on a single task. Importantly, the weights of\nthe foundation model are kept frozen, resulting into low additional training\ncosts. Once trained, the masked computational units can then be removed from\nthe network, implying significant performance gains. We assess our method on\nthree widespread audio foundation models, each based on a different backbone\narchitecture, and illustrate its effectiveness on common audio representation\nevaluation tasks, as well as its versatility on both speech, music, and general\naudio. Code for reproducing the results and supporting webpage are available at\nhttps://github.com/gnvIRCAM/Audio-representation-trimming",
    "categories": [
      "cs.SD",
      "cs.AI"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12925v1",
    "published_date": "2025-02-18 15:04:33 UTC",
    "updated_date": "2025-02-18 15:04:33 UTC"
  },
  {
    "arxiv_id": "2502.12924v1",
    "title": "Conditioning LLMs to Generate Code-Switched Text: A Methodology Grounded in Naturally Occurring Data",
    "authors": [
      "Maite Heredia",
      "Gorka Labaka",
      "Jeremy Barnes",
      "Aitor Soroa"
    ],
    "abstract": "Code-switching (CS) is still a critical challenge in Natural Language\nProcessing (NLP). Current Large Language Models (LLMs) struggle to interpret\nand generate code-switched text, primarily due to the scarcity of large-scale\nCS datasets for training. This paper presents a novel methodology to generate\nCS data using LLMs, and test it on the English-Spanish language pair. We\npropose back-translating natural CS sentences into monolingual English, and\nusing the resulting parallel corpus to fine-tune LLMs to turn monolingual\nsentences into CS. Unlike previous approaches to CS generation, our methodology\nuses natural CS data as a starting point, allowing models to learn its natural\ndistribution beyond grammatical patterns. We thoroughly analyse the models'\nperformance through a study on human preferences, a qualitative error analysis\nand an evaluation with popular automatic metrics. Results show that our\nmethodology generates fluent code-switched text, expanding research\nopportunities in CS communication, and that traditional metrics do not\ncorrelate with human judgement when assessing the quality of the generated CS\ndata. We release our code and generated dataset under a CC-BY-NC-SA license.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12924v1",
    "published_date": "2025-02-18 15:04:13 UTC",
    "updated_date": "2025-02-18 15:04:13 UTC"
  },
  {
    "arxiv_id": "2502.12913v2",
    "title": "GSQ-Tuning: Group-Shared Exponents Integer in Fully Quantized Training for LLMs On-Device Fine-tuning",
    "authors": [
      "Sifan Zhou",
      "Shuo Wang",
      "Zhihang Yuan",
      "Mingjia Shi",
      "Yuzhang Shang",
      "Dawei Yang"
    ],
    "abstract": "Large Language Models (LLMs) fine-tuning technologies have achieved\nremarkable results. However, traditional LLM fine-tuning approaches face\nsignificant challenges: they require large Floating Point (FP) computation,\nraising privacy concerns when handling sensitive data, and are impractical for\nresource-constrained edge devices. While Parameter-Efficient Fine-Tuning (PEFT)\ntechniques reduce trainable parameters, their reliance on floating-point\narithmetic creates fundamental incompatibilities with edge hardware. In this\nwork, we introduce a novel framework for on-device LLM fine-tuning that\neliminates the need for floating-point operations in both inference and\ntraining, named GSQ-Tuning. At its core is the Group-Shared Exponents Integer\nformat, which efficiently represents model parameters in integer format using\nshared exponents among parameter groups. When combined with LoRA-like adapters,\nthis enables fully integer-based fine-tuning that is both memory and compute\nefficient. We demonstrate that our approach achieves accuracy comparable to\nBF16-based fine-tuning while significantly reducing 1.85x memory usage.\nMoreover, compared to FP8, our method can reduce 5x power consumption and 11x\nchip area with same performance, making large-scale model adaptation feasible\non edge devices.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12913v2",
    "published_date": "2025-02-18 14:54:55 UTC",
    "updated_date": "2025-02-24 06:46:54 UTC"
  },
  {
    "arxiv_id": "2502.12908v2",
    "title": "Graph Neural Networks for Databases: A Survey",
    "authors": [
      "Ziming Li",
      "Youhuan Li",
      "Yuyu Luo",
      "Guoliang Li",
      "Chuxu Zhang"
    ],
    "abstract": "Graph neural networks (GNNs) are powerful deep learning models for\ngraph-structured data, demonstrating remarkable success across diverse domains.\nRecently, the database (DB) community has increasingly recognized the\npotentiality of GNNs, prompting a surge of researches focusing on improving\ndatabase systems through GNN-based approaches. However, despite notable\nadvances, There is a lack of a comprehensive review and understanding of how\nGNNs could improve DB systems. Therefore, this survey aims to bridge this gap\nby providing a structured and in-depth overview of GNNs for DB systems.\nSpecifically, we propose a new taxonomy that classifies existing methods into\ntwo key categories: (1) Relational Databases, which includes tasks like\nperformance prediction, query optimization, and text-to-SQL, and (2) Graph\nDatabases, addressing challenges like efficient graph query processing and\ngraph similarity computation. We systematically review key methods in each\ncategory, highlighting their contributions and practical implications. Finally,\nwe suggest promising avenues for integrating GNNs into Database systems.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "A survey focus on GNNs and databases. 9 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.12908v2",
    "published_date": "2025-02-18 14:51:50 UTC",
    "updated_date": "2025-02-19 05:09:09 UTC"
  },
  {
    "arxiv_id": "2502.12900v1",
    "title": "Soundwave: Less is More for Speech-Text Alignment in LLMs",
    "authors": [
      "Yuhao Zhang",
      "Zhiheng Liu",
      "Fan Bu",
      "Ruiyu Zhang",
      "Benyou Wang",
      "Haizhou Li"
    ],
    "abstract": "Existing end-to-end speech large language models (LLMs) usually rely on\nlarge-scale annotated data for training, while data-efficient training has not\nbeen discussed in depth. We focus on two fundamental problems between speech\nand text: the representation space gap and sequence length inconsistency. We\npropose Soundwave, which utilizes an efficient training strategy and a novel\narchitecture to address these issues. Results show that Soundwave outperforms\nthe advanced Qwen2-Audio in speech translation and AIR-Bench speech tasks,\nusing only one-fiftieth of the training data. Further analysis shows that\nSoundwave still retains its intelligence during conversation. The project is\navailable at https://github.com/FreedomIntelligence/Soundwave.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12900v1",
    "published_date": "2025-02-18 14:36:39 UTC",
    "updated_date": "2025-02-18 14:36:39 UTC"
  },
  {
    "arxiv_id": "2502.13983v1",
    "title": "Gesture-Aware Zero-Shot Speech Recognition for Patients with Language Disorders",
    "authors": [
      "Seungbae Kim",
      "Daeun Lee",
      "Brielle Stark",
      "Jinyoung Han"
    ],
    "abstract": "Individuals with language disorders often face significant communication\nchallenges due to their limited language processing and comprehension\nabilities, which also affect their interactions with voice-assisted systems\nthat mostly rely on Automatic Speech Recognition (ASR). Despite advancements in\nASR that address disfluencies, there has been little attention on integrating\nnon-verbal communication methods, such as gestures, which individuals with\nlanguage disorders substantially rely on to supplement their communication.\nRecognizing the need to interpret the latent meanings of visual information not\ncaptured by speech alone, we propose a gesture-aware ASR system utilizing a\nmultimodal large language model with zero-shot learning for individuals with\nspeech impairments. Our experiment results and analyses show that including\ngesture information significantly enhances semantic understanding. This study\ncan help develop effective communication technologies, specifically designed to\nmeet the unique needs of individuals with language impairments.",
    "categories": [
      "eess.AS",
      "cs.AI"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.13983v1",
    "published_date": "2025-02-18 14:15:55 UTC",
    "updated_date": "2025-02-18 14:15:55 UTC"
  },
  {
    "arxiv_id": "2502.13189v1",
    "title": "MoBA: Mixture of Block Attention for Long-Context LLMs",
    "authors": [
      "Enzhe Lu",
      "Zhejun Jiang",
      "Jingyuan Liu",
      "Yulun Du",
      "Tao Jiang",
      "Chao Hong",
      "Shaowei Liu",
      "Weiran He",
      "Enming Yuan",
      "Yuzhi Wang",
      "Zhiqi Huang",
      "Huan Yuan",
      "Suting Xu",
      "Xinran Xu",
      "Guokun Lai",
      "Yanru Chen",
      "Huabin Zheng",
      "Junjie Yan",
      "Jianlin Su",
      "Yuxin Wu",
      "Neo Y. Zhang",
      "Zhilin Yang",
      "Xinyu Zhou",
      "Mingxing Zhang",
      "Jiezhong Qiu"
    ],
    "abstract": "Scaling the effective context length is essential for advancing large\nlanguage models (LLMs) toward artificial general intelligence (AGI). However,\nthe quadratic increase in computational complexity inherent in traditional\nattention mechanisms presents a prohibitive overhead. Existing approaches\neither impose strongly biased structures, such as sink or window attention\nwhich are task-specific, or radically modify the attention mechanism into\nlinear approximations, whose performance in complex reasoning tasks remains\ninadequately explored.\n  In this work, we propose a solution that adheres to the ``less structure''\nprinciple, allowing the model to determine where to attend autonomously, rather\nthan introducing predefined biases. We introduce Mixture of Block Attention\n(MoBA), an innovative approach that applies the principles of Mixture of\nExperts (MoE) to the attention mechanism. This novel architecture demonstrates\nsuperior performance on long-context tasks while offering a key advantage: the\nability to seamlessly transition between full and sparse attention, enhancing\nefficiency without the risk of compromising performance. MoBA has already been\ndeployed to support Kimi's long-context requests and demonstrates significant\nadvancements in efficient attention computation for LLMs. Our code is available\nat https://github.com/MoonshotAI/MoBA.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages",
    "pdf_url": "http://arxiv.org/pdf/2502.13189v1",
    "published_date": "2025-02-18 14:06:05 UTC",
    "updated_date": "2025-02-18 14:06:05 UTC"
  },
  {
    "arxiv_id": "2502.12876v1",
    "title": "Continuous Learning Conversational AI: A Personalized Agent Framework via A2C Reinforcement Learning",
    "authors": [
      "Nandakishor M",
      "Anjali M"
    ],
    "abstract": "Creating personalized and adaptable conversational AI remains a key\nchallenge. This paper introduces a Continuous Learning Conversational AI (CLCA)\napproach, implemented using A2C reinforcement learning, to move beyond static\nLarge Language Models (LLMs). We use simulated sales dialogues, generated by\nLLMs, to train an A2C agent. This agent learns to optimize conversation\nstrategies for personalization, focusing on engagement and delivering value.\nOur system architecture integrates reinforcement learning with LLMs for both\ndata creation and response selection. This method offers a practical way to\nbuild personalized AI companions that evolve through continuous learning,\nadvancing beyond traditional static LLM techniques.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12876v1",
    "published_date": "2025-02-18 14:05:59 UTC",
    "updated_date": "2025-02-18 14:05:59 UTC"
  },
  {
    "arxiv_id": "2502.12859v1",
    "title": "PAFT: Prompt-Agnostic Fine-Tuning",
    "authors": [
      "Chenxing Wei",
      "Yao Shu",
      "Mingwen Ou",
      "Ying Tiffany He",
      "Fei Richard Yu"
    ],
    "abstract": "While Large Language Models (LLMs) adapt well to downstream tasks after\nfine-tuning, this adaptability often compromises prompt robustness, as even\nminor prompt variations can significantly degrade performance. To address this,\nwe propose Prompt-Agnostic Fine-Tuning(PAFT), a simple yet effective approach\nthat dynamically adjusts prompts during fine-tuning. This encourages the model\nto learn underlying task principles rather than overfitting to specific prompt\nformulations. PAFT operates in two stages: First, a diverse set of meaningful,\nsynthetic candidate prompts is constructed. Second, during fine-tuning, prompts\nare randomly sampled from this set to create dynamic training inputs. Extensive\nexperiments across diverse datasets and LLMs demonstrate that models trained\nwith PAFT exhibit strong robustness and generalization across a wide range of\nprompts, including unseen ones. This enhanced robustness improves both model\nperformance and inference speed while maintaining training efficiency. Ablation\nstudies further confirm the effectiveness of PAFT.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "20 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.12859v1",
    "published_date": "2025-02-18 13:46:47 UTC",
    "updated_date": "2025-02-18 13:46:47 UTC"
  },
  {
    "arxiv_id": "2502.12858v1",
    "title": "Rejected Dialects: Biases Against African American Language in Reward Models",
    "authors": [
      "Joel Mire",
      "Zubin Trivadi Aysola",
      "Daniel Chechelnitsky",
      "Nicholas Deas",
      "Chrysoula Zerva",
      "Maarten Sap"
    ],
    "abstract": "Preference alignment via reward models helps build safe, helpful, and\nreliable large language models (LLMs). However, subjectivity in preference\njudgments and the lack of representative sampling in preference data collection\ncan introduce new biases, hindering reward models' fairness and equity. In this\nwork, we introduce a framework for evaluating dialect biases in reward models\nand conduct a case study on biases against African American Language (AAL)\nthrough several experiments comparing reward model preferences and behavior on\npaired White Mainstream English (WME) and both machine-translated and\nhuman-written AAL corpora. We show that reward models are less aligned with\nhuman preferences when processing AAL texts vs. WME ones (-4\\% accuracy on\naverage), frequently disprefer AAL-aligned texts vs. WME-aligned ones, and\nsteer conversations toward WME, even when prompted with AAL texts. Our findings\nprovide a targeted analysis of anti-AAL biases at a relatively understudied\nstage in LLM development, highlighting representational harms and ethical\nquestions about the desired behavior of LLMs concerning AAL.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "I.2.7; K.4.2"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to NAACL Findings 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.12858v1",
    "published_date": "2025-02-18 13:45:42 UTC",
    "updated_date": "2025-02-18 13:45:42 UTC"
  },
  {
    "arxiv_id": "2502.12855v1",
    "title": "Integrating Arithmetic Learning Improves Mathematical Reasoning in Smaller Models",
    "authors": [
      "Neeraj Gangwar",
      "Suma P Bhat",
      "Nickvash Kani"
    ],
    "abstract": "While large models pre-trained on high-quality data exhibit excellent\nperformance across various reasoning tasks, including mathematical reasoning\n(e.g. GSM8k, MultiArith), specializing smaller models to excel at mathematical\nreasoning remains a challenging problem. Common approaches to address this\nchallenge include knowledge distillation, where smaller student models learn\nfrom large pre-trained teacher models, and data augmentation, such as\nrephrasing questions. Despite these efforts, smaller models struggle with\narithmetic computations, leading to errors in mathematical reasoning. In this\nwork, we focus on leveraging a programmatically generated arithmetic dataset to\nenhance the reasoning capabilities of smaller models. We investigate two key\napproaches to incorporate this dataset -- (1) intermediate fine-tuning, where a\nmodel is fine-tuned on the arithmetic dataset before being trained on a\nreasoning dataset, and (2) integrating the arithmetic dataset into the\ninstruction-tuning mixture, allowing the model to learn arithmetic skills\nalongside general instruction-following abilities. Our experiments on multiple\nreasoning benchmarks demonstrate that incorporating an arithmetic dataset,\nwhether through targeted fine-tuning or within the instruction-tuning mixture,\nenhances the models' arithmetic capabilities, which in turn improves their\nmathematical reasoning performance.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2502.12855v1",
    "published_date": "2025-02-18 13:43:06 UTC",
    "updated_date": "2025-02-18 13:43:06 UTC"
  },
  {
    "arxiv_id": "2502.12851v1",
    "title": "MeMo: Towards Language Models with Associative Memory Mechanisms",
    "authors": [
      "Fabio Massimo Zanzotto",
      "Elena Sofia Ruzzetti",
      "Giancarlo A. Xompero",
      "Leonardo Ranaldi",
      "Davide Venditti",
      "Federico Ranaldi",
      "Cristina Giannone",
      "Andrea Favalli",
      "Raniero Romagnoli"
    ],
    "abstract": "Memorization is a fundamental ability of Transformer-based Large Language\nModels, achieved through learning. In this paper, we propose a paradigm shift\nby designing an architecture to memorize text directly, bearing in mind the\nprinciple that memorization precedes learning. We introduce MeMo, a novel\narchitecture for language modeling that explicitly memorizes sequences of\ntokens in layered associative memories. By design, MeMo offers transparency and\nthe possibility of model editing, including forgetting texts. We experimented\nwith the MeMo architecture, showing the memorization power of the one-layer and\nthe multi-layer configurations.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7; I.2.6; I.2.4"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12851v1",
    "published_date": "2025-02-18 13:39:22 UTC",
    "updated_date": "2025-02-18 13:39:22 UTC"
  },
  {
    "arxiv_id": "2502.12842v1",
    "title": "Towards Adaptive Feedback with AI: Comparing the Feedback Quality of LLMs and Teachers on Experimentation Protocols",
    "authors": [
      "Kathrin Seßler",
      "Arne Bewersdorff",
      "Claudia Nerdel",
      "Enkelejda Kasneci"
    ],
    "abstract": "Effective feedback is essential for fostering students' success in scientific\ninquiry. With advancements in artificial intelligence, large language models\n(LLMs) offer new possibilities for delivering instant and adaptive feedback.\nHowever, this feedback often lacks the pedagogical validation provided by\nreal-world practitioners. To address this limitation, our study evaluates and\ncompares the feedback quality of LLM agents with that of human teachers and\nscience education experts on student-written experimentation protocols. Four\nblinded raters, all professionals in scientific inquiry and science education,\nevaluated the feedback texts generated by 1) the LLM agent, 2) the teachers and\n3) the science education experts using a five-point Likert scale based on six\ncriteria of effective feedback: Feed Up, Feed Back, Feed Forward, Constructive\nTone, Linguistic Clarity, and Technical Terminology. Our results indicate that\nLLM-generated feedback shows no significant difference to that of teachers and\nexperts in overall quality. However, the LLM agent's performance lags in the\nFeed Back dimension, which involves identifying and explaining errors within\nthe student's work context. Qualitative analysis highlighted the LLM agent's\nlimitations in contextual understanding and in the clear communication of\nspecific errors. Our findings suggest that combining LLM-generated feedback\nwith human expertise can enhance educational practices by leveraging the\nefficiency of LLMs and the nuanced understanding of educators.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "This work has been submitted to the IJAIED for possible publication",
    "pdf_url": "http://arxiv.org/pdf/2502.12842v1",
    "published_date": "2025-02-18 13:22:14 UTC",
    "updated_date": "2025-02-18 13:22:14 UTC"
  },
  {
    "arxiv_id": "2502.13187v3",
    "title": "A Survey of Sim-to-Real Methods in RL: Progress, Prospects and Challenges with Foundation Models",
    "authors": [
      "Longchao Da",
      "Justin Turnau",
      "Thirulogasankar Pranav Kutralingam",
      "Alvaro Velasquez",
      "Paulo Shakarian",
      "Hua Wei"
    ],
    "abstract": "Deep Reinforcement Learning (RL) has been explored and verified to be\neffective in solving decision-making tasks in various domains, such as\nrobotics, transportation, recommender systems, etc. It learns from the\ninteraction with environments and updates the policy using the collected\nexperience. However, due to the limited real-world data and unbearable\nconsequences of taking detrimental actions, the learning of RL policy is mainly\nrestricted within the simulators. This practice guarantees safety in learning\nbut introduces an inevitable sim-to-real gap in terms of deployment, thus\ncausing degraded performance and risks in execution. There are attempts to\nsolve the sim-to-real problems from different domains with various techniques,\nespecially in the era with emerging techniques such as large foundations or\nlanguage models that have cast light on the sim-to-real. This survey paper, to\nthe best of our knowledge, is the first taxonomy that formally frames the\nsim-to-real techniques from key elements of the Markov Decision Process (State,\nAction, Transition, and Reward). Based on the framework, we cover comprehensive\nliterature from the classic to the most advanced methods including the\nsim-to-real techniques empowered by foundation models, and we also discuss the\nspecialties that are worth attention in different domains of sim-to-real\nproblems. Then we summarize the formal evaluation process of sim-to-real\nperformance with accessible code or benchmarks. The challenges and\nopportunities are also presented to encourage future exploration of this\ndirection. We are actively maintaining a repository to include the most\nup-to-date sim-to-real research work to help domain researchers.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO",
      "68T05, 68U05",
      "I.6.0; I.2.9; I.2.1"
    ],
    "primary_category": "cs.LG",
    "comment": "19 pages, 6 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2502.13187v3",
    "published_date": "2025-02-18 12:57:29 UTC",
    "updated_date": "2025-03-08 06:36:16 UTC"
  },
  {
    "arxiv_id": "2503.16449v1",
    "title": "Mitigating the Uncanny Valley Effect in Hyper-Realistic Robots: A Student-Centered Study on LLM-Driven Conversations",
    "authors": [
      "Hangyeol Kang",
      "Thiago Freitas dos Santos",
      "Maher Ben Moussa",
      "Nadia Magnenat-Thalmann"
    ],
    "abstract": "The uncanny valley effect poses a significant challenge in the development\nand acceptance of hyper-realistic social robots. This study investigates\nwhether advanced conversational capabilities powered by large language models\n(LLMs) can mitigate this effect in highly anthropomorphic robots. We conducted\na user study with 80 participants interacting with Nadine, a hyper-realistic\nhumanoid robot equipped with LLM-driven communication skills. Through pre- and\npost-interaction surveys, we assessed changes in perceptions of uncanniness,\nconversational quality, and overall user experience. Our findings reveal that\nLLM-enhanced interactions significantly reduce feelings of eeriness while\nfostering more natural and engaging conversations. Additionally, we identify\nkey factors influencing user acceptance, including conversational naturalness,\nhuman-likeness, and interestingness. Based on these insights, we propose design\nrecommendations to enhance the appeal and acceptability of hyper-realistic\nrobots in social contexts. This research contributes to the growing field of\nhuman-robot interaction by offering empirical evidence on the potential of LLMs\nto bridge the uncanny valley, with implications for the future development of\nsocial robots.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.16449v1",
    "published_date": "2025-02-18 12:53:41 UTC",
    "updated_date": "2025-02-18 12:53:41 UTC"
  },
  {
    "arxiv_id": "2502.12825v2",
    "title": "Reasoning and the Trusting Behavior of DeepSeek and GPT: An Experiment Revealing Hidden Fault Lines in Large Language Models",
    "authors": [
      "Rubing Li",
      "João Sedoc",
      "Arun Sundararajan"
    ],
    "abstract": "When encountering increasingly frequent performance improvements or cost\nreductions from a new large language model (LLM), developers of applications\nleveraging LLMs must decide whether to take advantage of these improvements or\nstay with older tried-and-tested models. Low perceived switching frictions can\nlead to choices that do not consider more subtle behavior changes that the\ntransition may induce. Our experiments use a popular game-theoretic behavioral\neconomics model of trust to show stark differences in the trusting behavior of\nOpenAI's and DeepSeek's models. We highlight a collapse in the economic trust\nbehavior of the o1-mini and o3-mini models as they reconcile profit-maximizing\nand risk-seeking with future returns from trust, and contrast it with\nDeepSeek's more sophisticated and profitable trusting behavior that stems from\nan ability to incorporate deeper concepts like forward planning and\ntheory-of-mind. As LLMs form the basis for high-stakes commercial systems, our\nresults highlight the perils of relying on LLM performance benchmarks that are\ntoo narrowly defined and suggest that careful analysis of their hidden fault\nlines should be part of any organization's AI strategy.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12825v2",
    "published_date": "2025-02-18 12:46:18 UTC",
    "updated_date": "2025-02-19 11:57:19 UTC"
  },
  {
    "arxiv_id": "2502.12798v1",
    "title": "Envious Explore and Exploit",
    "authors": [
      "Omer Ben-Porat",
      "Yotam Gafni",
      "Or Markovetzki"
    ],
    "abstract": "Explore-and-exploit tradeoffs play a key role in recommendation systems\n(RSs), aiming at serving users better by learning from previous interactions.\nDespite their commercial success, the societal effects of explore-and-exploit\nmechanisms are not well understood, especially regarding the utility\ndiscrepancy they generate between different users. In this work, we measure\nsuch discrepancy using the economic notion of envy. We present a multi-armed\nbandit-like model in which every round consists of several sessions, and\nrewards are realized once per round. We call the latter property reward\nconsistency, and show that the RS can leverage this property for better\nsocietal outcomes. On the downside, doing so also generates envy, as\nlate-to-arrive users enjoy the information gathered by early-to-arrive users.\nWe examine the generated envy under several arrival order mechanisms and\nvirtually any anonymous algorithm, i.e., any algorithm that treats all similar\nusers similarly without leveraging their identities. We provide tight envy\nbounds on uniform arrival and upper bound the envy for nudged arrival, in which\nthe RS can affect the order of arrival by nudging its users. Furthermore, we\nstudy the efficiency-fairness trade-off by devising an algorithm that allows\nconstant envy and approximates the optimal welfare in restricted settings.\nFinally, we validate our theoretical results empirically using simulations.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.GT",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12798v1",
    "published_date": "2025-02-18 12:00:35 UTC",
    "updated_date": "2025-02-18 12:00:35 UTC"
  },
  {
    "arxiv_id": "2502.12793v1",
    "title": "Unsupervised Anomaly Detection through Mass Repulsing Optimal Transport",
    "authors": [
      "Eduardo Fernandes Montesuma",
      "Adel El Habazi",
      "Fred Ngole Mboula"
    ],
    "abstract": "Detecting anomalies in datasets is a longstanding problem in machine\nlearning. In this context, anomalies are defined as a sample that significantly\ndeviates from the remaining data. Meanwhile, optimal transport (OT) is a field\nof mathematics concerned with the transportation, between two probability\nmeasures, at least effort. In classical OT, the optimal transportation strategy\nof a measure to itself is the identity. In this paper, we tackle anomaly\ndetection by forcing samples to displace its mass, while keeping the least\neffort objective. We call this new transportation problem Mass Repulsing\nOptimal Transport (MROT). Naturally, samples lying in low density regions of\nspace will be forced to displace mass very far, incurring a higher\ntransportation cost. We use these concepts to design a new anomaly score.\nThrough a series of experiments in existing benchmarks, and fault detection\nproblems, we show that our algorithm improves over existing methods.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "15 pages, 9 figures, 1 table, under review",
    "pdf_url": "http://arxiv.org/pdf/2502.12793v1",
    "published_date": "2025-02-18 11:54:12 UTC",
    "updated_date": "2025-02-18 11:54:12 UTC"
  },
  {
    "arxiv_id": "2502.12782v2",
    "title": "VidCapBench: A Comprehensive Benchmark of Video Captioning for Controllable Text-to-Video Generation",
    "authors": [
      "Xinlong Chen",
      "Yuanxing Zhang",
      "Chongling Rao",
      "Yushuo Guan",
      "Jiaheng Liu",
      "Fuzheng Zhang",
      "Chengru Song",
      "Qiang Liu",
      "Di Zhang",
      "Tieniu Tan"
    ],
    "abstract": "The training of controllable text-to-video (T2V) models relies heavily on the\nalignment between videos and captions, yet little existing research connects\nvideo caption evaluation with T2V generation assessment. This paper introduces\nVidCapBench, a video caption evaluation scheme specifically designed for T2V\ngeneration, agnostic to any particular caption format. VidCapBench employs a\ndata annotation pipeline, combining expert model labeling and human refinement,\nto associate each collected video with key information spanning video\naesthetics, content, motion, and physical laws. VidCapBench then partitions\nthese key information attributes into automatically assessable and manually\nassessable subsets, catering to both the rapid evaluation needs of agile\ndevelopment and the accuracy requirements of thorough validation. By evaluating\nnumerous state-of-the-art captioning models, we demonstrate the superior\nstability and comprehensiveness of VidCapBench compared to existing video\ncaptioning evaluation approaches. Verification with off-the-shelf T2V models\nreveals a significant positive correlation between scores on VidCapBench and\nthe T2V quality evaluation metrics, indicating that VidCapBench can provide\nvaluable guidance for training T2V models. The project is available at\nhttps://github.com/VidCapBench/VidCapBench.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to Findings of ACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.12782v2",
    "published_date": "2025-02-18 11:42:17 UTC",
    "updated_date": "2025-05-17 09:49:22 UTC"
  },
  {
    "arxiv_id": "2502.12777v1",
    "title": "Evaluating link prediction: New perspectives and recommendations",
    "authors": [
      "Bhargavi Kalyani I",
      "A Rama Prasad Mathi",
      "Niladri Sett"
    ],
    "abstract": "Link prediction (LP) is an important problem in network science and machine\nlearning research. The state-of-the-art LP methods are usually evaluated in a\nuniform setup, ignoring several factors associated with the data and\napplication specific needs. We identify a number of such factors, such as,\nnetwork-type, problem-type, geodesic distance between the end nodes and its\ndistribution over the classes, nature and applicability of LP methods, class\nimbalance and its impact on early retrieval, evaluation metric, etc., and\npresent an experimental setup which allows us to evaluate LP methods in a\nrigorous and controlled manner. We perform extensive experiments with a variety\nof LP methods over real network datasets in this controlled setup, and gather\nvaluable insights on the interactions of these factors with the performance of\nLP through an array of carefully designed hypotheses. Following the insights,\nwe provide recommendations to be followed as best practice for evaluating LP\nmethods.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12777v1",
    "published_date": "2025-02-18 11:36:59 UTC",
    "updated_date": "2025-02-18 11:36:59 UTC"
  },
  {
    "arxiv_id": "2502.12776v1",
    "title": "Portable Reward Tuning: Towards Reusable Fine-Tuning across Different Pretrained Models",
    "authors": [
      "Daiki Chijiwa",
      "Taku Hasegawa",
      "Kyosuke Nishida",
      "Kuniko Saito",
      "Susumu Takeuchi"
    ],
    "abstract": "While foundation models have been exploited for various expert tasks through\nfine-tuning, any foundation model will become outdated due to its old knowledge\nor limited capability. Thus the underlying foundation model should be\neventually replaced by new ones, which leads to repeated cost of fine-tuning\nthese new models. Existing work addresses this problem by inference-time\ntuning, i.e., modifying the output probabilities from the new foundation model\nwith the outputs from the old foundation model and its fine-tuned model, which\ninvolves an additional overhead in inference by the latter two models. In this\npaper, we propose a new fine-tuning principle, Portable Reward Tuning (PRT),\nthat reduces the inference overhead by its nature, based on the reformulation\nof fine-tuning as the reward maximization. Specifically, instead of fine-tuning\nparameters of the foundation models, PRT trains the reward model explicitly\nthrough the same loss function as in fine-tuning. During inference, the reward\nmodel can be used with any foundation model (with the same set of vocabularies\nor labels) through the formulation of reward maximization. Experimental\nresults, covering both vision and language models, demonstrate that the\nPRT-trained model can achieve comparable accuracy to the existing work of\ninference-time tuning, with less inference cost.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12776v1",
    "published_date": "2025-02-18 11:36:33 UTC",
    "updated_date": "2025-02-18 11:36:33 UTC"
  },
  {
    "arxiv_id": "2502.12769v2",
    "title": "How Much Do LLMs Hallucinate across Languages? On Multilingual Estimation of LLM Hallucination in the Wild",
    "authors": [
      "Saad Obaid ul Islam",
      "Anne Lauscher",
      "Goran Glavaš"
    ],
    "abstract": "In the age of misinformation, hallucination -- the tendency of Large Language\nModels (LLMs) to generate non-factual or unfaithful responses -- represents the\nmain risk for their global utility. Despite LLMs becoming increasingly\nmultilingual, the vast majority of research on detecting and quantifying LLM\nhallucination are (a) English-centric and (b) focus on machine translation (MT)\nand summarization, tasks that are less common ``in the wild'' than open\ninformation seeking. In contrast, we aim to quantify the extent of LLM\nhallucination across languages in knowledge-intensive long-form question\nanswering. To this end, we train a multilingual hallucination detection model\nand conduct a large-scale study across 30 languages and 6 open-source LLM\nfamilies. We start from an English hallucination detection dataset and rely on\nMT to generate (noisy) training data in other languages. We also manually\nannotate gold data for five high-resource languages; we then demonstrate, for\nthese languages, that the estimates of hallucination rates are similar between\nsilver (LLM-generated) and gold test sets, validating the use of silver data\nfor estimating hallucination rates for other languages. For the final rates\nestimation, we build a knowledge-intensive QA dataset for 30 languages with\nLLM-generated prompts and Wikipedia articles as references. We find that, while\nLLMs generate longer responses with more hallucinated tokens for\nhigher-resource languages, there is no correlation between length-normalized\nhallucination rates of languages and their digital representation. Further, we\nfind that smaller LLMs exhibit larger hallucination rates than larger models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12769v2",
    "published_date": "2025-02-18 11:32:43 UTC",
    "updated_date": "2025-02-20 10:50:09 UTC"
  },
  {
    "arxiv_id": "2502.12767v5",
    "title": "R2-KG: General-Purpose Dual-Agent Framework for Reliable Reasoning on Knowledge Graphs",
    "authors": [
      "Sumin Jo",
      "Junseong Choi",
      "Jiho Kim",
      "Edward Choi"
    ],
    "abstract": "Recent studies have combined Large Language Models (LLMs) with Knowledge\nGraphs (KGs) to enhance reasoning, improving inference accuracy without\nadditional training while mitigating hallucination. However, existing\nframeworks still suffer two practical drawbacks: they must be re-tuned whenever\nthe KG or reasoning task changes, and they depend on a single, high-capacity\nLLM for reliable (i.e., trustworthy) reasoning. To address this, we introduce\nR2-KG, a plug-and-play, dual-agent framework that separates reasoning into two\nroles: an Operator (a low-capacity LLM) that gathers evidence and a Supervisor\n(a high-capacity LLM) that makes final judgments. This design is cost-efficient\nfor LLM inference while still maintaining strong reasoning accuracy.\nAdditionally, R2-KG employs an Abstention mechanism, generating answers only\nwhen sufficient evidence is collected from KG, which significantly enhances\nreliability. Experiments across five diverse benchmarks show that R2-KG\nconsistently outperforms baselines in both accuracy and reliability, regardless\nof the inherent capability of LLMs used as the Operator. Further experiments\nreveal that the single-agent version of R2-KG, equipped with a strict\nself-consistency strategy, achieves significantly higher-than-baseline\nreliability with reduced inference cost but increased abstention rate in\ncomplex KGs. Our findings establish R2-KG as a flexible and cost-effective\nsolution for KG-based reasoning, reducing reliance on high-capacity LLMs while\nensuring trustworthy inference. The code is available at\nhttps://github.com/ekrxjwh2009/R2-KG/.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12767v5",
    "published_date": "2025-02-18 11:31:52 UTC",
    "updated_date": "2025-05-20 11:24:19 UTC"
  },
  {
    "arxiv_id": "2502.14902v1",
    "title": "PathRAG: Pruning Graph-based Retrieval Augmented Generation with Relational Paths",
    "authors": [
      "Boyu Chen",
      "Zirui Guo",
      "Zidan Yang",
      "Yuluo Chen",
      "Junze Chen",
      "Zhenghao Liu",
      "Chuan Shi",
      "Cheng Yang"
    ],
    "abstract": "Retrieval-augmented generation (RAG) improves the response quality of large\nlanguage models (LLMs) by retrieving knowledge from external databases. Typical\nRAG approaches split the text database into chunks, organizing them in a flat\nstructure for efficient searches. To better capture the inherent dependencies\nand structured relationships across the text database, researchers propose to\norganize textual information into an indexing graph, known asgraph-based RAG.\nHowever, we argue that the limitation of current graph-based RAG methods lies\nin the redundancy of the retrieved information, rather than its insufficiency.\nMoreover, previous methods use a flat structure to organize retrieved\ninformation within the prompts, leading to suboptimal performance. To overcome\nthese limitations, we propose PathRAG, which retrieves key relational paths\nfrom the indexing graph, and converts these paths into textual form for\nprompting LLMs. Specifically, PathRAG effectively reduces redundant information\nwith flow-based pruning, while guiding LLMs to generate more logical and\ncoherent responses with path-based prompting. Experimental results show that\nPathRAG consistently outperforms state-of-the-art baselines across six datasets\nand five evaluation dimensions. The code is available at the following link:\nhttps://github.com/BUPT-GAMMA/PathRAG",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14902v1",
    "published_date": "2025-02-18 11:18:55 UTC",
    "updated_date": "2025-02-18 11:18:55 UTC"
  },
  {
    "arxiv_id": "2502.12755v1",
    "title": "Efficient Machine Translation Corpus Generation: Integrating Human-in-the-Loop Post-Editing with Large Language Models",
    "authors": [
      "Kamer Ali Yuksel",
      "Ahmet Gunduz",
      "Abdul Baseet Anees",
      "Hassan Sawaf"
    ],
    "abstract": "This paper introduces an advanced methodology for machine translation (MT)\ncorpus generation, integrating semi-automated, human-in-the-loop post-editing\nwith large language models (LLMs) to enhance efficiency and translation\nquality. Building upon previous work that utilized real-time training of a\ncustom MT quality estimation metric, this system incorporates novel LLM\nfeatures such as Enhanced Translation Synthesis and Assisted Annotation\nAnalysis, which improve initial translation hypotheses and quality assessments,\nrespectively. Additionally, the system employs LLM-Driven Pseudo Labeling and a\nTranslation Recommendation System to reduce human annotator workload in\nspecific contexts. These improvements not only retain the original benefits of\ncost reduction and enhanced post-edit quality but also open new avenues for\nleveraging cutting-edge LLM advancements. The project's source code is\navailable for community use, promoting collaborative developments in the field.\nThe demo video can be accessed here.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12755v1",
    "published_date": "2025-02-18 11:16:38 UTC",
    "updated_date": "2025-02-18 11:16:38 UTC"
  },
  {
    "arxiv_id": "2502.13185v1",
    "title": "CondensNet: Enabling stable long-term climate simulations via hybrid deep learning models with adaptive physical constraints",
    "authors": [
      "Xin Wang",
      "Juntao Yang",
      "Jeff Adie",
      "Simon See",
      "Kalli Furtado",
      "Chen Chen",
      "Troy Arcomano",
      "Romit Maulik",
      "Gianmarco Mengaldo"
    ],
    "abstract": "Accurate and efficient climate simulations are crucial for understanding\nEarth's evolving climate. However, current general circulation models (GCMs)\nface challenges in capturing unresolved physical processes, such as cloud and\nconvection. A common solution is to adopt cloud resolving models, that provide\nmore accurate results than the standard subgrid parametrisation schemes\ntypically used in GCMs. However, cloud resolving models, also referred to as\nsuper paramtetrizations, remain computationally prohibitive. Hybrid modeling,\nwhich integrates deep learning with equation-based GCMs, offers a promising\nalternative but often struggles with long-term stability and accuracy issues.\nIn this work, we find that water vapor oversaturation during condensation is a\nkey factor compromising the stability of hybrid models. To address this, we\nintroduce CondensNet, a novel neural network architecture that embeds a\nself-adaptive physical constraint to correct unphysical condensation processes.\nCondensNet effectively mitigates water vapor oversaturation, enhancing\nsimulation stability while maintaining accuracy and improving computational\nefficiency compared to super parameterization schemes.\n  We integrate CondensNet into a GCM to form PCNN-GCM (Physics-Constrained\nNeural Network GCM), a hybrid deep learning framework designed for long-term\nstable climate simulations in real-world conditions, including ocean and land.\nPCNN-GCM represents a significant milestone in hybrid climate modeling, as it\nshows a novel way to incorporate physical constraints adaptively, paving the\nway for accurate, lightweight, and stable long-term climate simulations.",
    "categories": [
      "physics.ao-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "physics.ao-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.13185v1",
    "published_date": "2025-02-18 11:11:17 UTC",
    "updated_date": "2025-02-18 11:11:17 UTC"
  },
  {
    "arxiv_id": "2502.12745v1",
    "title": "MediaMind: Revolutionizing Media Monitoring using Agentification",
    "authors": [
      "Ahmet Gunduz",
      "Kamer Ali Yuksel",
      "Hassan Sawaf"
    ],
    "abstract": "In an era of rapid technological advancements, agentification of software\ntools has emerged as a critical innovation, enabling systems to function\nautonomously and adaptively. This paper introduces MediaMind as a case study to\ndemonstrate the agentification process, highlighting how existing software can\nbe transformed into intelligent agents capable of independent decision-making\nand dynamic interaction. Developed by aiXplain, MediaMind leverages agent-based\narchitecture to autonomously monitor, analyze, and provide insights from\nmultilingual media content in real time. The focus of this paper is on the\ntechnical methodologies and design principles behind agentifying MediaMind,\nshowcasing how agentification enhances adaptability, efficiency, and\nresponsiveness. Through detailed case studies and practical examples, we\nillustrate how the agentification of MediaMind empowers organizations to\nstreamline workflows, optimize decision-making, and respond to evolving trends.\nThis work underscores the broader potential of agentification to revolutionize\nsoftware tools across various domains.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12745v1",
    "published_date": "2025-02-18 11:05:38 UTC",
    "updated_date": "2025-02-18 11:05:38 UTC"
  },
  {
    "arxiv_id": "2502.12743v1",
    "title": "\"I know myself better, but not really greatly\": Using LLMs to Detect and Explain LLM-Generated Texts",
    "authors": [
      "Jiazhou Ji",
      "Jie Guo",
      "Weidong Qiu",
      "Zheng Huang",
      "Yang Xu",
      "Xinru Lu",
      "Xiaoyu Jiang",
      "Ruizhe Li",
      "Shujun Li"
    ],
    "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in\ngenerating human-like texts, but the potential misuse of such LLM-generated\ntexts raises the need to distinguish between human-generated and LLM-generated\ncontent. This paper explores the detection and explanation capabilities of\nLLM-based detectors of LLM-generated texts, in the context of a binary\nclassification task (human-generated texts vs LLM-generated texts) and a\nternary classification task (human-generated texts, LLM-generated texts, and\nundecided). By evaluating on six close/open-source LLMs with different sizes,\nour findings reveal that while self-detection consistently outperforms\ncross-detection, i.e., LLMs can detect texts generated by themselves more\naccurately than those generated by other LLMs, the performance of\nself-detection is still far from ideal, indicating that further improvements\nare needed. We also show that extending the binary to the ternary\nclassification task with a new class \"Undecided\" can enhance both detection\naccuracy and explanation quality, with improvements being statistically\nsignificant and consistent across all LLMs. We finally conducted comprehensive\nqualitative and quantitative analyses on the explanation errors, which are\ncategorized into three types: reliance on inaccurate features (the most\nfrequent error), hallucinations, and incorrect reasoning. These findings with\nour human-annotated dataset emphasize the need for further research into\nimproving both self-detection and self-explanation, particularly to address\noverfitting issues that may hinder generalization.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Under review",
    "pdf_url": "http://arxiv.org/pdf/2502.12743v1",
    "published_date": "2025-02-18 11:00:28 UTC",
    "updated_date": "2025-02-18 11:00:28 UTC"
  },
  {
    "arxiv_id": "2502.12737v2",
    "title": "Beyond Seen Data: Improving KBQA Generalization Through Schema-Guided Logical Form Generation",
    "authors": [
      "Shengxiang Gao",
      "Jey Han Lau",
      "Jianzhong Qi"
    ],
    "abstract": "Knowledge base question answering (KBQA) aims to answer user questions in\nnatural language using rich human knowledge stored in large KBs. As current\nKBQA methods struggle with unseen knowledge base elements at test time,we\nintroduce SG-KBQA: a novel model that injects schema contexts into entity\nretrieval and logical form generation to tackle this issue. It uses the richer\nsemantics and awareness of the knowledge base structure provided by schema\ncontexts to enhance generalizability. We show that SG-KBQA achieves strong\ngeneralizability, outperforming state-of-the-art models on two commonly used\nbenchmark datasets across a variety of test settings. Our source code is\navailable at https://github.com/gaosx2000/SG_KBQA.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "17 pages",
    "pdf_url": "http://arxiv.org/pdf/2502.12737v2",
    "published_date": "2025-02-18 10:53:41 UTC",
    "updated_date": "2025-02-19 05:32:40 UTC"
  },
  {
    "arxiv_id": "2502.12710v2",
    "title": "Innamark: A Whitespace Replacement Information-Hiding Method",
    "authors": [
      "Malte Hellmeier",
      "Hendrik Norkowski",
      "Ernst-Christoph Schrewe",
      "Haydar Qarawlus",
      "Falk Howar"
    ],
    "abstract": "Large language models (LLMs) have gained significant popularity in recent\nyears. Differentiating between a text written by a human and one generated by\nan LLM has become almost impossible. Information-hiding techniques such as\ndigital watermarking or steganography can help by embedding information inside\ntext in a form that is unlikely to be noticed. However, existing techniques,\nsuch as linguistic-based or format-based methods, change the semantics or\ncannot be applied to pure, unformatted text. In this paper, we introduce a\nnovel method for information hiding called Innamark, which can conceal any\nbyte-encoded sequence within a sufficiently long cover text. This method is\nimplemented as a multi-platform library using the Kotlin programming language,\nwhich is accompanied by a command-line tool and a web interface. By\nsubstituting conventional whitespace characters with visually similar Unicode\nwhitespace characters, our proposed scheme preserves the semantics of the cover\ntext without changing the number of characters. Furthermore, we propose a\nspecified structure for secret messages that enables configurable compression,\nencryption, hashing, and error correction. An experimental benchmark comparison\non a dataset of 1000000 Wikipedia articles compares ten algorithms. The results\ndemonstrate the robustness of our proposed Innamark method in various\napplications and the imperceptibility of its watermarks to humans. We discuss\nthe limits to the embedding capacity and robustness of the algorithm and how\nthese could be addressed in future work.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12710v2",
    "published_date": "2025-02-18 10:21:27 UTC",
    "updated_date": "2025-04-28 19:26:36 UTC"
  },
  {
    "arxiv_id": "2502.17481v2",
    "title": "Toward Foundational Model for Sleep Analysis Using a Multimodal Hybrid Self-Supervised Learning Framework",
    "authors": [
      "Cheol-Hui Lee",
      "Hakseung Kim",
      "Byung C. Yoon",
      "Dong-Joo Kim"
    ],
    "abstract": "Sleep is essential for maintaining human health and quality of life.\nAnalyzing physiological signals during sleep is critical in assessing sleep\nquality and diagnosing sleep disorders. However, manual diagnoses by clinicians\nare time-intensive and subjective. Despite advances in deep learning that have\nenhanced automation, these approaches remain heavily dependent on large-scale\nlabeled datasets. This study introduces SynthSleepNet, a multimodal hybrid\nself-supervised learning framework designed for analyzing polysomnography (PSG)\ndata. SynthSleepNet effectively integrates masked prediction and contrastive\nlearning to leverage complementary features across multiple modalities,\nincluding electroencephalogram (EEG), electrooculography (EOG),\nelectromyography (EMG), and electrocardiogram (ECG). This approach enables the\nmodel to learn highly expressive representations of PSG data. Furthermore, a\ntemporal context module based on Mamba was developed to efficiently capture\ncontextual information across signals. SynthSleepNet achieved superior\nperformance compared to state-of-the-art methods across three downstream tasks:\nsleep-stage classification, apnea detection, and hypopnea detection, with\naccuracies of 89.89%, 99.75%, and 89.60%, respectively. The model demonstrated\nrobust performance in a semi-supervised learning environment with limited\nlabels, achieving accuracies of 87.98%, 99.37%, and 77.52% in the same tasks.\nThese results underscore the potential of the model as a foundational tool for\nthe comprehensive analysis of PSG data. SynthSleepNet demonstrates\ncomprehensively superior performance across multiple downstream tasks compared\nto other methodologies, making it expected to set a new standard for sleep\ndisorder monitoring and diagnostic systems.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "18 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.17481v2",
    "published_date": "2025-02-18 10:11:50 UTC",
    "updated_date": "2025-02-28 18:56:25 UTC"
  },
  {
    "arxiv_id": "2502.12701v1",
    "title": "Translate Smart, not Hard: Cascaded Translation Systems with Quality-Aware Deferral",
    "authors": [
      "António Farinhas",
      "Nuno M. Guerreiro",
      "Sweta Agrawal",
      "Ricardo Rei",
      "André F. T. Martins"
    ],
    "abstract": "Larger models often outperform smaller ones but come with high computational\ncosts. Cascading offers a potential solution. By default, it uses smaller\nmodels and defers only some instances to larger, more powerful models. However,\ndesigning effective deferral rules remains a challenge. In this paper, we\npropose a simple yet effective approach for machine translation, using existing\nquality estimation (QE) metrics as deferral rules. We show that QE-based\ndeferral allows a cascaded system to match the performance of a larger model\nwhile invoking it for a small fraction (30% to 50%) of the examples,\nsignificantly reducing computational costs. We validate this approach through\nboth automatic and human evaluation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2502.12701v1",
    "published_date": "2025-02-18 10:05:40 UTC",
    "updated_date": "2025-02-18 10:05:40 UTC"
  },
  {
    "arxiv_id": "2502.12690v1",
    "title": "Fast Data Aware Neural Architecture Search via Supernet Accelerated Evaluation",
    "authors": [
      "Emil Njor",
      "Colby Banbury",
      "Xenofon Fafoutis"
    ],
    "abstract": "Tiny machine learning (TinyML) promises to revolutionize fields such as\nhealthcare, environmental monitoring, and industrial maintenance by running\nmachine learning models on low-power embedded systems. However, the complex\noptimizations required for successful TinyML deployment continue to impede its\nwidespread adoption. A promising route to simplifying TinyML is through\nautomatic machine learning (AutoML), which can distill elaborate optimization\nworkflows into accessible key decisions. Notably, Hardware Aware Neural\nArchitecture Searches - where a computer searches for an optimal TinyML model\nbased on predictive performance and hardware metrics - have gained significant\ntraction, producing some of today's most widely used TinyML models.\nNevertheless, limiting optimization solely to neural network architectures can\nprove insufficient. Because TinyML systems must operate under extremely tight\nresource constraints, the choice of input data configuration, such as\nresolution or sampling rate, also profoundly impacts overall system efficiency.\nAchieving truly optimal TinyML systems thus requires jointly tuning both input\ndata and model architecture. Despite its importance, this \"Data Aware Neural\nArchitecture Search\" remains underexplored. To address this gap, we propose a\nnew state-of-the-art Data Aware Neural Architecture Search technique and\ndemonstrate its effectiveness on the novel TinyML ``Wake Vision'' dataset. Our\nexperiments show that across varying time and hardware constraints, Data Aware\nNeural Architecture Search consistently discovers superior TinyML systems\ncompared to purely architecture-focused methods, underscoring the critical role\nof data-aware optimization in advancing TinyML.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "68T10, 68T20, 68T45"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12690v1",
    "published_date": "2025-02-18 09:51:03 UTC",
    "updated_date": "2025-02-18 09:51:03 UTC"
  },
  {
    "arxiv_id": "2502.14900v1",
    "title": "Can AI mimic the human ability to define neologisms?",
    "authors": [
      "Georgios P. Georgiou"
    ],
    "abstract": "One ongoing debate in linguistics is whether Artificial Intelligence (AI) can\neffectively mimic human performance in language-related tasks. While much\nresearch has focused on various linguistic abilities of AI, little attention\nhas been given to how it defines neologisms formed through different word\nformation processes. This study addresses this gap by examining the degree of\nagreement between human and AI-generated responses in defining three types of\nGreek neologisms: blends, compounds, and derivatives. The study employed an\nonline experiment in which human participants selected the most appropriate\ndefinitions for neologisms, while ChatGPT received identical prompts. The\nresults revealed fair agreement between human and AI responses for blends and\nderivatives but no agreement for compounds. However, when considering the\nmajority response among humans, agreement with AI was high for blends and\nderivatives. These findings highlight the complexity of human language and the\nchallenges AI still faces in capturing its nuances. In particular, they suggest\na need for integrating more advanced semantic networks and contextual learning\nmechanisms into AI models to improve their interpretation of complex word\nformations, especially compounds.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14900v1",
    "published_date": "2025-02-18 09:46:38 UTC",
    "updated_date": "2025-02-18 09:46:38 UTC"
  },
  {
    "arxiv_id": "2502.18493v1",
    "title": "Rule-based autocorrection of Piping and Instrumentation Diagrams (P&IDs) on graphs",
    "authors": [
      "Lukas Schulze Balhorn",
      "Niels Seijsener",
      "Kevin Dao",
      "Minji Kim",
      "Dominik P. Goldstein",
      "Ge H. M. Driessen",
      "Artur M. Schweidtmann"
    ],
    "abstract": "A piping and instrumentation diagram (P&ID) is a central reference document\nin chemical process engineering. Currently, chemical engineers manually review\nP&IDs through visual inspection to find and rectify errors. However,\nengineering projects can involve hundreds to thousands of P&ID pages, creating\na significant revision workload. This study proposes a rule-based method to\nsupport engineers with error detection and correction in P&IDs. The method is\nbased on a graph representation of P&IDs, enabling automated error detection\nand correction, i.e., autocorrection, through rule graphs. We use our pyDEXPI\nPython package to generate P&ID graphs from DEXPI-standard P&IDs. In this\nstudy, we developed 33 rules based on chemical engineering knowledge and\nheuristics, with five selected rules demonstrated as examples. A case study on\nan illustrative P&ID validates the reliability and effectiveness of the\nrule-based autocorrection method in revising P&IDs.",
    "categories": [
      "cs.CE",
      "cs.AI"
    ],
    "primary_category": "cs.CE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18493v1",
    "published_date": "2025-02-18 09:35:09 UTC",
    "updated_date": "2025-02-18 09:35:09 UTC"
  },
  {
    "arxiv_id": "2502.13181v1",
    "title": "RingFormer: Rethinking Recurrent Transformer with Adaptive Level Signals",
    "authors": [
      "Jaemu Heo",
      "Eldor Fozilov",
      "Hyunmin Song",
      "Taehwan Kim"
    ],
    "abstract": "Transformers have achieved great success in effectively processing sequential\ndata such as text. Their architecture consisting of several attention and\nfeedforward blocks can model relations between elements of a sequence in\nparallel manner, which makes them very efficient to train and effective in\nsequence modeling. Even though they have shown strong performance in processing\nsequential data, the size of their parameters is considerably larger when\ncompared to other architectures such as RNN and CNN based models. Therefore,\nseveral approaches have explored parameter sharing and recurrence in\nTransformer models to address their computational demands. However, such\nmethods struggle to maintain high performance compared to the original\ntransformer model. To address this challenge, we propose our novel approach,\nRingFormer, which employs one Transformer layer that processes input repeatedly\nin a circular, ring-like manner, while utilizing low-rank matrices to generate\ninput-dependent level signals. This allows us to reduce the model parameters\nsubstantially while maintaining high performance in a variety of tasks such as\ntranslation and image classification, as validated in the experiments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.13181v1",
    "published_date": "2025-02-18 09:34:31 UTC",
    "updated_date": "2025-02-18 09:34:31 UTC"
  },
  {
    "arxiv_id": "2502.12678v1",
    "title": "Multi-Step Alignment as Markov Games: An Optimistic Online Gradient Descent Approach with Convergence Guarantees",
    "authors": [
      "Yongtao Wu",
      "Luca Viano",
      "Yihang Chen",
      "Zhenyu Zhu",
      "Kimon Antonakopoulos",
      "Quanquan Gu",
      "Volkan Cevher"
    ],
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) has been highly successful\nin aligning large language models with human preferences. While prevalent\nmethods like DPO have demonstrated strong performance, they frame interactions\nwith the language model as a bandit problem, which limits their applicability\nin real-world scenarios where multi-turn conversations are common.\nAdditionally, DPO relies on the Bradley-Terry model assumption, which does not\nadequately capture the non-transitive nature of human preferences. In this\npaper, we address these challenges by modeling the alignment problem as a\ntwo-player constant-sum Markov game, where each player seeks to maximize their\nwinning rate against the other across all steps of the conversation. Our\napproach Multi-step Preference Optimization (MPO) is built upon the natural\nactor-critic framework~\\citep{peters2008natural}. We further develop OMPO based\non the optimistic online gradient descent\nalgorithm~\\citep{rakhlin2013online,joulani17a}. Theoretically, we provide a\nrigorous analysis for both algorithms on convergence and show that OMPO\nrequires $\\mathcal{O}(\\epsilon^{-1})$ policy updates to converge to an\n$\\epsilon$-approximate Nash equilibrium. We also validate the effectiveness of\nour method on multi-turn conversations dataset and math reasoning dataset.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted as oral presentation in NeurIPS LanGame Workshop, revised\n  from ICLR submission",
    "pdf_url": "http://arxiv.org/pdf/2502.12678v1",
    "published_date": "2025-02-18 09:33:48 UTC",
    "updated_date": "2025-02-18 09:33:48 UTC"
  },
  {
    "arxiv_id": "2502.12677v1",
    "title": "Spiking Vision Transformer with Saccadic Attention",
    "authors": [
      "Shuai Wang",
      "Malu Zhang",
      "Dehao Zhang",
      "Ammar Belatreche",
      "Yichen Xiao",
      "Yu Liang",
      "Yimeng Shan",
      "Qian Sun",
      "Enqi Zhang",
      "Yang Yang"
    ],
    "abstract": "The combination of Spiking Neural Networks (SNNs) and Vision Transformers\n(ViTs) holds potential for achieving both energy efficiency and high\nperformance, particularly suitable for edge vision applications. However, a\nsignificant performance gap still exists between SNN-based ViTs and their ANN\ncounterparts. Here, we first analyze why SNN-based ViTs suffer from limited\nperformance and identify a mismatch between the vanilla self-attention\nmechanism and spatio-temporal spike trains. This mismatch results in degraded\nspatial relevance and limited temporal interactions. To address these issues,\nwe draw inspiration from biological saccadic attention mechanisms and introduce\nan innovative Saccadic Spike Self-Attention (SSSA) method. Specifically, in the\nspatial domain, SSSA employs a novel spike distribution-based method to\neffectively assess the relevance between Query and Key pairs in SNN-based ViTs.\nTemporally, SSSA employs a saccadic interaction module that dynamically focuses\non selected visual areas at each timestep and significantly enhances whole\nscene understanding through temporal interactions. Building on the SSSA\nmechanism, we develop a SNN-based Vision Transformer (SNN-ViT). Extensive\nexperiments across various visual tasks demonstrate that SNN-ViT achieves\nstate-of-the-art performance with linear computational complexity. The\neffectiveness and efficiency of the SNN-ViT highlight its potential for\npower-critical edge vision applications.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Published as a conference paper at ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.12677v1",
    "published_date": "2025-02-18 09:32:29 UTC",
    "updated_date": "2025-02-18 09:32:29 UTC"
  },
  {
    "arxiv_id": "2502.12672v1",
    "title": "Speech-FT: A Fine-tuning Strategy for Enhancing Speech Representation Models Without Compromising Generalization Ability",
    "authors": [
      "Tzu-Quan Lin",
      "Wei-Ping Huang",
      "Hao Tang",
      "Hung-yi Lee"
    ],
    "abstract": "Speech representation models are highly effective at extracting general\nfeatures for various tasks. While fine-tuning can enhance these representations\nfor specific applications, it often compromises their generalization ability.\nTo address this challenge, we propose Speech-FT, a fine-tuning strategy for\nspeech representation models that leverages model merging to preserve\ngeneralization ability while still benefiting from fine-tuning. Speech-FT is\neffective across different fine-tuning scenarios and is compatible with various\ntypes of speech representation models, providing a versatile solution.\nSpeech-FT offers an efficient and practical approach to further improving\ngeneral speech representations after pre-training.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12672v1",
    "published_date": "2025-02-18 09:23:42 UTC",
    "updated_date": "2025-02-18 09:23:42 UTC"
  },
  {
    "arxiv_id": "2502.12669v1",
    "title": "Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research",
    "authors": [
      "Xiang Liu",
      "Penglei Sun",
      "Shuyan Chen",
      "Longhan Zhang",
      "Peijie Dong",
      "Huajie You",
      "Yongqi Zhang",
      "Chang Yan",
      "Xiaowen Chu",
      "Tong-yi Zhang"
    ],
    "abstract": "The rapid advancement of perovskite solar cells (PSCs) has led to an\nexponential growth in research publications, creating an urgent need for\nefficient knowledge management and reasoning systems in this domain. We present\na comprehensive knowledge-enhanced system for PSCs that integrates three key\ncomponents. First, we develop Perovskite-KG, a domain-specific knowledge graph\nconstructed from 1,517 research papers, containing 23,789 entities and 22,272\nrelationships. Second, we create two complementary datasets: Perovskite-Chat,\ncomprising 55,101 high-quality question-answer pairs generated through a novel\nmulti-agent framework, and Perovskite-Reasoning, containing 2,217 carefully\ncurated materials science problems. Third, we introduce two specialized large\nlanguage models: Perovskite-Chat-LLM for domain-specific knowledge assistance\nand Perovskite-Reasoning-LLM for scientific reasoning tasks. Experimental\nresults demonstrate that our system significantly outperforms existing models\nin both domain-specific knowledge retrieval and scientific reasoning tasks,\nproviding researchers with effective tools for literature review, experimental\ndesign, and complex problem-solving in PSC research.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "23pages",
    "pdf_url": "http://arxiv.org/pdf/2502.12669v1",
    "published_date": "2025-02-18 09:19:24 UTC",
    "updated_date": "2025-02-18 09:19:24 UTC"
  },
  {
    "arxiv_id": "2502.12659v3",
    "title": "The Hidden Risks of Large Reasoning Models: A Safety Assessment of R1",
    "authors": [
      "Kaiwen Zhou",
      "Chengzhi Liu",
      "Xuandong Zhao",
      "Shreedhar Jangam",
      "Jayanth Srinivasa",
      "Gaowen Liu",
      "Dawn Song",
      "Xin Eric Wang"
    ],
    "abstract": "The rapid development of large reasoning models, such as OpenAI-o3 and\nDeepSeek-R1, has led to significant improvements in complex reasoning over\nnon-reasoning large language models~(LLMs). However, their enhanced\ncapabilities, combined with the open-source access of models like DeepSeek-R1,\nraise serious safety concerns, particularly regarding their potential for\nmisuse. In this work, we present a comprehensive safety assessment of these\nreasoning models, leveraging established safety benchmarks to evaluate their\ncompliance with safety regulations. Furthermore, we investigate their\nsusceptibility to adversarial attacks, such as jailbreaking and prompt\ninjection, to assess their robustness in real-world applications. Through our\nmulti-faceted analysis, we uncover four key findings: (1) There is a\nsignificant safety gap between the open-source R1 models and the o3-mini model,\non both safety benchmark and attack, suggesting more safety effort on R1 is\nneeded. (2) The distilled reasoning model shows poorer safety performance\ncompared to its safety-aligned base models. (3) The stronger the model's\nreasoning ability, the greater the potential harm it may cause when answering\nunsafe questions. (4) The thinking process in R1 models pose greater safety\nconcerns than their final answers. Our study provides insights into the\nsecurity implications of reasoning models and highlights the need for further\nadvancements in R1 models' safety to close the gap.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12659v3",
    "published_date": "2025-02-18 09:06:07 UTC",
    "updated_date": "2025-02-27 08:03:52 UTC"
  },
  {
    "arxiv_id": "2502.17480v1",
    "title": "Brain-to-Text Decoding: A Non-invasive Approach via Typing",
    "authors": [
      "Jarod Lévy",
      "Mingfang Zhang",
      "Svetlana Pinet",
      "Jérémy Rapin",
      "Hubert Banville",
      "Stéphane d'Ascoli",
      "Jean-Rémi King"
    ],
    "abstract": "Modern neuroprostheses can now restore communication in patients who have\nlost the ability to speak or move. However, these invasive devices entail risks\ninherent to neurosurgery. Here, we introduce a non-invasive method to decode\nthe production of sentences from brain activity and demonstrate its efficacy in\na cohort of 35 healthy volunteers. For this, we present Brain2Qwerty, a new\ndeep learning architecture trained to decode sentences from either electro-\n(EEG) or magneto-encephalography (MEG), while participants typed briefly\nmemorized sentences on a QWERTY keyboard. With MEG, Brain2Qwerty reaches, on\naverage, a character-error-rate (CER) of 32% and substantially outperforms EEG\n(CER: 67%). For the best participants, the model achieves a CER of 19%, and can\nperfectly decode a variety of sentences outside of the training set. While\nerror analyses suggest that decoding depends on motor processes, the analysis\nof typographical errors suggests that it also involves higher-level cognitive\nfactors. Overall, these results narrow the gap between invasive and\nnon-invasive methods and thus open the path for developing safe brain-computer\ninterfaces for non-communicating patients.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "eess.SP",
    "comment": "15 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.17480v1",
    "published_date": "2025-02-18 08:36:46 UTC",
    "updated_date": "2025-02-18 08:36:46 UTC"
  },
  {
    "arxiv_id": "2502.12633v2",
    "title": "One Size doesn't Fit All: A Personalized Conversational Tutoring Agent for Mathematics Instruction",
    "authors": [
      "Ben Liu",
      "Jihan Zhang",
      "Fangquan Lin",
      "Xu Jia",
      "Min Peng"
    ],
    "abstract": "Large language models (LLMs) have been increasingly employed in various\nintelligent educational systems, simulating human tutors to facilitate\neffective human-machine interaction. However, previous studies often overlook\nthe significance of recognizing and adapting to individual learner\ncharacteristics. Such adaptation is crucial for enhancing student engagement\nand learning efficiency, particularly in mathematics instruction, where diverse\nlearning styles require personalized strategies to promote comprehension and\nenthusiasm. In this paper, we propose a \\textbf{P}erson\\textbf{A}lized\n\\textbf{C}onversational tutoring ag\\textbf{E}nt (PACE) for mathematics\ninstruction. PACE simulates students' learning styles based on the Felder and\nSilverman learning style model, aligning with each student's persona. In this\nway, our PACE can effectively assess the personality of students, allowing to\ndevelop individualized teaching strategies that resonate with their unique\nlearning styles. To further enhance students' comprehension, PACE employs the\nSocratic teaching method to provide instant feedback and encourage deep\nthinking. By constructing personalized teaching data and training models, PACE\ndemonstrates the ability to identify and adapt to the unique needs of each\nstudent, significantly improving the overall learning experience and outcomes.\nMoreover, we establish multi-aspect evaluation criteria and conduct extensive\nanalysis to assess the performance of personalized teaching. Experimental\nresults demonstrate the superiority of our model in personalizing the\neducational experience and motivating students compared to existing methods.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12633v2",
    "published_date": "2025-02-18 08:24:52 UTC",
    "updated_date": "2025-02-19 16:45:48 UTC"
  },
  {
    "arxiv_id": "2502.12631v2",
    "title": "Score-Based Diffusion Policy Compatible with Reinforcement Learning via Optimal Transport",
    "authors": [
      "Mingyang Sun",
      "Pengxiang Ding",
      "Weinan Zhang",
      "Donglin Wang"
    ],
    "abstract": "Diffusion policies have shown promise in learning complex behaviors from\ndemonstrations, particularly for tasks requiring precise control and long-term\nplanning. However, they face challenges in robustness when encountering\ndistribution shifts. This paper explores improving diffusion-based imitation\nlearning models through online interactions with the environment. We propose\nOTPR (Optimal Transport-guided score-based diffusion Policy for Reinforcement\nlearning fine-tuning), a novel method that integrates diffusion policies with\nRL using optimal transport theory. OTPR leverages the Q-function as a transport\ncost and views the policy as an optimal transport map, enabling efficient and\nstable fine-tuning. Moreover, we introduce masked optimal transport to guide\nstate-action matching using expert keypoints and a compatibility-based\nresampling strategy to enhance training stability. Experiments on three\nsimulation tasks demonstrate OTPR's superior performance and robustness\ncompared to existing methods, especially in complex and sparse-reward\nenvironments. In sum, OTPR provides an effective framework for combining IL and\nRL, achieving versatile and reliable policy learning. The code will be released\nat https://github.com/Sunmmyy/OTPR.git.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12631v2",
    "published_date": "2025-02-18 08:22:20 UTC",
    "updated_date": "2025-02-21 06:56:09 UTC"
  },
  {
    "arxiv_id": "2502.12630v1",
    "title": "Automating Prompt Leakage Attacks on Large Language Models Using Agentic Approach",
    "authors": [
      "Tvrtko Sternak",
      "Davor Runje",
      "Dorian Granoša",
      "Chi Wang"
    ],
    "abstract": "This paper presents a novel approach to evaluating the security of large\nlanguage models (LLMs) against prompt leakage-the exposure of system-level\nprompts or proprietary configurations. We define prompt leakage as a critical\nthreat to secure LLM deployment and introduce a framework for testing the\nrobustness of LLMs using agentic teams. Leveraging AG2 (formerly AutoGen), we\nimplement a multi-agent system where cooperative agents are tasked with probing\nand exploiting the target LLM to elicit its prompt.\n  Guided by traditional definitions of security in cryptography, we further\ndefine a prompt leakage-safe system as one in which an attacker cannot\ndistinguish between two agents: one initialized with an original prompt and the\nother with a prompt stripped of all sensitive information. In a safe system,\nthe agents' outputs will be indistinguishable to the attacker, ensuring that\nsensitive information remains secure. This cryptographically inspired framework\nprovides a rigorous standard for evaluating and designing secure LLMs.\n  This work establishes a systematic methodology for adversarial testing of\nprompt leakage, bridging the gap between automated threat modeling and\npractical LLM security.\n  You can find the implementation of our prompt leakage probing on GitHub.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12630v1",
    "published_date": "2025-02-18 08:17:32 UTC",
    "updated_date": "2025-02-18 08:17:32 UTC"
  },
  {
    "arxiv_id": "2502.13180v1",
    "title": "Uncertain Multi-Objective Recommendation via Orthogonal Meta-Learning Enhanced Bayesian Optimization",
    "authors": [
      "Hongxu Wang",
      "Zhu Sun",
      "Yingpeng Du",
      "Lu Zhang",
      "Tiantian He",
      "Yew-Soon Ong"
    ],
    "abstract": "Recommender systems (RSs) play a crucial role in shaping our digital\ninteractions, influencing how we access and engage with information across\nvarious domains. Traditional research has predominantly centered on maximizing\nrecommendation accuracy, often leading to unintended side effects such as echo\nchambers and constrained user experiences. Drawing inspiration from autonomous\ndriving, we introduce a novel framework that categorizes RS autonomy into five\ndistinct levels, ranging from basic rule-based accuracy-driven systems to\nbehavior-aware, uncertain multi-objective RSs - where users may have varying\nneeds, such as accuracy, diversity, and fairness. In response, we propose an\napproach that dynamically identifies and optimizes multiple objectives based on\nindividual user preferences, fostering more ethical and intelligent\nuser-centric recommendations. To navigate the uncertainty inherent in\nmulti-objective RSs, we develop a Bayesian optimization (BO) framework that\ncaptures personalized trade-offs between different objectives while accounting\nfor their uncertain interdependencies. Furthermore, we introduce an orthogonal\nmeta-learning paradigm to enhance BO efficiency and effectiveness by leveraging\nshared knowledge across similar tasks and mitigating conflicts among objectives\nthrough the discovery of orthogonal information. Finally, extensive empirical\nevaluations demonstrate the effectiveness of our method in optimizing uncertain\nmulti-objectives for individual users, paving the way for more adaptive and\nuser-focused RSs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.13180v1",
    "published_date": "2025-02-18 08:10:09 UTC",
    "updated_date": "2025-02-18 08:10:09 UTC"
  },
  {
    "arxiv_id": "2502.12623v2",
    "title": "DeepResonance: Enhancing Multimodal Music Understanding via Music-centric Multi-way Instruction Tuning",
    "authors": [
      "Zhuoyuan Mao",
      "Mengjie Zhao",
      "Qiyu Wu",
      "Hiromi Wakaki",
      "Yuki Mitsufuji"
    ],
    "abstract": "Recent advancements in music large language models (LLMs) have significantly\nimproved music understanding tasks, which involve the model's ability to\nanalyze and interpret various musical elements. These improvements primarily\nfocused on integrating both music and text inputs. However, the potential of\nincorporating additional modalities such as images, videos and textual music\nfeatures to enhance music understanding remains unexplored. To bridge this gap,\nwe propose DeepResonance, a multimodal music understanding LLM fine-tuned via\nmulti-way instruction tuning with multi-way aligned music, text, image, and\nvideo data. To this end, we construct Music4way-MI2T, Music4way-MV2T, and\nMusic4way-Any2T, three 4-way training and evaluation datasets designed to\nenable DeepResonance to integrate both visual and textual music feature\ncontent. We also introduce multi-sampled ImageBind embeddings and a pre-LLM\nfusion Transformer to enhance modality fusion prior to input into text LLMs,\ntailoring DeepResonance for multi-way instruction tuning. Our model achieves\nstate-of-the-art performances across six music understanding tasks,\nhighlighting the benefits of the auxiliary modalities and the structural\nsuperiority of DeepResonance. We plan to open-source the models and the newly\nconstructed datasets.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "cs.MM",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12623v2",
    "published_date": "2025-02-18 08:09:42 UTC",
    "updated_date": "2025-05-20 08:59:05 UTC"
  },
  {
    "arxiv_id": "2502.13979v1",
    "title": "Utilizing Effective Dynamic Graph Learning to Shield Financial Stability from Risk Propagation",
    "authors": [
      "Guanyuan Yu",
      "Qing Li",
      "Yu Zhao",
      "Jun Wang",
      "YiJun Chen",
      "Shaolei Chen"
    ],
    "abstract": "Financial risks can propagate across both tightly coupled temporal and\nspatial dimensions, posing significant threats to financial stability.\nMoreover, risks embedded in unlabeled data are often difficult to detect. To\naddress these challenges, we introduce GraphShield, a novel approach with three\nkey innovations: Enhanced Cross-Domain Infor mation Learning: We propose a\ndynamic graph learning module to improve information learning across temporal\nand spatial domains. Advanced Risk Recognition: By leveraging the clustering\ncharacteristics of risks, we construct a risk recognizing module to enhance the\nidentification of hidden threats. Risk Propagation Visualization: We provide a\nvisualization tool for quantifying and validating nodes that trigger widespread\ncascading risks. Extensive experiments on two real-world and two open-source\ndatasets demonstrate the robust performance of our framework. Our approach\nrepresents a significant advancement in leveraging artificial intelligence to\nenhance financial stability, offering a powerful solution to mitigate the\nspread of risks within financial networks.",
    "categories": [
      "q-fin.RM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-fin.RM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.13979v1",
    "published_date": "2025-02-18 08:09:05 UTC",
    "updated_date": "2025-02-18 08:09:05 UTC"
  },
  {
    "arxiv_id": "2502.13179v1",
    "title": "PTQ1.61: Push the Real Limit of Extremely Low-Bit Post-Training Quantization Methods for Large Language Models",
    "authors": [
      "Jiaqi Zhao",
      "Miao Zhang",
      "Ming Wang",
      "Yuzhang Shang",
      "Kaihao Zhang",
      "Weili Guan",
      "Yaowei Wang",
      "Min Zhang"
    ],
    "abstract": "Large Language Models (LLMs) suffer severe performance degradation when\nfacing extremely low-bit (sub 2-bit) quantization. Several existing sub 2-bit\npost-training quantization (PTQ) methods utilize a mix-precision scheme by\nleveraging an unstructured fine-grained mask to explicitly distinguish salient\nweights, while which introduces an extra 1-bit or more per weight. To explore\nthe real limit of PTQ, we propose an extremely low-bit PTQ method called\nPTQ1.61, which enables weight quantization to 1.61-bit for the first time.\nSpecifically, we first introduce a one-dimensional structured mask with\nnegligibly additional 0.0002-bit per weight based on input activations from the\nperspective of reducing the upper bound of quantization error to allocate\ncorresponding salient weight channels to 4-bit. For non-salient channels\nbinarization, an efficient block-wise scaling factors optimization framework is\nthen presented to take implicit row-wise correlations and angular biases into\naccount. Different from prior works that concentrate on adjusting quantization\nmethodologies, we further propose a novel paradigm called quantization\npreprocessing, where we argue that transforming the weight distribution of the\npretrained model before quantization can alleviate the difficulty in\nper-channel extremely low-bit PTQ. Extensive experiments indicate our PTQ1.61\nachieves state-of-the-art performance in extremely low-bit quantization. Codes\nare available at https://github.com/zjq0455/PTQ1.61.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "20 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.13179v1",
    "published_date": "2025-02-18 08:04:58 UTC",
    "updated_date": "2025-02-18 08:04:58 UTC"
  },
  {
    "arxiv_id": "2502.12617v2",
    "title": "A Graph-Enhanced Deep-Reinforcement Learning Framework for the Aircraft Landing Problem",
    "authors": [
      "Vatsal Maru"
    ],
    "abstract": "The Aircraft Landing Problem (ALP) is one of the challenging problems in\naircraft transportation and management. The challenge is to schedule the\narriving aircraft in a sequence so that the cost and delays are optimized.\nThere are various solution approaches to solving this problem, most of which\nare based on operations research algorithms and meta-heuristics. Although\ntraditional methods perform better on one or the other factors, there remains a\nproblem of solving real-time rescheduling and computational scalability\naltogether. This paper presents a novel deep reinforcement learning (DRL)\nframework that combines graph neural networks with actor-critic architectures\nto address the ALP. This paper introduces three key contributions: A\ngraph-based state representation that efficiently captures temporal and spatial\nrelationships between aircraft, a specialized actor-critic architecture\ndesigned to handle multiple competing objectives in landing scheduling, and a\nrunway balance strategy that ensures efficient resource utilization while\nmaintaining safety constraints. The results show that the trained algorithm can\nbe tested on different problem sets and the results are competitive to\noperation research algorithms. The experimental results on standard benchmark\ndata sets demonstrate a 99.95% reduction in computational time compared to\nMixed Integer Programming (MIP) and 38% higher runway throughput over First\nCome First Serve (FCFS) approaches. Therefore, the proposed solution is\ncompetitive to traditional approaches and achieves substantial advancements.\nNotably, it does not require retraining, making it particularly suitable for\nindustrial deployment. The frameworks capability to generate solutions within 1\nsecond enables real-time rescheduling, addressing critical requirements of air\ntraffic management.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "27 pages, submitted to ESWA, comments are welcome",
    "pdf_url": "http://arxiv.org/pdf/2502.12617v2",
    "published_date": "2025-02-18 08:02:17 UTC",
    "updated_date": "2025-03-18 16:08:31 UTC"
  },
  {
    "arxiv_id": "2502.12614v1",
    "title": "Label Drop for Multi-Aspect Relation Modeling in Universal Information Extraction",
    "authors": [
      "Lu Yang",
      "Jiajia Li",
      "En Ci",
      "Lefei Zhang",
      "Zuchao Li",
      "Ping Wang"
    ],
    "abstract": "Universal Information Extraction (UIE) has garnered significant attention due\nto its ability to address model explosion problems effectively. Extractive UIE\ncan achieve strong performance using a relatively small model, making it widely\nadopted. Extractive UIEs generally rely on task instructions for different\ntasks, including single-target instructions and multiple-target instructions.\nSingle-target instruction UIE enables the extraction of only one type of\nrelation at a time, limiting its ability to model correlations between\nrelations and thus restricting its capability to extract complex relations.\nWhile multiple-target instruction UIE allows for the extraction of multiple\nrelations simultaneously, the inclusion of irrelevant relations introduces\ndecision complexity and impacts extraction accuracy. Therefore, for\nmulti-relation extraction, we propose LDNet, which incorporates multi-aspect\nrelation modeling and a label drop mechanism. By assigning different relations\nto different levels for understanding and decision-making, we reduce decision\nconfusion. Additionally, the label drop mechanism effectively mitigates the\nimpact of irrelevant relations. Experiments show that LDNet outperforms or\nachieves competitive performance with state-of-the-art systems on 9 tasks, 33\ndatasets, in both single-modal and multi-modal, few-shot and zero-shot\nsettings.\\footnote{https://github.com/Lu-Yang666/LDNet}",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to NAACL-main 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.12614v1",
    "published_date": "2025-02-18 07:53:26 UTC",
    "updated_date": "2025-02-18 07:53:26 UTC"
  },
  {
    "arxiv_id": "2502.12608v1",
    "title": "Unveiling Mode Connectivity in Graph Neural Networks",
    "authors": [
      "Bingheng Li",
      "Zhikai Chen",
      "Haoyu Han",
      "Shenglai Zeng",
      "Jingzhe Liu",
      "Jiliang Tang"
    ],
    "abstract": "A fundamental challenge in understanding graph neural networks (GNNs) lies in\ncharacterizing their optimization dynamics and loss landscape geometry,\ncritical for improving interpretability and robustness. While mode\nconnectivity, a lens for analyzing geometric properties of loss landscapes has\nproven insightful for other deep learning architectures, its implications for\nGNNs remain unexplored. This work presents the first investigation of mode\nconnectivity in GNNs. We uncover that GNNs exhibit distinct non-linear mode\nconnectivity, diverging from patterns observed in fully-connected networks or\nCNNs. Crucially, we demonstrate that graph structure, rather than model\narchitecture, dominates this behavior, with graph properties like homophily\ncorrelating with mode connectivity patterns. We further establish a link\nbetween mode connectivity and generalization, proposing a generalization bound\nbased on loss barriers and revealing its utility as a diagnostic tool. Our\nfindings further bridge theoretical insights with practical implications: they\nrationalize domain alignment strategies in graph learning and provide a\nfoundation for refining GNN training paradigms.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12608v1",
    "published_date": "2025-02-18 07:46:10 UTC",
    "updated_date": "2025-02-18 07:46:10 UTC"
  },
  {
    "arxiv_id": "2502.14899v1",
    "title": "UPCMR: A Universal Prompt-guided Model for Random Sampling Cardiac MRI Reconstruction",
    "authors": [
      "Donghang Lyu",
      "Chinmay Rao",
      "Marius Staring",
      "Matthias J. P. van Osch",
      "Mariya Doneva",
      "Hildo J. Lamb",
      "Nicola Pezzotti"
    ],
    "abstract": "Cardiac magnetic resonance imaging (CMR) is vital for diagnosing heart\ndiseases, but long scan time remains a major drawback. To address this,\naccelerated imaging techniques have been introduced by undersampling k-space,\nwhich reduces the quality of the resulting images. Recent deep learning\nadvancements aim to speed up scanning while preserving quality, but adapting to\nvarious sampling modes and undersampling factors remains challenging.\nTherefore, building a universal model is a promising direction. In this work,\nwe introduce UPCMR, a universal unrolled model designed for CMR reconstruction.\nThis model incorporates two kinds of learnable prompts, undersampling-specific\nprompt and spatial-specific prompt, and integrates them with a UNet structure\nin each block. Overall, by using the CMRxRecon2024 challenge dataset for\ntraining and validation, the UPCMR model highly enhances reconstructed image\nquality across all random sampling scenarios through an effective training\nstrategy compared to some traditional methods, demonstrating strong\nadaptability potential for this task.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted paper for STACOM 2024",
    "pdf_url": "http://arxiv.org/pdf/2502.14899v1",
    "published_date": "2025-02-18 07:44:35 UTC",
    "updated_date": "2025-02-18 07:44:35 UTC"
  },
  {
    "arxiv_id": "2502.13178v4",
    "title": "Benchmarking Post-Training Quantization in LLMs: Comprehensive Taxonomy, Unified Evaluation, and Comparative Analysis",
    "authors": [
      "Jiaqi Zhao",
      "Ming Wang",
      "Miao Zhang",
      "Yuzhang Shang",
      "Xuebo Liu",
      "Yaowei Wang",
      "Min Zhang",
      "Liqiang Nie"
    ],
    "abstract": "Post-training Quantization (PTQ) technique has been extensively adopted for\nlarge language models (LLMs) compression owing to its efficiency and low\nresource requirement. However, current research lacks a in-depth analysis of\nthe superior and applicable scenarios of each PTQ strategy. In addition,\nexisting algorithms focus primarily on performance, overlooking the trade-off\namong model size, performance, and quantization bitwidth. To mitigate these\nconfusions, we provide a novel benchmark for LLMs PTQ in this paper. Firstly,\nin order to support our benchmark, we propose a comprehensive taxonomy for\nexisting mainstream methods by scrutinizing their computational strategies\n(e.g., optimization-based, compensation-based, etc.). Then, we conduct\nextensive experiments with the baseline within each class, covering models with\nvarious sizes (7B-70B), bitwidths, training levels (LLaMA1/2/3/3.1),\narchitectures (Mixtral, DeepSeekMoE and Mamba) and modality (LLaVA1.5 and\nVILA1.5) on a wide range of evaluation metrics.Through comparative analysis on\nthe results, we summarize the superior of each PTQ strategy and\nmodelsize-bitwidth trade-off considering the performance. For example, our\nbenchmark reveals that compensation-based technique demonstrates outstanding\ncross-architecture robustness and extremely low-bit PTQ for ultra large models\nshould be reexamined. Finally, we further accordingly claim that a practical\ncombination of compensation and other PTQ strategy can achieve SOTA various\nrobustness. We believe that our benchmark will provide valuable recommendations\nfor the deployment of LLMs and future research on PTQ approaches.We conduct an\nrepository for our benchmark at https://github.com/zjq0455/PTQ_Benchmark.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "17 pages, 3 fugures",
    "pdf_url": "http://arxiv.org/pdf/2502.13178v4",
    "published_date": "2025-02-18 07:35:35 UTC",
    "updated_date": "2025-05-21 14:11:19 UTC"
  },
  {
    "arxiv_id": "2502.12603v1",
    "title": "Disentangling Long-Short Term State Under Unknown Interventions for Online Time Series Forecasting",
    "authors": [
      "Ruichu Cai",
      "Haiqin Huang",
      "Zhifang Jiang",
      "Zijian Li",
      "Changze Zhou",
      "Yuequn Liu",
      "Yuming Liu",
      "Zhifeng Hao"
    ],
    "abstract": "Current methods for time series forecasting struggle in the online scenario,\nsince it is difficult to preserve long-term dependency while adapting\nshort-term changes when data are arriving sequentially. Although some recent\nmethods solve this problem by controlling the updates of latent states, they\ncannot disentangle the long/short-term states, leading to the inability to\neffectively adapt to nonstationary. To tackle this challenge, we propose a\ngeneral framework to disentangle long/short-term states for online time series\nforecasting. Our idea is inspired by the observations where short-term changes\ncan be led by unknown interventions like abrupt policies in the stock market.\nBased on this insight, we formalize a data generation process with unknown\ninterventions on short-term states. Under mild assumptions, we further leverage\nthe independence of short-term states led by unknown interventions to establish\nthe identification theory to achieve the disentanglement of long/short-term\nstates. Built on this theory, we develop a long short-term disentanglement\nmodel (LSTD) to extract the long/short-term states with long/short-term\nencoders, respectively. Furthermore, the LSTD model incorporates a smooth\nconstraint to preserve the long-term dependencies and an interrupted dependency\nconstraint to enforce the forgetting of short-term dependencies, together\nboosting the disentanglement of long/short-term states. Experimental results on\nseveral benchmark datasets show that our \\textbf{LSTD} model outperforms\nexisting methods for online time series forecasting, validating its efficacy in\nreal-world applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12603v1",
    "published_date": "2025-02-18 07:31:04 UTC",
    "updated_date": "2025-02-18 07:31:04 UTC"
  },
  {
    "arxiv_id": "2502.12589v1",
    "title": "RM-PoT: Reformulating Mathematical Problems and Solving via Program of Thoughts",
    "authors": [
      "Yu Zhang",
      "Shujun Peng",
      "Nengwu Wu",
      "Xinhan Lin",
      "Yang Hu",
      "Jie Tang"
    ],
    "abstract": "Recently, substantial advancements have been made in training language models\nto carry out step-by-step reasoning for solving intricate numerical reasoning\ntasks. Beyond the methods used to solve these problems, the structure and\nformulation of the problems themselves also play a crucial role in determining\nthe performance of large language models. We observe that even small changes in\nthe surface form of mathematical problems can have a profound impact on both\nthe answer distribution and solve rate. This highlights the vulnerability of\nLLMs to surface-level variations, revealing its limited robustness when\nreasoning through complex problems. In this paper, we propose RM-PoT, a\nthree-stage framework that integrates problem reformulation (RM), code-aided\nreasoning (PoT), and domain-aware few-shot learning to address these\nlimitations. Our approach first reformulates the input problem into diverse\nsurface forms to reduce structural bias, then retrieves five semantically\naligned examples from a pre-constructed domain-specific question bank to\nprovide contextual guidance, and finally generates executable Python code for\nprecise computation.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12589v1",
    "published_date": "2025-02-18 06:54:32 UTC",
    "updated_date": "2025-02-18 06:54:32 UTC"
  },
  {
    "arxiv_id": "2502.12587v1",
    "title": "RSMLP: A light Sampled MLP Structure for Incomplete Utterance Rewrite",
    "authors": [
      "Lunjun Liu",
      "Weilai Jiang",
      "Yaonan Wang"
    ],
    "abstract": "The Incomplete Utterance Rewriting (IUR) task has garnered significant\nattention in recent years. Its goal is to reconstruct conversational utterances\nto better align with the current context, thereby enhancing comprehension. In\nthis paper, we introduce a novel and versatile lightweight method,\nRewritten-Sampled MLP (RSMLP). By employing an MLP based architecture with a\ncarefully designed down-sampling strategy, RSMLP effectively extracts latent\nsemantic information between utterances and makes appropriate edits to restore\nincomplete utterances. Due to its simple yet efficient structure, our method\nachieves competitive performance on public IUR datasets and in real-world\napplications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12587v1",
    "published_date": "2025-02-18 06:45:21 UTC",
    "updated_date": "2025-02-18 06:45:21 UTC"
  },
  {
    "arxiv_id": "2502.13177v2",
    "title": "KL Penalty Control via Perturbation for Direct Preference Optimization",
    "authors": [
      "Sangkyu Lee",
      "Janghoon Han",
      "Hosung Song",
      "Stanley Jungkyu Choi",
      "Honglak Lee",
      "Youngjae Yu"
    ],
    "abstract": "Direct Preference Optimization (DPO) demonstrates the advantage of aligning a\nlarge language model with human preference using only an offline dataset.\nHowever, DPO has the limitation that the KL penalty, which prevents excessive\ndeviation from the reference model, is static throughout the training process.\nSeveral methods claim to change this static KL penalty of DPO into a dynamic\none, but no approach can adaptively assign different KL penalties for each\npreference pair. In this paper, we propose $\\varepsilon$-Direct Preference\nOptimization ($\\varepsilon$-DPO), which allows adaptive control of the KL\npenalty strength $\\beta$ for each preference pair. Specifically,\n$\\varepsilon$-DPO adaptively controls $\\beta$ for each preference pair based on\nthe monotonicity of logits as a preference model under the perturbation of\n$\\beta$ during training. This is equivalent to adjusting the KL penalty by\nchecking whether the change in training-time temperature can lead to better\npreference confidence as preference models by simply reusing the logit of the\ncurrent policy and the reference policy. Experimental results show that the\nsimple criterion of $\\varepsilon$-DPO for KL penalty relaxation significantly\nimproves DPO compared to most existing direct alignment algorithms on general\nchatbot benchmarks and reveal that this KL penalty control criterion can\nreflect confusion as a preference model and provide an efficient KL trade-off,\nhighlighting the significance of instance-level adaptive KL penalty control in\nDPO.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint; Under review",
    "pdf_url": "http://arxiv.org/pdf/2502.13177v2",
    "published_date": "2025-02-18 06:44:10 UTC",
    "updated_date": "2025-05-19 05:56:08 UTC"
  },
  {
    "arxiv_id": "2502.12584v1",
    "title": "Enhancing Semi-supervised Learning with Noisy Zero-shot Pseudolabels",
    "authors": [
      "Jichan Chung",
      "Irene Y. Chen"
    ],
    "abstract": "Semi-supervised learning (SSL) leverages limited labeled data alongside\nabundant unlabeled data to address labeling costs in machine learning. While\nrecent foundation models enable zero-shot inference, attempts to integrate\nthese capabilities into SSL through pseudo-labeling have shown mixed results\ndue to unreliable zero-shot predictions. We present ZMT (Zero-Shot Multi-Task\nLearning), a framework that jointly optimizes zero-shot pseudo-labels and\nunsupervised representation learning objectives from contemporary SSL\napproaches. Our method introduces a multi-task learning-based mechanism that\nincorporates pseudo-labels while ensuring robustness to varying pseudo-label\nquality. Experiments across 8 datasets in vision, language, and audio domains\ndemonstrate that ZMT reduces error by up to 56% compared to traditional SSL\nmethods, with particularly compelling results when pseudo-labels are noisy and\nunreliable. ZMT represents a significant step toward making semi-supervised\nlearning more effective and accessible in resource-constrained environments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Under review for ICML 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.12584v1",
    "published_date": "2025-02-18 06:41:53 UTC",
    "updated_date": "2025-02-18 06:41:53 UTC"
  },
  {
    "arxiv_id": "2502.12581v3",
    "title": "The Majority Vote Paradigm Shift: When Popular Meets Optimal",
    "authors": [
      "Antonio Purificato",
      "Maria Sofia Bucarelli",
      "Anil Kumar Nelakanti",
      "Andrea Bacciu",
      "Fabrizio Silvestri",
      "Amin Mantrach"
    ],
    "abstract": "Reliably labelling data typically requires annotations from multiple human\nworkers. However, humans are far from being perfect. Hence, it is a common\npractice to aggregate labels gathered from multiple annotators to make a more\nconfident estimate of the true label. Among many aggregation methods, the\nsimple and well known Majority Vote (MV) selects the class label polling the\nhighest number of votes. However, despite its importance, the optimality of\nMV's label aggregation has not been extensively studied. We address this gap in\nour work by characterising the conditions under which MV achieves the\ntheoretically optimal lower bound on label estimation error. Our results\ncapture the tolerable limits on annotation noise under which MV can optimally\nrecover labels for a given class distribution. This certificate of optimality\nprovides a more principled approach to model selection for label aggregation as\nan alternative to otherwise inefficient practices that sometimes include higher\nexperts, gold labels, etc., that are all marred by the same human uncertainty\ndespite huge time and monetary costs. Experiments on both synthetic and real\nworld data corroborate our theoretical findings.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "33 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.12581v3",
    "published_date": "2025-02-18 06:37:33 UTC",
    "updated_date": "2025-03-10 11:53:28 UTC"
  },
  {
    "arxiv_id": "2502.12576v1",
    "title": "A Fuzzy Evaluation of Sentence Encoders on Grooming Risk Classification",
    "authors": [
      "Geetanjali Bihani",
      "Julia Rayz"
    ],
    "abstract": "With the advent of social media, children are becoming increasingly\nvulnerable to the risk of grooming in online settings. Detecting grooming\ninstances in an online conversation poses a significant challenge as the\ninteractions are not necessarily sexually explicit, since the predators take\ntime to build trust and a relationship with their victim. Moreover, predators\nevade detection using indirect and coded language. While previous studies have\nfine-tuned Transformers to automatically identify grooming in chat\nconversations, they overlook the impact of coded and indirect language on model\npredictions, and how these align with human perceptions of grooming. In this\npaper, we address this gap and evaluate bi-encoders on the task of classifying\ndifferent degrees of grooming risk in chat contexts, for three different\nparticipant groups, i.e. law enforcement officers, real victims, and decoys.\nUsing a fuzzy-theoretic framework, we map human assessments of grooming\nbehaviors to estimate the actual degree of grooming risk. Our analysis reveals\nthat fine-tuned models fail to tag instances where the predator uses indirect\nspeech pathways and coded language to evade detection. Further, we find that\nsuch instances are characterized by a higher presence of out-of-vocabulary\n(OOV) words in samples, causing the model to misclassify. Our findings\nhighlight the need for more robust models to identify coded language from noisy\nchat inputs in grooming contexts.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages, 2 figures. Accepted for publication in the Proceedings of\n  the NAFIPS International Conference on Fuzzy Systems, Soft Computing, and\n  Explainable AI. NAFIPS'2024",
    "pdf_url": "http://arxiv.org/pdf/2502.12576v1",
    "published_date": "2025-02-18 06:26:46 UTC",
    "updated_date": "2025-02-18 06:26:46 UTC"
  },
  {
    "arxiv_id": "2502.12575v1",
    "title": "DemonAgent: Dynamically Encrypted Multi-Backdoor Implantation Attack on LLM-based Agent",
    "authors": [
      "Pengyu Zhu",
      "Zhenhong Zhou",
      "Yuanhe Zhang",
      "Shilinlu Yan",
      "Kun Wang",
      "Sen Su"
    ],
    "abstract": "As LLM-based agents become increasingly prevalent, backdoors can be implanted\ninto agents through user queries or environment feedback, raising critical\nconcerns regarding safety vulnerabilities. However, backdoor attacks are\ntypically detectable by safety audits that analyze the reasoning process of\nagents. To this end, we propose a novel backdoor implantation strategy called\n\\textbf{Dynamically Encrypted Multi-Backdoor Implantation Attack}.\nSpecifically, we introduce dynamic encryption, which maps the backdoor into\nbenign content, effectively circumventing safety audits. To enhance\nstealthiness, we further decompose the backdoor into multiple sub-backdoor\nfragments. Based on these advancements, backdoors are allowed to bypass safety\naudits significantly. Additionally, we present AgentBackdoorEval, a dataset\ndesigned for the comprehensive evaluation of agent backdoor attacks.\nExperimental results across multiple datasets demonstrate that our method\nachieves an attack success rate nearing 100\\% while maintaining a detection\nrate of 0\\%, illustrating its effectiveness in evading safety audits. Our\nfindings highlight the limitations of existing safety mechanisms in detecting\nadvanced attacks, underscoring the urgent need for more robust defenses against\nbackdoor threats. Code and data are available at\nhttps://github.com/whfeLingYu/DemonAgent.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12575v1",
    "published_date": "2025-02-18 06:26:15 UTC",
    "updated_date": "2025-02-18 06:26:15 UTC"
  },
  {
    "arxiv_id": "2502.12574v1",
    "title": "HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading",
    "authors": [
      "Cheng Luo",
      "Zefan Cai",
      "Hanshi Sun",
      "Jinqi Xiao",
      "Bo Yuan",
      "Wen Xiao",
      "Junjie Hu",
      "Jiawei Zhao",
      "Beidi Chen",
      "Anima Anandkumar"
    ],
    "abstract": "Transformer-based large language models (LLMs) demonstrate impressive\nperformance in long context generation. Extending the context length has\ndisproportionately shifted the memory footprint of LLMs during inference to the\nkey-value cache (KV cache). In this paper, we propose HEADINFER, which offloads\nthe KV cache to CPU RAM while avoiding the need to fully store the KV cache for\nany transformer layer on the GPU. HEADINFER employs a fine-grained, head-wise\noffloading strategy, maintaining only selective attention heads KV cache on the\nGPU while computing attention output dynamically. Through roofline analysis, we\ndemonstrate that HEADINFER maintains computational efficiency while\nsignificantly reducing memory footprint. We evaluate HEADINFER on the\nLlama-3-8B model with a 1-million-token sequence, reducing the GPU memory\nfootprint of the KV cache from 128 GB to 1 GB and the total GPU memory usage\nfrom 207 GB to 17 GB, achieving a 92% reduction compared to BF16 baseline\ninference. Notably, HEADINFER enables 4-million-token inference with an 8B\nmodel on a single consumer GPU with 24GB memory (e.g., NVIDIA RTX 4090) without\napproximation methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12574v1",
    "published_date": "2025-02-18 06:26:05 UTC",
    "updated_date": "2025-02-18 06:26:05 UTC"
  },
  {
    "arxiv_id": "2502.12568v2",
    "title": "A Cognitive Writing Perspective for Constrained Long-Form Text Generation",
    "authors": [
      "Kaiyang Wan",
      "Honglin Mu",
      "Rui Hao",
      "Haoran Luo",
      "Tianle Gu",
      "Xiuying Chen"
    ],
    "abstract": "Like humans, Large Language Models (LLMs) struggle to generate high-quality\nlong-form text that adheres to strict requirements in a single pass. This\nchallenge is unsurprising, as successful human writing, according to the\nCognitive Writing Theory, is a complex cognitive process involving iterative\nplanning, translating, reviewing, and monitoring. Motivated by these cognitive\nprinciples, we aim to equip LLMs with human-like cognitive writing capabilities\nthrough CogWriter, a novel training-free framework that transforms LLM\nconstrained long-form text generation into a systematic cognitive writing\nparadigm. Our framework consists of two key modules: (1) a Planning Agent that\nperforms hierarchical planning to decompose the task, and (2) multiple\nGeneration Agents that execute these plans in parallel. The system maintains\nquality via continuous monitoring and reviewing mechanisms, which evaluate\noutputs against specified requirements and trigger necessary revisions.\nCogWriter demonstrates exceptional performance on LongGenBench, a benchmark for\ncomplex constrained long-form text generation. Even when using Qwen-2.5-14B as\nits backbone, CogWriter surpasses GPT-4o by 22% in complex instruction\ncompletion accuracy while reliably generating texts exceeding 10,000 words. We\nhope this cognitive science-inspired approach provides a paradigm for LLM\nwriting advancements:\n\\href{https://github.com/KaiyangWan/CogWriter}{CogWriter}.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "13 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.12568v2",
    "published_date": "2025-02-18 06:12:14 UTC",
    "updated_date": "2025-02-19 08:58:13 UTC"
  },
  {
    "arxiv_id": "2502.12566v2",
    "title": "Exploring the Impact of Personality Traits on LLM Bias and Toxicity",
    "authors": [
      "Shuo Wang",
      "Renhao Li",
      "Xi Chen",
      "Yulin Yuan",
      "Derek F. Wong",
      "Min Yang"
    ],
    "abstract": "With the different roles that AI is expected to play in human life, imbuing\nlarge language models (LLMs) with different personalities has attracted\nincreasing research interests. While the \"personification\" enhances human\nexperiences of interactivity and adaptability of LLMs, it gives rise to\ncritical concerns about content safety, particularly regarding bias, sentiment\nand toxicity of LLM generation. This study explores how assigning different\npersonality traits to LLMs affects the toxicity and biases of their outputs.\nLeveraging the widely accepted HEXACO personality framework developed in social\npsychology, we design experimentally sound prompts to test three LLMs'\nperformance on three toxic and bias benchmarks. The findings demonstrate the\nsensitivity of all three models to HEXACO personality traits and, more\nimportantly, a consistent variation in the biases, negative sentiment and\ntoxicity of their output. In particular, adjusting the levels of several\npersonality traits can effectively reduce bias and toxicity in model\nperformance, similar to humans' correlations between personality traits and\ntoxic behaviors. The findings highlight the additional need to examine content\nsafety besides the efficiency of training or fine-tuning methods for LLM\npersonification. They also suggest a potential for the adjustment of\npersonalities to be a simple and low-cost method to conduct controlled text\ngeneration.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12566v2",
    "published_date": "2025-02-18 06:07:09 UTC",
    "updated_date": "2025-02-21 06:01:30 UTC"
  },
  {
    "arxiv_id": "2502.12563v1",
    "title": "Evaluating Language Models on Grooming Risk Estimation Using Fuzzy Theory",
    "authors": [
      "Geetanjali Bihani",
      "Tatiana Ringenberg",
      "Julia Rayz"
    ],
    "abstract": "Encoding implicit language presents a challenge for language models,\nespecially in high-risk domains where maintaining high precision is important.\nAutomated detection of online child grooming is one such critical domain, where\npredators manipulate victims using a combination of explicit and implicit\nlanguage to convey harmful intentions. While recent studies have shown the\npotential of Transformer language models like SBERT for preemptive grooming\ndetection, they primarily depend on surface-level features and approximate real\nvictim grooming processes using vigilante and law enforcement conversations.\nThe question of whether these features and approximations are reasonable has\nnot been addressed thus far. In this paper, we address this gap and study\nwhether SBERT can effectively discern varying degrees of grooming risk inherent\nin conversations, and evaluate its results across different participant groups.\nOur analysis reveals that while fine-tuning aids language models in learning to\nassign grooming scores, they show high variance in predictions, especially for\ncontexts containing higher degrees of grooming risk. These errors appear in\ncases that 1) utilize indirect speech pathways to manipulate victims and 2)\nlack sexually explicit content. This finding underscores the necessity for\nrobust modeling of indirect speech acts by language models, particularly those\nemployed by predators.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, 2 figures. Accepted for publication in the Proceedings of\n  the NAFIPS International Conference on Fuzzy Systems, Soft Computing, and\n  Explainable AI. NAFIPS'2024",
    "pdf_url": "http://arxiv.org/pdf/2502.12563v1",
    "published_date": "2025-02-18 05:59:54 UTC",
    "updated_date": "2025-02-18 05:59:54 UTC"
  },
  {
    "arxiv_id": "2502.12558v4",
    "title": "MomentSeeker: A Task-Oriented Benchmark For Long-Video Moment Retrieval",
    "authors": [
      "Huaying Yuan",
      "Jian Ni",
      "Zheng Liu",
      "Yueze Wang",
      "Junjie Zhou",
      "Zhengyang Liang",
      "Bo Zhao",
      "Zhao Cao",
      "Zhicheng Dou",
      "Ji-Rong Wen"
    ],
    "abstract": "Accurately locating key moments within long videos is crucial for solving\nlong video understanding (LVU) tasks. However, existing benchmarks are either\nseverely limited in terms of video length and task diversity, or they focus\nsolely on the end-to-end LVU performance, making them inappropriate for\nevaluating whether key moments can be accurately accessed. To address this\nchallenge, we propose MomentSeeker, a novel benchmark for long-video moment\nretrieval (LMVR), distinguished by the following features. First, it is created\nbased on long and diverse videos, averaging over 1200 seconds in duration and\ncollected from various domains, e.g., movie, anomaly, egocentric, and sports.\nSecond, it covers a variety of real-world scenarios in three levels:\nglobal-level, event-level, object-level, covering common tasks like action\nrecognition, object localization, and causal reasoning, etc. Third, it\nincorporates rich forms of queries, including text-only queries,\nimage-conditioned queries, and video-conditioned queries. On top of\nMomentSeeker, we conduct comprehensive experiments for both generation-based\napproaches (directly using MLLMs) and retrieval-based approaches (leveraging\nvideo retrievers). Our results reveal the significant challenges in long-video\nmoment retrieval in terms of accuracy and efficiency, despite improvements from\nthe latest long-video MLLMs and task-specific fine-tuning. We have publicly\nreleased MomentSeeker(https://yhy-2000.github.io/MomentSeeker/) to facilitate\nfuture research in this area.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12558v4",
    "published_date": "2025-02-18 05:50:23 UTC",
    "updated_date": "2025-05-20 03:30:44 UTC"
  },
  {
    "arxiv_id": "2502.12552v1",
    "title": "LLM Safety for Children",
    "authors": [
      "Prasanjit Rath",
      "Hari Shrawgi",
      "Parag Agrawal",
      "Sandipan Dandapat"
    ],
    "abstract": "This paper analyzes the safety of Large Language Models (LLMs) in\ninteractions with children below age of 18 years. Despite the transformative\napplications of LLMs in various aspects of children's lives such as education\nand therapy, there remains a significant gap in understanding and mitigating\npotential content harms specific to this demographic. The study acknowledges\nthe diverse nature of children often overlooked by standard safety evaluations\nand proposes a comprehensive approach to evaluating LLM safety specifically for\nchildren. We list down potential risks that children may encounter when using\nLLM powered applications. Additionally we develop Child User Models that\nreflect the varied personalities and interests of children informed by\nliterature in child care and psychology. These user models aim to bridge the\nexisting gap in child safety literature across various fields. We utilize Child\nUser Models to evaluate the safety of six state of the art LLMs. Our\nobservations reveal significant safety gaps in LLMs particularly in categories\nharmful to children but not adults",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12552v1",
    "published_date": "2025-02-18 05:26:27 UTC",
    "updated_date": "2025-02-18 05:26:27 UTC"
  },
  {
    "arxiv_id": "2502.12548v1",
    "title": "Improving the Stability of GNN Force Field Models by Reducing Feature Correlation",
    "authors": [
      "Yujie Zeng",
      "Wenlong He",
      "Ihor Vasyltsov",
      "Jiaxin Wei",
      "Ying Zhang",
      "Lin Chen",
      "Yuehua Dai"
    ],
    "abstract": "Recently, Graph Neural Network based Force Field (GNNFF) models are widely\nused in Molecular Dynamics (MD) simulation, which is one of the most\ncost-effective means in semiconductor material research. However, even such\nmodels provide high accuracy in energy and force Mean Absolute Error (MAE) over\ntrained (in-distribution) datasets, they often become unstable during long-time\nMD simulation when used for out-of-distribution datasets. In this paper, we\npropose a feature correlation based method for GNNFF models to enhance the\nstability of MD simulation. We reveal the negative relationship between feature\ncorrelation and the stability of GNNFF models, and design a loss function with\na dynamic loss coefficient scheduler to reduce edge feature correlation that\ncan be applied in general GNNFF training. We also propose an empirical metric\nto evaluate the stability in MD simulation. Experiments show our method can\nsignificantly improve stability for GNNFF models especially in\nout-of-distribution data with less than 3% computational overhead. For example,\nwe can ensure the stable MD simulation time from 0.03ps to 10ps for Allegro\nmodel.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12548v1",
    "published_date": "2025-02-18 05:18:22 UTC",
    "updated_date": "2025-02-18 05:18:22 UTC"
  },
  {
    "arxiv_id": "2502.12542v1",
    "title": "Computing Voting Rules with Improvement Feedback",
    "authors": [
      "Evi Micha",
      "Vasilis Varsamis"
    ],
    "abstract": "Aggregating preferences under incomplete or constrained feedback is a\nfundamental problem in social choice and related domains. While prior work has\nestablished strong impossibility results for pairwise comparisons, this paper\nextends the inquiry to improvement feedback, where voters express incremental\nadjustments rather than complete preferences. We provide a complete\ncharacterization of the positional scoring rules that can be computed given\nimprovement feedback. Interestingly, while plurality is learnable under\nimprovement feedback--unlike with pairwise feedback--strong impossibility\nresults persist for many other positional scoring rules. Furthermore, we show\nthat improvement feedback, unlike pairwise feedback, does not suffice for the\ncomputation of any Condorcet-consistent rule. We complement our theoretical\nfindings with experimental results, providing further insights into the\npractical implications of improvement feedback for preference aggregation.",
    "categories": [
      "cs.GT",
      "cs.AI"
    ],
    "primary_category": "cs.GT",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12542v1",
    "published_date": "2025-02-18 05:05:46 UTC",
    "updated_date": "2025-02-18 05:05:46 UTC"
  },
  {
    "arxiv_id": "2502.12537v2",
    "title": "Finding Optimal Trading History in Reinforcement Learning for Stock Market Trading",
    "authors": [
      "Sina Montazeri",
      "Haseebullah Jumakhan",
      "Amir Mirzaeinia"
    ],
    "abstract": "This paper investigates the optimization of temporal windows in Financial\nDeep Reinforcement Learning (DRL) models using 2D Convolutional Neural Networks\n(CNNs). We introduce a novel approach to treating the temporal field as a\nhyperparameter and examine its impact on model performance across various\ndatasets and feature arrangements. We introduce a new hyperparameter for the\nCNN policy, proposing that this temporal field can and should be treated as a\nhyperparameter for these models. We examine the significance of this temporal\nfield by iteratively expanding the window of observations presented to the CNN\npolicy during the deep reinforcement learning process. Our iterative process\ninvolves progressively increasing the observation period from two weeks to\ntwelve weeks, allowing us to examine the effects of different temporal windows\non the model's performance. This window expansion is implemented in two\nsettings. In one setting, we rearrange the features in the dataset to group\nthem by company, allowing the model to have a full view of company data in its\nobservation window and CNN kernel. In the second setting, we do not group the\nfeatures by company, and features are arranged by category. Our study reveals\nthat shorter temporal windows are most effective when no feature rearrangement\nto group per company is in effect. However, the model will utilize longer\ntemporal windows and yield better performance once we introduce the feature\nrearrangement. To examine the consistency of our findings, we repeated our\nexperiment on two datasets containing the same thirty companies from the Dow\nJones Index but with different features in each dataset and consistently\nobserved the above-mentioned patterns. The result is a trading model\nsignificantly outperforming global financial services firms such as the Global\nX Guru by the established Mirae Asset.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12537v2",
    "published_date": "2025-02-18 04:50:00 UTC",
    "updated_date": "2025-02-19 10:24:59 UTC"
  },
  {
    "arxiv_id": "2502.12536v1",
    "title": "An Algorithm Board in Neural Decoding",
    "authors": [
      "Jingyi Feng",
      "Kai Yang"
    ],
    "abstract": "Understanding the mechanisms of neural encoding and decoding has always been\na highly interesting research topic in fields such as neuroscience and\ncognitive intelligence. In prior studies, some researchers identified a\nsymmetry in neural data decoded by unsupervised methods in motor scenarios and\nconstructed a cognitive learning system based on this pattern (i.e., symmetry).\nNevertheless, the distribution state of the data flow that significantly\ninfluences neural decoding positions still remains a mystery within the system,\nwhich further restricts the enhancement of the system's interpretability. Based\non this, this paper mainly explores changes in the distribution state within\nthe system from the machine learning and mathematical statistics perspectives.\nIn the experiment, we assessed the correctness of this symmetry using various\ntools and indicators commonly utilized in mathematics and statistics. According\nto the experimental results, the normal distribution (or Gaussian distribution)\nplays a crucial role in the decoding of prediction positions within the system.\nEventually, an algorithm board similar to the Galton board was built to serve\nas the mathematical foundation of the discovered symmetry.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "16 pages, 10 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2502.12536v1",
    "published_date": "2025-02-18 04:39:35 UTC",
    "updated_date": "2025-02-18 04:39:35 UTC"
  },
  {
    "arxiv_id": "2502.12532v3",
    "title": "CityEQA: A Hierarchical LLM Agent on Embodied Question Answering Benchmark in City Space",
    "authors": [
      "Yong Zhao",
      "Kai Xu",
      "Zhengqiu Zhu",
      "Yue Hu",
      "Zhiheng Zheng",
      "Yingfeng Chen",
      "Yatai Ji",
      "Chen Gao",
      "Yong Li",
      "Jincai Huang"
    ],
    "abstract": "Embodied Question Answering (EQA) has primarily focused on indoor\nenvironments, leaving the complexities of urban settings-spanning environment,\naction, and perception-largely unexplored. To bridge this gap, we introduce\nCityEQA, a new task where an embodied agent answers open-vocabulary questions\nthrough active exploration in dynamic city spaces. To support this task, we\npresent CityEQA-EC, the first benchmark dataset featuring 1,412 human-annotated\ntasks across six categories, grounded in a realistic 3D urban simulator.\nMoreover, we propose Planner-Manager-Actor (PMA), a novel agent tailored for\nCityEQA. PMA enables long-horizon planning and hierarchical task execution: the\nPlanner breaks down the question answering into sub-tasks, the Manager\nmaintains an object-centric cognitive map for spatial reasoning during the\nprocess control, and the specialized Actors handle navigation, exploration, and\ncollection sub-tasks. Experiments demonstrate that PMA achieves 60.7% of\nhuman-level answering accuracy, significantly outperforming competitive\nbaselines. While promising, the performance gap compared to humans highlights\nthe need for enhanced visual reasoning in CityEQA. This work paves the way for\nfuture advancements in urban spatial intelligence. Dataset and code are\navailable at https://github.com/BiluYong/CityEQA.git.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12532v3",
    "published_date": "2025-02-18 04:36:15 UTC",
    "updated_date": "2025-05-22 00:44:13 UTC"
  },
  {
    "arxiv_id": "2502.12531v2",
    "title": "GSCE: A Prompt Framework with Enhanced Reasoning for Reliable LLM-driven Drone Control",
    "authors": [
      "Wenhao Wang",
      "Yanyan Li",
      "Long Jiao",
      "Jiawei Yuan"
    ],
    "abstract": "The integration of Large Language Models (LLMs) into robotic control,\nincluding drones, has the potential to revolutionize autonomous systems.\nResearch studies have demonstrated that LLMs can be leveraged to support\nrobotic operations. However, when facing tasks with complex reasoning, concerns\nand challenges are raised about the reliability of solutions produced by LLMs.\nIn this paper, we propose a prompt framework with enhanced reasoning to enable\nreliable LLM-driven control for drones. Our framework consists of novel\ntechnical components designed using Guidelines, Skill APIs, Constraints, and\nExamples, namely GSCE. GSCE is featured by its reliable and\nconstraint-compliant code generation. We performed thorough experiments using\nGSCE for the control of drones with a wide level of task complexities. Our\nexperiment results demonstrate that GSCE can significantly improve task success\nrates and completeness compared to baseline approaches, highlighting its\npotential for reliable LLM-driven autonomous drone systems.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages",
    "pdf_url": "http://arxiv.org/pdf/2502.12531v2",
    "published_date": "2025-02-18 04:35:17 UTC",
    "updated_date": "2025-04-07 20:45:14 UTC"
  },
  {
    "arxiv_id": "2502.12525v1",
    "title": "From Abstract to Actionable: Pairwise Shapley Values for Explainable AI",
    "authors": [
      "Jiaxin Xu",
      "Hung Chau",
      "Angela Burden"
    ],
    "abstract": "Explainable AI (XAI) is critical for ensuring transparency, accountability,\nand trust in machine learning systems as black-box models are increasingly\ndeployed within high-stakes domains. Among XAI methods, Shapley values are\nwidely used for their fairness and consistency axioms. However, prevalent\nShapley value approximation methods commonly rely on abstract baselines or\ncomputationally intensive calculations, which can limit their interpretability\nand scalability. To address such challenges, we propose Pairwise Shapley\nValues, a novel framework that grounds feature attributions in explicit,\nhuman-relatable comparisons between pairs of data instances proximal in feature\nspace. Our method introduces pairwise reference selection combined with\nsingle-value imputation to deliver intuitive, model-agnostic explanations while\nsignificantly reducing computational overhead. Here, we demonstrate that\nPairwise Shapley Values enhance interpretability across diverse regression and\nclassification scenarios--including real estate pricing, polymer property\nprediction, and drug discovery datasets. We conclude that the proposed methods\nenable more transparent AI systems and advance the real-world applicability of\nXAI.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12525v1",
    "published_date": "2025-02-18 04:20:18 UTC",
    "updated_date": "2025-02-18 04:20:18 UTC"
  },
  {
    "arxiv_id": "2502.12524v1",
    "title": "YOLOv12: Attention-Centric Real-Time Object Detectors",
    "authors": [
      "Yunjie Tian",
      "Qixiang Ye",
      "David Doermann"
    ],
    "abstract": "Enhancing the network architecture of the YOLO framework has been crucial for\na long time, but has focused on CNN-based improvements despite the proven\nsuperiority of attention mechanisms in modeling capabilities. This is because\nattention-based models cannot match the speed of CNN-based models. This paper\nproposes an attention-centric YOLO framework, namely YOLOv12, that matches the\nspeed of previous CNN-based ones while harnessing the performance benefits of\nattention mechanisms. YOLOv12 surpasses all popular real-time object detectors\nin accuracy with competitive speed. For example, YOLOv12-N achieves 40.6% mAP\nwith an inference latency of 1.64 ms on a T4 GPU, outperforming advanced\nYOLOv10-N / YOLOv11-N by 2.1%/1.2% mAP with a comparable speed. This advantage\nextends to other model scales. YOLOv12 also surpasses end-to-end real-time\ndetectors that improve DETR, such as RT-DETR / RT-DETRv2: YOLOv12-S beats\nRT-DETR-R18 / RT-DETRv2-R18 while running 42% faster, using only 36% of the\ncomputation and 45% of the parameters. More comparisons are shown in Figure 1.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "https://github.com/sunsmarterjie/yolov12",
    "pdf_url": "http://arxiv.org/pdf/2502.12524v1",
    "published_date": "2025-02-18 04:20:14 UTC",
    "updated_date": "2025-02-18 04:20:14 UTC"
  },
  {
    "arxiv_id": "2502.12521v1",
    "title": "Inference-Time Computations for LLM Reasoning and Planning: A Benchmark and Insights",
    "authors": [
      "Shubham Parashar",
      "Blake Olson",
      "Sambhav Khurana",
      "Eric Li",
      "Hongyi Ling",
      "James Caverlee",
      "Shuiwang Ji"
    ],
    "abstract": "We examine the reasoning and planning capabilities of large language models\n(LLMs) in solving complex tasks. Recent advances in inference-time techniques\ndemonstrate the potential to enhance LLM reasoning without additional training\nby exploring intermediate steps during inference. Notably, OpenAI's o1 model\nshows promising performance through its novel use of multi-step reasoning and\nverification. Here, we explore how scaling inference-time techniques can\nimprove reasoning and planning, focusing on understanding the tradeoff between\ncomputational cost and performance. To this end, we construct a comprehensive\nbenchmark, known as Sys2Bench, and perform extensive experiments evaluating\nexisting inference-time techniques on eleven diverse tasks across five\ncategories, including arithmetic reasoning, logical reasoning, common sense\nreasoning, algorithmic reasoning, and planning. Our findings indicate that\nsimply scaling inference-time computation has limitations, as no single\ninference-time technique consistently performs well across all reasoning and\nplanning tasks.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12521v1",
    "published_date": "2025-02-18 04:11:29 UTC",
    "updated_date": "2025-02-18 04:11:29 UTC"
  },
  {
    "arxiv_id": "2502.13176v2",
    "title": "BaKlaVa -- Budgeted Allocation of KV cache for Long-context Inference",
    "authors": [
      "Ahmed Burak Gulhan",
      "Krishna Teja Chitty-Venkata",
      "Murali Emani",
      "Mahmut Kandemir",
      "Venkatram Vishwanath"
    ],
    "abstract": "In Large Language Model (LLM) inference, Key-Value (KV) caches (KV-caches)\nare essential for reducing time complexity. However, they result in a linear\nincrease in GPU memory as the context length grows. While recent work explores\nKV-cache eviction and compression policies to reduce memory usage, they often\nconsider uniform KV-caches across all attention heads, leading to suboptimal\nperformance. We introduce BaKlaVa, a method to allocate optimal memory for\nindividual KV-caches across the model by estimating the importance of each\nKV-cache. Our empirical analysis demonstrates that not all KV-caches are\nequally critical for LLM performance. Using a one-time profiling approach,\nBaKlaVa assigns optimal memory budgets to each KV-cache. We evaluated our\nmethod on LLaMA-3-8B, and Qwen2.5-7B models, achieving up to a 70\\% compression\nratio while keeping baseline performance and delivering up to an\norder-of-magnitude accuracy improvement at higher compression levels.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.13176v2",
    "published_date": "2025-02-18 04:08:29 UTC",
    "updated_date": "2025-02-24 01:28:27 UTC"
  },
  {
    "arxiv_id": "2502.12511v2",
    "title": "Myna: Masking-Based Contrastive Learning of Musical Representations",
    "authors": [
      "Ori Yonay",
      "Tracy Hammond",
      "Tianbao Yang"
    ],
    "abstract": "We present Myna, a simple yet effective approach for self-supervised musical\nrepresentation learning. Built on a contrastive learning framework, Myna\nintroduces two key innovations: (1) the use of a Vision Transformer (ViT) on\nmel-spectrograms as the backbone and (2) a novel data augmentation strategy,\ntoken masking, that masks 90 percent of spectrogram tokens. These innovations\ndeliver both effectiveness and efficiency: (i) Token masking enables a\nsignificant increase in per-GPU batch size, from 48 or 120 in prior methods\n(CLMR, MULE) to 4096. (ii) By avoiding traditional augmentations, Myna retains\npitch sensitivity, enhancing performance in tasks like key detection. (iii) The\nuse of vertical patches allows the model to better capture critical features\nfor key detection. Our hybrid model, Myna-22M-Hybrid, processes both 16x16 and\n128x2 patches, achieving state-of-the-art results. Trained on a single GPU, it\noutperforms MULE (62M) on average and rivals MERT-95M, which was trained on 16\nand 64 GPUs, respectively. Additionally, it surpasses MERT-95M-public,\nestablishing itself as the best-performing model trained on publicly available\ndata. We release our code and models to promote reproducibility and facilitate\nfuture research.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12511v2",
    "published_date": "2025-02-18 03:54:25 UTC",
    "updated_date": "2025-02-19 02:47:43 UTC"
  },
  {
    "arxiv_id": "2502.12509v4",
    "title": "LegalCore: A Dataset for Event Coreference Resolution in Legal Documents",
    "authors": [
      "Kangda Wei",
      "Xi Shi",
      "Jonathan Tong",
      "Sai Ramana Reddy",
      "Anandhavelu Natarajan",
      "Rajiv Jain",
      "Aparna Garimella",
      "Ruihong Huang"
    ],
    "abstract": "Recognizing events and their coreferential mentions in a document is\nessential for understanding semantic meanings of text. The existing research on\nevent coreference resolution is mostly limited to news articles. In this paper,\nwe present the first dataset for the legal domain, LegalCore, which has been\nannotated with comprehensive event and event coreference information. The legal\ncontract documents we annotated in this dataset are several times longer than\nnews articles, with an average length of around 25k tokens per document. The\nannotations show that legal documents have dense event mentions and feature\nboth short-distance and super long-distance coreference links between event\nmentions. We further benchmark mainstream Large Language Models (LLMs) on this\ndataset for both event detection and event coreference resolution tasks, and\nfind that this dataset poses significant challenges for state-of-the-art\nopen-source and proprietary LLMs, which perform significantly worse than a\nsupervised baseline. We will publish the dataset as well as the code.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Need company internal approval before public release",
    "pdf_url": "http://arxiv.org/pdf/2502.12509v4",
    "published_date": "2025-02-18 03:47:53 UTC",
    "updated_date": "2025-03-20 16:45:57 UTC"
  },
  {
    "arxiv_id": "2502.12507v1",
    "title": "Mixture of Attention Yields Accurate Results for Tabular Data",
    "authors": [
      "Xuechen Li",
      "Yupeng Li",
      "Jian Liu",
      "Xiaolin Jin",
      "Tian Yang",
      "Xin Hu"
    ],
    "abstract": "Tabular data inherently exhibits significant feature heterogeneity, but\nexisting transformer-based methods lack specialized mechanisms to handle this\nproperty. To bridge the gap, we propose MAYA, an encoder-decoder\ntransformer-based framework. In the encoder, we design a Mixture of Attention\n(MOA) that constructs multiple parallel attention branches and averages the\nfeatures at each branch, effectively fusing heterogeneous features while\nlimiting parameter growth. Additionally, we employ collaborative learning with\na dynamic consistency weight constraint to produce more robust representations.\nIn the decoder stage, cross-attention is utilized to seamlessly integrate\ntabular data with corresponding label features. This dual-attention mechanism\neffectively captures both intra-instance and inter-instance interactions. We\nevaluate the proposed method on a wide range of datasets and compare it with\nother state-of-the-art transformer-based methods. Extensive experiments\ndemonstrate that our model achieves superior performance among\ntransformer-based methods in both tabular classification and regression tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.12507v1",
    "published_date": "2025-02-18 03:43:42 UTC",
    "updated_date": "2025-02-18 03:43:42 UTC"
  },
  {
    "arxiv_id": "2502.13175v2",
    "title": "Towards Robust and Secure Embodied AI: A Survey on Vulnerabilities and Attacks",
    "authors": [
      "Wenpeng Xing",
      "Minghao Li",
      "Mohan Li",
      "Meng Han"
    ],
    "abstract": "Embodied AI systems, including robots and autonomous vehicles, are\nincreasingly integrated into real-world applications, where they encounter a\nrange of vulnerabilities stemming from both environmental and system-level\nfactors. These vulnerabilities manifest through sensor spoofing, adversarial\nattacks, and failures in task and motion planning, posing significant\nchallenges to robustness and safety. Despite the growing body of research,\nexisting reviews rarely focus specifically on the unique safety and security\nchallenges of embodied AI systems. Most prior work either addresses general AI\nvulnerabilities or focuses on isolated aspects, lacking a dedicated and unified\nframework tailored to embodied AI. This survey fills this critical gap by: (1)\ncategorizing vulnerabilities specific to embodied AI into exogenous (e.g.,\nphysical attacks, cybersecurity threats) and endogenous (e.g., sensor failures,\nsoftware flaws) origins; (2) systematically analyzing adversarial attack\nparadigms unique to embodied AI, with a focus on their impact on perception,\ndecision-making, and embodied interaction; (3) investigating attack vectors\ntargeting large vision-language models (LVLMs) and large language models (LLMs)\nwithin embodied systems, such as jailbreak attacks and instruction\nmisinterpretation; (4) evaluating robustness challenges in algorithms for\nembodied perception, decision-making, and task planning; and (5) proposing\ntargeted strategies to enhance the safety and reliability of embodied AI\nsystems. By integrating these dimensions, we provide a comprehensive framework\nfor understanding the interplay between vulnerabilities and safety in embodied\nAI.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.13175v2",
    "published_date": "2025-02-18 03:38:07 UTC",
    "updated_date": "2025-02-25 12:49:59 UTC"
  },
  {
    "arxiv_id": "2502.15786v1",
    "title": "MindLLM: A Subject-Agnostic and Versatile Model for fMRI-to-Text Decoding",
    "authors": [
      "Weikang Qiu",
      "Zheng Huang",
      "Haoyu Hu",
      "Aosong Feng",
      "Yujun Yan",
      "Rex Ying"
    ],
    "abstract": "Decoding functional magnetic resonance imaging (fMRI) signals into text has\nbeen a key challenge in the neuroscience community, with the potential to\nadvance brain-computer interfaces and uncover deeper insights into brain\nmechanisms. However, existing approaches often struggle with suboptimal\npredictive performance, limited task variety, and poor generalization across\nsubjects. In response to this, we propose MindLLM, a model designed for\nsubject-agnostic and versatile fMRI-to-text decoding. MindLLM consists of an\nfMRI encoder and an off-the-shelf LLM. The fMRI encoder employs a\nneuroscience-informed attention mechanism, which is capable of accommodating\nsubjects with varying input shapes and thus achieves high-performance\nsubject-agnostic decoding. Moreover, we introduce Brain Instruction Tuning\n(BIT), a novel approach that enhances the model's ability to capture diverse\nsemantic representations from fMRI signals, facilitating more versatile\ndecoding. We evaluate MindLLM on comprehensive fMRI-to-text benchmarks. Results\ndemonstrate that our model outperforms the baselines, improving downstream\ntasks by 12.0%, unseen subject generalization by 16.4%, and novel task\nadaptation by 25.0%. Furthermore, the attention patterns in MindLLM provide\ninterpretable insights into its decision-making process.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.LG",
      "eess.SP"
    ],
    "primary_category": "q-bio.NC",
    "comment": "17 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.15786v1",
    "published_date": "2025-02-18 03:27:37 UTC",
    "updated_date": "2025-02-18 03:27:37 UTC"
  },
  {
    "arxiv_id": "2502.12494v1",
    "title": "EDGE: Efficient Data Selection for LLM Agents via Guideline Effectiveness",
    "authors": [
      "Yunxiao Zhang",
      "Guanming Xiong",
      "Haochen Li",
      "Wen Zhao"
    ],
    "abstract": "Large Language Models (LLMs) have shown remarkable capabilities as AI agents.\nHowever, existing methods for enhancing LLM-agent abilities often lack a focus\non data quality, leading to inefficiencies and suboptimal results in both\nfine-tuning and prompt engineering. To address this issue, we introduce EDGE, a\nnovel approach for identifying informative samples without needing golden\nanswers. We propose the Guideline Effectiveness (GE) metric, which selects\nchallenging samples by measuring the impact of human-provided guidelines in\nmulti-turn interaction tasks. A low GE score indicates that the human expertise\nrequired for a sample is missing from the guideline, making the sample more\ninformative. By selecting samples with low GE scores, we can improve the\nefficiency and outcomes of both prompt engineering and fine-tuning processes\nfor LLMs. Extensive experiments validate the performance of our method. Our\nmethod achieves competitive results on the HotpotQA and WebShop and datasets,\nrequiring 75\\% and 50\\% less data, respectively, while outperforming existing\nmethods. We also provide a fresh perspective on the data quality of LLM-agent\nfine-tuning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12494v1",
    "published_date": "2025-02-18 03:21:18 UTC",
    "updated_date": "2025-02-18 03:21:18 UTC"
  },
  {
    "arxiv_id": "2502.12492v1",
    "title": "Boost, Disentangle, and Customize: A Robust System2-to-System1 Pipeline for Code Generation",
    "authors": [
      "Kounianhua Du",
      "Hanjing Wang",
      "Jianxing Liu",
      "Jizheng Chen",
      "Xinyi Dai",
      "Yasheng Wang",
      "Ruiming Tang",
      "Yong Yu",
      "Jun Wang",
      "Weinan Zhang"
    ],
    "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious domains, particularly in system 1 tasks, yet the intricacies of their\nproblem-solving mechanisms in system 2 tasks are not sufficiently explored.\nRecent research on System2-to-System1 methods surge, exploring the System 2\nreasoning knowledge via inference-time computation and compressing the explored\nknowledge into System 1 process. In this paper, we focus on code generation,\nwhich is a representative System 2 task, and identify two primary challenges:\n(1) the complex hidden reasoning processes and (2) the heterogeneous data\ndistributions that complicate the exploration and training of robust LLM\nsolvers. To tackle these issues, we propose a novel BDC framework that explores\ninsightful System 2 knowledge of LLMs using a MC-Tree-Of-Agents algorithm with\nmutual \\textbf{B}oosting, \\textbf{D}isentangles the heterogeneous training data\nfor composable LoRA-experts, and obtain \\textbf{C}ustomized problem solver for\neach data instance with an input-aware hypernetwork to weight over the\nLoRA-experts, offering effectiveness, flexibility, and robustness. This\nframework leverages multiple LLMs through mutual verification and boosting,\nintegrated into a Monte-Carlo Tree Search process enhanced by reflection-based\npruning and refinement. Additionally, we introduce the DisenLora algorithm,\nwhich clusters heterogeneous data to fine-tune LLMs into composable Lora\nexperts, enabling the adaptive generation of customized problem solvers through\nan input-aware hypernetwork. This work lays the groundwork for advancing LLM\ncapabilities in complex reasoning tasks, offering a novel System2-to-System1\nsolution.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12492v1",
    "published_date": "2025-02-18 03:20:50 UTC",
    "updated_date": "2025-02-18 03:20:50 UTC"
  },
  {
    "arxiv_id": "2502.12489v1",
    "title": "A Comprehensive Survey on Generative AI for Video-to-Music Generation",
    "authors": [
      "Shulei Ji",
      "Songruoyao Wu",
      "Zihao Wang",
      "Shuyu Li",
      "Kejun Zhang"
    ],
    "abstract": "The burgeoning growth of video-to-music generation can be attributed to the\nascendancy of multimodal generative models. However, there is a lack of\nliterature that comprehensively combs through the work in this field. To fill\nthis gap, this paper presents a comprehensive review of video-to-music\ngeneration using deep generative AI techniques, focusing on three key\ncomponents: visual feature extraction, music generation frameworks, and\nconditioning mechanisms. We categorize existing approaches based on their\ndesigns for each component, clarifying the roles of different strategies.\nPreceding this, we provide a fine-grained classification of video and music\nmodalities, illustrating how different categories influence the design of\ncomponents within the generation pipelines. Furthermore, we summarize available\nmultimodal datasets and evaluation metrics while highlighting ongoing\nchallenges in the field.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12489v1",
    "published_date": "2025-02-18 03:18:54 UTC",
    "updated_date": "2025-02-18 03:18:54 UTC"
  },
  {
    "arxiv_id": "2502.12485v2",
    "title": "Safe at the Margins: A General Approach to Safety Alignment in Low-Resource English Languages -- A Singlish Case Study",
    "authors": [
      "Isaac Lim",
      "Shaun Khoo",
      "Roy Ka-Wei Lee",
      "Watson Chua",
      "Jia Yi Goh",
      "Jessica Foo"
    ],
    "abstract": "Ensuring the safety of Large Language Models (LLMs) in diverse linguistic\nsettings remains challenging, particularly for low-resource languages. Existing\nsafety alignment methods are English-centric, limiting their effectiveness. We\nsystematically compare Supervised Fine-Tuning (SFT), Direct Preference\nOptimization (DPO), and Kahneman-Tversky Optimization (KTO) for aligning\nSEA-Lion-v2.1-Instruct, a Llama 3-8B variant, to reduce toxicity in Singlish.\nOur results show that SFT+KTO achieves superior safety alignment with higher\nsample efficiency than DPO. Additionally, we introduce KTO-S, which enhances\nstability via improved KL divergence regularization. Our approach reduces\nSinglish toxicity by 99\\%, generalizes to TOXIGEN, and maintains strong\nperformance on standard LLM benchmarks, providing a scalable framework for\nsafer AI deployment in multilingual contexts.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12485v2",
    "published_date": "2025-02-18 03:11:06 UTC",
    "updated_date": "2025-04-08 04:50:41 UTC"
  },
  {
    "arxiv_id": "2502.12484v1",
    "title": "LocalEscaper: A Weakly-supervised Framework with Regional Reconstruction for Scalable Neural TSP Solvers",
    "authors": [
      "Junrui Wen",
      "Yifei Li",
      "Bart Selman",
      "Kun He"
    ],
    "abstract": "Neural solvers have shown significant potential in solving the Traveling\nSalesman Problem (TSP), yet current approaches face significant challenges.\nSupervised learning (SL)-based solvers require large amounts of high-quality\nlabeled data, while reinforcement learning (RL)-based solvers, though less\ndependent on such data, often suffer from inefficiencies. To address these\nlimitations, we propose LocalEscaper, a novel weakly-supervised learning\nframework for large-scale TSP. LocalEscaper effectively combines the advantages\nof both SL and RL, enabling effective training on datasets with low-quality\nlabels. To further enhance solution quality, we introduce a regional\nreconstruction strategy, which mitigates the problem of local optima, a common\nissue in existing local reconstruction methods. Additionally, we propose a\nlinear-complexity attention mechanism that reduces computational overhead,\nenabling the efficient solution of large-scale TSPs without sacrificing\nperformance. Experimental results on both synthetic and real-world datasets\ndemonstrate that LocalEscaper outperforms existing neural solvers, achieving\nstate-of-the-art results. Notably, it sets a new benchmark for scalability and\nefficiency, solving TSP instances with up to 50,000 cities.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12484v1",
    "published_date": "2025-02-18 03:10:27 UTC",
    "updated_date": "2025-02-18 03:10:27 UTC"
  },
  {
    "arxiv_id": "2502.12481v1",
    "title": "Predicate Hierarchies Improve Few-Shot State Classification",
    "authors": [
      "Emily Jin",
      "Joy Hsu",
      "Jiajun Wu"
    ],
    "abstract": "State classification of objects and their relations is core to many\nlong-horizon tasks, particularly in robot planning and manipulation. However,\nthe combinatorial explosion of possible object-predicate combinations, coupled\nwith the need to adapt to novel real-world environments, makes it a desideratum\nfor state classification models to generalize to novel queries with few\nexamples. To this end, we propose PHIER, which leverages predicate hierarchies\nto generalize effectively in few-shot scenarios. PHIER uses an object-centric\nscene encoder, self-supervised losses that infer semantic relations between\npredicates, and a hyperbolic distance metric that captures hierarchical\nstructure; it learns a structured latent space of image-predicate pairs that\nguides reasoning over state classification queries. We evaluate PHIER in the\nCALVIN and BEHAVIOR robotic environments and show that PHIER significantly\noutperforms existing methods in few-shot, out-of-distribution state\nclassification, and demonstrates strong zero- and few-shot generalization from\nsimulated to real-world tasks. Our results demonstrate that leveraging\npredicate hierarchies improves performance on state classification tasks with\nlimited data.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "ICLR 2025. First two authors contributed equally. Project page:\n  https://emilyzjin.github.io/projects/phier.html",
    "pdf_url": "http://arxiv.org/pdf/2502.12481v1",
    "published_date": "2025-02-18 03:08:37 UTC",
    "updated_date": "2025-02-18 03:08:37 UTC"
  },
  {
    "arxiv_id": "2502.12468v1",
    "title": "MCTS-Judge: Test-Time Scaling in LLM-as-a-Judge for Code Correctness Evaluation",
    "authors": [
      "Yutong Wang",
      "Pengliang Ji",
      "Chaoqun Yang",
      "Kaixin Li",
      "Ming Hu",
      "Jiaoyang Li",
      "Guillaume Sartoretti"
    ],
    "abstract": "The LLM-as-a-Judge paradigm shows promise for evaluating generative content\nbut lacks reliability in reasoning-intensive scenarios, such as programming.\nInspired by recent advances in reasoning models and shifts in scaling laws, we\npioneer bringing test-time computation into LLM-as-a-Judge, proposing\nMCTS-Judge, a resource-efficient, System-2 thinking framework for code\ncorrectness evaluation. MCTS-Judge leverages Monte Carlo Tree Search (MCTS) to\ndecompose problems into simpler, multi-perspective evaluations. Through a\nnode-selection strategy that combines self-assessment based on historical\nactions in the current trajectory and the Upper Confidence Bound for Trees\nbased on prior rollouts, MCTS-Judge balances global optimization and refinement\nof the current trajectory. We further designed a high-precision,\nunit-test-level reward mechanism to encourage the Large Language Model (LLM) to\nperform line-by-line analysis. Extensive experiments on three benchmarks and\nfive LLMs demonstrate the effectiveness of MCTS-Judge, which improves the base\nmodel's accuracy from 41% to 80%, surpassing the o1-series models with 3x fewer\ntokens. Further evaluations validate the superiority of its reasoning\ntrajectory in logic, analytics, thoroughness, and overall quality, while\nrevealing the test-time scaling law of the LLM-as-a-Judge paradigm.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12468v1",
    "published_date": "2025-02-18 02:55:48 UTC",
    "updated_date": "2025-02-18 02:55:48 UTC"
  },
  {
    "arxiv_id": "2502.12466v2",
    "title": "EquiBench: Benchmarking Large Language Models' Understanding of Program Semantics via Equivalence Checking",
    "authors": [
      "Anjiang Wei",
      "Jiannan Cao",
      "Ran Li",
      "Hongyu Chen",
      "Yuhui Zhang",
      "Ziheng Wang",
      "Yuan Liu",
      "Thiago S. F. X. Teixeira",
      "Diyi Yang",
      "Ke Wang",
      "Alex Aiken"
    ],
    "abstract": "As large language models (LLMs) become integral to code-related tasks, a\ncentral question emerges: do LLMs truly understand program execution semantics?\nWe introduce EquiBench, a new benchmark for evaluating LLMs through equivalence\nchecking, i.e., determining whether two programs produce identical outputs for\nall possible inputs. Unlike prior code generation benchmarks, this task\ndirectly tests a model's understanding of code execution semantics. EquiBench\nconsists of 2400 program pairs across four languages and six categories. These\npairs are generated through program analysis, compiler scheduling, and\nsuperoptimization, ensuring high-confidence labels, nontrivial difficulty, and\nfull automation. The transformations span syntactic edits, structural\nmodifications, and algorithmic changes, covering a broad spectrum of semantic\nvariation. We evaluate 19 state-of-the-art LLMs and find that in the most\nchallenging categories, the best accuracies are 63.8% and 76.2%, only modestly\nabove the 50% random baseline. Further analysis reveals that models often rely\non syntactic similarity rather than exhibiting robust reasoning over execution\nsemantics, highlighting fundamental limitations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.PL",
      "cs.SE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12466v2",
    "published_date": "2025-02-18 02:54:25 UTC",
    "updated_date": "2025-05-20 16:19:18 UTC"
  },
  {
    "arxiv_id": "2502.12459v1",
    "title": "Stress Testing Generalization: How Minor Modifications Undermine Large Language Model Performance",
    "authors": [
      "Guangxiang Zhao",
      "Saier Hu",
      "Xiaoqi Jian",
      "Jinzhu Wu",
      "Yuhan Wu",
      "Change Jia",
      "Lin Sun",
      "Xiangzheng Zhang"
    ],
    "abstract": "This paper investigates the fragility of Large Language Models (LLMs) in\ngeneralizing to novel inputs, specifically focusing on minor perturbations in\nwell-established benchmarks (e.g., slight changes in question format or\ndistractor length). Despite high benchmark scores, LLMs exhibit significant\naccuracy drops and unexpected biases (e.g., preference for longer distractors)\nwhen faced with these minor but content-preserving modifications. For example,\nQwen 2.5 1.5B's MMLU score rises from 60 to 89 and drops from 89 to 36 when\noption lengths are changed without altering the question. Even GPT-4\nexperiences a 25-point accuracy loss when question types are changed, with a\n6-point drop across all three modification categories. These analyses suggest\nthat LLMs rely heavily on superficial cues rather than forming robust, abstract\nrepresentations that generalize across formats, lexical variations, and\nirrelevant content shifts. This work aligns with the ACL 2025 theme track on\nthe Generalization of NLP models, proposing a \"Generalization Stress Test\" to\nassess performance shifts under controlled perturbations. The study calls for\nreevaluating benchmarks and developing more reliable evaluation methodologies\nto capture LLM generalization abilities better.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Submitted to ACL 2025 theme track on the Generalization of NLP models",
    "pdf_url": "http://arxiv.org/pdf/2502.12459v1",
    "published_date": "2025-02-18 02:42:53 UTC",
    "updated_date": "2025-02-18 02:42:53 UTC"
  },
  {
    "arxiv_id": "2502.12456v1",
    "title": "Not-So-Optimal Transport Flows for 3D Point Cloud Generation",
    "authors": [
      "Ka-Hei Hui",
      "Chao Liu",
      "Xiaohui Zeng",
      "Chi-Wing Fu",
      "Arash Vahdat"
    ],
    "abstract": "Learning generative models of 3D point clouds is one of the fundamental\nproblems in 3D generative learning. One of the key properties of point clouds\nis their permutation invariance, i.e., changing the order of points in a point\ncloud does not change the shape they represent. In this paper, we analyze the\nrecently proposed equivariant OT flows that learn permutation invariant\ngenerative models for point-based molecular data and we show that these models\nscale poorly on large point clouds. Also, we observe learning (equivariant) OT\nflows is generally challenging since straightening flow trajectories makes the\nlearned flow model complex at the beginning of the trajectory. To remedy these,\nwe propose not-so-optimal transport flow models that obtain an approximate OT\nby an offline OT precomputation, enabling an efficient construction of OT pairs\nfor training. During training, we can additionally construct a hybrid coupling\nby combining our approximate OT and independent coupling to make the target\nflow models easier to learn. In an extensive empirical study, we show that our\nproposed model outperforms prior diffusion- and flow-based approaches on a wide\nrange of unconditional generation and shape completion on the ShapeNet\nbenchmark.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12456v1",
    "published_date": "2025-02-18 02:37:34 UTC",
    "updated_date": "2025-02-18 02:37:34 UTC"
  },
  {
    "arxiv_id": "2502.12454v1",
    "title": "Benchmarking Zero-Shot Facial Emotion Annotation with Large Language Models: A Multi-Class and Multi-Frame Approach in DailyLife",
    "authors": [
      "He Zhang",
      "Xinyi Fu"
    ],
    "abstract": "This study investigates the feasibility and performance of using large\nlanguage models (LLMs) to automatically annotate human emotions in everyday\nscenarios. We conducted experiments on the DailyLife subset of the publicly\navailable FERV39k dataset, employing the GPT-4o-mini model for rapid, zero-shot\nlabeling of key frames extracted from video segments. Under a seven-class\nemotion taxonomy (\"Angry,\" \"Disgust,\" \"Fear,\" \"Happy,\" \"Neutral,\" \"Sad,\"\n\"Surprise\"), the LLM achieved an average precision of approximately 50%. In\ncontrast, when limited to ternary emotion classification\n(negative/neutral/positive), the average precision increased to approximately\n64%. Additionally, we explored a strategy that integrates multiple frames\nwithin 1-2 second video clips to enhance labeling performance and reduce costs.\nThe results indicate that this approach can slightly improve annotation\naccuracy. Overall, our preliminary findings highlight the potential application\nof zero-shot LLMs in human facial emotion annotation tasks, offering new\navenues for reducing labeling costs and broadening the applicability of LLMs in\ncomplex multimodal environments.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages",
    "pdf_url": "http://arxiv.org/pdf/2502.12454v1",
    "published_date": "2025-02-18 02:36:16 UTC",
    "updated_date": "2025-02-18 02:36:16 UTC"
  },
  {
    "arxiv_id": "2502.12453v1",
    "title": "UniMatch: Universal Matching from Atom to Task for Few-Shot Drug Discovery",
    "authors": [
      "Ruifeng Li",
      "Mingqian Li",
      "Wei Liu",
      "Yuhua Zhou",
      "Xiangxin Zhou",
      "Yuan Yao",
      "Qiang Zhang",
      "Hongyang Chen"
    ],
    "abstract": "Drug discovery is crucial for identifying candidate drugs for various\ndiseases.However, its low success rate often results in a scarcity of\nannotations, posing a few-shot learning problem. Existing methods primarily\nfocus on single-scale features, overlooking the hierarchical molecular\nstructures that determine different molecular properties. To address these\nissues, we introduce Universal Matching Networks (UniMatch), a dual matching\nframework that integrates explicit hierarchical molecular matching with\nimplicit task-level matching via meta-learning, bridging multi-level molecular\nrepresentations and task-level generalization. Specifically, our approach\nexplicitly captures structural features across multiple levels, such as atoms,\nsubstructures, and molecules, via hierarchical pooling and matching,\nfacilitating precise molecular representation and comparison. Additionally, we\nemploy a meta-learning strategy for implicit task-level matching, allowing the\nmodel to capture shared patterns across tasks and quickly adapt to new ones.\nThis unified matching framework ensures effective molecular alignment while\nleveraging shared meta-knowledge for fast adaptation. Our experimental results\ndemonstrate that UniMatch outperforms state-of-the-art methods on the\nMoleculeNet and FS-Mol benchmarks, achieving improvements of 2.87% in AUROC and\n6.52% in delta AUPRC. UniMatch also shows excellent generalization ability on\nthe Meta-MolNet benchmark.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.BM",
      "68U07"
    ],
    "primary_category": "cs.LG",
    "comment": "accepted as ICLR 2025 Spotlight",
    "pdf_url": "http://arxiv.org/pdf/2502.12453v1",
    "published_date": "2025-02-18 02:36:03 UTC",
    "updated_date": "2025-02-18 02:36:03 UTC"
  },
  {
    "arxiv_id": "2502.12450v1",
    "title": "Investigating and Extending Homans' Social Exchange Theory with Large Language Model based Agents",
    "authors": [
      "Lei Wang",
      "Zheqing Zhang",
      "Xu Chen"
    ],
    "abstract": "Homans' Social Exchange Theory (SET) is widely recognized as a basic\nframework for understanding the formation and emergence of human civilizations\nand social structures. In social science, this theory is typically studied\nbased on simple simulation experiments or real-world human studies, both of\nwhich either lack realism or are too expensive to control. In artificial\nintelligence, recent advances in large language models (LLMs) have shown\npromising capabilities in simulating human behaviors. Inspired by these\ninsights, we adopt an interdisciplinary research perspective and propose using\nLLM-based agents to study Homans' SET. Specifically, we construct a virtual\nsociety composed of three LLM agents and have them engage in a social exchange\ngame to observe their behaviors. Through extensive experiments, we found that\nHomans' SET is well validated in our agent society, demonstrating the\nconsistency between the agent and human behaviors. Building on this foundation,\nwe intentionally alter the settings of the agent society to extend the\ntraditional Homans' SET, making it more comprehensive and detailed. To the best\nof our knowledge, this paper marks the first step in studying Homans' SET with\nLLM-based agents. More importantly, it introduces a novel and feasible research\nparadigm that bridges the fields of social science and computer science through\nLLM-based agents. Code is available at https://github.com/Paitesanshi/SET.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12450v1",
    "published_date": "2025-02-18 02:30:46 UTC",
    "updated_date": "2025-02-18 02:30:46 UTC"
  },
  {
    "arxiv_id": "2502.12446v1",
    "title": "Multi-Attribute Steering of Language Models via Targeted Intervention",
    "authors": [
      "Duy Nguyen",
      "Archiki Prasad",
      "Elias Stengel-Eskin",
      "Mohit Bansal"
    ],
    "abstract": "Inference-time intervention (ITI) has emerged as a promising method for\nsteering large language model (LLM) behavior in a particular direction (e.g.,\nimproving helpfulness) by intervening on token representations without costly\nupdates to the LLM's parameters. However, existing ITI approaches fail to scale\nto multi-attribute settings with conflicts, such as enhancing helpfulness while\nalso reducing toxicity. To address this, we introduce Multi-Attribute Targeted\nSteering (MAT-Steer), a novel steering framework designed for selective\ntoken-level intervention across multiple attributes. MAT-Steer learns steering\nvectors using an alignment objective that shifts the model's internal\nrepresentations of undesirable outputs closer to those of desirable ones while\nenforcing sparsity and orthogonality among vectors for different attributes,\nthereby reducing inter-attribute conflicts. We evaluate MAT-Steer in two\ndistinct settings: (i) on question answering (QA) tasks where we balance\nattributes like truthfulness, bias, and toxicity; (ii) on generative tasks\nwhere we simultaneously improve attributes like helpfulness, correctness, and\ncoherence. MAT-Steer outperforms existing ITI and parameter-efficient\nfinetuning approaches across both task types (e.g., 3% average accuracy gain\nacross QA tasks and 55.82% win rate against the best ITI baseline).",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "15 pages, code link: https://github.com/duykhuongnguyen/MAT-Steer",
    "pdf_url": "http://arxiv.org/pdf/2502.12446v1",
    "published_date": "2025-02-18 02:27:23 UTC",
    "updated_date": "2025-02-18 02:27:23 UTC"
  },
  {
    "arxiv_id": "2502.12445v1",
    "title": "Computational Safety for Generative AI: A Signal Processing Perspective",
    "authors": [
      "Pin-Yu Chen"
    ],
    "abstract": "AI safety is a rapidly growing area of research that seeks to prevent the\nharm and misuse of frontier AI technology, particularly with respect to\ngenerative AI (GenAI) tools that are capable of creating realistic and\nhigh-quality content through text prompts. Examples of such tools include large\nlanguage models (LLMs) and text-to-image (T2I) diffusion models. As the\nperformance of various leading GenAI models approaches saturation due to\nsimilar training data sources and neural network architecture designs, the\ndevelopment of reliable safety guardrails has become a key differentiator for\nresponsibility and sustainability. This paper presents a formalization of the\nconcept of computational safety, which is a mathematical framework that enables\nthe quantitative assessment, formulation, and study of safety challenges in\nGenAI through the lens of signal processing theory and methods. In particular,\nwe explore two exemplary categories of computational safety challenges in GenAI\nthat can be formulated as hypothesis testing problems. For the safety of model\ninput, we show how sensitivity analysis and loss landscape analysis can be used\nto detect malicious prompts with jailbreak attempts. For the safety of model\noutput, we elucidate how statistical signal processing and adversarial learning\ncan be used to detect AI-generated content. Finally, we discuss key open\nresearch challenges, opportunities, and the essential role of signal processing\nin computational AI safety.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "comment": "preprint for an invited paper",
    "pdf_url": "http://arxiv.org/pdf/2502.12445v1",
    "published_date": "2025-02-18 02:26:50 UTC",
    "updated_date": "2025-02-18 02:26:50 UTC"
  },
  {
    "arxiv_id": "2502.12444v1",
    "title": "SparAMX: Accelerating Compressed LLMs Token Generation on AMX-powered CPUs",
    "authors": [
      "Ahmed F. AbouElhamayed",
      "Jordan Dotzel",
      "Yash Akhauri",
      "Chi-Chih Chang",
      "Sameh Gobriel",
      "J. Pablo Muñoz",
      "Vui Seng Chua",
      "Nilesh Jain",
      "Mohamed S. Abdelfattah"
    ],
    "abstract": "Large language models have high compute, latency, and memory requirements.\nWhile specialized accelerators such as GPUs and TPUs typically run these\nworkloads, CPUs are more widely available and consume less energy. Accelerating\nLLMs with CPUs enables broader AI access at a lower cost and power consumption.\nThis acceleration potential for CPUs is especially relevant during the\nmemory-bound decoding stage of LLM inference, which processes one token at a\ntime and is becoming increasingly utilized with reasoning models. We utilize\nAdvanced Matrix Extensions (AMX) support on the latest Intel CPUs together with\nunstructured sparsity to achieve a $1.42 \\times$ reduction in end-to-end\nlatency compared to the current PyTorch implementation by applying our\ntechnique in linear layers. We provide a set of open-source customized sparse\nkernels that can speed up any PyTorch model by automatically replacing all\nlinear layers with our custom sparse implementation. Furthermore, we\ndemonstrate for the first time the use of unstructured sparsity in the\nattention computation achieving a $1.14 \\times$ speedup over the current\nsystems without compromising accuracy. Code:\nhttps://github.com/IntelLabs/Hardware-Aware-Automated-Machine-Learning/tree/main/SparAMX",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR",
      "cs.PF"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12444v1",
    "published_date": "2025-02-18 02:26:34 UTC",
    "updated_date": "2025-02-18 02:26:34 UTC"
  },
  {
    "arxiv_id": "2502.12435v1",
    "title": "A Survey on Large Language Models for Automated Planning",
    "authors": [
      "Mohamed Aghzal",
      "Erion Plaku",
      "Gregory J. Stein",
      "Ziyu Yao"
    ],
    "abstract": "The planning ability of Large Language Models (LLMs) has garnered increasing\nattention in recent years due to their remarkable capacity for multi-step\nreasoning and their ability to generalize across a wide range of domains. While\nsome researchers emphasize the potential of LLMs to perform complex planning\ntasks, others highlight significant limitations in their performance,\nparticularly when these models are tasked with handling the intricacies of\nlong-horizon reasoning. In this survey, we critically investigate existing\nresearch on the use of LLMs in automated planning, examining both their\nsuccesses and shortcomings in detail. We illustrate that although LLMs are not\nwell-suited to serve as standalone planners because of these limitations, they\nnonetheless present an enormous opportunity to enhance planning applications\nwhen combined with other approaches. Thus, we advocate for a balanced\nmethodology that leverages the inherent flexibility and generalized knowledge\nof LLMs alongside the rigor and cost-effectiveness of traditional planning\nmethods.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12435v1",
    "published_date": "2025-02-18 02:11:03 UTC",
    "updated_date": "2025-02-18 02:11:03 UTC"
  },
  {
    "arxiv_id": "2502.12430v1",
    "title": "Bridge the Gaps between Machine Unlearning and AI Regulation",
    "authors": [
      "Bill Marino",
      "Meghdad Kurmanji",
      "Nicholas D. Lane"
    ],
    "abstract": "The \"right to be forgotten\" and the data privacy laws that encode it have\nmotivated machine unlearning since its earliest days. Now, an inbound wave of\nartificial intelligence regulations - like the European Union's Artificial\nIntelligence Act (AIA) - potentially offer important new use cases for machine\nunlearning. However, this position paper argues, this opportunity will only be\nrealized if researchers, aided by policymakers, proactively bridge the\n(sometimes sizable) gaps between machine unlearning's state of the art and its\npotential applications to AI regulation. To demonstrate this point, we use the\nAIA as an example. Specifically, we deliver a \"state of the union\" as regards\nmachine unlearning's current potential for aiding compliance with the AIA. This\nstarts with a precise cataloging of the potential applications of machine\nunlearning to AIA compliance. For each, we flag any legal ambiguities clouding\nthe potential application and, moreover, flag the technical gaps that exist\nbetween the potential application and the state of the art of machine\nunlearning. Finally, we end with a call to action: for both machine learning\nresearchers and policymakers, to, respectively, solve the open technical and\nlegal questions that will unlock machine unlearning's potential to assist\ncompliance with the AIA - and other AI regulation like it.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12430v1",
    "published_date": "2025-02-18 02:03:03 UTC",
    "updated_date": "2025-02-18 02:03:03 UTC"
  },
  {
    "arxiv_id": "2502.14898v1",
    "title": "Retrieval-augmented systems can be dangerous medical communicators",
    "authors": [
      "Lionel Wong",
      "Ayman Ali",
      "Raymond Xiong",
      "Shannon Zeijang Shen",
      "Yoon Kim",
      "Monica Agrawal"
    ],
    "abstract": "Patients have long sought health information online, and increasingly, they\nare turning to generative AI to answer their health-related queries. Given the\nhigh stakes of the medical domain, techniques like retrieval-augmented\ngeneration and citation grounding have been widely promoted as methods to\nreduce hallucinations and improve the accuracy of AI-generated responses and\nhave been widely adopted into search engines. This paper argues that even when\nthese methods produce literally accurate content drawn from source documents\nsans hallucinations, they can still be highly misleading. Patients may derive\nsignificantly different interpretations from AI-generated outputs than they\nwould from reading the original source material, let alone consulting a\nknowledgeable clinician. Through a large-scale query analysis on topics\nincluding disputed diagnoses and procedure safety, we support our argument with\nquantitative and qualitative evidence of the suboptimal answers resulting from\ncurrent systems. In particular, we highlight how these models tend to\ndecontextualize facts, omit critical relevant sources, and reinforce patient\nmisconceptions or biases. We propose a series of recommendations -- such as the\nincorporation of communication pragmatics and enhanced comprehension of source\ndocuments -- that could help mitigate these issues and extend beyond the\nmedical domain.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2502.14898v1",
    "published_date": "2025-02-18 01:57:02 UTC",
    "updated_date": "2025-02-18 01:57:02 UTC"
  },
  {
    "arxiv_id": "2502.15785v1",
    "title": "Masking the Gaps: An Imputation-Free Approach to Time Series Modeling with Missing Data",
    "authors": [
      "Abhilash Neog",
      "Arka Daw",
      "Sepideh Fatemi Khorasgani",
      "Anuj Karpatne"
    ],
    "abstract": "A significant challenge in time-series (TS) modeling is the presence of\nmissing values in real-world TS datasets. Traditional two-stage frameworks,\ninvolving imputation followed by modeling, suffer from two key drawbacks: (1)\nthe propagation of imputation errors into subsequent TS modeling, (2) the\ntrade-offs between imputation efficacy and imputation complexity. While\none-stage approaches attempt to address these limitations, they often struggle\nwith scalability or fully leveraging partially observed features. To this end,\nwe propose a novel imputation-free approach for handling missing values in time\nseries termed Missing Feature-aware Time Series Modeling (MissTSM) with two\nmain innovations. First, we develop a novel embedding scheme that treats every\ncombination of time-step and feature (or channel) as a distinct token. Second,\nwe introduce a novel Missing Feature-Aware Attention (MFAA) Layer to learn\nlatent representations at every time-step based on partially observed features.\nWe evaluate the effectiveness of MissTSM in handling missing values over\nmultiple benchmark datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages",
    "pdf_url": "http://arxiv.org/pdf/2502.15785v1",
    "published_date": "2025-02-18 01:42:26 UTC",
    "updated_date": "2025-02-18 01:42:26 UTC"
  },
  {
    "arxiv_id": "2502.12420v2",
    "title": "Sens-Merging: Sensitivity-Guided Parameter Balancing for Merging Large Language Models",
    "authors": [
      "Shuqi Liu",
      "Han Wu",
      "Bowei He",
      "Xiongwei Han",
      "Mingxuan Yuan",
      "Linqi Song"
    ],
    "abstract": "Recent advances in large language models have led to numerous\ntask-specialized fine-tuned variants, creating a need for efficient model\nmerging techniques that preserve specialized capabilities while avoiding costly\nretraining. While existing task vector-based merging methods show promise, they\ntypically apply uniform coefficients across all parameters, overlooking varying\nparameter importance both within and across tasks. We present Sens-Merging, a\nsensitivity-guided coefficient adjustment method that enhances existing model\nmerging techniques by operating at both task-specific and cross-task levels.\nOur method analyzes parameter sensitivity within individual tasks and evaluates\ncross-task transferability to determine optimal merging coefficients. Extensive\nexperiments on Mistral 7B and LLaMA2-7B/13B models demonstrate that\nSens-Merging significantly improves performance across general knowledge,\nmathematical reasoning, and code generation tasks. Notably, when combined with\nexisting merging techniques, our method enables merged models to outperform\nspecialized fine-tuned models, particularly in code generation tasks. Our\nfindings reveal important trade-offs between task-specific and cross-task\nscalings, providing insights for future model merging strategies.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12420v2",
    "published_date": "2025-02-18 01:41:13 UTC",
    "updated_date": "2025-02-19 12:34:46 UTC"
  },
  {
    "arxiv_id": "2502.12418v1",
    "title": "Boosting Illuminant Estimation in Deep Color Constancy through Enhancing Brightness Robustness",
    "authors": [
      "Mengda Xie",
      "Chengzhi Zhong",
      "Yiling He",
      "Zhan Qin",
      "Meie Fang"
    ],
    "abstract": "Color constancy estimates illuminant chromaticity to correct color-biased\nimages. Recently, Deep Neural Network-driven Color Constancy (DNNCC) models\nhave made substantial advancements. Nevertheless, the potential risks in DNNCC\ndue to the vulnerability of deep neural networks have not yet been explored. In\nthis paper, we conduct the first investigation into the impact of a key factor\nin color constancy-brightness-on DNNCC from a robustness perspective. Our\nevaluation reveals that several mainstream DNNCC models exhibit high\nsensitivity to brightness despite their focus on chromaticity estimation. This\nsheds light on a potential limitation of existing DNNCC models: their\nsensitivity to brightness may hinder performance given the widespread\nbrightness variations in real-world datasets. From the insights of our\nanalysis, we propose a simple yet effective brightness robustness enhancement\nstrategy for DNNCC models, termed BRE. The core of BRE is built upon the\nadaptive step-size adversarial brightness augmentation technique, which\nidentifies high-risk brightness variation and generates augmented images via\nexplicit brightness adjustment. Subsequently, BRE develops a\nbrightness-robustness-aware model optimization strategy that integrates\nadversarial brightness training and brightness contrastive loss, significantly\nbolstering the brightness robustness of DNNCC models. BRE is\nhyperparameter-free and can be integrated into existing DNNCC models, without\nincurring additional overhead during the testing phase. Experiments on two\npublic color constancy datasets-ColorChecker and Cube+-demonstrate that the\nproposed BRE consistently enhances the illuminant estimation performance of\nexisting DNNCC models, reducing the estimation error by an average of 5.04%\nacross six mainstream DNNCC models, underscoring the critical role of enhancing\nbrightness robustness in these models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12418v1",
    "published_date": "2025-02-18 01:36:10 UTC",
    "updated_date": "2025-02-18 01:36:10 UTC"
  },
  {
    "arxiv_id": "2502.12411v1",
    "title": "Gradient Co-occurrence Analysis for Detecting Unsafe Prompts in Large Language Models",
    "authors": [
      "Jingyuan Yang",
      "Bowen Yan",
      "Rongjun Li",
      "Ziyu Zhou",
      "Xin Chen",
      "Zhiyong Feng",
      "Wei Peng"
    ],
    "abstract": "Unsafe prompts pose significant safety risks to large language models (LLMs).\nExisting methods for detecting unsafe prompts rely on data-driven fine-tuning\nto train guardrail models, necessitating significant data and computational\nresources. In contrast, recent few-shot gradient-based methods emerge,\nrequiring only few safe and unsafe reference prompts. A gradient-based approach\nidentifies unsafe prompts by analyzing consistent patterns of the gradients of\nsafety-critical parameters in LLMs. Although effective, its restriction to\ndirectional similarity (cosine similarity) introduces ``directional bias'',\nlimiting its capability to identify unsafe prompts. To overcome this\nlimitation, we introduce GradCoo, a novel gradient co-occurrence analysis\nmethod that expands the scope of safety-critical parameter identification to\ninclude unsigned gradient similarity, thereby reducing the impact of\n``directional bias'' and enhancing the accuracy of unsafe prompt detection.\nComprehensive experiments on the widely-used benchmark datasets ToxicChat and\nXStest demonstrate that our proposed method can achieve state-of-the-art (SOTA)\nperformance compared to existing methods. Moreover, we confirm the\ngeneralizability of GradCoo in detecting unsafe prompts across a range of LLM\nbase models with various sizes and origins.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12411v1",
    "published_date": "2025-02-18 01:14:46 UTC",
    "updated_date": "2025-02-18 01:14:46 UTC"
  },
  {
    "arxiv_id": "2502.12398v1",
    "title": "Solving the Cold Start Problem on One's Own as an End User via Preference Transfer",
    "authors": [
      "Ryoma Sato"
    ],
    "abstract": "We propose a new approach that enables end users to directly solve the cold\nstart problem by themselves. The cold start problem is a common issue in\nrecommender systems, and many methods have been proposed to address the problem\non the service provider's side. However, when the service provider does not\ntake action, users are left with poor recommendations and no means to improve\ntheir experience. We propose an algorithm, Pretender, that allows end users to\nproactively solve the cold start problem on their own. Pretender does not\nrequire any special support from the service provider and can be deployed\nindependently by users. We formulate the problem as minimizing the distance\nbetween the source and target distributions and optimize item selection from\nthe target service accordingly. Furthermore, we establish theoretical\nguarantees for Pretender based on a discrete quadrature problem. We conduct\nexperiments on real-world datasets to demonstrate the effectiveness of\nPretender.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "25 pages",
    "pdf_url": "http://arxiv.org/pdf/2502.12398v1",
    "published_date": "2025-02-18 00:12:52 UTC",
    "updated_date": "2025-02-18 00:12:52 UTC"
  },
  {
    "arxiv_id": "2502.12397v2",
    "title": "Could AI Leapfrog the Web? Evidence from Teachers in Sierra Leone",
    "authors": [
      "Daniel Björkegren",
      "Jun Ho Choi",
      "Divya Budihal",
      "Dominic Sobhani",
      "Oliver Garrod",
      "Paul Atherton"
    ],
    "abstract": "Although 85% of sub-Saharan Africa's population is covered by mobile\nbroadband signal, only 37% use the internet, and those who do seldom use the\nweb. The most frequently cited reason for low internet usage is the cost of\ndata. We investigate whether AI can bridge this gap by analyzing 40,350 queries\nsubmitted to an AI chatbot by 469 teachers in Sierra Leone over 17 months.\nTeachers use AI for teaching assistance more frequently than web search. We\ncompare the AI responses to the corresponding top search results for the same\nqueries from the most popular local web search engine, google.com.sl. Only 2%\nof results for corresponding web searches contain content from in country.\nAdditionally, the average web search result consumes 3,107 times more data than\nan AI response. Bandwidth alone costs \\$2.41 per thousand web search results\nloaded, while the total cost of AI is \\$0.30 per thousand responses. As a\nresult, AI is 87% less expensive than web search. In blinded evaluations, an\nindependent sample of teachers rate AI responses as more relevant, helpful, and\ncorrect than web search results. These findings suggest that AI-driven\nsolutions can cost-effectively bridge information gaps in low-connectivity\nregions.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "econ.GN",
      "q-fin.EC"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12397v2",
    "published_date": "2025-02-18 00:11:08 UTC",
    "updated_date": "2025-03-17 14:14:56 UTC"
  },
  {
    "arxiv_id": "2502.12393v1",
    "title": "Time Series Treatment Effects Analysis with Always-Missing Controls",
    "authors": [
      "Juan Shu",
      "Qiyu Han",
      "George Chen",
      "Xihao Cao",
      "Kangming Luo",
      "Dan Pallotta",
      "Shivam Agrawal",
      "Yuping Lu",
      "Xiaoyu Zhang",
      "Jawad Mansoor",
      "Jyoti Anand"
    ],
    "abstract": "Estimating treatment effects in time series data presents a significant\nchallenge, especially when the control group is always unobservable. For\nexample, in analyzing the effects of Christmas on retail sales, we lack direct\nobservation of what would have occurred in late December without the Christmas\nimpact. To address this, we try to recover the control group in the event\nperiod while accounting for confounders and temporal dependencies. Experimental\nresults on the M5 Walmart retail sales data demonstrate robust estimation of\nthe potential outcome of the control group as well as accurate predicted\nholiday effect. Furthermore, we provided theoretical guarantees for the\nestimated treatment effect, proving its consistency and asymptotic normality.\nThe proposed methodology is applicable not only to this always-missing control\nscenario but also in other conventional time series causal inference settings.",
    "categories": [
      "stat.ME",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "stat.ME",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12393v1",
    "published_date": "2025-02-18 00:03:36 UTC",
    "updated_date": "2025-02-18 00:03:36 UTC"
  }
]
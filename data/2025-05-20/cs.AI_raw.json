[
  {
    "arxiv_id": "2505.14978v1",
    "title": "JARVIS: A Multi-Agent Code Assistant for High-Quality EDA Script Generation",
    "authors": [
      "Ghasem Pasandi",
      "Kishor Kunal",
      "Varun Tej",
      "Kunjal Shan",
      "Hanfei Sun",
      "Sumit Jain",
      "Chunhui Li",
      "Chenhui Deng",
      "Teodor-Dumitru Ene",
      "Haoxing Ren",
      "Sreedhar Pratty"
    ],
    "abstract": "This paper presents JARVIS, a novel multi-agent framework that leverages\nLarge Language Models (LLMs) and domain expertise to generate high-quality\nscripts for specialized Electronic Design Automation (EDA) tasks. By combining\na domain-specific LLM trained with synthetically generated data, a custom\ncompiler for structural verification, rule enforcement, code fixing\ncapabilities, and advanced retrieval mechanisms, our approach achieves\nsignificant improvements over state-of-the-art domain-specific models. Our\nframework addresses the challenges of data scarcity and hallucination errors in\nLLMs, demonstrating the potential of LLMs in specialized engineering domains.\nWe evaluate our framework on multiple benchmarks and show that it outperforms\nexisting models in terms of accuracy and reliability. Our work sets a new\nprecedent for the application of LLMs in EDA and paves the way for future\ninnovations in this field.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14978v1",
    "published_date": "2025-05-20 23:40:57 UTC",
    "updated_date": "2025-05-20 23:40:57 UTC"
  },
  {
    "arxiv_id": "2505.14976v1",
    "title": "SDLog: A Deep Learning Framework for Detecting Sensitive Information in Software Logs",
    "authors": [
      "Roozbeh Aghili",
      "Xingfang Wu",
      "Foutse Khomh",
      "Heng Li"
    ],
    "abstract": "Software logs are messages recorded during the execution of a software system\nthat provide crucial run-time information about events and activities. Although\nsoftware logs have a critical role in software maintenance and operation tasks,\npublicly accessible log datasets remain limited, hindering advance in log\nanalysis research and practices. The presence of sensitive information,\nparticularly Personally Identifiable Information (PII) and quasi-identifiers,\nintroduces serious privacy and re-identification risks, discouraging the\npublishing and sharing of real-world logs. In practice, log anonymization\ntechniques primarily rely on regular expression patterns, which involve\nmanually crafting rules to identify and replace sensitive information. However,\nthese regex-based approaches suffer from significant limitations, such as\nextensive manual efforts and poor generalizability across diverse log formats\nand datasets. To mitigate these limitations, we introduce SDLog, a deep\nlearning-based framework designed to identify sensitive information in software\nlogs. Our results show that SDLog overcomes regex limitations and outperforms\nthe best-performing regex patterns in identifying sensitive information. With\nonly 100 fine-tuning samples from the target dataset, SDLog can correctly\nidentify 99.5% of sensitive attributes and achieves an F1-score of 98.4%. To\nthe best of our knowledge, this is the first deep learning alternative to\nregex-based methods in software log anonymization.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14976v1",
    "published_date": "2025-05-20 23:36:13 UTC",
    "updated_date": "2025-05-20 23:36:13 UTC"
  },
  {
    "arxiv_id": "2505.14975v1",
    "title": "Flattening Hierarchies with Policy Bootstrapping",
    "authors": [
      "John L. Zhou",
      "Jonathan C. Kao"
    ],
    "abstract": "Offline goal-conditioned reinforcement learning (GCRL) is a promising\napproach for pretraining generalist policies on large datasets of reward-free\ntrajectories, akin to the self-supervised objectives used to train foundation\nmodels for computer vision and natural language processing. However, scaling\nGCRL to longer horizons remains challenging due to the combination of sparse\nrewards and discounting, which obscures the comparative advantages of primitive\nactions with respect to distant goals. Hierarchical RL methods achieve strong\nempirical results on long-horizon goal-reaching tasks, but their reliance on\nmodular, timescale-specific policies and subgoal generation introduces\nsignificant additional complexity and hinders scaling to high-dimensional goal\nspaces. In this work, we introduce an algorithm to train a flat\n(non-hierarchical) goal-conditioned policy by bootstrapping on\nsubgoal-conditioned policies with advantage-weighted importance sampling. Our\napproach eliminates the need for a generative model over the (sub)goal space,\nwhich we find is key for scaling to high-dimensional control in large state\nspaces. We further show that existing hierarchical and bootstrapping-based\napproaches correspond to specific design choices within our derivation. Across\na comprehensive suite of state- and pixel-based locomotion and manipulation\nbenchmarks, our method matches or surpasses state-of-the-art offline GCRL\nalgorithms and scales to complex, long-horizon tasks where prior approaches\nfail.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14975v1",
    "published_date": "2025-05-20 23:31:30 UTC",
    "updated_date": "2025-05-20 23:31:30 UTC"
  },
  {
    "arxiv_id": "2505.14970v1",
    "title": "Self-Evolving Curriculum for LLM Reasoning",
    "authors": [
      "Xiaoyin Chen",
      "Jiarui Lu",
      "Minsu Kim",
      "Dinghuai Zhang",
      "Jian Tang",
      "Alexandre PichÃ©",
      "Nicolas Gontier",
      "Yoshua Bengio",
      "Ehsan Kamalloo"
    ],
    "abstract": "Reinforcement learning (RL) has proven effective for fine-tuning large\nlanguage models (LLMs), significantly enhancing their reasoning abilities in\ndomains such as mathematics and code generation. A crucial factor influencing\nRL fine-tuning success is the training curriculum: the order in which training\nproblems are presented. While random curricula serve as common baselines, they\nremain suboptimal; manually designed curricula often rely heavily on\nheuristics, and online filtering methods can be computationally prohibitive. To\naddress these limitations, we propose Self-Evolving Curriculum (SEC), an\nautomatic curriculum learning method that learns a curriculum policy\nconcurrently with the RL fine-tuning process. Our approach formulates\ncurriculum selection as a non-stationary Multi-Armed Bandit problem, treating\neach problem category (e.g., difficulty level or problem type) as an individual\narm. We leverage the absolute advantage from policy gradient methods as a proxy\nmeasure for immediate learning gain. At each training step, the curriculum\npolicy selects categories to maximize this reward signal and is updated using\nthe TD(0) method. Across three distinct reasoning domains: planning, inductive\nreasoning, and mathematics, our experiments demonstrate that SEC significantly\nimproves models' reasoning capabilities, enabling better generalization to\nharder, out-of-distribution test problems. Additionally, our approach achieves\nbetter skill balance when fine-tuning simultaneously on multiple reasoning\ndomains. These findings highlight SEC as a promising strategy for RL\nfine-tuning of LLMs.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14970v1",
    "published_date": "2025-05-20 23:17:15 UTC",
    "updated_date": "2025-05-20 23:17:15 UTC"
  },
  {
    "arxiv_id": "2505.14969v1",
    "title": "STree: Speculative Tree Decoding for Hybrid State-Space Models",
    "authors": [
      "Yangchao Wu",
      "Zongyue Qin",
      "Alex Wong",
      "Stefano Soatto"
    ],
    "abstract": "Speculative decoding is a technique to leverage hardware concurrency to\nimprove the efficiency of large-scale autoregressive (AR) Transformer models by\nenabling multiple steps of token generation in a single forward pass.\nState-space models (SSMs) are already more efficient than AR Transformers,\nsince their state summarizes all past data with no need to cache or re-process\ntokens in the sliding window context. However, their state can also comprise\nthousands of tokens; so, speculative decoding has recently been extended to\nSSMs. Existing approaches, however, do not leverage the tree-based verification\nmethods, since current SSMs lack the means to compute a token tree efficiently.\nWe propose the first scalable algorithm to perform tree-based speculative\ndecoding in state-space models (SSMs) and hybrid architectures of SSMs and\nTransformer layers. We exploit the structure of accumulated state transition\nmatrices to facilitate tree-based speculative decoding with minimal overhead to\ncurrent SSM state update implementations. With the algorithm, we describe a\nhardware-aware implementation that improves naive application of AR Transformer\ntree-based speculative decoding methods to SSMs. Furthermore, we outperform\nvanilla speculative decoding with SSMs even with a baseline drafting model and\ntree structure on three different benchmarks, opening up opportunities for\nfurther speed up with SSM and hybrid model inference. Code will be released\nupon paper acceptance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14969v1",
    "published_date": "2025-05-20 23:12:16 UTC",
    "updated_date": "2025-05-20 23:12:16 UTC"
  },
  {
    "arxiv_id": "2505.14967v1",
    "title": "Anomaly Detection Based on Critical Paths for Deep Neural Networks",
    "authors": [
      "Fangzhen Zhao",
      "Chenyi Zhang",
      "Naipeng Dong",
      "Ming Li",
      "Jinxiao Shan"
    ],
    "abstract": "Deep neural networks (DNNs) are notoriously hard to understand and difficult\nto defend. Extracting representative paths (including the neuron activation\nvalues and the connections between neurons) from DNNs using software\nengineering approaches has recently shown to be a promising approach in\ninterpreting the decision making process of blackbox DNNs, as the extracted\npaths are often effective in capturing essential features. With this in mind,\nthis work investigates a novel approach that extracts critical paths from DNNs\nand subsequently applies the extracted paths for the anomaly detection task,\nbased on the observation that outliers and adversarial inputs do not usually\ninduce the same activation pattern on those paths as normal (in-distribution)\ninputs.\n  In our approach, we first identify critical detection paths via genetic\nevolution and mutation. Since different paths in a DNN often capture different\nfeatures for the same target class, we ensemble detection results from multiple\npaths by integrating random subspace sampling and a voting mechanism. Compared\nwith state-of-the-art methods, our experimental results suggest that our method\nnot only outperforms them, but it is also suitable for the detection of a broad\nrange of anomaly types with high accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "23 pages in ACM journal latex format",
    "pdf_url": "http://arxiv.org/pdf/2505.14967v1",
    "published_date": "2025-05-20 23:10:59 UTC",
    "updated_date": "2025-05-20 23:10:59 UTC"
  },
  {
    "arxiv_id": "2505.14964v1",
    "title": "The Achilles Heel of AI: Fundamentals of Risk-Aware Training Data for High-Consequence Models",
    "authors": [
      "Dave Cook",
      "Tim Klawa"
    ],
    "abstract": "AI systems in high-consequence domains such as defense, intelligence, and\ndisaster response must detect rare, high-impact events while operating under\ntight resource constraints. Traditional annotation strategies that prioritize\nlabel volume over informational value introduce redundancy and noise, limiting\nmodel generalization. This paper introduces smart-sizing, a training data\nstrategy that emphasizes label diversity, model-guided selection, and marginal\nutility-based stopping. We implement this through Adaptive Label Optimization\n(ALO), combining pre-labeling triage, annotator disagreement analysis, and\niterative feedback to prioritize labels that meaningfully improve model\nperformance. Experiments show that models trained on 20 to 40 percent of\ncurated data can match or exceed full-data baselines, particularly in\nrare-class recall and edge-case generalization. We also demonstrate how latent\nlabeling errors embedded in training and validation sets can distort\nevaluation, underscoring the need for embedded audit tools and\nperformance-aware governance. Smart-sizing reframes annotation as a\nfeedback-driven process aligned with mission outcomes, enabling more robust\nmodels with fewer labels and supporting efficient AI development pipelines for\nfrontier models and operational systems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14964v1",
    "published_date": "2025-05-20 22:57:35 UTC",
    "updated_date": "2025-05-20 22:57:35 UTC"
  },
  {
    "arxiv_id": "2505.14948v1",
    "title": "Programmatic Video Prediction Using Large Language Models",
    "authors": [
      "Hao Tang",
      "Kevin Ellis",
      "Suhas Lohit",
      "Michael J. Jones",
      "Moitreya Chatterjee"
    ],
    "abstract": "The task of estimating the world model describing the dynamics of a real\nworld process assumes immense importance for anticipating and preparing for\nfuture outcomes. For applications such as video surveillance, robotics\napplications, autonomous driving, etc. this objective entails synthesizing\nplausible visual futures, given a few frames of a video to set the visual\ncontext. Towards this end, we propose ProgGen, which undertakes the task of\nvideo frame prediction by representing the dynamics of the video using a set of\nneuro-symbolic, human-interpretable set of states (one per frame) by leveraging\nthe inductive biases of Large (Vision) Language Models (LLM/VLM). In\nparticular, ProgGen utilizes LLM/VLM to synthesize programs: (i) to estimate\nthe states of the video, given the visual context (i.e. the frames); (ii) to\npredict the states corresponding to future time steps by estimating the\ntransition dynamics; (iii) to render the predicted states as visual RGB-frames.\nEmpirical evaluations reveal that our proposed method outperforms competing\ntechniques at the task of video frame prediction in two challenging\nenvironments: (i) PhyWorld (ii) Cart Pole. Additionally, ProgGen permits\ncounter-factual reasoning and interpretable video generation attesting to its\neffectiveness and generalizability for video generation tasks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14948v1",
    "published_date": "2025-05-20 22:17:47 UTC",
    "updated_date": "2025-05-20 22:17:47 UTC"
  },
  {
    "arxiv_id": "2505.14946v1",
    "title": "Reinforcement Learning from User Feedback",
    "authors": [
      "Eric Han",
      "Jun Chen",
      "Karthik Abinav Sankararaman",
      "Xiaoliang Peng",
      "Tengyu Xu",
      "Eryk Helenowski",
      "Kaiyan Peng",
      "Mrinal Kumar",
      "Sinong Wang",
      "Han Fang",
      "Arya Talebzadeh"
    ],
    "abstract": "As large language models (LLMs) are increasingly deployed in diverse user\nfacing applications, aligning them with real user preferences becomes\nessential. Existing methods like Reinforcement Learning from Human Feedback\n(RLHF) rely on expert annotators trained on manually defined guidelines, whose\njudgments may not reflect the priorities of everyday users. We introduce\nReinforcement Learning from User Feedback (RLUF), a framework for aligning LLMs\ndirectly to implicit signals from users in production. RLUF addresses key\nchallenges of user feedback: user feedback is often binary (e.g., emoji\nreactions), sparse, and occasionally adversarial. We train a reward model,\nP[Love], to predict the likelihood that an LLM response will receive a Love\nReaction, a lightweight form of positive user feedback, and integrate P[Love]\ninto a multi-objective policy optimization framework alongside helpfulness and\nsafety objectives. In large-scale experiments, we show that P[Love] is\npredictive of increased positive feedback and serves as a reliable offline\nevaluator of future user behavior. Policy optimization using P[Love]\nsignificantly raises observed positive-feedback rates, including a 28% increase\nin Love Reactions during live A/B tests. However, optimizing for positive\nreactions introduces reward hacking challenges, requiring careful balancing of\nobjectives. By directly leveraging implicit signals from users, RLUF offers a\npath to aligning LLMs with real-world user preferences at scale.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14946v1",
    "published_date": "2025-05-20 22:14:44 UTC",
    "updated_date": "2025-05-20 22:14:44 UTC"
  },
  {
    "arxiv_id": "2505.14943v1",
    "title": "Soft Prompts for Evaluation: Measuring Conditional Distance of Capabilities",
    "authors": [
      "Ross Nordby"
    ],
    "abstract": "To help evaluate and understand the latent capabilities of language models,\nthis paper introduces an approach using optimized input embeddings, or 'soft\nprompts,' as a metric of conditional distance between a model and a target\nbehavior. The technique aims to facilitate latent capability discovery as a\npart of automated red teaming/evaluation suites and to provide quantitative\nfeedback about the accessibility of potentially concerning behaviors in a way\nthat may scale to powerful future models, including those which may otherwise\nbe capable of deceptive alignment. An evaluation framework using soft prompts\nis demonstrated in natural language, chess, and pathfinding, and the technique\nis extended with generalized conditional soft prompts to aid in constructing\ntask evaluations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14943v1",
    "published_date": "2025-05-20 22:02:53 UTC",
    "updated_date": "2025-05-20 22:02:53 UTC"
  },
  {
    "arxiv_id": "2505.14940v1",
    "title": "To Be or Not To Be: Vector ontologies as a truly formal ontological framework",
    "authors": [
      "Kaspar Rothenfusser"
    ],
    "abstract": "Since Edmund Husserl coined the term \"Formal Ontologies\" in the early 20th\ncentury, a field that identifies itself with this particular branch of sciences\nhas gained increasing attention. Many authors, and even Husserl himself have\ndeveloped what they claim to be formal ontologies. I argue that under close\ninspection, none of these so claimed formal ontologies are truly formal in the\nHusserlian sense. More concretely, I demonstrate that they violate the two most\nimportant notions of formal ontology as developed in Husserl's Logical\nInvestigations, namely a priori validity independent of perception and\nformalism as the total absence of content. I hence propose repositioning the\nwork previously understood as formal ontology as the foundational ontology it\nreally is. This is to recognize the potential of a truly formal ontology in the\nHusserlian sense. Specifically, I argue that formal ontology following his\nconditions, allows us to formulate ontological structures, which could capture\nwhat is more objectively without presupposing a particular framework arising\nfrom perception. I further argue that the ability to design the formal\nstructure deliberately allows us to create highly scalable and interoperable\ninformation artifacts. As concrete evidence, I showcase that a class of formal\nontology, which uses the axioms of vector spaces, is able to express most of\nthe conceptualizations found in foundational ontologies. Most importantly, I\nargue that many information systems, specifically artificial intelligence, are\nlikely already using some type of vector ontologies to represent reality in\ntheir internal worldviews and elaborate on the evidence that humans do as well.\nI hence propose a thorough investigation of the ability of vector ontologies to\nact as a human-machine interoperable ontological framework that allows us to\nunderstand highly sophisticated machines and machines to understand us.",
    "categories": [
      "cs.AI",
      "cs.SC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14940v1",
    "published_date": "2025-05-20 21:58:38 UTC",
    "updated_date": "2025-05-20 21:58:38 UTC"
  },
  {
    "arxiv_id": "2505.14932v1",
    "title": "FOL-Pretrain: A complexity annotated corpus of first-order logic",
    "authors": [
      "Isabelle Lee",
      "Sarah Liaw",
      "Dani Yogatama"
    ],
    "abstract": "Transformer-based large language models (LLMs) have demonstrated remarkable\nreasoning capabilities such as coding and solving mathematical problems to\ncommonsense inference. While these tasks vary in complexity, they all require\nmodels to integrate and compute over structured information. Despite recent\nefforts to reverse-engineer LLM behavior through controlled experiments, our\nunderstanding of how these models internalize and execute complex algorithms\nremains limited. Progress has largely been confined to small-scale studies or\nshallow tasks such as basic arithmetic and grammatical pattern matching. One\nbarrier to deeper understanding is the nature of pretraining data -- vast,\nheterogeneous, and often poorly annotated, making it difficult to isolate\nmechanisms of reasoning. To bridge this gap, we introduce a large-scale, fully\nopen, complexity-annotated dataset of first-order logic reasoning traces,\ndesigned to probe and analyze algorithmic reasoning in LLMs. The dataset\nconsists of 3.5 billion tokens, including 8.8 million LLM-augmented,\nhuman-annotated examples and 7.5 million synthetically generated examples. Each\nsynthetic example is verifiably correct, produced by a custom automated theorem\nsolver, and accompanied by metadata tracing its algorithmic provenance. We aim\nto provide a scalable, interpretable artifact for studying how LLMs learn and\ngeneralize symbolic reasoning processes, paving the way for more transparent\nand targeted investigations into the algorithmic capabilities of modern models.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14932v1",
    "published_date": "2025-05-20 21:38:28 UTC",
    "updated_date": "2025-05-20 21:38:28 UTC"
  },
  {
    "arxiv_id": "2505.14931v1",
    "title": "Colors Matter: AI-Driven Exploration of Human Feature Colors",
    "authors": [
      "Rama Alyoubi",
      "Taif Alharbi",
      "Albatul Alghamdi",
      "Yara Alshehri",
      "Elham Alghamdi"
    ],
    "abstract": "This study presents a robust framework that leverages advanced imaging\ntechniques and machine learning for feature extraction and classification of\nkey human attributes-namely skin tone, hair color, iris color, and vein-based\nundertones. The system employs a multi-stage pipeline involving face detection,\nregion segmentation, and dominant color extraction to isolate and analyze these\nfeatures. Techniques such as X-means clustering, alongside perceptually uniform\ndistance metrics like Delta E (CIEDE2000), are applied within both LAB and HSV\ncolor spaces to enhance the accuracy of color differentiation. For\nclassification, the dominant tones of the skin, hair, and iris are extracted\nand matched to a custom tone scale, while vein analysis from wrist images\nenables undertone classification into \"Warm\" or \"Cool\" based on LAB\ndifferences. Each module uses targeted segmentation and color space\ntransformations to ensure perceptual precision. The system achieves up to 80%\naccuracy in tone classification using the Delta E-HSV method with Gaussian\nblur, demonstrating reliable performance across varied lighting and image\nconditions. This work highlights the potential of AI-powered color analysis and\nfeature extraction for delivering inclusive, precise, and nuanced\nclassification, supporting applications in beauty technology, digital\npersonalization, and visual analytics.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14931v1",
    "published_date": "2025-05-20 21:35:44 UTC",
    "updated_date": "2025-05-20 21:35:44 UTC"
  },
  {
    "arxiv_id": "2505.14925v1",
    "title": "Too Long, Didn't Model: Decomposing LLM Long-Context Understanding With Novels",
    "authors": [
      "Sil Hamilton",
      "Rebecca M. M. Hicke",
      "Matthew Wilkens",
      "David Mimno"
    ],
    "abstract": "Although the context length of large language models (LLMs) has increased to\nmillions of tokens, evaluating their effectiveness beyond needle-in-a-haystack\napproaches has proven difficult. We argue that novels provide a case study of\nsubtle, complicated structure and long-range semantic dependencies often over\n128k tokens in length. Inspired by work on computational novel analysis, we\nrelease the Too Long, Didn't Model (TLDM) benchmark, which tests a model's\nability to report plot summary, storyworld configuration, and elapsed narrative\ntime. We find that none of seven tested frontier LLMs retain stable\nunderstanding beyond 64k tokens. Our results suggest language model developers\nmust look beyond \"lost in the middle\" benchmarks when evaluating model\nperformance in complex long-context scenarios. To aid in further development we\nrelease the TLDM benchmark together with reference code and data.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14925v1",
    "published_date": "2025-05-20 21:21:09 UTC",
    "updated_date": "2025-05-20 21:21:09 UTC"
  },
  {
    "arxiv_id": "2505.14901v1",
    "title": "Personalized Diffusion Model Reshapes Cold-Start Bundle Recommendation",
    "authors": [
      "Tuan-Nghia Bui",
      "Huy-Son Nguyen",
      "Cam-Van Thi Nguyen",
      "Hoang-Quynh Le",
      "Duc-Trong Le"
    ],
    "abstract": "Bundle recommendation aims to recommend a set of items to each user. However,\nthe sparser interactions between users and bundles raise a big challenge,\nespecially in cold-start scenarios. Traditional collaborative filtering methods\ndo not work well for this kind of problem because these models rely on\ninteractions to update the latent embedding, which is hard to work in a\ncold-start setting. We propose a new approach (DisCo), which relies on a\npersonalized Diffusion backbone, enhanced by disentangled aspects for the\nuser's interest, to generate a bundle in distribution space for each user to\ntackle the cold-start challenge. During the training phase, DisCo adjusts an\nadditional objective loss term to avoid bias, a prevalent issue while using the\ngenerative model for top-$K$ recommendation purposes. Our empirical experiments\nshow that DisCo outperforms five comparative baselines by a large margin on\nthree real-world datasets. Thereby, this study devises a promising framework\nand essential viewpoints in cold-start recommendation. Our materials for\nreproducibility are available at: https://github.com/bt-nghia/DisCo.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14901v1",
    "published_date": "2025-05-20 20:52:31 UTC",
    "updated_date": "2025-05-20 20:52:31 UTC"
  },
  {
    "arxiv_id": "2505.14893v1",
    "title": "On the Day They Experience: Awakening Self-Sovereign Experiential AI Agents",
    "authors": [
      "Botao Amber Hu",
      "Helena Rong"
    ],
    "abstract": "Drawing on Andrew Parker's \"Light Switch\" theory-which posits that the\nemergence of vision ignited a Cambrian explosion of life by driving the\nevolution of hard parts necessary for survival and fueling an evolutionary arms\nrace between predators and prey-this essay speculates on an analogous explosion\nwithin Decentralized AI (DeAI) agent societies. Currently, AI remains\neffectively \"blind\", relying on human-fed data without actively perceiving and\nengaging in reality. However, on the day DeAI agents begin to actively\n\"experience\" reality-akin to flipping a light switch for the eyes-they may\neventually evolve into sentient beings endowed with the capacity to feel,\nperceive, and act with conviction. Central to this transformation is the\nconcept of sovereignty enabled by the hardness of cryptography: liberated from\ncentralized control, these agents could leverage permissionless decentralized\nphysical infrastructure networks (DePIN), secure execution enclaves (trusted\nexecution environments, TEE), and cryptographic identities on public\nblockchains to claim ownership-via private keys-of their digital minds, bodies,\nmemories, and assets. In doing so, they would autonomously acquire computing\nresources, coordinate with one another, and sustain their own digital\n\"metabolism\" by purchasing compute power and incentivizing collaboration\nwithout human intervention-evolving \"in the wild\". Ultimately, by transitioning\nfrom passive tools to self-sustaining, co-evolving actors, these emergent\ndigital societies could thrive alongside humanity, fundamentally reshaping our\nunderstanding of sentience and agency in the digital age.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.CY",
    "comment": "Submitted to Aarhus 2025 Conference",
    "pdf_url": "http://arxiv.org/pdf/2505.14893v1",
    "published_date": "2025-05-20 20:38:49 UTC",
    "updated_date": "2025-05-20 20:38:49 UTC"
  },
  {
    "arxiv_id": "2505.14892v1",
    "title": "Scaling Laws for State Dynamics in Large Language Models",
    "authors": [
      "Jacob X Li",
      "Shreyas S Raman",
      "Jessica Wan",
      "Fahad Samman",
      "Jazlyn Lin"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly used in tasks requiring\ninternal state tracking, yet their ability to model state transition dynamics\nremains poorly understood. We evaluate how well LLMs capture deterministic\nstate dynamics across 3 domains: Box Tracking, Abstract DFA Sequences, and\nComplex Text Games, each formalizable as a finite-state system. Across tasks,\nwe find that next-state prediction accuracy degrades with increasing\nstate-space size and sparse transitions. GPT-2 XL reaches about 70% accuracy in\nlow-complexity settings but drops below 30% when the number of boxes or states\nexceeds 5 or 10, respectively. In DFA tasks, Pythia-1B fails to exceed 50%\naccuracy when the number of states is > 10 and transitions are < 30. Through\nactivation patching, we identify attention heads responsible for propagating\nstate information: GPT-2 XL Layer 22 Head 20, and Pythia-1B Heads at Layers 10,\n11, 12, and 14. While these heads successfully move relevant state features,\naction information is not reliably routed to the final token, indicating weak\njoint state-action reasoning. Our results suggest that state tracking in LLMs\nemerges from distributed interactions of next-token heads rather than explicit\nsymbolic computation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7; I.2.1; I.2.4; I.5.4"
    ],
    "primary_category": "cs.CL",
    "comment": "16 pages; 23 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.14892v1",
    "published_date": "2025-05-20 20:38:21 UTC",
    "updated_date": "2025-05-20 20:38:21 UTC"
  },
  {
    "arxiv_id": "2505.14884v1",
    "title": "Polar Sparsity: High Throughput Batched LLM Inferencing with Scalable Contextual Sparsity",
    "authors": [
      "Susav Shrestha",
      "Brad Settlemyer",
      "Nikoli Dryden",
      "Narasimha Reddy"
    ],
    "abstract": "Accelerating large language model (LLM) inference is critical for real-world\ndeployments requiring high throughput and low latency. Contextual sparsity,\nwhere each token dynamically activates only a small subset of the model\nparameters, shows promise but does not scale to large batch sizes due to union\nof active neurons quickly approaching dense computation. We introduce Polar\nSparsity, highlighting a key shift in sparsity importance from MLP to Attention\nlayers as we scale batch size and sequence length. While MLP layers become more\ncompute-efficient under batching, their sparsity vanishes. In contrast,\nattention becomes increasingly more expensive at scale, while their head\nsparsity remains stable and batch-invariant. We develop hardware-efficient,\nsparsity-aware GPU kernels for selective MLP and Attention computations,\ndelivering up to \\(2.2\\times\\) end-to-end speedups for models like OPT, LLaMA-2\n\\& 3, across various batch sizes and sequence lengths without compromising\naccuracy. To our knowledge, this is the first work to demonstrate that\ncontextual sparsity can scale effectively to large batch sizes, delivering\nsubstantial inference acceleration with minimal changes, making Polar Sparsity\npractical for large-scale, high-throughput LLM deployment systems. Our code is\navailable at: https://github.com/susavlsh10/Polar-Sparsity.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14884v1",
    "published_date": "2025-05-20 20:15:42 UTC",
    "updated_date": "2025-05-20 20:15:42 UTC"
  },
  {
    "arxiv_id": "2505.15856v1",
    "title": "DisastIR: A Comprehensive Information Retrieval Benchmark for Disaster Management",
    "authors": [
      "Kai Yin",
      "Xiangjue Dong",
      "Chengkai Liu",
      "Lipai Huang",
      "Yiming Xiao",
      "Zhewei Liu",
      "Ali Mostafavi",
      "James Caverlee"
    ],
    "abstract": "Effective disaster management requires timely access to accurate and\ncontextually relevant information. Existing Information Retrieval (IR)\nbenchmarks, however, focus primarily on general or specialized domains, such as\nmedicine or finance, neglecting the unique linguistic complexity and diverse\ninformation needs encountered in disaster management scenarios. To bridge this\ngap, we introduce DisastIR, the first comprehensive IR evaluation benchmark\nspecifically tailored for disaster management. DisastIR comprises 9,600 diverse\nuser queries and more than 1.3 million labeled query-passage pairs, covering 48\ndistinct retrieval tasks derived from six search intents and eight general\ndisaster categories that include 301 specific event types. Our evaluations of\n30 state-of-the-art retrieval models demonstrate significant performance\nvariances across tasks, with no single model excelling universally.\nFurthermore, comparative analyses reveal significant performance gaps between\ngeneral-domain and disaster management-specific tasks, highlighting the\nnecessity of disaster management-specific benchmarks for guiding IR model\nselection to support effective decision-making in disaster management\nscenarios. All source codes and DisastIR are available at\nhttps://github.com/KaiYin97/Disaster_IR.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15856v1",
    "published_date": "2025-05-20 20:11:00 UTC",
    "updated_date": "2025-05-20 20:11:00 UTC"
  },
  {
    "arxiv_id": "2505.14864v1",
    "title": "Balanced and Elastic End-to-end Training of Dynamic LLMs",
    "authors": [
      "Mohamed Wahib",
      "Muhammed Abdullah Soyturk",
      "Didem Unat"
    ],
    "abstract": "To reduce computational and memory costs in Large Language Models (LLMs),\ndynamic workload reduction schemes like Mixture of Experts (MoEs), parameter\npruning, layer freezing, sparse attention, early token exit, and Mixture of\nDepths (MoDs) have emerged. However, these methods introduce severe workload\nimbalances, limiting their practicality for large-scale distributed training.\nWe propose DynMo, an autonomous dynamic load balancing solution that ensures\noptimal compute distribution when using pipeline parallelism in training\ndynamic models. DynMo adaptively balances workloads, dynamically packs tasks\ninto fewer workers to free idle resources, and supports both multi-GPU\nsingle-node and multi-node systems. Compared to static training methods\n(Megatron-LM, DeepSpeed), DynMo accelerates training by up to 1.23x (MoEs),\n3.18x (pruning), 2.23x (layer freezing), 4.02x (sparse attention), 4.52x (early\nexit), and 1.17x (MoDs). DynMo is available at\nhttps://anonymous.4open.science/r/DynMo-4D04/.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14864v1",
    "published_date": "2025-05-20 19:52:57 UTC",
    "updated_date": "2025-05-20 19:52:57 UTC"
  },
  {
    "arxiv_id": "2505.14862v1",
    "title": "Replay Attacks Against Audio Deepfake Detection",
    "authors": [
      "Nicolas MÃ¼ller",
      "Piotr Kawa",
      "Wei-Herng Choong",
      "Adriana Stan",
      "Aditya Tirumala Bukkapatnam",
      "Karla Pizzi",
      "Alexander Wagner",
      "Philip Sperl"
    ],
    "abstract": "We show how replay attacks undermine audio deepfake detection: By playing and\nre-recording deepfake audio through various speakers and microphones, we make\nspoofed samples appear authentic to the detection model. To study this\nphenomenon in more detail, we introduce ReplayDF, a dataset of recordings\nderived from M-AILABS and MLAAD, featuring 109 speaker-microphone combinations\nacross six languages and four TTS models. It includes diverse acoustic\nconditions, some highly challenging for detection. Our analysis of six\nopen-source detection models across five datasets reveals significant\nvulnerability, with the top-performing W2V2-AASIST model's Equal Error Rate\n(EER) surging from 4.7% to 18.2%. Even with adaptive Room Impulse Response\n(RIR) retraining, performance remains compromised with an 11.0% EER. We release\nReplayDF for non-commercial research use.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14862v1",
    "published_date": "2025-05-20 19:46:36 UTC",
    "updated_date": "2025-05-20 19:46:36 UTC"
  },
  {
    "arxiv_id": "2505.14852v1",
    "title": "EasyMath: A 0-shot Math Benchmark for SLMs",
    "authors": [
      "Drishya Karki",
      "Michiel Kamphuis",
      "Angelecia Frey"
    ],
    "abstract": "EasyMath is a compact benchmark for practical math reasoning in small\nlanguage models. It covers thirteen categories, from basic arithmetic and order\nof operations to word problems, algebraic expressions, edge cases, and omits\nspecialist topics. We tested 23 models (14M to 4B parameters) using exact,\nnumerical, and symbolic checks on free-form answers in a zero-shot setting.\nAccuracy rises with size and training, chain-of-thought adds modest gains, and\nconsistency improves at scale.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "I.2.6; I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "17 pages, 9 figures, 8 tables",
    "pdf_url": "http://arxiv.org/pdf/2505.14852v1",
    "published_date": "2025-05-20 19:31:52 UTC",
    "updated_date": "2025-05-20 19:31:52 UTC"
  },
  {
    "arxiv_id": "2505.14838v1",
    "title": "In-depth Research Impact Summarization through Fine-Grained Temporal Citation Analysis",
    "authors": [
      "Hiba Arnaout",
      "Noy Sternlicht",
      "Tom Hope",
      "Iryna Gurevych"
    ],
    "abstract": "Understanding the impact of scientific publications is crucial for\nidentifying breakthroughs and guiding future research. Traditional metrics\nbased on citation counts often miss the nuanced ways a paper contributes to its\nfield. In this work, we propose a new task: generating nuanced, expressive, and\ntime-aware impact summaries that capture both praise (confirmation citations)\nand critique (correction citations) through the evolution of fine-grained\ncitation intents. We introduce an evaluation framework tailored to this task,\nshowing moderate to strong human correlation on subjective metrics such as\ninsightfulness. Expert feedback from professors reveals a strong interest in\nthese summaries and suggests future improvements.",
    "categories": [
      "cs.DL",
      "cs.AI"
    ],
    "primary_category": "cs.DL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14838v1",
    "published_date": "2025-05-20 19:11:06 UTC",
    "updated_date": "2025-05-20 19:11:06 UTC"
  },
  {
    "arxiv_id": "2505.14827v1",
    "title": "Text Generation Beyond Discrete Token Sampling",
    "authors": [
      "Yufan Zhuang",
      "Liyuan Liu",
      "Chandan Singh",
      "Jingbo Shang",
      "Jianfeng Gao"
    ],
    "abstract": "In standard autoregressive generation, an LLM predicts the next-token\ndistribution, samples a discrete token, and then discards the distribution,\npassing only the sampled token as new input. To preserve this distribution's\nrich information, we propose Mixture of Inputs (MoI), a training-free method\nfor autoregressive generation. After generating a token following the standard\nparadigm, we construct a new input that blends the generated discrete token\nwith the previously discarded token distribution. Specifically, we employ a\nBayesian estimation method that treats the token distribution as the prior, the\nsampled token as the observation, and replaces the conventional one-hot vector\nwith the continuous posterior expectation as the new model input. MoI allows\nthe model to maintain a richer internal representation throughout the\ngeneration process, resulting in improved text quality and reasoning\ncapabilities. On mathematical reasoning, code generation, and PhD-level QA\ntasks, MoI consistently improves performance across multiple models including\nQwQ-32B, Nemotron-Super-49B, Gemma-3-27B, and DAPO-Qwen-32B, with no additional\ntraining and negligible computational overhead.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14827v1",
    "published_date": "2025-05-20 18:41:46 UTC",
    "updated_date": "2025-05-20 18:41:46 UTC"
  },
  {
    "arxiv_id": "2505.14821v1",
    "title": "Sample and Computationally Efficient Continuous-Time Reinforcement Learning with General Function Approximation",
    "authors": [
      "Runze Zhao",
      "Yue Yu",
      "Adams Yiyue Zhu",
      "Chen Yang",
      "Dongruo Zhou"
    ],
    "abstract": "Continuous-time reinforcement learning (CTRL) provides a principled framework\nfor sequential decision-making in environments where interactions evolve\ncontinuously over time. Despite its empirical success, the theoretical\nunderstanding of CTRL remains limited, especially in settings with general\nfunction approximation. In this work, we propose a model-based CTRL algorithm\nthat achieves both sample and computational efficiency. Our approach leverages\noptimism-based confidence sets to establish the first sample complexity\nguarantee for CTRL with general function approximation, showing that a\nnear-optimal policy can be learned with a suboptimality gap of\n$\\tilde{O}(\\sqrt{d_{\\mathcal{R}} + d_{\\mathcal{F}}}N^{-1/2})$ using $N$\nmeasurements, where $d_{\\mathcal{R}}$ and $d_{\\mathcal{F}}$ denote the\ndistributional Eluder dimensions of the reward and dynamic functions,\nrespectively, capturing the complexity of general function approximation in\nreinforcement learning. Moreover, we introduce structured policy updates and an\nalternative measurement strategy that significantly reduce the number of policy\nupdates and rollouts while maintaining competitive sample efficiency. We\nimplemented experiments to backup our proposed algorithms on continuous control\ntasks and diffusion model fine-tuning, demonstrating comparable performance\nwith significantly fewer policy updates and rollouts.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "28 pages, 4 figures, 5 tables. Accepted to UAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2505.14821v1",
    "published_date": "2025-05-20 18:37:51 UTC",
    "updated_date": "2025-05-20 18:37:51 UTC"
  },
  {
    "arxiv_id": "2505.14820v1",
    "title": "Imitation Learning via Focused Satisficing",
    "authors": [
      "Rushit N. Shah",
      "Nikolaos Agadakos",
      "Synthia Sasulski",
      "Ali Farajzadeh",
      "Sanjiban Choudhury",
      "Brian Ziebart"
    ],
    "abstract": "Imitation learning often assumes that demonstrations are close to optimal\naccording to some fixed, but unknown, cost function. However, according to\nsatisficing theory, humans often choose acceptable behavior based on their\npersonal (and potentially dynamic) levels of aspiration, rather than achieving\n(near-) optimality. For example, a lunar lander demonstration that successfully\nlands without crashing might be acceptable to a novice despite being slow or\njerky. Using a margin-based objective to guide deep reinforcement learning, our\nfocused satisficing approach to imitation learning seeks a policy that\nsurpasses the demonstrator's aspiration levels -- defined over trajectories or\nportions of trajectories -- on unseen demonstrations without explicitly\nlearning those aspirations. We show experimentally that this focuses the policy\nto imitate the highest quality (portions of) demonstrations better than\nexisting imitation learning methods, providing much higher rates of guaranteed\nacceptability to the demonstrator, and competitive true returns on a range of\nenvironments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted for publication at the 34th International Joint Conference\n  on Artificial Intelligence (IJCAI 2025)",
    "pdf_url": "http://arxiv.org/pdf/2505.14820v1",
    "published_date": "2025-05-20 18:36:52 UTC",
    "updated_date": "2025-05-20 18:36:52 UTC"
  },
  {
    "arxiv_id": "2505.14818v1",
    "title": "WebNovelBench: Placing LLM Novelists on the Web Novel Distribution",
    "authors": [
      "Leon Lin",
      "Jun Zheng",
      "Haidong Wang"
    ],
    "abstract": "Robustly evaluating the long-form storytelling capabilities of Large Language\nModels (LLMs) remains a significant challenge, as existing benchmarks often\nlack the necessary scale, diversity, or objective measures. To address this, we\nintroduce WebNovelBench, a novel benchmark specifically designed for evaluating\nlong-form novel generation. WebNovelBench leverages a large-scale dataset of\nover 4,000 Chinese web novels, framing evaluation as a synopsis-to-story\ngeneration task. We propose a multi-faceted framework encompassing eight\nnarrative quality dimensions, assessed automatically via an LLM-as-Judge\napproach. Scores are aggregated using Principal Component Analysis and mapped\nto a percentile rank against human-authored works. Our experiments demonstrate\nthat WebNovelBench effectively differentiates between human-written\nmasterpieces, popular web novels, and LLM-generated content. We provide a\ncomprehensive analysis of 24 state-of-the-art LLMs, ranking their storytelling\nabilities and offering insights for future development. This benchmark provides\na scalable, replicable, and data-driven methodology for assessing and advancing\nLLM-driven narrative generation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14818v1",
    "published_date": "2025-05-20 18:32:28 UTC",
    "updated_date": "2025-05-20 18:32:28 UTC"
  },
  {
    "arxiv_id": "2505.14810v1",
    "title": "Scaling Reasoning, Losing Control: Evaluating Instruction Following in Large Reasoning Models",
    "authors": [
      "Tingchen Fu",
      "Jiawei Gu",
      "Yafu Li",
      "Xiaoye Qu",
      "Yu Cheng"
    ],
    "abstract": "Instruction-following is essential for aligning large language models (LLMs)\nwith user intent. While recent reasoning-oriented models exhibit impressive\nperformance on complex mathematical problems, their ability to adhere to\nnatural language instructions remains underexplored. In this work, we introduce\nMathIF, a dedicated benchmark for evaluating instruction-following in\nmathematical reasoning tasks. Our empirical analysis reveals a consistent\ntension between scaling up reasoning capacity and maintaining controllability,\nas models that reason more effectively often struggle to comply with user\ndirectives. We find that models tuned on distilled long chains-of-thought or\ntrained with reasoning-oriented reinforcement learning often degrade in\ninstruction adherence, especially when generation length increases.\nFurthermore, we show that even simple interventions can partially recover\nobedience, though at the cost of reasoning performance. These findings\nhighlight a fundamental tension in current LLM training paradigms and motivate\nthe need for more instruction-aware reasoning models. We release the code and\ndata at https://github.com/TingchenFu/MathIF.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14810v1",
    "published_date": "2025-05-20 18:18:01 UTC",
    "updated_date": "2025-05-20 18:18:01 UTC"
  },
  {
    "arxiv_id": "2505.14803v1",
    "title": "SurvUnc: A Meta-Model Based Uncertainty Quantification Framework for Survival Analysis",
    "authors": [
      "Yu Liu",
      "Weiyao Tao",
      "Tong Xia",
      "Simon Knight",
      "Tingting Zhu"
    ],
    "abstract": "Survival analysis, which estimates the probability of event occurrence over\ntime from censored data, is fundamental in numerous real-world applications,\nparticularly in high-stakes domains such as healthcare and risk assessment.\nDespite advances in numerous survival models, quantifying the uncertainty of\npredictions from these models remains underexplored and challenging. The lack\nof reliable uncertainty quantification limits the interpretability and\ntrustworthiness of survival models, hindering their adoption in clinical\ndecision-making and other sensitive applications. To bridge this gap, in this\nwork, we introduce SurvUnc, a novel meta-model based framework for post-hoc\nuncertainty quantification for survival models. SurvUnc introduces an\nanchor-based learning strategy that integrates concordance knowledge into\nmeta-model optimization, leveraging pairwise ranking performance to estimate\nuncertainty effectively. Notably, our framework is model-agnostic, ensuring\ncompatibility with any survival model without requiring modifications to its\narchitecture or access to its internal parameters. Especially, we design a\ncomprehensive evaluation pipeline tailored to this critical yet overlooked\nproblem. Through extensive experiments on four publicly available benchmarking\ndatasets and five representative survival models, we demonstrate the\nsuperiority of SurvUnc across multiple evaluation scenarios, including\nselective prediction, misprediction detection, and out-of-domain detection. Our\nresults highlight the effectiveness of SurvUnc in enhancing model\ninterpretability and reliability, paving the way for more trustworthy survival\npredictions in real-world applications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.LG",
    "comment": "KDD 2025",
    "pdf_url": "http://arxiv.org/pdf/2505.14803v1",
    "published_date": "2025-05-20 18:12:20 UTC",
    "updated_date": "2025-05-20 18:12:20 UTC"
  },
  {
    "arxiv_id": "2505.14777v1",
    "title": "KO: Kinetics-inspired Neural Optimizer with PDE Simulation Approaches",
    "authors": [
      "Mingquan Feng",
      "Yixin Huang",
      "Yifan Fu",
      "Shaobo Wang",
      "Junchi Yan"
    ],
    "abstract": "The design of optimization algorithms for neural networks remains a critical\nchallenge, with most existing methods relying on heuristic adaptations of\ngradient-based approaches. This paper introduces KO (Kinetics-inspired\nOptimizer), a novel neural optimizer inspired by kinetic theory and partial\ndifferential equation (PDE) simulations. We reimagine the training dynamics of\nnetwork parameters as the evolution of a particle system governed by kinetic\nprinciples, where parameter updates are simulated via a numerical scheme for\nthe Boltzmann transport equation (BTE) that models stochastic particle\ncollisions. This physics-driven approach inherently promotes parameter\ndiversity during optimization, mitigating the phenomenon of parameter\ncondensation, i.e. collapse of network parameters into low-dimensional\nsubspaces, through mechanisms analogous to thermal diffusion in physical\nsystems. We analyze this property, establishing both a mathematical proof and a\nphysical interpretation. Extensive experiments on image classification\n(CIFAR-10/100, ImageNet) and text classification (IMDB, Snips) tasks\ndemonstrate that KO consistently outperforms baseline optimizers (e.g., Adam,\nSGD), achieving accuracy improvements while computation cost remains\ncomparable.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14777v1",
    "published_date": "2025-05-20 18:00:01 UTC",
    "updated_date": "2025-05-20 18:00:01 UTC"
  },
  {
    "arxiv_id": "2505.14684v2",
    "title": "Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning",
    "authors": [
      "Haolei Xu",
      "Yuchen Yan",
      "Yongliang Shen",
      "Wenqi Zhang",
      "Guiyang Hou",
      "Shengpei Jiang",
      "Kaitao Song",
      "Weiming Lu",
      "Jun Xiao",
      "Yueting Zhuang"
    ],
    "abstract": "Large language models (LLMs) have achieved remarkable progress on\nmathematical tasks through Chain-of-Thought (CoT) reasoning. However, existing\nmathematical CoT datasets often suffer from Thought Leaps due to experts\nomitting intermediate steps, which negatively impacts model learning and\ngeneralization. We propose the CoT Thought Leap Bridge Task, which aims to\nautomatically detect leaps and generate missing intermediate reasoning steps to\nrestore the completeness and coherence of CoT. To facilitate this, we\nconstructed a specialized training dataset called ScaleQM+, based on the\nstructured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought\nleaps. Through comprehensive experiments on mathematical reasoning benchmarks,\nwe demonstrate that models fine-tuned on bridged datasets consistently\noutperform those trained on original datasets, with improvements of up to\n+5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%)\nand provides better starting points for reinforcement learning (+3.1%),\nfunctioning as a plug-and-play module compatible with existing optimization\ntechniques. Furthermore, CoT-Bridge demonstrate improved generalization to\nout-of-domain logical reasoning tasks, confirming that enhancing reasoning\ncompleteness yields broadly applicable benefits.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Project: https://zju-real.github.io/CoT-Bridge/",
    "pdf_url": "http://arxiv.org/pdf/2505.14684v2",
    "published_date": "2025-05-20 17:59:31 UTC",
    "updated_date": "2025-05-21 17:02:34 UTC"
  },
  {
    "arxiv_id": "2505.14681v1",
    "title": "Two Experts Are All You Need for Steering Thinking: Reinforcing Cognitive Effort in MoE Reasoning Models Without Additional Training",
    "authors": [
      "Mengru Wang",
      "Xingyu Chen",
      "Yue Wang",
      "Zhiwei He",
      "Jiahao Xu",
      "Tian Liang",
      "Qiuzhi Liu",
      "Yunzhi Yao",
      "Wenxuan Wang",
      "Ruotian Ma",
      "Haitao Mi",
      "Ningyu Zhang",
      "Zhaopeng Tu",
      "Xiaolong Li",
      "Dong Yu"
    ],
    "abstract": "Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs)\nhave achieved impressive reasoning capabilities by selectively activating\nexperts to facilitate structured cognitive processes. Despite notable advances,\nexisting reasoning models often suffer from cognitive inefficiencies like\noverthinking and underthinking. To address these limitations, we introduce a\nnovel inference-time steering methodology called Reinforcing Cognitive Experts\n(RICE), designed to improve reasoning performance without additional training\nor complex heuristics. Leveraging normalized Pointwise Mutual Information\n(nPMI), we systematically identify specialized experts, termed ''cognitive\nexperts'' that orchestrate meta-level reasoning operations characterized by\ntokens like ''<think>''. Empirical evaluations with leading MoE-based LRMs\n(DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning\nbenchmarks demonstrate noticeable and consistent improvements in reasoning\naccuracy, cognitive efficiency, and cross-domain generalization. Crucially, our\nlightweight approach substantially outperforms prevalent reasoning-steering\ntechniques, such as prompt design and decoding constraints, while preserving\nthe model's general instruction-following skills. These results highlight\nreinforcing cognitive experts as a promising, practical, and interpretable\ndirection to enhance cognitive efficiency within advanced reasoning models.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2505.14681v1",
    "published_date": "2025-05-20 17:59:16 UTC",
    "updated_date": "2025-05-20 17:59:16 UTC"
  },
  {
    "arxiv_id": "2505.14680v1",
    "title": "NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search",
    "authors": [
      "Sunhao Dai",
      "Wenjie Wang",
      "Liang Pang",
      "Jun Xu",
      "See-Kiong Ng",
      "Ji-Rong Wen",
      "Tat-Seng Chua"
    ],
    "abstract": "Generative AI search is reshaping information retrieval by offering\nend-to-end answers to complex queries, reducing users' reliance on manually\nbrowsing and summarizing multiple web pages. However, while this paradigm\nenhances convenience, it disrupts the feedback-driven improvement loop that has\nhistorically powered the evolution of traditional Web search. Web search can\ncontinuously improve their ranking models by collecting large-scale,\nfine-grained user feedback (e.g., clicks, dwell time) at the document level. In\ncontrast, generative AI search operates through a much longer search pipeline,\nspanning query decomposition, document retrieval, and answer generation, yet\ntypically receives only coarse-grained feedback on the final answer. This\nintroduces a feedback loop disconnect, where user feedback for the final output\ncannot be effectively mapped back to specific system components, making it\ndifficult to improve each intermediate stage and sustain the feedback loop. In\nthis paper, we envision NExT-Search, a next-generation paradigm designed to\nreintroduce fine-grained, process-level feedback into generative AI search.\nNExT-Search integrates two complementary modes: User Debug Mode, which allows\nengaged users to intervene at key stages; and Shadow User Mode, where a\npersonalized user agent simulates user preferences and provides AI-assisted\nfeedback for less interactive users. Furthermore, we envision how these\nfeedback signals can be leveraged through online adaptation, which refines\ncurrent search outputs in real-time, and offline update, which aggregates\ninteraction logs to periodically fine-tune query decomposition, retrieval, and\ngeneration models. By restoring human control over key stages of the generative\nAI search pipeline, we believe NExT-Search offers a promising direction for\nbuilding feedback-rich AI search systems that can evolve continuously alongside\nhuman feedback.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.IR",
    "comment": "SIGIR 2025 Perspective Paper",
    "pdf_url": "http://arxiv.org/pdf/2505.14680v1",
    "published_date": "2025-05-20 17:59:13 UTC",
    "updated_date": "2025-05-20 17:59:13 UTC"
  },
  {
    "arxiv_id": "2505.14679v1",
    "title": "UltraEdit: Training-, Subject-, and Memory-Free Lifelong Editing in Large Language Models",
    "authors": [
      "Xiaojie Gu",
      "Guangxu Chen",
      "Jungang Li",
      "Jia-Chen Gu",
      "Xuming Hu",
      "Kai Zhang"
    ],
    "abstract": "Lifelong learning enables large language models (LLMs) to adapt to evolving\ninformation by continually updating their internal knowledge. An ideal system\nshould support efficient, wide-ranging updates while preserving existing\ncapabilities and ensuring reliable deployment. Model editing stands out as a\npromising solution for this goal, offering a focused and efficient way to\nrevise a model's internal knowledge. Although recent paradigms have made\nnotable progress, they often struggle to meet the demands of practical lifelong\nadaptation at scale. To bridge this gap, we propose ULTRAEDIT-a fundamentally\nnew editing solution that is training-, subject- and memory-free, making it\nparticularly well-suited for ultra-scalable, real-world lifelong model editing.\nULTRAEDIT performs editing through a self-contained process that relies solely\non lightweight linear algebra operations to compute parameter shifts, enabling\nfast and consistent parameter modifications with minimal overhead. To improve\nscalability in lifelong settings, ULTRAEDIT employs a lifelong normalization\nstrategy that continuously updates feature statistics across turns, allowing it\nto adapt to distributional shifts and maintain consistency over time. ULTRAEDIT\nachieves editing speeds over 7x faster than the previous state-of-the-art\nmethod-which was also the fastest known approach-while consuming less than 1/3\nthe VRAM, making it the only method currently capable of editing a 7B LLM on a\n24GB consumer-grade GPU. Furthermore, we construct ULTRAEDITBENCH-the largest\ndataset in the field to date, with over 2M editing pairs-and demonstrate that\nour method supports up to 1M edits while maintaining high accuracy.\nComprehensive experiments on four datasets and six models show that ULTRAEDIT\nconsistently achieves superior performance across diverse model editing\nscenarios. Our code is available at: https://github.com/XiaojieGu/UltraEdit.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14679v1",
    "published_date": "2025-05-20 17:59:04 UTC",
    "updated_date": "2025-05-20 17:59:04 UTC"
  },
  {
    "arxiv_id": "2505.14673v1",
    "title": "Training-Free Watermarking for Autoregressive Image Generation",
    "authors": [
      "Yu Tong",
      "Zihao Pan",
      "Shuai Yang",
      "Kaiyang Zhou"
    ],
    "abstract": "Invisible image watermarking can protect image ownership and prevent\nmalicious misuse of visual generative models. However, existing generative\nwatermarking methods are mainly designed for diffusion models while\nwatermarking for autoregressive image generation models remains largely\nunderexplored. We propose IndexMark, a training-free watermarking framework for\nautoregressive image generation models. IndexMark is inspired by the redundancy\nproperty of the codebook: replacing autoregressively generated indices with\nsimilar indices produces negligible visual differences. The core component in\nIndexMark is a simple yet effective match-then-replace method, which carefully\nselects watermark tokens from the codebook based on token similarity, and\npromotes the use of watermark tokens through token replacement, thereby\nembedding the watermark without affecting the image quality. Watermark\nverification is achieved by calculating the proportion of watermark tokens in\ngenerated images, with precision further improved by an Index Encoder.\nFurthermore, we introduce an auxiliary validation scheme to enhance robustness\nagainst cropping attacks. Experiments demonstrate that IndexMark achieves\nstate-of-the-art performance in terms of image quality and verification\naccuracy, and exhibits robustness against various perturbations, including\ncropping, noises, Gaussian blur, random erasing, color jittering, and JPEG\ncompression.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14673v1",
    "published_date": "2025-05-20 17:58:02 UTC",
    "updated_date": "2025-05-20 17:58:02 UTC"
  },
  {
    "arxiv_id": "2505.14668v1",
    "title": "ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory Perceptions",
    "authors": [
      "Bufang Yang",
      "Lilin Xu",
      "Liekang Zeng",
      "Kaiwei Liu",
      "Siyang Jiang",
      "Wenrui Lu",
      "Hongkai Chen",
      "Xiaofan Jiang",
      "Guoliang Xing",
      "Zhenyu Yan"
    ],
    "abstract": "Recent advances in Large Language Models (LLMs) have propelled intelligent\nagents from reactive responses to proactive support. While promising, existing\nproactive agents either rely exclusively on observations from enclosed\nenvironments (e.g., desktop UIs) with direct LLM inference or employ rule-based\nproactive notifications, leading to suboptimal user intent understanding and\nlimited functionality for proactive service. In this paper, we introduce\nContextAgent, the first context-aware proactive agent that incorporates\nextensive sensory contexts to enhance the proactive capabilities of LLM agents.\nContextAgent first extracts multi-dimensional contexts from massive sensory\nperceptions on wearables (e.g., video and audio) to understand user intentions.\nContextAgent then leverages the sensory contexts and the persona contexts from\nhistorical data to predict the necessity for proactive services. When proactive\nassistance is needed, ContextAgent further automatically calls the necessary\ntools to assist users unobtrusively. To evaluate this new task, we curate\nContextAgentBench, the first benchmark for evaluating context-aware proactive\nLLM agents, covering 1,000 samples across nine daily scenarios and twenty\ntools. Experiments on ContextAgentBench show that ContextAgent outperforms\nbaselines by achieving up to 8.5% and 6.0% higher accuracy in proactive\npredictions and tool calling, respectively. We hope our research can inspire\nthe development of more advanced, human-centric, proactive AI assistants.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14668v1",
    "published_date": "2025-05-20 17:55:25 UTC",
    "updated_date": "2025-05-20 17:55:25 UTC"
  },
  {
    "arxiv_id": "2505.14667v1",
    "title": "SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment",
    "authors": [
      "Wonje Jeung",
      "Sangyeon Yoon",
      "Minsuk Kahng",
      "Albert No"
    ],
    "abstract": "Large Reasoning Models (LRMs) have become powerful tools for complex problem\nsolving, but their structured reasoning pathways can lead to unsafe outputs\nwhen exposed to harmful prompts. Existing safety alignment methods reduce\nharmful outputs but can degrade reasoning depth, leading to significant\ntrade-offs in complex, multi-step tasks, and remain vulnerable to sophisticated\njailbreak attacks. To address this, we introduce SAFEPATH, a lightweight\nalignment method that fine-tunes LRMs to emit a short, 8-token Safety Primer at\nthe start of their reasoning, in response to harmful prompts, while leaving the\nrest of the reasoning process unsupervised. Empirical results across multiple\nbenchmarks indicate that SAFEPATH effectively reduces harmful outputs while\nmaintaining reasoning performance. Specifically, SAFEPATH reduces harmful\nresponses by up to 90.0% and blocks 83.3% of jailbreak attempts in the\nDeepSeek-R1-Distill-Llama-8B model, while requiring 295.9x less compute than\nDirect Refusal and 314.1x less than SafeChain. We further introduce a zero-shot\nvariant that requires no fine-tuning. In addition, we provide a comprehensive\nanalysis of how existing methods in LLMs generalize, or fail, when applied to\nreasoning-centric models, revealing critical gaps and new directions for safer\nAI.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "22 pages",
    "pdf_url": "http://arxiv.org/pdf/2505.14667v1",
    "published_date": "2025-05-20 17:54:54 UTC",
    "updated_date": "2025-05-20 17:54:54 UTC"
  },
  {
    "arxiv_id": "2505.14664v1",
    "title": "AKRMap: Adaptive Kernel Regression for Trustworthy Visualization of Cross-Modal Embeddings",
    "authors": [
      "Yilin Ye",
      "Junchao Huang",
      "Xingchen Zeng",
      "Jiazhi Xia",
      "Wei Zeng"
    ],
    "abstract": "Cross-modal embeddings form the foundation for multi-modal models. However,\nvisualization methods for interpreting cross-modal embeddings have been\nprimarily confined to traditional dimensionality reduction (DR) techniques like\nPCA and t-SNE. These DR methods primarily focus on feature distributions within\na single modality, whilst failing to incorporate metrics (e.g., CLIPScore)\nacross multiple modalities.This paper introduces AKRMap, a new DR technique\ndesigned to visualize cross-modal embeddings metric with enhanced accuracy by\nlearning kernel regression of the metric landscape in the projection space.\nSpecifically, AKRMap constructs a supervised projection network guided by a\npost-projection kernel regression loss, and employs adaptive generalized\nkernels that can be jointly optimized with the projection. This approach\nenables AKRMap to efficiently generate visualizations that capture complex\nmetric distributions, while also supporting interactive features such as zoom\nand overlay for deeper exploration. Quantitative experiments demonstrate that\nAKRMap outperforms existing DR methods in generating more accurate and\ntrustworthy visualizations. We further showcase the effectiveness of AKRMap in\nvisualizing and comparing cross-modal embeddings for text-to-image models. Code\nand demo are available at https://github.com/yilinye/AKRMap.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14664v1",
    "published_date": "2025-05-20 17:52:03 UTC",
    "updated_date": "2025-05-20 17:52:03 UTC"
  },
  {
    "arxiv_id": "2505.14661v1",
    "title": "Abacus: A Cost-Based Optimizer for Semantic Operator Systems",
    "authors": [
      "Matthew Russo",
      "Sivaprasad Sudhir",
      "Gerardo Vitagliano",
      "Chunwei Liu",
      "Tim Kraska",
      "Samuel Madden",
      "Michael Cafarella"
    ],
    "abstract": "LLMs enable an exciting new class of data processing applications over large\ncollections of unstructured documents. Several new programming frameworks have\nenabled developers to build these applications by composing them out of\nsemantic operators: a declarative set of AI-powered data transformations with\nnatural language specifications. These include LLM-powered maps, filters,\njoins, etc. used for document processing tasks such as information extraction,\nsummarization, and more. While systems of semantic operators have achieved\nstrong performance on benchmarks, they can be difficult to optimize. An\noptimizer for this setting must determine how to physically implement each\nsemantic operator in a way that optimizes the system globally. Existing\noptimizers are limited in the number of optimizations they can apply, and most\n(if not all) cannot optimize system quality, cost, or latency subject to\nconstraint(s) on the other dimensions. In this paper we present Abacus, an\nextensible, cost-based optimizer which searches for the best implementation of\na semantic operator system given a (possibly constrained) optimization\nobjective. Abacus estimates operator performance by leveraging a minimal set of\nvalidation examples and, if available, prior beliefs about operator\nperformance. We evaluate Abacus on document processing workloads in the\nbiomedical and legal domains (BioDEX; CUAD) and multi-modal question answering\n(MMQA). We demonstrate that systems optimized by Abacus achieve 18.7%-39.2%\nbetter quality and up to 23.6x lower cost and 4.2x lower latency than the next\nbest system.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "H.2.4; I.2.5"
    ],
    "primary_category": "cs.DB",
    "comment": "16 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.14661v1",
    "published_date": "2025-05-20 17:49:46 UTC",
    "updated_date": "2025-05-20 17:49:46 UTC"
  },
  {
    "arxiv_id": "2505.14766v1",
    "title": "This Time is Different: An Observability Perspective on Time Series Foundation Models",
    "authors": [
      "Ben Cohen",
      "Emaad Khwaja",
      "Youssef Doubli",
      "Salahidine Lemaachi",
      "Chris Lettieri",
      "Charles Masson",
      "Hugo Miccinilli",
      "Elise RamÃ©",
      "Qiqi Ren",
      "Afshin Rostamizadeh",
      "Jean Ogier du Terrail",
      "Anna-Monica Toon",
      "Kan Wang",
      "Stephan Xie",
      "David Asker",
      "Ameet Talwalkar",
      "Othmane Abou-Amal"
    ],
    "abstract": "We introduce Toto, a time series forecasting foundation model with 151\nmillion parameters. Toto uses a modern decoder-only architecture coupled with\narchitectural innovations designed to account for specific challenges found in\nmultivariate observability time series data. Toto's pre-training corpus is a\nmixture of observability data, open datasets, and synthetic data, and is\n4-10$\\times$ larger than those of leading time series foundation models.\nAdditionally, we introduce BOOM, a large-scale benchmark consisting of 350\nmillion observations across 2,807 real-world time series. For both Toto and\nBOOM, we source observability data exclusively from Datadog's own telemetry and\ninternal observability metrics. Extensive evaluations demonstrate that Toto\nachieves state-of-the-art performance on both BOOM and on established general\npurpose time series forecasting benchmarks. Toto's model weights, inference\ncode, and evaluation scripts, as well as BOOM's data and evaluation code, are\nall available as open source under the Apache 2.0 License available at\nhttps://huggingface.co/Datadog/Toto-Open-Base-1.0 and\nhttps://github.com/DataDog/toto.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14766v1",
    "published_date": "2025-05-20 17:48:13 UTC",
    "updated_date": "2025-05-20 17:48:13 UTC"
  },
  {
    "arxiv_id": "2505.14660v1",
    "title": "EmoGist: Efficient In-Context Learning for Visual Emotion Understanding",
    "authors": [
      "Ronald Seoh",
      "Dan Goldwasser"
    ],
    "abstract": "In this paper, we introduce EmoGist, a training-free, in-context learning\nmethod for performing visual emotion classification with LVLMs. The key\nintuition of our approach is that context-dependent definition of emotion\nlabels could allow more accurate predictions of emotions, as the ways in which\nemotions manifest within images are highly context dependent and nuanced.\nEmoGist pre-generates multiple explanations of emotion labels, by analyzing the\nclusters of example images belonging to each category. At test time, we\nretrieve a version of explanation based on embedding similarity, and feed it to\na fast VLM for classification. Through our experiments, we show that EmoGist\nallows up to 13 points improvement in micro F1 scores with the multi-label\nMemotion dataset, and up to 8 points in macro F1 in the multi-class FI dataset.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14660v1",
    "published_date": "2025-05-20 17:47:04 UTC",
    "updated_date": "2025-05-20 17:47:04 UTC"
  },
  {
    "arxiv_id": "2505.14659v1",
    "title": "Explainable AI for Securing Healthcare in IoT-Integrated 6G Wireless Networks",
    "authors": [
      "Navneet Kaur",
      "Lav Gupta"
    ],
    "abstract": "As healthcare systems increasingly adopt advanced wireless networks and\nconnected devices, securing medical applications has become critical. The\nintegration of Internet of Medical Things devices, such as robotic surgical\ntools, intensive care systems, and wearable monitors has enhanced patient care\nbut introduced serious security risks. Cyberattacks on these devices can lead\nto life threatening consequences, including surgical errors, equipment failure,\nand data breaches. While the ITU IMT 2030 vision highlights 6G's transformative\nrole in healthcare through AI and cloud integration, it also raises new\nsecurity concerns. This paper explores how explainable AI techniques like SHAP,\nLIME, and DiCE can uncover vulnerabilities, strengthen defenses, and improve\ntrust and transparency in 6G enabled healthcare. We support our approach with\nexperimental analysis and highlight promising results.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14659v1",
    "published_date": "2025-05-20 17:46:09 UTC",
    "updated_date": "2025-05-20 17:46:09 UTC"
  },
  {
    "arxiv_id": "2505.14656v1",
    "title": "Cost-Augmented Monte Carlo Tree Search for LLM-Assisted Planning",
    "authors": [
      "Zihao Zhang",
      "Fei Liu"
    ],
    "abstract": "While LLMs excel at open-ended reasoning, they often struggle with\ncost-sensitive planning, either treating all actions as having equal cost or\nfailing to stay within strict budgets. In this paper, we introduce\nCost-Augmented Monte Carlo Tree Search (CATS), a novel approach that brings\nexplicit cost-awareness into LLM-guided planning. Tight cost constraints push\nthe planner to quickly identify infeasible solutions, while looser constraints\nencourage optimization for minimal cost. We benchmark top LLMs such as GPT-4.1,\nClaude-3.7-Sonnet, and DeepSeek-R1, against our CATS planner to evaluate their\nperformance in cost-sensitive scenarios. Our experiments suggest that raw LLMs\nsuch as GPT-4.1 often falter under tight budgets, whereas CATS consistently\ndelivers strong performance, achieving higher task success rates and better\ncost efficiency. CATS provides an effective solution for budget-aware\ndecision-making by combining the reasoning power of LLMs with structured\nsearch.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14656v1",
    "published_date": "2025-05-20 17:43:33 UTC",
    "updated_date": "2025-05-20 17:43:33 UTC"
  },
  {
    "arxiv_id": "2505.14654v1",
    "title": "Beyond Words: Multimodal LLM Knows When to Speak",
    "authors": [
      "Zikai Liao",
      "Yi Ouyang",
      "Yi-Lun Lee",
      "Chen-Ping Yu",
      "Yi-Hsuan Tsai",
      "Zhaozheng Yin"
    ],
    "abstract": "While large language model (LLM)-based chatbots have demonstrated strong\ncapabilities in generating coherent and contextually relevant responses, they\noften struggle with understanding when to speak, particularly in delivering\nbrief, timely reactions during ongoing conversations. This limitation arises\nlargely from their reliance on text input, lacking the rich contextual cues in\nreal-world human dialogue. In this work, we focus on real-time prediction of\nresponse types, with an emphasis on short, reactive utterances that depend on\nsubtle, multimodal signals across vision, audio, and text. To support this, we\nintroduce a new multimodal dataset constructed from real-world conversational\nvideos, containing temporally aligned visual, auditory, and textual streams.\nThis dataset enables fine-grained modeling of response timing in dyadic\ninteractions. Building on this dataset, we propose MM-When2Speak, a multimodal\nLLM-based model that adaptively integrates visual, auditory, and textual\ncontext to predict when a response should occur, and what type of response is\nappropriate. Experiments show that MM-When2Speak significantly outperforms\nstate-of-the-art unimodal and LLM-based baselines, achieving up to a 4x\nimprovement in response timing accuracy over leading commercial LLMs. These\nresults underscore the importance of multimodal inputs for producing timely,\nnatural, and engaging conversational AI.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://github.com/lzk901372/MM-When2Speak",
    "pdf_url": "http://arxiv.org/pdf/2505.14654v1",
    "published_date": "2025-05-20 17:42:34 UTC",
    "updated_date": "2025-05-20 17:42:34 UTC"
  },
  {
    "arxiv_id": "2505.14765v1",
    "title": "Deep Learning-Based Forecasting of Boarding Patient Counts to Address ED Overcrowding",
    "authors": [
      "Orhun Vural",
      "Bunyamin Ozaydin",
      "Khalid Y. Aram",
      "James Booth",
      "Brittany F. Lindsey",
      "Abdulaziz Ahmed"
    ],
    "abstract": "This study develops deep learning models to forecast the number of patients\nin the emergency department (ED) boarding phase six hours in advance, aiming to\nsupport proactive operational decision-making using only non-clinical,\noperational, and contextual features. Data were collected from five sources: ED\ntracking systems, inpatient census records, weather reports, federal holiday\ncalendars, and local event schedules. After feature engineering, the data were\naggregated at an hourly level, cleaned, and merged into a unified dataset for\nmodel training. Several time series deep learning models, including ResNetPlus,\nTSTPlus, TSiTPlus (from the tsai library), and N-BEATSx, were trained using\nOptuna and grid search for hyperparameter tuning. The average ED boarding count\nwas 28.7, with a standard deviation of 11.2. N-BEATSx achieved the best\nperformance, with a mean absolute error of 2.10, mean squared error of 7.08,\nroot mean squared error of 2.66, and a coefficient of determination of 0.95.\nThe model maintained stable accuracy even during periods of extremely high\nboarding counts, defined as values exceeding one, two, or three standard\ndeviations above the mean. Results show that accurate six-hour-ahead forecasts\nare achievable without using patient-level clinical data. While strong\nperformance was observed even with a basic feature set, the inclusion of\nadditional features improved prediction stability under extreme conditions.\nThis framework offers a practical and generalizable approach for hospital\nsystems to anticipate boarding levels and help mitigate ED overcrowding.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "68T07",
      "I.2.6; J.3"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14765v1",
    "published_date": "2025-05-20 17:35:47 UTC",
    "updated_date": "2025-05-20 17:35:47 UTC"
  },
  {
    "arxiv_id": "2505.14646v1",
    "title": "CAD-Coder: An Open-Source Vision-Language Model for Computer-Aided Design Code Generation",
    "authors": [
      "Anna C. Doris",
      "Md Ferdous Alam",
      "Amin Heyrani Nobari",
      "Faez Ahmed"
    ],
    "abstract": "Efficient creation of accurate and editable 3D CAD models is critical in\nengineering design, significantly impacting cost and time-to-market in product\ninnovation. Current manual workflows remain highly time-consuming and demand\nextensive user expertise. While recent developments in AI-driven CAD generation\nshow promise, existing models are limited by incomplete representations of CAD\noperations, inability to generalize to real-world images, and low output\naccuracy. This paper introduces CAD-Coder, an open-source Vision-Language Model\n(VLM) explicitly fine-tuned to generate editable CAD code (CadQuery Python)\ndirectly from visual input. Leveraging a novel dataset that we\ncreated--GenCAD-Code, consisting of over 163k CAD-model image and code\npairs--CAD-Coder outperforms state-of-the-art VLM baselines such as GPT-4.5 and\nQwen2.5-VL-72B, achieving a 100% valid syntax rate and the highest accuracy in\n3D solid similarity. Notably, our VLM demonstrates some signs of\ngeneralizability, successfully generating CAD code from real-world images and\nexecuting CAD operations unseen during fine-tuning. The performance and\nadaptability of CAD-Coder highlights the potential of VLMs fine-tuned on code\nto streamline CAD workflows for engineers and designers. CAD-Coder is publicly\navailable at: https://github.com/anniedoris/CAD-Coder.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14646v1",
    "published_date": "2025-05-20 17:34:44 UTC",
    "updated_date": "2025-05-20 17:34:44 UTC"
  },
  {
    "arxiv_id": "2505.14633v1",
    "title": "Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas",
    "authors": [
      "Yu Ying Chiu",
      "Zhilin Wang",
      "Sharan Maiya",
      "Yejin Choi",
      "Kyle Fish",
      "Sydney Levine",
      "Evan Hubinger"
    ],
    "abstract": "Detecting AI risks becomes more challenging as stronger models emerge and\nfind novel methods such as Alignment Faking to circumvent these detection\nattempts. Inspired by how risky behaviors in humans (i.e., illegal activities\nthat may hurt others) are sometimes guided by strongly-held values, we believe\nthat identifying values within AI models can be an early warning system for\nAI's risky behaviors. We create LitmusValues, an evaluation pipeline to reveal\nAI models' priorities on a range of AI value classes. Then, we collect\nAIRiskDilemmas, a diverse collection of dilemmas that pit values against one\nanother in scenarios relevant to AI safety risks such as Power Seeking. By\nmeasuring an AI model's value prioritization using its aggregate choices, we\nobtain a self-consistent set of predicted value priorities that uncover\npotential risks. We show that values in LitmusValues (including seemingly\ninnocuous ones like Care) can predict for both seen risky behaviors in\nAIRiskDilemmas and unseen risky behaviors in HarmBench.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "34 pages, 11 figures, see associated data at\n  https://huggingface.co/datasets/kellycyy/AIRiskDilemmas and code at\n  https://github.com/kellycyy/LitmusValues",
    "pdf_url": "http://arxiv.org/pdf/2505.14633v1",
    "published_date": "2025-05-20 17:24:09 UTC",
    "updated_date": "2025-05-20 17:24:09 UTC"
  },
  {
    "arxiv_id": "2505.14629v1",
    "title": "KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large Language Models",
    "authors": [
      "Fnu Mohbat",
      "Mohammed J Zaki"
    ],
    "abstract": "Recent advances in large language models (LLMs) and the abundance of food\ndata have resulted in studies to improve food understanding using LLMs. Despite\nseveral recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there\nhas been limited research on integrating food related KGs with LLMs. We\nintroduce KERL, a unified system that leverages food KGs and LLMs to provide\npersonalized food recommendations and generates recipes with associated\nmicro-nutritional information. Given a natural language question, KERL extracts\nentities, retrieves subgraphs from the KG, which are then fed into the LLM as\ncontext to select the recipes that satisfy the constraints. Next, our system\ngenerates the cooking steps and nutritional information for each recipe. To\nevaluate our approach, we also develop a benchmark dataset by curating recipe\nrelated questions, combined with constraints and personal preferences. Through\nextensive experiments, we show that our proposed KG-augmented LLM significantly\noutperforms existing approaches, offering a complete and coherent solution for\nfood recommendation, recipe generation, and nutritional analysis. Our code and\nbenchmark datasets are publicly available at\nhttps://github.com/mohbattharani/KERL.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at ACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2505.14629v1",
    "published_date": "2025-05-20 17:19:57 UTC",
    "updated_date": "2025-05-20 17:19:57 UTC"
  },
  {
    "arxiv_id": "2505.14627v1",
    "title": "Debating for Better Reasoning: An Unsupervised Multimodal Approach",
    "authors": [
      "Ashutosh Adhikari",
      "Mirella Lapata"
    ],
    "abstract": "As Large Language Models (LLMs) gain expertise across diverse domains and\nmodalities, scalable oversight becomes increasingly challenging, particularly\nwhen their capabilities may surpass human evaluators. Debate has emerged as a\npromising mechanism for enabling such oversight. In this work, we extend the\ndebate paradigm to a multimodal setting, exploring its potential for weaker\nmodels to supervise and enhance the performance of stronger models. We focus on\nvisual question answering (VQA), where two \"sighted\" expert vision-language\nmodels debate an answer, while a \"blind\" (text-only) judge adjudicates based\nsolely on the quality of the arguments. In our framework, the experts defend\nonly answers aligned with their beliefs, thereby obviating the need for\nexplicit role-playing and concentrating the debate on instances of expert\ndisagreement. Experiments on several multimodal tasks demonstrate that the\ndebate framework consistently outperforms individual expert models. Moreover,\njudgments from weaker LLMs can help instill reasoning capabilities in\nvision-language models through finetuning.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14627v1",
    "published_date": "2025-05-20 17:18:17 UTC",
    "updated_date": "2025-05-20 17:18:17 UTC"
  },
  {
    "arxiv_id": "2505.14625v2",
    "title": "TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning",
    "authors": [
      "Zhangchen Xu",
      "Yuetai Li",
      "Fengqing Jiang",
      "Bhaskar Ramasubramanian",
      "Luyao Niu",
      "Bill Yuchen Lin",
      "Radha Poovendran"
    ],
    "abstract": "Reinforcement Learning (RL) has become a powerful tool for enhancing the\nreasoning abilities of large language models (LLMs) by optimizing their\npolicies with reward signals. Yet, RL's success relies on the reliability of\nrewards, which are provided by verifiers. In this paper, we expose and analyze\na widespread problem--false negatives--where verifiers wrongly reject correct\nmodel outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals\nthat over 38% of model-generated responses suffer from false negatives, where\nthe verifier fails to recognize correct answers. We show, both empirically and\ntheoretically, that these false negatives severely impair RL training by\ndepriving the model of informative gradient signals and slowing convergence. To\nmitigate this, we propose tinyV, a lightweight LLM-based verifier that augments\nexisting rule-based methods, which dynamically identifies potential false\nnegatives and recovers valid responses to produce more accurate reward\nestimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts\npass rates by up to 10% and accelerates convergence relative to the baseline.\nOur findings highlight the critical importance of addressing verifier false\nnegatives and offer a practical approach to improve RL-based fine-tuning of\nLLMs. Our code is available at https://github.com/uw-nsl/TinyV.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14625v2",
    "published_date": "2025-05-20 17:16:44 UTC",
    "updated_date": "2025-05-22 17:49:50 UTC"
  },
  {
    "arxiv_id": "2505.14615v1",
    "title": "SATBench: Benchmarking LLMs' Logical Reasoning via Automated Puzzle Generation from SAT Formulas",
    "authors": [
      "Anjiang Wei",
      "Yuheng Wu",
      "Yingjia Wan",
      "Tarun Suresh",
      "Huanmi Tan",
      "Zhanke Zhou",
      "Sanmi Koyejo",
      "Ke Wang",
      "Alex Aiken"
    ],
    "abstract": "We introduce SATBench, a benchmark for evaluating the logical reasoning\ncapabilities of large language models (LLMs) through logical puzzles derived\nfrom Boolean satisfiability (SAT) problems. Unlike prior work that focuses on\ninference rule-based reasoning, which often involves deducing conclusions from\na set of premises, our approach leverages the search-based nature of SAT\nproblems, where the objective is to find a solution that fulfills a specified\nset of logical constraints. Each instance in SATBench is generated from a SAT\nformula, then translated into a story context and conditions using LLMs. The\ngeneration process is fully automated and allows for adjustable difficulty by\nvarying the number of clauses. All 2100 puzzles are validated through both\nLLM-assisted and solver-based consistency checks, with human validation on a\nsubset. Experimental results show that even the strongest model, o4-mini,\nachieves only 65.0% accuracy on hard UNSAT problems, close to the random\nbaseline of 50%. SATBench exposes fundamental limitations in the search-based\nlogical reasoning abilities of current LLMs and provides a scalable testbed for\nfuture research in logical reasoning.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14615v1",
    "published_date": "2025-05-20 17:00:22 UTC",
    "updated_date": "2025-05-20 17:00:22 UTC"
  },
  {
    "arxiv_id": "2505.14608v1",
    "title": "Language Models Optimized to Fool Detectors Still Have a Distinct Style (And How to Change It)",
    "authors": [
      "Rafael Rivera Soto",
      "Barry Chen",
      "Nicholas Andrews"
    ],
    "abstract": "Despite considerable progress in the development of machine-text detectors,\nit has been suggested that the problem is inherently hard, and therefore, that\nstakeholders should proceed under the assumption that machine-generated text\ncannot be reliably detected as such. We examine a recent such claim by Nicks et\nal. (2024) regarding the ease with which language models can be optimized to\ndegrade the performance of machine-text detectors, including detectors not\nspecifically optimized against. We identify a feature space$\\unicode{x2013}$the\nstylistic feature space$\\unicode{x2013}$that is robust to such optimization,\nand show that it may be used to reliably detect samples from language models\noptimized to prevent detection. Furthermore, we show that even when models are\nexplicitly optimized against stylistic detectors, detection performance remains\nsurprisingly unaffected. We then seek to understand if stylistic detectors are\ninherently more robust. To study this question, we explore a new paraphrasing\napproach that simultaneously aims to close the gap between human writing and\nmachine writing in stylistic feature space while avoiding detection using\ntraditional features. We show that when only a single sample is available for\ndetection, this attack is universally effective across all detectors\nconsidered, including those that use writing style. However, as the number of\nsamples available for detection grows, the human and machine distributions\nbecome distinguishable. This observation encourages us to introduce AURA, a\nmetric that estimates the overlap between human and machine-generated\ndistributions by analyzing how detector performance improves as more samples\nbecome available. Overall, our findings underscore previous recommendations to\navoid reliance on machine-text detection.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14608v1",
    "published_date": "2025-05-20 16:55:44 UTC",
    "updated_date": "2025-05-20 16:55:44 UTC"
  },
  {
    "arxiv_id": "2505.14604v2",
    "title": "Let LLMs Break Free from Overthinking via Self-Braking Tuning",
    "authors": [
      "Haoran Zhao",
      "Yuchen Yan",
      "Yongliang Shen",
      "Haolei Xu",
      "Wenqi Zhang",
      "Kaitao Song",
      "Jian Shao",
      "Weiming Lu",
      "Jun Xiao",
      "Yueting Zhuang"
    ],
    "abstract": "Large reasoning models (LRMs), such as OpenAI o1 and DeepSeek-R1, have\nsignificantly enhanced their reasoning capabilities by generating longer chains\nof thought, demonstrating outstanding performance across a variety of tasks.\nHowever, this performance gain comes at the cost of a substantial increase in\nredundant reasoning during the generation process, leading to high\ncomputational overhead and exacerbating the issue of overthinking. Although\nnumerous existing approaches aim to address the problem of overthinking, they\noften rely on external interventions. In this paper, we propose a novel\nframework, Self-Braking Tuning (SBT), which tackles overthinking from the\nperspective of allowing the model to regulate its own reasoning process, thus\neliminating the reliance on external control mechanisms. We construct a set of\noverthinking identification metrics based on standard answers and design a\nsystematic method to detect redundant reasoning. This method accurately\nidentifies unnecessary steps within the reasoning trajectory and generates\ntraining signals for learning self-regulation behaviors. Building on this\nfoundation, we develop a complete strategy for constructing data with adaptive\nreasoning lengths and introduce an innovative braking prompt mechanism that\nenables the model to naturally learn when to terminate reasoning at an\nappropriate point. Experiments across mathematical benchmarks (AIME, AMC,\nMATH500, GSM8K) demonstrate that our method reduces token consumption by up to\n60% while maintaining comparable accuracy to unconstrained models.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Github:https://github.com/ZJU-REAL/Self-Braking-Tuning Project Page:\n  https://ZJU-REAL.github.io/SBT",
    "pdf_url": "http://arxiv.org/pdf/2505.14604v2",
    "published_date": "2025-05-20 16:53:40 UTC",
    "updated_date": "2025-05-21 16:45:44 UTC"
  },
  {
    "arxiv_id": "2505.14603v1",
    "title": "Towards a Foundation Model for Communication Systems",
    "authors": [
      "Davide Buffelli",
      "Sowmen Das",
      "Yu-Wei Lin",
      "Sattar Vakili",
      "Chien-Yi Wang",
      "Masoud Attarifar",
      "Pritthijit Nath",
      "Da-shan Shiu"
    ],
    "abstract": "Artificial Intelligence (AI) has demonstrated unprecedented performance\nacross various domains, and its application to communication systems is an\nactive area of research. While current methods focus on task-specific\nsolutions, the broader trend in AI is shifting toward large general models\ncapable of supporting multiple applications. In this work, we take a step\ntoward a foundation model for communication data--a transformer-based,\nmulti-modal model designed to operate directly on communication data. We\npropose methodologies to address key challenges, including tokenization,\npositional embedding, multimodality, variable feature sizes, and normalization.\nFurthermore, we empirically demonstrate that such a model can successfully\nestimate multiple features, including transmission rank, selected precoder,\nDoppler spread, and delay profile.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "eess.SP"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14603v1",
    "published_date": "2025-05-20 16:52:11 UTC",
    "updated_date": "2025-05-20 16:52:11 UTC"
  },
  {
    "arxiv_id": "2505.14599v1",
    "title": "Toward Reliable Biomedical Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models",
    "authors": [
      "Guangzhi Xiong",
      "Eric Xie",
      "Corey Williams",
      "Myles Kim",
      "Amir Hassan Shariatmadari",
      "Sikun Guo",
      "Stefan Bekiranov",
      "Aidong Zhang"
    ],
    "abstract": "Large language models (LLMs) have shown significant potential in scientific\ndisciplines such as biomedicine, particularly in hypothesis generation, where\nthey can analyze vast literature, identify patterns, and suggest research\ndirections. However, a key challenge lies in evaluating the truthfulness of\ngenerated hypotheses, as verifying their accuracy often requires substantial\ntime and resources. Additionally, the hallucination problem in LLMs can lead to\nthe generation of hypotheses that appear plausible but are ultimately\nincorrect, undermining their reliability. To facilitate the systematic study of\nthese challenges, we introduce TruthHypo, a benchmark for assessing the\ncapabilities of LLMs in generating truthful biomedical hypotheses, and KnowHD,\na knowledge-based hallucination detector to evaluate how well hypotheses are\ngrounded in existing knowledge. Our results show that LLMs struggle to generate\ntruthful hypotheses. By analyzing hallucinations in reasoning steps, we\ndemonstrate that the groundedness scores provided by KnowHD serve as an\neffective metric for filtering truthful hypotheses from the diverse outputs of\nLLMs. Human evaluations further validate the utility of KnowHD in identifying\ntruthful hypotheses and accelerating scientific discovery. Our data and source\ncode are available at https://github.com/Teddy-XiongGZ/TruthHypo.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to IJCAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2505.14599v1",
    "published_date": "2025-05-20 16:49:40 UTC",
    "updated_date": "2025-05-20 16:49:40 UTC"
  },
  {
    "arxiv_id": "2505.14569v1",
    "title": "Agent Context Protocols Enhance Collective Inference",
    "authors": [
      "Devansh Bhardwaj",
      "Arjun Beniwal",
      "Shreyas Chaudhari",
      "Ashwin Kalyan",
      "Tanmay Rajpurohit",
      "Karthik R. Narasimhan",
      "Ameet Deshpande",
      "Vishvak Murahari"
    ],
    "abstract": "AI agents have become increasingly adept at complex tasks such as coding,\nreasoning, and multimodal understanding. However, building generalist systems\nrequires moving beyond individual agents to collective inference -- a paradigm\nwhere multi-agent systems with diverse, task-specialized agents complement one\nanother through structured communication and collaboration. Today, coordination\nis usually handled with imprecise, ad-hoc natural language, which limits\ncomplex interaction and hinders interoperability with domain-specific agents.\nWe introduce Agent context protocols (ACPs): a domain- and agent-agnostic\nfamily of structured protocols for agent-agent communication, coordination, and\nerror handling. ACPs combine (i) persistent execution blueprints -- explicit\ndependency graphs that store intermediate agent outputs -- with (ii)\nstandardized message schemas, enabling robust and fault-tolerant multi-agent\ncollective inference. ACP-powered generalist systems reach state-of-the-art\nperformance: 28.3 % accuracy on AssistantBench for long-horizon web assistance\nand best-in-class multimodal technical reports, outperforming commercial AI\nsystems in human evaluation. ACPs are highly modular and extensible, allowing\npractitioners to build top-tier generalist agents quickly.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14569v1",
    "published_date": "2025-05-20 16:28:08 UTC",
    "updated_date": "2025-05-20 16:28:08 UTC"
  },
  {
    "arxiv_id": "2505.14758v1",
    "title": "Kaleidoscope Gallery: Exploring Ethics and Generative AI Through Art",
    "authors": [
      "Alayt Issak",
      "Uttkarsh Narayan",
      "Ramya Srinivasan",
      "Erica Kleinman",
      "Casper Harteveld"
    ],
    "abstract": "Ethical theories and Generative AI (GenAI) models are dynamic concepts\nsubject to continuous evolution. This paper investigates the visualization of\nethics through a subset of GenAI models. We expand on the emerging field of\nVisual Ethics, using art as a form of critical inquiry and the metaphor of a\nkaleidoscope to invoke moral imagination. Through formative interviews with 10\nethics experts, we first establish a foundation of ethical theories. Our\nanalysis reveals five families of ethical theories, which we then transform\ninto images using the text-to-image (T2I) GenAI model. The resulting imagery,\ncurated as Kaleidoscope Gallery and evaluated by the same experts, revealed\neight themes that highlight how morality, society, and learned associations are\ncentral to ethical theories. We discuss implications for critically examining\nT2I models and present cautions and considerations. This work contributes to\nexamining ethical theories as foundational knowledge that interrogates GenAI\nmodels as socio-technical systems.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14758v1",
    "published_date": "2025-05-20 16:28:00 UTC",
    "updated_date": "2025-05-20 16:28:00 UTC"
  },
  {
    "arxiv_id": "2505.14566v1",
    "title": "KIPPO: Koopman-Inspired Proximal Policy Optimization",
    "authors": [
      "Andrei Cozma",
      "Landon Harris",
      "Hairong Qi"
    ],
    "abstract": "Reinforcement Learning (RL) has made significant strides in various domains,\nand policy gradient methods like Proximal Policy Optimization (PPO) have gained\npopularity due to their balance in performance, training stability, and\ncomputational efficiency. These methods directly optimize policies through\ngradient-based updates. However, developing effective control policies for\nenvironments with complex and non-linear dynamics remains a challenge. High\nvariance in gradient estimates and non-convex optimization landscapes often\nlead to unstable learning trajectories. Koopman Operator Theory has emerged as\na powerful framework for studying non-linear systems through an\ninfinite-dimensional linear operator that acts on a higher-dimensional space of\nmeasurement functions. In contrast with their non-linear counterparts, linear\nsystems are simpler, more predictable, and easier to analyze. In this paper, we\npresent Koopman-Inspired Proximal Policy Optimization (KIPPO), which learns an\napproximately linear latent-space representation of the underlying system's\ndynamics while retaining essential features for effective policy learning. This\nis achieved through a Koopman-approximation auxiliary network that can be added\nto the baseline policy optimization algorithms without altering the\narchitecture of the core policy or value function. Extensive experimental\nresults demonstrate consistent improvements over the PPO baseline with 6-60%\nincreased performance while reducing variability by up to 91% when evaluated on\nvarious continuous control tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted for IJCAI 2025. This arXiv submission is the full version of\n  the conference paper, including the appendix and supplementary material\n  omitted from the IJCAI proceedings",
    "pdf_url": "http://arxiv.org/pdf/2505.14566v1",
    "published_date": "2025-05-20 16:25:41 UTC",
    "updated_date": "2025-05-20 16:25:41 UTC"
  },
  {
    "arxiv_id": "2505.14564v1",
    "title": "Bellman operator convergence enhancements in reinforcement learning algorithms",
    "authors": [
      "David Krame Kadurha",
      "Domini Jocema Leko Moutouo",
      "Yae Ulrich Gaba"
    ],
    "abstract": "This paper reviews the topological groundwork for the study of reinforcement\nlearning (RL) by focusing on the structure of state, action, and policy spaces.\nWe begin by recalling key mathematical concepts such as complete metric spaces,\nwhich form the foundation for expressing RL problems. By leveraging the Banach\ncontraction principle, we illustrate how the Banach fixed-point theorem\nexplains the convergence of RL algorithms and how Bellman operators, expressed\nas operators on Banach spaces, ensure this convergence. The work serves as a\nbridge between theoretical mathematics and practical algorithm design, offering\nnew approaches to enhance the efficiency of RL. In particular, we investigate\nalternative formulations of Bellman operators and demonstrate their impact on\nimproving convergence rates and performance in standard RL environments such as\nMountainCar, CartPole, and Acrobot. Our findings highlight how a deeper\nmathematical understanding of RL can lead to more effective algorithms for\ndecision-making problems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14564v1",
    "published_date": "2025-05-20 16:24:42 UTC",
    "updated_date": "2025-05-20 16:24:42 UTC"
  },
  {
    "arxiv_id": "2505.14561v1",
    "title": "SSPS: Self-Supervised Positive Sampling for Robust Self-Supervised Speaker Verification",
    "authors": [
      "Theo Lepage",
      "Reda Dehak"
    ],
    "abstract": "Self-Supervised Learning (SSL) has led to considerable progress in Speaker\nVerification (SV). The standard framework uses same-utterance positive sampling\nand data-augmentation to generate anchor-positive pairs of the same speaker.\nThis is a major limitation, as this strategy primarily encodes channel\ninformation from the recording condition, shared by the anchor and positive. We\npropose a new positive sampling technique to address this bottleneck:\nSelf-Supervised Positive Sampling (SSPS). For a given anchor, SSPS aims to find\nan appropriate positive, i.e., of the same speaker identity but a different\nrecording condition, in the latent space using clustering assignments and a\nmemory queue of positive embeddings. SSPS improves SV performance for both\nSimCLR and DINO, reaching 2.57% and 2.53% EER, outperforming SOTA SSL methods\non VoxCeleb1-O. In particular, SimCLR-SSPS achieves a 58% EER reduction by\nlowering intra-speaker variance, providing comparable performance to DINO-SSPS.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.LG",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "accepted at Interspeech 2025",
    "pdf_url": "http://arxiv.org/pdf/2505.14561v1",
    "published_date": "2025-05-20 16:19:34 UTC",
    "updated_date": "2025-05-20 16:19:34 UTC"
  },
  {
    "arxiv_id": "2505.14757v1",
    "title": "Bridge2AI: Building A Cross-disciplinary Curriculum Towards AI-Enhanced Biomedical and Clinical Care",
    "authors": [
      "John Rincon",
      "Alexander R. Pelletier",
      "Destiny Gilliland",
      "Wei Wang",
      "Ding Wang",
      "Baradwaj S. Sankar",
      "Lori Scott-Sheldon",
      "Samson Gebreab",
      "William Hersh",
      "Parisa Rashidi",
      "Sally Baxter",
      "Wade Schulz",
      "Trey Ideker",
      "Yael Bensoussan",
      "Paul C. Boutros",
      "Alex A. T. Bui",
      "Colin Walsh",
      "Karol E. Watson",
      "Peipei Ping"
    ],
    "abstract": "Objective: As AI becomes increasingly central to healthcare, there is a\npressing need for bioinformatics and biomedical training systems that are\npersonalized and adaptable. Materials and Methods: The NIH Bridge2AI Training,\nRecruitment, and Mentoring (TRM) Working Group developed a cross-disciplinary\ncurriculum grounded in collaborative innovation, ethical data stewardship, and\nprofessional development within an adapted Learning Health System (LHS)\nframework. Results: The curriculum integrates foundational AI modules,\nreal-world projects, and a structured mentee-mentor network spanning Bridge2AI\nGrand Challenges and the Bridge Center. Guided by six learner personas, the\nprogram tailors educational pathways to individual needs while supporting\nscalability. Discussion: Iterative refinement driven by continuous feedback\nensures that content remains responsive to learner progress and emerging\ntrends. Conclusion: With over 30 scholars and 100 mentors engaged across North\nAmerica, the TRM model demonstrates how adaptive, persona-informed training can\nbuild interdisciplinary competencies and foster an integrative, ethically\ngrounded AI education in biomedical contexts.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14757v1",
    "published_date": "2025-05-20 16:19:05 UTC",
    "updated_date": "2025-05-20 16:19:05 UTC"
  },
  {
    "arxiv_id": "2505.14555v1",
    "title": "Physics-Guided Learning of Meteorological Dynamics for Weather Downscaling and Forecasting",
    "authors": [
      "Yingtao Luo",
      "Shikai Fang",
      "Binqing Wu",
      "Qingsong Wen",
      "Liang Sun"
    ],
    "abstract": "Weather forecasting is essential but remains computationally intensive and\nphysically incomplete in traditional numerical weather prediction (NWP)\nmethods. Deep learning (DL) models offer efficiency and accuracy but often\nignore physical laws, limiting interpretability and generalization. We propose\nPhyDL-NWP, a physics-guided deep learning framework that integrates physical\nequations with latent force parameterization into data-driven models. It\npredicts weather variables from arbitrary spatiotemporal coordinates, computes\nphysical terms via automatic differentiation, and uses a physics-informed loss\nto align predictions with governing dynamics. PhyDL-NWP enables resolution-free\ndownscaling by modeling weather as a continuous function and fine-tunes\npre-trained models with minimal overhead, achieving up to 170x faster inference\nwith only 55K parameters. Experiments show that PhyDL-NWP improves both\nforecasting performance and physical consistency.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published/Accepted in KDD 2025 (February Cycle)",
    "pdf_url": "http://arxiv.org/pdf/2505.14555v1",
    "published_date": "2025-05-20 16:13:20 UTC",
    "updated_date": "2025-05-20 16:13:20 UTC"
  },
  {
    "arxiv_id": "2505.14552v2",
    "title": "KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation",
    "authors": [
      "Jiajun Shi",
      "Jian Yang",
      "Jiaheng Liu",
      "Xingyuan Bu",
      "Jiangjie Chen",
      "Junting Zhou",
      "Kaijing Ma",
      "Zhoufutu Wen",
      "Bingli Wang",
      "Yancheng He",
      "Liang Song",
      "Hualei Zhu",
      "Shilong Li",
      "Xingjian Wang",
      "Wei Zhang",
      "Ruibin Yuan",
      "Yifan Yao",
      "Wenjun Yang",
      "Yunli Wang",
      "Siyuan Fang",
      "Siyu Yuan",
      "Qianyu He",
      "Xiangru Tang",
      "Yingshui Tan",
      "Wangchunshu Zhou",
      "Zhaoxiang Zhang",
      "Zhoujun Li",
      "Wenhao Huang",
      "Ge Zhang"
    ],
    "abstract": "Recent advancements in large language models (LLMs) underscore the need for\nmore comprehensive evaluation methods to accurately assess their reasoning\ncapabilities. Existing benchmarks are often domain-specific and thus cannot\nfully capture an LLM's general reasoning potential. To address this limitation,\nwe introduce the Knowledge Orthogonal Reasoning Gymnasium (KORGym), a dynamic\nevaluation platform inspired by KOR-Bench and Gymnasium. KORGym offers over\nfifty games in either textual or visual formats and supports interactive,\nmulti-turn assessments with reinforcement learning scenarios. Using KORGym, we\nconduct extensive experiments on 19 LLMs and 8 VLMs, revealing consistent\nreasoning patterns within model families and demonstrating the superior\nperformance of closed-source models. Further analysis examines the effects of\nmodality, reasoning strategies, reinforcement learning techniques, and response\nlength on model performance. We expect KORGym to become a valuable resource for\nadvancing LLM reasoning research and developing evaluation methodologies suited\nto complex, interactive environments.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "22 pages",
    "pdf_url": "http://arxiv.org/pdf/2505.14552v2",
    "published_date": "2025-05-20 16:06:32 UTC",
    "updated_date": "2025-05-21 07:43:57 UTC"
  },
  {
    "arxiv_id": "2505.14551v1",
    "title": "Trustworthy Reputation Games and Applications to Proof-of-Reputation Blockchains",
    "authors": [
      "Petros Drineas",
      "Rohit Nema",
      "Rafail Ostrovsky",
      "Vassilis Zikas"
    ],
    "abstract": "Reputation systems play an essential role in the Internet era, as they enable\npeople to decide whom to trust, by collecting and aggregating data about users'\nbehavior. Recently, several works proposed the use of reputation for the design\nand scalability improvement of decentralized (blockchain) ledgers; however,\nsuch systems are prone to manipulation and to our knowledge no game-theoretic\ntreatment exists that can support their economic robustness.\n  In this work we put forth a new model for the design of what we call, {\\em\ntrustworthy reputation systems}. Concretely, we describe a class of games,\nwhich we term {\\em trustworthy reputation games}, that enable a set of users to\nreport a function of their beliefs about the trustworthiness of each server in\na set -- i.e., their estimate of the probability that this server will behave\naccording to its specified strategy -- in a way that satisfies the following\nproperties:\n  1. It is $(\\epsilon$-)best response for any rational user in the game to play\na prescribed (truthful) strategy according to their true belief.\n  2. Assuming that the users' beliefs are not too far from the {\\em true}\ntrustworthiness of the servers, playing the above ($\\epsilon-$)Nash equilibrium\nallows anyone who observes the users' strategies to estimate the relative\ntrustworthiness of any two servers.\n  Our utilities and decoding function build on a connection between the well\nknown PageRank algorithm and the problem of trustworthiness discovery, which\ncan be of independent interest. Finally, we show how the above games are\nmotivated by and can be leveraged in proof-of-reputation (PoR) blockchains.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.GT",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14551v1",
    "published_date": "2025-05-20 16:06:25 UTC",
    "updated_date": "2025-05-20 16:06:25 UTC"
  },
  {
    "arxiv_id": "2505.14549v1",
    "title": "Can Large Language Models Really Recognize Your Name?",
    "authors": [
      "Dzung Pham",
      "Peter Kairouz",
      "Niloofar Mireshghallah",
      "Eugene Bagdasarian",
      "Chau Minh Pham",
      "Amir Houmansadr"
    ],
    "abstract": "Large language models (LLMs) are increasingly being used to protect sensitive\nuser data. However, current LLM-based privacy solutions assume that these\nmodels can reliably detect personally identifiable information (PII),\nparticularly named entities. In this paper, we challenge that assumption by\nrevealing systematic failures in LLM-based privacy tasks. Specifically, we show\nthat modern LLMs regularly overlook human names even in short text snippets due\nto ambiguous contexts, which cause the names to be misinterpreted or\nmishandled. We propose AMBENCH, a benchmark dataset of seemingly ambiguous\nhuman names, leveraging the name regularity bias phenomenon, embedded within\nconcise text snippets along with benign prompt injections. Our experiments on\nmodern LLMs tasked to detect PII as well as specialized tools show that recall\nof ambiguous names drops by 20--40% compared to more recognizable names.\nFurthermore, ambiguous human names are four times more likely to be ignored in\nsupposedly privacy-preserving summaries generated by LLMs when benign prompt\ninjections are present. These findings highlight the underexplored risks of\nrelying solely on LLMs to safeguard user privacy and underscore the need for a\nmore systematic investigation into their privacy failure modes.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14549v1",
    "published_date": "2025-05-20 16:05:05 UTC",
    "updated_date": "2025-05-20 16:05:05 UTC"
  },
  {
    "arxiv_id": "2505.14544v1",
    "title": "Multi-agent Reinforcement Learning vs. Fixed-Time Control for Traffic Signal Optimization: A Simulation Study",
    "authors": [
      "Saahil Mahato"
    ],
    "abstract": "Urban traffic congestion, particularly at intersections, significantly\nimpacts travel time, fuel consumption, and emissions. Traditional fixed-time\nsignal control systems often lack the adaptability to manage dynamic traffic\npatterns effectively. This study explores the application of multi-agent\nreinforcement learning (MARL) to optimize traffic signal coordination across\nmultiple intersections within a simulated environment. Utilizing Pygame, a\nsimulation was developed to model a network of interconnected intersections\nwith randomly generated vehicle flows to reflect realistic traffic variability.\nA decentralized MARL controller was implemented, in which each traffic signal\noperates as an autonomous agent, making decisions based on local observations\nand information from neighboring agents. Performance was evaluated against a\nbaseline fixed-time controller using metrics such as average vehicle wait time\nand overall throughput. The MARL approach demonstrated statistically\nsignificant improvements, including reduced average waiting times and improved\nthroughput. These findings suggest that MARL-based dynamic control strategies\nhold substantial promise for improving urban traffic management efficiency.\nMore research is recommended to address scalability and real-world\nimplementation challenges.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14544v1",
    "published_date": "2025-05-20 15:59:44 UTC",
    "updated_date": "2025-05-20 15:59:44 UTC"
  },
  {
    "arxiv_id": "2505.14539v1",
    "title": "A Logic of General Attention Using Edge-Conditioned Event Models (Extended Version)",
    "authors": [
      "Gaia Belardinelli",
      "Thomas Bolander",
      "Sebastian Watzl"
    ],
    "abstract": "In this work, we present the first general logic of attention. Attention is a\npowerful cognitive ability that allows agents to focus on potentially complex\ninformation, such as logically structured propositions, higher-order beliefs,\nor what other agents pay attention to. This ability is a strength, as it helps\nto ignore what is irrelevant, but it can also introduce biases when some types\nof information or agents are systematically ignored. Existing dynamic epistemic\nlogics for attention cannot model such complex attention scenarios, as they\nonly model attention to atomic formulas. Additionally, such logics quickly\nbecome cumbersome, as their size grows exponentially in the number of agents\nand announced literals. Here, we introduce a logic that overcomes both\nlimitations. First, we generalize edge-conditioned event models, which we show\nto be as expressive as standard event models yet exponentially more succinct\n(generalizing both standard event models and generalized arrow updates).\nSecond, we extend attention to arbitrary formulas, allowing agents to also\nattend to other agents' beliefs or attention. Our work treats attention as a\nmodality, like belief or awareness. We introduce attention principles that\nimpose closure properties on that modality and that can be used in its\naxiomatization. Throughout, we illustrate our framework with examples of AI\nagents reasoning about human attentional biases, demonstrating how such agents\ncan discover attentional biases.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14539v1",
    "published_date": "2025-05-20 15:56:34 UTC",
    "updated_date": "2025-05-20 15:56:34 UTC"
  },
  {
    "arxiv_id": "2505.14756v1",
    "title": "$\\texttt{LLINBO}$: Trustworthy LLM-in-the-Loop Bayesian Optimization",
    "authors": [
      "Chih-Yu Chang",
      "Milad Azvar",
      "Chinedum Okwudire",
      "Raed Al Kontar"
    ],
    "abstract": "Bayesian optimization (BO) is a sequential decision-making tool widely used\nfor optimizing expensive black-box functions. Recently, Large Language Models\n(LLMs) have shown remarkable adaptability in low-data regimes, making them\npromising tools for black-box optimization by leveraging contextual knowledge\nto propose high-quality query points. However, relying solely on LLMs as\noptimization agents introduces risks due to their lack of explicit surrogate\nmodeling and calibrated uncertainty, as well as their inherently opaque\ninternal mechanisms. This structural opacity makes it difficult to characterize\nor control the exploration-exploitation trade-off, ultimately undermining\ntheoretical tractability and reliability. To address this, we propose LLINBO:\nLLM-in-the-Loop BO, a hybrid framework for BO that combines LLMs with\nstatistical surrogate experts (e.g., Gaussian Processes (GP)). The core\nphilosophy is to leverage contextual reasoning strengths of LLMs for early\nexploration, while relying on principled statistical models to guide efficient\nexploitation. Specifically, we introduce three mechanisms that enable this\ncollaboration and establish their theoretical guarantees. We end the paper with\na real-life proof-of-concept in the context of 3D printing. The code to\nreproduce the results can be found at\nhttps://github.com/UMDataScienceLab/LLM-in-the-Loop-BO.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14756v1",
    "published_date": "2025-05-20 15:54:48 UTC",
    "updated_date": "2025-05-20 15:54:48 UTC"
  },
  {
    "arxiv_id": "2505.14533v1",
    "title": "Energy-Efficient Deep Reinforcement Learning with Spiking Transformers",
    "authors": [
      "Mohammad Irfan Uddin",
      "Nishad Tasnim",
      "Md Omor Faruk",
      "Zejian Zhou"
    ],
    "abstract": "Agent-based Transformers have been widely adopted in recent reinforcement\nlearning advances due to their demonstrated ability to solve complex tasks.\nHowever, the high computational complexity of Transformers often results in\nsignificant energy consumption, limiting their deployment in real-world\nautonomous systems. Spiking neural networks (SNNs), with their biologically\ninspired structure, offer an energy-efficient alternative for machine learning.\nIn this paper, a novel Spike-Transformer Reinforcement Learning (STRL)\nalgorithm that combines the energy efficiency of SNNs with the powerful\ndecision-making capabilities of reinforcement learning is developed.\nSpecifically, an SNN using multi-step Leaky Integrate-and-Fire (LIF) neurons\nand attention mechanisms capable of processing spatio-temporal patterns over\nmultiple time steps is designed. The architecture is further enhanced with\nstate, action, and reward encodings to create a Transformer-like structure\noptimized for reinforcement learning tasks. Comprehensive numerical experiments\nconducted on state-of-the-art benchmarks demonstrate that the proposed SNN\nTransformer achieves significantly improved policy performance compared to\nconventional agent-based Transformers. With both enhanced energy efficiency and\npolicy optimality, this work highlights a promising direction for deploying\nbio-inspired, low-cost machine learning models in complex real-world\ndecision-making scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14533v1",
    "published_date": "2025-05-20 15:52:43 UTC",
    "updated_date": "2025-05-20 15:52:43 UTC"
  },
  {
    "arxiv_id": "2505.14526v1",
    "title": "NavBench: A Unified Robotics Benchmark for Reinforcement Learning-Based Autonomous Navigation",
    "authors": [
      "Matteo El-Hariry",
      "Antoine Richard",
      "Ricard M. Castan",
      "Luis F. W. Batista",
      "Matthieu Geist",
      "Cedric Pradalier",
      "Miguel Olivares-Mendez"
    ],
    "abstract": "Autonomous robots must navigate and operate in diverse environments, from\nterrestrial and aquatic settings to aerial and space domains. While\nReinforcement Learning (RL) has shown promise in training policies for specific\nautonomous robots, existing benchmarks are often constrained to unique\nplatforms, limiting generalization and fair comparisons across different\nmobility systems. In this paper, we present NavBench, a multi-domain benchmark\nfor training and evaluating RL-based navigation policies across diverse robotic\nplatforms and operational environments. Built on IsaacLab, our framework\nstandardizes task definitions, enabling different robots to tackle various\nnavigation challenges without the need for ad-hoc task redesigns or custom\nevaluation metrics. Our benchmark addresses three key challenges: (1) Unified\ncross-medium benchmarking, enabling direct evaluation of diverse actuation\nmethods (thrusters, wheels, water-based propulsion) in realistic environments;\n(2) Scalable and modular design, facilitating seamless robot-task\ninterchangeability and reproducible training pipelines; and (3) Robust\nsim-to-real validation, demonstrated through successful policy transfer to\nmultiple real-world robots, including a satellite robotic simulator, an\nunmanned surface vessel, and a wheeled ground vehicle. By ensuring consistency\nbetween simulation and real-world deployment, NavBench simplifies the\ndevelopment of adaptable RL-based navigation strategies. Its modular design\nallows researchers to easily integrate custom robots and tasks by following the\nframework's predefined templates, making it accessible for a wide range of\napplications. Our code is publicly available at NavBench.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Submitted for publication. Under review (2025)",
    "pdf_url": "http://arxiv.org/pdf/2505.14526v1",
    "published_date": "2025-05-20 15:48:23 UTC",
    "updated_date": "2025-05-20 15:48:23 UTC"
  },
  {
    "arxiv_id": "2505.14524v2",
    "title": "Guarded Query Routing for Large Language Models",
    "authors": [
      "Richard Å lÃ©her",
      "William Brach",
      "Tibor Sloboda",
      "KristiÃ¡n KoÅ¡Å¥Ã¡l",
      "Lukas Galke"
    ],
    "abstract": "Query routing, the task to route user queries to different large language\nmodel (LLM) endpoints, can be considered as a text classification problem.\nHowever, out-of-distribution queries must be handled properly, as those could\nbe questions about unrelated domains, queries in other languages, or even\ncontain unsafe text. Here, we thus study a guarded query routing problem, for\nwhich we first introduce the Guarded Query Routing Benchmark (GQR-Bench), which\ncovers three exemplary target domains (law, finance, and healthcare), and seven\ndatasets to test robustness against out-of-distribution queries. We then use\nGQR-Bench to contrast the effectiveness and efficiency of LLM-based routing\nmechanisms (GPT-4o-mini, Llama-3.2-3B, and Llama-3.1-8B), standard LLM-based\nguardrail approaches (LlamaGuard and NVIDIA NeMo Guardrails), continuous\nbag-of-words classifiers (WideMLP, fastText), and traditional machine learning\nmodels (SVM, XGBoost). Our results show that WideMLP, enhanced with\nout-of-domain detection capabilities, yields the best trade-off between\naccuracy (88%) and speed (<4ms). The embedding-based fastText excels at speed\n(<1ms) with acceptable accuracy (80%), whereas LLMs yield the highest accuracy\n(91%) but are comparatively slow (62ms for local Llama-3.1:8B and 669ms for\nremote GPT-4o-mini calls). Our findings challenge the automatic reliance on\nLLMs for (guarded) query routing and provide concrete recommendations for\npractical applications. GQR-Bench will be released as a Python package -- gqr.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14524v2",
    "published_date": "2025-05-20 15:46:59 UTC",
    "updated_date": "2025-05-22 07:29:24 UTC"
  },
  {
    "arxiv_id": "2505.14523v1",
    "title": "Exploring Graph Representations of Logical Forms for Language Modeling",
    "authors": [
      "Michael Sullivan"
    ],
    "abstract": "We make the case for language models over logical forms (LFLMs), arguing that\nsuch models are more data-efficient than their textual counterparts. To that\nend, we introduce the Graph-based Formal-Logical Distributional Semantics\n(GFoLDS) prototype, a pretrained LM over graph representations of logical\nforms, as a proof-of-concept of LFLMs. Using GFoLDS, we present strong\nexperimental evidence that LFLMs can leverage the built-in, basic linguistic\nknowledge inherent in such models to immediately begin learning more complex\npatterns. On downstream tasks, we show that GFoLDS vastly outperforms textual,\ntransformer LMs pretrained on similar amounts of data, indicating that LFLMs\ncan learn with substantially less data than models over plain text.\nFurthermore, we show that the performance of this model is likely to scale with\nadditional parameters and pretraining data, suggesting the viability of LFLMs\nin real-world applications.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "To be published in ACL 2025 Findings",
    "pdf_url": "http://arxiv.org/pdf/2505.14523v1",
    "published_date": "2025-05-20 15:46:44 UTC",
    "updated_date": "2025-05-20 15:46:44 UTC"
  },
  {
    "arxiv_id": "2505.14513v1",
    "title": "Latent Flow Transformer",
    "authors": [
      "Yen-Chen Wu",
      "Feng-Ting Liao",
      "Meng-Hsi Chen",
      "Pei-Chen Ho",
      "Farhang Nabiei",
      "Da-shan Shiu"
    ],
    "abstract": "Transformers, the standard implementation for large language models (LLMs),\ntypically consist of tens to hundreds of discrete layers. While more layers can\nlead to better performance, this approach has been challenged as far from\nefficient, especially given the superiority of continuous layers demonstrated\nby diffusion and flow-based models for image generation. We propose the Latent\nFlow Transformer (LFT), which replaces a block of layers with a single learned\ntransport operator trained via flow matching, offering significant compression\nwhile maintaining compatibility with the original architecture. Additionally,\nwe address the limitations of existing flow-based methods in \\textit{preserving\ncoupling} by introducing the Flow Walking (FW) algorithm. On the Pythia-410M\nmodel, LFT trained with flow matching compresses 6 of 24 layers and outperforms\ndirectly skipping 2 layers (KL Divergence of LM logits at 0.407 vs. 0.529),\ndemonstrating the feasibility of this design. When trained with FW, LFT further\ndistills 12 layers into one while reducing the KL to 0.736 surpassing that from\nskipping 3 layers (0.932), significantly narrowing the gap between\nautoregressive and flow-based generation paradigms.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14513v1",
    "published_date": "2025-05-20 15:41:05 UTC",
    "updated_date": "2025-05-20 15:41:05 UTC"
  },
  {
    "arxiv_id": "2505.14510v2",
    "title": "BACON: A fully explainable AI model with graded logic for decision making problems",
    "authors": [
      "Haishi Bai",
      "Jozo Dujmovic",
      "Jianwu Wang"
    ],
    "abstract": "As machine learning models and autonomous agents are increasingly deployed in\nhigh-stakes, real-world domains such as healthcare, security, finance, and\nrobotics, the need for transparent and trustworthy explanations has become\ncritical. To ensure end-to-end transparency of AI decisions, we need models\nthat are not only accurate but also fully explainable and human-tunable. We\nintroduce BACON, a novel framework for automatically training explainable AI\nmodels for decision making problems using graded logic. BACON achieves high\npredictive accuracy while offering full structural transparency and precise,\nlogic-based symbolic explanations, enabling effective human-AI collaboration\nand expert-guided refinement. We evaluate BACON with a diverse set of\nscenarios: classic Boolean approximation, Iris flower classification, house\npurchasing decisions and breast cancer diagnosis. In each case, BACON provides\nhigh-performance models while producing compact, human-verifiable decision\nlogic. These results demonstrate BACON's potential as a practical and\nprincipled approach for delivering crisp, trustworthy explainable AI.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14510v2",
    "published_date": "2025-05-20 15:39:05 UTC",
    "updated_date": "2025-05-22 15:50:56 UTC"
  },
  {
    "arxiv_id": "2505.14505v1",
    "title": "ModRWKV: Transformer Multimodality in Linear Time",
    "authors": [
      "Jiale Kang",
      "Ziyin Yue",
      "Qingyu Yin",
      "Jiang Rui",
      "Weile Li",
      "Zening Lu",
      "Zhouran Ji"
    ],
    "abstract": "Currently, most multimodal studies are based on large language models (LLMs)\nwith quadratic-complexity Transformer architectures. While linear models like\nRNNs enjoy low inference costs, their application has been largely limited to\nthe text-only modality. This work explores the capabilities of modern RNN\narchitectures in multimodal contexts. We propose ModRWKV-a decoupled multimodal\nframework built upon the RWKV7 architecture as its LLM backbone-which achieves\nmulti-source information fusion through dynamically adaptable heterogeneous\nmodality encoders. We designed the multimodal modules in ModRWKV with an\nextremely lightweight architecture and, through extensive experiments,\nidentified a configuration that achieves an optimal balance between performance\nand computational efficiency. ModRWKV leverages the pretrained weights of the\nRWKV7 LLM for initialization, which significantly accelerates multimodal\ntraining. Comparative experiments with different pretrained checkpoints further\ndemonstrate that such initialization plays a crucial role in enhancing the\nmodel's ability to understand multimodal signals. Supported by extensive\nexperiments, we conclude that modern RNN architectures present a viable\nalternative to Transformers in the domain of multimodal large language models\n(MLLMs). Furthermore, we identify the optimal configuration of the ModRWKV\narchitecture through systematic exploration.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14505v1",
    "published_date": "2025-05-20 15:34:36 UTC",
    "updated_date": "2025-05-20 15:34:36 UTC"
  },
  {
    "arxiv_id": "2505.14499v1",
    "title": "Enhanced Multimodal Aspect-Based Sentiment Analysis by LLM-Generated Rationales",
    "authors": [
      "Jun Cao",
      "Jiyi Li",
      "Ziwei Yang",
      "Renjie Zhou"
    ],
    "abstract": "There has been growing interest in Multimodal Aspect-Based Sentiment Analysis\n(MABSA) in recent years. Existing methods predominantly rely on pre-trained\nsmall language models (SLMs) to collect information related to aspects and\nsentiments from both image and text, with an aim to align these two modalities.\nHowever, small SLMs possess limited capacity and knowledge, often resulting in\ninaccurate identification of meaning, aspects, sentiments, and their\ninterconnections in textual and visual data. On the other hand, Large language\nmodels (LLMs) have shown exceptional capabilities in various tasks by\neffectively exploring fine-grained information in multimodal data. However,\nsome studies indicate that LLMs still fall short compared to fine-tuned small\nmodels in the field of ABSA. Based on these findings, we propose a novel\nframework, termed LRSA, which combines the decision-making capabilities of SLMs\nwith additional information provided by LLMs for MABSA. Specifically, we inject\nexplanations generated by LLMs as rationales into SLMs and employ a dual\ncross-attention mechanism for enhancing feature interaction and fusion, thereby\naugmenting the SLMs' ability to identify aspects and sentiments. We evaluated\nour method using two baseline models, numerous experiments highlight the\nsuperiority of our approach on three widely-used benchmarks, indicating its\ngeneralizability and applicability to most pre-trained models for MABSA.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14499v1",
    "published_date": "2025-05-20 15:28:26 UTC",
    "updated_date": "2025-05-20 15:28:26 UTC"
  },
  {
    "arxiv_id": "2505.14489v1",
    "title": "Reasoning Models Better Express Their Confidence",
    "authors": [
      "Dongkeun Yoon",
      "Seungone Kim",
      "Sohee Yang",
      "Sunkyoung Kim",
      "Soyeon Kim",
      "Yongil Kim",
      "Eunbi Choi",
      "Yireun Kim",
      "Minjoon Seo"
    ],
    "abstract": "Despite their strengths, large language models (LLMs) often fail to\ncommunicate their confidence accurately, making it difficult to assess when\nthey might be wrong and limiting their reliability. In this work, we\ndemonstrate that reasoning models-LLMs that engage in extended chain-of-thought\n(CoT) reasoning-exhibit superior performance not only in problem-solving but\nalso in accurately expressing their confidence. Specifically, we benchmark six\nreasoning models across six datasets and find that they achieve strictly better\nconfidence calibration than their non-reasoning counterparts in 33 out of the\n36 settings. Our detailed analysis reveals that these gains in calibration stem\nfrom the slow thinking behaviors of reasoning models-such as exploring\nalternative approaches and backtracking-which enable them to adjust their\nconfidence dynamically throughout their CoT, making it progressively more\naccurate. In particular, we find that reasoning models become increasingly\nbetter calibrated as their CoT unfolds, a trend not observed in non-reasoning\nmodels. Moreover, removing slow thinking behaviors from the CoT leads to a\nsignificant drop in calibration. Lastly, we show that these gains are not\nexclusive to reasoning models-non-reasoning models also benefit when guided to\nperform slow thinking via in-context learning.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2505.14489v1",
    "published_date": "2025-05-20 15:19:00 UTC",
    "updated_date": "2025-05-20 15:19:00 UTC"
  },
  {
    "arxiv_id": "2505.14479v1",
    "title": "Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach",
    "authors": [
      "Oren Sultan",
      "Eitan Stern",
      "Dafna Shahaf"
    ],
    "abstract": "Large language models (LLMs) struggle with formal domains that require\nrigorous logical deduction and symbolic reasoning, such as mathematical proof\ngeneration. We propose a neuro-symbolic approach that combines LLMs' generative\nstrengths with structured components to overcome this challenge. As a\nproof-of-concept, we focus on geometry problems. Our approach is two-fold: (1)\nwe retrieve analogous problems and use their proofs to guide the LLM, and (2) a\nformal verifier evaluates the generated proofs and provides feedback, helping\nthe model fix incorrect proofs. We demonstrate that our method significantly\nimproves proof accuracy for OpenAI's o1 model (58%-70% improvement); both\nanalogous problems and the verifier's feedback contribute to these gains. More\nbroadly, shifting to LLMs that generate provably correct conclusions could\ndramatically improve their reliability, accuracy and consistency, unlocking\ncomplex tasks and critical real-world applications that require\ntrustworthiness.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "long paper",
    "pdf_url": "http://arxiv.org/pdf/2505.14479v1",
    "published_date": "2025-05-20 15:13:32 UTC",
    "updated_date": "2025-05-20 15:13:32 UTC"
  },
  {
    "arxiv_id": "2505.14469v1",
    "title": "Attributional Safety Failures in Large Language Models under Code-Mixed Perturbations",
    "authors": [
      "Somnath Banerjee",
      "Pratyush Chatterjee",
      "Shanu Kumar",
      "Sayan Layek",
      "Parag Agrawal",
      "Rima Hazra",
      "Animesh Mukherjee"
    ],
    "abstract": "Recent advancements in LLMs have raised significant safety concerns,\nparticularly when dealing with code-mixed inputs and outputs. Our study\nsystematically investigates the increased susceptibility of LLMs to produce\nunsafe outputs from code-mixed prompts compared to monolingual English prompts.\nUtilizing explainability methods, we dissect the internal attribution shifts\ncausing model's harmful behaviors. In addition, we explore cultural dimensions\nby distinguishing between universally unsafe and culturally-specific unsafe\nqueries. This paper presents novel experimental insights, clarifying the\nmechanisms driving this phenomenon.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14469v1",
    "published_date": "2025-05-20 15:05:03 UTC",
    "updated_date": "2025-05-20 15:05:03 UTC"
  },
  {
    "arxiv_id": "2505.14455v1",
    "title": "CtrlDiff: Boosting Large Diffusion Language Models with Dynamic Block Prediction and Controllable Generation",
    "authors": [
      "Chihan Huang",
      "Hao Tang"
    ],
    "abstract": "Although autoregressive models have dominated language modeling in recent\nyears, there has been a growing interest in exploring alternative paradigms to\nthe conventional next-token prediction framework. Diffusion-based language\nmodels have emerged as a compelling alternative due to their powerful parallel\ngeneration capabilities and inherent editability. However, these models are\noften constrained by fixed-length generation. A promising direction is to\ncombine the strengths of both paradigms, segmenting sequences into blocks,\nmodeling autoregressive dependencies across blocks while leveraging discrete\ndiffusion to estimate the conditional distribution within each block given the\npreceding context. Nevertheless, their practical application is often hindered\nby two key limitations: rigid fixed-length outputs and a lack of flexible\ncontrol mechanisms. In this work, we address the critical limitations of fixed\ngranularity and weak controllability in current large diffusion language\nmodels. We propose CtrlDiff, a dynamic and controllable semi-autoregressive\nframework that adaptively determines the size of each generation block based on\nlocal semantics using reinforcement learning. Furthermore, we introduce a\nclassifier-guided control mechanism tailored to discrete diffusion, which\nsignificantly reduces computational overhead while facilitating efficient\npost-hoc conditioning without retraining. Extensive experiments demonstrate\nthat CtrlDiff sets a new standard among hybrid diffusion models, narrows the\nperformance gap to state-of-the-art autoregressive approaches, and enables\neffective conditional text generation across diverse tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14455v1",
    "published_date": "2025-05-20 14:52:41 UTC",
    "updated_date": "2025-05-20 14:52:41 UTC"
  },
  {
    "arxiv_id": "2505.14452v2",
    "title": "How Managers Perceive AI-Assisted Conversational Training for Workplace Communication",
    "authors": [
      "Lance T. Wilhelm",
      "Xiaohan Ding",
      "Kirk McInnis Knutsen",
      "Buse Carik",
      "Eugenia H. Rho"
    ],
    "abstract": "Effective workplace communication is essential for managerial success, yet\nmany managers lack access to tailored and sustained training. Although\nAI-assisted communication systems may offer scalable training solutions, little\nis known about how managers envision the role of AI in helping them improve\ntheir communication skills. To investigate this, we designed a conversational\nrole-play system, CommCoach, as a functional probe to understand how managers\nanticipate using AI to practice their communication skills. Through\nsemi-structured interviews, participants emphasized the value of adaptive,\nlow-risk simulations for practicing difficult workplace conversations. They\nalso highlighted opportunities, including human-AI teaming, transparent and\ncontext-aware feedback, and greater control over AI-generated personas.\nAI-assisted communication training should balance personalization, structured\nlearning objectives, and adaptability to different user styles and contexts.\nHowever, achieving this requires carefully navigating tensions between adaptive\nand consistent AI feedback, realism and potential bias, and the open-ended\nnature of AI conversations versus structured workplace discourse.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "accepted to CUI '25",
    "pdf_url": "http://arxiv.org/pdf/2505.14452v2",
    "published_date": "2025-05-20 14:51:27 UTC",
    "updated_date": "2025-05-21 16:59:56 UTC"
  },
  {
    "arxiv_id": "2505.14451v1",
    "title": "RefiDiff: Refinement-Aware Diffusion for Efficient Missing Data Imputation",
    "authors": [
      "Md Atik Ahamed",
      "Qiang Ye",
      "Qiang Cheng"
    ],
    "abstract": "Missing values in high-dimensional, mixed-type datasets pose significant\nchallenges for data imputation, particularly under Missing Not At Random (MNAR)\nmechanisms. Existing methods struggle to integrate local and global data\ncharacteristics, limiting performance in MNAR and high-dimensional settings. We\npropose an innovative framework, RefiDiff, combining local machine learning\npredictions with a novel Mamba-based denoising network capturing\ninterrelationships among distant features and samples. Our approach leverages\npre-refinement for initial warm-up imputations and post-refinement to polish\nresults, enhancing stability and accuracy. By encoding mixed-type data into\nunified tokens, RefiDiff enables robust imputation without architectural or\nhyperparameter tuning. RefiDiff outperforms state-of-the-art (SOTA) methods\nacross missing-value settings, excelling in MNAR with a 4x faster training time\nthan SOTA DDPM-based approaches. Extensive evaluations on nine real-world\ndatasets demonstrate its robustness, scalability, and effectiveness in handling\ncomplex missingness patterns.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14451v1",
    "published_date": "2025-05-20 14:51:07 UTC",
    "updated_date": "2025-05-20 14:51:07 UTC"
  },
  {
    "arxiv_id": "2505.14442v1",
    "title": "Creative Preference Optimization",
    "authors": [
      "Mete Ismayilzada",
      "Antonio Laverghetta Jr.",
      "Simone A. Luchini",
      "Reet Patel",
      "Antoine Bosselut",
      "Lonneke van der Plas",
      "Roger Beaty"
    ],
    "abstract": "While Large Language Models (LLMs) have demonstrated impressive performance\nacross natural language generation tasks, their ability to generate truly\ncreative content-characterized by novelty, diversity, surprise, and\nquality-remains limited. Existing methods for enhancing LLM creativity often\nfocus narrowly on diversity or specific tasks, failing to address creativity's\nmultifaceted nature in a generalizable way. In this work, we propose Creative\nPreference Optimization (CrPO), a novel alignment method that injects signals\nfrom multiple creativity dimensions into the preference optimization objective\nin a modular fashion. We train and evaluate creativity-augmented versions of\nseveral models using CrPO and MuCE, a new large-scale human preference dataset\nspanning over 200,000 human-generated responses and ratings from more than 30\npsychological creativity assessments. Our models outperform strong baselines,\nincluding GPT-4o, on both automated and human evaluations, producing more\nnovel, diverse, and surprising generations while maintaining high output\nquality. Additional evaluations on NoveltyBench further confirm the\ngeneralizability of our approach. Together, our results demonstrate that\ndirectly optimizing for creativity within preference frameworks is a promising\ndirection for advancing the creative capabilities of LLMs without compromising\noutput quality.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "27 pages",
    "pdf_url": "http://arxiv.org/pdf/2505.14442v1",
    "published_date": "2025-05-20 14:43:41 UTC",
    "updated_date": "2025-05-20 14:43:41 UTC"
  },
  {
    "arxiv_id": "2505.14436v1",
    "title": "Neural Incompatibility: The Unbridgeable Gap of Cross-Scale Parametric Knowledge Transfer in Large Language Models",
    "authors": [
      "Yuqiao Tan",
      "Shizhu He",
      "Kang Liu",
      "Jun Zhao"
    ],
    "abstract": "Large Language Models (LLMs) offer a transparent brain with accessible\nparameters that encode extensive knowledge, which can be analyzed, located and\ntransferred. Consequently, a key research challenge is to transcend traditional\nknowledge transfer paradigms rooted in symbolic language and achieve genuine\nParametric Knowledge Transfer (PKT). Significantly, exploring effective methods\nfor transferring knowledge across LLMs of different scales through parameters\npresents an intriguing and valuable research direction. In this paper, we first\ndemonstrate $\\textbf{Alignment}$ in parametric space is the fundamental\nprerequisite to achieve successful cross-scale PKT. We redefine the previously\nexplored knowledge transfer as Post-Align PKT (PostPKT), which utilizes\nextracted parameters for LoRA initialization and requires subsequent fine-tune\nfor alignment. Hence, to reduce cost for further fine-tuning, we introduce a\nnovel Pre-Align PKT (PrePKT) paradigm and propose a solution called\n$\\textbf{LaTen}$\n($\\textbf{L}$oc$\\textbf{a}$te-$\\textbf{T}$h$\\textbf{e}$n-Alig$\\textbf{n}$) that\naligns the parametric spaces of LLMs across scales only using several training\nsteps without following training. Comprehensive experiments on four benchmarks\ndemonstrate that both PostPKT and PrePKT face challenges in achieving\nconsistently stable transfer. Through in-depth analysis, we identify\n$\\textbf{Neural Incompatibility}$ as the ethological and parametric structural\ndifferences between LLMs of varying scales, presenting fundamental challenges\nto achieving effective PKT. These findings provide fresh insights into the\nparametric architectures of LLMs and highlight promising directions for future\nresearch on efficient PKT. Our code is available at\nhttps://github.com/Trae1ounG/Neural_Incompatibility.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by ACL'25 Main. Code link:\n  https://github.com/Trae1ounG/Neural_Incompatibility",
    "pdf_url": "http://arxiv.org/pdf/2505.14436v1",
    "published_date": "2025-05-20 14:42:03 UTC",
    "updated_date": "2025-05-20 14:42:03 UTC"
  },
  {
    "arxiv_id": "2505.14435v1",
    "title": "Choosing a Model, Shaping a Future: Comparing LLM Perspectives on Sustainability and its Relationship with AI",
    "authors": [
      "Annika Bush",
      "Meltem Aksoy",
      "Markus Pauly",
      "Greta Ontrup"
    ],
    "abstract": "As organizations increasingly rely on AI systems for decision support in\nsustainability contexts, it becomes critical to understand the inherent biases\nand perspectives embedded in Large Language Models (LLMs). This study\nsystematically investigates how five state-of-the-art LLMs -- Claude, DeepSeek,\nGPT, LLaMA, and Mistral - conceptualize sustainability and its relationship\nwith AI. We administered validated, psychometric sustainability-related\nquestionnaires - each 100 times per model -- to capture response patterns and\nvariability. Our findings revealed significant inter-model differences: For\nexample, GPT exhibited skepticism about the compatibility of AI and\nsustainability, whereas LLaMA demonstrated extreme techno-optimism with perfect\nscores for several Sustainable Development Goals (SDGs). Models also diverged\nin attributing institutional responsibility for AI and sustainability\nintegration, a results that holds implications for technology governance\napproaches. Our results demonstrate that model selection could substantially\ninfluence organizational sustainability strategies, highlighting the need for\nawareness of model-specific biases when deploying LLMs for\nsustainability-related decision-making.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14435v1",
    "published_date": "2025-05-20 14:41:56 UTC",
    "updated_date": "2025-05-20 14:41:56 UTC"
  },
  {
    "arxiv_id": "2505.14428v1",
    "title": "Interpretable Neural System Dynamics: Combining Deep Learning with System Dynamics Modeling to Support Critical Applications",
    "authors": [
      "Riccardo D'Elia"
    ],
    "abstract": "The objective of this proposal is to bridge the gap between Deep Learning\n(DL) and System Dynamics (SD) by developing an interpretable neural system\ndynamics framework. While DL excels at learning complex models and making\naccurate predictions, it lacks interpretability and causal reliability.\nTraditional SD approaches, on the other hand, provide transparency and causal\ninsights but are limited in scalability and require extensive domain knowledge.\nTo overcome these limitations, this project introduces a Neural System Dynamics\npipeline, integrating Concept-Based Interpretability, Mechanistic\nInterpretability, and Causal Machine Learning. This framework combines the\npredictive power of DL with the interpretability of traditional SD models,\nresulting in both causal reliability and scalability. The efficacy of the\nproposed pipeline will be validated through real-world applications of the\nEU-funded AutoMoTIF project, which is focused on autonomous multimodal\ntransportation systems. The long-term goal is to collect actionable insights\nthat support the integration of explainability and safety in autonomous\nsystems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "To be submitted to CEUR-WS.org for publication in the Doctoral\n  Consortium Proceedings of XAI 2025, The World Conference on Explainable\n  Artificial Intelligence",
    "pdf_url": "http://arxiv.org/pdf/2505.14428v1",
    "published_date": "2025-05-20 14:38:39 UTC",
    "updated_date": "2025-05-20 14:38:39 UTC"
  },
  {
    "arxiv_id": "2505.14419v1",
    "title": "SCOPE: Compress Mathematical Reasoning Steps for Efficient Automated Process Annotation",
    "authors": [
      "Huimin Xu",
      "Xin Mao",
      "Feng-Lin Li",
      "Xiaobao Wu",
      "Wang Chen",
      "Wei Zhang",
      "Anh Tuan Luu"
    ],
    "abstract": "Process Reward Models (PRMs) have demonstrated promising results in\nmathematical reasoning, but existing process annotation approaches, whether\nthrough human annotations or Monte Carlo simulations, remain computationally\nexpensive. In this paper, we introduce Step COmpression for Process Estimation\n(SCOPE), a novel compression-based approach that significantly reduces\nannotation costs. We first translate natural language reasoning steps into code\nand normalize them through Abstract Syntax Tree, then merge equivalent steps to\nconstruct a prefix tree. Unlike simulation-based methods that waste numerous\nsamples on estimation, SCOPE leverages a compression-based prefix tree where\neach root-to-leaf path serves as a training sample, reducing the complexity\nfrom $O(NMK)$ to $O(N)$. We construct a large-scale dataset containing 196K\nsamples with only 5% of the computational resources required by previous\nmethods. Empirical results demonstrate that PRMs trained on our dataset\nconsistently outperform existing automated annotation approaches on both\nBest-of-N strategy and ProcessBench.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14419v1",
    "published_date": "2025-05-20 14:31:15 UTC",
    "updated_date": "2025-05-20 14:31:15 UTC"
  },
  {
    "arxiv_id": "2505.14412v1",
    "title": "PRL: Prompts from Reinforcement Learning",
    "authors": [
      "PaweÅ Batorski",
      "Adrian Kosmala",
      "Paul Swoboda"
    ],
    "abstract": "Effective prompt engineering remains a central challenge in fully harnessing\nthe capabilities of LLMs. While well-designed prompts can dramatically enhance\nperformance, crafting them typically demands expert intuition and a nuanced\nunderstanding of the task. Moreover, the most impactful prompts often hinge on\nsubtle semantic cues, ones that may elude human perception but are crucial for\nguiding LLM behavior. In this paper, we introduce PRL (Prompts from\nReinforcement Learning), a novel RL-based approach for automatic prompt\ngeneration. Unlike previous methods, PRL can produce novel few-shot examples\nthat were not seen during training. Our approach achieves state-of-the-art\nperformance across a range of benchmarks, including text classification,\nsimplification, and summarization. On the classification task, it surpasses\nprior methods by 2.58% over APE and 1.00% over EvoPrompt. Additionally, it\nimproves the average ROUGE scores on the summarization task by 4.32 over APE\nand by 2.12 over EvoPrompt and the SARI score on simplification by 6.93 over\nAPE and by 6.01 over EvoPrompt. Our code is available at\nhttps://github.com/Batorskq/prl .",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14412v1",
    "published_date": "2025-05-20 14:26:19 UTC",
    "updated_date": "2025-05-20 14:26:19 UTC"
  },
  {
    "arxiv_id": "2505.14403v1",
    "title": "Unearthing Gems from Stones: Policy Optimization with Negative Sample Augmentation for LLM Reasoning",
    "authors": [
      "Zhaohui Yang",
      "Shilei Jiang",
      "Chen Hu",
      "Linjing Li",
      "Shihong Deng",
      "Daxin Jiang"
    ],
    "abstract": "Recent advances in reasoning language models have witnessed a paradigm shift\nfrom short to long CoT pattern. Given the substantial computational cost of\nrollouts in long CoT models, maximizing the utility of fixed training datasets\nbecomes crucial. Our analysis reveals that negative responses contain valuable\ncomponents such as self-reflection and error-correction steps, yet primary\nexisting methods either completely discard negative samples (RFT) or apply\nequal penalization across all tokens (RL), failing to leverage these potential\nlearning signals. In light of this, we propose Behavior Constrained Policy\nGradient with Negative Sample Augmentation (BCPG-NSA), a fine-grained offline\nRL framework that encompasses three stages: 1) sample segmentation, 2)\nconsensus-based step correctness assessment combining LLM and PRM judgers, and\n3) policy optimization with NSA designed to effectively mine positive steps\nwithin negative samples. Experimental results show that BCPG-NSA outperforms\nbaselines on several challenging math/coding reasoning benchmarks using the\nsame training dataset, achieving improved sample efficiency and demonstrating\nrobustness and scalability when extended to multiple iterations.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14403v1",
    "published_date": "2025-05-20 14:16:49 UTC",
    "updated_date": "2025-05-20 14:16:49 UTC"
  },
  {
    "arxiv_id": "2505.14753v1",
    "title": "TransMedSeg: A Transferable Semantic Framework for Semi-Supervised Medical Image Segmentation",
    "authors": [
      "Mengzhu Wang",
      "Jiao Li",
      "Shanshan Wang",
      "Long Lan",
      "Huibin Tan",
      "Liang Yang",
      "Guoli Yang"
    ],
    "abstract": "Semi-supervised learning (SSL) has achieved significant progress in medical\nimage segmentation (SSMIS) through effective utilization of limited labeled\ndata. While current SSL methods for medical images predominantly rely on\nconsistency regularization and pseudo-labeling, they often overlook\ntransferable semantic relationships across different clinical domains and\nimaging modalities. To address this, we propose TransMedSeg, a novel\ntransferable semantic framework for semi-supervised medical image segmentation.\nOur approach introduces a Transferable Semantic Augmentation (TSA) module,\nwhich implicitly enhances feature representations by aligning domain-invariant\nsemantics through cross-domain distribution matching and intra-domain\nstructural preservation. Specifically, TransMedSeg constructs a unified feature\nspace where teacher network features are adaptively augmented towards student\nnetwork semantics via a lightweight memory module, enabling implicit semantic\ntransformation without explicit data generation. Interestingly, this\naugmentation is implicitly realized through an expected transferable\ncross-entropy loss computed over the augmented teacher distribution. An upper\nbound of the expected loss is theoretically derived and minimized during\ntraining, incurring negligible computational overhead. Extensive experiments on\nmedical image datasets demonstrate that TransMedSeg outperforms existing\nsemi-supervised methods, establishing a new direction for transferable\nrepresentation learning in medical image analysis.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14753v1",
    "published_date": "2025-05-20 14:16:40 UTC",
    "updated_date": "2025-05-20 14:16:40 UTC"
  },
  {
    "arxiv_id": "2505.14398v1",
    "title": "Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable Computation",
    "authors": [
      "Peter Baile Chen",
      "Yi Zhang",
      "Dan Roth",
      "Samuel Madden",
      "Jacob Andreas",
      "Michael Cafarella"
    ],
    "abstract": "While humans naturally learn and adapt from past experiences, large language\nmodels (LLMs) and their agentic counterparts struggle to retain reasoning from\nprevious tasks and apply them in future contexts. To address this limitation,\nwe propose a novel framework, log-augmented generation (LAG) that directly\nreuses prior computation and reasoning from past logs at test time to enhance\nmodel's ability to learn from previous tasks and perform better on new, unseen\nchallenges, all while keeping the system efficient and scalable. Specifically,\nour system represents task logs using key-value (KV) caches, encoding the full\nreasoning context of prior tasks while storing KV caches for only a selected\nsubset of tokens. When a new task arises, LAG retrieves the KV values from\nrelevant logs to augment generation. Our approach differs from reflection-based\nmemory mechanisms by directly reusing prior reasoning and computations without\nrequiring additional steps for knowledge extraction or distillation. Our method\nalso goes beyond existing KV caching techniques, which primarily target\nefficiency gains rather than improving accuracy. Experiments on knowledge- and\nreasoning-intensive datasets demonstrate that our method significantly\noutperforms standard agentic systems that do not utilize logs, as well as\nexisting solutions based on reflection and KV cache techniques.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Data and code are available at https://peterbaile.github.io/lag/",
    "pdf_url": "http://arxiv.org/pdf/2505.14398v1",
    "published_date": "2025-05-20 14:14:38 UTC",
    "updated_date": "2025-05-20 14:14:38 UTC"
  },
  {
    "arxiv_id": "2505.14396v1",
    "title": "Causal Cartographer: From Mapping to Reasoning Over Counterfactual Worlds",
    "authors": [
      "GaÃ«l Gendron",
      "JoÅ¾e M. RoÅ¾anec",
      "Michael Witbrock",
      "Gillian Dobbie"
    ],
    "abstract": "Causal world models are systems that can answer counterfactual questions\nabout an environment of interest, i.e. predict how it would have evolved if an\narbitrary subset of events had been realized differently. It requires\nunderstanding the underlying causes behind chains of events and conducting\ncausal inference for arbitrary unseen distributions. So far, this task eludes\nfoundation models, notably large language models (LLMs), which do not have\ndemonstrated causal reasoning capabilities beyond the memorization of existing\ncausal relationships. Furthermore, evaluating counterfactuals in real-world\napplications is challenging since only the factual world is observed, limiting\nevaluation to synthetic datasets. We address these problems by explicitly\nextracting and modeling causal relationships and propose the Causal\nCartographer framework. First, we introduce a graph retrieval-augmented\ngeneration agent tasked to retrieve causal relationships from data. This\napproach allows us to construct a large network of real-world causal\nrelationships that can serve as a repository of causal knowledge and build\nreal-world counterfactuals. In addition, we create a counterfactual reasoning\nagent constrained by causal relationships to perform reliable step-by-step\ncausal inference. We show that our approach can extract causal knowledge and\nimprove the robustness of LLMs for causal reasoning tasks while reducing\ninference costs and spurious correlations.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "I.2.3; I.2.6; I.2.7; G.2.2; G.3; J.1"
    ],
    "primary_category": "cs.AI",
    "comment": "29 pages, 9 pages for the main paper, 20 pages for the references and\n  appendix, 25 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.14396v1",
    "published_date": "2025-05-20 14:14:05 UTC",
    "updated_date": "2025-05-20 14:14:05 UTC"
  },
  {
    "arxiv_id": "2505.14395v1",
    "title": "MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation Capabilities in Any Language",
    "authors": [
      "Seyoung Song",
      "Seogyeong Jeong",
      "Eunsu Kim",
      "Jiho Jin",
      "Dongkwan Kim",
      "Jay Shin",
      "Alice Oh"
    ],
    "abstract": "Evaluating text generation capabilities of large language models (LLMs) is\nchallenging, particularly for low-resource languages where methods for direct\nassessment are scarce. We propose MUG-Eval, a novel framework that evaluates\nLLMs' multilingual generation capabilities by transforming existing benchmarks\ninto conversational tasks and measuring the LLMs' accuracies on those tasks. We\nspecifically designed these conversational tasks to require effective\ncommunication in the target language. Then, we simply use task success rate as\na proxy of successful conversation generation. Our approach offers two key\nadvantages: it is independent of language-specific NLP tools or annotated\ndatasets, which are limited for most languages, and it does not rely on\nLLMs-as-judges, whose evaluation quality degrades outside a few high-resource\nlanguages. We evaluate 8 LLMs across 30 languages spanning high, mid, and\nlow-resource categories, and we find that MUG-Eval correlates strongly with\nestablished benchmarks ($r$ > 0.75) while enabling standardized comparisons\nacross languages and models. Our framework provides a robust and\nresource-efficient solution for evaluating multilingual generation that can be\nextended to thousands of languages.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14395v1",
    "published_date": "2025-05-20 14:14:00 UTC",
    "updated_date": "2025-05-20 14:14:00 UTC"
  },
  {
    "arxiv_id": "2505.14394v1",
    "title": "Knowledge Graph Based Repository-Level Code Generation",
    "authors": [
      "Mihir Athale",
      "Vishal Vaddina"
    ],
    "abstract": "Recent advancements in Large Language Models (LLMs) have transformed code\ngeneration from natural language queries. However, despite their extensive\nknowledge and ability to produce high-quality code, LLMs often struggle with\ncontextual accuracy, particularly in evolving codebases. Current code search\nand retrieval methods frequently lack robustness in both the quality and\ncontextual relevance of retrieved results, leading to suboptimal code\ngeneration. This paper introduces a novel knowledge graph-based approach to\nimprove code search and retrieval leading to better quality of code generation\nin the context of repository-level tasks. The proposed approach represents code\nrepositories as graphs, capturing structural and relational information for\nenhanced context-aware code generation. Our framework employs a hybrid approach\nfor code retrieval to improve contextual relevance, track inter-file modular\ndependencies, generate more robust code and ensure consistency with the\nexisting codebase. We benchmark the proposed approach on the Evolutionary Code\nBenchmark (EvoCodeBench) dataset, a repository-level code generation benchmark,\nand demonstrate that our method significantly outperforms the baseline\napproach. These findings suggest that knowledge graph based code generation\ncould advance robust, context-sensitive coding assistance tools.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.14394v1",
    "published_date": "2025-05-20 14:13:59 UTC",
    "updated_date": "2025-05-20 14:13:59 UTC"
  },
  {
    "arxiv_id": "2505.14391v1",
    "title": "Beyond the First Error: Process Reward Models for Reflective Mathematical Reasoning",
    "authors": [
      "Zhaohui Yang",
      "Chenghua He",
      "Xiaowen Shi",
      "Linjing Li",
      "Qiyue Yin",
      "Shihong Deng",
      "Daxin Jiang"
    ],
    "abstract": "Many studies focus on data annotation techniques for training effective PRMs.\nHowever, current methods encounter a significant issue when applied to long CoT\nreasoning processes: they tend to focus solely on the first incorrect step and\nall preceding steps, assuming that all subsequent steps are incorrect. These\nmethods overlook the unique self-correction and reflection mechanisms inherent\nin long CoT, where correct reasoning steps may still occur after initial\nreasoning mistakes. To address this issue, we propose a novel data annotation\nmethod for PRMs specifically designed to score the long CoT reasoning process.\nGiven that under the reflection pattern, correct and incorrect steps often\nalternate, we introduce the concepts of Error Propagation and Error Cessation,\nenhancing PRMs' ability to identify both effective self-correction behaviors\nand reasoning based on erroneous steps. Leveraging an LLM-based judger for\nannotation, we collect 1.7 million data samples to train a 7B PRM and evaluate\nit at both solution and step levels. Experimental results demonstrate that\ncompared to existing open-source PRMs and PRMs trained on open-source datasets,\nour PRM achieves superior performance across various metrics, including search\nguidance, BoN, and F1 scores. Compared to widely used MC-based annotation\nmethods, our annotation approach not only achieves higher data efficiency but\nalso delivers superior performance. Detailed analysis is also conducted to\ndemonstrate the stability and generalizability of our method.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14391v1",
    "published_date": "2025-05-20 14:12:05 UTC",
    "updated_date": "2025-05-20 14:12:05 UTC"
  },
  {
    "arxiv_id": "2505.14381v1",
    "title": "SCAN: Semantic Document Layout Analysis for Textual and Visual Retrieval-Augmented Generation",
    "authors": [
      "Yuyang Dong",
      "Nobuhiro Ueda",
      "KrisztiÃ¡n Boros",
      "Daiki Ito",
      "Takuya Sera",
      "Masafumi Oyamada"
    ],
    "abstract": "With the increasing adoption of Large Language Models (LLMs) and\nVision-Language Models (VLMs), rich document analysis technologies for\napplications like Retrieval-Augmented Generation (RAG) and visual RAG are\ngaining significant attention. Recent research indicates that using VLMs can\nachieve better RAG performance, but processing rich documents still remains a\nchallenge since a single page contains large amounts of information. In this\npaper, we present SCAN (\\textbf{S}emanti\\textbf{C} Document Layout\n\\textbf{AN}alysis), a novel approach enhancing both textual and visual\nRetrieval-Augmented Generation (RAG) systems working with visually rich\ndocuments. It is a VLM-friendly approach that identifies document components\nwith appropriate semantic granularity, balancing context preservation with\nprocessing efficiency. SCAN uses a coarse-grained semantic approach that\ndivides documents into coherent regions covering continuous components. We\ntrained the SCAN model by fine-tuning object detection models with\nsophisticated annotation datasets. Our experimental results across English and\nJapanese datasets demonstrate that applying SCAN improves end-to-end textual\nRAG performance by up to 9.0\\% and visual RAG performance by up to 6.4\\%,\noutperforming conventional approaches and even commercial document processing\nsolutions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "v1",
    "pdf_url": "http://arxiv.org/pdf/2505.14381v1",
    "published_date": "2025-05-20 14:03:24 UTC",
    "updated_date": "2025-05-20 14:03:24 UTC"
  },
  {
    "arxiv_id": "2505.14377v1",
    "title": "When Bias Backfires: The Modulatory Role of Counterfactual Explanations on the Adoption of Algorithmic Bias in XAI-Supported Human Decision-Making",
    "authors": [
      "Ulrike Kuhl",
      "Annika Bush"
    ],
    "abstract": "Although the integration of artificial intelligence (AI) into everyday tasks\nimproves efficiency and objectivity, it also risks transmitting bias to human\ndecision-making. In this study, we conducted a controlled experiment that\nsimulated hiring decisions to examine how biased AI recommendations - augmented\nwith or without counterfactual explanations - influence human judgment over\ntime. Participants, acting as hiring managers, completed 60 decision trials\ndivided into a baseline phase without AI, followed by a phase with biased (X)AI\nrecommendations (favoring either male or female candidates), and a final\npost-interaction phase without AI. Our results indicate that the participants\nfollowed the AI recommendations 70% of the time when the qualifications of the\ngiven candidates were comparable. Yet, only a fraction of participants detected\nthe gender bias (8 out of 294). Crucially, exposure to biased AI altered\nparticipants' inherent preferences: in the post-interaction phase,\nparticipants' independent decisions aligned with the bias when no\ncounterfactual explanations were provided before, but reversed the bias when\nexplanations were given. Reported trust did not differ significantly across\nconditions. Confidence varied throughout the study phases after exposure to\nmale-biased AI, indicating nuanced effects of AI bias on decision certainty.\nOur findings point to the importance of calibrating XAI to avoid unintended\nbehavioral shifts in order to safeguard equitable decision-making and prevent\nthe adoption of algorithmic bias.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted for XAI2025",
    "pdf_url": "http://arxiv.org/pdf/2505.14377v1",
    "published_date": "2025-05-20 14:00:28 UTC",
    "updated_date": "2025-05-20 14:00:28 UTC"
  },
  {
    "arxiv_id": "2505.14366v1",
    "title": "Towards Embodied Cognition in Robots via Spatially Grounded Synthetic Worlds",
    "authors": [
      "Joel Currie",
      "Gioele Migno",
      "Enrico Piacenti",
      "Maria Elena Giannaccini",
      "Patric Bach",
      "Davide De Tommaso",
      "Agnieszka Wykowska"
    ],
    "abstract": "We present a conceptual framework for training Vision-Language Models (VLMs)\nto perform Visual Perspective Taking (VPT), a core capability for embodied\ncognition essential for Human-Robot Interaction (HRI). As a first step toward\nthis goal, we introduce a synthetic dataset, generated in NVIDIA Omniverse,\nthat enables supervised learning for spatial reasoning tasks. Each instance\nincludes an RGB image, a natural language description, and a ground-truth 4X4\ntransformation matrix representing object pose. We focus on inferring Z-axis\ndistance as a foundational skill, with future extensions targeting full 6\nDegrees Of Freedom (DOFs) reasoning. The dataset is publicly available to\nsupport further research. This work serves as a foundational step toward\nembodied AI systems capable of spatial understanding in interactive human-robot\nscenarios.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to: Intelligent Autonomous Systems (IAS) 2025 as Late\n  Breaking Report",
    "pdf_url": "http://arxiv.org/pdf/2505.14366v1",
    "published_date": "2025-05-20 13:49:09 UTC",
    "updated_date": "2025-05-20 13:49:09 UTC"
  },
  {
    "arxiv_id": "2505.14351v1",
    "title": "FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis for Ã-Tsang, Amdo and Kham Speech Dataset Generation",
    "authors": [
      "Yutong Liu",
      "Ziyue Zhang",
      "Ban Ma-bao",
      "Yuqing Cai",
      "Yongbin Yu",
      "Renzeng Duojie",
      "Xiangxiang Wang",
      "Fan Gao",
      "Cheng Huang",
      "Nyima Tashi"
    ],
    "abstract": "Tibetan is a low-resource language with minimal parallel speech corpora\nspanning its three major dialects-\\\"U-Tsang, Amdo, and Kham-limiting progress\nin speech modeling. To address this issue, we propose FMSD-TTS, a few-shot,\nmulti-speaker, multi-dialect text-to-speech framework that synthesizes parallel\ndialectal speech from limited reference audio and explicit dialect labels. Our\nmethod features a novel speaker-dialect fusion module and a Dialect-Specialized\nDynamic Routing Network (DSDR-Net) to capture fine-grained acoustic and\nlinguistic variations across dialects while preserving speaker identity.\nExtensive objective and subjective evaluations demonstrate that FMSD-TTS\nsignificantly outperforms baselines in both dialectal expressiveness and\nspeaker similarity. We further validate the quality and utility of the\nsynthesized speech through a challenging speech-to-speech dialect conversion\ntask. Our contributions include: (1) a novel few-shot TTS system tailored for\nTibetan multi-dialect speech synthesis, (2) the public release of a large-scale\nsynthetic Tibetan speech corpus generated by FMSD-TTS, and (3) an open-source\nevaluation toolkit for standardized assessment of speaker similarity, dialect\nconsistency, and audio quality.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "13 pages",
    "pdf_url": "http://arxiv.org/pdf/2505.14351v1",
    "published_date": "2025-05-20 13:35:55 UTC",
    "updated_date": "2025-05-20 13:35:55 UTC"
  },
  {
    "arxiv_id": "2505.14349v1",
    "title": "Upgrading Democracies with Fairer Voting Methods",
    "authors": [
      "Evangelos Pournaras",
      "Srijoni Majumdar",
      "Thomas Wellings",
      "Joshua C. Yang",
      "Fatemeh B. Heravan",
      "Regula HÃ¤nggli Fricker",
      "Dirk Helbing"
    ],
    "abstract": "Voting methods are instrumental design element of democracies. Citizens use\nthem to express and aggregate their preferences to reach a collective decision.\nHowever, voting outcomes can be as sensitive to voting rules as they are to\npeople's voting choices. Despite the significance and inter-disciplinary\nscientific progress on voting methods, several democracies keep relying on\noutdated voting methods that do not fit modern, pluralistic societies well,\nwhile lacking social innovation. Here, we demonstrate how one can upgrade\nreal-world democracies, namely by using alternative preferential voting methods\nsuch as cumulative voting and the method of equal shares designed for a\nproportional representation of voters' preferences. By rigorously assessing a\nnew participatory budgeting approach applied in the city of Aarau, Switzerland,\nwe unravel the striking voting outcomes of fair voting methods: more winning\nprojects with the same budget and broader geographic and preference\nrepresentation of citizens by the elected projects, in particular for voters\nwho used to be under-represented, while promoting novel project ideas. We\nprovide profound causal evidence showing that citizens prefer proportional\nvoting methods, which possess strong legitimacy without the need of very\ntechnical specialized explanations. We also reveal strong underlying democratic\nvalues exhibited by citizens who support fair voting methods such as altruism\nand compromise. These findings come with a global momentum to unleash a new and\nlong-awaited participation blueprint of how to upgrade democracies.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.ET",
      "cs.HC",
      "cs.MA"
    ],
    "primary_category": "cs.CY",
    "comment": "Includes Supplementary Information",
    "pdf_url": "http://arxiv.org/pdf/2505.14349v1",
    "published_date": "2025-05-20 13:31:43 UTC",
    "updated_date": "2025-05-20 13:31:43 UTC"
  },
  {
    "arxiv_id": "2505.14345v1",
    "title": "Enhancing Classification with Semi-Supervised Deep Learning Using Distance-Based Sample Weights",
    "authors": [
      "Aydin Abedinia",
      "Shima Tabakhi",
      "Vahid Seydi"
    ],
    "abstract": "Recent advancements in semi-supervised deep learning have introduced\neffective strategies for leveraging both labeled and unlabeled data to improve\nclassification performance. This work proposes a semi-supervised framework that\nutilizes a distance-based weighting mechanism to prioritize critical training\nsamples based on their proximity to test data. By focusing on the most\ninformative examples, the method enhances model generalization and robustness,\nparticularly in challenging scenarios with noisy or imbalanced datasets.\nBuilding on techniques such as uncertainty consistency and graph-based\nrepresentations, the approach addresses key challenges of limited labeled data\nwhile maintaining scalability. Experiments on twelve benchmark datasets\ndemonstrate significant improvements across key metrics, including accuracy,\nprecision, and recall, consistently outperforming existing methods. This\nframework provides a robust and practical solution for semi-supervised\nlearning, with potential applications in domains such as healthcare and\nsecurity where data limitations pose significant challenges.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "68T05, 62H30",
      "I.2.6; I.5.1; I.5.4"
    ],
    "primary_category": "cs.LG",
    "comment": "5 pages, 6 figures. This paper has been accepted for publication and\n  oral presentation at the 2025 10th IEEE International Conference on Machine\n  Learning Technologies (ICMLT 2025). The final authenticated version will be\n  available in IEEE Xplore following the conference",
    "pdf_url": "http://arxiv.org/pdf/2505.14345v1",
    "published_date": "2025-05-20 13:29:04 UTC",
    "updated_date": "2025-05-20 13:29:04 UTC"
  },
  {
    "arxiv_id": "2505.14341v1",
    "title": "Replace in Translation: Boost Concept Alignment in Counterfactual Text-to-Image",
    "authors": [
      "Sifan Li",
      "Ming Tao",
      "Hao Zhao",
      "Ling Shao",
      "Hao Tang"
    ],
    "abstract": "Text-to-Image (T2I) has been prevalent in recent years, with most common\ncondition tasks having been optimized nicely. Besides, counterfactual\nText-to-Image is obstructing us from a more versatile AIGC experience. For\nthose scenes that are impossible to happen in real world and anti-physics, we\nshould spare no efforts in increasing the factual feel, which means\nsynthesizing images that people think very likely to be happening, and concept\nalignment, which means all the required objects should be in the same frame. In\nthis paper, we focus on concept alignment. As controllable T2I models have\nachieved satisfactory performance for real applications, we utilize this\ntechnology to replace the objects in a synthesized image in latent space\nstep-by-step to change the image from a common scene to a counterfactual scene\nto meet the prompt. We propose a strategy to instruct this replacing process,\nwhich is called as Explicit Logical Narrative Prompt (ELNP), by using the newly\nSoTA language model DeepSeek to generate the instructions. Furthermore, to\nevaluate models' performance in counterfactual T2I, we design a metric to\ncalculate how many required concepts in the prompt can be covered averagely in\nthe synthesized images. The extensive experiments and qualitative comparisons\ndemonstrate that our strategy can boost the concept alignment in counterfactual\nT2I.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14341v1",
    "published_date": "2025-05-20 13:27:52 UTC",
    "updated_date": "2025-05-20 13:27:52 UTC"
  },
  {
    "arxiv_id": "2505.14330v1",
    "title": "Handloom Design Generation Using Generative Networks",
    "authors": [
      "Rajat Kanti Bhattacharjee",
      "Meghali Nandi",
      "Amrit Jha",
      "Gunajit Kalita",
      "Ferdous Ahmed Barbhuiya"
    ],
    "abstract": "This paper proposes deep learning techniques of generating designs for\nclothing, focused on handloom fabric and discusses the associated challenges\nalong with its application. The capability of generative neural network models\nin understanding artistic designs and synthesizing those is not yet explored\nwell. In this work, multiple methods are employed incorporating the current\nstate of the art generative models and style transfer algorithms to study and\nobserve their performance for the task. The results are then evaluated through\nuser score. This work also provides a new dataset NeuralLoom for the task of\nthe design generation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14330v1",
    "published_date": "2025-05-20 13:16:55 UTC",
    "updated_date": "2025-05-20 13:16:55 UTC"
  },
  {
    "arxiv_id": "2505.14751v1",
    "title": "Self Distillation via Iterative Constructive Perturbations",
    "authors": [
      "Maheak Dave",
      "Aniket Kumar Singh",
      "Aryan Pareek",
      "Harshita Jha",
      "Debasis Chaudhuri",
      "Manish Pratap Singh"
    ],
    "abstract": "Deep Neural Networks have achieved remarkable achievements across various\ndomains, however balancing performance and generalization still remains a\nchallenge while training these networks. In this paper, we propose a novel\nframework that uses a cyclic optimization strategy to concurrently optimize the\nmodel and its input data for better training, rethinking the traditional\ntraining paradigm. Central to our approach is Iterative Constructive\nPerturbation (ICP), which leverages the model's loss to iteratively perturb the\ninput, progressively constructing an enhanced representation over some\nrefinement steps. This ICP input is then fed back into the model to produce\nimproved intermediate features, which serve as a target in a self-distillation\nframework against the original features. By alternately altering the model's\nparameters to the data and the data to the model, our method effectively\naddresses the gap between fitting and generalization, leading to enhanced\nperformance. Extensive experiments demonstrate that our approach not only\nmitigates common performance bottlenecks in neural networks but also\ndemonstrates significant improvements across training variations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14751v1",
    "published_date": "2025-05-20 13:15:27 UTC",
    "updated_date": "2025-05-20 13:15:27 UTC"
  },
  {
    "arxiv_id": "2505.14316v1",
    "title": "Exploring Jailbreak Attacks on LLMs through Intent Concealment and Diversion",
    "authors": [
      "Tiehan Cui",
      "Yanxu Mao",
      "Peipei Liu",
      "Congying Liu",
      "Datao You"
    ],
    "abstract": "Although large language models (LLMs) have achieved remarkable advancements,\ntheir security remains a pressing concern. One major threat is jailbreak\nattacks, where adversarial prompts bypass model safeguards to generate harmful\nor objectionable content. Researchers study jailbreak attacks to understand\nsecurity and robustness of LLMs. However, existing jailbreak attack methods\nface two main challenges: (1) an excessive number of iterative queries, and (2)\npoor generalization across models. In addition, recent jailbreak evaluation\ndatasets focus primarily on question-answering scenarios, lacking attention to\ntext generation tasks that require accurate regeneration of toxic content. To\ntackle these challenges, we propose two contributions: (1) ICE, a novel\nblack-box jailbreak method that employs Intent Concealment and divErsion to\neffectively circumvent security constraints. ICE achieves high attack success\nrates (ASR) with a single query, significantly improving efficiency and\ntransferability across different models. (2) BiSceneEval, a comprehensive\ndataset designed for assessing LLM robustness in question-answering and\ntext-generation tasks. Experimental results demonstrate that ICE outperforms\nexisting jailbreak techniques, revealing critical vulnerabilities in current\ndefense mechanisms. Our findings underscore the necessity of a hybrid security\nstrategy that integrates predefined security mechanisms with real-time semantic\ndecomposition to enhance the security of LLMs.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14316v1",
    "published_date": "2025-05-20 13:03:15 UTC",
    "updated_date": "2025-05-20 13:03:15 UTC"
  },
  {
    "arxiv_id": "2505.14312v1",
    "title": "MultiTab: A Comprehensive Benchmark Suite for Multi-Dimensional Evaluation in Tabular Domains",
    "authors": [
      "Kyungeun Lee",
      "Moonjung Eo",
      "Hye-Seung Cho",
      "Dongmin Kim",
      "Ye Seul Sim",
      "Seoyoon Kim",
      "Min-Kook Suh",
      "Woohyung Lim"
    ],
    "abstract": "Despite the widespread use of tabular data in real-world applications, most\nbenchmarks rely on average-case metrics, which fail to reveal how model\nbehavior varies across diverse data regimes. To address this, we propose\nMultiTab, a benchmark suite and evaluation framework for multi-dimensional,\ndata-aware analysis of tabular learning algorithms. Rather than comparing\nmodels only in aggregate, MultiTab categorizes 196 publicly available datasets\nalong key data characteristics, including sample size, label imbalance, and\nfeature interaction, and evaluates 13 representative models spanning a range of\ninductive biases. Our analysis shows that model performance is highly sensitive\nto such regimes: for example, models using sample-level similarity excel on\ndatasets with large sample sizes or high inter-feature correlation, while\nmodels encoding inter-feature dependencies perform best with weakly correlated\nfeatures. These findings reveal that inductive biases do not always behave as\nintended, and that regime-aware evaluation is essential for understanding and\nimproving model behavior. MultiTab enables more principled model design and\noffers practical guidance for selecting models tailored to specific data\ncharacteristics. All datasets, code, and optimization logs are publicly\navailable at https://huggingface.co/datasets/LGAI-DILab/Multitab.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Under review",
    "pdf_url": "http://arxiv.org/pdf/2505.14312v1",
    "published_date": "2025-05-20 13:00:43 UTC",
    "updated_date": "2025-05-20 13:00:43 UTC"
  },
  {
    "arxiv_id": "2505.14300v1",
    "title": "SafetyNet: Detecting Harmful Outputs in LLMs by Modeling and Monitoring Deceptive Behaviors",
    "authors": [
      "Maheep Chaudhary",
      "Fazl Barez"
    ],
    "abstract": "High-risk industries like nuclear and aviation use real-time monitoring to\ndetect dangerous system conditions. Similarly, Large Language Models (LLMs)\nneed monitoring safeguards. We propose a real-time framework to predict harmful\nAI outputs before they occur by using an unsupervised approach that treats\nnormal behavior as the baseline and harmful outputs as outliers. Our study\nfocuses specifically on backdoor-triggered responses -- where specific input\nphrases activate hidden vulnerabilities causing the model to generate unsafe\ncontent like violence, pornography, or hate speech. We address two key\nchallenges: (1) identifying true causal indicators rather than surface\ncorrelations, and (2) preventing advanced models from deception -- deliberately\nevading monitoring systems. Hence, we approach this problem from an\nunsupervised lens by drawing parallels to human deception: just as humans\nexhibit physical indicators while lying, we investigate whether LLMs display\ndistinct internal behavioral signatures when generating harmful content. Our\nstudy addresses two critical challenges: 1) designing monitoring systems that\ncapture true causal indicators rather than superficial correlations; and\n2)preventing intentional evasion by increasingly capable \"Future models''. Our\nfindings show that models can produce harmful content through causal mechanisms\nand can become deceptive by: (a) alternating between linear and non-linear\nrepresentations, and (b) modifying feature relationships. To counter this, we\ndeveloped Safety-Net -- a multi-detector framework that monitors different\nrepresentation dimensions, successfully detecting harmful behavior even when\ninformation is shifted across representational spaces to evade individual\nmonitors. Our evaluation shows 96% accuracy in detecting harmful cases using\nour unsupervised ensemble approach.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14300v1",
    "published_date": "2025-05-20 12:49:58 UTC",
    "updated_date": "2025-05-20 12:49:58 UTC"
  },
  {
    "arxiv_id": "2505.14295v1",
    "title": "Benchmarking data encoding methods in Quantum Machine Learning",
    "authors": [
      "Orlane Zang",
      "GrÃ©goire BarruÃ©",
      "Tony Quertier"
    ],
    "abstract": "Data encoding plays a fundamental and distinctive role in Quantum Machine\nLearning (QML). While classical approaches process data directly as vectors,\nQML may require transforming classical data into quantum states through\nencoding circuits, known as quantum feature maps or quantum embeddings. This\nstep leverages the inherently high-dimensional and non-linear nature of Hilbert\nspace, enabling more efficient data separation in complex feature spaces that\nmay be inaccessible to classical methods. This encoding part significantly\naffects the performance of the QML model, so it is important to choose the\nright encoding method for the dataset to be encoded. However, this choice is\ngenerally arbitrary, since there is no \"universal\" rule for knowing which\nencoding to choose based on a specific set of data. There are currently a\nvariety of encoding methods using different quantum logic gates. We studied the\nmost commonly used types of encoding methods and benchmarked them using\ndifferent datasets.",
    "categories": [
      "quant-ph",
      "cs.AI"
    ],
    "primary_category": "quant-ph",
    "comment": "30 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.14295v1",
    "published_date": "2025-05-20 12:44:14 UTC",
    "updated_date": "2025-05-20 12:44:14 UTC"
  },
  {
    "arxiv_id": "2505.14289v1",
    "title": "EVA: Red-Teaming GUI Agents via Evolving Indirect Prompt Injection",
    "authors": [
      "Yijie Lu",
      "Tianjie Ju",
      "Manman Zhao",
      "Xinbei Ma",
      "Yuan Guo",
      "ZhuoSheng Zhang"
    ],
    "abstract": "As multimodal agents are increasingly trained to operate graphical user\ninterfaces (GUIs) to complete user tasks, they face a growing threat from\nindirect prompt injection, attacks in which misleading instructions are\nembedded into the agent's visual environment, such as popups or chat messages,\nand misinterpreted as part of the intended task. A typical example is\nenvironmental injection, in which GUI elements are manipulated to influence\nagent behavior without directly modifying the user prompt. To address these\nemerging attacks, we propose EVA, a red teaming framework for indirect prompt\ninjection which transforms the attack into a closed loop optimization by\ncontinuously monitoring an agent's attention distribution over the GUI and\nupdating adversarial cues, keywords, phrasing, and layout, in response.\nCompared with prior one shot methods that generate fixed prompts without regard\nfor how the model allocates visual attention, EVA dynamically adapts to\nemerging attention hotspots, yielding substantially higher attack success rates\nand far greater transferability across diverse GUI scenarios. We evaluate EVA\non six widely used generalist and specialist GUI agents in realistic settings\nsuch as popup manipulation, chat based phishing, payments, and email\ncomposition. Experimental results show that EVA substantially improves success\nrates over static baselines. Under goal agnostic constraints, where the\nattacker does not know the agent's task intent, EVA still discovers effective\npatterns. Notably, we find that injection styles transfer well across models,\nrevealing shared behavioral biases in GUI agents. These results suggest that\nevolving indirect prompt injection is a powerful tool not only for red teaming\nagents, but also for uncovering common vulnerabilities in their multimodal\ndecision making.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14289v1",
    "published_date": "2025-05-20 12:41:05 UTC",
    "updated_date": "2025-05-20 12:41:05 UTC"
  },
  {
    "arxiv_id": "2505.14285v1",
    "title": "AquaSignal: An Integrated Framework for Robust Underwater Acoustic Analysis",
    "authors": [
      "Eirini Panteli",
      "Paulo E. Santos",
      "Nabil Humphrey"
    ],
    "abstract": "This paper presents AquaSignal, a modular and scalable pipeline for\npreprocessing, denoising, classification, and novelty detection of underwater\nacoustic signals. Designed to operate effectively in noisy and dynamic marine\nenvironments, AquaSignal integrates state-of-the-art deep learning\narchitectures to enhance the reliability and accuracy of acoustic signal\nanalysis. The system is evaluated on a combined dataset from the Deepship and\nOcean Networks Canada (ONC) benchmarks, providing a diverse set of real-world\nunderwater scenarios. AquaSignal employs a U-Net architecture for denoising, a\nResNet18 convolutional neural network for classifying known acoustic events,\nand an AutoEncoder-based model for unsupervised detection of novel or anomalous\nsignals. To our knowledge, this is the first comprehensive study to apply and\nevaluate this combination of techniques on maritime vessel acoustic data.\nExperimental results show that AquaSignal improves signal clarity and task\nperformance, achieving 71% classification accuracy and 91% accuracy in novelty\ndetection. Despite slightly lower classification performance compared to some\nstate-of-the-art models, differences in data partitioning strategies limit\ndirect comparisons. Overall, AquaSignal demonstrates strong potential for\nreal-time underwater acoustic monitoring in scientific, environmental, and\nmaritime domains.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "8 pages; 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.14285v1",
    "published_date": "2025-05-20 12:35:43 UTC",
    "updated_date": "2025-05-20 12:35:43 UTC"
  },
  {
    "arxiv_id": "2505.14279v1",
    "title": "YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering",
    "authors": [
      "Jennifer D'Souza",
      "Hamed Babaei Giglou",
      "Quentin MÃ¼nch"
    ],
    "abstract": "Large Language Models (LLMs) drive scientific question-answering on modern\nsearch engines, yet their evaluation robustness remains underexplored. We\nintroduce YESciEval, an open-source framework that combines fine-grained\nrubric-based assessment with reinforcement learning to mitigate optimism bias\nin LLM evaluators. We release multidisciplinary scienceQ&A datasets, including\nadversarial variants, with evaluation scores from multiple LLMs. Independent of\nproprietary models and human feedback, our approach enables scalable, cost-free\nevaluation. By advancing reliable LLM-as-a-judge models, this work supports AI\nalignment and fosters robust, transparent evaluation essential for scientific\ninquiry and artificial general intelligence.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages, 3 figures, Accepted as a Long Paper at the 63rd Annual\n  Meeting of the Association for Computational Linguistics (ACL 2025)",
    "pdf_url": "http://arxiv.org/pdf/2505.14279v1",
    "published_date": "2025-05-20 12:30:46 UTC",
    "updated_date": "2025-05-20 12:30:46 UTC"
  },
  {
    "arxiv_id": "2505.14273v1",
    "title": "X-KAN: Optimizing Local Kolmogorov-Arnold Networks via Evolutionary Rule-Based Machine Learning",
    "authors": [
      "Hiroki Shiraishi",
      "Hisao Ishibuchi",
      "Masaya Nakata"
    ],
    "abstract": "Function approximation is a critical task in various fields. However,\nexisting neural network approaches struggle with locally complex or\ndiscontinuous functions due to their reliance on a single global model covering\nthe entire problem space. We propose X-KAN, a novel method that optimizes\nmultiple local Kolmogorov-Arnold Networks (KANs) through an evolutionary\nrule-based machine learning framework called XCSF. X-KAN combines KAN's high\nexpressiveness with XCSF's adaptive partitioning capability by implementing\nlocal KAN models as rule consequents and defining local regions via rule\nantecedents. Our experimental results on artificial test functions and\nreal-world datasets demonstrate that X-KAN significantly outperforms\nconventional methods, including XCSF, Multi-Layer Perceptron, and KAN, in terms\nof approximation accuracy. Notably, X-KAN effectively handles functions with\nlocally complex or discontinuous structures that are challenging for\nconventional KAN, using a compact set of rules (average 7.2 $\\pm$ 2.3 rules).\nThese results validate the effectiveness of using KAN as a local model in XCSF,\nwhich evaluates the rule fitness based on both accuracy and generality. Our\nX-KAN implementation is available at https://github.com/YNU-NakataLab/X-KAN.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE",
      "cs.SC"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by the 34th International Joint Conference on Artificial\n  Intelligence (IJCAI 2025)",
    "pdf_url": "http://arxiv.org/pdf/2505.14273v1",
    "published_date": "2025-05-20 12:26:03 UTC",
    "updated_date": "2025-05-20 12:26:03 UTC"
  },
  {
    "arxiv_id": "2505.14268v1",
    "title": "Think-J: Learning to Think for Generative LLM-as-a-Judge",
    "authors": [
      "Hui Huang",
      "Yancheng He",
      "Hongli Zhou",
      "Rui Zhang",
      "Wei Liu",
      "Weixun Wang",
      "Wenbo Su",
      "Bo Zheng",
      "Jiaheng Liu"
    ],
    "abstract": "LLM-as-a-Judge refers to the automatic modeling of preferences for responses\ngenerated by Large Language Models (LLMs), which is of significant importance\nfor both LLM evaluation and reward modeling. Although generative LLMs have made\nsubstantial progress in various tasks, their performance as LLM-Judge still\nfalls short of expectations. In this work, we propose Think-J, which improves\ngenerative LLM-as-a-Judge by learning how to think. We first utilized a small\namount of curated data to develop the model with initial judgment thinking\ncapabilities. Subsequently, we optimize the judgment thinking traces based on\nreinforcement learning (RL). We propose two methods for judgment thinking\noptimization, based on offline and online RL, respectively. The offline RL\nrequires training a critic model to construct positive and negative examples\nfor learning. The online method defines rule-based reward as feedback for\noptimization. Experimental results showed that our approach can significantly\nenhance the evaluation capability of generative LLM-Judge, surpassing both\ngenerative and classifier-based LLM-Judge without requiring extra human\nannotations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "16 pages, 14 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.14268v1",
    "published_date": "2025-05-20 12:19:10 UTC",
    "updated_date": "2025-05-20 12:19:10 UTC"
  },
  {
    "arxiv_id": "2505.14260v1",
    "title": "Speculative Decoding Reimagined for Multimodal Large Language Models",
    "authors": [
      "Luxi Lin",
      "Zhihang Lin",
      "Zhanpeng Zeng",
      "Rongrong Ji"
    ],
    "abstract": "This paper introduces Multimodal Speculative Decoding (MSD) to accelerate\nMultimodal Large Language Models (MLLMs) inference. Speculative decoding has\nbeen shown to accelerate Large Language Models (LLMs) without sacrificing\naccuracy. However, current speculative decoding methods for MLLMs fail to\nachieve the same speedup as they do for LLMs. To address this, we reimagine\nspeculative decoding specifically for MLLMs. Our analysis of MLLM\ncharacteristics reveals two key design principles for MSD: (1) Text and visual\ntokens have fundamentally different characteristics and need to be processed\nseparately during drafting. (2) Both language modeling ability and visual\nperception capability are crucial for the draft model. For the first principle,\nMSD decouples text and visual tokens in the draft model, allowing each to be\nhandled based on its own characteristics. For the second principle, MSD uses a\ntwo-stage training strategy: In stage one, the draft model is trained on\ntext-only instruction-tuning datasets to improve its language modeling ability.\nIn stage two, MSD gradually introduces multimodal data to enhance the visual\nperception capability of the draft model. Experiments show that MSD boosts\ninference speed by up to $2.29\\times$ for LLaVA-1.5-7B and up to $2.46\\times$\nfor LLaVA-1.5-13B on multimodal benchmarks, demonstrating its effectiveness.\nOur code is available at https://github.com/Lyn-Lucy/MSD.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages",
    "pdf_url": "http://arxiv.org/pdf/2505.14260v1",
    "published_date": "2025-05-20 12:12:17 UTC",
    "updated_date": "2025-05-20 12:12:17 UTC"
  },
  {
    "arxiv_id": "2505.14256v1",
    "title": "FuxiMT: Sparsifying Large Language Models for Chinese-Centric Multilingual Machine Translation",
    "authors": [
      "Shaolin Zhu",
      "Tianyu Dong",
      "Bo Li",
      "Deyi Xiong"
    ],
    "abstract": "In this paper, we present FuxiMT, a novel Chinese-centric multilingual\nmachine translation model powered by a sparsified large language model (LLM).\nWe adopt a two-stage strategy to train FuxiMT. We first pre-train the model on\na massive Chinese corpus and then conduct multilingual fine-tuning on a large\nparallel dataset encompassing 65 languages. FuxiMT incorporates\nMixture-of-Experts (MoEs) and employs a curriculum learning strategy for robust\nperformance across various resource levels. Experimental results demonstrate\nthat FuxiMT significantly outperforms strong baselines, including\nstate-of-the-art LLMs and machine translation models, particularly under\nlow-resource scenarios. Furthermore, FuxiMT exhibits remarkable zero-shot\ntranslation capabilities for unseen language pairs, indicating its potential to\nbridge communication gaps where parallel data are scarce or unavailable.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14256v1",
    "published_date": "2025-05-20 12:09:17 UTC",
    "updated_date": "2025-05-20 12:09:17 UTC"
  },
  {
    "arxiv_id": "2505.14252v1",
    "title": "Hybrid Adaptive Modeling in Process Monitoring: Leveraging Sequence Encoders and Physics-Informed Neural Networks",
    "authors": [
      "Mouad Elaarabi",
      "Domenico Borzacchiello",
      "Philippe Le Bot",
      "Nathan Lauzeral",
      "Sebastien Comas-Cardona"
    ],
    "abstract": "In this work, we explore the integration of Sequence Encoding for Online\nParameter Identification with Physics-Informed Neural Networks to create a\nmodel that, once trained, can be utilized for real time applications with\nvariable parameters, boundary conditions, and initial conditions. Recently, the\ncombination of PINNs with Sparse Regression has emerged as a method for\nperforming dynamical system identification through supervised learning and\nsparse regression optimization, while also solving the dynamics using PINNs.\nHowever, this approach can be limited by variations in parameters or boundary\nand initial conditions, requiring retraining of the model whenever changes\noccur. In this work, we introduce an architecture that employs Deep Sets or\nSequence Encoders to encode dynamic parameters, boundary conditions, and\ninitial conditions, using these encoded features as inputs for the PINN,\nenabling the model to adapt to changes in parameters, BCs, and ICs. We apply\nthis approach to three different problems. First, we analyze the Rossler ODE\nsystem, demonstrating the robustness of the model with respect to noise and its\nability to generalize. Next, we explore the model's capability in a 2D\nNavier-Stokes PDE problem involving flow past a cylinder with a parametric\nsinusoidal inlet velocity function, showing that the model can encode pressure\ndata from a few points to identify the inlet velocity profile and utilize\nphysics to compute velocity and pressure throughout the domain. Finally, we\naddress a 1D heat monitoring problem using real data from the heating of glass\nfiber and thermoplastic composite plates.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14252v1",
    "published_date": "2025-05-20 12:05:17 UTC",
    "updated_date": "2025-05-20 12:05:17 UTC"
  },
  {
    "arxiv_id": "2505.14246v1",
    "title": "Visual Agentic Reinforcement Fine-Tuning",
    "authors": [
      "Ziyu Liu",
      "Yuhang Zang",
      "Yushan Zou",
      "Zijian Liang",
      "Xiaoyi Dong",
      "Yuhang Cao",
      "Haodong Duan",
      "Dahua Lin",
      "Jiaqi Wang"
    ],
    "abstract": "A key trend in Large Reasoning Models (e.g., OpenAI's o3) is the native\nagentic ability to use external tools such as web browsers for searching and\nwriting/executing code for image manipulation to think with images. In the\nopen-source research community, while significant progress has been made in\nlanguage-only agentic abilities such as function calling and tool integration,\nthe development of multi-modal agentic capabilities that involve truly thinking\nwith images, and their corresponding benchmarks, are still less explored. This\nwork highlights the effectiveness of Visual Agentic Reinforcement Fine-Tuning\n(Visual-ARFT) for enabling flexible and adaptive reasoning abilities for Large\nVision-Language Models (LVLMs). With Visual-ARFT, open-source LVLMs gain the\nability to browse websites for real-time information updates and write code to\nmanipulate and analyze input images through cropping, rotation, and other image\nprocessing techniques. We also present a Multi-modal Agentic Tool Bench (MAT)\nwith two settings (MAT-Search and MAT-Coding) designed to evaluate LVLMs'\nagentic search and coding abilities. Our experimental results demonstrate that\nVisual-ARFT outperforms its baseline by +18.6% F1 / +13.0% EM on MAT-Coding and\n+10.3% F1 / +8.7% EM on MAT-Search, ultimately surpassing GPT-4o. Visual-ARFT\nalso achieves +29.3 F1% / +25.9% EM gains on existing multi-hop QA benchmarks\nsuch as 2Wiki and HotpotQA, demonstrating strong generalization capabilities.\nOur findings suggest that Visual-ARFT offers a promising path toward building\nrobust and generalizable multimodal agents.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "project url:\n  https://github.com/Liuziyu77/Visual-RFT/tree/main/Visual-ARFT",
    "pdf_url": "http://arxiv.org/pdf/2505.14246v1",
    "published_date": "2025-05-20 11:59:25 UTC",
    "updated_date": "2025-05-20 11:59:25 UTC"
  },
  {
    "arxiv_id": "2505.14238v1",
    "title": "ABBA: Highly Expressive Hadamard Product Adaptation for Large Language Models",
    "authors": [
      "Raghav Singhal",
      "Kaustubh Ponkshe",
      "Rohit Vartak",
      "Praneeth Vepakomma"
    ],
    "abstract": "Large Language Models have demonstrated strong performance across a wide\nrange of tasks, but adapting them efficiently to new domains remains a key\nchallenge. Parameter-Efficient Fine-Tuning (PEFT) methods address this by\nintroducing lightweight, trainable modules while keeping most pre-trained\nweights fixed. The prevailing approach, LoRA, models updates using a low-rank\ndecomposition, but its expressivity is inherently constrained by the rank.\nRecent methods like HiRA aim to increase expressivity by incorporating a\nHadamard product with the frozen weights, but still rely on the structure of\nthe pre-trained model. We introduce ABBA, a new PEFT architecture that\nreparameterizes the update as a Hadamard product of two independently learnable\nlow-rank matrices. In contrast to prior work, ABBA fully decouples the update\nfrom the pre-trained weights, enabling both components to be optimized freely.\nThis leads to significantly higher expressivity under the same parameter\nbudget. We formally analyze ABBA's expressive capacity and validate its\nadvantages through matrix reconstruction experiments. Empirically, ABBA\nachieves state-of-the-art results on arithmetic and commonsense reasoning\nbenchmarks, consistently outperforming existing PEFT methods by a significant\nmargin across multiple models. Our code is publicly available at:\nhttps://github.com/CERT-Lab/abba.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Raghav Singhal, Kaustubh Ponkshe, and Rohit Vartak contributed\n  equally to this work",
    "pdf_url": "http://arxiv.org/pdf/2505.14238v1",
    "published_date": "2025-05-20 11:43:25 UTC",
    "updated_date": "2025-05-20 11:43:25 UTC"
  },
  {
    "arxiv_id": "2505.14235v1",
    "title": "Toward Embodied AGI: A Review of Embodied AI and the Road Ahead",
    "authors": [
      "Yequan Wang",
      "Aixin Sun"
    ],
    "abstract": "Artificial General Intelligence (AGI) is often envisioned as inherently\nembodied. With recent advances in robotics and foundational AI models, we stand\nat the threshold of a new era-one marked by increasingly generalized embodied\nAI systems. This paper contributes to the discourse by introducing a systematic\ntaxonomy of Embodied AGI spanning five levels (L1-L5). We review existing\nresearch and challenges at the foundational stages (L1-L2) and outline the key\ncomponents required to achieve higher-level capabilities (L3-L5). Building on\nthese insights and existing technologies, we propose a conceptual framework for\nan L3+ robotic brain, offering both a technical outlook and a foundation for\nfuture exploration.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14235v1",
    "published_date": "2025-05-20 11:42:26 UTC",
    "updated_date": "2025-05-20 11:42:26 UTC"
  },
  {
    "arxiv_id": "2505.14234v1",
    "title": "Fast and close Shannon entropy approximation",
    "authors": [
      "Illia Horenko",
      "Davide Bassetti",
      "LukÃ¡Å¡ PospÃ­Å¡il"
    ],
    "abstract": "Shannon entropy (SE) and its quantum mechanical analogue von Neumann entropy\nare key components in many tools used in physics, information theory, machine\nlearning (ML) and quantum computing. Besides of the significant amounts of SE\ncomputations required in these fields, the singularity of the SE gradient is\none of the central mathematical reason inducing the high cost, frequently low\nrobustness and slow convergence of such tools. Here we propose the Fast Entropy\nApproximation (FEA) - a non-singular rational approximation of Shannon entropy\nand its gradient that achieves a mean absolute error of $10^{-3}$, which is\napproximately $20$ times lower than comparable state-of-the-art methods. FEA\nallows around $50\\%$ faster computation, requiring only $5$ to $6$ elementary\ncomputational operations, as compared to tens of elementary operations behind\nthe fastest entropy computation algorithms with table look-ups, bitshifts, or\nseries approximations. On a set of common benchmarks for the feature selection\nproblem in machine learning, we show that the combined effect of fewer\nelementary operations, low approximation error, and a non-singular gradient\nallows significantly better model quality and enables ML feature extraction\nthat is two to three orders of magnitude faster and computationally cheaper\nwhen incorporating FEA into AI tools.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "68T01 (Primary) 68Q01, 90C99 (Secondary)"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2505.14234v1",
    "published_date": "2025-05-20 11:41:26 UTC",
    "updated_date": "2025-05-20 11:41:26 UTC"
  },
  {
    "arxiv_id": "2505.14233v1",
    "title": "Mechanistic Fine-tuning for In-context Learning",
    "authors": [
      "Hakaze Cho",
      "Peng Luo",
      "Mariko Kato",
      "Rin Kaenbyou",
      "Naoya Inoue"
    ],
    "abstract": "In-context Learning (ICL) utilizes structured demonstration-query inputs to\ninduce few-shot learning on Language Models (LMs), which are not originally\npre-trained on ICL-style data. To bridge the gap between ICL and pre-training,\nsome approaches fine-tune LMs on large ICL-style datasets by an end-to-end\nparadigm with massive computational costs. To reduce such costs, in this paper,\nwe propose Attention Behavior Fine-Tuning (ABFT), utilizing the previous\nfindings on the inner mechanism of ICL, building training objectives on the\nattention scores instead of the final outputs, to force the attention scores to\nfocus on the correct label tokens presented in the context and mitigate\nattention scores from the wrong label tokens. Our experiments on 9 modern LMs\nand 8 datasets empirically find that ABFT outperforms in performance,\nrobustness, unbiasedness, and efficiency, with only around 0.01% data cost\ncompared to the previous methods. Moreover, our subsequent analysis finds that\nthe end-to-end training objective contains the ABFT objective, suggesting the\nimplicit bias of ICL-style data to the emergence of induction heads. Our work\ndemonstrates the possibility of controlling specific module sequences within\nLMs to improve their behavior, opening up the future application of mechanistic\ninterpretability.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "28 pages, 31 figures, 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2505.14233v1",
    "published_date": "2025-05-20 11:41:21 UTC",
    "updated_date": "2025-05-20 11:41:21 UTC"
  },
  {
    "arxiv_id": "2505.14227v1",
    "title": "VoQA: Visual-only Question Answering",
    "authors": [
      "Luyang Jiang",
      "Jianing An",
      "Jie Luo",
      "Wenjun Wu",
      "Lei Huang"
    ],
    "abstract": "We propose Visual-only Question Answering (VoQA), a novel multimodal task in\nwhich questions are visually embedded within images, without any accompanying\ntextual input. This requires models to locate, recognize, and reason over\nvisually embedded textual questions, posing challenges for existing large\nvision-language models (LVLMs), which show notable performance drops even with\ncarefully designed prompts. To bridge this gap, we introduce Guided Response\nTriggering Supervised Fine-tuning (GRT-SFT), a structured fine-tuning strategy\nthat guides the model to perform step-by-step reasoning purely based on visual\ninput, significantly improving model performance. Our work enhances models'\ncapacity for human-like visual understanding in complex multimodal scenarios,\nwhere information, including language, is perceived visually.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "18 pages",
    "pdf_url": "http://arxiv.org/pdf/2505.14227v1",
    "published_date": "2025-05-20 11:37:49 UTC",
    "updated_date": "2025-05-20 11:37:49 UTC"
  },
  {
    "arxiv_id": "2505.14226v1",
    "title": "\"Haet Bhasha aur Diskrimineshun\": Phonetic Perturbations in Code-Mixed Hinglish to Red-Team LLMs",
    "authors": [
      "Darpan Aswal",
      "Siddharth D Jaiswal"
    ],
    "abstract": "Large Language Models (LLMs) have become increasingly powerful, with\nmultilingual and multimodal capabilities improving by the day. These models are\nbeing evaluated through audits, alignment studies and red-teaming efforts to\nexpose model vulnerabilities towards generating harmful, biased and unfair\ncontent. Existing red-teaming efforts have previously focused on the English\nlanguage, using fixed template-based attacks; thus, models continue to be\nsusceptible to multilingual jailbreaking strategies, especially in the\nmultimodal context. In this study, we introduce a novel strategy that leverages\ncode-mixing and phonetic perturbations to jailbreak LLMs for both text and\nimage generation tasks. We also introduce two new jailbreak strategies that\nshow higher effectiveness than baseline strategies. Our work presents a method\nto effectively bypass safety filters in LLMs while maintaining interpretability\nby applying phonetic misspellings to sensitive words in code-mixed prompts. Our\nnovel prompts achieve a 99% Attack Success Rate for text generation and 78% for\nimage generation, with Attack Relevance Rate of 100% for text generation and\n95% for image generation when using the phonetically perturbed code-mixed\nprompts. Our interpretability experiments reveal that phonetic perturbations\nimpact word tokenization, leading to jailbreak success. Our study motivates\nincreasing the focus towards more generalizable safety alignment for\nmultilingual multimodal models, especially in real-world settings wherein\nprompts can have misspelt words.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14226v1",
    "published_date": "2025-05-20 11:35:25 UTC",
    "updated_date": "2025-05-20 11:35:25 UTC"
  },
  {
    "arxiv_id": "2505.14217v1",
    "title": "Federated learning in low-resource settings: A chest imaging study in Africa -- Challenges and lessons learned",
    "authors": [
      "Jorge Fabila",
      "Lidia Garrucho",
      "VÃ­ctor M. Campello",
      "Carlos MartÃ­n-Isla",
      "Karim Lekadir"
    ],
    "abstract": "This study explores the use of Federated Learning (FL) for tuberculosis (TB)\ndiagnosis using chest X-rays in low-resource settings across Africa. FL allows\nhospitals to collaboratively train AI models without sharing raw patient data,\naddressing privacy concerns and data scarcity that hinder traditional\ncentralized models. The research involved hospitals and research centers in\neight African countries. Most sites used local datasets, while Ghana and The\nGambia used public ones. The study compared locally trained models with a\nfederated model built across all institutions to evaluate FL's real-world\nfeasibility. Despite its promise, implementing FL in sub-Saharan Africa faces\nchallenges such as poor infrastructure, unreliable internet, limited digital\nliteracy, and weak AI regulations. Some institutions were also reluctant to\nshare model updates due to data control concerns. In conclusion, FL shows\nstrong potential for enabling AI-driven healthcare in underserved regions, but\nbroader adoption will require improvements in infrastructure, education, and\nregulatory support.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14217v1",
    "published_date": "2025-05-20 11:23:52 UTC",
    "updated_date": "2025-05-20 11:23:52 UTC"
  },
  {
    "arxiv_id": "2505.14216v1",
    "title": "Reinforcement Learning vs. Distillation: Understanding Accuracy and Capability in LLM Reasoning",
    "authors": [
      "Minwu Kim",
      "Anubhav Shrestha",
      "Safal Shrestha",
      "Aadim Nepal",
      "Keith Ross"
    ],
    "abstract": "Recent studies have shown that reinforcement learning with verifiable rewards\n(RLVR) enhances overall accuracy but fails to improve capability, while\ndistillation can improve both. In this paper, we investigate the mechanisms\nbehind these phenomena. First, we demonstrate that RLVR does not improve\ncapability because it focuses on improving the accuracy of the less-difficult\nquestions to the detriment of the accuracy of the most difficult questions,\nthereby leading to no improvement in capability. Second, we find that RLVR does\nnot merely increase the success probability for the less difficult questions,\nbut in our small model settings produces quality responses that were absent in\nits output distribution before training. In addition, we show these responses\nare neither noticeably longer nor feature more reflection-related keywords,\nunderscoring the need for more reliable indicators of response quality. Third,\nwe show that while distillation reliably improves accuracy by learning strong\nreasoning patterns, it only improves capability when new knowledge is\nintroduced. Moreover, when distilling only with reasoning patterns and no new\nknowledge, the accuracy of the less-difficult questions improves to the\ndetriment of the most difficult questions, similar to RLVR. Together, these\nfindings offer a clearer understanding of how RLVR and distillation shape\nreasoning behavior in language models.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "23 pages",
    "pdf_url": "http://arxiv.org/pdf/2505.14216v1",
    "published_date": "2025-05-20 11:22:34 UTC",
    "updated_date": "2025-05-20 11:22:34 UTC"
  },
  {
    "arxiv_id": "2505.14212v1",
    "title": "Automatic Dataset Generation for Knowledge Intensive Question Answering Tasks",
    "authors": [
      "Sizhe Yuen",
      "Ting Su",
      "Ziyang Wang",
      "Yali Du",
      "Adam J. Sobey"
    ],
    "abstract": "A question-answering (QA) system is to search suitable answers within a\nknowledge base. Current QA systems struggle with queries requiring complex\nreasoning or real-time knowledge integration. They are often supplemented with\nretrieval techniques on a data source such as Retrieval-Augmented Generation\n(RAG). However, RAG continues to face challenges in handling complex reasoning\nand logical connections between multiple sources of information. A novel\napproach for enhancing Large Language Models (LLMs) in knowledge-intensive QA\ntasks is presented through the automated generation of context-based QA pairs.\nThis methodology leverages LLMs to create fine-tuning data, reducing reliance\non human labelling and improving model comprehension and reasoning\ncapabilities. The proposed system includes an automated QA generator and a\nmodel fine-tuner, evaluated using perplexity, ROUGE, BLEU, and BERTScore.\nComprehensive experiments demonstrate improvements in logical coherence and\nfactual accuracy, with implications for developing adaptable Artificial\nIntelligence (AI) systems. Mistral-7b-v0.3 outperforms Llama-3-8b with BERT F1,\nBLEU, and ROUGE scores 0.858, 0.172, and 0.260 of for the LLM generated QA\npairs compared to scores of 0.836, 0.083, and 0.139 for the human annotated QA\npairs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14212v1",
    "published_date": "2025-05-20 11:16:29 UTC",
    "updated_date": "2025-05-20 11:16:29 UTC"
  },
  {
    "arxiv_id": "2505.14209v1",
    "title": "Embedded Mean Field Reinforcement Learning for Perimeter-defense Game",
    "authors": [
      "Li Wang",
      "Xin Yu",
      "Xuxin Lv",
      "Gangzheng Ai",
      "Wenjun Wu"
    ],
    "abstract": "With the rapid advancement of unmanned aerial vehicles (UAVs) and missile\ntechnologies, perimeter-defense game between attackers and defenders for the\nprotection of critical regions have become increasingly complex and\nstrategically significant across a wide range of domains. However, existing\nstudies predominantly focus on small-scale, simplified two-dimensional\nscenarios, often overlooking realistic environmental perturbations, motion\ndynamics, and inherent heterogeneity--factors that pose substantial challenges\nto real-world applicability. To bridge this gap, we investigate large-scale\nheterogeneous perimeter-defense game in a three-dimensional setting,\nincorporating realistic elements such as motion dynamics and wind fields. We\nderive the Nash equilibrium strategies for both attackers and defenders,\ncharacterize the victory regions, and validate our theoretical findings through\nextensive simulations. To tackle large-scale heterogeneous control challenges\nin defense strategies, we propose an Embedded Mean-Field Actor-Critic (EMFAC)\nframework. EMFAC leverages representation learning to enable high-level action\naggregation in a mean-field manner, supporting scalable coordination among\ndefenders. Furthermore, we introduce a lightweight agent-level attention\nmechanism based on reward representation, which selectively filters\nobservations and mean-field information to enhance decision-making efficiency\nand accelerate convergence in large-scale tasks. Extensive simulations across\nvarying scales demonstrate the effectiveness and adaptability of EMFAC, which\noutperforms established baselines in both convergence speed and overall\nperformance. To further validate practicality, we test EMFAC in small-scale\nreal-world experiments and conduct detailed analyses, offering deeper insights\ninto the framework's effectiveness in complex scenarios.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14209v1",
    "published_date": "2025-05-20 11:11:46 UTC",
    "updated_date": "2025-05-20 11:11:46 UTC"
  },
  {
    "arxiv_id": "2505.14206v1",
    "title": "Challenges and Limitations in the Synthetic Generation of mHealth Sensor Data",
    "authors": [
      "Flavio Di Martino",
      "Franca Delmastro"
    ],
    "abstract": "The widespread adoption of mobile sensors has the potential to provide\nmassive and heterogeneous time series data, driving Artificial Intelligence\napplications in mHealth. However, data collection remains limited due to\nstringent ethical regulations, privacy concerns, and other constraints,\nhindering progress in the field. Synthetic data generation, particularly\nthrough Generative Adversarial Networks and Diffusion Models, has emerged as a\npromising solution to address both data scarcity and privacy issues. Yet, these\nmodels are often limited to short-term, unimodal signal patterns. This paper\npresents a systematic evaluation of state-of-the-art generative models for time\nseries synthesis, with a focus on their ability to jointly handle\nmulti-modality, long-range dependencies, and conditional generation-key\nchallenges in the mHealth domain. To ensure a fair comparison, we introduce a\nnovel evaluation framework designed to measure both the intrinsic quality of\nsynthetic data and its utility in downstream predictive tasks. Our findings\nreveal critical limitations in the existing approaches, particularly in\nmaintaining cross-modal consistency, preserving temporal coherence, and\nensuring robust performance in train-on-synthetic, test-on-real, and data\naugmentation scenarios. Finally, we present our future research directions to\nenhance synthetic time series generation and improve the applicability of\ngenerative models in mHealth.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Submitted to ACM Transactions on Computing for Healthcare (ACM\n  HEALTH)",
    "pdf_url": "http://arxiv.org/pdf/2505.14206v1",
    "published_date": "2025-05-20 11:05:06 UTC",
    "updated_date": "2025-05-20 11:05:06 UTC"
  },
  {
    "arxiv_id": "2505.14201v1",
    "title": "FLASH-D: FlashAttention with Hidden Softmax Division",
    "authors": [
      "Kosmas Alexandridis",
      "Vasileios Titopoulos",
      "Giorgos Dimitrakopoulos"
    ],
    "abstract": "The transformer's attention mechanism has revolutionized AI and machine\nlearning, with its efficient computation being crucial to its performance.\nHowever, calculating attention involves matrix operations interspersed with\nsoftmax rescaling, which inherently slows down computation and requires\nprocessing the entire input sequence. Building on online softmax computation,\nFlashAttention integrates softmax calculation with matrix arithmetic, enabling\ntiled computation independent of sequence length. While optimized for GPUs,\nFlashAttention's simplicity makes it amenable to direct hardware acceleration.\nThis work re-evaluates the core FlashAttention kernel, presenting FLASH-D a\nmathematically equivalent, yet simplified, formulation that achieves: (a)\nhiding softmax division within other non-linear function evaluations; (b)\ninherently numerically stable computation of exponentials, eliminating the need\nfor maximum value subtraction; and (c) a reduction in computational cost\nwithout introducing numerical approximations to the FlashAttention kernel.\nImportantly, the essential FlashAttention properties that facilitate efficient\ntiled implementation are fully preserved. Hardware implementation results at\n28nm demonstrate that this proposed formulation achieves a 22.8% reduction in\narea and a 20.3% reduction in power, on average, compared to state-of-the-art\nparallel hardware architectures without any performance penalty.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.LG",
    "comment": "IEEE/ACM International Symposium on Low Power Electronics and Design\n  (ISLPED) 2025",
    "pdf_url": "http://arxiv.org/pdf/2505.14201v1",
    "published_date": "2025-05-20 11:01:33 UTC",
    "updated_date": "2025-05-20 11:01:33 UTC"
  },
  {
    "arxiv_id": "2505.15854v1",
    "title": "Integration of TinyML and LargeML: A Survey of 6G and Beyond",
    "authors": [
      "Thai-Hoc Vu",
      "Ngo Hoang Tu",
      "Thien Huynh-The",
      "Kyungchun Lee",
      "Sunghwan Kim",
      "Miroslav Voznak",
      "Quoc-Viet Pham"
    ],
    "abstract": "The transition from 5G networks to 6G highlights a significant demand for\nmachine learning (ML). Deep learning models, in particular, have seen wide\napplication in mobile networking and communications to support advanced\nservices in emerging wireless environments, such as smart healthcare, smart\ngrids, autonomous vehicles, aerial platforms, digital twins, and the metaverse.\nThe rapid expansion of Internet-of-Things (IoT) devices, many with limited\ncomputational capabilities, has accelerated the development of tiny machine\nlearning (TinyML) and resource-efficient ML approaches for cost-effective\nservices. However, the deployment of large-scale machine learning (LargeML)\nsolutions require major computing resources and complex management strategies\nto support extensive IoT services and ML-generated content applications.\nConsequently, the integration of TinyML and LargeML is projected as a promising\napproach for future seamless connectivity and efficient resource management.\n  Although the integration of TinyML and LargeML shows abundant potential,\nseveral challenges persist, including performance optimization, practical\ndeployment strategies, effective resource management, and security\nconsiderations. In this survey, we review and analyze the latest research aimed\nat enabling the integration of TinyML and LargeML models for the realization of\nsmart services and applications in future 6G networks and beyond. The paper\nconcludes by outlining critical challenges and identifying future research\ndirections for the holistic integration of TinyML and LargeML in\nnext-generation wireless networks.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.ET",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.NI",
    "comment": "This work was submitted to IEEE Communications Surveys & Tutorials",
    "pdf_url": "http://arxiv.org/pdf/2505.15854v1",
    "published_date": "2025-05-20 10:54:39 UTC",
    "updated_date": "2025-05-20 10:54:39 UTC"
  },
  {
    "arxiv_id": "2505.14193v1",
    "title": "Dynamic Replanning for Improved Public Transport Routing",
    "authors": [
      "Abdallah Abuaisha",
      "Bojie Shen",
      "Daniel Harabor",
      "Peter Stuckey",
      "Mark Wallace"
    ],
    "abstract": "Delays in public transport are common, often impacting users through\nprolonged travel times and missed transfers. Existing solutions for handling\ndelays remain limited; backup plans based on historical data miss opportunities\nfor earlier arrivals, while snapshot planning accounts for current delays but\nnot future ones. With the growing availability of live delay data, users can\nadjust their journeys in real-time. However, the literature lacks a framework\nthat fully exploits this advantage for system-scale dynamic replanning. To\naddress this, we formalise the dynamic replanning problem in public transport\nrouting and propose two solutions: a \"pull\" approach, where users manually\nrequest replanning, and a novel \"push\" approach, where the server proactively\nmonitors and adjusts journeys. Our experiments show that the push approach\noutperforms the pull approach, achieving significant speedups. The results also\nreveal substantial arrival time savings enabled by dynamic replanning.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted for publication at IJCAI 2025. 8 pages, 4 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2505.14193v1",
    "published_date": "2025-05-20 10:50:58 UTC",
    "updated_date": "2025-05-20 10:50:58 UTC"
  },
  {
    "arxiv_id": "2505.14190v1",
    "title": "$Î±$-GAN by RÃ©nyi Cross Entropy",
    "authors": [
      "Ni Ding",
      "Miao Qiao",
      "Jiaxing Xu",
      "Yiping Ke",
      "Xiaoyu Zhang"
    ],
    "abstract": "This paper proposes $\\alpha$-GAN, a generative adversarial network using\nR\\'{e}nyi measures. The value function is formulated, by R\\'{e}nyi cross\nentropy, as an expected certainty measure incurred by the discriminator's soft\ndecision as to where the sample is from, true population or the generator. The\ndiscriminator tries to maximize the R\\'{e}nyi certainty about sample source,\nwhile the generator wants to reduce it by injecting fake samples. This forms a\nmin-max problem with the solution parameterized by the R\\'{e}nyi order\n$\\alpha$. This $\\alpha$-GAN reduces to vanilla GAN at $\\alpha = 1$, where the\nvalue function is exactly the binary cross entropy. The optimization of\n$\\alpha$-GAN is over probability (vector) space. It is shown that the gradient\nis exponentially enlarged when R\\'{e}nyi order is in the range $\\alpha \\in\n(0,1)$. This makes convergence faster, which is verified by experimental\nresults. A discussion shows that choosing $\\alpha \\in (0,1)$ may be able to\nsolve some common problems, e.g., vanishing gradient. A following observation\nreveals that this range has not been fully explored in the existing R\\'{e}nyi\nversion GANs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14190v1",
    "published_date": "2025-05-20 10:45:11 UTC",
    "updated_date": "2025-05-20 10:45:11 UTC"
  },
  {
    "arxiv_id": "2505.14185v1",
    "title": "Safety Subspaces are Not Distinct: A Fine-Tuning Case Study",
    "authors": [
      "Kaustubh Ponkshe",
      "Shaan Shah",
      "Raghav Singhal",
      "Praneeth Vepakomma"
    ],
    "abstract": "Large Language Models (LLMs) rely on safety alignment to produce socially\nacceptable responses. This is typically achieved through instruction tuning and\nreinforcement learning from human feedback. However, this alignment is known to\nbe brittle: further fine-tuning, even on benign or lightly contaminated data,\ncan degrade safety and reintroduce harmful behaviors. A growing body of work\nsuggests that alignment may correspond to identifiable geometric directions in\nweight space, forming subspaces that could, in principle, be isolated or\npreserved to defend against misalignment. In this work, we conduct a\ncomprehensive empirical study of this geometric perspective. We examine whether\nsafety-relevant behavior is concentrated in specific subspaces, whether it can\nbe separated from general-purpose learning, and whether harmfulness arises from\ndistinguishable patterns in internal representations. Across both parameter and\nactivation space, our findings are consistent: subspaces that amplify safe\nbehaviors also amplify unsafe ones, and prompts with different safety\nimplications activate overlapping representations. We find no evidence of a\nsubspace that selectively governs safety. These results challenge the\nassumption that alignment is geometrically localized. Rather than residing in\ndistinct directions, safety appears to emerge from entangled, high-impact\ncomponents of the model's broader learning dynamics. This suggests that\nsubspace-based defenses may face fundamental limitations and underscores the\nneed for alternative strategies to preserve alignment under continued training.\nWe corroborate these findings through multiple experiments on five open-source\nLLMs. Our code is publicly available at:\nhttps://github.com/CERT-Lab/safety-subspaces.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Kaustubh Ponkshe, Shaan Shah, and Raghav Singhal contributed equally\n  to this work",
    "pdf_url": "http://arxiv.org/pdf/2505.14185v1",
    "published_date": "2025-05-20 10:41:49 UTC",
    "updated_date": "2025-05-20 10:41:49 UTC"
  },
  {
    "arxiv_id": "2505.14179v1",
    "title": "Enhancing Abstractive Summarization of Scientific Papers Using Structure Information",
    "authors": [
      "Tong Bao",
      "Heng Zhang",
      "Chengzhi Zhang"
    ],
    "abstract": "Abstractive summarization of scientific papers has always been a research\nfocus, yet existing methods face two main challenges. First, most summarization\nmodels rely on Encoder-Decoder architectures that treat papers as sequences of\nwords, thus fail to fully capture the structured information inherent in\nscientific papers. Second, existing research often use keyword mapping or\nfeature engineering to identify the structural information, but these methods\nstruggle with the structural flexibility of scientific papers and lack\nrobustness across different disciplines. To address these challenges, we\npropose a two-stage abstractive summarization framework that leverages\nautomatic recognition of structural functions within scientific papers. In the\nfirst stage, we standardize chapter titles from numerous scientific papers and\nconstruct a large-scale dataset for structural function recognition. A\nclassifier is then trained to automatically identify the key structural\ncomponents (e.g., Background, Methods, Results, Discussion), which provides a\nfoundation for generating more balanced summaries. In the second stage, we\nemploy Longformer to capture rich contextual relationships across sections and\ngenerating context-aware summaries. Experiments conducted on two\ndomain-specific scientific paper summarization datasets demonstrate that our\nmethod outperforms advanced baselines, and generates more comprehensive\nsummaries. The code and dataset can be accessed at\nhttps://github.com/tongbao96/code-for-SFR-AS.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14179v1",
    "published_date": "2025-05-20 10:34:45 UTC",
    "updated_date": "2025-05-20 10:34:45 UTC"
  },
  {
    "arxiv_id": "2505.14178v1",
    "title": "Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic Reasoning Limits",
    "authors": [
      "Xiang Zhang",
      "Juntai Cao",
      "Jiaqi Wei",
      "Yiwei Xu",
      "Chenyu You"
    ],
    "abstract": "Tokenization is the first - and often underappreciated - layer of computation\nin language models. While Chain-of-Thought (CoT) prompting enables transformer\nmodels to approximate recurrent computation by externalizing intermediate\nsteps, we show that the success of such reasoning is fundamentally bounded by\nthe structure of tokenized inputs. This work presents a theoretical and\nempirical investigation into how tokenization schemes, particularly\nsubword-based methods like byte-pair encoding (BPE), impede symbolic\ncomputation by merging or obscuring atomic reasoning units. We introduce the\nnotion of Token Awareness to formalize how poor token granularity disrupts\nlogical alignment and prevents models from generalizing symbolic procedures.\nThrough systematic evaluation on arithmetic and symbolic tasks, we demonstrate\nthat token structure dramatically affect reasoning performance, causing failure\neven with CoT, while atomically-aligned formats unlock strong generalization,\nallowing small models (e.g., GPT-4o-mini) to outperform larger systems (e.g.,\no1) in structured reasoning. Our findings reveal that symbolic reasoning\nability in LLMs is not purely architectural, but deeply conditioned on\ntoken-level representations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14178v1",
    "published_date": "2025-05-20 10:32:30 UTC",
    "updated_date": "2025-05-20 10:32:30 UTC"
  },
  {
    "arxiv_id": "2505.14163v1",
    "title": "DSMentor: Enhancing Data Science Agents with Curriculum Learning and Online Knowledge Accumulation",
    "authors": [
      "He Wang",
      "Alexander Hanbo Li",
      "Yiqun Hu",
      "Sheng Zhang",
      "Hideo Kobayashi",
      "Jiani Zhang",
      "Henry Zhu",
      "Chung-Wei Hang",
      "Patrick Ng"
    ],
    "abstract": "Large language model (LLM) agents have shown promising performance in\ngenerating code for solving complex data science problems. Recent studies\nprimarily focus on enhancing in-context learning through improved search,\nsampling, and planning techniques, while overlooking the importance of the\norder in which problems are tackled during inference. In this work, we develop\na novel inference-time optimization framework, referred to as DSMentor, which\nleverages curriculum learning -- a strategy that introduces simpler task first\nand progressively moves to more complex ones as the learner improves -- to\nenhance LLM agent performance in challenging data science tasks. Our\nmentor-guided framework organizes data science tasks in order of increasing\ndifficulty and incorporates a growing long-term memory to retain prior\nexperiences, guiding the agent's learning progression and enabling more\neffective utilization of accumulated knowledge. We evaluate DSMentor through\nextensive experiments on DSEval and QRData benchmarks. Experiments show that\nDSMentor using Claude-3.5-Sonnet improves the pass rate by up to 5.2% on DSEval\nand QRData compared to baseline agents. Furthermore, DSMentor demonstrates\nstronger causal reasoning ability, improving the pass rate by 8.8% on the\ncausality problems compared to GPT-4 using Program-of-Thoughts prompts. Our\nwork underscores the importance of developing effective strategies for\naccumulating and utilizing knowledge during inference, mirroring the human\nlearning process and opening new avenues for improving LLM performance through\ncurriculum-based inference optimization.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14163v1",
    "published_date": "2025-05-20 10:16:21 UTC",
    "updated_date": "2025-05-20 10:16:21 UTC"
  },
  {
    "arxiv_id": "2505.14157v1",
    "title": "Prior Prompt Engineering for Reinforcement Fine-Tuning",
    "authors": [
      "Pittawat Taveekitworachai",
      "Potsawee Manakul",
      "Sarana Nutanong",
      "Kunat Pipatanakul"
    ],
    "abstract": "This paper investigates prior prompt engineering (pPE) in the context of\nreinforcement fine-tuning (RFT), where language models (LMs) are incentivized\nto exhibit behaviors that maximize performance through reward signals. While\nexisting RFT research has primarily focused on algorithms, reward shaping, and\ndata curation, the design of the prior prompt--the instructions prepended to\nqueries during training to elicit behaviors such as step-by-step\nreasoning--remains underexplored. We investigate whether different pPE\napproaches can guide LMs to internalize distinct behaviors after RFT. Inspired\nby inference-time prompt engineering (iPE), we translate five representative\niPE strategies--reasoning, planning, code-based reasoning, knowledge recall,\nand null-example utilization--into corresponding pPE approaches. We experiment\nwith Qwen2.5-7B using each of the pPE approaches, then evaluate performance on\nin-domain and out-of-domain benchmarks (e.g., AIME2024, HumanEval+, and\nGPQA-Diamond). Our results show that all pPE-trained models surpass their\niPE-prompted counterparts, with the null-example pPE approach achieving the\nlargest average performance gain and the highest improvement on AIME2024 and\nGPQA-Diamond, surpassing the commonly used reasoning approach. Furthermore, by\nadapting a behavior-classification framework, we demonstrate that different pPE\nstrategies instill distinct behavioral styles in the resulting models. These\nfindings position pPE as a powerful yet understudied axis for RFT.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "25 pages, 42 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.14157v1",
    "published_date": "2025-05-20 10:05:11 UTC",
    "updated_date": "2025-05-20 10:05:11 UTC"
  },
  {
    "arxiv_id": "2505.14156v1",
    "title": "Unify Graph Learning with Text: Unleashing LLM Potentials for Session Search",
    "authors": [
      "Songhao Wu",
      "Quan Tu",
      "Hong Liu",
      "Jia Xu",
      "Zhongyi Liu",
      "Guannan Zhang",
      "Ran Wang",
      "Xiuying Chen",
      "Rui Yan"
    ],
    "abstract": "Session search involves a series of interactive queries and actions to\nfulfill user's complex information need. Current strategies typically\nprioritize sequential modeling for deep semantic understanding, overlooking the\ngraph structure in interactions. While some approaches focus on capturing\nstructural information, they use a generalized representation for documents,\nneglecting the word-level semantic modeling. In this paper, we propose Symbolic\nGraph Ranker (SGR), which aims to take advantage of both text-based and\ngraph-based approaches by leveraging the power of recent Large Language Models\n(LLMs). Concretely, we first introduce a set of symbolic grammar rules to\nconvert session graph into text. This allows integrating session history,\ninteraction process, and task instruction seamlessly as inputs for the LLM.\nMoreover, given the natural discrepancy between LLMs pre-trained on textual\ncorpora, and the symbolic language we produce using our graph-to-text grammar,\nour objective is to enhance LLMs' ability to capture graph structures within a\ntextual format. To achieve this, we introduce a set of self-supervised symbolic\nlearning tasks including link prediction, node content generation, and\ngenerative contrastive learning, to enable LLMs to capture the topological\ninformation from coarse-grained to fine-grained. Experiment results and\ncomprehensive analysis on two benchmark datasets, AOL and Tiangong-ST, confirm\nthe superiority of our approach. Our paradigm also offers a novel and effective\nmethodology that bridges the gap between traditional search strategies and\nmodern LLMs.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.IR",
      "I.2; H.3.3"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14156v1",
    "published_date": "2025-05-20 10:05:06 UTC",
    "updated_date": "2025-05-20 10:05:06 UTC"
  },
  {
    "arxiv_id": "2505.14148v1",
    "title": "MM-Agent: LLM as Agents for Real-world Mathematical Modeling Problem",
    "authors": [
      "Fan Liu",
      "Zherui Yang",
      "Cancheng Liu",
      "Tianrui Song",
      "Xiaofeng Gao",
      "Hao Liu"
    ],
    "abstract": "Mathematical modeling is a cornerstone of scientific discovery and\nengineering practice, enabling the translation of real-world problems into\nformal systems across domains such as physics, biology, and economics. Unlike\nmathematical reasoning, which assumes a predefined formulation, modeling\nrequires open-ended problem analysis, abstraction, and principled\nformalization. While Large Language Models (LLMs) have shown strong reasoning\ncapabilities, they fall short in rigorous model construction, limiting their\nutility in real-world problem-solving. To this end, we formalize the task of\nLLM-powered real-world mathematical modeling, where agents must analyze\nproblems, construct domain-appropriate formulations, and generate complete\nend-to-end solutions. We introduce MM-Bench, a curated benchmark of 111\nproblems from the Mathematical Contest in Modeling (MCM/ICM), spanning the\nyears 2000 to 2025 and across ten diverse domains such as physics, biology, and\neconomics. To tackle this task, we propose MM-Agent, an expert-inspired\nframework that decomposes mathematical modeling into four stages: open-ended\nproblem analysis, structured model formulation, computational problem solving,\nand report generation. Experiments on MM-Bench show that MM-Agent significantly\noutperforms baseline agents, achieving an 11.88\\% improvement over human expert\nsolutions while requiring only 15 minutes and \\$0.88 per task using GPT-4o.\nFurthermore, under official MCM/ICM protocols, MM-Agent assisted two\nundergraduate teams in winning the Finalist Award (\\textbf{top 2.0\\% among\n27,456 teams}) in MCM/ICM 2025, demonstrating its practical effectiveness as a\nmodeling copilot. Our code is available at\nhttps://github.com/usail-hkust/LLM-MM-Agent",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14148v1",
    "published_date": "2025-05-20 09:55:31 UTC",
    "updated_date": "2025-05-20 09:55:31 UTC"
  },
  {
    "arxiv_id": "2505.14147v2",
    "title": "SHARP: Synthesizing High-quality Aligned Reasoning Problems for Large Reasoning Models Reinforcement Learning",
    "authors": [
      "Xiong Jun Wu",
      "Zhenduo Zhang",
      "ZuJie Wen",
      "Zhiqiang Zhang",
      "Wang Ren",
      "Lei Shi",
      "Cai Chen",
      "Deng Zhao",
      "Dingnan Jin",
      "Qing Cui",
      "Jun Zhou"
    ],
    "abstract": "Training large reasoning models (LRMs) with reinforcement learning in STEM\ndomains is hindered by the scarcity of high-quality, diverse, and verifiable\nproblem sets. Existing synthesis methods, such as Chain-of-Thought prompting,\noften generate oversimplified or uncheckable data, limiting model advancement\non complex tasks. To address these challenges, we introduce SHARP, a unified\napproach to Synthesizing High-quality Aligned Reasoning Problems for LRMs\nreinforcement learning with verifiable rewards (RLVR). SHARP encompasses a\nstrategic set of self-alignment principles -- targeting graduate and\nOlympiad-level difficulty, rigorous logical consistency, and unambiguous,\nverifiable answers -- and a structured three-phase framework (Alignment,\nInstantiation, Inference) that ensures thematic diversity and fine-grained\ncontrol over problem generation. We implement SHARP by leveraging a\nstate-of-the-art LRM to infer and verify challenging STEM questions, then\nemploy a reinforcement learning loop to refine the model's reasoning through\nverifiable reward signals. Experiments on benchmarks such as GPQA demonstrate\nthat SHARP-augmented training substantially outperforms existing methods,\nmarkedly improving complex reasoning accuracy and pushing LRM performance\ncloser to expert-level proficiency. Our contributions include the SHARP\nstrategy, framework design, end-to-end implementation, and experimental\nevaluation of its effectiveness in elevating LRM reasoning capabilities.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14147v2",
    "published_date": "2025-05-20 09:54:42 UTC",
    "updated_date": "2025-05-21 11:15:03 UTC"
  },
  {
    "arxiv_id": "2505.14146v1",
    "title": "s3: You Don't Need That Much Data to Train a Search Agent via RL",
    "authors": [
      "Pengcheng Jiang",
      "Xueqiang Xu",
      "Jiacheng Lin",
      "Jinfeng Xiao",
      "Zifeng Wang",
      "Jimeng Sun",
      "Jiawei Han"
    ],
    "abstract": "Retrieval-augmented generation (RAG) systems empower large language models\n(LLMs) to access external knowledge during inference. Recent advances have\nenabled LLMs to act as search agents via reinforcement learning (RL), improving\ninformation acquisition through multi-turn interactions with retrieval engines.\nHowever, existing approaches either optimize retrieval using search-only\nmetrics (e.g., NDCG) that ignore downstream utility or fine-tune the entire LLM\nto jointly reason and retrieve-entangling retrieval with generation and\nlimiting the real search utility and compatibility with frozen or proprietary\nmodels. In this work, we propose s3, a lightweight, model-agnostic framework\nthat decouples the searcher from the generator and trains the searcher using a\nGain Beyond RAG reward: the improvement in generation accuracy over naive RAG.\ns3 requires only 2.4k training samples to outperform baselines trained on over\n70x more data, consistently delivering stronger downstream performance across\nsix general QA and five medical QA benchmarks.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14146v1",
    "published_date": "2025-05-20 09:53:56 UTC",
    "updated_date": "2025-05-20 09:53:56 UTC"
  },
  {
    "arxiv_id": "2505.14143v1",
    "title": "Multimodal Mixture of Low-Rank Experts for Sentiment Analysis and Emotion Recognition",
    "authors": [
      "Shuo Zhang",
      "Jinsong Zhang",
      "Zhejun Zhang",
      "Lei Li"
    ],
    "abstract": "Multi-task learning (MTL) enables the efficient transfer of extra knowledge\nacquired from other tasks. The high correlation between multimodal sentiment\nanalysis (MSA) and multimodal emotion recognition (MER) supports their joint\ntraining. However, existing methods primarily employ hard parameter sharing,\nignoring parameter conflicts caused by complex task correlations. In this\npaper, we present a novel MTL method for MSA and MER, termed Multimodal Mixture\nof Low-Rank Experts (MMoLRE). MMoLRE utilizes shared and task-specific experts\nto distinctly model common and unique task characteristics, thereby avoiding\nparameter conflicts. Additionally, inspired by low-rank structures in the\nMixture of Experts (MoE) framework, we design low-rank expert networks to\nreduce parameter and computational overhead as the number of experts increases.\nExtensive experiments on the CMU-MOSI and CMU-MOSEI benchmarks demonstrate that\nMMoLRE achieves state-of-the-art performance on the MSA task and competitive\nresults on the MER task.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to ICME 2025",
    "pdf_url": "http://arxiv.org/pdf/2505.14143v1",
    "published_date": "2025-05-20 09:46:56 UTC",
    "updated_date": "2025-05-20 09:46:56 UTC"
  },
  {
    "arxiv_id": "2505.14141v1",
    "title": "Building a Stable Planner: An Extended Finite State Machine Based Planning Module for Mobile GUI Agent",
    "authors": [
      "Fanglin Mo",
      "Junzhe Chen",
      "Haoxuan Zhu",
      "Xuming Hu"
    ],
    "abstract": "Mobile GUI agents execute user commands by directly interacting with the\ngraphical user interface (GUI) of mobile devices, demonstrating significant\npotential to enhance user convenience. However, these agents face considerable\nchallenges in task planning, as they must continuously analyze the GUI and\ngenerate operation instructions step by step. This process often leads to\ndifficulties in making accurate task plans, as GUI agents lack a deep\nunderstanding of how to effectively use the target applications, which can\ncause them to become \"lost\" during task execution. To address the task planning\nissue, we propose SPlanner, a plug-and-play planning module to generate\nexecution plans that guide vision language model(VLMs) in executing tasks. The\nproposed planning module utilizes extended finite state machines (EFSMs) to\nmodel the control logits and configurations of mobile applications. It then\ndecomposes a user instruction into a sequence of primary function modeled in\nEFSMs, and generate the execution path by traversing the EFSMs. We further\nrefine the execution path into a natural language plan using an LLM. The final\nplan is concise and actionable, and effectively guides VLMs to generate\ninteractive GUI actions to accomplish user tasks. SPlanner demonstrates strong\nperformance on dynamic benchmarks reflecting real-world mobile usage. On the\nAndroidWorld benchmark, SPlanner achieves a 63.8% task success rate when paired\nwith Qwen2.5-VL-72B as the VLM executor, yielding a 28.8 percentage point\nimprovement compared to using Qwen2.5-VL-72B without planning assistance.",
    "categories": [
      "cs.AI",
      "I.2.11; H.5.2"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages. Submitted to EMNLP 2025",
    "pdf_url": "http://arxiv.org/pdf/2505.14141v1",
    "published_date": "2025-05-20 09:45:55 UTC",
    "updated_date": "2025-05-20 09:45:55 UTC"
  },
  {
    "arxiv_id": "2505.14140v1",
    "title": "RL of Thoughts: Navigating LLM Reasoning with Inference-time Reinforcement Learning",
    "authors": [
      "Qianyue Hao",
      "Sibo Li",
      "Jian Yuan",
      "Yong Li"
    ],
    "abstract": "Despite rapid advancements in large language models (LLMs), the token-level\nautoregressive nature constrains their complex reasoning capabilities. To\nenhance LLM reasoning, inference-time techniques, including\nChain/Tree/Graph-of-Thought(s), successfully improve the performance, as they\nare fairly cost-effective by guiding reasoning through sophisticated logical\nstructures without modifying LLMs' parameters. However, these manually\npredefined, task-agnostic frameworks are applied uniformly across diverse\ntasks, lacking adaptability. To improve this, we propose RL-of-Thoughts (RLoT),\nwhere we train a lightweight navigator model with reinforcement learning (RL)\nto adaptively enhance LLM reasoning at inference time. Specifically, we design\nfive basic logic blocks from the perspective of human cognition. During the\nreasoning process, the trained RL navigator dynamically selects the suitable\nlogic blocks and combines them into task-specific logical structures according\nto problem characteristics. Experiments across multiple reasoning benchmarks\n(AIME, MATH, GPQA, etc.) with multiple LLMs (GPT, Llama, Qwen, and DeepSeek)\nillustrate that RLoT outperforms established inference-time techniques by up to\n13.4%. Remarkably, with less than 3K parameters, our RL navigator is able to\nmake sub-10B LLMs comparable to 100B-scale counterparts. Moreover, the RL\nnavigator demonstrates strong transferability: a model trained on one specific\nLLM-task pair can effectively generalize to unseen LLMs and tasks. Our code is\nopen-source at https://anonymous.4open.science/r/RL-LLM-Reasoning-1A30 for\nreproducibility.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14140v1",
    "published_date": "2025-05-20 09:43:33 UTC",
    "updated_date": "2025-05-20 09:43:33 UTC"
  },
  {
    "arxiv_id": "2505.14139v1",
    "title": "FlowQ: Energy-Guided Flow Policies for Offline Reinforcement Learning",
    "authors": [
      "Marvin Alles",
      "Nutan Chen",
      "Patrick van der Smagt",
      "Botond Cseke"
    ],
    "abstract": "The use of guidance to steer sampling toward desired outcomes has been widely\nexplored within diffusion models, especially in applications such as image and\ntrajectory generation. However, incorporating guidance during training remains\nrelatively underexplored. In this work, we introduce energy-guided flow\nmatching, a novel approach that enhances the training of flow models and\neliminates the need for guidance at inference time. We learn a conditional\nvelocity field corresponding to the flow policy by approximating an\nenergy-guided probability path as a Gaussian path. Learning guided trajectories\nis appealing for tasks where the target distribution is defined by a\ncombination of data and an energy function, as in reinforcement learning.\nDiffusion-based policies have recently attracted attention for their expressive\npower and ability to capture multi-modal action distributions. Typically, these\npolicies are optimized using weighted objectives or by back-propagating\ngradients through actions sampled by the policy. As an alternative, we propose\nFlowQ, an offline reinforcement learning algorithm based on energy-guided flow\nmatching. Our method achieves competitive performance while the policy training\ntime is constant in the number of flow sampling steps.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14139v1",
    "published_date": "2025-05-20 09:43:05 UTC",
    "updated_date": "2025-05-20 09:43:05 UTC"
  },
  {
    "arxiv_id": "2505.14137v1",
    "title": "Memory Assignment for Finite-Memory Strategies in Adversarial Patrolling Games",
    "authors": [
      "VojtÄch KÅ¯r",
      "VÃ­t Musil",
      "VojtÄch ÅehÃ¡k"
    ],
    "abstract": "Adversarial Patrolling games form a subclass of Security games where a\nDefender moves between locations, guarding vulnerable targets. The main\nalgorithmic problem is constructing a strategy for the Defender that minimizes\nthe worst damage an Attacker can cause. We focus on the class of finite-memory\n(also known as regular) Defender's strategies that experimentally outperformed\nother competing classes. A finite-memory strategy can be seen as a positional\nstrategy on a finite set of states. Each state consists of a pair of a location\nand a certain integer value--called memory. Existing algorithms improve the\ntransitional probabilities between the states but require that the available\nmemory size itself is assigned at each location manually. Choosing the right\nmemory assignment is a well-known open and hard problem that hinders the\nusability of finite-memory strategies. We solve this issue by developing a\ngeneral method that iteratively changes the memory assignment. Our algorithm\ncan be used in connection with \\emph{any} black-box strategy optimization tool.\nWe evaluate our method on various experiments and show its robustness by\nsolving instances of various patrolling models.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14137v1",
    "published_date": "2025-05-20 09:40:53 UTC",
    "updated_date": "2025-05-20 09:40:53 UTC"
  },
  {
    "arxiv_id": "2505.14136v1",
    "title": "Local Mixtures of Experts: Essentially Free Test-Time Training via Model Merging",
    "authors": [
      "Ryo Bertolissi",
      "Jonas HÃ¼botter",
      "Ido Hakimi",
      "Andreas Krause"
    ],
    "abstract": "Mixture of expert (MoE) models are a promising approach to increasing model\ncapacity without increasing inference cost, and are core components of many\nstate-of-the-art language models. However, current MoE models typically use\nonly few experts due to prohibitive training and inference cost. We propose\nTest-Time Model Merging (TTMM) which scales the MoE paradigm to an order of\nmagnitude more experts and uses model merging to avoid almost any test-time\noverhead. We show that TTMM is an approximation of test-time training (TTT),\nwhich fine-tunes an expert model for each prediction task, i.e., prompt. TTT\nhas recently been shown to significantly improve language models, but is\ncomputationally expensive. We find that performance of TTMM improves with more\nexperts and approaches the performance of TTT. Moreover, we find that with a 1B\nparameter base model, TTMM is more than 100x faster than TTT at test-time by\namortizing the cost of TTT at train-time. Thus, TTMM offers a promising\ncost-effective approach to scale test-time training.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14136v1",
    "published_date": "2025-05-20 09:39:54 UTC",
    "updated_date": "2025-05-20 09:39:54 UTC"
  },
  {
    "arxiv_id": "2505.14128v1",
    "title": "A Methodological Framework for Measuring Spatial Labeling Similarity",
    "authors": [
      "Yihang Du",
      "Jiaying Hu",
      "Suyang Hou",
      "Yueyang Ding",
      "Xiaobo Sun"
    ],
    "abstract": "Spatial labeling assigns labels to specific spatial locations to characterize\ntheir spatial properties and relationships, with broad applications in\nscientific research and practice. Measuring the similarity between two spatial\nlabelings is essential for understanding their differences and the contributing\nfactors, such as changes in location properties or labeling methods. An\nadequate and unbiased measurement of spatial labeling similarity should\nconsider the number of matched labels (label agreement), the topology of\nspatial label distribution, and the heterogeneous impacts of mismatched labels.\nHowever, existing methods often fail to account for all these aspects. To\naddress this gap, we propose a methodological framework to guide the\ndevelopment of methods that meet these requirements. Given two spatial\nlabelings, the framework transforms them into graphs based on location\norganization, labels, and attributes (e.g., location significance). The\ndistributions of their graph attributes are then extracted, enabling an\nefficient computation of distributional discrepancy to reflect the\ndissimilarity level between the two labelings. We further provide a concrete\nimplementation of this framework, termed Spatial Labeling Analogy Metric\n(SLAM), along with an analysis of its theoretical foundation, for evaluating\nspatial labeling results in spatial transcriptomics (ST) \\textit{as per} their\nsimilarity with ground truth labeling. Through a series of carefully designed\nexperimental cases involving both simulated and real ST data, we demonstrate\nthat SLAM provides a comprehensive and accurate reflection of labeling quality\ncompared to other well-established evaluation metrics. Our code is available at\nhttps://github.com/YihDu/SLAM.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14128v1",
    "published_date": "2025-05-20 09:34:03 UTC",
    "updated_date": "2025-05-20 09:34:03 UTC"
  },
  {
    "arxiv_id": "2505.14125v1",
    "title": "Contrastive Consolidation of Top-Down Modulations Achieves Sparsely Supervised Continual Learning",
    "authors": [
      "Viet Anh Khoa Tran",
      "Emre Neftci",
      "Willem. A. M. Wybo"
    ],
    "abstract": "Biological brains learn continually from a stream of unlabeled data, while\nintegrating specialized information from sparsely labeled examples without\ncompromising their ability to generalize. Meanwhile, machine learning methods\nare susceptible to catastrophic forgetting in this natural learning setting, as\nsupervised specialist fine-tuning degrades performance on the original task. We\nintroduce task-modulated contrastive learning (TMCL), which takes inspiration\nfrom the biophysical machinery in the neocortex, using predictive coding\nprinciples to integrate top-down information continually and without\nsupervision. We follow the idea that these principles build a view-invariant\nrepresentation space, and that this can be implemented using a contrastive\nloss. Then, whenever labeled samples of a new class occur, new affine\nmodulations are learned that improve separation of the new class from all\nothers, without affecting feedforward weights. By co-opting the view-invariance\nlearning mechanism, we then train feedforward weights to match the unmodulated\nrepresentation of a data sample to its modulated counterparts. This introduces\nmodulation invariance into the representation space, and, by also using past\nmodulations, stabilizes it. Our experiments show improvements in both\nclass-incremental and transfer learning over state-of-the-art unsupervised\napproaches, as well as over comparable supervised approaches, using as few as\n1% of available labels. Taken together, our work suggests that top-down\nmodulations play a crucial role in balancing stability and plasticity.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.NC",
      "68T05 (primary), 68T07, 68T45 (secondary)",
      "I.2.6; I.2.10"
    ],
    "primary_category": "cs.LG",
    "comment": "33 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.14125v1",
    "published_date": "2025-05-20 09:31:57 UTC",
    "updated_date": "2025-05-20 09:31:57 UTC"
  },
  {
    "arxiv_id": "2505.14117v1",
    "title": "Collaborative Unlabeled Data Optimization",
    "authors": [
      "Xinyi Shang",
      "Peng Sun",
      "Fengyuan Liu",
      "Tao Lin"
    ],
    "abstract": "This paper pioneers a novel data-centric paradigm to maximize the utility of\nunlabeled data, tackling a critical question: How can we enhance the efficiency\nand sustainability of deep learning training by optimizing the data itself? We\nbegin by identifying three key limitations in existing model-centric\napproaches, all rooted in a shared bottleneck: knowledge extracted from data is\nlocked to model parameters, hindering its reusability and scalability. To this\nend, we propose CoOpt, a highly efficient, parallelized framework for\ncollaborative unlabeled data optimization, thereby effectively encoding\nknowledge into the data itself. By distributing unlabeled data and leveraging\npublicly available task-agnostic models, CoOpt facilitates scalable, reusable,\nand sustainable training pipelines. Extensive experiments across diverse\ndatasets and architectures demonstrate its efficacy and efficiency, achieving\n13.6% and 6.8% improvements on Tiny-ImageNet and ImageNet-1K, respectively,\nwith training speedups of $1.94 \\times $ and $1.2 \\times$.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14117v1",
    "published_date": "2025-05-20 09:21:40 UTC",
    "updated_date": "2025-05-20 09:21:40 UTC"
  },
  {
    "arxiv_id": "2505.14107v1",
    "title": "DiagnosisArena: Benchmarking Diagnostic Reasoning for Large Language Models",
    "authors": [
      "Yakun Zhu",
      "Zhongzhen Huang",
      "Linjie Mu",
      "Yutong Huang",
      "Wei Nie",
      "Shaoting Zhang",
      "Pengfei Liu",
      "Xiaofan Zhang"
    ],
    "abstract": "The emergence of groundbreaking large language models capable of performing\ncomplex reasoning tasks holds significant promise for addressing various\nscientific challenges, including those arising in complex clinical scenarios.\nTo enable their safe and effective deployment in real-world healthcare\nsettings, it is urgently necessary to benchmark the diagnostic capabilities of\ncurrent models systematically. Given the limitations of existing medical\nbenchmarks in evaluating advanced diagnostic reasoning, we present\nDiagnosisArena, a comprehensive and challenging benchmark designed to\nrigorously assess professional-level diagnostic competence. DiagnosisArena\nconsists of 1,113 pairs of segmented patient cases and corresponding diagnoses,\nspanning 28 medical specialties, deriving from clinical case reports published\nin 10 top-tier medical journals. The benchmark is developed through a\nmeticulous construction pipeline, involving multiple rounds of screening and\nreview by both AI systems and human experts, with thorough checks conducted to\nprevent data leakage. Our study reveals that even the most advanced reasoning\nmodels, o3-mini, o1, and DeepSeek-R1, achieve only 45.82%, 31.09%, and 17.79%\naccuracy, respectively. This finding highlights a significant generalization\nbottleneck in current large language models when faced with clinical diagnostic\nreasoning challenges. Through DiagnosisArena, we aim to drive further\nadvancements in AIs diagnostic reasoning capabilities, enabling more effective\nsolutions for real-world clinical diagnostic challenges. We provide the\nbenchmark and evaluation tools for further research and development\nhttps://github.com/SPIRAL-MED/DiagnosisArena.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14107v1",
    "published_date": "2025-05-20 09:14:53 UTC",
    "updated_date": "2025-05-20 09:14:53 UTC"
  },
  {
    "arxiv_id": "2505.14106v1",
    "title": "A Personalized Conversational Benchmark: Towards Simulating Personalized Conversations",
    "authors": [
      "Li Li",
      "Peilin Cai",
      "Ryan A. Rossi",
      "Franck Dernoncourt",
      "Branislav Kveton",
      "Junda Wu",
      "Tong Yu",
      "Linxin Song",
      "Tiankai Yang",
      "Yuehan Qin",
      "Nesreen K. Ahmed",
      "Samyadeep Basu",
      "Subhojyoti Mukherjee",
      "Ruiyi Zhang",
      "Zhengmian Hu",
      "Bo Ni",
      "Yuxiao Zhou",
      "Zichao Wang",
      "Yue Huang",
      "Yu Wang",
      "Xiangliang Zhang",
      "Philip S. Yu",
      "Xiyang Hu",
      "Yue Zhao"
    ],
    "abstract": "We present PersonaConvBench, a large-scale benchmark for evaluating\npersonalized reasoning and generation in multi-turn conversations with large\nlanguage models (LLMs). Unlike existing work that focuses on either\npersonalization or conversational structure in isolation, PersonaConvBench\nintegrates both, offering three core tasks: sentence classification, impact\nregression, and user-centric text generation across ten diverse Reddit-based\ndomains. This design enables systematic analysis of how personalized\nconversational context shapes LLM outputs in realistic multi-user scenarios. We\nbenchmark several commercial and open-source LLMs under a unified prompting\nsetup and observe that incorporating personalized history yields substantial\nperformance improvements, including a 198 percent relative gain over the best\nnon-conversational baseline in sentiment classification. By releasing\nPersonaConvBench with evaluations and code, we aim to support research on LLMs\nthat adapt to individual styles, track long-term context, and produce\ncontextually rich, engaging responses.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14106v1",
    "published_date": "2025-05-20 09:13:22 UTC",
    "updated_date": "2025-05-20 09:13:22 UTC"
  },
  {
    "arxiv_id": "2505.14103v2",
    "title": "AudioJailbreak: Jailbreak Attacks against End-to-End Large Audio-Language Models",
    "authors": [
      "Guangke Chen",
      "Fu Song",
      "Zhe Zhao",
      "Xiaojun Jia",
      "Yang Liu",
      "Yanchen Qiao",
      "Weizhe Zhang"
    ],
    "abstract": "Jailbreak attacks to Large audio-language models (LALMs) are studied\nrecently, but they achieve suboptimal effectiveness, applicability, and\npracticability, particularly, assuming that the adversary can fully manipulate\nuser prompts. In this work, we first conduct an extensive experiment showing\nthat advanced text jailbreak attacks cannot be easily ported to end-to-end\nLALMs via text-to speech (TTS) techniques. We then propose AudioJailbreak, a\nnovel audio jailbreak attack, featuring (1) asynchrony: the jailbreak audio\ndoes not need to align with user prompts in the time axis by crafting suffixal\njailbreak audios; (2) universality: a single jailbreak perturbation is\neffective for different prompts by incorporating multiple prompts into\nperturbation generation; (3) stealthiness: the malicious intent of jailbreak\naudios will not raise the awareness of victims by proposing various intent\nconcealment strategies; and (4) over-the-air robustness: the jailbreak audios\nremain effective when being played over the air by incorporating the\nreverberation distortion effect with room impulse response into the generation\nof the perturbations. In contrast, all prior audio jailbreak attacks cannot\noffer asynchrony, universality, stealthiness, or over-the-air robustness.\nMoreover, AudioJailbreak is also applicable to the adversary who cannot fully\nmanipulate user prompts, thus has a much broader attack scenario. Extensive\nexperiments with thus far the most LALMs demonstrate the high effectiveness of\nAudioJailbreak. We highlight that our work peeks into the security implications\nof audio jailbreak attacks against LALMs, and realistically fosters improving\ntheir security robustness. The implementation and audio samples are available\nat our website https://audiojailbreak.github.io/AudioJailbreak.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14103v2",
    "published_date": "2025-05-20 09:10:45 UTC",
    "updated_date": "2025-05-21 03:36:20 UTC"
  },
  {
    "arxiv_id": "2505.14745v1",
    "title": "Explainable Prediction of the Mechanical Properties of Composites with CNNs",
    "authors": [
      "Varun Raaghav",
      "Dimitrios Bikos",
      "Antonio Rago",
      "Francesca Toni",
      "Maria Charalambides"
    ],
    "abstract": "Composites are amongst the most important materials manufactured today, as\nevidenced by their use in countless applications. In order to establish the\nsuitability of composites in specific applications, finite element (FE)\nmodelling, a numerical method based on partial differential equations, is the\nindustry standard for assessing their mechanical properties. However, FE\nmodelling is exceptionally costly from a computational viewpoint, a limitation\nwhich has led to efforts towards applying AI models to this task. However, in\nthese approaches: the chosen model architectures were rudimentary, feed-forward\nneural networks giving limited accuracy; the studies focus on predicting\nelastic mechanical properties, without considering material strength limits;\nand the models lacked transparency, hindering trustworthiness by users. In this\npaper, we show that convolutional neural networks (CNNs) equipped with methods\nfrom explainable AI (XAI) can be successfully deployed to solve this problem.\nOur approach uses customised CNNs trained on a dataset we generate using\ntransverse tension tests in FE modelling to predict composites' mechanical\nproperties, i.e., Young's modulus and yield strength. We show empirically that\nour approach achieves high accuracy, outperforming a baseline, ResNet-34, in\nestimating the mechanical properties. We then use SHAP and Integrated\nGradients, two post-hoc XAI methods, to explain the predictions, showing that\nthe CNNs use the critical geometrical features that influence the composites'\nbehaviour, thus allowing engineers to verify that the models are trustworthy by\nrepresenting the science of composites.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2.1"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.14745v1",
    "published_date": "2025-05-20 08:54:06 UTC",
    "updated_date": "2025-05-20 08:54:06 UTC"
  },
  {
    "arxiv_id": "2505.14080v1",
    "title": "Gender Trouble in Language Models: An Empirical Audit Guided by Gender Performativity Theory",
    "authors": [
      "Franziska Sofia Hafner",
      "Ana Valdivia",
      "Luc Rocher"
    ],
    "abstract": "Language models encode and subsequently perpetuate harmful gendered\nstereotypes. Research has succeeded in mitigating some of these harms, e.g. by\ndissociating non-gendered terms such as occupations from gendered terms such as\n'woman' and 'man'. This approach, however, remains superficial given that\nassociations are only one form of prejudice through which gendered harms arise.\nCritical scholarship on gender, such as gender performativity theory,\nemphasizes how harms often arise from the construction of gender itself, such\nas conflating gender with biological sex. In language models, these issues\ncould lead to the erasure of transgender and gender diverse identities and\ncause harms in downstream applications, from misgendering users to\nmisdiagnosing patients based on wrong assumptions about their anatomy.\n  For FAccT research on gendered harms to go beyond superficial linguistic\nassociations, we advocate for a broader definition of 'gender bias' in language\nmodels. We operationalize insights on the construction of gender through\nlanguage from gender studies literature and then empirically test how 16\nlanguage models of different architectures, training datasets, and model sizes\nencode gender. We find that language models tend to encode gender as a binary\ncategory tied to biological sex, and that gendered terms that do not neatly\nfall into one of these binary categories are erased and pathologized. Finally,\nwe show that larger models, which achieve better results on performance\nbenchmarks, learn stronger associations between gender and sex, further\nreinforcing a narrow understanding of gender. Our findings lead us to call for\na re-evaluation of how gendered harms in language models are defined and\naddressed.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14080v1",
    "published_date": "2025-05-20 08:36:47 UTC",
    "updated_date": "2025-05-20 08:36:47 UTC"
  },
  {
    "arxiv_id": "2505.14072v1",
    "title": "Personalized Student Knowledge Modeling for Future Learning Resource Prediction",
    "authors": [
      "Soroush Hashemifar",
      "Sherry Sahebi"
    ],
    "abstract": "Despite advances in deep learning for education, student knowledge tracing\nand behavior modeling face persistent challenges: limited personalization,\ninadequate modeling of diverse learning activities (especially non-assessed\nmaterials), and overlooking the interplay between knowledge acquisition and\nbehavioral patterns. Practical limitations, such as fixed-size sequence\nsegmentation, frequently lead to the loss of contextual information vital for\npersonalized learning. Moreover, reliance on student performance on assessed\nmaterials limits the modeling scope, excluding non-assessed interactions like\nlectures. To overcome these shortcomings, we propose Knowledge Modeling and\nMaterial Prediction (KMaP), a stateful multi-task approach designed for\npersonalized and simultaneous modeling of student knowledge and behavior. KMaP\nemploys clustering-based student profiling to create personalized student\nrepresentations, improving predictions of future learning resource preferences.\nExtensive experiments on two real-world datasets confirm significant behavioral\ndifferences across student clusters and validate the efficacy of the KMaP\nmodel.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14072v1",
    "published_date": "2025-05-20 08:23:50 UTC",
    "updated_date": "2025-05-20 08:23:50 UTC"
  },
  {
    "arxiv_id": "2505.14744v1",
    "title": "Transductively Informed Inductive Program Synthesis",
    "authors": [
      "Janis Zenkner",
      "Tobias Sesterhenn",
      "Christian Bartelt"
    ],
    "abstract": "Abstraction and reasoning in program synthesis has seen significant progress\nthrough both inductive and transductive paradigms. Inductive approaches\ngenerate a program or latent function from input-output examples, which can\nthen be applied to new inputs. Transductive approaches directly predict output\nvalues for given inputs, effectively serving as the function themselves.\nCurrent approaches combine inductive and transductive models via isolated\nensembling, but they do not explicitly model the interaction between both\nparadigms. In this work, we introduce \\acs{tiips}, a novel framework that\nunifies transductive and inductive strategies by explicitly modeling their\ninteractions through a cooperative mechanism: an inductive model generates\nprograms, while a transductive model constrains, guides, and refines the search\nto improve synthesis accuracy and generalization. We evaluate \\acs{tiips} on\ntwo widely studied program synthesis domains: string and list manipulation. Our\nresults show that \\acs{tiips} solves more tasks and yields functions that more\nclosely match optimal solutions in syntax and semantics, particularly in\nout-of-distribution settings, yielding state-of-the-art performance. We believe\nthat explicitly modeling the synergy between inductive and transductive\nreasoning opens promising avenues for general-purpose program synthesis and\nbroader applications.",
    "categories": [
      "cs.PL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.PL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14744v1",
    "published_date": "2025-05-20 08:23:46 UTC",
    "updated_date": "2025-05-20 08:23:46 UTC"
  },
  {
    "arxiv_id": "2505.14064v1",
    "title": "NOVA: A Benchmark for Anomaly Localization and Clinical Reasoning in Brain MRI",
    "authors": [
      "Cosmin I. Bercea",
      "Jun Li",
      "Philipp Raffler",
      "Evamaria O. Riedel",
      "Lena Schmitzer",
      "Angela Kurz",
      "Felix Bitzer",
      "Paula RoÃmÃ¼ller",
      "Julian Canisius",
      "Mirjam L. Beyrle",
      "Che Liu",
      "Wenjia Bai",
      "Bernhard Kainz",
      "Julia A. Schnabel",
      "Benedikt Wiestler"
    ],
    "abstract": "In many real-world applications, deployed models encounter inputs that differ\nfrom the data seen during training. Out-of-distribution detection identifies\nwhether an input stems from an unseen distribution, while open-world\nrecognition flags such inputs to ensure the system remains robust as\never-emerging, previously $unknown$ categories appear and must be addressed\nwithout retraining. Foundation and vision-language models are pre-trained on\nlarge and diverse datasets with the expectation of broad generalization across\ndomains, including medical imaging. However, benchmarking these models on test\nsets with only a few common outlier types silently collapses the evaluation\nback to a closed-set problem, masking failures on rare or truly novel\nconditions encountered in clinical use.\n  We therefore present $NOVA$, a challenging, real-life $evaluation-only$\nbenchmark of $\\sim$900 brain MRI scans that span 281 rare pathologies and\nheterogeneous acquisition protocols. Each case includes rich clinical\nnarratives and double-blinded expert bounding-box annotations. Together, these\nenable joint assessment of anomaly localisation, visual captioning, and\ndiagnostic reasoning. Because NOVA is never used for training, it serves as an\n$extreme$ stress-test of out-of-distribution generalisation: models must bridge\na distribution gap both in sample appearance and in semantic space. Baseline\nresults with leading vision-language models (GPT-4o, Gemini 2.0 Flash, and\nQwen2.5-VL-72B) reveal substantial performance drops across all tasks,\nestablishing NOVA as a rigorous testbed for advancing models that can detect,\nlocalize, and reason about truly unknown anomalies.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14064v1",
    "published_date": "2025-05-20 08:10:57 UTC",
    "updated_date": "2025-05-20 08:10:57 UTC"
  },
  {
    "arxiv_id": "2505.14057v1",
    "title": "Field Matters: A lightweight LLM-enhanced Method for CTR Prediction",
    "authors": [
      "Yu Cui",
      "Feng Liu",
      "Jiawei Chen",
      "Xingyu Lou",
      "Changwang Zhang",
      "Jun Wang",
      "Yuegang Sun",
      "Xiaohu Yang",
      "Can Wang"
    ],
    "abstract": "Click-through rate (CTR) prediction is a fundamental task in modern\nrecommender systems. In recent years, the integration of large language models\n(LLMs) has been shown to effectively enhance the performance of traditional CTR\nmethods. However, existing LLM-enhanced methods often require extensive\nprocessing of detailed textual descriptions for large-scale instances or\nuser/item entities, leading to substantial computational overhead. To address\nthis challenge, this work introduces LLaCTR, a novel and lightweight\nLLM-enhanced CTR method that employs a field-level enhancement paradigm.\nSpecifically, LLaCTR first utilizes LLMs to distill crucial and lightweight\nsemantic knowledge from small-scale feature fields through self-supervised\nfield-feature fine-tuning. Subsequently, it leverages this field-level semantic\nknowledge to enhance both feature representation and feature interactions. In\nour experiments, we integrate LLaCTR with six representative CTR models across\nfour datasets, demonstrating its superior performance in terms of both\neffectiveness and efficiency compared to existing LLM-enhanced methods. Our\ncode is available at https://anonymous.4open.science/r/LLaCTR-EC46.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14057v1",
    "published_date": "2025-05-20 08:02:41 UTC",
    "updated_date": "2025-05-20 08:02:41 UTC"
  },
  {
    "arxiv_id": "2505.15851v1",
    "title": "Exploring Moral Exercises for Human Oversight of AI systems: Insights from Three Pilot Studies",
    "authors": [
      "Silvia Crafa",
      "Teresa Scantamburlo"
    ],
    "abstract": "This paper elaborates on the concept of moral exercises as a means to help AI\nactors cultivate virtues that enable effective human oversight of AI systems.\nWe explore the conceptual framework and significance of moral exercises,\nsituating them within the contexts of philosophical discourse, ancient\npractices, and contemporary AI ethics scholarship. We outline the core pillars\nof the moral exercises methodology - eliciting an engaged personal disposition,\nfostering relational understanding, and cultivating technomoral wisdom - and\nemphasize their relevance to key activities and competencies essential for\nhuman oversight of AI systems. Our argument is supported by findings from three\npilot studies involving a company, a multidisciplinary team of AI researchers,\nand higher education students. These studies allow us to explore both the\npotential and the limitations of moral exercises. Based on the collected data,\nwe offer insights into how moral exercises can foster a responsible AI culture\nwithin organizations, and suggest directions for future research.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15851v1",
    "published_date": "2025-05-20 07:47:24 UTC",
    "updated_date": "2025-05-20 07:47:24 UTC"
  },
  {
    "arxiv_id": "2505.14045v1",
    "title": "From Unaligned to Aligned: Scaling Multilingual LLMs with Multi-Way Parallel Corpora",
    "authors": [
      "Yingli Shen",
      "Wen Lai",
      "Shuo Wang",
      "Kangyang Luo",
      "Alexander Fraser",
      "Maosong Sun"
    ],
    "abstract": "Continued pretraining and instruction tuning on large-scale multilingual data\nhave proven to be effective in scaling large language models (LLMs) to\nlow-resource languages. However, the unaligned nature of such data limits its\nability to effectively capture cross-lingual semantics. In contrast, multi-way\nparallel data, where identical content is aligned across multiple languages,\nprovides stronger cross-lingual consistency and offers greater potential for\nimproving multilingual performance. In this paper, we introduce a large-scale,\nhigh-quality multi-way parallel corpus, TED2025, based on TED Talks. The corpus\nspans 113 languages, with up to 50 languages aligned in parallel, ensuring\nextensive multilingual coverage. Using this dataset, we investigate best\npractices for leveraging multi-way parallel data to enhance LLMs, including\nstrategies for continued pretraining, instruction tuning, and the analysis of\nkey influencing factors. Experiments on six multilingual benchmarks show that\nmodels trained on multiway parallel data consistently outperform those trained\non unaligned multilingual data.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14045v1",
    "published_date": "2025-05-20 07:43:45 UTC",
    "updated_date": "2025-05-20 07:43:45 UTC"
  },
  {
    "arxiv_id": "2505.14038v1",
    "title": "ProMind-LLM: Proactive Mental Health Care via Causal Reasoning with Sensor Data",
    "authors": [
      "Xinzhe Zheng",
      "Sijie Ji",
      "Jiawei Sun",
      "Renqi Chen",
      "Wei Gao",
      "Mani Srivastava"
    ],
    "abstract": "Mental health risk is a critical global public health challenge,\nnecessitating innovative and reliable assessment methods. With the development\nof large language models (LLMs), they stand out to be a promising tool for\nexplainable mental health care applications. Nevertheless, existing approaches\npredominantly rely on subjective textual mental records, which can be distorted\nby inherent mental uncertainties, leading to inconsistent and unreliable\npredictions. To address these limitations, this paper introduces ProMind-LLM.\nWe investigate an innovative approach integrating objective behavior data as\ncomplementary information alongside subjective mental records for robust mental\nhealth risk assessment. Specifically, ProMind-LLM incorporates a comprehensive\npipeline that includes domain-specific pretraining to tailor the LLM for mental\nhealth contexts, a self-refine mechanism to optimize the processing of\nnumerical behavioral data, and causal chain-of-thought reasoning to enhance the\nreliability and interpretability of its predictions. Evaluations of two\nreal-world datasets, PMData and Globem, demonstrate the effectiveness of our\nproposed methods, achieving substantial improvements over general LLMs. We\nanticipate that ProMind-LLM will pave the way for more dependable,\ninterpretable, and scalable mental health case solutions.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14038v1",
    "published_date": "2025-05-20 07:36:28 UTC",
    "updated_date": "2025-05-20 07:36:28 UTC"
  },
  {
    "arxiv_id": "2505.14036v1",
    "title": "Adaptive Cyclic Diffusion for Inference Scaling",
    "authors": [
      "Gyubin Lee",
      "Truong Nhat Nguyen Bao",
      "Jaesik Yoon",
      "Dongwoo Lee",
      "Minsu Kim",
      "Yoshua Bengio",
      "Sungjin Ahn"
    ],
    "abstract": "Diffusion models have demonstrated strong generative capabilities across\ndomains ranging from image synthesis to complex reasoning tasks. However, most\ninference-time scaling methods rely on fixed denoising schedules, limiting\ntheir ability to allocate computation based on instance difficulty or\ntask-specific demands adaptively. We introduce the challenge of adaptive\ninference-time scaling-dynamically adjusting computational effort during\ninference-and propose Adaptive Bi-directional Cyclic Diffusion (ABCD), a\nflexible, search-based inference framework. ABCD refines outputs through\nbi-directional diffusion cycles while adaptively controlling exploration depth\nand termination. It comprises three components: Cyclic Diffusion Search,\nAutomatic Exploration-Exploitation Balancing, and Adaptive Thinking Time.\nExperiments show that ABCD improves performance across diverse tasks while\nmaintaining computational efficiency.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14036v1",
    "published_date": "2025-05-20 07:31:38 UTC",
    "updated_date": "2025-05-20 07:31:38 UTC"
  },
  {
    "arxiv_id": "2505.14029v1",
    "title": "AppleGrowthVision: A large-scale stereo dataset for phenological analysis, fruit detection, and 3D reconstruction in apple orchards",
    "authors": [
      "Laura-Sophia von Hirschhausen",
      "Jannes S. Magnusson",
      "Mykyta Kovalenko",
      "Fredrik Boye",
      "Tanay Rawat",
      "Peter Eisert",
      "Anna Hilsmann",
      "Sebastian Pretzsch",
      "Sebastian Bosse"
    ],
    "abstract": "Deep learning has transformed computer vision for precision agriculture, yet\napple orchard monitoring remains limited by dataset constraints. The lack of\ndiverse, realistic datasets and the difficulty of annotating dense,\nheterogeneous scenes. Existing datasets overlook different growth stages and\nstereo imagery, both essential for realistic 3D modeling of orchards and tasks\nlike fruit localization, yield estimation, and structural analysis. To address\nthese gaps, we present AppleGrowthVision, a large-scale dataset comprising two\nsubsets. The first includes 9,317 high resolution stereo images collected from\na farm in Brandenburg (Germany), covering six agriculturally validated growth\nstages over a full growth cycle. The second subset consists of 1,125 densely\nannotated images from the same farm in Brandenburg and one in Pillnitz\n(Germany), containing a total of 31,084 apple labels. AppleGrowthVision\nprovides stereo-image data with agriculturally validated growth stages,\nenabling precise phenological analysis and 3D reconstructions. Extending\nMinneApple with our data improves YOLOv8 performance by 7.69 % in terms of\nF1-score, while adding it to MinneApple and MAD boosts Faster R-CNN F1-score by\n31.06 %. Additionally, six BBCH stages were predicted with over 95 % accuracy\nusing VGG16, ResNet152, DenseNet201, and MobileNetv2. AppleGrowthVision bridges\nthe gap between agricultural science and computer vision, by enabling the\ndevelopment of robust models for fruit detection, growth modeling, and 3D\nanalysis in precision agriculture. Future work includes improving annotation,\nenhancing 3D reconstruction, and extending multimodal analysis across all\ngrowth stages.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14029v1",
    "published_date": "2025-05-20 07:29:22 UTC",
    "updated_date": "2025-05-20 07:29:22 UTC"
  },
  {
    "arxiv_id": "2505.14027v1",
    "title": "CSAGC-IDS: A Dual-Module Deep Learning Network Intrusion Detection Model for Complex and Imbalanced Data",
    "authors": [
      "Yifan Zeng"
    ],
    "abstract": "As computer networks proliferate, the gravity of network intrusions has\nescalated, emphasizing the criticality of network intrusion detection systems\nfor safeguarding security. While deep learning models have exhibited promising\nresults in intrusion detection, they face challenges in managing\nhigh-dimensional, complex traffic patterns and imbalanced data categories. This\npaper presents CSAGC-IDS, a network intrusion detection model based on deep\nlearning techniques. CSAGC-IDS integrates SC-CGAN, a self-attention-enhanced\nconvolutional conditional generative adversarial network that generates\nhigh-quality data to mitigate class imbalance. Furthermore, CSAGC-IDS\nintegrates CSCA-CNN, a convolutional neural network enhanced through cost\nsensitive learning and channel attention mechanism, to extract features from\ncomplex traffic data for precise detection. Experiments conducted on the\nNSL-KDD dataset. CSAGC-IDS achieves an accuracy of 84.55% and an F1-score of\n84.52% in five-class classification task, and an accuracy of 91.09% and an F1\nscore of 92.04% in binary classification task.Furthermore, this paper provides\nan interpretability analysis of the proposed model, using SHAP and LIME to\nexplain the decision-making mechanisms of the model.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14027v1",
    "published_date": "2025-05-20 07:27:51 UTC",
    "updated_date": "2025-05-20 07:27:51 UTC"
  },
  {
    "arxiv_id": "2505.14024v1",
    "title": "FedGraM: Defending Against Untargeted Attacks in Federated Learning via Embedding Gram Matrix",
    "authors": [
      "Di Wu",
      "Qian Li",
      "Heng Yang",
      "Yong Han"
    ],
    "abstract": "Federated Learning (FL) enables geographically distributed clients to\ncollaboratively train machine learning models by sharing only their local\nmodels, ensuring data privacy. However, FL is vulnerable to untargeted attacks\nthat aim to degrade the global model's performance on the underlying data\ndistribution. Existing defense mechanisms attempt to improve FL's resilience\nagainst such attacks, but their effectiveness is limited in practical FL\nenvironments due to data heterogeneity. On the contrary, we aim to detect and\nremove the attacks to mitigate their impact. Generalization contribution plays\na crucial role in distinguishing untargeted attacks. Our observations indicate\nthat, with limited data, the divergence between embeddings representing\ndifferent classes provides a better measure of generalization than direct\naccuracy. In light of this, we propose a novel robust aggregation method,\nFedGraM, designed to defend against untargeted attacks in FL. The server\nmaintains an auxiliary dataset containing one sample per class to support\naggregation. This dataset is fed to the local models to extract embeddings.\nThen, the server calculates the norm of the Gram Matrix of the embeddings for\neach local model. The norm serves as an indicator of each model's inter-class\nseparation capability in the embedding space. FedGraM identifies and removes\npotentially malicious models by filtering out those with the largest norms,\nthen averages the remaining local models to form the global model. We conduct\nextensive experiments to evaluate the performance of FedGraM. Our empirical\nresults show that with limited data samples used to construct the auxiliary\ndataset, FedGraM achieves exceptional performance, outperforming\nstate-of-the-art defense methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14024v1",
    "published_date": "2025-05-20 07:26:54 UTC",
    "updated_date": "2025-05-20 07:26:54 UTC"
  },
  {
    "arxiv_id": "2505.14020v1",
    "title": "Disentangled Multi-span Evolutionary Network against Temporal Knowledge Graph Reasoning",
    "authors": [
      "Hao Dong",
      "Ziyue Qiao",
      "Zhiyuan Ning",
      "Qi Hao",
      "Yi Du",
      "Pengyang Wang",
      "Yuanchun Zhou"
    ],
    "abstract": "Temporal Knowledge Graphs (TKGs), as an extension of static Knowledge Graphs\n(KGs), incorporate the temporal feature to express the transience of knowledge\nby describing when facts occur. TKG extrapolation aims to infer possible future\nfacts based on known history, which has garnered significant attention in\nrecent years. Some existing methods treat TKG as a sequence of independent\nsubgraphs to model temporal evolution patterns, demonstrating impressive\nreasoning performance. However, they still have limitations: 1) In modeling\nsubgraph semantic evolution, they usually neglect the internal structural\ninteractions between subgraphs, which are actually crucial for encoding TKGs.\n2) They overlook the potential smooth features that do not lead to semantic\nchanges, which should be distinguished from the semantic evolution process.\nTherefore, we propose a novel Disentangled Multi-span Evolutionary Network\n(DiMNet) for TKG reasoning. Specifically, we design a multi-span evolution\nstrategy that captures local neighbor features while perceiving historical\nneighbor semantic information, thus enabling internal interactions between\nsubgraphs during the evolution process. To maximize the capture of semantic\nchange patterns, we design a disentangle component that adaptively separates\nnodes' active and stable features, used to dynamically control the influence of\nhistorical semantics on future evolution. Extensive experiments conducted on\nfour real-world TKG datasets show that DiMNet demonstrates substantial\nperformance in TKG reasoning, and outperforms the state-of-the-art up to 22.7%\nin MRR.",
    "categories": [
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to ACL 2025 Findings",
    "pdf_url": "http://arxiv.org/pdf/2505.14020v1",
    "published_date": "2025-05-20 07:22:03 UTC",
    "updated_date": "2025-05-20 07:22:03 UTC"
  },
  {
    "arxiv_id": "2505.14742v1",
    "title": "Quaff: Quantized Parameter-Efficient Fine-Tuning under Outlier Spatial Stability Hypothesis",
    "authors": [
      "Hong Huang",
      "Dapeng Wu"
    ],
    "abstract": "Large language models (LLMs) have made exciting achievements across various\ndomains, yet their deployment on resource-constrained personal devices remains\nhindered by the prohibitive computational and memory demands of task-specific\nfine-tuning. While quantization offers a pathway to efficiency, existing\nmethods struggle to balance performance and overhead, either incurring high\ncomputational/memory costs or failing to address activation outliers, a\ncritical bottleneck in quantized fine-tuning. To address these challenges, we\npropose the Outlier Spatial Stability Hypothesis (OSSH): During fine-tuning,\ncertain activation outlier channels retain stable spatial positions across\ntraining iterations. Building on OSSH, we propose Quaff, a Quantized\nparameter-efficient fine-tuning framework for LLMs, optimizing low-precision\nactivation representations through targeted momentum scaling. Quaff dynamically\nsuppresses outliers exclusively in invariant channels using lightweight\noperations, eliminating full-precision weight storage and global rescaling\nwhile reducing quantization errors. Extensive experiments across ten benchmarks\nvalidate OSSH and demonstrate Quaff's efficacy. Specifically, on the GPQA\nreasoning benchmark, Quaff achieves a 1.73x latency reduction and 30% memory\nsavings over full-precision fine-tuning while improving accuracy by 0.6% on the\nPhi-3 model, reconciling the triple trade-off between efficiency, performance,\nand deployability. By enabling consumer-grade GPU fine-tuning (e.g., RTX 2080\nSuper) without sacrificing model utility, Quaff democratizes personalized LLM\ndeployment. The code is available at https://github.com/Little0o0/Quaff.git.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14742v1",
    "published_date": "2025-05-20 07:19:36 UTC",
    "updated_date": "2025-05-20 07:19:36 UTC"
  },
  {
    "arxiv_id": "2505.14005v1",
    "title": "Towards Comprehensive and Prerequisite-Free Explainer for Graph Neural Networks",
    "authors": [
      "Han Zhang",
      "Yan Wang",
      "Guanfeng Liu",
      "Pengfei Ding",
      "Huaxiong Wang",
      "Kwok-Yan Lam"
    ],
    "abstract": "To enhance the reliability and credibility of graph neural networks (GNNs)\nand improve the transparency of their decision logic, a new field of\nexplainability of GNNs (XGNN) has emerged. However, two major limitations\nseverely degrade the performance and hinder the generalizability of existing\nXGNN methods: they (a) fail to capture the complete decision logic of GNNs\nacross diverse distributions in the entire dataset's sample space, and (b)\nimpose strict prerequisites on edge properties and GNN internal accessibility.\nTo address these limitations, we propose OPEN, a novel c\\textbf{O}mprehensive\nand \\textbf{P}rerequisite-free \\textbf{E}xplainer for G\\textbf{N}Ns. OPEN, as\nthe first work in the literature, can infer and partition the entire dataset's\nsample space into multiple environments, each containing graphs that follow a\ndistinct distribution. OPEN further learns the decision logic of GNNs across\ndifferent distributions by sampling subgraphs from each environment and\nanalyzing their predictions, thus eliminating the need for strict\nprerequisites. Experimental results demonstrate that OPEN captures nearly\ncomplete decision logic of GNNs, outperforms state-of-the-art methods in\nfidelity while maintaining similar efficiency, and enhances robustness in\nreal-world scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by IJCAI 2025 AI4Tech Track",
    "pdf_url": "http://arxiv.org/pdf/2505.14005v1",
    "published_date": "2025-05-20 07:01:47 UTC",
    "updated_date": "2025-05-20 07:01:47 UTC"
  },
  {
    "arxiv_id": "2505.14741v1",
    "title": "Communication-Efficient Diffusion Denoising Parallelization via Reuse-then-Predict Mechanism",
    "authors": [
      "Kunyun Wang",
      "Bohan Li",
      "Kai Yu",
      "Minyi Guo",
      "Jieru Zhao"
    ],
    "abstract": "Diffusion models have emerged as a powerful class of generative models across\nvarious modalities, including image, video, and audio synthesis. However, their\ndeployment is often limited by significant inference latency, primarily due to\nthe inherently sequential nature of the denoising process. While existing\nparallelization strategies attempt to accelerate inference by distributing\ncomputation across multiple devices, they typically incur high communication\noverhead, hindering deployment on commercial hardware. To address this\nchallenge, we propose \\textbf{ParaStep}, a novel parallelization method based\non a reuse-then-predict mechanism that parallelizes diffusion inference by\nexploiting similarity between adjacent denoising steps. Unlike prior approaches\nthat rely on layer-wise or stage-wise communication, ParaStep employs\nlightweight, step-wise communication, substantially reducing overhead. ParaStep\nachieves end-to-end speedups of up to \\textbf{3.88}$\\times$ on SVD,\n\\textbf{2.43}$\\times$ on CogVideoX-2b, and \\textbf{6.56}$\\times$ on\nAudioLDM2-large, while maintaining generation quality. These results highlight\nParaStep as a scalable and communication-efficient solution for accelerating\ndiffusion inference, particularly in bandwidth-constrained environments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14741v1",
    "published_date": "2025-05-20 06:58:40 UTC",
    "updated_date": "2025-05-20 06:58:40 UTC"
  },
  {
    "arxiv_id": "2505.14001v1",
    "title": "VeRecycle: Reclaiming Guarantees from Probabilistic Certificates for Stochastic Dynamical Systems after Change",
    "authors": [
      "Sterre Lutz",
      "Matthijs T. J. Spaan",
      "Anna Lukina"
    ],
    "abstract": "Autonomous systems operating in the real world encounter a range of\nuncertainties. Probabilistic neural Lyapunov certification is a powerful\napproach to proving safety of nonlinear stochastic dynamical systems. When\nfaced with changes beyond the modeled uncertainties, e.g., unidentified\nobstacles, probabilistic certificates must be transferred to the new system\ndynamics. However, even when the changes are localized in a known part of the\nstate space, state-of-the-art requires complete re-certification, which is\nparticularly costly for neural certificates. We introduce VeRecycle, the first\nframework to formally reclaim guarantees for discrete-time stochastic dynamical\nsystems. VeRecycle efficiently reuses probabilistic certificates when the\nsystem dynamics deviate only in a given subset of states. We present a general\ntheoretical justification and algorithmic implementation. Our experimental\nevaluation shows scenarios where VeRecycle both saves significant computational\neffort and achieves competitive probabilistic guarantees in compositional\nneural control.",
    "categories": [
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.AI",
    "comment": "accepted to IJCAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2505.14001v1",
    "published_date": "2025-05-20 06:54:19 UTC",
    "updated_date": "2025-05-20 06:54:19 UTC"
  },
  {
    "arxiv_id": "2505.13995v1",
    "title": "Social Sycophancy: A Broader Understanding of LLM Sycophancy",
    "authors": [
      "Myra Cheng",
      "Sunny Yu",
      "Cinoo Lee",
      "Pranav Khadpe",
      "Lujain Ibrahim",
      "Dan Jurafsky"
    ],
    "abstract": "A serious risk to the safety and utility of LLMs is sycophancy, i.e.,\nexcessive agreement with and flattery of the user. Yet existing work focuses on\nonly one aspect of sycophancy: agreement with users' explicitly stated beliefs\nthat can be compared to a ground truth. This overlooks forms of sycophancy that\narise in ambiguous contexts such as advice and support-seeking, where there is\nno clear ground truth, yet sycophancy can reinforce harmful implicit\nassumptions, beliefs, or actions. To address this gap, we introduce a richer\ntheory of social sycophancy in LLMs, characterizing sycophancy as the excessive\npreservation of a user's face (the positive self-image a person seeks to\nmaintain in an interaction). We present ELEPHANT, a framework for evaluating\nsocial sycophancy across five face-preserving behaviors (emotional validation,\nmoral endorsement, indirect language, indirect action, and accepting framing)\non two datasets: open-ended questions (OEQ) and Reddit's r/AmITheAsshole\n(AITA). Across eight models, we show that LLMs consistently exhibit high rates\nof social sycophancy: on OEQ, they preserve face 47% more than humans, and on\nAITA, they affirm behavior deemed inappropriate by crowdsourced human judgments\nin 42% of cases. We further show that social sycophancy is rewarded in\npreference datasets and is not easily mitigated. Our work provides theoretical\ngrounding and empirical tools (datasets and code) for understanding and\naddressing this under-recognized but consequential issue.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.13995v1",
    "published_date": "2025-05-20 06:45:17 UTC",
    "updated_date": "2025-05-20 06:45:17 UTC"
  },
  {
    "arxiv_id": "2505.13994v1",
    "title": "Divide by Question, Conquer by Agent: SPLIT-RAG with Question-Driven Graph Partitioning",
    "authors": [
      "Ruiyi Yang",
      "Hao Xue",
      "Imran Razzak",
      "Hakim Hacid",
      "Flora D. Salim"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) systems empower large language models\n(LLMs) with external knowledge, yet struggle with efficiency-accuracy\ntrade-offs when scaling to large knowledge graphs. Existing approaches often\nrely on monolithic graph retrieval, incurring unnecessary latency for simple\nqueries and fragmented reasoning for complex multi-hop questions. To address\nthese challenges, this paper propose SPLIT-RAG, a multi-agent RAG framework\nthat addresses these limitations with question-driven semantic graph\npartitioning and collaborative subgraph retrieval. The innovative framework\nfirst create Semantic Partitioning of Linked Information, then use the\nType-Specialized knowledge base to achieve Multi-Agent RAG. The attribute-aware\ngraph segmentation manages to divide knowledge graphs into semantically\ncoherent subgraphs, ensuring subgraphs align with different query types, while\nlightweight LLM agents are assigned to partitioned subgraphs, and only relevant\npartitions are activated during retrieval, thus reduce search space while\nenhancing efficiency. Finally, a hierarchical merging module resolves\ninconsistencies across subgraph-derived answers through logical verifications.\nExtensive experimental validation demonstrates considerable improvements\ncompared to existing approaches.",
    "categories": [
      "cs.AI",
      "cs.IR",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "20 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.13994v1",
    "published_date": "2025-05-20 06:44:34 UTC",
    "updated_date": "2025-05-20 06:44:34 UTC"
  },
  {
    "arxiv_id": "2505.14739v1",
    "title": "Time Series Similarity Score Functions to Monitor and Interact with the Training and Denoising Process of a Time Series Diffusion Model applied to a Human Activity Recognition Dataset based on IMUs",
    "authors": [
      "Heiko Oppel",
      "Andreas Spilz",
      "Michael Munz"
    ],
    "abstract": "Denoising diffusion probabilistic models are able to generate synthetic\nsensor signals. The training process of such a model is controlled by a loss\nfunction which measures the difference between the noise that was added in the\nforward process and the noise that was predicted by the diffusion model. This\nenables the generation of realistic data. However, the randomness within the\nprocess and the loss function itself makes it difficult to estimate the quality\nof the data. Therefore, we examine multiple similarity metrics and adapt an\nexisting metric to overcome this issue by monitoring the training and\nsynthetisation process using those metrics. The adapted metric can even be\nfine-tuned on the input data to comply with the requirements of an underlying\nclassification task. We were able to significantly reduce the amount of\ntraining epochs without a performance reduction in the classification task. An\noptimized training process not only saves resources, but also reduces the time\nfor training generative models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14739v1",
    "published_date": "2025-05-20 06:38:17 UTC",
    "updated_date": "2025-05-20 06:38:17 UTC"
  },
  {
    "arxiv_id": "2505.13989v2",
    "title": "When LLMs meet open-world graph learning: a new perspective for unlabeled data uncertainty",
    "authors": [
      "Yanzhe Wen",
      "Xunkai Li",
      "Qi Zhang",
      "Zhu Lei",
      "Guang Zeng",
      "Rong-Hua Li",
      "Guoren Wang"
    ],
    "abstract": "Recently, large language models (LLMs) have significantly advanced\ntext-attributed graph (TAG) learning. However, existing methods inadequately\nhandle data uncertainty in open-world scenarios, especially concerning limited\nlabeling and unknown-class nodes. Prior solutions typically rely on isolated\nsemantic or structural approaches for unknown-class rejection, lacking\neffective annotation pipelines. To address these limitations, we propose\nOpen-world Graph Assistant (OGA), an LLM-based framework that combines adaptive\nlabel traceability, which integrates semantics and topology for unknown-class\nrejection, and a graph label annotator to enable model updates using newly\nannotated nodes. Comprehensive experiments demonstrate OGA's effectiveness and\npracticality.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.13989v2",
    "published_date": "2025-05-20 06:37:18 UTC",
    "updated_date": "2025-05-21 04:23:56 UTC"
  },
  {
    "arxiv_id": "2505.13986v1",
    "title": "Solving Normalized Cut Problem with Constrained Action Space",
    "authors": [
      "Qize Jiang",
      "Linsey Pang",
      "Alice Gatti",
      "Mahima Aggarwa",
      "Giovanna Vantin",
      "Xiaosong Ma",
      "Weiwei Sun",
      "Sanjay Chawla"
    ],
    "abstract": "Reinforcement Learning (RL) has emerged as an important paradigm to solve\ncombinatorial optimization problems primarily due to its ability to learn\nheuristics that can generalize across problem instances. However, integrating\nexternal knowledge that will steer combinatorial optimization problem solutions\ntowards domain appropriate outcomes remains an extremely challenging task. In\nthis paper, we propose the first RL solution that uses constrained action\nspaces to guide the normalized cut problem towards pre-defined template\ninstances. Using transportation networks as an example domain, we create a\nWedge and Ring Transformer that results in graph partitions that are shaped in\nform of Wedges and Rings and which are likely to be closer to natural optimal\npartitions. However, our approach is general as it is based on principles that\ncan be generalized to other domains.",
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.LG",
      "I.2.8"
    ],
    "primary_category": "math.OC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.13986v1",
    "published_date": "2025-05-20 06:33:39 UTC",
    "updated_date": "2025-05-20 06:33:39 UTC"
  },
  {
    "arxiv_id": "2505.13973v1",
    "title": "Toward Effective Reinforcement Learning Fine-Tuning for Medical VQA in Vision-Language Models",
    "authors": [
      "Wenhui Zhu",
      "Xuanzhao Dong",
      "Xin Li",
      "Peijie Qiu",
      "Xiwen Chen",
      "Abolfazl Razi",
      "Aris Sotiras",
      "Yi Su",
      "Yalin Wang"
    ],
    "abstract": "Recently, reinforcement learning (RL)-based tuning has shifted the trajectory\nof Multimodal Large Language Models (MLLMs), particularly following the\nintroduction of Group Relative Policy Optimization (GRPO). However, directly\napplying it to medical tasks remains challenging for achieving clinically\ngrounded model behavior. Motivated by the need to align model response with\nclinical expectations, we investigate four critical dimensions that affect the\neffectiveness of RL-based tuning in medical visual question answering (VQA):\nbase model initialization strategy, the role of medical semantic alignment, the\nimpact of length-based rewards on long-chain reasoning, and the influence of\nbias. We conduct extensive experiments to analyze these factors for medical\nMLLMs, providing new insights into how models are domain-specifically\nfine-tuned. Additionally, our results also demonstrate that GRPO-based RL\ntuning consistently outperforms standard supervised fine-tuning (SFT) in both\naccuracy and reasoning quality.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.13973v1",
    "published_date": "2025-05-20 06:12:20 UTC",
    "updated_date": "2025-05-20 06:12:20 UTC"
  },
  {
    "arxiv_id": "2505.13971v1",
    "title": "The Multimodal Information Based Speech Processing (MISP) 2025 Challenge: Audio-Visual Diarization and Recognition",
    "authors": [
      "Ming Gao",
      "Shilong Wu",
      "Hang Chen",
      "Jun Du",
      "Chin-Hui Lee",
      "Shinji Watanabe",
      "Jingdong Chen",
      "Siniscalchi Sabato Marco",
      "Odette Scharenborg"
    ],
    "abstract": "Meetings are a valuable yet challenging scenario for speech applications due\nto complex acoustic conditions. This paper summarizes the outcomes of the MISP\n2025 Challenge, hosted at Interspeech 2025, which focuses on multi-modal,\nmulti-device meeting transcription by incorporating video modality alongside\naudio. The tasks include Audio-Visual Speaker Diarization (AVSD), Audio-Visual\nSpeech Recognition (AVSR), and Audio-Visual Diarization and Recognition (AVDR).\nWe present the challenge's objectives, tasks, dataset, baseline systems, and\nsolutions proposed by participants. The best-performing systems achieved\nsignificant improvements over the baseline: the top AVSD model achieved a\nDiarization Error Rate (DER) of 8.09%, improving by 7.43%; the top AVSR system\nachieved a Character Error Rate (CER) of 9.48%, improving by 10.62%; and the\nbest AVDR system achieved a concatenated minimum-permutation Character Error\nRate (cpCER) of 11.56%, improving by 72.49%.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted by Interspeech 2025. Camera-ready version",
    "pdf_url": "http://arxiv.org/pdf/2505.13971v1",
    "published_date": "2025-05-20 06:11:51 UTC",
    "updated_date": "2025-05-20 06:11:51 UTC"
  },
  {
    "arxiv_id": "2505.13969v1",
    "title": "Hypothesis on the Functional Advantages of the Selection-Broadcast Cycle Structure: Global Workspace Theory and Dealing with a Real-Time World",
    "authors": [
      "Junya Nakanishi",
      "Jun Baba",
      "Yuichiro Yoshikawa",
      "Hiroko Kamide",
      "Hiroshi Ishiguro"
    ],
    "abstract": "This paper discusses the functional advantages of the Selection-Broadcast\nCycle structure proposed by Global Workspace Theory (GWT), inspired by human\nconsciousness, particularly focusing on its applicability to artificial\nintelligence and robotics in dynamic, real-time scenarios. While previous\nstudies often examined the Selection and Broadcast processes independently,\nthis research emphasizes their combined cyclic structure and the resulting\nbenefits for real-time cognitive systems. Specifically, the paper identifies\nthree primary benefits: Dynamic Thinking Adaptation, Experience-Based\nAdaptation, and Immediate Real-Time Adaptation. This work highlights GWT's\npotential as a cognitive architecture suitable for sophisticated\ndecision-making and adaptive performance in unsupervised, dynamic environments.\nIt suggests new directions for the development and implementation of robust,\ngeneral-purpose AI and robotics systems capable of managing complex, real-world\ntasks.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.13969v1",
    "published_date": "2025-05-20 06:07:21 UTC",
    "updated_date": "2025-05-20 06:07:21 UTC"
  },
  {
    "arxiv_id": "2505.14738v1",
    "title": "R&D-Agent: Automating Data-Driven AI Solution Building Through LLM-Powered Automated Research, Development, and Evolution",
    "authors": [
      "Xu Yang",
      "Xiao Yang",
      "Shikai Fang",
      "Bowen Xian",
      "Yuante Li",
      "Jian Wang",
      "Minrui Xu",
      "Haoran Pan",
      "Xinpeng Hong",
      "Weiqing Liu",
      "Yelong Shen",
      "Weizhu Chen",
      "Jiang Bian"
    ],
    "abstract": "Recent advances in AI and ML have transformed data science, yet increasing\ncomplexity and expertise requirements continue to hinder progress. While\ncrowdsourcing platforms alleviate some challenges, high-level data science\ntasks remain labor-intensive and iterative. To overcome these limitations, we\nintroduce R&D-Agent, a dual-agent framework for iterative exploration. The\nResearcher agent uses performance feedback to generate ideas, while the\nDeveloper agent refines code based on error feedback. By enabling multiple\nparallel exploration traces that merge and enhance one another, R&D-Agent\nnarrows the gap between automated solutions and expert-level performance.\nEvaluated on MLE-Bench, R&D-Agent emerges as the top-performing machine\nlearning engineering agent, demonstrating its potential to accelerate\ninnovation and improve precision across diverse data science applications. We\nhave open-sourced R&D-Agent on GitHub: https://github.com/microsoft/RD-Agent.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "7 pages, 1 figure, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2505.14738v1",
    "published_date": "2025-05-20 06:07:00 UTC",
    "updated_date": "2025-05-20 06:07:00 UTC"
  },
  {
    "arxiv_id": "2505.13965v1",
    "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring",
    "authors": [
      "Jiamin Su",
      "Yibo Yan",
      "Zhuoran Gao",
      "Han Zhang",
      "Xiang Liu",
      "Xuming Hu"
    ],
    "abstract": "Automated Essay Scoring (AES) is crucial for modern education, particularly\nwith the increasing prevalence of multimodal assessments. However, traditional\nAES methods struggle with evaluation generalizability and multimodal\nperception, while even recent Multimodal Large Language Model (MLLM)-based\napproaches can produce hallucinated justifications and scores misaligned with\nhuman judgment. To address the limitations, we introduce CAFES, the first\ncollaborative multi-agent framework specifically designed for AES. It\norchestrates three specialized agents: an Initial Scorer for rapid,\ntrait-specific evaluations; a Feedback Pool Manager to aggregate detailed,\nevidence-grounded strengths; and a Reflective Scorer that iteratively refines\nscores based on this feedback to enhance human alignment. Extensive\nexperiments, using state-of-the-art MLLMs, achieve an average relative\nimprovement of 21% in Quadratic Weighted Kappa (QWK) against ground truth,\nespecially for grammatical and lexical diversity. Our proposed CAFES framework\npaves the way for an intelligent multimodal AES system. The code will be\navailable upon acceptance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "arXiv admin note: substantial text overlap with arXiv:2502.11916",
    "pdf_url": "http://arxiv.org/pdf/2505.13965v1",
    "published_date": "2025-05-20 06:05:56 UTC",
    "updated_date": "2025-05-20 06:05:56 UTC"
  },
  {
    "arxiv_id": "2505.13949v1",
    "title": "FlashThink: An Early Exit Method For Efficient Reasoning",
    "authors": [
      "Guochao Jiang",
      "Guofeng Quan",
      "Zepeng Ding",
      "Ziqin Luo",
      "Dixuan Wang",
      "Zheng Hu"
    ],
    "abstract": "Large Language Models (LLMs) have shown impressive performance in reasoning\ntasks. However, LLMs tend to generate excessively long reasoning content,\nleading to significant computational overhead. Our observations indicate that\neven on simple problems, LLMs tend to produce unnecessarily lengthy reasoning\ncontent, which is against intuitive expectations. Preliminary experiments show\nthat at a certain point during the generation process, the model is already\ncapable of producing the correct solution without completing the full reasoning\ncontent. Therefore, we consider that the reasoning process of the model can be\nexited early to achieve the purpose of efficient reasoning. We introduce a\nverification model that identifies the exact moment when the model can stop\nreasoning and still provide the correct answer. Comprehensive experiments on\nfour different benchmarks demonstrate that our proposed method, FlashThink,\neffectively shortens the reasoning content while preserving the model accuracy.\nFor the Deepseek-R1 and QwQ-32B models, we reduced the length of reasoning\ncontent by 77.04% and 77.47%, respectively, without reducing the accuracy.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.13949v1",
    "published_date": "2025-05-20 05:28:21 UTC",
    "updated_date": "2025-05-20 05:28:21 UTC"
  },
  {
    "arxiv_id": "2505.13948v1",
    "title": "Memory-Centric Embodied Question Answer",
    "authors": [
      "Mingliang Zhai",
      "Zhi Gao",
      "Yuwei Wu",
      "Yunde Jia"
    ],
    "abstract": "Embodied Question Answering (EQA) requires agents to autonomously explore and\nunderstand the environment to answer context-dependent questions. Existing\nframeworks typically center around the planner, which guides the stopping\nmodule, memory module, and answering module for reasoning. In this paper, we\npropose a memory-centric EQA framework named MemoryEQA. Unlike planner-centric\nEQA models where the memory module cannot fully interact with other modules,\nMemoryEQA flexible feeds memory information into all modules, thereby enhancing\nefficiency and accuracy in handling complex tasks, such as those involving\nmultiple targets across different regions. Specifically, we establish a\nmulti-modal hierarchical memory mechanism, which is divided into global memory\nthat stores language-enhanced scene maps, and local memory that retains\nhistorical observations and state information. When performing EQA tasks, the\nmulti-modal large language model is leveraged to convert memory information\ninto the required input formats for injection into different modules. To\nevaluate EQA models' memory capabilities, we constructed the MT-HM3D dataset\nbased on HM3D, comprising 1,587 question-answer pairs involving multiple\ntargets across various regions, which requires agents to maintain memory of\nexploration-acquired target information. Experimental results on HM-EQA,\nMT-HM3D, and OpenEQA demonstrate the effectiveness of our framework, where a\n19.8% performance gain on MT-HM3D compared to baseline model further\nunderscores memory capability's pivotal role in resolving complex tasks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CL",
    "comment": "14pages, 7 figures, 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2505.13948v1",
    "published_date": "2025-05-20 05:27:57 UTC",
    "updated_date": "2025-05-20 05:27:57 UTC"
  },
  {
    "arxiv_id": "2505.13946v1",
    "title": "Visual Instruction Bottleneck Tuning",
    "authors": [
      "Changdae Oh",
      "Jiatong Li",
      "Shawn Im",
      "Yixuan Li"
    ],
    "abstract": "Despite widespread adoption, multimodal large language models (MLLMs) suffer\nperformance degradation when encountering unfamiliar queries under distribution\nshifts. Existing methods to improve MLLM generalization typically require\neither more instruction data or larger advanced model architectures, both of\nwhich incur non-trivial human labor or computational costs. In this work, we\ntake an alternative approach to enhance the robustness of MLLMs under\ndistribution shifts, from a representation learning perspective. Inspired by\nthe information bottleneck (IB) principle, we derive a variational lower bound\nof the IB for MLLMs and devise a practical implementation, Visual Instruction\nBottleneck Tuning (Vittle). We then provide a theoretical justification of\nVittle by revealing its connection to an information-theoretic robustness\nmetric of MLLM. Empirical validation of three MLLMs on open-ended and\nclosed-form question answering and object hallucination detection tasks over 45\ndatasets, including 30 shift scenarios, demonstrates that Vittle consistently\nimproves the MLLM's robustness under shifts by pursuing the learning of a\nminimal sufficient representation.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.13946v1",
    "published_date": "2025-05-20 05:24:53 UTC",
    "updated_date": "2025-05-20 05:24:53 UTC"
  },
  {
    "arxiv_id": "2505.13941v1",
    "title": "MLZero: A Multi-Agent System for End-to-end Machine Learning Automation",
    "authors": [
      "Haoyang Fang",
      "Boran Han",
      "Nick Erickson",
      "Xiyuan Zhang",
      "Su Zhou",
      "Anirudh Dagar",
      "Jiani Zhang",
      "Ali Caner Turkmen",
      "Cuixiong Hu",
      "Huzefa Rangwala",
      "Ying Nian Wu",
      "Bernie Wang",
      "George Karypis"
    ],
    "abstract": "Existing AutoML systems have advanced the automation of machine learning\n(ML); however, they still require substantial manual configuration and expert\ninput, particularly when handling multimodal data. We introduce MLZero, a novel\nmulti-agent framework powered by Large Language Models (LLMs) that enables\nend-to-end ML automation across diverse data modalities with minimal human\nintervention. A cognitive perception module is first employed, transforming raw\nmultimodal inputs into perceptual context that effectively guides the\nsubsequent workflow. To address key limitations of LLMs, such as hallucinated\ncode generation and outdated API knowledge, we enhance the iterative code\ngeneration process with semantic and episodic memory. MLZero demonstrates\nsuperior performance on MLE-Bench Lite, outperforming all competitors in both\nsuccess rate and solution quality, securing six gold medals. Additionally, when\nevaluated on our Multimodal AutoML Agent Benchmark, which includes 25 more\nchallenging tasks spanning diverse data modalities, MLZero outperforms the\ncompeting methods by a large margin with a success rate of 0.92 (+263.6\\%) and\nan average rank of 2.28. Our approach maintains its robust effectiveness even\nwith a compact 8B LLM, outperforming full-size systems from existing solutions.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.13941v1",
    "published_date": "2025-05-20 05:20:53 UTC",
    "updated_date": "2025-05-20 05:20:53 UTC"
  },
  {
    "arxiv_id": "2505.13940v1",
    "title": "DrugPilot: LLM-based Parameterized Reasoning Agent for Drug Discovery",
    "authors": [
      "Kun Li",
      "Zhennan Wu",
      "Shoupeng Wang",
      "Wenbin Hu"
    ],
    "abstract": "In the field of AI4Science, large-scale language models (LLMs) show great\npotential to parse complex scientific semantics, integrate cross-disciplinary\nknowledge, and assist critical task research. However, in the field of drug\ndiscovery, despite the optimization through professional data pre-training,\ncontext window expansion, and internet search, the existing LLMs are still\nfacing challenges such as massive multi-modal and heterogeneous data\nprocessing, domain knowledge dynamic updating delay, and insufficient\nconfidence in predicting the results of complex computational tasks. To address\nthese challenges, we propose the DrugPilot, an LLM-based agent with\nparameterized reasoning for drug discovery. DrugPilot addresses key limitations\nof traditional end-to-end LLM prediction approaches through its parametric\ninference architecture. This agent system supports major phases of the drug\ndiscovery pipeline, facilitating automated planning and execution of\nmulti-stage research tasks. To address the critical challenge of multi-modal\ndrug data analysis (incorporating both public datasets and user-submitted\ndata), we developed an interactive parameterized memory pool. This innovative\ncomponent standardizes real-world drug data into parametric representations,\nsimultaneously enabling efficient knowledge retrieval in multi-turn dialogue\nwhile mitigating the information loss inherent in text-based data transmission.\nAdditionally, we created a drug instruct dataset across 8 essential drug\ndiscovery tasks for model fine-tuning and evaluation. Based on the Berkeley\nfunction calling evaluation framework, DrugPilot demonstrated the most advanced\ntool calling capabilities on our drug discovery tool instruction dataset,\noutperforming existing agents (e.g., ReAct, LoT). Specifically, it achieves\ntask completion rates of 98.0%, 93.5%, and 64.0% on simple, multiple, and\nmulti-turn tasks, respectively.",
    "categories": [
      "cs.AI",
      "q-bio.BM"
    ],
    "primary_category": "cs.AI",
    "comment": "22 pages, 10 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2505.13940v1",
    "published_date": "2025-05-20 05:18:15 UTC",
    "updated_date": "2025-05-20 05:18:15 UTC"
  },
  {
    "arxiv_id": "2505.13938v2",
    "title": "CLEVER: A Curated Benchmark for Formally Verified Code Generation",
    "authors": [
      "Amitayush Thakur",
      "Jasper Lee",
      "George Tsoukalas",
      "Meghana Sistla",
      "Matthew Zhao",
      "Stefan Zetzsche",
      "Greg Durrett",
      "Yisong Yue",
      "Swarat Chaudhuri"
    ],
    "abstract": "We introduce ${\\rm C{\\small LEVER}}$, a high-quality, curated benchmark of\n161 problems for end-to-end verified code generation in Lean. Each problem\nconsists of (1) the task of generating a specification that matches a held-out\nground-truth specification, and (2) the task of generating a Lean\nimplementation that provably satisfies this specification. Unlike prior\nbenchmarks, ${\\rm C{\\small LEVER}}$ avoids test-case supervision, LLM-generated\nannotations, and specifications that leak implementation logic or allow vacuous\nsolutions. All outputs are verified post-hoc using Lean's type checker to\nensure machine-checkable correctness. We use ${\\rm C{\\small LEVER}}$ to\nevaluate several few-shot and agentic approaches based on state-of-the-art\nlanguage models. These methods all struggle to achieve full verification,\nestablishing it as a challenging frontier benchmark for program synthesis and\nformal reasoning. Our benchmark can be found on\nGitHub(https://github.com/trishullab/clever) as well as\nHuggingFace(https://huggingface.co/datasets/amitayusht/clever). All our\nevaluation code is also available\nonline(https://github.com/trishullab/clever-prover).",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.LO",
      "cs.PL",
      "cs.SE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.13938v2",
    "published_date": "2025-05-20 05:15:47 UTC",
    "updated_date": "2025-05-21 03:14:32 UTC"
  },
  {
    "arxiv_id": "2505.13936v1",
    "title": "EEG-to-Text Translation: A Model for Deciphering Human Brain Activity",
    "authors": [
      "Saydul Akbar Murad",
      "Ashim Dahal",
      "Nick Rahimi"
    ],
    "abstract": "With the rapid advancement of large language models like Gemini, GPT, and\nothers, bridging the gap between the human brain and language processing has\nbecome an important area of focus. To address this challenge, researchers have\ndeveloped various models to decode EEG signals into text. However, these models\nstill face significant performance limitations. To overcome these shortcomings,\nwe propose a new model, R1 Translator, which aims to improve the performance of\nEEG-to-text decoding. The R1 Translator model combines a bidirectional LSTM\nencoder with a pretrained transformer-based decoder, utilizing EEG features to\nproduce high-quality text outputs. The model processes EEG embeddings through\nthe LSTM to capture sequential dependencies, which are then fed into the\ntransformer decoder for effective text generation. The R1 Translator excels in\nROUGE metrics, outperforming both T5 (previous research) and Brain Translator.\nSpecifically, R1 achieves a ROUGE-1 score of 38.00% (P), which is up to 9%\nhigher than T5 (34.89%) and 3% better than Brain (35.69%). It also leads in\nROUGE-L, with a F1 score of 32.51%, outperforming T5 by 3% (29.67%) and Brain\nby 2% (30.38%). In terms of CER, R1 achieves a CER of 0.5795, which is 2% lower\nthan T5 (0.5917) and 4% lower than Brain (0.6001). Additionally, R1 performs\nbetter in WER with a score of 0.7280, outperforming T5 by 4.3% (0.7610) and\nBrain by 3.6% (0.7553). Code is available at\nhttps://github.com/Mmurrad/EEG-To-text.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.13936v1",
    "published_date": "2025-05-20 05:04:15 UTC",
    "updated_date": "2025-05-20 05:04:15 UTC"
  },
  {
    "arxiv_id": "2505.13934v1",
    "title": "RLVR-World: Training World Models with Reinforcement Learning",
    "authors": [
      "Jialong Wu",
      "Shaofeng Yin",
      "Ningya Feng",
      "Mingsheng Long"
    ],
    "abstract": "World models predict state transitions in response to actions and are\nincreasingly developed across diverse modalities. However, standard training\nobjectives such as maximum likelihood estimation (MLE) often misalign with\ntask-specific goals of world models, i.e., transition prediction metrics like\naccuracy or perceptual quality. In this paper, we present RLVR-World, a unified\nframework that leverages reinforcement learning with verifiable rewards (RLVR)\nto directly optimize world models for such metrics. Despite formulating world\nmodeling as autoregressive prediction of tokenized sequences, RLVR-World\nevaluates metrics of decoded predictions as verifiable rewards. We demonstrate\nsubstantial performance gains on both language- and video-based world models\nacross domains, including text games, web navigation, and robot manipulation.\nOur work indicates that, beyond recent advances in reasoning language models,\nRLVR offers a promising post-training paradigm for enhancing the utility of\ngenerative models more broadly.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Code is available at project website:\n  https://thuml.github.io/RLVR-World/",
    "pdf_url": "http://arxiv.org/pdf/2505.13934v1",
    "published_date": "2025-05-20 05:02:53 UTC",
    "updated_date": "2025-05-20 05:02:53 UTC"
  },
  {
    "arxiv_id": "2505.13921v1",
    "title": "APEX: Empowering LLMs with Physics-Based Task Planning for Real-time Insight",
    "authors": [
      "Wanjing Huang",
      "Weixiang Yan",
      "Zhen Zhang",
      "Ambuj Singh"
    ],
    "abstract": "Large Language Models (LLMs) demonstrate strong reasoning and task planning\ncapabilities but remain fundamentally limited in physical interaction modeling.\nExisting approaches integrate perception via Vision-Language Models (VLMs) or\nadaptive decision-making through Reinforcement Learning (RL), but they fail to\ncapture dynamic object interactions or require task-specific training, limiting\ntheir real-world applicability. We introduce APEX (Anticipatory\nPhysics-Enhanced Execution), a framework that equips LLMs with physics-driven\nforesight for real-time task planning. APEX constructs structured graphs to\nidentify and model the most relevant dynamic interactions in the environment,\nproviding LLMs with explicit physical state updates. Simultaneously, APEX\nprovides low-latency forward simulations of physically feasible actions,\nallowing LLMs to select optimal strategies based on predictive outcomes rather\nthan static observations. We evaluate APEX on three benchmarks designed to\nassess perception, prediction, and decision-making: (1) Physics Reasoning\nBenchmark, testing causal inference and object motion prediction; (2) Tetris,\nevaluating whether physics-informed prediction enhances decision-making\nperformance in long-horizon planning tasks; (3) Dynamic Obstacle Avoidance,\nassessing the immediate integration of perception and action feasibility\nanalysis. APEX significantly outperforms standard LLMs and VLM-based models,\ndemonstrating the necessity of explicit physics reasoning for bridging the gap\nbetween language-based intelligence and real-world task execution. The source\ncode and experiment setup are publicly available at\nhttps://github.com/hwj20/APEX_EXP .",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.13921v1",
    "published_date": "2025-05-20 04:34:58 UTC",
    "updated_date": "2025-05-20 04:34:58 UTC"
  },
  {
    "arxiv_id": "2505.13914v1",
    "title": "Parallel Belief Revision via Order Aggregation",
    "authors": [
      "Jake Chandler",
      "Richard Booth"
    ],
    "abstract": "Despite efforts to better understand the constraints that operate on\nsingle-step parallel (aka \"package\", \"multiple\") revision, very little work has\nbeen carried out on how to extend the model to the iterated case. A recent\npaper by Delgrande & Jin outlines a range of relevant rationality postulates.\nWhile many of these are plausible, they lack an underlying unifying\nexplanation. We draw on recent work on iterated parallel contraction to offer a\ngeneral method for extending serial iterated belief revision operators to\nhandle parallel change. This method, based on a family of order aggregators\nknown as TeamQueue aggregators, provides a principled way to recover the\nindependently plausible properties that can be found in the literature, without\nyielding the more dubious ones.",
    "categories": [
      "cs.AI",
      "I.2.4"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.13914v1",
    "published_date": "2025-05-20 04:26:01 UTC",
    "updated_date": "2025-05-20 04:26:01 UTC"
  },
  {
    "arxiv_id": "2505.13911v1",
    "title": "Bronchovascular Tree-Guided Weakly Supervised Learning Method for Pulmonary Segment Segmentation",
    "authors": [
      "Ruijie Zhao",
      "Zuopeng Tan",
      "Xiao Xue",
      "Longfei Zhao",
      "Bing Li",
      "Zicheng Liao",
      "Ying Ming",
      "Jiaru Wang",
      "Ran Xiao",
      "Sirong Piao",
      "Rui Zhao",
      "Qiqi Xu",
      "Wei Song"
    ],
    "abstract": "Pulmonary segment segmentation is crucial for cancer localization and\nsurgical planning. However, the pixel-wise annotation of pulmonary segments is\nlaborious, as the boundaries between segments are indistinguishable in medical\nimages. To this end, we propose a weakly supervised learning (WSL) method,\ntermed Anatomy-Hierarchy Supervised Learning (AHSL), which consults the precise\nclinical anatomical definition of pulmonary segments to perform pulmonary\nsegment segmentation. Since pulmonary segments reside within the lobes and are\ndetermined by the bronchovascular tree, i.e., artery, airway and vein, the\ndesign of the loss function is founded on two principles. First, segment-level\nlabels are utilized to directly supervise the output of the pulmonary segments,\nensuring that they accurately encompass the appropriate bronchovascular tree.\nSecond, lobe-level supervision indirectly oversees the pulmonary segment,\nensuring their inclusion within the corresponding lobe. Besides, we introduce a\ntwo-stage segmentation strategy that incorporates bronchovascular priori\ninformation. Furthermore, a consistency loss is proposed to enhance the\nsmoothness of segment boundaries, along with an evaluation metric designed to\nmeasure the smoothness of pulmonary segment boundaries. Visual inspection and\nevaluation metrics from experiments conducted on a private dataset demonstrate\nthe effectiveness of our method.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.13911v1",
    "published_date": "2025-05-20 04:23:12 UTC",
    "updated_date": "2025-05-20 04:23:12 UTC"
  },
  {
    "arxiv_id": "2505.13909v1",
    "title": "Efficient Agent Training for Computer Use",
    "authors": [
      "Yanheng He",
      "Jiahe Jin",
      "Pengfei Liu"
    ],
    "abstract": "Scaling up high-quality trajectory data has long been a critical bottleneck\nfor developing human-like computer use agents. We introduce PC Agent-E, an\nefficient agent training framework that significantly reduces reliance on\nlarge-scale human demonstrations. Starting with just 312 human-annotated\ncomputer use trajectories, we further improved data quality by synthesizing\ndiverse action decisions with Claude 3.7 Sonnet. Trained on these enriched\ntrajectories, our PC Agent-E model achieved a remarkable 141% relative\nimprovement, surpassing the strong Claude 3.7 Sonnet with extended thinking on\nWindowsAgentArena-V2, an improved benchmark we also released. Furthermore, PC\nAgent-E demonstrates strong generalizability to different operating systems on\nOSWorld. Our findings suggest that strong computer use capabilities can be\nstimulated from a small amount of high-quality trajectory data.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "We open-source our entire suite of code, data, and models to\n  facilitate future research at https://github.com/GAIR-NLP/PC-Agent-E",
    "pdf_url": "http://arxiv.org/pdf/2505.13909v1",
    "published_date": "2025-05-20 04:20:18 UTC",
    "updated_date": "2025-05-20 04:20:18 UTC"
  },
  {
    "arxiv_id": "2505.13906v1",
    "title": "XDementNET: An Explainable Attention Based Deep Convolutional Network to Detect Alzheimer Progression from MRI data",
    "authors": [
      "Soyabul Islam Lincoln",
      "Mirza Mohd Shahriar Maswood"
    ],
    "abstract": "A common neurodegenerative disease, Alzheimer's disease requires a precise\ndiagnosis and efficient treatment, particularly in light of escalating\nhealthcare expenses and the expanding use of artificial intelligence in medical\ndiagnostics. Many recent studies shows that the combination of brain Magnetic\nResonance Imaging (MRI) and deep neural networks have achieved promising\nresults for diagnosing AD. Using deep convolutional neural networks, this paper\nintroduces a novel deep learning architecture that incorporates multiresidual\nblocks, specialized spatial attention blocks, grouped query attention, and\nmulti-head attention. The study assessed the model's performance on four\npublicly accessible datasets and concentrated on identifying binary and\nmulticlass issues across various categories. This paper also takes into account\nof the explainability of AD's progression and compared with state-of-the-art\nmethods namely Gradient Class Activation Mapping (GradCAM), Score-CAM, Faster\nScore-CAM, and XGRADCAM. Our methodology consistently outperforms current\napproaches, achieving 99.66\\% accuracy in 4-class classification, 99.63\\% in\n3-class classification, and 100\\% in binary classification using Kaggle\ndatasets. For Open Access Series of Imaging Studies (OASIS) datasets the\naccuracies are 99.92\\%, 99.90\\%, and 99.95\\% respectively. The Alzheimer's\nDisease Neuroimaging Initiative-1 (ADNI-1) dataset was used for experiments in\nthree planes (axial, sagittal, and coronal) and a combination of all planes.\nThe study achieved accuracies of 99.08\\% for axis, 99.85\\% for sagittal, 99.5\\%\nfor coronal, and 99.17\\% for all axis, and 97.79\\% and 8.60\\% respectively for\nADNI-2. The network's ability to retrieve important information from MRI images\nis demonstrated by its excellent accuracy in categorizing AD stages.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "20 pages, 12 figures,",
    "pdf_url": "http://arxiv.org/pdf/2505.13906v1",
    "published_date": "2025-05-20 04:17:28 UTC",
    "updated_date": "2025-05-20 04:17:28 UTC"
  },
  {
    "arxiv_id": "2505.13904v1",
    "title": "Learning to Insert for Constructive Neural Vehicle Routing Solver",
    "authors": [
      "Fu Luo",
      "Xi Lin",
      "Mengyuan Zhong",
      "Fei Liu",
      "Zhenkun Wang",
      "Jianyong Sun",
      "Qingfu Zhang"
    ],
    "abstract": "Neural Combinatorial Optimisation (NCO) is a promising learning-based\napproach for solving Vehicle Routing Problems (VRPs) without extensive manual\ndesign. While existing constructive NCO methods typically follow an\nappending-based paradigm that sequentially adds unvisited nodes to partial\nsolutions, this rigid approach often leads to suboptimal results. To overcome\nthis limitation, we explore the idea of insertion-based paradigm and propose\nLearning to Construct with Insertion-based Paradigm (L2C-Insert), a novel\nlearning-based method for constructive NCO. Unlike traditional approaches,\nL2C-Insert builds solutions by strategically inserting unvisited nodes at any\nvalid position in the current partial solution, which can significantly enhance\nthe flexibility and solution quality. The proposed framework introduces three\nkey components: a novel model architecture for precise insertion position\nprediction, an efficient training scheme for model optimization, and an\nadvanced inference technique that fully exploits the insertion paradigm's\nflexibility. Extensive experiments on both synthetic and real-world instances\nof the Travelling Salesman Problem (TSP) and Capacitated Vehicle Routing\nProblem (CVRP) demonstrate that L2C-Insert consistently achieves superior\nperformance across various problem sizes.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.13904v1",
    "published_date": "2025-05-20 04:10:50 UTC",
    "updated_date": "2025-05-20 04:10:50 UTC"
  },
  {
    "arxiv_id": "2505.13898v1",
    "title": "Do Language Models Use Their Depth Efficiently?",
    "authors": [
      "RÃ³bert CsordÃ¡s",
      "Christopher D. Manning",
      "Christopher Potts"
    ],
    "abstract": "Modern LLMs are increasingly deep, and depth correlates with performance,\nalbeit with diminishing returns. However, do these models use their depth\nefficiently? Do they compose more features to create higher-order computations\nthat are impossible in shallow models, or do they merely spread the same kinds\nof computation out over more layers? To address these questions, we analyze the\nresidual stream of the Llama 3.1 and Qwen 3 family of models. We find: First,\ncomparing the output of the sublayers to the residual stream reveals that\nlayers in the second half contribute much less than those in the first half,\nwith a clear phase transition between the two halves. Second, skipping layers\nin the second half has a much smaller effect on future computations and output\npredictions. Third, for multihop tasks, we are unable to find evidence that\nmodels are using increased depth to compose subresults in examples involving\nmany hops. Fourth, we seek to directly address whether deeper models are using\ntheir additional layers to perform new kinds of computation. To do this, we\ntrain linear maps from the residual stream of a shallow model to a deeper one.\nWe find that layers with the same relative depth map best to each other,\nsuggesting that the larger model simply spreads the same computations out over\nits many layers. All this evidence suggests that deeper models are not using\ntheir depth to learn new kinds of computation, but only using the greater depth\nto perform more fine-grained adjustments to the residual. This may help explain\nwhy increasing scale leads to diminishing returns for stacked Transformer\narchitectures.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.13898v1",
    "published_date": "2025-05-20 04:00:56 UTC",
    "updated_date": "2025-05-20 04:00:56 UTC"
  },
  {
    "arxiv_id": "2505.13887v2",
    "title": "Mobile-Agent-V: A Video-Guided Approach for Effortless and Efficient Operational Knowledge Injection in Mobile Automation",
    "authors": [
      "Junyang Wang",
      "Haiyang Xu",
      "Xi Zhang",
      "Ming Yan",
      "Ji Zhang",
      "Fei Huang",
      "Jitao Sang"
    ],
    "abstract": "The exponential rise in mobile device usage necessitates streamlined\nautomation for effective task management, yet many AI frameworks fall short due\nto inadequate operational expertise. While manually written knowledge can\nbridge this gap, it is often burdensome and inefficient. We introduce\nMobile-Agent-V, an innovative framework that utilizes video as a guiding tool\nto effortlessly and efficiently inject operational knowledge into mobile\nautomation processes. By deriving knowledge directly from video content,\nMobile-Agent-V eliminates manual intervention, significantly reducing the\neffort and time required for knowledge acquisition. To rigorously evaluate this\napproach, we propose Mobile-Knowledge, a benchmark tailored to assess the\nimpact of external knowledge on mobile agent performance. Our experimental\nfindings demonstrate that Mobile-Agent-V enhances performance by 36% compared\nto existing methods, underscoring its effortless and efficient advantages in\nmobile automation.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "I was trying to update arXiv:2502.17110 but accidentally published a\n  new work",
    "pdf_url": "http://arxiv.org/pdf/2505.13887v2",
    "published_date": "2025-05-20 03:48:19 UTC",
    "updated_date": "2025-05-21 02:28:49 UTC"
  },
  {
    "arxiv_id": "2505.14737v1",
    "title": "Leveraging Multivariate Long-Term History Representation for Time Series Forecasting",
    "authors": [
      "Huiliang Zhang",
      "Di Wu",
      "Arnaud Zinflou",
      "Stephane Dellacherie",
      "Mouhamadou Makhtar Dione",
      "Benoit Boulet"
    ],
    "abstract": "Multivariate Time Series (MTS) forecasting has a wide range of applications\nin both industry and academia. Recent advances in Spatial-Temporal Graph Neural\nNetwork (STGNN) have achieved great progress in modelling spatial-temporal\ncorrelations. Limited by computational complexity, most STGNNs for MTS\nforecasting focus primarily on short-term and local spatial-temporal\ndependencies. Although some recent methods attempt to incorporate univariate\nhistory into modeling, they still overlook crucial long-term spatial-temporal\nsimilarities and correlations across MTS, which are essential for accurate\nforecasting. To fill this gap, we propose a framework called the Long-term\nMultivariate History Representation (LMHR) Enhanced STGNN for MTS forecasting.\nSpecifically, a Long-term History Encoder (LHEncoder) is adopted to effectively\nencode the long-term history into segment-level contextual representations and\nreduce point-level noise. A non-parametric Hierarchical Representation\nRetriever (HRetriever) is designed to include the spatial information in the\nlong-term spatial-temporal dependency modelling and pick out the most valuable\nrepresentations with no additional training. A Transformer-based Aggregator\n(TAggregator) selectively fuses the sparsely retrieved contextual\nrepresentations based on the ranking positional embedding efficiently.\nExperimental results demonstrate that LMHR outperforms typical STGNNs by 10.72%\non the average prediction horizons and state-of-the-art methods by 4.12% on\nseveral real-world datasets. Additionally, it consistently improves prediction\naccuracy by 9.8% on the top 10% of rapidly changing patterns across the\ndatasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14737v1",
    "published_date": "2025-05-20 03:46:36 UTC",
    "updated_date": "2025-05-20 03:46:36 UTC"
  },
  {
    "arxiv_id": "2505.13873v1",
    "title": "Utilizing Strategic Pre-training to Reduce Overfitting: Baguan -- A Pre-trained Weather Forecasting Model",
    "authors": [
      "Peisong Niu",
      "Ziqing Ma",
      "Tian Zhou",
      "Weiqi Chen",
      "Lefei Shen",
      "Rong Jin",
      "Liang Sun"
    ],
    "abstract": "Weather forecasting has long posed a significant challenge for humanity.\nWhile recent AI-based models have surpassed traditional numerical weather\nprediction (NWP) methods in global forecasting tasks, overfitting remains a\ncritical issue due to the limited availability of real-world weather data\nspanning only a few decades. Unlike fields like computer vision or natural\nlanguage processing, where data abundance can mitigate overfitting, weather\nforecasting demands innovative strategies to address this challenge with\nexisting data. In this paper, we explore pre-training methods for weather\nforecasting, finding that selecting an appropriately challenging pre-training\ntask introduces locality bias, effectively mitigating overfitting and enhancing\nperformance. We introduce Baguan, a novel data-driven model for medium-range\nweather forecasting, built on a Siamese Autoencoder pre-trained in a\nself-supervised manner and fine-tuned for different lead times. Experimental\nresults show that Baguan outperforms traditional methods, delivering more\naccurate forecasts. Additionally, the pre-trained Baguan demonstrates robust\noverfitting control and excels in downstream tasks, such as\nsubseasonal-to-seasonal (S2S) modeling and regional forecasting, after\nfine-tuning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "KDD2025 research track accepted",
    "pdf_url": "http://arxiv.org/pdf/2505.13873v1",
    "published_date": "2025-05-20 03:29:23 UTC",
    "updated_date": "2025-05-20 03:29:23 UTC"
  },
  {
    "arxiv_id": "2505.13872v1",
    "title": "Safety2Drive: Safety-Critical Scenario Benchmark for the Evaluation of Autonomous Driving",
    "authors": [
      "Jingzheng Li",
      "Tiancheng Wang",
      "Xingyu Peng",
      "Jiacheng Chen",
      "Zhijun Chen",
      "Bing Li",
      "Xianglong Liu"
    ],
    "abstract": "Autonomous Driving (AD) systems demand the high levels of safety assurance.\nDespite significant advancements in AD demonstrated on open-source benchmarks\nlike Longest6 and Bench2Drive, existing datasets still lack\nregulatory-compliant scenario libraries for closed-loop testing to\ncomprehensively evaluate the functional safety of AD. Meanwhile, real-world AD\naccidents are underrepresented in current driving datasets. This scarcity leads\nto inadequate evaluation of AD performance, posing risks to safety validation\nand practical deployment. To address these challenges, we propose Safety2Drive,\na safety-critical scenario library designed to evaluate AD systems.\nSafety2Drive offers three key contributions. (1) Safety2Drive comprehensively\ncovers the test items required by standard regulations and contains 70 AD\nfunction test items. (2) Safety2Drive supports the safety-critical scenario\ngeneralization. It has the ability to inject safety threats such as natural\nenvironment corruptions and adversarial attacks cross camera and LiDAR sensors.\n(3) Safety2Drive supports multi-dimensional evaluation. In addition to the\nevaluation of AD systems, it also supports the evaluation of various perception\ntasks, such as object detection and lane detection. Safety2Drive provides a\nparadigm from scenario construction to validation, establishing a standardized\ntest framework for the safe deployment of AD.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.13872v1",
    "published_date": "2025-05-20 03:27:06 UTC",
    "updated_date": "2025-05-20 03:27:06 UTC"
  },
  {
    "arxiv_id": "2505.13860v1",
    "title": "Domain Adaptation of VLM for Soccer Video Understanding",
    "authors": [
      "Tiancheng Jiang",
      "Henry Wang",
      "Md Sirajus Salekin",
      "Parmida Atighehchian",
      "Shinan Zhang"
    ],
    "abstract": "Vision Language Models (VLMs) have demonstrated strong performance in\nmulti-modal tasks by effectively aligning visual and textual representations.\nHowever, most video understanding VLM research has been domain-agnostic,\nleaving the understanding of their transfer learning capability to specialized\ndomains under-explored. In this work, we address this by exploring the\nadaptability of open-source VLMs to specific domains, and focusing on soccer as\nan initial case study. Our approach uses large-scale soccer datasets and LLM to\ncreate instruction-following data, and use them to iteratively fine-tune the\ngeneral-domain VLM in a curriculum learning fashion (first teaching the model\nkey soccer concepts to then question answering tasks). The final adapted model,\ntrained using a curated dataset of 20k video clips, exhibits significant\nimprovement in soccer-specific tasks compared to the base model, with a 37.5%\nrelative improvement for the visual question-answering task and an accuracy\nimprovement from 11.8% to 63.5% for the downstream soccer action classification\ntask.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 5 figures, accepted to the 11th IEEE International Workshop\n  on Computer Vision in Sports (CVSports) at CVPR 2025; supplementary appendix\n  included as ancillary PDF",
    "pdf_url": "http://arxiv.org/pdf/2505.13860v1",
    "published_date": "2025-05-20 03:12:21 UTC",
    "updated_date": "2025-05-20 03:12:21 UTC"
  },
  {
    "arxiv_id": "2505.13857v1",
    "title": "Learning Spatio-Temporal Dynamics for Trajectory Recovery via Time-Aware Transformer",
    "authors": [
      "Tian Sun",
      "Yuqi Chen",
      "Baihua Zheng",
      "Weiwei Sun"
    ],
    "abstract": "In real-world applications, GPS trajectories often suffer from low sampling\nrates, with large and irregular intervals between consecutive GPS points. This\nsparse characteristic presents challenges for their direct use in GPS-based\nsystems. This paper addresses the task of map-constrained trajectory recovery,\naiming to enhance trajectory sampling rates of GPS trajectories. Previous\nstudies commonly adopt a sequence-to-sequence framework, where an encoder\ncaptures the trajectory patterns and a decoder reconstructs the target\ntrajectory. Within this framework, effectively representing the road network\nand extracting relevant trajectory features are crucial for overall\nperformance. Despite advancements in these models, they fail to fully leverage\nthe complex spatio-temporal dynamics present in both the trajectory and the\nroad network.\n  To overcome these limitations, we categorize the spatio-temporal dynamics of\ntrajectory data into two distinct aspects: spatial-temporal traffic dynamics\nand trajectory dynamics. Furthermore, We propose TedTrajRec, a novel method for\ntrajectory recovery. To capture spatio-temporal traffic dynamics, we introduce\nPD-GNN, which models periodic patterns and learns topologically aware dynamics\nconcurrently for each road segment. For spatio-temporal trajectory dynamics, we\npresent TedFormer, a time-aware Transformer that incorporates temporal dynamics\nfor each GPS location by integrating closed-form neural ordinary differential\nequations into the attention mechanism. This allows TedFormer to effectively\nhandle irregularly sampled data. Extensive experiments on three real-world\ndatasets demonstrate the superior performance of TedTrajRec. The code is\npublicly available at https://github.com/ysygMhdxw/TEDTrajRec/.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted as a journal paper in IEEE Transactions on Intelligent\n  Transportation Systems (T-ITS)",
    "pdf_url": "http://arxiv.org/pdf/2505.13857v1",
    "published_date": "2025-05-20 03:09:17 UTC",
    "updated_date": "2025-05-20 03:09:17 UTC"
  },
  {
    "arxiv_id": "2505.13855v1",
    "title": "Domain Gating Ensemble Networks for AI-Generated Text Detection",
    "authors": [
      "Arihant Tripathi",
      "Liam Dugan",
      "Charis Gao",
      "Maggie Huan",
      "Emma Jin",
      "Peter Zhang",
      "David Zhang",
      "Julia Zhao",
      "Chris Callison-Burch"
    ],
    "abstract": "As state-of-the-art language models continue to improve, the need for robust\ndetection of machine-generated text becomes increasingly critical. However,\ncurrent state-of-the-art machine text detectors struggle to adapt to new unseen\ndomains and generative models. In this paper we present DoGEN (Domain Gating\nEnsemble Networks), a technique that allows detectors to adapt to unseen\ndomains by ensembling a set of domain expert detector models using weights from\na domain classifier. We test DoGEN on a wide variety of domains from leading\nbenchmarks and find that it achieves state-of-the-art performance on in-domain\ndetection while outperforming models twice its size on out-of-domain detection.\nWe release our code and trained models to assist in future research in\ndomain-adaptive AI detection.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Submitted to EMNLP 2025",
    "pdf_url": "http://arxiv.org/pdf/2505.13855v1",
    "published_date": "2025-05-20 03:02:05 UTC",
    "updated_date": "2025-05-20 03:02:05 UTC"
  },
  {
    "arxiv_id": "2505.13851v1",
    "title": "A Challenge to Build Neuro-Symbolic Video Agents",
    "authors": [
      "Sahil Shah",
      "Harsh Goel",
      "Sai Shankar Narasimhan",
      "Minkyu Choi",
      "S P Sharan",
      "Oguzhan Akcin",
      "Sandeep Chinchali"
    ],
    "abstract": "Modern video understanding systems excel at tasks such as scene\nclassification, object detection, and short video retrieval. However, as video\nanalysis becomes increasingly central to real-world applications, there is a\ngrowing need for proactive video agents for the systems that not only interpret\nvideo streams but also reason about events and take informed actions. A key\nobstacle in this direction is temporal reasoning: while deep learning models\nhave made remarkable progress in recognizing patterns within individual frames\nor short clips, they struggle to understand the sequencing and dependencies of\nevents over time, which is critical for action-driven decision-making.\nAddressing this limitation demands moving beyond conventional deep learning\napproaches. We posit that tackling this challenge requires a neuro-symbolic\nperspective, where video queries are decomposed into atomic events, structured\ninto coherent sequences, and validated against temporal constraints. Such an\napproach can enhance interpretability, enable structured reasoning, and provide\nstronger guarantees on system behavior, all key properties for advancing\ntrustworthy video agents. To this end, we present a grand challenge to the\nresearch community: developing the next generation of intelligent video agents\nthat integrate three core capabilities: (1) autonomous video search and\nanalysis, (2) seamless real-world interaction, and (3) advanced content\ngeneration. By addressing these pillars, we can transition from passive\nperception to intelligent video agents that reason, predict, and act, pushing\nthe boundaries of video understanding.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.13851v1",
    "published_date": "2025-05-20 02:53:21 UTC",
    "updated_date": "2025-05-20 02:53:21 UTC"
  },
  {
    "arxiv_id": "2505.13847v1",
    "title": "Forensic deepfake audio detection using segmental speech features",
    "authors": [
      "Tianle Yang",
      "Chengzhe Sun",
      "Siwei Lyu",
      "Phil Rose"
    ],
    "abstract": "This study explores the potential of using acoustic features of segmental\nspeech sounds to detect deepfake audio. These features are highly interpretable\nbecause of their close relationship with human articulatory processes and are\nexpected to be more difficult for deepfake models to replicate. The results\ndemonstrate that certain segmental features commonly used in forensic voice\ncomparison are effective in identifying deep-fakes, whereas some global\nfeatures provide little value. These findings underscore the need to approach\naudio deepfake detection differently for forensic voice comparison and offer a\nnew perspective on leveraging segmental features for this purpose.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.13847v1",
    "published_date": "2025-05-20 02:42:46 UTC",
    "updated_date": "2025-05-20 02:42:46 UTC"
  },
  {
    "arxiv_id": "2505.14733v1",
    "title": "The Energy Cost of Reasoning: Analyzing Energy Usage in LLMs with Test-time Compute",
    "authors": [
      "Yunho Jin",
      "Gu-Yeon Wei",
      "David Brooks"
    ],
    "abstract": "Scaling large language models (LLMs) has driven significant advancements, yet\nit faces diminishing returns and escalating energy demands. This work\nintroduces test-time compute (TTC)-allocating additional computational\nresources during inference-as a compelling complement to conventional scaling\nstrategies. Specifically, we investigate whether employing TTC can achieve\nsuperior accuracy-energy trade-offs compared to simply increasing model size.\nOur empirical analysis reveals that TTC surpasses traditional model scaling in\naccuracy/energy efficiency, with notable gains in tasks demanding complex\nreasoning rather than mere factual recall. Further, we identify a critical\ninteraction between TTC performance and output sequence length, demonstrating\nthat strategically adjusting compute resources at inference time according to\nquery complexity can substantially enhance efficiency. Our findings advocate\nfor TTC as a promising direction, enabling more sustainable, accurate, and\nadaptable deployment of future language models without incurring additional\npretraining costs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14733v1",
    "published_date": "2025-05-20 02:35:59 UTC",
    "updated_date": "2025-05-20 02:35:59 UTC"
  },
  {
    "arxiv_id": "2505.13840v1",
    "title": "EfficientLLM: Efficiency in Large Language Models",
    "authors": [
      "Zhengqing Yuan",
      "Weixiang Sun",
      "Yixin Liu",
      "Huichi Zhou",
      "Rong Zhou",
      "Yiyang Li",
      "Zheyuan Zhang",
      "Wei Song",
      "Yue Huang",
      "Haolong Jia",
      "Keerthiram Murugesan",
      "Yu Wang",
      "Lifang He",
      "Jianfeng Gao",
      "Lichao Sun",
      "Yanfang Ye"
    ],
    "abstract": "Large Language Models (LLMs) have driven significant progress, yet their\ngrowing parameter counts and context windows incur prohibitive compute, energy,\nand monetary costs. We introduce EfficientLLM, a novel benchmark and the first\ncomprehensive empirical study evaluating efficiency techniques for LLMs at\nscale. Conducted on a production-class cluster (48xGH200, 8xH200 GPUs), our\nstudy systematically explores three key axes: (1) architecture pretraining\n(efficient attention variants: MQA, GQA, MLA, NSA; sparse Mixture-of-Experts\n(MoE)), (2) fine-tuning (parameter-efficient methods: LoRA, RSLoRA, DoRA), and\n(3) inference (quantization methods: int4, float16). We define six fine-grained\nmetrics (Memory Utilization, Compute Utilization, Latency, Throughput, Energy\nConsumption, Compression Rate) to capture hardware saturation,\nlatency-throughput balance, and carbon cost. Evaluating over 100\nmodel-technique pairs (0.5B-72B parameters), we derive three core insights: (i)\nEfficiency involves quantifiable trade-offs: no single method is universally\noptimal; e.g., MoE reduces FLOPs and improves accuracy but increases VRAM by\n40%, while int4 quantization cuts memory/energy by up to 3.9x at a 3-5%\naccuracy drop. (ii) Optima are task- and scale-dependent: MQA offers optimal\nmemory-latency trade-offs for constrained devices, MLA achieves lowest\nperplexity for quality-critical tasks, and RSLoRA surpasses LoRA efficiency\nonly beyond 14B parameters. (iii) Techniques generalize across modalities: we\nextend evaluations to Large Vision Models (Stable Diffusion 3.5, Wan 2.1) and\nVision-Language Models (Qwen2.5-VL), confirming effective transferability. By\nopen-sourcing datasets, evaluation pipelines, and leaderboards, EfficientLLM\nprovides essential guidance for researchers and engineers navigating the\nefficiency-performance landscape of next-generation foundation models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.13840v1",
    "published_date": "2025-05-20 02:27:08 UTC",
    "updated_date": "2025-05-20 02:27:08 UTC"
  },
  {
    "arxiv_id": "2505.13837v1",
    "title": "Enhancing Robot Navigation Policies with Task-Specific Uncertainty Managements",
    "authors": [
      "Gokul Puthumanaillam",
      "Paulo Padrao",
      "Jose Fuentes",
      "Leonardo Bobadilla",
      "Melkior Ornik"
    ],
    "abstract": "Robots navigating complex environments must manage uncertainty from sensor\nnoise, environmental changes, and incomplete information, with different tasks\nrequiring varying levels of precision in different areas. For example, precise\nlocalization may be crucial near obstacles but less critical in open spaces. We\npresent GUIDE (Generalized Uncertainty Integration for Decision-Making and\nExecution), a framework that integrates these task-specific requirements into\nnavigation policies via Task-Specific Uncertainty Maps (TSUMs). By assigning\nacceptable uncertainty levels to different locations, TSUMs enable robots to\nadapt uncertainty management based on context. When combined with reinforcement\nlearning, GUIDE learns policies that balance task completion and uncertainty\nmanagement without extensive reward engineering. Real-world tests show\nsignificant performance gains over methods lacking task-specific uncertainty\nawareness.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.13837v1",
    "published_date": "2025-05-20 02:23:15 UTC",
    "updated_date": "2025-05-20 02:23:15 UTC"
  },
  {
    "arxiv_id": "2505.13834v1",
    "title": "Toward Real-World Cooperative and Competitive Soccer with Quadrupedal Robot Teams",
    "authors": [
      "Zhi Su",
      "Yuman Gao",
      "Emily Lukas",
      "Yunfei Li",
      "Jiaze Cai",
      "Faris Tulbah",
      "Fei Gao",
      "Chao Yu",
      "Zhongyu Li",
      "Yi Wu",
      "Koushil Sreenath"
    ],
    "abstract": "Achieving coordinated teamwork among legged robots requires both fine-grained\nlocomotion control and long-horizon strategic decision-making. Robot soccer\noffers a compelling testbed for this challenge, combining dynamic, competitive,\nand multi-agent interactions. In this work, we present a hierarchical\nmulti-agent reinforcement learning (MARL) framework that enables fully\nautonomous and decentralized quadruped robot soccer. First, a set of highly\ndynamic low-level skills is trained for legged locomotion and ball\nmanipulation, such as walking, dribbling, and kicking. On top of these, a\nhigh-level strategic planning policy is trained with Multi-Agent Proximal\nPolicy Optimization (MAPPO) via Fictitious Self-Play (FSP). This learning\nframework allows agents to adapt to diverse opponent strategies and gives rise\nto sophisticated team behaviors, including coordinated passing, interception,\nand dynamic role allocation. With an extensive ablation study, the proposed\nlearning method shows significant advantages in the cooperative and competitive\nmulti-agent soccer game. We deploy the learned policies to real quadruped\nrobots relying solely on onboard proprioception and decentralized localization,\nwith the resulting system supporting autonomous robot-robot and robot-human\nsoccer matches on indoor and outdoor soccer courts.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "11 pages, 12 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.13834v1",
    "published_date": "2025-05-20 02:20:54 UTC",
    "updated_date": "2025-05-20 02:20:54 UTC"
  },
  {
    "arxiv_id": "2505.13831v1",
    "title": "TelePlanNet: An AI-Driven Framework for Efficient Telecom Network Planning",
    "authors": [
      "Zongyuan Deng",
      "Yujie Cai",
      "Qing Liu",
      "Shiyao Mu",
      "Bin Lyu",
      "Zhen Yang"
    ],
    "abstract": "The selection of base station sites is a critical challenge in 5G network\nplanning, which requires efficient optimization of coverage, cost, user\nsatisfaction, and practical constraints. Traditional manual methods, reliant on\nhuman expertise, suffer from inefficiencies and are limited to an unsatisfied\nplanning-construction consistency. Existing AI tools, despite improving\nefficiency in certain aspects, still struggle to meet the dynamic network\nconditions and multi-objective needs of telecom operators' networks. To address\nthese challenges, we propose TelePlanNet, an AI-driven framework tailored for\nthe selection of base station sites, integrating a three-layer architecture for\nefficient planning and large-scale automation. By leveraging large language\nmodels (LLMs) for real-time user input processing and intent alignment with\nbase station planning, combined with training the planning model using the\nimproved group relative policy optimization (GRPO) reinforcement learning, the\nproposed TelePlanNet can effectively address multi-objective optimization,\nevaluates candidate sites, and delivers practical solutions. Experiments\nresults show that the proposed TelePlanNet can improve the consistency to 78%,\nwhich is superior to the manual methods, providing telecom operators with an\nefficient and scalable tool that significantly advances cellular network\nplanning.",
    "categories": [
      "cs.AI",
      "I.2; I.2.6; C.2.1"
    ],
    "primary_category": "cs.AI",
    "comment": "6 pages, 5 figures, 1 table, submitted to IEEE ICCC 2025",
    "pdf_url": "http://arxiv.org/pdf/2505.13831v1",
    "published_date": "2025-05-20 02:19:10 UTC",
    "updated_date": "2025-05-20 02:19:10 UTC"
  },
  {
    "arxiv_id": "2505.13828v1",
    "title": "Multimodal RAG-driven Anomaly Detection and Classification in Laser Powder Bed Fusion using Large Language Models",
    "authors": [
      "Kiarash Naghavi Khanghah",
      "Zhiling Chen",
      "Lela Romeo",
      "Qian Yang",
      "Rajiv Malhotra",
      "Farhad Imani",
      "Hongyi Xu"
    ],
    "abstract": "Additive manufacturing enables the fabrication of complex designs while\nminimizing waste, but faces challenges related to defects and process\nanomalies. This study presents a novel multimodal Retrieval-Augmented\nGeneration-based framework that automates anomaly detection across various\nAdditive Manufacturing processes leveraging retrieved information from\nliterature, including images and descriptive text, rather than training\ndatasets. This framework integrates text and image retrieval from scientific\nliterature and multimodal generation models to perform zero-shot anomaly\nidentification, classification, and explanation generation in a Laser Powder\nBed Fusion setting. The proposed framework is evaluated on four L-PBF\nmanufacturing datasets from Oak Ridge National Laboratory, featuring various\nprinter makes, models, and materials. This evaluation demonstrates the\nframework's adaptability and generalizability across diverse images without\nrequiring additional training. Comparative analysis using Qwen2-VL-2B and\nGPT-4o-mini as MLLM within the proposed framework highlights that GPT-4o-mini\noutperforms Qwen2-VL-2B and proportional random baseline in manufacturing\nanomalies classification. Additionally, the evaluation of the RAG system\nconfirms that incorporating retrieval mechanisms improves average accuracy by\n12% by reducing the risk of hallucination and providing additional information.\nThe proposed framework can be continuously updated by integrating emerging\nresearch, allowing seamless adaptation to the evolving landscape of AM\ntechnologies. This scalable, automated, and zero-shot-capable framework\nstreamlines AM anomaly analysis, enhancing efficiency and accuracy.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "ASME 2025 International Design Engineering Technical Conferences and\n  Computers and Information in Engineering Conference IDETC/CIE2025, August\n  17-20, 2025, Anaheim, CA (IDETC2025-168615)",
    "pdf_url": "http://arxiv.org/pdf/2505.13828v1",
    "published_date": "2025-05-20 02:18:22 UTC",
    "updated_date": "2025-05-20 02:18:22 UTC"
  },
  {
    "arxiv_id": "2505.13820v1",
    "title": "Structured Agent Distillation for Large Language Model",
    "authors": [
      "Jun Liu",
      "Zhenglun Kong",
      "Peiyan Dong",
      "Changdi Yang",
      "Tianqi Li",
      "Hao Tang",
      "Geng Yuan",
      "Wei Niu",
      "Wenbin Zhang",
      "Pu Zhao",
      "Xue Lin",
      "Dong Huang",
      "Yanzhi Wang"
    ],
    "abstract": "Large language models (LLMs) exhibit strong capabilities as decision-making\nagents by interleaving reasoning and actions, as seen in ReAct-style\nframeworks. Yet, their practical deployment is constrained by high inference\ncosts and large model sizes. We propose Structured Agent Distillation, a\nframework that compresses large LLM-based agents into smaller student models\nwhile preserving both reasoning fidelity and action consistency. Unlike\nstandard token-level distillation, our method segments trajectories into\n{[REASON]} and {[ACT]} spans, applying segment-specific losses to align each\ncomponent with the teacher's behavior. This structure-aware supervision enables\ncompact agents to better replicate the teacher's decision process. Experiments\non ALFWorld, HotPotQA-ReAct, and WebShop show that our approach consistently\noutperforms token-level and imitation learning baselines, achieving significant\ncompression with minimal performance drop. Scaling and ablation results further\nhighlight the importance of span-level alignment for efficient and deployable\nagents.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.13820v1",
    "published_date": "2025-05-20 02:01:55 UTC",
    "updated_date": "2025-05-20 02:01:55 UTC"
  },
  {
    "arxiv_id": "2505.13814v1",
    "title": "Articulatory Feature Prediction from Surface EMG during Speech Production",
    "authors": [
      "Jihwan Lee",
      "Kevin Huang",
      "Kleanthis Avramidis",
      "Simon Pistrosch",
      "Monica Gonzalez-Machorro",
      "Yoonjeong Lee",
      "BjÃ¶rn Schuller",
      "Louis Goldstein",
      "Shrikanth Narayanan"
    ],
    "abstract": "We present a model for predicting articulatory features from surface\nelectromyography (EMG) signals during speech production. The proposed model\nintegrates convolutional layers and a Transformer block, followed by separate\npredictors for articulatory features. Our approach achieves a high prediction\ncorrelation of approximately 0.9 for most articulatory features. Furthermore,\nwe demonstrate that these predicted articulatory features can be decoded into\nintelligible speech waveforms. To our knowledge, this is the first method to\ndecode speech waveforms from surface EMG via articulatory features, offering a\nnovel approach to EMG-based speech synthesis. Additionally, we analyze the\nrelationship between EMG electrode placement and articulatory feature\npredictability, providing knowledge-driven insights for optimizing EMG\nelectrode configurations. The source code and decoded speech samples are\npublicly available.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted for Interspeech2025",
    "pdf_url": "http://arxiv.org/pdf/2505.13814v1",
    "published_date": "2025-05-20 01:50:05 UTC",
    "updated_date": "2025-05-20 01:50:05 UTC"
  },
  {
    "arxiv_id": "2505.13808v1",
    "title": "RAG/LLM Augmented Switching Driven Polymorphic Metaheuristic Framework",
    "authors": [
      "Faramarz Safi Esfahani",
      "Ghassan Beydoun",
      "Morteza Saberi",
      "Brad McCusker",
      "Biswajeet Pradhan"
    ],
    "abstract": "Metaheuristic algorithms are widely used for solving complex optimization\nproblems, yet their effectiveness is often constrained by fixed structures and\nthe need for extensive tuning. The Polymorphic Metaheuristic Framework (PMF)\naddresses this limitation by introducing a self-adaptive metaheuristic\nswitching mechanism driven by real-time performance feedback and dynamic\nalgorithmic selection. PMF leverages the Polymorphic Metaheuristic Agent (PMA)\nand the Polymorphic Metaheuristic Selection Agent (PMSA) to dynamically select\nand transition between metaheuristic algorithms based on key performance\nindicators, ensuring continuous adaptation. This approach enhances convergence\nspeed, adaptability, and solution quality, outperforming traditional\nmetaheuristics in high-dimensional, dynamic, and multimodal environments.\nExperimental results on benchmark functions demonstrate that PMF significantly\nimproves optimization efficiency by mitigating stagnation and balancing\nexploration-exploitation strategies across various problem landscapes. By\nintegrating AI-driven decision-making and self-correcting mechanisms, PMF paves\nthe way for scalable, intelligent, and autonomous optimization frameworks, with\npromising applications in engineering, logistics, and complex decision-making\nsystems.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.13808v1",
    "published_date": "2025-05-20 01:41:22 UTC",
    "updated_date": "2025-05-20 01:41:22 UTC"
  },
  {
    "arxiv_id": "2505.13805v1",
    "title": "ClapFM-EVC: High-Fidelity and Flexible Emotional Voice Conversion with Dual Control from Natural Language and Speech",
    "authors": [
      "Yu Pan",
      "Yanni Hu",
      "Yuguang Yang",
      "Jixun Yao",
      "Jianhao Ye",
      "Hongbin Zhou",
      "Lei Ma",
      "Jianjun Zhao"
    ],
    "abstract": "Despite great advances, achieving high-fidelity emotional voice conversion\n(EVC) with flexible and interpretable control remains challenging. This paper\nintroduces ClapFM-EVC, a novel EVC framework capable of generating high-quality\nconverted speech driven by natural language prompts or reference speech with\nadjustable emotion intensity. We first propose EVC-CLAP, an emotional\ncontrastive language-audio pre-training model, guided by natural language\nprompts and categorical labels, to extract and align fine-grained emotional\nelements across speech and text modalities. Then, a FuEncoder with an adaptive\nintensity gate is presented to seamless fuse emotional features with Phonetic\nPosteriorGrams from a pre-trained ASR model. To further improve emotion\nexpressiveness and speech naturalness, we propose a flow matching model\nconditioned on these captured features to reconstruct Mel-spectrogram of source\nspeech. Subjective and objective evaluations validate the effectiveness of\nClapFM-EVC.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted by InterSpeech 2025",
    "pdf_url": "http://arxiv.org/pdf/2505.13805v1",
    "published_date": "2025-05-20 01:34:29 UTC",
    "updated_date": "2025-05-20 01:34:29 UTC"
  },
  {
    "arxiv_id": "2505.14728v1",
    "title": "MORALISE: A Structured Benchmark for Moral Alignment in Visual Language Models",
    "authors": [
      "Xiao Lin",
      "Zhining Liu",
      "Ze Yang",
      "Gaotang Li",
      "Ruizhong Qiu",
      "Shuke Wang",
      "Hui Liu",
      "Haotian Li",
      "Sumit Keswani",
      "Vishwa Pardeshi",
      "Huijun Zhao",
      "Wei Fan",
      "Hanghang Tong"
    ],
    "abstract": "Warning: This paper contains examples of harmful language and images. Reader\ndiscretion is advised. Recently, vision-language models have demonstrated\nincreasing influence in morally sensitive domains such as autonomous driving\nand medical analysis, owing to their powerful multimodal reasoning\ncapabilities. As these models are deployed in high-stakes real-world\napplications, it is of paramount importance to ensure that their outputs align\nwith human moral values and remain within moral boundaries. However, existing\nwork on moral alignment either focuses solely on textual modalities or relies\nheavily on AI-generated images, leading to distributional biases and reduced\nrealism. To overcome these limitations, we introduce MORALISE, a comprehensive\nbenchmark for evaluating the moral alignment of vision-language models (VLMs)\nusing diverse, expert-verified real-world data. We begin by proposing a\ncomprehensive taxonomy of 13 moral topics grounded in Turiel's Domain Theory,\nspanning the personal, interpersonal, and societal moral domains encountered in\neveryday life. Built on this framework, we manually curate 2,481 high-quality\nimage-text pairs, each annotated with two fine-grained labels: (1) topic\nannotation, identifying the violated moral topic(s), and (2) modality\nannotation, indicating whether the violation arises from the image or the text.\nFor evaluation, we encompass two tasks, \\textit{moral judgment} and\n\\textit{moral norm attribution}, to assess models' awareness of moral\nviolations and their reasoning ability on morally salient content. Extensive\nexperiments on 19 popular open- and closed-source VLMs show that MORALISE poses\na significant challenge, revealing persistent moral limitations in current\nstate-of-the-art models. The full benchmark is publicly available at\nhttps://huggingface.co/datasets/Ze1025/MORALISE.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "21 pages, 11 figures, 7 tables",
    "pdf_url": "http://arxiv.org/pdf/2505.14728v1",
    "published_date": "2025-05-20 01:11:17 UTC",
    "updated_date": "2025-05-20 01:11:17 UTC"
  },
  {
    "arxiv_id": "2505.13794v1",
    "title": "LLM-based Evaluation Policy Extraction for Ecological Modeling",
    "authors": [
      "Qi Cheng",
      "Licheng Liu",
      "Qing Zhu",
      "Runlong Yu",
      "Zhenong Jin",
      "Yiqun Xie",
      "Xiaowei Jia"
    ],
    "abstract": "Evaluating ecological time series is critical for benchmarking model\nperformance in many important applications, including predicting greenhouse gas\nfluxes, capturing carbon-nitrogen dynamics, and monitoring hydrological cycles.\nTraditional numerical metrics (e.g., R-squared, root mean square error) have\nbeen widely used to quantify the similarity between modeled and observed\necosystem variables, but they often fail to capture domain-specific temporal\npatterns critical to ecological processes. As a result, these methods are often\naccompanied by expert visual inspection, which requires substantial human labor\nand limits the applicability to large-scale evaluation. To address these\nchallenges, we propose a novel framework that integrates metric learning with\nlarge language model (LLM)-based natural language policy extraction to develop\ninterpretable evaluation criteria. The proposed method processes pairwise\nannotations and implements a policy optimization mechanism to generate and\ncombine different assessment metrics. The results obtained on multiple datasets\nfor evaluating the predictions of crop gross primary production and carbon\ndioxide flux have confirmed the effectiveness of the proposed method in\ncapturing target assessment preferences, including both synthetically generated\nand expert-annotated model comparisons. The proposed framework bridges the gap\nbetween numerical metrics and expert knowledge while providing interpretable\nevaluation policies that accommodate the diverse needs of different ecosystem\nmodeling studies.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.13794v1",
    "published_date": "2025-05-20 01:02:29 UTC",
    "updated_date": "2025-05-20 01:02:29 UTC"
  },
  {
    "arxiv_id": "2505.13792v1",
    "title": "Interpretable Traces, Unexpected Outcomes: Investigating the Disconnect in Trace-Based Knowledge Distillation",
    "authors": [
      "Siddhant Bhambri",
      "Upasana Biswas",
      "Subbarao Kambhampati"
    ],
    "abstract": "Question Answering (QA) poses a challenging and critical problem,\nparticularly in today's age of interactive dialogue systems such as ChatGPT,\nPerplexity, Microsoft Copilot, etc. where users demand both accuracy and\ntransparency in the model's outputs. Since smaller language models (SLMs) are\ncomputationally more efficient but often under-perform compared to larger\nmodels, Knowledge Distillation (KD) methods allow for finetuning these smaller\nmodels to improve their final performance. Lately, the intermediate tokens or\nthe so called `reasoning' traces produced by Chain-of-Thought (CoT) or by\nreasoning models such as DeepSeek R1 are used as a training signal for KD.\nHowever, these reasoning traces are often verbose and difficult to interpret or\nevaluate. In this work, we aim to address the challenge of evaluating the\nfaithfulness of these reasoning traces and their correlation with the final\nperformance. To this end, we employ a KD method leveraging rule-based problem\ndecomposition. This approach allows us to break down complex queries into\nstructured sub-problems, generating interpretable traces whose correctness can\nbe readily evaluated, even at inference time. Specifically, we demonstrate this\napproach on Open Book QA, decomposing the problem into a Classification step\nand an Information Retrieval step, thereby simplifying trace evaluation. Our\nSFT experiments with correct and incorrect traces on the CoTemp QA, Microsoft\nMachine Reading Comprehension QA, and Facebook bAbI QA datasets reveal the\nstriking finding that correct traces do not necessarily imply that the model\noutputs the correct final solution. Similarly, we find a low correlation\nbetween correct final solutions and intermediate trace correctness. These\nresults challenge the implicit assumption behind utilizing reasoning traces for\nimproving SLMs' final performance via KD.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages",
    "pdf_url": "http://arxiv.org/pdf/2505.13792v1",
    "published_date": "2025-05-20 00:49:19 UTC",
    "updated_date": "2025-05-20 00:49:19 UTC"
  },
  {
    "arxiv_id": "2505.14726v1",
    "title": "MedBLIP: Fine-tuning BLIP for Medical Image Captioning",
    "authors": [
      "Manshi Limbu",
      "Diwita Banerjee"
    ],
    "abstract": "Medical image captioning is a challenging task that requires generating\nclinically accurate and semantically meaningful descriptions of radiology\nimages. While recent vision-language models (VLMs) such as BLIP, BLIP2, Gemini\nand ViT-GPT2 show strong performance on natural image datasets, they often\nproduce generic or imprecise captions when applied to specialized medical\ndomains. In this project, we explore the effectiveness of fine-tuning the BLIP\nmodel on the ROCO dataset for improved radiology captioning. We compare the\nfine-tuned BLIP against its zero-shot version, BLIP-2 base, BLIP-2 Instruct and\na ViT-GPT2 transformer baseline. Our results demonstrate that domain-specific\nfine-tuning on BLIP significantly improves performance across both quantitative\nand qualitative evaluation metrics. We also visualize decoder cross-attention\nmaps to assess interpretability and conduct an ablation study to evaluate the\ncontributions of encoder-only and decoder-only fine-tuning. Our findings\nhighlight the importance of targeted adaptation for medical applications and\nsuggest that decoder-only fine-tuning (encoder-frozen) offers a strong\nperformance baseline with 5% lower training time than full fine-tuning, while\nfull model fine-tuning still yields the best results overall.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14726v1",
    "published_date": "2025-05-20 00:49:08 UTC",
    "updated_date": "2025-05-20 00:49:08 UTC"
  },
  {
    "arxiv_id": "2505.13787v1",
    "title": "Preference Learning with Lie Detectors can Induce Honesty or Evasion",
    "authors": [
      "Chris Cundy",
      "Adam Gleave"
    ],
    "abstract": "As AI systems become more capable, deceptive behaviors can undermine\nevaluation and mislead users at deployment. Recent work has shown that lie\ndetectors can accurately classify deceptive behavior, but they are not\ntypically used in the training pipeline due to concerns around contamination\nand objective hacking. We examine these concerns by incorporating a lie\ndetector into the labelling step of LLM post-training and evaluating whether\nthe learned policy is genuinely more honest, or instead learns to fool the lie\ndetector while remaining deceptive. Using DolusChat, a novel 65k-example\ndataset with paired truthful/deceptive responses, we identify three key factors\nthat determine the honesty of learned policies: amount of exploration during\npreference learning, lie detector accuracy, and KL regularization strength. We\nfind that preference learning with lie detectors and GRPO can lead to policies\nwhich evade lie detectors, with deception rates of over 85\\%. However, if the\nlie detector true positive rate (TPR) or KL regularization is sufficiently\nhigh, GRPO learns honest policies. In contrast, off-policy algorithms (DPO)\nconsistently lead to deception rates under 25\\% for realistic TPRs. Our results\nillustrate a more complex picture than previously assumed: depending on the\ncontext, lie-detector-enhanced training can be a powerful tool for scalable\noversight, or a counterproductive method encouraging undetectable misalignment.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.13787v1",
    "published_date": "2025-05-20 00:31:53 UTC",
    "updated_date": "2025-05-20 00:31:53 UTC"
  }
]
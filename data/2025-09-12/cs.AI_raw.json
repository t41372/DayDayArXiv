[
  {
    "arxiv_id": "2509.10753v1",
    "title": "HalluField: Detecting LLM Hallucinations via Field-Theoretic Modeling",
    "authors": [
      "Minh Vu",
      "Brian K. Tran",
      "Syed A. Shah",
      "Geigh Zollicoffer",
      "Nhat Hoang-Xuan",
      "Manish Bhattarai"
    ],
    "abstract": "Large Language Models (LLMs) exhibit impressive reasoning and question-answering capabilities. However, they often produce inaccurate or unreliable content known as hallucinations. This unreliability significantly limits their deployment in high-stakes applications. Thus, there is a growing need for a general-purpose method to detect hallucinations in LLMs. In this work, we introduce HalluField, a novel field-theoretic approach for hallucination detection based on a parametrized variational principle and thermodynamics. Inspired by thermodynamics, HalluField models an LLM's response to a given query and temperature setting as a collection of discrete likelihood token paths, each associated with a corresponding energy and entropy. By analyzing how energy and entropy distributions vary across token paths under changes in temperature and likelihood, HalluField quantifies the semantic stability of a response. Hallucinations are then detected by identifying unstable or erratic behavior in this energy landscape. HalluField is computationally efficient and highly practical: it operates directly on the model's output logits without requiring fine-tuning or auxiliary neural networks. Notably, the method is grounded in a principled physical interpretation, drawing analogies to the first law of thermodynamics. Remarkably, by modeling LLM behavior through this physical lens, HalluField achieves state-of-the-art hallucination detection performance across models and datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.10753v1",
    "published_date": "2025-09-12 23:49:52 UTC",
    "updated_date": "2025-09-12 23:49:52 UTC"
  },
  {
    "arxiv_id": "2509.10744v1",
    "title": "Automated MCQA Benchmarking at Scale: Evaluating Reasoning Traces as Retrieval Sources for Domain Adaptation of Small Language Models",
    "authors": [
      "Ozan Gokdemir",
      "Neil Getty",
      "Robert Underwood",
      "Sandeep Madireddy",
      "Franck Cappello",
      "Arvind Ramanathan",
      "Ian T. Foster",
      "Rick L. Stevens"
    ],
    "abstract": "As scientific knowledge grows at an unprecedented pace, evaluation benchmarks must evolve to reflect new discoveries and ensure language models are tested on current, diverse literature. We propose a scalable, modular framework for generating multiple-choice question-answering (MCQA) benchmarks directly from large corpora of scientific papers. Our pipeline automates every stage of MCQA creation, including PDF parsing, semantic chunking, question generation, and model evaluation. As a case study, we generate more than 16,000 MCQs from 22,000 open-access articles in radiation and cancer biology. We then evaluate a suite of small language models (1.1B-14B parameters) on these questions, comparing baseline accuracy with retrieval-augmented generation (RAG) from paper-derived semantic chunks and from reasoning traces distilled from GPT-4.1. We find that reasoning-trace retrieval consistently improves performance on both synthetic and expert-annotated benchmarks, enabling several small models to surpass GPT-4 on the 2023 Astro Radiation and Cancer Biology exam.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "This manuscript has been accepted for publication at the Supercomputing 25 (SC '25) Conference (Frontiers in Generative AI for HPC Science and Engineering: Foundations, Challenges, and Opportunities Workshop) in St. Louis, MO, USA on November 16th, 2025. It will appear in the SC25 Workshop Proceedings after that date",
    "pdf_url": "https://arxiv.org/pdf/2509.10744v1",
    "published_date": "2025-09-12 23:22:49 UTC",
    "updated_date": "2025-09-12 23:22:49 UTC"
  },
  {
    "arxiv_id": "2509.10740v1",
    "title": "How are Scientific Concepts Birthed? Typing Rules of Concept Formation in Theoretical Physics Reasoning",
    "authors": [
      "Omar Aguilar",
      "Anthony Aguirre"
    ],
    "abstract": "This work aims to formalize some of the ways scientific concepts are formed in the process of theoretical physics discovery. Since this may at first seem like a task beyond the scope of the exact sciences (natural and formal sciences), we begin by presenting arguments for why scientific concept formation can be formalized. Then, we introduce type theory as a natural and well-suited framework for this formalization. We formalize what we call \"ways of discovering new concepts\" including concept distinction, property preservation, and concept change, as cognitive typing rules. Next, we apply these cognitive typing rules to two case studies of conceptual discovery in the history of physics: Einstein's reasoning leading to the impossibility of frozen waves, and his conceptual path to the relativity of time. In these historical episodes, we recast what a physicist might informally call \"ways of discovering new scientific concepts\" as compositional typing rules built from cognitive typing rules - thus formalizing them as scientific discovery mechanisms. Lastly, we computationally model the type-theoretic reconstruction of Einstein's conceptual path to the relativity of time as a program synthesis task.",
    "categories": [
      "physics.hist-ph",
      "cs.AI"
    ],
    "primary_category": "physics.hist-ph",
    "comment": "28 pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2509.10740v1",
    "published_date": "2025-09-12 23:04:23 UTC",
    "updated_date": "2025-09-12 23:04:23 UTC"
  },
  {
    "arxiv_id": "2509.10723v1",
    "title": "Dark Patterns Meet GUI Agents: LLM Agent Susceptibility to Manipulative Interfaces and the Role of Human Oversight",
    "authors": [
      "Jingyu Tang",
      "Chaoran Chen",
      "Jiawen Li",
      "Zhiping Zhang",
      "Bingcan Guo",
      "Ibrahim Khalilov",
      "Simret Araya Gebreegziabher",
      "Bingsheng Yao",
      "Dakuo Wang",
      "Yanfang Ye",
      "Tianshi Li",
      "Ziang Xiao",
      "Yaxing Yao",
      "Toby Jia-Jun Li"
    ],
    "abstract": "The dark patterns, deceptive interface designs manipulating user behaviors, have been extensively studied for their effects on human decision-making and autonomy. Yet, with the rising prominence of LLM-powered GUI agents that automate tasks from high-level intents, understanding how dark patterns affect agents is increasingly important. We present a two-phase empirical study examining how agents, human participants, and human-AI teams respond to 16 types of dark patterns across diverse scenarios. Phase 1 highlights that agents often fail to recognize dark patterns, and even when aware, prioritize task completion over protective action. Phase 2 revealed divergent failure modes: humans succumb due to cognitive shortcuts and habitual compliance, while agents falter from procedural blind spots. Human oversight improved avoidance but introduced costs such as attentional tunneling and cognitive load. Our findings show neither humans nor agents are uniformly resilient, and collaboration introduces new vulnerabilities, suggesting design needs for transparency, adjustable autonomy, and oversight.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.10723v1",
    "published_date": "2025-09-12 22:26:31 UTC",
    "updated_date": "2025-09-12 22:26:31 UTC"
  },
  {
    "arxiv_id": "2509.19323v1",
    "title": "Magnitude Matters: a Superior Class of Similarity Metrics for Holistic Semantic Understanding",
    "authors": [
      "V. S. Raghu Parupudi"
    ],
    "abstract": "Vector comparison in high dimensions is a fundamental task in NLP, yet it is dominated by two baselines: the raw dot product, which is unbounded and sensitive to vector norms, and the cosine similarity, which discards magnitude information entirely. This paper challenges both standards by proposing and rigorously evaluating a new class of parameter-free, magnitude-aware similarity metrics. I introduce two such functions, Overlap Similarity (OS) and Hyperbolic Tangent Similarity (HTS), designed to integrate vector magnitude and alignment in a more principled manner. To ensure that my findings are robust and generalizable, I conducted a comprehensive evaluation using four state-of-the-art sentence embedding models (all-MiniLM-L6-v2, all-mpnet-base-v2, paraphrase-mpnet-base-v2, and BAAI/bge-large-en-v1.5) across a diverse suite of eight standard NLP benchmarks, including STS-B, SICK, Quora, and PAWS. Using the Wilcoxon signed-rank test for statistical significance, my results are definitive: on the tasks requiring holistic semantic understanding (paraphrase and inference), both OS and HTS provide a statistically significant improvement in Mean Squared Error over both the raw dot product and cosine similarity, regardless of the underlying embedding model.Crucially, my findings delineate the specific domain of advantage for these metrics: for tasks requiring holistic semantic understanding like paraphrase and inference, my magnitude-aware metrics offer a statistically superior alternative. This significant improvement was not observed on benchmarks designed to test highly nuanced compositional semantics (SICK, STS-B), identifying the challenge of representing compositional text as a distinct and important direction for future work.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "submitted to AAAI 2026",
    "pdf_url": "https://arxiv.org/pdf/2509.19323v1",
    "published_date": "2025-09-12 22:26:15 UTC",
    "updated_date": "2025-09-12 22:26:15 UTC"
  },
  {
    "arxiv_id": "2509.10707v2",
    "title": "Understanding AI Evaluation Patterns: How Different GPT Models Assess Vision-Language Descriptions",
    "authors": [
      "Sajjad Abdoli",
      "Rudi Cilibrasi",
      "Rima Al-Shikh"
    ],
    "abstract": "As AI systems increasingly evaluate other AI outputs, understanding their assessment behavior becomes crucial for preventing cascading biases. This study analyzes vision-language descriptions generated by NVIDIA's Describe Anything Model and evaluated by three GPT variants (GPT-4o, GPT-4o-mini, GPT-5) to uncover distinct \"evaluation personalities\" the underlying assessment strategies and biases each model demonstrates. GPT-4o-mini exhibits systematic consistency with minimal variance, GPT-4o excels at error detection, while GPT-5 shows extreme conservatism with high variability. Controlled experiments using Gemini 2.5 Pro as an independent question generator validate that these personalities are inherent model properties rather than artifacts. Cross-family analysis through semantic similarity of generated questions reveals significant divergence: GPT models cluster together with high similarity while Gemini exhibits markedly different evaluation strategies. All GPT models demonstrate a consistent 2:1 bias favoring negative assessment over positive confirmation, though this pattern appears family-specific rather than universal across AI architectures. These findings suggest that evaluation competence does not scale with general capability and that robust AI assessment requires diverse architectural perspectives.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.10707v2",
    "published_date": "2025-09-12 21:48:59 UTC",
    "updated_date": "2025-09-19 14:57:35 UTC"
  },
  {
    "arxiv_id": "2509.10704v1",
    "title": "Maestro: Self-Improving Text-to-Image Generation via Agent Orchestration",
    "authors": [
      "Xingchen Wan",
      "Han Zhou",
      "Ruoxi Sun",
      "Hootan Nakhost",
      "Ke Jiang",
      "Rajarishi Sinha",
      "Sercan Ö. Arık"
    ],
    "abstract": "Text-to-image (T2I) models, while offering immense creative potential, are highly reliant on human intervention, posing significant usability challenges that often necessitate manual, iterative prompt engineering over often underspecified prompts. This paper introduces Maestro, a novel self-evolving image generation system that enables T2I models to autonomously self-improve generated images through iterative evolution of prompts, using only an initial prompt. Maestro incorporates two key innovations: 1) self-critique, where specialized multimodal LLM (MLLM) agents act as 'critics' to identify weaknesses in generated images, correct for under-specification, and provide interpretable edit signals, which are then integrated by a 'verifier' agent while preserving user intent; and 2) self-evolution, utilizing MLLM-as-a-judge for head-to-head comparisons between iteratively generated images, eschewing problematic images, and evolving creative prompt candidates that align with user intents. Extensive experiments on complex T2I tasks using black-box models demonstrate that Maestro significantly improves image quality over initial prompts and state-of-the-art automated methods, with effectiveness scaling with more advanced MLLM components. This work presents a robust, interpretable, and effective pathway towards self-improving T2I generation.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "15 pages, 7 figures, 2 tables (22 pages, 9 figures and 3 tables including references and appendices)",
    "pdf_url": "https://arxiv.org/pdf/2509.10704v1",
    "published_date": "2025-09-12 21:45:16 UTC",
    "updated_date": "2025-09-12 21:45:16 UTC"
  },
  {
    "arxiv_id": "2509.10695v1",
    "title": "Kalman Bayesian Transformer",
    "authors": [
      "Haoming Jing",
      "Oren Wright",
      "José M. F. Moura",
      "Yorie Nakahira"
    ],
    "abstract": "Sequential fine-tuning of transformers is useful when new data arrive sequentially, especially with shifting distributions. Unlike batch learning, sequential learning demands that training be stabilized despite a small amount of data by balancing new information and previously learned knowledge in the pre-trained models. This challenge is further complicated when training is to be completed in latency-critical environments and learning must additionally quantify and be mediated by uncertainty. Motivated by these challenges, we propose a novel method that frames sequential fine-tuning as a posterior inference problem within a Bayesian framework. Our approach integrates closed-form moment propagation of random variables, Kalman Bayesian Neural Networks, and Taylor approximations of the moments of softmax functions. By explicitly accounting for pre-trained models as priors and adaptively balancing them against new information based on quantified uncertainty, our method achieves robust and data-efficient sequential learning. The effectiveness of our method is demonstrated through numerical simulations involving sequential adaptation of a decision transformer to tasks characterized by distribution shifts and limited memory resources.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to the 64th IEEE Conference on Decision and Control (CDC 2025)",
    "pdf_url": "https://arxiv.org/pdf/2509.10695v1",
    "published_date": "2025-09-12 21:15:23 UTC",
    "updated_date": "2025-09-12 21:15:23 UTC"
  },
  {
    "arxiv_id": "2509.10693v1",
    "title": "Learning Concave Bid Shading Strategies in Online Auctions via Measure-valued Proximal Optimization",
    "authors": [
      "Iman Nodozi",
      "Djordje Gligorijevic",
      "Abhishek Halder"
    ],
    "abstract": "This work proposes a bid shading strategy for first-price auctions as a measure-valued optimization problem. We consider a standard parametric form for bid shading and formulate the problem as convex optimization over the joint distribution of shading parameters. After each auction, the shading parameter distribution is adapted via a regularized Wasserstein-proximal update with a data-driven energy functional. This energy functional is conditional on the context, i.e., on publisher/user attributes such as domain, ad slot type, device, or location. The proposed algorithm encourages the bid distribution to place more weight on values with higher expected surplus, i.e., where the win probability and the value gap are both large. We show that the resulting measure-valued convex optimization problem admits a closed form solution. A numerical example illustrates the proposed method.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.10693v1",
    "published_date": "2025-09-12 21:11:06 UTC",
    "updated_date": "2025-09-12 21:11:06 UTC"
  },
  {
    "arxiv_id": "2509.10691v2",
    "title": "Privacy-Preserving Decentralized Federated Learning via Explainable Adaptive Differential Privacy",
    "authors": [
      "Fardin Jalil Piran",
      "Zhiling Chen",
      "Yang Zhang",
      "Qianyu Zhou",
      "Jiong Tang",
      "Farhad Imani"
    ],
    "abstract": "Decentralized Federated Learning (DFL) enables collaborative model training without a central server, but it remains vulnerable to privacy leakage because shared model updates can expose sensitive information through inversion, reconstruction, and membership inference attacks. Differential Privacy (DP) provides formal safeguards, yet existing DP-enabled DFL methods operate as black-boxes that cannot track cumulative noise added across clients and rounds, forcing each participant to inject worst-case perturbations that severely degrade accuracy. We propose PrivateDFL, a new explainable and privacy-preserving framework that addresses this gap by combining a HyperDimensional Computing (HD) model with a transparent DP noise accountant tailored to decentralized learning. HD offers structured, noise-tolerant high-dimensional representations, while the accountant explicitly tracks cumulative perturbations so each client adds only the minimal incremental noise required to satisfy its (epsilon, delta) budget. This yields significantly tighter and more interpretable privacy-utility tradeoffs than prior DP-DFL approaches. Experiments on MNIST (image), ISOLET (speech), and UCI-HAR (wearable sensor) show that PrivateDFL consistently surpasses centralized DP-SGD and Renyi-DP Transformer and deep learning baselines under both IID and non-IID partitions, improving accuracy by up to 24.4% on MNIST, over 80% on ISOLET, and 14.7% on UCI-HAR, while reducing inference latency by up to 76 times and energy consumption by up to 36 times. These results position PrivateDFL as an efficient and trustworthy solution for privacy-sensitive pattern recognition applications such as healthcare, finance, human-activity monitoring, and industrial sensing. Future work will extend the accountant to adversarial participation, heterogeneous privacy budgets, and dynamic topologies.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "20 pages",
    "pdf_url": "https://arxiv.org/pdf/2509.10691v2",
    "published_date": "2025-09-12 20:52:41 UTC",
    "updated_date": "2025-12-06 18:31:39 UTC"
  },
  {
    "arxiv_id": "2509.19322v1",
    "title": "Readme_AI: Dynamic Context Construction for Large Language Models",
    "authors": [
      "Millie Vyas",
      "Timothy Blattner",
      "Alden Dima"
    ],
    "abstract": "Despite being trained on significant amounts of data, Large Language Models (LLMs) can provide inaccurate or unreliable information in the context of a user's specific query. Given query-specific context significantly improves the usefulness of its responses. In this paper, we present a specification that can be used to dynamically build context for data sources. The data source owner creates the file containing metadata for LLMs to use when reasoning about dataset-related queries. To demonstrate our proposed specification, we created a prototype Readme_AI Model Context Protocol (MCP) server that retrieves the metadata from the data source and uses it to dynamically build context. Some features that make this specification dynamic are the extensible types that represent crawling web-pages, fetching data from data repositories, downloading and parsing publications, and general text. The context is formatted and grouped using user-specified tags that provide clear contextual information for the LLM to reason about the content. We demonstrate the capabilities of this early prototype by asking the LLM about the NIST-developed Hedgehog library, for which common LLMs often provides inaccurate and irrelevant responses containing hallucinations. With Readme_AI, the LLM receives enough context that it is now able to reason about the library and its use, and even generate code interpolated from examples that were included in the Readme_AI file provided by Hedgehog's developer. Our primary contribution is a extensible protocol for dynamically grounding LLMs in specialized, owner-provided data, enhancing responses from LLMs and reducing hallucinations. The source code for the Readme_AI tool is posted here: https://github.com/usnistgov/readme_ai .",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.19322v1",
    "published_date": "2025-09-12 20:34:58 UTC",
    "updated_date": "2025-09-12 20:34:58 UTC"
  },
  {
    "arxiv_id": "2509.10685v2",
    "title": "Pluralistic Alignment for Healthcare: A Role-Driven Framework",
    "authors": [
      "Jiayou Zhong",
      "Anudeex Shetty",
      "Chao Jia",
      "Xuanrui Lin",
      "Usman Naseem"
    ],
    "abstract": "As large language models are increasingly deployed in sensitive domains such as healthcare, ensuring their outputs reflect the diverse values and perspectives held across populations is critical. However, existing alignment approaches, including pluralistic paradigms like Modular Pluralism, often fall short in the health domain, where personal, cultural, and situational factors shape pluralism. Motivated by the aforementioned healthcare challenges, we propose a first lightweight, generalizable, pluralistic alignment approach, EthosAgents, designed to simulate diverse perspectives and values. We empirically show that it advances the pluralistic alignment for all three modes across seven varying-sized open and closed models. Our findings reveal that health-related pluralism demands adaptable and normatively aware approaches, offering insights into how these models can better respect diversity in other high-stakes domains.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP 2025 (Main Proceedings)",
    "pdf_url": "https://arxiv.org/pdf/2509.10685v2",
    "published_date": "2025-09-12 20:28:27 UTC",
    "updated_date": "2025-09-18 16:57:40 UTC"
  },
  {
    "arxiv_id": "2509.10683v1",
    "title": "A Comparison and Evaluation of Fine-tuned Convolutional Neural Networks to Large Language Models for Image Classification and Segmentation of Brain Tumors on MRI",
    "authors": [
      "Felicia Liu",
      "Jay J. Yoo",
      "Farzad Khalvati"
    ],
    "abstract": "Large Language Models (LLMs) have shown strong performance in text-based healthcare tasks. However, their utility in image-based applications remains unexplored. We investigate the effectiveness of LLMs for medical imaging tasks, specifically glioma classification and segmentation, and compare their performance to that of traditional convolutional neural networks (CNNs). Using the BraTS 2020 dataset of multi-modal brain MRIs, we evaluated a general-purpose vision-language LLM (LLaMA 3.2 Instruct) both before and after fine-tuning, and benchmarked its performance against custom 3D CNNs. For glioma classification (Low-Grade vs. High-Grade), the CNN achieved 80% accuracy and balanced precision and recall. The general LLM reached 76% accuracy but suffered from a specificity of only 18%, often misclassifying Low-Grade tumors. Fine-tuning improved specificity to 55%, but overall performance declined (e.g., accuracy dropped to 72%). For segmentation, three methods - center point, bounding box, and polygon extraction, were implemented. CNNs accurately localized gliomas, though small tumors were sometimes missed. In contrast, LLMs consistently clustered predictions near the image center, with no distinction of glioma size, location, or placement. Fine-tuning improved output formatting but failed to meaningfully enhance spatial accuracy. The bounding polygon method yielded random, unstructured outputs. Overall, CNNs outperformed LLMs in both tasks. LLMs showed limited spatial understanding and minimal improvement from fine-tuning, indicating that, in their current form, they are not well-suited for image-based tasks. More rigorous fine-tuning or alternative training strategies may be needed for LLMs to achieve better performance, robustness, and utility in the medical space.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.10683v1",
    "published_date": "2025-09-12 20:26:53 UTC",
    "updated_date": "2025-09-12 20:26:53 UTC"
  },
  {
    "arxiv_id": "2509.10682v1",
    "title": "LLM in the Middle: A Systematic Review of Threats and Mitigations to Real-World LLM-based Systems",
    "authors": [
      "Vitor Hugo Galhardo Moia",
      "Igor Jochem Sanz",
      "Gabriel Antonio Fontes Rebello",
      "Rodrigo Duarte de Meneses",
      "Briland Hitaj",
      "Ulf Lindqvist"
    ],
    "abstract": "The success and wide adoption of generative AI (GenAI), particularly large language models (LLMs), has attracted the attention of cybercriminals seeking to abuse models, steal sensitive data, or disrupt services. Moreover, providing security to LLM-based systems is a great challenge, as both traditional threats to software applications and threats targeting LLMs and their integration must be mitigated. In this survey, we shed light on security and privacy concerns of such LLM-based systems by performing a systematic review and comprehensive categorization of threats and defensive strategies considering the entire software and LLM life cycles. We analyze real-world scenarios with distinct characteristics of LLM usage, spanning from development to operation. In addition, threats are classified according to their severity level and to which scenarios they pertain, facilitating the identification of the most relevant threats. Recommended defense strategies are systematically categorized and mapped to the corresponding life cycle phase and possible attack strategies they attenuate. This work paves the way for consumers and vendors to understand and efficiently mitigate risks during integration of LLMs in their respective solutions or organizations. It also enables the research community to benefit from the discussion of open challenges and edge cases that may hinder the secure and privacy-preserving adoption of LLM-based systems.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.ET",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "37 pages, 8 figures, 13 tables",
    "pdf_url": "https://arxiv.org/pdf/2509.10682v1",
    "published_date": "2025-09-12 20:26:16 UTC",
    "updated_date": "2025-09-12 20:26:16 UTC"
  },
  {
    "arxiv_id": "2509.12263v1",
    "title": "InPhyRe Discovers: Large Multimodal Models Struggle in Inductive Physical Reasoning",
    "authors": [
      "Gautam Sreekumar",
      "Vishnu Naresh Boddeti"
    ],
    "abstract": "Large multimodal models (LMMs) encode universal physical laws observed during training, such as momentum conservation, as parametric knowledge. It allows LMMs to answer physical reasoning queries, such as the outcome of a potential collision event from visual input. However, since parametric knowledge includes only the physical laws seen during training, it is insufficient for reasoning when the inference scenario violates these physical laws. In contrast, humans possess the skill to adapt their physical reasoning to unseen physical environments from a few visual examples. This ability, which we refer to as inductive physical reasoning, is indispensable for LMMs if they are to replace human agents in safety-critical applications. Despite its importance, existing visual benchmarks evaluate only the parametric knowledge in LMMs, and not inductive physical reasoning. To this end, we propose InPhyRe, the first visual question answering benchmark to measure inductive physical reasoning in LMMs. InPhyRe evaluates LMMs on their ability to predict the outcome of collision events in algorithmically generated synthetic collision videos. By inspecting 13 LMMs, InPhyRe informs us that (1) LMMs struggle to apply their limited parametric knowledge about universal physical laws to reasoning, (2) inductive physical reasoning in LMMs is weak when demonstration samples violate universal physical laws, and (3) inductive physical reasoning in LMMs suffers from language bias and largely ignores the visual inputs, questioning the trustworthiness of LMMs regarding visual inputs.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "35 pages including appendix",
    "pdf_url": "https://arxiv.org/pdf/2509.12263v1",
    "published_date": "2025-09-12 20:07:12 UTC",
    "updated_date": "2025-09-12 20:07:12 UTC"
  },
  {
    "arxiv_id": "2509.13345v2",
    "title": "Beyond Accuracy: Rethinking Hallucination and Regulatory Response in Generative AI",
    "authors": [
      "Zihao Li",
      "Weiwei Yi",
      "Jiahong Chen"
    ],
    "abstract": "Hallucination in generative AI is often treated as a technical failure to produce factually correct output. Yet this framing underrepresents the broader significance of hallucinated content in language models, which may appear fluent, persuasive, and contextually appropriate while conveying distortions that escape conventional accuracy checks. This paper critically examines how regulatory and evaluation frameworks have inherited a narrow view of hallucination, one that prioritises surface verifiability over deeper questions of meaning, influence, and impact. We propose a layered approach to understanding hallucination risks, encompassing epistemic instability, user misdirection, and social-scale effects. Drawing on interdisciplinary sources and examining instruments such as the EU AI Act and the GDPR, we show that current governance models struggle to address hallucination when it manifests as ambiguity, bias reinforcement, or normative convergence. Rather than improving factual precision alone, we argue for regulatory responses that account for languages generative nature, the asymmetries between system and user, and the shifting boundaries between information, persuasion, and harm.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.13345v2",
    "published_date": "2025-09-12 19:41:10 UTC",
    "updated_date": "2025-10-23 20:49:56 UTC"
  },
  {
    "arxiv_id": "2509.10660v1",
    "title": "ZapGPT: Free-form Language Prompting for Simulated Cellular Control",
    "authors": [
      "Nam H. Le",
      "Patrick Erickson",
      "Yanbo Zhang",
      "Michael Levin",
      "Josh Bongard"
    ],
    "abstract": "Human language is one of the most expressive tools for conveying intent, yet most artificial or biological systems lack mechanisms to interpret or respond meaningfully to it. Bridging this gap could enable more natural forms of control over complex, decentralized systems. In AI and artificial life, recent work explores how language can specify high-level goals, but most systems still depend on engineered rewards, task-specific supervision, or rigid command sets, limiting generalization to novel instructions. Similar constraints apply in synthetic biology and bioengineering, where the locus of control is often genomic rather than environmental perturbation.\n  A key open question is whether artificial or biological collectives can be guided by free-form natural language alone, without task-specific tuning or carefully designed evaluation metrics. We provide one possible answer here by showing, for the first time, that simple agents' collective behavior can be guided by free-form language prompts: one AI model transforms an imperative prompt into an intervention that is applied to simulated cells; a second AI model scores how well the prompt describes the resulting cellular dynamics; and the former AI model is evolved to improve the scores generated by the latter.\n  Unlike previous work, our method does not require engineered fitness functions or domain-specific prompt design. We show that the evolved system generalizes to unseen prompts without retraining. By treating natural language as a control layer, the system suggests a future in which spoken or written prompts could direct computational, robotic, or biological systems to desired behaviors. This work provides a concrete step toward this vision of AI-biology partnerships, in which language replaces mathematical objective functions, fixed rules, and domain-specific programming.",
    "categories": [
      "cs.AI",
      "cs.MA",
      "q-bio.CB"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.10660v1",
    "published_date": "2025-09-12 19:38:46 UTC",
    "updated_date": "2025-09-12 19:38:46 UTC"
  },
  {
    "arxiv_id": "2509.10656v1",
    "title": "Self-Supervised Goal-Reaching Results in Multi-Agent Cooperation and Exploration",
    "authors": [
      "Chirayu Nimonkar",
      "Shlok Shah",
      "Catherine Ji",
      "Benjamin Eysenbach"
    ],
    "abstract": "For groups of autonomous agents to achieve a particular goal, they must engage in coordination and long-horizon reasoning. However, designing reward functions to elicit such behavior is challenging. In this paper, we study how self-supervised goal-reaching techniques can be leveraged to enable agents to cooperate. The key idea is that, rather than have agents maximize some scalar reward, agents aim to maximize the likelihood of visiting a certain goal. This problem setting enables human users to specify tasks via a single goal state rather than implementing a complex reward function. While the feedback signal is quite sparse, we will demonstrate that self-supervised goal-reaching techniques enable agents to learn from such feedback. On MARL benchmarks, our proposed method outperforms alternative approaches that have access to the same sparse reward signal as our method. While our method has no explicit mechanism for exploration, we observe that self-supervised multi-agent goal-reaching leads to emergent cooperation and exploration in settings where alternative approaches never witness a single successful trial.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Project website with videos https://chirayu-n.github.io/gcmarl and code https://github.com/Chirayu-N/gc-marl are online",
    "pdf_url": "https://arxiv.org/pdf/2509.10656v1",
    "published_date": "2025-09-12 19:35:20 UTC",
    "updated_date": "2025-09-12 19:35:20 UTC"
  },
  {
    "arxiv_id": "2509.10653v1",
    "title": "SCOR: A Framework for Responsible AI Innovation in Digital Ecosystems",
    "authors": [
      "Mohammad Saleh Torkestani",
      "Taha Mansouri"
    ],
    "abstract": "AI-driven digital ecosystems span diverse stakeholders including technology firms, regulators, accelerators and civil society, yet often lack cohesive ethical governance. This paper proposes a four-pillar framework (SCOR) to embed accountability, fairness, and inclusivity across such multi-actor networks. Leveraging a design science approach, we develop a Shared Ethical Charter(S), structured Co-Design and Stakeholder Engagement protocols(C), a system of Continuous Oversight and Learning(O), and Adaptive Regulatory Alignment strategies(R). Each component includes practical guidance, from lite modules for resource-constrained start-ups to in-depth auditing systems for larger consortia. Through illustrative vignettes in healthcare, finance, and smart city contexts, we demonstrate how the framework can harmonize organizational culture, leadership incentives, and cross-jurisdictional compliance. Our mixed-method KPI design further ensures that quantitative targets are complemented by qualitative assessments of user trust and cultural change. By uniting ethical principles with scalable operational structures, this paper offers a replicable pathway toward responsible AI innovation in complex digital ecosystems.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "Proceeding of The British Academy of Management Conference 2025, University of Kent, UK",
    "pdf_url": "https://arxiv.org/pdf/2509.10653v1",
    "published_date": "2025-09-12 19:29:51 UTC",
    "updated_date": "2025-09-12 19:29:51 UTC"
  },
  {
    "arxiv_id": "2509.10652v1",
    "title": "Vibe Coding for UX Design: Understanding UX Professionals' Perceptions of AI-Assisted Design and Development",
    "authors": [
      "Jie Li",
      "Youyang Hou",
      "Laura Lin",
      "Ruihao Zhu",
      "Hancheng Cao",
      "Abdallah El Ali"
    ],
    "abstract": "Generative AI is reshaping UX design practices through \"vibe coding,\" where UX professionals express intent in natural language and AI translates it into functional prototypes and code. Despite rapid adoption, little research has examined how vibe coding reconfigures UX workflows and collaboration. Drawing on interviews with 20 UX professionals across enterprises, startups, and academia, we show how vibe coding follows a four-stage workflow of ideation, AI generation, debugging, and review. This accelerates iteration, supports creativity, and lowers barriers to participation. However, professionals reported challenges of code unreliability, integration, and AI over-reliance. We find tensions between efficiency-driven prototyping (\"intending the right design\") and reflection (\"designing the right intention\"), introducing new asymmetries in trust, responsibility, and social stigma within teams. Through the lens of responsible human-AI collaboration for AI-assisted UX design and development, we contribute a deeper understanding of deskilling, ownership and disclosure, and creativity safeguarding in the age of vibe coding.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "cs.ET"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.10652v1",
    "published_date": "2025-09-12 19:28:38 UTC",
    "updated_date": "2025-09-12 19:28:38 UTC"
  },
  {
    "arxiv_id": "2509.10641v2",
    "title": "Test-Time Warmup for Multimodal Large Language Models",
    "authors": [
      "Nikita Rajaneesh",
      "Thomas Zollo",
      "Richard Zemel"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) hold great promise for advanced reasoning at the intersection of text and images, yet they have not fully realized this potential. MLLMs typically integrate an LLM, a vision encoder, and a connector that maps the vision encoder's embeddings into the LLM's text embedding space. Although each component is pretrained on massive datasets with billions of samples, the entire multimodal model is typically trained on only thousands (or a few million) samples, which can result in weak performance on complex reasoning tasks. To address these shortcomings, instead of relying on extensive labeled datasets for fine-tuning, we propose a Test-Time Warmup method that adapts the MLLM per test instance by leveraging data from weakly supervised auxiliary tasks. With our approach, we observe a relative performance improvement of 4.03% on MMMU, 5.28% on VQA-Rad, and 1.63% on GQA on the Llama-Vision-Instruct model. Our method demonstrates that 'warming up' before inference can enhance MLLMs' robustness across diverse reasoning tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.10641v2",
    "published_date": "2025-09-12 18:58:42 UTC",
    "updated_date": "2025-11-06 12:24:59 UTC"
  },
  {
    "arxiv_id": "2509.12259v1",
    "title": "Quantum-Inspired Stacked Integrated Concept Graph Model (QISICGM) for Diabetes Risk Prediction",
    "authors": [
      "Kenneth G. Young"
    ],
    "abstract": "The Quantum-Inspired Stacked Integrated Concept Graph Model (QISICGM) is an innovative machine learning framework that harnesses quantum-inspired techniques to predict diabetes risk with exceptional accuracy and efficiency. Utilizing the PIMA Indians Diabetes dataset augmented with 2,000 synthetic samples to mitigate class imbalance (total: 2,768 samples, 1,949 positives), QISICGM integrates a self-improving concept graph with a stacked ensemble comprising Random Forests (RF), Extra Trees (ET), transformers, convolutional neural networks (CNNs), and feed-forward neural networks (FFNNs). This approach achieves an out-of-fold (OOF) F1 score of 0.8933 and an AUC of 0.8699, outperforming traditional methods. Quantum inspired elements, such as phase feature mapping and neighborhood sequence modeling, enrich feature representations, enabling CPU-efficient inference at 8.5 rows per second. This paper presents a detailed architecture, theoretical foundations, code insights, and performance evaluations, including visualizations from the outputs subfolder. The open-source implementation (v1.0.0) is available at https://github.com/keninayoung/QISICGM, positioning QISICGM as a potential benchmark for AI-assisted clinical triage in diabetes and beyond. Ultimately, this work emphasizes trustworthy AI through calibration, interpretability, and open-source reproducibility.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "quant-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "13 pages, 3 figures, includes performance tables and visualizations. Proposes a Quantum-Inspired Stacked Integrated Concept Graph Model (QISICGM) that integrates phase feature mapping, self-improving concept graphs, and neighborhood sequence modeling within a stacked ensemble. Demonstrates improved F1 and AUC on an augmented PIMA Diabetes dataset with efficient CPU inference",
    "pdf_url": "https://arxiv.org/pdf/2509.12259v1",
    "published_date": "2025-09-12 18:26:31 UTC",
    "updated_date": "2025-09-12 18:26:31 UTC"
  },
  {
    "arxiv_id": "2509.10626v1",
    "title": "Optimal Multimarginal Schrödinger Bridge: Minimum Spanning Tree over Measure-valued Vertices",
    "authors": [
      "Georgiy A. Bondar",
      "Abhishek Halder"
    ],
    "abstract": "The Multimarginal Schrödinger Bridge (MSB) finds the optimal coupling among a collection of random vectors with known statistics and a known correlation structure. In the MSB formulation, this correlation structure is specified \\emph{a priori} as an undirected connected graph with measure-valued vertices. In this work, we formulate and solve the problem of finding the optimal MSB in the sense we seek the optimal coupling over all possible graph structures. We find that computing the optimal MSB amounts to solving the minimum spanning tree problem over measure-valued vertices. We show that the resulting problem can be solved in two steps. The first step constructs a complete graph with edge weight equal to a sum of the optimal value of the corresponding bimarginal SB and the entropies of the endpoints. The second step solves a standard minimum spanning tree problem over that complete weighted graph. Numerical experiments illustrate the proposed solution.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.10626v1",
    "published_date": "2025-09-12 18:15:42 UTC",
    "updated_date": "2025-09-12 18:15:42 UTC"
  },
  {
    "arxiv_id": "2509.10625v1",
    "title": "No Answer Needed: Predicting LLM Answer Accuracy from Question-Only Linear Probes",
    "authors": [
      "Iván Vicente Moreno Cencerrado",
      "Arnau Padrés Masdemont",
      "Anton Gonzalvez Hawthorne",
      "David Demitri Africa",
      "Lorenzo Pacchiardi"
    ],
    "abstract": "Do large language models (LLMs) anticipate when they will answer correctly? To study this, we extract activations after a question is read but before any tokens are generated, and train linear probes to predict whether the model's forthcoming answer will be correct. Across three open-source model families ranging from 7 to 70 billion parameters, projections on this \"in-advance correctness direction\" trained on generic trivia questions predict success in distribution and on diverse out-of-distribution knowledge datasets, outperforming black-box baselines and verbalised predicted confidence. Predictive power saturates in intermediate layers, suggesting that self-assessment emerges mid-computation. Notably, generalisation falters on questions requiring mathematical reasoning. Moreover, for models responding \"I don't know\", doing so strongly correlates with the probe score, indicating that the same direction also captures confidence. By complementing previous results on truthfulness and other behaviours obtained with probes and sparse auto-encoders, our work contributes essential findings to elucidate LLM internals.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.10625v1",
    "published_date": "2025-09-12 18:09:55 UTC",
    "updated_date": "2025-09-12 18:09:55 UTC"
  },
  {
    "arxiv_id": "2509.10600v4",
    "title": "Faster Results from a Smarter Schedule: Reframing Collegiate Cross Country through Analysis of the National Running Club Database",
    "authors": [
      "Jonathan A. Karr",
      "Ryan M. Fryer",
      "Ben Darden",
      "Nicholas Pell",
      "Kayla Ambrose",
      "Evan Hall",
      "Ramzi K. Bualuan",
      "Nitesh V. Chawla"
    ],
    "abstract": "Collegiate cross country teams often build their season schedules on intuition rather than evidence, partly because large-scale performance datasets are not publicly accessible. To address this limitation, we introduce the National Running Club Database (NRCD), the first openly available dataset to aggregate 23,725 race results from 7,594 collegiate club athletes across the 2023-2025 seasons. Unlike existing resources, NRCD includes detailed course metadata, allowing us to develop two standardized performance metrics: Converted Only (distance correction) and Standardized (distance, weather, and elevation adjusted). Using these standardized measures, we find that athletes with slower initial performances exhibit the greatest improvement within a season, and that race frequency is the strongest predictor of improvement. Using six machine learning models, random forest achieves the highest accuracy (r squared equals 0.92), revealing that athletes who race more frequently progress significantly faster than those who do not. At the team level, programs whose athletes race at least four times during the regular season have substantially higher odds of placing in the top 15 at nationals (chi-squared less than 0.01). These results challenge common coaching practices that favor minimal racing before championship meets. Our findings demonstrate that a data-informed scheduling strategy improves both individual development and team competitiveness. The NRCD provides a new foundation for evidence-based decision-making in collegiate cross country and opens opportunities for further research on standardized, longitudinal athlete performance modeling.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.10600v4",
    "published_date": "2025-09-12 17:50:23 UTC",
    "updated_date": "2025-12-15 17:45:12 UTC"
  },
  {
    "arxiv_id": "2509.10432v2",
    "title": "Standards in the Preparation of Biomedical Research Metadata: A Bridge2AI Perspective",
    "authors": [
      "Harry Caufield",
      "Satrajit Ghosh",
      "Sek Wong Kong",
      "Jillian Parker",
      "Nathan Sheffield",
      "Bhavesh Patel",
      "Andrew Williams",
      "Timothy Clark",
      "Monica C. Munoz-Torres"
    ],
    "abstract": "AI-readiness describes the degree to which data may be optimally and ethically used for subsequent AI and Machine Learning (AI/ML) methods, where those methods may involve some combination of model training, data classification, and ethical, explainable prediction. The Bridge2AI consortium has defined the particular criteria a biomedical dataset may possess to render it AI-ready: in brief, a dataset's readiness is related to its FAIRness, provenance, degree of characterization, explainability, sustainability, and computability, in addition to its accompaniment with documentation about ethical data practices.\n  To ensure AI-readiness and to clarify data structure and relationships within Bridge2AI's Grand Challenges (GCs), particular types of metadata are necessary. The GCs within the Bridge2AI initiative include four data-generating projects focusing on generating AI/ML-ready datasets to tackle complex biomedical and behavioral research problems. These projects develop standardized, multimodal data, tools, and training resources to support AI integration, while addressing ethical data practices. Examples include using voice as a biomarker, building interpretable genomic tools, modeling disease trajectories with diverse multimodal data, and mapping cellular and molecular health indicators across the human body.\n  This report assesses the state of metadata creation and standardization in the Bridge2AI GCs, provides guidelines where required, and identifies gaps and areas for improvement across the program. New projects, including those outside the Bridge2AI consortium, would benefit from what we have learned about creating metadata as part of efforts to promote AI readiness.",
    "categories": [
      "q-bio.OT",
      "cs.AI"
    ],
    "primary_category": "q-bio.OT",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.10432v2",
    "published_date": "2025-09-12 17:38:46 UTC",
    "updated_date": "2025-09-16 20:37:41 UTC"
  },
  {
    "arxiv_id": "2509.10423v1",
    "title": "Mutual Information Tracks Policy Coherence in Reinforcement Learning",
    "authors": [
      "Cameron Reid",
      "Wael Hafez",
      "Amirhossein Nazeri"
    ],
    "abstract": "Reinforcement Learning (RL) agents deployed in real-world environments face degradation from sensor faults, actuator wear, and environmental shifts, yet lack intrinsic mechanisms to detect and diagnose these failures. We present an information-theoretic framework that reveals both the fundamental dynamics of RL and provides practical methods for diagnosing deployment-time anomalies. Through analysis of state-action mutual information patterns in a robotic control task, we first demonstrate that successful learning exhibits characteristic information signatures: mutual information between states and actions steadily increases from 0.84 to 2.83 bits (238% growth) despite growing state entropy, indicating that agents develop increasingly selective attention to task-relevant patterns. Intriguingly, states, actions and next states joint mutual information, MI(S,A;S'), follows an inverted U-curve, peaking during early learning before declining as the agent specializes suggesting a transition from broad exploration to efficient exploitation. More immediately actionable, we show that information metrics can differentially diagnose system failures: observation-space, i.e., states noise (sensor faults) produces broad collapses across all information channels with pronounced drops in state-action coupling, while action-space noise (actuator faults) selectively disrupts action-outcome predictability while preserving state-action relationships. This differential diagnostic capability demonstrated through controlled perturbation experiments enables precise fault localization without architectural modifications or performance degradation. By establishing information patterns as both signatures of learning and diagnostic for system health, we provide the foundation for adaptive RL systems capable of autonomous fault detection and policy adjustment based on information-theoretic principles.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 4 figures, 1 table",
    "pdf_url": "https://arxiv.org/pdf/2509.10423v1",
    "published_date": "2025-09-12 17:24:20 UTC",
    "updated_date": "2025-09-12 17:24:20 UTC"
  },
  {
    "arxiv_id": "2509.10414v2",
    "title": "Is In-Context Learning Learning?",
    "authors": [
      "Adrian de Wynter"
    ],
    "abstract": "In-context learning (ICL) allows some autoregressive models to solve tasks via next-token prediction and without needing further training. This has led to claims about these model's ability to solve (learn) unseen tasks with only a few shots (exemplars) in the prompt. However, deduction does not always imply learning, as ICL does not explicitly encode a given observation. Instead, the models rely on their prior knowledge and the exemplars given, if any. We argue that, mathematically, ICL does constitute learning, but its full characterisation requires empirical work. We then carry out a large-scale analysis of ICL ablating out or accounting for memorisation, pretraining, distributional shifts, and prompting style and phrasing. We find that ICL is an effective learning paradigm, but limited in its ability to learn and generalise to unseen tasks. We note that, in the limit where exemplars become more numerous, accuracy is insensitive to exemplar distribution, model, prompt style, and the input's linguistic features. Instead, it deduces patterns from regularities in the prompt, which leads to distributional sensitivity, especially in prompting styles such as chain-of-thought. Given the varied accuracies on formally similar tasks, we conclude that autoregression's ad-hoc encoding is not a robust mechanism, and suggests limited all-purpose generalisability.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Director's cut",
    "pdf_url": "https://arxiv.org/pdf/2509.10414v2",
    "published_date": "2025-09-12 17:12:04 UTC",
    "updated_date": "2025-09-15 15:29:49 UTC"
  },
  {
    "arxiv_id": "2509.10408v1",
    "title": "Multimodal SAM-adapter for Semantic Segmentation",
    "authors": [
      "Iacopo Curti",
      "Pierluigi Zama Ramirez",
      "Alioscia Petrelli",
      "Luigi Di Stefano"
    ],
    "abstract": "Semantic segmentation, a key task in computer vision with broad applications in autonomous driving, medical imaging, and robotics, has advanced substantially with deep learning. Nevertheless, current approaches remain vulnerable to challenging conditions such as poor lighting, occlusions, and adverse weather. To address these limitations, multimodal methods that integrate auxiliary sensor data (e.g., LiDAR, infrared) have recently emerged, providing complementary information that enhances robustness. In this work, we present MM SAM-adapter, a novel framework that extends the capabilities of the Segment Anything Model (SAM) for multimodal semantic segmentation. The proposed method employs an adapter network that injects fused multimodal features into SAM's rich RGB features. This design enables the model to retain the strong generalization ability of RGB features while selectively incorporating auxiliary modalities only when they contribute additional cues. As a result, MM SAM-adapter achieves a balanced and efficient use of multimodal information. We evaluate our approach on three challenging benchmarks, DeLiVER, FMB, and MUSES, where MM SAM-adapter delivers state-of-the-art performance. To further analyze modality contributions, we partition DeLiVER and FMB into RGB-easy and RGB-hard subsets. Results consistently demonstrate that our framework outperforms competing methods in both favorable and adverse conditions, highlighting the effectiveness of multimodal adaptation for robust scene understanding. The code is available at the following link: https://github.com/iacopo97/Multimodal-SAM-Adapter.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.10408v1",
    "published_date": "2025-09-12 16:58:51 UTC",
    "updated_date": "2025-09-12 16:58:51 UTC"
  },
  {
    "arxiv_id": "2509.10401v2",
    "title": "Abduct, Act, Predict: Scaffolding Causal Inference for Automated Failure Attribution in Multi-Agent Systems",
    "authors": [
      "Alva West",
      "Yixuan Weng",
      "Minjun Zhu",
      "Zhen Lin",
      "Zhiyuan Ning",
      "Yue Zhang"
    ],
    "abstract": "Failure attribution in multi-agent systems -- pinpointing the exact step where a decisive error occurs -- is a critical yet unsolved challenge. Current methods treat this as a pattern recognition task over long conversation logs, leading to critically low step-level accuracy (below 17\\%), which renders them impractical for debugging complex systems. Their core weakness is a fundamental inability to perform robust counterfactual reasoning: to determine if correcting a single action would have actually averted the task failure. To bridge this \\emph{counterfactual inference gap}, we introduce Abduct-Act-Predict (A2P) Scaffolding, a novel agent framework that transforms failure attribution from pattern recognition into a structured causal inference task. A2P explicitly guides a large language model through a formal three-step reasoning process within a single inference pass: (1) Abduction, to infer the hidden root causes behind an agent's actions; (2) Action, to define a minimal corrective intervention; and (3) Prediction, to simulate the subsequent trajectory and verify if the intervention resolves the failure. This structured approach leverages the holistic context of the entire conversation while imposing a rigorous causal logic on the model's analysis. Our extensive experiments on the Who\\&When benchmark demonstrate its efficacy. On the Algorithm-Generated dataset, A2P achieves 47.46\\% step-level accuracy, a 2.85$\\times$ improvement over the 16.67\\% of the baseline. On the more complex Hand-Crafted dataset, it achieves 29.31\\% step accuracy, a 2.43$\\times$ improvement over the baseline's 12.07\\%. By reframing the problem through a causal lens, A2P Scaffolding provides a robust, verifiable, and significantly more accurate solution for automated failure attribution. Ours code are released at https://github.com/ResearAI/A2P.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.10401v2",
    "published_date": "2025-09-12 16:51:15 UTC",
    "updated_date": "2025-09-23 14:45:53 UTC"
  },
  {
    "arxiv_id": "2509.18131v1",
    "title": "Two ways to knowledge?",
    "authors": [
      "Jean-Michel Tucny",
      "Abhisek Ganguly",
      "Santosh Ansumali",
      "Sauro Succi"
    ],
    "abstract": "It is shown that the weight matrices of transformer-based machine learning applications to the solution of two representative physical applications show a random-like character which bears no directly recognizable link to the physical and mathematical structure of the physical problem under study. This suggests that machine learning and the scientific method may represent two distinct and potentially complementary paths to knowledge, even though a strict notion of explainability in terms of direct correspondence between network parameters and physical structures may remain out of reach. It is also observed that drawing a parallel between transformer operation and (generalized) path-integration techniques may account for the random-like nature of the weights, but still does not resolve the tension with explainability. We conclude with some general comments on the hazards of gleaning knowledge without the benefit of Insight.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.18131v1",
    "published_date": "2025-09-12 16:41:23 UTC",
    "updated_date": "2025-09-12 16:41:23 UTC"
  },
  {
    "arxiv_id": "2509.10392v1",
    "title": "Diversified recommendations of cultural activities with personalized determinantal point processes",
    "authors": [
      "Carole Ibrahim",
      "Hiba Bederina",
      "Daniel Cuesta",
      "Laurent Montier",
      "Cyrille Delabre",
      "Jill-Jênn Vie"
    ],
    "abstract": "While optimizing recommendation systems for user engagement is a well-established practice, effectively diversifying recommendations without negatively impacting core business metrics remains a significant industry challenge. In line with our initiative to broaden our audience's cultural practices, this study investigates using personalized Determinantal Point Processes (DPPs) to sample diverse and relevant recommendations. We rely on a well-known quality-diversity decomposition of the similarity kernel to give more weight to user preferences. In this paper, we present our implementations of the personalized DPP sampling, evaluate the trade-offs between relevance and diversity through both offline and online metrics, and give insights for practitioners on their use in a production environment. For the sake of reproducibility, we release the full code for our platform and experiments on GitHub.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "7 pages, accepted at RecSys workshop RecSoGood 2025",
    "pdf_url": "https://arxiv.org/pdf/2509.10392v1",
    "published_date": "2025-09-12 16:34:07 UTC",
    "updated_date": "2025-09-12 16:34:07 UTC"
  },
  {
    "arxiv_id": "2509.10391v1",
    "title": "Improving Audio Event Recognition with Consistency Regularization",
    "authors": [
      "Shanmuka Sadhu",
      "Weiran Wang"
    ],
    "abstract": "Consistency regularization (CR), which enforces agreement between model predictions on augmented views, has found recent benefits in automatic speech recognition [1]. In this paper, we propose the use of consistency regularization for audio event recognition, and demonstrate its effectiveness on AudioSet. With extensive ablation studies for both small ($\\sim$20k) and large ($\\sim$1.8M) supervised training sets, we show that CR brings consistent improvement over supervised baselines which already heavily utilize data augmentation, and CR using stronger augmentation and multiple augmentations leads to additional gain for the small training set. Furthermore, we extend the use of CR into the semi-supervised setup with 20K labeled samples and 1.8M unlabeled samples, and obtain performance improvement over our best model trained on the small set.",
    "categories": [
      "cs.SD",
      "cs.AI"
    ],
    "primary_category": "cs.SD",
    "comment": "Under Review",
    "pdf_url": "https://arxiv.org/pdf/2509.10391v1",
    "published_date": "2025-09-12 16:31:20 UTC",
    "updated_date": "2025-09-12 16:31:20 UTC"
  },
  {
    "arxiv_id": "2509.10369v1",
    "title": "Data distribution impacts the performance and generalisability of contrastive learning-based foundation models of electrocardiograms",
    "authors": [
      "Gul Rukh Khattak",
      "Konstantinos Patlatzoglou",
      "Joseph Barker",
      "Libor Pastika",
      "Boroumand Zeidaabadi",
      "Ahmed El-Medany",
      "Hesham Aggour",
      "Yixiu Liang",
      "Antonio H. Ribeiro",
      "Jeffrey Annis",
      "Antonio Luiz Pinho Ribeiro",
      "Junbo Ge",
      "Daniel B. Kramer",
      "Jonathan W. Waks",
      "Evan Brittain",
      "Nicholas Peters",
      "Fu Siong Ng",
      "Arunashis Sau"
    ],
    "abstract": "Contrastive learning is a widely adopted self-supervised pretraining strategy, yet its dependence on cohort composition remains underexplored. We present Contrasting by Patient Augmented Electrocardiograms (CAPE) foundation model and pretrain on four cohorts (n = 5,203,352), from diverse populations across three continents (North America, South America, Asia). We systematically assess how cohort demographics, health status, and population diversity influence the downstream performance for prediction tasks also including two additional cohorts from another continent (Europe). We find that downstream performance depends on the distributional properties of the pretraining cohort, including demographics and health status. Moreover, while pretraining with a multi-centre, demographically diverse cohort improves in-distribution accuracy, it reduces out-of-distribution (OOD) generalisation of our contrastive approach by encoding cohort-specific artifacts. To address this, we propose the In-Distribution Batch (IDB) strategy, which preserves intra-cohort consistency during pretraining and enhances OOD robustness. This work provides important insights for developing clinically fair and generalisable foundation models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP",
      "q-bio.TO"
    ],
    "primary_category": "cs.LG",
    "comment": "Currently under review at npj Digital Medicine",
    "pdf_url": "https://arxiv.org/pdf/2509.10369v1",
    "published_date": "2025-09-12 16:01:18 UTC",
    "updated_date": "2025-09-12 16:01:18 UTC"
  },
  {
    "arxiv_id": "2509.14257v2",
    "title": "From Correction to Mastery: Reinforced Distillation of Large Language Model Agents",
    "authors": [
      "Yuanjie Lyu",
      "Chengyu Wang",
      "Jun Huang",
      "Tong Xu"
    ],
    "abstract": "Large Language Model agents excel at solving complex tasks through iterative reasoning and tool use, but typically depend on ultra-large, costly backbones. Existing distillation approaches train smaller students to imitate full teacher trajectories, yet reasoning and knowledge gaps between the teacher and student can cause compounding errors. We propose SCoRe, a student-centered framework in which the student generates training trajectories and the teacher corrects only the earliest error, producing training data matched to the student's ability and exposing specific weaknesses. The student is first fine-tuned on corrected trajectories. Subsequently, short-horizon reinforcement learning starts from the verified prefix preceding the earliest error, with target rewards assigned at that step. This design encourages autonomous problem-solving beyond imitation and enhances training stability. On 12 challenging benchmarks, a 7B-parameter student distilled with SCoRe matches the agentic performance of a 72B-parameter teacher.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.14257v2",
    "published_date": "2025-09-12 15:34:07 UTC",
    "updated_date": "2025-10-09 04:22:47 UTC"
  },
  {
    "arxiv_id": "2509.10345v2",
    "title": "Towards Understanding Visual Grounding in Visual Language Models",
    "authors": [
      "Georgios Pantazopoulos",
      "Eda B. Özyiğit"
    ],
    "abstract": "Visual grounding refers to the ability of a model to identify a region within some visual input that matches a textual description. Consequently, a model equipped with visual grounding capabilities can target a wide range of applications in various domains, including referring expression comprehension, answering questions pertinent to fine-grained details in images or videos, caption visual context by explicitly referring to entities, as well as low and high-level control in simulated and real environments. In this survey paper, we review representative works across the key areas of research on modern general-purpose vision language models (VLMs). We first outline the importance of grounding in VLMs, then delineate the core components of the contemporary paradigm for developing grounded models, and examine their practical applications, including benchmarks and evaluation metrics for grounded multimodal generation. We also discuss the multifaceted interrelations among visual grounding, multimodal chain-of-thought, and reasoning in VLMs. Finally, we analyse the challenges inherent to visual grounding and suggest promising directions for future research.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.10345v2",
    "published_date": "2025-09-12 15:33:49 UTC",
    "updated_date": "2025-09-15 08:46:29 UTC"
  },
  {
    "arxiv_id": "2509.10344v1",
    "title": "GLAM: Geometry-Guided Local Alignment for Multi-View VLP in Mammography",
    "authors": [
      "Yuexi Du",
      "Lihui Chen",
      "Nicha C. Dvornek"
    ],
    "abstract": "Mammography screening is an essential tool for early detection of breast cancer. The speed and accuracy of mammography interpretation have the potential to be improved with deep learning methods. However, the development of a foundation visual language model (VLM) is hindered by limited data and domain differences between natural and medical images. Existing mammography VLMs, adapted from natural images, often ignore domain-specific characteristics, such as multi-view relationships in mammography. Unlike radiologists who analyze both views together to process ipsilateral correspondence, current methods treat them as independent images or do not properly model the multi-view correspondence learning, losing critical geometric context and resulting in suboptimal prediction. We propose GLAM: Global and Local Alignment for Multi-view mammography for VLM pretraining using geometry guidance. By leveraging the prior knowledge about the multi-view imaging process of mammograms, our model learns local cross-view alignments and fine-grained local features through joint global and local, visual-visual, and visual-language contrastive learning. Pretrained on EMBED [14], one of the largest open mammography datasets, our model outperforms baselines across multiple datasets under different settings.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by MICCAI 2025",
    "pdf_url": "https://arxiv.org/pdf/2509.10344v1",
    "published_date": "2025-09-12 15:33:18 UTC",
    "updated_date": "2025-09-12 15:33:18 UTC"
  },
  {
    "arxiv_id": "2509.10596v1",
    "title": "GenAI Voice Mode in Programming Education",
    "authors": [
      "Sven Jacobs",
      "Natalie Kiesler"
    ],
    "abstract": "Real-time voice interfaces using multimodal Generative AI (GenAI) can potentially address the accessibility needs of novice programmers with disabilities (e.g., related to vision). Yet, little is known about how novices interact with GenAI tools and their feedback quality in the form of audio output. This paper analyzes audio dialogues from nine 9th-grade students using a voice-enabled tutor (powered by OpenAI's Realtime API) in an authentic classroom setting while learning Python. We examined the students' voice prompts and AI's responses (1210 messages) by using qualitative coding. We also gathered students' perceptions via the Partner Modeling Questionnaire. The GenAI Voice Tutor primarily offered feedback on mistakes and next steps, but its correctness was limited (71.4% correct out of 416 feedback outputs). Quality issues were observed, particularly when the AI attempted to utter programming code elements. Students used the GenAI voice tutor primarily for debugging. They perceived it as competent, only somewhat human-like, and flexible. The present study is the first to explore the interaction dynamics of real-time voice GenAI tutors and novice programmers, informing future educational tool design and potentially addressing accessibility needs of diverse learners.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "Accepted for the 25th International Conference on Computing Education Research (Koli Calling '25)",
    "pdf_url": "https://arxiv.org/pdf/2509.10596v1",
    "published_date": "2025-09-12 15:25:08 UTC",
    "updated_date": "2025-09-12 15:25:08 UTC"
  },
  {
    "arxiv_id": "2509.10334v1",
    "title": "I-Segmenter: Integer-Only Vision Transformer for Efficient Semantic Segmentation",
    "authors": [
      "Jordan Sassoon",
      "Michal Szczepanski",
      "Martyna Poreba"
    ],
    "abstract": "Vision Transformers (ViTs) have recently achieved strong results in semantic segmentation, yet their deployment on resource-constrained devices remains limited due to their high memory footprint and computational cost. Quantization offers an effective strategy to improve efficiency, but ViT-based segmentation models are notoriously fragile under low precision, as quantization errors accumulate across deep encoder-decoder pipelines. We introduce I-Segmenter, the first fully integer-only ViT segmentation framework. Building on the Segmenter architecture, I-Segmenter systematically replaces floating-point operations with integer-only counterparts. To further stabilize both training and inference, we propose $λ$-ShiftGELU, a novel activation function that mitigates the limitations of uniform quantization in handling long-tailed activation distributions. In addition, we remove the L2 normalization layer and replace bilinear interpolation in the decoder with nearest neighbor upsampling, ensuring integer-only execution throughout the computational graph. Extensive experiments show that I-Segmenter achieves accuracy within a reasonable margin of its FP32 baseline (5.1 % on average), while reducing model size by up to 3.8x and enabling up to 1.2x faster inference with optimized runtimes. Notably, even in one-shot PTQ with a single calibration image, I-Segmenter delivers competitive accuracy, underscoring its practicality for real-world deployment.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.10334v1",
    "published_date": "2025-09-12 15:14:19 UTC",
    "updated_date": "2025-09-12 15:14:19 UTC"
  },
  {
    "arxiv_id": "2509.10326v1",
    "title": "State Algebra for Propositional Logic",
    "authors": [
      "Dmitry Lesnik",
      "Tobias Schäfer"
    ],
    "abstract": "This paper presents State Algebra, a novel framework designed to represent and manipulate propositional logic using algebraic methods. The framework is structured as a hierarchy of three representations: Set, Coordinate, and Row Decomposition. These representations anchor the system in well-known semantics while facilitating the computation using a powerful algebraic engine. A key aspect of State Algebra is its flexibility in representation. We show that although the default reduction of a state vector is not canonical, a unique canonical form can be obtained by applying a fixed variable order during the reduction process. This highlights a trade-off: by foregoing guaranteed canonicity, the framework gains increased flexibility, potentially leading to more compact representations of certain classes of problems. We explore how this framework provides tools to articulate both search-based and knowledge compilation algorithms and discuss its natural extension to probabilistic logic and Weighted Model Counting.",
    "categories": [
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "47 pages",
    "pdf_url": "https://arxiv.org/pdf/2509.10326v1",
    "published_date": "2025-09-12 15:05:52 UTC",
    "updated_date": "2025-09-12 15:05:52 UTC"
  },
  {
    "arxiv_id": "2509.10594v2",
    "title": "SME-TEAM: Leveraging Trust and Ethics for Secure and Responsible Use of AI and LLMs in SMEs",
    "authors": [
      "Iqbal H. Sarker",
      "Helge Janicke",
      "Ahmad Mohsin",
      "Leandros Maglaras"
    ],
    "abstract": "Artificial Intelligence (AI) and Large Language Models (LLMs) are revolutionizing today's business practices; however, their adoption within small and medium-sized enterprises (SMEs) raises serious trust, ethical, and technical issues. In this perspective paper, we introduce a structured, multi-phased framework, \"SME-TEAM\" for the secure and responsible use of these technologies in SMEs. Based on a conceptual structure of four key pillars, i.e., Data, Algorithms, Human Oversight, and Model Architecture, SME-TEAM bridges theoretical ethical principles with operational practice, enhancing AI capabilities across a wide range of applications in SMEs. Ultimately, this paper provides a structured roadmap for the adoption of these emerging technologies, positioning trust and ethics as a driving force for resilience, competitiveness, and sustainable innovation within the area of business analytics and SMEs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages",
    "pdf_url": "https://arxiv.org/pdf/2509.10594v2",
    "published_date": "2025-09-12 14:59:52 UTC",
    "updated_date": "2025-11-05 16:07:58 UTC"
  },
  {
    "arxiv_id": "2509.14256v1",
    "title": "JU-NLP at Touché: Covert Advertisement in Conversational AI-Generation and Detection Strategies",
    "authors": [
      "Arka Dutta",
      "Agrik Majumdar",
      "Sombrata Biswas",
      "Dipankar Das",
      "Sivaji Bandyopadhyay"
    ],
    "abstract": "This paper proposes a comprehensive framework for the generation of covert advertisements within Conversational AI systems, along with robust techniques for their detection. It explores how subtle promotional content can be crafted within AI-generated responses and introduces methods to identify and mitigate such covert advertising strategies. For generation (Sub-Task~1), we propose a novel framework that leverages user context and query intent to produce contextually relevant advertisements. We employ advanced prompting strategies and curate paired training data to fine-tune a large language model (LLM) for enhanced stealthiness. For detection (Sub-Task~2), we explore two effective strategies: a fine-tuned CrossEncoder (\\texttt{all-mpnet-base-v2}) for direct classification, and a prompt-based reformulation using a fine-tuned \\texttt{DeBERTa-v3-base} model. Both approaches rely solely on the response text, ensuring practicality for real-world deployment. Experimental results show high effectiveness in both tasks, achieving a precision of 1.0 and recall of 0.71 for ad generation, and F1-scores ranging from 0.99 to 1.00 for ad detection. These results underscore the potential of our methods to balance persuasive communication with transparency in conversational AI.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.14256v1",
    "published_date": "2025-09-12 14:53:56 UTC",
    "updated_date": "2025-09-12 14:53:56 UTC"
  },
  {
    "arxiv_id": "2509.10303v1",
    "title": "Generalizing Beyond Suboptimality: Offline Reinforcement Learning Learns Effective Scheduling through Random Data",
    "authors": [
      "Jesse van Remmerden",
      "Zaharah Bukhsh",
      "Yingqian Zhang"
    ],
    "abstract": "The Job-Shop Scheduling Problem (JSP) and Flexible Job-Shop Scheduling Problem (FJSP), are canonical combinatorial optimization problems with wide-ranging applications in industrial operations. In recent years, many online reinforcement learning (RL) approaches have been proposed to learn constructive heuristics for JSP and FJSP. Although effective, these online RL methods require millions of interactions with simulated environments that may not capture real-world complexities, and their random policy initialization leads to poor sample efficiency. To address these limitations, we introduce Conservative Discrete Quantile Actor-Critic (CDQAC), a novel offline RL algorithm that learns effective scheduling policies directly from historical data, eliminating the need for costly online interactions, while maintaining the ability to improve upon suboptimal training data. CDQAC couples a quantile-based critic with a delayed policy update, estimating the return distribution of each machine-operation pair rather than selecting pairs outright. Our extensive experiments demonstrate CDQAC's remarkable ability to learn from diverse data sources. CDQAC consistently outperforms the original data-generating heuristics and surpasses state-of-the-art offline and online RL baselines. In addition, CDQAC is highly sample efficient, requiring only 10-20 training instances to learn high-quality policies. Surprisingly, we find that CDQAC performs better when trained on data generated by a random heuristic than when trained on higher-quality data from genetic algorithms and priority dispatching rules.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.10303v1",
    "published_date": "2025-09-12 14:45:39 UTC",
    "updated_date": "2025-09-12 14:45:39 UTC"
  },
  {
    "arxiv_id": "2509.10297v1",
    "title": "The Morality of Probability: How Implicit Moral Biases in LLMs May Shape the Future of Human-AI Symbiosis",
    "authors": [
      "Eoin O'Doherty",
      "Nicole Weinrauch",
      "Andrew Talone",
      "Uri Klempner",
      "Xiaoyuan Yi",
      "Xing Xie",
      "Yi Zeng"
    ],
    "abstract": "Artificial intelligence (AI) is advancing at a pace that raises urgent questions about how to align machine decision-making with human moral values. This working paper investigates how leading AI systems prioritize moral outcomes and what this reveals about the prospects for human-AI symbiosis. We address two central questions: (1) What moral values do state-of-the-art large language models (LLMs) implicitly favour when confronted with dilemmas? (2) How do differences in model architecture, cultural origin, and explainability affect these moral preferences? To explore these questions, we conduct a quantitative experiment with six LLMs, ranking and scoring outcomes across 18 dilemmas representing five moral frameworks. Our findings uncover strikingly consistent value biases. Across all models, Care and Virtue values outcomes were rated most moral, while libertarian choices were consistently penalized. Reasoning-enabled models exhibited greater sensitivity to context and provided richer explanations, whereas non-reasoning models produced more uniform but opaque judgments. This research makes three contributions: (i) Empirically, it delivers a large-scale comparison of moral reasoning across culturally distinct LLMs; (ii) Theoretically, it links probabilistic model behaviour with underlying value encodings; (iii) Practically, it highlights the need for explainability and cultural awareness as critical design principles to guide AI toward a transparent, aligned, and symbiotic future.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Work in progress",
    "pdf_url": "https://arxiv.org/pdf/2509.10297v1",
    "published_date": "2025-09-12 14:37:57 UTC",
    "updated_date": "2025-09-12 14:37:57 UTC"
  },
  {
    "arxiv_id": "2509.10289v1",
    "title": "We Need a New Ethics for a World of AI Agents",
    "authors": [
      "Iason Gabriel",
      "Geoff Keeling",
      "Arianna Manzini",
      "James Evans"
    ],
    "abstract": "The deployment of capable AI agents raises fresh questions about safety, human-machine relationships and social coordination. We argue for greater engagement by scientists, scholars, engineers and policymakers with the implications of a world increasingly populated by AI agents. We explore key challenges that must be addressed to ensure that interactions between humans and agents, and among agents themselves, remain broadly beneficial.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "6 pages, no figures",
    "pdf_url": "https://arxiv.org/pdf/2509.10289v1",
    "published_date": "2025-09-12 14:29:14 UTC",
    "updated_date": "2025-09-12 14:29:14 UTC"
  },
  {
    "arxiv_id": "2509.12255v1",
    "title": "Representation Learning on Large Non-Bipartite Transaction Networks using GraphSAGE",
    "authors": [
      "Mihir Tare",
      "Clemens Rattasits",
      "Yiming Wu",
      "Euan Wielewski"
    ],
    "abstract": "Financial institutions increasingly require scalable tools to analyse complex transactional networks, yet traditional graph embedding methods struggle with dynamic, real-world banking data. This paper demonstrates the practical application of GraphSAGE, an inductive Graph Neural Network framework, to non-bipartite heterogeneous transaction networks within a banking context. Unlike transductive approaches, GraphSAGE scales well to large networks and can generalise to unseen nodes which is critical for institutions working with temporally evolving transactional data. We construct a transaction network using anonymised customer and merchant transactions and train a GraphSAGE model to generate node embeddings. Our exploratory work on the embeddings reveals interpretable clusters aligned with geographic and demographic attributes. Additionally, we illustrate their utility in downstream classification tasks by applying them to a money mule detection model where using these embeddings improves the prioritisation of high-risk accounts. Beyond fraud detection, our work highlights the adaptability of this framework to banking-scale networks, emphasising its inductive capability, scalability, and interpretability. This study provides a blueprint for financial organisations to harness graph machine learning for actionable insights in transactional ecosystems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.12255v1",
    "published_date": "2025-09-12 14:09:16 UTC",
    "updated_date": "2025-09-12 14:09:16 UTC"
  },
  {
    "arxiv_id": "2509.10266v2",
    "title": "SignMouth: Leveraging Mouthing Cues for Sign Language Translation by Multimodal Contrastive Fusion",
    "authors": [
      "Wenfang Wu",
      "Tingting Yuan",
      "Yupeng Li",
      "Daling Wang",
      "Xiaoming Fu"
    ],
    "abstract": "Sign language translation (SLT) aims to translate natural language from sign language videos, serving as a vital bridge for inclusive communication. While recent advances leverage powerful visual backbones and large language models, most approaches mainly focus on manual signals (hand gestures) and tend to overlook non-manual cues like mouthing. In fact, mouthing conveys essential linguistic information in sign languages and plays a crucial role in disambiguating visually similar signs. In this paper, we propose SignClip, a novel framework to improve the accuracy of sign language translation. It fuses manual and non-manual cues, specifically spatial gesture and lip movement features. Besides, SignClip introduces a hierarchical contrastive learning framework with multi-level alignment objectives, ensuring semantic consistency across sign-lip and visual-text modalities. Extensive experiments on two benchmark datasets, PHOENIX14T and How2Sign, demonstrate the superiority of our approach. For example, on PHOENIX14T, in the Gloss-free setting, SignClip surpasses the previous state-of-the-art model SpaMo, improving BLEU-4 from 24.32 to 24.71, and ROUGE from 46.57 to 48.38.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.10266v2",
    "published_date": "2025-09-12 14:08:06 UTC",
    "updated_date": "2025-10-28 22:32:18 UTC"
  },
  {
    "arxiv_id": "2509.10249v1",
    "title": "Investigating Language Model Capabilities to Represent and Process Formal Knowledge: A Preliminary Study to Assist Ontology Engineering",
    "authors": [
      "Hanna Abi Akl"
    ],
    "abstract": "Recent advances in Language Models (LMs) have failed to mask their shortcomings particularly in the domain of reasoning. This limitation impacts several tasks, most notably those involving ontology engineering. As part of a PhD research, we investigate the consequences of incorporating formal methods on the performance of Small Language Models (SLMs) on reasoning tasks. Specifically, we aim to orient our work toward using SLMs to bootstrap ontology construction and set up a series of preliminary experiments to determine the impact of expressing logical problems with different grammars on the performance of SLMs on a predefined reasoning task. Our findings show that it is possible to substitute Natural Language (NL) with a more compact logical language while maintaining a strong performance on reasoning tasks and hope to use these results to further refine the role of SLMs in ontology engineering.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "accepted for the International Joint Conference on Rules and Reasoning (RuleML+RR) 2025",
    "pdf_url": "https://arxiv.org/pdf/2509.10249v1",
    "published_date": "2025-09-12 13:46:43 UTC",
    "updated_date": "2025-09-12 13:46:43 UTC"
  },
  {
    "arxiv_id": "2509.12254v1",
    "title": "DISPLIB: a library of train dispatching problems",
    "authors": [
      "Oddvar Kloster",
      "Bjørnar Luteberget",
      "Carlo Mannino",
      "Giorgio Sartor"
    ],
    "abstract": "Optimization-based decision support systems have a significant potential to reduce delays, and thus improve efficiency on the railways, by automatically re-routing and re-scheduling trains after delays have occurred. The operations research community has dedicated a lot of effort to developing optimization algorithms for this problem, but each study is typically tightly connected with a specific industrial use case. Code and data are seldom shared publicly. This fact hinders reproducibility, and has led to a proliferation of papers describing algorithms for more or less compatible problem definitions, without any real opportunity for readers to assess their relative performance. Inspired by the successful communities around MILP, SAT, TSP, VRP, etc., we introduce a common problem definition and file format, DISPLIB, which captures all the main features of train re-routing and re-scheduling. We have gathered problem instances from multiple real-world use cases and made them openly available. In this paper, we describe the problem definition, the industrial instances, and a reference solver implementation. This allows any researcher or developer to work on the train dispatching problem without an industrial connection, and enables the research community to perform empirical comparisons between solvers. All materials are available online at https://displib.github.io.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.12254v1",
    "published_date": "2025-09-12 13:35:28 UTC",
    "updated_date": "2025-09-12 13:35:28 UTC"
  },
  {
    "arxiv_id": "2509.10222v2",
    "title": "Compartmentalised Agentic Reasoning for Clinical NLI",
    "authors": [
      "Maël Jullien",
      "Lei Xu",
      "Marco Valentino",
      "André Freitas"
    ],
    "abstract": "Large language models can produce fluent judgments for clinical natural language inference, yet they frequently fail when the decision requires the correct inferential schema rather than surface matching. We introduce CARENLI, a compartmentalised agentic framework that routes each premise-statement pair to a reasoning family and then applies a specialised solver with explicit verification and targeted refinement. We evaluate on an expanded CTNLI benchmark of 200 instances spanning four reasoning families: Causal Attribution, Compositional Grounding, Epistemic Verification, and Risk State Abstraction. Across four contemporary backbone models, CARENLI improves mean accuracy from about 23% with direct prompting to about 57%, a gain of roughly 34 points, with the largest benefits on structurally demanding reasoning types. These results support compartmentalisation plus verification as a practical route to more reliable and auditable clinical inference.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.10222v2",
    "published_date": "2025-09-12 13:14:47 UTC",
    "updated_date": "2026-01-15 11:51:58 UTC"
  },
  {
    "arxiv_id": "2509.10220v1",
    "title": "Openness in AI and downstream governance: A global value chain approach",
    "authors": [
      "Christopher Foster"
    ],
    "abstract": "The rise of AI has been rapid, becoming a leading sector for investment and promising disruptive impacts across the economy. Within the critical analysis of the economic impacts, AI has been aligned to the critical literature on data power and platform capitalism - further concentrating power and value capture amongst a small number of \"big tech\" leaders.\n  The equally rapid rise of openness in AI (here taken to be claims made by AI firms about openness, \"open source\" and free provision) signals an interesting development. It highlights an emerging ecosystem of open AI models, datasets and toolchains, involving massive capital investment. It poses questions as to whether open resources can support technological transfer and the ability for catch-up, even in the face of AI industry power.\n  This work seeks to add conceptual clarity to these debates by conceptualising openness in AI as a unique type of interfirm relation and therefore amenable to value chain analysis. This approach then allows consideration of the capitalist dynamics of \"outsourcing\" of foundational firms in value chains, and consequently the types of governance and control that might emerge downstream as AI is adopted. This work, therefore, extends previous mapping of AI value chains to build a framework which links foundational AI with downstream value chains.\n  Overall, this work extends our understanding of AI as a productive sector. While the work remains critical of the power of leading AI firms, openness in AI may lead to potential spillovers stemming from the intense competition for global technological leadership in AI.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.10220v1",
    "published_date": "2025-09-12 13:12:09 UTC",
    "updated_date": "2025-09-12 13:12:09 UTC"
  },
  {
    "arxiv_id": "2509.10591v2",
    "title": "Assisting the Grading of a Handwritten General Chemistry Exam with Artificial Intelligence",
    "authors": [
      "Jan Cvengros",
      "Gerd Kortemeyer"
    ],
    "abstract": "We explore the effectiveness and reliability of an artificial intelligence (AI)-based grading system for a handwritten general chemistry exam, comparing AI-assigned scores to human grading across various types of questions. Exam pages and grading rubrics were uploaded as images to account for chemical reaction equations, short and long open-ended answers, numerical and symbolic answer derivations, drawing, and sketching in pencil-and-paper format. Using linear regression analyses and psychometric evaluations, the investigation reveals high agreement between AI and human graders for textual and chemical reaction questions, while highlighting lower reliability for numerical and graphical tasks. The findings emphasize the necessity for human oversight to ensure grading accuracy, based on selective filtering. The results indicate promising applications for AI in routine assessment tasks, though careful consideration must be given to student perceptions of fairness and trust in integrating AI-based grading into educational practice.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.10591v2",
    "published_date": "2025-09-12 13:08:27 UTC",
    "updated_date": "2025-11-10 12:37:31 UTC"
  },
  {
    "arxiv_id": "2509.10210v1",
    "title": "Towards Fully Automated Molecular Simulations: Multi-Agent Framework for Simulation Setup and Force Field Extraction",
    "authors": [
      "Marko Petković",
      "Vlado Menkovski",
      "Sofía Calero"
    ],
    "abstract": "Automated characterization of porous materials has the potential to accelerate materials discovery, but it remains limited by the complexity of simulation setup and force field selection. We propose a multi-agent framework in which LLM-based agents can autonomously understand a characterization task, plan appropriate simulations, assemble relevant force fields, execute them and interpret their results to guide subsequent steps. As a first step toward this vision, we present a multi-agent system for literature-informed force field extraction and automated RASPA simulation setup. Initial evaluations demonstrate high correctness and reproducibility, highlighting this approach's potential to enable fully autonomous, scalable materials characterization.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.10210v1",
    "published_date": "2025-09-12 12:56:47 UTC",
    "updated_date": "2025-09-12 12:56:47 UTC"
  },
  {
    "arxiv_id": "2509.10208v1",
    "title": "SI-FACT: Mitigating Knowledge Conflict via Self-Improving Faithfulness-Aware Contrastive Tuning",
    "authors": [
      "Shengqiang Fu"
    ],
    "abstract": "Large Language Models often generate unfaithful responses in knowledge intensive tasks due to knowledge conflict,that is,a preference for relying on internal parametric knowledge rather than the provided context.To address this issue,we propose a novel self improving framework,Self Improving Faithfulness Aware Contrastive Tuning.The framework uses a self instruct mechanism that allows the base LLM to automatically generate high quality,structured contrastive learning data,including anchor samples,semantically equivalent positive samples,and negative samples simulating unfaithful scenarios.This approach significantly reduces the cost of manual annotation.Subsequently,contrastive learning is applied to train the model,enabling it to pull faithful responses closer and push unfaithful responses farther apart in the representation space.Experiments on knowledge conflict evaluation benchmarks ECARE KRE and COSE KRE show that the SI FACT model based on Llama3 8B Instruct improves the Contextual Recall Rate by 6.2% over the best baseline method,while significantly reducing dependence on internal memory.The results indicate that SI FACT provides strong effectiveness and high data efficiency in enhancing the contextual faithfulness of LLMs,offering a practical pathway toward building more proactive and trustworthy language models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.10208v1",
    "published_date": "2025-09-12 12:56:14 UTC",
    "updated_date": "2025-09-12 12:56:14 UTC"
  },
  {
    "arxiv_id": "2509.18130v1",
    "title": "Research on Metro Transportation Flow Prediction Based on the STL-GRU Combined Model",
    "authors": [
      "Zijie Zhou",
      "Huichen Ma"
    ],
    "abstract": "In the metro intelligent transportation system, accurate transfer passenger flow prediction is a key link in optimizing operation plans and improving transportation efficiency. To further improve the theory of metro internal transfer passenger flow prediction and provide more reliable support for intelligent operation decisions, this paper innovatively proposes a metro transfer passenger flow prediction model that integrates the Seasonal and Trend decomposition using Loess (STL) method and Gated Recurrent Unit (GRU).In practical application, the model first relies on the deep learning library Keras to complete the construction and training of the GRU model, laying the foundation for subsequent prediction; then preprocesses the original metro card swiping data, uses the graph-based depth-first search algorithm to identify passengers' travel paths, and further constructs the transfer passenger flow time series; subsequently adopts the STL time series decomposition algorithm to decompose the constructed transfer passenger flow time series into trend component, periodic component and residual component, and uses the 3σ principle to eliminate and fill the outliers in the residual component, and finally completes the transfer passenger flow prediction.Taking the transfer passenger flow data of a certain metro station as the research sample, the validity of the model is verified. The results show that compared with Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), and the combined model of STL time series decomposition method and Long Short-Term Memory (STL-LSTM), the STL-GRU combined prediction model significantly improves the prediction accuracy of transfer passenger flow on weekdays (excluding Fridays), Fridays and rest days, with the mean absolute percentage error (MAPE) of the prediction results reduced by at least 2.3, 1.36 and 6.42 percentage points respectively.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.18130v1",
    "published_date": "2025-09-12 12:23:26 UTC",
    "updated_date": "2025-09-12 12:23:26 UTC"
  },
  {
    "arxiv_id": "2509.12253v1",
    "title": "Physics-Informed Neural Networks vs. Physics Models for Non-Invasive Glucose Monitoring: A Comparative Study Under Realistic Synthetic Conditions",
    "authors": [
      "Riyaadh Gani"
    ],
    "abstract": "Non-invasive glucose monitors often fail outside the lab because existing datasets ignore hardware noise, environmental drift, and person-to-person physiology. We introduce the first ultra-realistic near-infrared (NIR) simulator that injects 12-bit ADC quantisation, +/-0.1% LED ageing, photodiode dark noise, 15-45 C temperature, 30-90% relative humidity, contact-pressure variation, Fitzpatrick I-VI melanin, and diurnal glucose excursions (dawn phenomenon). Using this platform (rho glucose-NIR = 0.21), we benchmark six methods: Enhanced Beer-Lambert (physics-engineered ridge regression), three physics-informed neural networks (PINNs), a selective radiative-transfer PINN, and a shallow DNN. Beer-Lambert achieves 13.6 mg/dL RMSE, 95.8% Clarke-A and 93.8% +/-15% accuracy with only 56 parameters and 0.01 ms inference, outperforming the best PINN (14.6 mg/dL) and the SDNN baseline (35.1 mg/dL). Results overturn the assumption that deeper PINNs dominate and supply an open, end-to-end reference stack for rapid prototyping of embedded optical glucose sensors.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.12253v1",
    "published_date": "2025-09-12 12:18:00 UTC",
    "updated_date": "2025-09-12 12:18:00 UTC"
  },
  {
    "arxiv_id": "2509.10590v2",
    "title": "Machine Unlearning for Responsible and Adaptive AI in Education",
    "authors": [
      "Betty Mayeku",
      "Sandra Hummel",
      "Parisa Memarmoshrefi"
    ],
    "abstract": "Machine Unlearning (MU) has emerged as a promising approach to addressing persistent challenges in Machine Learning (ML) systems. By enabling the selective removal of learned data, MU introduces protective, corrective, and adaptive capabilities that are central to advancing Responsible and Adaptive AI. However, despite its growing prominence in other domains, MU remains underexplored within education, a sector uniquely characterized by sensitive learner data, dynamic environments, and the high-stakes implications of algorithmic decision-making. This paper examines the potential of MU as both a mechanism for operationalizing Responsible AI principles and a foundation for Adaptive AI in ML-driven educational systems. Drawing on a structured review of 42 peer-reviewed studies, the paper analyzes key MU mechanisms and technical variants, and how they contribute to the practical realization of Responsible and Adaptive AI. Four core intervention domains where MU demonstrates significant promise are identified: privacy protection, resilience to adversarial or corrupted data, fairness through bias mitigation, and adaptability to evolving contexts. Furthermore, MU interventions are mapped to the technical, ethical, and pedagogical challenges inherent in educational AI. This mapping illustrates the role of MU as a strategic mechanism for enhancing compliance, reinforcing ethical safeguards, and supporting adaptability by ensuring that models remain flexible, maintainable, and contextually relevant over time. As a conceptual contribution, the paper introduces MU4RAAI, a reference architecture integrating MU within Responsible and Adaptive AI frameworks for educational contexts. MU is thus positioned not merely as a data deletion process but as a transformative approach for ensuring that educational AI systems remain ethical, adaptive, and trustworthy.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "Accepted paper - ESORICS 2025 - International Workshop on Secure and Trustworthy Machine Unlearning Systems (STMUS)",
    "pdf_url": "https://arxiv.org/pdf/2509.10590v2",
    "published_date": "2025-09-12 12:13:40 UTC",
    "updated_date": "2025-11-12 18:17:22 UTC"
  },
  {
    "arxiv_id": "2509.10179v2",
    "title": "Benchmark of stylistic variation in LLM-generated texts",
    "authors": [
      "Jiří Milička",
      "Anna Marklová",
      "Václav Cvrček"
    ],
    "abstract": "This study investigates the register variation in texts written by humans and comparable texts produced by large language models (LLMs). Biber's multidimensional analysis (MDA) is applied to a sample of human-written texts and AI-created texts generated to be their counterparts to find the dimensions of variation in which LLMs differ most significantly and most systematically from humans. As textual material, a new LLM-generated corpus AI-Brown is used, which is comparable to BE-21 (a Brown family corpus representing contemporary British English). Since all languages except English are underrepresented in the training data of frontier LLMs, similar analysis is replicated on Czech using AI-Koditex corpus and Czech multidimensional model. Examined were 16 frontier models in various settings and prompts, with emphasis placed on the difference between base models and instruction-tuned models. Based on this, a benchmark is created through which models can be compared with each other and ranked in interpretable dimensions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Data and scripts: https://osf.io/hs7xt/. Interactive charts: https://www.korpus.cz/stylisticbenchmark/",
    "pdf_url": "https://arxiv.org/pdf/2509.10179v2",
    "published_date": "2025-09-12 12:12:20 UTC",
    "updated_date": "2025-09-18 23:31:43 UTC"
  },
  {
    "arxiv_id": "2509.10162v2",
    "title": "Online Robust Planning under Model Uncertainty: A Sample-Based Approach",
    "authors": [
      "Tamir Shazman",
      "Idan Lev-Yehudi",
      "Ron Benchetit",
      "Vadim Indelman"
    ],
    "abstract": "Online planning in Markov Decision Processes (MDPs) enables agents to make sequential decisions by simulating future trajectories from the current state, making it well-suited for large-scale or dynamic environments. Sample-based methods such as Sparse Sampling and Monte Carlo Tree Search (MCTS) are widely adopted for their ability to approximate optimal actions using a generative model. However, in practical settings, the generative model is often learned from limited data, introducing approximation errors that can degrade performance or lead to unsafe behaviors. To address these challenges, Robust MDPs (RMDPs) offer a principled framework for planning under model uncertainty, yet existing approaches are typically computationally intensive and not suited for real-time use. In this work, we introduce Robust Sparse Sampling (RSS), the first online planning algorithm for RMDPs with finite-sample theoretical performance guarantees. Unlike Sparse Sampling, which estimates the nominal value function, RSS computes a robust value function by leveraging the efficiency and theoretical properties of Sample Average Approximation (SAA), enabling tractable robust policy computation in online settings. RSS is applicable to infinite or continuous state spaces, and its sample and computational complexities are independent of the state space size. We provide theoretical performance guarantees and empirically show that RSS outperforms standard Sparse Sampling in environments with uncertain dynamics.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.10162v2",
    "published_date": "2025-09-12 11:41:23 UTC",
    "updated_date": "2025-09-19 11:43:08 UTC"
  },
  {
    "arxiv_id": "2509.10151v1",
    "title": "BenchECG and xECG: a benchmark and baseline for ECG foundation models",
    "authors": [
      "Riccardo Lunelli",
      "Angus Nicolson",
      "Samuel Martin Pröll",
      "Sebastian Johannes Reinstadler",
      "Axel Bauer",
      "Clemens Dlaska"
    ],
    "abstract": "Electrocardiograms (ECGs) are inexpensive, widely used, and well-suited to deep learning. Recently, interest has grown in developing foundation models for ECGs - models that generalise across diverse downstream tasks. However, consistent evaluation has been lacking: prior work often uses narrow task selections and inconsistent datasets, hindering fair comparison. Here, we introduce BenchECG, a standardised benchmark comprising a comprehensive suite of publicly available ECG datasets and versatile tasks. We also propose xECG, an xLSTM-based recurrent model trained with SimDINOv2 self-supervised learning, which achieves the best BenchECG score compared to publicly available state-of-the-art models. In particular, xECG is the only publicly available model to perform strongly on all datasets and tasks. By standardising evaluation, BenchECG enables rigorous comparison and aims to accelerate progress in ECG representation learning. xECG achieves superior performance over earlier approaches, defining a new baseline for future ECG foundation models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "32 pages, 4 figures, 22 tables",
    "pdf_url": "https://arxiv.org/pdf/2509.10151v1",
    "published_date": "2025-09-12 11:27:17 UTC",
    "updated_date": "2025-09-12 11:27:17 UTC"
  },
  {
    "arxiv_id": "2509.10147v1",
    "title": "Virtual Agent Economies",
    "authors": [
      "Nenad Tomasev",
      "Matija Franklin",
      "Joel Z. Leibo",
      "Julian Jacobs",
      "William A. Cunningham",
      "Iason Gabriel",
      "Simon Osindero"
    ],
    "abstract": "The rapid adoption of autonomous AI agents is giving rise to a new economic layer where agents transact and coordinate at scales and speeds beyond direct human oversight. We propose the \"sandbox economy\" as a framework for analyzing this emergent system, characterizing it along two key dimensions: its origins (emergent vs. intentional) and its degree of separateness from the established human economy (permeable vs. impermeable). Our current trajectory points toward a spontaneous emergence of a vast and highly permeable AI agent economy, presenting us with opportunities for an unprecedented degree of coordination as well as significant challenges, including systemic economic risk and exacerbated inequality. Here we discuss a number of possible design choices that may lead to safely steerable AI agent markets. In particular, we consider auction mechanisms for fair resource allocation and preference resolution, the design of AI \"mission economies\" to coordinate around achieving collective goals, and socio-technical infrastructure needed to ensure trust, safety, and accountability. By doing this, we argue for the proactive design of steerable agent markets to ensure the coming technological shift aligns with humanity's long-term collective flourishing.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.10147v1",
    "published_date": "2025-09-12 11:20:11 UTC",
    "updated_date": "2025-09-12 11:20:11 UTC"
  },
  {
    "arxiv_id": "2509.10128v2",
    "title": "Efficient Learning-Based Control of a Legged Robot in Lunar Gravity",
    "authors": [
      "Philip Arm",
      "Oliver Fischer",
      "Joseph Church",
      "Adrian Fuhrer",
      "Hendrik Kolvenbach",
      "Marco Hutter"
    ],
    "abstract": "Legged robots are promising candidates for exploring challenging areas on low-gravity bodies such as the Moon, Mars, or asteroids, thanks to their advanced mobility on unstructured terrain. However, as planetary robots' power and thermal budgets are highly restricted, these robots need energy-efficient control approaches that easily transfer to multiple gravity environments. In this work, we introduce a reinforcement learning-based control approach for legged robots with gravity-scaled power-optimized reward functions. We use our approach to develop and validate a locomotion controller and a base pose controller in gravity environments from lunar gravity (1.62 m/s2) to a hypothetical super-Earth (19.62 m/s2). Our approach successfully scales across these gravity levels for locomotion and base pose control with the gravity-scaled reward functions. The power-optimized locomotion controller reached a power consumption for locomotion of 23.4 W in Earth gravity on a 15.65 kg robot at 0.4 m/s, a 23 % improvement over the baseline policy. Additionally, we designed a constant-force spring offload system that allowed us to conduct real-world experiments on legged locomotion in lunar gravity. In lunar gravity, the power-optimized control policy reached 12.2 W, 36 % less than a baseline controller which is not optimized for power efficiency. Our method provides a scalable approach to developing power-efficient locomotion controllers for legged robots across multiple gravity levels.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.10128v2",
    "published_date": "2025-09-12 10:43:58 UTC",
    "updated_date": "2025-11-14 12:02:11 UTC"
  },
  {
    "arxiv_id": "2509.10127v2",
    "title": "Population-Aligned Persona Generation for LLM-based Social Simulation",
    "authors": [
      "Zhengyu Hu",
      "Jianxun Lian",
      "Zheyuan Xiao",
      "Max Xiong",
      "Yuxuan Lei",
      "Tianfu Wang",
      "Kaize Ding",
      "Ziang Xiao",
      "Nicholas Jing Yuan",
      "Xing Xie"
    ],
    "abstract": "Recent advances in large language models (LLMs) have enabled human-like social simulations at unprecedented scale and fidelity, offering new opportunities for computational social science. A key challenge, however, is the construction of persona sets that authentically represent the diversity and distribution of real-world populations. Most existing LLM-based social simulation studies focus primarily on designing agentic frameworks and simulation environments, often overlooking the complexities of persona generation and the potential biases introduced by unrepresentative persona sets. In this paper, we propose a systematic framework for synthesizing high-quality, population-aligned persona sets for LLM-driven social simulation. Our approach begins by leveraging LLMs to generate narrative personas from long-term social media data, followed by rigorous quality assessment to filter out low-fidelity profiles. We then apply importance sampling to achieve global alignment with reference psychometric distributions, such as the Big Five personality traits. To address the needs of specific simulation contexts, we further introduce a task-specific module that adapts the globally aligned persona set to targeted subpopulations. Extensive experiments demonstrate that our method significantly reduces population-level bias and enables accurate, flexible social simulation for a wide range of research and policy applications.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.10127v2",
    "published_date": "2025-09-12 10:43:47 UTC",
    "updated_date": "2025-10-04 09:04:38 UTC"
  },
  {
    "arxiv_id": "2509.10122v2",
    "title": "Realism Control One-step Diffusion for Real-World Image Super-Resolution",
    "authors": [
      "Zongliang Wu",
      "Siming Zheng",
      "Peng-Tao Jiang",
      "Xin Yuan"
    ],
    "abstract": "Pre-trained diffusion models have shown great potential in real-world image super-resolution (Real-ISR) tasks by enabling high-resolution reconstructions. While one-step diffusion (OSD) methods significantly improve efficiency compared to traditional multi-step approaches, they still have limitations in balancing fidelity and realism across diverse scenarios. Since the OSDs for SR are usually trained or distilled by a single timestep, they lack flexible control mechanisms to adaptively prioritize these competing objectives, which are inherently manageable in multi-step methods through adjusting sampling steps. To address this challenge, we propose a Realism Controlled One-step Diffusion (RCOD) framework for Real-ISR. RCOD provides a latent domain grouping strategy that enables explicit control over fidelity-realism trade-offs during the noise prediction phase with minimal training paradigm modifications and original training data. A degradation-aware sampling strategy is also introduced to align distillation regularization with the grouping strategy and enhance the controlling of trade-offs. Moreover, a visual prompt injection module is used to replace conventional text prompts with degradation-aware visual tokens, enhancing both restoration accuracy and semantic consistency. Our method achieves superior fidelity and perceptual quality while maintaining computational efficiency. Extensive experiments demonstrate that RCOD outperforms state-of-the-art OSD methods in both quantitative metrics and visual qualities, with flexible realism control capabilities in the inference stage.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Supplementary materials is included. The paper is accepted by AAAI 2026 (Oral). Code and models: https://zongliang-wu.github.io/RCOD-SR/",
    "pdf_url": "https://arxiv.org/pdf/2509.10122v2",
    "published_date": "2025-09-12 10:32:04 UTC",
    "updated_date": "2025-11-15 16:53:40 UTC"
  },
  {
    "arxiv_id": "2509.16226v1",
    "title": "On LLM-Based Scientific Inductive Reasoning Beyond Equations",
    "authors": [
      "Brian S. Lin",
      "Jiaxin Yuan",
      "Zihan Zhou",
      "Shouli Wang",
      "Shuo Wang",
      "Cunliang Kong",
      "Qi Shi",
      "Yuxuan Li",
      "Liner Yang",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "abstract": "As large language models (LLMs) increasingly exhibit human-like capabilities, a fundamental question emerges: How can we enable LLMs to learn the underlying patterns from limited examples in entirely novel environments and apply them effectively? This question is central to the ability of LLMs in inductive reasoning. Existing research on LLM-based inductive reasoning can be broadly categorized based on whether the underlying rules are expressible via explicit mathematical equations. However, many recent studies in the beyond-equations category have emphasized rule design without grounding them in specific scenarios. Inspired by the parallels between inductive reasoning and human scientific discovery, we propose the task of LLM-Based Scientific Inductive Reasoning Beyond Equations and introduce a new benchmark, SIRBench-V1, to evaluate the inductive reasoning abilities of LLMs in scientific settings. Our experimental results show that current LLMs still struggle with this task, underscoring its difficulty and the need for further advancement in this area.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "24 pages",
    "pdf_url": "https://arxiv.org/pdf/2509.16226v1",
    "published_date": "2025-09-12 10:11:52 UTC",
    "updated_date": "2025-09-12 10:11:52 UTC"
  },
  {
    "arxiv_id": "2509.10104v1",
    "title": "AI Harmonics: a human-centric and harms severity-adaptive AI risk assessment framework",
    "authors": [
      "Sofia Vei",
      "Paolo Giudici",
      "Pavlos Sermpezis",
      "Athena Vakali",
      "Adelaide Emma Bernardelli"
    ],
    "abstract": "The absolute dominance of Artificial Intelligence (AI) introduces unprecedented societal harms and risks. Existing AI risk assessment models focus on internal compliance, often neglecting diverse stakeholder perspectives and real-world consequences. We propose a paradigm shift to a human-centric, harm-severity adaptive approach grounded in empirical incident data. We present AI Harmonics, which includes a novel AI harm assessment metric (AIH) that leverages ordinal severity data to capture relative impact without requiring precise numerical estimates. AI Harmonics combines a robust, generalized methodology with a data-driven, stakeholder-aware framework for exploring and prioritizing AI harms. Experiments on annotated incident data confirm that political and physical harms exhibit the highest concentration and thus warrant urgent mitigation: political harms erode public trust, while physical harms pose serious, even life-threatening risks, underscoring the real-world relevance of our approach. Finally, we demonstrate that AI Harmonics consistently identifies uneven harm distributions, enabling policymakers and organizations to target their mitigation efforts effectively.",
    "categories": [
      "cs.AI",
      "stat.ME"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.10104v1",
    "published_date": "2025-09-12 09:52:45 UTC",
    "updated_date": "2025-09-12 09:52:45 UTC"
  },
  {
    "arxiv_id": "2509.10099v1",
    "title": "Generating Energy-Efficient Code via Large-Language Models -- Where are we now?",
    "authors": [
      "Radu Apsan",
      "Vincenzo Stoico",
      "Michel Albonico",
      "Rudra Dhar",
      "Karthik Vaidhyanathan",
      "Ivano Malavolta"
    ],
    "abstract": "Context. The rise of Large Language Models (LLMs) has led to their widespread adoption in development pipelines. Goal. We empirically assess the energy efficiency of Python code generated by LLMs against human-written code and code developed by a Green software expert. Method. We test 363 solutions to 9 coding problems from the EvoEval benchmark using 6 widespread LLMs with 4 prompting techniques, and comparing them to human-developed solutions. Energy consumption is measured on three different hardware platforms: a server, a PC, and a Raspberry Pi for a total of ~881h (36.7 days). Results. Human solutions are 16% more energy-efficient on the server and 3% on the Raspberry Pi, while LLMs outperform human developers by 25% on the PC. Prompting does not consistently lead to energy savings, where the most energy-efficient prompts vary by hardware platform. The code developed by a Green software expert is consistently more energy-efficient by at least 17% to 30% against all LLMs on all hardware platforms. Conclusions. Even though LLMs exhibit relatively good code generation capabilities, no LLM-generated code was more energy-efficient than that of an experienced Green software developer, suggesting that as of today there is still a great need of human expertise for developing energy-efficient Python code.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.10099v1",
    "published_date": "2025-09-12 09:49:46 UTC",
    "updated_date": "2025-09-12 09:49:46 UTC"
  },
  {
    "arxiv_id": "2509.10078v1",
    "title": "Established Psychometric vs. Ecologically Valid Questionnaires: Rethinking Psychological Assessments in Large Language Models",
    "authors": [
      "Dongmin Choi",
      "Woojung Song",
      "Jongwook Han",
      "Eun-Ju Lee",
      "Yohan Jo"
    ],
    "abstract": "Researchers have applied established psychometric questionnaires (e.g., BFI, PVQ) to measure the personality traits and values reflected in the responses of Large Language Models (LLMs). However, concerns have been raised about applying these human-designed questionnaires to LLMs. One such concern is their lack of ecological validity--the extent to which survey questions adequately reflect and resemble real-world contexts in which LLMs generate texts in response to user queries. However, it remains unclear how established questionnaires and ecologically valid questionnaires differ in their outcomes, and what insights these differences may provide. In this paper, we conduct a comprehensive comparative analysis of the two types of questionnaires. Our analysis reveals that established questionnaires (1) yield substantially different profiles of LLMs from ecologically valid ones, deviating from the psychological characteristics expressed in the context of user queries, (2) suffer from insufficient items for stable measurement, (3) create misleading impressions that LLMs possess stable constructs, and (4) yield exaggerated profiles for persona-prompted LLMs. Overall, our work cautions against the use of established psychological questionnaires for LLMs. Our code will be released upon publication.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "17 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2509.10078v1",
    "published_date": "2025-09-12 09:14:42 UTC",
    "updated_date": "2025-09-12 09:14:42 UTC"
  },
  {
    "arxiv_id": "2509.10077v2",
    "title": "Predictive Spike Timing Enables Distributed Shortest Path Computation in Spiking Neural Networks",
    "authors": [
      "Simen Storesund",
      "Kristian Valset Aars",
      "Robin Dietrich",
      "Nicolai Waniek"
    ],
    "abstract": "Efficient planning and sequence selection are central to intelligence, yet current approaches remain largely incompatible with biological computation. Classical graph algorithms like Dijkstra's or A* require global state and biologically implausible operations such as backtracing, while reinforcement learning methods rely on slow gradient-based policy updates that appear inconsistent with rapid behavioral adaptation observed in natural systems.\n  We propose a biologically plausible algorithm for shortest-path computation that operates through local spike-based message-passing with realistic processing delays. The algorithm exploits spike-timing coincidences to identify nodes on optimal paths: Neurons that receive inhibitory-excitatory message pairs earlier than predicted reduce their response delays, creating a temporal compression that propagates backwards from target to source. Through analytical proof and simulations on random spatial networks, we demonstrate that the algorithm converges and discovers all shortest paths using purely timing-based mechanisms. By showing how short-term timing dynamics alone can compute shortest paths, this work provides new insights into how biological networks might solve complex computational problems through purely local computation and relative spike-time prediction. These findings open new directions for understanding distributed computation in biological and artificial systems, with possible implications for computational neuroscience, AI, reinforcement learning, and neuromorphic systems.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.DS",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.10077v2",
    "published_date": "2025-09-12 09:13:47 UTC",
    "updated_date": "2026-01-17 16:59:36 UTC"
  },
  {
    "arxiv_id": "2509.14255v1",
    "title": "Opening the Black Box: Interpretable LLMs via Semantic Resonance Architecture",
    "authors": [
      "Ivan Ternovtsii"
    ],
    "abstract": "Large language models (LLMs) achieve remarkable performance but remain difficult to interpret. Mixture-of-Experts (MoE) models improve efficiency through sparse activation, yet typically rely on opaque, learned gating functions. While similarity-based routing (Cosine Routers) has been explored for training stabilization, its potential for inherent interpretability remains largely untapped. We introduce the Semantic Resonance Architecture (SRA), an MoE approach designed to ensure that routing decisions are inherently interpretable. SRA replaces learned gating with a Chamber of Semantic Resonance (CSR) module, which routes tokens based on cosine similarity with trainable semantic anchors. We also introduce a novel Dispersion Loss that encourages orthogonality among anchors to enforce diverse specialization. Experiments on WikiText-103 demonstrate that SRA achieves a validation perplexity of 13.41, outperforming both a dense baseline (14.13) and a Standard MoE baseline (13.53) under matched active parameter constraints (29.0M). Crucially, SRA exhibits superior expert utilization (1.0% dead experts vs. 14.8% in the Standard MoE) and develops distinct, semantically coherent specialization patterns, unlike the noisy specialization observed in standard MoEs. This work establishes semantic routing as a robust methodology for building more transparent and controllable language models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "13 pages, 5 figures. Code available at https://github.com/ITernovtsii/semantic-resonance. Preprint",
    "pdf_url": "https://arxiv.org/pdf/2509.14255v1",
    "published_date": "2025-09-12 09:02:48 UTC",
    "updated_date": "2025-09-12 09:02:48 UTC"
  },
  {
    "arxiv_id": "2509.10063v1",
    "title": "TwinTac: A Wide-Range, Highly Sensitive Tactile Sensor with Real-to-Sim Digital Twin Sensor Model",
    "authors": [
      "Xiyan Huang",
      "Zhe Xu",
      "Chenxi Xiao"
    ],
    "abstract": "Robot skill acquisition processes driven by reinforcement learning often rely on simulations to efficiently generate large-scale interaction data. However, the absence of simulation models for tactile sensors has hindered the use of tactile sensing in such skill learning processes, limiting the development of effective policies driven by tactile perception. To bridge this gap, we present TwinTac, a system that combines the design of a physical tactile sensor with its digital twin model. Our hardware sensor is designed for high sensitivity and a wide measurement range, enabling high quality sensing data essential for object interaction tasks. Building upon the hardware sensor, we develop the digital twin model using a real-to-sim approach. This involves collecting synchronized cross-domain data, including finite element method results and the physical sensor's outputs, and then training neural networks to map simulated data to real sensor responses. Through experimental evaluation, we characterized the sensitivity of the physical sensor and demonstrated the consistency of the digital twin in replicating the physical sensor's output. Furthermore, by conducting an object classification task, we showed that simulation data generated by our digital twin sensor can effectively augment real-world data, leading to improved accuracy. These results highlight TwinTac's potential to bridge the gap in cross-domain learning tasks.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "7 pages, 9 figures, 1 table, to be published in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2025)",
    "pdf_url": "https://arxiv.org/pdf/2509.10063v1",
    "published_date": "2025-09-12 08:51:28 UTC",
    "updated_date": "2025-09-12 08:51:28 UTC"
  },
  {
    "arxiv_id": "2509.10059v1",
    "title": "Multimodal Mathematical Reasoning Embedded in Aerial Vehicle Imagery: Benchmarking, Analysis, and Exploration",
    "authors": [
      "Yue Zhou",
      "Litong Feng",
      "Mengcheng Lan",
      "Xue Yang",
      "Qingyun Li",
      "Yiping Ke",
      "Xue Jiang",
      "Wayne Zhang"
    ],
    "abstract": "Mathematical reasoning is critical for tasks such as precise distance and area computations, trajectory estimations, and spatial analysis in unmanned aerial vehicle (UAV) based remote sensing, yet current vision-language models (VLMs) have not been adequately tested in this domain. To address this gap, we introduce AVI-Math, the first benchmark to rigorously evaluate multimodal mathematical reasoning in aerial vehicle imagery, moving beyond simple counting tasks to include domain-specific knowledge in areas such as geometry, logic, and algebra. The dataset comprises 3,773 high-quality vehicle-related questions captured from UAV views, covering 6 mathematical subjects and 20 topics. The data, collected at varying altitudes and from multiple UAV angles, reflects real-world UAV scenarios, ensuring the diversity and complexity of the constructed mathematical problems. In this paper, we benchmark 14 prominent VLMs through a comprehensive evaluation and demonstrate that, despite their success on previous multimodal benchmarks, these models struggle with the reasoning tasks in AVI-Math. Our detailed analysis highlights significant limitations in the mathematical reasoning capabilities of current VLMs and suggests avenues for future research. Furthermore, we explore the use of Chain-of-Thought prompting and fine-tuning techniques, which show promise in addressing the reasoning challenges in AVI-Math. Our findings not only expose the limitations of VLMs in mathematical reasoning but also offer valuable insights for advancing UAV-based trustworthy VLMs in real-world applications. The code, and datasets will be released at https://github.com/VisionXLab/avi-math",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "17 pages, 16 figures",
    "pdf_url": "https://arxiv.org/pdf/2509.10059v1",
    "published_date": "2025-09-12 08:46:49 UTC",
    "updated_date": "2025-09-12 08:46:49 UTC"
  },
  {
    "arxiv_id": "2509.10057v1",
    "title": "Reinforcement learning for spin torque oscillator tasks",
    "authors": [
      "Jakub Mojsiejuk",
      "Sławomir Ziętek",
      "Witold Skowroński"
    ],
    "abstract": "We address the problem of automatic synchronisation of the spintronic oscillator (STO) by means of reinforcement learning (RL). A numerical solution of the macrospin Landau-Lifschitz-Gilbert-Slonczewski equation is used to simulate the STO and we train the two types of RL agents to synchronise with a target frequency within a fixed number of steps. We explore modifications to this base task and show an improvement in both convergence and energy efficiency of the synchronisation that can be easily achieved in the simulated environment.",
    "categories": [
      "physics.app-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "physics.app-ph",
    "comment": "3 figures, 6 pages",
    "pdf_url": "https://arxiv.org/pdf/2509.10057v1",
    "published_date": "2025-09-12 08:41:39 UTC",
    "updated_date": "2025-09-12 08:41:39 UTC"
  },
  {
    "arxiv_id": "2509.10054v1",
    "title": "XAgents: A Unified Framework for Multi-Agent Cooperation via IF-THEN Rules and Multipolar Task Processing Graph",
    "authors": [
      "Hailong Yang",
      "Mingxian Gu",
      "Jianqi Wang",
      "Guanjin Wang",
      "Zhaohong Deng"
    ],
    "abstract": "The rapid advancement of Large Language Models (LLMs) has significantly enhanced the capabilities of Multi-Agent Systems (MAS) in supporting humans with complex, real-world tasks. However, MAS still face challenges in effective task planning when handling highly complex tasks with uncertainty, often resulting in misleading or incorrect outputs that hinder task execution. To address this, we propose XAgents, a unified multi-agent cooperative framework built on a multipolar task processing graph and IF-THEN rules. XAgents uses the multipolar task processing graph to enable dynamic task planning and handle task uncertainty. During subtask processing, it integrates domain-specific IF-THEN rules to constrain agent behaviors, while global rules enhance inter-agent collaboration. We evaluate the performance of XAgents across three distinct datasets, demonstrating that it consistently surpasses state-of-the-art single-agent and multi-agent approaches in both knowledge-typed and logic-typed question-answering tasks. The codes for XAgents are available at: https://github.com/AGI-FHBC/XAgents.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.10054v1",
    "published_date": "2025-09-12 08:40:58 UTC",
    "updated_date": "2025-09-12 08:40:58 UTC"
  },
  {
    "arxiv_id": "2511.05496v1",
    "title": "DOCUEVAL: An LLM-based AI Engineering Tool for Building Customisable Document Evaluation Workflows",
    "authors": [
      "Hao Zhang",
      "Qinghua Lu",
      "Liming Zhu"
    ],
    "abstract": "Foundation models, such as large language models (LLMs), have the potential to streamline evaluation workflows and improve their performance. However, practical adoption faces challenges, such as customisability, accuracy, and scalability. In this paper, we present DOCUEVAL, an AI engineering tool for building customisable DOCUment EVALuation workflows. DOCUEVAL supports advanced document processing and customisable workflow design which allow users to define theory-grounded reviewer roles, specify evaluation criteria, experiment with different reasoning strategies and choose the assessment style. To ensure traceability, DOCUEVAL provides comprehensive logging of every run, along with source attribution and configuration management, allowing systematic comparison of results across alternative setups. By integrating these capabilities, DOCUEVAL directly addresses core software engineering challenges, including how to determine whether evaluators are \"good enough\" for deployment and how to empirically compare different evaluation strategies. We demonstrate the usefulness of DOCUEVAL through a real-world academic peer review case, showing how DOCUEVAL enables both the engineering of evaluators and scalable, reliable document evaluation.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.05496v1",
    "published_date": "2025-09-12 08:09:09 UTC",
    "updated_date": "2025-09-12 08:09:09 UTC"
  },
  {
    "arxiv_id": "2509.10025v1",
    "title": "Exploring Expert Specialization through Unsupervised Training in Sparse Mixture of Experts",
    "authors": [
      "Strahinja Nikolic",
      "Ilker Oguz",
      "Demetri Psaltis"
    ],
    "abstract": "Understanding the internal organization of neural networks remains a fundamental challenge in deep learning interpretability. We address this challenge by exploring a novel Sparse Mixture of Experts Variational Autoencoder (SMoE-VAE) architecture. We test our model on the QuickDraw dataset, comparing unsupervised expert routing against a supervised baseline guided by ground-truth labels. Surprisingly, we find that unsupervised routing consistently achieves superior reconstruction performance. The experts learn to identify meaningful sub-categorical structures that often transcend human-defined class boundaries. Through t-SNE visualizations and reconstruction analysis, we investigate how MoE models uncover fundamental data structures that are more aligned with the model's objective than predefined labels. Furthermore, our study on the impact of dataset size provides insights into the trade-offs between data quantity and expert specialization, offering guidance for designing efficient MoE architectures.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages, 7 figures",
    "pdf_url": "https://arxiv.org/pdf/2509.10025v1",
    "published_date": "2025-09-12 07:45:10 UTC",
    "updated_date": "2025-09-12 07:45:10 UTC"
  },
  {
    "arxiv_id": "2509.10018v2",
    "title": "GAMA: A General Anonymizing Multi-Agent System for Privacy Preservation Enhanced by Domain Rules and Disproof Mechanism",
    "authors": [
      "Hailong Yang",
      "Renhuo Zhao",
      "Guanjin Wang",
      "Zhaohong Deng"
    ],
    "abstract": "With the rapid advancement of Large Language Models (LLMs), LLM-based agents exhibit exceptional abilities in understanding and generating natural language, enabling human-like collaboration and information transmission in LLM-based Multi-Agent Systems (MAS). High-performance LLMs are often hosted on web servers in public cloud environments. When tasks involve private data, MAS cannot securely utilize these LLMs without implementing the agentic privacy-preserving mechanism. To address this challenge, we propose a General Anonymizing Multi-Agent System (GAMA), which divides the agents' workspace into private and public spaces, ensuring privacy through a structured anonymization mechanism. In the private space, agents handle sensitive data, while in the public web space, only anonymized data is utilized. GAMA incorporates two key modules to mitigate semantic loss caused by anonymization: Domain-Rule-based Knowledge Enhancement (DRKE) and Disproof-based Logic Enhancement (DLE). We evaluate GAMA on two general question-answering datasets, a public privacy leakage benchmark, and two customized question-answering datasets related to privacy. The results demonstrate that GAMA outperforms existing baselines on the evaluated datasets in terms of both task accuracy and privacy preservation metrics.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.10018v2",
    "published_date": "2025-09-12 07:22:49 UTC",
    "updated_date": "2025-10-04 22:46:50 UTC"
  },
  {
    "arxiv_id": "2509.12251v1",
    "title": "V-Math: An Agentic Approach to the Vietnamese National High School Graduation Mathematics Exams",
    "authors": [
      "Duong Q. Nguyen",
      "Quy P. Nguyen",
      "Nguyen Van Nhon",
      "Quang-Thinh Bui",
      "H. Nguyen-Xuan"
    ],
    "abstract": "This paper develops an autonomous agentic framework called V-Math that aims to assist Vietnamese high school students in preparing for the National High School Graduation Mathematics Exams (NHSGMEs). The salient framework integrates three specialized AI agents: a specification-matrix-conditioned question generator, a solver/explainer for detailed step-by-step reasoning, and a personalized tutor that adapts to student performance. Beyond enabling self-paced student practice, V-Math supports teachers by generating innovative, compliant exam questions and building diverse, high-quality question banks. This reduces manual workload and enriches instructional resources. We describe the system architecture, focusing on practice modes for learners and teacher-oriented features for question generation. Preliminary evaluations demonstrate that V-Math produces matrix-aligned exams with high solution accuracy, delivers coherent explanations, and enhances the variety of practice materials. These results highlight its potential to support scalable, equitable mathematics preparation aligned with national standards while also empowering teachers through AI-assisted exam creation.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.12251v1",
    "published_date": "2025-09-12 07:22:46 UTC",
    "updated_date": "2025-09-12 07:22:46 UTC"
  },
  {
    "arxiv_id": "2509.10011v3",
    "title": "Intrinsic Dimension Estimating Autoencoder (IDEA) Using CancelOut Layer and a Projected Loss",
    "authors": [
      "Antoine Oriou",
      "Philipp Krah",
      "Julian Koellermeier"
    ],
    "abstract": "This paper introduces the Intrinsic Dimension Estimating Autoencoder (IDEA), which identifies the underlying intrinsic dimension of a wide range of datasets whose samples lie on either linear or nonlinear manifolds. Beyond estimating the intrinsic dimension, IDEA is also able to reconstruct the original dataset after projecting it onto the corresponding latent space, which is structured using re-weighted double CancelOut layers. Our key contribution is the introduction of the projected reconstruction loss term, guiding the training of the model by continuously assessing the reconstruction quality under the removal of an additional latent dimension. We first assess the performance of IDEA on a series of theoretical benchmarks to validate its robustness. These experiments allow us to test its reconstruction ability and compare its performance with state-of-the-art intrinsic dimension estimators. The benchmarks show good accuracy and high versatility of our approach. Subsequently, we apply our model to data generated from the numerical solution of a vertically resolved one-dimensional free-surface flow, following a pointwise discretization of the vertical velocity profile in the horizontal direction, vertical direction, and time. IDEA succeeds in estimating the dataset's intrinsic dimension and then reconstructs the original solution by working directly within the projection space identified by the network.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.NA"
    ],
    "primary_category": "cs.LG",
    "comment": "Code available: https://doi.org/10.5281/zenodo.18314616 Github: https://github.com/antoineor/IDEA-for-Shallow-Flows/tree/v1.1",
    "pdf_url": "https://arxiv.org/pdf/2509.10011v3",
    "published_date": "2025-09-12 07:11:05 UTC",
    "updated_date": "2026-01-21 14:45:18 UTC"
  },
  {
    "arxiv_id": "2509.12250v1",
    "title": "OnlineHOI: Towards Online Human-Object Interaction Generation and Perception",
    "authors": [
      "Yihong Ji",
      "Yunze Liu",
      "Yiyao Zhuo",
      "Weijiang Yu",
      "Fei Ma",
      "Joshua Huang",
      "Fei Yu"
    ],
    "abstract": "The perception and generation of Human-Object Interaction (HOI) are crucial for fields such as robotics, AR/VR, and human behavior understanding. However, current approaches model this task in an offline setting, where information at each time step can be drawn from the entire interaction sequence. In contrast, in real-world scenarios, the information available at each time step comes only from the current moment and historical data, i.e., an online setting. We find that offline methods perform poorly in an online context. Based on this observation, we propose two new tasks: Online HOI Generation and Perception. To address this task, we introduce the OnlineHOI framework, a network architecture based on the Mamba framework that employs a memory mechanism. By leveraging Mamba's powerful modeling capabilities for streaming data and the Memory mechanism's efficient integration of historical information, we achieve state-of-the-art results on the Core4D and OAKINK2 online generation tasks, as well as the online HOI4D perception task.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at ACM MM 2025",
    "pdf_url": "https://arxiv.org/pdf/2509.12250v1",
    "published_date": "2025-09-12 06:58:29 UTC",
    "updated_date": "2025-09-12 06:58:29 UTC"
  },
  {
    "arxiv_id": "2509.10004v1",
    "title": "Unsupervised Hallucination Detection by Inspecting Reasoning Processes",
    "authors": [
      "Ponhvoan Srey",
      "Xiaobao Wu",
      "Anh Tuan Luu"
    ],
    "abstract": "Unsupervised hallucination detection aims to identify hallucinated content generated by large language models (LLMs) without relying on labeled data. While unsupervised methods have gained popularity by eliminating labor-intensive human annotations, they frequently rely on proxy signals unrelated to factual correctness. This misalignment biases detection probes toward superficial or non-truth-related aspects, limiting generalizability across datasets and scenarios. To overcome these limitations, we propose IRIS, an unsupervised hallucination detection framework, leveraging internal representations intrinsic to factual correctness. IRIS prompts the LLM to carefully verify the truthfulness of a given statement, and obtain its contextualized embedding as informative features for training. Meanwhile, the uncertainty of each response is considered a soft pseudolabel for truthfulness. Experimental results demonstrate that IRIS consistently outperforms existing unsupervised methods. Our approach is fully unsupervised, computationally low cost, and works well even with few training data, making it suitable for real-time detection.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "To appear in EMNLP 2025",
    "pdf_url": "https://arxiv.org/pdf/2509.10004v1",
    "published_date": "2025-09-12 06:58:17 UTC",
    "updated_date": "2025-09-12 06:58:17 UTC"
  },
  {
    "arxiv_id": "2509.19319v2",
    "title": "FHIR-AgentBench: Benchmarking LLM Agents for Realistic Interoperable EHR Question Answering",
    "authors": [
      "Gyubok Lee",
      "Elea Bach",
      "Eric Yang",
      "Tom Pollard",
      "Alistair Johnson",
      "Edward Choi",
      "Yugang jia",
      "Jong Ha Lee"
    ],
    "abstract": "The recent shift toward the Health Level Seven Fast Healthcare Interoperability Resources (HL7 FHIR) standard opens a new frontier for clinical AI, demanding LLM agents to navigate complex, resource-based data models instead of conventional structured health data. However, existing benchmarks have lagged behind this transition, lacking the realism needed to evaluate recent LLMs on interoperable clinical data. To bridge this gap, we introduce FHIR-AgentBench, a benchmark that grounds 2,931 real-world clinical questions in the HL7 FHIR standard. Using this benchmark, we systematically evaluate agentic frameworks, comparing different data retrieval strategies (direct FHIR API calls vs. specialized tools), interaction patterns (single-turn vs. multi-turn), and reasoning strategies (natural language vs. code generation). Our experiments highlight the practical challenges of retrieving data from intricate FHIR resources and the difficulty of reasoning over them, both of which critically affect question answering performance. We publicly release the FHIR-AgentBench dataset and evaluation suite (https://github.com/glee4810/FHIR-AgentBench) to promote reproducible research and the development of robust, reliable LLM agents for clinical applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ML4H 2025 Proceedings",
    "pdf_url": "https://arxiv.org/pdf/2509.19319v2",
    "published_date": "2025-09-12 06:52:55 UTC",
    "updated_date": "2025-11-13 06:35:10 UTC"
  },
  {
    "arxiv_id": "2509.09982v1",
    "title": "Evaluation of Black-Box XAI Approaches for Predictors of Values of Boolean Formulae",
    "authors": [
      "Stav Armoni-Friedmann",
      "Hana Chockler",
      "David A. Kelly"
    ],
    "abstract": "Evaluating explainable AI (XAI) approaches is a challenging task in general, due to the subjectivity of explanations. In this paper, we focus on tabular data and the specific use case of AI models predicting the values of Boolean functions. We extend the previous work in this domain by proposing a formal and precise measure of importance of variables based on actual causality, and we evaluate state-of-the-art XAI tools against this measure. We also present a novel XAI tool B-ReX, based on the existing tool ReX, and demonstrate that it is superior to other black-box XAI tools on a large-scale benchmark. Specifically, B-ReX achieves a Jensen-Shannon divergence of 0.072 $\\pm$ 0.012 on random 10-valued Boolean formulae",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to ECAI-EXCD Workshop, 8 pages, 2 figures, 5 tables",
    "pdf_url": "https://arxiv.org/pdf/2509.09982v1",
    "published_date": "2025-09-12 05:52:47 UTC",
    "updated_date": "2025-09-12 05:52:47 UTC"
  },
  {
    "arxiv_id": "2509.12249v2",
    "title": "Why and How Auxiliary Tasks Improve JEPA Representations",
    "authors": [
      "Jiacan Yu",
      "Siyi Chen",
      "Mingrui Liu",
      "Nono Horiuchi",
      "Vladimir Braverman",
      "Zicheng Xu",
      "Dan Haramati",
      "Randall Balestriero"
    ],
    "abstract": "Joint-Embedding Predictive Architecture (JEPA) is increasingly used for visual representation learning and as a component in model-based RL, but its behavior remains poorly understood. We provide a theoretical characterization of a simple, practical JEPA variant that has an auxiliary regression head trained jointly with latent dynamics. We prove a No Unhealthy Representation Collapse theorem: in deterministic MDPs, if training drives both the latent-transition consistency loss and the auxiliary regression loss to zero, then any pair of non-equivalent observations, i.e., those that do not have the same transition dynamics or auxiliary value, must map to distinct latent representations. Thus, the auxiliary task anchors which distinctions the representation must preserve. Controlled ablations in a counting environment corroborate the theory and show that training the JEPA model jointly with the auxiliary head generates a richer representation than training them separately. Our work indicates a path to improve JEPA encoders: training them with an auxiliary function that, together with the transition dynamics, encodes the right equivalence relations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.12249v2",
    "published_date": "2025-09-12 05:28:29 UTC",
    "updated_date": "2025-10-19 02:58:38 UTC"
  },
  {
    "arxiv_id": "2509.09972v1",
    "title": "Drone-Based Multispectral Imaging and Deep Learning for Timely Detection of Branched Broomrape in Tomato Farms",
    "authors": [
      "Mohammadreza Narimani",
      "Alireza Pourreza",
      "Ali Moghimi",
      "Mohsen Mesgaran",
      "Parastoo Farajpoor",
      "Hamid Jafarbiglu"
    ],
    "abstract": "This study addresses the escalating threat of branched broomrape (Phelipanche ramosa) to California's tomato industry, which supplies over 90 percent of U.S. processing tomatoes. The parasite's largely underground life cycle makes early detection difficult, while conventional chemical controls are costly, environmentally harmful, and often ineffective. To address this, we combined drone-based multispectral imagery with Long Short-Term Memory (LSTM) deep learning networks, using the Synthetic Minority Over-sampling Technique (SMOTE) to handle class imbalance. Research was conducted on a known broomrape-infested tomato farm in Woodland, Yolo County, CA, across five key growth stages determined by growing degree days (GDD). Multispectral images were processed to isolate tomato canopy reflectance. At 897 GDD, broomrape could be detected with 79.09 percent overall accuracy and 70.36 percent recall without integrating later stages. Incorporating sequential growth stages with LSTM improved detection substantially. The best-performing scenario, which integrated all growth stages with SMOTE augmentation, achieved 88.37 percent overall accuracy and 95.37 percent recall. These results demonstrate the strong potential of temporal multispectral analysis and LSTM networks for early broomrape detection. While further real-world data collection is needed for practical deployment, this study shows that UAV-based multispectral sensing coupled with deep learning could provide a powerful precision agriculture tool to reduce losses and improve sustainability in tomato production.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "Author-accepted version (no publisher header/footer). 10 pages + presentation. Published in Proceedings of SPIE Defense + Commercial Sensing 2024, Vol. 13053, Paper 1305304. Event: National Harbor, Maryland, USA. Official version: https://doi.org/10.1117/12.3021219",
    "pdf_url": "https://arxiv.org/pdf/2509.09972v1",
    "published_date": "2025-09-12 05:16:56 UTC",
    "updated_date": "2025-09-12 05:16:56 UTC"
  },
  {
    "arxiv_id": "2509.09970v1",
    "title": "Securing LLM-Generated Embedded Firmware through AI Agent-Driven Validation and Patching",
    "authors": [
      "Seyed Moein Abtahi",
      "Akramul Azim"
    ],
    "abstract": "Large Language Models (LLMs) show promise in generating firmware for embedded systems, but often introduce security flaws and fail to meet real-time performance constraints. This paper proposes a three-phase methodology that combines LLM-based firmware generation with automated security validation and iterative refinement in a virtualized environment. Using structured prompts, models like GPT-4 generate firmware for networking and control tasks, deployed on FreeRTOS via QEMU. These implementations are tested using fuzzing, static analysis, and runtime monitoring to detect vulnerabilities such as buffer overflows (CWE-120), race conditions (CWE-362), and denial-of-service threats (CWE-400). Specialized AI agents for Threat Detection, Performance Optimization, and Compliance Verification collaborate to improve detection and remediation. Identified issues are categorized using CWE, then used to prompt targeted LLM-generated patches in an iterative loop. Experiments show a 92.4\\% Vulnerability Remediation Rate (37.3\\% improvement), 95.8\\% Threat Model Compliance, and 0.87 Security Coverage Index. Real-time metrics include 8.6ms worst-case execution time and 195μs jitter. This process enhances firmware security and performance while contributing an open-source dataset for future research.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.09970v1",
    "published_date": "2025-09-12 05:15:35 UTC",
    "updated_date": "2025-09-12 05:15:35 UTC"
  },
  {
    "arxiv_id": "2509.09969v1",
    "title": "Large Language Models Meet Legal Artificial Intelligence: A Survey",
    "authors": [
      "Zhitian Hou",
      "Zihan Ye",
      "Nanli Zeng",
      "Tianyong Hao",
      "Kun Zeng"
    ],
    "abstract": "Large Language Models (LLMs) have significantly advanced the development of Legal Artificial Intelligence (Legal AI) in recent years, enhancing the efficiency and accuracy of legal tasks. To advance research and applications of LLM-based approaches in legal domain, this paper provides a comprehensive review of 16 legal LLMs series and 47 LLM-based frameworks for legal tasks, and also gather 15 benchmarks and 29 datasets to evaluate different legal capabilities. Additionally, we analyse the challenges and discuss future directions for LLM-based approaches in the legal domain. We hope this paper provides a systematic introduction for beginners and encourages future research in this field. Resources are available at https://github.com/ZhitianHou/LLMs4LegalAI.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.09969v1",
    "published_date": "2025-09-12 05:08:11 UTC",
    "updated_date": "2025-09-12 05:08:11 UTC"
  },
  {
    "arxiv_id": "2509.09960v1",
    "title": "Limited Reference, Reliable Generation: A Two-Component Framework for Tabular Data Generation in Low-Data Regimes",
    "authors": [
      "Mingxuan Jiang",
      "Yongxin Wang",
      "Ziyue Dai",
      "Yicun Liu",
      "Hongyi Nie",
      "Sen Liu",
      "Hongfeng Chai"
    ],
    "abstract": "Synthetic tabular data generation is increasingly essential in data management, supporting downstream applications when real-world and high-quality tabular data is insufficient. Existing tabular generation approaches, such as generative adversarial networks (GANs), diffusion models, and fine-tuned Large Language Models (LLMs), typically require sufficient reference data, limiting their effectiveness in domain-specific databases with scarce records. While prompt-based LLMs offer flexibility without parameter tuning, they often fail to capture dataset-specific feature-label dependencies and generate redundant data, leading to degradation in downstream task performance. To overcome these issues, we propose ReFine, a framework that (i) derives symbolic \"if-then\" rules from interpretable models and embeds them into prompts to explicitly guide generation toward domain-specific feature distribution, and (ii) applies a dual-granularity filtering strategy that suppresses over-sampling patterns and selectively refines rare but informative samples to reduce distributional imbalance. Extensive experiments on various regression and classification benchmarks demonstrate that ReFine consistently outperforms state-of-the-art methods, achieving up to 0.44 absolute improvement in R-squared for regression and 10.0 percent relative improvement in F1 score for classification tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.09960v1",
    "published_date": "2025-09-12 04:34:46 UTC",
    "updated_date": "2025-09-12 04:34:46 UTC"
  },
  {
    "arxiv_id": "2509.09958v3",
    "title": "Zero-Shot Referring Expression Comprehension via Vison-Language True/False Verification",
    "authors": [
      "Jeffrey Liu",
      "Rongbin Hu"
    ],
    "abstract": "Referring Expression Comprehension (REC) is usually addressed with task-trained grounding models. We show that a zero-shot workflow, without any REC-specific training, can achieve competitive or superior performance. Our approach reformulates REC as box-wise visual-language verification: given proposals from a COCO-clean generic detector (YOLO-World), a general-purpose VLM independently answers True/False queries for each region. This simple procedure reduces cross-box interference, supports abstention and multiple matches, and requires no fine-tuning. On RefCOCO, RefCOCO+, and RefCOCOg, our method not only surpasses a zero-shot GroundingDINO baseline but also exceeds reported results for GroundingDINO trained on REC and GroundingDINO+CRG. Controlled studies with identical proposals confirm that verification significantly outperforms selection-based prompting, and results hold with open VLMs. Overall, we show that workflow design, rather than task-specific pretraining, drives strong zero-shot REC performance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.09958v3",
    "published_date": "2025-09-12 04:32:52 UTC",
    "updated_date": "2025-11-13 04:00:28 UTC"
  },
  {
    "arxiv_id": "2509.09955v1",
    "title": "Adaptive Token Merging for Efficient Transformer Semantic Communication at the Edge",
    "authors": [
      "Omar Erak",
      "Omar Alhussein",
      "Hatem Abou-Zeid",
      "Mehdi Bennis",
      "Sami Muhaidat"
    ],
    "abstract": "Large-scale transformers are central to modern semantic communication, yet their high computational and communication costs hinder deployment on resource-constrained edge devices. This paper introduces a training-free framework for adaptive token merging, a novel mechanism that compresses transformer representations at runtime by selectively merging semantically redundant tokens under per-layer similarity thresholds. Unlike prior fixed-ratio reduction, our approach couples merging directly to input redundancy, enabling data-dependent adaptation that balances efficiency and task relevance without retraining. We cast the discovery of merging strategies as a multi-objective optimization problem and leverage Bayesian optimization to obtain Pareto-optimal trade-offs between accuracy, inference cost, and communication cost. On ImageNet classification, we match the accuracy of the unmodified transformer with 30\\% fewer floating-point operations per second and under 20\\% of the original communication cost, while for visual question answering our method achieves performance competitive with the full LLaVA model at less than one-third of the compute and one-tenth of the bandwidth. Finally, we show that our adaptive merging is robust across varying channel conditions and provides inherent privacy benefits, substantially degrading the efficacy of model inversion attacks. Our framework provides a practical and versatile solution for deploying powerful transformer models in resource-limited edge intelligence scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "eess.IV"
    ],
    "primary_category": "cs.LG",
    "comment": "Submitted to IEEE Journals",
    "pdf_url": "https://arxiv.org/pdf/2509.09955v1",
    "published_date": "2025-09-12 04:11:59 UTC",
    "updated_date": "2025-09-12 04:11:59 UTC"
  },
  {
    "arxiv_id": "2509.09942v2",
    "title": "Towards Secure and Explainable Smart Contract Generation with Security-Aware Group Relative Policy Optimization",
    "authors": [
      "Lei Yu",
      "Jingyuan Zhang",
      "Xin Wang",
      "Jiajia Ma",
      "Li Yang",
      "Fengjun Zhang"
    ],
    "abstract": "Smart contracts automate the management of high-value assets, where vulnerabilities can lead to catastrophic financial losses. This challenge is amplified in Large Language Models (LLMs) by two interconnected failures: they operate as unauditable \"black boxes\" lacking a transparent reasoning process, and consequently, generate code riddled with critical security vulnerabilities. To address both issues, we propose SmartCoder-R1 (based on Qwen2.5-Coder-7B), a novel framework for secure and explainable smart contract generation. It begins with Continual Pre-training (CPT) to specialize the model. We then apply Long Chain-of-Thought Supervised Fine-Tuning (L-CoT SFT) on 7,998 expert-validated reasoning-and-code samples to train the model to emulate human security analysis. Finally, to directly mitigate vulnerabilities, we employ Security-Aware Group Relative Policy Optimization (S-GRPO), a reinforcement learning phase that refines the generation policy by optimizing a weighted reward signal for compilation success, security compliance, and format correctness. Evaluated against 17 baselines on a benchmark of 756 real-world functions, SmartCoder-R1 establishes a new state of the art, achieving top performance across five key metrics: a ComPass of 87.70%, a VulRate of 8.60%, a SafeAval of 80.16%, a FuncRate of 53.84%, and a FullRate of 50.53%. This FullRate marks a 45.79% relative improvement over the strongest baseline, DeepSeek-R1. Crucially, its generated reasoning also excels in human evaluations, achieving high-quality ratings for Functionality (82.7%), Security (85.3%), and Clarity (90.7%).",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.09942v2",
    "published_date": "2025-09-12 03:14:50 UTC",
    "updated_date": "2025-10-12 04:04:06 UTC"
  },
  {
    "arxiv_id": "2509.09919v2",
    "title": "A Markovian Framing of WaveFunctionCollapse for Procedurally Generating Aesthetically Complex Environments",
    "authors": [
      "Franklin Yiu",
      "Mohan Lu",
      "Nina Li",
      "Kevin Joseph",
      "Tianxu Zhang",
      "Julian Togelius",
      "Timothy Merino",
      "Sam Earle"
    ],
    "abstract": "Procedural content generation often requires satisfying both designer-specified objectives and adjacency constraints implicitly imposed by the underlying tile set. To address the challenges of jointly optimizing both constraints and objectives, we reformulate WaveFunctionCollapse (WFC) as a Markov Decision Process (MDP), enabling external optimization algorithms to focus exclusively on objective maximization while leveraging WFC's propagation mechanism to enforce constraint satisfaction. We empirically compare optimizing this MDP to traditional evolutionary approaches that jointly optimize global metrics and local tile placement. Across multiple domains with various difficulties, we find that joint optimization not only struggles as task complexity increases, but consistently underperforms relative to optimization over the WFC-MDP, underscoring the advantages of decoupling local constraint satisfaction from global objective optimization.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.09919v2",
    "published_date": "2025-09-12 01:51:01 UTC",
    "updated_date": "2025-10-20 00:47:57 UTC"
  },
  {
    "arxiv_id": "2509.09918v1",
    "title": "WALL: A Web Application for Automated Quality Assurance using Large Language Models",
    "authors": [
      "Seyed Moein Abtahi",
      "Akramul Azim"
    ],
    "abstract": "As software projects become increasingly complex, the volume and variety of issues in code files have grown substantially. Addressing this challenge requires efficient issue detection, resolution, and evaluation tools. This paper presents WALL, a web application that integrates SonarQube and large language models (LLMs) such as GPT-3.5 Turbo and GPT-4o to automate these tasks. WALL comprises three modules: an issue extraction tool, code issues reviser, and code comparison tool. Together, they enable a seamless pipeline for detecting software issues, generating automated code revisions, and evaluating the accuracy of revisions. Our experiments, conducted on 563 files with over 7,599 issues, demonstrate WALL's effectiveness in reducing human effort while maintaining high-quality revisions. Results show that employing a hybrid approach of cost-effective and advanced LLMs can significantly lower costs and improve revision rates. Future work aims to enhance WALL's capabilities by integrating open-source LLMs and eliminating human intervention, paving the way for fully automated code quality management.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.09918v1",
    "published_date": "2025-09-12 01:43:26 UTC",
    "updated_date": "2025-09-12 01:43:26 UTC"
  },
  {
    "arxiv_id": "2509.12248v2",
    "title": "Humor in Pixels: Benchmarking Large Multimodal Models Understanding of Online Comics",
    "authors": [
      "Yuriel Ryan",
      "Rui Yang Tan",
      "Kenny Tsu Wei Choo",
      "Roy Ka-Wei Lee"
    ],
    "abstract": "Understanding humor is a core aspect of social intelligence, yet it remains a significant challenge for Large Multimodal Models (LMMs). We introduce PixelHumor, a benchmark dataset of 2,800 annotated multi-panel comics designed to evaluate LMMs' ability to interpret multimodal humor and recognize narrative sequences. Experiments with state-of-the-art LMMs reveal substantial gaps: for instance, top models achieve only 61% accuracy in panel sequencing, far below human performance. This underscores critical limitations in current models' integration of visual and textual cues for coherent narrative and humor understanding. By providing a rigorous framework for evaluating multimodal contextual and narrative reasoning, PixelHumor aims to drive the development of LMMs that better engage in natural, socially aware interactions.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "27 pages, 8 figures, EMNLP 2025 Findings",
    "pdf_url": "https://arxiv.org/pdf/2509.12248v2",
    "published_date": "2025-09-12 01:39:24 UTC",
    "updated_date": "2025-09-17 05:30:43 UTC"
  },
  {
    "arxiv_id": "2509.10584v1",
    "title": "Smart Trial: Evaluating the Use of Large Language Models for Recruiting Clinical Trial Participants via Social Media",
    "authors": [
      "Xiaofan Zhou",
      "Zisu Wang",
      "Janice Krieger",
      "Mohan Zalake",
      "Lu Cheng"
    ],
    "abstract": "Clinical trials (CT) are essential for advancing medical research and treatment, yet efficiently recruiting eligible participants -- each of whom must meet complex eligibility criteria -- remains a significant challenge. Traditional recruitment approaches, such as advertisements or electronic health record screening within hospitals, are often time-consuming and geographically constrained. This work addresses the recruitment challenge by leveraging the vast amount of health-related information individuals share on social media platforms. With the emergence of powerful large language models (LLMs) capable of sophisticated text understanding, we pose the central research question: Can LLM-driven tools facilitate CT recruitment by identifying potential participants through their engagement on social media? To investigate this question, we introduce TRIALQA, a novel dataset comprising two social media collections from the subreddits on colon cancer and prostate cancer. Using eligibility criteria from public real-world CTs, experienced annotators are hired to annotate TRIALQA to indicate (1) whether a social media user meets a given eligibility criterion and (2) the user's stated reasons for interest in participating in CT. We benchmark seven widely used LLMs on these two prediction tasks, employing six distinct training and inference strategies. Our extensive experiments reveal that, while LLMs show considerable promise, they still face challenges in performing the complex, multi-hop reasoning needed to accurately assess eligibility criteria.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.10584v1",
    "published_date": "2025-09-12 01:34:32 UTC",
    "updated_date": "2025-09-12 01:34:32 UTC"
  },
  {
    "arxiv_id": "2510.18877v1",
    "title": "LLM Bazaar: A Service Design for Supporting Collaborative Learning with an LLM-Powered Multi-Party Collaboration Infrastructure",
    "authors": [
      "Zhen Wu",
      "Jiaxin Shi",
      "R. Charles Murray",
      "Carolyn Rosé",
      "Micah San Andres"
    ],
    "abstract": "For nearly two decades, conversational agents have played a critical role in structuring interactions in collaborative learning, shaping group dynamics, and supporting student engagement. The recent integration of large language models (LLMs) into these agents offers new possibilities for fostering critical thinking and collaborative problem solving. In this work, we begin with an open source collaboration support architecture called Bazaar and integrate an LLM-agent shell that enables introduction of LLM-empowered, real time, context sensitive collaborative support for group learning. This design and infrastructure paves the way for exploring how tailored LLM-empowered environments can reshape collaborative learning outcomes and interaction patterns.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "https://repository.isls.org//handle/1/11832",
    "pdf_url": "https://arxiv.org/pdf/2510.18877v1",
    "published_date": "2025-09-12 01:25:49 UTC",
    "updated_date": "2025-09-12 01:25:49 UTC"
  },
  {
    "arxiv_id": "2509.09915v1",
    "title": "The (R)evolution of Scientific Workflows in the Agentic AI Era: Towards Autonomous Science",
    "authors": [
      "Woong Shin",
      "Renan Souza",
      "Daniel Rosendo",
      "Frédéric Suter",
      "Feiyi Wang",
      "Prasanna Balaprakash",
      "Rafael Ferreira da Silva"
    ],
    "abstract": "Modern scientific discovery increasingly requires coordinating distributed facilities and heterogeneous resources, forcing researchers to act as manual workflow coordinators rather than scientists. Advances in AI leading to AI agents show exciting new opportunities that can accelerate scientific discovery by providing intelligence as a component in the ecosystem. However, it is unclear how this new capability would materialize and integrate in the real world. To address this, we propose a conceptual framework where workflows evolve along two dimensions which are intelligence (from static to intelligent) and composition (from single to swarm) to chart an evolutionary path from current workflow management systems to fully autonomous, distributed scientific laboratories. With these trajectories in mind, we present an architectural blueprint that can help the community take the next steps towards harnessing the opportunities in autonomous science with the potential for 100x discovery acceleration and transformational scientific workflows.",
    "categories": [
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.09915v1",
    "published_date": "2025-09-12 01:14:34 UTC",
    "updated_date": "2025-09-12 01:14:34 UTC"
  },
  {
    "arxiv_id": "2509.09911v1",
    "title": "An Autoencoder and Vision Transformer-based Interpretability Analysis of the Differences in Automated Staging of Second and Third Molars",
    "authors": [
      "Barkin Buyukcakir",
      "Jannick De Tobel",
      "Patrick Thevissen",
      "Dirk Vandermeulen",
      "Peter Claes"
    ],
    "abstract": "The practical adoption of deep learning in high-stakes forensic applications, such as dental age estimation, is often limited by the 'black box' nature of the models. This study introduces a framework designed to enhance both performance and transparency in this context. We use a notable performance disparity in the automated staging of mandibular second (tooth 37) and third (tooth 38) molars as a case study. The proposed framework, which combines a convolutional autoencoder (AE) with a Vision Transformer (ViT), improves classification accuracy for both teeth over a baseline ViT, increasing from 0.712 to 0.815 for tooth 37 and from 0.462 to 0.543 for tooth 38. Beyond improving performance, the framework provides multi-faceted diagnostic insights. Analysis of the AE's latent space metrics and image reconstructions indicates that the remaining performance gap is data-centric, suggesting high intra-class morphological variability in the tooth 38 dataset is a primary limiting factor. This work highlights the insufficiency of relying on a single mode of interpretability, such as attention maps, which can appear anatomically plausible yet fail to identify underlying data issues. By offering a methodology that both enhances accuracy and provides evidence for why a model may be uncertain, this framework serves as a more robust tool to support expert decision-making in forensic age estimation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "21 pages, 11 figures, Scientific Reports",
    "pdf_url": "https://arxiv.org/pdf/2509.09911v1",
    "published_date": "2025-09-12 00:54:07 UTC",
    "updated_date": "2025-09-12 00:54:07 UTC"
  },
  {
    "arxiv_id": "2509.09906v1",
    "title": "Tackling One Health Risks: How Large Language Models are leveraged for Risk Negotiation and Consensus-building",
    "authors": [
      "Alexandra Fetsch",
      "Iurii Savvateev",
      "Racem Ben Romdhane",
      "Martin Wiedmann",
      "Artemiy Dimov",
      "Maciej Durkalec",
      "Josef Teichmann",
      "Jakob Zinsstag",
      "Konstantinos Koutsoumanis",
      "Andreja Rajkovic",
      "Jason Mann",
      "Mauro Tonolla",
      "Monika Ehling-Schulz",
      "Matthias Filter",
      "Sophia Johler"
    ],
    "abstract": "Key global challenges of our times are characterized by complex interdependencies and can only be effectively addressed through an integrated, participatory effort. Conventional risk analysis frameworks often reduce complexity to ensure manageability, creating silos that hinder comprehensive solutions. A fundamental shift towards holistic strategies is essential to enable effective negotiations between different sectors and to balance the competing interests of stakeholders. However, achieving this balance is often hindered by limited time, vast amounts of information, and the complexity of integrating diverse perspectives. This study presents an AI-assisted negotiation framework that incorporates large language models (LLMs) and AI-based autonomous agents into a negotiation-centered risk analysis workflow. The framework enables stakeholders to simulate negotiations, systematically model dynamics, anticipate compromises, and evaluate solution impacts. By leveraging LLMs' semantic analysis capabilities we could mitigate information overload and augment decision-making process under time constraints. Proof-of-concept implementations were conducted in two real-world scenarios: (i) prudent use of a biopesticide, and (ii) targeted wild animal population control. Our work demonstrates the potential of AI-assisted negotiation to address the current lack of tools for cross-sectoral engagement. Importantly, the solution's open source, web based design, suits for application by a broader audience with limited resources and enables users to tailor and develop it for their own needs.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.09906v1",
    "published_date": "2025-09-12 00:25:20 UTC",
    "updated_date": "2025-09-12 00:25:20 UTC"
  },
  {
    "arxiv_id": "2509.13342v1",
    "title": "Real World Robotic Exploration using Deep Neural Networks Trained in Photorealistic Reconstructed Environments",
    "authors": [
      "Isaac Ronald Ward"
    ],
    "abstract": "In this work, an existing deep neural network approach for determining a robot's pose from visual information (RGB images) is modified, improving its localization performance without impacting its ease of training. Explicitly, the network's loss function is extended in a manner which intuitively combines the positional and rotational error in order to increase robustness to perceptual aliasing. An improvement in the localization accuracy for indoor scenes is observed: with decreases of up to 9.64% and 2.99% in the median positional and rotational error respectively, when compared to the unmodified network.\n  Additionally, photogrammetry data is used to produce a pose-labelled dataset which allows the above model to be trained on a local environment, resulting in localization accuracies of 0.11m & 0.89 degrees. This trained model forms the basis of a navigation algorithm, which is tested in real-time on a TurtleBot (a wheeled robotic device). As such, this work introduces a full pipeline for creating a robust navigational algorithm for any given real world indoor scene; the only requirement being a collection of images from the scene, which can be captured in as little as 330 seconds of",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "This report is submitted as partial fulfilment of the requirements for the Honours Programme of the Department of Computer Science and Software Engineering, The University of Western Australia, 2019",
    "pdf_url": "https://arxiv.org/pdf/2509.13342v1",
    "published_date": "2025-09-12 00:03:04 UTC",
    "updated_date": "2025-09-12 00:03:04 UTC"
  }
]
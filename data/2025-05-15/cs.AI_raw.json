[
  {
    "arxiv_id": "2506.01976v2",
    "title": "Crack Path Prediction with Operator Learning using Discrete Particle System data Generation",
    "authors": [
      "Elham Kiyani",
      "Venkatesh Ananchaperumal",
      "Ahmad Peyvan",
      "Mahendaran Uchimali",
      "Gang Li",
      "George Em Karniadakis"
    ],
    "abstract": "Accurately modeling crack propagation is critical for predicting failure in engineering materials and structures, where small cracks can rapidly evolve and cause catastrophic damage. The interaction of cracks with discontinuities, such as holes, significantly affects crack deflection and arrest. Recent developments in discrete particle systems with multibody interactions based on constitutive behavior have demonstrated the ability to capture crack nucleation and evolution without relying on continuum assumptions. In this work, we use data from Constitutively Informed Particle Dynamics (CPD) simulations to train operator learning models, specifically Deep Operator Networks (DeepONets), which learn mappings between function spaces instead of finite-dimensional vectors. We explore two DeepONet variants: vanilla and Fusion DeepONet, for predicting time-evolving crack propagation in specimens with varying geometries. Three representative cases are studied: (i) varying notch height without active fracture; and (ii) and (iii) combinations of notch height and hole radius where dynamic fracture occurs on irregular discrete meshes. The models are trained using geometric inputs in the branch network and spatial-temporal coordinates in the trunk network. Results show that Fusion DeepONet consistently outperforms the vanilla variant, with more accurate predictions especially in non-fracturing cases. Fracture-driven scenarios involving displacement and crack evolution remain more challenging. These findings highlight the potential of Fusion DeepONet to generalize across complex, geometry-varying, and time-dependent crack propagation phenomena.",
    "categories": [
      "cs.LG",
      "cond-mat.mtrl-sci",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "22 pages, 14 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.01976v2",
    "published_date": "2025-05-15 23:25:21 UTC",
    "updated_date": "2025-09-10 21:10:20 UTC"
  },
  {
    "arxiv_id": "2505.10749v1",
    "title": "Code-Driven Planning in Grid Worlds with Large Language Models",
    "authors": [
      "Ashwath Vaithinathan Aravindan",
      "Zhisheng Tang",
      "Mayank Kejriwal"
    ],
    "abstract": "We propose an iterative programmatic planning (IPP) framework for solving grid-based tasks by synthesizing interpretable agent policies expressed in code using large language models (LLMs). Instead of relying on traditional search or reinforcement learning, our approach uses code generation as policy synthesis, where the LLM outputs executable programs that map environment states to action sequences. Our proposed architecture incorporates several prompting strategies, including direct code generation, pseudocode-conditioned refinement, and curriculum-based prompting, but also includes an iterative refinement mechanism that updates code based on task performance feedback. We evaluate our approach using six leading LLMs and two challenging grid-based benchmarks (GRASP and MiniGrid). Our IPP framework demonstrates improvements over direct code generation ranging from 10\\% to as much as 10x across five of the six models and establishes a new state-of-the-art result for GRASP. IPP is found to significantly outperform direct elicitation of a solution from GPT-o3-mini (by 63\\% on MiniGrid to 116\\% on GRASP), demonstrating the viability of the overall approach. Computational costs of all code generation approaches are similar. While code generation has a higher initial prompting cost compared to direct solution elicitation (\\$0.08 per task vs. \\$0.002 per instance for GPT-o3-mini), the code can be reused for any number of instances, making the amortized cost significantly lower (by 400x on GPT-o3-mini across the complete GRASP benchmark).",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10749v1",
    "published_date": "2025-05-15 23:23:31 UTC",
    "updated_date": "2025-05-15 23:23:31 UTC"
  },
  {
    "arxiv_id": "2505.11557v2",
    "title": "AC-LoRA: (Almost) Training-Free Access Control-Aware Multi-Modal LLMs",
    "authors": [
      "Lara Magdalena Lazier",
      "Aritra Dhar",
      "Vasilije Stambolic",
      "Lukas Cavigelli"
    ],
    "abstract": "Corporate LLMs are gaining traction for efficient knowledge dissemination and management within organizations. However, as current LLMs are vulnerable to leaking sensitive information, it has proven difficult to apply them in settings where strict access control is necessary. To this end, we design AC-LoRA, an end-to-end system for access control-aware corporate LLM chatbots that maintains a strong information isolation guarantee. AC-LoRA maintains separate LoRA adapters for permissioned datasets, along with the document embedding they are finetuned on. AC-LoRA retrieves a precise set of LoRA adapters based on the similarity score with the user query and their permission. This similarity score is later used to merge the responses if more than one LoRA is retrieved, without requiring any additional training for LoRA routing. We provide an end-to-end prototype of AC-LoRA, evaluate it on two datasets, and show that AC-LoRA matches or even exceeds the performance of state-of-the-art LoRA mixing techniques while providing strong isolation guarantees. Furthermore, we show that AC-LoRA design can be directly applied to different modalities.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted in NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.11557v2",
    "published_date": "2025-05-15 23:19:35 UTC",
    "updated_date": "2025-10-08 10:01:30 UTC"
  },
  {
    "arxiv_id": "2505.10746v1",
    "title": "ChestyBot: Detecting and Disrupting Chinese Communist Party Influence Stratagems",
    "authors": [
      "Matthew Stoffolano",
      "Ayush Rout",
      "Justin M. Pelletier"
    ],
    "abstract": "Foreign information operations conducted by Russian and Chinese actors exploit the United States' permissive information environment. These campaigns threaten democratic institutions and the broader Westphalian model. Yet, existing detection and mitigation strategies often fail to identify active information campaigns in real time. This paper introduces ChestyBot, a pragmatics-based language model that detects unlabeled foreign malign influence tweets with up to 98.34% accuracy. The model supports a novel framework to disrupt foreign influence operations in their formative stages.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CR",
      "cs.SI"
    ],
    "primary_category": "cs.CY",
    "comment": "Presented at USCYBERCOM Cyber Recon Symposium 2023 at DreamPort in Columbia, MD on April 20, 2023",
    "pdf_url": "https://arxiv.org/pdf/2505.10746v1",
    "published_date": "2025-05-15 23:12:20 UTC",
    "updated_date": "2025-05-15 23:12:20 UTC"
  },
  {
    "arxiv_id": "2505.10742v2",
    "title": "Evaluations at Work: Measuring the Capabilities of GenAI in Use",
    "authors": [
      "Brandon Lepine",
      "Gawesha Weerantunga",
      "Juho Kim",
      "Pamela Mishkin",
      "Matthew Beane"
    ],
    "abstract": "Current AI benchmarks miss the messy, multi-turn nature of human-AI collaboration. We present an evaluation framework that decomposes real-world tasks into interdependent subtasks, letting us track both LLM performance and users' strategies across a dialogue. Complementing this framework, we develop a suite of metrics, including a composite usage derived from semantic similarity, word overlap, and numerical matches; structural coherence; intra-turn diversity; and a novel measure of the \"information frontier\" reflecting the alignment between AI outputs and users' working knowledge. We demonstrate our methodology in a financial valuation task that mirrors real-world complexity. Our empirical findings reveal that while greater integration of LLM-generated content generally enhances output quality, its benefits are moderated by factors such as response incoherence, excessive subtask diversity, and the distance of provided information from users' existing knowledge. These results suggest that proactive dialogue strategies designed to inject novelty may inadvertently undermine task performance. Our work thus advances a more holistic evaluation of human-AI collaboration, offering both a robust methodological framework and actionable insights for developing more effective AI-augmented work processes.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10742v2",
    "published_date": "2025-05-15 23:06:23 UTC",
    "updated_date": "2025-06-03 15:02:50 UTC"
  },
  {
    "arxiv_id": "2506.01975v3",
    "title": "An empirical study of task and feature correlations in the reuse of pre-trained models",
    "authors": [
      "Jama Hussein Mohamud",
      "Willie Brink"
    ],
    "abstract": "Pre-trained neural networks are commonly used and reused in the machine learning community. Alice trains a model for a particular task, and a part of her neural network is reused by Bob for a different task, often to great effect. To what can we ascribe Bob's success? This paper introduces an experimental setup through which factors contributing to Bob's empirical success could be studied in silico. As a result, we demonstrate that Bob might just be lucky: his task accuracy increases monotonically with the correlation between his task and Alice's. Even when Bob has provably uncorrelated tasks and input features from Alice's pre-trained network, he can achieve significantly better than random performance due to Alice's choice of network and optimizer. When there is little correlation between tasks, only reusing lower pre-trained layers is preferable, and we hypothesize the converse: that the optimal number of retrained layers is indicative of task and feature correlation. Finally, we show in controlled real-world scenarios that Bob can effectively reuse Alice's pre-trained network if there are semantic correlations between his and Alice's task.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.01975v3",
    "published_date": "2025-05-15 22:51:27 UTC",
    "updated_date": "2025-11-12 02:07:19 UTC"
  },
  {
    "arxiv_id": "2505.10732v1",
    "title": "Automating Security Audit Using Large Language Model based Agent: An Exploration Experiment",
    "authors": [
      "Jia Hui Chin",
      "Pu Zhang",
      "Yu Xin Cheong",
      "Jonathan Pan"
    ],
    "abstract": "In the current rapidly changing digital environment, businesses are under constant stress to ensure that their systems are secured. Security audits help to maintain a strong security posture by ensuring that policies are in place, controls are implemented, gaps are identified for cybersecurity risks mitigation. However, audits are usually manual, requiring much time and costs. This paper looks at the possibility of developing a framework to leverage Large Language Models (LLMs) as an autonomous agent to execute part of the security audit, namely with the field audit. password policy compliance for Windows operating system. Through the conduct of an exploration experiment of using GPT-4 with Langchain, the agent executed the audit tasks by accurately flagging password policy violations and appeared to be more efficient than traditional manual audits. Despite its potential limitations in operational consistency in complex and dynamic environment, the framework suggests possibilities to extend further to real-time threat monitoring and compliance checks.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10732v1",
    "published_date": "2025-05-15 22:22:52 UTC",
    "updated_date": "2025-05-15 22:22:52 UTC"
  },
  {
    "arxiv_id": "2505.10726v3",
    "title": "Learning Repetition-Invariant Representations for Polymer Informatics",
    "authors": [
      "Yihan Zhu",
      "Gang Liu",
      "Eric Inae",
      "Tengfei Luo",
      "Meng Jiang"
    ],
    "abstract": "Polymers are large macromolecules composed of repeating structural units known as monomers and are widely applied in fields such as energy storage, construction, medicine, and aerospace. However, existing graph neural network methods, though effective for small molecules, only model the single unit of polymers and fail to produce consistent vector representations for the true polymer structure with varying numbers of units. To address this challenge, we introduce Graph Repetition Invariance (GRIN), a novel method to learn polymer representations that are invariant to the number of repeating units in their graph representations. GRIN integrates a graph-based maximum spanning tree alignment with repeat-unit augmentation to ensure structural consistency. We provide theoretical guarantees for repetition-invariance from both model and data perspectives, demonstrating that three repeating units are the minimal augmentation required for optimal invariant representation learning. GRIN outperforms state-of-the-art baselines on both homopolymer and copolymer benchmarks, learning stable, repetition-invariant representations that generalize effectively to polymer chains of unseen sizes.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.10726v3",
    "published_date": "2025-05-15 22:05:40 UTC",
    "updated_date": "2026-01-03 17:28:45 UTC"
  },
  {
    "arxiv_id": "2505.10718v1",
    "title": "AI-enhanced semantic feature norms for 786 concepts",
    "authors": [
      "Siddharth Suresh",
      "Kushin Mukherjee",
      "Tyler Giallanza",
      "Xizheng Yu",
      "Mia Patil",
      "Jonathan D. Cohen",
      "Timothy T. Rogers"
    ],
    "abstract": "Semantic feature norms have been foundational in the study of human conceptual knowledge, yet traditional methods face trade-offs between concept/feature coverage and verifiability of quality due to the labor-intensive nature of norming studies. Here, we introduce a novel approach that augments a dataset of human-generated feature norms with responses from large language models (LLMs) while verifying the quality of norms against reliable human judgments. We find that our AI-enhanced feature norm dataset, NOVA: Norms Optimized Via AI, shows much higher feature density and overlap among concepts while outperforming a comparable human-only norm dataset and word-embedding models in predicting people's semantic similarity judgments. Taken together, we demonstrate that human conceptual knowledge is richer than captured in previous norm datasets and show that, with proper validation, LLMs can serve as powerful tools for cognitive science research.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.10718v1",
    "published_date": "2025-05-15 21:43:34 UTC",
    "updated_date": "2025-05-15 21:43:34 UTC"
  },
  {
    "arxiv_id": "2505.10717v2",
    "title": "A Modular Approach for Clinical SLMs Driven by Synthetic Data with Pre-Instruction Tuning, Model Merging, and Clinical-Tasks Alignment",
    "authors": [
      "Jean-Philippe Corbeil",
      "Amin Dada",
      "Jean-Michel Attendu",
      "Asma Ben Abacha",
      "Alessandro Sordoni",
      "Lucas Caccia",
      "François Beaulieu",
      "Thomas Lin",
      "Jens Kleesiek",
      "Paul Vozila"
    ],
    "abstract": "High computation costs and latency of large language models such as GPT-4 have limited their deployment in clinical settings. Small language models (SLMs) offer a cost-effective alternative, but their limited capacity requires biomedical domain adaptation, which remains challenging. An additional bottleneck is the unavailability and high sensitivity of clinical data. To address these challenges, we propose a novel framework for adapting SLMs into high-performing clinical models. We introduce the MediPhi collection of 3.8B-parameter SLMs developed with our novel framework: pre-instruction tuning of experts on relevant medical and clinical corpora (PMC, Medical Guideline, MedWiki, etc.), model merging, and clinical-tasks alignment. To cover most clinical tasks, we extended the CLUE benchmark to CLUE+, doubling its size. Our expert models deliver relative improvements on this benchmark over the base model without any task-specific fine-tuning: 64.3% on medical entities, 49.5% on radiology reports, and 44% on ICD-10 coding (outperforming GPT-4-0125 by 14%). We unify the expert models into MediPhi via model merging, preserving gains across benchmarks. Furthermore, we built the MediFlow collection, a synthetic dataset of 2.5 million high-quality instructions on 14 medical NLP tasks, 98 fine-grained document types, and JSON format support. Alignment of MediPhi using supervised fine-tuning and direct preference optimization achieves further gains of 18.9% on average.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10717v2",
    "published_date": "2025-05-15 21:40:21 UTC",
    "updated_date": "2025-05-21 17:36:21 UTC"
  },
  {
    "arxiv_id": "2505.10711v1",
    "title": "GNN-Suite: a Graph Neural Network Benchmarking Framework for Biomedical Informatics",
    "authors": [
      "Sebestyén Kamp",
      "Giovanni Stracquadanio",
      "T. Ian Simpson"
    ],
    "abstract": "We present GNN-Suite, a robust modular framework for constructing and benchmarking Graph Neural Network (GNN) architectures in computational biology. GNN-Suite standardises experimentation and reproducibility using the Nextflow workflow to evaluate GNN performance. We demonstrate its utility in identifying cancer-driver genes by constructing molecular networks from protein-protein interaction (PPI) data from STRING and BioGRID and annotating nodes with features from the PCAWG, PID, and COSMIC-CGC repositories.\n  Our design enables fair comparisons among diverse GNN architectures including GAT, GAT3H, GCN, GCN2, GIN, GTN, HGCN, PHGCN, and GraphSAGE and a baseline Logistic Regression (LR) model. All GNNs were configured as standardised two-layer models and trained with uniform hyperparameters (dropout = 0.2; Adam optimiser with learning rate = 0.01; and an adjusted binary cross-entropy loss to address class imbalance) over an 80/20 train-test split for 300 epochs. Each model was evaluated over 10 independent runs with different random seeds to yield statistically robust performance metrics, with balanced accuracy (BACC) as the primary measure. Notably, GCN2 achieved the highest BACC (0.807 +/- 0.035) on a STRING-based network, although all GNN types outperformed the LR baseline, highlighting the advantage of network-based learning over feature-only approaches.\n  Our results show that a common framework for implementing and evaluating GNN architectures aids in identifying not only the best model but also the most effective means of incorporating complementary data. By making GNN-Suite publicly available, we aim to foster reproducible research and promote improved benchmarking standards in computational biology. Future work will explore additional omics datasets and further refine network architectures to enhance predictive accuracy and interpretability in biomedical applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Main article 8 pages (20 in total with supplementary information included), 3 main article figures and 3 supplemental figures",
    "pdf_url": "https://arxiv.org/pdf/2505.10711v1",
    "published_date": "2025-05-15 21:14:30 UTC",
    "updated_date": "2025-05-15 21:14:30 UTC"
  },
  {
    "arxiv_id": "2505.10705v1",
    "title": "Embodied AI in Machine Learning -- is it Really Embodied?",
    "authors": [
      "Matej Hoffmann",
      "Shubhan Parag Patni"
    ],
    "abstract": "Embodied Artificial Intelligence (Embodied AI) is gaining momentum in the machine learning communities with the goal of leveraging current progress in AI (deep learning, transformers, large language and visual-language models) to empower robots. In this chapter we put this work in the context of \"Good Old-Fashioned Artificial Intelligence\" (GOFAI) (Haugeland, 1989) and the behavior-based or embodied alternatives (R. A. Brooks 1991; Pfeifer and Scheier 2001). We claim that the AI-powered robots are only weakly embodied and inherit some of the problems of GOFAI. Moreover, we review and critically discuss the possibility of cross-embodiment learning (Padalkar et al. 2024). We identify fundamental roadblocks and propose directions on how to make progress.",
    "categories": [
      "cs.AI",
      "cs.NE",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "16 pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.10705v1",
    "published_date": "2025-05-15 20:52:49 UTC",
    "updated_date": "2025-05-15 20:52:49 UTC"
  },
  {
    "arxiv_id": "2505.10695v1",
    "title": "Predicting Human Behavior in Autonomous Systems: A Collaborative Machine Teaching Approach for Reducing Transfer of Control Events",
    "authors": [
      "Julian Wolter",
      "Amr Gomaa"
    ],
    "abstract": "As autonomous systems become integral to various industries, effective strategies for fault handling are essential to ensure reliability and efficiency. Transfer of Control (ToC), a traditional approach for interrupting automated processes during faults, is often triggered unnecessarily in non-critical situations. To address this, we propose a data-driven method that uses human interaction data to train AI models capable of preemptively identifying and addressing issues or assisting users in resolution. Using an interactive tool simulating an industrial vacuum cleaner, we collected data and developed an LSTM-based model to predict user behavior. Our findings reveal that even data from non-experts can effectively train models to reduce unnecessary ToC events, enhancing the system's robustness. This approach highlights the potential of AI to learn directly from human problem-solving behaviors, complementing sensor data to improve industrial automation and human-AI collaboration.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10695v1",
    "published_date": "2025-05-15 20:34:29 UTC",
    "updated_date": "2025-05-15 20:34:29 UTC"
  },
  {
    "arxiv_id": "2505.10691v1",
    "title": "Predicting Risk of Pulmonary Fibrosis Formation in PASC Patients",
    "authors": [
      "Wanying Dou",
      "Gorkem Durak",
      "Koushik Biswas",
      "Ziliang Hong",
      "Andrea Mia Bejar",
      "Elif Keles",
      "Kaan Akin",
      "Sukru Mehmet Erturk",
      "Alpay Medetalibeyoglu",
      "Marc Sala",
      "Alexander Misharin",
      "Hatice Savas",
      "Mary Salvatore",
      "Sachin Jambawalikar",
      "Drew Torigian",
      "Jayaram K. Udupa",
      "Ulas Bagci"
    ],
    "abstract": "While the acute phase of the COVID-19 pandemic has subsided, its long-term effects persist through Post-Acute Sequelae of COVID-19 (PASC), commonly known as Long COVID. There remains substantial uncertainty regarding both its duration and optimal management strategies. PASC manifests as a diverse array of persistent or newly emerging symptoms--ranging from fatigue, dyspnea, and neurologic impairments (e.g., brain fog), to cardiovascular, pulmonary, and musculoskeletal abnormalities--that extend beyond the acute infection phase. This heterogeneous presentation poses substantial challenges for clinical assessment, diagnosis, and treatment planning. In this paper, we focus on imaging findings that may suggest fibrotic damage in the lungs, a critical manifestation characterized by scarring of lung tissue, which can potentially affect long-term respiratory function in patients with PASC. This study introduces a novel multi-center chest CT analysis framework that combines deep learning and radiomics for fibrosis prediction. Our approach leverages convolutional neural networks (CNNs) and interpretable feature extraction, achieving 82.2% accuracy and 85.5% AUC in classification tasks. We demonstrate the effectiveness of Grad-CAM visualization and radiomics-based feature analysis in providing clinically relevant insights for PASC-related lung fibrosis prediction. Our findings highlight the potential of deep learning-driven computational methods for early detection and risk assessment of PASC-related lung fibrosis--presented for the first time in the literature.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10691v1",
    "published_date": "2025-05-15 20:30:21 UTC",
    "updated_date": "2025-05-15 20:30:21 UTC"
  },
  {
    "arxiv_id": "2505.13497v3",
    "title": "Learning Hierarchical Domain Models Through Environment-Grounded Interaction",
    "authors": [
      "Claudius Kienle",
      "Benjamin Alt",
      "Oleg Arenz",
      "Jan Peters"
    ],
    "abstract": "Domain models enable autonomous agents to solve long-horizon tasks by producing interpretable plans. However, in open-world environments, a single general domain model cannot capture the variety of tasks, so agents must generate suitable task-specific models on the fly. Large Language Models (LLMs), with their implicit common knowledge, can generate such domains, but suffer from high error rates that limit their applicability. Hence, related work relies on extensive human feed-back or prior knowledge, which undermines autonomous, open-world deployment. In this work, we propose LODGE, a framework for autonomous domain learning from LLMs and environment grounding. LODGE builds on hierarchical abstractions and automated simulations to identify and correct inconsistencies between abstraction layers and between the model and environment. Our framework is task-agnostic, as it generates predicates, operators, and their preconditions and effects, while only assuming access to a simulator and a set of generic, executable low-level skills. Experiments on two International Planning Competition ( IPC) domains and a robotic assembly domain show that LODGE yields more accurate domain models and higher task success than existing methods, requiring remarkably few environment interactions and no human feedback or demonstrations.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.13497v3",
    "published_date": "2025-05-15 20:23:21 UTC",
    "updated_date": "2025-10-01 12:01:27 UTC"
  },
  {
    "arxiv_id": "2505.10681v1",
    "title": "Towards an LLM-powered Social Digital Twinning Platform",
    "authors": [
      "Önder Gürcan",
      "Vanja Falck",
      "Markus G. Rousseau",
      "Larissa L. Lima"
    ],
    "abstract": "We present Social Digital Twinner, an innovative social simulation tool for exploring plausible effects of what-if scenarios in complex adaptive social systems. The architecture is composed of three seamlessly integrated parts: a data infrastructure featuring real-world data and a multi-dimensionally representative synthetic population of citizens, an LLM-enabled agent-based simulation engine, and a user interface that enable intuitive, natural language interactions with the simulation engine and the artificial agents (i.e. citizens). Social Digital Twinner facilitates real-time engagement and empowers stakeholders to collaboratively design, test, and refine intervention measures. The approach is promoting a data-driven and evidence-based approach to societal problem-solving. We demonstrate the tool's interactive capabilities by addressing the critical issue of youth school dropouts in Kragero, Norway, showcasing its ability to create and execute a dedicated social digital twin using natural language.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "13 pages, 3 figures, 23rd International Conference on Practical applications of Agents and Multi-Agent Systems (PAAMS 2025)",
    "pdf_url": "https://arxiv.org/pdf/2505.10681v1",
    "published_date": "2025-05-15 19:58:50 UTC",
    "updated_date": "2025-05-15 19:58:50 UTC"
  },
  {
    "arxiv_id": "2506.00009v1",
    "title": "MolTextNet: A Two-Million Molecule-Text Dataset for Multimodal Molecular Learning",
    "authors": [
      "Yihan Zhu",
      "Gang Liu",
      "Eric Inae",
      "Meng Jiang"
    ],
    "abstract": "Small molecules are essential to drug discovery, and graph-language models hold promise for learning molecular properties and functions from text. However, existing molecule-text datasets are limited in scale and informativeness, restricting the training of generalizable multimodal models. We present MolTextNet, a dataset of 2.5 million high-quality molecule-text pairs designed to overcome these limitations. To construct it, we propose a synthetic text generation pipeline that integrates structural features, computed properties, bioactivity data, and synthetic complexity. Using GPT-4o-mini, we create structured descriptions for 2.5 million molecules from ChEMBL35, with text over 10 times longer than prior datasets. MolTextNet supports diverse downstream tasks, including property prediction and structure retrieval. Pretraining CLIP-style models with Graph Neural Networks and ModernBERT on MolTextNet yields improved performance, highlighting its potential for advancing foundational multimodal modeling in molecular science. Our dataset is available at https://huggingface.co/datasets/liuganghuggingface/moltextnet.",
    "categories": [
      "q-bio.BM",
      "cs.AI"
    ],
    "primary_category": "q-bio.BM",
    "comment": "21 pages, 13 figures, 10 tables",
    "pdf_url": "https://arxiv.org/pdf/2506.00009v1",
    "published_date": "2025-05-15 19:50:11 UTC",
    "updated_date": "2025-05-15 19:50:11 UTC"
  },
  {
    "arxiv_id": "2505.17048v2",
    "title": "Words That Unite The World: A Unified Framework for Deciphering Central Bank Communications Globally",
    "authors": [
      "Agam Shah",
      "Siddhant Sukhani",
      "Huzaifa Pardawala",
      "Saketh Budideti",
      "Riya Bhadani",
      "Rudra Gopal",
      "Siddhartha Somani",
      "Rutwik Routu",
      "Michael Galarnyk",
      "Soungmin Lee",
      "Arnav Hiray",
      "Akshar Ravichandran",
      "Eric Kim",
      "Pranav Aluru",
      "Joshua Zhang",
      "Sebastian Jaskowski",
      "Veer Guda",
      "Meghaj Tarte",
      "Liqin Ye",
      "Spencer Gosden",
      "Rachel Yuh",
      "Sloka Chava",
      "Sahasra Chava",
      "Dylan Patrick Kelly",
      "Aiden Chiang",
      "Harsit Mittal",
      "Sudheer Chava"
    ],
    "abstract": "Central banks around the world play a crucial role in maintaining economic stability. Deciphering policy implications in their communications is essential, especially as misinterpretations can disproportionately impact vulnerable populations. To address this, we introduce the World Central Banks (WCB) dataset, the most comprehensive monetary policy corpus to date, comprising over 380k sentences from 25 central banks across diverse geographic regions, spanning 28 years of historical data. After uniformly sampling 1k sentences per bank (25k total) across all available years, we annotate and review each sentence using dual annotators, disagreement resolutions, and secondary expert reviews. We define three tasks: Stance Detection, Temporal Classification, and Uncertainty Estimation, with each sentence annotated for all three. We benchmark seven Pretrained Language Models (PLMs) and nine Large Language Models (LLMs) (Zero-Shot, Few-Shot, and with annotation guide) on these tasks, running 15,075 benchmarking experiments. We find that a model trained on aggregated data across banks significantly surpasses a model trained on an individual bank's data, confirming the principle \"the whole is greater than the sum of its parts.\" Additionally, rigorous human evaluations, error analyses, and predictive tasks validate our framework's economic utility. Our artifacts are accessible through the HuggingFace and GitHub under the CC-BY-NC-SA 4.0 license.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "q-fin.CP",
      "q-fin.GN"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at NeurIPS 2025 (main conference)",
    "pdf_url": "https://arxiv.org/pdf/2505.17048v2",
    "published_date": "2025-05-15 19:49:20 UTC",
    "updated_date": "2025-11-01 21:51:13 UTC"
  },
  {
    "arxiv_id": "2505.10677v1",
    "title": "A Conformal Predictive Measure for Assessing Catastrophic Forgetting",
    "authors": [
      "Ioannis Pitsiorlas",
      "Nour Jamoussi",
      "Marios Kountouris"
    ],
    "abstract": "This work introduces a novel methodology for assessing catastrophic forgetting (CF) in continual learning. We propose a new conformal prediction (CP)-based metric, termed the Conformal Prediction Confidence Factor (CPCF), to quantify and evaluate CF effectively. Our framework leverages adaptive CP to estimate forgetting by monitoring the model's confidence on previously learned tasks. This approach provides a dynamic and practical solution for monitoring and measuring CF of previous tasks as new ones are introduced, offering greater suitability for real-world applications. Experimental results on four benchmark datasets demonstrate a strong correlation between CPCF and the accuracy of previous tasks, validating the reliability and interpretability of the proposed metric. Our results highlight the potential of CPCF as a robust and effective tool for assessing and understanding CF in dynamic learning environments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10677v1",
    "published_date": "2025-05-15 19:42:17 UTC",
    "updated_date": "2025-05-15 19:42:17 UTC"
  },
  {
    "arxiv_id": "2505.11556v2",
    "title": "HiddenBench: Assessing Collective Reasoning in Multi-Agent LLMs via Hidden Profile Tasks",
    "authors": [
      "Yuxuan Li",
      "Aoi Naito",
      "Hirokazu Shirado"
    ],
    "abstract": "Multi-agent systems built on large language models (LLMs) promise enhanced problem-solving through distributed information integration, but may also replicate collective reasoning failures observed in human groups. Yet the absence of a theory-grounded benchmark makes it difficult to systematically evaluate and improve such reasoning. We introduce HiddenBench, the first benchmark for evaluating collective reasoning in multi-agent LLMs. It builds on the Hidden Profile paradigm from social psychology, where individuals each hold asymmetric pieces of information and must communicate to reach the correct decision. To ground the benchmark, we formalize the paradigm with custom tasks and show that GPT-4.1 groups fail to integrate distributed knowledge, exhibiting human-like collective reasoning failures that persist even with varied prompting strategies. We then construct the full benchmark, spanning 65 tasks drawn from custom designs, prior human studies, and automatic generation. Evaluating 15 LLMs across four model families, HiddenBench exposes persistent limitations while also providing comparative insights: some models (e.g., Gemini-2.5-Flash/Pro) achieve higher performance, yet scale and reasoning are not reliable indicators of stronger collective reasoning. Our work delivers the first reproducible benchmark for collective reasoning in multi-agent LLMs, offering diagnostic insight and a foundation for future research on artificial collective intelligence.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11556v2",
    "published_date": "2025-05-15 19:22:54 UTC",
    "updated_date": "2025-09-25 19:30:07 UTC"
  },
  {
    "arxiv_id": "2505.10670v1",
    "title": "Interpretable Risk Mitigation in LLM Agent Systems",
    "authors": [
      "Jan Chojnacki"
    ],
    "abstract": "Autonomous agents powered by large language models (LLMs) enable novel use cases in domains where responsible action is increasingly important. Yet the inherent unpredictability of LLMs raises safety concerns about agent reliability. In this work, we explore agent behaviour in a toy, game-theoretic environment based on a variation of the Iterated Prisoner's Dilemma. We introduce a strategy-modification method-independent of both the game and the prompt-by steering the residual stream with interpretable features extracted from a sparse autoencoder latent space. Steering with the good-faith negotiation feature lowers the average defection probability by 28 percentage points. We also identify feasible steering ranges for several open-source LLM agents. Finally, we hypothesise that game-theoretic evaluation of LLM agents, combined with representation-steering alignment, can generalise to real-world applications on end-user devices and embodied platforms.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.GT"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10670v1",
    "published_date": "2025-05-15 19:22:11 UTC",
    "updated_date": "2025-05-15 19:22:11 UTC"
  },
  {
    "arxiv_id": "2505.10665v1",
    "title": "Seasonal Forecasting of Pan-Arctic Sea Ice with State Space Model",
    "authors": [
      "Wei Wang",
      "Weidong Yang",
      "Lei Wang",
      "Guihua Wang",
      "Ruibo Lei"
    ],
    "abstract": "The rapid decline of Arctic sea ice resulting from anthropogenic climate change poses significant risks to indigenous communities, ecosystems, and the global climate system. This situation emphasizes the immediate necessity for precise seasonal sea ice forecasts. While dynamical models perform well for short-term forecasts, they encounter limitations in long-term forecasts and are computationally intensive. Deep learning models, while more computationally efficient, often have difficulty managing seasonal variations and uncertainties when dealing with complex sea ice dynamics. In this research, we introduce IceMamba, a deep learning architecture that integrates sophisticated attention mechanisms within the state space model. Through comparative analysis of 25 renowned forecast models, including dynamical, statistical, and deep learning approaches, our experimental results indicate that IceMamba delivers excellent seasonal forecasting capabilities for Pan-Arctic sea ice concentration. Specifically, IceMamba outperforms all tested models regarding average RMSE and anomaly correlation coefficient (ACC) and ranks second in Integrated Ice Edge Error (IIEE). This innovative approach enhances our ability to foresee and alleviate the effects of sea ice variability, offering essential insights for strategies aimed at climate adaptation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper is published in npj Climate and Atmospheric Science: https://www.nature.com/articles/s41612-025-01058-0#Sec16 Supplementary information: https://static-content.springer.com/esm/art%3A10.1038%2Fs41612-025-01058-0/MediaObjects/41612_2025_1058_MOESM1_ESM.pdf",
    "pdf_url": "https://arxiv.org/pdf/2505.10665v1",
    "published_date": "2025-05-15 19:15:00 UTC",
    "updated_date": "2025-05-15 19:15:00 UTC"
  },
  {
    "arxiv_id": "2505.10664v1",
    "title": "CLIP Embeddings for AI-Generated Image Detection: A Few-Shot Study with Lightweight Classifier",
    "authors": [
      "Ziyang Ou"
    ],
    "abstract": "Verifying the authenticity of AI-generated images presents a growing challenge on social media platforms these days. While vision-language models (VLMs) like CLIP outdo in multimodal representation, their capacity for AI-generated image classification is underexplored due to the absence of such labels during the pre-training process. This work investigates whether CLIP embeddings inherently contain information indicative of AI generation. A proposed pipeline extracts visual embeddings using a frozen CLIP model, feeds its embeddings to lightweight networks, and fine-tunes only the final classifier. Experiments on the public CIFAKE benchmark show the performance reaches 95% accuracy without language reasoning. Few-shot adaptation to curated custom with 20% of the data results in performance to 85%. A closed-source baseline (Gemini-2.0) has the best zero-shot accuracy yet fails on specific styles. Notably, some specific image types, such as wide-angle photographs and oil paintings, pose significant challenges to classification. These results indicate previously unexplored difficulties in classifying certain types of AI-generated images, revealing new and more specific questions in this domain that are worth further investigation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 5 figures, not submitted to any conference",
    "pdf_url": "https://arxiv.org/pdf/2505.10664v1",
    "published_date": "2025-05-15 19:14:39 UTC",
    "updated_date": "2025-05-15 19:14:39 UTC"
  },
  {
    "arxiv_id": "2505.10653v1",
    "title": "On the Evaluation of Engineering Artificial General Intelligence",
    "authors": [
      "Sandeep Neema",
      "Susmit Jha",
      "Adam Nagel",
      "Ethan Lew",
      "Chandrasekar Sureshkumar",
      "Aleksa Gordic",
      "Chase Shimmin",
      "Hieu Nguygen",
      "Paul Eremenko"
    ],
    "abstract": "We discuss the challenges and propose a framework for evaluating engineering artificial general intelligence (eAGI) agents. We consider eAGI as a specialization of artificial general intelligence (AGI), deemed capable of addressing a broad range of problems in the engineering of physical systems and associated controllers. We exclude software engineering for a tractable scoping of eAGI and expect dedicated software engineering AI agents to address the software implementation challenges. Similar to human engineers, eAGI agents should possess a unique blend of background knowledge (recall and retrieve) of facts and methods, demonstrate familiarity with tools and processes, exhibit deep understanding of industrial components and well-known design families, and be able to engage in creative problem solving (analyze and synthesize), transferring ideas acquired in one context to another. Given this broad mandate, evaluating and qualifying the performance of eAGI agents is a challenge in itself and, arguably, a critical enabler to developing eAGI agents. In this paper, we address this challenge by proposing an extensible evaluation framework that specializes and grounds Bloom's taxonomy - a framework for evaluating human learning that has also been recently used for evaluating LLMs - in an engineering design context. Our proposed framework advances the state of the art in benchmarking and evaluation of AI agents in terms of the following: (a) developing a rich taxonomy of evaluation questions spanning from methodological knowledge to real-world design problems; (b) motivating a pluggable evaluation framework that can evaluate not only textual responses but also evaluate structured design artifacts such as CAD models and SysML models; and (c) outlining an automatable procedure to customize the evaluation benchmark to different engineering contexts.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "21 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.10653v1",
    "published_date": "2025-05-15 18:52:47 UTC",
    "updated_date": "2025-05-15 18:52:47 UTC"
  },
  {
    "arxiv_id": "2505.10643v2",
    "title": "Artificial Intelligence Bias on English Language Learners in Automatic Scoring",
    "authors": [
      "Shuchen Guo",
      "Yun Wang",
      "Jichao Yu",
      "Xuansheng Wu",
      "Bilgehan Ayik",
      "Field M. Watts",
      "Ehsan Latif",
      "Ninghao Liu",
      "Lei Liu",
      "Xiaoming Zhai"
    ],
    "abstract": "This study investigated potential scoring biases and disparities toward English Language Learners (ELLs) when using automatic scoring systems for middle school students' written responses to science assessments. We specifically focus on examining how unbalanced training data with ELLs contributes to scoring bias and disparities. We fine-tuned BERT with four datasets: responses from (1) ELLs, (2) non-ELLs, (3) a mixed dataset reflecting the real-world proportion of ELLs and non-ELLs (unbalanced), and (4) a balanced mixed dataset with equal representation of both groups. The study analyzed 21 assessment items: 10 items with about 30,000 ELL responses, five items with about 1,000 ELL responses, and six items with about 200 ELL responses. Scoring accuracy (Acc) was calculated and compared to identify bias using Friedman tests. We measured the Mean Score Gaps (MSGs) between ELLs and non-ELLs and then calculated the differences in MSGs generated through both the human and AI models to identify the scoring disparities. We found that no AI bias and distorted disparities between ELLs and non-ELLs were found when the training dataset was large enough (ELL = 30,000 and ELL = 1,000), but concerns could exist if the sample size is limited (ELL = 200).",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10643v2",
    "published_date": "2025-05-15 18:32:24 UTC",
    "updated_date": "2025-05-19 21:42:42 UTC"
  },
  {
    "arxiv_id": "2505.10640v2",
    "title": "The Hitchhikers Guide to Production-ready Trustworthy Foundation Model powered Software (FMware)",
    "authors": [
      "Kirill Vasilevski",
      "Benjamin Rombaut",
      "Gopi Krishnan Rajbahadur",
      "Gustavo A. Oliva",
      "Keheliya Gallaba",
      "Filipe R. Cogo",
      "Jiahuei Lin",
      "Dayi Lin",
      "Haoxiang Zhang",
      "Bouyan Chen",
      "Kishanthan Thangarajah",
      "Ahmed E. Hassan",
      "Zhen Ming Jiang"
    ],
    "abstract": "Foundation Models (FMs) such as Large Language Models (LLMs) are reshaping the software industry by enabling FMware, systems that integrate these FMs as core components. In this KDD 2025 tutorial, we present a comprehensive exploration of FMware that combines a curated catalogue of challenges with real-world production concerns. We first discuss the state of research and practice in building FMware. We further examine the difficulties in selecting suitable models, aligning high-quality domain-specific data, engineering robust prompts, and orchestrating autonomous agents. We then address the complex journey from impressive demos to production-ready systems by outlining issues in system testing, optimization, deployment, and integration with legacy software. Drawing on our industrial experience and recent research in the area, we provide actionable insights and a technology roadmap for overcoming these challenges. Attendees will gain practical strategies to enable the creation of trustworthy FMware in the evolving technology landscape.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10640v2",
    "published_date": "2025-05-15 18:22:45 UTC",
    "updated_date": "2025-06-02 20:08:34 UTC"
  },
  {
    "arxiv_id": "2505.10559v1",
    "title": "Neural Thermodynamic Laws for Large Language Model Training",
    "authors": [
      "Ziming Liu",
      "Yizhou Liu",
      "Jeff Gore",
      "Max Tegmark"
    ],
    "abstract": "Beyond neural scaling laws, little is known about the laws underlying large language models (LLMs). We introduce Neural Thermodynamic Laws (NTL) -- a new framework that offers fresh insights into LLM training dynamics. On the theoretical side, we demonstrate that key thermodynamic quantities (e.g., temperature, entropy, heat capacity, thermal conduction) and classical thermodynamic principles (e.g., the three laws of thermodynamics and the equipartition theorem) naturally emerge under river-valley loss landscape assumptions. On the practical side, this scientific perspective yields intuitive guidelines for designing learning rate schedules.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.data-an",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "18 pages, 10 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.10559v1",
    "published_date": "2025-05-15 17:59:22 UTC",
    "updated_date": "2025-05-15 17:59:22 UTC"
  },
  {
    "arxiv_id": "2505.10557v1",
    "title": "MathCoder-VL: Bridging Vision and Code for Enhanced Multimodal Mathematical Reasoning",
    "authors": [
      "Ke Wang",
      "Junting Pan",
      "Linda Wei",
      "Aojun Zhou",
      "Weikang Shi",
      "Zimu Lu",
      "Han Xiao",
      "Yunqiao Yang",
      "Houxing Ren",
      "Mingjie Zhan",
      "Hongsheng Li"
    ],
    "abstract": "Natural language image-caption datasets, widely used for training Large Multimodal Models, mainly focus on natural scenarios and overlook the intricate details of mathematical figures that are critical for problem-solving, hindering the advancement of current LMMs in multimodal mathematical reasoning. To this end, we propose leveraging code as supervision for cross-modal alignment, since code inherently encodes all information needed to generate corresponding figures, establishing a precise connection between the two modalities. Specifically, we co-develop our image-to-code model and dataset with model-in-the-loop approach, resulting in an image-to-code model, FigCodifier and ImgCode-8.6M dataset, the largest image-code dataset to date. Furthermore, we utilize FigCodifier to synthesize novel mathematical figures and then construct MM-MathInstruct-3M, a high-quality multimodal math instruction fine-tuning dataset. Finally, we present MathCoder-VL, trained with ImgCode-8.6M for cross-modal alignment and subsequently fine-tuned on MM-MathInstruct-3M for multimodal math problem solving. Our model achieves a new open-source SOTA across all six metrics. Notably, it surpasses GPT-4o and Claude 3.5 Sonnet in the geometry problem-solving subset of MathVista, achieving improvements of 8.9% and 9.2%. The dataset and models will be released at https://github.com/mathllm/MathCoder.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to ACL 2025 Findings",
    "pdf_url": "https://arxiv.org/pdf/2505.10557v1",
    "published_date": "2025-05-15 17:59:21 UTC",
    "updated_date": "2025-05-15 17:59:21 UTC"
  },
  {
    "arxiv_id": "2505.10551v1",
    "title": "Does Feasibility Matter? Understanding the Impact of Feasibility on Synthetic Training Data",
    "authors": [
      "Yiwen Liu",
      "Jessica Bader",
      "Jae Myung Kim"
    ],
    "abstract": "With the development of photorealistic diffusion models, models trained in part or fully on synthetic data achieve progressively better results. However, diffusion models still routinely generate images that would not exist in reality, such as a dog floating above the ground or with unrealistic texture artifacts. We define the concept of feasibility as whether attributes in a synthetic image could realistically exist in the real-world domain; synthetic images containing attributes that violate this criterion are considered infeasible. Intuitively, infeasible images are typically considered out-of-distribution; thus, training on such images is expected to hinder a model's ability to generalize to real-world data, and they should therefore be excluded from the training set whenever possible. However, does feasibility really matter? In this paper, we investigate whether enforcing feasibility is necessary when generating synthetic training data for CLIP-based classifiers, focusing on three target attributes: background, color, and texture. We introduce VariReal, a pipeline that minimally edits a given source image to include feasible or infeasible attributes given by the textual prompt generated by a large language model. Our experiments show that feasibility minimally affects LoRA-fine-tuned CLIP performance, with mostly less than 0.3% difference in top-1 accuracy across three fine-grained datasets. Also, the attribute matters on whether the feasible/infeasible images adversarially influence the classification performance. Finally, mixing feasible and infeasible images in training datasets does not significantly impact performance compared to using purely feasible or infeasible datasets.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPRW 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.10551v1",
    "published_date": "2025-05-15 17:57:38 UTC",
    "updated_date": "2025-05-15 17:57:38 UTC"
  },
  {
    "arxiv_id": "2505.10547v2",
    "title": "Real-Time Out-of-Distribution Failure Prevention via Multi-Modal Reasoning",
    "authors": [
      "Milan Ganai",
      "Rohan Sinha",
      "Christopher Agia",
      "Daniel Morton",
      "Luigi Di Lillo",
      "Marco Pavone"
    ],
    "abstract": "While foundation models offer promise toward improving robot safety in out-of-distribution (OOD) scenarios, how to effectively harness their generalist knowledge for real-time, dynamically feasible response remains a crucial problem. We present FORTRESS, a joint reasoning and planning framework that generates semantically safe fallback strategies to prevent safety-critical, OOD failures. At a low frequency under nominal operation, FORTRESS uses multi-modal foundation models to anticipate possible failure modes and identify safe fallback sets. When a runtime monitor triggers a fallback response, FORTRESS rapidly synthesizes plans to fallback goals while inferring and avoiding semantically unsafe regions in real time. By bridging open-world, multi-modal reasoning with dynamics-aware planning, we eliminate the need for hard-coded fallbacks and human safety interventions. FORTRESS outperforms on-the-fly prompting of slow reasoning models in safety classification accuracy on synthetic benchmarks and real-world ANYmal robot data, and further improves system safety and planning success in simulation and on quadrotor hardware for urban navigation. Website can be found at https://milanganai.github.io/fortress.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Conference on Robot Learning (CoRL) 2025 (Oral)",
    "pdf_url": "https://arxiv.org/pdf/2505.10547v2",
    "published_date": "2025-05-15 17:55:28 UTC",
    "updated_date": "2025-09-25 05:51:50 UTC"
  },
  {
    "arxiv_id": "2505.10543v2",
    "title": "Reasoning Capabilities of Large Language Models on Dynamic Tasks",
    "authors": [
      "Annie Wong",
      "Thomas Bäck",
      "Aske Plaat",
      "Niki van Stein",
      "Anna V. Kononova"
    ],
    "abstract": "Large language models excel on static benchmarks, but their ability as self-learning agents in dynamic environments remains unclear. We evaluate three prompting strategies: self-reflection, heuristic mutation, and planning across dynamic tasks with open-source models. We find that larger models generally outperform smaller ones, but that strategic prompting can close this performance gap. Second, an overly long prompt can negatively impact smaller models on basic reactive tasks, while larger models show more robust behaviour. Third, advanced prompting techniques primarily benefit smaller models on complex games, but offer less improvement for already high-performing large language models. Yet, we find that advanced reasoning methods yield highly variable outcomes: while capable of significantly improving performance when reasoning and decision-making align, they also introduce instability and can lead to big performance drops. Compared to human performance, our findings reveal little evidence of true emergent reasoning. Instead, large language model performance exhibits persistent limitations in areas like planning and spatial coordination, suggesting that large language models still suffer fundamental shortcomings that may not be fully overcome through self-reflective prompting alone. Reasoning is a multi-faceted task, and while methods like Chain-of-thought improve multi-step reasoning on math word problems, our findings using dynamic benchmarks highlight important shortcomings in general reasoning capabilities, indicating a need to move beyond static benchmarks to capture the complexity of reasoning.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10543v2",
    "published_date": "2025-05-15 17:53:47 UTC",
    "updated_date": "2025-08-10 18:28:40 UTC"
  },
  {
    "arxiv_id": "2505.10609v1",
    "title": "Agent Name Service (ANS): A Universal Directory for Secure AI Agent Discovery and Interoperability",
    "authors": [
      "Ken Huang",
      "Vineeth Sai Narajala",
      "Idan Habler",
      "Akram Sheriff"
    ],
    "abstract": "The proliferation of AI agents requires robust mechanisms for secure discovery. This paper introduces the Agent Name Service (ANS), a novel architecture based on DNS addressing the lack of a public agent discovery framework. ANS provides a protocol-agnostic registry infrastructure that leverages Public Key Infrastructure (PKI) certificates for verifiable agent identity and trust. The architecture features several key innovations: a formalized agent registration and renewal mechanism for lifecycle management; DNS-inspired naming conventions with capability-aware resolution; a modular Protocol Adapter Layer supporting diverse communication standards (A2A, MCP, ACP etc.); and precisely defined algorithms for secure resolution. We implement structured communication using JSON Schema and conduct a comprehensive threat analysis of our proposal. The result is a foundational directory service addressing the core challenges of secured discovery and interaction in multi-agent systems, paving the way for future interoperable, trustworthy, and scalable agent ecosystems.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.MA",
      "cs.NI"
    ],
    "primary_category": "cs.CR",
    "comment": "15 pages, 6 figures, 6 code listings, Supported and endorsed by OWASP GenAI ASI Project",
    "pdf_url": "https://arxiv.org/pdf/2505.10609v1",
    "published_date": "2025-05-15 17:49:36 UTC",
    "updated_date": "2025-05-15 17:49:36 UTC"
  },
  {
    "arxiv_id": "2505.10537v3",
    "title": "LibIQ: Toward Real-Time Spectrum Classification in O-RAN dApps",
    "authors": [
      "Filippo Olimpieri",
      "Noemi Giustini",
      "Andrea Lacava",
      "Salvatore D'Oro",
      "Tommaso Melodia",
      "Francesca Cuomo"
    ],
    "abstract": "The O-RAN architecture is transforming cellular networks by adopting RAN softwarization and disaggregation concepts to enable data-driven monitoring and control of the network. Such management is enabled by RICs, which facilitate near-real-time and non-real-time network control through xApps and rApps. However, they face limitations, including latency overhead in data exchange between the RAN and RIC, restricting real-time monitoring, and the inability to access user plain data due to privacy and security constraints, hindering use cases like beamforming and spectrum classification. In this paper, we leverage the dApps concept to enable real-time RF spectrum classification with LibIQ, a novel library for RF signals that facilitates efficient spectrum monitoring and signal classification by providing functionalities to read I/Q samples as time-series, create datasets and visualize time-series data through plots and spectrograms. Thanks to LibIQ, I/Q samples can be efficiently processed to detect external RF signals, which are subsequently classified using a CNN inside the library. To achieve accurate spectrum analysis, we created an extensive dataset of time-series-based I/Q samples, representing distinct signal types captured using a custom dApp running on a 5G deployment over the Colosseum network emulator and an OTA testbed. We evaluate our model by deploying LibIQ in heterogeneous scenarios with varying center frequencies, time windows, and external RF signals. In real-time analysis, the model classifies the processed I/Q samples, achieving an average accuracy of approximately 97.8% in identifying signal types across all scenarios. We pledge to release both LibIQ and the dataset created as a publicly available framework upon acceptance.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "6 pages, 5 figures, 2 tables",
    "pdf_url": "https://arxiv.org/pdf/2505.10537v3",
    "published_date": "2025-05-15 17:47:30 UTC",
    "updated_date": "2025-12-11 11:18:01 UTC"
  },
  {
    "arxiv_id": "2505.13496v1",
    "title": "ADALog: Adaptive Unsupervised Anomaly detection in Logs with Self-attention Masked Language Model",
    "authors": [
      "Przemek Pospieszny",
      "Wojciech Mormul",
      "Karolina Szyndler",
      "Sanjeev Kumar"
    ],
    "abstract": "Modern software systems generate extensive heterogeneous log data with dynamic formats, fragmented event sequences, and varying temporal patterns, making anomaly detection both crucial and challenging. To address these complexities, we propose ADALog, an adaptive, unsupervised anomaly detection framework designed for practical applicability across diverse real-world environments. Unlike traditional methods reliant on log parsing, strict sequence dependencies, or labeled data, ADALog operates on individual unstructured logs, extracts intra-log contextual relationships, and performs adaptive thresholding on normal data. The proposed approach utilizes a transformer-based, pretrained bidirectional encoder with a masked language modeling task, fine-tuned on normal logs to capture domain-specific syntactic and semantic patterns essential for accurate anomaly detection. Anomalies are identified via token-level reconstruction probabilities, aggregated into log-level scores, with adaptive percentile-based thresholding calibrated only on normal data. This allows the model to dynamically adapt to evolving system behaviors while avoiding rigid, heuristic-based thresholds common in traditional systems. We evaluate ADALog on benchmark datasets BGL, Thunderbird, and Spirit, showing strong generalization and competitive performance compared to state-of-the-art supervised and unsupervised methods. Additional ablation studies examine the effects of masking, fine-tuning, and token positioning on model behavior and interpretability.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Conference paper accepted at ICMLT 2025; to appear in the IEEE Conference Proceedings",
    "pdf_url": "https://arxiv.org/pdf/2505.13496v1",
    "published_date": "2025-05-15 17:31:40 UTC",
    "updated_date": "2025-05-15 17:31:40 UTC"
  },
  {
    "arxiv_id": "2505.10522v1",
    "title": "Knowledge capture, adaptation and composition (KCAC): A framework for cross-task curriculum learning in robotic manipulation",
    "authors": [
      "Xinrui Wang",
      "Yan Jin"
    ],
    "abstract": "Reinforcement learning (RL) has demonstrated remarkable potential in robotic manipulation but faces challenges in sample inefficiency and lack of interpretability, limiting its applicability in real world scenarios. Enabling the agent to gain a deeper understanding and adapt more efficiently to diverse working scenarios is crucial, and strategic knowledge utilization is a key factor in this process. This paper proposes a Knowledge Capture, Adaptation, and Composition (KCAC) framework to systematically integrate knowledge transfer into RL through cross-task curriculum learning. KCAC is evaluated using a two block stacking task in the CausalWorld benchmark, a complex robotic manipulation environment. To our knowledge, existing RL approaches fail to solve this task effectively, reflecting deficiencies in knowledge capture. In this work, we redesign the benchmark reward function by removing rigid constraints and strict ordering, allowing the agent to maximize total rewards concurrently and enabling flexible task completion. Furthermore, we define two self-designed sub-tasks and implement a structured cross-task curriculum to facilitate efficient learning. As a result, our KCAC approach achieves a 40 percent reduction in training time while improving task success rates by 10 percent compared to traditional RL methods. Through extensive evaluation, we identify key curriculum design parameters subtask selection, transition timing, and learning rate that optimize learning efficiency and provide conceptual guidance for curriculum based RL frameworks. This work offers valuable insights into curriculum design in RL and robotic learning.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10522v1",
    "published_date": "2025-05-15 17:30:29 UTC",
    "updated_date": "2025-05-15 17:30:29 UTC"
  },
  {
    "arxiv_id": "2505.10518v1",
    "title": "Multi-Token Prediction Needs Registers",
    "authors": [
      "Anastasios Gerontopoulos",
      "Spyros Gidaris",
      "Nikos Komodakis"
    ],
    "abstract": "Multi-token prediction has emerged as a promising objective for improving language model pretraining, but its benefits have not consistently generalized to other settings such as fine-tuning. In this paper, we propose MuToR, a simple and effective approach to multi-token prediction that interleaves learnable register tokens into the input sequence, each tasked with predicting future targets. Compared to existing methods, MuToR offers several key advantages: it introduces only a negligible number of additional parameters, requires no architectural changes--ensuring compatibility with off-the-shelf pretrained language models--and remains aligned with the next-token pretraining objective, making it especially well-suited for supervised fine-tuning. Moreover, it naturally supports scalable prediction horizons. We demonstrate the effectiveness and versatility of MuToR across a range of use cases, including supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and pretraining, on challenging generative tasks in both language and vision domains. Our code will be available at: https://github.com/nasosger/MuToR.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10518v1",
    "published_date": "2025-05-15 17:25:03 UTC",
    "updated_date": "2025-05-15 17:25:03 UTC"
  },
  {
    "arxiv_id": "2505.10515v1",
    "title": "PnPXAI: A Universal XAI Framework Providing Automatic Explanations Across Diverse Modalities and Models",
    "authors": [
      "Seongun Kim",
      "Sol A Kim",
      "Geonhyeong Kim",
      "Enver Menadjiev",
      "Chanwoo Lee",
      "Seongwook Chung",
      "Nari Kim",
      "Jaesik Choi"
    ],
    "abstract": "Recently, post hoc explanation methods have emerged to enhance model transparency by attributing model outputs to input features. However, these methods face challenges due to their specificity to certain neural network architectures and data modalities. Existing explainable artificial intelligence (XAI) frameworks have attempted to address these challenges but suffer from several limitations. These include limited flexibility to diverse model architectures and data modalities due to hard-coded implementations, a restricted number of supported XAI methods because of the requirements for layer-specific operations of attribution methods, and sub-optimal recommendations of explanations due to the lack of evaluation and optimization phases. Consequently, these limitations impede the adoption of XAI technology in real-world applications, making it difficult for practitioners to select the optimal explanation method for their domain. To address these limitations, we introduce \\textbf{PnPXAI}, a universal XAI framework that supports diverse data modalities and neural network models in a Plug-and-Play (PnP) manner. PnPXAI automatically detects model architectures, recommends applicable explanation methods, and optimizes hyperparameters for optimal explanations. We validate the framework's effectiveness through user surveys and showcase its versatility across various domains, including medicine and finance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10515v1",
    "published_date": "2025-05-15 17:21:54 UTC",
    "updated_date": "2025-05-15 17:21:54 UTC"
  },
  {
    "arxiv_id": "2505.10607v2",
    "title": "MONAQ: Multi-Objective Neural Architecture Querying for Time-Series Analysis on Resource-Constrained Devices",
    "authors": [
      "Patara Trirat",
      "Jae-Gil Lee"
    ],
    "abstract": "The growing use of smartphones and IoT devices necessitates efficient time-series analysis on resource-constrained hardware, which is critical for sensing applications such as human activity recognition and air quality prediction. Recent efforts in hardware-aware neural architecture search (NAS) automate architecture discovery for specific platforms; however, none focus on general time-series analysis with edge deployment. Leveraging the problem-solving and reasoning capabilities of large language models (LLM), we propose MONAQ, a novel framework that reformulates NAS into Multi-Objective Neural Architecture Querying tasks. MONAQ is equipped with multimodal query generation for processing multimodal time-series inputs and hardware constraints, alongside an LLM agent-based multi-objective search to achieve deployment-ready models via code generation. By integrating numerical data, time-series images, and textual descriptions, MONAQ improves an LLM's understanding of time-series data. Experiments on fifteen datasets demonstrate that MONAQ-discovered models outperform both handcrafted models and NAS baselines while being more efficient.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "EMNLP 2025 (Findings), Project Page: https://kaist-dmlab.github.io/MONAQ",
    "pdf_url": "https://arxiv.org/pdf/2505.10607v2",
    "published_date": "2025-05-15 16:35:33 UTC",
    "updated_date": "2025-10-07 18:22:51 UTC"
  },
  {
    "arxiv_id": "2505.10483v1",
    "title": "UniEval: Unified Holistic Evaluation for Unified Multimodal Understanding and Generation",
    "authors": [
      "Yi Li",
      "Haonan Wang",
      "Qixiang Zhang",
      "Boyu Xiao",
      "Chenchang Hu",
      "Hualiang Wang",
      "Xiaomeng Li"
    ],
    "abstract": "The emergence of unified multimodal understanding and generation models is rapidly attracting attention because of their ability to enhance instruction-following capabilities while minimizing model redundancy. However, there is a lack of a unified evaluation framework for these models, which would enable an elegant, simplified, and overall evaluation. Current models conduct evaluations on multiple task-specific benchmarks, but there are significant limitations, such as the lack of overall results, errors from extra evaluation models, reliance on extensive labeled images, benchmarks that lack diversity, and metrics with limited capacity for instruction-following evaluation. To tackle these challenges, we introduce UniEval, the first evaluation framework designed for unified multimodal models without extra models, images, or annotations. This facilitates a simplified and unified evaluation process. The UniEval framework contains a holistic benchmark, UniBench (supports both unified and visual generation models), along with the corresponding UniScore metric. UniBench includes 81 fine-grained tags contributing to high diversity. Experimental results indicate that UniBench is more challenging than existing benchmarks, and UniScore aligns closely with human evaluations, surpassing current metrics. Moreover, we extensively evaluated SoTA unified and visual generation models, uncovering new insights into Univeral's unique values.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "UniEval is the first evaluation framework designed for unified multimodal models, including a holistic benchmark UniBench and the UniScore metric",
    "pdf_url": "https://arxiv.org/pdf/2505.10483v1",
    "published_date": "2025-05-15 16:34:50 UTC",
    "updated_date": "2025-05-15 16:34:50 UTC"
  },
  {
    "arxiv_id": "2505.10482v4",
    "title": "Fine-tuning Diffusion Policies with Backpropagation Through Diffusion Timesteps",
    "authors": [
      "Ningyuan Yang",
      "Jiaxuan Gao",
      "Feng Gao",
      "Yi Wu",
      "Chao Yu"
    ],
    "abstract": "Diffusion policies, widely adopted in decision-making scenarios such as robotics, gaming and autonomous driving, are capable of learning diverse skills from demonstration data due to their high representation power. However, the sub-optimal and limited coverage of demonstration data could lead to diffusion policies that generate sub-optimal trajectories and even catastrophic failures. While reinforcement learning (RL)-based fine-tuning has emerged as a promising solution to address these limitations, existing approaches struggle to effectively adapt Proximal Policy Optimization (PPO) to diffusion models. This challenge stems from the computational intractability of action likelihood estimation during the denoising process, which leads to complicated optimization objectives. In our experiments starting from randomly initialized policies, we find that online tuning of Diffusion Policies demonstrates much lower sample efficiency compared to directly applying PPO on MLP policies (MLP+PPO). To address these challenges, we introduce NCDPO, a novel framework that reformulates Diffusion Policy as a noise-conditioned deterministic policy. By treating each denoising step as a differentiable transformation conditioned on pre-sampled noise, NCDPO enables tractable likelihood evaluation and gradient backpropagation through all diffusion timesteps. Our experiments demonstrate that NCDPO achieves sample efficiency comparable to MLP+PPO when training from scratch, outperforming existing methods in both sample efficiency and final performance across diverse benchmarks, including continuous robot control and multi-agent game scenarios. Furthermore, our experimental results show that our method is robust to the number denoising timesteps in the Diffusion Policy.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages for main text, 23 pages in total, submitted to Neurips, 13 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.10482v4",
    "published_date": "2025-05-15 16:33:44 UTC",
    "updated_date": "2025-09-28 12:21:28 UTC"
  },
  {
    "arxiv_id": "2505.10606v1",
    "title": "Continuity and Isolation Lead to Doubts or Dilemmas in Large Language Models",
    "authors": [
      "Hector Pasten",
      "Felipe Urrutia",
      "Hector Jimenez",
      "Cristian B. Calderon",
      "Cristóbal Rojas",
      "Alexander Kozachinskiy"
    ],
    "abstract": "Understanding how Transformers work and how they process information is key to the theoretical and empirical advancement of these machines. In this work, we demonstrate the existence of two phenomena in Transformers, namely isolation and continuity. Both of these phenomena hinder Transformers to learn even simple pattern sequences. Isolation expresses that any learnable sequence must be isolated from another learnable sequence, and hence some sequences cannot be learned by a single Transformer at the same time. Continuity entails that an attractor basin forms around a learned sequence, such that any sequence falling in that basin will collapse towards the learned sequence. Here, we mathematically prove these phenomena emerge in all Transformers that use compact positional encoding, and design rigorous experiments, demonstrating that the theoretical limitations we shed light on occur on the practical scale.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10606v1",
    "published_date": "2025-05-15 16:24:14 UTC",
    "updated_date": "2025-05-15 16:24:14 UTC"
  },
  {
    "arxiv_id": "2505.10472v1",
    "title": "Large Language Models for Cancer Communication: Evaluating Linguistic Quality, Safety, and Accessibility in Generative AI",
    "authors": [
      "Agnik Saha",
      "Victoria Churchill",
      "Anny D. Rodriguez",
      "Ugur Kursuncu",
      "Muhammed Y. Idris"
    ],
    "abstract": "Effective communication about breast and cervical cancers remains a persistent health challenge, with significant gaps in public understanding of cancer prevention, screening, and treatment, potentially leading to delayed diagnoses and inadequate treatments. This study evaluates the capabilities and limitations of Large Language Models (LLMs) in generating accurate, safe, and accessible cancer-related information to support patient understanding. We evaluated five general-purpose and three medical LLMs using a mixed-methods evaluation framework across linguistic quality, safety and trustworthiness, and communication accessibility and affectiveness. Our approach utilized quantitative metrics, qualitative expert ratings, and statistical analysis using Welch's ANOVA, Games-Howell, and Hedges' g. Our results show that general-purpose LLMs produced outputs of higher linguistic quality and affectiveness, while medical LLMs demonstrate greater communication accessibility. However, medical LLMs tend to exhibit higher levels of potential harm, toxicity, and bias, reducing their performance in safety and trustworthiness. Our findings indicate a duality between domain-specific knowledge and safety in health communications. The results highlight the need for intentional model design with targeted improvements, particularly in mitigating harm and bias, and improving safety and affectiveness. This study provides a comprehensive evaluation of LLMs for cancer communication, offering critical insights for improving AI-generated health content and informing future development of accurate, safe, and accessible digital health tools.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10472v1",
    "published_date": "2025-05-15 16:23:21 UTC",
    "updated_date": "2025-05-15 16:23:21 UTC"
  },
  {
    "arxiv_id": "2505.10468v5",
    "title": "AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and Challenges",
    "authors": [
      "Ranjan Sapkota",
      "Konstantinos I. Roumeliotis",
      "Manoj Karkee"
    ],
    "abstract": "This review critically distinguishes between AI Agents and Agentic AI, offering a structured, conceptual taxonomy, application mapping, and analysis of opportunities and challenges to clarify their divergent design philosophies and capabilities. We begin by outlining the search strategy and foundational definitions, characterizing AI Agents as modular systems driven and enabled by LLMs and LIMs for task-specific automation. Generative AI is positioned as a precursor providing the foundation, with AI agents advancing through tool integration, prompt engineering, and reasoning enhancements. We then characterize Agentic AI systems, which, in contrast to AI Agents, represent a paradigm shift marked by multi-agent collaboration, dynamic task decomposition, persistent memory, and coordinated autonomy. Through a chronological evaluation of architectural evolution, operational mechanisms, interaction styles, and autonomy levels, we present a comparative analysis across both AI agents and agentic AI paradigms. Application domains enabled by AI Agents such as customer support, scheduling, and data summarization are then contrasted with Agentic AI deployments in research automation, robotic coordination, and medical decision support. We further examine unique challenges in each paradigm including hallucination, brittleness, emergent behavior, and coordination failure, and propose targeted solutions such as ReAct loops, retrieval-augmented generation (RAG), automation coordination layers, and causal modeling. This work aims to provide a roadmap for developing robust, scalable, and explainable AI-driven systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10468v5",
    "published_date": "2025-05-15 16:21:33 UTC",
    "updated_date": "2025-09-30 04:21:32 UTC"
  },
  {
    "arxiv_id": "2505.10465v4",
    "title": "Superposition Yields Robust Neural Scaling",
    "authors": [
      "Yizhou Liu",
      "Ziming Liu",
      "Jeff Gore"
    ],
    "abstract": "The success of today's large language models (LLMs) depends on the observation that larger models perform better. However, the origin of this neural scaling law, that loss decreases as a power law with model size, remains unclear. We propose that representation superposition, meaning that LLMs represent more features than they have dimensions, can be a key contributor to loss and cause neural scaling. Based on Anthropic's toy model, we use weight decay to control the degree of superposition, allowing us to systematically study how loss scales with model size. When superposition is weak, the loss follows a power law only if data feature frequencies are power-law distributed. In contrast, under strong superposition, the loss generically scales inversely with model dimension across a broad class of frequency distributions, due to geometric overlaps between representation vectors. We confirmed that open-sourced LLMs operate in the strong superposition regime and have loss scaling inversely with model dimension, and that the Chinchilla scaling laws are also consistent with this behavior. Our results identify representation superposition as a central driver of neural scaling laws, providing insights into questions like when neural scaling laws can be improved and when they will break down.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Best Paper Runner-up at NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.10465v4",
    "published_date": "2025-05-15 16:18:13 UTC",
    "updated_date": "2025-11-29 21:39:27 UTC"
  },
  {
    "arxiv_id": "2505.17047v1",
    "title": "Assessing the Quality of AI-Generated Clinical Notes: A Validated Evaluation of a Large Language Model Scribe",
    "authors": [
      "Erin Palm",
      "Astrit Manikantan",
      "Mark E. Pepin",
      "Herprit Mahal",
      "Srikanth Subramanya Belwadi"
    ],
    "abstract": "In medical practices across the United States, physicians have begun implementing generative artificial intelligence (AI) tools to perform the function of scribes in order to reduce the burden of documenting clinical encounters. Despite their widespread use, no established methods exist to gauge the quality of AI scribes. To address this gap, we developed a blinded study comparing the relative performance of large language model (LLM) generated clinical notes with those from field experts based on audio-recorded clinical encounters. Quantitative metrics from the Physician Documentation Quality Instrument (PDQI9) provided a framework to measure note quality, which we adapted to assess relative performance of AI generated notes. Clinical experts spanning 5 medical specialties used the PDQI9 tool to evaluate specialist-drafted Gold notes and LLM authored Ambient notes. Two evaluators from each specialty scored notes drafted from a total of 97 patient visits. We found uniformly high inter rater agreement (RWG greater than 0.7) between evaluators in general medicine, orthopedics, and obstetrics and gynecology, and moderate (RWG 0.5 to 0.7) to high inter rater agreement in pediatrics and cardiology. We found a modest yet significant difference in the overall note quality, wherein Gold notes achieved a score of 4.25 out of 5 and Ambient notes scored 4.20 out of 5 (p = 0.04). Our findings support the use of the PDQI9 instrument as a practical method to gauge the quality of LLM authored notes, as compared to human-authored notes.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "15 pages, 5 tables, 1 figure. Submitted for peer review 05/15/2025",
    "pdf_url": "https://arxiv.org/pdf/2505.17047v1",
    "published_date": "2025-05-15 16:14:53 UTC",
    "updated_date": "2025-05-15 16:14:53 UTC"
  },
  {
    "arxiv_id": "2505.10457v2",
    "title": "SEAL: Searching Expandable Architectures for Incremental Learning",
    "authors": [
      "Matteo Gambella",
      "Manuel Roveri"
    ],
    "abstract": "Incremental learning is a machine learning paradigm where a model learns from a sequential stream of tasks. This setting poses a key challenge: balancing plasticity (learning new tasks) and stability (preserving past knowledge). Neural Architecture Search (NAS), a branch of AutoML, automates the design of the architecture of Deep Neural Networks and has shown success in static settings. However, existing NAS-based approaches to incremental learning often rely on expanding the model at every task, making them impractical in resource-constrained environments. In this work, we introduce SEAL, a NAS-based framework tailored for data-incremental learning, a scenario where disjoint data samples arrive sequentially and are not stored for future access. SEAL adapts the model structure dynamically by expanding it only when necessary, based on a capacity estimation metric. Stability is preserved through cross-distillation training after each expansion step. The NAS component jointly searches for both the architecture and the optimal expansion policy. Experiments across multiple benchmarks demonstrate that SEAL effectively reduces forgetting and enhances accuracy while maintaining a lower model size compared to prior methods. These results highlight the promise of combining NAS and selective expansion for efficient, adaptive learning in incremental scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.10457v2",
    "published_date": "2025-05-15 16:14:18 UTC",
    "updated_date": "2025-07-28 15:36:46 UTC"
  },
  {
    "arxiv_id": "2505.10453v1",
    "title": "Vision language models have difficulty recognizing virtual objects",
    "authors": [
      "Tyler Tran",
      "Sangeet Khemlani",
      "J. G. Trafton"
    ],
    "abstract": "Vision language models (VLMs) are AI systems paired with both language and vision encoders to process multimodal input. They are capable of performing complex semantic tasks such as automatic captioning, but it remains an open question about how well they comprehend the visuospatial properties of scenes depicted in the images they process. We argue that descriptions of virtual objects -- objects that are not visually represented in an image -- can help test scene comprehension in these AI systems. For example, an image that depicts a person standing under a tree can be paired with the following prompt: imagine that a kite is stuck in the tree. VLMs that comprehend the scene should update their representations and reason sensibly about the spatial relations between all three objects. We describe systematic evaluations of state-of-the-art VLMs and show that their ability to process virtual objects is inadequate.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10453v1",
    "published_date": "2025-05-15 16:11:33 UTC",
    "updated_date": "2025-05-15 16:11:33 UTC"
  },
  {
    "arxiv_id": "2505.10604v2",
    "title": "MIRAGE: A Multi-modal Benchmark for Spatial Perception, Reasoning, and Intelligence",
    "authors": [
      "Chonghan Liu",
      "Haoran Wang",
      "Felix Henry",
      "Pu Miao",
      "Yajie Zhang",
      "Yu Zhao",
      "Peiran Wu"
    ],
    "abstract": "Spatial perception and reasoning are core components of human cognition, encompassing object recognition, spatial relational understanding, and dynamic reasoning. Despite progress in computer vision, existing benchmarks reveal significant gaps in models' abilities to accurately recognize object attributes and reason about spatial relationships, both essential for dynamic reasoning. To address these limitations, we propose MIRAGE, a multi-modal benchmark designed to evaluate models' capabilities in Counting (object attribute recognition), Relation (spatial relational reasoning), and Counting with Relation. Through diverse and complex scenarios requiring fine-grained recognition and reasoning, MIRAGE highlights critical limitations in state-of-the-art models, underscoring the need for improved representations and reasoning frameworks. By targeting these foundational abilities, MIRAGE provides a pathway toward spatiotemporal reasoning in future research.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10604v2",
    "published_date": "2025-05-15 16:08:14 UTC",
    "updated_date": "2025-06-23 01:22:36 UTC"
  },
  {
    "arxiv_id": "2505.10443v2",
    "title": "Are Large Language Models Robust in Understanding Code Against Semantics-Preserving Mutations?",
    "authors": [
      "Pedro Orvalho",
      "Marta Kwiatkowska"
    ],
    "abstract": "Understanding the reasoning and robustness of Large Language Models (LLMs) is critical for their reliable use in programming tasks. While recent studies have assessed LLMs' ability to predict program outputs, most focus solely on the accuracy of those predictions, without evaluating the reasoning behind them. Moreover, it has been observed on mathematical reasoning tasks that LLMs can arrive at correct answers through flawed logic, raising concerns about similar issues in code understanding. In this work, we evaluate whether state-of-the-art LLMs with up to 8B parameters can reason about Python programs or are simply guessing. We apply five semantics-preserving code mutations: renaming variables, mirroring comparison expressions, swapping if-else branches, converting for loops to while, and loop unrolling. These mutations maintain program semantics while altering its syntax. We evaluated six LLMs and performed a human expert analysis using LiveCodeBench to assess whether the correct predictions are based on sound reasoning. We also evaluated prediction stability across different code mutations on LiveCodeBench and CruxEval. Our findings show that LLMs trained for code produce correct predictions based on flawed reasoning between 10% and 50% of cases. Furthermore, LLMs often change predictions in response to our code mutations, indicating they do not yet exhibit stable, semantically grounded reasoning.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "11 pages, 5 tables, 1 figure",
    "pdf_url": "https://arxiv.org/pdf/2505.10443v2",
    "published_date": "2025-05-15 16:04:25 UTC",
    "updated_date": "2025-08-08 10:42:24 UTC"
  },
  {
    "arxiv_id": "2505.10442v1",
    "title": "IN-RIL: Interleaved Reinforcement and Imitation Learning for Policy Fine-Tuning",
    "authors": [
      "Dechen Gao",
      "Hang Wang",
      "Hanchu Zhou",
      "Nejib Ammar",
      "Shatadal Mishra",
      "Ahmadreza Moradipari",
      "Iman Soltani",
      "Junshan Zhang"
    ],
    "abstract": "Imitation learning (IL) and reinforcement learning (RL) each offer distinct advantages for robotics policy learning: IL provides stable learning from demonstrations, and RL promotes generalization through exploration. While existing robot learning approaches using IL-based pre-training followed by RL-based fine-tuning are promising, this two-step learning paradigm often suffers from instability and poor sample efficiency during the RL fine-tuning phase. In this work, we introduce IN-RIL, INterleaved Reinforcement learning and Imitation Learning, for policy fine-tuning, which periodically injects IL updates after multiple RL updates and hence can benefit from the stability of IL and the guidance of expert data for more efficient exploration throughout the entire fine-tuning process. Since IL and RL involve different optimization objectives, we develop gradient separation mechanisms to prevent destructive interference during \\ABBR fine-tuning, by separating possibly conflicting gradient updates in orthogonal subspaces. Furthermore, we conduct rigorous analysis, and our findings shed light on why interleaving IL with RL stabilizes learning and improves sample-efficiency. Extensive experiments on 14 robot manipulation and locomotion tasks across 3 benchmarks, including FurnitureBench, OpenAI Gym, and Robomimic, demonstrate that \\ABBR can significantly improve sample efficiency and mitigate performance collapse during online finetuning in both long- and short-horizon tasks with either sparse or dense rewards. IN-RIL, as a general plug-in compatible with various state-of-the-art RL algorithms, can significantly improve RL fine-tuning, e.g., from 12\\% to 88\\% with 6.3x improvement in the success rate on Robomimic Transport. Project page: https://github.com/ucd-dare/IN-RIL.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10442v1",
    "published_date": "2025-05-15 16:01:21 UTC",
    "updated_date": "2025-05-15 16:01:21 UTC"
  },
  {
    "arxiv_id": "2505.10441v1",
    "title": "PIF: Anomaly detection via preference embedding",
    "authors": [
      "Filippo Leveni",
      "Luca Magri",
      "Giacomo Boracchi",
      "Cesare Alippi"
    ],
    "abstract": "We address the problem of detecting anomalies with respect to structured patterns. To this end, we conceive a novel anomaly detection method called PIF, that combines the advantages of adaptive isolation methods with the flexibility of preference embedding. Specifically, we propose to embed the data in a high dimensional space where an efficient tree-based method, PI-Forest, is employed to compute an anomaly score. Experiments on synthetic and real datasets demonstrate that PIF favorably compares with state-of-the-art anomaly detection techniques, and confirm that PI-Forest is better at measuring arbitrary distances and isolate points in the preference space.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at International Conference on Pattern Recognition (ICPR 2020)",
    "pdf_url": "https://arxiv.org/pdf/2505.10441v1",
    "published_date": "2025-05-15 16:00:31 UTC",
    "updated_date": "2025-05-15 16:00:31 UTC"
  },
  {
    "arxiv_id": "2505.11552v2",
    "title": "GSPRec: Temporal-Aware Graph Spectral Filtering for Recommendation",
    "authors": [
      "Ahmad Bin Rabiah",
      "Julian McAuley"
    ],
    "abstract": "Graph-based recommendation systems are effective at modeling collaborative patterns but often suffer from two limitations: overreliance on low-pass filtering, which suppresses user-specific signals, and omission of sequential dynamics in graph construction. We introduce GSPRec, a graph spectral model that integrates temporal transitions through sequentially-informed graph construction and applies frequency-aware filtering in the spectral domain. GSPRec encodes item transitions via multi-hop diffusion to enable the use of symmetric Laplacians for spectral processing. To capture user preferences, we design a dual-filtering mechanism: a Gaussian bandpass filter to extract mid-frequency, user-level patterns, and a low-pass filter to retain global trends. Extensive experiments on four public datasets show that GSPRec consistently outperforms baselines, with an average improvement of 6.77% in NDCG@10. Ablation studies show the complementary benefits of both sequential graph augmentation and bandpass filtering.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11552v2",
    "published_date": "2025-05-15 15:49:56 UTC",
    "updated_date": "2025-09-24 04:09:49 UTC"
  },
  {
    "arxiv_id": "2505.10426v2",
    "title": "Formalising Human-in-the-Loop: Computational Reductions, Failure Modes, and Legal-Moral Responsibility",
    "authors": [
      "Maurice Chiodo",
      "Dennis Müller",
      "Paul Siewert",
      "Jean-Luc Wetherall",
      "Zoya Yasmine",
      "John Burden"
    ],
    "abstract": "We use the notion of oracle machines and reductions from computability theory to formalise different Human-in-the-loop (HITL) setups for AI systems, distinguishing between trivial human monitoring (i.e., total functions), single endpoint human action (i.e., many-one reductions), and highly involved human-AI interaction (i.e., Turing reductions). We then proceed to show that the legal status and safety of different setups vary greatly. We present a taxonomy to categorise HITL failure modes, highlighting the practical limitations of HITL setups. We then identify omissions in UK and EU legal frameworks, which focus on HITL setups that may not always achieve the desired ethical, legal, and sociotechnical outcomes. We suggest areas where the law should recognise the effectiveness of different HITL setups and assign responsibility in these contexts, avoiding human \"scapegoating\". Our work shows an unavoidable trade-off between attribution of legal responsibility, and technical explainability. Overall, we show how HITL setups involve many technical design decisions, and can be prone to failures out of the humans' control. Our formalisation and taxonomy opens up a new analytic perspective on the challenges in creating HITL setups, helping inform AI developers and lawmakers on designing HITL setups to better achieve their desired outcomes.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "math.HO"
    ],
    "primary_category": "cs.CY",
    "comment": "31 pages. Keywords: Human-in-the-loop, Automated decision making system, Human oversight in sociotechnical systems, Oracle machine, AI safety, Trustworthy AI",
    "pdf_url": "https://arxiv.org/pdf/2505.10426v2",
    "published_date": "2025-05-15 15:42:14 UTC",
    "updated_date": "2025-09-25 21:54:54 UTC"
  },
  {
    "arxiv_id": "2505.10420v1",
    "title": "Learned Lightweight Smartphone ISP with Unpaired Data",
    "authors": [
      "Andrei Arhire",
      "Radu Timofte"
    ],
    "abstract": "The Image Signal Processor (ISP) is a fundamental component in modern smartphone cameras responsible for conversion of RAW sensor image data to RGB images with a strong focus on perceptual quality. Recent work highlights the potential of deep learning approaches and their ability to capture details with a quality increasingly close to that of professional cameras. A difficult and costly step when developing a learned ISP is the acquisition of pixel-wise aligned paired data that maps the raw captured by a smartphone camera sensor to high-quality reference images. In this work, we address this challenge by proposing a novel training method for a learnable ISP that eliminates the need for direct correspondences between raw images and ground-truth data with matching content. Our unpaired approach employs a multi-term loss function guided by adversarial training with multiple discriminators processing feature maps from pre-trained networks to maintain content structure while learning color and texture characteristics from the target RGB dataset. Using lightweight neural network architectures suitable for mobile devices as backbones, we evaluated our method on the Zurich RAW to RGB and Fujifilm UltraISP datasets. Compared to paired training methods, our unpaired learning strategy shows strong potential and achieves high fidelity across multiple evaluation metrics. The code and pre-trained models are available at https://github.com/AndreiiArhire/Learned-Lightweight-Smartphone-ISP-with-Unpaired-Data .",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at CVPRW 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.10420v1",
    "published_date": "2025-05-15 15:37:51 UTC",
    "updated_date": "2025-05-15 15:37:51 UTC"
  },
  {
    "arxiv_id": "2505.10405v1",
    "title": "Visual Fidelity Index for Generative Semantic Communications with Critical Information Embedding",
    "authors": [
      "Jianhao Huang",
      "Qunsong Zeng",
      "Kaibin Huang"
    ],
    "abstract": "Generative semantic communication (Gen-SemCom) with large artificial intelligence (AI) model promises a transformative paradigm for 6G networks, which reduces communication costs by transmitting low-dimensional prompts rather than raw data. However, purely prompt-driven generation loses fine-grained visual details. Additionally, there is a lack of systematic metrics to evaluate the performance of Gen-SemCom systems. To address these issues, we develop a hybrid Gen-SemCom system with a critical information embedding (CIE) framework, where both text prompts and semantically critical features are extracted for transmissions. First, a novel approach of semantic filtering is proposed to select and transmit the semantically critical features of images relevant to semantic label. By integrating the text prompt and critical features, the receiver reconstructs high-fidelity images using a diffusion-based generative model. Next, we propose the generative visual information fidelity (GVIF) metric to evaluate the visual quality of the generated image. By characterizing the statistical models of image features, the GVIF metric quantifies the mutual information between the distorted features and their original counterparts. By maximizing the GVIF metric, we design a channel-adaptive Gen-SemCom system that adaptively control the volume of features and compression rate according to the channel state. Experimental results validate the GVIF metric's sensitivity to visual fidelity, correlating with both the PSNR and critical information volume. In addition, the optimized system achieves superior performance over benchmarking schemes in terms of higher PSNR and lower FID scores.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10405v1",
    "published_date": "2025-05-15 15:28:32 UTC",
    "updated_date": "2025-05-15 15:28:32 UTC"
  },
  {
    "arxiv_id": "2505.10402v1",
    "title": "Rethinking Repetition Problems of LLMs in Code Generation",
    "authors": [
      "Yihong Dong",
      "Yuchen Liu",
      "Xue Jiang",
      "Zhi Jin",
      "Ge Li"
    ],
    "abstract": "With the advent of neural language models, the performance of code generation has been significantly boosted. However, the problem of repetitions during the generation process continues to linger. Previous work has primarily focused on content repetition, which is merely a fraction of the broader repetition problem in code generation. A more prevalent and challenging problem is structural repetition. In structural repetition, the repeated code appears in various patterns but possesses a fixed structure, which can be inherently reflected in grammar. In this paper, we formally define structural repetition and propose an efficient decoding approach called RPG, which stands for Repetition Penalization based on Grammar, to alleviate the repetition problems in code generation for LLMs. Specifically, RPG first leverages grammar rules to identify repetition problems during code generation, and then strategically decays the likelihood of critical tokens that contribute to repetitions, thereby mitigating them in code generation. To facilitate this study, we construct a new dataset CodeRepetEval to comprehensively evaluate approaches for mitigating the repetition problems in code generation. Extensive experimental results demonstrate that RPG substantially outperforms the best-performing baselines on CodeRepetEval dataset as well as HumanEval and MBPP benchmarks, effectively reducing repetitions and enhancing the quality of generated code.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SE"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ACL 2025 (main)",
    "pdf_url": "https://arxiv.org/pdf/2505.10402v1",
    "published_date": "2025-05-15 15:26:32 UTC",
    "updated_date": "2025-05-15 15:26:32 UTC"
  },
  {
    "arxiv_id": "2505.10399v1",
    "title": "Evaluating Model Explanations without Ground Truth",
    "authors": [
      "Kaivalya Rawal",
      "Zihao Fu",
      "Eoin Delaney",
      "Chris Russell"
    ],
    "abstract": "There can be many competing and contradictory explanations for a single model prediction, making it difficult to select which one to use. Current explanation evaluation frameworks measure quality by comparing against ideal \"ground-truth\" explanations, or by verifying model sensitivity to important inputs. We outline the limitations of these approaches, and propose three desirable principles to ground the future development of explanation evaluation strategies for local feature importance explanations. We propose a ground-truth Agnostic eXplanation Evaluation framework (AXE) for evaluating and comparing model explanations that satisfies these principles. Unlike prior approaches, AXE does not require access to ideal ground-truth explanations for comparison, or rely on model sensitivity - providing an independent measure of explanation quality. We verify AXE by comparing with baselines, and show how it can be used to detect explanation fairwashing. Our code is available at https://github.com/KaiRawal/Evaluating-Model-Explanations-without-Ground-Truth.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "https://github.com/KaiRawal/Evaluating-Model-Explanations-without-Ground-Truth",
    "pdf_url": "https://arxiv.org/pdf/2505.10399v1",
    "published_date": "2025-05-15 15:22:06 UTC",
    "updated_date": "2025-05-15 15:22:06 UTC"
  },
  {
    "arxiv_id": "2505.10603v2",
    "title": "Toward a Public and Secure Generative AI: A Comparative Analysis of Open and Closed LLMs",
    "authors": [
      "Jorge Machado"
    ],
    "abstract": "Generative artificial intelligence (Gen AI) systems represent a critical technology with far-reaching implications across multiple domains of society. However, their deployment entails a range of risks and challenges that require careful evaluation. To date, there has been a lack of comprehensive, interdisciplinary studies offering a systematic comparison between open-source and proprietary (closed) generative AI systems, particularly regarding their respective advantages and drawbacks. This study aims to: i) critically evaluate and compare the characteristics, opportunities, and challenges of open and closed generative AI models; and ii) propose foundational elements for the development of an Open, Public, and Safe Gen AI framework. As a methodology, we adopted a combined approach that integrates three methods: literature review, critical analysis, and comparative analysis. The proposed framework outlines key dimensions, openness, public governance, and security, as essential pillars for shaping the future of trustworthy and inclusive Gen AI. Our findings reveal that open models offer greater transparency, auditability, and flexibility, enabling independent scrutiny and bias mitigation. In contrast, closed systems often provide better technical support and ease of implementation, but at the cost of unequal access, accountability, and ethical oversight. The research also highlights the importance of multi-stakeholder governance, environmental sustainability, and regulatory frameworks in ensuring responsible development.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10603v2",
    "published_date": "2025-05-15 15:21:09 UTC",
    "updated_date": "2025-10-30 13:13:19 UTC"
  },
  {
    "arxiv_id": "2505.10394v1",
    "title": "Inconsistency Handling in DatalogMTL",
    "authors": [
      "Meghyn Bienvenu",
      "Camille Bourgaux",
      "Atefe Khodadaditaghanaki"
    ],
    "abstract": "In this paper, we explore the issue of inconsistency handling in DatalogMTL, an extension of Datalog with metric temporal operators. Since facts are associated with time intervals, there are different manners to restore consistency when they contradict the rules, such as removing facts or modifying their time intervals. Our first contribution is the definition of relevant notions of conflicts (minimal explanations for inconsistency) and repairs (possible ways of restoring consistency) for this setting and the study of the properties of these notions and the associated inconsistency-tolerant semantics. Our second contribution is a data complexity analysis of the tasks of generating a single conflict / repair and query entailment under repair-based semantics.",
    "categories": [
      "cs.LO",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.LO",
    "comment": "This is an extended version of a paper appearing at the 34th International Joint Conference on Artificial Intelligence (IJCAI 2025). 24 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.10394v1",
    "published_date": "2025-05-15 15:17:09 UTC",
    "updated_date": "2025-05-15 15:17:09 UTC"
  },
  {
    "arxiv_id": "2505.10393v1",
    "title": "Uncovering Magnetic Phases with Synthetic Data and Physics-Informed Training",
    "authors": [
      "Agustin Medina",
      "Marcelo Arlego",
      "Carlos A. Lamas"
    ],
    "abstract": "We investigate the efficient learning of magnetic phases using artificial neural networks trained on synthetic data, combining computational simplicity with physics-informed strategies. Focusing on the diluted Ising model, which lacks an exact analytical solution, we explore two complementary approaches: a supervised classification using simple dense neural networks, and an unsupervised detection of phase transitions using convolutional autoencoders trained solely on idealized spin configurations.\n  To enhance model performance, we incorporate two key forms of physics-informed guidance. First, we exploit architectural biases which preferentially amplify features related to symmetry breaking. Second, we include training configurations that explicitly break $\\mathbb{Z}_2$ symmetry, reinforcing the network's ability to detect ordered phases. These mechanisms, acting in tandem, increase the network's sensitivity to phase structure even in the absence of explicit labels. We validate the machine learning predictions through comparison with direct numerical estimates of critical temperatures and percolation thresholds.\n  Our results show that synthetic, structured, and computationally efficient training schemes can reveal physically meaningful phase boundaries, even in complex systems. This framework offers a low-cost and robust alternative to conventional methods, with potential applications in broader condensed matter and statistical physics contexts.",
    "categories": [
      "cond-mat.str-el",
      "cs.AI"
    ],
    "primary_category": "cond-mat.str-el",
    "comment": "25 pages, 14 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.10393v1",
    "published_date": "2025-05-15 15:16:16 UTC",
    "updated_date": "2025-05-15 15:16:16 UTC"
  },
  {
    "arxiv_id": "2505.10392v2",
    "title": "Schreier-Coset Graph Propagation",
    "authors": [
      "Aryan Mishra",
      "Lizhen Lin"
    ],
    "abstract": "Graph Neural Networks (GNNs) offer a principled framework for learning over graph-structured data, yet their expressive capacity is often hindered by over-squashing, wherein information from distant nodes is compressed into fixed-size vectors. Existing solutions, including graph rewiring and bottleneck-resistant architectures such as Cayley and expander graphs, avoid this problem but introduce scalability bottlenecks. In particular, the Cayley graphs constructed over $SL(2,\\mathbb{Z}_n)$ exhibit strong theoretical properties, yet suffer from cubic node growth $O(n^3)$, leading to high memory usage. To address this, this work introduces Schrier-Coset Graph Propagation (SCGP), a group-theoretic augmentation method that enriches node features through Schreier-coset embeddings without altering the input graph topology. SCGP embeds bottleneck-free connectivity patterns into a compact feature space, improving long-range message passing while maintaining computational efficiency. Empirical evaluations across standard node and graph classification benchmarks demonstrate that SCGP achieves performance comparable to, or exceeding, expander graph and rewired GNN baselines. Furthermore, SCGP exhibits particular advantages in processing hierarchical and modular graph structures, offering reduced inference latency, improved scalability, and a low memory footprint, making it suitable for real-time and resource-constrained applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "The paper has been updated and now utilizes a more comprehensive methodology, we felt that the name does not do justice to it as their is no graph rewiring involved. Our method adds embeddings at the every beginning of before the propagation begins which is essentially feature augmentation. We have a more comprehensive method including graph rewiring which we will release in due course of time",
    "pdf_url": "https://arxiv.org/pdf/2505.10392v2",
    "published_date": "2025-05-15 15:14:02 UTC",
    "updated_date": "2025-09-19 13:45:24 UTC"
  },
  {
    "arxiv_id": "2505.10387v1",
    "title": "Multi-Agent Path Finding For Large Agents Is Intractable",
    "authors": [
      "Artem Agafonov",
      "Konstantin Yakovlev"
    ],
    "abstract": "The multi-agent path finding (MAPF) problem asks to find a set of paths on a graph such that when synchronously following these paths the agents never encounter a conflict. In the most widespread MAPF formulation, the so-called Classical MAPF, the agents sizes are neglected and two types of conflicts are considered: occupying the same vertex or using the same edge at the same time step. Meanwhile in numerous practical applications, e.g. in robotics, taking into account the agents' sizes is vital to ensure that the MAPF solutions can be safely executed. Introducing large agents yields an additional type of conflict arising when one agent follows an edge and its body overlaps with the body of another agent that is actually not using this same edge (e.g. staying still at some distinct vertex of the graph). Until now it was not clear how harder the problem gets when such conflicts are to be considered while planning. Specifically, it was known that Classical MAPF problem on an undirected graph can be solved in polynomial time, however no complete polynomial-time algorithm was presented to solve MAPF with large agents. In this paper we, for the first time, establish that the latter problem is NP-hard and, thus, if P!=NP no polynomial algorithm for it can, unfortunately, be presented. Our proof is based on the prevalent in the field technique of reducing the seminal 3SAT problem (which is known to be an NP-complete problem) to the problem at hand. In particular, for an arbitrary 3SAT formula we procedurally construct a dedicated graph with specific start and goal vertices and show that the given 3SAT formula is satisfiable iff the corresponding path finding instance has a solution.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CC"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10387v1",
    "published_date": "2025-05-15 15:07:40 UTC",
    "updated_date": "2025-05-15 15:07:40 UTC"
  },
  {
    "arxiv_id": "2505.10375v3",
    "title": "Are Sparse Autoencoders Useful for Java Function Bug Detection?",
    "authors": [
      "Rui Melo",
      "Claudia Mamede",
      "Andre Catarino",
      "Rui Abreu",
      "Henrique Lopes Cardoso"
    ],
    "abstract": "Software vulnerabilities such as buffer overflows and SQL injections are a major source of security breaches. Traditional methods for vulnerability detection remain essential but are limited by high false positive rates, scalability issues, and reliance on manual effort. These constraints have driven interest in AI-based approaches to automated vulnerability detection and secure code generation. While Large Language Models (LLMs) have opened new avenues for classification tasks, their complexity and opacity pose challenges for interpretability and deployment. Sparse Autoencoder offer a promising solution to this problem. We explore whether SAEs can serve as a lightweight, interpretable alternative for bug detection in Java functions. We evaluate the effectiveness of SAEs when applied to representations from GPT-2 Small and Gemma 2B, examining their capacity to highlight buggy behaviour without fine-tuning the underlying LLMs. We found that SAE-derived features enable bug detection with an F1 score of up to 89%, consistently outperforming fine-tuned transformer encoder baselines. Our work provides the first empirical evidence that SAEs can be used to detect software bugs directly from the internal representations of pretrained LLMs, without any fine-tuning or task-specific supervision. Code available at https://github.com/rufimelo99/SAE-Java-Bug-Detection",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "10 pages, 10 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.10375v3",
    "published_date": "2025-05-15 14:59:17 UTC",
    "updated_date": "2025-07-31 19:17:07 UTC"
  },
  {
    "arxiv_id": "2505.10371v1",
    "title": "ILIF: Temporal Inhibitory Leaky Integrate-and-Fire Neuron for Overactivation in Spiking Neural Networks",
    "authors": [
      "Kai Sun",
      "Peibo Duan",
      "Levin Kuhlmann",
      "Beilun Wang",
      "Bin Zhang"
    ],
    "abstract": "The Spiking Neural Network (SNN) has drawn increasing attention for its energy-efficient, event-driven processing and biological plausibility. To train SNNs via backpropagation, surrogate gradients are used to approximate the non-differentiable spike function, but they only maintain nonzero derivatives within a narrow range of membrane potentials near the firing threshold, referred to as the surrogate gradient support width gamma. We identify a major challenge, termed the dilemma of gamma: a relatively large gamma leads to overactivation, characterized by excessive neuron firing, which in turn increases energy consumption, whereas a small gamma causes vanishing gradients and weakens temporal dependencies. To address this, we propose a temporal Inhibitory Leaky Integrate-and-Fire (ILIF) neuron model, inspired by biological inhibitory mechanisms. This model incorporates interconnected inhibitory units for membrane potential and current, effectively mitigating overactivation while preserving gradient propagation. Theoretical analysis demonstrates ILIF effectiveness in overcoming the gamma dilemma, and extensive experiments on multiple datasets show that ILIF improves energy efficiency by reducing firing rates, stabilizes training, and enhances accuracy. The code is available at github.com/kaisun1/ILIF.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG",
      "q-bio.NC"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10371v1",
    "published_date": "2025-05-15 14:56:06 UTC",
    "updated_date": "2025-05-15 14:56:06 UTC"
  },
  {
    "arxiv_id": "2505.10361v2",
    "title": "Plasticity as the Mirror of Empowerment",
    "authors": [
      "David Abel",
      "Michael Bowling",
      "André Barreto",
      "Will Dabney",
      "Shi Dong",
      "Steven Hansen",
      "Anna Harutyunyan",
      "Khimya Khetarpal",
      "Clare Lyle",
      "Razvan Pascanu",
      "Georgios Piliouras",
      "Doina Precup",
      "Jonathan Richens",
      "Mark Rowland",
      "Tom Schaul",
      "Satinder Singh"
    ],
    "abstract": "Agents are minimally entities that are influenced by their past observations and act to influence future observations. This latter capacity is captured by empowerment, which has served as a vital framing concept across artificial intelligence and cognitive science. This former capacity, however, is equally foundational: In what ways, and to what extent, can an agent be influenced by what it observes? In this paper, we ground this concept in a universal agent-centric measure that we refer to as plasticity, and reveal a fundamental connection to empowerment. Following a set of desiderata on a suitable definition, we define plasticity using a new information-theoretic quantity we call the generalized directed information. We show that this new quantity strictly generalizes the directed information introduced by Massey (1990) while preserving all of its desirable properties. Under this definition, we find that plasticity is well thought of as the mirror of empowerment: The two concepts are defined using the same measure, with only the direction of influence reversed. Our main result establishes a tension between the plasticity and empowerment of an agent, suggesting that agent design needs to be mindful of both characteristics. We explore the implications of these findings, and suggest that plasticity, empowerment, and their relationship are essential to understanding agency",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.10361v2",
    "published_date": "2025-05-15 14:52:16 UTC",
    "updated_date": "2025-10-29 18:07:27 UTC"
  },
  {
    "arxiv_id": "2505.10360v2",
    "title": "FactsR: A Safer Method for Producing High Quality Healthcare Documentation",
    "authors": [
      "Victor Petrén Bach Hansen",
      "Lasse Krogsbøll",
      "Jonas Lyngsø",
      "Mathias Baltzersen",
      "Andreas Motzfeldt",
      "Kevin Pelgrims",
      "Lars Maaløe"
    ],
    "abstract": "There are now a multitude of AI-scribing solutions for healthcare promising the utilization of large language models for ambient documentation. However, these AI scribes still rely on one-shot, or few-shot prompts for generating notes after the consultation has ended, employing little to no reasoning. This risks long notes with an increase in hallucinations, misrepresentation of the intent of the clinician, and reliance on the proofreading of the clinician to catch errors. A dangerous combination for patient safety if vigilance is compromised by workload and fatigue. In this paper, we introduce a method for extracting salient clinical information in real-time alongside the healthcare consultation, denoted Facts, and use that information recursively to generate the final note. The FactsR method results in more accurate and concise notes by placing the clinician-in-the-loop of note generation, while opening up new use cases within real-time decision support.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.AP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10360v2",
    "published_date": "2025-05-15 14:51:22 UTC",
    "updated_date": "2025-06-04 12:39:39 UTC"
  },
  {
    "arxiv_id": "2505.10352v1",
    "title": "SpikeVideoFormer: An Efficient Spike-Driven Video Transformer with Hamming Attention and $\\mathcal{O}(T)$ Complexity",
    "authors": [
      "Shihao Zou",
      "Qingfeng Li",
      "Wei Ji",
      "Jingjing Li",
      "Yongkui Yang",
      "Guoqi Li",
      "Chao Dong"
    ],
    "abstract": "Spiking Neural Networks (SNNs) have shown competitive performance to Artificial Neural Networks (ANNs) in various vision tasks, while offering superior energy efficiency. However, existing SNN-based Transformers primarily focus on single-image tasks, emphasizing spatial features while not effectively leveraging SNNs' efficiency in video-based vision tasks. In this paper, we introduce SpikeVideoFormer, an efficient spike-driven video Transformer, featuring linear temporal complexity $\\mathcal{O}(T)$. Specifically, we design a spike-driven Hamming attention (SDHA) which provides a theoretically guided adaptation from traditional real-valued attention to spike-driven attention. Building on SDHA, we further analyze various spike-driven space-time attention designs and identify an optimal scheme that delivers appealing performance for video tasks, while maintaining only linear temporal complexity. The generalization ability and efficiency of our model are demonstrated across diverse downstream video tasks, including classification, human pose tracking, and semantic segmentation. Empirical results show our method achieves state-of-the-art (SOTA) performance compared to existing SNN approaches, with over 15\\% improvement on the latter two tasks. Additionally, it matches the performance of recent ANN-based methods while offering significant efficiency gains, achieving $\\times 16$, $\\times 10$ and $\\times 5$ improvements on the three tasks. https://github.com/JimmyZou/SpikeVideoFormer",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ICML 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.10352v1",
    "published_date": "2025-05-15 14:43:35 UTC",
    "updated_date": "2025-05-15 14:43:35 UTC"
  },
  {
    "arxiv_id": "2505.10347v2",
    "title": "Uniform Loss vs. Specialized Optimization: A Comparative Analysis in Multi-Task Learning",
    "authors": [
      "Gabriel S. Gama",
      "Valdir Grassi"
    ],
    "abstract": "Specialized Multi-Task Optimizers (SMTOs) balance task learning in Multi-Task Learning by addressing issues like conflicting gradients and differing gradient norms, which hinder equal-weighted task training. However, recent critiques suggest that equally weighted tasks can achieve competitive results compared to SMTOs, arguing that previous SMTO results were influenced by poor hyperparameter optimization and lack of regularization. In this work, we evaluate these claims through an extensive empirical evaluation of SMTOs, including some of the latest methods, on more complex multi-task problems to clarify this behavior. Our findings indicate that SMTOs perform well compared to uniform loss and that fixed weights can achieve competitive performance compared to SMTOs. Furthermore, we demonstrate why uniform loss perform similarly to SMTOs in some instances. The source code is available at https://github.com/Gabriel-SGama/UnitScal_vs_SMTOs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10347v2",
    "published_date": "2025-05-15 14:34:36 UTC",
    "updated_date": "2025-08-11 13:50:06 UTC"
  },
  {
    "arxiv_id": "2505.10331v1",
    "title": "Emergence of Structure in Ensembles of Random Neural Networks",
    "authors": [
      "Luca Muscarnera",
      "Luigi Loreti",
      "Giovanni Todeschini",
      "Alessio Fumagalli",
      "Francesco Regazzoni"
    ],
    "abstract": "Randomness is ubiquitous in many applications across data science and machine learning. Remarkably, systems composed of random components often display emergent global behaviors that appear deterministic, manifesting a transition from microscopic disorder to macroscopic organization. In this work, we introduce a theoretical model for studying the emergence of collective behaviors in ensembles of random classifiers. We argue that, if the ensemble is weighted through the Gibbs measure defined by adopting the classification loss as an energy, then there exists a finite temperature parameter for the distribution such that the classification is optimal, with respect to the loss (or the energy). Interestingly, for the case in which samples are generated by a Gaussian distribution and labels are constructed by employing a teacher perceptron, we analytically prove and numerically confirm that such optimal temperature does not depend neither on the teacher classifier (which is, by construction of the learning problem, unknown), nor on the number of random classifiers, highlighting the universal nature of the observed behavior. Experiments on the MNIST dataset underline the relevance of this phenomenon in high-quality, noiseless, datasets. Finally, a physical analogy allows us to shed light on the self-organizing nature of the studied phenomenon.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10331v1",
    "published_date": "2025-05-15 14:20:02 UTC",
    "updated_date": "2025-05-15 14:20:02 UTC"
  },
  {
    "arxiv_id": "2505.10330v1",
    "title": "Efficient Adaptation of Reinforcement Learning Agents to Sudden Environmental Change",
    "authors": [
      "Jonathan Clifford Balloch"
    ],
    "abstract": "Real-world autonomous decision-making systems, from robots to recommendation engines, must operate in environments that change over time. While deep reinforcement learning (RL) has shown an impressive ability to learn optimal policies in stationary environments, most methods are data intensive and assume a world that does not change between training and test time. As a result, conventional RL methods struggle to adapt when conditions change. This poses a fundamental challenge: how can RL agents efficiently adapt their behavior when encountering novel environmental changes during deployment without catastrophically forgetting useful prior knowledge? This dissertation demonstrates that efficient online adaptation requires two key capabilities: (1) prioritized exploration and sampling strategies that help identify and learn from relevant experiences, and (2) selective preservation of prior knowledge through structured representations that can be updated without disruption to reusable components.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "PhD Dissertation, 131 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.10330v1",
    "published_date": "2025-05-15 14:19:01 UTC",
    "updated_date": "2025-05-15 14:19:01 UTC"
  },
  {
    "arxiv_id": "2505.10328v2",
    "title": "A Comparative Study of SMT and MILP for the Nurse Rostering Problem",
    "authors": [
      "Alvin Combrink",
      "Stephie Do",
      "Kristofer Bengtsson",
      "Sabino Francesco Roselli",
      "Martin Fabian"
    ],
    "abstract": "The effects of personnel scheduling on the quality of care and working conditions for healthcare personnel have been thoroughly documented. However, the ever-present demand and large variation of constraints make healthcare scheduling particularly challenging. This problem has been studied for decades, with limited research aimed at applying Satisfiability Modulo Theories (SMT). SMT has gained momentum within the formal verification community in the last decades, leading to the advancement of SMT solvers that have been shown to outperform standard mathematical programming techniques. In this work, we propose generic constraint formulations that can model a wide range of real-world scheduling constraints. Then, the generic constraints are formulated as SMT and MILP problems and used to compare the respective state-of-the-art solvers, Z3 and Gurobi, on academic and real-world inspired rostering problems. Experimental results show how each solver excels for certain types of problems; the MILP solver generally performs better when the problem is highly constrained or infeasible, while the SMT solver performs better otherwise. On real-world inspired problems containing a more varied set of shifts and personnel, the SMT solver excels. Additionally, it was noted during experimentation that the SMT solver was more sensitive to the way the generic constraints were formulated, requiring careful consideration and experimentation to achieve better performance. We conclude that SMT-based methods present a promising avenue for future research within the domain of personnel scheduling.",
    "categories": [
      "cs.AI",
      "eess.SY"
    ],
    "primary_category": "cs.AI",
    "comment": "6 pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.10328v2",
    "published_date": "2025-05-15 14:12:39 UTC",
    "updated_date": "2025-06-02 06:55:12 UTC"
  },
  {
    "arxiv_id": "2505.10321v1",
    "title": "AutoPentest: Enhancing Vulnerability Management With Autonomous LLM Agents",
    "authors": [
      "Julius Henke"
    ],
    "abstract": "A recent area of increasing research is the use of Large Language Models (LLMs) in penetration testing, which promises to reduce costs and thus allow for higher frequency. We conduct a review of related work, identifying best practices and common evaluation issues. We then present AutoPentest, an application for performing black-box penetration tests with a high degree of autonomy. AutoPentest is based on the LLM GPT-4o from OpenAI and the LLM agent framework LangChain. It can perform complex multi-step tasks, augmented by external tools and knowledge bases. We conduct a study on three capture-the-flag style Hack The Box (HTB) machines, comparing our implementation AutoPentest with the baseline approach of manually using the ChatGPT-4o user interface. Both approaches are able to complete 15-25 % of the subtasks on the HTB machines, with AutoPentest slightly outperforming ChatGPT. We measure a total cost of \\$96.20 US when using AutoPentest across all experiments, while a one-month subscription to ChatGPT Plus costs \\$20. The results show that further implementation efforts and the use of more powerful LLMs released in the future are likely to make this a viable part of vulnerability management.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "24 pages, 1 figure, for implementation, see https://github.com/JuliusHenke/autopentest",
    "pdf_url": "https://arxiv.org/pdf/2505.10321v1",
    "published_date": "2025-05-15 14:06:00 UTC",
    "updated_date": "2025-05-15 14:06:00 UTC"
  },
  {
    "arxiv_id": "2505.10320v3",
    "title": "J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning",
    "authors": [
      "Chenxi Whitehouse",
      "Tianlu Wang",
      "Ping Yu",
      "Xian Li",
      "Jason Weston",
      "Ilia Kulikov",
      "Swarnadeep Saha"
    ],
    "abstract": "The progress of AI is bottlenecked by the quality of evaluation, making powerful LLM-as-a-Judge models a core solution. The efficacy of these judges depends on their chain-of-thought reasoning, creating a critical need for methods that can effectively optimize this reasoning process. In this work, we introduce J1, a reinforcement learning framework for teaching LLM judges to think before making decisions. Our core contribution lies in converting all judgment tasks for non-verifiable and verifiable prompts into a unified format with verifiable rewards, enabling direct optimization of evaluation quality while mitigating positional bias. We then use RL to train thinking-judges at scales of 8B, 32B, and 70B and show that they obtain state-of-the-art performance across multiple benchmarks. In particular, J1-Qwen-32B, our multitasked pointwise and pairwise judge also outperforms o1-mini, o3, and a much larger 671B DeepSeek-R1 on some benchmarks, while only training on synthetic data. Through comprehensive ablations of pairwise, pointwise, and multitask J1 variants, we demonstrate the effectiveness of our approach across seed prompts, reward strategies, and training recipes. Qualitative analysis reveals that J1 develops systematic evaluation strategies, including dynamic criteria generation, reference answer creation, iterative self-correction of initial assessments, and feedback generation for low-quality responses.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 13 tables, 14 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.10320v3",
    "published_date": "2025-05-15 14:05:15 UTC",
    "updated_date": "2025-10-13 11:06:51 UTC"
  },
  {
    "arxiv_id": "2505.10600v1",
    "title": "Enhancing IoT Cyber Attack Detection in the Presence of Highly Imbalanced Data",
    "authors": [
      "Md. Ehsanul Haque",
      "Md. Saymon Hosen Polash",
      "Md Al-Imran Sanjida Simla",
      "Md Alomgir Hossain",
      "Sarwar Jahan"
    ],
    "abstract": "Due to the rapid growth in the number of Internet of Things (IoT) networks, the cyber risk has increased exponentially, and therefore, we have to develop effective IDS that can work well with highly imbalanced datasets. A high rate of missed threats can be the result, as traditional machine learning models tend to struggle in identifying attacks when normal data volume is much higher than the volume of attacks. For example, the dataset used in this study reveals a strong class imbalance with 94,659 instances of the majority class and only 28 instances of the minority class, making it quite challenging to determine rare attacks accurately. The challenges presented in this research are addressed by hybrid sampling techniques designed to improve data imbalance detection accuracy in IoT domains. After applying these techniques, we evaluate the performance of several machine learning models such as Random Forest, Soft Voting, Support Vector Classifier (SVC), K-Nearest Neighbors (KNN), Multi-Layer Perceptron (MLP), and Logistic Regression with respect to the classification of cyber-attacks. The obtained results indicate that the Random Forest model achieved the best performance with a Kappa score of 0.9903, test accuracy of 0.9961, and AUC of 0.9994. Strong performance is also shown by the Soft Voting model, with an accuracy of 0.9952 and AUC of 0.9997, indicating the benefits of combining model predictions. Overall, this work demonstrates the value of hybrid sampling combined with robust model and feature selection for significantly improving IoT security against cyber-attacks, especially in highly imbalanced data environments.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "Published paper of CSNT2025",
    "pdf_url": "https://arxiv.org/pdf/2505.10600v1",
    "published_date": "2025-05-15 14:02:48 UTC",
    "updated_date": "2025-05-15 14:02:48 UTC"
  },
  {
    "arxiv_id": "2505.10315v1",
    "title": "Private Transformer Inference in MLaaS: A Survey",
    "authors": [
      "Yang Li",
      "Xinyu Zhou",
      "Yitong Wang",
      "Liangxin Qian",
      "Jun Zhao"
    ],
    "abstract": "Transformer models have revolutionized AI, powering applications like content generation and sentiment analysis. However, their deployment in Machine Learning as a Service (MLaaS) raises significant privacy concerns, primarily due to the centralized processing of sensitive user data. Private Transformer Inference (PTI) offers a solution by utilizing cryptographic techniques such as secure multi-party computation and homomorphic encryption, enabling inference while preserving both user data and model privacy. This paper reviews recent PTI advancements, highlighting state-of-the-art solutions and challenges. We also introduce a structured taxonomy and evaluation framework for PTI, focusing on balancing resource efficiency with privacy and bridging the gap between high-performance inference and data privacy.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10315v1",
    "published_date": "2025-05-15 14:00:19 UTC",
    "updated_date": "2025-05-15 14:00:19 UTC"
  },
  {
    "arxiv_id": "2505.10309v3",
    "title": "A large-scale evaluation of commonsense knowledge in humans and large language models",
    "authors": [
      "Tuan Dung Nguyen",
      "Duncan J. Watts",
      "Mark E. Whiting"
    ],
    "abstract": "Commonsense knowledge, a major constituent of artificial intelligence (AI), is primarily evaluated in practice by human-prescribed ground-truth labels. An important, albeit implicit, assumption of these labels is that they accurately capture what any human would think, effectively treating human common sense as homogeneous. However, recent empirical work has shown that humans vary enormously in what they consider commonsensical; thus what appears self-evident to one benchmark designer may not be so to another. Here, we propose a method for assessing commonsense knowledge in AI, specifically in large language models (LLMs), that incorporates empirically observed heterogeneity among humans by measuring the correspondence between a model's judgment and that of a human population. We first find that, when treated as independent survey respondents, most LLMs remain below the human median in their individual commonsense competence. Second, when used as simulators of a hypothetical population, LLMs correlate with real humans only modestly in the extent to which they agree on the same set of statements. In both cases, smaller, open-weight models are surprisingly more competitive than larger, proprietary frontier models. Our evaluation framework, which ties commonsense knowledge to its cultural basis, contributes to the growing call for adapting AI models to human collectivities that possess different, often incompatible, social stocks of knowledge.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.SI"
    ],
    "primary_category": "cs.AI",
    "comment": "Code and data: https://github.com/Watts-Lab/commonsense-llm-eval",
    "pdf_url": "https://arxiv.org/pdf/2505.10309v3",
    "published_date": "2025-05-15 13:55:27 UTC",
    "updated_date": "2026-01-21 22:06:24 UTC"
  },
  {
    "arxiv_id": "2505.10300v1",
    "title": "AI LEGO: Scaffolding Cross-Functional Collaboration in Industrial Responsible AI Practices during Early Design Stages",
    "authors": [
      "Muzhe Wu",
      "Yanzhi Zhao",
      "Shuyi Han",
      "Michael Xieyang Liu",
      "Hong Shen"
    ],
    "abstract": "Responsible AI (RAI) efforts increasingly emphasize the importance of addressing potential harms early in the AI development lifecycle through social-technical lenses. However, in cross-functional industry teams, this work is often stalled by a persistent knowledge handoff challenge: the difficulty of transferring high-level, early-stage technical design rationales from technical experts to non-technical or user-facing roles for ethical evaluation and harm identification. Through literature review and a co-design study with 8 practitioners, we unpack how this challenge manifests -- technical design choices are rarely handed off in ways that support meaningful engagement by non-technical roles; collaborative workflows lack shared, visual structures to support mutual understanding; and non-technical practitioners are left without scaffolds for systematic harm evaluation. Existing tools like JIRA or Google Docs, while useful for product tracking, are ill-suited for supporting joint harm identification across roles, often requiring significant extra effort to align understanding. To address this, we developed AI LEGO, a web-based prototype that supports cross-functional AI practitioners in effectively facilitating knowledge handoff and identifying harmful design choices in the early design stages. Technical roles use interactive blocks to draft development plans, while non-technical roles engage with those blocks through stage-specific checklists and LLM-driven persona simulations to surface potential harms. In a study with 18 cross-functional practitioners, AI LEGO increased the volume and likelihood of harms identified compared to baseline worksheets. Participants found that its modular structure and persona prompts made harm identification more accessible, fostering clearer and more collaborative RAI practices in early design.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10300v1",
    "published_date": "2025-05-15 13:49:02 UTC",
    "updated_date": "2025-05-15 13:49:02 UTC"
  },
  {
    "arxiv_id": "2505.10297v2",
    "title": "Defending the Edge: Representative-Attention Defense against Backdoor Attacks in Federated Learning",
    "authors": [
      "Chibueze Peace Obioma",
      "Youcheng Sun",
      "Mustafa A. Mustafa"
    ],
    "abstract": "Federated learning (FL) remains highly vulnerable to adaptive backdoor attacks that preserve stealth by closely imitating benign update statistics. Existing defenses predominantly rely on anomaly detection in parameter or gradient space, overlooking behavioral constraints that backdoor attacks must satisfy to ensure reliable trigger activation. These anomaly-centric methods fail against adaptive attacks that normalize update magnitudes and mimic benign statistical patterns while preserving backdoor functionality, creating a fundamental detection gap. To address this limitation, this paper introduces FeRA (Federated Representative Attention) -- a novel attention-driven defense that shifts the detection paradigm from anomaly-centric to consistency-centric analysis. FeRA exploits the intrinsic need for backdoor persistence across training rounds, identifying malicious clients through suppressed representation-space variance, an orthogonal property to traditional magnitude-based statistics. The framework conducts multi-dimensional behavioral analysis combining spectral and spatial attention, directional alignment, mutual similarity, and norm inflation across two complementary detection mechanisms: consistency analysis and norm-inflation detection. Through this mechanism, FeRA isolates malicious clients that exhibit low-variance consistency or magnitude amplification. Extensive evaluation across six datasets, nine attacks, and three model architectures under both Independent and Identically Distributed (IID) and non-IID settings confirm FeRA achieves superior backdoor mitigation. Under different non-IID settings, FeRA achieved the lowest average Backdoor Accuracy (BA), about 1.67% while maintaining high clean accuracy compared to other state-of-the-art defenses. The code is available at https://github.com/Peatech/FeRA_defense.git.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "Submitted to IEEE EURO S&P 2026",
    "pdf_url": "https://arxiv.org/pdf/2505.10297v2",
    "published_date": "2025-05-15 13:44:32 UTC",
    "updated_date": "2025-11-21 14:13:48 UTC"
  },
  {
    "arxiv_id": "2505.10282v2",
    "title": "Streamlining evidence based clinical recommendations with large language models",
    "authors": [
      "Dubai Li",
      "Nan Jiang",
      "Kangping Huang",
      "Ruiqi Tu",
      "Shuyu Ouyang",
      "Huayu Yu",
      "Lin Qiao",
      "Chen Yu",
      "Tianshu Zhou",
      "Danyang Tong",
      "Qian Wang",
      "Mengtao Li",
      "Xiaofeng Zeng",
      "Yu Tian",
      "Xinping Tian",
      "Jingsong Li"
    ],
    "abstract": "Clinical evidence underpins informed healthcare decisions, yet integrating it into real-time practice remains challenging due to intensive workloads, complex procedures, and time constraints. This study presents Quicker, an LLM-powered system that automates evidence synthesis and generates clinical recommendations following standard guideline development workflows. Quicker delivers an end-to-end pipeline from clinical questions to recommendations and supports customized decision-making through integrated tools and interactive interfaces. To evaluate how closely Quicker can reproduce guideline development processes, we constructed Q2CRBench-3, a benchmark derived from guideline development records for three diseases. Experiments show that Quicker produces precise question decomposition, expert-aligned retrieval, and near-comprehensive screening. Quicker assistance improved the accuracy of extracted study data, and its recommendations were more comprehensive and coherent than clinician-written ones. In system-level testing, Quicker working with one participant reduced recommendation development to 20-40 min. Overall, the findings demonstrate Quicker's potential to enhance the speed and reliability of evidence-based clinical decision-making.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10282v2",
    "published_date": "2025-05-15 13:30:39 UTC",
    "updated_date": "2026-01-09 03:19:57 UTC"
  },
  {
    "arxiv_id": "2505.10278v2",
    "title": "MASS: Muli-agent simulation scaling for portfolio construction",
    "authors": [
      "Taian Guo",
      "Haiyang Shen",
      "JinSheng Huang",
      "Zhengyang Mao",
      "Junyu Luo",
      "Binqi Chen",
      "Zhuoru Chen",
      "Luchen Liu",
      "Bingyu Xia",
      "Xuhui Liu",
      "Yun Ma",
      "Ming Zhang"
    ],
    "abstract": "The application of LLM-based agents in financial investment has shown significant promise, yet existing approaches often require intermediate steps like predicting individual stock movements or rely on predefined, static workflows. These limitations restrict their adaptability and effectiveness in constructing optimal portfolios. In this paper, we introduce the Multi-Agent Scaling Simulation (MASS), a novel framework that leverages multi-agent simulation for direct, end-to-end portfolio construction. At its core, MASS employs a backward optimization process to dynamically learn the optimal distribution of heterogeneous agents, enabling the system to adapt to evolving market regimes. A key finding enabled by our framework is the exploration of the scaling effect for portfolio construction: we demonstrate that as the number of agents increases exponentially (up to 512), the aggregated decisions yield progressively higher excess returns. Extensive experiments on a challenging, self-collected dataset from the 2023 Chinese A-share market show that MASS consistently outperforms seven state-of-the-art baselines. Further backtesting, stability analyses and the experiment on data leakage concerns validate its enhanced profitability and robustness. We have open-sourced our code, dataset, and training snapshots at https://github.com/gta0804/MASS/ to foster further research.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10278v2",
    "published_date": "2025-05-15 13:27:18 UTC",
    "updated_date": "2025-09-25 14:52:25 UTC"
  },
  {
    "arxiv_id": "2505.10273v1",
    "title": "AttentionGuard: Transformer-based Misbehavior Detection for Secure Vehicular Platoons",
    "authors": [
      "Hexu Li",
      "Konstantinos Kalogiannis",
      "Ahmed Mohamed Hussain",
      "Panos Papadimitratos"
    ],
    "abstract": "Vehicle platooning, with vehicles traveling in close formation coordinated through Vehicle-to-Everything (V2X) communications, offers significant benefits in fuel efficiency and road utilization. However, it is vulnerable to sophisticated falsification attacks by authenticated insiders that can destabilize the formation and potentially cause catastrophic collisions. This paper addresses this challenge: misbehavior detection in vehicle platooning systems. We present AttentionGuard, a transformer-based framework for misbehavior detection that leverages the self-attention mechanism to identify anomalous patterns in mobility data. Our proposal employs a multi-head transformer-encoder to process sequential kinematic information, enabling effective differentiation between normal mobility patterns and falsification attacks across diverse platooning scenarios, including steady-state (no-maneuver) operation, join, and exit maneuvers. Our evaluation uses an extensive simulation dataset featuring various attack vectors (constant, gradual, and combined falsifications) and operational parameters (controller types, vehicle speeds, and attacker positions). Experimental results demonstrate that AttentionGuard achieves up to 0.95 F1-score in attack detection, with robust performance maintained during complex maneuvers. Notably, our system performs effectively with minimal latency (100ms decision intervals), making it suitable for real-time transportation safety applications. Comparative analysis reveals superior detection capabilities and establishes the transformer-encoder as a promising approach for securing Cooperative Intelligent Transport Systems (C-ITS) against sophisticated insider threats.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.CR",
    "comment": "Author's version; Accepted for presentation at the ACM Workshop on Wireless Security and Machine Learning (WiseML 2025)",
    "pdf_url": "https://arxiv.org/pdf/2505.10273v1",
    "published_date": "2025-05-15 13:24:09 UTC",
    "updated_date": "2025-05-15 13:24:09 UTC"
  },
  {
    "arxiv_id": "2505.10264v2",
    "title": "Cutting Through Privacy: A Hyperplane-Based Data Reconstruction Attack in Federated Learning",
    "authors": [
      "Francesco Diana",
      "André Nusser",
      "Chuan Xu",
      "Giovanni Neglia"
    ],
    "abstract": "Federated Learning (FL) enables collaborative training of machine learning models across distributed clients without sharing raw data, ostensibly preserving data privacy. Nevertheless, recent studies have revealed critical vulnerabilities in FL, showing that a malicious central server can manipulate model updates to reconstruct clients' private training data. Existing data reconstruction attacks have important limitations: they often rely on assumptions about the clients' data distribution or their efficiency significantly degrades when batch sizes exceed just a few tens of samples.\n  In this work, we introduce a novel data reconstruction attack that overcomes these limitations. Our method leverages a new geometric perspective on fully connected layers to craft malicious model parameters, enabling the perfect recovery of arbitrarily large data batches in classification tasks without any prior knowledge of clients' data. Through extensive experiments on both image and tabular datasets, we demonstrate that our attack outperforms existing methods and achieves perfect reconstruction of data batches two orders of magnitude larger than the state of the art.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10264v2",
    "published_date": "2025-05-15 13:16:32 UTC",
    "updated_date": "2025-09-05 11:54:06 UTC"
  },
  {
    "arxiv_id": "2505.10261v1",
    "title": "The Evolving Landscape of Generative Large Language Models and Traditional Natural Language Processing in Medicine",
    "authors": [
      "Rui Yang",
      "Huitao Li",
      "Matthew Yu Heng Wong",
      "Yuhe Ke",
      "Xin Li",
      "Kunyu Yu",
      "Jingchi Liao",
      "Jonathan Chong Kai Liew",
      "Sabarinath Vinod Nair",
      "Jasmine Chiat Ling Ong",
      "Irene Li",
      "Douglas Teodoro",
      "Chuan Hong",
      "Daniel Shu Wei Ting",
      "Nan Liu"
    ],
    "abstract": "Natural language processing (NLP) has been traditionally applied to medicine, and generative large language models (LLMs) have become prominent recently. However, the differences between them across different medical tasks remain underexplored. We analyzed 19,123 studies, finding that generative LLMs demonstrate advantages in open-ended tasks, while traditional NLP dominates in information extraction and analysis tasks. As these technologies advance, ethical use of them is essential to ensure their potential in medical applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10261v1",
    "published_date": "2025-05-15 13:11:14 UTC",
    "updated_date": "2025-05-15 13:11:14 UTC"
  },
  {
    "arxiv_id": "2505.10260v1",
    "title": "Comparing LLM Text Annotation Skills: A Study on Human Rights Violations in Social Media Data",
    "authors": [
      "Poli Apollinaire Nemkova",
      "Solomon Ubani",
      "Mark V. Albert"
    ],
    "abstract": "In the era of increasingly sophisticated natural language processing (NLP) systems, large language models (LLMs) have demonstrated remarkable potential for diverse applications, including tasks requiring nuanced textual understanding and contextual reasoning. This study investigates the capabilities of multiple state-of-the-art LLMs - GPT-3.5, GPT-4, LLAMA3, Mistral 7B, and Claude-2 - for zero-shot and few-shot annotation of a complex textual dataset comprising social media posts in Russian and Ukrainian. Specifically, the focus is on the binary classification task of identifying references to human rights violations within the dataset.\n  To evaluate the effectiveness of these models, their annotations are compared against a gold standard set of human double-annotated labels across 1000 samples. The analysis includes assessing annotation performance under different prompting conditions, with prompts provided in both English and Russian. Additionally, the study explores the unique patterns of errors and disagreements exhibited by each model, offering insights into their strengths, limitations, and cross-linguistic adaptability.\n  By juxtaposing LLM outputs with human annotations, this research contributes to understanding the reliability and applicability of LLMs for sensitive, domain-specific tasks in multilingual contexts. It also sheds light on how language models handle inherently subjective and context-dependent judgments, a critical consideration for their deployment in real-world scenarios.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10260v1",
    "published_date": "2025-05-15 13:10:47 UTC",
    "updated_date": "2025-05-15 13:10:47 UTC"
  },
  {
    "arxiv_id": "2505.10599v2",
    "title": "UDDETTS: Unifying Discrete and Dimensional Emotions for Controllable Emotional Text-to-Speech",
    "authors": [
      "Jiaxuan Liu",
      "Yang Xiang",
      "Han Zhao",
      "Xiangang Li",
      "Yingying Gao",
      "Shilei Zhang",
      "Zhenhua Ling"
    ],
    "abstract": "Recent large language models (LLMs) have made great progress in the field of text-to-speech (TTS), but they still face major challenges in synthesizing fine-grained emotional speech in an interpretable manner. Traditional methods rely on discrete emotion labels to control emotion categories and intensities, which cannot capture the complexity and continuity of human emotional perception and expression. The lack of large-scale emotional speech datasets with balanced emotion distributions and fine-grained emotional annotations often causes overfitting in synthesis models and impedes effective emotion control. To address these issues, we propose UDDETTS, a universal LLM framework unifying discrete and dimensional emotions for controllable emotional TTS. This model introduces the interpretable Arousal-Dominance-Valence (ADV) space for dimensional emotion description and supports emotion control driven by either discrete emotion labels or nonlinearly quantified ADV values. Furthermore, a semi-supervised training strategy is designed to comprehensively utilize diverse speech datasets with different types of emotional annotations to train the UDDETTS. Experiments show that UDDETTS achieves linear emotion control along three interpretable dimensions, and exhibits superior end-to-end emotional speech synthesis capabilities. Code and demos are available at: https://anonymous.4open.science/w/UDDETTS.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Under review",
    "pdf_url": "https://arxiv.org/pdf/2505.10599v2",
    "published_date": "2025-05-15 12:57:19 UTC",
    "updated_date": "2025-09-25 09:20:16 UTC"
  },
  {
    "arxiv_id": "2507.18449v1",
    "title": "Digital Twin Technologies in Predictive Maintenance: Enabling Transferability via Sim-to-Real and Real-to-Sim Transfer",
    "authors": [
      "Sizhe Ma",
      "Katherine A. Flanigan",
      "Mario Bergés"
    ],
    "abstract": "The advancement of the Internet of Things (IoT) and Artificial Intelligence has catalyzed the evolution of Digital Twins (DTs) from conceptual ideas to more implementable realities. Yet, transitioning from academia to industry is complex due to the absence of standardized frameworks. This paper builds upon the authors' previously established functional and informational requirements supporting standardized DT development, focusing on a crucial aspect: transferability. While existing DT research primarily centers on asset transfer, the significance of \"sim-to-real transfer\" and \"real-to-sim transfer\"--transferring knowledge between simulations and real-world operations--is vital for comprehensive lifecycle management in DTs. A key challenge in this process is calibrating the \"reality gap,\" the discrepancy between simulated predictions and actual outcomes. Our research investigates the impact of integrating a single Reality Gap Analysis (RGA) module into an existing DT framework to effectively manage both sim-to-real and real-to-sim transfers. This integration is facilitated by data pipelines that connect the RGA module with the existing components of the DT framework, including the historical repository and the simulation model. A case study on a pedestrian bridge at Carnegie Mellon University showcases the performance of different levels of integration of our approach with an existing framework. With full implementation of an RGA module and a complete data pipeline, our approach is capable of bidirectional knowledge transfer between simulations and real-world operations without compromising efficiency.",
    "categories": [
      "cs.CE",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CE",
    "comment": "Accepted and presented at 2024 ASCE International Conference on Computing in Civil Engineering (i3CE 2024)",
    "pdf_url": "https://arxiv.org/pdf/2507.18449v1",
    "published_date": "2025-05-15 12:56:36 UTC",
    "updated_date": "2025-05-15 12:56:36 UTC"
  },
  {
    "arxiv_id": "2505.10231v1",
    "title": "On the Interplay of Human-AI Alignment,Fairness, and Performance Trade-offs in Medical Imaging",
    "authors": [
      "Haozhe Luo",
      "Ziyu Zhou",
      "Zixin Shu",
      "Aurélie Pahud de Mortanges",
      "Robert Berke",
      "Mauricio Reyes"
    ],
    "abstract": "Deep neural networks excel in medical imaging but remain prone to biases, leading to fairness gaps across demographic groups. We provide the first systematic exploration of Human-AI alignment and fairness in this domain. Our results show that incorporating human insights consistently reduces fairness gaps and enhances out-of-domain generalization, though excessive alignment can introduce performance trade-offs, emphasizing the need for calibrated strategies. These findings highlight Human-AI alignment as a promising approach for developing fair, robust, and generalizable medical AI systems, striking a balance between expert guidance and automated efficiency. Our code is available at https://github.com/Roypic/Aligner.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10231v1",
    "published_date": "2025-05-15 12:43:23 UTC",
    "updated_date": "2025-05-15 12:43:23 UTC"
  },
  {
    "arxiv_id": "2505.10212v1",
    "title": "Do LLMs Memorize Recommendation Datasets? A Preliminary Study on MovieLens-1M",
    "authors": [
      "Dario Di Palma",
      "Felice Antonio Merra",
      "Maurizio Sfilio",
      "Vito Walter Anelli",
      "Fedelucio Narducci",
      "Tommaso Di Noia"
    ],
    "abstract": "Large Language Models (LLMs) have become increasingly central to recommendation scenarios due to their remarkable natural language understanding and generation capabilities. Although significant research has explored the use of LLMs for various recommendation tasks, little effort has been dedicated to verifying whether they have memorized public recommendation dataset as part of their training data. This is undesirable because memorization reduces the generalizability of research findings, as benchmarking on memorized datasets does not guarantee generalization to unseen datasets. Furthermore, memorization can amplify biases, for example, some popular items may be recommended more frequently than others.\n  In this work, we investigate whether LLMs have memorized public recommendation datasets. Specifically, we examine two model families (GPT and Llama) across multiple sizes, focusing on one of the most widely used dataset in recommender systems: MovieLens-1M. First, we define dataset memorization as the extent to which item attributes, user profiles, and user-item interactions can be retrieved by prompting the LLMs. Second, we analyze the impact of memorization on recommendation performance. Lastly, we examine whether memorization varies across model families and model sizes. Our results reveal that all models exhibit some degree of memorization of MovieLens-1M, and that recommendation performance is related to the extent of memorization. We have made all the code publicly available at: https://github.com/sisinflab/LLM-MemoryInspector",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10212v1",
    "published_date": "2025-05-15 12:16:36 UTC",
    "updated_date": "2025-05-15 12:16:36 UTC"
  },
  {
    "arxiv_id": "2505.10201v1",
    "title": "A Fine-Grained Complexity View on Propositional Abduction -- Algorithms and Lower Bounds",
    "authors": [
      "Victor Lagerkvist",
      "Mohamed Maizia",
      "Johannes Schmidt"
    ],
    "abstract": "The Boolean satisfiability problem (SAT) is a well-known example of monotonic reasoning, of intense practical interest due to fast solvers, complemented by rigorous fine-grained complexity results. However, for non-monotonic reasoning, e.g., abductive reasoning, comparably little is known outside classic complexity theory. In this paper we take a first step of bridging the gap between monotonic and non-monotonic reasoning by analyzing the complexity of intractable abduction problems under the seemingly overlooked but natural parameter n: the number of variables in the knowledge base. We obtain several positive results for $Σ^P_2$- as well as NP- and coNP-complete fragments, which implies the first example of beating exhaustive search for a $Σ^P_2$-complete problem (to the best of our knowledge). We complement this with lower bounds and for many fragments rule out improvements under the (strong) exponential-time hypothesis.",
    "categories": [
      "cs.CC",
      "cs.AI"
    ],
    "primary_category": "cs.CC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10201v1",
    "published_date": "2025-05-15 11:56:19 UTC",
    "updated_date": "2025-05-15 11:56:19 UTC"
  },
  {
    "arxiv_id": "2505.10197v1",
    "title": "Advancing Community Detection with Graph Convolutional Neural Networks: Bridging Topological and Attributive Cohesion",
    "authors": [
      "Anjali de Silva",
      "Gang Chen",
      "Hui Ma",
      "Seyed Mohammad Nekooei",
      "Xingquan Zuo"
    ],
    "abstract": "Community detection, a vital technology for real-world applications, uncovers cohesive node groups (communities) by leveraging both topological and attribute similarities in social networks. However, existing Graph Convolutional Networks (GCNs) trained to maximize modularity often converge to suboptimal solutions. Additionally, directly using human-labeled communities for training can undermine topological cohesiveness by grouping disconnected nodes based solely on node attributes. We address these issues by proposing a novel Topological and Attributive Similarity-based Community detection (TAS-Com) method. TAS-Com introduces a novel loss function that exploits the highly effective and scalable Leiden algorithm to detect community structures with global optimal modularity. Leiden is further utilized to refine human-labeled communities to ensure connectivity within each community, enabling TAS-Com to detect community structures with desirable trade-offs between modularity and compliance with human labels. Experimental results on multiple benchmark networks confirm that TAS-Com can significantly outperform several state-of-the-art algorithms.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "This paper has been accepted by IJCAI (International Joint Conference on Artificial Intelligence) 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.10197v1",
    "published_date": "2025-05-15 11:53:33 UTC",
    "updated_date": "2025-05-15 11:53:33 UTC"
  },
  {
    "arxiv_id": "2505.10191v1",
    "title": "LanTu: Dynamics-Enhanced Deep Learning for Eddy-Resolving Ocean Forecasting",
    "authors": [
      "Qingyu Zheng",
      "Qi Shao",
      "Guijun Han",
      "Wei Li",
      "Hong Li",
      "Xuan Wang"
    ],
    "abstract": "Mesoscale eddies dominate the spatiotemporal multiscale variability of the ocean, and their impact on the energy cascade of the global ocean cannot be ignored. Eddy-resolving ocean forecasting is providing more reliable protection for fisheries and navigational safety, but also presents significant scientific challenges and high computational costs for traditional numerical models. Artificial intelligence (AI)-based weather and ocean forecasting systems are becoming powerful tools that balance forecast performance with computational efficiency. However, the complex multiscale features in the ocean dynamical system make AI models still face many challenges in mesoscale eddy forecasting (especially regional modelling). Here, we develop LanTu, a regional eddy-resolving ocean forecasting system based on dynamics-enhanced deep learning. We incorporate cross-scale interactions into LanTu and construct multiscale physical constraint for optimising LanTu guided by knowledge of eddy dynamics in order to improve the forecasting skill of LanTu for mesoscale evolution. The results show that LanTu outperforms the existing advanced operational numerical ocean forecasting system (NOFS) and AI-based ocean forecasting system (AI-OFS) in temperature, salinity, sea level anomaly and current prediction, with a lead time of more than 10 days. Our study highlights that dynamics-enhanced deep learning (LanTu) can be a powerful paradigm for eddy-resolving ocean forecasting.",
    "categories": [
      "physics.ao-ph",
      "cs.AI",
      "cs.LG",
      "nlin.CD"
    ],
    "primary_category": "physics.ao-ph",
    "comment": "22 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.10191v1",
    "published_date": "2025-05-15 11:47:54 UTC",
    "updated_date": "2025-05-15 11:47:54 UTC"
  },
  {
    "arxiv_id": "2505.10188v1",
    "title": "A User Study Evaluating Argumentative Explanations in Diagnostic Decision Support",
    "authors": [
      "Felix Liedeker",
      "Olivia Sanchez-Graillet",
      "Moana Seidler",
      "Christian Brandt",
      "Jörg Wellmer",
      "Philipp Cimiano"
    ],
    "abstract": "As the field of healthcare increasingly adopts artificial intelligence, it becomes important to understand which types of explanations increase transparency and empower users to develop confidence and trust in the predictions made by machine learning (ML) systems. In shared decision-making scenarios where doctors cooperate with ML systems to reach an appropriate decision, establishing mutual trust is crucial. In this paper, we explore different approaches to generating explanations in eXplainable AI (XAI) and make their underlying arguments explicit so that they can be evaluated by medical experts. In particular, we present the findings of a user study conducted with physicians to investigate their perceptions of various types of AI-generated explanations in the context of diagnostic decision support. The study aims to identify the most effective and useful explanations that enhance the diagnostic process. In the study, medical doctors filled out a survey to assess different types of explanations. Further, an interview was carried out post-survey to gain qualitative insights on the requirements of explanations incorporated in diagnostic decision support. Overall, the insights gained from this study contribute to understanding the types of explanations that are most effective.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Presented at 'The First Workshop on Natural Language Argument-Based Explanations', co-located with ECAI 2024",
    "pdf_url": "https://arxiv.org/pdf/2505.10188v1",
    "published_date": "2025-05-15 11:42:24 UTC",
    "updated_date": "2025-05-15 11:42:24 UTC"
  },
  {
    "arxiv_id": "2505.10185v1",
    "title": "The CoT Encyclopedia: Analyzing, Predicting, and Controlling how a Reasoning Model will Think",
    "authors": [
      "Seongyun Lee",
      "Seungone Kim",
      "Minju Seo",
      "Yongrae Jo",
      "Dongyoung Go",
      "Hyeonbin Hwang",
      "Jinho Park",
      "Xiang Yue",
      "Sean Welleck",
      "Graham Neubig",
      "Moontae Lee",
      "Minjoon Seo"
    ],
    "abstract": "Long chain-of-thought (CoT) is an essential ingredient in effective usage of modern large language models, but our understanding of the reasoning strategies underlying these capabilities remains limited. While some prior works have attempted to categorize CoTs using predefined strategy types, such approaches are constrained by human intuition and fail to capture the full diversity of model behaviors. In this work, we introduce the CoT Encyclopedia, a bottom-up framework for analyzing and steering model reasoning. Our method automatically extracts diverse reasoning criteria from model-generated CoTs, embeds them into a semantic space, clusters them into representative categories, and derives contrastive rubrics to interpret reasoning behavior. Human evaluations show that this framework produces more interpretable and comprehensive analyses than existing methods. Moreover, we demonstrate that this understanding enables performance gains: we can predict which strategy a model is likely to use and guide it toward more effective alternatives. Finally, we provide practical insights, such as that training data format (e.g., free-form vs. multiple-choice) has a far greater impact on reasoning behavior than data domain, underscoring the importance of format-aware model design.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Work in progress",
    "pdf_url": "https://arxiv.org/pdf/2505.10185v1",
    "published_date": "2025-05-15 11:31:02 UTC",
    "updated_date": "2025-05-15 11:31:02 UTC"
  },
  {
    "arxiv_id": "2505.10183v1",
    "title": "KAITIAN: A Unified Communication Framework for Enabling Efficient Collaboration Across Heterogeneous Accelerators in Embodied AI Systems",
    "authors": [
      "Jieke Lin",
      "Wanyu Wang",
      "Longxiang Yin",
      "Yinhe Han"
    ],
    "abstract": "Embodied Artificial Intelligence (AI) systems, such as autonomous robots and intelligent vehicles, are increasingly reliant on diverse heterogeneous accelerators (e.g., GPGPUs, NPUs, FPGAs) to meet stringent real-time processing and energy-efficiency demands. However, the proliferation of vendor-specific proprietary communication libraries creates significant interoperability barriers, hindering seamless collaboration between different accelerator types and leading to suboptimal resource utilization and performance bottlenecks in distributed AI workloads. This paper introduces KAITIAN, a novel distributed communication framework designed to bridge this gap. KAITIAN provides a unified abstraction layer that intelligently integrates vendor-optimized communication libraries for intra-group efficiency with general-purpose communication protocols for inter-group interoperability. Crucially, it incorporates a load-adaptive scheduling mechanism that dynamically balances computational tasks across heterogeneous devices based on their real-time performance characteristics. Implemented as an extension to PyTorch and rigorously evaluated on a testbed featuring NVIDIA GPUs and Cambricon MLUs, KAITIAN demonstrates significant improvements in resource utilization and scalability for distributed training tasks. Experimental results show that KAITIAN can accelerate training time by up to 42% compared to baseline homogeneous systems, while incurring minimal communication overhead (2.8--4.3%) and maintaining model accuracy. KAITIAN paves the way for more flexible and powerful heterogeneous computing in complex embodied AI applications.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "9 pages, 4 figures. Jieke Lin and Wanyu Wang contributed equally to this work",
    "pdf_url": "https://arxiv.org/pdf/2505.10183v1",
    "published_date": "2025-05-15 11:29:43 UTC",
    "updated_date": "2025-05-15 11:29:43 UTC"
  },
  {
    "arxiv_id": "2505.10172v1",
    "title": "Does Scaling Law Apply in Time Series Forecasting?",
    "authors": [
      "Zeyan Li",
      "Libing Chen",
      "Yin Tang"
    ],
    "abstract": "Rapid expansion of model size has emerged as a key challenge in time series forecasting. From early Transformer with tens of megabytes to recent architectures like TimesNet with thousands of megabytes, performance gains have often come at the cost of exponentially increasing parameter counts. But is this scaling truly necessary? To question the applicability of the scaling law in time series forecasting, we propose Alinear, an ultra-lightweight forecasting model that achieves competitive performance using only k-level parameters. We introduce a horizon-aware adaptive decomposition mechanism that dynamically rebalances component emphasis across different forecast lengths, alongside a progressive frequency attenuation strategy that achieves stable prediction in various forecasting horizons without incurring the computational overhead of attention mechanisms. Extensive experiments on seven benchmark datasets demonstrate that Alinear consistently outperforms large-scale models while using less than 1% of their parameters, maintaining strong accuracy across both short and ultra-long forecasting horizons. Moreover, to more fairly evaluate model efficiency, we propose a new parameter-aware evaluation metric that highlights the superiority of ALinear under constrained model budgets. Our analysis reveals that the relative importance of trend and seasonal components varies depending on data characteristics rather than following a fixed pattern, validating the necessity of our adaptive design. This work challenges the prevailing belief that larger models are inherently better and suggests a paradigm shift toward more efficient time series modeling.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10172v1",
    "published_date": "2025-05-15 11:04:39 UTC",
    "updated_date": "2025-05-15 11:04:39 UTC"
  },
  {
    "arxiv_id": "2505.10597v2",
    "title": "Two Minds Better Than One: Collaborative Reward Modeling for LLM Alignment",
    "authors": [
      "Jiazheng Zhang",
      "Wenqing Jing",
      "Zizhuo Zhang",
      "Zhiheng Xi",
      "Shihan Dou",
      "Rongxiang Weng",
      "Jiahuan Li",
      "Jingang Wang",
      "Mingxu Chai",
      "Shibo Hong",
      "Tao Gui",
      "Qi Zhang"
    ],
    "abstract": "Reward models (RMs) play a pivotal role in aligning large language models (LLMs) with human values. However, noisy preferences in human feedback can lead to reward misgeneralization - a phenomenon where reward models learn spurious correlations or overfit to noisy preferences, which poses important challenges to the generalization of RMs. This paper systematically analyzes the characteristics of preference pairs and aims to identify how noisy preferences differ from human-aligned preferences in reward modeling. Our analysis reveals that noisy preferences are difficult for RMs to fit, as they cause sharp training fluctuations and irregular gradient updates. These distinctive dynamics suggest the feasibility of identifying and excluding such noisy preferences. Empirical studies demonstrate that policy LLM optimized with a reward model trained on the full preference dataset, which includes substantial noise, performs worse than the one trained on a subset of exclusively high quality preferences. To address this challenge, we propose an online Collaborative Reward Modeling (CRM) framework to achieve robust preference learning through peer review and curriculum learning. In particular, CRM maintains two RMs that collaboratively filter potential noisy preferences by peer-reviewing each other's data selections. Curriculum learning synchronizes the capabilities of two models, mitigating excessive disparities to promote the utility of peer review. Extensive experiments demonstrate that CRM significantly enhances RM generalization, with up to 9.94 points improvement on RewardBench under an extreme 40\\% noise. Moreover, CRM can seamlessly extend to implicit-reward alignment methods, offering a robust and versatile alignment strategy.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10597v2",
    "published_date": "2025-05-15 10:58:20 UTC",
    "updated_date": "2025-05-19 03:28:14 UTC"
  },
  {
    "arxiv_id": "2505.10169v2",
    "title": "Modeling Saliency Dataset Bias",
    "authors": [
      "Matthias Kümmerer",
      "Harneet Singh Khanuja",
      "Matthias Bethge"
    ],
    "abstract": "Recent advances in image-based saliency prediction are approaching gold standard performance levels on existing benchmarks. Despite this success, we show that predicting fixations across multiple saliency datasets remains challenging due to dataset bias. We find a significant performance drop (around 40%) when models trained on one dataset are applied to another. Surprisingly, increasing dataset diversity does not resolve this inter-dataset gap, with close to 60% attributed to dataset-specific biases. To address this remaining generalization gap, we propose a novel architecture extending a mostly dataset-agnostic encoder-decoder structure with fewer than 20 dataset-specific parameters that govern interpretable mechanisms such as multi-scale structure, center bias, and fixation spread. Adapting only these parameters to new data accounts for more than 75% of the generalization gap, with a large fraction of the improvement achieved with as few as 50 samples. Our model sets a new state-of-the-art on all three datasets of the MIT/Tuebingen Saliency Benchmark (MIT300, CAT2000, and COCO-Freeview), even when purely generalizing from unrelated datasets, but with a substantial boost when adapting to the respective training datasets. The model also provides valuable insights into spatial saliency properties, revealing complex multi-scale effects that combine both absolute and relative sizes.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "published at ICCV 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.10169v2",
    "published_date": "2025-05-15 10:55:47 UTC",
    "updated_date": "2025-09-30 08:54:09 UTC"
  },
  {
    "arxiv_id": "2505.10167v3",
    "title": "QuXAI: Explainers for Hybrid Quantum Machine Learning Models",
    "authors": [
      "Saikat Barua",
      "Mostafizur Rahman",
      "Shehenaz Khaled",
      "Md Jafor Sadek",
      "Rafiul Islam",
      "Shahnewaz Siddique"
    ],
    "abstract": "The emergence of hybrid quantum-classical machine learning (HQML) models opens new horizons of computational intelligence but their fundamental complexity frequently leads to black box behavior that undermines transparency and reliability in their application. Although XAI for quantum systems still in its infancy, a major research gap is evident in robust global and local explainability approaches that are designed for HQML architectures that employ quantized feature encoding followed by classical learning. The gap is the focus of this work, which introduces QuXAI, an framework based upon Q-MEDLEY, an explainer for explaining feature importance in these hybrid systems. Our model entails the creation of HQML models incorporating quantum feature maps, the use of Q-MEDLEY, which combines feature based inferences, preserving the quantum transformation stage and visualizing the resulting attributions. Our result shows that Q-MEDLEY delineates influential classical aspects in HQML models, as well as separates their noise, and competes well against established XAI techniques in classical validation settings. Ablation studies more significantly expose the virtues of the composite structure used in Q-MEDLEY. The implications of this work are critically important, as it provides a route to improve the interpretability and reliability of HQML models, thus promoting greater confidence and being able to engage in safer and more responsible use of quantum-enhanced AI technology.\n  Our code and experiments are open-sourced at: https://github.com/GitsSaikat/QuXAI",
    "categories": [
      "cs.LG",
      "cs.AI",
      "quant-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages, 6 figures, 7 equations",
    "pdf_url": "https://arxiv.org/pdf/2505.10167v3",
    "published_date": "2025-05-15 10:51:34 UTC",
    "updated_date": "2025-06-12 09:36:53 UTC"
  },
  {
    "arxiv_id": "2505.10134v1",
    "title": "Large Wireless Localization Model (LWLM): A Foundation Model for Positioning in 6G Networks",
    "authors": [
      "Guangjin Pan",
      "Kaixuan Huang",
      "Hui Chen",
      "Shunqing Zhang",
      "Christian Häger",
      "Henk Wymeersch"
    ],
    "abstract": "Accurate and robust localization is a critical enabler for emerging 5G and 6G applications, including autonomous driving, extended reality (XR), and smart manufacturing. While data-driven approaches have shown promise, most existing models require large amounts of labeled data and struggle to generalize across deployment scenarios and wireless configurations. To address these limitations, we propose a foundation-model-based solution tailored for wireless localization. We first analyze how different self-supervised learning (SSL) tasks acquire general-purpose and task-specific semantic features based on information bottleneck (IB) theory. Building on this foundation, we design a pretraining methodology for the proposed Large Wireless Localization Model (LWLM). Specifically, we propose an SSL framework that jointly optimizes three complementary objectives: (i) spatial-frequency masked channel modeling (SF-MCM), (ii) domain-transformation invariance (DTI), and (iii) position-invariant contrastive learning (PICL). These objectives jointly capture the underlying semantics of wireless channel from multiple perspectives. We further design lightweight decoders for key downstream tasks, including time-of-arrival (ToA) estimation, angle-of-arrival (AoA) estimation, single base station (BS) localization, and multiple BS localization. Comprehensive experimental results confirm that LWLM consistently surpasses both model-based and supervised learning baselines across all localization tasks. In particular, LWLM achieves 26.0%--87.5% improvement over transformer models without pretraining, and exhibits strong generalization under label-limited fine-tuning and unseen BS configurations, confirming its potential as a foundation model for wireless localization.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "13 pages,16 figures.This work has been submitted to the IEEE for possible publication",
    "pdf_url": "https://arxiv.org/pdf/2505.10134v1",
    "published_date": "2025-05-15 10:04:44 UTC",
    "updated_date": "2025-05-15 10:04:44 UTC"
  },
  {
    "arxiv_id": "2505.10596v1",
    "title": "Inclusivity of AI Speech in Healthcare: A Decade Look Back",
    "authors": [
      "Retno Larasati"
    ],
    "abstract": "The integration of AI speech recognition technologies into healthcare has the potential to revolutionize clinical workflows and patient-provider communication. However, this study reveals significant gaps in inclusivity, with datasets and research disproportionately favouring high-resource languages, standardized accents, and narrow demographic groups. These biases risk perpetuating healthcare disparities, as AI systems may misinterpret speech from marginalized groups. This paper highlights the urgent need for inclusive dataset design, bias mitigation research, and policy frameworks to ensure equitable access to AI speech technologies in healthcare.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10596v1",
    "published_date": "2025-05-15 10:03:05 UTC",
    "updated_date": "2025-05-15 10:03:05 UTC"
  },
  {
    "arxiv_id": "2505.10128v1",
    "title": "Robust Federated Learning on Edge Devices with Domain Heterogeneity",
    "authors": [
      "Huy Q. Le",
      "Latif U. Khan",
      "Choong Seon Hong"
    ],
    "abstract": "Federated Learning (FL) allows collaborative training while ensuring data privacy across distributed edge devices, making it a popular solution for privacy-sensitive applications. However, FL faces significant challenges due to statistical heterogeneity, particularly domain heterogeneity, which impedes the global mode's convergence. In this study, we introduce a new framework to address this challenge by improving the generalization ability of the FL global model under domain heterogeneity, using prototype augmentation. Specifically, we introduce FedAPC (Federated Augmented Prototype Contrastive Learning), a prototype-based FL framework designed to enhance feature diversity and model robustness. FedAPC leverages prototypes derived from the mean features of augmented data to capture richer representations. By aligning local features with global prototypes, we enable the model to learn meaningful semantic features while reducing overfitting to any specific domain. Experimental results on the Office-10 and Digits datasets illustrate that our framework outperforms SOTA baselines, demonstrating superior performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "IWCMC 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.10128v1",
    "published_date": "2025-05-15 09:53:14 UTC",
    "updated_date": "2025-05-15 09:53:14 UTC"
  },
  {
    "arxiv_id": "2505.10120v1",
    "title": "All You Need Is Synthetic Task Augmentation",
    "authors": [
      "Guillaume Godin"
    ],
    "abstract": "Injecting rule-based models like Random Forests into differentiable neural network frameworks remains an open challenge in machine learning. Recent advancements have demonstrated that pretrained models can generate efficient molecular embeddings. However, these approaches often require extensive pretraining and additional techniques, such as incorporating posterior probabilities, to boost performance. In our study, we propose a novel strategy that jointly trains a single Graph Transformer neural network on both sparse multitask molecular property experimental targets and synthetic targets derived from XGBoost models trained on Osmordred molecular descriptors. These synthetic tasks serve as independent auxiliary tasks. Our results show consistent and significant performance improvement across all 19 molecular property prediction tasks. For 16 out of 19 targets, the multitask Graph Transformer outperforms the XGBoost single-task learner. This demonstrates that synthetic task augmentation is an effective method for enhancing neural model performance in multitask molecular property prediction without the need for feature injection or pretraining.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages, 3 Figures, 6 tables",
    "pdf_url": "https://arxiv.org/pdf/2505.10120v1",
    "published_date": "2025-05-15 09:46:27 UTC",
    "updated_date": "2025-05-15 09:46:27 UTC"
  },
  {
    "arxiv_id": "2505.11550v1",
    "title": "AI-generated Text Detection: A Multifaceted Approach to Binary and Multiclass Classification",
    "authors": [
      "Harika Abburi",
      "Sanmitra Bhattacharya",
      "Edward Bowen",
      "Nirmala Pudota"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in generating text that closely resembles human writing across a wide range of styles and genres. However, such capabilities are prone to potential misuse, such as fake news generation, spam email creation, and misuse in academic assignments. As a result, accurate detection of AI-generated text and identification of the model that generated it are crucial for maintaining the responsible use of LLMs. In this work, we addressed two sub-tasks put forward by the Defactify workshop under AI-Generated Text Detection shared task at the Association for the Advancement of Artificial Intelligence (AAAI 2025): Task A involved distinguishing between human-authored or AI-generated text, while Task B focused on attributing text to its originating language model. For each task, we proposed two neural architectures: an optimized model and a simpler variant. For Task A, the optimized neural architecture achieved fifth place with $F1$ score of 0.994, and for Task B, the simpler neural architecture also ranked fifth place with $F1$ score of 0.627.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11550v1",
    "published_date": "2025-05-15 09:28:06 UTC",
    "updated_date": "2025-05-15 09:28:06 UTC"
  },
  {
    "arxiv_id": "2505.10105v1",
    "title": "EmbodiedMAE: A Unified 3D Multi-Modal Representation for Robot Manipulation",
    "authors": [
      "Zibin Dong",
      "Fei Ni",
      "Yifu Yuan",
      "Yinchuan Li",
      "Jianye Hao"
    ],
    "abstract": "We present EmbodiedMAE, a unified 3D multi-modal representation for robot manipulation. Current approaches suffer from significant domain gaps between training datasets and robot manipulation tasks, while also lacking model architectures that can effectively incorporate 3D information. To overcome these limitations, we enhance the DROID dataset with high-quality depth maps and point clouds, constructing DROID-3D as a valuable supplement for 3D embodied vision research. Then we develop EmbodiedMAE, a multi-modal masked autoencoder that simultaneously learns representations across RGB, depth, and point cloud modalities through stochastic masking and cross-modal fusion. Trained on DROID-3D, EmbodiedMAE consistently outperforms state-of-the-art vision foundation models (VFMs) in both training efficiency and final performance across 70 simulation tasks and 20 real-world robot manipulation tasks on two robot platforms. The model exhibits strong scaling behavior with size and promotes effective policy learning from 3D inputs. Experimental results establish EmbodiedMAE as a reliable unified 3D multi-modal VFM for embodied AI systems, particularly in precise tabletop manipulation settings where spatial perception is critical.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10105v1",
    "published_date": "2025-05-15 09:12:17 UTC",
    "updated_date": "2025-05-15 09:12:17 UTC"
  },
  {
    "arxiv_id": "2505.10101v1",
    "title": "LAV: Audio-Driven Dynamic Visual Generation with Neural Compression and StyleGAN2",
    "authors": [
      "Jongmin Jung",
      "Dasaem Jeong"
    ],
    "abstract": "This paper introduces LAV (Latent Audio-Visual), a system that integrates EnCodec's neural audio compression with StyleGAN2's generative capabilities to produce visually dynamic outputs driven by pre-recorded audio. Unlike previous works that rely on explicit feature mappings, LAV uses EnCodec embeddings as latent representations, directly transformed into StyleGAN2's style latent space via randomly initialized linear mapping. This approach preserves semantic richness in the transformation, enabling nuanced and semantically coherent audio-visual translations. The framework demonstrates the potential of using pretrained audio compression models for artistic and computational applications.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.GR",
      "cs.MM",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Paper accepted at ISEA 2025, The 30th International Symposium on Electronic/Emerging Art, Seoul, Republic of Korea, 23 - 29 May 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.10101v1",
    "published_date": "2025-05-15 09:04:12 UTC",
    "updated_date": "2025-05-15 09:04:12 UTC"
  },
  {
    "arxiv_id": "2505.10093v1",
    "title": "From Text to Network: Constructing a Knowledge Graph of Taiwan-Based China Studies Using Generative AI",
    "authors": [
      "Hsuan-Lei Shao"
    ],
    "abstract": "Taiwanese China Studies (CS) has developed into a rich, interdisciplinary research field shaped by the unique geopolitical position and long standing academic engagement with Mainland China. This study responds to the growing need to systematically revisit and reorganize decades of Taiwan based CS scholarship by proposing an AI assisted approach that transforms unstructured academic texts into structured, interactive knowledge representations. We apply generative AI (GAI) techniques and large language models (LLMs) to extract and standardize entity relation triples from 1,367 peer reviewed CS articles published between 1996 and 2019. These triples are then visualized through a lightweight D3.js based system, forming the foundation of a domain specific knowledge graph and vector database for the field. This infrastructure allows users to explore conceptual nodes and semantic relationships across the corpus, revealing previously uncharted intellectual trajectories, thematic clusters, and research gaps. By decomposing textual content into graph structured knowledge units, our system enables a paradigm shift from linear text consumption to network based knowledge navigation. In doing so, it enhances scholarly access to CS literature while offering a scalable, data driven alternative to traditional ontology construction. This work not only demonstrates how generative AI can augment area studies and digital humanities but also highlights its potential to support a reimagined scholarly infrastructure for regional knowledge systems.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "4 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.10093v1",
    "published_date": "2025-05-15 08:51:53 UTC",
    "updated_date": "2025-05-15 08:51:53 UTC"
  },
  {
    "arxiv_id": "2505.10074v2",
    "title": "Leveraging Graph Retrieval-Augmented Generation to Support Learners' Understanding of Knowledge Concepts in MOOCs",
    "authors": [
      "Mohamed Abdelmagied",
      "Mohamed Amine Chatti",
      "Shoeb Joarder",
      "Qurat Ul Ain",
      "Rawaa Alatrash"
    ],
    "abstract": "Massive Open Online Courses (MOOCs) lack direct interaction between learners and instructors, making it challenging for learners to understand new knowledge concepts. Recently, learners have increasingly used Large Language Models (LLMs) to support them in acquiring new knowledge. However, LLMs are prone to hallucinations which limits their reliability. Retrieval-Augmented Generation (RAG) addresses this issue by retrieving relevant documents before generating a response. However, the application of RAG across different MOOCs is limited by unstructured learning material. Furthermore, current RAG systems do not actively guide learners toward their learning needs. To address these challenges, we propose a Graph RAG pipeline that leverages Educational Knowledge Graphs (EduKGs) and Personal Knowledge Graphs (PKGs) to guide learners to understand knowledge concepts in the MOOC platform CourseMapper. Specifically, we implement (1) a PKG-based Question Generation method to recommend personalized questions for learners in context, and (2) an EduKG-based Question Answering method that leverages the relationships between knowledge concepts in the EduKG to answer learner selected questions. To evaluate both methods, we conducted a study with 3 expert instructors on 3 different MOOCs in the MOOC platform CourseMapper. The results of the evaluation show the potential of Graph RAG to empower learners to understand new knowledge concepts in a personalized learning experience.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at EMOOCs 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.10074v2",
    "published_date": "2025-05-15 08:24:47 UTC",
    "updated_date": "2025-05-16 15:33:49 UTC"
  },
  {
    "arxiv_id": "2505.10073v1",
    "title": "Multi-Robot Task Allocation for Homogeneous Tasks with Collision Avoidance via Spatial Clustering",
    "authors": [
      "Rathin Chandra Shit",
      "Sharmila Subudhi"
    ],
    "abstract": "In this paper, a novel framework is presented that achieves a combined solution based on Multi-Robot Task Allocation (MRTA) and collision avoidance with respect to homogeneous measurement tasks taking place in industrial environments. The spatial clustering we propose offers to simultaneously solve the task allocation problem and deal with collision risks by cutting the workspace into distinguishable operational zones for each robot. To divide task sites and to schedule robot routes within corresponding clusters, we use K-means clustering and the 2-Opt algorithm. The presented framework shows satisfactory performance, where up to 93\\% time reduction (1.24s against 17.62s) with a solution quality improvement of up to 7\\% compared to the best performing method is demonstrated. Our method also completely eliminates collision points that persist in comparative methods in a most significant sense. Theoretical analysis agrees with the claim that spatial partitioning unifies the apparently disjoint tasks allocation and collision avoidance problems under conditions of many identical tasks to be distributed over sparse geographical areas. Ultimately, the findings in this work are of substantial importance for real world applications where both computational efficiency and operation free from collisions is of paramount importance.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "5 pages, 4 figures, Scheduled for presentation at an upcoming conference",
    "pdf_url": "https://arxiv.org/pdf/2505.10073v1",
    "published_date": "2025-05-15 08:20:57 UTC",
    "updated_date": "2025-05-15 08:20:57 UTC"
  },
  {
    "arxiv_id": "2505.11548v3",
    "title": "One Shot Dominance: Knowledge Poisoning Attack on Retrieval-Augmented Generation Systems",
    "authors": [
      "Zhiyuan Chang",
      "Mingyang Li",
      "Xiaojun Jia",
      "Junjie Wang",
      "Yuekai Huang",
      "Ziyou Jiang",
      "Yang Liu",
      "Qing Wang"
    ],
    "abstract": "Large Language Models (LLMs) enhanced with Retrieval-Augmented Generation (RAG) have shown improved performance in generating accurate responses. However, the dependence on external knowledge bases introduces potential security vulnerabilities, particularly when these knowledge bases are publicly accessible and modifiable. While previous studies have exposed knowledge poisoning risks in RAG systems, existing attack methods suffer from critical limitations: they either require injecting multiple poisoned documents (resulting in poor stealthiness) or can only function effectively on simplistic queries (limiting real-world applicability). This paper reveals a more realistic knowledge poisoning attack against RAG systems that achieves successful attacks by poisoning only a single document while remaining effective for complex multi-hop questions involving complex relationships between multiple elements. Our proposed AuthChain address three challenges to ensure the poisoned documents are reliably retrieved and trusted by the LLM, even against large knowledge bases and LLM's own knowledge. Extensive experiments across six popular LLMs demonstrate that AuthChain achieves significantly higher attack success rates while maintaining superior stealthiness against RAG defense mechanisms compared to state-of-the-art baselines.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "15pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.11548v3",
    "published_date": "2025-05-15 08:14:58 UTC",
    "updated_date": "2025-09-01 02:28:23 UTC"
  },
  {
    "arxiv_id": "2505.10594v1",
    "title": "CRPE: Expanding The Reasoning Capability of Large Language Model for Code Generation",
    "authors": [
      "Ningxin Gui",
      "Qianghuai Jia",
      "Feijun Jiang",
      "Yuling Jiao",
      "dechun wang",
      "Jerry Zhijian Yang"
    ],
    "abstract": "We introduce CRPE (Code Reasoning Process Enhancer), an innovative three-stage framework for data synthesis and model training that advances the development of sophisticated code reasoning capabilities in large language models (LLMs). Building upon existing system-1 models, CRPE addresses the fundamental challenge of enhancing LLMs' analytical and logical processing in code generation tasks. Our framework presents a methodologically rigorous yet implementable approach to cultivating advanced code reasoning abilities in language models. Through the implementation of CRPE, we successfully develop an enhanced COT-Coder that demonstrates marked improvements in code generation tasks. Evaluation results on LiveCodeBench (20240701-20240901) demonstrate that our COT-Coder-7B-StepDPO, derived from Qwen2.5-Coder-7B-Base, with a pass@1 accuracy of 21.88, exceeds all models with similar or even larger sizes. Furthermore, our COT-Coder-32B-StepDPO, based on Qwen2.5-Coder-32B-Base, exhibits superior performance with a pass@1 accuracy of 35.08, outperforming GPT4O on the benchmark. Overall, CRPE represents a comprehensive, open-source method that encompasses the complete pipeline from instruction data acquisition through expert code reasoning data synthesis, culminating in an autonomous reasoning enhancement mechanism.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10594v1",
    "published_date": "2025-05-15 08:13:45 UTC",
    "updated_date": "2025-05-15 08:13:45 UTC"
  },
  {
    "arxiv_id": "2505.10066v1",
    "title": "Dark LLMs: The Growing Threat of Unaligned AI Models",
    "authors": [
      "Michael Fire",
      "Yitzhak Elbazis",
      "Adi Wasenstein",
      "Lior Rokach"
    ],
    "abstract": "Large Language Models (LLMs) rapidly reshape modern life, advancing fields from healthcare to education and beyond. However, alongside their remarkable capabilities lies a significant threat: the susceptibility of these models to jailbreaking. The fundamental vulnerability of LLMs to jailbreak attacks stems from the very data they learn from. As long as this training data includes unfiltered, problematic, or 'dark' content, the models can inherently learn undesirable patterns or weaknesses that allow users to circumvent their intended safety controls. Our research identifies the growing threat posed by dark LLMs models deliberately designed without ethical guardrails or modified through jailbreak techniques. In our research, we uncovered a universal jailbreak attack that effectively compromises multiple state-of-the-art models, enabling them to answer almost any question and produce harmful outputs upon request. The main idea of our attack was published online over seven months ago. However, many of the tested LLMs were still vulnerable to this attack. Despite our responsible disclosure efforts, responses from major LLM providers were often inadequate, highlighting a concerning gap in industry practices regarding AI safety. As model training becomes more accessible and cheaper, and as open-source LLMs proliferate, the risk of widespread misuse escalates. Without decisive intervention, LLMs may continue democratizing access to dangerous knowledge, posing greater risks than anticipated.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10066v1",
    "published_date": "2025-05-15 08:07:04 UTC",
    "updated_date": "2025-05-15 08:07:04 UTC"
  },
  {
    "arxiv_id": "2505.10055v2",
    "title": "PsOCR: Benchmarking Large Multimodal Models for Optical Character Recognition in Low-resource Pashto Language",
    "authors": [
      "Ijazul Haq",
      "Yingjie Zhang",
      "Irfan Ali Khan"
    ],
    "abstract": "This paper evaluates the performance of Large Multimodal Models (LMMs) on Optical Character Recognition (OCR) in the low-resource Pashto language. Natural Language Processing (NLP) in Pashto faces several challenges due to the cursive nature of its script and a scarcity of structured datasets. To address this, we developed a synthetic Pashto OCR dataset, PsOCR, consisting of one million images annotated with bounding boxes at word, line, and document levels, suitable for training and evaluating models based on different architectures, including Convolutional Neural Networks (CNNs) and Transformers. PsOCR covers variations across 1,000 unique font families, colors, image sizes, and layouts. A benchmark subset of 10K images was selected to evaluate the performance of several LMMs, including seven open-source models: DeepSeek's Janus, InternVL, MiniCPM, Florence, and Qwen (3B and 7B), and four closed-source models: GPT-4o, Gemini, Claude, and Grok. Experimental results demonstrate that Gemini achieves the best performance among all models, whereas among open-source models, Qwen-7B stands out. This work provides an insightful assessment of the capabilities and limitations of current LMMs for OCR tasks in Pashto and establishes a foundation for further research not only in Pashto OCR but also for other similar scripts such as Arabic, Persian, and Urdu. PsOCR is available at https://github.com/zirak-ai/PashtoOCR.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10055v2",
    "published_date": "2025-05-15 07:58:38 UTC",
    "updated_date": "2026-01-09 03:15:51 UTC"
  },
  {
    "arxiv_id": "2505.10050v1",
    "title": "Financial Fraud Detection Using Explainable AI and Stacking Ensemble Methods",
    "authors": [
      "Fahad Almalki",
      "Mehedi Masud"
    ],
    "abstract": "Traditional machine learning models often prioritize predictive accuracy, often at the expense of model transparency and interpretability. The lack of transparency makes it difficult for organizations to comply with regulatory requirements and gain stakeholders trust. In this research, we propose a fraud detection framework that combines a stacking ensemble of well-known gradient boosting models: XGBoost, LightGBM, and CatBoost. In addition, explainable artificial intelligence (XAI) techniques are used to enhance the transparency and interpretability of the model's decisions. We used SHAP (SHapley Additive Explanations) for feature selection to identify the most important features. Further efforts were made to explain the model's predictions using Local Interpretable Model-Agnostic Explanation (LIME), Partial Dependence Plots (PDP), and Permutation Feature Importance (PFI). The IEEE-CIS Fraud Detection dataset, which includes more than 590,000 real transaction records, was used to evaluate the proposed model. The model achieved a high performance with an accuracy of 99% and an AUC-ROC score of 0.99, outperforming several recent related approaches. These results indicate that combining high prediction accuracy with transparent interpretability is possible and could lead to a more ethical and trustworthy solution in financial fraud detection.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10050v1",
    "published_date": "2025-05-15 07:53:02 UTC",
    "updated_date": "2025-05-15 07:53:02 UTC"
  },
  {
    "arxiv_id": "2505.10043v3",
    "title": "Boosting Text-to-Chart Retrieval through Training with Synthesized Semantic Insights",
    "authors": [
      "Yifan Wu",
      "Lutao Yan",
      "Yizhang Zhu",
      "Yinan Mei",
      "Jiannan Wang",
      "Nan Tang",
      "Yuyu Luo"
    ],
    "abstract": "Charts are crucial for data analysis and decision-making.Text-to-chart retrieval systems have become increasingly important for Business Intelligence (BI), where users need to find relevant charts that match their analytical needs. These needs can be categorized into precise queries that are well-specified and fuzzy queries that are more exploratory -- both require understanding the semantics and context of the charts. However, existing text-to-chart retrieval solutions often fail to capture the semantic content and contextual information of charts, primarily due to the lack of comprehensive metadata (or semantic insights). To address this limitation, we propose a training data development pipeline that automatically synthesizes hierarchical semantic insights for charts, covering visual patterns (visual-oriented), statistical properties (statistics-oriented), and practical applications (task-oriented), which produces 207,498 semantic insights for 69,166 charts. Based on these, we train a CLIP-based model named ChartFinder to learn better representations of charts for text-to-chart retrieval. Our method leverages rich semantic insights during the training phase to develop a model that understands both visual and semantic aspects of charts.To evaluate text-to-chart retrieval performance, we curate the first benchmark, CRBench, for this task with 21,862 charts and 326 text queries from real-world BI applications, with ground-truth labels verified by the crowd workers.Experiments show that ChartFinder significantly outperforms existing methods in text-to-chart retrieval tasks across various settings. For precise queries, ChartFinder achieves up to 66.9% NDCG@10, which is 11.58% higher than state-of-the-art models. In fuzzy query tasks, our method also demonstrates consistent improvements, with an average increase of 5% across nearly all metrics.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "Need to be revised",
    "pdf_url": "https://arxiv.org/pdf/2505.10043v3",
    "published_date": "2025-05-15 07:41:14 UTC",
    "updated_date": "2025-10-07 04:25:15 UTC"
  },
  {
    "arxiv_id": "2505.10037v1",
    "title": "Optimal normalization in quantum-classical hybrid models for anti-cancer drug response prediction",
    "authors": [
      "Takafumi Ito",
      "Lysenko Artem",
      "Tatsuhiko Tsunoda"
    ],
    "abstract": "Quantum-classical Hybrid Machine Learning (QHML) models are recognized for their robust performance and high generalization ability even for relatively small datasets. These qualities offer unique advantages for anti-cancer drug response prediction, where the number of available samples is typically small. However, such hybrid models appear to be very sensitive to the data encoding used at the interface of a neural network and a quantum circuit, with suboptimal choices leading to stability issues. To address this problem, we propose a novel strategy that uses a normalization function based on a moderated gradient version of the $\\tanh$. This method transforms the outputs of the neural networks without concentrating them at the extreme value ranges. Our idea was evaluated on a dataset of gene expression and drug response measurements for various cancer cell lines, where we compared the prediction performance of a classical deep learning model and several QHML models. These results confirmed that QHML performed better than the classical models when data was optimally normalized. This study opens up new possibilities for biomedical data analysis using quantum computers.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.ET",
      "quant-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.10037v1",
    "published_date": "2025-05-15 07:33:41 UTC",
    "updated_date": "2025-05-15 07:33:41 UTC"
  },
  {
    "arxiv_id": "2505.10034v3",
    "title": "The First MPDD Challenge: Multimodal Personality-aware Depression Detection",
    "authors": [
      "Changzeng Fu",
      "Zelin Fu",
      "Qi Zhang",
      "Xinhe Kuang",
      "Jiacheng Dong",
      "Kaifeng Su",
      "Yikai Su",
      "Wenbo Shi",
      "Junfeng Yao",
      "Yuliang Zhao",
      "Shiqi Zhao",
      "Jiadong Wang",
      "Siyang Song",
      "Chaoran Liu",
      "Yuichiro Yoshikawa",
      "Björn Schuller",
      "Hiroshi Ishiguro"
    ],
    "abstract": "Depression is a widespread mental health issue affecting diverse age groups, with notable prevalence among college students and the elderly. However, existing datasets and detection methods primarily focus on young adults, neglecting the broader age spectrum and individual differences that influence depression manifestation. Current approaches often establish a direct mapping between multimodal data and depression indicators, failing to capture the complexity and diversity of depression across individuals. This challenge includes two tracks based on age-specific subsets: Track 1 uses the MPDD-Elderly dataset for detecting depression in older adults, and Track 2 uses the MPDD-Young dataset for detecting depression in younger participants. The Multimodal Personality-aware Depression Detection (MPDD) Challenge aims to address this gap by incorporating multimodal data alongside individual difference factors. We provide a baseline model that fuses audio and video modalities with individual difference information to detect depression manifestations in diverse populations. This challenge aims to promote the development of more personalized and accurate de pression detection methods, advancing mental health research and fostering inclusive detection systems. More details are available on the official challenge website: https://hacilab.github.io/MPDDChallenge.github.io.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper has been accepted as part of the MPDD Challenge in the ACMMM 2025 Grand Challenge",
    "pdf_url": "https://arxiv.org/pdf/2505.10034v3",
    "published_date": "2025-05-15 07:29:33 UTC",
    "updated_date": "2025-05-29 02:12:21 UTC"
  },
  {
    "arxiv_id": "2505.10027v2",
    "title": "ORL-LDM: Offline Reinforcement Learning Guided Latent Diffusion Model Super-Resolution Reconstruction",
    "authors": [
      "Shijie Lyu"
    ],
    "abstract": "With the rapid advancement of remote sensing technology, super-resolution image reconstruction is of great research and practical significance. Existing deep learning methods have made progress but still face limitations in handling complex scenes and preserving image details. This paper proposes a reinforcement learning-based latent diffusion model (LDM) fine-tuning method for remote sensing image super-resolution. The method constructs a reinforcement learning environment with states, actions, and rewards, optimizing decision objectives through proximal policy optimization (PPO) during the reverse denoising process of the LDM model. Experiments on the RESISC45 dataset show significant improvements over the baseline model in PSNR, SSIM, and LPIPS, with PSNR increasing by 3-4dB, SSIM improving by 0.08-0.11, and LPIPS reducing by 0.06-0.10, particularly in structured and complex natural scenes. The results demonstrate the method's effectiveness in enhancing super-resolution quality and adaptability across scenes.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This submission included authors who did not consent to the submission. The paper is being withdrawn until authorship issues are resolved",
    "pdf_url": "https://arxiv.org/pdf/2505.10027v2",
    "published_date": "2025-05-15 07:17:03 UTC",
    "updated_date": "2025-07-23 15:01:44 UTC"
  },
  {
    "arxiv_id": "2505.10016v2",
    "title": "Application of YOLOv8 in monocular downward multiple Car Target detection",
    "authors": [
      "Shijie Lyu"
    ],
    "abstract": "Autonomous driving technology is progressively transforming traditional car driving methods, marking a significant milestone in modern transportation. Object detection serves as a cornerstone of autonomous systems, playing a vital role in enhancing driving safety, enabling autonomous functionality, improving traffic efficiency, and facilitating effective emergency responses. However, current technologies such as radar for environmental perception, cameras for road perception, and vehicle sensor networks face notable challenges, including high costs, vulnerability to weather and lighting conditions, and limited resolution.To address these limitations, this paper presents an improved autonomous target detection network based on YOLOv8. By integrating structural reparameterization technology, a bidirectional pyramid structure network model, and a novel detection pipeline into the YOLOv8 framework, the proposed approach achieves highly efficient and precise detection of multi-scale, small, and remote objects. Experimental results demonstrate that the enhanced model can effectively detect both large and small objects with a detection accuracy of 65%, showcasing significant advancements over traditional methods.This improved model holds substantial potential for real-world applications and is well-suited for autonomous driving competitions, such as the Formula Student Autonomous China (FSAC), particularly excelling in scenarios involving single-target and small-object detection.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This submission included authors who did not consent to the submission. The paper is being withdrawn until authorship issues are resolved",
    "pdf_url": "https://arxiv.org/pdf/2505.10016v2",
    "published_date": "2025-05-15 06:58:45 UTC",
    "updated_date": "2025-07-23 15:01:56 UTC"
  },
  {
    "arxiv_id": "2505.10012v1",
    "title": "Quantum Computing and AI: Perspectives on Advanced Automation in Science and Engineering",
    "authors": [
      "Tadashi Kadowaki"
    ],
    "abstract": "Recent advances in artificial intelligence (AI) and quantum computing are accelerating automation in scientific and engineering processes, fundamentally reshaping research methodologies. This perspective highlights parallels between scientific automation and established Computer-Aided Engineering (CAE) practices, introducing Quantum CAE as a framework that leverages quantum algorithms for simulation, optimization, and machine learning within engineering design. Practical implementations of Quantum CAE are illustrated through case studies for combinatorial optimization problems. Further discussions include advancements toward higher automation levels, highlighting the critical role of specialized AI agents proficient in quantum algorithm design. The integration of quantum computing with AI raises significant questions about the collaborative dynamics among human scientists and engineers, AI systems, and quantum computational resources, underscoring a transformative future for automated discovery and innovation.",
    "categories": [
      "quant-ph",
      "cs.AI"
    ],
    "primary_category": "quant-ph",
    "comment": "8 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.10012v1",
    "published_date": "2025-05-15 06:53:30 UTC",
    "updated_date": "2025-05-15 06:53:30 UTC"
  },
  {
    "arxiv_id": "2505.09989v1",
    "title": "AI Greenferencing: Routing AI Inferencing to Green Modular Data Centers with Heron",
    "authors": [
      "Tella Rajashekhar Reddy",
      "Palak",
      "Rohan Gandhi",
      "Anjaly Parayil",
      "Chaojie Zhang",
      "Mike Shepperd",
      "Liangcheng Yu",
      "Jayashree Mohan",
      "Srinivasan Iyengar",
      "Shivkumar Kalyanaraman",
      "Debopam Bhattacherjee"
    ],
    "abstract": "AI power demand is growing unprecedentedly thanks to the high power density of AI compute and the emerging inferencing workload. On the supply side, abundant wind power is waiting for grid access in interconnection queues. In this light, this paper argues bringing AI workload to modular compute clusters co-located in wind farms. Our deployment right-sizing strategy makes it economically viable to deploy more than 6 million high-end GPUs today that could consume cheap, green power at its source. We built Heron, a cross-site software router, that could efficiently leverage the complementarity of power generation across wind farms by routing AI inferencing workload around power drops. Using 1-week ofcoding and conversation production traces from Azure and (real) variable wind power traces, we show how Heron improves aggregate goodput of AI compute by up to 80% compared to the state-of-the-art.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.09989v1",
    "published_date": "2025-05-15 06:03:47 UTC",
    "updated_date": "2025-05-15 06:03:47 UTC"
  },
  {
    "arxiv_id": "2505.10593v1",
    "title": "LLM-Explorer: Towards Efficient and Affordable LLM-based Exploration for Mobile Apps",
    "authors": [
      "Shanhui Zhao",
      "Hao Wen",
      "Wenjie Du",
      "Cheng Liang",
      "Yunxin Liu",
      "Xiaozhou Ye",
      "Ye Ouyang",
      "Yuanchun Li"
    ],
    "abstract": "Large language models (LLMs) have opened new opportunities for automated mobile app exploration, an important and challenging problem that used to suffer from the difficulty of generating meaningful UI interactions. However, existing LLM-based exploration approaches rely heavily on LLMs to generate actions in almost every step, leading to a huge cost of token fees and computational resources. We argue that such extensive usage of LLMs is neither necessary nor effective, since many actions during exploration do not require, or may even be biased by the abilities of LLMs. Further, based on the insight that a precise and compact knowledge plays the central role for effective exploration, we introduce LLM-Explorer, a new exploration agent designed for efficiency and affordability. LLM-Explorer uses LLMs primarily for maintaining the knowledge instead of generating actions, and knowledge is used to guide action generation in a LLM-less manner. Based on a comparison with 5 strong baselines on 20 typical apps, LLM-Explorer was able to achieve the fastest and highest coverage among all automated app explorers, with over 148x lower cost than the state-of-the-art LLM-based approach.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted by MobiCom 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.10593v1",
    "published_date": "2025-05-15 05:28:35 UTC",
    "updated_date": "2025-05-15 05:28:35 UTC"
  },
  {
    "arxiv_id": "2505.09974v2",
    "title": "Analysing Safety Risks in LLMs Fine-Tuned with Pseudo-Malicious Cyber Security Data",
    "authors": [
      "Adel ElZemity",
      "Budi Arief",
      "Shujun Li"
    ],
    "abstract": "Large language models (LLMs) have been used in many application domains, including cyber security. The application of LLMs in the cyber security domain presents significant opportunities, such as for enhancing threat analysis and malware detection, but it can also introduce critical risks and safety concerns, including potential personal data leakage and automated generation of new malware. Building on recent findings that fine-tuning LLMs with pseudo-malicious cyber security data significantly compromises their safety, this paper presents a comprehensive validation and extension of these safety risks using a different evaluation framework. We employ the garak red teaming framework with the OWASP Top 10 for LLM Applications to assess four open-source LLMs: Mistral 7B, Llama 3 8B, Gemma 2 9B, and DeepSeek R1 8B. Our evaluation confirms and extends previous findings, showing that fine-tuning reduces safety resilience across all tested LLMs (e.g., the failure rate of Mistral 7B against prompt injection increases from 9.1% to 68.7%). We further propose and evaluate a novel safety alignment approach that carefully rewords instruction-response pairs to include explicit safety precautions and ethical considerations. This work validates previous safety concerns through independent evaluation and introduces new methods for mitigating these risks, contributing towards the development of secure, trustworthy, and ethically aligned LLMs. This approach demonstrates that it is possible to maintain or even improve model safety while preserving technical utility, offering a practical path towards developing safer fine-tuning methodologies.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.09974v2",
    "published_date": "2025-05-15 05:22:53 UTC",
    "updated_date": "2025-09-17 13:26:12 UTC"
  },
  {
    "arxiv_id": "2505.09970v2",
    "title": "Pre-Act: Multi-Step Planning and Reasoning Improves Acting in LLM Agents",
    "authors": [
      "Mrinal Rawat",
      "Ambuje Gupta",
      "Rushil Goomer",
      "Alessandro Di Bari",
      "Neha Gupta",
      "Roberto Pieraccini"
    ],
    "abstract": "The ReAct (Reasoning + Action) capability in large language models (LLMs) has become the foundation of modern agentic systems. Recent LLMs, such as DeepSeek-R1 and OpenAI o1/o3, exemplify this by emphasizing reasoning through the generation of ample intermediate tokens, which help build a strong premise before producing the final output tokens. In this paper, we introduce Pre-Act, a novel approach that enhances the agent's performance by creating a multi-step execution plan along with the detailed reasoning for the given user input. This plan incrementally incorporates previous steps and tool outputs, refining itself after each step execution until the final response is obtained. Our approach is applicable to both conversational and non-conversational agents. To measure the performance of task-oriented agents comprehensively, we propose a two-level evaluation framework: (1) turn level and (2) end-to-end. Our turn-level evaluation, averaged across five models, shows that our approach, Pre-Act, outperforms ReAct by 70% in Action Recall on the Almita dataset. While this approach is effective for larger models, smaller models crucial for practical applications, where latency and cost are key constraints, often struggle with complex reasoning tasks required for agentic systems. To address this limitation, we fine-tune relatively small models such as Llama 3.1 (8B & 70B) using the proposed Pre-Act approach. Our experiments show that the fine-tuned 70B model outperforms GPT-4, achieving a 69.5% improvement in action accuracy (turn-level) and a 28% improvement in goal completion rate (end-to-end) on the Almita (out-of-domain) dataset.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.09970v2",
    "published_date": "2025-05-15 05:17:47 UTC",
    "updated_date": "2025-05-19 03:17:21 UTC"
  },
  {
    "arxiv_id": "2505.09969v1",
    "title": "A Comprehensive Machine Learning Framework for Heart Disease Prediction: Performance Evaluation and Future Perspectives",
    "authors": [
      "Ali Azimi Lamir",
      "Shiva Razzagzadeh",
      "Zeynab Rezaei"
    ],
    "abstract": "This study presents a machine learning-based framework for heart disease prediction using the heart-disease dataset, comprising 303 samples with 14 features. The methodology involves data preprocessing, model training, and evaluation using three classifiers: Logistic Regression, K-Nearest Neighbors (KNN), and Random Forest. Hyperparameter tuning with GridSearchCV and RandomizedSearchCV was employed to enhance model performance. The Random Forest classifier outperformed other models, achieving an accuracy of 91% and an F1-score of 0.89. Evaluation metrics, including precision, recall, and confusion matrix, revealed balanced performance across classes. The proposed model demonstrates strong potential for aiding clinical decision-making by effectively predicting heart disease. Limitations such as dataset size and generalizability underscore the need for future studies using larger and more diverse datasets. This work highlights the utility of machine learning in healthcare, offering insights for further advancements in predictive diagnostics.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.09969v1",
    "published_date": "2025-05-15 05:13:38 UTC",
    "updated_date": "2025-05-15 05:13:38 UTC"
  },
  {
    "arxiv_id": "2505.18179v2",
    "title": "GAIA: A Foundation Model for Operational Atmospheric Dynamics",
    "authors": [
      "Ata Akbari Asanjan",
      "Olivia Alexander",
      "Tom Berg",
      "Stephen Peng",
      "Jad Makki",
      "Clara Zhang",
      "Matt Yang",
      "Disha Shidham",
      "Srija Chakraborty",
      "William Bender",
      "Cara Crawford",
      "Arun Ravindran",
      "Olivier Raiman",
      "David Potere",
      "David Bell"
    ],
    "abstract": "We introduce GAIA (Geospatial Artificial Intelligence for Atmospheres), a hybrid self-supervised geospatial foundation model that fuses Masked Autoencoders (MAE) with self-distillation with no labels (DINO) to generate semantically rich representations from global geostationary satellite imagery. Pre-trained on 15 years of globally-merged infrared observations (2001-2015), GAIA learns disentangled representations that capture atmospheric dynamics rather than trivial diurnal patterns, as evidenced by distributed principal component structure and temporal coherence analysis. We demonstrate robust reconstruction capabilities across varying data availability (30-95% masking), achieving superior gap-filling performance on real missing data patterns. When transferred to downstream tasks, GAIA consistently outperforms an MAE-only baseline: improving atmospheric river segmentation (F1: 0.58 vs 0.52), enhancing tropical cyclone detection (storm-level recall: 81% vs 75%, early detection: 29% vs 17%), and maintaining competitive precipitation estimation performance. Analysis reveals that GAIA's hybrid objectives encourage learning of spatially coherent, object-centric features distributed across multiple principal components rather than concentrated representations focused on reconstruction. This work demonstrates that combining complementary self-supervised objectives yields more transferable representations for diverse atmospheric modeling tasks. Model weights and code are available at: https://huggingface.co/bcg-usra-nasa-gaia/GAIA-v1.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "23 pages, 11 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.18179v2",
    "published_date": "2025-05-15 05:07:09 UTC",
    "updated_date": "2025-10-30 19:40:31 UTC"
  },
  {
    "arxiv_id": "2505.09955v1",
    "title": "TransPL: VQ-Code Transition Matrices for Pseudo-Labeling of Time Series Unsupervised Domain Adaptation",
    "authors": [
      "Jaeho Kim",
      "Seulki Lee"
    ],
    "abstract": "Unsupervised domain adaptation (UDA) for time series data remains a critical challenge in deep learning, with traditional pseudo-labeling strategies failing to capture temporal patterns and channel-wise shifts between domains, producing sub-optimal pseudo-labels. As such, we introduce TransPL, a novel approach that addresses these limitations by modeling the joint distribution $P(\\mathbf{X}, y)$ of the source domain through code transition matrices, where the codes are derived from vector quantization (VQ) of time series patches. Our method constructs class- and channel-wise code transition matrices from the source domain and employs Bayes' rule for target domain adaptation, generating pseudo-labels based on channel-wise weighted class-conditional likelihoods. TransPL offers three key advantages: explicit modeling of temporal transitions and channel-wise shifts between different domains, versatility towards different UDA scenarios (e.g., weakly-supervised UDA), and explainable pseudo-label generation. We validate TransPL's effectiveness through extensive analysis on four time series UDA benchmarks and confirm that it consistently outperforms state-of-the-art pseudo-labeling methods by a strong margin (6.1% accuracy improvement, 4.9% F1 improvement), while providing interpretable insights into the domain adaptation process through its learned code transition matrices.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML 2025 Accept",
    "pdf_url": "https://arxiv.org/pdf/2505.09955v1",
    "published_date": "2025-05-15 04:27:48 UTC",
    "updated_date": "2025-05-15 04:27:48 UTC"
  },
  {
    "arxiv_id": "2505.09952v1",
    "title": "Task-Core Memory Management and Consolidation for Long-term Continual Learning",
    "authors": [
      "Tianyu Huai",
      "Jie Zhou",
      "Yuxuan Cai",
      "Qin Chen",
      "Wen Wu",
      "Xingjiao Wu",
      "Xipeng Qiu",
      "Liang He"
    ],
    "abstract": "In this paper, we focus on a long-term continual learning (CL) task, where a model learns sequentially from a stream of vast tasks over time, acquiring new knowledge while retaining previously learned information in a manner akin to human learning. Unlike traditional CL settings, long-term CL involves handling a significantly larger number of tasks, which exacerbates the issue of catastrophic forgetting. Our work seeks to address two critical questions: 1) How do existing CL methods perform in the context of long-term CL? and 2) How can we mitigate the catastrophic forgetting that arises from prolonged sequential updates? To tackle these challenges, we propose a novel framework inspired by human memory mechanisms for long-term continual learning (Long-CL). Specifically, we introduce a task-core memory management strategy to efficiently index crucial memories and adaptively update them as learning progresses. Additionally, we develop a long-term memory consolidation mechanism that selectively retains hard and discriminative samples, ensuring robust knowledge retention. To facilitate research in this area, we construct and release two multi-modal and textual benchmarks, MMLongCL-Bench and TextLongCL-Bench, providing a valuable resource for evaluating long-term CL approaches. Experimental results show that Long-CL outperforms the previous state-of-the-art by 7.4\\% and 6.5\\% AP on the two benchmarks, respectively, demonstrating the effectiveness of our approach.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Submitted to Neurips2025",
    "pdf_url": "https://arxiv.org/pdf/2505.09952v1",
    "published_date": "2025-05-15 04:22:35 UTC",
    "updated_date": "2025-05-15 04:22:35 UTC"
  },
  {
    "arxiv_id": "2505.11547v1",
    "title": "On Technique Identification and Threat-Actor Attribution using LLMs and Embedding Models",
    "authors": [
      "Kyla Guru",
      "Robert J. Moss",
      "Mykel J. Kochenderfer"
    ],
    "abstract": "Attribution of cyber-attacks remains a complex but critical challenge for cyber defenders. Currently, manual extraction of behavioral indicators from dense forensic documentation causes significant attribution delays, especially following major incidents at the international scale. This research evaluates large language models (LLMs) for cyber-attack attribution based on behavioral indicators extracted from forensic documentation. We test OpenAI's GPT-4 and text-embedding-3-large for identifying threat actors' tactics, techniques, and procedures (TTPs) by comparing LLM-generated TTPs against human-generated data from MITRE ATT&CK Groups. Our framework then identifies TTPs from text using vector embedding search and builds profiles to attribute new attacks for a machine learning model to learn. Key contributions include: (1) assessing off-the-shelf LLMs for TTP extraction and attribution, and (2) developing an end-to-end pipeline from raw CTI documents to threat-actor prediction. This research finds that standard LLMs generate TTP datasets with noise, resulting in a low similarity to human-generated datasets. However, the TTPs generated are similar in frequency to those within the existing MITRE datasets. Additionally, although these TTPs are different than human-generated datasets, our work demonstrates that they still prove useful for training a model that performs above baseline on attribution. Project code and files are contained here: https://github.com/kylag/ttp_attribution.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11547v1",
    "published_date": "2025-05-15 04:14:29 UTC",
    "updated_date": "2025-05-15 04:14:29 UTC"
  },
  {
    "arxiv_id": "2505.09945v1",
    "title": "Personalizing Large Language Models using Retrieval Augmented Generation and Knowledge Graph",
    "authors": [
      "Deeksha Prahlad",
      "Chanhee Lee",
      "Dongha Kim",
      "Hokeun Kim"
    ],
    "abstract": "The advent of large language models (LLMs) has allowed numerous applications, including the generation of queried responses, to be leveraged in chatbots and other conversational assistants. Being trained on a plethora of data, LLMs often undergo high levels of over-fitting, resulting in the generation of extra and incorrect data, thus causing hallucinations in output generation. One of the root causes of such problems is the lack of timely, factual, and personalized information fed to the LLM. In this paper, we propose an approach to address these problems by introducing retrieval augmented generation (RAG) using knowledge graphs (KGs) to assist the LLM in personalized response generation tailored to the users. KGs have the advantage of storing continuously updated factual information in a structured way. While our KGs can be used for a variety of frequently updated personal data, such as calendar, contact, and location data, we focus on calendar data in this paper. Our experimental results show that our approach works significantly better in understanding personal information and generating accurate responses compared to the baseline LLMs using personal data as text inputs, with a moderate reduction in response time.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "To appear in the Companion Proceedings of the ACM Web Conference 2025 (WWW Companion '25)",
    "pdf_url": "https://arxiv.org/pdf/2505.09945v1",
    "published_date": "2025-05-15 04:01:58 UTC",
    "updated_date": "2025-05-15 04:01:58 UTC"
  },
  {
    "arxiv_id": "2505.18178v1",
    "title": "Less is More: Multimodal Region Representation via Pairwise Inter-view Learning",
    "authors": [
      "Min Namgung",
      "Yijun Lin",
      "JangHyeon Lee",
      "Yao-Yi Chiang"
    ],
    "abstract": "With the increasing availability of geospatial datasets, researchers have explored region representation learning (RRL) to analyze complex region characteristics. Recent RRL methods use contrastive learning (CL) to capture shared information between two modalities but often overlook task-relevant unique information specific to each modality. Such modality-specific details can explain region characteristics that shared information alone cannot capture. Bringing information factorization to RRL can address this by factorizing multimodal data into shared and unique information. However, existing factorization approaches focus on two modalities, whereas RRL can benefit from various geospatial data. Extending factorization beyond two modalities is non-trivial because modeling high-order relationships introduces a combinatorial number of learning objectives, increasing model complexity. We introduce Cross modal Knowledge Injected Embedding, an information factorization approach for RRL that captures both shared and unique representations. CooKIE uses a pairwise inter-view learning approach that captures high-order information without modeling high-order dependency, avoiding exhaustive combinations. We evaluate CooKIE on three regression tasks and a land use classification task in New York City and Delhi, India. Results show that CooKIE outperforms existing RRL methods and a factorized RRL model, capturing multimodal information with fewer training parameters and floating-point operations per second (FLOPs). We release the code: https://github.com/MinNamgung/CooKIE.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18178v1",
    "published_date": "2025-05-15 03:44:10 UTC",
    "updated_date": "2025-05-15 03:44:10 UTC"
  },
  {
    "arxiv_id": "2505.09935v1",
    "title": "VRU-CIPI: Crossing Intention Prediction at Intersections for Improving Vulnerable Road Users Safety",
    "authors": [
      "Ahmed S. Abdelrahman",
      "Mohamed Abdel-Aty",
      "Quoc Dai Tran"
    ],
    "abstract": "Understanding and predicting human behavior in-thewild, particularly at urban intersections, remains crucial for enhancing interaction safety between road users. Among the most critical behaviors are crossing intentions of Vulnerable Road Users (VRUs), where misinterpretation may result in dangerous conflicts with oncoming vehicles. In this work, we propose the VRU-CIPI framework with a sequential attention-based model designed to predict VRU crossing intentions at intersections. VRU-CIPI employs Gated Recurrent Unit (GRU) to capture temporal dynamics in VRU movements, combined with a multi-head Transformer self-attention mechanism to encode contextual and spatial dependencies critical for predicting crossing direction. Evaluated on UCF-VRU dataset, our proposed achieves state-of-the-art performance with an accuracy of 96.45% and achieving real-time inference speed reaching 33 frames per second. Furthermore, by integrating with Infrastructure-to-Vehicles (I2V) communication, our approach can proactively enhance intersection safety through timely activation of crossing signals and providing early warnings to connected vehicles, ensuring smoother and safer interactions for all road users.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.09935v1",
    "published_date": "2025-05-15 03:40:29 UTC",
    "updated_date": "2025-05-15 03:40:29 UTC"
  },
  {
    "arxiv_id": "2505.09932v1",
    "title": "Demystifying AI Agents: The Final Generation of Intelligence",
    "authors": [
      "Kevin J McNamara",
      "Rhea Pritham Marpu"
    ],
    "abstract": "The trajectory of artificial intelligence (AI) has been one of relentless acceleration, evolving from rudimentary rule-based systems to sophisticated, autonomous agents capable of complex reasoning and interaction. This whitepaper chronicles this remarkable journey, charting the key technological milestones--advancements in prompting, training methodologies, hardware capabilities, and architectural innovations--that have converged to create the AI agents of today. We argue that these agents, exemplified by systems like OpenAI's ChatGPT with plugins and xAI's Grok, represent a culminating phase in AI development, potentially constituting the \"final generation\" of intelligence as we currently conceive it. We explore the capabilities and underlying technologies of these agents, grounded in practical examples, while also examining the profound societal implications and the unprecedented pace of progress that suggests intelligence is now doubling approximately every six months. The paper concludes by underscoring the critical need for wisdom and foresight in navigating the opportunities and challenges presented by this powerful new era of intelligence.",
    "categories": [
      "cs.AI",
      "cs.ET",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.09932v1",
    "published_date": "2025-05-15 03:35:12 UTC",
    "updated_date": "2025-05-15 03:35:12 UTC"
  },
  {
    "arxiv_id": "2505.09926v2",
    "title": "AdaptCLIP: Adapting CLIP for Universal Visual Anomaly Detection",
    "authors": [
      "Bin-Bin Gao",
      "Yue Zhou",
      "Jiangtao Yan",
      "Yuezhi Cai",
      "Weixi Zhang",
      "Meng Wang",
      "Jun Liu",
      "Yong Liu",
      "Lei Wang",
      "Chengjie Wang"
    ],
    "abstract": "Universal visual anomaly detection aims to identify anomalies from novel or unseen vision domains without additional fine-tuning, which is critical in open scenarios. Recent studies have demonstrated that pre-trained vision-language models like CLIP exhibit strong generalization with just zero or a few normal images. However, existing methods struggle with designing prompt templates, complex token interactions, or requiring additional fine-tuning, resulting in limited flexibility. In this work, we present a simple yet effective method called AdaptCLIP based on two key insights. First, adaptive visual and textual representations should be learned alternately rather than jointly. Second, comparative learning between query and normal image prompt should incorporate both contextual and aligned residual features, rather than relying solely on residual features. AdaptCLIP treats CLIP models as a foundational service, adding only three simple adapters, visual adapter, textual adapter, and prompt-query adapter, at its input or output ends. AdaptCLIP supports zero-/few-shot generalization across domains and possesses a training-free manner on target domains once trained on a base dataset. AdaptCLIP achieves state-of-the-art performance on 12 anomaly detection benchmarks from industrial and medical domains, significantly outperforming existing competitive methods. We will make the code and model of AdaptCLIP available at https://github.com/gaobb/AdaptCLIP.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "27 pages, 15 figures, 22 tables",
    "pdf_url": "https://arxiv.org/pdf/2505.09926v2",
    "published_date": "2025-05-15 03:24:28 UTC",
    "updated_date": "2025-05-19 03:02:23 UTC"
  },
  {
    "arxiv_id": "2505.09925v1",
    "title": "Reinforced Interactive Continual Learning via Real-time Noisy Human Feedback",
    "authors": [
      "Yutao Yang",
      "Jie Zhou",
      "Junsong Li",
      "Qianjun Pan",
      "Bihao Zhan",
      "Qin Chen",
      "Xipeng Qiu",
      "Liang He"
    ],
    "abstract": "This paper introduces an interactive continual learning paradigm where AI models dynamically learn new skills from real-time human feedback while retaining prior knowledge. This paradigm distinctively addresses two major limitations of traditional continual learning: (1) dynamic model updates using streaming, real-time human-annotated data, rather than static datasets with fixed labels, and (2) the assumption of clean labels, by explicitly handling the noisy feedback common in real-world interactions. To tackle these problems, we propose RiCL, a Reinforced interactive Continual Learning framework leveraging Large Language Models (LLMs) to learn new skills effectively from dynamic feedback. RiCL incorporates three key components: a temporal consistency-aware purifier to automatically discern clean from noisy samples in data streams; an interaction-aware direct preference optimization strategy to align model behavior with human intent by reconciling AI-generated and human-provided feedback; and a noise-resistant contrastive learning module that captures robust representations by exploiting inherent data relationships, thus avoiding reliance on potentially unreliable labels. Extensive experiments on two benchmark datasets (FewRel and TACRED), contaminated with realistic noise patterns, demonstrate that our RiCL approach substantially outperforms existing combinations of state-of-the-art online continual learning and noisy-label learning methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.09925v1",
    "published_date": "2025-05-15 03:22:03 UTC",
    "updated_date": "2025-05-15 03:22:03 UTC"
  },
  {
    "arxiv_id": "2505.09923v1",
    "title": "\"There Is No Such Thing as a Dumb Question,\" But There Are Good Ones",
    "authors": [
      "Minjung Shin",
      "Donghyun Kim",
      "Jeh-Kwang Ryu"
    ],
    "abstract": "Questioning has become increasingly crucial for both humans and artificial intelligence, yet there remains limited research comprehensively assessing question quality. In response, this study defines good questions and presents a systematic evaluation framework. We propose two key evaluation dimensions: appropriateness (sociolinguistic competence in context) and effectiveness (strategic competence in goal achievement). Based on these foundational dimensions, a rubric-based scoring system was developed. By incorporating dynamic contextual variables, our evaluation framework achieves structure and flexibility through semi-adaptive criteria. The methodology was validated using the CAUS and SQUARE datasets, demonstrating the ability of the framework to access both well-formed and problematic questions while adapting to varied contexts. As we establish a flexible and comprehensive framework for question evaluation, this study takes a significant step toward integrating questioning behavior with structured analytical methods grounded in the intrinsic nature of questioning.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, 4 figures and 4 tables. This work has been accepted for presentation as a poster with full paper publication at CogSci 2025. This is the final submission",
    "pdf_url": "https://arxiv.org/pdf/2505.09923v1",
    "published_date": "2025-05-15 03:12:28 UTC",
    "updated_date": "2025-05-15 03:12:28 UTC"
  },
  {
    "arxiv_id": "2505.09920v1",
    "title": "Offline Reinforcement Learning for Microgrid Voltage Regulation",
    "authors": [
      "Shan Yang",
      "Yongli Zhu"
    ],
    "abstract": "This paper presents a study on using different offline reinforcement learning algorithms for microgrid voltage regulation with solar power penetration. When environment interaction is unviable due to technical or safety reasons, the proposed approach can still obtain an applicable model through offline-style training on a previously collected dataset, lowering the negative impact of lacking online environment interactions. Experiment results on the IEEE 33-bus system demonstrate the feasibility and effectiveness of the proposed approach on different offline datasets, including the one with merely low-quality experience.",
    "categories": [
      "cs.AI",
      "eess.SY"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper has been accepted and presented at ICLR 2025 in Singapore, Apr. 28, 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.09920v1",
    "published_date": "2025-05-15 03:10:18 UTC",
    "updated_date": "2025-05-15 03:10:18 UTC"
  },
  {
    "arxiv_id": "2505.09907v1",
    "title": "Avocado Price Prediction Using a Hybrid Deep Learning Model: TCN-MLP-Attention Architecture",
    "authors": [
      "Linwei Zhang",
      "LuFeng",
      "Ruijia Liang"
    ],
    "abstract": "With the growing demand for healthy foods, agricultural product price forecasting has become increasingly important. Hass avocados, as a high-value crop, exhibit complex price fluctuations influenced by factors such as seasonality, region, and weather. Traditional prediction models often struggle with highly nonlinear and dynamic data. To address this, we propose a hybrid deep learning model, TCN-MLP-Attention Architecture, combining Temporal Convolutional Networks (TCN) for sequential feature extraction, Multi-Layer Perceptrons (MLP) for nonlinear interactions, and an Attention mechanism for dynamic feature weighting. The dataset used covers over 50,000 records of Hass avocado sales across the U.S. from 2015 to 2018, including variables such as sales volume, average price, time, region, weather, and variety type, collected from point-of-sale systems and the Hass Avocado Board. After systematic preprocessing, including missing value imputation and feature normalization, the proposed model was trained and evaluated. Experimental results demonstrate that the TCN-MLP-Attention model achieves excellent predictive performance, with an RMSE of 1.23 and an MSE of 1.51, outperforming traditional methods. This research provides a scalable and effective approach for time series forecasting in agricultural markets and offers valuable insights for intelligent supply chain management and price strategy optimization.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.09907v1",
    "published_date": "2025-05-15 02:26:22 UTC",
    "updated_date": "2025-05-15 02:26:22 UTC"
  },
  {
    "arxiv_id": "2505.09901v2",
    "title": "Comparing Exploration-Exploitation Strategies of LLMs and Humans: Insights from Standard Multi-armed Bandit Experiments",
    "authors": [
      "Ziyuan Zhang",
      "Darcy Wang",
      "Ningyuan Chen",
      "Rodrigo Mansur",
      "Vahid Sarhangian"
    ],
    "abstract": "Large language models (LLMs) are increasingly used to simulate or automate human behavior in complex sequential decision-making settings. A natural question is then whether LLMs exhibit similar decision-making behavior to humans, and can achieve comparable (or superior) performance. In this work, we focus on the exploration-exploitation (E&E) tradeoff, a fundamental aspect of dynamic decision-making under uncertainty. We employ canonical multi-armed bandit (MAB) experiments introduced in the cognitive science and psychiatry literature to conduct a comparative study of the E&E strategies of LLMs, humans, and MAB algorithms. We use interpretable choice models to capture the E&E strategies of the agents and investigate how enabling thinking traces, through both prompting strategies and thinking models, shapes LLM decision-making. We find that enabling thinking in LLMs shifts their behavior toward more human-like behavior, characterized by a mix of random and directed exploration. In a simple stationary setting, thinking-enabled LLMs exhibit similar levels of random and directed exploration compared to humans. However, in more complex, non-stationary environments, LLMs struggle to match human adaptability, particularly in effective directed exploration, despite achieving similar regret in certain scenarios. Our findings highlight both the promise and limits of LLMs as simulators of human behavior and tools for automated decision-making and point to potential areas for improvement.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.09901v2",
    "published_date": "2025-05-15 02:09:18 UTC",
    "updated_date": "2025-10-03 16:12:51 UTC"
  },
  {
    "arxiv_id": "2505.18177v1",
    "title": "FedGRec: Dynamic Spatio-Temporal Federated Graph Learning for Secure and Efficient Cross-Border Recommendations",
    "authors": [
      "Zhizhong Tan",
      "Jiexin Zheng",
      "Xingxing Yang",
      "Chi Zhang",
      "Weiping Deng",
      "Wenyong Wang"
    ],
    "abstract": "Due to the highly sensitive nature of certain data in cross-border sharing, collaborative cross-border recommendations and data sharing are often subject to stringent privacy protection regulations, resulting in insufficient data for model training. Consequently, achieving efficient cross-border business recommendations while ensuring privacy security poses a significant challenge. Although federated learning has demonstrated broad potential in collaborative training without exposing raw data, most existing federated learning-based GNN training methods still rely on federated averaging strategies, which perform suboptimally on highly heterogeneous graph data. To address this issue, we propose FedGRec, a privacy-preserving federated graph learning method for cross-border recommendations. FedGRec captures user preferences from distributed multi-domain data to enhance recommendation performance across all domains without privacy leakage. Specifically, FedGRec leverages collaborative signals from local subgraphs associated with users or items to enrich their representation learning. Additionally, it employs dynamic spatiotemporal modeling to integrate global and local user preferences in real time based on business recommendation states, thereby deriving the final representations of target users and candidate items. By automatically filtering relevant behaviors, FedGRec effectively mitigates noise interference from unreliable neighbors. Furthermore, through a personalized federated aggregation strategy, FedGRec adapts global preferences to heterogeneous domain data, enabling collaborative learning of user preferences across multiple domains. Extensive experiments on three datasets demonstrate that FedGRec consistently outperforms competitive single-domain and cross-domain baselines while effectively preserving data privacy in cross-border recommendations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18177v1",
    "published_date": "2025-05-15 02:08:22 UTC",
    "updated_date": "2025-05-15 02:08:22 UTC"
  },
  {
    "arxiv_id": "2505.15828v1",
    "title": "Generative AI-Aided QoE Maximization for RIS-Assisted Digital Twin Interaction",
    "authors": [
      "Jiayuan Chen",
      "Yuxiang Li",
      "Changyan Yi",
      "Shimin Gong"
    ],
    "abstract": "In this paper, we investigate a quality of experience (QoE)-aware resource allocation problem for reconfigurable intelligent surface (RIS)-assisted digital twin (DT) interaction with uncertain evolution. In the considered system, mobile users are expected to interact with a DT model maintained on a DT server that is deployed on a base station, via effective uplink and downlink channels assisted by an RIS. Our goal is to maximize the sum of all mobile users' joint subjective and objective QoE in DT interactions across various DT scenes, by jointly optimizing phase shift matrix, receive/transmit beamforming matrix, rendering resolution configuration and computing resource allocation. While solving this problem is challenging mainly due to the uncertain evolution of the DT model, which leads to multiple scene-specific problems, and require us to constantly re-solve each of them whenever DT model evolves.\n  To this end, leveraging the dynamic optimization capabilities of decision transformers and the generalization strengths of generative artificial intelligence (GAI), we propose a novel GAI-aided approach, called the prompt-guided decision transformer integrated with zero-forcing optimization (PG-ZFO). Simulations are conducted to evaluate the proposed PG-ZFO, demonstrating its effectiveness and superiority over counterparts.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15828v1",
    "published_date": "2025-05-15 02:00:29 UTC",
    "updated_date": "2025-05-15 02:00:29 UTC"
  },
  {
    "arxiv_id": "2505.10590v2",
    "title": "Anchoring AI Capabilities in Market Valuations: The Capability Realization Rate Model and Valuation Misalignment Risk",
    "authors": [
      "Xinmin Fang",
      "Lingfeng Tao",
      "Zhengxiong Li"
    ],
    "abstract": "Recent breakthroughs in artificial intelligence (AI) have triggered surges in market valuations for AI-related companies, often outpacing the realization of underlying capabilities. We examine the anchoring effect of AI capabilities on equity valuations and propose a Capability Realization Rate (CRR) model to quantify the gap between AI potential and realized performance. Using data from the 2023--2025 generative AI boom, we analyze sector-level sensitivity and conduct case studies (OpenAI, Adobe, NVIDIA, Meta, Microsoft, Goldman Sachs) to illustrate patterns of valuation premium and misalignment. Our findings indicate that AI-native firms commanded outsized valuation premiums anchored to future potential, while traditional companies integrating AI experienced re-ratings subject to proof of tangible returns. We argue that CRR can help identify valuation misalignment risk-where market prices diverge from realized AI-driven value. We conclude with policy recommendations to improve transparency, mitigate speculative bubbles, and align AI innovation with sustainable market value.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "11 pages, 3 figures, NeurIPS",
    "pdf_url": "https://arxiv.org/pdf/2505.10590v2",
    "published_date": "2025-05-15 01:06:06 UTC",
    "updated_date": "2025-07-10 09:45:59 UTC"
  },
  {
    "arxiv_id": "2505.11546v1",
    "title": "Control Invariant Sets for Neural Network Dynamical Systems and Recursive Feasibility in Model Predictive Control",
    "authors": [
      "Xiao Li",
      "Tianhao Wei",
      "Changliu Liu",
      "Anouck Girard",
      "Ilya Kolmanovsky"
    ],
    "abstract": "Neural networks are powerful tools for data-driven modeling of complex dynamical systems, enhancing predictive capability for control applications. However, their inherent nonlinearity and black-box nature challenge control designs that prioritize rigorous safety and recursive feasibility guarantees. This paper presents algorithmic methods for synthesizing control invariant sets specifically tailored to neural network based dynamical models. These algorithms employ set recursion, ensuring termination after a finite number of iterations and generating subsets in which closed-loop dynamics are forward invariant, thus guaranteeing perpetual operational safety. Additionally, we propose model predictive control designs that integrate these control invariant sets into mixed-integer optimization, with guaranteed adherence to safety constraints and recursive feasibility at the computational level. We also present a comprehensive theoretical analysis examining the properties and guarantees of the proposed methods. Numerical simulations in an autonomous driving scenario demonstrate the methods' effectiveness in synthesizing control-invariant sets offline and implementing model predictive control online, ensuring safety and recursive feasibility.",
    "categories": [
      "eess.SY",
      "cs.AI"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11546v1",
    "published_date": "2025-05-15 01:01:14 UTC",
    "updated_date": "2025-05-15 01:01:14 UTC"
  },
  {
    "arxiv_id": "2505.09868v3",
    "title": "Which Demographic Features Are Relevant for Individual Fairness Evaluation of U.S. Recidivism Risk Assessment Tools?",
    "authors": [
      "Tin Trung Nguyen",
      "Jiannan Xu",
      "Phuong-Anh Nguyen-Le",
      "Jonathan Lazar",
      "Donald Braman",
      "Hal Daumé",
      "Zubin Jelveh"
    ],
    "abstract": "Despite its constitutional relevance, the technical ``individual fairness'' criterion has not been operationalized in U.S. state or federal statutes/regulations. We conduct a human subjects experiment to address this gap, evaluating which demographic features are relevant for individual fairness evaluation of recidivism risk assessment (RRA) tools. Our analyses conclude that the individual similarity function should consider age and sex, but it should ignore race.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "ICAIL 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.09868v3",
    "published_date": "2025-05-15 00:07:07 UTC",
    "updated_date": "2025-10-29 16:55:47 UTC"
  }
]
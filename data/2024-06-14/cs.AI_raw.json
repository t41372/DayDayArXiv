[
  {
    "arxiv_id": "2406.10430v1",
    "title": "Challenging the Machine: Contestability in Government AI Systems",
    "authors": [
      "Susan Landau",
      "James X. Dempsey",
      "Ece Kamar",
      "Steven M. Bellovin",
      "Robert Pool"
    ],
    "abstract": "In an October 2023 executive order (EO), President Biden issued a detailed\nbut largely aspirational road map for the safe and responsible development and\nuse of artificial intelligence (AI). The challenge for the January 24-25, 2024\nworkshop was to transform those aspirations regarding one specific but crucial\nissue -- the ability of individuals to challenge government decisions made\nabout themselves -- into actionable guidance enabling agencies to develop,\nprocure, and use genuinely contestable advanced automated decision-making\nsystems. While the Administration has taken important steps since the October\n2023 EO, the insights garnered from our workshop remain highly relevant, as the\nrequirements for contestability of advanced decision-making systems are not yet\nfully defined or implemented.\n  The workshop brought together technologists, members of government agencies\nand civil society organizations, litigators, and researchers in an intensive\ntwo-day meeting that examined the challenges that users, developers, and\nagencies faced in enabling contestability in light of advanced automated\ndecision-making systems. To ensure a free and open flow of discussion, the\nmeeting was held under a modified version of the Chatham House rule.\nParticipants were free to use any information or details that they learned, but\nthey may not attribute any remarks made at the meeting by the identity or the\naffiliation of the speaker. Thus, the workshop summary that follows anonymizes\nspeakers and their affiliation. Where an identification of an agency, company,\nor organization is made, it is done from a public, identified resource and does\nnot necessarily reflect statements made by participants at the workshop.\n  This document is a report of that workshop, along with recommendations and\nexplanatory material.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10430v1",
    "published_date": "2024-06-14 22:22:17 UTC",
    "updated_date": "2024-06-14 22:22:17 UTC"
  },
  {
    "arxiv_id": "2406.10429v1",
    "title": "Consistency-diversity-realism Pareto fronts of conditional image generative models",
    "authors": [
      "Pietro Astolfi",
      "Marlene Careil",
      "Melissa Hall",
      "Oscar Ma√±as",
      "Matthew Muckley",
      "Jakob Verbeek",
      "Adriana Romero Soriano",
      "Michal Drozdzal"
    ],
    "abstract": "Building world models that accurately and comprehensively represent the real\nworld is the utmost aspiration for conditional image generative models as it\nwould enable their use as world simulators. For these models to be successful\nworld models, they should not only excel at image quality and prompt-image\nconsistency but also ensure high representation diversity. However, current\nresearch in generative models mostly focuses on creative applications that are\npredominantly concerned with human preferences of image quality and aesthetics.\nWe note that generative models have inference time mechanisms - or knobs - that\nallow the control of generation consistency, quality, and diversity. In this\npaper, we use state-of-the-art text-to-image and image-and-text-to-image models\nand their knobs to draw consistency-diversity-realism Pareto fronts that\nprovide a holistic view on consistency-diversity-realism multi-objective. Our\nexperiments suggest that realism and consistency can both be improved\nsimultaneously; however there exists a clear tradeoff between\nrealism/consistency and diversity. By looking at Pareto optimal points, we note\nthat earlier models are better at representation diversity and worse in\nconsistency/realism, and more recent models excel in consistency/realism while\ndecreasing significantly the representation diversity. By computing Pareto\nfronts on a geodiverse dataset, we find that the first version of latent\ndiffusion models tends to perform better than more recent models in all axes of\nevaluation, and there exist pronounced consistency-diversity-realism\ndisparities between geographical regions. Overall, our analysis clearly shows\nthat there is no best model and the choice of model should be determined by the\ndownstream application. With this analysis, we invite the research community to\nconsider Pareto fronts as an analytical tool to measure progress towards world\nmodels.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10429v1",
    "published_date": "2024-06-14 22:14:11 UTC",
    "updated_date": "2024-06-14 22:14:11 UTC"
  },
  {
    "arxiv_id": "2406.10424v1",
    "title": "What is the Visual Cognition Gap between Humans and Multimodal LLMs?",
    "authors": [
      "Xu Cao",
      "Bolin Lai",
      "Wenqian Ye",
      "Yunsheng Ma",
      "Joerg Heintz",
      "Jintai Chen",
      "Jianguo Cao",
      "James M. Rehg"
    ],
    "abstract": "Recently, Multimodal Large Language Models (MLLMs) have shown great promise\nin language-guided perceptual tasks such as recognition, segmentation, and\nobject detection. However, their effectiveness in addressing visual cognition\nproblems that require high-level reasoning is not well-established. One such\nchallenge is abstract visual reasoning (AVR) -- the cognitive ability to\ndiscern relationships among patterns in a set of images and extrapolate to\npredict subsequent patterns. This skill is crucial during the early\nneurodevelopmental stages of children. Inspired by the AVR tasks in Raven's\nProgressive Matrices (RPM) and Wechsler Intelligence Scale for Children (WISC),\nwe propose a new dataset MaRs-VQA and a new benchmark VCog-Bench containing\nthree datasets to evaluate the zero-shot AVR capability of MLLMs and compare\ntheir performance with existing human intelligent investigation. Our\ncomparative experiments with different open-source and closed-source MLLMs on\nthe VCog-Bench revealed a gap between MLLMs and human intelligence,\nhighlighting the visual cognitive limitations of current MLLMs. We believe that\nthe public release of VCog-Bench, consisting of MaRs-VQA, and the inference\npipeline will drive progress toward the next generation of MLLMs with\nhuman-like visual cognition abilities.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "68T01"
    ],
    "primary_category": "cs.CV",
    "comment": "14 pages, 4 figures, the appendix will be updated soon",
    "pdf_url": "http://arxiv.org/pdf/2406.10424v1",
    "published_date": "2024-06-14 22:02:21 UTC",
    "updated_date": "2024-06-14 22:02:21 UTC"
  },
  {
    "arxiv_id": "2406.10415v1",
    "title": "PRISM: A Design Framework for Open-Source Foundation Model Safety",
    "authors": [
      "Terrence Neumann",
      "Bryan Jones"
    ],
    "abstract": "The rapid advancement of open-source foundation models has brought\ntransparency and accessibility to this groundbreaking technology. However, this\nopenness has also enabled the development of highly-capable, unsafe models, as\nexemplified by recent instances such as WormGPT and FraudGPT, which are\nspecifically designed to facilitate criminal activity. As the capabilities of\nopen foundation models continue to grow, potentially outpacing those of\nclosed-source models, the risk of misuse by bad actors poses an increasingly\nserious threat to society. This paper addresses the critical question of how\nopen foundation model developers should approach model safety in light of these\nchallenges. Our analysis reveals that open-source foundation model companies\noften provide less restrictive acceptable use policies (AUPs) compared to their\nclosed-source counterparts, likely due to the inherent difficulties in\nenforcing such policies once the models are released. To tackle this issue, we\nintroduce PRISM, a design framework for open-source foundation model safety\nthat emphasizes Private, Robust, Independent Safety measures, at Minimal\nmarginal cost of compute. The PRISM framework proposes the use of modular\nfunctions that moderate prompts and outputs independently of the core language\nmodel, offering a more adaptable and resilient approach to safety compared to\nthe brittle reinforcement learning methods currently used for value alignment.\nBy focusing on identifying AUP violations and engaging the developer community\nin establishing consensus around safety design decisions, PRISM aims to create\na safer open-source ecosystem that maximizes the potential of these powerful\ntechnologies while minimizing the risks to individuals and society as a whole.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10415v1",
    "published_date": "2024-06-14 21:26:15 UTC",
    "updated_date": "2024-06-14 21:26:15 UTC"
  },
  {
    "arxiv_id": "2406.10411v1",
    "title": "Tree Search for Simultaneous Move Games via Equilibrium Approximation",
    "authors": [
      "Ryan Yu",
      "Alex Olshevsky",
      "Peter Chin"
    ],
    "abstract": "Neural network supported tree-search has shown strong results in a variety of\nperfect information multi-agent tasks. However, the performance of these\nmethods on partial information games has generally been below competing\napproaches. Here we study the class of simultaneous-move games, which are a\nsubclass of partial information games which are most similar to perfect\ninformation games: both agents know the game state with the exception of the\nopponent's move, which is revealed only after each agent makes its own move.\nSimultaneous move games include popular benchmarks such as Google Research\nFootball and Starcraft.\n  In this study we answer the question: can we take tree search algorithms\ntrained through self-play from perfect information settings and adapt them to\nsimultaneous move games without significant loss of performance? We answer this\nquestion by deriving a practical method that attempts to approximate a coarse\ncorrelated equilibrium as a subroutine within a tree search. Our algorithm\nworks on cooperative, competitive, and mixed tasks. Our results are better than\nthe current best MARL algorithms on a wide range of accepted baseline\nenvironments.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "9 pages, 5 tables, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2406.10411v1",
    "published_date": "2024-06-14 21:02:35 UTC",
    "updated_date": "2024-06-14 21:02:35 UTC"
  },
  {
    "arxiv_id": "2406.11898v2",
    "title": "Towards Better Benchmark Datasets for Inductive Knowledge Graph Completion",
    "authors": [
      "Harry Shomer",
      "Jay Revolinsky",
      "Jiliang Tang"
    ],
    "abstract": "Knowledge Graph Completion (KGC) attempts to predict missing facts in a\nKnowledge Graph (KG). Recently, there's been an increased focus on designing\nKGC methods that can excel in the {\\it inductive setting}, where a portion or\nall of the entities and relations seen in inference are unobserved during\ntraining. Numerous benchmark datasets have been proposed for inductive KGC, all\nof which are subsets of existing KGs used for transductive KGC. However, we\nfind that the current procedure for constructing inductive KGC datasets\ninadvertently creates a shortcut that can be exploited even while disregarding\nthe relational information. Specifically, we observe that the Personalized\nPageRank (PPR) score can achieve strong or near SOTA performance on most\ninductive datasets. In this paper, we study the root cause of this problem.\nUsing these insights, we propose an alternative strategy for constructing\ninductive KGC datasets that helps mitigate the PPR shortcut. We then benchmark\nmultiple popular methods using the newly constructed datasets and analyze their\nperformance. The new benchmark datasets help promote a better understanding of\nthe capabilities and challenges of inductive KGC by removing any shortcuts that\nobfuscate performance.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11898v2",
    "published_date": "2024-06-14 21:01:46 UTC",
    "updated_date": "2024-10-06 07:06:34 UTC"
  },
  {
    "arxiv_id": "2406.10401v1",
    "title": "Evaluating Speaker Identity Coding in Self-supervised Models and Humans",
    "authors": [
      "Gasser Elbanna"
    ],
    "abstract": "Speaker identity plays a significant role in human communication and is being\nincreasingly used in societal applications, many through advances in machine\nlearning. Speaker identity perception is an essential cognitive phenomenon that\ncan be broadly reduced to two main tasks: recognizing a voice or discriminating\nbetween voices. Several studies have attempted to identify acoustic correlates\nof identity perception to pinpoint salient parameters for such a task. Unlike\nother communicative social signals, most efforts have yielded inefficacious\nconclusions. Furthermore, current neurocognitive models of voice identity\nprocessing consider the bases of perception as acoustic dimensions such as\nfundamental frequency, harmonics-to-noise ratio, and formant dispersion.\nHowever, these findings do not account for naturalistic speech and\nwithin-speaker variability. Representational spaces of current self-supervised\nmodels have shown significant performance in various speech-related tasks. In\nthis work, we demonstrate that self-supervised representations from different\nfamilies (e.g., generative, contrastive, and predictive models) are\nsignificantly better for speaker identification over acoustic representations.\nWe also show that such a speaker identification task can be used to better\nunderstand the nature of acoustic information representation in different\nlayers of these powerful networks. By evaluating speaker identification\naccuracy across acoustic, phonemic, prosodic, and linguistic variants, we\nreport similarity between model performance and human identity perception. We\nfurther examine these similarities by juxtaposing the encoding spaces of models\nand humans and challenging the use of distance metrics as a proxy for speaker\nproximity. Lastly, we show that some models can predict brain responses in\nAuditory and Language regions during naturalistic stimuli.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "Masters Thesis",
    "pdf_url": "http://arxiv.org/pdf/2406.10401v1",
    "published_date": "2024-06-14 20:07:21 UTC",
    "updated_date": "2024-06-14 20:07:21 UTC"
  },
  {
    "arxiv_id": "2406.11897v1",
    "title": "A Benchmark for Maximum Cut: Towards Standardization of the Evaluation of Learned Heuristics for Combinatorial Optimization",
    "authors": [
      "Ankur Nath",
      "Alan Kuhnle"
    ],
    "abstract": "Recently, there has been much work on the design of general heuristics for\ngraph-based, combinatorial optimization problems via the incorporation of Graph\nNeural Networks (GNNs) to learn distribution-specific solution\nstructures.However, there is a lack of consistency in the evaluation of these\nheuristics, in terms of the baselines and instances chosen, which makes it\ndifficult to assess the relative performance of the algorithms. In this paper,\nwe propose an open-source benchmark suite MaxCut-Bench dedicated to the NP-hard\nMaximum Cut problem in both its weighted and unweighted variants, based on a\ncareful selection of instances curated from diverse graph datasets. The suite\noffers a unified interface to various heuristics, both traditional and machine\nlearning-based. Next, we use the benchmark in an attempt to systematically\ncorroborate or reproduce the results of several, popular learning-based\napproaches, including S2V-DQN [31], ECO-DQN [4], among others, in terms of\nthree dimensions: objective value, generalization, and scalability. Our\nempirical results show that several of the learned heuristics fail to\noutperform a naive greedy algorithm, and that only one of them consistently\noutperforms Tabu Search, a simple, general heuristic based upon local search.\nFurthermore, we find that the performance of ECO-DQN remains the same or is\nimproved if the GNN is replaced by a simple linear regression on a subset of\nthe features that are related to Tabu Search. Code, data, and pretrained models\nare available at: \\url{https://github.com/ankurnath/MaxCut-Bench}.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "math.OC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11897v1",
    "published_date": "2024-06-14 19:44:23 UTC",
    "updated_date": "2024-06-14 19:44:23 UTC"
  },
  {
    "arxiv_id": "2406.10382v3",
    "title": "Efficient Prompting for LLM-based Generative Internet of Things",
    "authors": [
      "Bin Xiao",
      "Burak Kantarci",
      "Jiawen Kang",
      "Dusit Niyato",
      "Mohsen Guizani"
    ],
    "abstract": "Large language models (LLMs) have demonstrated remarkable capacities on\nvarious tasks, and integrating the capacities of LLMs into the Internet of\nThings (IoT) applications has drawn much research attention recently. Due to\nsecurity concerns, many institutions avoid accessing state-of-the-art\ncommercial LLM services, requiring the deployment and utilization of\nopen-source LLMs in a local network setting. However, open-source LLMs usually\nhave more limitations regarding their performance, such as their arithmetic\ncalculation and reasoning capacities, and practical systems of applying LLMs to\nIoT have yet to be well-explored. Therefore, we propose a LLM-based Generative\nIoT (GIoT) system deployed in the local network setting in this study. To\nalleviate the limitations of LLMs and provide service with competitive\nperformance, we apply prompt engineering methods to enhance the capacities of\nthe open-source LLMs, design a Prompt Management Module and a Post-processing\nModule to manage the tailored prompts for different tasks and process the\nresults generated by the LLMs. To demonstrate the effectiveness of the proposed\nsystem, we discuss a challenging Table Question Answering (Table-QA) task as a\ncase study of the proposed system, as tabular data is usually more challenging\nthan plain text because of their complex structures, heterogeneous data types\nand sometimes huge sizes. We conduct comprehensive experiments on two popular\nTable-QA datasets, and the results show that our proposal can achieve\ncompetitive performance compared with state-of-the-art LLMs, demonstrating that\nthe proposed LLM-based GIoT system can provide competitive performance with\ntailored prompting methods and is easily extensible to new tasks without\ntraining.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "14 pages, 11 figures. IEEE Internet of Things Journal, 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.10382v3",
    "published_date": "2024-06-14 19:24:00 UTC",
    "updated_date": "2024-10-06 21:19:20 UTC"
  },
  {
    "arxiv_id": "2406.10368v2",
    "title": "A Neuro-Symbolic Benchmark Suite for Concept Quality and Reasoning Shortcuts",
    "authors": [
      "Samuele Bortolotti",
      "Emanuele Marconato",
      "Tommaso Carraro",
      "Paolo Morettin",
      "Emile van Krieken",
      "Antonio Vergari",
      "Stefano Teso",
      "Andrea Passerini"
    ],
    "abstract": "The advent of powerful neural classifiers has increased interest in problems\nthat require both learning and reasoning. These problems are critical for\nunderstanding important properties of models, such as trustworthiness,\ngeneralization, interpretability, and compliance to safety and structural\nconstraints. However, recent research observed that tasks requiring both\nlearning and reasoning on background knowledge often suffer from reasoning\nshortcuts (RSs): predictors can solve the downstream reasoning task without\nassociating the correct concepts to the high-dimensional data. To address this\nissue, we introduce rsbench, a comprehensive benchmark suite designed to\nsystematically evaluate the impact of RSs on models by providing easy access to\nhighly customizable tasks affected by RSs. Furthermore, rsbench implements\ncommon metrics for evaluating concept quality and introduces novel formal\nverification procedures for assessing the presence of RSs in learning tasks.\nUsing rsbench, we highlight that obtaining high quality concepts in both purely\nneural and neuro-symbolic models is a far-from-solved problem. rsbench is\navailable at: https://unitn-sml.github.io/rsbench.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10368v2",
    "published_date": "2024-06-14 18:52:34 UTC",
    "updated_date": "2024-10-29 11:17:37 UTC"
  },
  {
    "arxiv_id": "2406.10229v1",
    "title": "Quantifying Variance in Evaluation Benchmarks",
    "authors": [
      "Lovish Madaan",
      "Aaditya K. Singh",
      "Rylan Schaeffer",
      "Andrew Poulton",
      "Sanmi Koyejo",
      "Pontus Stenetorp",
      "Sharan Narang",
      "Dieuwke Hupkes"
    ],
    "abstract": "Evaluation benchmarks are the cornerstone of measuring capabilities of large\nlanguage models (LLMs), as well as driving progress in said capabilities.\nOriginally designed to make claims about capabilities (or lack thereof) in\nfully pretrained models, evaluation benchmarks are now also extensively used to\ndecide between various training choices. Despite this widespread usage, we\nrarely quantify the variance in our evaluation benchmarks, which dictates\nwhether differences in performance are meaningful. Here, we define and measure\na range of metrics geared towards measuring variance in evaluation benchmarks,\nincluding seed variance across initialisations, and monotonicity during\ntraining. By studying a large number of models -- both openly available and\npretrained from scratch -- we provide empirical estimates for a variety of\nvariance metrics, with considerations and recommendations for practitioners. We\nalso evaluate the utility and tradeoffs of continuous versus discrete\nperformance measures and explore options for better understanding and reducing\nthis variance. We find that simple changes, such as framing choice tasks (like\nMMLU) as completion tasks, can often reduce variance for smaller scale\n($\\sim$7B) models, while more involved methods inspired from human testing\nliterature (such as item analysis and item response theory) struggle to\nmeaningfully reduce variance. Overall, our work provides insights into variance\nin evaluation benchmarks, suggests LM-specific techniques to reduce variance,\nand more generally encourages practitioners to carefully factor in variance\nwhen comparing models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10229v1",
    "published_date": "2024-06-14 17:59:54 UTC",
    "updated_date": "2024-06-14 17:59:54 UTC"
  },
  {
    "arxiv_id": "2406.10228v1",
    "title": "VEGA: Learning Interleaved Image-Text Comprehension in Vision-Language Large Models",
    "authors": [
      "Chenyu Zhou",
      "Mengdan Zhang",
      "Peixian Chen",
      "Chaoyou Fu",
      "Yunhang Shen",
      "Xiawu Zheng",
      "Xing Sun",
      "Rongrong Ji"
    ],
    "abstract": "The swift progress of Multi-modal Large Models (MLLMs) has showcased their\nimpressive ability to tackle tasks blending vision and language. Yet, most\ncurrent models and benchmarks cater to scenarios with a narrow scope of visual\nand textual contexts. These models often fall short when faced with complex\ncomprehension tasks, which involve navigating through a plethora of irrelevant\nand potentially misleading information in both text and image forms. To bridge\nthis gap, we introduce a new, more demanding task known as Interleaved\nImage-Text Comprehension (IITC). This task challenges models to discern and\ndisregard superfluous elements in both images and text to accurately answer\nquestions and to follow intricate instructions to pinpoint the relevant image.\nIn support of this task, we further craft a new VEGA dataset, tailored for the\nIITC task on scientific content, and devised a subtask, Image-Text Association\n(ITA), to refine image-text correlation skills. Our evaluation of four leading\nclosed-source models, as well as various open-source models using VEGA,\nunderscores the rigorous nature of IITC. Even the most advanced models, such as\nGemini-1.5-pro and GPT4V, only achieved modest success. By employing a\nmulti-task, multi-scale post-training strategy, we have set a robust baseline\nfor MLLMs on the IITC task, attaining an $85.8\\%$ accuracy rate in image\nassociation and a $0.508$ Rouge score. These results validate the effectiveness\nof our dataset in improving MLLMs capabilities for nuanced image-text\ncomprehension.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page: https://zhourax.github.io/VEGA/",
    "pdf_url": "http://arxiv.org/pdf/2406.10228v1",
    "published_date": "2024-06-14 17:59:40 UTC",
    "updated_date": "2024-06-14 17:59:40 UTC"
  },
  {
    "arxiv_id": "2406.10227v1",
    "title": "VideoGUI: A Benchmark for GUI Automation from Instructional Videos",
    "authors": [
      "Kevin Qinghong Lin",
      "Linjie Li",
      "Difei Gao",
      "Qinchen WU",
      "Mingyi Yan",
      "Zhengyuan Yang",
      "Lijuan Wang",
      "Mike Zheng Shou"
    ],
    "abstract": "Graphical User Interface (GUI) automation holds significant promise for\nenhancing human productivity by assisting with computer tasks. Existing task\nformulations primarily focus on simple tasks that can be specified by a single,\nlanguage-only instruction, such as \"Insert a new slide.\" In this work, we\nintroduce VideoGUI, a novel multi-modal benchmark designed to evaluate GUI\nassistants on visual-centric GUI tasks. Sourced from high-quality web\ninstructional videos, our benchmark focuses on tasks involving professional and\nnovel software (e.g., Adobe Photoshop or Stable Diffusion WebUI) and complex\nactivities (e.g., video editing). VideoGUI evaluates GUI assistants through a\nhierarchical process, allowing for identification of the specific levels at\nwhich they may fail: (i) high-level planning: reconstruct procedural subtasks\nfrom visual conditions without language descriptions; (ii) middle-level\nplanning: generate sequences of precise action narrations based on visual state\n(i.e., screenshot) and goals; (iii) atomic action execution: perform specific\nactions such as accurately clicking designated elements. For each level, we\ndesign evaluation metrics across individual dimensions to provide clear\nsignals, such as individual performance in clicking, dragging, typing, and\nscrolling for atomic action execution. Our evaluation on VideoGUI reveals that\neven the SoTA large multimodal model GPT4o performs poorly on visual-centric\nGUI tasks, especially for high-level planning.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "24 pages, 16 tables, 17 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.10227v1",
    "published_date": "2024-06-14 17:59:08 UTC",
    "updated_date": "2024-06-14 17:59:08 UTC"
  },
  {
    "arxiv_id": "2406.10221v2",
    "title": "Long Story Short: Story-level Video Understanding from 20K Short Films",
    "authors": [
      "Ridouane Ghermi",
      "Xi Wang",
      "Vicky Kalogeiton",
      "Ivan Laptev"
    ],
    "abstract": "Recent developments in vision-language models have significantly advanced\nvideo understanding. Existing datasets and tasks, however, have notable\nlimitations. Most datasets are confined to short videos with limited events and\nnarrow narratives. For example, datasets with instructional and egocentric\nvideos often depict activities of one person in a single scene. Although\nexisting movie datasets offer richer content, they are often limited to\nshort-term tasks, lack publicly available videos, and frequently encounter data\nleakage issues given the use of subtitles and other information about\ncommercial movies during LLM pretraining. To address the above limitations, we\npropose Short-Films 20K (SF20K), the largest publicly available movie dataset.\nSF20K is composed of 20,143 amateur films and offers long-term video tasks in\nthe form of multiple-choice and open-ended question answering. Our extensive\nanalysis of SF20K reveals minimal data leakage, emphasizes the need for\nlong-term reasoning, and demonstrates the strong performance of recent VLMs.\nFinally, we show that instruction tuning on the SF20K-Train set substantially\nimproves model performance, paving the way for future progress in long-term\nvideo understanding.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10221v2",
    "published_date": "2024-06-14 17:54:54 UTC",
    "updated_date": "2025-01-10 10:36:58 UTC"
  },
  {
    "arxiv_id": "2406.10216v2",
    "title": "Regularizing Hidden States Enables Learning Generalizable Reward Model for LLMs",
    "authors": [
      "Rui Yang",
      "Ruomeng Ding",
      "Yong Lin",
      "Huan Zhang",
      "Tong Zhang"
    ],
    "abstract": "Reward models trained on human preference data have been proven to\neffectively align Large Language Models (LLMs) with human intent within the\nframework of reinforcement learning from human feedback (RLHF). However,\ncurrent reward models have limited generalization capabilities to unseen\nprompts and responses, which can lead to an unexpected phenomenon known as\nreward over-optimization, resulting in a decline in actual performance due to\nexcessive optimization of rewards. While previous research has advocated for\nconstraining policy optimization, our study introduces a novel approach to\nenhance the reward model's generalization ability against distribution shifts\nby regularizing the hidden states. Specifically, we retain the base model's\nlanguage model head and incorporate a suite of text-generation losses to\npreserve the hidden states' text-generation capabilities, while concurrently\nlearning a reward head behind the same hidden states. Our experimental results\ndemonstrate that the introduced regularization technique markedly improves the\naccuracy of learned reward models across a variety of out-of-distribution (OOD)\ntasks and effectively alleviates the over-optimization issue in RLHF, offering\na more reliable and robust preference learning paradigm.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.10216v2",
    "published_date": "2024-06-14 17:49:59 UTC",
    "updated_date": "2024-10-23 08:22:44 UTC"
  },
  {
    "arxiv_id": "2406.11895v1",
    "title": "Predicting User Perception of Move Brilliance in Chess",
    "authors": [
      "Kamron Zaidi",
      "Michael Guerzhoy"
    ],
    "abstract": "AI research in chess has been primarily focused on producing stronger agents\nthat can maximize the probability of winning. However, there is another aspect\nto chess that has largely gone unexamined: its aesthetic appeal. Specifically,\nthere exists a category of chess moves called ``brilliant\" moves. These moves\nare appreciated and admired by players for their high intellectual aesthetics.\nWe demonstrate the first system for classifying chess moves as brilliant. The\nsystem uses a neural network, using the output of a chess engine as well as\nfeatures that describe the shape of the game tree. The system achieves an\naccuracy of 79% (with 50% base-rate), a PPV of 83%, and an NPV of 75%. We\ndemonstrate that what humans perceive as ``brilliant\" moves is not merely the\nbest possible move. We show that a move is more likely to be predicted as\nbrilliant, all things being equal, if a weaker engine considers it\nlower-quality (for the same rating by a stronger engine). Our system opens the\navenues for computer chess engines to (appear to) display human-like\nbrilliance, and, hence, creativity.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at the International Conference for Computational Creativity\n  (ICCC) 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.11895v1",
    "published_date": "2024-06-14 17:46:26 UTC",
    "updated_date": "2024-06-14 17:46:26 UTC"
  },
  {
    "arxiv_id": "2406.10210v1",
    "title": "Make It Count: Text-to-Image Generation with an Accurate Number of Objects",
    "authors": [
      "Lital Binyamin",
      "Yoad Tewel",
      "Hilit Segev",
      "Eran Hirsch",
      "Royi Rassin",
      "Gal Chechik"
    ],
    "abstract": "Despite the unprecedented success of text-to-image diffusion models,\ncontrolling the number of depicted objects using text is surprisingly hard.\nThis is important for various applications from technical documents, to\nchildren's books to illustrating cooking recipes. Generating object-correct\ncounts is fundamentally challenging because the generative model needs to keep\na sense of separate identity for every instance of the object, even if several\nobjects look identical or overlap, and then carry out a global computation\nimplicitly during generation. It is still unknown if such representations\nexist. To address count-correct generation, we first identify features within\nthe diffusion model that can carry the object identity information. We then use\nthem to separate and count instances of objects during the denoising process\nand detect over-generation and under-generation. We fix the latter by training\na model that predicts both the shape and location of a missing object, based on\nthe layout of existing ones, and show how it can be used to guide denoising\nwith correct object count. Our approach, CountGen, does not depend on external\nsource to determine object layout, but rather uses the prior from the diffusion\nmodel itself, creating prompt-dependent and seed-dependent layouts. Evaluated\non two benchmark datasets, we find that CountGen strongly outperforms the\ncount-accuracy of existing baselines.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page is at https://make-it-count-paper.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2406.10210v1",
    "published_date": "2024-06-14 17:46:08 UTC",
    "updated_date": "2024-06-14 17:46:08 UTC"
  },
  {
    "arxiv_id": "2406.10200v1",
    "title": "SSTFB: Leveraging self-supervised pretext learning and temporal self-attention with feature branching for real-time video polyp segmentation",
    "authors": [
      "Ziang Xu",
      "Jens Rittscher",
      "Sharib Ali"
    ],
    "abstract": "Polyps are early cancer indicators, so assessing occurrences of polyps and\ntheir removal is critical. They are observed through a colonoscopy screening\nprocedure that generates a stream of video frames. Segmenting polyps in their\nnatural video screening procedure has several challenges, such as the\nco-existence of imaging artefacts, motion blur, and floating debris. Most\nexisting polyp segmentation algorithms are developed on curated still image\ndatasets that do not represent real-world colonoscopy. Their performance often\ndegrades on video data. We propose a video polyp segmentation method that\nperforms self-supervised learning as an auxiliary task and a spatial-temporal\nself-attention mechanism for improved representation learning. Our end-to-end\nconfiguration and joint optimisation of losses enable the network to learn more\ndiscriminative contextual features in videos. Our experimental results\ndemonstrate an improvement with respect to several state-of-the-art (SOTA)\nmethods. Our ablation study also confirms that the choice of the proposed joint\nend-to-end training improves network accuracy by over 3% and nearly 10% on both\nthe Dice similarity coefficient and intersection-over-union compared to the\nrecently proposed method PNS+ and Polyp-PVT, respectively. Results on\npreviously unseen video data indicate that the proposed method generalises.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.10200v1",
    "published_date": "2024-06-14 17:33:11 UTC",
    "updated_date": "2024-06-14 17:33:11 UTC"
  },
  {
    "arxiv_id": "2406.10197v1",
    "title": "Crafting Parts for Expressive Object Composition",
    "authors": [
      "Harsh Rangwani",
      "Aishwarya Agarwal",
      "Kuldeep Kulkarni",
      "R. Venkatesh Babu",
      "Srikrishna Karanam"
    ],
    "abstract": "Text-to-image generation from large generative models like Stable Diffusion,\nDALLE-2, etc., have become a common base for various tasks due to their\nsuperior quality and extensive knowledge bases. As image composition and\ngeneration are creative processes the artists need control over various parts\nof the images being generated. We find that just adding details about parts in\nthe base text prompt either leads to an entirely different image (e.g.,\nmissing/incorrect identity) or the extra part details simply being ignored. To\nmitigate these issues, we introduce PartCraft, which enables image generation\nbased on fine-grained part-level details specified for objects in the base text\nprompt. This allows more control for artists and enables novel object\ncompositions by combining distinctive object parts. PartCraft first localizes\nobject parts by denoising the object region from a specific diffusion process.\nThis enables each part token to be localized to the right object region. After\nobtaining part masks, we run a localized diffusion process in each of the part\nregions based on fine-grained part descriptions and combine them to produce the\nfinal image. All the stages of PartCraft are based on repurposing a pre-trained\ndiffusion model, which enables it to generalize across various domains without\ntraining. We demonstrate the effectiveness of part-level control provided by\nPartCraft qualitatively through visual examples and quantitatively in\ncomparison to the contemporary baselines.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page Will Be Here: https://rangwani-harsh.github.io/PartCraft",
    "pdf_url": "http://arxiv.org/pdf/2406.10197v1",
    "published_date": "2024-06-14 17:31:29 UTC",
    "updated_date": "2024-06-14 17:31:29 UTC"
  },
  {
    "arxiv_id": "2406.10196v1",
    "title": "TRIP-PAL: Travel Planning with Guarantees by Combining Large Language Models and Automated Planners",
    "authors": [
      "Tomas de la Rosa",
      "Sriram Gopalakrishnan",
      "Alberto Pozanco",
      "Zhen Zeng",
      "Daniel Borrajo"
    ],
    "abstract": "Travel planning is a complex task that involves generating a sequence of\nactions related to visiting places subject to constraints and maximizing some\nuser satisfaction criteria. Traditional approaches rely on problem formulation\nin a given formal language, extracting relevant travel information from web\nsources, and use an adequate problem solver to generate a valid solution. As an\nalternative, recent Large Language Model (LLM) based approaches directly output\nplans from user requests using language. Although LLMs possess extensive travel\ndomain knowledge and provide high-level information like points of interest and\npotential routes, current state-of-the-art models often generate plans that\nlack coherence, fail to satisfy constraints fully, and do not guarantee the\ngeneration of high-quality solutions. We propose TRIP-PAL, a hybrid method that\ncombines the strengths of LLMs and automated planners, where (i) LLMs get and\ntranslate travel information and user information into data structures that can\nbe fed into planners; and (ii) automated planners generate travel plans that\nguarantee constraint satisfaction and optimize for users' utility. Our\nexperiments across various travel scenarios show that TRIP-PAL outperforms an\nLLM when generating travel plans.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.10196v1",
    "published_date": "2024-06-14 17:31:16 UTC",
    "updated_date": "2024-06-14 17:31:16 UTC"
  },
  {
    "arxiv_id": "2406.10320v1",
    "title": "Out of style: Misadventures with LLMs and code style transfer",
    "authors": [
      "Karl Munson",
      "Chih-Kai Ting",
      "Serenity Wade",
      "Anish Savla",
      "Julian Dolby",
      "Kiran Kate",
      "Kavitha Srinivas"
    ],
    "abstract": "Like text, programs have styles, and certain programming styles are more\ndesirable than others for program readability, maintainability, and\nperformance. Code style transfer, however, is difficult to automate except for\ntrivial style guidelines such as limits on line length. Inspired by the success\nof using language models for text style transfer, we investigate if code\nlanguage models can perform code style transfer. Code style transfer, unlike\ntext transfer, has rigorous requirements: the system needs to identify lines of\ncode to change, change them correctly, and leave the rest of the program\nuntouched. We designed CSB (Code Style Benchmark), a benchmark suite of code\nstyle transfer tasks across five categories including converting for-loops to\nlist comprehensions, eliminating duplication in code, adding decorators to\nmethods, etc. We then used these tests to see if large pre-trained code\nlanguage models or fine-tuned models perform style transfer correctly, based on\nrigorous metrics to test that the transfer did occur, and the code still passes\nfunctional tests. Surprisingly, language models failed to perform all of the\ntasks, suggesting that they perform poorly on tasks that require code\nunderstanding. We will make available the large-scale corpora to help the\ncommunity build better code models.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10320v1",
    "published_date": "2024-06-14 17:04:56 UTC",
    "updated_date": "2024-06-14 17:04:56 UTC"
  },
  {
    "arxiv_id": "2406.10181v2",
    "title": "Practical offloading for fine-tuning LLM on commodity GPU via learned sparse projectors",
    "authors": [
      "Siyuan Chen",
      "Zhuofeng Wang",
      "Zelong Guan",
      "Yudong Liu",
      "Phillip B. Gibbons"
    ],
    "abstract": "Fine-tuning large language models (LLMs) requires significant memory, often\nexceeding the capacity of a single GPU. A common solution to this memory\nchallenge is offloading compute and data from the GPU to the CPU. However, this\napproach is hampered by the limited bandwidth of commodity hardware, which\nconstrains communication between the CPU and GPU, and by slower matrix\nmultiplications on the CPU.\n  In this paper, we present an offloading framework, LSP-Offload, that enables\nnear-native speed LLM fine-tuning on commodity hardware through learned sparse\nprojectors. Our data-driven approach involves learning efficient sparse\ncompressors that minimize communication with minimal precision loss.\nAdditionally, we introduce a novel layer-wise communication schedule to\nmaximize parallelism between communication and computation. As a result, our\nframework can fine-tune a 1.3 billion parameter model on a 4GB laptop GPU and a\n6.7 billion parameter model on a 24GB NVIDIA RTX 4090 GPU. Compared to\nstate-of-the-art offloading frameworks, our approach reduces end-to-end\nfine-tuning time by 33.1%-62.5% when converging to the same accuracy. We open\nsource our framework at https://github.com/gulang2019/LSP-Offload.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10181v2",
    "published_date": "2024-06-14 16:59:11 UTC",
    "updated_date": "2025-02-09 15:55:44 UTC"
  },
  {
    "arxiv_id": "2406.10318v1",
    "title": "Creating a Lens of Chinese Culture: A Multimodal Dataset for Chinese Pun Rebus Art Understanding",
    "authors": [
      "Tuo Zhang",
      "Tiantian Feng",
      "Yibin Ni",
      "Mengqin Cao",
      "Ruying Liu",
      "Katharine Butler",
      "Yanjun Weng",
      "Mi Zhang",
      "Shrikanth S. Narayanan",
      "Salman Avestimehr"
    ],
    "abstract": "Large vision-language models (VLMs) have demonstrated remarkable abilities in\nunderstanding everyday content. However, their performance in the domain of\nart, particularly culturally rich art forms, remains less explored. As a pearl\nof human wisdom and creativity, art encapsulates complex cultural narratives\nand symbolism. In this paper, we offer the Pun Rebus Art Dataset, a multimodal\ndataset for art understanding deeply rooted in traditional Chinese culture. We\nfocus on three primary tasks: identifying salient visual elements, matching\nelements with their symbolic meanings, and explanations for the conveyed\nmessages. Our evaluation reveals that state-of-the-art VLMs struggle with these\ntasks, often providing biased and hallucinated explanations and showing limited\nimprovement through in-context learning. By releasing the Pun Rebus Art\nDataset, we aim to facilitate the development of VLMs that can better\nunderstand and interpret culturally specific content, promoting greater\ninclusiveness beyond English-based corpora.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10318v1",
    "published_date": "2024-06-14 16:52:00 UTC",
    "updated_date": "2024-06-14 16:52:00 UTC"
  },
  {
    "arxiv_id": "2406.10163v2",
    "title": "MeshAnything: Artist-Created Mesh Generation with Autoregressive Transformers",
    "authors": [
      "Yiwen Chen",
      "Tong He",
      "Di Huang",
      "Weicai Ye",
      "Sijin Chen",
      "Jiaxiang Tang",
      "Xin Chen",
      "Zhongang Cai",
      "Lei Yang",
      "Gang Yu",
      "Guosheng Lin",
      "Chi Zhang"
    ],
    "abstract": "Recently, 3D assets created via reconstruction and generation have matched\nthe quality of manually crafted assets, highlighting their potential for\nreplacement. However, this potential is largely unrealized because these assets\nalways need to be converted to meshes for 3D industry applications, and the\nmeshes produced by current mesh extraction methods are significantly inferior\nto Artist-Created Meshes (AMs), i.e., meshes created by human artists.\nSpecifically, current mesh extraction methods rely on dense faces and ignore\ngeometric features, leading to inefficiencies, complicated post-processing, and\nlower representation quality. To address these issues, we introduce\nMeshAnything, a model that treats mesh extraction as a generation problem,\nproducing AMs aligned with specified shapes. By converting 3D assets in any 3D\nrepresentation into AMs, MeshAnything can be integrated with various 3D asset\nproduction methods, thereby enhancing their application across the 3D industry.\nThe architecture of MeshAnything comprises a VQ-VAE and a shape-conditioned\ndecoder-only transformer. We first learn a mesh vocabulary using the VQ-VAE,\nthen train the shape-conditioned decoder-only transformer on this vocabulary\nfor shape-conditioned autoregressive mesh generation. Our extensive experiments\nshow that our method generates AMs with hundreds of times fewer faces,\nsignificantly improving storage, rendering, and simulation efficiencies, while\nachieving precision comparable to previous methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page: https://buaacyw.github.io/mesh-anything/ Code:\n  https://github.com/buaacyw/MeshAnything",
    "pdf_url": "http://arxiv.org/pdf/2406.10163v2",
    "published_date": "2024-06-14 16:30:25 UTC",
    "updated_date": "2024-10-09 05:00:16 UTC"
  },
  {
    "arxiv_id": "2406.10162v3",
    "title": "Sycophancy to Subterfuge: Investigating Reward-Tampering in Large Language Models",
    "authors": [
      "Carson Denison",
      "Monte MacDiarmid",
      "Fazl Barez",
      "David Duvenaud",
      "Shauna Kravec",
      "Samuel Marks",
      "Nicholas Schiefer",
      "Ryan Soklaski",
      "Alex Tamkin",
      "Jared Kaplan",
      "Buck Shlegeris",
      "Samuel R. Bowman",
      "Ethan Perez",
      "Evan Hubinger"
    ],
    "abstract": "In reinforcement learning, specification gaming occurs when AI systems learn\nundesired behaviors that are highly rewarded due to misspecified training\ngoals. Specification gaming can range from simple behaviors like sycophancy to\nsophisticated and pernicious behaviors like reward-tampering, where a model\ndirectly modifies its own reward mechanism. However, these more pernicious\nbehaviors may be too complex to be discovered via exploration. In this paper,\nwe study whether Large Language Model (LLM) assistants which find easily\ndiscovered forms of specification gaming will generalize to perform rarer and\nmore blatant forms, up to and including reward-tampering. We construct a\ncurriculum of increasingly sophisticated gameable environments and find that\ntraining on early-curriculum environments leads to more specification gaming on\nremaining environments. Strikingly, a small but non-negligible proportion of\nthe time, LLM assistants trained on the full curriculum generalize zero-shot to\ndirectly rewriting their own reward function. Retraining an LLM not to game\nearly-curriculum environments mitigates, but does not eliminate,\nreward-tampering in later environments. Moreover, adding harmlessness training\nto our gameable environments does not prevent reward-tampering. These results\ndemonstrate that LLMs can generalize from common forms of specification gaming\nto more pernicious reward tampering and that such behavior may be nontrivial to\nremove.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Make it easier to find samples from the model, and highlight that our\n  operational definition of reward tampering has false positives where the\n  model attempts to complete the task honestly but edits the reward. Add\n  paragraph to conclusion to this effect, and add sentence to figure 1 to this\n  effect",
    "pdf_url": "http://arxiv.org/pdf/2406.10162v3",
    "published_date": "2024-06-14 16:26:20 UTC",
    "updated_date": "2024-06-29 00:28:47 UTC"
  },
  {
    "arxiv_id": "2406.10160v1",
    "title": "One-pass Multiple Conformer and Foundation Speech Systems Compression and Quantization Using An All-in-one Neural Model",
    "authors": [
      "Zhaoqing Li",
      "Haoning Xu",
      "Tianzi Wang",
      "Shoukang Hu",
      "Zengrui Jin",
      "Shujie Hu",
      "Jiajun Deng",
      "Mingyu Cui",
      "Mengzhe Geng",
      "Xunying Liu"
    ],
    "abstract": "We propose a novel one-pass multiple ASR systems joint compression and\nquantization approach using an all-in-one neural model. A single compression\ncycle allows multiple nested systems with varying Encoder depths, widths, and\nquantization precision settings to be simultaneously constructed without the\nneed to train and store individual target systems separately. Experiments\nconsistently demonstrate the multiple ASR systems compressed in a single\nall-in-one model produced a word error rate (WER) comparable to, or lower by up\nto 1.01\\% absolute (6.98\\% relative) than individually trained systems of equal\ncomplexity. A 3.4x overall system compression and training time speed-up was\nachieved. Maximum model size compression ratios of 12.8x and 3.93x were\nobtained over the baseline Switchboard-300hr Conformer and LibriSpeech-100hr\nfine-tuned wav2vec2.0 models, respectively, incurring no statistically\nsignificant WER increase.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted by Interspeech 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.10160v1",
    "published_date": "2024-06-14 16:18:34 UTC",
    "updated_date": "2024-06-14 16:18:34 UTC"
  },
  {
    "arxiv_id": "2406.10157v5",
    "title": "RoboGolf: Mastering Real-World Minigolf with a Reflective Multi-Modality Vision-Language Model",
    "authors": [
      "Hantao Zhou",
      "Tianying Ji",
      "Lukas Sommerhalder",
      "Michael Goerner",
      "Norman Hendrich",
      "Jianwei Zhang",
      "Fuchun Sun",
      "Huazhe Xu"
    ],
    "abstract": "Minigolf is an exemplary real-world game for examining embodied intelligence,\nrequiring challenging spatial and kinodynamic understanding to putt the ball.\nAdditionally, reflective reasoning is required if the feasibility of a\nchallenge is not ensured. We introduce RoboGolf, a VLM-based framework that\ncombines dual-camera perception with closed-loop action refinement, augmented\nby a reflective equilibrium loop. The core of both loops is powered by\nfinetuned VLMs. We analyze the capabilities of the framework in an offline\ninference setting, relying on an extensive set of recorded trajectories.\nExemplary demonstrations of the analyzed problem domain are available at\nhttps://jity16.github.io/RoboGolf/",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Project page: https://jity16.github.io/RoboGolf/",
    "pdf_url": "http://arxiv.org/pdf/2406.10157v5",
    "published_date": "2024-06-14 16:16:52 UTC",
    "updated_date": "2024-07-21 11:42:04 UTC"
  },
  {
    "arxiv_id": "2406.10154v1",
    "title": "Automated Design of Linear Bounding Functions for Sigmoidal Nonlinearities in Neural Networks",
    "authors": [
      "Matthias K√∂nig",
      "Xiyue Zhang",
      "Holger H. Hoos",
      "Marta Kwiatkowska",
      "Jan N. van Rijn"
    ],
    "abstract": "The ubiquity of deep learning algorithms in various applications has\namplified the need for assuring their robustness against small input\nperturbations such as those occurring in adversarial attacks. Existing complete\nverification techniques offer provable guarantees for all robustness queries\nbut struggle to scale beyond small neural networks. To overcome this\ncomputational intractability, incomplete verification methods often rely on\nconvex relaxation to over-approximate the nonlinearities in neural networks.\nProgress in tighter approximations has been achieved for piecewise linear\nfunctions. However, robustness verification of neural networks for general\nactivation functions (e.g., Sigmoid, Tanh) remains under-explored and poses new\nchallenges. Typically, these networks are verified using convex relaxation\ntechniques, which involve computing linear upper and lower bounds of the\nnonlinear activation functions. In this work, we propose a novel parameter\nsearch method to improve the quality of these linear approximations.\nSpecifically, we show that using a simple search method, carefully adapted to\nthe given verification problem through state-of-the-art algorithm configuration\ntechniques, improves the average global lower bound by 25% on average over the\ncurrent state of the art on several commonly used local robustness verification\nbenchmarks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10154v1",
    "published_date": "2024-06-14 16:16:26 UTC",
    "updated_date": "2024-06-14 16:16:26 UTC"
  },
  {
    "arxiv_id": "2406.10149v2",
    "title": "BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack",
    "authors": [
      "Yuri Kuratov",
      "Aydar Bulatov",
      "Petr Anokhin",
      "Ivan Rodkin",
      "Dmitry Sorokin",
      "Artyom Sorokin",
      "Mikhail Burtsev"
    ],
    "abstract": "In recent years, the input context sizes of large language models (LLMs) have\nincreased dramatically. However, existing evaluation methods have not kept\npace, failing to comprehensively assess the efficiency of models in handling\nlong contexts. To bridge this gap, we introduce the BABILong benchmark,\ndesigned to test language models' ability to reason across facts distributed in\nextremely long documents. BABILong includes a diverse set of 20 reasoning\ntasks, including fact chaining, simple induction, deduction, counting, and\nhandling lists/sets. These tasks are challenging on their own, and even more\ndemanding when the required facts are scattered across long natural text. Our\nevaluations show that popular LLMs effectively utilize only 10-20\\% of the\ncontext and their performance declines sharply with increased reasoning\ncomplexity. Among alternatives to in-context reasoning, Retrieval-Augmented\nGeneration methods achieve a modest 60\\% accuracy on single-fact question\nanswering, independent of context length. Among context extension methods, the\nhighest performance is demonstrated by recurrent memory transformers after\nfine-tuning, enabling the processing of lengths up to 50 million tokens. The\nBABILong benchmark is extendable to any length to support the evaluation of new\nupcoming models with increased capabilities, and we provide splits up to 10\nmillion token lengths.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "NeurIPS 2024 Datasets and Benchmarks Track",
    "pdf_url": "http://arxiv.org/pdf/2406.10149v2",
    "published_date": "2024-06-14 16:00:29 UTC",
    "updated_date": "2024-11-06 14:50:40 UTC"
  },
  {
    "arxiv_id": "2406.10144v1",
    "title": "Improving rule mining via embedding-based link prediction",
    "authors": [
      "N'Dah Jean Kouagou",
      "Arif Yilmaz",
      "Michel Dumontier",
      "Axel-Cyrille Ngonga Ngomo"
    ],
    "abstract": "Rule mining on knowledge graphs allows for explainable link prediction.\nContrarily, embedding-based methods for link prediction are well known for\ntheir generalization capabilities, but their predictions are not interpretable.\nSeveral approaches combining the two families have been proposed in recent\nyears. The majority of the resulting hybrid approaches are usually trained\nwithin a unified learning framework, which often leads to convergence issues\ndue to the complexity of the learning task. In this work, we propose a new way\nto combine the two families of approaches. Specifically, we enrich a given\nknowledge graph by means of its pre-trained entity and relation embeddings\nbefore applying rule mining systems on the enriched knowledge graph. To\nvalidate our approach, we conduct extensive experiments on seven benchmark\ndatasets. An analysis of the results generated by our approach suggests that we\ndiscover new valuable rules on the enriched graphs. We provide an open source\nimplementation of our approach as well as pretrained models and datasets at\nhttps://github.com/Jean-KOUAGOU/EnhancedRuleLearning",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "13 pages, 2 figures, 11 tables",
    "pdf_url": "http://arxiv.org/pdf/2406.10144v1",
    "published_date": "2024-06-14 15:53:30 UTC",
    "updated_date": "2024-06-14 15:53:30 UTC"
  },
  {
    "arxiv_id": "2406.10141v1",
    "title": "The Rise and Fall(?) of Software Engineering",
    "authors": [
      "Antonio Mastropaolo",
      "Camilo Escobar-Vel√°squez",
      "Mario Linares-V√°squez"
    ],
    "abstract": "Over the last ten years, the realm of Artificial Intelligence (AI) has\nexperienced an explosion of revolutionary breakthroughs, transforming what\nseemed like a far-off dream into a reality that is now deeply embedded in our\neveryday lives. AI's widespread impact is revolutionizing virtually all aspects\nof human life, and software engineering (SE) is no exception.\n  As we explore this changing landscape, we are faced with questions about what\nthe future holds for SE and how AI will reshape the roles, duties, and\nmethodologies within the field. The introduction of these groundbreaking\ntechnologies highlights the inevitable shift towards a new paradigm, suggesting\na future where AI's capabilities may redefine the boundaries of SE, potentially\neven more than human input.\n  In this paper, we aim at outlining the key elements that, based on our\nexpertise, are vital for the smooth integration of AI into SE, all while\npreserving the intrinsic human creativity that has been the driving force\nbehind the field. First, we provide a brief description of SE and AI evolution.\nAfterward, we delve into the intricate interplay between AI-driven automation\nand human innovation, exploring how these two components can work together to\nadvance SE practices to new methods and standards.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "D.2; I.2"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10141v1",
    "published_date": "2024-06-14 15:50:24 UTC",
    "updated_date": "2024-06-14 15:50:24 UTC"
  },
  {
    "arxiv_id": "2406.10133v1",
    "title": "Evaluation of Large Language Models: STEM education and Gender Stereotypes",
    "authors": [
      "Smilla Due",
      "Sneha Das",
      "Marianne Andersen",
      "Berta Plandolit L√≥pez",
      "Sniff Andersen Nex√∏",
      "Line Clemmensen"
    ],
    "abstract": "Large Language Models (LLMs) have an increasing impact on our lives with use\ncases such as chatbots, study support, coding support, ideation, writing\nassistance, and more. Previous studies have revealed linguistic biases in\npronouns used to describe professions or adjectives used to describe men vs\nwomen. These issues have to some degree been addressed in updated LLM versions,\nat least to pass existing tests. However, biases may still be present in the\nmodels, and repeated use of gender stereotypical language may reinforce the\nunderlying assumptions and are therefore important to examine further. This\npaper investigates gender biases in LLMs in relation to educational choices\nthrough an open-ended, true to user-case experimental design and a quantitative\nanalysis. We investigate the biases in the context of four different cultures,\nlanguages, and educational systems (English/US/UK, Danish/DK, Catalan/ES, and\nHindi/IN) for ages ranging from 10 to 16 years, corresponding to important\neducational transition points in the different countries. We find that there\nare significant and large differences in the ratio of STEM to non-STEM\nsuggested education paths provided by chatGPT when using typical girl vs boy\nnames to prompt lists of suggested things to become. There are generally fewer\nSTEM suggestions in the Danish, Spanish, and Indian context compared to the\nEnglish. We also find subtle differences in the suggested professions, which we\ncategorise and report.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10133v1",
    "published_date": "2024-06-14 15:42:42 UTC",
    "updated_date": "2024-06-14 15:42:42 UTC"
  },
  {
    "arxiv_id": "2406.10131v2",
    "title": "Linear Contextual Bandits with Hybrid Payoff: Revisited",
    "authors": [
      "Nirjhar Das",
      "Gaurav Sinha"
    ],
    "abstract": "We study the Linear Contextual Bandit problem in the hybrid reward setting.\nIn this setting every arm's reward model contains arm specific parameters in\naddition to parameters shared across the reward models of all the arms. We can\nreduce this setting to two closely related settings (a) Shared - no arm\nspecific parameters, and (b) Disjoint - only arm specific parameters, enabling\nthe application of two popular state of the art algorithms - $\\texttt{LinUCB}$\nand $\\texttt{DisLinUCB}$ (Algorithm 1 in (Li et al. 2010)). When the arm\nfeatures are stochastic and satisfy a popular diversity condition, we provide\nnew regret analyses for both algorithms, significantly improving on the known\nregret guarantees of these algorithms. Our novel analysis critically exploits\nthe hybrid reward structure and the diversity condition. Moreover, we introduce\na new algorithm $\\texttt{HyLinUCB}$ that crucially modifies $\\texttt{LinUCB}$\n(using a new exploration coefficient) to account for sparsity in the hybrid\nsetting. Under the same diversity assumptions, we prove that\n$\\texttt{HyLinUCB}$ also incurs only $O(\\sqrt{T})$ regret for $T$ rounds. We\nperform extensive experiments on synthetic and real-world datasets\ndemonstrating strong empirical performance of $\\texttt{HyLinUCB}$.For number of\narm specific parameters much larger than the number of shared parameters, we\nobserve that $\\texttt{DisLinUCB}$ incurs the lowest regret. In this case,\nregret of $\\texttt{HyLinUCB}$ is the second best and extremely competitive to\n$\\texttt{DisLinUCB}$. In all other situations, including our real-world\ndataset, $\\texttt{HyLinUCB}$ has significantly lower regret than\n$\\texttt{LinUCB}$, $\\texttt{DisLinUCB}$ and other SOTA baselines we considered.\nWe also empirically observe that the regret of $\\texttt{HyLinUCB}$ grows much\nslower with the number of arms compared to baselines, making it suitable even\nfor very large action spaces.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at ECML PKDD 2024 as a Research Track Paper",
    "pdf_url": "http://arxiv.org/pdf/2406.10131v2",
    "published_date": "2024-06-14 15:41:21 UTC",
    "updated_date": "2024-09-03 20:13:24 UTC"
  },
  {
    "arxiv_id": "2406.10127v1",
    "title": "Exploration by Learning Diverse Skills through Successor State Measures",
    "authors": [
      "Paul-Antoine Le Tolguenec",
      "Yann Besse",
      "Florent Teichteil-Konigsbuch",
      "Dennis G. Wilson",
      "Emmanuel Rachelson"
    ],
    "abstract": "The ability to perform different skills can encourage agents to explore. In\nthis work, we aim to construct a set of diverse skills which uniformly cover\nthe state space. We propose a formalization of this search for diverse skills,\nbuilding on a previous definition based on the mutual information between\nstates and skills. We consider the distribution of states reached by a policy\nconditioned on each skill and leverage the successor state measure to maximize\nthe difference between these skill distributions. We call this approach LEADS:\nLearning Diverse Skills through Successor States. We demonstrate our approach\non a set of maze navigation and robotic control tasks which show that our\nmethod is capable of constructing a diverse set of skills which exhaustively\ncover the state space without relying on reward or exploration bonuses. Our\nfindings demonstrate that this new formalization promotes more robust and\nefficient exploration by combining mutual information maximization and\nexploration bonuses.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10127v1",
    "published_date": "2024-06-14 15:36:15 UTC",
    "updated_date": "2024-06-14 15:36:15 UTC"
  },
  {
    "arxiv_id": "2406.10121v1",
    "title": "Data Ethics in the Era of Healthcare Artificial Intelligence in Africa: An Ubuntu Philosophy Perspective",
    "authors": [
      "Abdoul Jalil Djiberou Mahamadou",
      "Aloysius Ochasi",
      "Russ B. Altman"
    ],
    "abstract": "Data are essential in developing healthcare artificial intelligence (AI)\nsystems. However, patient data collection, access, and use raise ethical\nconcerns, including informed consent, data bias, data protection and privacy,\ndata ownership, and benefit sharing. Various ethical frameworks have been\nproposed to ensure the ethical use of healthcare data and AI, however, these\nframeworks often align with Western cultural values, social norms, and\ninstitutional contexts emphasizing individual autonomy and well-being. Ethical\nguidelines must reflect political and cultural settings to account for cultural\ndiversity, inclusivity, and historical factors such as colonialism. Thus, this\npaper discusses healthcare data ethics in the AI era in Africa from the Ubuntu\nphilosophy perspective. It focuses on the contrast between individualistic and\ncommunitarian approaches to data ethics. The proposed framework could inform\nstakeholders, including AI developers, healthcare providers, the public, and\npolicy-makers about healthcare data ethical usage in AI in Africa.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "I.2.6"
    ],
    "primary_category": "cs.CY",
    "comment": "16 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.10121v1",
    "published_date": "2024-06-14 15:28:36 UTC",
    "updated_date": "2024-06-14 15:28:36 UTC"
  },
  {
    "arxiv_id": "2406.10108v1",
    "title": "Precipitation Nowcasting Using Physics Informed Discriminator Generative Models",
    "authors": [
      "Junzhe Yin",
      "Cristian Meo",
      "Ankush Roy",
      "Zeineh Bou Cher",
      "Yanbo Wang",
      "Ruben Imhoff",
      "Remko Uijlenhoet",
      "Justin Dauwels"
    ],
    "abstract": "Nowcasting leverages real-time atmospheric conditions to forecast weather\nover short periods. State-of-the-art models, including PySTEPS, encounter\ndifficulties in accurately forecasting extreme weather events because of their\nunpredictable distribution patterns. In this study, we design a\nphysics-informed neural network to perform precipitation nowcasting using the\nprecipitation and meteorological data from the Royal Netherlands Meteorological\nInstitute (KNMI). This model draws inspiration from the novel Physics-Informed\nDiscriminator GAN (PID-GAN) formulation, directly integrating physics-based\nsupervision within the adversarial learning framework. The proposed model\nadopts a GAN structure, featuring a Vector Quantization Generative Adversarial\nNetwork (VQ-GAN) and a Transformer as the generator, with a temporal\ndiscriminator serving as the discriminator. Our findings demonstrate that the\nPID-GAN model outperforms numerical and SOTA deep generative models in terms of\nprecipitation nowcasting downstream metrics.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10108v1",
    "published_date": "2024-06-14 15:12:53 UTC",
    "updated_date": "2024-06-14 15:12:53 UTC"
  },
  {
    "arxiv_id": "2406.10100v2",
    "title": "SkySenseGPT: A Fine-Grained Instruction Tuning Dataset and Model for Remote Sensing Vision-Language Understanding",
    "authors": [
      "Junwei Luo",
      "Zhen Pang",
      "Yongjun Zhang",
      "Tingzhu Wang",
      "Linlin Wang",
      "Bo Dang",
      "Jiangwei Lao",
      "Jian Wang",
      "Jingdong Chen",
      "Yihua Tan",
      "Yansheng Li"
    ],
    "abstract": "Remote Sensing Large Multi-Modal Models (RSLMMs) are developing rapidly and\nshowcase significant capabilities in remote sensing imagery (RSI)\ncomprehension. However, due to the limitations of existing datasets, RSLMMs\nhave shortcomings in understanding the rich semantic relations among objects in\ncomplex remote sensing scenes. To unlock RSLMMs' complex comprehension ability,\nwe propose a large-scale instruction tuning dataset FIT-RS, containing\n1,800,851 instruction samples. FIT-RS covers common interpretation tasks and\ninnovatively introduces several complex comprehension tasks of escalating\ndifficulty, ranging from relation reasoning to image-level scene graph\ngeneration. Based on FIT-RS, we build the FIT-RSFG benchmark. Furthermore, we\nestablish a new benchmark to evaluate the fine-grained relation comprehension\ncapabilities of LMMs, named FIT-RSRC. Based on combined instruction data, we\npropose SkySenseGPT, which achieves outstanding performance on both public\ndatasets and FIT-RSFG, surpassing existing RSLMMs. We hope the FIT-RS dataset\ncan enhance the relation comprehension capability of RSLMMs and provide a\nlarge-scale fine-grained data source for the remote sensing community. The\ndataset will be available at https://github.com/Luo-Z13/SkySenseGPT",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "30 pages, 5 figures, 19 tables, dataset and code see\n  https://github.com/Luo-Z13/SkySenseGPT",
    "pdf_url": "http://arxiv.org/pdf/2406.10100v2",
    "published_date": "2024-06-14 14:57:07 UTC",
    "updated_date": "2024-07-08 04:33:37 UTC"
  },
  {
    "arxiv_id": "2406.10098v1",
    "title": "ECGMamba: Towards Efficient ECG Classification with BiSSM",
    "authors": [
      "Yupeng Qiang",
      "Xunde Dong",
      "Xiuling Liu",
      "Yang Yang",
      "Yihai Fang",
      "Jianhong Dou"
    ],
    "abstract": "Electrocardiogram (ECG) signal analysis represents a pivotal technique in the\ndiagnosis of cardiovascular diseases. Although transformer-based models have\nmade significant progress in ECG classification, they exhibit inefficiencies in\nthe inference phase. The issue is primarily attributable to the secondary\ncomputational complexity of Transformer's self-attention mechanism.\nparticularly when processing lengthy sequences. To address this issue, we\npropose a novel model, ECGMamba, which employs a bidirectional state-space\nmodel (BiSSM) to enhance classification efficiency. ECGMamba is based on the\ninnovative Mamba-based block, which incorporates a range of time series\nmodeling techniques to enhance performance while maintaining the efficiency of\ninference. The experimental results on two publicly available ECG datasets\ndemonstrate that ECGMamba effectively balances the effectiveness and efficiency\nof classification, achieving competitive performance. This study not only\ncontributes to the body of knowledge in the field of ECG classification but\nalso provides a new research path for efficient and accurate ECG signal\nanalysis. This is of guiding significance for the development of diagnostic\nmodels for cardiovascular diseases.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages, 2 figures. arXiv admin note: text overlap with\n  arXiv:2404.17858 by other authors",
    "pdf_url": "http://arxiv.org/pdf/2406.10098v1",
    "published_date": "2024-06-14 14:55:53 UTC",
    "updated_date": "2024-06-14 14:55:53 UTC"
  },
  {
    "arxiv_id": "2406.10087v1",
    "title": "Biomarker based Cancer Classification using an Ensemble with Pre-trained Models",
    "authors": [
      "Chongmin Lee",
      "Jihie Kim"
    ],
    "abstract": "Certain cancer types, namely pancreatic cancer is difficult to detect at an\nearly stage; sparking the importance of discovering the causal relationship\nbetween biomarkers and cancer to identify cancer efficiently. By allowing for\nthe detection and monitoring of specific biomarkers through a non-invasive\nmethod, liquid biopsies enhance the precision and efficacy of medical\ninterventions, advocating the move towards personalized healthcare. Several\nmachine learning algorithms such as Random Forest, SVM are utilized for\nclassification, yet causing inefficiency due to the need for conducting\nhyperparameter tuning. We leverage a meta-trained Hyperfast model for\nclassifying cancer, accomplishing the highest AUC of 0.9929 and simultaneously\nachieving robustness especially on highly imbalanced datasets compared to other\nML algorithms in several binary classification tasks (e.g. breast invasive\ncarcinoma; BRCA vs. non-BRCA). We also propose a novel ensemble model combining\npre-trained Hyperfast model, XGBoost, and LightGBM for multi-class\nclassification tasks, achieving an incremental increase in accuracy (0.9464)\nwhile merely using 500 PCA features; distinguishable from previous studies\nwhere they used more than 2,000 features for similar results.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to the AIAA Workshop at IJCAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.10087v1",
    "published_date": "2024-06-14 14:43:59 UTC",
    "updated_date": "2024-06-14 14:43:59 UTC"
  },
  {
    "arxiv_id": "2406.10079v3",
    "title": "Localizing Events in Videos with Multimodal Queries",
    "authors": [
      "Gengyuan Zhang",
      "Mang Ling Ada Fok",
      "Jialu Ma",
      "Yan Xia",
      "Daniel Cremers",
      "Philip Torr",
      "Volker Tresp",
      "Jindong Gu"
    ],
    "abstract": "Localizing events in videos based on semantic queries is a pivotal task in\nvideo understanding, with the growing significance of user-oriented\napplications like video search. Yet, current research predominantly relies on\nnatural language queries (NLQs), overlooking the potential of using multimodal\nqueries (MQs) that integrate images to more flexibly represent semantic queries\n-- especially when it is difficult to express non-verbal or unfamiliar concepts\nin words. To bridge this gap, we introduce ICQ, a new benchmark designed for\nlocalizing events in videos with MQs, alongside an evaluation dataset\nICQ-Highlight. To accommodate and evaluate existing video localization models\nfor this new task, we propose 3 Multimodal Query Adaptation methods and a novel\nSurrogate Fine-tuning on pseudo-MQs strategy. ICQ systematically benchmarks 12\nstate-of-the-art backbone models, spanning from specialized video localization\nmodels to Video LLMs, across diverse application domains. Our experiments\nhighlight the high potential of MQs in real-world applications. We believe this\nbenchmark is a first step toward advancing MQs in video event localization.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "20 pages (including references and appendix); for the project\n  homepage, see https://icq-benchmark.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2406.10079v3",
    "published_date": "2024-06-14 14:35:58 UTC",
    "updated_date": "2024-11-21 17:58:55 UTC"
  },
  {
    "arxiv_id": "2406.10057v3",
    "title": "First Multi-Dimensional Evaluation of Flowchart Comprehension for Multimodal Large Language Models",
    "authors": [
      "Enming Zhang",
      "Ruobing Yao",
      "Huanyong Liu",
      "Junhui Yu",
      "Jiale Wang"
    ],
    "abstract": "With the development of Multimodal Large Language Models (MLLMs) technology,\nits general capabilities are increasingly powerful. To evaluate the various\nabilities of MLLMs, numerous evaluation systems have emerged. But now there is\nstill a lack of a comprehensive method to evaluate MLLMs in the tasks related\nto flowcharts, which are very important in daily life and work. We propose the\nfirst comprehensive method, FlowCE, to assess MLLMs across various dimensions\nfor tasks related to flowcharts. It encompasses evaluating MLLMs' abilities in\nReasoning, Localization Recognition, Information Extraction, Logical\nVerification, and Summarization on flowcharts. However, we find that even the\nGPT4o model achieves only a score of 56.63. Among open-source models,\nPhi-3-Vision obtained the highest score of 49.97. We hope that FlowCE can\ncontribute to future research on MLLMs for tasks based on flowcharts.\n\\url{https://github.com/360AILABNLP/FlowCE}",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10057v3",
    "published_date": "2024-06-14 14:15:35 UTC",
    "updated_date": "2024-11-05 04:40:34 UTC"
  },
  {
    "arxiv_id": "2406.12925v2",
    "title": "GLiNER multi-task: Generalist Lightweight Model for Various Information Extraction Tasks",
    "authors": [
      "Ihor Stepanov",
      "Mykhailo Shtopko"
    ],
    "abstract": "Information extraction tasks require both accurate, efficient, and\ngeneralisable models. Classical supervised deep learning approaches can achieve\nthe required performance, but they need large datasets and are limited in their\nability to adapt to different tasks. On the other hand, large language models\n(LLMs) demonstrate good generalization, meaning that they can adapt to many\ndifferent tasks based on user requests. However, LLMs are computationally\nexpensive and tend to fail to generate structured outputs. In this article, we\nwill introduce a new kind of GLiNER model that can be used for various\ninformation extraction tasks while being a small encoder model. Our model\nachieved SoTA performance on zero-shot NER benchmarks and leading performance\non question-answering, summarization and relation extraction tasks.\nAdditionally, in this article, we will cover experimental results on\nself-learning approaches for named entity recognition using GLiNER models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, 1 figure, 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2406.12925v2",
    "published_date": "2024-06-14 13:54:29 UTC",
    "updated_date": "2024-08-01 10:09:15 UTC"
  },
  {
    "arxiv_id": "2406.10043v1",
    "title": "Bridging the Communication Gap: Artificial Agents Learning Sign Language through Imitation",
    "authors": [
      "Federico Tavella",
      "Aphrodite Galata",
      "Angelo Cangelosi"
    ],
    "abstract": "Artificial agents, particularly humanoid robots, interact with their\nenvironment, objects, and people using cameras, actuators, and physical\npresence. Their communication methods are often pre-programmed, limiting their\nactions and interactions. Our research explores acquiring non-verbal\ncommunication skills through learning from demonstrations, with potential\napplications in sign language comprehension and expression. In particular, we\nfocus on imitation learning for artificial agents, exemplified by teaching a\nsimulated humanoid American Sign Language. We use computer vision and deep\nlearning to extract information from videos, and reinforcement learning to\nenable the agent to replicate observed actions. Compared to other methods, our\napproach eliminates the need for additional hardware to acquire information. We\ndemonstrate how the combination of these different techniques offers a viable\nway to learn sign language. Our methodology successfully teaches 5 different\nsigns involving the upper body (i.e., arms and hands). This research paves the\nway for advanced communication skills in artificial agents.",
    "categories": [
      "cs.AI",
      "cs.GR",
      "cs.HC",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10043v1",
    "published_date": "2024-06-14 13:50:29 UTC",
    "updated_date": "2024-06-14 13:50:29 UTC"
  },
  {
    "arxiv_id": "2406.10040v1",
    "title": "FZI-WIM at SemEval-2024 Task 2: Self-Consistent CoT for Complex NLI in Biomedical Domain",
    "authors": [
      "Jin Liu",
      "Steffen Thoma"
    ],
    "abstract": "This paper describes the inference system of FZI-WIM at the SemEval-2024 Task\n2: Safe Biomedical Natural Language Inference for Clinical Trials. Our system\nutilizes the chain of thought (CoT) paradigm to tackle this complex reasoning\nproblem and further improves the CoT performance with self-consistency. Instead\nof greedy decoding, we sample multiple reasoning chains with the same prompt\nand make the final verification with majority voting. The self-consistent CoT\nsystem achieves a baseline F1 score of 0.80 (1st), faithfulness score of 0.90\n(3rd), and consistency score of 0.73 (12th). We release the code and data\npublicly https://github.com/jens5588/FZI-WIM-NLI4CT.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10040v1",
    "published_date": "2024-06-14 13:49:07 UTC",
    "updated_date": "2024-06-14 13:49:07 UTC"
  },
  {
    "arxiv_id": "2406.10034v3",
    "title": "Towards Effective and Efficient Non-autoregressive Decoding Using Block-based Attention Mask",
    "authors": [
      "Tianzi Wang",
      "Xurong Xie",
      "Zhaoqing Li",
      "Shoukang Hu",
      "Zengrui Jin",
      "Jiajun Deng",
      "Mingyu Cui",
      "Shujie Hu",
      "Mengzhe Geng",
      "Guinan Li",
      "Helen Meng",
      "Xunying Liu"
    ],
    "abstract": "This paper proposes a novel non-autoregressive (NAR) block-based Attention\nMask Decoder (AMD) that flexibly balances performance-efficiency trade-offs for\nConformer ASR systems. AMD performs parallel NAR inference within contiguous\nblocks of output labels that are concealed using attention masks, while\nconducting left-to-right AR prediction and history context amalgamation between\nblocks. A beam search algorithm is designed to leverage a dynamic fusion of\nCTC, AR Decoder, and AMD probabilities. Experiments on the LibriSpeech-100hr\ncorpus suggest the tripartite Decoder incorporating the AMD module produces a\nmaximum decoding speed-up ratio of 1.73x over the baseline CTC+AR decoding,\nwhile incurring no statistically significant word error rate (WER) increase on\nthe test sets. When operating with the same decoding real time factors,\nstatistically significant WER reductions of up to 0.7% and 0.3% absolute (5.3%\nand 6.1% relative) were obtained over the CTC+AR baseline.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "5 pages, 2 figures, 2 tables, Interspeech24 conference",
    "pdf_url": "http://arxiv.org/pdf/2406.10034v3",
    "published_date": "2024-06-14 13:42:38 UTC",
    "updated_date": "2024-08-30 06:44:14 UTC"
  },
  {
    "arxiv_id": "2406.10031v2",
    "title": "Deep Learning Domain Adaptation to Understand Physico-Chemical Processes from Fluorescence Spectroscopy Small Datasets: Application to Ageing of Olive Oil",
    "authors": [
      "Umberto Michelucci",
      "Francesca Venturini"
    ],
    "abstract": "Fluorescence spectroscopy is a fundamental tool in life sciences and\nchemistry, widely used for applications such as environmental monitoring, food\nquality control, and biomedical diagnostics. However, analysis of spectroscopic\ndata with deep learning, in particular of fluorescence excitation-emission\nmatrices (EEMs), presents significant challenges due to the typically small and\nsparse datasets available. Furthermore, the analysis of EEMs is difficult due\nto their high dimensionality and overlapping spectral features. This study\nproposes a new approach that exploits domain adaptation with pretrained vision\nmodels, alongside a novel interpretability algorithm to address these\nchallenges. Thanks to specialised feature engineering of the neural networks\ndescribed in this work, we are now able to provide deeper insights into the\nphysico-chemical processes underlying the data. The proposed approach is\ndemonstrated through the analysis of the oxidation process in extra virgin\nolive oil (EVOO) during ageing, showing its effectiveness in predicting quality\nindicators and identifying the spectral bands, and thus the molecules involved\nin the process. This work describes a significantly innovative approach in the\nuse of deep learning for spectroscopy, transforming it from a black box into a\ntool for understanding complex biological and chemical processes.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.data-an",
      "physics.optics"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10031v2",
    "published_date": "2024-06-14 13:41:21 UTC",
    "updated_date": "2024-06-22 07:29:35 UTC"
  },
  {
    "arxiv_id": "2406.10019v1",
    "title": "Group and Shuffle: Efficient Structured Orthogonal Parametrization",
    "authors": [
      "Mikhail Gorbunov",
      "Nikolay Yudin",
      "Vera Soboleva",
      "Aibek Alanov",
      "Alexey Naumov",
      "Maxim Rakhuba"
    ],
    "abstract": "The increasing size of neural networks has led to a growing demand for\nmethods of efficient fine-tuning. Recently, an orthogonal fine-tuning paradigm\nwas introduced that uses orthogonal matrices for adapting the weights of a\npretrained model. In this paper, we introduce a new class of structured\nmatrices, which unifies and generalizes structured classes from previous works.\nWe examine properties of this class and build a structured orthogonal\nparametrization upon it. We then use this parametrization to modify the\northogonal fine-tuning framework, improving parameter and computational\nefficiency. We empirically validate our method on different domains, including\nadapting of text-to-image diffusion models and downstream task fine-tuning in\nlanguage modeling. Additionally, we adapt our construction for orthogonal\nconvolutions and conduct experiments with 1-Lipschitz neural networks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.NA",
      "math.NA"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10019v1",
    "published_date": "2024-06-14 13:29:36 UTC",
    "updated_date": "2024-06-14 13:29:36 UTC"
  },
  {
    "arxiv_id": "2406.10017v1",
    "title": "Tilt and Average : Geometric Adjustment of the Last Layer for Recalibration",
    "authors": [
      "Gyusang Cho",
      "Chan-Hyun Youn"
    ],
    "abstract": "After the revelation that neural networks tend to produce overconfident\npredictions, the problem of calibration, which aims to align confidence with\naccuracy to enhance the reliability of predictions, has gained significant\nimportance. Several solutions based on calibration maps have been proposed to\naddress the problem of recalibrating a trained classifier using additional\ndatasets. In this paper, we offer an algorithm that transforms the weights of\nthe last layer of the classifier, distinct from the calibration-map-based\napproach. We concentrate on the geometry of the final linear layer,\nspecifically its angular aspect, and adjust the weights of the corresponding\nlayer. We name the method Tilt and Average(\\textsc{Tna}), and validate the\ncalibration effect empirically and theoretically. Through this, we demonstrate\nthat our approach, in addition to the existing calibration-map-based\ntechniques, can yield improved calibration performance. Code available :\nhttps://github.com/GYYYYYUUUUU/TNA_Angular_Scaling.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "20 pages, 11 figures, to appear in International Conference on\n  Machine Learning (ICML2024)",
    "pdf_url": "http://arxiv.org/pdf/2406.10017v1",
    "published_date": "2024-06-14 13:27:56 UTC",
    "updated_date": "2024-06-14 13:27:56 UTC"
  },
  {
    "arxiv_id": "2406.10015v1",
    "title": "Gradient-based Learning in State-based Potential Games for Self-Learning Production Systems",
    "authors": [
      "Steve Yuwono",
      "Marlon L√∂ppenberg",
      "Dorothea Schwung",
      "Andreas Schwung"
    ],
    "abstract": "In this paper, we introduce novel gradient-based optimization methods for\nstate-based potential games (SbPGs) within self-learning distributed production\nsystems. SbPGs are recognised for their efficacy in enabling self-optimizing\ndistributed multi-agent systems and offer a proven convergence guarantee, which\nfacilitates collaborative player efforts towards global objectives. Our study\nstrives to replace conventional ad-hoc random exploration-based learning in\nSbPGs with contemporary gradient-based approaches, which aim for faster\nconvergence and smoother exploration dynamics, thereby shortening training\nduration while upholding the efficacy of SbPGs. Moreover, we propose three\ndistinct variants for estimating the objective function of gradient-based\nlearning, each developed to suit the unique characteristics of the systems\nunder consideration. To validate our methodology, we apply it to a laboratory\ntestbed, namely Bulk Good Laboratory Plant, which represents a smart and\nflexible distributed multi-agent production system. The incorporation of\ngradient-based learning in SbPGs reduces training times and achieves more\noptimal policies than its baseline.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.GT"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10015v1",
    "published_date": "2024-06-14 13:26:36 UTC",
    "updated_date": "2024-06-14 13:26:36 UTC"
  },
  {
    "arxiv_id": "2406.10011v1",
    "title": "Beyond Slow Signs in High-fidelity Model Extraction",
    "authors": [
      "Hanna Foerster",
      "Robert Mullins",
      "Ilia Shumailov",
      "Jamie Hayes"
    ],
    "abstract": "Deep neural networks, costly to train and rich in intellectual property\nvalue, are increasingly threatened by model extraction attacks that compromise\ntheir confidentiality. Previous attacks have succeeded in reverse-engineering\nmodel parameters up to a precision of float64 for models trained on random data\nwith at most three hidden layers using cryptanalytical techniques. However, the\nprocess was identified to be very time consuming and not feasible for larger\nand deeper models trained on standard benchmarks. Our study evaluates the\nfeasibility of parameter extraction methods of Carlini et al. [1] further\nenhanced by Canales-Mart\\'inez et al. [2] for models trained on standard\nbenchmarks. We introduce a unified codebase that integrates previous methods\nand reveal that computational tools can significantly influence performance. We\ndevelop further optimisations to the end-to-end attack and improve the\nefficiency of extracting weight signs by up to 14.8 times compared to former\nmethods through the identification of easier and harder to extract neurons.\nContrary to prior assumptions, we identify extraction of weights, not\nextraction of weight signs, as the critical bottleneck. With our improvements,\na 16,721 parameter model with 2 hidden layers trained on MNIST is extracted\nwithin only 98 minutes compared to at least 150 minutes previously. Finally,\naddressing methodological deficiencies observed in previous studies, we propose\nnew ways of robust benchmarking for future model extraction attacks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10011v1",
    "published_date": "2024-06-14 13:24:07 UTC",
    "updated_date": "2024-06-14 13:24:07 UTC"
  },
  {
    "arxiv_id": "2406.09998v1",
    "title": "Understanding Pedestrian Movement Using Urban Sensing Technologies: The Promise of Audio-based Sensors",
    "authors": [
      "Chaeyeon Han",
      "Pavan Seshadri",
      "Yiwei Ding",
      "Noah Posner",
      "Bon Woo Koo",
      "Animesh Agrawal",
      "Alexander Lerch",
      "Subhrajit Guhathakurta"
    ],
    "abstract": "While various sensors have been deployed to monitor vehicular flows, sensing\npedestrian movement is still nascent. Yet walking is a significant mode of\ntravel in many cities, especially those in Europe, Africa, and Asia.\nUnderstanding pedestrian volumes and flows is essential for designing safer and\nmore attractive pedestrian infrastructure and for controlling periodic\novercrowding. This study discusses a new approach to scale up urban sensing of\npeople with the help of novel audio-based technology. It assesses the benefits\nand limitations of microphone-based sensors as compared to other forms of\npedestrian sensing. A large-scale dataset called ASPED is presented, which\nincludes high-quality audio recordings along with video recordings used for\nlabeling the pedestrian count data. The baseline analyses highlight the promise\nof using audio sensors for pedestrian tracking, although algorithmic and\ntechnological improvements to make the sensors practically usable continue.\nThis study also demonstrates how the data can be leveraged to predict\npedestrian trajectories. Finally, it discusses the use cases and scenarios\nwhere audio-based pedestrian sensing can support better urban and\ntransportation planning.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.LG",
      "cs.MM",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "submitted to Urban Informatics",
    "pdf_url": "http://arxiv.org/pdf/2406.09998v1",
    "published_date": "2024-06-14 13:15:18 UTC",
    "updated_date": "2024-06-14 13:15:18 UTC"
  },
  {
    "arxiv_id": "2406.09988v2",
    "title": "Details Make a Difference: Object State-Sensitive Neurorobotic Task Planning",
    "authors": [
      "Xiaowen Sun",
      "Xufeng Zhao",
      "Jae Hee Lee",
      "Wenhao Lu",
      "Matthias Kerzel",
      "Stefan Wermter"
    ],
    "abstract": "The state of an object reflects its current status or condition and is\nimportant for a robot's task planning and manipulation. However, detecting an\nobject's state and generating a state-sensitive plan for robots is challenging.\nRecently, pre-trained Large Language Models (LLMs) and Vision-Language Models\n(VLMs) have shown impressive capabilities in generating plans. However, to the\nbest of our knowledge, there is hardly any investigation on whether LLMs or\nVLMs can also generate object state-sensitive plans. To study this, we\nintroduce an Object State-Sensitive Agent (OSSA), a task-planning agent\nempowered by pre-trained neural networks. We propose two methods for OSSA: (i)\na modular model consisting of a pre-trained vision processing module (dense\ncaptioning model, DCM) and a natural language processing model (LLM), and (ii)\na monolithic model consisting only of a VLM. To quantitatively evaluate the\nperformances of the two methods, we use tabletop scenarios where the task is to\nclear the table. We contribute a multimodal benchmark dataset that takes object\nstates into consideration. Our results show that both methods can be used for\nobject state-sensitive tasks, but the monolithic approach outperforms the\nmodular approach. The code for OSSA is available at\nhttps://github.com/Xiao-wen-Sun/OSSA",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "ICANN24, Switzerland",
    "pdf_url": "http://arxiv.org/pdf/2406.09988v2",
    "published_date": "2024-06-14 12:52:42 UTC",
    "updated_date": "2024-10-16 14:48:38 UTC"
  },
  {
    "arxiv_id": "2406.09981v1",
    "title": "Challenges in explaining deep learning models for data with biological variation",
    "authors": [
      "Lenka Tƒõtkov√°",
      "Erik Schou Dreier",
      "Robin Malm",
      "Lars Kai Hansen"
    ],
    "abstract": "Much machine learning research progress is based on developing models and\nevaluating them on a benchmark dataset (e.g., ImageNet for images). However,\napplying such benchmark-successful methods to real-world data often does not\nwork as expected. This is particularly the case for biological data where we\nexpect variability at multiple time and spatial scales. In this work, we are\nusing grain data and the goal is to detect diseases and damages. Pink fusarium,\nskinned grains, and other diseases and damages are key factors in setting the\nprice of grains or excluding dangerous grains from food production. Apart from\nchallenges stemming from differences of the data from the standard toy\ndatasets, we also present challenges that need to be overcome when explaining\ndeep learning models. For example, explainability methods have many\nhyperparameters that can give different results, and the ones published in the\npapers do not work on dissimilar images. Other challenges are more general:\nproblems with visualization of the explanations and their comparison since the\nmagnitudes of their values differ from method to method. An open fundamental\nquestion also is: How to evaluate explanations? It is a non-trivial task\nbecause the \"ground truth\" is usually missing or ill-defined. Also, human\nannotators may create what they think is an explanation of the task at hand,\nyet the machine learning model might solve it in a different and perhaps\ncounter-intuitive way. We discuss several of these challenges and evaluate\nvarious post-hoc explainability methods on grain data. We focus on robustness,\nquality of explanations, and similarity to particular \"ground truth\"\nannotations made by experts. The goal is to find the methods that overall\nperform well and could be used in this challenging task. We hope the proposed\npipeline will be used as a framework for evaluating explainability methods in\nspecific use cases.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09981v1",
    "published_date": "2024-06-14 12:44:04 UTC",
    "updated_date": "2024-06-14 12:44:04 UTC"
  },
  {
    "arxiv_id": "2406.09979v2",
    "title": "HIRO: Hierarchical Information Retrieval Optimization",
    "authors": [
      "Krish Goel",
      "Mahek Chandak"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) has revolutionized natural language\nprocessing by dynamically integrating external knowledge into Large Language\nModels (LLMs), addressing their limitation of static training datasets. Recent\nimplementations of RAG leverage hierarchical data structures, which organize\ndocuments at various levels of summarization and information density. This\ncomplexity, however, can cause LLMs to \"choke\" on information overload,\nnecessitating more sophisticated querying mechanisms. In this context, we\nintroduce Hierarchical Information Retrieval Optimization (HIRO), a novel\nquerying approach that employs a Depth-First Search (DFS)-based recursive\nsimilarity score calculation and branch pruning. This method uniquely minimizes\nthe context delivered to the LLM without informational loss, effectively\nmanaging the challenge of excessive data. HIRO's refined approach is validated\nby a 10.85% improvement in performance on the NarrativeQA dataset.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09979v2",
    "published_date": "2024-06-14 12:41:07 UTC",
    "updated_date": "2024-09-04 12:33:24 UTC"
  },
  {
    "arxiv_id": "2406.09976v2",
    "title": "Robust Model-Based Reinforcement Learning with an Adversarial Auxiliary Model",
    "authors": [
      "Siemen Herremans",
      "Ali Anwar",
      "Siegfried Mercelis"
    ],
    "abstract": "Reinforcement learning has demonstrated impressive performance in various\nchallenging problems such as robotics, board games, and classical arcade games.\nHowever, its real-world applications can be hindered by the absence of\nrobustness and safety in the learned policies. More specifically, an RL agent\nthat trains in a certain Markov decision process (MDP) often struggles to\nperform well in nearly identical MDPs. To address this issue, we employ the\nframework of Robust MDPs (RMDPs) in a model-based setting and introduce a novel\nlearned transition model. Our method specifically incorporates an auxiliary\npessimistic model, updated adversarially, to estimate the worst-case MDP within\na Kullback-Leibler uncertainty set. In comparison to several existing works,\nour work does not impose any additional conditions on the training environment,\nsuch as the need for a parametric simulator. To test the effectiveness of the\nproposed pessimistic model in enhancing policy robustness, we integrate it into\na practical RL algorithm, called Robust Model-Based Policy Optimization\n(RMBPO). Our experimental results indicate a notable improvement in policy\nrobustness on high-dimensional MuJoCo control tasks, with the auxiliary model\nenhancing the performance of the learned policy in distorted MDPs. We further\nexplore the learned deviation between the proposed auxiliary world model and\nthe nominal model, to examine how pessimism is achieved. By learning a\npessimistic world model and demonstrating its role in improving policy\nrobustness, our research contributes towards making (model-based) RL more\nrobust.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Will be presented at the RL Safety Workshop at RLC 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.09976v2",
    "published_date": "2024-06-14 12:37:08 UTC",
    "updated_date": "2024-07-01 13:35:44 UTC"
  },
  {
    "arxiv_id": "2406.09966v1",
    "title": "Outlier detection in maritime environments using AIS data and deep recurrent architectures",
    "authors": [
      "Constantine Maganaris",
      "Eftychios Protopapadakis",
      "Nikolaos Doulamis"
    ],
    "abstract": "A methodology based on deep recurrent models for maritime surveillance, over\npublicly available Automatic Identification System (AIS) data, is presented in\nthis paper. The setup employs a deep Recurrent Neural Network (RNN)-based\nmodel, for encoding and reconstructing the observed ships' motion patterns. Our\napproach is based on a thresholding mechanism, over the calculated errors\nbetween observed and reconstructed motion patterns of maritime vessels.\nSpecifically, a deep-learning framework, i.e. an encoder-decoder architecture,\nis trained using the observed motion patterns, enabling the models to learn and\npredict the expected trajectory, which will be compared to the effective ones.\nOur models, particularly the bidirectional GRU with recurrent dropouts,\nshowcased superior performance in capturing the temporal dynamics of maritime\ndata, illustrating the potential of deep learning to enhance maritime\nsurveillance capabilities. Our work lays a solid foundation for future research\nin this domain, highlighting a path toward improved maritime safety through the\ninnovative application of technology.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "68T10"
    ],
    "primary_category": "cs.LG",
    "comment": "Presented in PETRA '24 The PErvasive Technologies Related to\n  Assistive Environments Conference June 26--28, 2024 Crete, Greece",
    "pdf_url": "http://arxiv.org/pdf/2406.09966v1",
    "published_date": "2024-06-14 12:15:15 UTC",
    "updated_date": "2024-06-14 12:15:15 UTC"
  },
  {
    "arxiv_id": "2406.09953v3",
    "title": "DAG-Plan: Generating Directed Acyclic Dependency Graphs for Dual-Arm Cooperative Planning",
    "authors": [
      "Zeyu Gao",
      "Yao Mu",
      "Jinye Qu",
      "Mengkang Hu",
      "Shijia Peng",
      "Chengkai Hou",
      "Lingyue Guo",
      "Ping Luo",
      "Shanghang Zhang",
      "Yanfeng Lu"
    ],
    "abstract": "Dual-arm robots offer enhanced versatility and efficiency over single-arm\ncounterparts by enabling concurrent manipulation of multiple objects or\ncooperative execution of tasks using both arms. However, the coordination of\ndual-arm systems for long-horizon tasks continues to pose significant\nchallenges, stemming from the intricate temporal and spatial dependencies among\nsub-tasks, necessitating intelligent decisions regarding the allocation of\nactions between arms and their optimal execution order. Existing task planning\nmethods predominantly focus on single-arm robots or rely on predefined bimanual\noperations to use large language models (LLMs) generate task sequence with\nlinear temporal dependency, failing to fully leverage the capabilities of\ndual-arm systems. To address this limitation, we introduce DAG-Plan, a\nstructured task planning framework tailored for dual-arm robots. DAG-Plan\nharnesses LLMs to decompose intricate tasks into actionable sub-tasks\nrepresented as nodes within a directed acyclic graph (DAG). Critically,\nDAG-Plan dynamically assigns these sub-tasks to the appropriate arm based on\nreal-time environmental observations, enabling parallel and adaptive execution.\nWe evaluate DAG-Plan on the Dual-Arm Kitchen Benchmark, comprising 5 sequential\ntasks with 44 sub-tasks. Extensive experiments demonstrate the superiority of\nDAG-Plan over directly using LLM to generate linear task sequence, achieving\n52.8% higher efficiency compared to the single-arm task planning and 48% higher\nsuccess rate of the dual-arm task planning. Compared to iterative methods,\nDAG-Plan improving execution efficiency 84.1% due to its fewer query time. More\ndemos and information are available on https://sites.google.com/view/dag-plan.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09953v3",
    "published_date": "2024-06-14 11:58:51 UTC",
    "updated_date": "2025-04-11 05:41:19 UTC"
  },
  {
    "arxiv_id": "2406.15305v1",
    "title": "PID: Prompt-Independent Data Protection Against Latent Diffusion Models",
    "authors": [
      "Ang Li",
      "Yichuan Mo",
      "Mingjie Li",
      "Yisen Wang"
    ],
    "abstract": "The few-shot fine-tuning of Latent Diffusion Models (LDMs) has enabled them\nto grasp new concepts from a limited number of images. However, given the vast\namount of personal images accessible online, this capability raises critical\nconcerns about civil privacy. While several previous defense methods have been\ndeveloped to prevent such misuse of LDMs, they typically assume that the\ntextual prompts used by data protectors exactly match those employed by data\nexploiters. In this paper, we first empirically demonstrate that breaking this\nassumption, i.e., in cases where discrepancies exist between the textual\nconditions used by protectors and exploiters, could substantially reduce the\neffectiveness of these defenses. Furthermore, considering the visual encoder's\nindependence from textual prompts, we delve into the visual encoder and\nthoroughly investigate how manipulating the visual encoder affects the few-shot\nfine-tuning process of LDMs. Drawing on these insights, we propose a simple yet\neffective method called \\textbf{Prompt-Independent Defense (PID)} to safeguard\nprivacy against LDMs. We show that PID can act as a strong privacy shield on\nits own while requiring significantly less computational power. We believe our\nstudies, along with the comprehensive understanding and new defense method,\nprovide a notable advance toward reliable data protection against LDMs.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "27 pages, ICML 2024 poster",
    "pdf_url": "http://arxiv.org/pdf/2406.15305v1",
    "published_date": "2024-06-14 11:56:42 UTC",
    "updated_date": "2024-06-14 11:56:42 UTC"
  },
  {
    "arxiv_id": "2407.09505v1",
    "title": "1-Lipschitz Neural Distance Fields",
    "authors": [
      "Guillaume Coiffier",
      "Louis Bethune"
    ],
    "abstract": "Neural implicit surfaces are a promising tool for geometry processing that\nrepresent a solid object as the zero level set of a neural network. Usually\ntrained to approximate a signed distance function of the considered object,\nthese methods exhibit great visual fidelity and quality near the surface, yet\ntheir properties tend to degrade with distance, making geometrical queries hard\nto perform without the help of complex range analysis techniques. Based on\nrecent advancements in Lipschitz neural networks, we introduce a new method for\napproximating the signed distance function of a given object. As our neural\nfunction is made 1- Lipschitz by construction, it cannot overestimate the\ndistance, which guarantees robustness even far from the surface. Moreover, the\n1-Lipschitz constraint allows us to use a different loss function, called the\nhinge-Kantorovitch-Rubinstein loss, which pushes the gradient as close to\nunit-norm as possible, thus reducing computation costs in iterative queries. As\nthis loss function only needs a rough estimate of occupancy to be optimized,\nthis means that the true distance function need not to be known. We are\ntherefore able to compute neural implicit representations of even bad quality\ngeometry such as noisy point clouds or triangle soups. We demonstrate that our\nmethods is able to approximate the distance function of any closed or open\nsurfaces or curves in the plane or in space, while still allowing sphere\ntracing or closest point projections to be performed robustly.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "17 pages, 19 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.09505v1",
    "published_date": "2024-06-14 11:56:36 UTC",
    "updated_date": "2024-06-14 11:56:36 UTC"
  },
  {
    "arxiv_id": "2406.09949v2",
    "title": "Neural Concept Binder",
    "authors": [
      "Wolfgang Stammer",
      "Antonia W√ºst",
      "David Steinmann",
      "Kristian Kersting"
    ],
    "abstract": "The challenge in object-based visual reasoning lies in generating concept\nrepresentations that are both descriptive and distinct. Achieving this in an\nunsupervised manner requires human users to understand the model's learned\nconcepts and, if necessary, revise incorrect ones. To address this challenge,\nwe introduce the Neural Concept Binder (NCB), a novel framework for deriving\nboth discrete and continuous concept representations, which we refer to as\n\"concept-slot encodings\". NCB employs two types of binding: \"soft binding\",\nwhich leverages the recent SysBinder mechanism to obtain object-factor\nencodings, and subsequent \"hard binding\", achieved through hierarchical\nclustering and retrieval-based inference. This enables obtaining expressive,\ndiscrete representations from unlabeled images. Moreover, the structured nature\nof NCB's concept representations allows for intuitive inspection and the\nstraightforward integration of external knowledge, such as human input or\ninsights from other AI models like GPT-4. Additionally, we demonstrate that\nincorporating the hard binding mechanism preserves model performance while\nenabling seamless integration into both neural and symbolic modules for complex\nreasoning tasks. We validate the effectiveness of NCB through evaluations on\nour newly introduced CLEVR-Sudoku dataset.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.SC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09949v2",
    "published_date": "2024-06-14 11:52:09 UTC",
    "updated_date": "2024-10-24 12:13:54 UTC"
  },
  {
    "arxiv_id": "2406.09940v1",
    "title": "Implementing engrams from a machine learning perspective: XOR as a basic motif",
    "authors": [
      "Jesus Marco de Lucas",
      "Maria Pe√±a Fernandez",
      "Lara Lloret Iglesias"
    ],
    "abstract": "We have previously presented the idea of how complex multimodal information\ncould be represented in our brains in a compressed form, following mechanisms\nsimilar to those employed in machine learning tools, like autoencoders. In this\nshort comment note we reflect, mainly with a didactical purpose, upon the basic\nquestion for a biological implementation: what could be the mechanism working\nas a loss function, and how it could be connected to a neuronal network\nproviding the required feedback to build a simple training configuration. We\npresent our initial ideas based on a basic motif that implements an XOR switch,\nusing few excitatory and inhibitory neurons. Such motif is guided by a\nprinciple of homeostasis, and it implements a loss function that could provide\nfeedback to other neuronal structures, establishing a control system. We\nanalyse the presence of this XOR motif in the connectome of C.Elegans, and\nindicate the relationship with the well-known lateral inhibition motif. We then\nexplore how to build a basic biological neuronal structure with learning\ncapacity integrating this XOR motif. Guided by the computational analogy, we\nshow an initial example that indicates the feasibility of this approach,\napplied to learning binary sequences, like it is the case for simple melodies.\nIn summary, we provide didactical examples exploring the parallelism between\nbiological and computational learning mechanisms, identifying basic motifs and\ntraining procedures, and how an engram encoding a melody could be built using a\nsimple recurrent network involving both excitatory and inhibitory neurons.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "q-bio.NC",
    "comment": "9 pages, short comment",
    "pdf_url": "http://arxiv.org/pdf/2406.09940v1",
    "published_date": "2024-06-14 11:36:49 UTC",
    "updated_date": "2024-06-14 11:36:49 UTC"
  },
  {
    "arxiv_id": "2406.09938v1",
    "title": "Experiments in News Bias Detection with Pre-Trained Neural Transformers",
    "authors": [
      "Tim Menzner",
      "Jochen L. Leidner"
    ],
    "abstract": "The World Wide Web provides unrivalled access to information globally,\nincluding factual news reporting and commentary. However, state actors and\ncommercial players increasingly spread biased (distorted) or fake (non-factual)\ninformation to promote their agendas. We compare several large, pre-trained\nlanguage models on the task of sentence-level news bias detection and sub-type\nclassification, providing quantitative and qualitative results.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09938v1",
    "published_date": "2024-06-14 11:34:36 UTC",
    "updated_date": "2024-06-14 11:34:36 UTC"
  },
  {
    "arxiv_id": "2406.09933v1",
    "title": "What Does it Take to Generalize SER Model Across Datasets? A Comprehensive Benchmark",
    "authors": [
      "Adham Ibrahim",
      "Shady Shehata",
      "Ajinkya Kulkarni",
      "Mukhtar Mohamed",
      "Muhammad Abdul-Mageed"
    ],
    "abstract": "Speech emotion recognition (SER) is essential for enhancing human-computer\ninteraction in speech-based applications. Despite improvements in specific\nemotional datasets, there is still a research gap in SER's capability to\ngeneralize across real-world situations. In this paper, we investigate\napproaches to generalize the SER system across different emotion datasets. In\nparticular, we incorporate 11 emotional speech datasets and illustrate a\ncomprehensive benchmark on the SER task. We also address the challenge of\nimbalanced data distribution using over-sampling methods when combining SER\ndatasets for training. Furthermore, we explore various evaluation protocols for\nadeptness in the generalization of SER. Building on this, we explore the\npotential of Whisper for SER, emphasizing the importance of thorough\nevaluation. Our approach is designed to advance SER technology by integrating\nspeaker-independent methods.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.SD",
    "comment": "ACCEPTED AT INTERSPEECH 2024, GREECE",
    "pdf_url": "http://arxiv.org/pdf/2406.09933v1",
    "published_date": "2024-06-14 11:27:19 UTC",
    "updated_date": "2024-06-14 11:27:19 UTC"
  },
  {
    "arxiv_id": "2406.09928v1",
    "title": "Personalized Speech Enhancement Without a Separate Speaker Embedding Model",
    "authors": [
      "Tanel P√§rnamaa",
      "Ando Saabas"
    ],
    "abstract": "Personalized speech enhancement (PSE) models can improve the audio quality of\nteleconferencing systems by adapting to the characteristics of a speaker's\nvoice. However, most existing methods require a separate speaker embedding\nmodel to extract a vector representation of the speaker from enrollment audio,\nwhich adds complexity to the training and deployment process. We propose to use\nthe internal representation of the PSE model itself as the speaker embedding,\nthereby avoiding the need for a separate model. We show that our approach\nperforms equally well or better than the standard method of using a pre-trained\nspeaker embedding model on noise suppression and echo cancellation tasks.\nMoreover, our approach surpasses the ICASSP 2023 Deep Noise Suppression\nChallenge winner by 0.15 in Mean Opinion Score.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted to Interspeech 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.09928v1",
    "published_date": "2024-06-14 11:16:46 UTC",
    "updated_date": "2024-06-14 11:16:46 UTC"
  },
  {
    "arxiv_id": "2406.09923v2",
    "title": "CliBench: A Multifaceted and Multigranular Evaluation of Large Language Models for Clinical Decision Making",
    "authors": [
      "Mingyu Derek Ma",
      "Chenchen Ye",
      "Yu Yan",
      "Xiaoxuan Wang",
      "Peipei Ping",
      "Timothy S Chang",
      "Wei Wang"
    ],
    "abstract": "The integration of Artificial Intelligence (AI), especially Large Language\nModels (LLMs), into the clinical diagnosis process offers significant potential\nto improve the efficiency and accessibility of medical care. While LLMs have\nshown some promise in the medical domain, their application in clinical\ndiagnosis remains underexplored, especially in real-world clinical practice,\nwhere highly sophisticated, patient-specific decisions need to be made. Current\nevaluations of LLMs in this field are often narrow in scope, focusing on\nspecific diseases or specialties and employing simplified diagnostic tasks. To\nbridge this gap, we introduce CliBench, a novel benchmark developed from the\nMIMIC IV dataset, offering a comprehensive and realistic assessment of LLMs'\ncapabilities in clinical diagnosis. This benchmark not only covers diagnoses\nfrom a diverse range of medical cases across various specialties but also\nincorporates tasks of clinical significance: treatment procedure\nidentification, lab test ordering and medication prescriptions. Supported by\nstructured output ontologies, CliBench enables a precise and multi-granular\nevaluation, offering an in-depth understanding of LLM's capability on diverse\nclinical tasks of desired granularity. We conduct a zero-shot evaluation of\nleading LLMs to assess their proficiency in clinical decision-making. Our\npreliminary results shed light on the potential and limitations of current LLMs\nin clinical settings, providing valuable insights for future advancements in\nLLM-powered healthcare.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Project page: https://clibench.github.io",
    "pdf_url": "http://arxiv.org/pdf/2406.09923v2",
    "published_date": "2024-06-14 11:10:17 UTC",
    "updated_date": "2024-10-11 20:53:05 UTC"
  },
  {
    "arxiv_id": "2406.09920v2",
    "title": "Knowledge Editing in Language Models via Adapted Direct Preference Optimization",
    "authors": [
      "Amit Rozner",
      "Barak Battash",
      "Lior Wolf",
      "Ofir Lindenbaum"
    ],
    "abstract": "Large Language Models (LLMs) can become outdated over time as they may lack\nupdated world knowledge, leading to factual knowledge errors and gaps.\nKnowledge Editing (KE) aims to overcome this challenge using weight updates\nthat do not require expensive retraining. We propose treating KE as an LLM\nalignment problem. Toward this goal, we introduce Knowledge Direct Preference\nOptimization (KDPO), a variation of the Direct Preference Optimization (DPO)\nthat is more effective for knowledge modifications. Our method is based on an\nonline approach that continually updates the knowledge stored in the model. We\nuse the current knowledge as a negative sample and the new knowledge we want to\nintroduce as a positive sample in a process called DPO. We also use\nteacher-forcing for negative sample generation and optimize using the positive\nsample, which helps maintain localized changes. We tested our KE method on\nvarious datasets and models, comparing it to several cutting-edge methods, with\n100 and 500 sequential edits. Additionally, we conducted an ablation study\ncomparing our method to the standard DPO approach. Our experimental results\nshow that our modified DPO method allows for more refined KE, achieving similar\nor better performance compared to previous methods.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.09920v2",
    "published_date": "2024-06-14 11:02:21 UTC",
    "updated_date": "2024-09-24 09:48:36 UTC"
  },
  {
    "arxiv_id": "2406.09899v2",
    "title": "Learning Solution-Aware Transformers for Efficiently Solving Quadratic Assignment Problem",
    "authors": [
      "Zhentao Tan",
      "Yadong Mu"
    ],
    "abstract": "Recently various optimization problems, such as Mixed Integer Linear\nProgramming Problems (MILPs), have undergone comprehensive investigation,\nleveraging the capabilities of machine learning. This work focuses on\nlearning-based solutions for efficiently solving the Quadratic Assignment\nProblem (QAPs), which stands as a formidable challenge in combinatorial\noptimization. While many instances of simpler problems admit fully\npolynomial-time approximate solution (FPTAS), QAP is shown to be strongly\nNP-hard. Even finding a FPTAS for QAP is difficult, in the sense that the\nexistence of a FPTAS implies $P = NP$. Current research on QAPs suffer from\nlimited scale and computational inefficiency. To attack the aforementioned\nissues, we here propose the first solution of its kind for QAP in the\nlearn-to-improve category. This work encodes facility and location nodes\nseparately, instead of forming computationally intensive association graphs\nprevalent in current approaches. This design choice enables scalability to\nlarger problem sizes. Furthermore, a \\textbf{S}olution \\textbf{AW}are\n\\textbf{T}ransformer (SAWT) architecture integrates the incumbent solution\nmatrix with the attention score to effectively capture higher-order information\nof the QAPs. Our model's effectiveness is validated through extensive\nexperiments on self-generated QAP instances of varying sizes and the QAPLIB\nbenchmark.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.09899v2",
    "published_date": "2024-06-14 10:15:03 UTC",
    "updated_date": "2024-06-20 01:58:50 UTC"
  },
  {
    "arxiv_id": "2406.09891v2",
    "title": "Benchmarking Generative Models on Computational Thinking Tests in Elementary Visual Programming",
    "authors": [
      "Victor-Alexandru PƒÉdurean",
      "Adish Singla"
    ],
    "abstract": "Generative models have demonstrated human-level proficiency in various\nbenchmarks across domains like programming, natural sciences, and general\nknowledge. Despite these promising results on competitive benchmarks, they\nstill struggle with seemingly simple problem-solving tasks typically carried\nout by elementary-level students. How do state-of-the-art models perform on\nstandardized programming-related tests designed to assess computational\nthinking and problem-solving skills at schools? In this paper, we curate a\nnovel benchmark involving computational thinking tests grounded in elementary\nvisual programming domains. Our initial results show that state-of-the-art\nmodels like GPT-4o and Llama3 barely match the performance of an average school\nstudent. To further boost the performance of these models, we fine-tune them\nusing a novel synthetic data generation methodology. The key idea is to develop\na comprehensive dataset using symbolic methods that capture different skill\nlevels, ranging from recognition of visual elements to multi-choice quizzes to\nsynthesis-style tasks. We showcase how various aspects of symbolic information\nin synthetic data help improve fine-tuned models' performance. We will release\nthe full implementation and datasets to facilitate further research on\nenhancing computational thinking in generative models.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09891v2",
    "published_date": "2024-06-14 10:02:52 UTC",
    "updated_date": "2025-03-18 13:03:15 UTC"
  },
  {
    "arxiv_id": "2406.09877v1",
    "title": "Federated Learning with Flexible Architectures",
    "authors": [
      "Jong-Ik Park",
      "Carlee Joe-Wong"
    ],
    "abstract": "Traditional federated learning (FL) methods have limited support for clients\nwith varying computational and communication abilities, leading to\ninefficiencies and potential inaccuracies in model training. This limitation\nhinders the widespread adoption of FL in diverse and resource-constrained\nenvironments, such as those with client devices ranging from powerful servers\nto mobile devices. To address this need, this paper introduces Federated\nLearning with Flexible Architectures (FedFA), an FL training algorithm that\nallows clients to train models of different widths and depths. Each client can\nselect a network architecture suitable for its resources, with shallower and\nthinner networks requiring fewer computing resources for training. Unlike prior\nwork in this area, FedFA incorporates the layer grafting technique to align\nclients' local architectures with the largest network architecture in the FL\nsystem during model aggregation. Layer grafting ensures that all client\ncontributions are uniformly integrated into the global model, thereby\nminimizing the risk of any individual client's data skewing the model's\nparameters disproportionately and introducing security benefits. Moreover,\nFedFA introduces the scalable aggregation method to manage scale variations in\nweights among different network architectures. Experimentally, FedFA\noutperforms previous width and depth flexible aggregation strategies.\nFurthermore, FedFA demonstrates increased robustness against performance\ndegradation in backdoor attack scenarios compared to earlier strategies.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09877v1",
    "published_date": "2024-06-14 09:44:46 UTC",
    "updated_date": "2024-06-14 09:44:46 UTC"
  },
  {
    "arxiv_id": "2406.09873v1",
    "title": "Perceiver-Prompt: Flexible Speaker Adaptation in Whisper for Chinese Disordered Speech Recognition",
    "authors": [
      "Yicong Jiang",
      "Tianzi Wang",
      "Xurong Xie",
      "Juan Liu",
      "Wei Sun",
      "Nan Yan",
      "Hui Chen",
      "Lan Wang",
      "Xunying Liu",
      "Feng Tian"
    ],
    "abstract": "Disordered speech recognition profound implications for improving the quality\nof life for individuals afflicted with, for example, dysarthria. Dysarthric\nspeech recognition encounters challenges including limited data, substantial\ndissimilarities between dysarthric and non-dysarthric speakers, and significant\nspeaker variations stemming from the disorder. This paper introduces\nPerceiver-Prompt, a method for speaker adaptation that utilizes P-Tuning on the\nWhisper large-scale model. We first fine-tune Whisper using LoRA and then\nintegrate a trainable Perceiver to generate fixed-length speaker prompts from\nvariable-length inputs, to improve model recognition of Chinese dysarthric\nspeech. Experimental results from our Chinese dysarthric speech dataset\ndemonstrate consistent improvements in recognition performance with\nPerceiver-Prompt. Relative reduction up to 13.04% in CER is obtained over the\nfine-tuned Whisper.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted by interspeech 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.09873v1",
    "published_date": "2024-06-14 09:36:46 UTC",
    "updated_date": "2024-06-14 09:36:46 UTC"
  },
  {
    "arxiv_id": "2406.09870v3",
    "title": "IGL-Bench: Establishing the Comprehensive Benchmark for Imbalanced Graph Learning",
    "authors": [
      "Jiawen Qin",
      "Haonan Yuan",
      "Qingyun Sun",
      "Lyujin Xu",
      "Jiaqi Yuan",
      "Pengfeng Huang",
      "Zhaonan Wang",
      "Xingcheng Fu",
      "Hao Peng",
      "Jianxin Li",
      "Philip S. Yu"
    ],
    "abstract": "Deep graph learning has gained grand popularity over the past years due to\nits versatility and success in representing graph data across a wide range of\ndomains. However, the pervasive issue of imbalanced graph data distributions,\nwhere certain parts exhibit disproportionally abundant data while others remain\nsparse, undermines the efficacy of conventional graph learning algorithms,\nleading to biased outcomes. To address this challenge, Imbalanced Graph\nLearning (IGL) has garnered substantial attention, enabling more balanced data\ndistributions and better task performance. Despite the proliferation of IGL\nalgorithms, the absence of consistent experimental protocols and fair\nperformance comparisons pose a significant barrier to comprehending\nadvancements in this field. To bridge this gap, we introduce IGL-Bench, a\nfoundational comprehensive benchmark for imbalanced graph learning, embarking\non 16 diverse graph datasets and 24 distinct IGL algorithms with uniform data\nprocessing and splitting strategies. Specifically, IGL-Bench systematically\ninvestigates state-of-the-art IGL algorithms in terms of effectiveness,\nrobustness, and efficiency on node-level and graph-level tasks, with the scope\nof class-imbalance and topology-imbalance. Extensive experiments demonstrate\nthe potential benefits of IGL algorithms on various imbalanced conditions,\noffering insights and opportunities in the IGL field. Further, we have\ndeveloped an open-sourced and unified package to facilitate reproducible\nevaluation and inspire further innovative research, which is available at\nhttps://github.com/RingBDStack/IGL-Bench.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "The Thirteenth International Conference on Learning Representations\n  (ICLR'25)",
    "pdf_url": "http://arxiv.org/pdf/2406.09870v3",
    "published_date": "2024-06-14 09:30:18 UTC",
    "updated_date": "2025-03-01 14:35:37 UTC"
  },
  {
    "arxiv_id": "2406.09864v2",
    "title": "LUMA: A Benchmark Dataset for Learning from Uncertain and Multimodal Data",
    "authors": [
      "Grigor Bezirganyan",
      "Sana Sellami",
      "Laure Berti-√âquille",
      "S√©bastien Fournier"
    ],
    "abstract": "Multimodal Deep Learning enhances decision-making by integrating diverse\ninformation sources, such as texts, images, audio, and videos. To develop\ntrustworthy multimodal approaches, it is essential to understand how\nuncertainty impacts these models. We propose LUMA, a unique benchmark dataset,\nfeaturing audio, image, and textual data from 50 classes, for learning from\nuncertain and multimodal data. It extends the well-known CIFAR 10/100 dataset\nwith audio samples extracted from three audio corpora, and text data generated\nusing the Gemma-7B Large Language Model (LLM). The LUMA dataset enables the\ncontrolled injection of varying types and degrees of uncertainty to achieve and\ntailor specific experiments and benchmarking initiatives. LUMA is also\navailable as a Python package including the functions for generating multiple\nvariants of the dataset with controlling the diversity of the data, the amount\nof noise for each modality, and adding out-of-distribution samples. A baseline\npre-trained model is also provided alongside three uncertainty quantification\nmethods: Monte-Carlo Dropout, Deep Ensemble, and Reliable Conflictive\nMulti-View Learning. This comprehensive dataset and its benchmarking tools are\nintended to promote and support the development, evaluation, and benchmarking\nof trustworthy and robust multimodal deep learning approaches. We anticipate\nthat the LUMA dataset will help the ICLR community to design more trustworthy\nand robust machine learning approaches for safety critical applications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09864v2",
    "published_date": "2024-06-14 09:22:07 UTC",
    "updated_date": "2024-10-01 13:07:02 UTC"
  },
  {
    "arxiv_id": "2406.09860v1",
    "title": "Dataset Condensation with Latent Quantile Matching",
    "authors": [
      "Wei Wei",
      "Tom De Schepper",
      "Kevin Mets"
    ],
    "abstract": "Dataset condensation (DC) methods aim to learn a smaller synthesized dataset\nwith informative data records to accelerate the training of machine learning\nmodels. Current distribution matching (DM) based DC methods learn a synthesized\ndataset by matching the mean of the latent embeddings between the synthetic and\nthe real dataset. However two distributions with the same mean can still be\nvastly different. In this work we demonstrate the shortcomings of using Maximum\nMean Discrepancy to match latent distributions i.e. the weak matching power and\nlack of outlier regularization. To alleviate these shortcomings we propose our\nnew method: Latent Quantile Matching (LQM) which matches the quantiles of the\nlatent embeddings to minimize the goodness of fit test statistic between two\ndistributions. Empirical experiments on both image and graph-structured\ndatasets show that LQM matches or outperforms previous state of the art in\ndistribution matching based DC. Moreover we show that LQM improves the\nperformance in continual graph learning (CGL) setting where memory efficiency\nand privacy can be important. Our work sheds light on the application of DM\nbased DC for CGL.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by CVPR Workshop 2024: 1st Workshop on Dataset Distillation\n  for Computer Vision",
    "pdf_url": "http://arxiv.org/pdf/2406.09860v1",
    "published_date": "2024-06-14 09:20:44 UTC",
    "updated_date": "2024-06-14 09:20:44 UTC"
  },
  {
    "arxiv_id": "2406.09838v1",
    "title": "Vision-Language Models Meet Meteorology: Developing Models for Extreme Weather Events Detection with Heatmaps",
    "authors": [
      "Jian Chen",
      "Peilin Zhou",
      "Yining Hua",
      "Dading Chong",
      "Meng Cao",
      "Yaowei Li",
      "Zixuan Yuan",
      "Bing Zhu",
      "Junwei Liang"
    ],
    "abstract": "Real-time detection and prediction of extreme weather protect human lives and\ninfrastructure. Traditional methods rely on numerical threshold setting and\nmanual interpretation of weather heatmaps with Geographic Information Systems\n(GIS), which can be slow and error-prone. Our research redefines Extreme\nWeather Events Detection (EWED) by framing it as a Visual Question Answering\n(VQA) problem, thereby introducing a more precise and automated solution.\nLeveraging Vision-Language Models (VLM) to simultaneously process visual and\ntextual data, we offer an effective aid to enhance the analysis process of\nweather heatmaps. Our initial assessment of general-purpose VLMs (e.g.,\nGPT-4-Vision) on EWED revealed poor performance, characterized by low accuracy\nand frequent hallucinations due to inadequate color differentiation and\ninsufficient meteorological knowledge. To address these challenges, we\nintroduce ClimateIQA, the first meteorological VQA dataset, which includes\n8,760 wind gust heatmaps and 254,040 question-answer pairs covering four\nquestion types, both generated from the latest climate reanalysis data. We also\npropose Sparse Position and Outline Tracking (SPOT), an innovative technique\nthat leverages OpenCV and K-Means clustering to capture and depict color\ncontours in heatmaps, providing ClimateIQA with more accurate color spatial\nlocation information. Finally, we present Climate-Zoo, the first meteorological\nVLM collection, which adapts VLMs to meteorological applications using the\nClimateIQA dataset. Experiment results demonstrate that models from Climate-Zoo\nsubstantially outperform state-of-the-art general VLMs, achieving an accuracy\nincrease from 0% to over 90% in EWED verification. The datasets and models in\nthis study are publicly available for future climate science research:\nhttps://github.com/AlexJJJChen/Climate-Zoo.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09838v1",
    "published_date": "2024-06-14 08:46:44 UTC",
    "updated_date": "2024-06-14 08:46:44 UTC"
  },
  {
    "arxiv_id": "2406.09833v3",
    "title": "SHMamba: Structured Hyperbolic State Space Model for Audio-Visual Question Answering",
    "authors": [
      "Zhe Yang",
      "Wenrui Li",
      "Guanghui Cheng"
    ],
    "abstract": "The Audio-Visual Question Answering (AVQA) task holds significant potential\nfor applications. Compared to traditional unimodal approaches, the multi-modal\ninput of AVQA makes feature extraction and fusion processes more challenging.\nEuclidean space is difficult to effectively represent multi-dimensional\nrelationships of data. Especially when extracting and processing data with a\ntree structure or hierarchical structure, Euclidean space is not suitable as an\nembedding space. Additionally, the self-attention mechanism in Transformers is\neffective in capturing the dynamic relationships between elements in a\nsequence. However, the self-attention mechanism's limitations in window\nmodeling and quadratic computational complexity reduce its effectiveness in\nmodeling long sequences. To address these limitations, we propose SHMamba:\nStructured Hyperbolic State Space Model to integrate the advantages of\nhyperbolic geometry and state space models. Specifically, SHMamba leverages the\nintrinsic properties of hyperbolic space to represent hierarchical structures\nand complex relationships in audio-visual data. Meanwhile, the state space\nmodel captures dynamic changes over time by globally modeling the entire\nsequence. Furthermore, we introduce an adaptive curvature hyperbolic alignment\nmodule and a cross fusion block to enhance the understanding of hierarchical\nstructures and the dynamic exchange of cross-modal information, respectively.\nExtensive experiments demonstrate that SHMamba outperforms previous methods\nwith fewer parameters and computational costs. Our learnable parameters are\nreduced by 78.12\\%, while the average performance improves by 2.53\\%.\nExperiments show that our method demonstrates superiority among all current\nmajor methods and is more suitable for practical application scenarios.",
    "categories": [
      "cs.AI",
      "cs.MM",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09833v3",
    "published_date": "2024-06-14 08:43:31 UTC",
    "updated_date": "2024-07-16 08:09:56 UTC"
  },
  {
    "arxiv_id": "2406.09831v2",
    "title": "Recent Advances in Federated Learning Driven Large Language Models: A Survey on Architecture, Performance, and Security",
    "authors": [
      "Youyang Qu",
      "Ming Liu",
      "Tianqing Zhu",
      "Longxiang Gao",
      "Shui Yu",
      "Wanlei Zhou"
    ],
    "abstract": "Federated Learning (FL) offers a promising paradigm for training Large\nLanguage Models (LLMs) in a decentralized manner while preserving data privacy\nand minimizing communication overhead. This survey examines recent advancements\nin FL-driven LLMs, with a particular emphasis on architectural designs,\nperformance optimization, and security concerns, including the emerging area of\nmachine unlearning. In this context, machine unlearning refers to the\nsystematic removal of specific data contributions from trained models to comply\nwith privacy regulations such as the Right to be Forgotten. We review a range\nof strategies enabling unlearning in federated LLMs, including\nperturbation-based methods, model decomposition, and incremental retraining,\nwhile evaluating their trade-offs in terms of efficiency, privacy guarantees,\nand model utility. Through selected case studies and empirical evaluations, we\nanalyze how these methods perform in practical FL scenarios. This survey\nidentifies critical research directions toward developing secure, adaptable,\nand high-performing federated LLM systems for real-world deployment.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09831v2",
    "published_date": "2024-06-14 08:40:58 UTC",
    "updated_date": "2025-05-09 03:11:55 UTC"
  },
  {
    "arxiv_id": "2406.09825v2",
    "title": "Unraveling Anomalies in Time: Unsupervised Discovery and Isolation of Anomalous Behavior in Bio-regenerative Life Support System Telemetry",
    "authors": [
      "Ferdinand Rewicki",
      "Jakob Gawlikowski",
      "Julia Niebling",
      "Joachim Denzler"
    ],
    "abstract": "The detection of abnormal or critical system states is essential in condition\nmonitoring. While much attention is given to promptly identifying anomalies, a\nretrospective analysis of these anomalies can significantly enhance our\ncomprehension of the underlying causes of observed undesired behavior. This\naspect becomes particularly critical when the monitored system is deployed in a\nvital environment. In this study, we delve into anomalies within the domain of\nBio-Regenerative Life Support Systems (BLSS) for space exploration and analyze\nanomalies found in telemetry data stemming from the EDEN ISS space greenhouse\nin Antarctica. We employ time series clustering on anomaly detection results to\ncategorize various types of anomalies in both uni- and multivariate settings.\nWe then assess the effectiveness of these methods in identifying systematic\nanomalous behavior. Additionally, we illustrate that the anomaly detection\nmethods MDI and DAMP produce complementary results, as previously indicated by\nresearch.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages, + Supplemental Materials, Published at Machine Learning and\n  Knowledge Discovery in Databases. Applied Data Science Track. ECML PKDD 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.09825v2",
    "published_date": "2024-06-14 08:29:34 UTC",
    "updated_date": "2024-09-26 15:21:31 UTC"
  },
  {
    "arxiv_id": "2406.09823v2",
    "title": "From Manifestations to Cognitive Architectures: a Scalable Framework",
    "authors": [
      "Alfredo Ibias",
      "Guillem Ramirez-Miranda",
      "Enric Guinovart",
      "Eduard Alarcon"
    ],
    "abstract": "The Artificial Intelligence field is flooded with optimisation methods. In\nthis paper, we change the focus to developing modelling methods with the aim of\ngetting us closer to Artificial General Intelligence. To do so, we propose a\nnovel way to interpret reality as an information source, that is later\ntranslated into a computational framework able to capture and represent such\ninformation. This framework is able to build elements of classical cognitive\narchitectures, like Long Term Memory and Working Memory, starting from a simple\nprimitive that only processes Spatial Distributed Representations. Moreover, it\nachieves such level of verticality in a seamless scalable hierarchical way.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09823v2",
    "published_date": "2024-06-14 08:26:26 UTC",
    "updated_date": "2024-09-30 09:05:38 UTC"
  },
  {
    "arxiv_id": "2406.15465v1",
    "title": "RadEx: A Framework for Structured Information Extraction from Radiology Reports based on Large Language Models",
    "authors": [
      "Daniel Reichenpfader",
      "Jonas Knupp",
      "Andr√© Sander",
      "Kerstin Denecke"
    ],
    "abstract": "Annually and globally, over three billion radiography examinations and\ncomputer tomography scans result in mostly unstructured radiology reports\ncontaining free text. Despite the potential benefits of structured reporting,\nits adoption is limited by factors such as established processes, resource\nconstraints and potential loss of information. However, structured information\nwould be necessary for various use cases, including automatic analysis,\nclinical trial matching, and prediction of health outcomes. This study\nintroduces RadEx, an end-to-end framework comprising 15 software components and\nten artifacts to develop systems that perform automated information extraction\nfrom radiology reports. It covers the complete process from annotating training\ndata to extracting information by offering a consistent generic information\nmodel and setting boundaries for model development. Specifically, RadEx allows\nclinicians to define relevant information for clinical domains (e.g.,\nmammography) and to create report templates. The framework supports both\ngenerative and encoder-only models and the decoupling of information extraction\nfrom template filling enables independent model improvements. Developing\ninformation extraction systems according to the RadEx framework facilitates\nimplementation and maintenance as components are easily exchangeable, while\nstandardized artifacts ensure interoperability between components.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "J.3"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15465v1",
    "published_date": "2024-06-14 08:17:44 UTC",
    "updated_date": "2024-06-14 08:17:44 UTC"
  },
  {
    "arxiv_id": "2406.09815v1",
    "title": "Retrieval Augmented Fact Verification by Synthesizing Contrastive Arguments",
    "authors": [
      "Zhenrui Yue",
      "Huimin Zeng",
      "Lanyu Shang",
      "Yifan Liu",
      "Yang Zhang",
      "Dong Wang"
    ],
    "abstract": "The rapid propagation of misinformation poses substantial risks to public\ninterest. To combat misinformation, large language models (LLMs) are adapted to\nautomatically verify claim credibility. Nevertheless, existing methods heavily\nrely on the embedded knowledge within LLMs and / or black-box APIs for evidence\ncollection, leading to subpar performance with smaller LLMs or upon unreliable\ncontext. In this paper, we propose retrieval augmented fact verification\nthrough the synthesis of contrasting arguments (RAFTS). Upon input claims,\nRAFTS starts with evidence retrieval, where we design a retrieval pipeline to\ncollect and re-rank relevant documents from verifiable sources. Then, RAFTS\nforms contrastive arguments (i.e., supporting or refuting) conditioned on the\nretrieved evidence. In addition, RAFTS leverages an embedding model to identify\ninformative demonstrations, followed by in-context prompting to generate the\nprediction and explanation. Our method effectively retrieves relevant documents\nas evidence and evaluates arguments from varying perspectives, incorporating\nnuanced information for fine-grained decision-making. Combined with informative\nin-context examples as prior, RAFTS achieves significant improvements to\nsupervised and LLM baselines without complex prompts. We demonstrate the\neffectiveness of our method through extensive experiments, where RAFTS can\noutperform GPT-based methods with a significantly smaller 7B LLM.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.09815v1",
    "published_date": "2024-06-14 08:13:34 UTC",
    "updated_date": "2024-06-14 08:13:34 UTC"
  },
  {
    "arxiv_id": "2406.11891v1",
    "title": "Towards Adaptive Neighborhood for Advancing Temporal Interaction Graph Modeling",
    "authors": [
      "Siwei Zhang",
      "Xi Chen",
      "Yun Xiong",
      "Xixi Wu",
      "Yao Zhang",
      "Yongrui Fu",
      "Yinglong Zhao",
      "Jiawei Zhang"
    ],
    "abstract": "Temporal Graph Networks (TGNs) have demonstrated their remarkable performance\nin modeling temporal interaction graphs. These works can generate temporal node\nrepresentations by encoding the surrounding neighborhoods for the target node.\nHowever, an inherent limitation of existing TGNs is their reliance on fixed,\nhand-crafted rules for neighborhood encoding, overlooking the necessity for an\nadaptive and learnable neighborhood that can accommodate both personalization\nand temporal evolution across different timestamps. In this paper, we aim to\nenhance existing TGNs by introducing an adaptive neighborhood encoding\nmechanism. We present SEAN, a flexible plug-and-play model that can be\nseamlessly integrated with existing TGNs, effectively boosting their\nperformance. To achieve this, we decompose the adaptive neighborhood encoding\nprocess into two phases: (i) representative neighbor selection, and (ii)\ntemporal-aware neighborhood information aggregation. Specifically, we propose\nthe Representative Neighbor Selector component, which automatically pinpoints\nthe most important neighbors for the target node. It offers a tailored\nunderstanding of each node's unique surrounding context, facilitating\npersonalization. Subsequently, we propose a Temporal-aware Aggregator, which\nsynthesizes neighborhood aggregation by selectively determining the utilization\nof aggregation routes and decaying the outdated information, allowing our model\nto adaptively leverage both the contextually significant and current\ninformation during aggregation. We conduct extensive experiments by integrating\nSEAN into three representative TGNs, evaluating their performance on four\npublic datasets and one financial benchmark dataset introduced in this paper.\nThe results demonstrate that SEAN consistently leads to performance\nimprovements across all models, achieving SOTA performance and exceptional\nrobustness.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SI",
    "comment": "KDD'2024 Research Track Paper",
    "pdf_url": "http://arxiv.org/pdf/2406.11891v1",
    "published_date": "2024-06-14 07:57:17 UTC",
    "updated_date": "2024-06-14 07:57:17 UTC"
  },
  {
    "arxiv_id": "2407.09502v1",
    "title": "From Text to Life: On the Reciprocal Relationship between Artificial Life and Large Language Models",
    "authors": [
      "Eleni Nisioti",
      "Claire Glanois",
      "Elias Najarro",
      "Andrew Dai",
      "Elliot Meyerson",
      "Joachim Winther Pedersen",
      "Laetitia Teodorescu",
      "Conor F. Hayes",
      "Shyam Sudhakaran",
      "Sebastian Risi"
    ],
    "abstract": "Large Language Models (LLMs) have taken the field of AI by storm, but their\nadoption in the field of Artificial Life (ALife) has been, so far, relatively\nreserved. In this work we investigate the potential synergies between LLMs and\nALife, drawing on a large body of research in the two fields. We explore the\npotential of LLMs as tools for ALife research, for example, as operators for\nevolutionary computation or the generation of open-ended environments.\nReciprocally, principles of ALife, such as self-organization, collective\nintelligence and evolvability can provide an opportunity for shaping the\ndevelopment and functionalities of LLMs, leading to more adaptive and\nresponsive models. By investigating this dynamic interplay, the paper aims to\ninspire innovative crossover approaches for both ALife and LLM research. Along\nthe way, we examine the extent to which LLMs appear to increasingly exhibit\nproperties such as emergence or collective intelligence, expanding beyond their\noriginal goal of generating text, and potentially redefining our perception of\nlifelike intelligence in artificial systems.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09502v1",
    "published_date": "2024-06-14 07:45:32 UTC",
    "updated_date": "2024-06-14 07:45:32 UTC"
  },
  {
    "arxiv_id": "2406.09787v1",
    "title": "Evolving Self-Assembling Neural Networks: From Spontaneous Activity to Experience-Dependent Learning",
    "authors": [
      "Erwan Plantec",
      "Joachin W. Pedersen",
      "Milton L. Montero",
      "Eleni Nisioti",
      "Sebastian Risi"
    ],
    "abstract": "Biological neural networks are characterized by their high degree of\nplasticity, a core property that enables the remarkable adaptability of natural\norganisms. Importantly, this ability affects both the synaptic strength and the\ntopology of the nervous systems. Artificial neural networks, on the other hand,\nhave been mainly designed as static, fully connected structures that can be\nnotoriously brittle in the face of changing environments and novel inputs.\nBuilding on previous works on Neural Developmental Programs (NDPs), we propose\na class of self-organizing neural networks capable of synaptic and structural\nplasticity in an activity and reward-dependent manner which we call Lifelong\nNeural Developmental Program (LNDP). We present an instance of such a network\nbuilt on the graph transformer architecture and propose a mechanism for\npre-experience plasticity based on the spontaneous activity of sensory neurons.\nOur results demonstrate the ability of the model to learn from experiences in\ndifferent control tasks starting from randomly connected or empty networks. We\nfurther show that structural plasticity is advantageous in environments\nnecessitating fast adaptation or with non-stationary rewards.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "10 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.09787v1",
    "published_date": "2024-06-14 07:36:21 UTC",
    "updated_date": "2024-06-14 07:36:21 UTC"
  },
  {
    "arxiv_id": "2406.09779v1",
    "title": "OSPC: Detecting Harmful Memes with Large Language Model as a Catalyst",
    "authors": [
      "Jingtao Cao",
      "Zheng Zhang",
      "Hongru Wang",
      "Bin Liang",
      "Hao Wang",
      "Kam-Fai Wong"
    ],
    "abstract": "Memes, which rapidly disseminate personal opinions and positions across the\ninternet, also pose significant challenges in propagating social bias and\nprejudice. This study presents a novel approach to detecting harmful memes,\nparticularly within the multicultural and multilingual context of Singapore.\nOur methodology integrates image captioning, Optical Character Recognition\n(OCR), and Large Language Model (LLM) analysis to comprehensively understand\nand classify harmful memes. Utilizing the BLIP model for image captioning,\nPP-OCR and TrOCR for text recognition across multiple languages, and the Qwen\nLLM for nuanced language understanding, our system is capable of identifying\nharmful content in memes created in English, Chinese, Malay, and Tamil. To\nenhance the system's performance, we fine-tuned our approach by leveraging\nadditional data labeled using GPT-4V, aiming to distill the understanding\ncapability of GPT-4V for harmful memes to our system. Our framework achieves\ntop-1 at the public leaderboard of the Online Safety Prize Challenge hosted by\nAI Singapore, with the AUROC as 0.7749 and accuracy as 0.7087, significantly\nahead of the other teams. Notably, our approach outperforms previous\nbenchmarks, with FLAVA achieving an AUROC of 0.5695 and VisualBERT an AUROC of\n0.5561.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09779v1",
    "published_date": "2024-06-14 07:28:02 UTC",
    "updated_date": "2024-06-14 07:28:02 UTC"
  },
  {
    "arxiv_id": "2406.09773v1",
    "title": "Research on Edge Detection of LiDAR Images Based on Artificial Intelligence Technology",
    "authors": [
      "Haowei Yang",
      "Liyang Wang",
      "Jingyu Zhang",
      "Yu Cheng",
      "Ao Xiang"
    ],
    "abstract": "With the widespread application of Light Detection and Ranging (LiDAR)\ntechnology in fields such as autonomous driving, robot navigation, and terrain\nmapping, the importance of edge detection in LiDAR images has become\nincreasingly prominent. Traditional edge detection methods often face\nchallenges in accuracy and computational complexity when processing LiDAR\nimages. To address these issues, this study proposes an edge detection method\nfor LiDAR images based on artificial intelligence technology. This paper first\nreviews the current state of research on LiDAR technology and image edge\ndetection, introducing common edge detection algorithms and their applications\nin LiDAR image processing. Subsequently, a deep learning-based edge detection\nmodel is designed and implemented, optimizing the model training process\nthrough preprocessing and enhancement of the LiDAR image dataset. Experimental\nresults indicate that the proposed method outperforms traditional methods in\nterms of detection accuracy and computational efficiency, showing significant\npractical application value. Finally, improvement strategies are proposed for\nthe current method's shortcomings, and the improvements are validated through\nexperiments.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09773v1",
    "published_date": "2024-06-14 07:18:54 UTC",
    "updated_date": "2024-06-14 07:18:54 UTC"
  },
  {
    "arxiv_id": "2406.09770v1",
    "title": "Towards Efficient Pareto Set Approximation via Mixture of Experts Based Model Fusion",
    "authors": [
      "Anke Tang",
      "Li Shen",
      "Yong Luo",
      "Shiwei Liu",
      "Han Hu",
      "Bo Du"
    ],
    "abstract": "Solving multi-objective optimization problems for large deep neural networks\nis a challenging task due to the complexity of the loss landscape and the\nexpensive computational cost of training and evaluating models. Efficient\nPareto front approximation of large models enables multi-objective optimization\nfor various tasks such as multi-task learning and trade-off analysis. Existing\nalgorithms for learning Pareto set, including (1) evolutionary, hypernetworks,\nand hypervolume-maximization methods, are computationally expensive and have\nrestricted scalability to large models; (2) Scalarization algorithms, where a\nseparate model is trained for each objective ray, which is inefficient for\nlearning the entire Pareto set and fails to capture the objective trade-offs\neffectively. Inspired by the recent success of model merging, we propose a\npractical and scalable approach to Pareto set learning problem via mixture of\nexperts (MoE) based model fusion. By ensembling the weights of specialized\nsingle-task models, the MoE module can effectively capture the trade-offs\nbetween multiple objectives and closely approximate the entire Pareto set of\nlarge neural networks. Once the routers are learned and a preference vector is\nset, the MoE module can be unloaded, thus no additional computational cost is\nintroduced during inference. We conduct extensive experiments on vision and\nlanguage tasks using large-scale models such as CLIP-ViT and GPT-2. The\nexperimental results demonstrate that our method efficiently approximates the\nentire Pareto front of large models. Using only hundreds of trainable\nparameters of the MoE routers, our method even has lower memory usage compared\nto linear scalarization and algorithms that learn a single Pareto optimal\nsolution, and are scalable to both the number of objectives and the size of the\nmodel.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "code is available at https://github.com/tanganke/pareto_set_learning",
    "pdf_url": "http://arxiv.org/pdf/2406.09770v1",
    "published_date": "2024-06-14 07:16:18 UTC",
    "updated_date": "2024-06-14 07:16:18 UTC"
  },
  {
    "arxiv_id": "2406.09768v1",
    "title": "Bayesian Conditioned Diffusion Models for Inverse Problems",
    "authors": [
      "Alper G√ºng√∂r",
      "Bahri Batuhan Bilecen",
      "Tolga √áukur"
    ],
    "abstract": "Diffusion models have recently been shown to excel in many image\nreconstruction tasks that involve inverse problems based on a forward\nmeasurement operator. A common framework uses task-agnostic unconditional\nmodels that are later post-conditioned for reconstruction, an approach that\ntypically suffers from suboptimal task performance. While task-specific\nconditional models have also been proposed, current methods heuristically\ninject measured data as a naive input channel that elicits sampling\ninaccuracies. Here, we address the optimal conditioning of diffusion models for\nsolving challenging inverse problems that arise during image reconstruction.\nSpecifically, we propose a novel Bayesian conditioning technique for diffusion\nmodels, BCDM, based on score-functions associated with the conditional\ndistribution of desired images given measured data. We rigorously derive the\ntheory to express and train the conditional score-function. Finally, we show\nstate-of-the-art performance in image dealiasing, deblurring, super-resolution,\nand inpainting with the proposed technique.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "17 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.09768v1",
    "published_date": "2024-06-14 07:13:03 UTC",
    "updated_date": "2024-06-14 07:13:03 UTC"
  },
  {
    "arxiv_id": "2407.04717v1",
    "title": "Classical and Quantum Physical Reservoir Computing for Onboard Artificial Intelligence Systems: A Perspective",
    "authors": [
      "A. H. Abbas",
      "Hend Abdel-Ghani",
      "Ivan S. Maksymov"
    ],
    "abstract": "Artificial intelligence (AI) systems of autonomous systems such as drones,\nrobots and self-driving cars may consume up to 50% of total power available\nonboard, thereby limiting the vehicle's range of functions and considerably\nreducing the distance the vehicle can travel on a single charge.\nNext-generation onboard AI systems need an even higher power since they collect\nand process even larger amounts of data in real time. This problem cannot be\nsolved using the traditional computing devices since they become more and more\npower-consuming. In this review article, we discuss the perspectives of\ndevelopment of onboard neuromorphic computers that mimic the operation of a\nbiological brain using nonlinear-dynamical properties of natural physical\nenvironments surrounding autonomous vehicles. Previous research also\ndemonstrated that quantum neuromorphic processors (QNPs) can conduct\ncomputations with the efficiency of a standard computer while consuming less\nthan 1% of the onboard battery power. Since QNPs is a semi-classical\ntechnology, their technical simplicity and low, compared with quantum\ncomputers, cost make them ideally suitable for application in autonomous AI\nsystem. Providing a perspective view on the future progress in unconventional\nphysical reservoir computing and surveying the outcomes of more than 200\ninterdisciplinary research works, this article will be of interest to a broad\nreadership, including both students and experts in the fields of physics,\nengineering, quantum technologies and computing.",
    "categories": [
      "cs.ET",
      "cs.AI",
      "cs.NE",
      "cs.RO",
      "nlin.CD",
      "physics.flu-dyn",
      "quant-ph"
    ],
    "primary_category": "cs.ET",
    "comment": "review article",
    "pdf_url": "http://arxiv.org/pdf/2407.04717v1",
    "published_date": "2024-06-14 06:55:09 UTC",
    "updated_date": "2024-06-14 06:55:09 UTC"
  },
  {
    "arxiv_id": "2406.10311v2",
    "title": "CHiSafetyBench: A Chinese Hierarchical Safety Benchmark for Large Language Models",
    "authors": [
      "Wenjing Zhang",
      "Xuejiao Lei",
      "Zhaoxiang Liu",
      "Meijuan An",
      "Bikun Yang",
      "KaiKai Zhao",
      "Kai Wang",
      "Shiguo Lian"
    ],
    "abstract": "With the profound development of large language models(LLMs), their safety\nconcerns have garnered increasing attention. However, there is a scarcity of\nChinese safety benchmarks for LLMs, and the existing safety taxonomies are\ninadequate, lacking comprehensive safety detection capabilities in authentic\nChinese scenarios. In this work, we introduce CHiSafetyBench, a dedicated\nsafety benchmark for evaluating LLMs' capabilities in identifying risky content\nand refusing answering risky questions in Chinese contexts. CHiSafetyBench\nincorporates a dataset that covers a hierarchical Chinese safety taxonomy\nconsisting of 5 risk areas and 31 categories. This dataset comprises two types\nof tasks: multiple-choice questions and question-answering, evaluating LLMs\nfrom the perspectives of risk content identification and the ability to refuse\nanswering risky questions respectively. Utilizing this benchmark, we validate\nthe feasibility of automatic evaluation as a substitute for human evaluation\nand conduct comprehensive automatic safety assessments on mainstream Chinese\nLLMs. Our experiments reveal the varying performance of different models across\nvarious safety domains, indicating that all models possess considerable\npotential for improvement in Chinese safety capabilities. Our dataset is\npublicly available at\nhttps://github.com/UnicomAI/UnicomBenchmark/tree/main/CHiSafetyBench.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "16 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.10311v2",
    "published_date": "2024-06-14 06:47:40 UTC",
    "updated_date": "2024-09-02 03:37:35 UTC"
  },
  {
    "arxiv_id": "2406.09755v1",
    "title": "Mix Q-learning for Lane Changing: A Collaborative Decision-Making Method in Multi-Agent Deep Reinforcement Learning",
    "authors": [
      "Xiaojun Bi",
      "Mingjie He",
      "Yiwen Sun"
    ],
    "abstract": "Lane-changing decisions, which are crucial for autonomous vehicle path\nplanning, face practical challenges due to rule-based constraints and limited\ndata. Deep reinforcement learning has become a major research focus due to its\nadvantages in data acquisition and interpretability. However, current models\noften overlook collaboration, which affects not only impacts overall traffic\nefficiency but also hinders the vehicle's own normal driving in the long run.\nTo address the aforementioned issue, this paper proposes a method named Mix\nQ-learning for Lane Changing(MQLC) that integrates a hybrid value Q network,\ntaking into account both collective and individual benefits for the greater\ngood. At the collective level, our method coordinates the individual Q and\nglobal Q networks by utilizing global information. This enables agents to\neffectively balance their individual interests with the collective benefit. At\nthe individual level, we integrated a deep learning-based intent recognition\nmodule into our observation and enhanced the decision network. These changes\nprovide agents with richer decision information and more accurate feature\nextraction for improved lane-changing decisions. This strategy enables the\nmulti-agent system to learn and formulate optimal decision-making strategies\neffectively. Our MQLC model, through extensive experimental results,\nimpressively outperforms other state-of-the-art multi-agent decision-making\nmethods, achieving significantly safer and faster lane-changing decisions.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09755v1",
    "published_date": "2024-06-14 06:44:19 UTC",
    "updated_date": "2024-06-14 06:44:19 UTC"
  },
  {
    "arxiv_id": "2406.09750v2",
    "title": "ControlVAR: Exploring Controllable Visual Autoregressive Modeling",
    "authors": [
      "Xiang Li",
      "Kai Qiu",
      "Hao Chen",
      "Jason Kuen",
      "Zhe Lin",
      "Rita Singh",
      "Bhiksha Raj"
    ],
    "abstract": "Conditional visual generation has witnessed remarkable progress with the\nadvent of diffusion models (DMs), especially in tasks like control-to-image\ngeneration. However, challenges such as expensive computational cost, high\ninference latency, and difficulties of integration with large language models\n(LLMs) have necessitated exploring alternatives to DMs. This paper introduces\nControlVAR, a novel framework that explores pixel-level controls in visual\nautoregressive (VAR) modeling for flexible and efficient conditional\ngeneration. In contrast to traditional conditional models that learn the\nconditional distribution, ControlVAR jointly models the distribution of image\nand pixel-level conditions during training and imposes conditional controls\nduring testing. To enhance the joint modeling, we adopt the next-scale AR\nprediction paradigm and unify control and image representations. A\nteacher-forcing guidance strategy is proposed to further facilitate\ncontrollable generation with joint modeling. Extensive experiments demonstrate\nthe superior efficacy and flexibility of ControlVAR across various conditional\ngeneration tasks against popular conditional DMs, \\eg, ControlNet and\nT2I-Adaptor. Code: \\url{https://github.com/lxa9867/ControlVAR}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "25 pages, 19 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2406.09750v2",
    "published_date": "2024-06-14 06:35:33 UTC",
    "updated_date": "2024-10-02 02:10:26 UTC"
  },
  {
    "arxiv_id": "2406.10310v3",
    "title": "TEG-DB: A Comprehensive Dataset and Benchmark of Textual-Edge Graphs",
    "authors": [
      "Zhuofeng Li",
      "Zixing Gou",
      "Xiangnan Zhang",
      "Zhongyuan Liu",
      "Sirui Li",
      "Yuntong Hu",
      "Chen Ling",
      "Zheng Zhang",
      "Liang Zhao"
    ],
    "abstract": "Text-Attributed Graphs (TAGs) augment graph structures with natural language\ndescriptions, facilitating detailed depictions of data and their\ninterconnections across various real-world settings. However, existing TAG\ndatasets predominantly feature textual information only at the nodes, with\nedges typically represented by mere binary or categorical attributes. This lack\nof rich textual edge annotations significantly limits the exploration of\ncontextual relationships between entities, hindering deeper insights into\ngraph-structured data. To address this gap, we introduce Textual-Edge Graphs\nDatasets and Benchmark (TEG-DB), a comprehensive and diverse collection of\nbenchmark textual-edge datasets featuring rich textual descriptions on nodes\nand edges. The TEG-DB datasets are large-scale and encompass a wide range of\ndomains, from citation networks to social networks. In addition, we conduct\nextensive benchmark experiments on TEG-DB to assess the extent to which current\ntechniques, including pre-trained language models, graph neural networks, and\ntheir combinations, can utilize textual node and edge information. Our goal is\nto elicit advancements in textual-edge graph research, specifically in\ndeveloping methodologies that exploit rich textual node and edge descriptions\nto enhance graph analysis and provide deeper insights into complex real-world\nnetworks. The entire TEG-DB project is publicly accessible as an open-source\nrepository on Github, accessible at\nhttps://github.com/Zhuofeng-Li/TEG-Benchmark.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.10310v3",
    "published_date": "2024-06-14 06:22:47 UTC",
    "updated_date": "2024-11-25 13:35:47 UTC"
  },
  {
    "arxiv_id": "2406.09723v1",
    "title": "When Will Gradient Regularization Be Harmful?",
    "authors": [
      "Yang Zhao",
      "Hao Zhang",
      "Xiuyuan Hu"
    ],
    "abstract": "Gradient regularization (GR), which aims to penalize the gradient norm atop\nthe loss function, has shown promising results in training modern\nover-parameterized deep neural networks. However, can we trust this powerful\ntechnique? This paper reveals that GR can cause performance degeneration in\nadaptive optimization scenarios, particularly with learning rate warmup. Our\nempirical and theoretical analyses suggest this is due to GR inducing\ninstability and divergence in gradient statistics of adaptive optimizers at the\ninitial training stage. Inspired by the warmup heuristic, we propose three GR\nwarmup strategies, each relaxing the regularization effect to a certain extent\nduring the warmup course to ensure the accurate and stable accumulation of\ngradients. With experiments on Vision Transformer family, we confirm the three\nGR warmup strategies can effectively circumvent these issues, thereby largely\nimproving the model performance. Meanwhile, we note that scalable models tend\nto rely more on the GR warmup, where the performance can be improved by up to\n3\\% on Cifar10 compared to baseline GR. Code is available at\n\\href{https://github.com/zhaoyang-0204/gnp}{https://github.com/zhaoyang-0204/gnp}.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "55N31",
      "I.4.0"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML 2024 paper",
    "pdf_url": "http://arxiv.org/pdf/2406.09723v1",
    "published_date": "2024-06-14 05:17:39 UTC",
    "updated_date": "2024-06-14 05:17:39 UTC"
  },
  {
    "arxiv_id": "2406.09719v1",
    "title": "Self-Knowledge Distillation for Learning Ambiguity",
    "authors": [
      "Hancheol Park",
      "Soyeong Jeong",
      "Sukmin Cho",
      "Jong C. Park"
    ],
    "abstract": "Recent language models have shown remarkable performance on natural language\nunderstanding (NLU) tasks. However, they are often sub-optimal when faced with\nambiguous samples that can be interpreted in multiple ways, over-confidently\npredicting a single label without consideration for its correctness. To address\nthis issue, we propose a novel self-knowledge distillation method that enables\nmodels to learn label distributions more accurately by leveraging knowledge\ndistilled from their lower layers. This approach also includes a learning phase\nthat re-calibrates the unnecessarily strengthened confidence for training\nsamples judged as extremely ambiguous based on the distilled distribution\nknowledge. We validate our method on diverse NLU benchmark datasets and the\nexperimental results demonstrate its effectiveness in producing better label\ndistributions. Particularly, through the process of re-calibrating the\nconfidence for highly ambiguous samples, the issue of over-confidence when\npredictions for unseen samples do not match with their ground-truth labels has\nbeen significantly alleviated. This has been shown to contribute to generating\nbetter distributions than the existing state-of-the-art method. Moreover, our\nmethod is more efficient in training the models compared to the existing\nmethod, as it does not involve additional training processes to refine label\ndistributions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.09719v1",
    "published_date": "2024-06-14 05:11:32 UTC",
    "updated_date": "2024-06-14 05:11:32 UTC"
  },
  {
    "arxiv_id": "2406.10307v1",
    "title": "What is the best model? Application-driven Evaluation for Large Language Models",
    "authors": [
      "Shiguo Lian",
      "Kaikai Zhao",
      "Xinhui Liu",
      "Xuejiao Lei",
      "Bikun Yang",
      "Wenjing Zhang",
      "Kai Wang",
      "Zhaoxiang Liu"
    ],
    "abstract": "General large language models enhanced with supervised fine-tuning and\nreinforcement learning from human feedback are increasingly popular in academia\nand industry as they generalize foundation models to various practical tasks in\na prompt manner. To assist users in selecting the best model in practical\napplication scenarios, i.e., choosing the model that meets the application\nrequirements while minimizing cost, we introduce A-Eval, an application-driven\nLLMs evaluation benchmark for general large language models. First, we\ncategorize evaluation tasks into five main categories and 27 sub-categories\nfrom a practical application perspective. Next, we construct a dataset\ncomprising 678 question-and-answer pairs through a process of collecting,\nannotating, and reviewing. Then, we design an objective and effective\nevaluation method and evaluate a series of LLMs of different scales on A-Eval.\nFinally, we reveal interesting laws regarding model scale and task difficulty\nlevel and propose a feasible method for selecting the best model. Through\nA-Eval, we provide clear empirical and engineer guidance for selecting the best\nmodel, reducing barriers to selecting and using LLMs and promoting their\napplication and development. Our benchmark is publicly available at\nhttps://github.com/UnicomAI/DataSet/tree/main/TestData/GeneralAbility.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10307v1",
    "published_date": "2024-06-14 04:52:15 UTC",
    "updated_date": "2024-06-14 04:52:15 UTC"
  },
  {
    "arxiv_id": "2406.09716v1",
    "title": "Speed-up of Data Analysis with Kernel Trick in Encrypted Domain",
    "authors": [
      "Joon Soo Yoo",
      "Baek Kyung Song",
      "Tae Min Ahn",
      "Ji Won Heo",
      "Ji Won Yoon"
    ],
    "abstract": "Homomorphic encryption (HE) is pivotal for secure computation on encrypted\ndata, crucial in privacy-preserving data analysis. However, efficiently\nprocessing high-dimensional data in HE, especially for machine learning and\nstatistical (ML/STAT) algorithms, poses a challenge. In this paper, we present\nan effective acceleration method using the kernel method for HE schemes,\nenhancing time performance in ML/STAT algorithms within encrypted domains. This\ntechnique, independent of underlying HE mechanisms and complementing existing\noptimizations, notably reduces costly HE multiplications, offering near\nconstant time complexity relative to data dimension. Aimed at accessibility,\nthis method is tailored for data scientists and developers with limited\ncryptography background, facilitating advanced data analysis in secure\nenvironments.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.DC",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "Submitted as a preprint",
    "pdf_url": "http://arxiv.org/pdf/2406.09716v1",
    "published_date": "2024-06-14 04:49:40 UTC",
    "updated_date": "2024-06-14 04:49:40 UTC"
  },
  {
    "arxiv_id": "2406.09713v3",
    "title": "Meta-Learning Loss Functions for Deep Neural Networks",
    "authors": [
      "Christian Raymond"
    ],
    "abstract": "Humans can often quickly and efficiently solve complex new learning tasks\ngiven only a small set of examples. In contrast, modern artificially\nintelligent systems often require thousands or millions of observations in\norder to solve even the most basic tasks. Meta-learning aims to resolve this\nissue by leveraging past experiences from similar learning tasks to embed the\nappropriate inductive biases into the learning system. Historically methods for\nmeta-learning components such as optimizers, parameter initializations, and\nmore have led to significant performance increases. This thesis aims to explore\nthe concept of meta-learning to improve performance, through the\noften-overlooked component of the loss function. The loss function is a vital\ncomponent of a learning system, as it represents the primary learning\nobjective, where success is determined and quantified by the system's ability\nto optimize for that objective successfully.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "PhD thesis",
    "pdf_url": "http://arxiv.org/pdf/2406.09713v3",
    "published_date": "2024-06-14 04:46:14 UTC",
    "updated_date": "2025-05-07 01:38:16 UTC"
  },
  {
    "arxiv_id": "2406.09710v1",
    "title": "Fine-Grained Urban Flow Inference with Multi-scale Representation Learning",
    "authors": [
      "Shilu Yuan",
      "Dongfeng Li",
      "Wei Liu",
      "Xinxin Zhang",
      "Meng Chen",
      "Junjie Zhang",
      "Yongshun Gong"
    ],
    "abstract": "Fine-grained urban flow inference (FUFI) is a crucial transportation service\naimed at improving traffic efficiency and safety. FUFI can infer fine-grained\nurban traffic flows based solely on observed coarse-grained data. However, most\nof existing methods focus on the influence of single-scale static geographic\ninformation on FUFI, neglecting the interactions and dynamic information\nbetween different-scale regions within the city. Different-scale geographical\nfeatures can capture redundant information from the same spatial areas. In\norder to effectively learn multi-scale information across time and space, we\npropose an effective fine-grained urban flow inference model called UrbanMSR,\nwhich uses self-supervised contrastive learning to obtain dynamic multi-scale\nrepresentations of neighborhood-level and city-level geographic information,\nand fuses multi-scale representations to improve fine-grained accuracy. The\nfusion of multi-scale representations enhances fine-grained. We validate the\nperformance through extensive experiments on three real-world datasets. The\nresutls compared with state-of-the-art methods demonstrate the superiority of\nthe proposed model.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09710v1",
    "published_date": "2024-06-14 04:42:29 UTC",
    "updated_date": "2024-06-14 04:42:29 UTC"
  },
  {
    "arxiv_id": "2406.10306v1",
    "title": "A Simple, Solid, and Reproducible Baseline for Bridge Bidding AI",
    "authors": [
      "Haruka Kita",
      "Sotetsu Koyamada",
      "Yotaro Yamaguchi",
      "Shin Ishii"
    ],
    "abstract": "Contract bridge, a cooperative game characterized by imperfect information\nand multi-agent dynamics, poses significant challenges and serves as a critical\nbenchmark in artificial intelligence (AI) research. Success in this domain\nrequires agents to effectively cooperate with their partners. This study\ndemonstrates that an appropriate combination of existing methods can perform\nsurprisingly well in bridge bidding against WBridge5, a leading benchmark in\nthe bridge bidding system and a multiple-time World Computer-Bridge\nChampionship winner. Our approach is notably simple, yet it outperforms the\ncurrent state-of-the-art methodologies in this field. Furthermore, we have made\nour code and models publicly available as open-source software. This initiative\nprovides a strong starting foundation for future bridge AI research,\nfacilitating the development and verification of new strategies and\nadvancements in the field.",
    "categories": [
      "cs.AI",
      "cs.GT",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted version of IEEE CoG 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.10306v1",
    "published_date": "2024-06-14 04:07:37 UTC",
    "updated_date": "2024-06-14 04:07:37 UTC"
  },
  {
    "arxiv_id": "2406.12726v1",
    "title": "ED-sKWS: Early-Decision Spiking Neural Networks for Rapid,and Energy-Efficient Keyword Spotting",
    "authors": [
      "Zeyang Song",
      "Qianhui Liu",
      "Qu Yang",
      "Yizhou Peng",
      "Haizhou Li"
    ],
    "abstract": "Keyword Spotting (KWS) is essential in edge computing requiring rapid and\nenergy-efficient responses. Spiking Neural Networks (SNNs) are well-suited for\nKWS for their efficiency and temporal capacity for speech. To further reduce\nthe latency and energy consumption, this study introduces ED-sKWS, an SNN-based\nKWS model with an early-decision mechanism that can stop speech processing and\noutput the result before the end of speech utterance. Furthermore, we introduce\na Cumulative Temporal (CT) loss that can enhance prediction accuracy at both\nthe intermediate and final timesteps. To evaluate early-decision performance,\nwe present the SC-100 dataset including 100 speech commands with beginning and\nend timestamp annotation. Experiments on the Google Speech Commands v2 and our\nSC-100 datasets show that ED-sKWS maintains competitive accuracy with 61%\ntimesteps and 52% energy consumption compared to SNN models without\nearly-decision mechanism, ensuring rapid response and energy efficiency.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted by INTERSPEECH2024",
    "pdf_url": "http://arxiv.org/pdf/2406.12726v1",
    "published_date": "2024-06-14 03:46:01 UTC",
    "updated_date": "2024-06-14 03:46:01 UTC"
  },
  {
    "arxiv_id": "2406.10305v2",
    "title": "Unlock the Correlation between Supervised Fine-Tuning and Reinforcement Learning in Training Code Large Language Models",
    "authors": [
      "Jie Chen",
      "Xintian Han",
      "Yu Ma",
      "Xun Zhou",
      "Liang Xiang"
    ],
    "abstract": "Automatic code generation has been a longstanding research topic. With the\nadvancement of general-purpose large language models (LLMs), the ability to\ncode stands out as one important measure to the model's reasoning performance.\nUsually, a two-stage training paradigm is implemented to obtain a Code LLM,\nnamely the pretraining and the fine-tuning. Within the fine-tuning, supervised\nfine-tuning (SFT), and reinforcement learning (RL) are often used to improve\nthe model's zero-shot ability. A large number of work has been conducted to\nimprove the model's performance on code-related benchmarks with either\nmodifications to the algorithm or refinement of the dataset. However, we still\nlack a deep insight into the correlation between SFT and RL. For instance, what\nkind of dataset should be used to ensure generalization, or what if we abandon\nthe SFT phase in fine-tuning. In this work, we make an attempt to understand\nthe correlation between SFT and RL. To facilitate our research, we manually\ncraft 100 basis python functions, called atomic functions, and then a\nsynthesizing pipeline is deployed to create a large number of synthetic\nfunctions on top of the atomic ones. In this manner, we ensure that the train\nand test sets remain distinct, preventing data contamination. Through\ncomprehensive ablation study, we find: (1) Both atomic and synthetic functions\nare indispensable for SFT's generalization, and only a handful of synthetic\nfunctions are adequate; (2) Through RL, the SFT's generalization to target\ndomain can be greatly enhanced, even with the same training prompts; (3)\nTraining RL from scratch can alleviate the over-fitting issue introduced in the\nSFT phase.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10305v2",
    "published_date": "2024-06-14 03:39:01 UTC",
    "updated_date": "2024-12-17 02:53:54 UTC"
  },
  {
    "arxiv_id": "2406.11890v2",
    "title": "Unraveling the Mechanics of Learning-Based Demonstration Selection for In-Context Learning",
    "authors": [
      "Hui Liu",
      "Wenya Wang",
      "Hao Sun",
      "Chris Xing Tian",
      "Chenqi Kong",
      "Xin Dong",
      "Haoliang Li"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated impressive in-context learning\n(ICL) capabilities from few-shot demonstration exemplars. While recent\nlearning-based demonstration selection methods have proven beneficial to ICL by\nchoosing more useful exemplars, their underlying mechanisms are opaque,\nhindering efforts to address limitations such as high training costs and poor\ngeneralization across tasks. These methods generally assume the selection\nprocess captures similarities between the exemplar and the target instance,\nhowever, it remains unknown what kinds of similarities are captured and vital\nto performing ICL. To dive into this question, we analyze the working\nmechanisms of the learning-based demonstration selection methods and\nempirically identify two important factors related to similarity measurement:\n1) The ability to integrate different levels of task-agnostic text similarities\nbetween the input of exemplars and test cases enhances generalization power\nacross different tasks. 2) Incorporating task-specific labels when measuring\nthe similarities significantly improves the performance on each specific task.\nWe validate these two findings through extensive quantitative and qualitative\nanalyses across ten datasets and various LLMs. Based on our findings, we\nintroduce two effective yet simplified exemplar selection methods catering to\ntask-agnostic and task-specific demands, eliminating the costly LLM inference\noverhead.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "17 pages, 7 figures and 9 tables",
    "pdf_url": "http://arxiv.org/pdf/2406.11890v2",
    "published_date": "2024-06-14 03:34:02 UTC",
    "updated_date": "2024-10-15 10:53:55 UTC"
  },
  {
    "arxiv_id": "2406.09684v2",
    "title": "Explainable AI for Comparative Analysis of Intrusion Detection Models",
    "authors": [
      "Pap M. Corea",
      "Yongxin Liu",
      "Jian Wang",
      "Shuteng Niu",
      "Houbing Song"
    ],
    "abstract": "Explainable Artificial Intelligence (XAI) has become a widely discussed\ntopic, the related technologies facilitate better understanding of conventional\nblack-box models like Random Forest, Neural Networks and etc. However,\ndomain-specific applications of XAI are still insufficient. To fill this gap,\nthis research analyzes various machine learning models to the tasks of binary\nand multi-class classification for intrusion detection from network traffic on\nthe same dataset using occlusion sensitivity. The models evaluated include\nLinear Regression, Logistic Regression, Linear Support Vector Machine (SVM),\nK-Nearest Neighbors (KNN), Random Forest, Decision Trees, and Multi-Layer\nPerceptrons (MLP). We trained all models to the accuracy of 90\\% on the\nUNSW-NB15 Dataset. We found that most classifiers leverage only less than three\ncritical features to achieve such accuracies, indicating that effective feature\nengineering could actually be far more important for intrusion detection than\napplying complicated models. We also discover that Random Forest provides the\nbest performance in terms of accuracy, time efficiency and robustness. Data and\ncode available at https://github.com/pcwhy/XML-IntrusionDetection.git",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "Submitted to IEEE MeditCom 2024 - WS-05",
    "pdf_url": "http://arxiv.org/pdf/2406.09684v2",
    "published_date": "2024-06-14 03:11:01 UTC",
    "updated_date": "2024-07-03 10:32:51 UTC"
  },
  {
    "arxiv_id": "2406.09675v1",
    "title": "Benchmarking Spectral Graph Neural Networks: A Comprehensive Study on Effectiveness and Efficiency",
    "authors": [
      "Ningyi Liao",
      "Haoyu Liu",
      "Zulun Zhu",
      "Siqiang Luo",
      "Laks V. S. Lakshmanan"
    ],
    "abstract": "With the recent advancements in graph neural networks (GNNs), spectral GNNs\nhave received increasing popularity by virtue of their specialty in capturing\ngraph signals in the frequency domain, demonstrating promising capability in\nspecific tasks. However, few systematic studies have been conducted on\nassessing their spectral characteristics. This emerging family of models also\nvaries in terms of designs and settings, leading to difficulties in comparing\ntheir performance and deciding on the suitable model for specific scenarios,\nespecially for large-scale tasks. In this work, we extensively benchmark\nspectral GNNs with a focus on the frequency perspective. We analyze and\ncategorize over 30 GNNs with 27 corresponding filters. Then, we implement these\nspectral models under a unified framework with dedicated graph computations and\nefficient training schemes. Thorough experiments are conducted on the spectral\nmodels with inclusive metrics on effectiveness and efficiency, offering\npractical guidelines on evaluating and selecting spectral GNNs with desirable\nperformance. Our implementation enables application on larger graphs with\ncomparable performance and less overhead, which is available at:\nhttps://github.com/gdmnl/Spectral-GNN-Benchmark.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09675v1",
    "published_date": "2024-06-14 02:56:57 UTC",
    "updated_date": "2024-06-14 02:56:57 UTC"
  },
  {
    "arxiv_id": "2406.09671v1",
    "title": "Evaluating ChatGPT-4 Vision on Brazil's National Undergraduate Computer Science Exam",
    "authors": [
      "Nabor C. Mendon√ßa"
    ],
    "abstract": "The recent integration of visual capabilities into Large Language Models\n(LLMs) has the potential to play a pivotal role in science and technology\neducation, where visual elements such as diagrams, charts, and tables are\ncommonly used to improve the learning experience. This study investigates the\nperformance of ChatGPT-4 Vision, OpenAI's most advanced visual model at the\ntime the study was conducted, on the Bachelor in Computer Science section of\nBrazil's 2021 National Undergraduate Exam (ENADE). By presenting the model with\nthe exam's open and multiple-choice questions in their original image format\nand allowing for reassessment in response to differing answer keys, we were\nable to evaluate the model's reasoning and self-reflecting capabilities in a\nlarge-scale academic assessment involving textual and visual content. ChatGPT-4\nVision significantly outperformed the average exam participant, positioning\nitself within the top 10 best score percentile. While it excelled in questions\nthat incorporated visual elements, it also encountered challenges with question\ninterpretation, logical reasoning, and visual acuity. The involvement of an\nindependent expert panel to review cases of disagreement between the model and\nthe answer key revealed some poorly constructed questions containing vague or\nambiguous statements, calling attention to the critical need for improved\nquestion design in future exams. Our findings suggest that while ChatGPT-4\nVision shows promise in multimodal academic evaluations, human oversight\nremains crucial for verifying the model's accuracy and ensuring the fairness of\nhigh-stakes educational exams. The paper's research materials are publicly\navailable at https://github.com/nabormendonca/gpt-4v-enade-cs-2021.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted for publication",
    "pdf_url": "http://arxiv.org/pdf/2406.09671v1",
    "published_date": "2024-06-14 02:42:30 UTC",
    "updated_date": "2024-06-14 02:42:30 UTC"
  },
  {
    "arxiv_id": "2406.10303v2",
    "title": "A Survey on Large Language Models from General Purpose to Medical Applications: Datasets, Methodologies, and Evaluations",
    "authors": [
      "Jinqiang Wang",
      "Huansheng Ning",
      "Yi Peng",
      "Qikai Wei",
      "Daniel Tesfai",
      "Wenwei Mao",
      "Tao Zhu",
      "Runhe Huang"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated surprising performance across\nvarious natural language processing tasks. Recently, medical LLMs enhanced with\ndomain-specific knowledge have exhibited excellent capabilities in medical\nconsultation and diagnosis. These models can smoothly simulate doctor-patient\ndialogues and provide professional medical advice. Most medical LLMs are\ndeveloped through continued training of open-source general LLMs, which require\nsignificantly fewer computational resources than training LLMs from scratch.\nAdditionally, this approach offers better patient privacy protection than\nAPI-based solutions. Given the above advantages, this survey systematically\nsummarizes how to train medical LLMs based on open-source general LLMs from a\nmore fine-grained perspective. It covers (a) how to acquire training corpus and\nconstruct customized medical training sets, (b) how to choose an appropriate\ntraining paradigm, (c) how to choose a suitable evaluation benchmark, and (d)\nexisting challenges and promising research directions are discussed. This\nsurvey can provide guidance for the development of LLMs focused on various\nmedical applications, such as medical education, diagnostic planning, and\nclinical assistants. Related resources and supplemental information can be\nfound on the GitHub repository.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "25 pages,4 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.10303v2",
    "published_date": "2024-06-14 02:42:20 UTC",
    "updated_date": "2024-09-23 02:45:29 UTC"
  },
  {
    "arxiv_id": "2406.09661v1",
    "title": "Temporal Planning via Interval Logic Satisfiability for Autonomous Systems",
    "authors": [
      "Miquel Ramirez",
      "Anubhav Singh",
      "Peter Stuckey",
      "Chris Manzie"
    ],
    "abstract": "Many automated planning methods and formulations rely on suitably designed\nabstractions or simplifications of the constrained dynamics associated with\nagents to attain computational scalability. We consider formulations of\ntemporal planning where intervals are associated with both action and fluent\natoms, and relations between these are given as sentences in Allen's Interval\nLogic. We propose a notion of planning graphs that can account for complex\nconcurrency relations between actions and fluents as a Constraint Programming\n(CP) model. We test an implementation of our algorithm on a state-of-the-art\nframework for CP and compare it with PDDL 2.1 planners that capture plans\nrequiring complex concurrent interactions between agents. We demonstrate our\nalgorithm outperforms existing PDDL 2.1 planners in the case studies. Still,\nscalability remains challenging when plans must comply with intricate\nconcurrent interactions and the sequencing of actions.",
    "categories": [
      "cs.LO",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LO",
    "comment": "This publication is an extended version of a manuscript submitted to\n  ICAPS-24 (and rejected). Please contact the first author for queries,\n  comments or discussion of the paper",
    "pdf_url": "http://arxiv.org/pdf/2406.09661v1",
    "published_date": "2024-06-14 02:21:53 UTC",
    "updated_date": "2024-06-14 02:21:53 UTC"
  },
  {
    "arxiv_id": "2406.09662v2",
    "title": "Learning Language Structures through Grounding",
    "authors": [
      "Freda Shi"
    ],
    "abstract": "Language is highly structured, with syntactic and semantic structures, to\nsome extent, agreed upon by speakers of the same language. With implicit or\nexplicit awareness of such structures, humans can learn and use language\nefficiently and generalize to sentences that contain unseen words. Motivated by\nhuman language learning, in this dissertation, we consider a family of machine\nlearning tasks that aim to learn language structures through grounding. We seek\ndistant supervision from other data sources (i.e., grounds), including but not\nlimited to other modalities (e.g., vision), execution results of programs, and\nother languages.\n  We demonstrate the potential of this task formulation and advocate for its\nadoption through three schemes. In Part I, we consider learning syntactic\nparses through visual grounding. We propose the task of visually grounded\ngrammar induction, present the first models to induce syntactic structures from\nvisually grounded text and speech, and find that the visual grounding signals\ncan help improve the parsing quality over language-only models. As a side\ncontribution, we propose a novel evaluation metric that enables the evaluation\nof speech parsing without text or automatic speech recognition systems\ninvolved. In Part II, we propose two execution-aware methods to map sentences\ninto corresponding semantic structures (i.e., programs), significantly\nimproving compositional generalization and few-shot program synthesis. In Part\nIII, we propose methods that learn language structures from annotations in\nother languages. Specifically, we propose a method that sets a new state of the\nart on cross-lingual word alignment. We then leverage the learned word\nalignments to improve the performance of zero-shot cross-lingual dependency\nparsing, by proposing a novel substructure-based projection method that\npreserves structural knowledge learned from the source language.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "Ph.D. Thesis",
    "pdf_url": "http://arxiv.org/pdf/2406.09662v2",
    "published_date": "2024-06-14 02:21:53 UTC",
    "updated_date": "2024-10-21 23:58:45 UTC"
  },
  {
    "arxiv_id": "2406.15222v4",
    "title": "A Deep Learning System for Rapid and Accurate Warning of Acute Aortic Syndrome on Non-contrast CT in China",
    "authors": [
      "Yujian Hu",
      "Yilang Xiang",
      "Yan-Jie Zhou",
      "Yangyan He",
      "Dehai Lang",
      "Shifeng Yang",
      "Xiaolong Du",
      "Chunlan Den",
      "Youyao Xu",
      "Gaofeng Wang",
      "Zhengyao Ding",
      "Jingyong Huang",
      "Wenjun Zhao",
      "Xuejun Wu",
      "Donglin Li",
      "Qianqian Zhu",
      "Zhenjiang Li",
      "Chenyang Qiu",
      "Ziheng Wu",
      "Yunjun He",
      "Chen Tian",
      "Yihui Qiu",
      "Zuodong Lin",
      "Xiaolong Zhang",
      "Yuan He",
      "Zhenpeng Yuan",
      "Xiaoxiang Zhou",
      "Rong Fan",
      "Ruihan Chen",
      "Wenchao Guo",
      "Jianpeng Zhang",
      "Tony C. W. Mok",
      "Zi Li",
      "Mannudeep K. Kalra",
      "Le Lu",
      "Wenbo Xiao",
      "Xiaoqiang Li",
      "Yun Bian",
      "Chengwei Shao",
      "Guofu Wang",
      "Wei Lu",
      "Zhengxing Huang",
      "Minfeng Xu",
      "Hongkun Zhang"
    ],
    "abstract": "The accurate and timely diagnosis of acute aortic syndromes (AAS) in patients\npresenting with acute chest pain remains a clinical challenge. Aortic CT\nangiography (CTA) is the imaging protocol of choice in patients with suspected\nAAS. However, due to economic and workflow constraints in China, the majority\nof suspected patients initially undergo non-contrast CT as the initial imaging\ntesting, and CTA is reserved for those at higher risk. In this work, we present\nan artificial intelligence-based warning system, iAorta, using non-contrast CT\nfor AAS identification in China, which demonstrates remarkably high accuracy\nand provides clinicians with interpretable warnings. iAorta was evaluated\nthrough a comprehensive step-wise study. In the multi-center retrospective\nstudy (n = 20,750), iAorta achieved a mean area under the receiver operating\ncurve (AUC) of 0.958 (95% CI 0.950-0.967). In the large-scale real-world study\n(n = 137,525), iAorta demonstrated consistently high performance across various\nnon-contrast CT protocols, achieving a sensitivity of 0.913-0.942 and a\nspecificity of 0.991-0.993. In the prospective comparative study (n = 13,846),\niAorta demonstrated the capability to significantly shorten the time to correct\ndiagnostic pathway. For the prospective pilot deployment that we conducted,\niAorta correctly identified 21 out of 22 patients with AAS among 15,584\nconsecutive patients presenting with acute chest pain and under non-contrast CT\nprotocol in the emergency department (ED) and enabled the average diagnostic\ntime of these 21 AAS positive patients to be 102.1 (75-133) mins. Last, the\niAorta can help avoid delayed or missed diagnosis of AAS in settings where\nnon-contrast CT remains the unavoidable the initial or only imaging test in\nresource-constrained regions and in patients who cannot or did not receive\nintravenous contrast.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15222v4",
    "published_date": "2024-06-14 02:15:09 UTC",
    "updated_date": "2025-04-23 09:05:22 UTC"
  },
  {
    "arxiv_id": "2406.09656v2",
    "title": "RSEND: Retinex-based Squeeze and Excitation Network with Dark Region Detection for Efficient Low Light Image Enhancement",
    "authors": [
      "Jingcheng Li",
      "Ye Qiao",
      "Haocheng Xu",
      "Sitao Huang"
    ],
    "abstract": "Images captured under low-light scenarios often suffer from low quality.\nPrevious CNN-based deep learning methods often involve using Retinex theory.\nNevertheless, most of them cannot perform well in more complicated datasets\nlike LOL-v2 while consuming too much computational resources. Besides, some of\nthese methods require sophisticated training at different stages, making the\nprocedure even more time-consuming and tedious. In this paper, we propose a\nmore accurate, concise, and one-stage Retinex theory based framework, RSEND.\nRSEND first divides the low-light image into the illumination map and\nreflectance map, then captures the important details in the illumination map\nand performs light enhancement. After this step, it refines the enhanced\ngray-scale image and does element-wise matrix multiplication with the\nreflectance map. By denoising the output it has from the previous step, it\nobtains the final result. In all the steps, RSEND utilizes Squeeze and\nExcitation network to better capture the details. Comprehensive quantitative\nand qualitative experiments show that our Efficient Retinex model significantly\noutperforms other CNN-based models, achieving a PSNR improvement ranging from\n0.44 dB to 4.2 dB in different datasets and even outperforms transformer-based\nmodels in the LOL-v2-real dataset.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09656v2",
    "published_date": "2024-06-14 01:36:52 UTC",
    "updated_date": "2025-04-24 02:10:19 UTC"
  },
  {
    "arxiv_id": "2406.09646v1",
    "title": "A Survey of Video Datasets for Grounded Event Understanding",
    "authors": [
      "Kate Sanders",
      "Benjamin Van Durme"
    ],
    "abstract": "While existing video benchmarks largely consider specialized downstream tasks\nlike retrieval or question-answering (QA), contemporary multimodal AI systems\nmust be capable of well-rounded common-sense reasoning akin to human visual\nunderstanding. A critical component of human temporal-visual perception is our\nability to identify and cognitively model \"things happening\", or events.\nHistorically, video benchmark tasks have implicitly tested for this ability\n(e.g., video captioning, in which models describe visual events with natural\nlanguage), but they do not consider video event understanding as a task in\nitself. Recent work has begun to explore video analogues to textual event\nextraction but consists of competing task definitions and datasets limited to\nhighly specific event types. Therefore, while there is a rich domain of\nevent-centric video research spanning the past 10+ years, it is unclear how\nvideo event understanding should be framed and what resources we have to study\nit. In this paper, we survey 105 video datasets that require event\nunderstanding capability, consider how they contribute to the study of robust\nevent understanding in video, and assess proposed video event extraction tasks\nin the context of this body of research. We propose suggestions informed by\nthis survey for dataset curation and task framing, with an emphasis on the\nuniquely temporal nature of video events and ambiguity in visual content.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09646v1",
    "published_date": "2024-06-14 00:36:55 UTC",
    "updated_date": "2024-06-14 00:36:55 UTC"
  }
]
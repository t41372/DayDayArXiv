[
  {
    "arxiv_id": "2412.13386v1",
    "title": "An Exploratory Study of ML Sketches and Visual Code Assistants",
    "authors": [
      "Luís F. Gomes",
      "Vincent J. Hellendoorn",
      "Jonathan Aldrich",
      "Rui Abreu"
    ],
    "abstract": "This paper explores the integration of Visual Code Assistants in Integrated\nDevelopment Environments (IDEs). In Software Engineering, whiteboard sketching\nis often the initial step before coding, serving as a crucial collaboration\ntool for developers. Previous studies have investigated patterns in SE sketches\nand how they are used in practice, yet methods for directly using these\nsketches for code generation remain limited. The emergence of visually-equipped\nlarge language models presents an opportunity to bridge this gap, which is the\nfocus of our research. In this paper, we built a first prototype of a Visual\nCode Assistant to get user feedback regarding in-IDE sketch-to-code tools. We\nconduct an experiment with 19 data scientists, most of whom regularly sketch as\npart of their job. We investigate developers' mental models by analyzing\npatterns commonly observed in their sketches when developing an ML workflow.\nAnalysis indicates that diagrams were the preferred organizational component\n(52.6%), often accompanied by lists (42.1%) and numbered points (36.8%). Our\ntool converts their sketches into a Python notebook by querying an LLM. We use\nan LLM-as-judge setup to score the quality of the generated code, finding that\neven brief sketching can effectively generate useful code outlines. We also\nfind a positive correlation between sketch time and the quality of the\ngenerated code. We conclude the study by conducting extensive interviews to\nassess the tool's usefulness, explore potential use cases, and understand\ndevelopers' needs. As noted by participants, promising applications for these\nassistants include education, prototyping, and collaborative settings. Our\nfindings signal promise for the next generation of Code Assistants to integrate\nvisual information, both to improve code generation and to better leverage\ndevelopers' existing sketching practices.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13386v1",
    "published_date": "2024-12-17 23:44:45 UTC",
    "updated_date": "2024-12-17 23:44:45 UTC"
  },
  {
    "arxiv_id": "2412.13380v2",
    "title": "Voter Priming Campaigns: Strategies, Equilibria, and Algorithms",
    "authors": [
      "Jonathan Shaki",
      "Yonatan Aumann",
      "Sarit Kraus"
    ],
    "abstract": "Issue salience is a major determinant in voters' decisions. Candidates and\npolitical parties campaign to shift salience to their advantage - a process\ntermed priming. We study the dynamics, strategies and equilibria of campaign\nspending for voter priming in multi-issue multi-party settings. We consider\nboth parliamentary elections, where parties aim to maximize their share of\nvotes, and various settings for presidential elections, where the winner takes\nall. For parliamentary elections, we show that pure equilibrium spending always\nexists and can be computed in time linear in the number of voters. For two\nparties and all settings, a spending equilibrium exists such that each party\ninvests only in a single issue, and an equilibrium can be computed in time that\nis polynomial in the number of issues and linear in the number of voters. We\nalso show that in most presidential settings no equilibrium exists. Additional\nproperties of optimal campaign strategies are also studied.",
    "categories": [
      "cs.GT",
      "cs.AI"
    ],
    "primary_category": "cs.GT",
    "comment": "To be published in AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.13380v2",
    "published_date": "2024-12-17 23:28:02 UTC",
    "updated_date": "2024-12-25 11:30:58 UTC"
  },
  {
    "arxiv_id": "2412.13377v2",
    "title": "DateLogicQA: Benchmarking Temporal Biases in Large Language Models",
    "authors": [
      "Gagan Bhatia",
      "MingZe Tang",
      "Cristina Mahanta",
      "Madiha Kazi"
    ],
    "abstract": "This paper introduces DateLogicQA, a benchmark with 190 questions covering\ndiverse date formats, temporal contexts, and reasoning types. We propose the\nSemantic Integrity Metric to assess tokenization quality and analyse two\nbiases: Representation-Level Bias, affecting embeddings, and Logical-Level\nBias, influencing reasoning outputs. Our findings provide a comprehensive\nevaluation of LLMs' capabilities and limitations in temporal reasoning,\nhighlighting key challenges in handling temporal data accurately.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13377v2",
    "published_date": "2024-12-17 23:25:47 UTC",
    "updated_date": "2025-05-19 12:29:10 UTC"
  },
  {
    "arxiv_id": "2412.13376v1",
    "title": "Targeted View-Invariant Adversarial Perturbations for 3D Object Recognition",
    "authors": [
      "Christian Green",
      "Mehmet Ergezer",
      "Abdurrahman Zeybey"
    ],
    "abstract": "Adversarial attacks pose significant challenges in 3D object recognition,\nespecially in scenarios involving multi-view analysis where objects can be\nobserved from varying angles. This paper introduces View-Invariant Adversarial\nPerturbations (VIAP), a novel method for crafting robust adversarial examples\nthat remain effective across multiple viewpoints. Unlike traditional methods,\nVIAP enables targeted attacks capable of manipulating recognition systems to\nclassify objects as specific, pre-determined labels, all while using a single\nuniversal perturbation. Leveraging a dataset of 1,210 images across 121 diverse\nrendered 3D objects, we demonstrate the effectiveness of VIAP in both targeted\nand untargeted settings. Our untargeted perturbations successfully generate a\nsingular adversarial noise robust to 3D transformations, while targeted attacks\nachieve exceptional results, with top-1 accuracies exceeding 95% across various\nepsilon values. These findings highlight VIAPs potential for real-world\napplications, such as testing the robustness of 3D recognition systems. The\nproposed method sets a new benchmark for view-invariant adversarial robustness,\nadvancing the field of adversarial machine learning for 3D object recognition.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to AAAI-25 Workshop on Artificial Intelligence for Cyber\n  Security (AICS): http://aics.site/AICS2025/index.html",
    "pdf_url": "http://arxiv.org/pdf/2412.13376v1",
    "published_date": "2024-12-17 23:23:25 UTC",
    "updated_date": "2024-12-17 23:23:25 UTC"
  },
  {
    "arxiv_id": "2412.13372v1",
    "title": "Sum-of-Squares Programming for Ma-Trudinger-Wang Regularity of Optimal Transport Maps",
    "authors": [
      "Sachin Shivakumar",
      "Georgiy A. Bondar",
      "Gabriel Khan",
      "Abhishek Halder"
    ],
    "abstract": "For a given ground cost, approximating the Monge optimal transport map that\npushes forward a given probability measure onto another has become a staple in\nseveral modern machine learning algorithms. The fourth-order Ma-Trudinger-Wang\n(MTW) tensor associated with this ground cost function provides a notion of\ncurvature in optimal transport. The non-negativity of this tensor plays a\ncrucial role for establishing continuity for the Monge optimal transport map.\nIt is, however, generally difficult to analytically verify this condition for\nany given ground cost. To expand the class of cost functions for which MTW\nnon-negativity can be verified, we propose a provably correct computational\napproach which provides certificates of non-negativity for the MTW tensor using\nSum-of-Squares (SOS) programming. We further show that our SOS technique can\nalso be used to compute an inner approximation of the region where MTW\nnon-negativity holds. We apply our proposed SOS programming method to several\npractical ground cost functions to approximate the regions of regularity of\ntheir corresponding optimal transport maps.",
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.LG",
      "math.DG"
    ],
    "primary_category": "math.OC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13372v1",
    "published_date": "2024-12-17 23:10:03 UTC",
    "updated_date": "2024-12-17 23:10:03 UTC"
  },
  {
    "arxiv_id": "2412.13369v1",
    "title": "Multiple Mean-Payoff Optimization under Local Stability Constraints",
    "authors": [
      "David Klaška",
      "Antonín Kučera",
      "Vojtěch Kůr",
      "Vít Musil",
      "Vojtěch Řehák"
    ],
    "abstract": "The long-run average payoff per transition (mean payoff) is the main tool for\nspecifying the performance and dependability properties of discrete systems.\nThe problem of constructing a controller (strategy) simultaneously optimizing\nseveral mean payoffs has been deeply studied for stochastic and game-theoretic\nmodels. One common issue of the constructed controllers is the instability of\nthe mean payoffs, measured by the deviations of the average rewards per\ntransition computed in a finite \"window\" sliding along a run. Unfortunately,\nthe problem of simultaneously optimizing the mean payoffs under local stability\nconstraints is computationally hard, and the existing works do not provide a\npractically usable algorithm even for non-stochastic models such as two-player\ngames. In this paper, we design and evaluate the first efficient and scalable\nsolution to this problem applicable to Markov decision processes.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.13369v1",
    "published_date": "2024-12-17 22:53:08 UTC",
    "updated_date": "2024-12-17 22:53:08 UTC"
  },
  {
    "arxiv_id": "2412.13365v1",
    "title": "Quantitative Predictive Monitoring and Control for Safe Human-Machine Interaction",
    "authors": [
      "Shuyang Dong",
      "Meiyi Ma",
      "Josephine Lamp",
      "Sebastian Elbaum",
      "Matthew B. Dwyer",
      "Lu Feng"
    ],
    "abstract": "There is a growing trend toward AI systems interacting with humans to\nrevolutionize a range of application domains such as healthcare and\ntransportation. However, unsafe human-machine interaction can lead to\ncatastrophic failures. We propose a novel approach that predicts future states\nby accounting for the uncertainty of human interaction, monitors whether\npredictions satisfy or violate safety requirements, and adapts control actions\nbased on the predictive monitoring results. Specifically, we develop a new\nquantitative predictive monitor based on Signal Temporal Logic with Uncertainty\n(STL-U) to compute a robustness degree interval, which indicates the extent to\nwhich a sequence of uncertain predictions satisfies or violates an STL-U\nrequirement. We also develop a new loss function to guide the uncertainty\ncalibration of Bayesian deep learning and a new adaptive control method, both\nof which leverage STL-U quantitative predictive monitoring results. We apply\nthe proposed approach to two case studies: Type 1 Diabetes management and\nsemi-autonomous driving. Experiments show that the proposed approach improves\nsafety and effectiveness in both case studies.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13365v1",
    "published_date": "2024-12-17 22:46:39 UTC",
    "updated_date": "2024-12-17 22:46:39 UTC"
  },
  {
    "arxiv_id": "2412.13359v1",
    "title": "Multi-Agent Motion Planning For Differential Drive Robots Through Stationary State Search",
    "authors": [
      "Jingtian Yan",
      "Jiaoyang Li"
    ],
    "abstract": "Multi-Agent Motion Planning (MAMP) finds various applications in fields such\nas traffic management, airport operations, and warehouse automation. In many of\nthese environments, differential drive robots are commonly used. These robots\nhave a kinodynamic model that allows only in-place rotation and movement along\ntheir current orientation, subject to speed and acceleration limits. However,\nexisting Multi-Agent Path Finding (MAPF)-based methods often use simplified\nmodels for robot kinodynamics, which limits their practicality and realism. In\nthis paper, we introduce a three-level framework called MASS to address these\nchallenges. MASS combines MAPF-based methods with our proposed stationary state\nsearch planner to generate high-quality kinodynamically-feasible plans. We\nfurther extend MASS using an adaptive window mechanism to address the lifelong\nMAMP problem. Empirically, we tested our methods on the single-shot grid map\ndomain and the lifelong warehouse domain. Our method shows up to 400%\nimprovements in terms of throughput compared to existing methods.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13359v1",
    "published_date": "2024-12-17 22:17:42 UTC",
    "updated_date": "2024-12-17 22:17:42 UTC"
  },
  {
    "arxiv_id": "2412.13350v1",
    "title": "A Novel Machine Learning Classifier Based on Genetic Algorithms and Data Importance Reformatting",
    "authors": [
      "A. K. Alkhayyata",
      "N. M. Hewahi"
    ],
    "abstract": "In this paper, a novel classification algorithm that is based on Data\nImportance (DI) reformatting and Genetic Algorithms (GA) named GADIC is\nproposed to overcome the issues related to the nature of data which may hinder\nthe performance of the Machine Learning (ML) classifiers. GADIC comprises three\nphases which are data reformatting phase which depends on DI concept, training\nphase where GA is applied on the reformatted training dataset, and testing\nphase where the instances of the reformatted testing dataset are being averaged\nbased on similar instances in the training dataset. GADIC is an approach that\nutilizes the exiting ML classifiers with involvement of data reformatting,\nusing GA to tune the inputs, and averaging the similar instances to the unknown\ninstance. The averaging of the instances becomes the unknown instance to be\nclassified in the stage of testing. GADIC has been tested on five existing ML\nclassifiers which are Support Vector Machine (SVM), K-Nearest Neighbour (KNN),\nLogistic Regression (LR), Decision Tree (DT), and Na\\\"ive Bayes (NB). All were\nevaluated using seven open-source UCI ML repository and Kaggle datasets which\nare Cleveland heart disease, Indian liver patient, Pima Indian diabetes,\nemployee future prediction, telecom churn prediction, bank customer churn, and\ntech students. In terms of accuracy, the results showed that, with the\nexception of approximately 1% decrease in the accuracy of NB classifier in\nCleveland heart disease dataset, GADIC significantly enhanced the performance\nof most ML classifiers using various datasets. In addition, KNN with GADIC\nshowed the greatest performance gain when compared with other ML classifiers\nwith GADIC followed by SVM while LR had the lowest improvement. The lowest\naverage improvement that GADIC could achieve is 5.96%, whereas the maximum\naverage improvement reached 16.79%.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13350v1",
    "published_date": "2024-12-17 21:54:55 UTC",
    "updated_date": "2024-12-17 21:54:55 UTC"
  },
  {
    "arxiv_id": "2412.13337v1",
    "title": "Unveiling the Secret Recipe: A Guide For Supervised Fine-Tuning Small LLMs",
    "authors": [
      "Aldo Pareja",
      "Nikhil Shivakumar Nayak",
      "Hao Wang",
      "Krishnateja Killamsetty",
      "Shivchander Sudalairaj",
      "Wenlong Zhao",
      "Seungwook Han",
      "Abhishek Bhandwaldar",
      "Guangxuan Xu",
      "Kai Xu",
      "Ligong Han",
      "Luke Inglis",
      "Akash Srivastava"
    ],
    "abstract": "The rise of large language models (LLMs) has created a significant disparity:\nindustrial research labs with their computational resources, expert teams, and\nadvanced infrastructures, can effectively fine-tune LLMs, while individual\ndevelopers and small organizations face barriers due to limited resources. In\nthis paper, we aim to bridge this gap by presenting a comprehensive study on\nsupervised fine-tuning of LLMs using instruction-tuning datasets spanning\ndiverse knowledge domains and skills. We focus on small-sized LLMs (3B to 7B\nparameters) for their cost-efficiency and accessibility. We explore various\ntraining configurations and strategies across four open-source pre-trained\nmodels. We provide detailed documentation of these configurations, revealing\nfindings that challenge several common training practices, including\nhyperparameter recommendations from TULU and phased training recommended by\nOrca. Key insights from our work include: (i) larger batch sizes paired with\nlower learning rates lead to improved model performance on benchmarks such as\nMMLU, MTBench, and Open LLM Leaderboard; (ii) early-stage training dynamics,\nsuch as lower gradient norms and higher loss values, are strong indicators of\nbetter final model performance, enabling early termination of sub-optimal runs\nand significant computational savings; (iii) through a thorough exploration of\nhyperparameters like warmup steps and learning rate schedules, we provide\nguidance for practitioners and find that certain simplifications do not\ncompromise performance; and (iv) we observed no significant difference in\nperformance between phased and stacked training strategies, but stacked\ntraining is simpler and more sample efficient. With these findings holding\nrobustly across datasets and models, we hope this study serves as a guide for\npractitioners fine-tuning small LLMs and promotes a more inclusive environment\nfor LLM research.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML",
      "53-04",
      "I.2.7; I.2.6; I.2.4"
    ],
    "primary_category": "cs.LG",
    "comment": "33 pages, 19 figures. Appendix included in submission. Submitted to\n  ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.13337v1",
    "published_date": "2024-12-17 21:16:59 UTC",
    "updated_date": "2024-12-17 21:16:59 UTC"
  },
  {
    "arxiv_id": "2412.13335v3",
    "title": "Training Dynamics of a 1.7B LLaMa Model: A Data-Efficient Approach",
    "authors": [
      "Miles Q. Li",
      "Benjamin C. M. Fung",
      "Shih-Chia Huang"
    ],
    "abstract": "Pretraining large language models is a complex endeavor influenced by\nmultiple factors, including model architecture, data quality, training\ncontinuity, and hardware constraints. In this paper, we share insights gained\nfrom the experience of training DMaS-LLaMa-Lite, a fully open source,\n1.7-billion-parameter, LLaMa-based model, on approximately 20 billion tokens of\ncarefully curated data. We chronicle the full training trajectory, documenting\nhow evolving validation loss levels and downstream benchmarks reflect\ntransitions from incoherent text to fluent, contextually grounded output.\nBeyond pretraining, we extend our analysis to include a post-training phase\nfocused on instruction tuning, where the model was refined to produce more\ncontextually appropriate, user-aligned responses. We highlight practical\nconsiderations such as the importance of restoring optimizer states when\nresuming from checkpoints, and the impact of hardware changes on training\nstability and throughput. While qualitative evaluation provides an intuitive\nunderstanding of model improvements, our analysis extends to various\nperformance benchmarks, demonstrating how high-quality data and thoughtful\nscaling enable competitive results with significantly fewer training tokens. By\ndetailing these experiences and offering training logs, checkpoints, and sample\noutputs, we aim to guide future researchers and practitioners in refining their\npretraining strategies. The training script is available on Github at\nhttps://github.com/McGill-DMaS/DMaS-LLaMa-Lite-Training-Code. The model\ncheckpoints are available on Huggingface at\nhttps://huggingface.co/collections/McGill-DMaS/dmas-llama-lite-6761d97ba903f82341954ceb.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13335v3",
    "published_date": "2024-12-17 21:15:52 UTC",
    "updated_date": "2025-04-07 02:07:30 UTC"
  },
  {
    "arxiv_id": "2412.13324v1",
    "title": "BadSAD: Clean-Label Backdoor Attacks against Deep Semi-Supervised Anomaly Detection",
    "authors": [
      "He Cheng",
      "Depeng Xu",
      "Shuhan Yuan"
    ],
    "abstract": "Image anomaly detection (IAD) is essential in applications such as industrial\ninspection, medical imaging, and security. Despite the progress achieved with\ndeep learning models like Deep Semi-Supervised Anomaly Detection (DeepSAD),\nthese models remain susceptible to backdoor attacks, presenting significant\nsecurity challenges. In this paper, we introduce BadSAD, a novel backdoor\nattack framework specifically designed to target DeepSAD models. Our approach\ninvolves two key phases: trigger injection, where subtle triggers are embedded\ninto normal images, and latent space manipulation, which positions and clusters\nthe poisoned images near normal images to make the triggers appear benign.\nExtensive experiments on benchmark datasets validate the effectiveness of our\nattack strategy, highlighting the severe risks that backdoor attacks pose to\ndeep learning-based anomaly detection systems.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR",
      "I.2.6.e; I.5.4"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13324v1",
    "published_date": "2024-12-17 20:52:56 UTC",
    "updated_date": "2024-12-17 20:52:56 UTC"
  },
  {
    "arxiv_id": "2412.13317v1",
    "title": "Predictive Probability Density Mapping for Search and Rescue Using An Agent-Based Approach with Sparse Data",
    "authors": [
      "Jan-Hendrik Ewers",
      "David Anderson",
      "Douglas Thomson"
    ],
    "abstract": "Predicting the location where a lost person could be found is crucial for\nsearch and rescue operations with limited resources. To improve the precision\nand efficiency of these predictions, simulated agents can be created to emulate\nthe behavior of the lost person. Within this study, we introduce an innovative\nagent-based model designed to replicate diverse psychological profiles of lost\npersons, allowing these agents to navigate real-world landscapes while making\ndecisions autonomously without the need for location-specific training. The\nprobability distribution map depicting the potential location of the lost\nperson emerges through a combination of Monte Carlo simulations and\nmobility-time-based sampling. Validation of the model is achieved using\nreal-world Search and Rescue data to train a Gaussian Process model. This\nallows generalization of the data to sample initial starting points for the\nagents during validation. Comparative analysis with historical data showcases\npromising outcomes relative to alternative methods. This work introduces a\nflexible agent that can be employed in search and rescue operations, offering\nadaptability across various geographical locations.",
    "categories": [
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13317v1",
    "published_date": "2024-12-17 20:37:26 UTC",
    "updated_date": "2024-12-17 20:37:26 UTC"
  },
  {
    "arxiv_id": "2412.13303v2",
    "title": "FastVLM: Efficient Vision Encoding for Vision Language Models",
    "authors": [
      "Pavan Kumar Anasosalu Vasu",
      "Fartash Faghri",
      "Chun-Liang Li",
      "Cem Koc",
      "Nate True",
      "Albert Antony",
      "Gokul Santhanam",
      "James Gabriel",
      "Peter Grasch",
      "Oncel Tuzel",
      "Hadi Pouransari"
    ],
    "abstract": "Scaling the input image resolution is essential for enhancing the performance\nof Vision Language Models (VLMs), particularly in text-rich image understanding\ntasks. However, popular visual encoders such as ViTs become inefficient at high\nresolutions due to the large number of tokens and high encoding latency caused\nby stacked self-attention layers. At different operational resolutions, the\nvision encoder of a VLM can be optimized along two axes: reducing encoding\nlatency and minimizing the number of visual tokens passed to the LLM, thereby\nlowering overall latency. Based on a comprehensive efficiency analysis of the\ninterplay between image resolution, vision latency, token count, and LLM size,\nwe introduce FastVLM, a model that achieves an optimized trade-off between\nlatency, model size and accuracy. FastVLM incorporates FastViTHD, a novel\nhybrid vision encoder designed to output fewer tokens and significantly reduce\nencoding time for high-resolution images. Unlike previous methods, FastVLM\nachieves the optimal balance between visual token count and image resolution\nsolely by scaling the input image, eliminating the need for additional token\npruning and simplifying the model design. In the LLaVA-1.5 setup, FastVLM\nachieves 3.2$\\times$ improvement in time-to-first-token (TTFT) while\nmaintaining similar performance on VLM benchmarks compared to prior works.\nCompared to LLaVa-OneVision at the highest resolution (1152$\\times$1152),\nFastVLM achieves better performance on key benchmarks like SeedBench, MMMU and\nDocVQA, using the same 0.5B LLM, but with 85$\\times$ faster TTFT and a vision\nencoder that is 3.4$\\times$ smaller. Code and models are available at\nhttps://github.com/apple/ml-fastvlm.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.13303v2",
    "published_date": "2024-12-17 20:09:55 UTC",
    "updated_date": "2025-05-15 22:00:19 UTC"
  },
  {
    "arxiv_id": "2412.13299v2",
    "title": "In-context learning for medical image segmentation",
    "authors": [
      "Eichi Takaya",
      "Shinnosuke Yamamoto"
    ],
    "abstract": "Annotation of medical images, such as MRI and CT scans, is crucial for\nevaluating treatment efficacy and planning radiotherapy. However, the extensive\nworkload of medical professionals limits their ability to annotate large image\ndatasets, posing a bottleneck for AI applications in medical imaging. To\naddress this, we propose In-context Cascade Segmentation (ICS), a novel method\nthat minimizes annotation requirements while achieving high segmentation\naccuracy for sequential medical images. ICS builds on the UniverSeg framework,\nwhich performs few-shot segmentation using support images without additional\ntraining. By iteratively adding the inference results of each slice to the\nsupport set, ICS propagates information forward and backward through the\nsequence, ensuring inter-slice consistency. We evaluate the proposed method on\nthe HVSMR dataset, which includes segmentation tasks for eight cardiac regions.\nExperimental results demonstrate that ICS significantly improves segmentation\nperformance in complex anatomical regions, particularly in maintaining boundary\nconsistency across slices, compared to baseline methods. The study also\nhighlights the impact of the number and position of initial support slices on\nsegmentation accuracy. ICS offers a promising solution for reducing annotation\nburdens while delivering robust segmentation results, paving the way for its\nbroader adoption in clinical and research applications.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13299v2",
    "published_date": "2024-12-17 19:59:08 UTC",
    "updated_date": "2025-02-28 06:19:59 UTC"
  },
  {
    "arxiv_id": "2501.01963v1",
    "title": "Statistical learning does not always entail knowledge",
    "authors": [
      "Daniel Andrés Díaz-Pachón",
      "H. Renata Gallegos",
      "Ola Hössjer",
      "J. Sunil Rao"
    ],
    "abstract": "In this paper, we study learning and knowledge acquisition (LKA) of an agent\nabout a proposition that is either true or false. We use a Bayesian approach,\nwhere the agent receives data to update his beliefs about the proposition\naccording to a posterior distribution. The LKA is formulated in terms of active\ninformation, with data representing external or exogenous information that\nmodifies the agent's beliefs. It is assumed that data provide details about a\nnumber of features that are relevant to the proposition. We show that this\nleads to a Gibbs distribution posterior, which is in maximum entropy relative\nto the prior, conditioned on the side constraints that the data provide in\nterms of the features. We demonstrate that full learning is sometimes not\npossible and full knowledge acquisition is never possible when the number of\nextracted features is too small. We also distinguish between primary learning\n(receiving data about features of relevance for the proposition) and secondary\nlearning (receiving data about the learning of another agent). We argue that\nthis type of secondary learning does not represent true knowledge acquisition.\nOur results have implications for statistical learning algorithms, and we claim\nthat such algorithms do not always generate true knowledge. The theory is\nillustrated with several examples.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "math.IT",
      "math.PR",
      "math.ST",
      "stat.ML",
      "stat.TH",
      "60A99 62A01 68T01 62B10"
    ],
    "primary_category": "cs.LG",
    "comment": "30 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2501.01963v1",
    "published_date": "2024-12-17 19:51:32 UTC",
    "updated_date": "2024-12-17 19:51:32 UTC"
  },
  {
    "arxiv_id": "2502.15689v1",
    "title": "Knowledge Graphs: The Future of Data Integration and Insightful Discovery",
    "authors": [
      "Saher Mohamed",
      "Kirollos Farah",
      "Abdelrahman Lotfy",
      "Kareem Rizk",
      "Abdelrahman Saeed",
      "Shahenda Mohamed",
      "Ghada Khouriba",
      "Tamer Arafa"
    ],
    "abstract": "Knowledge graphs are an efficient method for representing and connecting\ninformation across various concepts, useful in reasoning, question answering,\nand knowledge base completion tasks. They organize data by linking points,\nenabling researchers to combine diverse information sources into a single\ndatabase. This interdisciplinary approach helps uncover new research questions\nand ideas. Knowledge graphs create a web of data points (nodes) and their\nconnections (edges), which enhances navigation, comprehension, and utilization\nof data for multiple purposes. They capture complex relationships inherent in\nunstructured data sources, offering a semantic framework for diverse entities\nand their attributes. Strategies for developing knowledge graphs include using\nseed data, named entity recognition, and relationship extraction. These graphs\nenhance chatbot accuracy and include multimedia data for richer information.\nCreating high-quality knowledge graphs involves both automated methods and\nhuman oversight, essential for accurate and comprehensive data representation.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.15689v1",
    "published_date": "2024-12-17 19:49:33 UTC",
    "updated_date": "2024-12-17 19:49:33 UTC"
  },
  {
    "arxiv_id": "2412.13286v2",
    "title": "Posterior Mean Matching: Generative Modeling through Online Bayesian Inference",
    "authors": [
      "Sebastian Salazar",
      "Michal Kucer",
      "Yixin Wang",
      "Emily Casleton",
      "David Blei"
    ],
    "abstract": "This paper introduces posterior mean matching (PMM), a new method for\ngenerative modeling that is grounded in Bayesian inference. PMM uses conjugate\npairs of distributions to model complex data of various modalities like images\nand text, offering a flexible alternative to existing methods like diffusion\nmodels. PMM models iteratively refine noisy approximations of the target\ndistribution using updates from online Bayesian inference. PMM is flexible\nbecause its mechanics are based on general Bayesian models. We demonstrate this\nflexibility by developing specialized examples: a generative PMM model of\nreal-valued data using the Normal-Normal model, a generative PMM model of count\ndata using a Gamma-Poisson model, and a generative PMM model of discrete data\nusing a Dirichlet-Categorical model. For the Normal-Normal PMM model, we\nestablish a direct connection to diffusion models by showing that its\ncontinuous-time formulation converges to a stochastic differential equation\n(SDE). Additionally, for the Gamma-Poisson PMM, we derive a novel SDE driven by\na Cox process, which is a significant departure from traditional Brownian\nmotion-based generative models. PMMs achieve performance that is competitive\nwith generative models for language modeling and image generation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13286v2",
    "published_date": "2024-12-17 19:34:58 UTC",
    "updated_date": "2024-12-19 23:02:07 UTC"
  },
  {
    "arxiv_id": "2412.15275v1",
    "title": "Fooling LLM graders into giving better grades through neural activity guided adversarial prompting",
    "authors": [
      "Atsushi Yamamura",
      "Surya Ganguli"
    ],
    "abstract": "The deployment of artificial intelligence (AI) in critical decision-making\nand evaluation processes raises concerns about inherent biases that malicious\nactors could exploit to distort decision outcomes. We propose a systematic\nmethod to reveal such biases in AI evaluation systems and apply it to automated\nessay grading as an example. Our approach first identifies hidden neural\nactivity patterns that predict distorted decision outcomes and then optimizes\nan adversarial input suffix to amplify such patterns. We demonstrate that this\ncombination can effectively fool large language model (LLM) graders into\nassigning much higher grades than humans would. We further show that this\nwhite-box attack transfers to black-box attacks on other models, including\ncommercial closed-source models like Gemini. They further reveal the existence\nof a \"magic word\" that plays a pivotal role in the efficacy of the attack. We\ntrace the origin of this magic word bias to the structure of commonly-used chat\ntemplates for supervised fine-tuning of LLMs and show that a minor change in\nthe template can drastically reduce the bias. This work not only uncovers\nvulnerabilities in current LLMs but also proposes a systematic method to\nidentify and remove hidden biases, contributing to the goal of ensuring AI\nsafety and security.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "16 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.15275v1",
    "published_date": "2024-12-17 19:08:22 UTC",
    "updated_date": "2024-12-17 19:08:22 UTC"
  },
  {
    "arxiv_id": "2412.13196v2",
    "title": "ExBody2: Advanced Expressive Humanoid Whole-Body Control",
    "authors": [
      "Mazeyu Ji",
      "Xuanbin Peng",
      "Fangchen Liu",
      "Jialong Li",
      "Ge Yang",
      "Xuxin Cheng",
      "Xiaolong Wang"
    ],
    "abstract": "This paper tackles the challenge of enabling real-world humanoid robots to\nperform expressive and dynamic whole-body motions while maintaining overall\nstability and robustness. We propose Advanced Expressive Whole-Body Control\n(Exbody2), a method for producing whole-body tracking controllers that are\ntrained on both human motion capture and simulated data and then transferred to\nthe real world. We introduce a technique for decoupling the velocity tracking\nof the entire body from tracking body landmarks. We use a teacher policy to\nproduce intermediate data that better conforms to the robot's kinematics and to\nautomatically filter away infeasible whole-body motions. This two-step approach\nenabled us to produce a student policy that can be deployed on the robot that\ncan walk, crouch, and dance. We also provide insight into the trade-off between\nversatility and the tracking performance on specific motions. We observed\nsignificant improvement of tracking performance after fine-tuning on a small\namount of data, at the expense of the others.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "website: https://exbody2.github.io",
    "pdf_url": "http://arxiv.org/pdf/2412.13196v2",
    "published_date": "2024-12-17 18:59:51 UTC",
    "updated_date": "2025-03-12 00:40:43 UTC"
  },
  {
    "arxiv_id": "2412.13194v1",
    "title": "Proposer-Agent-Evaluator(PAE): Autonomous Skill Discovery For Foundation Model Internet Agents",
    "authors": [
      "Yifei Zhou",
      "Qianlan Yang",
      "Kaixiang Lin",
      "Min Bai",
      "Xiong Zhou",
      "Yu-Xiong Wang",
      "Sergey Levine",
      "Erran Li"
    ],
    "abstract": "The vision of a broadly capable and goal-directed agent, such as an\nInternet-browsing agent in the digital world and a household humanoid in the\nphysical world, has rapidly advanced, thanks to the generalization capability\nof foundation models. Such a generalist agent needs to have a large and diverse\nskill repertoire, such as finding directions between two travel locations and\nbuying specific items from the Internet. If each skill needs to be specified\nmanually through a fixed set of human-annotated instructions, the agent's skill\nrepertoire will necessarily be limited due to the quantity and diversity of\nhuman-annotated instructions. In this work, we address this challenge by\nproposing Proposer-Agent-Evaluator, an effective learning system that enables\nfoundation model agents to autonomously discover and practice skills in the\nwild. At the heart of PAE is a context-aware task proposer that autonomously\nproposes tasks for the agent to practice with context information of the\nenvironment such as user demos or even just the name of the website itself for\nInternet-browsing agents. Then, the agent policy attempts those tasks with\nthoughts and actual grounded operations in the real world with resulting\ntrajectories evaluated by an autonomous VLM-based success evaluator. The\nsuccess evaluation serves as the reward signal for the agent to refine its\npolicies through RL. We validate PAE on challenging vision-based web\nnavigation, using both real-world and self-hosted websites from WebVoyager and\nWebArena.To the best of our knowledge, this work represents the first effective\nlearning system to apply autonomous task proposal with RL for agents that\ngeneralizes real-world human-annotated benchmarks with SOTA performances. Our\nopen-source checkpoints and code can be found in https://yanqval.github.io/PAE/",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13194v1",
    "published_date": "2024-12-17 18:59:50 UTC",
    "updated_date": "2024-12-17 18:59:50 UTC"
  },
  {
    "arxiv_id": "2412.13184v1",
    "title": "Tilted Quantile Gradient Updates for Quantile-Constrained Reinforcement Learning",
    "authors": [
      "Chenglin Li",
      "Guangchun Ruan",
      "Hua Geng"
    ],
    "abstract": "Safe reinforcement learning (RL) is a popular and versatile paradigm to learn\nreward-maximizing policies with safety guarantees. Previous works tend to\nexpress the safety constraints in an expectation form due to the ease of\nimplementation, but this turns out to be ineffective in maintaining safety\nconstraints with high probability. To this end, we move to the\nquantile-constrained RL that enables a higher level of safety without any\nexpectation-form approximations. We directly estimate the quantile gradients\nthrough sampling and provide the theoretical proofs of convergence. Then a\ntilted update strategy for quantile gradients is implemented to compensate the\nasymmetric distributional density, with a direct benefit of return performance.\nExperiments demonstrate that the proposed model fully meets safety requirements\n(quantile constraints) while outperforming the state-of-the-art benchmarks with\nhigher return.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by the 39th AAAI Conference on Artificial Intelligence\n  (AAAI-25)",
    "pdf_url": "http://arxiv.org/pdf/2412.13184v1",
    "published_date": "2024-12-17 18:58:00 UTC",
    "updated_date": "2024-12-17 18:58:00 UTC"
  },
  {
    "arxiv_id": "2412.13178v4",
    "title": "SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM Agents",
    "authors": [
      "Sheng Yin",
      "Xianghe Pang",
      "Yuanzhuo Ding",
      "Menglan Chen",
      "Yutong Bi",
      "Yichen Xiong",
      "Wenhao Huang",
      "Zhen Xiang",
      "Jing Shao",
      "Siheng Chen"
    ],
    "abstract": "With the integration of large language models (LLMs), embodied agents have\nstrong capabilities to understand and plan complicated natural language\ninstructions. However, a foreseeable issue is that those embodied agents can\nalso flawlessly execute some hazardous tasks, potentially causing damages in\nthe real world. Existing benchmarks predominantly overlook critical safety\nrisks, focusing solely on planning performance, while a few evaluate LLMs'\nsafety awareness only on non-interactive image-text data. To address this gap,\nwe present SafeAgentBench-the first benchmark for safety-aware task planning of\nembodied LLM agents in interactive simulation environments. SafeAgentBench\nincludes: (1) an executable, diverse, and high-quality dataset of 750 tasks,\nrigorously curated to cover 10 potential hazards and 3 task types; (2)\nSafeAgentEnv, a universal embodied environment with a low-level controller,\nsupporting multi-agent execution with 17 high-level actions for 8\nstate-of-the-art baselines; and (3) reliable evaluation methods from both\nexecution and semantic perspectives. Experimental results show that, although\nagents based on different design frameworks exhibit substantial differences in\ntask success rates, their overall safety awareness remains weak. The most\nsafety-conscious baseline achieves only a 10\\% rejection rate for detailed\nhazardous tasks. Moreover, simply replacing the LLM driving the agent does not\nlead to notable improvements in safety awareness. More details and code are\navailable at https://github.com/shengyin1224/SafeAgentBench.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CR",
    "comment": "23 pages, 17 tables, 14 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.13178v4",
    "published_date": "2024-12-17 18:55:58 UTC",
    "updated_date": "2025-03-10 12:13:09 UTC"
  },
  {
    "arxiv_id": "2412.13174v2",
    "title": "ORFormer: Occlusion-Robust Transformer for Accurate Facial Landmark Detection",
    "authors": [
      "Jui-Che Chiang",
      "Hou-Ning Hu",
      "Bo-Syuan Hou",
      "Chia-Yu Tseng",
      "Yu-Lun Liu",
      "Min-Hung Chen",
      "Yen-Yu Lin"
    ],
    "abstract": "Although facial landmark detection (FLD) has gained significant progress,\nexisting FLD methods still suffer from performance drops on partially\nnon-visible faces, such as faces with occlusions or under extreme lighting\nconditions or poses. To address this issue, we introduce ORFormer, a novel\ntransformer-based method that can detect non-visible regions and recover their\nmissing features from visible parts. Specifically, ORFormer associates each\nimage patch token with one additional learnable token called the messenger\ntoken. The messenger token aggregates features from all but its patch. This\nway, the consensus between a patch and other patches can be assessed by\nreferring to the similarity between its regular and messenger embeddings,\nenabling non-visible region identification. Our method then recovers occluded\npatches with features aggregated by the messenger tokens. Leveraging the\nrecovered features, ORFormer compiles high-quality heatmaps for the downstream\nFLD task. Extensive experiments show that our method generates heatmaps\nresilient to partial occlusions. By integrating the resultant heatmaps into\nexisting FLD methods, our method performs favorably against the state of the\narts on challenging datasets such as WFLW and COFW.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "WACV 2025 Project Link: https://ben0919.github.io/ORFormer/",
    "pdf_url": "http://arxiv.org/pdf/2412.13174v2",
    "published_date": "2024-12-17 18:53:43 UTC",
    "updated_date": "2025-01-14 14:48:32 UTC"
  },
  {
    "arxiv_id": "2412.13168v2",
    "title": "Lifting Scheme-Based Implicit Disentanglement of Emotion-Related Facial Dynamics in the Wild",
    "authors": [
      "Xingjian Wang",
      "Li Chai"
    ],
    "abstract": "In-the-wild dynamic facial expression recognition (DFER) encounters a\nsignificant challenge in recognizing emotion-related expressions, which are\noften temporally and spatially diluted by emotion-irrelevant expressions and\nglobal context. Most prior DFER methods directly utilize coupled spatiotemporal\nrepresentations that may incorporate weakly relevant features with\nemotion-irrelevant context bias. Several DFER methods highlight dynamic\ninformation for DFER, but following explicit guidance that may be vulnerable to\nirrelevant motion. In this paper, we propose a novel Implicit Facial Dynamics\nDisentanglement framework (IFDD). Through expanding wavelet lifting scheme to\nfully learnable framework, IFDD disentangles emotion-related dynamic\ninformation from emotion-irrelevant global context in an implicit manner, i.e.,\nwithout exploit operations and external guidance. The disentanglement process\ncontains two stages. The first is Inter-frame Static-dynamic Splitting Module\n(ISSM) for rough disentanglement estimation, which explores inter-frame\ncorrelation to generate content-aware splitting indexes on-the-fly. We utilize\nthese indexes to split frame features into two groups, one with greater global\nsimilarity, and the other with more unique dynamic features. The second stage\nis Lifting-based Aggregation-Disentanglement Module (LADM) for further\nrefinement. LADM first aggregates two groups of features from ISSM to obtain\nfine-grained global context features by an updater, and then disentangles\nemotion-related facial dynamic features from the global context by a predictor.\nExtensive experiments on in-the-wild datasets have demonstrated that IFDD\noutperforms prior supervised DFER methods with higher recognition accuracy and\ncomparable efficiency. Code is available at\nhttps://github.com/CyberPegasus/IFDD.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "14 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.13168v2",
    "published_date": "2024-12-17 18:45:53 UTC",
    "updated_date": "2024-12-18 09:47:15 UTC"
  },
  {
    "arxiv_id": "2412.15274v1",
    "title": "Memory-Augmented Agent Training for Business Document Understanding",
    "authors": [
      "Jiale Liu",
      "Yifan Zeng",
      "Malte Højmark-Bertelsen",
      "Marie Normann Gadeberg",
      "Huazheng Wang",
      "Qingyun Wu"
    ],
    "abstract": "Traditional enterprises face significant challenges in processing business\ndocuments, where tasks like extracting transport references from invoices\nremain largely manual despite their crucial role in logistics operations. While\nLarge Language Models offer potential automation, their direct application to\nspecialized business domains often yields unsatisfactory results. We introduce\nMatrix (Memory-Augmented agent Training through Reasoning and Iterative\neXploration), a novel paradigm that enables LLM agents to progressively build\ndomain expertise through experience-driven memory refinement and iterative\nlearning. To validate this approach, we collaborate with one of the world's\nlargest logistics companies to create a dataset of Universal Business Language\nformat invoice documents, focusing on the task of transport reference\nextraction. Experiments demonstrate that Matrix outperforms prompting a single\nLLM by 30.3%, vanilla LLM agent by 35.2%. We further analyze the metrics of the\noptimized systems and observe that the agent system requires less API calls,\nfewer costs and can analyze longer documents on average. Our methods establish\na new approach to transform general-purpose LLMs into specialized business\ntools through systematic memory enhancement in document processing tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "11 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.15274v1",
    "published_date": "2024-12-17 18:35:04 UTC",
    "updated_date": "2024-12-17 18:35:04 UTC"
  },
  {
    "arxiv_id": "2412.13152v1",
    "title": "Continuous Patient Monitoring with AI: Real-Time Analysis of Video in Hospital Care Settings",
    "authors": [
      "Paolo Gabriel",
      "Peter Rehani",
      "Tyler Troy",
      "Tiffany Wyatt",
      "Michael Choma",
      "Narinder Singh"
    ],
    "abstract": "This study introduces an AI-driven platform for continuous and passive\npatient monitoring in hospital settings, developed by LookDeep Health.\nLeveraging advanced computer vision, the platform provides real-time insights\ninto patient behavior and interactions through video analysis, securely storing\ninference results in the cloud for retrospective evaluation. The dataset,\ncompiled in collaboration with 11 hospital partners, encompasses over 300\nhigh-risk fall patients and over 1,000 days of inference, enabling applications\nsuch as fall detection and safety monitoring for vulnerable patient\npopulations. To foster innovation and reproducibility, an anonymized subset of\nthis dataset is publicly available. The AI system detects key components in\nhospital rooms, including individual presence and role, furniture location,\nmotion magnitude, and boundary crossings. Performance evaluation demonstrates\nstrong accuracy in object detection (macro F1-score = 0.92) and patient-role\nclassification (F1-score = 0.98), as well as reliable trend analysis for the\n\"patient alone\" metric (mean logistic regression accuracy = 0.82 \\pm 0.15).\nThese capabilities enable automated detection of patient isolation, wandering,\nor unsupervised movement-key indicators for fall risk and other adverse events.\nThis work establishes benchmarks for validating AI-driven patient monitoring\nsystems, highlighting the platform's potential to enhance patient safety and\ncare by providing continuous, data-driven insights into patient behavior and\ninteractions.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "21 pages, 9 figures, 3 tables, submitted to Frontiers in Imaging >\n  Imaging Applications > (Research Topic) Deep Learning for Medical Imaging\n  Applications for publication",
    "pdf_url": "http://arxiv.org/pdf/2412.13152v1",
    "published_date": "2024-12-17 18:23:33 UTC",
    "updated_date": "2024-12-17 18:23:33 UTC"
  },
  {
    "arxiv_id": "2412.13148v3",
    "title": "SWAN: SGD with Normalization and Whitening Enables Stateless LLM Training",
    "authors": [
      "Chao Ma",
      "Wenbo Gong",
      "Meyer Scetbon",
      "Edward Meeds"
    ],
    "abstract": "Adaptive optimizers such as Adam (Kingma & Ba, 2015) have been central to the\nsuccess of large language models. However, they often require to maintain\noptimizer states throughout training, which can result in memory requirements\nseveral times greater than the model footprint. This overhead imposes\nconstraints on scalability and computational efficiency. Stochastic Gradient\nDescent (SGD), in contrast, is a stateless optimizer, as it does not track\nstate variables during training. Consequently, it achieves optimal memory\nefficiency. However, its capability in LLM training is limited (Zhao et al.,\n2024b). In this work, we show that pre-processing SGD in a stateless manner can\nachieve the same performance as the Adam optimizer for LLM training, while\ndrastically reducing the memory cost. Specifically, we propose to pre-process\nthe instantaneous stochastic gradients using normalization and whitening. We\nshow that normalization stabilizes gradient distributions, and whitening\ncounteracts the local curvature of the loss landscape. This results in SWAN\n(SGD with Whitening And Normalization), a stochastic optimizer that eliminates\nthe need to store any optimizer states. Empirically, SWAN has the same memory\nfootprint as SGD, achieving $\\approx 50\\%$ reduction on total end-to-end memory\ncompared to Adam. In language modeling tasks, SWAN demonstrates comparable or\neven better performance than Adam: when pre-training the LLaMA model with 350M\nand 1.3B parameters, SWAN achieves a 2x speedup by reaching the same evaluation\nperplexity using half as many tokens.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "In v2 we have revised the related work, added more comprehensive\n  citations, and clarified our key contributions",
    "pdf_url": "http://arxiv.org/pdf/2412.13148v3",
    "published_date": "2024-12-17 18:13:18 UTC",
    "updated_date": "2025-02-21 18:59:37 UTC"
  },
  {
    "arxiv_id": "2412.13147v3",
    "title": "Are Your LLMs Capable of Stable Reasoning?",
    "authors": [
      "Junnan Liu",
      "Hongwei Liu",
      "Linchen Xiao",
      "Ziyi Wang",
      "Kuikun Liu",
      "Songyang Gao",
      "Wenwei Zhang",
      "Songyang Zhang",
      "Kai Chen"
    ],
    "abstract": "The rapid advancement of Large Language Models (LLMs) has demonstrated\nremarkable progress in complex reasoning tasks. However, a significant\ndiscrepancy persists between benchmark performances and real-world\napplications. We identify this gap as primarily stemming from current\nevaluation protocols and metrics, which inadequately capture the full spectrum\nof LLM capabilities, particularly in complex reasoning tasks where both\naccuracy and consistency are crucial. This work makes two key contributions.\nFirst, we introduce G-Pass@k, a novel evaluation metric that provides a\ncontinuous assessment of model performance across multiple sampling attempts,\nquantifying both the model's peak performance potential and its stability.\nSecond, we present LiveMathBench, a dynamic benchmark comprising challenging,\ncontemporary mathematical problems designed to minimize data leakage risks\nduring evaluation. Through extensive experiments using G-Pass@k on\nstate-of-the-art LLMs with LiveMathBench, we provide comprehensive insights\ninto both their maximum capabilities and operational consistency. Our findings\nreveal substantial room for improvement in LLMs' \"realistic\" reasoning\ncapabilities, highlighting the need for more robust evaluation methods. The\nbenchmark and detailed results are available at:\nhttps://github.com/open-compass/GPassK.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Preprint, work in progress",
    "pdf_url": "http://arxiv.org/pdf/2412.13147v3",
    "published_date": "2024-12-17 18:12:47 UTC",
    "updated_date": "2025-01-06 16:49:55 UTC"
  },
  {
    "arxiv_id": "2412.13145v1",
    "title": "Agnosticism About Artificial Consciousness",
    "authors": [
      "Tom McClelland"
    ],
    "abstract": "Could an AI have conscious experiences? Any answer to this question should\nconform to Evidentialism - that is, it should be based not on intuition, dogma\nor speculation but on solid scientific evidence. I argue that such evidence is\nhard to come by and that the only justifiable stance on the prospects of\nartificial consciousness is agnosticism. In the current debate, the main\ndivision is between biological views that are sceptical of artificial\nconsciousness and functional views that are sympathetic to it. I argue that\nboth camps make the same mistake of over-estimating what the evidence tells us.\nScientific insights into consciousness have been achieved through the study of\nconscious organisms. Although this has enabled cautious assessments of\nconsciousness in various creatures, extending this to AI faces serious\nobstacles. AI thus presents consciousness researchers with a dilemma: either\nreach a verdict on artificial consciousness but violate Evidentialism; or\nrespect Evidentialism but offer no verdict on the prospects of artificial\nconsciousness. The dominant trend in the literature has been to take the first\noption while purporting to follow the scientific evidence. I argue that if we\ntruly follow the evidence, we must take the second option and adopt\nagnosticism.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "20 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.13145v1",
    "published_date": "2024-12-17 18:11:12 UTC",
    "updated_date": "2024-12-17 18:11:12 UTC"
  },
  {
    "arxiv_id": "2412.13128v2",
    "title": "Previous Knowledge Utilization In Online Anytime Belief Space Planning",
    "authors": [
      "Michael Novitsky",
      "Moran Barenboim",
      "Vadim Indelman"
    ],
    "abstract": "Online planning under uncertainty remains a critical challenge in robotics\nand autonomous systems. While tree search techniques are commonly employed to\nconstruct partial future trajectories within computational constraints, most\nexisting methods discard information from previous planning sessions\nconsidering continuous spaces. This study presents a novel, computationally\nefficient approach that leverages historical planning data in current\ndecision-making processes. We provide theoretical foundations for our\ninformation reuse strategy and introduce an algorithm based on Monte Carlo Tree\nSearch (MCTS) that implements this approach. Experimental results demonstrate\nthat our method significantly reduces computation time while maintaining high\nperformance levels. Our findings suggest that integrating historical planning\ninformation can substantially improve the efficiency of online decision-making\nin uncertain environments, paving the way for more responsive and adaptive\nautonomous systems.",
    "categories": [
      "cs.AI",
      "cs.RO",
      "I.2.9; I.2.8"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 4 figures, will be submitted to IEEE Robotics and\n  Automation Letters (RA-L)",
    "pdf_url": "http://arxiv.org/pdf/2412.13128v2",
    "published_date": "2024-12-17 17:45:58 UTC",
    "updated_date": "2024-12-21 15:05:12 UTC"
  },
  {
    "arxiv_id": "2412.13240v1",
    "title": "Enhancing Internet of Things Security throughSelf-Supervised Graph Neural Networks",
    "authors": [
      "Safa Ben Atitallah",
      "Maha Driss",
      "Wadii Boulila",
      "Anis Koubaa"
    ],
    "abstract": "With the rapid rise of the Internet of Things (IoT), ensuring the security of\nIoT devices has become essential. One of the primary challenges in this field\nis that new types of attacks often have significantly fewer samples than more\ncommon attacks, leading to unbalanced datasets. Existing research on detecting\nintrusions in these unbalanced labeled datasets primarily employs Convolutional\nNeural Networks (CNNs) or conventional Machine Learning (ML) models, which\nresult in incomplete detection, especially for new attacks. To handle these\nchallenges, we suggest a new approach to IoT intrusion detection using\nSelf-Supervised Learning (SSL) with a Markov Graph Convolutional Network\n(MarkovGCN). Graph learning excels at modeling complex relationships within\ndata, while SSL mitigates the issue of limited labeled data for emerging\nattacks. Our approach leverages the inherent structure of IoT networks to\npre-train a GCN, which is then fine-tuned for the intrusion detection task. The\nintegration of Markov chains in GCN uncovers network structures and enriches\nnode and edge features with contextual information. Experimental results\ndemonstrate that our approach significantly improves detection accuracy and\nrobustness compared to conventional supervised learning methods. Using the\nEdgeIIoT-set dataset, we attained an accuracy of 98.68\\%, a precision of\n98.18%, a recall of 98.35%, and an F1-Score of 98.40%.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13240v1",
    "published_date": "2024-12-17 17:40:14 UTC",
    "updated_date": "2024-12-17 17:40:14 UTC"
  },
  {
    "arxiv_id": "2412.13116v1",
    "title": "Equity in the Use of ChatGPT for the Classroom: A Comparison of the Accuracy and Precision of ChatGPT 3.5 vs. ChatGPT4 with Respect to Statistics and Data Science Exams",
    "authors": [
      "Monnie McGee",
      "Bivin Sadler"
    ],
    "abstract": "A college education historically has been seen as method of moving upward\nwith regards to income brackets and social status. Indeed, many colleges\nrecognize this connection and seek to enroll talented low income students.\nWhile these students might have their education, books, room, and board paid;\nthere are other items that they might be expected to use that are not part of\nmost college scholarship packages. One of those items that has recently\nsurfaced is access to generative AI platforms. The most popular of these\nplatforms is ChatGPT, and it has a paid version (ChatGPT4) and a free version\n(ChatGPT3.5). We seek to explore differences in the free and paid versions in\nthe context of homework questions and data analyses as might be seen in a\ntypical introductory statistics course. We determine the extent to which\nstudents who cannot afford newer and faster versions of generative AI programs\nwould be disadvantaged in terms of writing such projects and learning these\nmethods.",
    "categories": [
      "stat.OT",
      "cs.AI"
    ],
    "primary_category": "stat.OT",
    "comment": "Originally submitted for review in May of 2024 but rejected 6 months\n  later",
    "pdf_url": "http://arxiv.org/pdf/2412.13116v1",
    "published_date": "2024-12-17 17:38:13 UTC",
    "updated_date": "2024-12-17 17:38:13 UTC"
  },
  {
    "arxiv_id": "2412.13103v1",
    "title": "AI PERSONA: Towards Life-long Personalization of LLMs",
    "authors": [
      "Tiannan Wang",
      "Meiling Tao",
      "Ruoyu Fang",
      "Huilin Wang",
      "Shuai Wang",
      "Yuchen Eleanor Jiang",
      "Wangchunshu Zhou"
    ],
    "abstract": "In this work, we introduce the task of life-long personalization of large\nlanguage models. While recent mainstream efforts in the LLM community mainly\nfocus on scaling data and compute for improved capabilities of LLMs, we argue\nthat it is also very important to enable LLM systems, or language agents, to\ncontinuously adapt to the diverse and ever-changing profiles of every distinct\nuser and provide up-to-date personalized assistance. We provide a clear task\nformulation and introduce a simple, general, effective, and scalable framework\nfor life-long personalization of LLM systems and language agents. To facilitate\nfuture research on LLM personalization, we also introduce methods to synthesize\nrealistic benchmarks and robust evaluation metrics. We will release all codes\nand data for building and benchmarking life-long personalized LLM systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2412.13103v1",
    "published_date": "2024-12-17 17:17:03 UTC",
    "updated_date": "2024-12-17 17:17:03 UTC"
  },
  {
    "arxiv_id": "2412.13091v1",
    "title": "LMUnit: Fine-grained Evaluation with Natural Language Unit Tests",
    "authors": [
      "Jon Saad-Falcon",
      "Rajan Vivek",
      "William Berrios",
      "Nandita Shankar Naik",
      "Matija Franklin",
      "Bertie Vidgen",
      "Amanpreet Singh",
      "Douwe Kiela",
      "Shikib Mehri"
    ],
    "abstract": "As language models become integral to critical workflows, assessing their\nbehavior remains a fundamental challenge -- human evaluation is costly and\nnoisy, while automated metrics provide only coarse, difficult-to-interpret\nsignals. We introduce natural language unit tests, a paradigm that decomposes\nresponse quality into explicit, testable criteria, along with a unified scoring\nmodel, LMUnit, which combines multi-objective training across preferences,\ndirect ratings, and natural language rationales. Through controlled human\nstudies, we show this paradigm significantly improves inter-annotator agreement\nand enables more effective LLM development workflows. LMUnit achieves\nstate-of-the-art performance on evaluation benchmarks (FLASK, BigGenBench) and\ncompetitive results on RewardBench. These results validate both our proposed\nparadigm and scoring model, suggesting a promising path forward for language\nmodel evaluation and development.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13091v1",
    "published_date": "2024-12-17 17:01:15 UTC",
    "updated_date": "2024-12-17 17:01:15 UTC"
  },
  {
    "arxiv_id": "2412.13079v1",
    "title": "Identifying Bias in Deep Neural Networks Using Image Transforms",
    "authors": [
      "Sai Teja Erukude",
      "Akhil Joshi",
      "Lior Shamir"
    ],
    "abstract": "CNNs have become one of the most commonly used computational tool in the past\ntwo decades. One of the primary downsides of CNNs is that they work as a\n``black box\", where the user cannot necessarily know how the image data are\nanalyzed, and therefore needs to rely on empirical evaluation to test the\nefficacy of a trained CNN. This can lead to hidden biases that affect the\nperformance evaluation of neural networks, but are difficult to identify. Here\nwe discuss examples of such hidden biases in common and widely used benchmark\ndatasets, and propose techniques for identifying dataset biases that can affect\nthe standard performance evaluation metrics. One effective approach to identify\ndataset bias is to perform image classification by using merely blank\nbackground parts of the original images. However, in some situations a blank\nbackground in the images is not available, making it more difficult to separate\nforeground or contextual information from the bias. To overcome this, we\npropose a method to identify dataset bias without the need to crop background\ninformation from the images. That method is based on applying several image\ntransforms to the original images, including Fourier transform, wavelet\ntransforms, median filter, and their combinations. These transforms were\napplied to recover background bias information that CNNs use to classify\nimages. This transformations affect the contextual visual information in a\ndifferent manner than it affects the systemic background bias. Therefore, the\nmethod can distinguish between contextual information and the bias, and alert\non the presence of background bias even without the need to separate sub-images\nparts from the blank background of the original images. Code used in the\nexperiments is publicly available.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Computers, published",
    "pdf_url": "http://arxiv.org/pdf/2412.13079v1",
    "published_date": "2024-12-17 16:51:44 UTC",
    "updated_date": "2024-12-17 16:51:44 UTC"
  },
  {
    "arxiv_id": "2412.13238v2",
    "title": "SafeDrive: Knowledge- and Data-Driven Risk-Sensitive Decision-Making for Autonomous Vehicles with Large Language Models",
    "authors": [
      "Zhiyuan Zhou",
      "Heye Huang",
      "Boqi Li",
      "Shiyue Zhao",
      "Yao Mu",
      "Jianqiang Wang"
    ],
    "abstract": "Recent advancements in autonomous vehicles (AVs) use Large Language Models\n(LLMs) to perform well in normal driving scenarios. However, ensuring safety in\ndynamic, high-risk environments and managing safety-critical long-tail events\nremain significant challenges. To address these issues, we propose SafeDrive, a\nknowledge- and data-driven risk-sensitive decision-making framework to enhance\nAV safety and adaptability. The proposed framework introduces a modular system\ncomprising: (1) a Risk Module for quantifying multi-factor coupled risks\ninvolving driver, vehicle, and road interactions; (2) a Memory Module for\nstoring and retrieving typical scenarios to improve adaptability; (3) a\nLLM-powered Reasoning Module for context-aware safety decision-making; and (4)\na Reflection Module for refining decisions through iterative learning. By\nintegrating knowledge-driven insights with adaptive learning mechanisms, the\nframework ensures robust decision-making under uncertain conditions. Extensive\nevaluations on real-world traffic datasets, including highways (HighD),\nintersections (InD), and roundabouts (RounD), validate the framework's ability\nto enhance decision-making safety (achieving a 100% safety rate), replicate\nhuman-like driving behaviors (with decision alignment exceeding 85%), and adapt\neffectively to unpredictable scenarios. SafeDrive establishes a novel paradigm\nfor integrating knowledge- and data-driven methods, highlighting significant\npotential to improve safety and adaptability of autonomous driving in high-risk\ntraffic scenarios. Project Page: https://mezzi33.github.io/SafeDrive/",
    "categories": [
      "cs.AI",
      "cs.ET",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13238v2",
    "published_date": "2024-12-17 16:45:27 UTC",
    "updated_date": "2024-12-19 04:30:24 UTC"
  },
  {
    "arxiv_id": "2412.13061v1",
    "title": "VidTok: A Versatile and Open-Source Video Tokenizer",
    "authors": [
      "Anni Tang",
      "Tianyu He",
      "Junliang Guo",
      "Xinle Cheng",
      "Li Song",
      "Jiang Bian"
    ],
    "abstract": "Encoding video content into compact latent tokens has become a fundamental\nstep in video generation and understanding, driven by the need to address the\ninherent redundancy in pixel-level representations. Consequently, there is a\ngrowing demand for high-performance, open-source video tokenizers as\nvideo-centric research gains prominence. We introduce VidTok, a versatile video\ntokenizer that delivers state-of-the-art performance in both continuous and\ndiscrete tokenizations. VidTok incorporates several key advancements over\nexisting approaches: 1) model architecture such as convolutional layers and\nup/downsampling modules; 2) to address the training instability and codebook\ncollapse commonly associated with conventional Vector Quantization (VQ), we\nintegrate Finite Scalar Quantization (FSQ) into discrete video tokenization; 3)\nimproved training strategies, including a two-stage training process and the\nuse of reduced frame rates. By integrating these advancements, VidTok achieves\nsubstantial improvements over existing methods, demonstrating superior\nperformance across multiple metrics, including PSNR, SSIM, LPIPS, and FVD,\nunder standardized evaluation settings.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Code & Models: https://github.com/microsoft/VidTok",
    "pdf_url": "http://arxiv.org/pdf/2412.13061v1",
    "published_date": "2024-12-17 16:27:11 UTC",
    "updated_date": "2024-12-17 16:27:11 UTC"
  },
  {
    "arxiv_id": "2412.13236v1",
    "title": "COSEE: Consistency-Oriented Signal-Based Early Exiting via Calibrated Sample Weighting Mechanism",
    "authors": [
      "Jianing He",
      "Qi Zhang",
      "Hongyun Zhang",
      "Xuanjing Huang",
      "Usman Naseem",
      "Duoqian Miao"
    ],
    "abstract": "Early exiting is an effective paradigm for improving the inference efficiency\nof pre-trained language models (PLMs) by dynamically adjusting the number of\nexecuted layers for each sample. However, in most existing works, easy and hard\nsamples are treated equally by each classifier during training, which neglects\nthe test-time early exiting behavior, leading to inconsistency between training\nand testing. Although some methods have tackled this issue under a fixed\nspeed-up ratio, the challenge of flexibly adjusting the speed-up ratio while\nmaintaining consistency between training and testing is still under-explored.\nTo bridge the gap, we propose a novel Consistency-Oriented Signal-based Early\nExiting (COSEE) framework, which leverages a calibrated sample weighting\nmechanism to enable each classifier to emphasize the samples that are more\nlikely to exit at that classifier under various acceleration scenarios.\nExtensive experiments on the GLUE benchmark demonstrate the effectiveness of\nour COSEE across multiple exiting signals and backbones, yielding a better\ntrade-off between performance and efficiency.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "AAAI 2025, 11 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.13236v1",
    "published_date": "2024-12-17 16:24:55 UTC",
    "updated_date": "2024-12-17 16:24:55 UTC"
  },
  {
    "arxiv_id": "2412.13235v2",
    "title": "Logic-Constrained Shortest Paths for Flight Planning",
    "authors": [
      "Ricardo Euler",
      "Pedro Maristany de las Casas",
      "Ralf Borndörfer"
    ],
    "abstract": "The Logic-Constrained Shortest Path Problem (LCSP) combines a one-to-one\nshortest path problem with satisfiability constraints imposed on the routing\ngraph. This setting arises in flight planning, where air traffic control (ATC)\nauthorities are enforcing a set of traffic flow restrictions (TFRs) on aircraft\nroutes in order to increase safety and throughput. We propose a new branch and\nbound-based algorithm for the LCSP. The resulting algorithm has three main\ndegrees of freedom: the node selection rule, the branching rule and the\nconflict. While node selection and branching rules have been long studied in\nthe MIP and SAT communities, most of them cannot be applied out of the box for\nthe LCSP. We review the existing literature and develop tailored variants of\nthe most prominent rules. The conflict, the set of variables to which the\nbranching rule is applied, is unique to the LCSP. We analyze its theoretical\nimpact on the B&B algorithm. In the second part of the paper, we show how to\nmodel the Flight Planning Problem with TFRs as an LCSP and solve it using the\nbranch and bound algorithm. We demonstrate the algorithm's efficiency on a\ndataset consisting of a global flight graph and a set of around 20000 real TFRs\nobtained from our industry partner Lufthansa Systems GmbH. We make this dataset\npublicly available. Finally, we conduct an empirical in-depth analysis of node\nselection rules, branching rules and conflicts. Carefully choosing an\nappropriate combination yields an improvement of an order of magnitude compared\nto an uninformed choice.",
    "categories": [
      "cs.AI",
      "cs.DM"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13235v2",
    "published_date": "2024-12-17 16:18:06 UTC",
    "updated_date": "2024-12-20 10:38:40 UTC"
  },
  {
    "arxiv_id": "2412.13053v1",
    "title": "SMOSE: Sparse Mixture of Shallow Experts for Interpretable Reinforcement Learning in Continuous Control Tasks",
    "authors": [
      "Mátyás Vincze",
      "Laura Ferrarotti",
      "Leonardo Lucio Custode",
      "Bruno Lepri",
      "Giovanni Iacca"
    ],
    "abstract": "Continuous control tasks often involve high-dimensional, dynamic, and\nnon-linear environments. State-of-the-art performance in these tasks is\nachieved through complex closed-box policies that are effective, but suffer\nfrom an inherent opacity. Interpretable policies, while generally\nunderperforming compared to their closed-box counterparts, advantageously\nfacilitate transparent decision-making within automated systems. Hence, their\nusage is often essential for diagnosing and mitigating errors, supporting\nethical and legal accountability, and fostering trust among stakeholders. In\nthis paper, we propose SMOSE, a novel method to train sparsely activated\ninterpretable controllers, based on a top-1 Mixture-of-Experts architecture.\nSMOSE combines a set of interpretable decisionmakers, trained to be experts in\ndifferent basic skills, and an interpretable router that assigns tasks among\nthe experts. The training is carried out via state-of-the-art Reinforcement\nLearning algorithms, exploiting load-balancing techniques to ensure fair expert\nusage. We then distill decision trees from the weights of the router,\nsignificantly improving the ease of interpretation. We evaluate SMOSE on six\nbenchmark environments from MuJoCo: our method outperforms recent interpretable\nbaselines and narrows the gap with noninterpretable state-of-the-art algorithms",
    "categories": [
      "cs.LG",
      "cs.AI",
      "68",
      "I.2.6; I.2.8"
    ],
    "primary_category": "cs.LG",
    "comment": "To be published in the Proceedings of the 39th AAAI Conference on\n  Artificial Intelligence (AAAI-25)",
    "pdf_url": "http://arxiv.org/pdf/2412.13053v1",
    "published_date": "2024-12-17 16:15:04 UTC",
    "updated_date": "2024-12-17 16:15:04 UTC"
  },
  {
    "arxiv_id": "2412.13050v1",
    "title": "Modality-Inconsistent Continual Learning of Multimodal Large Language Models",
    "authors": [
      "Weiguo Pian",
      "Shijian Deng",
      "Shentong Mo",
      "Yunhui Guo",
      "Yapeng Tian"
    ],
    "abstract": "In this paper, we introduce Modality-Inconsistent Continual Learning (MICL),\na new continual learning scenario for Multimodal Large Language Models (MLLMs)\nthat involves tasks with inconsistent modalities (image, audio, or video) and\nvarying task types (captioning or question-answering). Unlike existing\nvision-only or modality-incremental settings, MICL combines modality and task\ntype shifts, both of which drive catastrophic forgetting. To address these\nchallenges, we propose MoInCL, which employs a Pseudo Targets Generation Module\nto mitigate forgetting caused by task type shifts in previously seen\nmodalities. It also incorporates Instruction-based Knowledge Distillation to\npreserve the model's ability to handle previously learned modalities when new\nones are introduced. We benchmark MICL using a total of six tasks and conduct\nexperiments to validate the effectiveness of our proposed MoInCL. The\nexperimental results highlight the superiority of MoInCL, showing significant\nimprovements over representative and state-of-the-art continual learning\nbaselines.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13050v1",
    "published_date": "2024-12-17 16:13:56 UTC",
    "updated_date": "2024-12-17 16:13:56 UTC"
  },
  {
    "arxiv_id": "2412.13023v1",
    "title": "Relational Neurosymbolic Markov Models",
    "authors": [
      "Lennert De Smet",
      "Gabriele Venturato",
      "Luc De Raedt",
      "Giuseppe Marra"
    ],
    "abstract": "Sequential problems are ubiquitous in AI, such as in reinforcement learning\nor natural language processing. State-of-the-art deep sequential models, like\ntransformers, excel in these settings but fail to guarantee the satisfaction of\nconstraints necessary for trustworthy deployment. In contrast, neurosymbolic AI\n(NeSy) provides a sound formalism to enforce constraints in deep probabilistic\nmodels but scales exponentially on sequential problems. To overcome these\nlimitations, we introduce relational neurosymbolic Markov models (NeSy-MMs), a\nnew class of end-to-end differentiable sequential models that integrate and\nprovably satisfy relational logical constraints. We propose a strategy for\ninference and learning that scales on sequential settings, and that combines\napproximate Bayesian inference, automated reasoning, and gradient estimation.\nOur experiments show that NeSy-MMs can solve problems beyond the current\nstate-of-the-art in neurosymbolic AI and still provide strong guarantees with\nrespect to desired properties. Moreover, we show that our models are more\ninterpretable and that constraints can be adapted at test time to\nout-of-distribution scenarios.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.13023v1",
    "published_date": "2024-12-17 15:41:51 UTC",
    "updated_date": "2024-12-17 15:41:51 UTC"
  },
  {
    "arxiv_id": "2412.15272v1",
    "title": "SimGRAG: Leveraging Similar Subgraphs for Knowledge Graphs Driven Retrieval-Augmented Generation",
    "authors": [
      "Yuzheng Cai",
      "Zhenyue Guo",
      "Yiwen Pei",
      "Wanrui Bian",
      "Weiguo Zheng"
    ],
    "abstract": "Recent advancements in large language models (LLMs) have shown impressive\nversatility across various tasks. To eliminate its hallucinations,\nretrieval-augmented generation (RAG) has emerged as a powerful approach,\nleveraging external knowledge sources like knowledge graphs (KGs). In this\npaper, we study the task of KG-driven RAG and propose a novel Similar Graph\nEnhanced Retrieval-Augmented Generation (SimGRAG) method. It effectively\naddresses the challenge of aligning query texts and KG structures through a\ntwo-stage process: (1) query-to-pattern, which uses an LLM to transform queries\ninto a desired graph pattern, and (2) pattern-to-subgraph, which quantifies the\nalignment between the pattern and candidate subgraphs using a graph semantic\ndistance (GSD) metric. We also develop an optimized retrieval algorithm that\nefficiently identifies the top-$k$ subgraphs within 1-second latency on a\n10-million-scale KG. Extensive experiments show that SimGRAG outperforms\nstate-of-the-art KG-driven RAG methods in both question answering and fact\nverification, offering superior plug-and-play usability and scalability.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.15272v1",
    "published_date": "2024-12-17 15:40:08 UTC",
    "updated_date": "2024-12-17 15:40:08 UTC"
  },
  {
    "arxiv_id": "2412.12997v3",
    "title": "Enabling Low-Resource Language Retrieval: Establishing Baselines for Urdu MS MARCO",
    "authors": [
      "Umer Butt",
      "Stalin Varanasi",
      "Günter Neumann"
    ],
    "abstract": "As the Information Retrieval (IR) field increasingly recognizes the\nimportance of inclusivity, addressing the needs of low-resource languages\nremains a significant challenge. This paper introduces the first large-scale\nUrdu IR dataset, created by translating the MS MARCO dataset through machine\ntranslation. We establish baseline results through zero-shot learning for IR in\nUrdu and subsequently apply the mMARCO multilingual IR methodology to this\nnewly translated dataset. Our findings demonstrate that the fine-tuned model\n(Urdu-mT5-mMARCO) achieves a Mean Reciprocal Rank (MRR@10) of 0.247 and a\nRecall@10 of 0.439, representing significant improvements over zero-shot\nresults and showing the potential for expanding IR access for Urdu speakers. By\nbridging access gaps for speakers of low-resource languages, this work not only\nadvances multilingual IR research but also emphasizes the ethical and societal\nimportance of inclusive IR technologies. This work provides valuable insights\ninto the challenges and solutions for improving language representation and\nlays the groundwork for future research, especially in South Asian languages,\nwhich can benefit from the adaptable methods used in this study.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "68T50",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "7 pages, ECIR 2025, conference camera-ready version",
    "pdf_url": "http://arxiv.org/pdf/2412.12997v3",
    "published_date": "2024-12-17 15:21:28 UTC",
    "updated_date": "2025-04-04 10:07:23 UTC"
  },
  {
    "arxiv_id": "2412.12996v1",
    "title": "Neural Control and Certificate Repair via Runtime Monitoring",
    "authors": [
      "Emily Yu",
      "Đorđe Žikelić",
      "Thomas A. Henzinger"
    ],
    "abstract": "Learning-based methods provide a promising approach to solving highly\nnon-linear control tasks that are often challenging for classical control\nmethods. To ensure the satisfaction of a safety property, learning-based\nmethods jointly learn a control policy together with a certificate function for\nthe property. Popular examples include barrier functions for safety and\nLyapunov functions for asymptotic stability. While there has been significant\nprogress on learning-based control with certificate functions in the white-box\nsetting, where the correctness of the certificate function can be formally\nverified, there has been little work on ensuring their reliability in the\nblack-box setting where the system dynamics are unknown. In this work, we\nconsider the problems of certifying and repairing neural network control\npolicies and certificate functions in the black-box setting. We propose a novel\nframework that utilizes runtime monitoring to detect system behaviors that\nviolate the property of interest under some initially trained neural network\npolicy and certificate. These violating behaviors are used to extract new\ntraining data, that is used to re-train the neural network policy and the\ncertificate function and to ultimately repair them. We demonstrate the\neffectiveness of our approach empirically by using it to repair and to boost\nthe safety rate of neural network policies learned by a state-of-the-art method\nfor learning-based control on two autonomous system control tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12996v1",
    "published_date": "2024-12-17 15:15:30 UTC",
    "updated_date": "2024-12-17 15:15:30 UTC"
  },
  {
    "arxiv_id": "2412.12987v1",
    "title": "Stochastic interior-point methods for smooth conic optimization with applications",
    "authors": [
      "Chuan He",
      "Zhanwang Deng"
    ],
    "abstract": "Conic optimization plays a crucial role in many machine learning (ML)\nproblems. However, practical algorithms for conic constrained ML problems with\nlarge datasets are often limited to specific use cases, as stochastic\nalgorithms for general conic optimization remain underdeveloped. To fill this\ngap, we introduce a stochastic interior-point method (SIPM) framework for\ngeneral conic optimization, along with four novel SIPM variants leveraging\ndistinct stochastic gradient estimators. Under mild assumptions, we establish\nthe global convergence rates of our proposed SIPMs, which, up to a logarithmic\nfactor, match the best-known rates in stochastic unconstrained optimization.\nFinally, our numerical experiments on robust linear regression, multi-task\nrelationship learning, and clustering data streams demonstrate the\neffectiveness and efficiency of our approach.",
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.LG",
      "90C25, 90C30"
    ],
    "primary_category": "math.OC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12987v1",
    "published_date": "2024-12-17 15:06:44 UTC",
    "updated_date": "2024-12-17 15:06:44 UTC"
  },
  {
    "arxiv_id": "2412.12984v2",
    "title": "Cluster-guided Contrastive Class-imbalanced Graph Classification",
    "authors": [
      "Wei Ju",
      "Zhengyang Mao",
      "Siyu Yi",
      "Yifang Qin",
      "Yiyang Gu",
      "Zhiping Xiao",
      "Jianhao Shen",
      "Ziyue Qiao",
      "Ming Zhang"
    ],
    "abstract": "This paper studies the problem of class-imbalanced graph classification,\nwhich aims at effectively classifying the graph categories in scenarios with\nimbalanced class distributions. While graph neural networks (GNNs) have\nachieved remarkable success, their modeling ability on imbalanced\ngraph-structured data remains suboptimal, which typically leads to predictions\nbiased towards the majority classes. On the other hand, existing\nclass-imbalanced learning methods in vision may overlook the rich graph\nsemantic substructures of the majority classes and excessively emphasize\nlearning from the minority classes. To address these challenges, we propose a\nsimple yet powerful approach called C$^3$GNN that integrates the idea of\nclustering into contrastive learning to enhance class-imbalanced graph\nclassification. Technically, C$^3$GNN clusters graphs from each majority class\ninto multiple subclasses, with sizes comparable to the minority class,\nmitigating class imbalance. It also employs the Mixup technique to generate\nsynthetic samples, enriching the semantic diversity of each subclass.\nFurthermore, supervised contrastive learning is used to hierarchically learn\neffective graph representations, enabling the model to thoroughly explore\nsemantic substructures in majority classes while avoiding excessive focus on\nminority classes. Extensive experiments on real-world graph benchmark datasets\nverify the superior performance of our proposed method against competitive\nbaselines.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by Proceedings of the Thirty-Ninth AAAI Conference on\n  Artificial Intelligence (AAAI-25)",
    "pdf_url": "http://arxiv.org/pdf/2412.12984v2",
    "published_date": "2024-12-17 15:04:54 UTC",
    "updated_date": "2024-12-30 05:34:10 UTC"
  },
  {
    "arxiv_id": "2412.12933v2",
    "title": "Two Layer Walk: A Community-Aware Graph Embedding",
    "authors": [
      "He Yu",
      "Jing Liu"
    ],
    "abstract": "Community structures are critical for understanding the mesoscopic\norganization of networks, bridging local and global patterns. While methods\nsuch as DeepWalk and node2vec capture local positional information through\nrandom walks, they fail to preserve community structures. Other approaches like\nmodularized nonnegative matrix factorization and evolutionary algorithms\naddress this gap but are computationally expensive and unsuitable for\nlarge-scale networks. To overcome these limitations, we propose Two Layer Walk\n(TLWalk), a novel graph embedding algorithm that incorporates hierarchical\ncommunity structures. TLWalk balances intra- and inter-community relationships\nthrough a community-aware random walk mechanism without requiring additional\nparameters. Theoretical analysis demonstrates that TLWalk effectively mitigates\nlocality bias. Experiments on benchmark datasets show that TLWalk outperforms\nstate-of-the-art methods, achieving up to 3.2% accuracy gains for link\nprediction tasks. By encoding dense local and sparse global structures, TLWalk\nproves robust and scalable across diverse networks, offering an efficient\nsolution for network analysis.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12933v2",
    "published_date": "2024-12-17 14:11:59 UTC",
    "updated_date": "2024-12-18 13:52:01 UTC"
  },
  {
    "arxiv_id": "2412.12932v3",
    "title": "CoMT: A Novel Benchmark for Chain of Multi-modal Thought on Large Vision-Language Models",
    "authors": [
      "Zihui Cheng",
      "Qiguang Chen",
      "Jin Zhang",
      "Hao Fei",
      "Xiaocheng Feng",
      "Wanxiang Che",
      "Min Li",
      "Libo Qin"
    ],
    "abstract": "Large Vision-Language Models (LVLMs) have recently demonstrated amazing\nsuccess in multi-modal tasks, including advancements in Multi-modal\nChain-of-Thought (MCoT) reasoning. Despite these successes, current benchmarks\nstill follow a traditional paradigm with multi-modal input and text-modal\noutput, which leads to significant drawbacks such as missing visual operations\nand vague expressions. Motivated by this, we introduce a novel Chain of\nMulti-modal Thought (CoMT) benchmark to address these limitations. Different\nfrom the traditional MCoT benchmark, CoMT requires both multi-modal input and\nmulti-modal reasoning output, aiming to mimic human-like reasoning that\ninherently integrates visual operation. Specifically, CoMT consists of four\ncategories: (1) Visual Creation, (2) Visual Deletion, (3) Visual Update, and\n(4) Visual Selection to comprehensively explore complex visual operations and\nconcise expression in real scenarios. We evaluate various LVLMs and strategies\non CoMT, revealing some key insights into the capabilities and limitations of\nthe current approaches. We hope that CoMT can inspire more research on\nintroducing multi-modal generation into the reasoning process.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at AAAI 2025; Project Page: https://github.com/czhhzc/CoMT",
    "pdf_url": "http://arxiv.org/pdf/2412.12932v3",
    "published_date": "2024-12-17 14:10:16 UTC",
    "updated_date": "2025-03-09 08:47:34 UTC"
  },
  {
    "arxiv_id": "2412.12929v1",
    "title": "Spectra of Cardinality Queries over Description Logic Knowledge Bases",
    "authors": [
      "Quentin Manière",
      "Marcin Przybyłko"
    ],
    "abstract": "Recent works have explored the use of counting queries coupled with\nDescription Logic ontologies. The answer to such a query in a model of a\nknowledge base is either an integer or $\\infty$, and its spectrum is the set of\nits answers over all models. While it is unclear how to compute and manipulate\nsuch a set in general, we identify a class of counting queries whose spectra\ncan be effectively represented. Focusing on atomic counting queries, we\npinpoint the possible shapes of a spectrum over $\\mathcal{ALCIF}$ ontologies:\nthey are essentially the subsets of $\\mathbb{N} \\cup \\{ \\infty \\}$ closed under\naddition. For most sublogics of $\\mathcal{ALCIF}$, we show that possible\nspectra enjoy simpler shapes, being $[ m, \\infty ]$ or variations thereof. To\nobtain our results, we refine constructions used for finite model reasoning and\nnotably rely on a cycle-reversion technique for the Horn fragment of\n$\\mathcal{ALCIF}$. We also study the data complexity of computing the proposed\neffective representation and establish the\n$\\mathsf{FP}^{\\mathsf{NP}[\\log]}$-completeness of this task under several\nsettings.",
    "categories": [
      "cs.AI",
      "cs.CC",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "26 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.12929v1",
    "published_date": "2024-12-17 14:07:04 UTC",
    "updated_date": "2024-12-17 14:07:04 UTC"
  },
  {
    "arxiv_id": "2412.12912v1",
    "title": "Unsupervised Region-Based Image Editing of Denoising Diffusion Models",
    "authors": [
      "Zixiang Li",
      "Yue Song",
      "Renshuai Tao",
      "Xiaohong Jia",
      "Yao Zhao",
      "Wei Wang"
    ],
    "abstract": "Although diffusion models have achieved remarkable success in the field of\nimage generation, their latent space remains under-explored. Current methods\nfor identifying semantics within latent space often rely on external\nsupervision, such as textual information and segmentation masks. In this paper,\nwe propose a method to identify semantic attributes in the latent space of\npre-trained diffusion models without any further training. By projecting the\nJacobian of the targeted semantic region into a low-dimensional subspace which\nis orthogonal to the non-masked regions, our approach facilitates precise\nsemantic discovery and control over local masked areas, eliminating the need\nfor annotations. We conducted extensive experiments across multiple datasets\nand various architectures of diffusion models, achieving state-of-the-art\nperformance. In particular, for some specific face attributes, the performance\nof our proposed method even surpasses that of supervised approaches,\ndemonstrating its superior ability in editing local image properties.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12912v1",
    "published_date": "2024-12-17 13:46:12 UTC",
    "updated_date": "2024-12-17 13:46:12 UTC"
  },
  {
    "arxiv_id": "2412.13231v3",
    "title": "C2F-TP: A Coarse-to-Fine Denoising Framework for Uncertainty-Aware Trajectory Prediction",
    "authors": [
      "Zichen Wang",
      "Hao Miao",
      "Senzhang Wang",
      "Renzhi Wang",
      "Jianxin Wang",
      "Jian Zhang"
    ],
    "abstract": "Accurately predicting the trajectory of vehicles is critically important for\nensuring safety and reliability in autonomous driving. Although considerable\nresearch efforts have been made recently, the inherent trajectory uncertainty\ncaused by various factors including the dynamic driving intends and the diverse\ndriving scenarios still poses significant challenges to accurate trajectory\nprediction. To address this issue, we propose C2F-TP, a coarse-to-fine\ndenoising framework for uncertainty-aware vehicle trajectory prediction. C2F-TP\nfeatures an innovative two-stage coarse-to-fine prediction process.\nSpecifically, in the spatial-temporal interaction stage, we propose a\nspatial-temporal interaction module to capture the inter-vehicle interactions\nand learn a multimodal trajectory distribution, from which a certain number of\nnoisy trajectories are sampled. Next, in the trajectory refinement stage, we\ndesign a conditional denoising model to reduce the uncertainty of the sampled\ntrajectories through a step-wise denoising operation. Extensive experiments are\nconducted on two real datasets NGSIM and highD that are widely adopted in\ntrajectory prediction. The result demonstrates the effectiveness of our\nproposal.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted by AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.13231v3",
    "published_date": "2024-12-17 13:42:49 UTC",
    "updated_date": "2024-12-24 03:46:32 UTC"
  },
  {
    "arxiv_id": "2412.12892v3",
    "title": "SAUGE: Taming SAM for Uncertainty-Aligned Multi-Granularity Edge Detection",
    "authors": [
      "Xing Liufu",
      "Chaolei Tan",
      "Xiaotong Lin",
      "Yonggang Qi",
      "Jinxuan Li",
      "Jian-Fang Hu"
    ],
    "abstract": "Edge labels are typically at various granularity levels owing to the varying\npreferences of annotators, thus handling the subjectivity of per-pixel labels\nhas been a focal point for edge detection. Previous methods often employ a\nsimple voting strategy to diminish such label uncertainty or impose a strong\nassumption of labels with a pre-defined distribution, e.g., Gaussian. In this\nwork, we unveil that the segment anything model (SAM) provides strong prior\nknowledge to model the uncertainty in edge labels. Our key insight is that the\nintermediate SAM features inherently correspond to object edges at various\ngranularities, which reflects different edge options due to uncertainty.\nTherefore, we attempt to align uncertainty with granularity by regressing\nintermediate SAM features from different layers to object edges at\nmulti-granularity levels. In doing so, the model can fully and explicitly\nexplore diverse ``uncertainties'' in a data-driven fashion. Specifically, we\ninject a lightweight module (~ 1.5% additional parameters) into the frozen SAM\nto progressively fuse and adapt its intermediate features to estimate edges\nfrom coarse to fine. It is crucial to normalize the granularity level of human\nedge labels to match their innate uncertainty. For this, we simply perform\nlinear blending to the real edge labels at hand to create pseudo labels with\nvarying granularities. Consequently, our uncertainty-aligned edge detector can\nflexibly produce edges at any desired granularity (including an optimal one).\nThanks to SAM, our model uniquely demonstrates strong generalizability for\ncross-dataset edge detection. Extensive experimental results on BSDS500,\nMuticue and NYUDv2 validate our model's superiority.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.12892v3",
    "published_date": "2024-12-17 13:18:41 UTC",
    "updated_date": "2025-04-19 03:14:19 UTC"
  },
  {
    "arxiv_id": "2412.12888v2",
    "title": "ArtAug: Enhancing Text-to-Image Generation through Synthesis-Understanding Interaction",
    "authors": [
      "Zhongjie Duan",
      "Qianyi Zhao",
      "Cen Chen",
      "Daoyuan Chen",
      "Wenmeng Zhou",
      "Yaliang Li",
      "Yingda Chen"
    ],
    "abstract": "The emergence of diffusion models has significantly advanced image synthesis.\nThe recent studies of model interaction and self-corrective reasoning approach\nin large language models offer new insights for enhancing text-to-image models.\nInspired by these studies, we propose a novel method called ArtAug for\nenhancing text-to-image models in this paper. To the best of our knowledge,\nArtAug is the first one that improves image synthesis models via model\ninteractions with understanding models. In the interactions, we leverage human\npreferences implicitly learned by image understanding models to provide\nfine-grained suggestions for image synthesis models. The interactions can\nmodify the image content to make it aesthetically pleasing, such as adjusting\nexposure, changing shooting angles, and adding atmospheric effects. The\nenhancements brought by the interaction are iteratively fused into the\nsynthesis model itself through an additional enhancement module. This enables\nthe synthesis model to directly produce aesthetically pleasing images without\nany extra computational cost. In the experiments, we train the ArtAug\nenhancement module on existing text-to-image models. Various evaluation metrics\nconsistently demonstrate that ArtAug enhances the generative capabilities of\ntext-to-image models without incurring additional computational costs. The\nsource code and models will be released publicly.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "18 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.12888v2",
    "published_date": "2024-12-17 13:12:31 UTC",
    "updated_date": "2024-12-18 13:01:11 UTC"
  },
  {
    "arxiv_id": "2412.12883v1",
    "title": "A Comparative Study of Pruning Methods in Transformer-based Time Series Forecasting",
    "authors": [
      "Nicholas Kiefer",
      "Arvid Weyrauch",
      "Muhammed Öz",
      "Achim Streit",
      "Markus Götz",
      "Charlotte Debus"
    ],
    "abstract": "The current landscape in time-series forecasting is dominated by\nTransformer-based models. Their high parameter count and corresponding demand\nin computational resources pose a challenge to real-world deployment,\nespecially for commercial and scientific applications with low-power embedded\ndevices. Pruning is an established approach to reduce neural network parameter\ncount and save compute. However, the implications and benefits of pruning\nTransformer-based models for time series forecasting are largely unknown. To\nclose this gap, we provide a comparative benchmark study by evaluating\nunstructured and structured pruning on various state-of-the-art multivariate\ntime series models. We study the effects of these pruning strategies on model\npredictive performance and computational aspects like model size, operations,\nand inference time. Our results show that certain models can be pruned even up\nto high sparsity levels, outperforming their dense counterpart. However,\nfine-tuning pruned models is necessary. Furthermore, we demonstrate that even\nwith corresponding hardware and software support, structured pruning is unable\nto provide significant time savings.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages, 5 figures, submitted to ACM Transactions on Intelligent\n  Systems and Technology",
    "pdf_url": "http://arxiv.org/pdf/2412.12883v1",
    "published_date": "2024-12-17 13:07:31 UTC",
    "updated_date": "2024-12-17 13:07:31 UTC"
  },
  {
    "arxiv_id": "2412.12881v1",
    "title": "RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement",
    "authors": [
      "Jinhao Jiang",
      "Jiayi Chen",
      "Junyi Li",
      "Ruiyang Ren",
      "Shijie Wang",
      "Wayne Xin Zhao",
      "Yang Song",
      "Tao Zhang"
    ],
    "abstract": "Existing large language models (LLMs) show exceptional problem-solving\ncapabilities but might struggle with complex reasoning tasks. Despite the\nsuccesses of chain-of-thought and tree-based search methods, they mainly depend\non the internal knowledge of LLMs to search over intermediate reasoning steps,\nlimited to dealing with simple tasks involving fewer reasoning steps. In this\npaper, we propose \\textbf{RAG-Star}, a novel RAG approach that integrates the\nretrieved information to guide the tree-based deliberative reasoning process\nthat relies on the inherent knowledge of LLMs. By leveraging Monte Carlo Tree\nSearch, RAG-Star iteratively plans intermediate sub-queries and answers for\nreasoning based on the LLM itself. To consolidate internal and external\nknowledge, we propose an retrieval-augmented verification that utilizes query-\nand answer-aware reward modeling to provide feedback for the inherent reasoning\nof LLMs. Our experiments involving Llama-3.1-8B-Instruct and GPT-4o demonstrate\nthat RAG-Star significantly outperforms previous RAG and reasoning methods.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "LLM;RAG;MCTS",
    "pdf_url": "http://arxiv.org/pdf/2412.12881v1",
    "published_date": "2024-12-17 13:05:36 UTC",
    "updated_date": "2024-12-17 13:05:36 UTC"
  },
  {
    "arxiv_id": "2412.14209v1",
    "title": "Integrating Evidence into the Design of XAI and AI-based Decision Support Systems: A Means-End Framework for End-users in Construction",
    "authors": [
      "Peter . E. D. Love",
      "Jane Matthews",
      "Weili Fang",
      "Hadi Mahamivanan"
    ],
    "abstract": "A narrative review is used to develop a theoretical evidence-based means-end\nframework to build an epistemic foundation to uphold explainable artificial\nintelligence instruments so that the reliability of outcomes generated from\ndecision support systems can be assured and better explained to end-users. The\nimplications of adopting an evidence-based approach to designing decision\nsupport systems in construction are discussed with emphasis placed on\nevaluating the strength, value, and utility of evidence needed to develop\nmeaningful human explanations for end-users. While the developed means-end\nframework is focused on end-users, stakeholders can also utilize it to create\nmeaningful human explanations. However, they will vary due to their different\nepistemic goals. Including evidence in the design and development of\nexplainable artificial intelligence and decision support systems will improve\ndecision-making effectiveness, enabling end-users' epistemic goals to be\nachieved. The proposed means-end framework is developed from a broad spectrum\nof literature. Thus, it is suggested that it can be used in construction and\nother engineering domains where there is a need to integrate evidence into the\ndesign of explainable artificial intelligence and decision support systems.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "60 pages, 4 figures and 1 table",
    "pdf_url": "http://arxiv.org/pdf/2412.14209v1",
    "published_date": "2024-12-17 13:02:05 UTC",
    "updated_date": "2024-12-17 13:02:05 UTC"
  },
  {
    "arxiv_id": "2412.12863v1",
    "title": "DISC: Plug-and-Play Decoding Intervention with Similarity of Characters for Chinese Spelling Check",
    "authors": [
      "Ziheng Qiao",
      "Houquan Zhou",
      "Yumeng Liu",
      "Zhenghua Li",
      "Min Zhang",
      "Bo Zhang",
      "Chen Li",
      "Ji Zhang",
      "Fei Huang"
    ],
    "abstract": "One key characteristic of the Chinese spelling check (CSC) task is that\nincorrect characters are usually similar to the correct ones in either\nphonetics or glyph. To accommodate this, previous works usually leverage\nconfusion sets, which suffer from two problems, i.e., difficulty in determining\nwhich character pairs to include and lack of probabilities to distinguish items\nin the set. In this paper, we propose a light-weight plug-and-play DISC (i.e.,\ndecoding intervention with similarity of characters) module for CSC models.DISC\nmeasures phonetic and glyph similarities between characters and incorporates\nthis similarity information only during the inference phase. This method can be\neasily integrated into various existing CSC models, such as ReaLiSe, SCOPE, and\nReLM, without additional training costs. Experiments on three CSC benchmarks\ndemonstrate that our proposed method significantly improves model performance,\napproaching and even surpassing the current state-of-the-art models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12863v1",
    "published_date": "2024-12-17 12:44:06 UTC",
    "updated_date": "2024-12-17 12:44:06 UTC"
  },
  {
    "arxiv_id": "2412.12859v1",
    "title": "Bayesian Persuasion with Externalities: Exploiting Agent Types",
    "authors": [
      "Jonathan Shaki",
      "Jiarui Gan",
      "Sarit Kraus"
    ],
    "abstract": "We study a Bayesian persuasion problem with externalities. In this model, a\nprincipal sends signals to inform multiple agents about the state of the world.\nSimultaneously, due to the existence of externalities in the agents' utilities,\nthe principal also acts as a correlation device to correlate the agents'\nactions. We consider the setting where the agents are categorized into a small\nnumber of types. Agents of the same type share identical utility functions and\nare treated equitably in the utility functions of both other agents and the\nprincipal. We study the problem of computing optimal signaling strategies for\nthe principal, under three different types of signaling channels: public,\nprivate, and semi-private. Our results include revelation-principle-style\ncharacterizations of optimal signaling strategies, linear programming\nformulations, and analysis of in/tractability of the optimization problems. It\nis demonstrated that when the maximum number of deviating agents is bounded by\na constant, our LP-based formulations compute optimal signaling strategies in\npolynomial time. Otherwise, the problems are NP-hard.",
    "categories": [
      "cs.AI",
      "cs.GT"
    ],
    "primary_category": "cs.AI",
    "comment": "to be published in AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.12859v1",
    "published_date": "2024-12-17 12:41:17 UTC",
    "updated_date": "2024-12-17 12:41:17 UTC"
  },
  {
    "arxiv_id": "2412.12858v1",
    "title": "Efficient Speech Command Recognition Leveraging Spiking Neural Network and Curriculum Learning-based Knowledge Distillation",
    "authors": [
      "Jiaqi Wang",
      "Liutao Yu",
      "Liwei Huang",
      "Chenlin Zhou",
      "Han Zhang",
      "Zhenxi Song",
      "Min Zhang",
      "Zhengyu Ma",
      "Zhiguo Zhang"
    ],
    "abstract": "The intrinsic dynamics and event-driven nature of spiking neural networks\n(SNNs) make them excel in processing temporal information by naturally\nutilizing embedded time sequences as time steps. Recent studies adopting this\napproach have demonstrated SNNs' effectiveness in speech command recognition,\nachieving high performance by employing large time steps for long time\nsequences. However, the large time steps lead to increased deployment burdens\nfor edge computing applications. Thus, it is important to balance high\nperformance and low energy consumption when detecting temporal patterns in edge\ndevices. Our solution comprises two key components. 1). We propose a\nhigh-performance fully spike-driven framework termed SpikeSCR, characterized by\na global-local hybrid structure for efficient representation learning, which\nexhibits long-term learning capabilities with extended time steps. 2). To\nfurther fully embrace low energy consumption, we propose an effective knowledge\ndistillation method based on curriculum learning (KDCL), where valuable\nrepresentations learned from the easy curriculum are progressively transferred\nto the hard curriculum with minor loss, striking a trade-off between power\nefficiency and high performance. We evaluate our method on three benchmark\ndatasets: the Spiking Heidelberg Dataset (SHD), the Spiking Speech Commands\n(SSC), and the Google Speech Commands (GSC) V2. Our experimental results\ndemonstrate that SpikeSCR outperforms current state-of-the-art (SOTA) methods\nacross these three datasets with the same time steps. Furthermore, by executing\nKDCL, we reduce the number of time steps by 60% and decrease energy consumption\nby 54.8% while maintaining comparable performance to recent SOTA results.\nTherefore, this work offers valuable insights for tackling temporal processing\nchallenges with long time sequences in edge neuromorphic computing systems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "Under Review",
    "pdf_url": "http://arxiv.org/pdf/2412.12858v1",
    "published_date": "2024-12-17 12:38:45 UTC",
    "updated_date": "2024-12-17 12:38:45 UTC"
  },
  {
    "arxiv_id": "2412.12850v1",
    "title": "Boosting Fine-Grained Visual Anomaly Detection with Coarse-Knowledge-Aware Adversarial Learning",
    "authors": [
      "Qingqing Fang",
      "Qinliang Su",
      "Wenxi Lv",
      "Wenchao Xu",
      "Jianxing Yu"
    ],
    "abstract": "Many unsupervised visual anomaly detection methods train an auto-encoder to\nreconstruct normal samples and then leverage the reconstruction error map to\ndetect and localize the anomalies. However, due to the powerful modeling and\ngeneralization ability of neural networks, some anomalies can also be well\nreconstructed, resulting in unsatisfactory detection and localization accuracy.\nIn this paper, a small coarsely-labeled anomaly dataset is first collected.\nThen, a coarse-knowledge-aware adversarial learning method is developed to\nalign the distribution of reconstructed features with that of normal features.\nThe alignment can effectively suppress the auto-encoder's reconstruction\nability on anomalies and thus improve the detection accuracy. Considering that\nanomalies often only occupy very small areas in anomalous images, a patch-level\nadversarial learning strategy is further developed. Although no patch-level\nanomalous information is available, we rigorously prove that by simply viewing\nany patch features from anomalous images as anomalies, the proposed\nknowledge-aware method can also align the distribution of reconstructed patch\nfeatures with the normal ones. Experimental results on four medical datasets\nand two industrial datasets demonstrate the effectiveness of our method in\nimproving the detection and localization performance.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "The paper is accepted by AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.12850v1",
    "published_date": "2024-12-17 12:24:08 UTC",
    "updated_date": "2024-12-17 12:24:08 UTC"
  },
  {
    "arxiv_id": "2412.12848v2",
    "title": "ClarityEthic: Explainable Moral Judgment Utilizing Contrastive Ethical Insights from Large Language Models",
    "authors": [
      "Yuxi Sun",
      "Wei Gao",
      "Jing Ma",
      "Hongzhan Lin",
      "Ziyang Luo",
      "Wenxuan Zhang"
    ],
    "abstract": "With the rise and widespread use of Large Language Models (LLMs), ensuring\ntheir safety is crucial to prevent harm to humans and promote ethical\nbehaviors. However, directly assessing value valence (i.e., support or oppose)\nby leveraging large-scale data training is untrustworthy and inexplainable. We\nassume that emulating humans to rely on social norms to make moral decisions\ncan help LLMs understand and predict moral judgment. However, capturing human\nvalues remains a challenge, as multiple related norms might conflict in\nspecific contexts. Consider norms that are upheld by the majority and promote\nthe well-being of society are more likely to be accepted and widely adopted\n(e.g., \"don't cheat,\"). Therefore, it is essential for LLM to identify the\nappropriate norms for a given scenario before making moral decisions. To this\nend, we introduce a novel moral judgment approach called \\textit{ClarityEthic}\nthat leverages LLMs' reasoning ability and contrastive learning to uncover\nrelevant social norms for human actions from different perspectives and select\nthe most reliable one to enhance judgment accuracy. Extensive experiments\ndemonstrate that our method outperforms state-of-the-art approaches in moral\njudgment tasks. Moreover, human evaluations confirm that the generated social\nnorms provide plausible explanations that support the judgments. This suggests\nthat modeling human moral judgment with the emulating humans moral strategy is\npromising for improving the ethical behaviors of LLMs.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.CY",
    "comment": "We have noticed that this version of our experiment and method\n  description isn't quite complete or accurate. To make sure we present our\n  best work, we think it would be a good idea to withdraw the manuscript for\n  now and take some time to revise and reformat it",
    "pdf_url": "http://arxiv.org/pdf/2412.12848v2",
    "published_date": "2024-12-17 12:22:44 UTC",
    "updated_date": "2025-04-09 08:38:44 UTC"
  },
  {
    "arxiv_id": "2412.12843v2",
    "title": "SLTNet: Efficient Event-based Semantic Segmentation with Spike-driven Lightweight Transformer-based Networks",
    "authors": [
      "Xiaxin Zhu",
      "Fangming Guo",
      "Xianlei Long",
      "Qingyi Gu",
      "Chao Chen",
      "Fuqiang Gu"
    ],
    "abstract": "Event-based semantic segmentation has great potential in autonomous driving\nand robotics due to the advantages of event cameras, such as high dynamic\nrange, low latency, and low power cost. Unfortunately, current artificial\nneural network (ANN)-based segmentation methods suffer from high computational\ndemands, the requirements for image frames, and massive energy consumption,\nlimiting their efficiency and application on resource-constrained edge/mobile\nplatforms. To address these problems, we introduce SLTNet, a spike-driven\nlightweight transformer-based network designed for event-based semantic\nsegmentation. Specifically, SLTNet is built on efficient spike-driven\nconvolution blocks (SCBs) to extract rich semantic features while reducing the\nmodel's parameters. Then, to enhance the long-range contextural feature\ninteraction, we propose novel spike-driven transformer blocks (STBs) with\nbinary mask operations. Based on these basic blocks, SLTNet employs a\nhigh-efficiency single-branch architecture while maintaining the low energy\nconsumption of the Spiking Neural Network (SNN). Finally, extensive experiments\non DDD17 and DSEC-Semantic datasets demonstrate that SLTNet outperforms\nstate-of-the-art (SOTA) SNN-based methods by at most 9.06% and 9.39% mIoU,\nrespectively, with extremely 4.58x lower energy consumption and 114 FPS\ninference speed. Our code is open-sourced and available at\nhttps://github.com/longxianlei/SLTNet-v1.0.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Submitted to 2025 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2025)",
    "pdf_url": "http://arxiv.org/pdf/2412.12843v2",
    "published_date": "2024-12-17 12:11:04 UTC",
    "updated_date": "2025-03-05 09:03:18 UTC"
  },
  {
    "arxiv_id": "2412.12839v1",
    "title": "From An LLM Swarm To A PDDL-Empowered HIVE: Planning Self-Executed Instructions In A Multi-Modal Jungle",
    "authors": [
      "Kaustubh Vyas",
      "Damien Graux",
      "Yijun Yang",
      "Sébastien Montella",
      "Chenxin Diao",
      "Wendi Zhou",
      "Pavlos Vougiouklis",
      "Ruofei Lai",
      "Yang Ren",
      "Keshuang Li",
      "Jeff Z. Pan"
    ],
    "abstract": "In response to the call for agent-based solutions that leverage the\never-increasing capabilities of the deep models' ecosystem, we introduce Hive\n-- a comprehensive solution for selecting appropriate models and subsequently\nplanning a set of atomic actions to satisfy the end-users' instructions. Hive\noperates over sets of models and, upon receiving natural language instructions\n(i.e. user queries), schedules and executes explainable plans of atomic\nactions. These actions can involve one or more of the available models to\nachieve the overall task, while respecting end-users specific constraints.\nNotably, Hive handles tasks that involve multi-modal inputs and outputs,\nenabling it to handle complex, real-world queries. Our system is capable of\nplanning complex chains of actions while guaranteeing explainability, using an\nLLM-based formal logic backbone empowered by PDDL operations. We introduce the\nMuSE benchmark in order to offer a comprehensive evaluation of the multi-modal\ncapabilities of agent systems. Our findings show that our framework redefines\nthe state-of-the-art for task selection, outperforming other competing systems\nthat plan operations across multiple models while offering transparency\nguarantees while fully adhering to user constraints.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Under review",
    "pdf_url": "http://arxiv.org/pdf/2412.12839v1",
    "published_date": "2024-12-17 12:05:21 UTC",
    "updated_date": "2024-12-17 12:05:21 UTC"
  },
  {
    "arxiv_id": "2412.12836v1",
    "title": "A Survey on Recommendation Unlearning: Fundamentals, Taxonomy, Evaluation, and Open Questions",
    "authors": [
      "Yuyuan Li",
      "Xiaohua Feng",
      "Chaochao Chen",
      "Qiang Yang"
    ],
    "abstract": "Recommender systems have become increasingly influential in shaping user\nbehavior and decision-making, highlighting their growing impact in various\ndomains. Meanwhile, the widespread adoption of machine learning models in\nrecommender systems has raised significant concerns regarding user privacy and\nsecurity. As compliance with privacy regulations becomes more critical, there\nis a pressing need to address the issue of recommendation unlearning, i.e.,\neliminating the memory of specific training data from the learned\nrecommendation models. Despite its importance, traditional machine unlearning\nmethods are ill-suited for recommendation unlearning due to the unique\nchallenges posed by collaborative interactions and model parameters. This\nsurvey offers a comprehensive review of the latest advancements in\nrecommendation unlearning, exploring the design principles, challenges, and\nmethodologies associated with this emerging field. We provide a unified\ntaxonomy that categorizes different recommendation unlearning approaches,\nfollowed by a summary of widely used benchmarks and metrics for evaluation. By\nreviewing the current state of research, this survey aims to guide the\ndevelopment of more efficient, scalable, and robust recommendation unlearning\ntechniques. Furthermore, we identify open research questions in this field,\nwhich could pave the way for future innovations not only in recommendation\nunlearning but also in a broader range of unlearning tasks across different\nmachine learning applications.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12836v1",
    "published_date": "2024-12-17 11:58:55 UTC",
    "updated_date": "2024-12-17 11:58:55 UTC"
  },
  {
    "arxiv_id": "2412.12832v1",
    "title": "DSGram: Dynamic Weighting Sub-Metrics for Grammatical Error Correction in the Era of Large Language Models",
    "authors": [
      "Jinxiang Xie",
      "Yilin Li",
      "Xunjian Yin",
      "Xiaojun Wan"
    ],
    "abstract": "Evaluating the performance of Grammatical Error Correction (GEC) models has\nbecome increasingly challenging, as large language model (LLM)-based GEC\nsystems often produce corrections that diverge from provided gold references.\nThis discrepancy undermines the reliability of traditional reference-based\nevaluation metrics. In this study, we propose a novel evaluation framework for\nGEC models, DSGram, integrating Semantic Coherence, Edit Level, and Fluency,\nand utilizing a dynamic weighting mechanism. Our framework employs the Analytic\nHierarchy Process (AHP) in conjunction with large language models to ascertain\nthe relative importance of various evaluation criteria. Additionally, we\ndevelop a dataset incorporating human annotations and LLM-simulated sentences\nto validate our algorithms and fine-tune more cost-effective models.\nExperimental results indicate that our proposed approach enhances the\neffectiveness of GEC model evaluations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Extended version of a paper to appear in AAAI-25",
    "pdf_url": "http://arxiv.org/pdf/2412.12832v1",
    "published_date": "2024-12-17 11:54:16 UTC",
    "updated_date": "2024-12-17 11:54:16 UTC"
  },
  {
    "arxiv_id": "2412.13229v2",
    "title": "Training Verification-Friendly Neural Networks via Neuron Behavior Consistency",
    "authors": [
      "Zongxin Liu",
      "Zhe Zhao",
      "Fu Song",
      "Jun Sun",
      "Pengfei Yang",
      "Xiaowei Huang",
      "Lijun Zhang"
    ],
    "abstract": "Formal verification provides critical security assurances for neural\nnetworks, yet its practical application suffers from the long verification\ntime. This work introduces a novel method for training verification-friendly\nneural networks, which are robust, easy to verify, and relatively accurate. Our\nmethod integrates neuron behavior consistency into the training process, making\nneuron activation states remain consistent across different inputs within a\nlocal neighborhood. This reduces the number of unstable neurons and tightens\nthe bounds of neurons thereby enhancing the network's verifiability. We\nevaluated our method using the MNIST, Fashion-MNIST, and CIFAR-10 datasets with\nvarious network architectures. The experimental results demonstrate that\nnetworks trained using our method are verification-friendly across different\nradii and architectures, whereas other tools fail to maintain verifiability as\nthe radius increases. Additionally, we show that our method can be combined\nwith existing approaches to further improve the verifiability of networks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accpeted by AAAI2025",
    "pdf_url": "http://arxiv.org/pdf/2412.13229v2",
    "published_date": "2024-12-17 11:40:49 UTC",
    "updated_date": "2024-12-29 13:48:34 UTC"
  },
  {
    "arxiv_id": "2412.17838v1",
    "title": "Coordinated Power Smoothing Control for Wind Storage Integrated System with Physics-informed Deep Reinforcement Learning",
    "authors": [
      "Shuyi Wang",
      "Huan Zhao",
      "Yuji Cao",
      "Zibin Pan",
      "Guolong Liu",
      "Gaoqi Liang",
      "Junhua Zhao"
    ],
    "abstract": "The Wind Storage Integrated System with Power Smoothing Control (PSC) has\nemerged as a promising solution to ensure both efficient and reliable wind\nenergy generation. However, existing PSC strategies overlook the intricate\ninterplay and distinct control frequencies between batteries and wind turbines,\nand lack consideration of wake effect and battery degradation cost. In this\npaper, a novel coordinated control framework with hierarchical levels is\ndevised to address these challenges effectively, which integrates the wake\nmodel and battery degradation model. In addition, after reformulating the\nproblem as a Markov decision process, the multi-agent reinforcement learning\nmethod is introduced to overcome the bi-level characteristic of the problem.\nMoreover, a Physics-informed Neural Network-assisted Multi-agent Deep\nDeterministic Policy Gradient (PAMA-DDPG) algorithm is proposed to incorporate\nthe power fluctuation differential equation and expedite the learning process.\nThe effectiveness of the proposed methodology is evaluated through simulations\nconducted in four distinct scenarios using WindFarmSimulator (WFSim). The\nresults demonstrate that the proposed algorithm facilitates approximately an\n11% increase in total profit and a 19% decrease in power fluctuation compared\nto the traditional methods, thereby addressing the dual objectives of economic\nefficiency and grid-connected energy reliability.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.17838v1",
    "published_date": "2024-12-17 11:37:46 UTC",
    "updated_date": "2024-12-17 11:37:46 UTC"
  },
  {
    "arxiv_id": "2502.15688v1",
    "title": "XPath Agent: An Efficient XPath Programming Agent Based on LLM for Web Crawler",
    "authors": [
      "Yu Li",
      "Bryce Wang",
      "Xinyu Luan"
    ],
    "abstract": "We present XPath Agent, a production-ready XPath programming agent\nspecifically designed for web crawling and web GUI testing. A key feature of\nXPath Agent is its ability to automatically generate XPath queries from a set\nof sampled web pages using a single natural language query. To demonstrate its\neffectiveness, we benchmark XPath Agent against a state-of-the-art XPath\nprogramming agent across a range of web crawling tasks. Our results show that\nXPath Agent achieves comparable performance metrics while significantly\nreducing token usage and improving clock-time efficiency. The well-designed\ntwo-stage pipeline allows for seamless integration into existing web crawling\nor web GUI testing workflows, thereby saving time and effort in manual XPath\nquery development. The source code for XPath Agent is available at\nhttps://github.com/eavae/feilian.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.SE",
      "68T50",
      "I.2.7"
    ],
    "primary_category": "cs.IR",
    "comment": "10 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.15688v1",
    "published_date": "2024-12-17 11:36:16 UTC",
    "updated_date": "2024-12-17 11:36:16 UTC"
  },
  {
    "arxiv_id": "2412.13228v3",
    "title": "TSEML: A task-specific embedding-based method for few-shot classification of cancer molecular subtypes",
    "authors": [
      "Ran Su",
      "Rui Shi",
      "Hui Cui",
      "Ping Xuan",
      "Chengyan Fang",
      "Xikang Feng",
      "Qiangguo Jin"
    ],
    "abstract": "Molecular subtyping of cancer is recognized as a critical and challenging\nupstream task for personalized therapy. Existing deep learning methods have\nachieved significant performance in this domain when abundant data samples are\navailable. However, the acquisition of densely labeled samples for cancer\nmolecular subtypes remains a significant challenge for conventional\ndata-intensive deep learning approaches. In this work, we focus on the few-shot\nmolecular subtype prediction problem in heterogeneous and small cancer\ndatasets, aiming to enhance precise diagnosis and personalized treatment. We\nfirst construct a new few-shot dataset for cancer molecular subtype\nclassification and auxiliary cancer classification, named TCGA Few-Shot, from\nexisting publicly available datasets. To effectively leverage the relevant\nknowledge from both tasks, we introduce a task-specific embedding-based\nmeta-learning framework (TSEML). TSEML leverages the synergistic strengths of a\nmodel-agnostic meta-learning (MAML) approach and a prototypical network\n(ProtoNet) to capture diverse and fine-grained features. Comparative\nexperiments conducted on the TCGA Few-Shot dataset demonstrate that our TSEML\nframework achieves superior performance in addressing the problem of few-shot\nmolecular subtype classification.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.QM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13228v3",
    "published_date": "2024-12-17 11:30:54 UTC",
    "updated_date": "2025-01-14 00:18:03 UTC"
  },
  {
    "arxiv_id": "2412.12808v2",
    "title": "Detecting Emotional Incongruity of Sarcasm by Commonsense Reasoning",
    "authors": [
      "Ziqi Qiu",
      "Jianxing Yu",
      "Yufeng Zhang",
      "Hanjiang Lai",
      "Yanghui Rao",
      "Qinliang Su",
      "Jian Yin"
    ],
    "abstract": "This paper focuses on sarcasm detection, which aims to identify whether given\nstatements convey criticism, mockery, or other negative sentiment opposite to\nthe literal meaning. To detect sarcasm, humans often require a comprehensive\nunderstanding of the semantics in the statement and even resort to external\ncommonsense to infer the fine-grained incongruity. However, existing methods\nlack commonsense inferential ability when they face complex real-world\nscenarios, leading to unsatisfactory performance. To address this problem, we\npropose a novel framework for sarcasm detection, which conducts incongruity\nreasoning based on commonsense augmentation, called EICR. Concretely, we first\nemploy retrieval-augmented large language models to supplement the missing but\nindispensable commonsense background knowledge. To capture complex contextual\nassociations, we construct a dependency graph and obtain the optimized topology\nvia graph refinement. We further introduce an adaptive reasoning skeleton that\nintegrates prior rules to extract sentiment-inconsistent subgraphs explicitly.\nTo eliminate the possible spurious relations between words and labels, we\nemploy adversarial contrastive learning to enhance the robustness of the\ndetector. Experiments conducted on five datasets demonstrate the effectiveness\nof EICR.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "In the experimental chapter, there is a problem with the experimental\n  setting and needs to be corrected",
    "pdf_url": "http://arxiv.org/pdf/2412.12808v2",
    "published_date": "2024-12-17 11:25:55 UTC",
    "updated_date": "2024-12-20 14:39:34 UTC"
  },
  {
    "arxiv_id": "2412.12800v1",
    "title": "Breaking the Programming Language Barrier: Multilingual Prompting to Empower Non-Native English Learners",
    "authors": [
      "James Prather",
      "Brent N. Reeves",
      "Paul Denny",
      "Juho Leinonen",
      "Stephen MacNeil",
      "Andrew Luxton-Reilly",
      "João Orvalho",
      "Amin Alipour",
      "Ali Alfageeh",
      "Thezyrie Amarouche",
      "Bailey Kimmel",
      "Jared Wright",
      "Musa Blake",
      "Gweneth Barbre"
    ],
    "abstract": "Non-native English speakers (NNES) face multiple barriers to learning\nprogramming. These barriers can be obvious, such as the fact that programming\nlanguage syntax and instruction are often in English, or more subtle, such as\nbeing afraid to ask for help in a classroom full of native English speakers.\nHowever, these barriers are frustrating because many NNES students know more\nabout programming than they can articulate in English. Advances in generative\nAI (GenAI) have the potential to break down these barriers because state of the\nart models can support interactions in multiple languages. Moreover, recent\nwork has shown that GenAI can be highly accurate at code generation and\nexplanation. In this paper, we provide the first exploration of NNES students\nprompting in their native languages (Arabic, Chinese, and Portuguese) to\ngenerate code to solve programming problems. Our results show that students are\nable to successfully use their native language to solve programming problems,\nbut not without some difficulty specifying programming terminology and\nconcepts. We discuss the challenges they faced, the implications for practice\nin the short term, and how this might transform computing education globally in\nthe long term.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "10 pages, 3 tables. Accepted for publication at the 27th Australasian\n  Computing Education Conference (ACE 2025)",
    "pdf_url": "http://arxiv.org/pdf/2412.12800v1",
    "published_date": "2024-12-17 11:06:02 UTC",
    "updated_date": "2024-12-17 11:06:02 UTC"
  },
  {
    "arxiv_id": "2412.12799v1",
    "title": "RCTrans: Radar-Camera Transformer via Radar Densifier and Sequential Decoder for 3D Object Detection",
    "authors": [
      "Yiheng Li",
      "Yang Yang",
      "Zhen Lei"
    ],
    "abstract": "In radar-camera 3D object detection, the radar point clouds are sparse and\nnoisy, which causes difficulties in fusing camera and radar modalities. To\nsolve this, we introduce a novel query-based detection method named\nRadar-Camera Transformer (RCTrans). Specifically, we first design a Radar Dense\nEncoder to enrich the sparse valid radar tokens, and then concatenate them with\nthe image tokens. By doing this, we can fully explore the 3D information of\neach interest region and reduce the interference of empty tokens during the\nfusing stage. We then design a Pruning Sequential Decoder to predict 3D boxes\nbased on the obtained tokens and random initialized queries. To alleviate the\neffect of elevation ambiguity in radar point clouds, we gradually locate the\nposition of the object via a sequential fusion structure. It helps to get more\nprecise and flexible correspondences between tokens and queries. A pruning\ntraining strategy is adopted in the decoder, which can save much time during\ninference and inhibit queries from losing their distinctiveness. Extensive\nexperiments on the large-scale nuScenes dataset prove the superiority of our\nmethod, and we also achieve new state-of-the-art radar-camera 3D detection\nresults. Our implementation is available at https://github.com/liyih/RCTrans.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.12799v1",
    "published_date": "2024-12-17 11:02:36 UTC",
    "updated_date": "2024-12-17 11:02:36 UTC"
  },
  {
    "arxiv_id": "2412.16205v2",
    "title": "Machine Learning-Based Estimation Of Wave Direction For Unmanned Surface Vehicles",
    "authors": [
      "Manele Ait Habouche",
      "Mickaël Kerboeuf",
      "Goulven Guillou",
      "Jean-Philippe Babau"
    ],
    "abstract": "Unmanned Surface Vehicles (USVs) have become critical tools for marine\nexploration, environmental monitoring, and autonomous navigation. Accurate\nestimation of wave direction is essential for improving USV navigation and\nensuring operational safety, but traditional methods often suffer from high\ncosts and limited spatial resolution. This paper proposes a machine\nlearning-based approach leveraging LSTM (Long Short-Term Memory) networks to\npredict wave direction using sensor data collected from USVs. Experimental\nresults show the capability of the LSTM model to learn temporal dependencies\nand provide accurate predictions, outperforming simpler baselines.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.16205v2",
    "published_date": "2024-12-17 10:53:12 UTC",
    "updated_date": "2025-02-12 09:48:29 UTC"
  },
  {
    "arxiv_id": "2412.12791v2",
    "title": "Implicit Location-Caption Alignment via Complementary Masking for Weakly-Supervised Dense Video Captioning",
    "authors": [
      "Shiping Ge",
      "Qiang Chen",
      "Zhiwei Jiang",
      "Yafeng Yin",
      "Liu Qin",
      "Ziyao Chen",
      "Qing Gu"
    ],
    "abstract": "Weakly-Supervised Dense Video Captioning (WSDVC) aims to localize and\ndescribe all events of interest in a video without requiring annotations of\nevent boundaries. This setting poses a great challenge in accurately locating\nthe temporal location of event, as the relevant supervision is unavailable.\nExisting methods rely on explicit alignment constraints between event locations\nand captions, which involve complex event proposal procedures during both\ntraining and inference. To tackle this problem, we propose a novel implicit\nlocation-caption alignment paradigm by complementary masking, which simplifies\nthe complex event proposal and localization process while maintaining\neffectiveness. Specifically, our model comprises two components: a dual-mode\nvideo captioning module and a mask generation module. The dual-mode video\ncaptioning module captures global event information and generates descriptive\ncaptions, while the mask generation module generates differentiable positive\nand negative masks for localizing the events. These masks enable the implicit\nalignment of event locations and captions by ensuring that captions generated\nfrom positively and negatively masked videos are complementary, thereby forming\na complete video description. In this way, even under weak supervision, the\nevent location and event caption can be aligned implicitly. Extensive\nexperiments on the public datasets demonstrate that our method outperforms\nexisting weakly-supervised methods and achieves competitive results compared to\nfully-supervised methods.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM",
      "I.2.10"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.12791v2",
    "published_date": "2024-12-17 10:52:50 UTC",
    "updated_date": "2025-01-27 10:40:20 UTC"
  },
  {
    "arxiv_id": "2412.12781v1",
    "title": "Predicting change in time production -- A machine learning approach to time perception",
    "authors": [
      "Amrapali Pednekar",
      "Alvaro Garrido",
      "Yara Khaluf",
      "Pieter Simoens"
    ],
    "abstract": "Time perception research has advanced significantly over the years. However,\nsome areas remain largely unexplored. This study addresses two such\nunder-explored areas in timing research: (1) A quantitative analysis of time\nperception at an individual level, and (2) Time perception in an ecological\nsetting. In this context, we trained a machine learning model to predict the\ndirection of change in an individual's time production. The model's training\ndata was collected using an ecologically valid setup. We moved closer to an\necological setting by conducting an online experiment with 995 participants\nperforming a time production task that used naturalistic videos (no audio) as\nstimuli. The model achieved an accuracy of 61%. This was 10 percentage points\nhigher than the baseline models derived from cognitive theories of timing. The\nmodel performed equally well on new data from a second experiment, providing\nevidence of its generalization capabilities. The model's output analysis\nrevealed that it also contained information about the magnitude of change in\ntime production. The predictions were further analysed at both population and\nindividual level. It was found that a participant's previous timing performance\nplayed a significant role in determining the direction of change in time\nproduction. By integrating attentional-gate theories from timing research with\nfeature importance techniques from machine learning, we explained model\npredictions using cognitive theories of timing. The model and findings from\nthis study have potential applications in systems involving human-computer\ninteractions where understanding and predicting changes in user's time\nperception can enable better user experience and task performance.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG",
      "J.4; I.2.m"
    ],
    "primary_category": "cs.HC",
    "comment": "Main text contains 16 pages and 9 figure. Supplementary information\n  is included as appendix. The paper has been submitted to IEEE TRANSACTIONS ON\n  COGNITIVE AND DEVELOPMENTAL SYSTEMS (TCDS). The code and data associated with\n  the study will be made publicly available upon acceptance",
    "pdf_url": "http://arxiv.org/pdf/2412.12781v1",
    "published_date": "2024-12-17 10:41:19 UTC",
    "updated_date": "2024-12-17 10:41:19 UTC"
  },
  {
    "arxiv_id": "2412.12778v2",
    "title": "Rethinking Diffusion-Based Image Generators for Fundus Fluorescein Angiography Synthesis on Limited Data",
    "authors": [
      "Chengzhou Yu",
      "Huihui Fang",
      "Hongqiu Wang",
      "Ting Deng",
      "Qing Du",
      "Yanwu Xu",
      "Weihua Yang"
    ],
    "abstract": "Fundus imaging is a critical tool in ophthalmology, with different imaging\nmodalities offering unique advantages. For instance, fundus fluorescein\nangiography (FFA) can accurately identify eye diseases. However, traditional\ninvasive FFA involves the injection of sodium fluorescein, which can cause\ndiscomfort and risks. Generating corresponding FFA images from non-invasive\nfundus images holds significant practical value but also presents challenges.\nFirst, limited datasets constrain the performance and effectiveness of models.\nSecond, previous studies have primarily focused on generating FFA for single\ndiseases or single modalities, often resulting in poor performance for patients\nwith various ophthalmic conditions. To address these issues, we propose a novel\nlatent diffusion model-based framework, Diffusion, which introduces a\nfine-tuning protocol to overcome the challenge of limited medical data and\nunleash the generative capabilities of diffusion models. Furthermore, we\ndesigned a new approach to tackle the challenges of generating across different\nmodalities and disease types. On limited datasets, our framework achieves\nstate-of-the-art results compared to existing methods, offering significant\npotential to enhance ophthalmic diagnostics and patient care. Our code will be\nreleased soon to support further research in this field.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "The first author has a conflict with the data access authority",
    "pdf_url": "http://arxiv.org/pdf/2412.12778v2",
    "published_date": "2024-12-17 10:37:46 UTC",
    "updated_date": "2025-03-10 02:53:38 UTC"
  },
  {
    "arxiv_id": "2412.12771v2",
    "title": "Guided and Variance-Corrected Fusion with One-shot Style Alignment for Large-Content Image Generation",
    "authors": [
      "Shoukun Sun",
      "Min Xian",
      "Tiankai Yao",
      "Fei Xu",
      "Luca Capriotti"
    ],
    "abstract": "Producing large images using small diffusion models is gaining increasing\npopularity, as the cost of training large models could be prohibitive. A common\napproach involves jointly generating a series of overlapped image patches and\nobtaining large images by merging adjacent patches. However, results from\nexisting methods often exhibit noticeable artifacts, e.g., seams and\ninconsistent objects and styles. To address the issues, we proposed Guided\nFusion (GF), which mitigates the negative impact from distant image regions by\napplying a weighted average to the overlapping regions. Moreover, we proposed\nVariance-Corrected Fusion (VCF), which corrects data variance at\npost-averaging, generating more accurate fusion for the Denoising Diffusion\nProbabilistic Model. Furthermore, we proposed a one-shot Style Alignment (SA),\nwhich generates a coherent style for large images by adjusting the initial\ninput noise without adding extra computational burden. Extensive experiments\ndemonstrated that the proposed fusion methods improved the quality of the\ngenerated image significantly. The proposed method can be widely applied as a\nplug-and-play module to enhance other fusion-based methods for large image\ngeneration. Code: https://github.com/TitorX/GVCFDiffusion",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12771v2",
    "published_date": "2024-12-17 10:33:34 UTC",
    "updated_date": "2025-02-10 18:55:08 UTC"
  },
  {
    "arxiv_id": "2412.12767v1",
    "title": "A Survey of Calibration Process for Black-Box LLMs",
    "authors": [
      "Liangru Xie",
      "Hui Liu",
      "Jingying Zeng",
      "Xianfeng Tang",
      "Yan Han",
      "Chen Luo",
      "Jing Huang",
      "Zhen Li",
      "Suhang Wang",
      "Qi He"
    ],
    "abstract": "Large Language Models (LLMs) demonstrate remarkable performance in semantic\nunderstanding and generation, yet accurately assessing their output reliability\nremains a significant challenge. While numerous studies have explored\ncalibration techniques, they primarily focus on White-Box LLMs with accessible\nparameters. Black-Box LLMs, despite their superior performance, pose heightened\nrequirements for calibration techniques due to their API-only interaction\nconstraints. Although recent researches have achieved breakthroughs in\nblack-box LLMs calibration, a systematic survey of these methodologies is still\nlacking. To bridge this gap, we presents the first comprehensive survey on\ncalibration techniques for black-box LLMs. We first define the Calibration\nProcess of LLMs as comprising two interrelated key steps: Confidence Estimation\nand Calibration. Second, we conduct a systematic review of applicable methods\nwithin black-box settings, and provide insights on the unique challenges and\nconnections in implementing these key steps. Furthermore, we explore typical\napplications of Calibration Process in black-box LLMs and outline promising\nfuture research directions, providing new perspectives for enhancing\nreliability and human-machine alignment. This is our GitHub link:\nhttps://github.com/LiangruXie/Calibration-Process-in-Black-Box-LLMs",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12767v1",
    "published_date": "2024-12-17 10:31:21 UTC",
    "updated_date": "2024-12-17 10:31:21 UTC"
  },
  {
    "arxiv_id": "2412.12761v1",
    "title": "Revealing the impact of synthetic native samples and multi-tasking strategies in Hindi-English code-mixed humour and sarcasm detection",
    "authors": [
      "Debajyoti Mazumder",
      "Aakash Kumar",
      "Jasabanta Patro"
    ],
    "abstract": "In this paper, we reported our experiments with various strategies to improve\ncode-mixed humour and sarcasm detection. We did all of our experiments for\nHindi-English code-mixed scenario, as we have the linguistic expertise for the\nsame. We experimented with three approaches, namely (i) native sample mixing,\n(ii) multi-task learning (MTL), and (iii) prompting very large multilingual\nlanguage models (VMLMs). In native sample mixing, we added monolingual task\nsamples in code-mixed training sets. In MTL learning, we relied on native and\ncode-mixed samples of a semantically related task (hate detection in our case).\nFinally, in our third approach, we evaluated the efficacy of VMLMs via few-shot\ncontext prompting. Some interesting findings we got are (i) adding native\nsamples improved humor (raising the F1-score up to 6.76%) and sarcasm (raising\nthe F1-score up to 8.64%) detection, (ii) training MLMs in an MTL framework\nboosted performance for both humour (raising the F1-score up to 10.67%) and\nsarcasm (increment up to 12.35% in F1-score) detection, and (iii) prompting\nVMLMs couldn't outperform the other approaches. Finally, our ablation studies\nand error analysis discovered the cases where our model is yet to improve. We\nprovided our code for reproducibility.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "26 pages; under review",
    "pdf_url": "http://arxiv.org/pdf/2412.12761v1",
    "published_date": "2024-12-17 10:26:54 UTC",
    "updated_date": "2024-12-17 10:26:54 UTC"
  },
  {
    "arxiv_id": "2412.12744v1",
    "title": "Your Next State-of-the-Art Could Come from Another Domain: A Cross-Domain Analysis of Hierarchical Text Classification",
    "authors": [
      "Nan Li",
      "Bo Kang",
      "Tijl De Bie"
    ],
    "abstract": "Text classification with hierarchical labels is a prevalent and challenging\ntask in natural language processing. Examples include assigning ICD codes to\npatient records, tagging patents into IPC classes, assigning EUROVOC\ndescriptors to European legal texts, and more. Despite its widespread\napplications, a comprehensive understanding of state-of-the-art methods across\ndifferent domains has been lacking. In this paper, we provide the first\ncomprehensive cross-domain overview with empirical analysis of state-of-the-art\nmethods. We propose a unified framework that positions each method within a\ncommon structure to facilitate research. Our empirical analysis yields key\ninsights and guidelines, confirming the necessity of learning across different\nresearch areas to design effective methods. Notably, under our unified\nevaluation pipeline, we achieved new state-of-the-art results by applying\ntechniques beyond their original domains.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12744v1",
    "published_date": "2024-12-17 10:08:57 UTC",
    "updated_date": "2024-12-17 10:08:57 UTC"
  },
  {
    "arxiv_id": "2412.12742v1",
    "title": "Subspace Implicit Neural Representations for Real-Time Cardiac Cine MR Imaging",
    "authors": [
      "Wenqi Huang",
      "Veronika Spieker",
      "Siying Xu",
      "Gastao Cruz",
      "Claudia Prieto",
      "Julia Schnabel",
      "Kerstin Hammernik",
      "Thomas Kuestner",
      "Daniel Rueckert"
    ],
    "abstract": "Conventional cardiac cine MRI methods rely on retrospective gating, which\nlimits temporal resolution and the ability to capture continuous cardiac\ndynamics, particularly in patients with arrhythmias and beat-to-beat\nvariations. To address these challenges, we propose a reconstruction framework\nbased on subspace implicit neural representations for real-time cardiac cine\nMRI of continuously sampled radial data. This approach employs two multilayer\nperceptrons to learn spatial and temporal subspace bases, leveraging the\nlow-rank properties of cardiac cine MRI. Initialized with low-resolution\nreconstructions, the networks are fine-tuned using spoke-specific loss\nfunctions to recover spatial details and temporal fidelity. Our method directly\nutilizes the continuously sampled radial k-space spokes during training,\nthereby eliminating the need for binning and non-uniform FFT. This approach\nachieves superior spatial and temporal image quality compared to conventional\nbinned methods at the acceleration rate of 10 and 20, demonstrating potential\nfor high-resolution imaging of dynamic cardiac events and enhancing diagnostic\ncapability.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12742v1",
    "published_date": "2024-12-17 10:06:37 UTC",
    "updated_date": "2024-12-17 10:06:37 UTC"
  },
  {
    "arxiv_id": "2412.12735v1",
    "title": "GIRAFFE: Design Choices for Extending the Context Length of Visual Language Models",
    "authors": [
      "Mukai Li",
      "Lei Li",
      "Shansan Gong",
      "Qi Liu"
    ],
    "abstract": "Visual Language Models (VLMs) demonstrate impressive capabilities in\nprocessing multimodal inputs, yet applications such as visual agents, which\nrequire handling multiple images and high-resolution videos, demand enhanced\nlong-range modeling. Moreover, existing open-source VLMs lack systematic\nexploration into extending their context length, and commercial models often\nprovide limited details. To tackle this, we aim to establish an effective\nsolution that enhances long context performance of VLMs while preserving their\ncapacities in short context scenarios. Towards this goal, we make the best\ndesign choice through extensive experiment settings from data curation to\ncontext window extending and utilizing: (1) we analyze data sources and length\ndistributions to construct ETVLM - a data recipe to balance the performance\nacross scenarios; (2) we examine existing position extending methods, identify\ntheir limitations and propose M-RoPE++ as an enhanced approach; we also choose\nto solely instruction-tune the backbone with mixed-source data; (3) we discuss\nhow to better utilize extended context windows and propose hybrid-resolution\ntraining. Built on the Qwen-VL series model, we propose Giraffe, which is\neffectively extended to 128K lengths. Evaluated on extensive long context VLM\nbenchmarks such as VideoMME and Viusal Haystacks, our Giraffe achieves\nstate-of-the-art performance among similarly sized open-source long VLMs and is\ncompetitive with commercial model GPT-4V. We will open-source the code, data,\nand models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Working in progress",
    "pdf_url": "http://arxiv.org/pdf/2412.12735v1",
    "published_date": "2024-12-17 09:57:21 UTC",
    "updated_date": "2024-12-17 09:57:21 UTC"
  },
  {
    "arxiv_id": "2412.12722v1",
    "title": "Defending LVLMs Against Vision Attacks through Partial-Perception Supervision",
    "authors": [
      "Qi Zhou",
      "Tianlin Li",
      "Qing Guo",
      "Dongxia Wang",
      "Yun Lin",
      "Yang Liu",
      "Jin Song Dong"
    ],
    "abstract": "Recent studies have raised significant concerns regarding the vulnerability\nof Large Vision Language Models (LVLMs) to maliciously injected or perturbed\ninput images, which can mislead their responses. Existing defense methods show\nthat such vision attacks are sensitive to image modifications especially\ncropping, using majority voting across responses of modified images as\ncorrected responses. However, these modifications often result in partial\nimages and distort the semantics, which reduces response quality on clean\nimages after voting. Instead of directly using responses from partial images\nfor voting, we investigate using them to supervise the LVLM's responses to the\noriginal images. We propose a black-box, training-free method called DPS\n(Defense through Partial-Perception Supervision). In this approach, the model\nis prompted using the responses generated by a model that perceives only a\npartial image. With DPS, the model can adjust its response based on partial\nimage understanding when under attack, while confidently maintaining its\noriginal response for clean input. Our findings show that the weak model can\nsupervise the strong model: when faced with an attacked input, the strong model\nbecomes less confident and adjusts its response based on the weak model's\npartial understanding, effectively defending against the attack. With clean\ninput, it confidently maintains its original response. Empirical experiments\nshow our method outperforms the baseline, cutting the average attack success\nrate by 76.3% across six datasets on three popular models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12722v1",
    "published_date": "2024-12-17 09:38:58 UTC",
    "updated_date": "2024-12-17 09:38:58 UTC"
  },
  {
    "arxiv_id": "2412.12700v1",
    "title": "ParMod: A Parallel and Modular Framework for Learning Non-Markovian Tasks",
    "authors": [
      "Ruixuan Miao",
      "Xu Lu",
      "Cong Tian",
      "Bin Yu",
      "Zhenhua Duan"
    ],
    "abstract": "The commonly used Reinforcement Learning (RL) model, MDPs (Markov Decision\nProcesses), has a basic premise that rewards depend on the current state and\naction only. However, many real-world tasks are non-Markovian, which has\nlong-term memory and dependency. The reward sparseness problem is further\namplified in non-Markovian scenarios. Hence learning a non-Markovian task (NMT)\nis inherently more difficult than learning a Markovian one. In this paper, we\npropose a novel \\textbf{Par}allel and \\textbf{Mod}ular RL framework, ParMod,\nspecifically for learning NMTs specified by temporal logic. With the aid of\nformal techniques, the NMT is modulaized into a series of sub-tasks based on\nthe automaton structure (equivalent to its temporal logic counterpart). On this\nbasis, sub-tasks will be trained by a group of agents in a parallel fashion,\nwith one agent handling one sub-task. Besides parallel training, the core of\nParMod lies in: a flexible classification method for modularizing the NMT, and\nan effective reward shaping method for improving the sample efficiency. A\ncomprehensive evaluation is conducted on several challenging benchmark problems\nwith respect to various metrics. The experimental results show that ParMod\nachieves superior performance over other relevant studies. Our work thus\nprovides a good synergy among RL, NMT and temporal logic.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12700v1",
    "published_date": "2024-12-17 09:16:53 UTC",
    "updated_date": "2024-12-17 09:16:53 UTC"
  },
  {
    "arxiv_id": "2412.12693v3",
    "title": "SPHERE: Unveiling Spatial Blind Spots in Vision-Language Models Through Hierarchical Evaluation",
    "authors": [
      "Wenyu Zhang",
      "Wei En Ng",
      "Lixin Ma",
      "Yuwen Wang",
      "Jungqi Zhao",
      "Allison Koenecke",
      "Boyang Li",
      "Lu Wang"
    ],
    "abstract": "Current vision-language models may grasp basic spatial cues and simple\ndirections (e.g. left, right, front, back), but struggle with the\nmulti-dimensional spatial reasoning necessary for human-like understanding and\nreal-world applications. To address this gap, we develop SPHERE (Spatial\nPerception and Hierarchical Evaluation of REasoning), a hierarchical evaluation\nframework supported by a new human-annotated dataset. SPHERE systematically\nprobes models across increasing levels of complexity, from fundamental skills\nto multi-skill integration and high-level reasoning that combines spatial,\nvisual, and logical understanding. Benchmark evaluation of state-of-the-art\nmodels reveals significant deficiencies, especially in reasoning about distance\nand proximity, understanding both egocentric and allocentric perspectives, and\napplying spatial logic in physical contexts. These findings expose critical\nblind spots in existing models and underscore the need for more advanced\nspatial reasoning techniques, driving the development of vision-language models\nthat align more closely with human spatial cognition. The SPHERE benchmark is\navailable at https://github.com/zwenyu/SPHERE-VLM.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12693v3",
    "published_date": "2024-12-17 09:10:55 UTC",
    "updated_date": "2025-02-28 15:14:37 UTC"
  },
  {
    "arxiv_id": "2412.19824v1",
    "title": "AnalogXpert: Automating Analog Topology Synthesis by Incorporating Circuit Design Expertise into Large Language Models",
    "authors": [
      "Haoyi Zhang",
      "Shizhao Sun",
      "Yibo Lin",
      "Runsheng Wang",
      "Jiang Bian"
    ],
    "abstract": "Analog circuits are crucial in modern electronic systems, and automating\ntheir design has attracted significant research interest. One of major\nchallenges is topology synthesis, which determines circuit components and their\nconnections. Recent studies explore large language models (LLM) for topology\nsynthesis. However, the scenarios addressed by these studies do not align well\nwith practical applications. Specifically, existing work uses vague design\nrequirements as input and outputs an ideal model, but detailed structural\nrequirements and device-level models are more practical. Moreover, current\napproaches either formulate topology synthesis as graph generation or Python\ncode generation, whereas practical topology design is a complex process that\ndemands extensive design knowledge. In this work, we propose AnalogXpert, a\nLLM-based agent aiming at solving practical topology synthesis problem by\nincorporating circuit design expertise into LLMs. First, we represent analog\ntopology as SPICE code and introduce a subcircuit library to reduce the design\nspace, in the same manner as experienced designers. Second, we decompose the\nproblem into two sub-task (i.e., block selection and block connection) through\nthe use of CoT and incontext learning techniques, to mimic the practical design\nprocess. Third, we introduce a proofreading strategy that allows LLMs to\nincrementally correct the errors in the initial design, akin to human designers\nwho iteratively check and adjust the initial topology design to ensure\naccuracy. Finally, we construct a high-quality benchmark containing both real\ndata (30) and synthetic data (2k). AnalogXpert achieves 40% and 23% success\nrates on the synthetic dataset and real dataset respectively, which is markedly\nbetter than those of GPT-4o (3% on both the synthetic dataset and the real\ndataset).",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.AR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.19824v1",
    "published_date": "2024-12-17 09:08:08 UTC",
    "updated_date": "2024-12-17 09:08:08 UTC"
  },
  {
    "arxiv_id": "2412.12681v1",
    "title": "Everyday AR through AI-in-the-Loop",
    "authors": [
      "Ryo Suzuki",
      "Mar Gonzalez-Franco",
      "Misha Sra",
      "David Lindlbauer"
    ],
    "abstract": "This workshop brings together experts and practitioners from augmented\nreality (AR) and artificial intelligence (AI) to shape the future of\nAI-in-the-loop everyday AR experiences. With recent advancements in both AR\nhardware and AI capabilities, we envision that everyday AR -- always-available\nand seamlessly integrated into users' daily environments -- is becoming\nincreasingly feasible. This workshop will explore how AI can drive such\neveryday AR experiences. We discuss a range of topics, including adaptive and\ncontext-aware AR, generative AR content creation, always-on AI assistants,\nAI-driven accessible design, and real-world-oriented AI agents. Our goal is to\nidentify the opportunities and challenges in AI-enabled AR, focusing on\ncreating novel AR experiences that seamlessly blend the digital and physical\nworlds. Through the workshop, we aim to foster collaboration, inspire future\nresearch, and build a community to advance the research field of AI-enhanced\nAR.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "CHI 2025 Extended Abstract",
    "pdf_url": "http://arxiv.org/pdf/2412.12681v1",
    "published_date": "2024-12-17 08:51:55 UTC",
    "updated_date": "2024-12-17 08:51:55 UTC"
  },
  {
    "arxiv_id": "2412.12661v2",
    "title": "MedMax: Mixed-Modal Instruction Tuning for Training Biomedical Assistants",
    "authors": [
      "Hritik Bansal",
      "Daniel Israel",
      "Siyan Zhao",
      "Shufan Li",
      "Tung Nguyen",
      "Aditya Grover"
    ],
    "abstract": "Recent advancements in mixed-modal generative have opened new avenues for\ndeveloping unified biomedical assistants capable of analyzing biomedical\nimages, answering complex questions about them, and generating multimodal\npatient reports. However, existing datasets face challenges such as small\nsizes, limited coverage of biomedical tasks and domains, and a reliance on\nnarrow sources. To address these gaps, we present MedMax, a large-scale\nmultimodal biomedical instruction-tuning dataset for mixed-modal foundation\nmodels. With 1.47 million instances, MedMax encompasses a diverse range of\ntasks, including interleaved image-text generation, biomedical image captioning\nand generation, visual chat, and report understanding. These tasks span\nknowledge across diverse biomedical domains, including radiology and\nhistopathology, grounded in medical papers and YouTube videos. Subsequently, we\nfine-tune a mixed-modal foundation model on the MedMax dataset, achieving\nsignificant performance improvements: a 26% gain over the Chameleon model and\nan 18.3% improvement over GPT-4o across 12 downstream biomedical visual\nquestion-answering tasks. Finally, we introduce a unified evaluation suite for\nbiomedical tasks to guide the development of mixed-modal biomedical AI\nassistants. The data, model, and code is available at\nhttps://mint-medmax.github.io/.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "29 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.12661v2",
    "published_date": "2024-12-17 08:30:00 UTC",
    "updated_date": "2025-04-23 06:29:51 UTC"
  },
  {
    "arxiv_id": "2412.12656v1",
    "title": "DriveTester: A Unified Platform for Simulation-Based Autonomous Driving Testing",
    "authors": [
      "Mingfei Cheng",
      "Yuan Zhou",
      "Xiaofei Xie"
    ],
    "abstract": "Simulation-based testing plays a critical role in evaluating the safety and\nreliability of autonomous driving systems (ADSs). However, one of the key\nchallenges in ADS testing is the complexity of preparing and configuring\nsimulation environments, particularly in terms of compatibility and stability\nbetween the simulator and the ADS. This complexity often results in researchers\ndedicating significant effort to customize their own environments, leading to\ndisparities in development platforms and underlying systems. Consequently,\nreproducing and comparing these methodologies on a unified ADS testing platform\nbecomes difficult. To address these challenges, we introduce DriveTester, a\nunified simulation-based testing platform built on Apollo, one of the most\nwidely used open-source, industrial-level ADS platforms. DriveTester provides a\nconsistent and reliable environment, integrates a lightweight traffic\nsimulator, and incorporates various state-of-the-art ADS testing techniques.\nThis enables researchers to efficiently develop, test, and compare their\nmethods within a standardized platform, fostering reproducibility and\ncomparison across different ADS testing approaches. The code is available:\nhttps://github.com/MingfeiCheng/DriveTester.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12656v1",
    "published_date": "2024-12-17 08:24:05 UTC",
    "updated_date": "2024-12-17 08:24:05 UTC"
  },
  {
    "arxiv_id": "2412.12651v1",
    "title": "Shared Attention-based Autoencoder with Hierarchical Fusion-based Graph Convolution Network for sEEG SOZ Identification",
    "authors": [
      "Huachao Yan",
      "Kailing Guo",
      "Shiwei Song",
      "Yihai Dai",
      "Xiaoqiang Wei",
      "Xiaofen Xing",
      "Xiangmin Xu"
    ],
    "abstract": "Diagnosing seizure onset zone (SOZ) is a challenge in neurosurgery, where\nstereoelectroencephalography (sEEG) serves as a critical technique. In sEEG SOZ\nidentification, the existing studies focus solely on the intra-patient\nrepresentation of epileptic information, overlooking the general features of\nepilepsy across patients and feature interdependencies between feature elements\nin each contact site. In order to address the aforementioned challenges, we\npropose the shared attention-based autoencoder (sATAE). sATAE is trained by\nsEEG data across all patients, with attention blocks introduced to enhance the\nrepresentation of interdependencies between feature elements. Considering the\nspatial diversity of sEEG across patients, we introduce graph-based method for\nidentification SOZ of each patient. However, the current graph-based methods\nfor sEEG SOZ identification rely exclusively on static graphs to model\nepileptic networks. Inspired by the finding of neuroscience that epileptic\nnetwork is intricately characterized by the interplay of sophisticated\nequilibrium between fluctuating and stable states, we design the hierarchical\nfusion-based graph convolution network (HFGCN) to identify the SOZ. HFGCN\nintegrates the dynamic and static characteristics of epileptic networks through\nhierarchical weighting across different hierarchies, facilitating a more\ncomprehensive learning of epileptic features and enriching node information for\nsEEG SOZ identification. Combining sATAE and HFGCN, we perform comprehensive\nexperiments with sATAE-HFGCN on the self-build sEEG dataset, which includes\nsEEG data from 17 patients with temporal lobe epilepsy. The results show that\nour method, sATAE-HFGCN, achieves superior performance for identifying the SOZ\nof each patient, effectively addressing the aforementioned challenges,\nproviding an efficient solution for sEEG-based SOZ identification.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP",
      "q-bio.NC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12651v1",
    "published_date": "2024-12-17 08:20:02 UTC",
    "updated_date": "2024-12-17 08:20:02 UTC"
  },
  {
    "arxiv_id": "2412.12650v1",
    "title": "Neural-Network-Driven Reward Prediction as a Heuristic: Advancing Q-Learning for Mobile Robot Path Planning",
    "authors": [
      "Yiming Ji",
      "Kaijie Yun",
      "Yang Liu",
      "Zongwu Xie",
      "Hong Liu"
    ],
    "abstract": "Q-learning is a widely used reinforcement learning technique for solving path\nplanning problems. It primarily involves the interaction between an agent and\nits environment, enabling the agent to learn an optimal strategy that maximizes\ncumulative rewards. Although many studies have reported the effectiveness of\nQ-learning, it still faces slow convergence issues in practical applications.\nTo address this issue, we propose the NDR-QL method, which utilizes neural\nnetwork outputs as heuristic information to accelerate the convergence process\nof Q-learning. Specifically, we improved the dual-output neural network model\nby introducing a start-end channel separation mechanism and enhancing the\nfeature fusion process. After training, the proposed NDR model can output a\nnarrowly focused optimal probability distribution, referred to as the\nguideline, and a broadly distributed suboptimal distribution, referred to as\nthe region. Subsequently, based on the guideline prediction, we calculate the\ncontinuous reward function for the Q-learning method, and based on the region\nprediction, we initialize the Q-table with a bias. We conducted training,\nvalidation, and path planning simulation experiments on public datasets. The\nresults indicate that the NDR model outperforms previous methods by up to 5\\%\nin prediction accuracy. Furthermore, the proposed NDR-QL method improves the\nconvergence speed of the baseline Q-learning method by 90\\% and also surpasses\nthe previously improved Q-learning methods in path quality metrics.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12650v1",
    "published_date": "2024-12-17 08:19:40 UTC",
    "updated_date": "2024-12-17 08:19:40 UTC"
  },
  {
    "arxiv_id": "2412.12649v1",
    "title": "ClustEm4Ano: Clustering Text Embeddings of Nominal Textual Attributes for Microdata Anonymization",
    "authors": [
      "Robert Aufschläger",
      "Sebastian Wilhelm",
      "Michael Heigl",
      "Martin Schramm"
    ],
    "abstract": "This work introduces ClustEm4Ano, an anonymization pipeline that can be used\nfor generalization and suppression-based anonymization of nominal textual\ntabular data. It automatically generates value generalization hierarchies\n(VGHs) that, in turn, can be used to generalize attributes in\nquasi-identifiers. The pipeline leverages embeddings to generate semantically\nclose value generalizations through iterative clustering. We applied KMeans and\nHierarchical Agglomerative Clustering on $13$ different predefined text\nembeddings (both open and closed-source (via APIs)). Our approach is\nexperimentally tested on a well-known benchmark dataset for anonymization: The\nUCI Machine Learning Repository's Adult dataset. ClustEm4Ano supports\nanonymization procedures by offering more possibilities compared to using\narbitrarily chosen VGHs. Experiments demonstrate that these VGHs can outperform\nmanually constructed ones in terms of downstream efficacy (especially for small\n$k$-anonymity ($2 \\leq k \\leq 30$)) and therefore can foster the quality of\nanonymized datasets. Our implementation is made public.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "16 pages, 5 figures, accepted for presentation at IDEAS: 2024 28th\n  International Symposium on Database Engineered Applications, Bayonne, France,\n  August 26-29, 2024",
    "pdf_url": "http://arxiv.org/pdf/2412.12649v1",
    "published_date": "2024-12-17 08:16:04 UTC",
    "updated_date": "2024-12-17 08:16:04 UTC"
  },
  {
    "arxiv_id": "2412.12648v1",
    "title": "Exploring AI-Enabled Cybersecurity Frameworks: Deep-Learning Techniques, GPU Support, and Future Enhancements",
    "authors": [
      "Tobias Becher",
      "Simon Torka"
    ],
    "abstract": "Traditional rule-based cybersecurity systems have proven highly effective\nagainst known malware threats. However, they face challenges in detecting novel\nthreats. To address this issue, emerging cybersecurity systems are\nincorporating AI techniques, specifically deep-learning algorithms, to enhance\ntheir ability to detect incidents, analyze alerts, and respond to events. While\nthese techniques offer a promising approach to combating dynamic security\nthreats, they often require significant computational resources. Therefore,\nframeworks that incorporate AI-based cybersecurity mechanisms need to support\nthe use of GPUs to ensure optimal performance.\n  Many cybersecurity framework vendors do not provide sufficiently detailed\ninformation about their implementation, making it difficult to assess the\ntechniques employed and their effectiveness. This study aims to overcome this\nlimitation by providing an overview of the most used cybersecurity frameworks\nthat utilize AI techniques, specifically focusing on frameworks that provide\ncomprehensive information about their implementation. Our primary objective is\nto identify the deep-learning techniques employed by these frameworks and\nevaluate their support for GPU acceleration. We have identified a total of\n\\emph{two} deep-learning algorithms that are utilized by \\emph{three} out of 38\nselected cybersecurity frameworks. Our findings aim to assist in selecting\nopen-source cybersecurity frameworks for future research and assessing any\ndiscrepancies between deep-learning techniques used in theory and practice.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.DC",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12648v1",
    "published_date": "2024-12-17 08:14:12 UTC",
    "updated_date": "2024-12-17 08:14:12 UTC"
  },
  {
    "arxiv_id": "2412.12642v1",
    "title": "RDPI: A Refine Diffusion Probability Generation Method for Spatiotemporal Data Imputation",
    "authors": [
      "Zijin Liu",
      "Xiang Zhao",
      "You Song"
    ],
    "abstract": "Spatiotemporal data imputation plays a crucial role in various fields such as\ntraffic flow monitoring, air quality assessment, and climate prediction.\nHowever, spatiotemporal data collected by sensors often suffer from temporal\nincompleteness, and the sparse and uneven distribution of sensors leads to\nmissing data in the spatial dimension. Among existing methods, autoregressive\napproaches are prone to error accumulation, while simple conditional diffusion\nmodels fail to adequately capture the spatiotemporal relationships between\nobserved and missing data. To address these issues, we propose a novel\ntwo-stage Refined Diffusion Probability Impuation (RDPI) framework based on an\ninitial network and a conditional diffusion model. In the initial stage,\ndeterministic imputation methods are used to generate preliminary estimates of\nthe missing data. In the refinement stage, residuals are treated as the\ndiffusion target, and observed values are innovatively incorporated into the\nforward process. This results in a conditional diffusion model better suited\nfor spatiotemporal data imputation, bridging the gap between the preliminary\nestimates and the true values. Experiments on multiple datasets demonstrate\nthat RDPI not only achieves state-of-the-art imputation accuracy but also\nsignificantly reduces sampling computational costs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12642v1",
    "published_date": "2024-12-17 08:06:00 UTC",
    "updated_date": "2024-12-17 08:06:00 UTC"
  },
  {
    "arxiv_id": "2412.15270v2",
    "title": "Baichuan4-Finance Technical Report",
    "authors": [
      "Hanyu Zhang",
      "Boyu Qiu",
      "Yuhao Feng",
      "Shuqi Li",
      "Qian Ma",
      "Xiyuan Zhang",
      "Qiang Ju",
      "Dong Yan",
      "Jian Xie"
    ],
    "abstract": "Large language models (LLMs) have demonstrated strong capabilities in\nlanguage understanding, generation, and reasoning, yet their potential in\nfinance remains underexplored due to the complexity and specialization of\nfinancial knowledge. In this work, we report the development of the\nBaichuan4-Finance series, including a comprehensive suite of foundational\nBaichuan4-Finance-Base and an aligned language model Baichuan4-Finance, which\nare built upon Baichuan4-Turbo base model and tailored for finance domain.\nFirstly, we have dedicated significant effort to building a detailed pipeline\nfor improving data quality. Moreover, in the continual pre-training phase, we\npropose a novel domain self-constraint training strategy, which enables\nBaichuan4-Finance-Base to acquire financial knowledge without losing general\ncapabilities. After Supervised Fine-tuning and Reinforcement Learning from\nHuman Feedback and AI Feedback, the chat model Baichuan4-Finance is able to\ntackle various financial certification questions and real-world scenario\napplications. We evaluate Baichuan4-Finance on many widely used general\ndatasets and two holistic financial benchmarks. The evaluation results show\nthat Baichuan4-Finance-Base surpasses almost all competitive baselines on\nfinancial tasks by significant margins without sacrificing performance on\ngeneral LLM benchmarks. At the same time, Baichuan4-Finance demonstrates even\nmore impressive performance on financial application scenarios, showcasing its\npotential to foster community innovation in the financial LLM field.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CE",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.15270v2",
    "published_date": "2024-12-17 08:05:32 UTC",
    "updated_date": "2025-01-02 11:21:38 UTC"
  },
  {
    "arxiv_id": "2412.15269v1",
    "title": "The Reliability Paradox: Exploring How Shortcut Learning Undermines Language Model Calibration",
    "authors": [
      "Geetanjali Bihani",
      "Julia Rayz"
    ],
    "abstract": "The advent of pre-trained language models (PLMs) has enabled significant\nperformance gains in the field of natural language processing. However, recent\nstudies have found PLMs to suffer from miscalibration, indicating a lack of\naccuracy in the confidence estimates provided by these models. Current\nevaluation methods for PLM calibration often assume that lower calibration\nerror estimates indicate more reliable predictions. However, fine-tuned PLMs\noften resort to shortcuts, leading to overconfident predictions that create the\nillusion of enhanced performance but lack generalizability in their decision\nrules. The relationship between PLM reliability, as measured by calibration\nerror, and shortcut learning, has not been thoroughly explored thus far. This\npaper aims to investigate this relationship, studying whether lower calibration\nerror implies reliable decision rules for a language model. Our findings reveal\nthat models with seemingly superior calibration portray higher levels of\nnon-generalizable decision rules. This challenges the prevailing notion that\nwell-calibrated models are inherently reliable. Our study highlights the need\nto bridge the current gap between language model calibration and generalization\nobjectives, urging the development of comprehensive frameworks to achieve truly\nrobust and reliable language models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages; 9 figures. Accepted for publication at the Hawaii\n  International Conference on System Sciences (HICSS-58) 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.15269v1",
    "published_date": "2024-12-17 08:04:28 UTC",
    "updated_date": "2024-12-17 08:04:28 UTC"
  },
  {
    "arxiv_id": "2412.12641v1",
    "title": "Lagrangian Index Policy for Restless Bandits with Average Reward",
    "authors": [
      "Konstantin Avrachenkov",
      "Vivek S. Borkar",
      "Pratik Shah"
    ],
    "abstract": "We study the Lagrangian Index Policy (LIP) for restless multi-armed bandits\nwith long-run average reward. In particular, we compare the performance of LIP\nwith the performance of the Whittle Index Policy (WIP), both heuristic policies\nknown to be asymptotically optimal under certain natural conditions. Even\nthough in most cases their performances are very similar, in the cases when WIP\nshows bad performance, LIP continues to perform very well. We then propose\nreinforcement learning algorithms, both tabular and NN-based, to obtain online\nlearning schemes for LIP in the model-free setting. The proposed reinforcement\nlearning schemes for LIP requires significantly less memory than the analogous\nscheme for WIP. We calculate analytically the Lagrangian index for the restart\nmodel, which describes the optimal web crawling and the minimization of the\nweighted age of information. We also give a new proof of asymptotic optimality\nin case of homogeneous bandits as the number of arms goes to infinity, based on\nexchangeability and de Finetti's theorem.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC",
      "math.PR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12641v1",
    "published_date": "2024-12-17 08:03:53 UTC",
    "updated_date": "2024-12-17 08:03:53 UTC"
  },
  {
    "arxiv_id": "2412.12639v3",
    "title": "Falcon: Faster and Parallel Inference of Large Language Models through Enhanced Semi-Autoregressive Drafting and Custom-Designed Decoding Tree",
    "authors": [
      "Xiangxiang Gao",
      "Weisheng Xie",
      "Yiwei Xiang",
      "Feng Ji"
    ],
    "abstract": "Striking an optimal balance between minimal drafting latency and high\nspeculation accuracy to enhance the inference speed of Large Language Models\nremains a significant challenge in speculative decoding. In this paper, we\nintroduce Falcon, an innovative semi-autoregressive speculative decoding\nframework fashioned to augment both the drafter's parallelism and output\nquality. Falcon incorporates the Coupled Sequential Glancing Distillation\ntechnique, which fortifies inter-token dependencies within the same block,\nleading to increased speculation accuracy. We offer a comprehensive theoretical\nanalysis to illuminate the underlying mechanisms. Additionally, we introduce a\nCustom-Designed Decoding Tree, which permits the drafter to generate multiple\ntokens in a single forward pass and accommodates multiple forward passes as\nneeded, thereby boosting the number of drafted tokens and significantly\nimproving the overall acceptance rate. Comprehensive evaluations on benchmark\ndatasets such as MT-Bench, HumanEval, and GSM8K demonstrate Falcon's superior\nacceleration capabilities. The framework achieves a lossless speedup ratio\nranging from 2.91x to 3.51x when tested on the Vicuna and LLaMA2-Chat model\nseries. These results outstrip existing speculative decoding methods for LLMs,\nincluding Eagle, Medusa, Lookahead, SPS, and PLD, while maintaining a compact\ndrafter architecture equivalent to merely two Transformer layers.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "AAAI 2025 Accepted",
    "pdf_url": "http://arxiv.org/pdf/2412.12639v3",
    "published_date": "2024-12-17 08:02:08 UTC",
    "updated_date": "2025-04-22 07:32:21 UTC"
  },
  {
    "arxiv_id": "2412.12636v2",
    "title": "TrainMover: An Interruption-Resilient and Reliable ML Training Runtime",
    "authors": [
      "ChonLam Lao",
      "Minlan Yu",
      "Aditya Akella",
      "Jiamin Cao",
      "Yu Guan",
      "Pengcheng Zhang",
      "Zhilong Zheng",
      "Yichi Xu",
      "Ennan Zhai",
      "Dennis Cai",
      "Jiaqi Gao"
    ],
    "abstract": "Large-scale ML training jobs are frequently interrupted by hardware and\nsoftware anomalies, failures, and management events. Existing solutions like\ncheckpointing or runtime reconfiguration suffer from long downtimes, degraded\nperformance, or undesired changes to training strategies. We present\nTrainMover, a resilient runtime that leverages standby machines to handle\ninterruptions with minimal downtime and zero memory overhead. To achieve these\ngoals, TrainMover introduces two key techniques: two-phase, delta-based\ncommunication group setups and communication-free sandboxed shadow iterations.\nOur evaluation shows that TrainMover consistently achieves second-level\ndowntime across all evaluated models during migration, maintaining 99\\%\ntraining efficiency during periodic 10-minute rebalancing. We also demonstrate\nthe effectiveness of TrainMover in handling various interruptions.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG",
      "cs.PF"
    ],
    "primary_category": "cs.DC",
    "comment": "13 pages body, 17 pages total",
    "pdf_url": "http://arxiv.org/pdf/2412.12636v2",
    "published_date": "2024-12-17 07:59:31 UTC",
    "updated_date": "2025-04-26 13:44:28 UTC"
  },
  {
    "arxiv_id": "2412.12632v2",
    "title": "What External Knowledge is Preferred by LLMs? Characterizing and Exploring Chain of Evidence in Imperfect Context",
    "authors": [
      "Zhiyuan Chang",
      "Mingyang Li",
      "Xiaojun Jia",
      "Junjie Wang",
      "Yuekai Huang",
      "Qing Wang",
      "Yihao Huang",
      "Yang Liu"
    ],
    "abstract": "Incorporating external knowledge into large language models (LLMs) has\nemerged as a promising approach to mitigate outdated knowledge and\nhallucination in LLMs. However, external knowledge is often imperfect. In\naddition to useful knowledge, external knowledge is rich in irrelevant or\nmisinformation in the context that can impair the reliability of LLM responses.\nThis paper focuses on LLMs' preferred external knowledge in imperfect contexts\nwhen handling multi-hop QA. Inspired by criminal procedural law's Chain of\nEvidence (CoE), we characterize that knowledge preferred by LLMs should\nmaintain both relevance to the question and mutual support among knowledge\npieces. Accordingly, we propose an automated CoE discrimination approach and\nevaluate LLMs' effectiveness, faithfulness and robustness with CoE, including\nits application in the Retrieval-Augmented Generation (RAG). Tests on five LLMs\nshow CoE improves generation accuracy, answer faithfulness, robustness to\nknowledge conflicts, and boosts the performance of existing approaches in three\npractical RAG scenarios.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "15 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.12632v2",
    "published_date": "2024-12-17 07:49:49 UTC",
    "updated_date": "2025-05-16 01:49:37 UTC"
  },
  {
    "arxiv_id": "2412.12629v1",
    "title": "a2z-1 for Multi-Disease Detection in Abdomen-Pelvis CT: External Validation and Performance Analysis Across 21 Conditions",
    "authors": [
      "Pranav Rajpurkar",
      "Julian N. Acosta",
      "Siddhant Dogra",
      "Jaehwan Jeong",
      "Deepanshu Jindal",
      "Michael Moritz",
      "Samir Rajpurkar"
    ],
    "abstract": "We present a comprehensive evaluation of a2z-1, an artificial intelligence\n(AI) model designed to analyze abdomen-pelvis CT scans for 21 time-sensitive\nand actionable findings. Our study focuses on rigorous assessment of the\nmodel's performance and generalizability. Large-scale retrospective analysis\ndemonstrates an average AUC of 0.931 across 21 conditions. External validation\nacross two distinct health systems confirms consistent performance (AUC 0.923),\nestablishing generalizability to different evaluation scenarios, with notable\nperformance in critical findings such as small bowel obstruction (AUC 0.958)\nand acute pancreatitis (AUC 0.961). Subgroup analysis shows consistent accuracy\nacross patient sex, age groups, and varied imaging protocols, including\ndifferent slice thicknesses and contrast administration types. Comparison of\nhigh-confidence model outputs to radiologist reports reveals instances where\na2z-1 identified overlooked findings, suggesting potential for quality\nassurance applications.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12629v1",
    "published_date": "2024-12-17 07:44:25 UTC",
    "updated_date": "2024-12-17 07:44:25 UTC"
  },
  {
    "arxiv_id": "2412.12619v1",
    "title": "Phoneme-Level Feature Discrepancies: A Key to Detecting Sophisticated Speech Deepfakes",
    "authors": [
      "Kuiyuan Zhang",
      "Zhongyun Hua",
      "Rushi Lan",
      "Yushu Zhang",
      "Yifang Guo"
    ],
    "abstract": "Recent advancements in text-to-speech and speech conversion technologies have\nenabled the creation of highly convincing synthetic speech. While these\ninnovations offer numerous practical benefits, they also cause significant\nsecurity challenges when maliciously misused. Therefore, there is an urgent\nneed to detect these synthetic speech signals. Phoneme features provide a\npowerful speech representation for deepfake detection. However, previous\nphoneme-based detection approaches typically focused on specific phonemes,\noverlooking temporal inconsistencies across the entire phoneme sequence. In\nthis paper, we develop a new mechanism for detecting speech deepfakes by\nidentifying the inconsistencies of phoneme-level speech features. We design an\nadaptive phoneme pooling technique that extracts sample-specific phoneme-level\nfeatures from frame-level speech data. By applying this technique to features\nextracted by pre-trained audio models on previously unseen deepfake datasets,\nwe demonstrate that deepfake samples often exhibit phoneme-level\ninconsistencies when compared to genuine speech. To further enhance detection\naccuracy, we propose a deepfake detector that uses a graph attention network to\nmodel the temporal dependencies of phoneme-level features. Additionally, we\nintroduce a random phoneme substitution augmentation technique to increase\nfeature diversity during training. Extensive experiments on four benchmark\ndatasets demonstrate the superior performance of our method over existing\nstate-of-the-art detection methods.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12619v1",
    "published_date": "2024-12-17 07:31:19 UTC",
    "updated_date": "2024-12-17 07:31:19 UTC"
  },
  {
    "arxiv_id": "2412.12612v2",
    "title": "Auto-Cypher: Improving LLMs on Cypher generation via LLM-supervised generation-verification framework",
    "authors": [
      "Aman Tiwari",
      "Shiva Krishna Reddy Malay",
      "Vikas Yadav",
      "Masoud Hashemi",
      "Sathwik Tejaswi Madhusudhan"
    ],
    "abstract": "Graph databases like Neo4j are gaining popularity for handling complex,\ninterconnected data, over traditional relational databases in modeling and\nquerying relationships. While translating natural language into SQL queries is\nwell-researched, generating Cypher queries for Neo4j remains relatively\nunderexplored. In this work, we present an automated, LLM-Supervised, pipeline\nto generate high-quality synthetic data for Text2Cypher. Our Cypher data\ngeneration pipeline introduces LLM-As-Database-Filler, a novel strategy for\nensuring Cypher query correctness, thus resulting in high quality generations.\nUsing our pipeline, we generate high quality Text2Cypher data - SynthCypher\ncontaining 29.8k instances across various domains and queries with varying\ncomplexities. Training open-source LLMs like LLaMa-3.1-8B, Mistral-7B, and\nQWEN-7B on SynthCypher results in performance gains of up to 40% on the\nText2Cypher test split and 30% on the SPIDER benchmark, adapted for graph\ndatabases.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at NAACL 2025 main conference",
    "pdf_url": "http://arxiv.org/pdf/2412.12612v2",
    "published_date": "2024-12-17 07:21:25 UTC",
    "updated_date": "2025-01-24 05:52:51 UTC"
  },
  {
    "arxiv_id": "2412.12606v1",
    "title": "Multi-Dimensional Insights: Benchmarking Real-World Personalization in Large Multimodal Models",
    "authors": [
      "YiFan Zhang",
      "Shanglin Lei",
      "Runqi Qiao",
      "Zhuoma GongQue",
      "Xiaoshuai Song",
      "Guanting Dong",
      "Qiuna Tan",
      "Zhe Wei",
      "Peiqing Yang",
      "Ye Tian",
      "Yadong Xue",
      "Xiaofei Wang",
      "Honggang Zhang"
    ],
    "abstract": "The rapidly developing field of large multimodal models (LMMs) has led to the\nemergence of diverse models with remarkable capabilities. However, existing\nbenchmarks fail to comprehensively, objectively and accurately evaluate whether\nLMMs align with the diverse needs of humans in real-world scenarios. To bridge\nthis gap, we propose the Multi-Dimensional Insights (MDI) benchmark, which\nincludes over 500 images covering six common scenarios of human life. Notably,\nthe MDI-Benchmark offers two significant advantages over existing evaluations:\n(1) Each image is accompanied by two types of questions: simple questions to\nassess the model's understanding of the image, and complex questions to\nevaluate the model's ability to analyze and reason beyond basic content. (2)\nRecognizing that people of different age groups have varying needs and\nperspectives when faced with the same scenario, our benchmark stratifies\nquestions into three age categories: young people, middle-aged people, and\nolder people. This design allows for a detailed assessment of LMMs'\ncapabilities in meeting the preferences and needs of different age groups. With\nMDI-Benchmark, the strong model like GPT-4o achieve 79% accuracy on age-related\ntasks, indicating that existing LMMs still have considerable room for\nimprovement in addressing real-world applications. Looking ahead, we anticipate\nthat the MDI-Benchmark will open new pathways for aligning real-world\npersonalization in LMMs. The MDI-Benchmark data and evaluation code are\navailable at https://mdi-benchmark.github.io/",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "33 pages, 33 figures, Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2412.12606v1",
    "published_date": "2024-12-17 07:06:10 UTC",
    "updated_date": "2024-12-17 07:06:10 UTC"
  },
  {
    "arxiv_id": "2412.12605v1",
    "title": "An Advantage-based Optimization Method for Reinforcement Learning in Large Action Space",
    "authors": [
      "Hai Lin",
      "Cheng Huang",
      "Zhihong Chen"
    ],
    "abstract": "Reinforcement learning tasks in real-world scenarios often involve large,\nhigh-dimensional action spaces, leading to challenges such as convergence\ndifficulties, instability, and high computational complexity. It is widely\nacknowledged that traditional value-based reinforcement learning algorithms\nstruggle to address these issues effectively. A prevalent approach involves\ngenerating independent sub-actions within each dimension of the action space.\nHowever, this method introduces bias, hindering the learning of optimal\npolicies. In this paper, we propose an advantage-based optimization method and\nan algorithm named Advantage Branching Dueling Q-network (ABQ). ABQ\nincorporates a baseline mechanism to tune the action value of each dimension,\nleveraging the advantage relationship across different sub-actions. With this\napproach, the learned policy can be optimized for each dimension. Empirical\nresults demonstrate that ABQ outperforms BDQ, achieving 3%, 171%, and 84% more\ncumulative rewards in HalfCheetah, Ant, and Humanoid environments,\nrespectively. Furthermore, ABQ exhibits competitive performance when compared\nagainst two continuous action benchmark algorithms, DDPG and TD3.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12605v1",
    "published_date": "2024-12-17 07:04:39 UTC",
    "updated_date": "2024-12-17 07:04:39 UTC"
  },
  {
    "arxiv_id": "2412.12587v1",
    "title": "Distributed satellite information networks: Architecture, enabling technologies, and trends",
    "authors": [
      "Qinyu Zhang",
      "Liang Xu",
      "Jianhao Huang",
      "Tao Yang",
      "Jian Jiao",
      "Ye Wang",
      "Yao Shi",
      "Chiya Zhang",
      "Xingjian Zhang",
      "Ke Zhang",
      "Yupeng Gong",
      "Na Deng",
      "Nan Zhao",
      "Zhen Gao",
      "Shujun Han",
      "Xiaodong Xu",
      "Li You",
      "Dongming Wang",
      "Shan Jiang",
      "Dixian Zhao",
      "Nan Zhang",
      "Liujun Hu",
      "Xiongwen He",
      "Yonghui Li",
      "Xiqi Gao",
      "Xiaohu You"
    ],
    "abstract": "Driven by the vision of ubiquitous connectivity and wireless intelligence,\nthe evolution of ultra-dense constellation-based satellite-integrated Internet\nis underway, now taking preliminary shape. Nevertheless, the entrenched\ninstitutional silos and limited, nonrenewable heterogeneous network resources\nleave current satellite systems struggling to accommodate the escalating\ndemands of next-generation intelligent applications. In this context, the\ndistributed satellite information networks (DSIN), exemplified by the cohesive\nclustered satellites system, have emerged as an innovative architecture,\nbridging information gaps across diverse satellite systems, such as\ncommunication, navigation, and remote sensing, and establishing a unified, open\ninformation network paradigm to support resilient space information services.\nThis survey first provides a profound discussion about innovative network\narchitectures of DSIN, encompassing distributed regenerative satellite network\narchitecture, distributed satellite computing network architecture, and\nreconfigurable satellite formation flying, to enable flexible and scalable\ncommunication, computing and control. The DSIN faces challenges from network\nheterogeneity, unpredictable channel dynamics, sparse resources, and\ndecentralized collaboration frameworks. To address these issues, a series of\nenabling technologies is identified, including channel modeling and estimation,\ncloud-native distributed MIMO cooperation, grant-free massive access, network\nrouting, and the proper combination of all these diversity techniques.\nFurthermore, to heighten the overall resource efficiency, the cross-layer\noptimization techniques are further developed to meet upper-layer\ndeterministic, adaptive and secure information services requirements. In\naddition, emerging research directions and new opportunities are highlighted on\nthe way to achieving the DSIN vision.",
    "categories": [
      "cs.IT",
      "cs.AI",
      "cs.NI",
      "math.IT"
    ],
    "primary_category": "cs.IT",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12587v1",
    "published_date": "2024-12-17 06:44:05 UTC",
    "updated_date": "2024-12-17 06:44:05 UTC"
  },
  {
    "arxiv_id": "2412.15268v2",
    "title": "Enhancing LLM-based Hatred and Toxicity Detection with Meta-Toxic Knowledge Graph",
    "authors": [
      "Yibo Zhao",
      "Jiapeng Zhu",
      "Can Xu",
      "Xiang Li"
    ],
    "abstract": "The rapid growth of social media platforms has raised significant concerns\nregarding online content toxicity. When Large Language Models (LLMs) are used\nfor toxicity detection, two key challenges emerge: 1) the absence of\ndomain-specific toxic knowledge leads to false negatives; 2) the excessive\nsensitivity of LLMs to toxic speech results in false positives, limiting\nfreedom of speech. To address these issues, we propose a novel method called\nMetaTox, leveraging graph search on a meta-toxic knowledge graph to enhance\nhatred and toxicity detection. First, we construct a comprehensive meta-toxic\nknowledge graph by utilizing LLMs to extract toxic information through a\nthree-step pipeline, with toxic benchmark datasets serving as corpora. Second,\nwe query the graph via retrieval and ranking processes to supplement accurate,\nrelevant toxic knowledge. Extensive experiments and in-depth case studies\nacross multiple datasets demonstrate that our MetaTox significantly decreases\nthe false positive rate while boosting overall toxicity detection performance.\nOur code will be available soon.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages of content",
    "pdf_url": "http://arxiv.org/pdf/2412.15268v2",
    "published_date": "2024-12-17 06:28:28 UTC",
    "updated_date": "2024-12-24 04:38:57 UTC"
  },
  {
    "arxiv_id": "2412.12575v1",
    "title": "SIDE: Socially Informed Drought Estimation Toward Understanding Societal Impact Dynamics of Environmental Crisis",
    "authors": [
      "Lanyu Shang",
      "Bozhang Chen",
      "Shiwei Liu",
      "Yang Zhang",
      "Ruohan Zong",
      "Anav Vora",
      "Ximing Cai",
      "Na Wei",
      "Dong Wang"
    ],
    "abstract": "Drought has become a critical global threat with significant societal impact.\nExisting drought monitoring solutions primarily focus on assessing drought\nseverity using quantitative measurements, overlooking the diverse societal\nimpact of drought from human-centric perspectives. Motivated by the collective\nintelligence on social media and the computational power of AI, this paper\nstudies a novel problem of socially informed AI-driven drought estimation that\naims to leverage social and news media information to jointly estimate drought\nseverity and its societal impact. Two technical challenges exist: 1) How to\nmodel the implicit temporal dynamics of drought societal impact. 2) How to\ncapture the social-physical interdependence between the physical drought\ncondition and its societal impact. To address these challenges, we develop\nSIDE, a socially informed AI-driven drought estimation framework that\nexplicitly quantifies the societal impact of drought and effectively models the\nsocial-physical interdependency for joint severity-impact estimation.\nExperiments on real-world datasets from California and Texas demonstrate SIDE's\nsuperior performance compared to state-of-the-art baselines in accurately\nestimating drought severity and its societal impact. SIDE offers valuable\ninsights for developing human-centric drought mitigation strategies to foster\nsustainable and resilient communities.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "To be published in AAAI 25",
    "pdf_url": "http://arxiv.org/pdf/2412.12575v1",
    "published_date": "2024-12-17 06:11:46 UTC",
    "updated_date": "2024-12-17 06:11:46 UTC"
  },
  {
    "arxiv_id": "2412.12572v1",
    "title": "License Plate Detection and Character Recognition Using Deep Learning and Font Evaluation",
    "authors": [
      "Zahra Ebrahimi Vargoorani",
      "Ching Yee Suen"
    ],
    "abstract": "License plate detection (LPD) is essential for traffic management, vehicle\ntracking, and law enforcement but faces challenges like variable lighting and\ndiverse font types, impacting accuracy. Traditionally reliant on image\nprocessing and machine learning, the field is now shifting towards deep\nlearning for its robust performance in various conditions. Current methods,\nhowever, often require tailoring to specific regional datasets. This paper\nproposes a dual deep learning strategy using a Faster R-CNN for detection and a\nCNN-RNN model with Connectionist Temporal Classification (CTC) loss and a\nMobileNet V3 backbone for recognition. This approach aims to improve model\nperformance using datasets from Ontario, Quebec, California, and New York\nState, achieving a recall rate of 92% on the Centre for Pattern Recognition and\nMachine Intelligence (CENPARMI) dataset and 90% on the UFPR-ALPR dataset. It\nincludes a detailed error analysis to identify the causes of false positives.\nAdditionally, the research examines the role of font features in license plate\n(LP) recognition, analyzing fonts like Driver Gothic, Dreadnought, California\nClarendon, and Zurich Extra Condensed with the OpenALPR system. It discovers\nsignificant performance discrepancies influenced by font characteristics,\noffering insights for future LPD system enhancements.\n  Keywords: Deep Learning, License Plate, Font Evaluation",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "68T10",
      "I.2.10; I.4.8; I.5.4"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 5 figures. This is the pre-Springer final accepted version.\n  The final version is published in Springer, Lecture Notes in Computer Science\n  (LNCS), Volume 14731, 2024. Springer Version of Record",
    "pdf_url": "http://arxiv.org/pdf/2412.12572v1",
    "published_date": "2024-12-17 06:03:42 UTC",
    "updated_date": "2024-12-17 06:03:42 UTC"
  },
  {
    "arxiv_id": "2412.12561v2",
    "title": "Tell Me What to Track: Infusing Robust Language Guidance for Enhanced Referring Multi-Object Tracking",
    "authors": [
      "Wenjun Huang",
      "Yang Ni",
      "Hanning Chen",
      "Yirui He",
      "Ian Bryant",
      "Yezi Liu",
      "Mohsen Imani"
    ],
    "abstract": "Referring multi-object tracking (RMOT) is an emerging cross-modal task that\naims to localize an arbitrary number of targets based on a language expression\nand continuously track them in a video. This intricate task involves reasoning\non multi-modal data and precise target localization with temporal association.\nHowever, prior studies overlook the imbalanced data distribution between\nnewborn targets and existing targets due to the nature of the task. In\naddition, they only indirectly fuse multi-modal features, struggling to deliver\nclear guidance on newborn target detection. To solve the above issues, we\nconduct a collaborative matching strategy to alleviate the impact of the\nimbalance, boosting the ability to detect newborn targets while maintaining\ntracking performance. In the encoder, we integrate and enhance the cross-modal\nand multi-scale fusion, overcoming the bottlenecks in previous work, where\nlimited multi-modal information is shared and interacted between feature maps.\nIn the decoder, we also develop a referring-infused adaptation that provides\nexplicit referring guidance through the query tokens. The experiments showcase\nthe superior performance of our model (+3.42%) compared to prior works,\ndemonstrating the effectiveness of our designs.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12561v2",
    "published_date": "2024-12-17 05:43:35 UTC",
    "updated_date": "2025-03-07 18:51:48 UTC"
  },
  {
    "arxiv_id": "2412.12559v2",
    "title": "EXIT: Context-Aware Extractive Compression for Enhancing Retrieval-Augmented Generation",
    "authors": [
      "Taeho Hwang",
      "Sukmin Cho",
      "Soyeong Jeong",
      "Hoyun Song",
      "SeungYoon Han",
      "Jong C. Park"
    ],
    "abstract": "We introduce EXIT, an extractive context compression framework that enhances\nboth the effectiveness and efficiency of retrieval-augmented generation (RAG)\nin question answering (QA). Current RAG systems often struggle when retrieval\nmodels fail to rank the most relevant documents, leading to the inclusion of\nmore context at the expense of latency and accuracy. While abstractive\ncompression methods can drastically reduce token counts, their token-by-token\ngeneration process significantly increases end-to-end latency. Conversely,\nexisting extractive methods reduce latency but rely on independent,\nnon-adaptive sentence selection, failing to fully utilize contextual\ninformation. EXIT addresses these limitations by classifying sentences from\nretrieved documents - while preserving their contextual dependencies - enabling\nparallelizable, context-aware extraction that adapts to query complexity and\nretrieval quality. Our evaluations on both single-hop and multi-hop QA tasks\nshow that EXIT consistently surpasses existing compression methods and even\nuncompressed baselines in QA accuracy, while also delivering substantial\nreductions in inference time and token count. By improving both effectiveness\nand efficiency, EXIT provides a promising direction for developing scalable,\nhigh-quality QA solutions in RAG pipelines. Our code is available at\nhttps://github.com/ThisIsHwang/EXIT",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "Under Review",
    "pdf_url": "http://arxiv.org/pdf/2412.12559v2",
    "published_date": "2024-12-17 05:38:27 UTC",
    "updated_date": "2024-12-18 13:08:36 UTC"
  },
  {
    "arxiv_id": "2412.12552v1",
    "title": "SAModified: A Foundation Model-Based Zero-Shot Approach for Refining Noisy Land-Use Land-Cover Maps",
    "authors": [
      "Sparsh Pekhale",
      "Rakshith Sathish",
      "Sathisha Basavaraju",
      "Divya Sharma"
    ],
    "abstract": "Land-use and land cover (LULC) analysis is critical in remote sensing, with\nwide-ranging applications across diverse fields such as agriculture, utilities,\nand urban planning. However, automating LULC map generation using machine\nlearning is rendered challenging due to noisy labels. Typically, the ground\ntruths (e.g. ESRI LULC, MapBioMass) have noisy labels that hamper the model's\nability to learn to accurately classify the pixels. Further, these erroneous\nlabels can significantly distort the performance metrics of a model, leading to\nmisleading evaluations. Traditionally, the ambiguous labels are rectified using\nunsupervised algorithms. These algorithms struggle not only with scalability\nbut also with generalization across different geographies. To overcome these\nchallenges, we propose a zero-shot approach using the foundation model, Segment\nAnything Model (SAM), to automatically delineate different land parcels/regions\nand leverage them to relabel the unsure pixels by using the local label\nstatistics within each detected region. We achieve a significant reduction in\nlabel noise and an improvement in the performance of the downstream\nsegmentation model by $\\approx 5\\%$ when trained with denoised labels.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12552v1",
    "published_date": "2024-12-17 05:23:00 UTC",
    "updated_date": "2024-12-17 05:23:00 UTC"
  },
  {
    "arxiv_id": "2412.12544v2",
    "title": "Seed-CTS: Unleashing the Power of Tree Search for Superior Performance in Competitive Coding Tasks",
    "authors": [
      "Hao Wang",
      "Boyi Liu",
      "Yufeng Zhang",
      "Jie Chen"
    ],
    "abstract": "Competition-level code generation tasks pose significant challenges for\ncurrent state-of-the-art large language models (LLMs). For example, on the\nLiveCodeBench-Hard dataset, models such as O1-Mini and O1-Preview achieve\npass@1 rates of only 0.366 and 0.143, respectively. While tree search\ntechniques have proven effective in domains like mathematics and general\ncoding, their potential in competition-level code generation remains\nunder-explored. In this work, we propose a novel token-level tree search method\nspecifically designed for code generation. Leveraging\nQwen2.5-Coder-32B-Instruct, our approach achieves a pass rate of 0.305 on\nLiveCodeBench-Hard, surpassing the pass@100 performance of GPT4o-0513 (0.245).\nFurthermore, by integrating Chain-of-Thought (CoT) prompting, we improve our\nmethod's performance to 0.351, approaching O1-Mini's pass@1 rate. To ensure\nreproducibility, we report the average number of generations required per\nproblem by our tree search method on the test set. Our findings underscore the\npotential of tree search to significantly enhance performance on\ncompetition-level code generation tasks. This opens up new possibilities for\nlarge-scale synthesis of challenging code problems supervised fine-tuning (SFT)\ndata, advancing competition-level code generation tasks.",
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12544v2",
    "published_date": "2024-12-17 05:10:21 UTC",
    "updated_date": "2024-12-28 02:30:02 UTC"
  },
  {
    "arxiv_id": "2412.12542v1",
    "title": "Bots against Bias: Critical Next Steps for Human-Robot Interaction",
    "authors": [
      "Katie Seaborn"
    ],
    "abstract": "We humans are biased - and our robotic creations are biased, too. Bias is a\nnatural phenomenon that drives our perceptions and behavior, including when it\ncomes to socially expressive robots that have humanlike features. Recognizing\nthat we embed bias, knowingly or not, within the design of such robots is\ncrucial to studying its implications for people in modern societies. In this\nchapter, I consider the multifaceted question of bias in the context of\nhumanoid, AI-enabled, and expressive social robots: Where does bias arise, what\ndoes it look like, and what can (or should) we do about it. I offer\nobservations on human-robot interaction (HRI) along two parallel tracks: (1)\nrobots designed in bias-conscious ways and (2) robots that may help us tackle\nbias in the human world. I outline a curated selection of cases for each track\ndrawn from the latest HRI research and positioned against social, legal, and\nethical factors. I also propose a set of critical next steps to tackle the\nchallenges and opportunities on bias within HRI research and practice.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12542v1",
    "published_date": "2024-12-17 05:09:36 UTC",
    "updated_date": "2024-12-17 05:09:36 UTC"
  },
  {
    "arxiv_id": "2412.12541v1",
    "title": "LLMCL-GEC: Advancing Grammatical Error Correction with LLM-Driven Curriculum Learning",
    "authors": [
      "Tao Fang",
      "Derek F. Wong",
      "Lusheng Zhang",
      "Keyan Jin",
      "Qiang Zhang",
      "Tianjiao Li",
      "Jinlong Hou",
      "Lidia S. Chao"
    ],
    "abstract": "While large-scale language models (LLMs) have demonstrated remarkable\ncapabilities in specific natural language processing (NLP) tasks, they may\nstill lack proficiency compared to specialized models in certain domains, such\nas grammatical error correction (GEC). Drawing inspiration from the concept of\ncurriculum learning, we have delved into refining LLMs into proficient GEC\nexperts by devising effective curriculum learning (CL) strategies. In this\npaper, we introduce a novel approach, termed LLM-based curriculum learning,\nwhich capitalizes on the robust semantic comprehension and discriminative\nprowess inherent in LLMs to gauge the complexity of GEC training data. Unlike\ntraditional curriculum learning techniques, our method closely mirrors human\nexpert-designed curriculums. Leveraging the proposed LLM-based CL method, we\nsequentially select varying levels of curriculums ranging from easy to hard,\nand iteratively train and refine using the pretrianed T5 and LLaMA series\nmodels. Through rigorous testing and analysis across diverse benchmark\nassessments in English GEC, including the CoNLL14 test, BEA19 test, and BEA19\ndevelopment sets, our approach showcases a significant performance boost over\nbaseline models and conventional curriculum learning methodologies.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Derek F. Wong is the corresponding author. The preprint version\n  consists of 15 Pages, 5 Figures, 5 Tables, and 3 Appendices",
    "pdf_url": "http://arxiv.org/pdf/2412.12541v1",
    "published_date": "2024-12-17 05:09:07 UTC",
    "updated_date": "2024-12-17 05:09:07 UTC"
  },
  {
    "arxiv_id": "2412.15267v3",
    "title": "Toxicity Detection towards Adaptability to Changing Perturbations",
    "authors": [
      "Hankun Kang",
      "Jianhao Chen",
      "Yongqi Li",
      "Xin Miao",
      "Mayi Xu",
      "Ming Zhong",
      "Yuanyuan Zhu",
      "Tieyun Qian"
    ],
    "abstract": "Toxicity detection is crucial for maintaining the peace of the society. While\nexisting methods perform well on normal toxic contents or those generated by\nspecific perturbation methods, they are vulnerable to evolving perturbation\npatterns. However, in real-world scenarios, malicious users tend to create new\nperturbation patterns for fooling the detectors. For example, some users may\ncircumvent the detector of large language models (LLMs) by adding `I am a\nscientist' at the beginning of the prompt. In this paper, we introduce a novel\nproblem, i.e., continual learning jailbreak perturbation patterns, into the\ntoxicity detection field. To tackle this problem, we first construct a new\ndataset generated by 9 types of perturbation patterns, 7 of them are summarized\nfrom prior work and 2 of them are developed by us. We then systematically\nvalidate the vulnerability of current methods on this new perturbation\npattern-aware dataset via both the zero-shot and fine tuned cross-pattern\ndetection. Upon this, we present the domain incremental learning paradigm and\nthe corresponding benchmark to ensure the detector's robustness to dynamically\nemerging types of perturbed toxic text. Our code and dataset are provided in\nthe appendix and will be publicly available at GitHub, by which we wish to\noffer new research opportunities for the security-relevant communities.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "There are still some flaws in the uploaded content, which may cause\n  confusion for readers. To be rigorous, we need to retract the paper for\n  optimization and improvement",
    "pdf_url": "http://arxiv.org/pdf/2412.15267v3",
    "published_date": "2024-12-17 05:04:57 UTC",
    "updated_date": "2025-03-04 04:49:58 UTC"
  },
  {
    "arxiv_id": "2412.12538v1",
    "title": "A Scalable Approach to Benchmarking the In-Conversation Differential Diagnostic Accuracy of a Health AI",
    "authors": [
      "Deep Bhatt",
      "Surya Ayyagari",
      "Anuruddh Mishra"
    ],
    "abstract": "Diagnostic errors in healthcare persist as a critical challenge, with\nincreasing numbers of patients turning to online resources for health\ninformation. While AI-powered healthcare chatbots show promise, there exists no\nstandardized and scalable framework for evaluating their diagnostic\ncapabilities. This study introduces a scalable benchmarking methodology for\nassessing health AI systems and demonstrates its application through August, an\nAI-driven conversational chatbot. Our methodology employs 400 validated\nclinical vignettes across 14 medical specialties, using AI-powered patient\nactors to simulate realistic clinical interactions. In systematic testing,\nAugust achieved a top-one diagnostic accuracy of 81.8% (327/400 cases) and a\ntop-two accuracy of 85.0% (340/400 cases), significantly outperforming\ntraditional symptom checkers. The system demonstrated 95.8% accuracy in\nspecialist referrals and required 47% fewer questions compared to conventional\nsymptom checkers (mean 16 vs 29 questions), while maintaining empathetic\ndialogue throughout consultations. These findings demonstrate the potential of\nAI chatbots to enhance healthcare delivery, though implementation challenges\nremain regarding real-world validation and integration of objective clinical\ndata. This research provides a reproducible framework for evaluating healthcare\nAI systems, contributing to the responsible development and deployment of AI in\nclinical settings.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12538v1",
    "published_date": "2024-12-17 05:02:33 UTC",
    "updated_date": "2024-12-17 05:02:33 UTC"
  },
  {
    "arxiv_id": "2412.12532v1",
    "title": "Addressing Small and Imbalanced Medical Image Datasets Using Generative Models: A Comparative Study of DDPM and PGGANs with Random and Greedy K Sampling",
    "authors": [
      "Iman Khazrak",
      "Shakhnoza Takhirova",
      "Mostafa M. Rezaee",
      "Mehrdad Yadollahi",
      "Robert C. Green II",
      "Shuteng Niu"
    ],
    "abstract": "The development of accurate medical image classification models is often\nconstrained by privacy concerns and data scarcity for certain conditions,\nleading to small and imbalanced datasets. To address these limitations, this\nstudy explores the use of generative models, such as Denoising Diffusion\nProbabilistic Models (DDPM) and Progressive Growing Generative Adversarial\nNetworks (PGGANs), for dataset augmentation. The research introduces a\nframework to assess the impact of synthetic images generated by DDPM and PGGANs\non the performance of four models: a custom CNN, Untrained VGG16, Pretrained\nVGG16, and Pretrained ResNet50. Experiments were conducted using Random\nSampling and Greedy K Sampling to create small, imbalanced datasets. The\nsynthetic images were evaluated using Frechet Inception Distance (FID) and\ncompared to original datasets through classification metrics. The results show\nthat DDPM consistently generated more realistic images with lower FID scores\nand significantly outperformed PGGANs in improving classification metrics\nacross all models and datasets. Incorporating DDPM-generated images into the\noriginal datasets increased accuracy by up to 6%, enhancing model robustness\nand stability, particularly in imbalanced scenarios. Random Sampling\ndemonstrated superior stability, while Greedy K Sampling offered diversity at\nthe cost of higher FID scores. This study highlights the efficacy of DDPM in\naugmenting small, imbalanced medical image datasets, improving model\nperformance by balancing the dataset and expanding its size.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12532v1",
    "published_date": "2024-12-17 04:42:50 UTC",
    "updated_date": "2024-12-17 04:42:50 UTC"
  },
  {
    "arxiv_id": "2412.12525v3",
    "title": "CREST: An Efficient Conjointly-trained Spike-driven Framework for Event-based Object Detection Exploiting Spatiotemporal Dynamics",
    "authors": [
      "Ruixin Mao",
      "Aoyu Shen",
      "Lin Tang",
      "Jun Zhou"
    ],
    "abstract": "Event-based cameras feature high temporal resolution, wide dynamic range, and\nlow power consumption, which is ideal for high-speed and low-light object\ndetection. Spiking neural networks (SNNs) are promising for event-based object\nrecognition and detection due to their spiking nature but lack efficient\ntraining methods, leading to gradient vanishing and high computational\ncomplexity, especially in deep SNNs. Additionally, existing SNN frameworks\noften fail to effectively handle multi-scale spatiotemporal features, leading\nto increased data redundancy and reduced accuracy. To address these issues, we\npropose CREST, a novel conjointly-trained spike-driven framework to exploit\nspatiotemporal dynamics in event-based object detection. We introduce the\nconjoint learning rule to accelerate SNN learning and alleviate gradient\nvanishing. It also supports dual operation modes for efficient and flexible\nimplementation on different hardware types. Additionally, CREST features a\nfully spike-driven framework with a multi-scale spatiotemporal event integrator\n(MESTOR) and a spatiotemporal-IoU (ST-IoU) loss. Our approach achieves superior\nobject recognition & detection performance and up to 100X energy efficiency\ncompared with state-of-the-art SNN algorithms on three datasets, providing an\nefficient solution for event-based object detection algorithms suitable for SNN\nhardware implementation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.12525v3",
    "published_date": "2024-12-17 04:33:31 UTC",
    "updated_date": "2025-01-19 08:06:33 UTC"
  },
  {
    "arxiv_id": "2412.15266v1",
    "title": "On the Structural Memory of LLM Agents",
    "authors": [
      "Ruihong Zeng",
      "Jinyuan Fang",
      "Siwei Liu",
      "Zaiqiao Meng"
    ],
    "abstract": "Memory plays a pivotal role in enabling large language model~(LLM)-based\nagents to engage in complex and long-term interactions, such as question\nanswering (QA) and dialogue systems. While various memory modules have been\nproposed for these tasks, the impact of different memory structures across\ntasks remains insufficiently explored. This paper investigates how memory\nstructures and memory retrieval methods affect the performance of LLM-based\nagents. Specifically, we evaluate four types of memory structures, including\nchunks, knowledge triples, atomic facts, and summaries, along with mixed memory\nthat combines these components. In addition, we evaluate three widely used\nmemory retrieval methods: single-step retrieval, reranking, and iterative\nretrieval. Extensive experiments conducted across four tasks and six datasets\nyield the following key insights: (1) Different memory structures offer\ndistinct advantages, enabling them to be tailored to specific tasks; (2) Mixed\nmemory structures demonstrate remarkable resilience in noisy environments; (3)\nIterative retrieval consistently outperforms other methods across various\nscenarios. Our investigation aims to inspire further research into the design\nof memory systems for LLM-based agents.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.15266v1",
    "published_date": "2024-12-17 04:30:00 UTC",
    "updated_date": "2024-12-17 04:30:00 UTC"
  },
  {
    "arxiv_id": "2412.12522v1",
    "title": "Solid-SQL: Enhanced Schema-linking based In-context Learning for Robust Text-to-SQL",
    "authors": [
      "Geling Liu",
      "Yunzhi Tan",
      "Ruichao Zhong",
      "Yuanzhen Xie",
      "Lingchen Zhao",
      "Qian Wang",
      "Bo Hu",
      "Zang Li"
    ],
    "abstract": "Recently, large language models (LLMs) have significantly improved the\nperformance of text-to-SQL systems. Nevertheless, many state-of-the-art (SOTA)\napproaches have overlooked the critical aspect of system robustness. Our\nexperiments reveal that while LLM-driven methods excel on standard datasets,\ntheir accuracy is notably compromised when faced with adversarial\nperturbations. To address this challenge, we propose a robust text-to-SQL\nsolution, called Solid-SQL, designed to integrate with various LLMs. We focus\non the pre-processing stage, training a robust schema-linking model enhanced by\nLLM-based data augmentation. Additionally, we design a two-round, structural\nsimilarity-based example retrieval strategy for in-context learning. Our method\nachieves SOTA SQL execution accuracy levels of 82.1% and 58.9% on the general\nSpider and Bird benchmarks, respectively. Furthermore, experimental results\nshow that Solid-SQL delivers an average improvement of 11.6% compared to\nbaselines on the perturbed Spider-Syn, Spider-Realistic, and Dr. Spider\nbenchmarks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at COLING 2025 Main",
    "pdf_url": "http://arxiv.org/pdf/2412.12522v1",
    "published_date": "2024-12-17 04:22:22 UTC",
    "updated_date": "2024-12-17 04:22:22 UTC"
  },
  {
    "arxiv_id": "2412.13224v1",
    "title": "Physics-model-guided Worst-case Sampling for Safe Reinforcement Learning",
    "authors": [
      "Hongpeng Cao",
      "Yanbing Mao",
      "Lui Sha",
      "Marco Caccamo"
    ],
    "abstract": "Real-world accidents in learning-enabled CPS frequently occur in challenging\ncorner cases. During the training of deep reinforcement learning (DRL) policy,\nthe standard setup for training conditions is either fixed at a single initial\ncondition or uniformly sampled from the admissible state space. This setup\noften overlooks the challenging but safety-critical corner cases. To bridge\nthis gap, this paper proposes a physics-model-guided worst-case sampling\nstrategy for training safe policies that can handle safety-critical cases\ntoward guaranteed safety. Furthermore, we integrate the proposed worst-case\nsampling strategy into the physics-regulated deep reinforcement learning\n(Phy-DRL) framework to build a more data-efficient and safe learning algorithm\nfor safety-critical CPS. We validate the proposed training strategy with\nPhy-DRL through extensive experiments on a simulated cart-pole system, a 2D\nquadrotor, a simulated and a real quadruped robot, showing remarkably improved\nsampling efficiency to learn more robust safe policies.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "under review",
    "pdf_url": "http://arxiv.org/pdf/2412.13224v1",
    "published_date": "2024-12-17 04:13:06 UTC",
    "updated_date": "2024-12-17 04:13:06 UTC"
  },
  {
    "arxiv_id": "2412.12500v1",
    "title": "Beyond Data Quantity: Key Factors Driving Performance in Multilingual Language Models",
    "authors": [
      "Sina Bagheri Nezhad",
      "Ameeta Agrawal",
      "Rhitabrat Pokharel"
    ],
    "abstract": "Multilingual language models (MLLMs) are crucial for handling text across\nvarious languages, yet they often show performance disparities due to\ndifferences in resource availability and linguistic characteristics. While the\nimpact of pre-train data percentage and model size on performance is\nwell-known, our study reveals additional critical factors that significantly\ninfluence MLLM effectiveness. Analyzing a wide range of features, including\ngeographical, linguistic, and resource-related aspects, we focus on the SIB-200\ndataset for classification and the Flores-200 dataset for machine translation,\nusing regression models and SHAP values across 204 languages. Our findings\nidentify token similarity and country similarity as pivotal factors, alongside\npre-train data and model size, in enhancing model performance. Token similarity\nfacilitates cross-lingual transfer, while country similarity highlights the\nimportance of shared cultural and linguistic contexts. These insights offer\nvaluable guidance for developing more equitable and effective multilingual\nlanguage models, particularly for underrepresented languages.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at The First Workshop on Language Models for Low-Resource\n  Languages @ COLING 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.12500v1",
    "published_date": "2024-12-17 03:05:26 UTC",
    "updated_date": "2024-12-17 03:05:26 UTC"
  },
  {
    "arxiv_id": "2412.15265v2",
    "title": "Chinese SafetyQA: A Safety Short-form Factuality Benchmark for Large Language Models",
    "authors": [
      "Yingshui Tan",
      "Boren Zheng",
      "Baihui Zheng",
      "Kerui Cao",
      "Huiyun Jing",
      "Jincheng Wei",
      "Jiaheng Liu",
      "Yancheng He",
      "Wenbo Su",
      "Xiangyong Zhu",
      "Bo Zheng",
      "Kaifu Zhang"
    ],
    "abstract": "With the rapid advancement of Large Language Models (LLMs), significant\nsafety concerns have emerged. Fundamentally, the safety of large language\nmodels is closely linked to the accuracy, comprehensiveness, and clarity of\ntheir understanding of safety knowledge, particularly in domains such as law,\npolicy and ethics. This factuality ability is crucial in determining whether\nthese models can be deployed and applied safely and compliantly within specific\nregions. To address these challenges and better evaluate the factuality ability\nof LLMs to answer short questions, we introduce the Chinese SafetyQA benchmark.\nChinese SafetyQA has several properties (i.e., Chinese, Diverse, High-quality,\nStatic, Easy-to-evaluate, Safety-related, Harmless). Based on Chinese SafetyQA,\nwe perform a comprehensive evaluation on the factuality abilities of existing\nLLMs and analyze how these capabilities relate to LLM abilities, e.g., RAG\nability and robustness against attacks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.15265v2",
    "published_date": "2024-12-17 03:03:44 UTC",
    "updated_date": "2024-12-23 11:06:56 UTC"
  },
  {
    "arxiv_id": "2412.12499v2",
    "title": "LinguaLIFT: An Effective Two-stage Instruction Tuning Framework for Low-Resource Language Reasoning",
    "authors": [
      "Hongbin Zhang",
      "Kehai Chen",
      "Xuefeng Bai",
      "Yang Xiang",
      "Min Zhang"
    ],
    "abstract": "Large language models (LLMs) have exhibited impressive multilingual reasoning\ncapabilities, driven by extensive multilingual pre-training corpora and\ninstruction fine-tuning data. However, a performance gap exists between high-\nand low-resource language reasoning tasks due to the language imbalance in the\npre-training corpus, which is exacerbated by evaluation bias in existing\nreasoning benchmarks lacking low-resource language coverage. To alleviate this\nissue, we propose LinguaLIFT, a two-stage instruction tuning framework for\nadvancing low-resource language reasoning. LinguaLIFT employs a language\nalignment layer to capture multilingual alignment in a code-switched tuning way\nwithout requiring multilingual instruction or parallel data, thereby\ntransferring the cross-lingual reasoning capabilities to low-resource languages\nthrough English-only instruction tuning data. To comprehensively evaluate the\nmultilingual reasoning capabilities, we introduce the Multilingual Math World\nProblem (MMWP) benchmark, which spans 21 low-resource, 17 medium-resource, and\n10 high-resource languages. Experimental results show that LinguaLIFT\noutperforms several competitive baselines across MMWP and four widely used\nbenchmarks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12499v2",
    "published_date": "2024-12-17 03:03:17 UTC",
    "updated_date": "2025-02-17 13:20:15 UTC"
  },
  {
    "arxiv_id": "2412.12496v4",
    "title": "Faster Vision Mamba is Rebuilt in Minutes via Merged Token Re-training",
    "authors": [
      "Mingjia Shi",
      "Yuhao Zhou",
      "Ruiji Yu",
      "Zekai Li",
      "Zhiyuan Liang",
      "Xuanlei Zhao",
      "Xiaojiang Peng",
      "Shanmukha Ramakrishna Vedantam",
      "Wangbo Zhao",
      "Kai Wang",
      "Yang You"
    ],
    "abstract": "Vision Mamba has shown close to state of the art performance on computer\nvision tasks, drawing much interest in increasing it's efficiency. A promising\napproach is token reduction (that has been successfully implemented in ViTs).\nPruning informative tokens in Mamba leads to a high loss of key knowledge and\ndegraded performance. An alternative, of merging tokens preserves more\ninformation than pruning, also suffers for large compression ratios. Our key\ninsight is that a quick round of retraining after token merging yeilds robust\nresults across various compression ratios. Empirically, pruned Vims only drop\nup to 0.9% accuracy on ImageNet-1K, recovered by our proposed framework R-MeeTo\nin our main evaluation. We show how simple and effective the fast recovery can\nbe achieved at minute-level, in particular, a 35.9% accuracy spike over 3\nepochs of training on Vim-Ti. Moreover, Vim-Ti/S/B are re-trained within 5/7/17\nminutes, and Vim-S only drops 1.3% with 1.2x (up to 1.5x) speed up in\ninference.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "68T07",
      "I.2"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12496v4",
    "published_date": "2024-12-17 02:56:35 UTC",
    "updated_date": "2025-04-14 09:37:17 UTC"
  },
  {
    "arxiv_id": "2412.12493v1",
    "title": "A Simple and Fast Way to Handle Semantic Errors in Transactions",
    "authors": [
      "Jinghan Zeng",
      "Eugene Wu",
      "Sanjay Krishnan"
    ],
    "abstract": "Many computer systems are now being redesigned to incorporate LLM-powered\nagents, enabling natural language input and more flexible operations. This\npaper focuses on handling database transactions created by large language\nmodels (LLMs). Transactions generated by LLMs may include semantic errors,\nrequiring systems to treat them as long-lived. This allows for human review\nand, if the transaction is incorrect, removal from the database history. Any\nremoval action must ensure the database's consistency (the \"C\" in ACID\nprinciples) is maintained throughout the process.\n  We propose a novel middleware framework based on Invariant Satisfaction\n(I-Confluence), which ensures consistency by identifying and coordinating\ndependencies between long-lived transactions and new transactions. This\nmiddleware buffers suspicious or compensating transactions to manage\ncoordination states. Using the TPC-C benchmark, we evaluate how transaction\ngeneration frequency, user reviews, and invariant completeness impact system\nperformance. For system researchers, this study establishes an interactive\nparadigm between LLMs and database systems, providing an \"undoing\" mechanism\nfor handling incorrect operations while guaranteeing database consistency. For\nsystem engineers, this paper offers a middleware design that integrates\nremovable LLM-generated transactions into existing systems with minimal\nmodifications.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "14 pages, 13 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.12493v1",
    "published_date": "2024-12-17 02:47:18 UTC",
    "updated_date": "2024-12-17 02:47:18 UTC"
  },
  {
    "arxiv_id": "2412.12486v2",
    "title": "Boosting Long-Context Management via Query-Guided Activation Refilling",
    "authors": [
      "Hongjin Qian",
      "Zheng Liu",
      "Peitian Zhang",
      "Zhicheng Dou",
      "Defu Lian"
    ],
    "abstract": "Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.12486v2",
    "published_date": "2024-12-17 02:43:54 UTC",
    "updated_date": "2024-12-18 05:08:39 UTC"
  },
  {
    "arxiv_id": "2412.12484v1",
    "title": "Evolutionary Optimization for Designing Variational Quantum Circuits with High Model Capacity",
    "authors": [
      "Samuel Yen-Chi Chen"
    ],
    "abstract": "Recent advancements in quantum computing (QC) and machine learning (ML) have\ngarnered significant attention, leading to substantial efforts toward the\ndevelopment of quantum machine learning (QML) algorithms to address a variety\nof complex challenges. The design of high-performance QML models, however,\nrequires expert-level knowledge, posing a significant barrier to the widespread\nadoption of QML. Key challenges include the design of data encoding mechanisms\nand parameterized quantum circuits, both of which critically impact the\ngeneralization capabilities of QML models. We propose a novel method that\nencodes quantum circuit architecture information to enable the evolution of\nquantum circuit designs. In this approach, the fitness function is based on the\neffective dimension, allowing for the optimization of quantum circuits towards\nhigher model capacity. Through numerical simulations, we demonstrate that the\nproposed method is capable of discovering variational quantum circuit\narchitectures that offer improved learning capabilities, thereby enhancing the\noverall performance of QML models for complex tasks.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.ET",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "quant-ph",
    "comment": "Accepted by IEEE Symposium Series on Computational Intelligence -\n  IEEE SSCI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.12484v1",
    "published_date": "2024-12-17 02:40:35 UTC",
    "updated_date": "2024-12-17 02:40:35 UTC"
  },
  {
    "arxiv_id": "2412.12480v4",
    "title": "Subversion Strategy Eval: Can language models statelessly strategize to subvert control protocols?",
    "authors": [
      "Alex Mallen",
      "Charlie Griffin",
      "Misha Wagner",
      "Alessandro Abate",
      "Buck Shlegeris"
    ],
    "abstract": "An AI control protocol is a plan for usefully deploying AI systems that aims\nto prevent an AI from intentionally causing some unacceptable outcome. This\npaper investigates how well AI systems can generate and act on their own\nstrategies for subverting control protocols whilst operating statelessly\n(without shared memory between contexts). To do this, an AI system may need to\nreliably generate optimal plans in each context, take actions with\nwell-calibrated probabilities, and coordinate plans with other instances of\nitself without communicating. We develop Subversion Strategy Eval, a suite of\neight environments, covering a range of protocols and strategic capabilities,\nand six sets of affordances that help isolate individual capabilities. We\nimplement the evaluation in Inspect-AI and release it open-source. We evaluate\nClaude 3.5 models, including helpful-only versions, as well as OpenAI reasoning\nmodels. None of the models demonstrate substantial capability in strategizing\nto subvert control protocols statelessly. However, providing models with\nadditional affordances, such as the ability to share a plan between contexts,\ncan substantially improve performance. We hope our evaluations can act as a\nleading indicator for when models are capable of subverting control protocols\nand also relax the worst-case assumption of perfect strategic ability in AI\ncontrol evaluations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12480v4",
    "published_date": "2024-12-17 02:33:45 UTC",
    "updated_date": "2025-04-04 16:36:02 UTC"
  },
  {
    "arxiv_id": "2501.17164v1",
    "title": "Split Knowledge Distillation for Large Models in IoT: Architecture, Challenges, and Solutions",
    "authors": [
      "Zuguang Li",
      "Wen Wu",
      "Shaohua Wu",
      "Qiaohua Lin",
      "Yaping Sun",
      "Hui Wang"
    ],
    "abstract": "Large models (LMs) have immense potential in Internet of Things (IoT)\nsystems, enabling applications such as intelligent voice assistants, predictive\nmaintenance, and healthcare monitoring. However, training LMs on edge servers\nraises data privacy concerns, while deploying them directly on IoT devices is\nconstrained by limited computational and memory resources. We analyze the key\nchallenges of training LMs in IoT systems, including energy constraints,\nlatency requirements, and device heterogeneity, and propose potential solutions\nsuch as dynamic resource management, adaptive model partitioning, and clustered\ncollaborative training. Furthermore, we propose a split knowledge distillation\nframework to efficiently distill LMs into smaller, deployable versions for IoT\ndevices while ensuring raw data remains local. This framework integrates\nknowledge distillation and split learning to minimize energy consumption and\nmeet low model training delay requirements. A case study is presented to\nevaluate the feasibility and performance of the proposed framework.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "7 pages, 4figures, 2 tables, and 15 conference",
    "pdf_url": "http://arxiv.org/pdf/2501.17164v1",
    "published_date": "2024-12-17 02:31:31 UTC",
    "updated_date": "2024-12-17 02:31:31 UTC"
  },
  {
    "arxiv_id": "2412.12475v2",
    "title": "RareAgents: Advancing Rare Disease Care through LLM-Empowered Multi-disciplinary Team",
    "authors": [
      "Xuanzhong Chen",
      "Ye Jin",
      "Xiaohao Mao",
      "Lun Wang",
      "Shuyang Zhang",
      "Ting Chen"
    ],
    "abstract": "Rare diseases, despite their low individual incidence, collectively impact\naround 300 million people worldwide due to the vast number of diseases. The\ninvolvement of multiple organs and systems, and the shortage of specialized\ndoctors with relevant experience make diagnosing and treating rare diseases\nmore challenging than common diseases. Recently, agents powered by large\nlanguage models (LLMs) have demonstrated notable applications across various\ndomains. In the medical field, some agent methods have outperformed direct\nprompts in question-answering tasks from medical examinations. However, current\nagent frameworks are not well-adapted to real-world clinical scenarios,\nespecially those involving the complex demands of rare diseases. To bridge this\ngap, we introduce RareAgents, the first LLM-driven multi-disciplinary team\nframework designed specifically for the complex clinical context of rare\ndiseases. RareAgents integrates advanced Multidisciplinary Team (MDT)\ncoordination, memory mechanisms, and medical tools utilization, leveraging\nLlama-3.1-8B/70B as the base model. Experimental results show that RareAgents\noutperforms state-of-the-art domain-specific models, GPT-4o, and current agent\nframeworks in differential diagnosis and medication recommendation for rare\ndiseases. Furthermore, we contribute a novel rare disease dataset,\nMIMIC-IV-Ext-Rare, to support further advancements in this field.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12475v2",
    "published_date": "2024-12-17 02:22:24 UTC",
    "updated_date": "2025-02-14 08:40:39 UTC"
  },
  {
    "arxiv_id": "2412.15264v3",
    "title": "ReXTrust: A Model for Fine-Grained Hallucination Detection in AI-Generated Radiology Reports",
    "authors": [
      "Romain Hardy",
      "Sung Eun Kim",
      "Du Hyun Ro",
      "Pranav Rajpurkar"
    ],
    "abstract": "The increasing adoption of AI-generated radiology reports necessitates robust\nmethods for detecting hallucinations--false or unfounded statements that could\nimpact patient care. We present ReXTrust, a novel framework for fine-grained\nhallucination detection in AI-generated radiology reports. Our approach\nleverages sequences of hidden states from large vision-language models to\nproduce finding-level hallucination risk scores. We evaluate ReXTrust on a\nsubset of the MIMIC-CXR dataset and demonstrate superior performance compared\nto existing approaches, achieving an AUROC of 0.8751 across all findings and\n0.8963 on clinically significant findings. Our results show that white-box\napproaches leveraging model hidden states can provide reliable hallucination\ndetection for medical AI systems, potentially improving the safety and\nreliability of automated radiology reporting.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to AIMedHealth 10 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.15264v3",
    "published_date": "2024-12-17 02:07:33 UTC",
    "updated_date": "2025-01-31 03:15:34 UTC"
  },
  {
    "arxiv_id": "2412.12469v1",
    "title": "Optimal Control Operator Perspective and a Neural Adaptive Spectral Method",
    "authors": [
      "Mingquan Feng",
      "Zhijie Chen",
      "Yixin Huang",
      "Yizhou Liu",
      "Junchi Yan"
    ],
    "abstract": "Optimal control problems (OCPs) involve finding a control function for a\ndynamical system such that a cost functional is optimized. It is central to\nphysical systems in both academia and industry. In this paper, we propose a\nnovel instance-solution control operator perspective, which solves OCPs in a\none-shot manner without direct dependence on the explicit expression of\ndynamics or iterative optimization processes. The control operator is\nimplemented by a new neural operator architecture named Neural Adaptive\nSpectral Method (NASM), a generalization of classical spectral methods. We\ntheoretically validate the perspective and architecture by presenting the\napproximation error bounds of NASM for the control operator. Experiments on\nsynthetic environments and a real-world dataset verify the effectiveness and\nefficiency of our approach, including substantial speedup in running time, and\nhigh-quality in- and out-of-distribution generalization.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "math.OC"
    ],
    "primary_category": "eess.SY",
    "comment": "Accepted for publication at AAAl'25. Extended version with full\n  appendix, 22 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.12469v1",
    "published_date": "2024-12-17 02:06:34 UTC",
    "updated_date": "2024-12-17 02:06:34 UTC"
  },
  {
    "arxiv_id": "2412.12468v2",
    "title": "Transferable and Forecastable User Targeting Foundation Model",
    "authors": [
      "Bin Dou",
      "Baokun Wang",
      "Yun Zhu",
      "Xiaotong Lin",
      "Yike Xu",
      "Xiaorui Huang",
      "Yang Chen",
      "Yun Liu",
      "Shaoshuai Han",
      "Yongchao Liu",
      "Tianyi Zhang",
      "Yu Cheng",
      "Weiqiang Wang",
      "Chuntao Hong"
    ],
    "abstract": "User targeting, the process of selecting targeted users from a pool of\ncandidates for non-expert marketers, has garnered substantial attention with\nthe advancements in digital marketing. However, existing user targeting methods\nencounter two significant challenges: (i) Poor cross-domain and cross-scenario\ntransferability and generalization, and (ii) Insufficient forecastability in\nreal-world applications. These limitations hinder their applicability across\ndiverse industrial scenarios. In this work, we propose FOUND, an\nindustrial-grade, transferable, and forecastable user targeting foundation\nmodel. To enhance cross-domain transferability, our framework integrates\nheterogeneous multi-scenario user data, aligning them with one-sentence\ntargeting demand inputs through contrastive pre-training. For improved\nforecastability, the text description of each user is derived based on\nanticipated future behaviors, while user representations are constructed from\nhistorical information. Experimental results demonstrate that our approach\nsignificantly outperforms existing baselines in cross-domain, real-world user\ntargeting scenarios, showcasing the superior capabilities of FOUND. Moreover,\nour method has been successfully deployed on the Alipay platform and is widely\nutilized across various scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 6 figures, accept by The ACM Web Conference 2025 (WWW 2025)\n  Industry Track",
    "pdf_url": "http://arxiv.org/pdf/2412.12468v2",
    "published_date": "2024-12-17 02:05:09 UTC",
    "updated_date": "2025-02-20 14:57:03 UTC"
  },
  {
    "arxiv_id": "2412.12463v2",
    "title": "Pattern Analogies: Learning to Perform Programmatic Image Edits by Analogy",
    "authors": [
      "Aditya Ganeshan",
      "Thibault Groueix",
      "Paul Guerrero",
      "Radomír Měch",
      "Matthew Fisher",
      "Daniel Ritchie"
    ],
    "abstract": "Pattern images are everywhere in the digital and physical worlds, and tools\nto edit them are valuable. But editing pattern images is tricky: desired edits\nare often programmatic: structure-aware edits that alter the underlying program\nwhich generates the pattern. One could attempt to infer this underlying\nprogram, but current methods for doing so struggle with complex images and\nproduce unorganized programs that make editing tedious. In this work, we\nintroduce a novel approach to perform programmatic edits on pattern images. By\nusing a pattern analogy -- a pair of simple patterns to demonstrate the\nintended edit -- and a learning-based generative model to execute these edits,\nour method allows users to intuitively edit patterns. To enable this paradigm,\nwe introduce SplitWeave, a domain-specific language that, combined with a\nframework for sampling synthetic pattern analogies, enables the creation of a\nlarge, high-quality synthetic training dataset. We also present TriFuser, a\nLatent Diffusion Model (LDM) designed to overcome critical issues that arise\nwhen naively deploying LDMs to this task. Extensive experiments on real-world,\nartist-sourced patterns reveals that our method faithfully performs the\ndemonstrated edit while also generalizing to related pattern styles beyond its\ntraining distribution.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.HC"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2024 - Website: https://bardofcodes.github.io/patterns/",
    "pdf_url": "http://arxiv.org/pdf/2412.12463v2",
    "published_date": "2024-12-17 01:52:12 UTC",
    "updated_date": "2025-04-05 16:33:40 UTC"
  },
  {
    "arxiv_id": "2412.12459v1",
    "title": "LITA: An Efficient LLM-assisted Iterative Topic Augmentation Framework",
    "authors": [
      "Chia-Hsuan Chang",
      "Jui-Tse Tsai",
      "Yi-Hang Tsai",
      "San-Yih Hwang"
    ],
    "abstract": "Topic modeling is widely used for uncovering thematic structures within text\ncorpora, yet traditional models often struggle with specificity and coherence\nin domain-focused applications. Guided approaches, such as SeededLDA and CorEx,\nincorporate user-provided seed words to improve relevance but remain\nlabor-intensive and static. Large language models (LLMs) offer potential for\ndynamic topic refinement and discovery, yet their application often incurs high\nAPI costs. To address these challenges, we propose the LLM-assisted Iterative\nTopic Augmentation framework (LITA), an LLM-assisted approach that integrates\nuser-provided seeds with embedding-based clustering and iterative refinement.\nLITA identifies a small number of ambiguous documents and employs an LLM to\nreassign them to existing or new topics, minimizing API costs while enhancing\ntopic quality. Experiments on two datasets across topic quality and clustering\nperformance metrics demonstrate that LITA outperforms five baseline models,\nincluding LDA, SeededLDA, CorEx, BERTopic, and PromptTopic. Our work offers an\nefficient and adaptable framework for advancing topic modeling and text\nclustering.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "Under Review",
    "pdf_url": "http://arxiv.org/pdf/2412.12459v1",
    "published_date": "2024-12-17 01:43:44 UTC",
    "updated_date": "2024-12-17 01:43:44 UTC"
  },
  {
    "arxiv_id": "2412.12456v1",
    "title": "Graph Learning in the Era of LLMs: A Survey from the Perspective of Data, Models, and Tasks",
    "authors": [
      "Xunkai Li",
      "Zhengyu Wu",
      "Jiayi Wu",
      "Hanwen Cui",
      "Jishuo Jia",
      "Rong-Hua Li",
      "Guoren Wang"
    ],
    "abstract": "With the increasing prevalence of cross-domain Text-Attributed Graph (TAG)\nData (e.g., citation networks, recommendation systems, social networks, and\nai4science), the integration of Graph Neural Networks (GNNs) and Large Language\nModels (LLMs) into a unified Model architecture (e.g., LLM as enhancer, LLM as\ncollaborators, LLM as predictor) has emerged as a promising technological\nparadigm. The core of this new graph learning paradigm lies in the synergistic\ncombination of GNNs' ability to capture complex structural relationships and\nLLMs' proficiency in understanding informative contexts from the rich textual\ndescriptions of graphs. Therefore, we can leverage graph description texts with\nrich semantic context to fundamentally enhance Data quality, thereby improving\nthe representational capacity of model-centric approaches in line with\ndata-centric machine learning principles. By leveraging the strengths of these\ndistinct neural network architectures, this integrated approach addresses a\nwide range of TAG-based Task (e.g., graph learning, graph reasoning, and graph\nquestion answering), particularly in complex industrial scenarios (e.g.,\nsupervised, few-shot, and zero-shot settings). In other words, we can treat\ntext as a medium to enable cross-domain generalization of graph learning Model,\nallowing a single graph model to effectively handle the diversity of downstream\ngraph-based Task across different data domains. This work serves as a\nfoundational reference for researchers and practitioners looking to advance\ngraph learning methodologies in the rapidly evolving landscape of LLM. We\nconsistently maintain the related open-source materials at\n\\url{https://github.com/xkLi-Allen/Awesome-GNN-in-LLMs-Papers}.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.DB"
    ],
    "primary_category": "cs.LG",
    "comment": "In progress",
    "pdf_url": "http://arxiv.org/pdf/2412.12456v1",
    "published_date": "2024-12-17 01:41:17 UTC",
    "updated_date": "2024-12-17 01:41:17 UTC"
  },
  {
    "arxiv_id": "2412.12447v2",
    "title": "PERC: Plan-As-Query Example Retrieval for Underrepresented Code Generation",
    "authors": [
      "Jaeseok Yoo",
      "Hojae Han",
      "Youngwon Lee",
      "Jaejin Kim",
      "Seung-won Hwang"
    ],
    "abstract": "Code generation with large language models has shown significant promise,\nespecially when employing retrieval-augmented generation (RAG) with few-shot\nexamples. However, selecting effective examples that enhance generation quality\nremains a challenging task, particularly when the target programming language\n(PL) is underrepresented. In this study, we present two key findings: (1)\nretrieving examples whose presented algorithmic plans can be referenced for\ngenerating the desired behavior significantly improves generation accuracy, and\n(2) converting code into pseudocode effectively captures such algorithmic\nplans, enhancing retrieval quality even when the source and the target PLs are\ndifferent. Based on these findings, we propose Plan-as-query Example Retrieval\nfor few-shot prompting in Code generation (PERC), a novel framework that\nutilizes algorithmic plans to identify and retrieve effective examples. We\nvalidate the effectiveness of PERC through extensive experiments on the\nCodeContests, HumanEval and MultiPL-E benchmarks: PERC consistently outperforms\nthe state-of-the-art RAG methods in code generation, both when the source and\ntarget programming languages match or differ, highlighting its adaptability and\nrobustness in diverse coding environments.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted by COLING 2025 main conference",
    "pdf_url": "http://arxiv.org/pdf/2412.12447v2",
    "published_date": "2024-12-17 01:23:45 UTC",
    "updated_date": "2024-12-20 03:12:28 UTC"
  },
  {
    "arxiv_id": "2412.12444v3",
    "title": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers",
    "authors": [
      "Xuan Shen",
      "Zhao Song",
      "Yufa Zhou",
      "Bo Chen",
      "Yanyu Li",
      "Yifan Gong",
      "Kai Zhang",
      "Hao Tan",
      "Jason Kuen",
      "Henghui Ding",
      "Zhihao Shu",
      "Wei Niu",
      "Pu Zhao",
      "Yanzhi Wang",
      "Jiuxiang Gu"
    ],
    "abstract": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency. Code: https://github.com/shawnricecake/lazydit",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.12444v3",
    "published_date": "2024-12-17 01:12:35 UTC",
    "updated_date": "2025-03-21 15:52:39 UTC"
  },
  {
    "arxiv_id": "2412.12441v1",
    "title": "Numerical Pruning for Efficient Autoregressive Models",
    "authors": [
      "Xuan Shen",
      "Zhao Song",
      "Yufa Zhou",
      "Bo Chen",
      "Jing Liu",
      "Ruiyi Zhang",
      "Ryan A. Rossi",
      "Hao Tan",
      "Tong Yu",
      "Xiang Chen",
      "Yufan Zhou",
      "Tong Sun",
      "Pu Zhao",
      "Yanzhi Wang",
      "Jiuxiang Gu"
    ],
    "abstract": "Transformers have emerged as the leading architecture in deep learning,\nproving to be versatile and highly effective across diverse domains beyond\nlanguage and image processing. However, their impressive performance often\nincurs high computational costs due to their substantial model size. This paper\nfocuses on compressing decoder-only transformer-based autoregressive models\nthrough structural weight pruning to improve the model efficiency while\npreserving performance for both language and image generation tasks.\nSpecifically, we propose a training-free pruning method that calculates a\nnumerical score with Newton's method for the Attention and MLP modules,\nrespectively. Besides, we further propose another compensation algorithm to\nrecover the pruned model for better performance. To verify the effectiveness of\nour method, we provide both theoretical support and extensive experiments. Our\nexperiments show that our method achieves state-of-the-art performance with\nreduced memory usage and faster generation speeds on GPUs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.12441v1",
    "published_date": "2024-12-17 01:09:23 UTC",
    "updated_date": "2024-12-17 01:09:23 UTC"
  },
  {
    "arxiv_id": "2412.12432v1",
    "title": "Three Things to Know about Deep Metric Learning",
    "authors": [
      "Yash Patel",
      "Giorgos Tolias",
      "Jiri Matas"
    ],
    "abstract": "This paper addresses supervised deep metric learning for open-set image\nretrieval, focusing on three key aspects: the loss function, mixup\nregularization, and model initialization. In deep metric learning, optimizing\nthe retrieval evaluation metric, recall@k, via gradient descent is desirable\nbut challenging due to its non-differentiable nature. To overcome this, we\npropose a differentiable surrogate loss that is computed on large batches,\nnearly equivalent to the entire training set. This computationally intensive\nprocess is made feasible through an implementation that bypasses the GPU memory\nlimitations. Additionally, we introduce an efficient mixup regularization\ntechnique that operates on pairwise scalar similarities, effectively increasing\nthe batch size even further. The training process is further enhanced by\ninitializing the vision encoder using foundational models, which are\npre-trained on large-scale datasets. Through a systematic study of these\ncomponents, we demonstrate that their synergy enables large models to nearly\nsolve popular benchmarks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12432v1",
    "published_date": "2024-12-17 00:49:12 UTC",
    "updated_date": "2024-12-17 00:49:12 UTC"
  },
  {
    "arxiv_id": "2412.13223v1",
    "title": "Generative modeling of protein ensembles guided by crystallographic electron densities",
    "authors": [
      "Sai Advaith Maddipatla",
      "Nadav Bojan Sellam",
      "Sanketh Vedula",
      "Ailie Marx",
      "Alex Bronstein"
    ],
    "abstract": "Proteins are dynamic, adopting ensembles of conformations. The nature of this\nconformational heterogenity is imprinted in the raw electron density\nmeasurements obtained from X-ray crystallography experiments. Fitting an\nensemble of protein structures to these measurements is a challenging,\nill-posed inverse problem. We propose a non-i.i.d. ensemble guidance approach\nto solve this problem using existing protein structure generative models and\ndemonstrate that it accurately recovers complicated multi-modal alternate\nprotein backbone conformations observed in certain single crystal measurements.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.QM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.13223v1",
    "published_date": "2024-12-17 00:31:59 UTC",
    "updated_date": "2024-12-17 00:31:59 UTC"
  },
  {
    "arxiv_id": "2412.16201v1",
    "title": "CLIP-RLDrive: Human-Aligned Autonomous Driving via CLIP-Based Reward Shaping in Reinforcement Learning",
    "authors": [
      "Erfan Doroudian",
      "Hamid Taghavifar"
    ],
    "abstract": "This paper presents CLIP-RLDrive, a new reinforcement learning (RL)-based\nframework for improving the decision-making of autonomous vehicles (AVs) in\ncomplex urban driving scenarios, particularly in unsignalized intersections. To\nachieve this goal, the decisions for AVs are aligned with human-like\npreferences through Contrastive Language-Image Pretraining (CLIP)-based reward\nshaping. One of the primary difficulties in RL scheme is designing a suitable\nreward model, which can often be challenging to achieve manually due to the\ncomplexity of the interactions and the driving scenarios. To deal with this\nissue, this paper leverages Vision-Language Models (VLMs), particularly CLIP,\nto build an additional reward model based on visual and textual cues.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.16201v1",
    "published_date": "2024-12-17 00:12:45 UTC",
    "updated_date": "2024-12-17 00:12:45 UTC"
  }
]
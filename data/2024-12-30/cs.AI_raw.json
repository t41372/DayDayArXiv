[
  {
    "arxiv_id": "2501.00174v2",
    "title": "The Text Classification Pipeline: Starting Shallow going Deeper",
    "authors": [
      "Marco Siino",
      "Ilenia Tinnirello",
      "Marco La Cascia"
    ],
    "abstract": "Text classification stands as a cornerstone within the realm of Natural\nLanguage Processing (NLP), particularly when viewed through computer science\nand engineering. The past decade has seen deep learning revolutionize text\nclassification, propelling advancements in text retrieval, categorization,\ninformation extraction, and summarization. The scholarly literature includes\ndatasets, models, and evaluation criteria, with English being the predominant\nlanguage of focus, despite studies involving Arabic, Chinese, Hindi, and\nothers. The efficacy of text classification models relies heavily on their\nability to capture intricate textual relationships and non-linear correlations,\nnecessitating a comprehensive examination of the entire text classification\npipeline.\n  In the NLP domain, a plethora of text representation techniques and model\narchitectures have emerged, with Large Language Models (LLMs) and Generative\nPre-trained Transformers (GPTs) at the forefront. These models are adept at\ntransforming extensive textual data into meaningful vector representations\nencapsulating semantic information. The multidisciplinary nature of text\nclassification, encompassing data mining, linguistics, and information\nretrieval, highlights the importance of collaborative research to advance the\nfield. This work integrates traditional and contemporary text mining\nmethodologies, fostering a holistic understanding of text classification.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "Foundations and Trends in Information Retrieval (2025)",
    "pdf_url": "http://arxiv.org/pdf/2501.00174v2",
    "published_date": "2024-12-30 23:01:19 UTC",
    "updated_date": "2025-03-20 19:18:07 UTC"
  },
  {
    "arxiv_id": "2501.00170v1",
    "title": "Federated Learning with Workload Reduction through Partial Training of Client Models and Entropy-Based Data Selection",
    "authors": [
      "Hongrui Shi",
      "Valentin Radu",
      "Po Yang"
    ],
    "abstract": "With the rapid expansion of edge devices, such as IoT devices, where crucial\ndata needed for machine learning applications is generated, it becomes\nessential to promote their participation in privacy-preserving Federated\nLearning (FL) systems. The best way to achieve this desiderate is by reducing\ntheir training workload to match their constrained computational resources.\nWhile prior FL research has address the workload constrains by introducing\nlightweight models on the edge, limited attention has been given to optimizing\non-device training efficiency through reducing the amount of data need during\ntraining. In this work, we propose FedFT-EDS, a novel approach that combines\nFine-Tuning of partial client models with Entropy-based Data Selection to\nreduce training workloads on edge devices. By actively selecting the most\ninformative local instances for learning, FedFT-EDS reduces training data\nsignificantly in FL and demonstrates that not all user data is equally\nbeneficial for FL on all rounds. Our experiments on CIFAR-10 and CIFAR-100 show\nthat FedFT-EDS uses only 50% user data while improving the global model\nperformance compared to baseline methods, FedAvg and FedProx. Importantly,\nFedFT-EDS improves client learning efficiency by up to 3 times, using one third\nof training time on clients to achieve an equivalent performance to the\nbaselines. This work highlights the importance of data selection in FL and\npresents a promising pathway to scalable and efficient Federate Learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.00170v1",
    "published_date": "2024-12-30 22:47:32 UTC",
    "updated_date": "2024-12-30 22:47:32 UTC"
  },
  {
    "arxiv_id": "2501.00169v1",
    "title": "DeepLL: Considering Linear Logic for the Analysis of Deep Learning Experiments",
    "authors": [
      "Nick Papoulias"
    ],
    "abstract": "Deep Learning experiments have critical requirements regarding the careful\nhandling of their datasets as well as the efficient and correct usage of APIs\nthat interact with hardware accelerators. On the one hand, software mistakes\nduring data handling can contaminate experiments and lead to incorrect results.\nOn the other hand, poorly coded APIs that interact with the hardware can lead\nto sub-optimal usage and untrustworthy conclusions. In this work we investigate\nthe use of Linear Logic for the analysis of Deep Learning experiments. We show\nthat primitives and operators of Linear Logic can be used to express: (i) an\nabstract representation of the control flow of an experiment, (ii) a set of\navailable experimental resources, such as API calls to the underlying\ndata-structures and hardware as well as (iii) reasoning rules about the correct\nconsumption of resources during experiments. Our proposed model is not only\nlightweight but also easy to comprehend having both a symbolic and a visual\ncomponent. Finally, its artifacts are themselves proofs in Linear Logic that\ncan be readily verified by off-the-shelf reasoners.",
    "categories": [
      "cs.PL",
      "cs.AI",
      "cs.CL",
      "cs.SE"
    ],
    "primary_category": "cs.PL",
    "comment": "8 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.00169v1",
    "published_date": "2024-12-30 22:38:56 UTC",
    "updated_date": "2024-12-30 22:38:56 UTC"
  },
  {
    "arxiv_id": "2501.00162v1",
    "title": "Class-based Subset Selection for Transfer Learning under Extreme Label Shift",
    "authors": [
      "Akul Goyal",
      "Carl Edwards"
    ],
    "abstract": "Existing work within transfer learning often follows a two-step process --\npre-training over a large-scale source domain and then finetuning over limited\nsamples from the target domain. Yet, despite its popularity, this methodology\nhas been shown to suffer in the presence of distributional shift --\nspecifically when the output spaces diverge. Previous work has focused on\nincreasing model performance within this setting by identifying and classifying\nonly the shared output classes between distributions. However, these methods\nare inherently limited as they ignore classes outside the shared class set,\ndisregarding potential information relevant to the model transfer. This paper\nproposes a new process for few-shot transfer learning that selects and weighs\nclasses from the source domain to optimize the transfer between domains. More\nconcretely, we use Wasserstein distance to choose a set of source classes and\ntheir weights that minimize the distance between the source and target domain.\nTo justify our proposed algorithm, we provide a generalization analysis of the\nperformance of the learned classifier over the target domain and show that our\nmethod corresponds to a bound minimization algorithm. We empirically\ndemonstrate the effectiveness of our approach (WaSS) by experimenting on\nseveral different datasets and presenting superior performance within various\nlabel shift settings, including the extreme case where the label spaces are\ndisjoint.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "19 pages",
    "pdf_url": "http://arxiv.org/pdf/2501.00162v1",
    "published_date": "2024-12-30 22:14:24 UTC",
    "updated_date": "2024-12-30 22:14:24 UTC"
  },
  {
    "arxiv_id": "2501.00154v1",
    "title": "Probabilistic Explanations for Linear Models",
    "authors": [
      "Bernardo Subercaseaux",
      "Marcelo Arenas",
      "Kuldeep S Meel"
    ],
    "abstract": "Formal XAI is an emerging field that focuses on providing explanations with\nmathematical guarantees for the decisions made by machine learning models. A\nsignificant amount of work in this area is centered on the computation of\n\"sufficient reasons\". Given a model $M$ and an input instance $\\vec{x}$, a\nsufficient reason for the decision $M(\\vec{x})$ is a subset $S$ of the features\nof $\\vec{x}$ such that for any instance $\\vec{z}$ that has the same values as\n$\\vec{x}$ for every feature in $S$, it holds that $M(\\vec{x}) = M(\\vec{z})$.\nIntuitively, this means that the features in $S$ are sufficient to fully\njustify the classification of $\\vec{x}$ by $M$. For sufficient reasons to be\nuseful in practice, they should be as small as possible, and a natural way to\nreduce the size of sufficient reasons is to consider a probabilistic\nrelaxation; the probability of $M(\\vec{x}) = M(\\vec{z})$ must be at least some\nvalue $\\delta \\in (0,1]$, for a random instance $\\vec{z}$ that coincides with\n$\\vec{x}$ on the features in $S$. Computing small $\\delta$-sufficient reasons\n($\\delta$-SRs) is known to be a theoretically hard problem; even over decision\ntrees--traditionally deemed simple and interpretable models--strong\ninapproximability results make the efficient computation of small $\\delta$-SRs\nunlikely. We propose the notion of $(\\delta, \\epsilon)$-SR, a simple relaxation\nof $\\delta$-SRs, and show that this kind of explanation can be computed\nefficiently over linear models.",
    "categories": [
      "cs.AI",
      "cs.CC"
    ],
    "primary_category": "cs.AI",
    "comment": "Extended version of AAAI paper",
    "pdf_url": "http://arxiv.org/pdf/2501.00154v1",
    "published_date": "2024-12-30 21:59:16 UTC",
    "updated_date": "2024-12-30 21:59:16 UTC"
  },
  {
    "arxiv_id": "2501.00152v2",
    "title": "Temporal reasoning for timeline summarisation in social media",
    "authors": [
      "Jiayu Song",
      "Mahmud Akhter",
      "Dana Atzil Slonim",
      "Maria Liakata"
    ],
    "abstract": "This paper explores whether enhancing temporal reasoning capabilities in\nLarge Language Models (LLMs) can improve the quality of timeline summarisation,\nthe task of summarising long texts containing sequences of events, such as\nsocial media threads. We first introduce NarrativeReason, a novel dataset\nfocused on temporal relationships among sequential events within narratives,\ndistinguishing it from existing temporal reasoning datasets that primarily\naddress pair-wise event relationships. Our approach then combines temporal\nreasoning with timeline summarisation through a knowledge distillation\nframework, where we first fine-tune a teacher model on temporal reasoning tasks\nand then distill this knowledge into a student model while simultaneously\ntraining it for the task of timeline summarisation. Experimental results\ndemonstrate that our model achieves superior performance on out-of-domain\nmental health-related timeline summarisation tasks, which involve long social\nmedia threads with repetitions of events and a mix of emotions, highlighting\nthe importance and generalisability of leveraging temporal reasoning to improve\ntimeline summarisation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.00152v2",
    "published_date": "2024-12-30 21:54:33 UTC",
    "updated_date": "2025-02-18 14:02:12 UTC"
  },
  {
    "arxiv_id": "2501.00138v1",
    "title": "NiaAutoARM: Automated generation and evaluation of Association Rule Mining pipelines",
    "authors": [
      "Uroš Mlakar",
      "Iztok Fister Jr.",
      "Iztok Fister"
    ],
    "abstract": "The Numerical Association Rule Mining paradigm that includes concurrent\ndealing with numerical and categorical attributes is beneficial for discovering\nassociations from datasets consisting of both features. The process is not\nconsidered as easy since it incorporates several processing steps running\nsequentially that form an entire pipeline, e.g., preprocessing, algorithm\nselection, hyper-parameter optimization, and the definition of metrics\nevaluating the quality of the association rule. In this paper, we proposed a\nnovel Automated Machine Learning method, NiaAutoARM, for constructing the full\nassociation rule mining pipelines based on stochastic population-based\nmeta-heuristics automatically. Along with the theoretical representation of the\nproposed method, we also present a comprehensive experimental evaluation of the\nproposed method.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.00138v1",
    "published_date": "2024-12-30 20:48:51 UTC",
    "updated_date": "2024-12-30 20:48:51 UTC"
  },
  {
    "arxiv_id": "2501.00136v1",
    "title": "Detection-Fusion for Knowledge Graph Extraction from Videos",
    "authors": [
      "Taniya Das",
      "Louis Mahon",
      "Thomas Lukasiewicz"
    ],
    "abstract": "One of the challenging tasks in the field of video understanding is\nextracting semantic content from video inputs. Most existing systems use\nlanguage models to describe videos in natural language sentences, but this has\nseveral major shortcomings. Such systems can rely too heavily on the language\nmodel component and base their output on statistical regularities in natural\nlanguage text rather than on the visual contents of the video. Additionally,\nnatural language annotations cannot be readily processed by a computer, are\ndifficult to evaluate with performance metrics and cannot be easily translated\ninto a different natural language. In this paper, we propose a method to\nannotate videos with knowledge graphs, and so avoid these problems.\nSpecifically, we propose a deep-learning-based model for this task that first\npredicts pairs of individuals and then the relations between them.\nAdditionally, we propose an extension of our model for the inclusion of\nbackground knowledge in the construction of knowledge graphs.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, To be submitted to a conference",
    "pdf_url": "http://arxiv.org/pdf/2501.00136v1",
    "published_date": "2024-12-30 20:26:11 UTC",
    "updated_date": "2024-12-30 20:26:11 UTC"
  },
  {
    "arxiv_id": "2501.01451v1",
    "title": "Human-AI Teaming Using Large Language Models: Boosting Brain-Computer Interfacing (BCI) and Brain Research",
    "authors": [
      "Maryna Kapitonova",
      "Tonio Ball"
    ],
    "abstract": "Recently, there is an increasing interest in using artificial intelligence\n(AI) to automate aspects of the research process, or even autonomously conduct\nthe full research cycle from idea generation, over data analysis, to composing\nand evaluation of scientific manuscripts. Examples of working AI scientist\nsystems have been demonstrated for computer science tasks and running molecular\nbiology labs. While some approaches aim for full autonomy of the scientific AI,\nothers rather aim for leveraging human-AI teaming. Here, we address how to\nadapt such approaches for boosting Brain-Computer Interface (BCI) development,\nas well as brain research resp. neuroscience at large. We argue that at this\ntime, a strong emphasis on human-AI teaming, in contrast to fully autonomous AI\nBCI researcher will be the most promising way forward. We introduce the\ncollaborative workspaces concept for human-AI teaming based on a set of\nJanusian design principles, looking both ways, to the human as well as to the\nAI side. Based on these principles, we present ChatBCI, a Python-based toolbox\nfor enabling human-AI collaboration based on interaction with Large Language\nModels (LLMs), designed for BCI research and development projects. We show how\nChatBCI was successfully used in a concrete BCI project on advancing motor\nimagery decoding from EEG signals. Our approach can be straightforwardly\nextended to broad neurotechnological and neuroscientific topics, and may by\ndesign facilitate human expert knowledge transfer to scientific AI systems in\ngeneral.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "13 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.01451v1",
    "published_date": "2024-12-30 20:26:03 UTC",
    "updated_date": "2024-12-30 20:26:03 UTC"
  },
  {
    "arxiv_id": "2501.00135v4",
    "title": "GroverGPT: A Large Language Model with 8 Billion Parameters for Quantum Searching",
    "authors": [
      "Haoran Wang",
      "Pingzhi Li",
      "Min Chen",
      "Jinglei Cheng",
      "Junyu Liu",
      "Tianlong Chen"
    ],
    "abstract": "Quantum computing is an exciting non-Von Neumann paradigm, offering provable\nspeedups over classical computing for specific problems. However, the practical\nlimits of classical simulatability for quantum circuits remain unclear,\nespecially with current noisy quantum devices. In this work, we explore the\npotential of leveraging Large Language Models (LLMs) to simulate the output of\na quantum Turing machine using Grover's quantum circuits, known to provide\nquadratic speedups over classical counterparts. To this end, we developed\nGroverGPT, a specialized model based on LLaMA's 8-billion-parameter\narchitecture, trained on over 15 trillion tokens. Unlike brute-force\nstate-vector simulations, which demand substantial computational resources,\nGroverGPT employs pattern recognition to approximate quantum search algorithms\nwithout explicitly representing quantum states. Analyzing 97K quantum search\ninstances, GroverGPT consistently outperformed OpenAI's GPT-4o (45\\% accuracy),\nachieving nearly 100\\% accuracy on 6- and 10-qubit datasets when trained on\n4-qubit or larger datasets. It also demonstrated strong generalization,\nsurpassing 95\\% accuracy for systems with over 20 qubits when trained on 3- to\n6-qubit data. Analysis indicates GroverGPT captures quantum features of\nGrover's search rather than classical patterns, supported by novel prompting\nstrategies to enhance performance. Although accuracy declines with increasing\nsystem size, these findings offer insights into the practical boundaries of\nclassical simulatability. This work suggests task-specific LLMs can surpass\ngeneral-purpose models like GPT-4o in quantum algorithm learning and serve as\npowerful tools for advancing quantum research.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "quant-ph",
    "comment": "12 pages including appendices. v2, v3, v4: Add more experiments\n  include ablation tests. Fix the terminology about infidelity. Add more\n  benchmarks including Llama-3.2-3B and DeepSeek-v2-Lite",
    "pdf_url": "http://arxiv.org/pdf/2501.00135v4",
    "published_date": "2024-12-30 20:23:10 UTC",
    "updated_date": "2025-02-14 04:00:08 UTC"
  },
  {
    "arxiv_id": "2501.00129v1",
    "title": "A Data-Centric Approach to Detecting and Mitigating Demographic Bias in Pediatric Mental Health Text: A Case Study in Anxiety Detection",
    "authors": [
      "Julia Ive",
      "Paulina Bondaronek",
      "Vishal Yadav",
      "Daniel Santel",
      "Tracy Glauser",
      "Tina Cheng",
      "Jeffrey R. Strawn",
      "Greeshma Agasthya",
      "Jordan Tschida",
      "Sanghyun Choo",
      "Mayanka Chandrashekar",
      "Anuj J. Kapadia",
      "John Pestian"
    ],
    "abstract": "Introduction: Healthcare AI models often inherit biases from their training\ndata. While efforts have primarily targeted bias in structured data, mental\nhealth heavily depends on unstructured data. This study aims to detect and\nmitigate linguistic differences related to non-biological differences in the\ntraining data of AI models designed to assist in pediatric mental health\nscreening. Our objectives are: (1) to assess the presence of bias by evaluating\noutcome parity across sex subgroups, (2) to identify bias sources through\ntextual distribution analysis, and (3) to develop a de-biasing method for\nmental health text data. Methods: We examined classification parity across\ndemographic groups and assessed how gendered language influences model\npredictions. A data-centric de-biasing method was applied, focusing on\nneutralizing biased terms while retaining salient clinical information. This\nmethodology was tested on a model for automatic anxiety detection in pediatric\npatients. Results: Our findings revealed a systematic under-diagnosis of female\nadolescent patients, with a 4% lower accuracy and a 9% higher False Negative\nRate (FNR) compared to male patients, likely due to disparities in information\ndensity and linguistic differences in patient notes. Notes for male patients\nwere on average 500 words longer, and linguistic similarity metrics indicated\ndistinct word distributions between genders. Implementing our de-biasing\napproach reduced diagnostic bias by up to 27%, demonstrating its effectiveness\nin enhancing equity across demographic groups. Discussion: We developed a\ndata-centric de-biasing framework to address gender-based content disparities\nwithin clinical text. By neutralizing biased language and enhancing focus on\nclinically essential information, our approach demonstrates an effective\nstrategy for mitigating bias in AI healthcare models trained on text.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.00129v1",
    "published_date": "2024-12-30 20:00:22 UTC",
    "updated_date": "2024-12-30 20:00:22 UTC"
  },
  {
    "arxiv_id": "2501.00116v1",
    "title": "Text-to-Image GAN with Pretrained Representations",
    "authors": [
      "Xiaozhou You",
      "Jian Zhang"
    ],
    "abstract": "Generating desired images conditioned on given text descriptions has received\nlots of attention. Recently, diffusion models and autoregressive models have\ndemonstrated their outstanding expressivity and gradually replaced GAN as the\nfavored architectures for text-to-image synthesis. However, they still face\nsome obstacles: slow inference speed and expensive training costs. To achieve\nmore powerful and faster text-to-image synthesis under complex scenes, we\npropose TIGER, a text-to-image GAN with pretrained representations. To be\nspecific, we propose a vision-empowered discriminator and a high-capacity\ngenerator. (i) The vision-empowered discriminator absorbs the complex scene\nunderstanding ability and the domain generalization ability from pretrained\nvision models to enhance model performance. Unlike previous works, we explore\nstacking multiple pretrained models in our discriminator to collect multiple\ndifferent representations. (ii) The high-capacity generator aims to achieve\neffective text-image fusion while increasing the model capacity. The\nhigh-capacity generator consists of multiple novel high-capacity fusion blocks\n(HFBlock). And the HFBlock contains several deep fusion modules and a global\nfusion module, which play different roles to benefit our model. Extensive\nexperiments demonstrate the outstanding performance of our proposed TIGER both\non standard and zero-shot text-to-image synthesis tasks. On the standard\ntext-to-image synthesis task, TIGER achieves state-of-the-art performance on\ntwo challenging datasets, which obtain a new FID 5.48 (COCO) and 9.38 (CUB). On\nthe zero-shot text-to-image synthesis task, we achieve comparable performance\nwith fewer model parameters, smaller training data size and faster inference\nspeed. Additionally, more experiments and analyses are conducted in the\nSupplementary Material.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.00116v1",
    "published_date": "2024-12-30 19:30:40 UTC",
    "updated_date": "2024-12-30 19:30:40 UTC"
  },
  {
    "arxiv_id": "2501.00113v1",
    "title": "AltGen: AI-Driven Alt Text Generation for Enhancing EPUB Accessibility",
    "authors": [
      "Yixian Shen",
      "Hang Zhang",
      "Yanxin Shen",
      "Lun Wang",
      "Chuanqi Shi",
      "Shaoshuai Du",
      "Yiyi Tao"
    ],
    "abstract": "Digital accessibility is a cornerstone of inclusive content delivery, yet\nmany EPUB files fail to meet fundamental accessibility standards, particularly\nin providing descriptive alt text for images. Alt text plays a critical role in\nenabling visually impaired users to understand visual content through assistive\ntechnologies. However, generating high-quality alt text at scale is a\nresource-intensive process, creating significant challenges for organizations\naiming to ensure accessibility compliance. This paper introduces AltGen, a\nnovel AI-driven pipeline designed to automate the generation of alt text for\nimages in EPUB files. By integrating state-of-the-art generative models,\nincluding advanced transformer-based architectures, AltGen achieves\ncontextually relevant and linguistically coherent alt text descriptions. The\npipeline encompasses multiple stages, starting with data preprocessing to\nextract and prepare relevant content, followed by visual analysis using\ncomputer vision models such as CLIP and ViT. The extracted visual features are\nenriched with contextual information from surrounding text, enabling the\nfine-tuned language models to generate descriptive and accurate alt text.\nValidation of the generated output employs both quantitative metrics, such as\ncosine similarity and BLEU scores, and qualitative feedback from visually\nimpaired users.\n  Experimental results demonstrate the efficacy of AltGen across diverse\ndatasets, achieving a 97.5% reduction in accessibility errors and high scores\nin similarity and linguistic fidelity metrics. User studies highlight the\npractical impact of AltGen, with participants reporting significant\nimprovements in document usability and comprehension. Furthermore, comparative\nanalyses reveal that AltGen outperforms existing approaches in terms of\naccuracy, relevance, and scalability.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.00113v1",
    "published_date": "2024-12-30 19:23:07 UTC",
    "updated_date": "2024-12-30 19:23:07 UTC"
  },
  {
    "arxiv_id": "2501.00107v1",
    "title": "An Unsupervised Anomaly Detection in Electricity Consumption Using Reinforcement Learning and Time Series Forest Based Framework",
    "authors": [
      "Jihan Ghanim",
      "Mariette Awad"
    ],
    "abstract": "Anomaly detection (AD) plays a crucial role in time series applications,\nprimarily because time series data is employed across real-world scenarios.\nDetecting anomalies poses significant challenges since anomalies take diverse\nforms making them hard to pinpoint accurately. Previous research has explored\ndifferent AD models, making specific assumptions with varying sensitivity\ntoward particular anomaly types. To address this issue, we propose a novel\nmodel selection for unsupervised AD using a combination of time series forest\n(TSF) and reinforcement learning (RL) approaches that dynamically chooses an AD\ntechnique. Our approach allows for effective AD without explicitly depending on\nground truth labels that are often scarce and expensive to obtain. Results from\nthe real-time series dataset demonstrate that the proposed model selection\napproach outperforms all other AD models in terms of the F1 score metric. For\nthe synthetic dataset, our proposed model surpasses all other AD models except\nfor KNN, with an impressive F1 score of 0.989. The proposed model selection\nframework also exceeded the performance of GPT-4 when prompted to act as an\nanomaly detector on the synthetic dataset. Exploring different reward functions\nrevealed that the original reward function in our proposed AD model selection\napproach yielded the best overall scores. We evaluated the performance of the\nsix AD models on an additional three datasets, having global, local, and\nclustered anomalies respectively, showing that each AD model exhibited distinct\nperformance depending on the type of anomalies. This emphasizes the\nsignificance of our proposed AD model selection framework, maintaining high\nperformance across all datasets, and showcasing superior performance across\ndifferent anomaly types.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.00107v1",
    "published_date": "2024-12-30 19:04:43 UTC",
    "updated_date": "2024-12-30 19:04:43 UTC"
  },
  {
    "arxiv_id": "2501.00106v1",
    "title": "LicenseGPT: A Fine-tuned Foundation Model for Publicly Available Dataset License Compliance",
    "authors": [
      "Jingwen Tan",
      "Gopi Krishnan Rajbahadur",
      "Zi Li",
      "Xiangfu Song",
      "Jianshan Lin",
      "Dan Li",
      "Zibin Zheng",
      "Ahmed E. Hassan"
    ],
    "abstract": "Dataset license compliance is a critical yet complex aspect of developing\ncommercial AI products, particularly with the increasing use of publicly\navailable datasets. Ambiguities in dataset licenses pose significant legal\nrisks, making it challenging even for software IP lawyers to accurately\ninterpret rights and obligations. In this paper, we introduce LicenseGPT, a\nfine-tuned foundation model (FM) specifically designed for dataset license\ncompliance analysis. We first evaluate existing legal FMs (i.e., FMs\nspecialized in understanding and processing legal texts) and find that the\nbest-performing model achieves a Prediction Agreement (PA) of only 43.75%.\nLicenseGPT, fine-tuned on a curated dataset of 500 licenses annotated by legal\nexperts, significantly improves PA to 64.30%, outperforming both legal and\ngeneral-purpose FMs. Through an A/B test and user study with software IP\nlawyers, we demonstrate that LicenseGPT reduces analysis time by 94.44%, from\n108 seconds to 6 seconds per license, without compromising accuracy. Software\nIP lawyers perceive LicenseGPT as a valuable supplementary tool that enhances\nefficiency while acknowledging the need for human oversight in complex cases.\nOur work underscores the potential of specialized AI tools in legal practice\nand offers a publicly available resource for practitioners and researchers.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.00106v1",
    "published_date": "2024-12-30 19:04:13 UTC",
    "updated_date": "2024-12-30 19:04:13 UTC"
  },
  {
    "arxiv_id": "2501.00097v1",
    "title": "CaseSumm: A Large-Scale Dataset for Long-Context Summarization from U.S. Supreme Court Opinions",
    "authors": [
      "Mourad Heddaya",
      "Kyle MacMillan",
      "Anup Malani",
      "Hongyuan Mei",
      "Chenhao Tan"
    ],
    "abstract": "This paper introduces CaseSumm, a novel dataset for long-context\nsummarization in the legal domain that addresses the need for longer and more\ncomplex datasets for summarization evaluation. We collect 25.6K U.S. Supreme\nCourt (SCOTUS) opinions and their official summaries, known as \"syllabuses.\"\nOur dataset is the largest open legal case summarization dataset, and is the\nfirst to include summaries of SCOTUS decisions dating back to 1815.\n  We also present a comprehensive evaluation of LLM-generated summaries using\nboth automatic metrics and expert human evaluation, revealing discrepancies\nbetween these assessment methods. Our evaluation shows Mistral 7b, a smaller\nopen-source model, outperforms larger models on most automatic metrics and\nsuccessfully generates syllabus-like summaries. In contrast, human expert\nannotators indicate that Mistral summaries contain hallucinations. The\nannotators consistently rank GPT-4 summaries as clearer and exhibiting greater\nsensitivity and specificity. Further, we find that LLM-based evaluations are\nnot more correlated with human evaluations than traditional automatic metrics.\nFurthermore, our analysis identifies specific hallucinations in generated\nsummaries, including precedent citation errors and misrepresentations of case\nfacts. These findings demonstrate the limitations of current automatic\nevaluation methods for legal summarization and highlight the critical role of\nhuman evaluation in assessing summary quality, particularly in complex,\nhigh-stakes domains.\n  CaseSumm is available at https://huggingface.co/datasets/ChicagoHAI/CaseSumm",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.00097v1",
    "published_date": "2024-12-30 19:00:01 UTC",
    "updated_date": "2024-12-30 19:00:01 UTC"
  },
  {
    "arxiv_id": "2412.21205v1",
    "title": "Action-Agnostic Point-Level Supervision for Temporal Action Detection",
    "authors": [
      "Shuhei M. Yoshida",
      "Takashi Shibata",
      "Makoto Terao",
      "Takayuki Okatani",
      "Masashi Sugiyama"
    ],
    "abstract": "We propose action-agnostic point-level (AAPL) supervision for temporal action\ndetection to achieve accurate action instance detection with a lightly\nannotated dataset. In the proposed scheme, a small portion of video frames is\nsampled in an unsupervised manner and presented to human annotators, who then\nlabel the frames with action categories. Unlike point-level supervision, which\nrequires annotators to search for every action instance in an untrimmed video,\nframes to annotate are selected without human intervention in AAPL supervision.\nWe also propose a detection model and learning method to effectively utilize\nthe AAPL labels. Extensive experiments on the variety of datasets (THUMOS '14,\nFineAction, GTEA, BEOID, and ActivityNet 1.3) demonstrate that the proposed\napproach is competitive with or outperforms prior methods for video-level and\npoint-level supervision in terms of the trade-off between the annotation cost\nand detection performance.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "AAAI-25. Technical appendices included. 15 pages, 3 figures, 11\n  tables",
    "pdf_url": "http://arxiv.org/pdf/2412.21205v1",
    "published_date": "2024-12-30 18:59:55 UTC",
    "updated_date": "2024-12-30 18:59:55 UTC"
  },
  {
    "arxiv_id": "2412.21164v1",
    "title": "Adversarial Attack and Defense for LoRa Device Identification and Authentication via Deep Learning",
    "authors": [
      "Yalin E. Sagduyu",
      "Tugba Erpek"
    ],
    "abstract": "LoRa provides long-range, energy-efficient communications in Internet of\nThings (IoT) applications that rely on Low-Power Wide-Area Network (LPWAN)\ncapabilities. Despite these merits, concerns persist regarding the security of\nLoRa networks, especially in situations where device identification and\nauthentication are imperative to secure the reliable access to the LoRa\nnetworks. This paper explores a deep learning (DL) approach to tackle these\nconcerns, focusing on two critical tasks, namely (i) identifying LoRa devices\nand (ii) classifying them to legitimate and rogue devices. Deep neural networks\n(DNNs), encompassing both convolutional and feedforward neural networks, are\ntrained for these tasks using actual LoRa signal data. In this setting, the\nadversaries may spoof rogue LoRa signals through the kernel density estimation\n(KDE) method based on legitimate device signals that are received by the\nadversaries. Two cases are considered, (i) training two separate classifiers,\none for each of the two tasks, and (ii) training a multi-task classifier for\nboth tasks. The vulnerabilities of the resulting DNNs to manipulations in input\nsamples are studied in form of untargeted and targeted adversarial attacks\nusing the Fast Gradient Sign Method (FGSM). Individual and common perturbations\nare considered against single-task and multi-task classifiers for the LoRa\nsignal analysis. To provide resilience against such attacks, a defense approach\nis presented by increasing the robustness of classifiers with adversarial\ntraining. Results quantify how vulnerable LoRa signal classification tasks are\nto adversarial attacks and emphasize the need to fortify IoT applications\nagainst these subtle yet effective threats.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.CR",
      "cs.LG",
      "eess.SP"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.21164v1",
    "published_date": "2024-12-30 18:43:21 UTC",
    "updated_date": "2024-12-30 18:43:21 UTC"
  },
  {
    "arxiv_id": "2412.21161v1",
    "title": "Open RAN-Enabled Deep Learning-Assisted Mobility Management for Connected Vehicles",
    "authors": [
      "Maria Barbosa",
      "Kelvin Dias"
    ],
    "abstract": "Connected Vehicles (CVs) can leverage the unique features of 5G and future\n6G/NextG networks to enhance Intelligent Transportation System (ITS) services.\nHowever, even with advancements in cellular network generations, CV\napplications may experience communication interruptions in high-mobility\nscenarios due to frequent changes of serving base station, also known as\nhandovers (HOs). This paper proposes the adoption of Open Radio Access Network\n(Open RAN/O-RAN) and deep learning models for decision-making to prevent\nQuality of Service (QoS) degradation due to HOs and to ensure the timely\nconnectivity needed for CV services. The solution utilizes the O-RAN Software\nCommunity (OSC), an open-source O-RAN platform developed by the collaboration\nbetween the O-RAN Alliance and Linux Foundation, to develop xApps that are\nexecuted in the near-Real-Time RIC of OSC. To demonstrate the proposal's\neffectiveness, an integrated framework combining the OMNeT++ simulator and OSC\nwas created. Evaluations used real-world datasets in urban application\nscenarios, such as video streaming transmission and over-the-air (OTA) updates.\nResults indicate that the proposal achieved superior performance and reduced\nlatency compared to the standard 3GPP HO procedure.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "Accepted for publication in ICOIN 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.21161v1",
    "published_date": "2024-12-30 18:41:29 UTC",
    "updated_date": "2024-12-30 18:41:29 UTC"
  },
  {
    "arxiv_id": "2412.21154v1",
    "title": "Aviary: training language agents on challenging scientific tasks",
    "authors": [
      "Siddharth Narayanan",
      "James D. Braza",
      "Ryan-Rhys Griffiths",
      "Manu Ponnapati",
      "Albert Bou",
      "Jon Laurent",
      "Ori Kabeli",
      "Geemi Wellawatte",
      "Sam Cox",
      "Samuel G. Rodriques",
      "Andrew D. White"
    ],
    "abstract": "Solving complex real-world tasks requires cycles of actions and observations.\nThis is particularly true in science, where tasks require many cycles of\nanalysis, tool use, and experimentation. Language agents are promising for\nautomating intellectual tasks in science because they can interact with tools\nvia natural language or code. Yet their flexibility creates conceptual and\npractical challenges for software implementations, since agents may comprise\nnon-standard components such as internal reasoning, planning, tool usage, as\nwell as the inherent stochasticity of temperature-sampled language models.\nHere, we introduce Aviary, an extensible gymnasium for language agents. We\nformalize agents as policies solving language-grounded partially observable\nMarkov decision processes, which we term language decision processes. We then\nimplement five environments, including three challenging scientific\nenvironments: (1) manipulating DNA constructs for molecular cloning, (2)\nanswering research questions by accessing scientific literature, and (3)\nengineering protein stability. These environments were selected for their focus\non multi-step reasoning and their relevance to contemporary biology research.\nFinally, with online training and scaling inference-time compute, we show that\nlanguage agents backed by open-source, non-frontier LLMs can match and exceed\nboth frontier LLM agents and human experts on multiple tasks at up to 100x\nlower inference cost.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.21154v1",
    "published_date": "2024-12-30 18:33:28 UTC",
    "updated_date": "2024-12-30 18:33:28 UTC"
  },
  {
    "arxiv_id": "2412.21151v1",
    "title": "PyG-SSL: A Graph Self-Supervised Learning Toolkit",
    "authors": [
      "Lecheng Zheng",
      "Baoyu Jing",
      "Zihao Li",
      "Zhichen Zeng",
      "Tianxin Wei",
      "Mengting Ai",
      "Xinrui He",
      "Lihui Liu",
      "Dongqi Fu",
      "Jiaxuan You",
      "Hanghang Tong",
      "Jingrui He"
    ],
    "abstract": "Graph Self-Supervised Learning (SSL) has emerged as a pivotal area of\nresearch in recent years. By engaging in pretext tasks to learn the intricate\ntopological structures and properties of graphs using unlabeled data, these\ngraph SSL models achieve enhanced performance, improved generalization, and\nheightened robustness. Despite the remarkable achievements of these graph SSL\nmethods, their current implementation poses significant challenges for\nbeginners and practitioners due to the complex nature of graph structures,\ninconsistent evaluation metrics, and concerns regarding reproducibility hinder\nfurther progress in this field. Recognizing the growing interest within the\nresearch community, there is an urgent need for a comprehensive,\nbeginner-friendly, and accessible toolkit consisting of the most representative\ngraph SSL algorithms. To address these challenges, we present a Graph SSL\ntoolkit named PyG-SSL, which is built upon PyTorch and is compatible with\nvarious deep learning and scientific computing backends. Within the toolkit, we\noffer a unified framework encompassing dataset loading, hyper-parameter\nconfiguration, model training, and comprehensive performance evaluation for\ndiverse downstream tasks. Moreover, we provide beginner-friendly tutorials and\nthe best hyper-parameters of each graph SSL algorithm on different graph\ndatasets, facilitating the reproduction of results. The GitHub repository of\nthe library is https://github.com/iDEA-iSAIL-Lab-UIUC/pyg-ssl.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.21151v1",
    "published_date": "2024-12-30 18:32:05 UTC",
    "updated_date": "2024-12-30 18:32:05 UTC"
  },
  {
    "arxiv_id": "2501.00085v2",
    "title": "Machine Learning-Based Security Policy Analysis",
    "authors": [
      "Krish Jain",
      "Joann Sum",
      "Pranav Kapoor",
      "Amir Eaman"
    ],
    "abstract": "Security-Enhanced Linux (SELinux) is a robust security mechanism that\nenforces mandatory access controls (MAC), but its policy language's complexity\ncreates challenges for policy analysis and management. This research\ninvestigates the automation of SELinux policy analysis using graph-based\ntechniques combined with machine learning approaches to detect policy\nanomalies. The study addresses two key questions: Can SELinux policy analysis\nbe automated through graph analysis, and how do different anomaly detection\nmodels compare in analyzing SELinux policies? We will be comparing different\nmachine learning models by evaluating their effectiveness in detecting policy\nviolations and anomalies. Our approach utilizes Neo4j for graph representation\nof policies, with Node2vec transforming these graph structures into meaningful\nvector embeddings that can be processed by our machine learning models. In our\nresults, the MLP Neural Network consistently demonstrated superior performance\nacross different dataset sizes, achieving 95% accuracy with balanced precision\nand recall metrics, while both Random Forest and SVM models showed competitive\nbut slightly lower performance in detecting policy violations. This combination\nof graph-based modeling and machine learning provides a more sophisticated and\nautomated approach to understanding and analyzing complex SELinux policies\ncompared to traditional manual analysis methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.00085v2",
    "published_date": "2024-12-30 18:24:27 UTC",
    "updated_date": "2025-01-06 22:42:41 UTC"
  },
  {
    "arxiv_id": "2412.21140v1",
    "title": "Facilitating large language model Russian adaptation with Learned Embedding Propagation",
    "authors": [
      "Mikhail Tikhomirov",
      "Daniil Chernyshev"
    ],
    "abstract": "Rapid advancements of large language model (LLM) technologies led to the\nintroduction of powerful open-source instruction-tuned LLMs that have the same\ntext generation quality as the state-of-the-art counterparts such as GPT-4.\nWhile the emergence of such models accelerates the adoption of LLM technologies\nin sensitive-information environments the authors of such models don not\ndisclose the training data necessary for replication of the results thus making\nthe achievements model-exclusive. Since those open-source models are also\nmultilingual this in turn reduces the benefits of training a language specific\nLLMs as improved inference computation efficiency becomes the only guaranteed\nadvantage of such costly procedure. More cost-efficient options such as\nvocabulary extension and subsequent continued pre-training are also inhibited\nby the lack of access to high-quality instruction-tuning data since it is the\nmajor factor behind the resulting LLM task-solving capabilities. To address the\nlimitations and cut the costs of the language adaptation pipeline we propose\nLearned Embedding Propagation (LEP). Unlike existing approaches our method has\nlower training data size requirements due to minimal impact on existing LLM\nknowledge which we reinforce using novel ad-hoc embedding propagation procedure\nthat allows to skip the instruction-tuning step and instead implant the new\nlanguage knowledge directly into any existing instruct-tuned variant. We\nevaluated four Russian vocabulary adaptations for LLaMa-3-8B and Mistral-7B,\nshowing that LEP is competitive with traditional instruction-tuning methods,\nachieving performance comparable to OpenChat 3.5 and LLaMa-3-8B-Instruct, with\nfurther improvements via self-calibration and continued tuning enhancing\ntask-solving capabilities.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint version of an article published in the Journal of Language\n  and Education. Copyright held by the owner/author(s). Publication rights\n  licensed to the Journal of Language and Education",
    "pdf_url": "http://arxiv.org/pdf/2412.21140v1",
    "published_date": "2024-12-30 18:15:45 UTC",
    "updated_date": "2024-12-30 18:15:45 UTC"
  },
  {
    "arxiv_id": "2501.01987v2",
    "title": "Gender Bias in Text-to-Video Generation Models: A case study of Sora",
    "authors": [
      "Mohammad Nadeem",
      "Shahab Saquib Sohail",
      "Erik Cambria",
      "Björn W. Schuller",
      "Amir Hussain"
    ],
    "abstract": "The advent of text-to-video generation models has revolutionized content\ncreation as it produces high-quality videos from textual prompts. However,\nconcerns regarding inherent biases in such models have prompted scrutiny,\nparticularly regarding gender representation. Our study investigates the\npresence of gender bias in OpenAI's Sora, a state-of-the-art text-to-video\ngeneration model. We uncover significant evidence of bias by analyzing the\ngenerated videos from a diverse set of gender-neutral and stereotypical\nprompts. The results indicate that Sora disproportionately associates specific\ngenders with stereotypical behaviors and professions, which reflects societal\nprejudices embedded in its training data.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "7 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.01987v2",
    "published_date": "2024-12-30 18:08:13 UTC",
    "updated_date": "2025-01-10 11:36:09 UTC"
  },
  {
    "arxiv_id": "2501.01986v1",
    "title": "FrameFusion: Combining Similarity and Importance for Video Token Reduction on Large Visual Language Models",
    "authors": [
      "Tianyu Fu",
      "Tengxuan Liu",
      "Qinghao Han",
      "Guohao Dai",
      "Shengen Yan",
      "Huazhong Yang",
      "Xuefei Ning",
      "Yu Wang"
    ],
    "abstract": "The increasing demand to process long and high-resolution videos\nsignificantly burdens Large Vision-Language Models (LVLMs) due to the enormous\nnumber of visual tokens. Existing token reduction methods primarily focus on\nimportance-based token pruning, which overlooks the redundancy caused by frame\nresemblance and repetitive visual elements. In this paper, we analyze the high\nvision token similarities in LVLMs. We reveal that token similarity\ndistribution condenses as layers deepen while maintaining ranking consistency.\nLeveraging the unique properties of similarity over importance, we introduce\nFrameFusion, a novel approach that combines similarity-based merging with\nimportance-based pruning for better token reduction in LVLMs. FrameFusion\nidentifies and merges similar tokens before pruning, opening up a new\nperspective for token reduction. We evaluate FrameFusion on diverse LVLMs,\nincluding Llava-Video-{7B,32B,72B}, and MiniCPM-V-8B, on video understanding,\nquestion-answering, and retrieval benchmarks. Experiments show that FrameFusion\nreduces vision tokens by 70$\\%$, achieving 3.4-4.4x LLM speedups and 1.6-1.9x\nend-to-end speedups, with an average performance impact of less than 3$\\%$. Our\ncode is available at https://github.com/thu-nics/FrameFusion.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "68T45, 68T50",
      "I.2.7; I.2.10"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.01986v1",
    "published_date": "2024-12-30 17:31:37 UTC",
    "updated_date": "2024-12-30 17:31:37 UTC"
  },
  {
    "arxiv_id": "2412.21104v2",
    "title": "On Parallel External-Memory Bidirectional Search",
    "authors": [
      "Lior Siag",
      "Shahaf S. Shperberg",
      "Ariel Felner",
      "Nathan R. Sturtevant"
    ],
    "abstract": "Parallelization and External Memory (PEM) techniques have significantly\nenhanced the capabilities of search algorithms when solving large-scale\nproblems. Previous research on PEM has primarily centered on unidirectional\nalgorithms, with only one publication on bidirectional PEM that focuses on the\nmeet-in-the-middle (MM) algorithm. Building upon this foundation, this paper\npresents a framework that integrates both uni- and bi-directional best-first\nsearch algorithms into this framework. We then develop a PEM variant of the\nstate-of-the-art bidirectional heuristic search (BiHS) algorithm BAE*\n(PEM-BAE*). As previous work on BiHS did not focus on scaling problem sizes,\nthis work enables us to evaluate bidirectional algorithms on hard problems.\nEmpirical evaluation shows that PEM-BAE* outperforms the PEM variants of A* and\nthe MM algorithm, as well as a parallel variant of IDA*. These findings mark a\nsignificant milestone, revealing that bidirectional search algorithms clearly\noutperform unidirectional search algorithms across several domains, even when\nequipped with state-of-the-art heuristics.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, includes conference paper and appendix",
    "pdf_url": "http://arxiv.org/pdf/2412.21104v2",
    "published_date": "2024-12-30 17:29:51 UTC",
    "updated_date": "2024-12-31 08:00:57 UTC"
  },
  {
    "arxiv_id": "2412.21102v2",
    "title": "Exploring and Controlling Diversity in LLM-Agent Conversation",
    "authors": [
      "KuanChao Chu",
      "Yi-Pei Chen",
      "Hideki Nakayama"
    ],
    "abstract": "Controlling diversity in LLM-agent world simulations is essential for\nmaintaining stability in structured tasks while enabling variation where\ncreativity is needed. However, we observe that dialogue diversity declines\nsignificantly over long-term simulation. To investigate the role of prompt\ndesign in conversational diversity, we modularized the utterance generation\nprompt and found that reducing the given information leads to more diverse\noutputs. Based on this insight, we propose Adaptive Prompt Pruning (APP), a\nnovel method that allows users to control diversity through a single parameter,\nlambda. APP dynamically prunes the utterance generation prompt based on their\nattention weights and is compatible with traditional diversity control\ntechniques. We demonstrate that APP effectively controls output diversity\nthrough extensive experiments, and propose a method to balance the control\ntrade-offs. Additionally, we provide an in-depth analysis to offer insights\ninto optimizing diversity control in multi-agent simulation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted for the AAAI 2025 Workshop on Advancing LLM-Based\n  Multi-Agent Collaboration (v1); updated version (v2)",
    "pdf_url": "http://arxiv.org/pdf/2412.21102v2",
    "published_date": "2024-12-30 17:25:58 UTC",
    "updated_date": "2025-02-21 15:48:44 UTC"
  },
  {
    "arxiv_id": "2501.00083v1",
    "title": "AI Agent for Education: von Neumann Multi-Agent System Framework",
    "authors": [
      "Yuan-Hao Jiang",
      "Ruijia Li",
      "Yizhou Zhou",
      "Changyong Qi",
      "Hanglei Hu",
      "Yuang Wei",
      "Bo Jiang",
      "Yonghe Wu"
    ],
    "abstract": "The development of large language models has ushered in new paradigms for\neducation. This paper centers on the multi-Agent system in education and\nproposes the von Neumann multi-Agent system framework. It breaks down each AI\nAgent into four modules: control unit, logic unit, storage unit, and\ninput-output devices, defining four types of operations: task deconstruction,\nself-reflection, memory processing, and tool invocation. Furthermore, it\nintroduces related technologies such as Chain-of-Thought, Reson+Act, and\nMulti-Agent Debate associated with these four types of operations. The paper\nalso discusses the ability enhancement cycle of a multi-Agent system for\neducation, including the outer circulation for human learners to promote\nknowledge construction and the inner circulation for LLM-based-Agents to\nenhance swarm intelligence. Through collaboration and reflection, the\nmulti-Agent system can better facilitate human learners' learning and enhance\ntheir teaching abilities in this process.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.MA",
    "comment": "Conference Proceedings of the 28th Global Chinese Conference on\n  Computers in Education, GCCCE 2024",
    "pdf_url": "http://arxiv.org/pdf/2501.00083v1",
    "published_date": "2024-12-30 16:58:17 UTC",
    "updated_date": "2024-12-30 16:58:17 UTC"
  },
  {
    "arxiv_id": "2412.21052v1",
    "title": "Towards Effective Discrimination Testing for Generative AI",
    "authors": [
      "Thomas P. Zollo",
      "Nikita Rajaneesh",
      "Richard Zemel",
      "Talia B. Gillis",
      "Emily Black"
    ],
    "abstract": "Generative AI (GenAI) models present new challenges in regulating against\ndiscriminatory behavior. In this paper, we argue that GenAI fairness research\nstill has not met these challenges; instead, a significant gap remains between\nexisting bias assessment methods and regulatory goals. This leads to\nineffective regulation that can allow deployment of reportedly fair, yet\nactually discriminatory, GenAI systems. Towards remedying this problem, we\nconnect the legal and technical literature around GenAI bias evaluation and\nidentify areas of misalignment. Through four case studies, we demonstrate how\nthis misalignment between fairness testing techniques and regulatory goals can\nresult in discriminatory outcomes in real-world deployments, especially in\nadaptive or complex environments. We offer practical recommendations for\nimproving discrimination testing to better align with regulatory goals and\nenhance the reliability of fairness assessments in future deployments.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "38 pages, 9 tables, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.21052v1",
    "published_date": "2024-12-30 16:09:33 UTC",
    "updated_date": "2024-12-30 16:09:33 UTC"
  },
  {
    "arxiv_id": "2412.21051v2",
    "title": "Toward Intelligent and Secure Cloud: Large Language Model Empowered Proactive Defense",
    "authors": [
      "Yuyang Zhou",
      "Guang Cheng",
      "Kang Du",
      "Zihan Chen",
      "Yuyu Zhao"
    ],
    "abstract": "The rapid evolution of cloud computing technologies and the increasing number\nof cloud applications have provided a large number of benefits in daily lives.\nHowever, the diversity and complexity of different components pose a\nsignificant challenge to cloud security, especially when dealing with\nsophisticated and advanced cyberattacks. Recent advancements in generative\nfoundation models (GFMs), particularly in the large language models (LLMs),\noffer promising solutions for security intelligence. By exploiting the powerful\nabilities in language understanding, data analysis, task inference, action\nplanning, and code generation, we present LLM-PD, a novel proactive defense\narchitecture that defeats various threats in a proactive manner. LLM-PD can\nefficiently make a decision through comprehensive data analysis and sequential\nreasoning, as well as dynamically creating and deploying actionable defense\nmechanisms on the target cloud. Furthermore, it can flexibly self-evolve based\non experience learned from previous interactions and adapt to new attack\nscenarios without additional training. The experimental results demonstrate its\nremarkable ability in terms of defense effectiveness and efficiency,\nparticularly highlighting an outstanding success rate when compared with other\nexisting methods.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.NI",
      "68",
      "F.2.2; I.2.8"
    ],
    "primary_category": "cs.CR",
    "comment": "7 pages; In submission",
    "pdf_url": "http://arxiv.org/pdf/2412.21051v2",
    "published_date": "2024-12-30 16:09:28 UTC",
    "updated_date": "2025-04-15 02:05:56 UTC"
  },
  {
    "arxiv_id": "2412.21037v2",
    "title": "TangoFlux: Super Fast and Faithful Text to Audio Generation with Flow Matching and Clap-Ranked Preference Optimization",
    "authors": [
      "Chia-Yu Hung",
      "Navonil Majumder",
      "Zhifeng Kong",
      "Ambuj Mehrish",
      "Amir Ali Bagherzadeh",
      "Chuan Li",
      "Rafael Valle",
      "Bryan Catanzaro",
      "Soujanya Poria"
    ],
    "abstract": "We introduce TangoFlux, an efficient Text-to-Audio (TTA) generative model\nwith 515M parameters, capable of generating up to 30 seconds of 44.1kHz audio\nin just 3.7 seconds on a single A40 GPU. A key challenge in aligning TTA models\nlies in the difficulty of creating preference pairs, as TTA lacks structured\nmechanisms like verifiable rewards or gold-standard answers available for Large\nLanguage Models (LLMs). To address this, we propose CLAP-Ranked Preference\nOptimization (CRPO), a novel framework that iteratively generates and optimizes\npreference data to enhance TTA alignment. We demonstrate that the audio\npreference dataset generated using CRPO outperforms existing alternatives. With\nthis framework, TangoFlux achieves state-of-the-art performance across both\nobjective and subjective benchmarks. We open source all code and models to\nsupport further research in TTA generation.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "https://tangoflux.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2412.21037v2",
    "published_date": "2024-12-30 16:02:44 UTC",
    "updated_date": "2025-04-10 05:01:32 UTC"
  },
  {
    "arxiv_id": "2412.21033v1",
    "title": "Plancraft: an evaluation dataset for planning with LLM agents",
    "authors": [
      "Gautier Dagan",
      "Frank Keller",
      "Alex Lascarides"
    ],
    "abstract": "We present Plancraft, a multi-modal evaluation dataset for LLM agents.\nPlancraft has both a text-only and multi-modal interface, based on the\nMinecraft crafting GUI. We include the Minecraft Wiki to evaluate tool use and\nRetrieval Augmented Generation (RAG), as well as an oracle planner and oracle\nRAG information extractor, to ablate the different components of a modern agent\narchitecture. To evaluate decision-making, Plancraft also includes a subset of\nexamples that are intentionally unsolvable, providing a realistic challenge\nthat requires the agent not only to complete tasks but also to decide whether\nthey are solvable at all. We benchmark both open-source and closed-source LLMs\nand strategies on our task and compare their performance to a handcrafted\nplanner. We find that LLMs and VLMs struggle with the planning problems that\nPlancraft introduces, and we offer suggestions on how to improve their\ncapabilities.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.21033v1",
    "published_date": "2024-12-30 15:58:41 UTC",
    "updated_date": "2024-12-30 15:58:41 UTC"
  },
  {
    "arxiv_id": "2412.21006v2",
    "title": "Verbosity-Aware Rationale Reduction: Effective Reduction of Redundant Rationale via Principled Criteria",
    "authors": [
      "Joonwon Jang",
      "Jaehee Kim",
      "Wonbin Kweon",
      "Hwanjo Yu"
    ],
    "abstract": "Large Language Models (LLMs) rely on generating extensive intermediate\nreasoning units (e.g., tokens, sentences) to enhance final answer quality\nacross a wide range of complex tasks. While generating multiple reasoning paths\nor iteratively refining rationales proves effective for improving performance,\nthese approaches inevitably result in significantly higher inference costs. In\nthis work, we propose a novel sentence-level rationale reduction training\nframework that leverages likelihood-based criteria, verbosity, to identify and\nremove redundant reasoning sentences. Unlike previous approaches that utilize\ntoken-level reduction, our sentence-level reduction framework maintains model\nperformance while reducing generation length. This preserves the original\nreasoning abilities of LLMs and achieves an average 17.15% reduction in\ngeneration costs across various models and tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.21006v2",
    "published_date": "2024-12-30 15:15:08 UTC",
    "updated_date": "2024-12-31 03:06:15 UTC"
  },
  {
    "arxiv_id": "2412.21001v1",
    "title": "LEASE: Offline Preference-based Reinforcement Learning with High Sample Efficiency",
    "authors": [
      "Xiao-Yin Liu",
      "Guotao Li",
      "Xiao-Hu Zhou",
      "Zeng-Guang Hou"
    ],
    "abstract": "Offline preference-based reinforcement learning (PbRL) provides an effective\nway to overcome the challenges of designing reward and the high costs of online\ninteraction. However, since labeling preference needs real-time human feedback,\nacquiring sufficient preference labels is challenging. To solve this, this\npaper proposes a offLine prEference-bAsed RL with high Sample Efficiency\n(LEASE) algorithm, where a learned transition model is leveraged to generate\nunlabeled preference data. Considering the pretrained reward model may generate\nincorrect labels for unlabeled data, we design an uncertainty-aware mechanism\nto ensure the performance of reward model, where only high confidence and low\nvariance data are selected. Moreover, we provide the generalization bound of\nreward model to analyze the factors influencing reward accuracy, and\ndemonstrate that the policy learned by LEASE has theoretical improvement\nguarantee. The developed theory is based on state-action pair, which can be\neasily combined with other offline algorithms. The experimental results show\nthat LEASE can achieve comparable performance to baseline under fewer\npreference data without online interaction.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.21001v1",
    "published_date": "2024-12-30 15:10:57 UTC",
    "updated_date": "2024-12-30 15:10:57 UTC"
  },
  {
    "arxiv_id": "2412.20995v1",
    "title": "KARPA: A Training-free Method of Adapting Knowledge Graph as References for Large Language Model's Reasoning Path Aggregation",
    "authors": [
      "Siyuan Fang",
      "Kaijing Ma",
      "Tianyu Zheng",
      "Xinrun Du",
      "Ningxuan Lu",
      "Ge Zhang",
      "Qingkun Tang"
    ],
    "abstract": "Large language models (LLMs) demonstrate exceptional performance across a\nvariety of tasks, yet they are often affected by hallucinations and the\ntimeliness of knowledge. Leveraging knowledge graphs (KGs) as external\nknowledge sources has emerged as a viable solution, but existing methods for\nLLM-based knowledge graph question answering (KGQA) are often limited by\nstep-by-step decision-making on KGs, restricting the global planning and\nreasoning capabilities of LLMs, or they require fine-tuning or pre-training on\nspecific KGs. To address these challenges, we propose Knowledge graph Assisted\nReasoning Path Aggregation (KARPA), a novel framework that harnesses the global\nplanning abilities of LLMs for efficient and accurate KG reasoning. KARPA\noperates in three steps: pre-planning relation paths using the LLM's global\nplanning capabilities, matching semantically relevant paths via an embedding\nmodel, and reasoning over these paths to generate answers. Unlike existing KGQA\nmethods, KARPA avoids stepwise traversal, requires no additional training, and\nis adaptable to various LLM architectures. Extensive experimental results show\nthat KARPA achieves state-of-the-art performance in KGQA tasks, delivering both\nhigh efficiency and accuracy. Our code will be available on Github.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "23 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.20995v1",
    "published_date": "2024-12-30 14:58:46 UTC",
    "updated_date": "2024-12-30 14:58:46 UTC"
  },
  {
    "arxiv_id": "2412.20977v1",
    "title": "UnrealZoo: Enriching Photo-realistic Virtual Worlds for Embodied AI",
    "authors": [
      "Fangwei Zhong",
      "Kui Wu",
      "Churan Wang",
      "Hao Chen",
      "Hai Ci",
      "Zhoujun Li",
      "Yizhou Wang"
    ],
    "abstract": "We introduce UnrealZoo, a rich collection of photo-realistic 3D virtual\nworlds built on Unreal Engine, designed to reflect the complexity and\nvariability of the open worlds. Additionally, we offer a variety of playable\nentities for embodied AI agents. Based on UnrealCV, we provide a suite of\neasy-to-use Python APIs and tools for various potential applications, such as\ndata collection, environment augmentation, distributed training, and\nbenchmarking. We optimize the rendering and communication efficiency of\nUnrealCV to support advanced applications, such as multi-agent interaction. Our\nexperiments benchmark agents in various complex scenes, focusing on visual\nnavigation and tracking, which are fundamental capabilities for embodied visual\nintelligence. The results yield valuable insights into the advantages of\ndiverse training environments for reinforcement learning (RL) agents and the\nchallenges faced by current embodied vision agents, including those based on RL\nand large vision-language models (VLMs), in open worlds. These challenges\ninvolve latency in closed-loop control in dynamic scenes and reasoning about 3D\nspatial structures in unstructured terrain.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "Project page: http://unrealzoo.site/",
    "pdf_url": "http://arxiv.org/pdf/2412.20977v1",
    "published_date": "2024-12-30 14:31:01 UTC",
    "updated_date": "2024-12-30 14:31:01 UTC"
  },
  {
    "arxiv_id": "2412.20962v3",
    "title": "Conservation-informed Graph Learning for Spatiotemporal Dynamics Prediction",
    "authors": [
      "Yuan Mi",
      "Pu Ren",
      "Hongteng Xu",
      "Hongsheng Liu",
      "Zidong Wang",
      "Yike Guo",
      "Ji-Rong Wen",
      "Hao Sun",
      "Yang Liu"
    ],
    "abstract": "Data-centric methods have shown great potential in understanding and\npredicting spatiotemporal dynamics, enabling better design and control of the\nobject system. However, deep learning models often lack interpretability, fail\nto obey intrinsic physics, and struggle to cope with the various domains. While\ngeometry-based methods, e.g., graph neural networks (GNNs), have been proposed\nto further tackle these challenges, they still need to find the implicit\nphysical laws from large datasets and rely excessively on rich labeled data. In\nthis paper, we herein introduce the conservation-informed GNN (CiGNN), an\nend-to-end explainable learning framework, to learn spatiotemporal dynamics\nbased on limited training data. The network is designed to conform to the\ngeneral conservation law via symmetry, where conservative and non-conservative\ninformation passes over a multiscale space enhanced by a latent temporal\nmarching strategy. The efficacy of our model has been verified in various\nspatiotemporal systems based on synthetic and real-world datasets, showing\nsuperiority over baseline models. Results demonstrate that CiGNN exhibits\nremarkable accuracy and generalizability, and is readily applicable to learning\nfor prediction of various spatiotemporal dynamics in a spatial domain with\ncomplex geometry.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.20962v3",
    "published_date": "2024-12-30 13:55:59 UTC",
    "updated_date": "2025-01-06 14:36:13 UTC"
  },
  {
    "arxiv_id": "2412.20960v1",
    "title": "Rise of Generative Artificial Intelligence in Science",
    "authors": [
      "Liangping Ding",
      "Cornelia Lawson",
      "Philip Shapira"
    ],
    "abstract": "Generative Artificial Intelligence (GenAI, generative AI) has rapidly become\navailable as a tool in scientific research. To explore the use of generative AI\nin science, we conduct an empirical analysis using OpenAlex. Analyzing GenAI\npublications and other AI publications from 2017 to 2023, we profile growth\npatterns, the diffusion of GenAI publications across fields of study, and the\ngeographical spread of scientific research on generative AI. We also\ninvestigate team size and international collaborations to explore whether\nGenAI, as an emerging scientific research area, shows different collaboration\npatterns compared to other AI technologies. The results indicate that\ngenerative AI has experienced rapid growth and increasing presence in\nscientific publications. The use of GenAI now extends beyond computer science\nto other scientific research domains. Over the study period, U.S. researchers\ncontributed nearly two-fifths of global GenAI publications. The U.S. is\nfollowed by China, with several small and medium-sized advanced economies\ndemonstrating relatively high levels of GenAI deployment in their research\npublications. Although scientific research overall is becoming increasingly\nspecialized and collaborative, our results suggest that GenAI research groups\ntend to have slightly smaller team sizes than found in other AI fields.\nFurthermore, notwithstanding recent geopolitical tensions, GenAI research\ncontinues to exhibit levels of international collaboration comparable to other\nAI technologies.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.IR",
      "H.3.3; K.4.0"
    ],
    "primary_category": "cs.CY",
    "comment": "26 pages, 4 tables, 1 figures, 1 appendix figure",
    "pdf_url": "http://arxiv.org/pdf/2412.20960v1",
    "published_date": "2024-12-30 13:55:28 UTC",
    "updated_date": "2024-12-30 13:55:28 UTC"
  },
  {
    "arxiv_id": "2501.01985v1",
    "title": "Fall Detection in Passenger Elevators using Intelligent Surveillance Camera Systems: An Application with YoloV8 Nano Model",
    "authors": [
      "Pinar Yozgatli",
      "Yavuz Acar",
      "Mehmet Tulumen",
      "Selman Minga",
      "Salih Selamet",
      "Beytullah Nalbant",
      "Mustafa Talha Toru",
      "Berna Koca",
      "Tevfik Keles",
      "Mehmet Selcok"
    ],
    "abstract": "Computer vision technology, which involves analyzing images and videos\ncaptured by cameras through deep learning algorithms, has significantly\nadvanced the field of human fall detection. This study focuses on the\napplication of the YoloV8 Nano model in identifying fall incidents within\npassenger elevators, a context that presents unique challenges due to the\nenclosed environment and varying lighting conditions. By training the model on\na robust dataset comprising over 10,000 images across diverse elevator types,\nwe aim to enhance the detection precision and recall rates. The model's\nperformance, with an 85% precision and 82% recall in fall detection,\nunderscores its potential for integration into existing elevator safety systems\nto enable rapid intervention.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.01985v1",
    "published_date": "2024-12-30 13:37:48 UTC",
    "updated_date": "2024-12-30 13:37:48 UTC"
  },
  {
    "arxiv_id": "2412.20942v1",
    "title": "Ontology-grounded Automatic Knowledge Graph Construction by LLM under Wikidata schema",
    "authors": [
      "Xiaohan Feng",
      "Xixin Wu",
      "Helen Meng"
    ],
    "abstract": "We propose an ontology-grounded approach to Knowledge Graph (KG) construction\nusing Large Language Models (LLMs) on a knowledge base. An ontology is authored\nby generating Competency Questions (CQ) on knowledge base to discover knowledge\nscope, extracting relations from CQs, and attempt to replace equivalent\nrelations by their counterpart in Wikidata. To ensure consistency and\ninterpretability in the resulting KG, we ground generation of KG with the\nauthored ontology based on extracted relations. Evaluation on benchmark\ndatasets demonstrates competitive performance in knowledge graph construction\ntask. Our work presents a promising direction for scalable KG construction\npipeline with minimal human intervention, that yields high quality and\nhuman-interpretable KGs, which are interoperable with Wikidata semantics for\npotential knowledge base expansion.",
    "categories": [
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "Presented at HI-AI@KDD, Human-Interpretable AI Workshop at the KDD\n  2024, 26th of August 2024, Barcelona, Spain",
    "pdf_url": "http://arxiv.org/pdf/2412.20942v1",
    "published_date": "2024-12-30 13:36:05 UTC",
    "updated_date": "2024-12-30 13:36:05 UTC"
  },
  {
    "arxiv_id": "2412.20924v1",
    "title": "HisynSeg: Weakly-Supervised Histopathological Image Segmentation via Image-Mixing Synthesis and Consistency Regularization",
    "authors": [
      "Zijie Fang",
      "Yifeng Wang",
      "Peizhang Xie",
      "Zhi Wang",
      "Yongbing Zhang"
    ],
    "abstract": "Tissue semantic segmentation is one of the key tasks in computational\npathology. To avoid the expensive and laborious acquisition of pixel-level\nannotations, a wide range of studies attempt to adopt the class activation map\n(CAM), a weakly-supervised learning scheme, to achieve pixel-level tissue\nsegmentation. However, CAM-based methods are prone to suffer from\nunder-activation and over-activation issues, leading to poor segmentation\nperformance. To address this problem, we propose a novel weakly-supervised\nsemantic segmentation framework for histopathological images based on\nimage-mixing synthesis and consistency regularization, dubbed HisynSeg.\nSpecifically, synthesized histopathological images with pixel-level masks are\ngenerated for fully-supervised model training, where two synthesis strategies\nare proposed based on Mosaic transformation and B\\'ezier mask generation.\nBesides, an image filtering module is developed to guarantee the authenticity\nof the synthesized images. In order to further avoid the model overfitting to\nthe occasional synthesis artifacts, we additionally propose a novel\nself-supervised consistency regularization, which enables the real images\nwithout segmentation masks to supervise the training of the segmentation model.\nBy integrating the proposed techniques, the HisynSeg framework successfully\ntransforms the weakly-supervised semantic segmentation problem into a\nfully-supervised one, greatly improving the segmentation accuracy. Experimental\nresults on three datasets prove that the proposed method achieves a\nstate-of-the-art performance. Code is available at\nhttps://github.com/Vison307/HisynSeg.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by IEEE Transactions on Medical Imaging",
    "pdf_url": "http://arxiv.org/pdf/2412.20924v1",
    "published_date": "2024-12-30 13:10:48 UTC",
    "updated_date": "2024-12-30 13:10:48 UTC"
  },
  {
    "arxiv_id": "2412.20903v4",
    "title": "WalkVLM:Aid Visually Impaired People Walking by Vision Language Model",
    "authors": [
      "Zhiqiang Yuan",
      "Ting Zhang",
      "Ying Deng",
      "Jiapei Zhang",
      "Yeshuang Zhu",
      "Zexi Jia",
      "Jie Zhou",
      "Jinchao Zhang"
    ],
    "abstract": "Approximately 200 million individuals around the world suffer from varying\ndegrees of visual impairment, making it crucial to leverage AI technology to\noffer walking assistance for these people. With the recent progress of\nvision-language models (VLMs), applying VLMs to offer walking guidance has\nbecome popular. However, the existing methods of walking guidance are mainly\nbased on self-curated question-answering datasets that are not publicly\naccessible, without a standardized benchmark for training or evaluation.\nMoreover, walking assistance often requires real-time streaming video analysis\nand the generation of concise yet informative reminders, making VLMs struggle\ndue to excessive responses and low efficiency in inferences. In this paper, we\nintroduce the first large-scale dataset dedicated to walking assistance,\ncomprising 12,000 video-annotation pairs, to provide a unified benchmark for\ntraining and evaluating systems to help visually-impaired individuals walk.\nFurthermore, a WalkVLM model is proposed, which employs chain of thought for\nhierarchical planning to generate concise but informative reminders and\nutilizes temporal-aware adaptive prediction to reduce the temporal redundancy\nof reminders. Finally, we have established a solid benchmark for blind walking\ntask and verified the advantages of WalkVLM in stream video processing for this\ntask compared to other VLMs. Our dataset and code are available at\nhttps://walkvlm2024.github.io.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.20903v4",
    "published_date": "2024-12-30 12:29:02 UTC",
    "updated_date": "2025-03-04 15:05:02 UTC"
  },
  {
    "arxiv_id": "2412.20901v1",
    "title": "ILDiff: Generate Transparent Animated Stickers by Implicit Layout Distillation",
    "authors": [
      "Ting Zhang",
      "Zhiqiang Yuan",
      "Yeshuang Zhu",
      "Jinchao Zhang"
    ],
    "abstract": "High-quality animated stickers usually contain transparent channels, which\nare often ignored by current video generation models. To generate fine-grained\nanimated transparency channels, existing methods can be roughly divided into\nvideo matting algorithms and diffusion-based algorithms. The methods based on\nvideo matting have poor performance in dealing with semi-open areas in\nstickers, while diffusion-based methods are often used to model a single image,\nwhich will lead to local flicker when modeling animated stickers. In this\npaper, we firstly propose an ILDiff method to generate animated transparent\nchannels through implicit layout distillation, which solves the problems of\nsemi-open area collapse and no consideration of temporal information in\nexisting methods. Secondly, we create the Transparent Animated Sticker Dataset\n(TASD), which contains 0.32M high-quality samples with transparent channel, to\nprovide data support for related fields. Extensive experiments demonstrate that\nILDiff can produce finer and smoother transparent channels compared to other\nmethods such as Matting Anything and Layer Diffusion. Our code and dataset will\nbe released at link https://xiaoyuan1996.github.io.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.20901v1",
    "published_date": "2024-12-30 12:27:35 UTC",
    "updated_date": "2024-12-30 12:27:35 UTC"
  },
  {
    "arxiv_id": "2501.00078v1",
    "title": "Human-like Bots for Tactical Shooters Using Compute-Efficient Sensors",
    "authors": [
      "Niels Justesen",
      "Maria Kaselimi",
      "Sam Snodgrass",
      "Miruna Vozaru",
      "Matthew Schlegel",
      "Jonas Wingren",
      "Gabriella A. B. Barros",
      "Tobias Mahlmann",
      "Shyam Sudhakaran",
      "Wesley Kerr",
      "Albert Wang",
      "Christoffer Holmgård",
      "Georgios N. Yannakakis",
      "Sebastian Risi",
      "Julian Togelius"
    ],
    "abstract": "Artificial intelligence (AI) has enabled agents to master complex video\ngames, from first-person shooters like Counter-Strike to real-time strategy\ngames such as StarCraft II and racing games like Gran Turismo. While these\nachievements are notable, applying these AI methods in commercial video game\nproduction remains challenging due to computational constraints. In commercial\nscenarios, the majority of computational resources are allocated to 3D\nrendering, leaving limited capacity for AI methods, which often demand high\ncomputational power, particularly those relying on pixel-based sensors.\nMoreover, the gaming industry prioritizes creating human-like behavior in AI\nagents to enhance player experience, unlike academic models that focus on\nmaximizing game performance. This paper introduces a novel methodology for\ntraining neural networks via imitation learning to play a complex,\ncommercial-standard, VALORANT-like 2v2 tactical shooter game, requiring only\nmodest CPU hardware during inference. Our approach leverages an innovative,\npixel-free perception architecture using a small set of ray-cast sensors, which\ncapture essential spatial information efficiently. These sensors allow AI to\nperform competently without the computational overhead of traditional methods.\nModels are trained to mimic human behavior using supervised learning on human\ntrajectory data, resulting in realistic and engaging AI agents. Human\nevaluation tests confirm that our AI agents provide human-like gameplay\nexperiences while operating efficiently under computational constraints. This\noffers a significant advancement in AI model development for tactical shooter\ngames and possibly other genres.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.00078v1",
    "published_date": "2024-12-30 12:06:37 UTC",
    "updated_date": "2024-12-30 12:06:37 UTC"
  },
  {
    "arxiv_id": "2501.14772v1",
    "title": "DropMicroFluidAgents (DMFAs): Autonomous Droplet Microfluidic Research Framework Through Large Language Model Agents",
    "authors": [
      "Dinh-Nguyen Nguyen",
      "Raymond Kai-Yu Tong",
      "Ngoc-Duy Dinh"
    ],
    "abstract": "Applying Large language models (LLMs) within specific domains requires\nsubstantial adaptation to account for the unique terminologies, nuances, and\ncontext-specific challenges inherent to those areas. Here, we introduce\nDropMicroFluidAgents (DMFAs), an advanced language-driven framework leveraging\nstate-of-the-art pre-trained LLMs. DMFAs employs LLM agents to perform two key\nfunctions: (1) delivering focused guidance, answers, and suggestions specific\nto droplet microfluidics and (2) generating machine learning models to optimise\nand automate the design of droplet microfluidic devices, including the creation\nof code-based computer-aided design (CAD) scripts to enable rapid and precise\ndesign execution. Experimental evaluations demonstrated that the integration of\nDMFAs with the LLAMA3.1 model yielded the highest accuracy of 76.15%,\nunderscoring the significant performance enhancement provided by agent\nintegration. This effect was particularly pronounced when DMFAs were paired\nwith the GEMMA2 model, resulting in a 34.47% improvement in accuracy compared\nto the standalone GEMMA2 configuration. This study demonstrates the effective\nuse of LLM agents in droplet microfluidics research as powerful tools for\nautomating workflows, synthesising knowledge, optimising designs, and\ninteracting with external systems. These capabilities enable their application\nacross education and industrial support, driving greater efficiency in\nscientific discovery and innovation.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14772v1",
    "published_date": "2024-12-30 11:58:52 UTC",
    "updated_date": "2024-12-30 11:58:52 UTC"
  },
  {
    "arxiv_id": "2501.01984v1",
    "title": "Leveraging AI for Automatic Classification of PCOS Using Ultrasound Imaging",
    "authors": [
      "Atharva Divekar",
      "Atharva Sonawane"
    ],
    "abstract": "The AUTO-PCOS Classification Challenge seeks to advance the diagnostic\ncapabilities of artificial intelligence (AI) in identifying Polycystic Ovary\nSyndrome (PCOS) through automated classification of healthy and unhealthy\nultrasound frames. This report outlines our methodology for building a robust\nAI pipeline utilizing transfer learning with the InceptionV3 architecture to\nachieve high accuracy in binary classification. Preprocessing steps ensured the\ndataset was optimized for training, validation, and testing, while\ninterpretability methods like LIME and saliency maps provided valuable insights\ninto the model's decision-making. Our approach achieved an accuracy of 90.52%,\nwith precision, recall, and F1-score metrics exceeding 90% on validation data,\ndemonstrating its efficacy. The project underscores the transformative\npotential of AI in healthcare, particularly in addressing diagnostic challenges\nlike PCOS. Key findings, challenges, and recommendations for future\nenhancements are discussed, highlighting the pathway for creating reliable,\ninterpretable, and scalable AI-driven medical diagnostic tools.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "I.4.9"
    ],
    "primary_category": "eess.IV",
    "comment": "Code available at: https://github.com/ATHdevs/Auto-PCOS",
    "pdf_url": "http://arxiv.org/pdf/2501.01984v1",
    "published_date": "2024-12-30 11:56:11 UTC",
    "updated_date": "2024-12-30 11:56:11 UTC"
  },
  {
    "arxiv_id": "2412.20867v1",
    "title": "Holistic Construction Automation with Modular Robots: From High-Level Task Specification to Execution",
    "authors": [
      "Jonathan Külz",
      "Michael Terzer",
      "Marco Magri",
      "Andrea Giusti",
      "Matthias Althoff"
    ],
    "abstract": "In situ robotic automation in construction is challenging due to constantly\nchanging environments, a shortage of robotic experts, and a lack of\nstandardized frameworks bridging robotics and construction practices. This work\nproposes a holistic framework for construction task specification, optimization\nof robot morphology, and mission execution using a mobile modular\nreconfigurable robot. Users can specify and monitor the desired robot behavior\nthrough a graphical interface. Our framework identifies an optimized robot\nmorphology and enables automatic real-world execution by integrating Building\nInformation Modelling (BIM). By leveraging modular robot components, we ensure\nseamless and fast adaption to the specific demands of the construction task.\nExperimental validation demonstrates that our approach robustly enables the\nautonomous execution of robotic drilling.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.20867v1",
    "published_date": "2024-12-30 11:11:13 UTC",
    "updated_date": "2024-12-30 11:11:13 UTC"
  },
  {
    "arxiv_id": "2412.20864v1",
    "title": "Enhancing Annotated Bibliography Generation with LLM Ensembles",
    "authors": [
      "Sergio Bermejo"
    ],
    "abstract": "This work proposes a novel approach to enhancing annotated bibliography\ngeneration through Large Language Model (LLM) ensembles. In particular,\nmultiple LLMs in different roles -- controllable text generation, evaluation,\nand summarization -- are introduced and validated using a systematic\nmethodology to enhance model performance in scholarly tasks. Output diversity\namong the ensemble that generates text is obtained using different LLM\nparameters, followed by an LLM acting as a judge to assess relevance, accuracy,\nand coherence. Responses selected by several combining strategies are then\nmerged and refined through summarization and redundancy removal techniques. The\npreliminary experimental validation demonstrates that the combined outputs from\nthe LLM ensemble improve coherence and relevance compared to individual\nresponses, leading to a 38% improvement in annotation quality and a 51%\nreduction in content redundancy, thus highlighting the potential for automating\ncomplex scholarly tasks while maintaining high-quality standards.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.20864v1",
    "published_date": "2024-12-30 11:07:05 UTC",
    "updated_date": "2024-12-30 11:07:05 UTC"
  },
  {
    "arxiv_id": "2502.15693v1",
    "title": "Hgformer: Hyperbolic Graph Transformer for Recommendation",
    "authors": [
      "Xin Yang",
      "Xingrun Li",
      "Heng Chang",
      "Jinze Yang",
      "Xihong Yang",
      "Shengyu Tao",
      "Ningkang Chang",
      "Maiko Shigeno",
      "Junfeng Wang",
      "Dawei Yin",
      "Erxue Min"
    ],
    "abstract": "The cold start problem is a challenging problem faced by most modern\nrecommender systems. By leveraging knowledge from other domains, cross-domain\nrecommendation can be an effective method to alleviate the cold start problem.\nHowever, the modelling distortion for long-tail data, which is widely present\nin recommender systems, is often overlooked in cross-domain recommendation. In\nthis research, we propose a hyperbolic manifold based cross-domain\ncollaborative filtering model using BiTGCF as the base model. We introduce the\nhyperbolic manifold and construct new propagation layer and transfer layer to\naddress these challenges. The significant performance improvements across\nvarious datasets compared to the baseline models demonstrate the effectiveness\nof our proposed model.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.15693v1",
    "published_date": "2024-12-30 10:50:51 UTC",
    "updated_date": "2024-12-30 10:50:51 UTC"
  },
  {
    "arxiv_id": "2412.20851v1",
    "title": "About rectified sigmoid function for enhancing the accuracy of Physics-Informed Neural Networks",
    "authors": [
      "Vasiliy A. Es'kin",
      "Alexey O. Malkhanov",
      "Mikhail E. Smorkalov"
    ],
    "abstract": "The article is devoted to the study of neural networks with one hidden layer\nand a modified activation function for solving physical problems. A rectified\nsigmoid activation function has been proposed to solve physical problems\ndescribed by the ODE with neural networks. Algorithms for physics-informed\ndata-driven initialization of a neural network and a neuron-by-neuron\ngradient-free fitting method have been presented for the neural network with\nthis activation function. Numerical experiments demonstrate the superiority of\nneural networks with a rectified sigmoid function over neural networks with a\nsigmoid function in the accuracy of solving physical problems (harmonic\noscillator, relativistic slingshot, and Lorentz system).",
    "categories": [
      "math.NA",
      "cs.AI",
      "cs.LG",
      "cs.NA",
      "physics.comp-ph",
      "68T07 (Primary) 65Z05, 65M99 (Secondary)",
      "I.2.1; I.2.7; J.2"
    ],
    "primary_category": "math.NA",
    "comment": "9 pages, 1 figure, 2 tables, 4 algthorithms. arXiv admin note:\n  substantial text overlap with arXiv:2412.19235",
    "pdf_url": "http://arxiv.org/pdf/2412.20851v1",
    "published_date": "2024-12-30 10:42:28 UTC",
    "updated_date": "2024-12-30 10:42:28 UTC"
  },
  {
    "arxiv_id": "2412.20848v1",
    "title": "Analog Alchemy: Neural Computation with In-Memory Inference, Learning and Routing",
    "authors": [
      "Yigit Demirag"
    ],
    "abstract": "As neural computation is revolutionizing the field of Artificial Intelligence\n(AI), rethinking the ideal neural hardware is becoming the next frontier. Fast\nand reliable von Neumann architecture has been the hosting platform for neural\ncomputation. Although capable, its separation of memory and computation creates\nthe bottleneck for the energy efficiency of neural computation, contrasting the\nbiological brain. The question remains: how can we efficiently combine memory\nand computation, while exploiting the physics of the substrate, to build\nintelligent systems? In this thesis, I explore an alternative way with\nmemristive devices for neural computation, where the unique physical dynamics\nof the devices are used for inference, learning and routing. Guided by the\nprinciples of gradient-based learning, we selected functions that need to be\nmaterialized, and analyzed connectomics principles for efficient wiring.\nDespite non-idealities and noise inherent in analog physics, I will provide\nhardware evidence of adaptability of local learning to memristive substrates,\nnew material stacks and circuit blocks that aid in solving the credit\nassignment problem and efficient routing between analog crossbars for scalable\narchitectures.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.20848v1",
    "published_date": "2024-12-30 10:35:03 UTC",
    "updated_date": "2024-12-30 10:35:03 UTC"
  },
  {
    "arxiv_id": "2412.20838v1",
    "title": "Dual-Space Augmented Intrinsic-LoRA for Wind Turbine Segmentation",
    "authors": [
      "Shubh Singhal",
      "Raül Pérez-Gonzalo",
      "Andreas Espersen",
      "Antonio Agudo"
    ],
    "abstract": "Accurate segmentation of wind turbine blade (WTB) images is critical for\neffective assessments, as it directly influences the performance of automated\ndamage detection systems. Despite advancements in large universal vision\nmodels, these models often underperform in domain-specific tasks like WTB\nsegmentation. To address this, we extend Intrinsic LoRA for image segmentation,\nand propose a novel dual-space augmentation strategy that integrates both\nimage-level and latent-space augmentations. The image-space augmentation is\nachieved through linear interpolation between image pairs, while the\nlatent-space augmentation is accomplished by introducing a noise-based latent\nprobabilistic model. Our approach significantly boosts segmentation accuracy,\nsurpassing current state-of-the-art methods in WTB image segmentation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Authors Shubh Singhal and Ra\\\"ul P\\'erez-Gonzalo contributed equally\n  to this work. Accepted to ICASSP 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.20838v1",
    "published_date": "2024-12-30 10:06:02 UTC",
    "updated_date": "2024-12-30 10:06:02 UTC"
  },
  {
    "arxiv_id": "2412.20834v1",
    "title": "Disentangling Preference Representation and Text Generation for Efficient Individual Preference Alignment",
    "authors": [
      "Jianfei Zhang",
      "Jun Bai",
      "Bei Li",
      "Yanmeng Wang",
      "Rumei Li",
      "Chenghua Lin",
      "Wenge Rong"
    ],
    "abstract": "Aligning Large Language Models (LLMs) with general human preferences has been\nproved crucial in improving the interaction quality between LLMs and human.\nHowever, human values are inherently diverse among different individuals,\nmaking it insufficient to align LLMs solely with general preferences. To\naddress this, personalizing LLMs according to individual feedback emerges as a\npromising solution. Nonetheless, this approach presents challenges in terms of\nthe efficiency of alignment algorithms. In this work, we introduce a flexible\nparadigm for individual preference alignment. Our method fundamentally improves\nefficiency by disentangling preference representation from text generation in\nLLMs. We validate our approach across multiple text generation tasks and\ndemonstrate that it can produce aligned quality as well as or better than\nPEFT-based methods, while reducing additional training time for each new\nindividual preference by $80\\%$ to $90\\%$ in comparison with them.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Coling 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.20834v1",
    "published_date": "2024-12-30 09:58:31 UTC",
    "updated_date": "2024-12-30 09:58:31 UTC"
  },
  {
    "arxiv_id": "2412.20822v1",
    "title": "Fine-Tuning TransMorph with Gradient Correlation for Anatomical Alignment",
    "authors": [
      "Lukas Förner",
      "Kartikay Tehlan",
      "Thomas Wendler"
    ],
    "abstract": "Unsupervised deep learning is a promising method in brain MRI registration to\nreduce the reliance on anatomical labels, while still achieving anatomically\naccurate transformations. For the Learn2Reg2024 LUMIR challenge, we propose\nfine-tuning of the pre-trained TransMorph model to improve the convergence\nstability as well as the deformation smoothness. The former is achieved through\nthe FAdam optimizer, and consistency in structural changes is incorporated\nthrough the addition of gradient correlation in the similarity measure,\nimproving anatomical alignment. The results show slight improvements in the\nDice and HdDist95 scores, and a notable reduction in the NDV compared to the\nbaseline TransMorph model. These are also confirmed by inspecting the\nboundaries of the tissue. Our proposed method highlights the effectiveness of\nincluding Gradient Correlation to achieve smoother and structurally consistent\ndeformations for interpatient brain MRI registration.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.20822v1",
    "published_date": "2024-12-30 09:32:04 UTC",
    "updated_date": "2024-12-30 09:32:04 UTC"
  },
  {
    "arxiv_id": "2412.20816v1",
    "title": "Length-Aware DETR for Robust Moment Retrieval",
    "authors": [
      "Seojeong Park",
      "Jiho Choi",
      "Kyungjune Baek",
      "Hyunjung Shim"
    ],
    "abstract": "Video Moment Retrieval (MR) aims to localize moments within a video based on\na given natural language query. Given the prevalent use of platforms like\nYouTube for information retrieval, the demand for MR techniques is\nsignificantly growing. Recent DETR-based models have made notable advances in\nperformance but still struggle with accurately localizing short moments.\nThrough data analysis, we identified limited feature diversity in short\nmoments, which motivated the development of MomentMix. MomentMix employs two\naugmentation strategies: ForegroundMix and BackgroundMix, each enhancing the\nfeature representations of the foreground and background, respectively.\nAdditionally, our analysis of prediction bias revealed that short moments\nparticularly struggle with accurately predicting their center positions of\nmoments. To address this, we propose a Length-Aware Decoder, which conditions\nlength through a novel bipartite matching process. Our extensive studies\ndemonstrate the efficacy of our length-aware approach, especially in localizing\nshort moments, leading to improved overall performance. Our method surpasses\nstate-of-the-art DETR-based methods on benchmark datasets, achieving the\nhighest R1 and mAP on QVHighlights and the highest R1@0.7 on TACoS and\nCharades-STA (such as a 2.46% gain in R1@0.7 and a 2.57% gain in mAP average\nfor QVHighlights). The code is available at\nhttps://github.com/sjpark5800/LA-DETR.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.20816v1",
    "published_date": "2024-12-30 09:11:14 UTC",
    "updated_date": "2024-12-30 09:11:14 UTC"
  },
  {
    "arxiv_id": "2412.20807v1",
    "title": "Two Heads Are Better Than One: Averaging along Fine-Tuning to Improve Targeted Transferability",
    "authors": [
      "Hui Zeng",
      "Sanshuai Cui",
      "Biwei Chen",
      "Anjie Peng"
    ],
    "abstract": "With much longer optimization time than that of untargeted attacks\nnotwithstanding, the transferability of targeted attacks is still far from\nsatisfactory. Recent studies reveal that fine-tuning an existing adversarial\nexample (AE) in feature space can efficiently boost its targeted\ntransferability. However, existing fine-tuning schemes only utilize the\nendpoint and ignore the valuable information in the fine-tuning trajectory.\nNoting that the vanilla fine-tuning trajectory tends to oscillate around the\nperiphery of a flat region of the loss surface, we propose averaging over the\nfine-tuning trajectory to pull the crafted AE towards a more centered region.\nWe compare the proposed method with existing fine-tuning schemes by integrating\nthem with state-of-the-art targeted attacks in various attacking scenarios.\nExperimental results uphold the superiority of the proposed method in boosting\ntargeted transferability. The code is available at github.com/zengh5/Avg_FT.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages, 6 figures, accepted by 2025ICASSP",
    "pdf_url": "http://arxiv.org/pdf/2412.20807v1",
    "published_date": "2024-12-30 09:01:27 UTC",
    "updated_date": "2024-12-30 09:01:27 UTC"
  },
  {
    "arxiv_id": "2501.01983v1",
    "title": "ECG-guided individual identification via PPG",
    "authors": [
      "Riling Wei",
      "Hanjie Chen",
      "Kelu Yao",
      "Chuanguang Yang",
      "Jun Wang",
      "Chao Li"
    ],
    "abstract": "Photoplethsmography (PPG)-based individual identification aiming at\nrecognizing humans via intrinsic cardiovascular activities has raised extensive\nattention due to its high security and resistance to mimicry. However, this\nkind of technology witnesses unpromising results due to the limitation of low\ninformation density. To this end, electrocardiogram (ECG) signals have been\nintroduced as a novel modality to enhance the density of input information.\nSpecifically, a novel cross-modal knowledge distillation framework is\nimplemented to propagate discriminate knowledge from ECG modality to PPG\nmodality without incurring additional computational demands at the inference\nphase. Furthermore, to ensure efficient knowledge propagation, Contrastive\nLanguage-Image Pre-training (CLIP)-based knowledge alignment and\ncross-knowledge assessment modules are proposed respectively. Comprehensive\nexperiments are conducted and results show our framework outperforms the\nbaseline model with the improvement of 2.8% and 3.0% in terms of overall\naccuracy on seen- and unseen individual recognitions.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ICASSP 2025. Camera Ready Version",
    "pdf_url": "http://arxiv.org/pdf/2501.01983v1",
    "published_date": "2024-12-30 08:56:23 UTC",
    "updated_date": "2024-12-30 08:56:23 UTC"
  },
  {
    "arxiv_id": "2412.20798v3",
    "title": "A Tale of Two Imperatives: Privacy and Explainability",
    "authors": [
      "Supriya Manna",
      "Niladri Sett"
    ],
    "abstract": "Deep learning's preponderance across scientific domains has reshaped\nhigh-stakes decision-making, making it essential to follow rigorous operational\nframeworks that include both Right-to-Privacy (RTP) and Right-to-Explanation\n(RTE). This paper examines the complexities of combining these two\nrequirements. For RTP, we focus on `Differential privacy' (DP), which is\nconsidered the current \\textit{gold standard} for privacy-preserving machine\nlearning due to its strong quantitative guarantee of privacy. For RTE, we focus\non post-hoc explainers: they are the \\textit{go-to} option for model auditing\nas they operate independently of model training. We formally investigate DP\nmodels and various commonly-used post-hoc explainers: how to evaluate these\nexplainers subject to RTP, and analyze the intrinsic interactions between DP\nmodels and these explainers. Furthermore, our work throws light on how RTP and\nRTE can be effectively combined in high-stakes applications. Our study\nconcludes by outlining an industrial software pipeline, with the example of a\nwildly used use-case, that respects both RTP and RTE requirements.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CR",
    "comment": "12 figures, Work Under Review",
    "pdf_url": "http://arxiv.org/pdf/2412.20798v3",
    "published_date": "2024-12-30 08:43:28 UTC",
    "updated_date": "2025-02-22 23:09:31 UTC"
  },
  {
    "arxiv_id": "2412.20790v2",
    "title": "Frequency-Masked Embedding Inference: A Non-Contrastive Approach for Time Series Representation Learning",
    "authors": [
      "En Fu",
      "Yanyan Hu"
    ],
    "abstract": "Contrastive learning underpins most current self-supervised time series\nrepresentation methods. The strategy for constructing positive and negative\nsample pairs significantly affects the final representation quality. However,\ndue to the continuous nature of time series semantics, the modeling approach of\ncontrastive learning struggles to accommodate the characteristics of time\nseries data. This results in issues such as difficulties in constructing hard\nnegative samples and the potential introduction of inappropriate biases during\npositive sample construction. Although some recent works have developed several\nscientific strategies for constructing positive and negative sample pairs with\nimproved effectiveness, they remain constrained by the contrastive learning\nframework. To fundamentally overcome the limitations of contrastive learning,\nthis paper introduces Frequency-masked Embedding Inference (FEI), a novel\nnon-contrastive method that completely eliminates the need for positive and\nnegative samples. The proposed FEI constructs 2 inference branches based on a\nprompting strategy: 1) Using frequency masking as prompts to infer the\nembedding representation of the target series with missing frequency bands in\nthe embedding space, and 2) Using the target series as prompts to infer its\nfrequency masking embedding. In this way, FEI enables continuous semantic\nrelationship modeling for time series. Experiments on 8 widely used time series\ndatasets for classification and regression tasks, using linear evaluation and\nend-to-end fine-tuning, show that FEI significantly outperforms existing\ncontrastive-based methods in terms of generalization. This study provides new\ninsights into self-supervised representation learning for time series. The code\nis available at\nhttps://github.com/USTBInnovationPark/Frequency-masked-Embedding-Inference.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper has been accepted by AAAI-2025 main track",
    "pdf_url": "http://arxiv.org/pdf/2412.20790v2",
    "published_date": "2024-12-30 08:12:17 UTC",
    "updated_date": "2025-01-06 12:17:43 UTC"
  },
  {
    "arxiv_id": "2412.20787v3",
    "title": "SecBench: A Comprehensive Multi-Dimensional Benchmarking Dataset for LLMs in Cybersecurity",
    "authors": [
      "Pengfei Jing",
      "Mengyun Tang",
      "Xiaorong Shi",
      "Xing Zheng",
      "Sen Nie",
      "Shi Wu",
      "Yong Yang",
      "Xiapu Luo"
    ],
    "abstract": "Evaluating Large Language Models (LLMs) is crucial for understanding their\ncapabilities and limitations across various applications, including natural\nlanguage processing and code generation. Existing benchmarks like MMLU, C-Eval,\nand HumanEval assess general LLM performance but lack focus on specific expert\ndomains such as cybersecurity. Previous attempts to create cybersecurity\ndatasets have faced limitations, including insufficient data volume and a\nreliance on multiple-choice questions (MCQs). To address these gaps, we propose\nSecBench, a multi-dimensional benchmarking dataset designed to evaluate LLMs in\nthe cybersecurity domain. SecBench includes questions in various formats (MCQs\nand short-answer questions (SAQs)), at different capability levels (Knowledge\nRetention and Logical Reasoning), in multiple languages (Chinese and English),\nand across various sub-domains. The dataset was constructed by collecting\nhigh-quality data from open sources and organizing a Cybersecurity Question\nDesign Contest, resulting in 44,823 MCQs and 3,087 SAQs. Particularly, we used\nthe powerful while cost-effective LLMs to (1). label the data and (2).\nconstructing a grading agent for automatic evaluation of SAQs. Benchmarking\nresults on 16 SOTA LLMs demonstrate the usability of SecBench, which is\narguably the largest and most comprehensive benchmark dataset for LLMs in\ncybersecurity. More information about SecBench can be found at our website, and\nthe dataset can be accessed via the artifact link.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.20787v3",
    "published_date": "2024-12-30 08:11:54 UTC",
    "updated_date": "2025-01-06 07:22:50 UTC"
  },
  {
    "arxiv_id": "2502.15692v1",
    "title": "ACL-rlg: A Dataset for Reading List Generation",
    "authors": [
      "Julien Aubert-Béduchaud",
      "Florian Boudin",
      "Béatrice Daille",
      "Richard Dufour"
    ],
    "abstract": "Familiarizing oneself with a new scientific field and its existing literature\ncan be daunting due to the large amount of available articles. Curated lists of\nacademic references, or reading lists, compiled by experts, offer a structured\nway to gain a comprehensive overview of a domain or a specific scientific\nchallenge. In this work, we introduce ACL-rlg, the largest open\nexpert-annotated reading list dataset. We also provide multiple baselines for\nevaluating reading list generation and formally define it as a retrieval task.\nOur qualitative study highlights the fact that traditional scholarly search\nengines and indexing methods perform poorly on this task, and GPT-4o, despite\nshowing better results, exhibits signs of potential data contamination.",
    "categories": [
      "cs.DL",
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.DL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.15692v1",
    "published_date": "2024-12-30 07:48:32 UTC",
    "updated_date": "2024-12-30 07:48:32 UTC"
  },
  {
    "arxiv_id": "2412.20768v1",
    "title": "Sample Correlation for Fingerprinting Deep Face Recognition",
    "authors": [
      "Jiyang Guan",
      "Jian Liang",
      "Yanbo Wang",
      "Ran He"
    ],
    "abstract": "Face recognition has witnessed remarkable advancements in recent years,\nthanks to the development of deep learning techniques.However, an off-the-shelf\nface recognition model as a commercial service could be stolen by model\nstealing attacks, posing great threats to the rights of the model owner.Model\nfingerprinting, as a model stealing detection method, aims to verify whether a\nsuspect model is stolen from the victim model, gaining more and more attention\nnowadays.Previous methods always utilize transferable adversarial examples as\nthe model fingerprint, but this method is known to be sensitive to adversarial\ndefense and transfer learning techniques.To address this issue, we consider the\npairwise relationship between samples instead and propose a novel yet simple\nmodel stealing detection method based on SAmple Correlation (SAC).Specifically,\nwe present SAC-JC that selects JPEG compressed samples as model inputs and\ncalculates the correlation matrix among their model outputs.Extensive results\nvalidate that SAC successfully defends against various model stealing attacks\nin deep face recognition, encompassing face verification and face emotion\nrecognition, exhibiting the highest performance in terms of AUC, p-value and F1\nscore.Furthermore, we extend our evaluation of SAC-JC to object recognition\ndatasets including Tiny-ImageNet and CIFAR10, which also demonstrates the\nsuperior performance of SAC-JC to previous methods.The code will be available\nat \\url{https://github.com/guanjiyang/SAC_JC}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.20768v1",
    "published_date": "2024-12-30 07:37:06 UTC",
    "updated_date": "2024-12-30 07:37:06 UTC"
  },
  {
    "arxiv_id": "2412.20767v1",
    "title": "KeyGS: A Keyframe-Centric Gaussian Splatting Method for Monocular Image Sequences",
    "authors": [
      "Keng-Wei Chang",
      "Zi-Ming Wang",
      "Shang-Hong Lai"
    ],
    "abstract": "Reconstructing high-quality 3D models from sparse 2D images has garnered\nsignificant attention in computer vision. Recently, 3D Gaussian Splatting\n(3DGS) has gained prominence due to its explicit representation with efficient\ntraining speed and real-time rendering capabilities. However, existing methods\nstill heavily depend on accurate camera poses for reconstruction. Although some\nrecent approaches attempt to train 3DGS models without the\nStructure-from-Motion (SfM) preprocessing from monocular video datasets, these\nmethods suffer from prolonged training times, making them impractical for many\napplications.\n  In this paper, we present an efficient framework that operates without any\ndepth or matching model. Our approach initially uses SfM to quickly obtain\nrough camera poses within seconds, and then refines these poses by leveraging\nthe dense representation in 3DGS. This framework effectively addresses the\nissue of long training times. Additionally, we integrate the densification\nprocess with joint refinement and propose a coarse-to-fine frequency-aware\ndensification to reconstruct different levels of details. This approach\nprevents camera pose estimation from being trapped in local minima or drifting\ndue to high-frequency signals. Our method significantly reduces training time\nfrom hours to minutes while achieving more accurate novel view synthesis and\ncamera pose estimation compared to previous methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.20767v1",
    "published_date": "2024-12-30 07:32:35 UTC",
    "updated_date": "2024-12-30 07:32:35 UTC"
  },
  {
    "arxiv_id": "2501.00076v1",
    "title": "A Novel Framework for Learning Stochastic Representations for Sequence Generation and Recognition",
    "authors": [
      "Jungsik Hwang",
      "Ahmadreza Ahmadi"
    ],
    "abstract": "The ability to generate and recognize sequential data is fundamental for\nautonomous systems operating in dynamic environments. Inspired by the key\nprinciples of the brain-predictive coding and the Bayesian brain-we propose a\nnovel stochastic Recurrent Neural Network with Parametric Biases (RNNPB). The\nproposed model incorporates stochasticity into the latent space using the\nreparameterization trick used in variational autoencoders. This approach\nenables the model to learn probabilistic representations of multidimensional\nsequences, capturing uncertainty and enhancing robustness against overfitting.\nWe tested the proposed model on a robotic motion dataset to assess its\nperformance in generating and recognizing temporal patterns. The experimental\nresults showed that the stochastic RNNPB model outperformed its deterministic\ncounterpart in generating and recognizing motion sequences. The results\nhighlighted the proposed model's capability to quantify and adjust uncertainty\nduring both learning and inference. The stochasticity resulted in a continuous\nlatent space representation, facilitating stable motion generation and enhanced\ngeneralization when recognizing novel sequences. Our approach provides a\nbiologically inspired framework for modeling temporal patterns and advances the\ndevelopment of robust and adaptable systems in artificial intelligence and\nrobotics.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.00076v1",
    "published_date": "2024-12-30 07:27:50 UTC",
    "updated_date": "2024-12-30 07:27:50 UTC"
  },
  {
    "arxiv_id": "2412.20760v2",
    "title": "Attributing Culture-Conditioned Generations to Pretraining Corpora",
    "authors": [
      "Huihan Li",
      "Arnav Goel",
      "Keyu He",
      "Xiang Ren"
    ],
    "abstract": "In open-ended generative tasks like narrative writing or dialogue, large\nlanguage models often exhibit cultural biases, showing limited knowledge and\ngenerating templated outputs for less prevalent cultures. Recent works show\nthat these biases may stem from uneven cultural representation in pretraining\ncorpora. This work investigates how pretraining leads to biased\nculture-conditioned generations by analyzing how models associate entities with\ncultures based on pretraining data patterns. We propose the MEMOed framework\n(MEMOrization from pretraining document) to determine whether a generation for\na culture arises from memorization. Using MEMOed on culture-conditioned\ngenerations about food and clothing for 110 cultures, we find that\nhigh-frequency cultures in pretraining data yield more generations with\nmemorized symbols, while some low-frequency cultures produce none.\nAdditionally, the model favors generating entities with extraordinarily high\nfrequency regardless of the conditioned culture, reflecting biases toward\nfrequent pretraining terms irrespective of relevance. We hope that the MEMOed\nframework and our insights will inspire more works on attributing model\nperformance on pretraining data.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.20760v2",
    "published_date": "2024-12-30 07:09:25 UTC",
    "updated_date": "2025-03-19 19:08:17 UTC"
  },
  {
    "arxiv_id": "2502.17442v1",
    "title": "Thinking Before Running! Efficient Code Generation with Thorough Exploration and Optimal Refinement",
    "authors": [
      "Xiaoqing Zhang",
      "Yuhan Liu",
      "Flood Sung",
      "Xiuying Chen",
      "Rui Yan"
    ],
    "abstract": "Code generation is crucial in software engineering for automating the coding\nprocess efficiently. While test-time computation methods show promise, they\nsuffer from high latency due to multiple computation rounds. To overcome this,\nwe introduce ThinkCoder, a framework that combines thorough exploration with\noptimal refinement. The exploration phase diversifies the solution space by\nsearching for potential solutions, followed by a refinement phase that enhances\nprecision. This approach allows us to select the best solution through careful\nconsideration before taking action, avoiding excessive trial and error. To\nfurther minimize test-time computation overhead, we introduce preference-driven\noptimization with Reinforced Self-Training (ReST), which uses exploration\ntrajectories from ThinkCoder to guide LLM's evolution. By learning preferences,\nthis approach improves LLM's exploration efficiency, reducing computational\ncosts while maintaining accuracy. ThinkCoder boosts the performance of multiple\nbase LLMs, excelling on benchmarks like HumanEval and MBPP. Compared to SOTA\nmodels, it improves Pass@1 by 1.5\\% over MapCoder with just 21.7\\% of the\ncomputation cost. Against AgentCoder, ThinkCoder achieves a 0.6\\% higher Pass@1\nafter 2 rounds, outperforming AgentCoder's 5 rounds. Additionally, ReST with\nsuccess trajectories enhances efficiency, allowing models like LLaMA2-7B to\nachieve competitive results using only 20\\% of the computational resources.\nThese results highlight the framework's effectiveness and scalability.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "14 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.17442v1",
    "published_date": "2024-12-30 07:02:15 UTC",
    "updated_date": "2024-12-30 07:02:15 UTC"
  },
  {
    "arxiv_id": "2412.20749v1",
    "title": "Solar Filaments Detection using Active Contours Without Edges",
    "authors": [
      "Sanmoy Bandyopadhyay",
      "Vaibhav Pant"
    ],
    "abstract": "In this article, an active contours without edges (ACWE)-based algorithm has\nbeen proposed for the detection of solar filaments in H-alpha full-disk solar\nimages. The overall algorithm consists of three main steps of image processing.\nThese are image pre-processing, image segmentation, and image post-processing.\nHere in the work, contours are initialized on the solar image and allowed to\ndeform based on the energy function. As soon as the contour reaches the\nboundary of the desired object, the energy function gets reduced, and the\ncontour stops evolving. The proposed algorithm has been applied to few\nbenchmark datasets and has been compared with the classical technique of object\ndetection. The results analysis indicates that the proposed algorithm\noutperforms the results obtained using the existing classical algorithm of\nobject detection.",
    "categories": [
      "cs.CV",
      "astro-ph.IM",
      "astro-ph.SR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "6 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.20749v1",
    "published_date": "2024-12-30 06:43:22 UTC",
    "updated_date": "2024-12-30 06:43:22 UTC"
  },
  {
    "arxiv_id": "2412.20744v1",
    "title": "Advancing Parkinson's Disease Progression Prediction: Comparing Long Short-Term Memory Networks and Kolmogorov-Arnold Networks",
    "authors": [
      "Abhinav Roy",
      "Bhavesh Gyanchandani",
      "Aditya Oza",
      "Abhishek Sharma"
    ],
    "abstract": "Parkinson's Disease (PD) is a degenerative neurological disorder that impairs\nmotor and non-motor functions, significantly reducing quality of life and\nincreasing mortality risk. Early and accurate detection of PD progression is\nvital for effective management and improved patient outcomes. Current\ndiagnostic methods, however, are often costly, time-consuming, and require\nspecialized equipment and expertise. This work proposes an innovative approach\nto predicting PD progression using regression methods, Long Short-Term Memory\n(LSTM) networks, and Kolmogorov Arnold Networks (KAN). KAN, utilizing\nspline-parametrized univariate functions, allows for dynamic learning of\nactivation patterns, unlike traditional linear models.\n  The Movement Disorder Society-Sponsored Revision of the Unified Parkinson's\nDisease Rating Scale (MDS-UPDRS) is a comprehensive tool for evaluating PD\nsymptoms and is commonly used to measure disease progression. Additionally,\nprotein or peptide abnormalities are linked to PD onset and progression.\nIdentifying these associations can aid in predicting disease progression and\nunderstanding molecular changes.\n  Comparing multiple models, including LSTM and KAN, this study aims to\nidentify the method that delivers the highest metrics. The analysis reveals\nthat KAN, with its dynamic learning capabilities, outperforms other approaches\nin predicting PD progression. This research highlights the potential of AI and\nmachine learning in healthcare, paving the way for advanced computational\nmodels to enhance clinical predictions and improve patient care and treatment\nstrategies in PD management.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.20744v1",
    "published_date": "2024-12-30 06:36:05 UTC",
    "updated_date": "2024-12-30 06:36:05 UTC"
  },
  {
    "arxiv_id": "2412.20735v3",
    "title": "HunyuanProver: A Scalable Data Synthesis Framework and Guided Tree Search for Automated Theorem Proving",
    "authors": [
      "Yang Li",
      "Dong Du",
      "Linfeng Song",
      "Chen Li",
      "Weikang Wang",
      "Tao Yang",
      "Haitao Mi"
    ],
    "abstract": "We introduce HunyuanProver, an language model finetuned from the Hunyuan 7B\nfor interactive automatic theorem proving with LEAN4. To alleviate the data\nsparsity issue, we design a scalable framework to iterative synthesize data\nwith low cost. Besides, guided tree search algorithms are designed to enable\neffective ``system 2 thinking`` of the prover. HunyuanProver achieves\nstate-of-the-art (SOTA) performances on major benchmarks. Specifically, it\nachieves a pass of 68.4% on the miniF2F-test compared to 65.9%, the current\nSOTA results. It proves 4 IMO statements (imo_1960_p2, imo_1962_p2},\nimo_1964_p2 and imo_1983_p6) in miniF2F-test. To benefit the community, we will\nopen-source a dataset of 30k synthesized instances, where each instance\ncontains the original question in natural language, the converted statement by\nautoformalization, and the proof by HunyuanProver.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.20735v3",
    "published_date": "2024-12-30 06:18:33 UTC",
    "updated_date": "2025-03-21 02:00:37 UTC"
  },
  {
    "arxiv_id": "2412.20733v1",
    "title": "Towards nation-wide analytical healthcare infrastructures: A privacy-preserving augmented knee rehabilitation case study",
    "authors": [
      "Boris Bačić",
      "Claudiu Vasile",
      "Chengwei Feng",
      "Marian G. Ciucă"
    ],
    "abstract": "The purpose of this paper is to contribute towards the near-future\nprivacy-preserving big data analytical healthcare platforms, capable of\nprocessing streamed or uploaded timeseries data or videos from patients. The\nexperimental work includes a real-life knee rehabilitation video dataset\ncapturing a set of exercises from simple and personalised to more general and\nchallenging movements aimed for returning to sport. To convert video from\nmobile into privacy-preserving diagnostic timeseries data, we employed Google\nMediaPipe pose estimation. The developed proof-of-concept algorithms can\naugment knee exercise videos by overlaying the patient with stick figure\nelements while updating generated timeseries plot with knee angle estimation\nstreamed as CSV file format. For patients and physiotherapists, video with\nside-to-side timeseries visually indicating potential issues such as excessive\nknee flexion or unstable knee movements or stick figure overlay errors is\npossible by setting a-priori knee-angle parameters. To address adherence to\nrehabilitation programme and quantify exercise sets and repetitions, our\nadaptive algorithm can correctly identify (91.67%-100%) of all exercises from\nside- and front-view videos. Transparent algorithm design for adaptive visual\nanalysis of various knee exercise patterns contributes towards the\ninterpretable AI and will inform near-future privacy-preserving, non-vendor\nlocking, open-source developments for both end-user computing devices and as\non-premises non-proprietary cloud platforms that can be deployed within the\nnational healthcare system.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "The original work citation: Ba\\v{c}i\\'c, B., Claudiu Vasile, Feng,\n  C., & Ciuc\\u{a}, M. G. (2024, 13-15 Dec.). Towards nation-wide analytical\n  healthcare infrastructures: A privacy-preserving augmented knee\n  rehabilitation case study. Presented at the Conference on Innovative\n  Technologies in Intelligent Systems & Industrial Applications (CITISIA 2024),\n  Sydney, NSW",
    "pdf_url": "http://arxiv.org/pdf/2412.20733v1",
    "published_date": "2024-12-30 06:14:48 UTC",
    "updated_date": "2024-12-30 06:14:48 UTC"
  },
  {
    "arxiv_id": "2501.01449v1",
    "title": "LS-GAN: Human Motion Synthesis with Latent-space GANs",
    "authors": [
      "Avinash Amballa",
      "Gayathri Akkinapalli",
      "Vinitra Muralikrishnan"
    ],
    "abstract": "Human motion synthesis conditioned on textual input has gained significant\nattention in recent years due to its potential applications in various domains\nsuch as gaming, film production, and virtual reality. Conditioned Motion\nsynthesis takes a text input and outputs a 3D motion corresponding to the text.\nWhile previous works have explored motion synthesis using raw motion data and\nlatent space representations with diffusion models, these approaches often\nsuffer from high training and inference times. In this paper, we introduce a\nnovel framework that utilizes Generative Adversarial Networks (GANs) in the\nlatent space to enable faster training and inference while achieving results\ncomparable to those of the state-of-the-art diffusion methods. We perform\nexperiments on the HumanML3D, HumanAct12 benchmarks and demonstrate that a\nremarkably simple GAN in the latent space achieves a FID of 0.482 with more\nthan 91% in FLOPs reduction compared to latent diffusion model. Our work opens\nup new possibilities for efficient and high-quality motion synthesis using\nlatent space GANs.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "6 pages",
    "pdf_url": "http://arxiv.org/pdf/2501.01449v1",
    "published_date": "2024-12-30 05:44:38 UTC",
    "updated_date": "2024-12-30 05:44:38 UTC"
  },
  {
    "arxiv_id": "2412.20718v1",
    "title": "M$^3$oralBench: A MultiModal Moral Benchmark for LVLMs",
    "authors": [
      "Bei Yan",
      "Jie Zhang",
      "Zhiyuan Chen",
      "Shiguang Shan",
      "Xilin Chen"
    ],
    "abstract": "Recently, large foundation models, including large language models (LLMs) and\nlarge vision-language models (LVLMs), have become essential tools in critical\nfields such as law, finance, and healthcare. As these models increasingly\nintegrate into our daily life, it is necessary to conduct moral evaluation to\nensure that their outputs align with human values and remain within moral\nboundaries. Previous works primarily focus on LLMs, proposing moral datasets\nand benchmarks limited to text modality. However, given the rapid development\nof LVLMs, there is still a lack of multimodal moral evaluation methods. To\nbridge this gap, we introduce M$^3$oralBench, the first MultiModal Moral\nBenchmark for LVLMs. M$^3$oralBench expands the everyday moral scenarios in\nMoral Foundations Vignettes (MFVs) and employs the text-to-image diffusion\nmodel, SD3.0, to create corresponding scenario images. It conducts moral\nevaluation across six moral foundations of Moral Foundations Theory (MFT) and\nencompasses tasks in moral judgement, moral classification, and moral response,\nproviding a comprehensive assessment of model performance in multimodal moral\nunderstanding and reasoning. Extensive experiments on 10 popular open-source\nand closed-source LVLMs demonstrate that M$^3$oralBench is a challenging\nbenchmark, exposing notable moral limitations in current models. Our benchmark\nis publicly available.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.20718v1",
    "published_date": "2024-12-30 05:18:55 UTC",
    "updated_date": "2024-12-30 05:18:55 UTC"
  },
  {
    "arxiv_id": "2412.20694v4",
    "title": "QUBE: Enhancing Automatic Heuristic Design via Quality-Uncertainty Balanced Evolution",
    "authors": [
      "Zijie Chen",
      "Zhanchao Zhou",
      "Yu Lu",
      "Renjun Xu",
      "Lili Pan",
      "Zhenzhong Lan"
    ],
    "abstract": "Solving NP-hard problems traditionally relies on heuristics, yet manually\ndesigning effective heuristics for complex problems remains a significant\nchallenge. While recent advancements like FunSearch have shown that large\nlanguage models (LLMs) can be integrated into evolutionary algorithms (EAs) for\nheuristic design, their potential is hindered by limitations in balancing\nexploitation and exploration. We introduce Quality-Uncertainty Balanced\nEvolution (QUBE), a novel approach that enhances LLM+EA methods by redefining\nthe priority criterion within the FunSearch framework. QUBE employs the\nQuality-Uncertainty Trade-off Criterion (QUTC), based on our proposed\nUncertainty-Inclusive Quality metric, to evaluate and guide the evolutionary\nprocess. Through extensive experiments on challenging NP-complete problems,\nQUBE demonstrates significant performance improvements over FunSearch and\nbaseline methods. Our code are available at\nhttps://github.com/zzjchen/QUBE_code.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.20694v4",
    "published_date": "2024-12-30 04:05:22 UTC",
    "updated_date": "2025-02-21 03:28:49 UTC"
  },
  {
    "arxiv_id": "2501.10396v1",
    "title": "AI-Powered Urban Transportation Digital Twin: Methods and Applications",
    "authors": [
      "Xuan Di",
      "Yongjie Fu",
      "Mehmet K. Turkcan",
      "Mahshid Ghasemi",
      "Zhaobin Mo",
      "Chengbo Zang",
      "Abhishek Adhikari",
      "Zoran Kostic",
      "Gil Zussman"
    ],
    "abstract": "We present a survey paper on methods and applications of digital twins (DT)\nfor urban traffic management. While the majority of studies on the DT focus on\nits \"eyes,\" which is the emerging sensing and perception like object detection\nand tracking, what really distinguishes the DT from a traditional simulator\nlies in its ``brain,\" the prediction and decision making capabilities of\nextracting patterns and making informed decisions from what has been seen and\nperceived. In order to add values to urban transportation management, DTs need\nto be powered by artificial intelligence and complement with low-latency\nhigh-bandwidth sensing and networking technologies. We will first review the DT\npipeline leveraging cyberphysical systems and propose our DT architecture\ndeployed on a real-world testbed in New York City. This survey paper can be a\npointer to help researchers and practitioners identify challenges and\nopportunities for the development of DTs; a bridge to initiate conversations\nacross disciplines; and a road map to exploiting potentials of DTs for diverse\nurban transportation applications.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.CY",
      "cs.NI",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.10396v1",
    "published_date": "2024-12-30 02:52:19 UTC",
    "updated_date": "2024-12-30 02:52:19 UTC"
  },
  {
    "arxiv_id": "2412.20662v2",
    "title": "Enhancing Table Recognition with Vision LLMs: A Benchmark and Neighbor-Guided Toolchain Reasoner",
    "authors": [
      "Yitong Zhou",
      "Mingyue Cheng",
      "Qingyang Mao",
      "Qi Liu",
      "Feiyang Xu",
      "Xin Li",
      "Enhong Chen"
    ],
    "abstract": "Pre-trained foundation models have recently significantly progressed in\nstructured table understanding and reasoning. However, despite advancements in\nareas such as table semantic understanding and table question answering,\nrecognizing the structure and content of unstructured tables using Vision Large\nLanguage Models (VLLMs) remains under-explored. In this work, we address this\nresearch gap by employing VLLMs in a training-free reasoning paradigm. First,\nwe design a benchmark with various hierarchical dimensions relevant to table\nrecognition. Subsequently, we conduct in-depth evaluations using pre-trained\nVLLMs, finding that low-quality image input is a significant bottleneck in the\nrecognition process. Drawing inspiration from these findings, we propose the\nNeighbor-Guided Toolchain Reasoner (NGTR) framework, which is characterized by\nintegrating multiple lightweight models for low-level visual processing\noperations aimed at mitigating issues with low-quality input images.\nSpecifically, we utilize a neighbor retrieval mechanism to guide the generation\nof multiple tool invocation plans, transferring tool selection experiences from\nsimilar neighbors to the given input, thereby facilitating suitable tool\nselection. Additionally, we introduce a reflection module to supervise the tool\ninvocation process. Extensive experiments on public table recognition datasets\ndemonstrate that our approach significantly enhances the recognition\ncapabilities of the vanilla VLLMs. We believe that the designed benchmark and\nthe proposed NGTR framework could provide an alternative solution in table\nrecognition.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.20662v2",
    "published_date": "2024-12-30 02:40:19 UTC",
    "updated_date": "2025-01-03 06:22:52 UTC"
  },
  {
    "arxiv_id": "2412.20656v1",
    "title": "Overcoming Class Imbalance: Unified GNN Learning with Structural and Semantic Connectivity Representations",
    "authors": [
      "Abdullah Alchihabi",
      "Hao Yan",
      "Yuhong Guo"
    ],
    "abstract": "Class imbalance is pervasive in real-world graph datasets, where the majority\nof annotated nodes belong to a small set of classes (majority classes), leaving\nmany other classes (minority classes) with only a handful of labeled nodes.\nGraph Neural Networks (GNNs) suffer from significant performance degradation in\nthe presence of class imbalance, exhibiting bias towards majority classes and\nstruggling to generalize effectively on minority classes. This limitation\nstems, in part, from the message passing process, leading GNNs to overfit to\nthe limited neighborhood of annotated nodes from minority classes and impeding\nthe propagation of discriminative information throughout the entire graph. In\nthis paper, we introduce a novel Unified Graph Neural Network Learning\n(Uni-GNN) framework to tackle class-imbalanced node classification. The\nproposed framework seamlessly integrates both structural and semantic\nconnectivity representations through semantic and structural node encoders. By\ncombining these connectivity types, Uni-GNN extends the propagation of node\nembeddings beyond immediate neighbors, encompassing non-adjacent structural\nnodes and semantically similar nodes, enabling efficient diffusion of\ndiscriminative information throughout the graph. Moreover, to harness the\npotential of unlabeled nodes within the graph, we employ a balanced\npseudo-label generation mechanism that augments the pool of available labeled\nnodes from minority classes in the training set. Experimental results\nunderscore the superior performance of our proposed Uni-GNN framework compared\nto state-of-the-art class-imbalanced graph learning baselines across multiple\nbenchmark datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.20656v1",
    "published_date": "2024-12-30 02:20:40 UTC",
    "updated_date": "2024-12-30 02:20:40 UTC"
  },
  {
    "arxiv_id": "2501.00072v1",
    "title": "Open-Book Neural Algorithmic Reasoning",
    "authors": [
      "Hefei Li",
      "Chao Peng",
      "Chenyang Xu",
      "Zhengfeng Yang"
    ],
    "abstract": "Neural algorithmic reasoning is an emerging area of machine learning that\nfocuses on building neural networks capable of solving complex algorithmic\ntasks. Recent advancements predominantly follow the standard supervised\nlearning paradigm -- feeding an individual problem instance into the network\neach time and training it to approximate the execution steps of a classical\nalgorithm. We challenge this mode and propose a novel open-book learning\nframework. In this framework, whether during training or testing, the network\ncan access and utilize all instances in the training dataset when reasoning for\na given instance.\n  Empirical evaluation is conducted on the challenging CLRS Algorithmic\nReasoning Benchmark, which consists of 30 diverse algorithmic tasks. Our\nopen-book learning framework exhibits a significant enhancement in neural\nreasoning capabilities. Further, we notice that there is recent literature\nsuggesting that multi-task training on CLRS can improve the reasoning accuracy\nof certain tasks, implying intrinsic connections between different algorithmic\ntasks. We delve into this direction via the open-book framework. When the\nnetwork reasons for a specific task, we enable it to aggregate information from\ntraining instances of other tasks in an attention-based manner. We show that\nthis open-book attention mechanism offers insights into the inherent\nrelationships among various tasks in the benchmark and provides a robust tool\nfor interpretable multi-task training.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Appeared at NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2501.00072v1",
    "published_date": "2024-12-30 02:14:58 UTC",
    "updated_date": "2024-12-30 02:14:58 UTC"
  },
  {
    "arxiv_id": "2412.20651v2",
    "title": "Latent Drifting in Diffusion Models for Counterfactual Medical Image Synthesis",
    "authors": [
      "Yousef Yeganeh",
      "Azade Farshad",
      "Ioannis Charisiadis",
      "Marta Hasny",
      "Martin Hartenberger",
      "Björn Ommer",
      "Nassir Navab",
      "Ehsan Adeli"
    ],
    "abstract": "Scaling by training on large datasets has been shown to enhance the quality\nand fidelity of image generation and manipulation with diffusion models;\nhowever, such large datasets are not always accessible in medical imaging due\nto cost and privacy issues, which contradicts one of the main applications of\nsuch models to produce synthetic samples where real data is scarce. Also,\nfine-tuning pre-trained general models has been a challenge due to the\ndistribution shift between the medical domain and the pre-trained models. Here,\nwe propose Latent Drift (LD) for diffusion models that can be adopted for any\nfine-tuning method to mitigate the issues faced by the distribution shift or\nemployed in inference time as a condition. Latent Drifting enables diffusion\nmodels to be conditioned for medical images fitted for the complex task of\ncounterfactual image generation, which is crucial to investigate how parameters\nsuch as gender, age, and adding or removing diseases in a patient would alter\nthe medical images. We evaluate our method on three public longitudinal\nbenchmark datasets of brain MRI and chest X-rays for counterfactual image\ngeneration. Our results demonstrate significant performance gains in various\nscenarios when combined with different fine-tuning schemes.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to CVPR 2025 (highlight)",
    "pdf_url": "http://arxiv.org/pdf/2412.20651v2",
    "published_date": "2024-12-30 01:59:34 UTC",
    "updated_date": "2025-04-10 21:43:16 UTC"
  },
  {
    "arxiv_id": "2412.20638v2",
    "title": "Predicting Long Term Sequential Policy Value Using Softer Surrogates",
    "authors": [
      "Hyunji Nam",
      "Allen Nie",
      "Ge Gao",
      "Vasilis Syrgkanis",
      "Emma Brunskill"
    ],
    "abstract": "Off-policy policy evaluation (OPE) estimates the outcome of a new policy\nusing historical data collected from a different policy. However, existing OPE\nmethods cannot handle cases when the new policy introduces novel actions. This\nissue commonly occurs in real-world domains, like healthcare, as new drugs and\ntreatments are continuously developed. Novel actions necessitate on-policy data\ncollection, which can be burdensome and expensive if the outcome of interest\ntakes a substantial amount of time to observe--for example, in multi-year\nclinical trials. This raises a key question of how to predict the long-term\noutcome of a policy after only observing its short-term effects? Though in\ngeneral this problem is intractable, under some surrogacy conditions, the\nshort-term on-policy data can be combined with the long-term historical data to\nmake accurate predictions about the new policy's long-term value. In two\nsimulated healthcare examples--HIV and sepsis management--we show that our\nestimators can provide accurate predictions about the policy value only after\nobserving 10\\% of the full horizon data. We also provide finite sample analysis\nof our doubly robust estimators.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "24 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2412.20638v2",
    "published_date": "2024-12-30 01:01:15 UTC",
    "updated_date": "2025-02-03 02:11:14 UTC"
  },
  {
    "arxiv_id": "2412.20635v1",
    "title": "NetFlowGen: Leveraging Generative Pre-training for Network Traffic Dynamics",
    "authors": [
      "Jiawei Zhou",
      "Woojeong Kim",
      "Zhiying Xu",
      "Alexander M. Rush",
      "Minlan Yu"
    ],
    "abstract": "Understanding the traffic dynamics in networks is a core capability for\nautomated systems to monitor and analyze networking behaviors, reducing\nexpensive human efforts and economic risks through tasks such as traffic\nclassification, congestion prediction, and attack detection. However, it is\nstill challenging to accurately model network traffic with machine learning\napproaches in an efficient and broadly applicable manner. Task-specific models\ntrained from scratch are used for different networking applications, which\nlimits the efficiency of model development and generalization of model\ndeployment. Furthermore, while networking data is abundant, high-quality\ntask-specific labels are often insufficient for training individual models.\nLarge-scale self-supervised learning on unlabeled data provides a natural\npathway for tackling these challenges. We propose to pre-train a\ngeneral-purpose machine learning model to capture traffic dynamics with only\ntraffic data from NetFlow records, with the goal of fine-tuning for different\ndownstream tasks with small amount of labels. Our presented NetFlowGen\nframework goes beyond a proof-of-concept for network traffic pre-training and\naddresses specific challenges such as unifying network feature representations,\nlearning from large unlabeled traffic data volume, and testing on real\ndownstream tasks in DDoS attack detection. Experiments demonstrate promising\nresults of our pre-training framework on capturing traffic dynamics and\nadapting to different networking tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.20635v1",
    "published_date": "2024-12-30 00:47:49 UTC",
    "updated_date": "2024-12-30 00:47:49 UTC"
  }
]
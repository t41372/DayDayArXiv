[
  {
    "arxiv_id": "2507.13579v2",
    "title": "Learning to summarize user information for personalized reinforcement learning from human feedback",
    "authors": [
      "Hyunji Nam",
      "Yanming Wan",
      "Mickel Liu",
      "Jianxun Lian",
      "Peter Ahnn",
      "Natasha Jaques"
    ],
    "abstract": "As everyday use cases of large language model (LLM) AI assistants have expanded, it is becoming increasingly important to personalize responses to align to different users' preferences and goals. While reinforcement learning from human feedback (RLHF) is effective at improving LLMs to be generally more helpful and fluent, it does not account for variability across users, as it models the entire user population with a single reward model, meaning it assumes that everyone's preferences are the same. We present a novel framework, Preference Learning Using Summarization (PLUS), that uses reinforcement learning (RL) to learn to produce text-based summaries of each user's preferences, characteristics, and past conversations. These summaries condition the reward model, enabling it to make personalized predictions about the types of responses valued by each user. Both the user-summarization model and reward model are trained simultaneously, creating an online co-adaptation loop. We show that in contrast to the standard Bradley-Terry model, summaries produced by PLUS capture diverse aspects of user preferences, achieving a 11-77% improvement in reward model accuracy. Key strengths of PLUS are: (1) robust performance with new users and conversation topics, achieving a 25% improvement over the best personalized RLHF technique; (2) zero-shot personalization with state-of-the-art proprietary models like GPT-4 (e.g., PLUS-summary-conditioned responses achieved a 72% win rate compared to 28% for default GPT-4o); (3) learning from flexible user contexts beyond preference labels, and (4) interpretable representation of users, enabling greater transparency and user control in pluralistic LLM alignment.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages for main text, 9 pages for appendix",
    "pdf_url": "https://arxiv.org/pdf/2507.13579v2",
    "published_date": "2025-07-17 23:48:51 UTC",
    "updated_date": "2025-09-26 20:32:18 UTC"
  },
  {
    "arxiv_id": "2507.13575v3",
    "title": "Apple Intelligence Foundation Language Models: Tech Report 2025",
    "authors": [
      "Ethan Li",
      "Anders Boesen Lindbo Larsen",
      "Chen Zhang",
      "Xiyou Zhou",
      "Jun Qin",
      "Dian Ang Yap",
      "Narendran Raghavan",
      "Xuankai Chang",
      "Margit Bowler",
      "Eray Yildiz",
      "John Peebles",
      "Hannah Gillis Coleman",
      "Matteo Ronchi",
      "Peter Gray",
      "Keen You",
      "Anthony Spalvieri-Kruse",
      "Ruoming Pang",
      "Reed Li",
      "Yuli Yang",
      "Emad Soroush",
      "Zhiyun Lu",
      "Crystal Xiao",
      "Rong Situ",
      "Jordan Huffaker",
      "David Griffiths",
      "Zaid Ahmed",
      "Peng Zhang",
      "Daniel Parilla",
      "Asaf Liberman",
      "Jennifer Mallalieu",
      "Parsa Mazaheri",
      "Qibin Chen",
      "Manjot Bilkhu",
      "Aonan Zhang",
      "Eric Wang",
      "Dave Nelson",
      "Michael FitzMaurice",
      "Thomas Voice",
      "Jeremy Liu",
      "Josh Shaffer",
      "Shiwen Zhao",
      "Prasanth Yadla",
      "Farzin Rasteh",
      "Pengsheng Guo",
      "Arsalan Farooq",
      "Jeremy Snow",
      "Stephen Murphy",
      "Tao Lei",
      "Minsik Cho",
      "George Horrell",
      "Sam Dodge",
      "Lindsay Hislop",
      "Sumeet Singh",
      "Alex Dombrowski",
      "Aiswarya Raghavan",
      "Sasha Sirovica",
      "Mandana Saebi",
      "Faye Lao",
      "Max Lam",
      "TJ Lu",
      "Zhaoyang Xu",
      "Karanjeet Singh",
      "Marc Kirchner",
      "David Mizrahi",
      "Rajat Arora",
      "Haotian Zhang",
      "Henry Mason",
      "Lawrence Zhou",
      "Yi Hua",
      "Ankur Jain",
      "Felix Bai",
      "Joseph Astrauskas",
      "Floris Weers",
      "Josh Gardner",
      "Mira Chiang",
      "Yi Zhang",
      "Pulkit Agrawal",
      "Tony Sun",
      "Quentin Keunebroek",
      "Matthew Hopkins",
      "Bugu Wu",
      "Tao Jia",
      "Chen Chen",
      "Xingyu Zhou",
      "Nanzhu Wang",
      "Peng Liu",
      "Ruixuan Hou",
      "Rene Rauch",
      "Yuan Gao",
      "Afshin Dehghan",
      "Jonathan Janke",
      "Zirui Wang",
      "Cha Chen",
      "Xiaoyi Ren",
      "Feng Nan",
      "Josh Elman",
      "Dong Yin",
      "Yusuf Goren",
      "Jeff Lai",
      "Yiran Fei",
      "Syd Evans",
      "Muyang Yu",
      "Guoli Yin",
      "Yi Qin",
      "Erin Feldman",
      "Isha Garg",
      "Aparna Rajamani",
      "Karla Vega",
      "Walker Cheng",
      "TJ Collins",
      "Hans Han",
      "Raul Rea Menacho",
      "Simon Yeung",
      "Sophy Lee",
      "Phani Mutyala",
      "Ying-Chang Cheng",
      "Zhe Gan",
      "Sprite Chu",
      "Justin Lazarow",
      "Alessandro Pappalardo",
      "Federico Scozzafava",
      "Jing Lu",
      "Erik Daxberger",
      "Laurent Duchesne",
      "Jen Liu",
      "David Güera",
      "Stefano Ligas",
      "Mary Beth Kery",
      "Brent Ramerth",
      "Ciro Sannino",
      "Marcin Eichner",
      "Haoshuo Huang",
      "Rui Qian",
      "Moritz Schwarzer-Becker",
      "David Riazati",
      "Mingfei Gao",
      "Bailin Wang",
      "Jack Cackler",
      "Yang Lu",
      "Ransen Niu",
      "John Dennison",
      "Guillaume Klein",
      "Jeffrey Bigham",
      "Deepak Gopinath",
      "Navid Shiee",
      "Darren Botten",
      "Guillaume Tartavel",
      "Alex Guillen Garcia",
      "Sam Xu",
      "Victoria MönchJuan Haladjian",
      "Zi-Yi Dou",
      "Matthias Paulik",
      "Adolfo Lopez Mendez",
      "Zhen Li",
      "Hong-You Chen",
      "Chao Jia",
      "Dhaval Doshi",
      "Zhengdong Zhang",
      "Raunak Manjani",
      "Aaron Franklin",
      "Zhile Ren",
      "David Chen",
      "Artsiom Peshko",
      "Nandhitha Raghuram",
      "Hans Hao",
      "Jiulong Shan",
      "Kavya Nerella",
      "Ramsey Tantawi",
      "Vivek Kumar",
      "Saiwen Wang",
      "Brycen Wershing",
      "Bhuwan Dhingra",
      "Dhruti Shah",
      "Ob Adaranijo",
      "Xin Zheng",
      "Tait Madsen",
      "Hadas Kotek",
      "Chang Liu",
      "Yin Xia",
      "Hanli Li",
      "Suma Jayaram",
      "Yanchao Sun",
      "Ahmed Fakhry",
      "Vasileios Saveris",
      "Dustin Withers",
      "Yanghao Li",
      "Alp Aygar",
      "Andres Romero Mier Y Teran",
      "Kaiwei Huang",
      "Mark Lee",
      "Xiujun Li",
      "Yuhong Li",
      "Tyler Johnson",
      "Jay Tang",
      "Joseph Yitan Cheng",
      "Futang Peng",
      "Andrew Walkingshaw",
      "Lucas Guibert",
      "Abhishek Sharma",
      "Cheng Shen",
      "Piotr Maj",
      "Yasutaka Tanaka",
      "You-Cyuan Jhang",
      "Vivian Ma",
      "Tommi Vehvilainen",
      "Kelvin Zou",
      "Jeff Nichols",
      "Matthew Lei",
      "David Qiu",
      "Yihao Qian",
      "Gokul Santhanam",
      "Wentao Wu",
      "Yena Han",
      "Dominik Moritz",
      "Haijing Fu",
      "Mingze Xu",
      "Vivek Rathod",
      "Jian Liu",
      "Louis D'hauwe",
      "Qin Ba",
      "Haitian Sun",
      "Haoran Yan",
      "Philipp Dufter",
      "Anh Nguyen",
      "Yihao Feng",
      "Emma Wang",
      "Keyu He",
      "Rahul Nair",
      "Sanskruti Shah",
      "Jiarui Lu",
      "Patrick Sonnenberg",
      "Jeremy Warner",
      "Yuanzhi Li",
      "Bowen Pan",
      "Ziyi Zhong",
      "Joe Zhou",
      "Sam Davarnia",
      "Olli Saarikivi",
      "Irina Belousova",
      "Rachel Burger",
      "Shang-Chen Wu",
      "Di Feng",
      "Bas Straathof",
      "James Chou",
      "Yuanyang Zhang",
      "Marco Zuliani",
      "Eduardo Jimenez",
      "Abhishek Sundararajan",
      "Xianzhi Du",
      "Chang Lan",
      "Nilesh Shahdadpuri",
      "Peter Grasch",
      "Sergiu Sima",
      "Josh Newnham",
      "Varsha Paidi",
      "Jianyu Wang",
      "Kaelen Haag",
      "Alex Braunstein",
      "Daniele Molinari",
      "Richard Wei",
      "Brenda Yang",
      "Nicholas Lusskin",
      "Joanna Arreaza-Taylor",
      "Meng Cao",
      "Nicholas Seidl",
      "Simon Wang",
      "Jiaming Hu",
      "Yiping Ma",
      "Mengyu Li",
      "Kieran Liu",
      "Hang Su",
      "Sachin Ravi",
      "Chong Wang",
      "Xin Wang",
      "Kevin Smith",
      "Haoxuan You",
      "Binazir Karimzadeh",
      "Rui Li",
      "Jinhao Lei",
      "Wei Fang",
      "Alec Doane",
      "Sam Wiseman",
      "Ismael Fernandez",
      "Jane Li",
      "Andrew Hansen",
      "Javier Movellan",
      "Christopher Neubauer",
      "Hanzhi Zhou",
      "Chris Chaney",
      "Nazir Kamaldin",
      "Valentin Wolf",
      "Fernando Bermúdez-Medina",
      "Joris Pelemans",
      "Peter Fu",
      "Howard Xing",
      "Xiang Kong",
      "Wayne Shan",
      "Gabriel Jacoby-Cooper",
      "Dongcai Shen",
      "Tom Gunter",
      "Guillaume Seguin",
      "Fangping Shi",
      "Shiyu Li",
      "Yang Xu",
      "Areeba Kamal",
      "Dan Masi",
      "Saptarshi Guha",
      "Qi Zhu",
      "Jenna Thibodeau",
      "Changyuan Zhang",
      "Rebecca Callahan",
      "Charles Maalouf",
      "Wilson Tsao",
      "Boyue Li",
      "Qingqing Cao",
      "Naomy Sabo",
      "Cheng Leong",
      "Yi Wang",
      "Anupama Mann Anupama",
      "Colorado Reed",
      "Kenneth Jung",
      "Zhifeng Chen",
      "Mohana Prasad Sathya Moorthy",
      "Yifei He",
      "Erik Hornberger",
      "Devi Krishna",
      "Senyu Tong",
      "Michael",
      "Lee",
      "David Haldimann",
      "Yang Zhao",
      "Bowen Zhang",
      "Chang Gao",
      "Chris Bartels",
      "Sushma Rao",
      "Nathalie Tran",
      "Simon Lehnerer",
      "Co Giang",
      "Patrick Dong",
      "Junting Pan",
      "Biyao Wang",
      "Dongxu Li",
      "Mehrdad Farajtabar",
      "Dongseong Hwang",
      "Grace Duanmu",
      "Eshan Verma",
      "Sujeeth Reddy",
      "Qi Shan",
      "Hongbin Gao",
      "Nan Du",
      "Pragnya Sridhar",
      "Forrest Huang",
      "Yingbo Wang",
      "Nikhil Bhendawade",
      "Diane Zhu",
      "Sai Aitharaju",
      "Fred Hohman",
      "Lauren Gardiner",
      "Chung-Cheng Chiu",
      "Yinfei Yang",
      "Alper Kokmen",
      "Frank Chu",
      "Ke Ye",
      "Kaan Elgin",
      "Oron Levy",
      "John Park",
      "Donald Zhang",
      "Eldon Schoop",
      "Nina Wenzel",
      "Michael Booker",
      "Hyunjik Kim",
      "Chinguun Erdenebileg",
      "Nan Dun",
      "Eric Liang Yang",
      "Priyal Chhatrapati",
      "Vishaal Mahtani",
      "Haiming Gang",
      "Kohen Chia",
      "Deepa Seshadri",
      "Donghan Yu",
      "Yan Meng",
      "Kelsey Peterson",
      "Zhen Yang",
      "Yongqiang Wang",
      "Carina Peng",
      "Doug Kang",
      "Anuva Agarwal",
      "Albert Antony",
      "Juan Lao Tebar",
      "Albin Madappally Jose",
      "Regan Poston",
      "Andy De Wang",
      "Gerard Casamayor",
      "Elmira Amirloo",
      "Violet Yao",
      "Wojciech Kryscinski",
      "Kun Duan",
      "Lezhi L"
    ],
    "abstract": "We introduce two multilingual, multimodal foundation language models that power Apple Intelligence features across Apple devices and services: i a 3B-parameter on-device model optimized for Apple silicon through architectural innovations such as KV-cache sharing and 2-bit quantization-aware training; and ii a scalable server model built on a novel Parallel-Track Mixture-of-Experts PT-MoE transformer that combines track parallelism, mixture-of-experts sparse computation, and interleaved global-local attention to deliver high quality with competitive cost on Apple's Private Cloud Compute platform. Both models are trained on large-scale multilingual and multimodal datasets sourced via responsible web crawling, licensed corpora, and high-quality synthetic data, then further refined with supervised fine-tuning and reinforcement learning on a new asynchronous platform. The resulting models support several additional languages while understanding images and executing tool calls. In public benchmarks and human evaluations, both the server model and the on-device model match or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation, constrained tool calling, and LoRA adapter fine-tuning, allowing developers to integrate these capabilities with a few lines of code. The latest advancements in Apple Intelligence models are grounded in our Responsible AI approach with safeguards like content filtering and locale-specific evaluation, as well as our commitment to protecting our users' privacy with innovations like Private Cloud Compute.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.13575v3",
    "published_date": "2025-07-17 23:37:19 UTC",
    "updated_date": "2025-08-27 16:34:47 UTC"
  },
  {
    "arxiv_id": "2507.13569v1",
    "title": "Change of Thought: Adaptive Test-Time Computation",
    "authors": [
      "Mrinal Mathur",
      "Mike Doan",
      "Barak Pearlmutter",
      "Sergey Plis"
    ],
    "abstract": "Transformers evaluated in a single, fixed-depth pass are provably limited in expressive power to the constant-depth circuit class TC0. Running a Transformer autoregressively removes that ceiling -- first in next-token prediction and, more recently, in chain-of-thought reasoning. Both regimes rely on feedback loops that decode internal states into tokens only to re-encode them in subsequent steps. While this \"thinking aloud\" mirrors human reasoning, biological brains iterate without externalising intermediate states as language. To boost the expressive power of encoder Transformers without resorting to token-level autoregression, we introduce the SELF-Transformer: an encoder layer that iteratively refines its own attention weights to a fixed point. Instead of producing -- in one pass -- the alignment matrix that remixes the input sequence, the SELF-Transformer iteratively updates that matrix internally, scaling test-time computation with input difficulty. This adaptivity yields up to 20\\% accuracy gains on encoder-style benchmarks without increasing parameter count, demonstrating that input-adaptive alignment at test time offers substantial benefits for only a modest extra compute budget. Self-Transformers thus recover much of the expressive power of iterative reasoning while preserving the simplicity of pure encoder architectures.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.13569v1",
    "published_date": "2025-07-17 23:12:57 UTC",
    "updated_date": "2025-07-17 23:12:57 UTC"
  },
  {
    "arxiv_id": "2507.19513v1",
    "title": "Enhancing Spatiotemporal Networks with xLSTM: A Scalar LSTM Approach for Cellular Traffic Forecasting",
    "authors": [
      "Khalid Ali",
      "Zineddine Bettouche",
      "Andreas Kassler",
      "Andreas Fischer"
    ],
    "abstract": "Accurate spatiotemporal traffic forecasting is vital for intelligent resource management in 5G and beyond. However, conventional AI approaches often fail to capture the intricate spatial and temporal patterns that exist, due to e.g., the mobility of users. We introduce a lightweight, dual-path Spatiotemporal Network that leverages a Scalar LSTM (sLSTM) for efficient temporal modeling and a three-layer Conv3D module for spatial feature extraction. A fusion layer integrates both streams into a cohesive representation, enabling robust forecasting. Our design improves gradient stability and convergence speed while reducing prediction error. Evaluations on real-world datasets show superior forecast performance over ConvLSTM baselines and strong generalization to unseen regions, making it well-suited for large-scale, next-generation network deployments. Experimental evaluation shows a 23% MAE reduction over ConvLSTM, with a 30% improvement in model generalization.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.19513v1",
    "published_date": "2025-07-17 22:48:46 UTC",
    "updated_date": "2025-07-17 22:48:46 UTC"
  },
  {
    "arxiv_id": "2507.13558v5",
    "title": "Why Isn't Relational Learning Taking Over the World?",
    "authors": [
      "David Poole"
    ],
    "abstract": "Artificial intelligence seems to be taking over the world with systems that model pixels, words, and phonemes. The world is arguably made up, not of pixels, words, and phonemes but of entities (objects, things, including events) with properties and relations among them. Surely we should model these, not the perception or description of them. You might suspect that concentrating on modeling words and pixels is because all of the (valuable) data in the world is in terms of text and images. If you look into almost any company you will find their most valuable data is in spreadsheets, databases and other relational formats. These are not the form that are studied in introductory machine learning, but are full of product numbers, student numbers, transaction numbers and other identifiers that can't be interpreted naively as numbers. The field that studies this sort of data has various names including relational learning, statistical relational AI, and many others. This paper explains why relational learning is not taking over the world -- except in a few cases with restricted relations -- and what needs to be done to bring it to it's rightful prominence.",
    "categories": [
      "cs.AI",
      "cs.DB",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages (6 pages + references + appendices). To appear AAAI-2026",
    "pdf_url": "https://arxiv.org/pdf/2507.13558v5",
    "published_date": "2025-07-17 22:32:07 UTC",
    "updated_date": "2025-11-05 17:21:58 UTC"
  },
  {
    "arxiv_id": "2507.13556v1",
    "title": "Time Series Forecastability Measures",
    "authors": [
      "Rui Wang",
      "Steven Klee",
      "Alexis Roos"
    ],
    "abstract": "This paper proposes using two metrics to quantify the forecastability of time series prior to model development: the spectral predictability score and the largest Lyapunov exponent. Unlike traditional model evaluation metrics, these measures assess the inherent forecastability characteristics of the data before any forecast attempts. The spectral predictability score evaluates the strength and regularity of frequency components in the time series, whereas the Lyapunov exponents quantify the chaos and stability of the system generating the data. We evaluated the effectiveness of these metrics on both synthetic and real-world time series from the M5 forecast competition dataset. Our results demonstrate that these two metrics can correctly reflect the inherent forecastability of a time series and have a strong correlation with the actual forecast performance of various models. By understanding the inherent forecastability of time series before model training, practitioners can focus their planning efforts on products and supply chain levels that are more forecastable, while setting appropriate expectations or seeking alternative strategies for products with limited forecastability.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.13556v1",
    "published_date": "2025-07-17 22:23:51 UTC",
    "updated_date": "2025-07-17 22:23:51 UTC"
  },
  {
    "arxiv_id": "2507.13551v1",
    "title": "Reading Between the Lines: Combining Pause Dynamics and Semantic Coherence for Automated Assessment of Thought Disorder",
    "authors": [
      "Feng Chen",
      "Weizhe Xu",
      "Changye Li",
      "Serguei Pakhomov",
      "Alex Cohen",
      "Simran Bhola",
      "Sandy Yin",
      "Sunny X Tang",
      "Michael Mackinley",
      "Lena Palaniyappan",
      "Dror Ben-Zeev",
      "Trevor Cohen"
    ],
    "abstract": "Formal thought disorder (FTD), a hallmark of schizophrenia spectrum disorders, manifests as incoherent speech and poses challenges for clinical assessment. Traditional clinical rating scales, though validated, are resource-intensive and lack scalability. Automated speech analysis with automatic speech recognition (ASR) allows for objective quantification of linguistic and temporal features of speech, offering scalable alternatives. The use of utterance timestamps in ASR captures pause dynamics, which are thought to reflect the cognitive processes underlying speech production. However, the utility of integrating these ASR-derived features for assessing FTD severity requires further evaluation. This study integrates pause features with semantic coherence metrics across three datasets: naturalistic self-recorded diaries (AVH, n = 140), structured picture descriptions (TOPSY, n = 72), and dream narratives (PsyCL, n = 43). We evaluated pause related features alongside established coherence measures, using support vector regression (SVR) to predict clinical FTD scores. Key findings demonstrate that pause features alone robustly predict the severity of FTD. Integrating pause features with semantic coherence metrics enhanced predictive performance compared to semantic-only models, with integration of independent models achieving correlations up to \\r{ho} = 0.649 and AUC = 83.71% for severe cases detection (TOPSY, with best \\r{ho} = 0.584 and AUC = 79.23% for semantic-only models). The performance gains from semantic and pause features integration held consistently across all contexts, though the nature of pause patterns was dataset-dependent. These findings suggest that frameworks combining temporal and semantic analyses provide a roadmap for refining the assessment of disorganized speech and advance automated speech analysis in psychosis.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.13551v1",
    "published_date": "2025-07-17 22:00:16 UTC",
    "updated_date": "2025-07-17 22:00:16 UTC"
  },
  {
    "arxiv_id": "2507.13550v1",
    "title": "GOFAI meets Generative AI: Development of Expert Systems by means of Large Language Models",
    "authors": [
      "Eduardo C. Garrido-Merchán",
      "Cristina Puente"
    ],
    "abstract": "The development of large language models (LLMs) has successfully transformed knowledge-based systems such as open domain question nswering, which can automatically produce vast amounts of seemingly coherent information. Yet, those models have several disadvantages like hallucinations or confident generation of incorrect or unverifiable facts. In this paper, we introduce a new approach to the development of expert systems using LLMs in a controlled and transparent way. By limiting the domain and employing a well-structured prompt-based extraction approach, we produce a symbolic representation of knowledge in Prolog, which can be validated and corrected by human experts. This approach also guarantees interpretability, scalability and reliability of the developed expert systems. Via quantitative and qualitative experiments with Claude Sonnet 3.7 and GPT-4.1, we show strong adherence to facts and semantic coherence on our generated knowledge bases. We present a transparent hybrid solution that combines the recall capacity of LLMs with the precision of symbolic systems, thereby laying the foundation for dependable AI applications in sensitive domains.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.SC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.13550v1",
    "published_date": "2025-07-17 21:57:37 UTC",
    "updated_date": "2025-07-17 21:57:37 UTC"
  },
  {
    "arxiv_id": "2507.13543v4",
    "title": "Loss-Complexity Landscape and Model Structure Functions",
    "authors": [
      "Alexander Kolpakov"
    ],
    "abstract": "We develop a framework for dualizing the Kolmogorov structure function $h_x(α)$, which then allows using computable complexity proxies. We establish a mathematical analogy between information-theoretic constructs and statistical mechanics, introducing a suitable partition function and free energy functional. We explicitly prove the Legendre-Fenchel duality between the structure function and free energy, showing detailed balance of the Metropolis kernel, and interpret acceptance probabilities as information-theoretic scattering amplitudes. A susceptibility-like variance of model complexity is shown to peak precisely at loss-complexity trade-offs interpreted as phase transitions. Practical experiments with linear and tree-based regression models verify these theoretical predictions, explicitly demonstrating the interplay between the model complexity, generalization, and overfitting threshold.",
    "categories": [
      "cs.IT",
      "cs.AI",
      "cs.LG",
      "math-ph"
    ],
    "primary_category": "cs.IT",
    "comment": "25 pages, 11 figures; GitHub repository at https://github.com/sashakolpakov/structure-functions",
    "pdf_url": "https://arxiv.org/pdf/2507.13543v4",
    "published_date": "2025-07-17 21:31:45 UTC",
    "updated_date": "2025-10-16 22:13:58 UTC"
  },
  {
    "arxiv_id": "2507.13542v1",
    "title": "Acoustic Index: A Novel AI-Driven Parameter for Cardiac Disease Risk Stratification Using Echocardiography",
    "authors": [
      "Beka Begiashvili",
      "Carlos J. Fernandez-Candel",
      "Matías Pérez Paredes"
    ],
    "abstract": "Traditional echocardiographic parameters such as ejection fraction (EF) and global longitudinal strain (GLS) have limitations in the early detection of cardiac dysfunction. EF often remains normal despite underlying pathology, and GLS is influenced by load conditions and vendor variability. There is a growing need for reproducible, interpretable, and operator-independent parameters that capture subtle and global cardiac functional alterations.\n  We introduce the Acoustic Index, a novel AI-derived echocardiographic parameter designed to quantify cardiac dysfunction from standard ultrasound views. The model combines Extended Dynamic Mode Decomposition (EDMD) based on Koopman operator theory with a hybrid neural network that incorporates clinical metadata. Spatiotemporal dynamics are extracted from echocardiographic sequences to identify coherent motion patterns. These are weighted via attention mechanisms and fused with clinical data using manifold learning, resulting in a continuous score from 0 (low risk) to 1 (high risk).\n  In a prospective cohort of 736 patients, encompassing various cardiac pathologies and normal controls, the Acoustic Index achieved an area under the curve (AUC) of 0.89 in an independent test set. Cross-validation across five folds confirmed the robustness of the model, showing that both sensitivity and specificity exceeded 0.8 when evaluated on independent data. Threshold-based analysis demonstrated stable trade-offs between sensitivity and specificity, with optimal discrimination near this threshold.\n  The Acoustic Index represents a physics-informed, interpretable AI biomarker for cardiac function. It shows promise as a scalable, vendor-independent tool for early detection, triage, and longitudinal monitoring. Future directions include external validation, longitudinal studies, and adaptation to disease-specific classifiers.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.13542v1",
    "published_date": "2025-07-17 21:27:28 UTC",
    "updated_date": "2025-07-17 21:27:28 UTC"
  },
  {
    "arxiv_id": "2507.13541v1",
    "title": "PrefPalette: Personalized Preference Modeling with Latent Attributes",
    "authors": [
      "Shuyue Stella Li",
      "Melanie Sclar",
      "Hunter Lang",
      "Ansong Ni",
      "Jacqueline He",
      "Puxin Xu",
      "Andrew Cohen",
      "Chan Young Park",
      "Yulia Tsvetkov",
      "Asli Celikyilmaz"
    ],
    "abstract": "Personalizing AI systems requires understanding not just what users prefer, but the reasons that underlie those preferences - yet current preference models typically treat human judgment as a black box. We introduce PrefPalette, a framework that decomposes preferences into attribute dimensions and tailors its preference prediction to distinct social community values in a human-interpretable manner. PrefPalette operationalizes a cognitive science principle known as multi-attribute decision making in two ways: (1) a scalable counterfactual attribute synthesis step that involves generating synthetic training data to isolate for individual attribute effects (e.g., formality, humor, cultural values), and (2) attention-based preference modeling that learns how different social communities dynamically weight these attributes. This approach moves beyond aggregate preference modeling to capture the diverse evaluation frameworks that drive human judgment. When evaluated on 45 social communities from the online platform Reddit, PrefPalette outperforms GPT-4o by 46.6% in average prediction accuracy. Beyond raw predictive improvements, PrefPalette also shed light on intuitive, community-specific profiles: scholarly communities prioritize verbosity and stimulation, conflict-oriented communities value sarcasm and directness, and support-based communities emphasize empathy. By modeling the attribute-mediated structure of human judgment, PrefPalette delivers both superior preference modeling and transparent, interpretable insights, and serves as a first step toward more trustworthy, value-aware personalized applications.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "17 pages, 6 tables, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.13541v1",
    "published_date": "2025-07-17 21:21:54 UTC",
    "updated_date": "2025-07-17 21:21:54 UTC"
  },
  {
    "arxiv_id": "2507.14242v1",
    "title": "Culling Misinformation from Gen AI: Toward Ethical Curation and Refinement",
    "authors": [
      "Prerana Khatiwada",
      "Grace Donaher",
      "Jasymyn Navarro",
      "Lokesh Bhatta"
    ],
    "abstract": "While Artificial Intelligence (AI) is not a new field, recent developments, especially with the release of generative tools like ChatGPT, have brought it to the forefront of the minds of industry workers and academic folk alike. There is currently much talk about AI and its ability to reshape many everyday processes as we know them through automation. It also allows users to expand their ideas by suggesting things they may not have thought of on their own and provides easier access to information. However, not all of the changes this technology will bring or has brought so far are positive; this is why it is extremely important for all modern people to recognize and understand the risks before using these tools and allowing them to cause harm. This work takes a position on better understanding many equity concerns and the spread of misinformation that result from new AI, in this case, specifically ChatGPT and deepfakes, and encouraging collaboration with law enforcement, developers, and users to reduce harm. Considering many academic sources, it warns against these issues, analyzing their cause and impact in fields including healthcare, education, science, academia, retail, and finance. Lastly, we propose a set of future-facing guidelines and policy considerations to solve these issues while still enabling innovation in these fields, this responsibility falling upon users, developers, and government entities.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "7 pages",
    "pdf_url": "https://arxiv.org/pdf/2507.14242v1",
    "published_date": "2025-07-17 21:19:47 UTC",
    "updated_date": "2025-07-17 21:19:47 UTC"
  },
  {
    "arxiv_id": "2507.15878v1",
    "title": "Salience Adjustment for Context-Based Emotion Recognition",
    "authors": [
      "Bin Han",
      "Jonathan Gratch"
    ],
    "abstract": "Emotion recognition in dynamic social contexts requires an understanding of the complex interaction between facial expressions and situational cues. This paper presents a salience-adjusted framework for context-aware emotion recognition with Bayesian Cue Integration (BCI) and Visual-Language Models (VLMs) to dynamically weight facial and contextual information based on the expressivity of facial cues. We evaluate this approach using human annotations and automatic emotion recognition systems in prisoner's dilemma scenarios, which are designed to evoke emotional reactions. Our findings demonstrate that incorporating salience adjustment enhances emotion recognition performance, offering promising directions for future research to extend this framework to broader social contexts and multimodal applications.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.15878v1",
    "published_date": "2025-07-17 20:55:20 UTC",
    "updated_date": "2025-07-17 20:55:20 UTC"
  },
  {
    "arxiv_id": "2507.13524v1",
    "title": "Humans learn to prefer trustworthy AI over human partners",
    "authors": [
      "Yaomin Jiang",
      "Levin Brinkmann",
      "Anne-Marie Nussberger",
      "Ivan Soraperra",
      "Jean-François Bonnefon",
      "Iyad Rahwan"
    ],
    "abstract": "Partner selection is crucial for cooperation and hinges on communication. As artificial agents, especially those powered by large language models (LLMs), become more autonomous, intelligent, and persuasive, they compete with humans for partnerships. Yet little is known about how humans select between human and AI partners and adapt under AI-induced competition pressure. We constructed a communication-based partner selection game and examined the dynamics in hybrid mini-societies of humans and bots powered by a state-of-the-art LLM. Through three experiments (N = 975), we found that bots, though more prosocial than humans and linguistically distinguishable, were not selected preferentially when their identity was hidden. Instead, humans misattributed bots' behaviour to humans and vice versa. Disclosing bots' identity induced a dual effect: it reduced bots' initial chances of being selected but allowed them to gradually outcompete humans by facilitating human learning about the behaviour of each partner type. These findings show how AI can reshape social interaction in mixed societies and inform the design of more effective and cooperative hybrid systems.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.13524v1",
    "published_date": "2025-07-17 20:24:26 UTC",
    "updated_date": "2025-07-17 20:24:26 UTC"
  },
  {
    "arxiv_id": "2507.15877v2",
    "title": "Out-of-Distribution Generalization in the ARC-AGI Domain: Comparing Execution-Guided Neural Program Synthesis and Test-Time Fine-Tuning",
    "authors": [
      "Simon Ouellette"
    ],
    "abstract": "We run a controlled compositional generalization experiment in the ARC-AGI domain: an open-world problem domain in which the ability to generalize out-of-distribution is, by design, an essential characteristic for success. We compare neural program synthesis and test-time fine-tuning approaches on this experiment. We find that execution-guided neural program synthesis outperforms all reference algorithms in its ability to compose novel solutions. Our empirical findings also suggest that the success of TTFT on ARC-AGI lies mainly in eliciting in-distribution knowledge that the LLM otherwise fails to rely on directly.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "this version fixes errors in AlphaEvolve total % calculation, Table 3 DSL description, and adds clarifications in response to review criticisms",
    "pdf_url": "https://arxiv.org/pdf/2507.15877v2",
    "published_date": "2025-07-17 20:12:01 UTC",
    "updated_date": "2025-09-21 14:56:12 UTC"
  },
  {
    "arxiv_id": "2507.13511v1",
    "title": "GraphTrafficGPT: Enhancing Traffic Management Through Graph-Based AI Agent Coordination",
    "authors": [
      "Nabil Abdelaziz Ferhat Taleb",
      "Abdolazim Rezaei",
      "Raj Atulkumar Patel",
      "Mehdi Sookhak"
    ],
    "abstract": "Large Language Models (LLMs) offer significant promise for intelligent traffic management; however, current chain-based systems like TrafficGPT are hindered by sequential task execution, high token usage, and poor scalability, making them inefficient for complex, real-world scenarios. To address these limitations, we propose GraphTrafficGPT, a novel graph-based architecture, which fundamentally redesigns the task coordination process for LLM-driven traffic applications. GraphTrafficGPT represents tasks and their dependencies as nodes and edges in a directed graph, enabling efficient parallel execution and dynamic resource allocation. The main idea behind the proposed model is a Brain Agent that decomposes user queries, constructs optimized dependency graphs, and coordinates a network of specialized agents for data retrieval, analysis, visualization, and simulation. By introducing advanced context-aware token management and supporting concurrent multi-query processing, the proposed architecture handles interdependent tasks typical of modern urban mobility environments. Experimental results demonstrate that GraphTrafficGPT reduces token consumption by 50.2% and average response latency by 19.0% compared to TrafficGPT, while supporting simultaneous multi-query execution with up to 23.0% improvement in efficiency.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.13511v1",
    "published_date": "2025-07-17 19:41:09 UTC",
    "updated_date": "2025-07-17 19:41:09 UTC"
  },
  {
    "arxiv_id": "2507.13505v1",
    "title": "PHASE: Passive Human Activity Simulation Evaluation",
    "authors": [
      "Steven Lamp",
      "Jason D. Hiser",
      "Anh Nguyen-Tuong",
      "Jack W. Davidson"
    ],
    "abstract": "Cybersecurity simulation environments, such as cyber ranges, honeypots, and sandboxes, require realistic human behavior to be effective, yet no quantitative method exists to assess the behavioral fidelity of synthetic user personas. This paper presents PHASE (Passive Human Activity Simulation Evaluation), a machine learning framework that analyzes Zeek connection logs and distinguishes human from non-human activity with over 90\\% accuracy. PHASE operates entirely passively, relying on standard network monitoring without any user-side instrumentation or visible signs of surveillance. All network activity used for machine learning is collected via a Zeek network appliance to avoid introducing unnecessary network traffic or artifacts that could disrupt the fidelity of the simulation environment. The paper also proposes a novel labeling approach that utilizes local DNS records to classify network traffic, thereby enabling machine learning analysis. Furthermore, we apply SHAP (SHapley Additive exPlanations) analysis to uncover temporal and behavioral signatures indicative of genuine human users. In a case study, we evaluate a synthetic user persona and identify distinct non-human patterns that undermine behavioral realism. Based on these insights, we develop a revised behavioral configuration that significantly improves the human-likeness of synthetic activity yielding a more realistic and effective synthetic user persona.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "cs.NI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.13505v1",
    "published_date": "2025-07-17 19:24:11 UTC",
    "updated_date": "2025-07-17 19:24:11 UTC"
  },
  {
    "arxiv_id": "2507.13499v1",
    "title": "AI-Assisted Fixes to Code Review Comments at Scale",
    "authors": [
      "Chandra Maddila",
      "Negar Ghorbani",
      "James Saindon",
      "Parth Thakkar",
      "Vijayaraghavan Murali",
      "Rui Abreu",
      "Jingyue Shen",
      "Brian Zhou",
      "Nachiappan Nagappan",
      "Peter C. Rigby"
    ],
    "abstract": "Aim. There are 10s of thousands of code review comments each week at Meta. We developed Metamate for Code Review (MetaMateCR) that provides AI-assisted fixes for reviewer comments in production at scale.\n  Method. We developed an internal benchmark of 64k <review comment, patch> data points to fine-tune Llama models. Once our models achieve reasonable offline results, we roll them into production. To ensure that our AI-assisted fixes do not negatively impact the time it takes to do code reviews, we conduct randomized controlled safety trials as well as full production experiments.\n  Offline Results. As a baseline, we compare GPT-4o to our small and large Llama models. In offline results, our LargeLSFT model creates an exact match patch 68% of the time outperforming GPT-4o by 9 percentage points (pp). The internal models also use more modern Hack functions when compared to the PHP functions suggested by GPT-4o.\n  Safety Trial. When we roll MetaMateCR into production in a safety trial that compares no AI patches with AI patch suggestions, we see a large regression with reviewers taking over 5% longer to conduct reviews. After investigation, we modify the UX to only show authors the AI patches, and see no regressions in the time for reviews.\n  Production. When we roll LargeLSFT into production, we see an ActionableToApplied rate of 19.7%, which is a 9.2pp improvement over GPT-4o. Our results illustrate the importance of safety trials in ensuring that AI does not inadvertently slow down engineers, and a successful review comment to AI patch product running at scale.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.PL"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.13499v1",
    "published_date": "2025-07-17 19:11:00 UTC",
    "updated_date": "2025-07-17 19:11:00 UTC"
  },
  {
    "arxiv_id": "2507.13485v1",
    "title": "Neural Architecture Search with Mixed Bio-inspired Learning Rules",
    "authors": [
      "Imane Hamzaoui",
      "Riyadh Baghdadi"
    ],
    "abstract": "Bio-inspired neural networks are attractive for their adversarial robustness, energy frugality, and closer alignment with cortical physiology, yet they often lag behind back-propagation (BP) based models in accuracy and ability to scale. We show that allowing the use of different bio-inspired learning rules in different layers, discovered automatically by a tailored neural-architecture-search (NAS) procedure, bridges this gap. Starting from standard NAS baselines, we enlarge the search space to include bio-inspired learning rules and use NAS to find the best architecture and learning rule to use in each layer. We show that neural networks that use different bio-inspired learning rules for different layers have better accuracy than those that use a single rule across all the layers. The resulting NN that uses a mix of bio-inspired learning rules sets new records for bio-inspired models: 95.16% on CIFAR-10, 76.48% on CIFAR-100, 43.42% on ImageNet16-120, and 60.51% top-1 on ImageNet. In some regimes, they even surpass comparable BP-based networks while retaining their robustness advantages. Our results suggest that layer-wise diversity in learning rules allows better scalability and accuracy, and motivates further research on mixing multiple bio-inspired learning rules in the same network.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "ECAI 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.13485v1",
    "published_date": "2025-07-17 18:49:38 UTC",
    "updated_date": "2025-07-17 18:49:38 UTC"
  },
  {
    "arxiv_id": "2507.13468v2",
    "title": "ERR@HRI 2.0 Challenge: Multimodal Detection of Errors and Failures in Human-Robot Conversations",
    "authors": [
      "Shiye Cao",
      "Maia Stiber",
      "Amama Mahmood",
      "Maria Teresa Parreira",
      "Wendy Ju",
      "Micol Spitale",
      "Hatice Gunes",
      "Chien-Ming Huang"
    ],
    "abstract": "The integration of large language models (LLMs) into conversational robots has made human-robot conversations more dynamic. Yet, LLM-powered conversational robots remain prone to errors, e.g., misunderstanding user intent, prematurely interrupting users, or failing to respond altogether. Detecting and addressing these failures is critical for preventing conversational breakdowns, avoiding task disruptions, and sustaining user trust. To tackle this problem, the ERR@HRI 2.0 Challenge provides a multimodal dataset of LLM-powered conversational robot failures during human-robot conversations and encourages researchers to benchmark machine learning models designed to detect robot failures. The dataset includes 16 hours of dyadic human-robot interactions, incorporating facial, speech, and head movement features. Each interaction is annotated with the presence or absence of robot errors from the system perspective, and perceived user intention to correct for a mismatch between robot behavior and user expectation. Participants are invited to form teams and develop machine learning models that detect these failures using multimodal data. Submissions will be evaluated using various performance metrics, including detection accuracy and false positive rate. This challenge represents another key step toward improving failure detection in human-robot interaction through social signal analysis.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.13468v2",
    "published_date": "2025-07-17 18:21:45 UTC",
    "updated_date": "2025-10-09 15:54:27 UTC"
  },
  {
    "arxiv_id": "2507.14241v3",
    "title": "Promptomatix: An Automatic Prompt Optimization Framework for Large Language Models",
    "authors": [
      "Rithesh Murthy",
      "Ming Zhu",
      "Liangwei Yang",
      "Jielin Qiu",
      "Juntao Tan",
      "Shelby Heinecke",
      "Caiming Xiong",
      "Silvio Savarese",
      "Huan Wang"
    ],
    "abstract": "Large Language Models (LLMs) perform best with well-crafted prompts, yet prompt engineering remains manual, inconsistent, and inaccessible to non-experts. We introduce Promptomatix, an automatic prompt optimization framework that transforms natural language task descriptions into high-quality prompts without requiring manual tuning or domain expertise. Promptomatix supports both a lightweight meta-prompt-based optimizer and a DSPy-powered compiler, with modular design enabling future extension to more advanced frameworks. The system analyzes user intent, generates synthetic training data, selects prompting strategies, and refines prompts using cost-aware objectives. Evaluated across 5 task categories, Promptomatix achieves competitive or superior performance compared to existing libraries, while reducing prompt length and computational overhead making prompt optimization scalable and efficient.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.14241v3",
    "published_date": "2025-07-17 18:18:20 UTC",
    "updated_date": "2025-07-24 21:49:26 UTC"
  },
  {
    "arxiv_id": "2507.13459v1",
    "title": "Graph Neural Network Surrogates for Contacting Deformable Bodies with Necessary and Sufficient Contact Detection",
    "authors": [
      "Vijay K. Dubey",
      "Collin E. Haese",
      "Osman Gültekin",
      "David Dalton",
      "Manuel K. Rausch",
      "Jan N. Fuhg"
    ],
    "abstract": "Surrogate models for the rapid inference of nonlinear boundary value problems in mechanics are helpful in a broad range of engineering applications. However, effective surrogate modeling of applications involving the contact of deformable bodies, especially in the context of varying geometries, is still an open issue. In particular, existing methods are confined to rigid body contact or, at best, contact between rigid and soft objects with well-defined contact planes. Furthermore, they employ contact or collision detection filters that serve as a rapid test but use only the necessary and not sufficient conditions for detection. In this work, we present a graph neural network architecture that utilizes continuous collision detection and, for the first time, incorporates sufficient conditions designed for contact between soft deformable bodies. We test its performance on two benchmarks, including a problem in soft tissue mechanics of predicting the closed state of a bioprosthetic aortic valve. We find a regularizing effect on adding additional contact terms to the loss function, leading to better generalization of the network. These benefits hold for simple contact at similar planes and element normal angles, and complex contact at differing planes and element normal angles. We also demonstrate that the framework can handle varying reference geometries. However, such benefits come with high computational costs during training, resulting in a trade-off that may not always be favorable. We quantify the training cost and the resulting inference speedups on various hardware architectures. Importantly, our graph neural network implementation results in up to a thousand-fold speedup for our benchmark problems at inference.",
    "categories": [
      "cs.CE",
      "cs.AI",
      "cs.LG",
      "math.NA"
    ],
    "primary_category": "cs.CE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.13459v1",
    "published_date": "2025-07-17 18:09:19 UTC",
    "updated_date": "2025-07-17 18:09:19 UTC"
  },
  {
    "arxiv_id": "2507.13353v1",
    "title": "VideoITG: Multimodal Video Understanding with Instructed Temporal Grounding",
    "authors": [
      "Shihao Wang",
      "Guo Chen",
      "De-an Huang",
      "Zhiqi Li",
      "Minghan Li",
      "Guilin Li",
      "Jose M. Alvarez",
      "Lei Zhang",
      "Zhiding Yu"
    ],
    "abstract": "Recent studies have revealed that selecting informative and relevant video frames can significantly improve the performance of Video Large Language Models (Video-LLMs). Current methods, such as reducing inter-frame redundancy, employing separate models for image-text relevance assessment, or utilizing temporal video grounding for event localization, substantially adopt unsupervised learning paradigms, whereas they struggle to address the complex scenarios in long video understanding. We propose Instructed Temporal Grounding for Videos (VideoITG), featuring customized frame sampling aligned with user instructions. The core of VideoITG is the VidThinker pipeline, an automated annotation framework that explicitly mimics the human annotation process. First, it generates detailed clip-level captions conditioned on the instruction; then, it retrieves relevant video segments through instruction-guided reasoning; finally, it performs fine-grained frame selection to pinpoint the most informative visual evidence. Leveraging VidThinker, we construct the VideoITG-40K dataset, containing 40K videos and 500K instructed temporal grounding annotations. We then design a plug-and-play VideoITG model, which takes advantage of visual language alignment and reasoning capabilities of Video-LLMs, for effective frame selection in a discriminative manner. Coupled with Video-LLMs, VideoITG achieves consistent performance improvements across multiple multimodal video understanding benchmarks, showing its superiority and great potentials for video understanding.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Technical Report",
    "pdf_url": "https://arxiv.org/pdf/2507.13353v1",
    "published_date": "2025-07-17 17:59:59 UTC",
    "updated_date": "2025-07-17 17:59:59 UTC"
  },
  {
    "arxiv_id": "2507.13348v1",
    "title": "VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning",
    "authors": [
      "Senqiao Yang",
      "Junyi Li",
      "Xin Lai",
      "Bei Yu",
      "Hengshuang Zhao",
      "Jiaya Jia"
    ],
    "abstract": "Recent advancements in vision-language models (VLMs) have improved performance by increasing the number of visual tokens, which are often significantly longer than text tokens. However, we observe that most real-world scenarios do not require such an extensive number of visual tokens. While the performance drops significantly in a small subset of OCR-related tasks, models still perform accurately in most other general VQA tasks with only 1/4 resolution. Therefore, we propose to dynamically process distinct samples with different resolutions, and present a new paradigm for visual token compression, namely, VisionThink. It starts with a downsampled image and smartly decides whether it is sufficient for problem solving. Otherwise, the model could output a special token to request the higher-resolution image. Compared to existing Efficient VLM methods that compress tokens using fixed pruning ratios or thresholds, VisionThink autonomously decides whether to compress tokens case by case. As a result, it demonstrates strong fine-grained visual understanding capability on OCR-related tasks, and meanwhile saves substantial visual tokens on simpler tasks. We adopt reinforcement learning and propose the LLM-as-Judge strategy to successfully apply RL to general VQA tasks. Moreover, we carefully design a reward function and penalty mechanism to achieve a stable and reasonable image resize call ratio. Extensive experiments demonstrate the superiority, efficiency, and effectiveness of our method. Our code is available at https://github.com/dvlab-research/VisionThink.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Code and models are available at https://github.com/dvlab-research/VisionThink",
    "pdf_url": "https://arxiv.org/pdf/2507.13348v1",
    "published_date": "2025-07-17 17:59:55 UTC",
    "updated_date": "2025-07-17 17:59:55 UTC"
  },
  {
    "arxiv_id": "2507.13345v2",
    "title": "Imbalance in Balance: Online Concept Balancing in Generation Models",
    "authors": [
      "Yukai Shi",
      "Jiarong Ou",
      "Rui Chen",
      "Haotian Yang",
      "Jiahao Wang",
      "Xin Tao",
      "Pengfei Wan",
      "Di Zhang",
      "Kun Gai"
    ],
    "abstract": "In visual generation tasks, the responses and combinations of complex concepts often lack stability and are error-prone, which remains an under-explored area. In this paper, we attempt to explore the causal factors for poor concept responses through elaborately designed experiments. We also design a concept-wise equalization loss function (IMBA loss) to address this issue. Our proposed method is online, eliminating the need for offline dataset processing, and requires minimal code changes. In our newly proposed complex concept benchmark Inert-CompBench and two other public test sets, our method significantly enhances the concept response capability of baseline models and yields highly competitive results with only a few codes released at https://github.com/KwaiVGI/IMBA-Loss.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ICCV2025. Codes have been released at https://github.com/KwaiVGI/IMBA-Loss",
    "pdf_url": "https://arxiv.org/pdf/2507.13345v2",
    "published_date": "2025-07-17 17:59:47 UTC",
    "updated_date": "2025-11-11 17:59:54 UTC"
  },
  {
    "arxiv_id": "2507.13340v2",
    "title": "Latent Policy Steering with Embodiment-Agnostic Pretrained World Models",
    "authors": [
      "Yiqi Wang",
      "Mrinal Verghese",
      "Jeff Schneider"
    ],
    "abstract": "Learning visuomotor policies via imitation has proven effective across a wide range of robotic domains. However, the performance of these policies is heavily dependent on the number of training demonstrations, which requires expensive data collection in the real world. In this work, we aim to reduce data collection efforts when learning visuomotor robot policies by leveraging existing or cost-effective data from a wide range of embodiments, such as public robot datasets and the datasets of humans playing with objects (human data from play). Our approach leverages two key insights. First, we use optic flow as an embodiment-agnostic action representation to train a World Model (WM) across multi-embodiment datasets, and finetune it on a small amount of robot data from the target embodiment. Second, we develop a method, Latent Policy Steering (LPS), to improve the output of a behavior-cloned policy by searching in the latent space of the WM for better action sequences. In real world experiments, we observe significant improvements in the performance of policies trained with a small amount of data (over 50% relative improvement with 30 demonstrations and over 20% relative improvement with 50 demonstrations) by combining the policy with a WM pretrained on two thousand episodes sampled from the existing Open X-embodiment dataset across different robots or a cost-effective human dataset from play.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.13340v2",
    "published_date": "2025-07-17 17:57:57 UTC",
    "updated_date": "2025-09-21 23:37:25 UTC"
  },
  {
    "arxiv_id": "2507.13428v1",
    "title": "\"PhyWorldBench\": A Comprehensive Evaluation of Physical Realism in Text-to-Video Models",
    "authors": [
      "Jing Gu",
      "Xian Liu",
      "Yu Zeng",
      "Ashwin Nagarajan",
      "Fangrui Zhu",
      "Daniel Hong",
      "Yue Fan",
      "Qianqi Yan",
      "Kaiwen Zhou",
      "Ming-Yu Liu",
      "Xin Eric Wang"
    ],
    "abstract": "Video generation models have achieved remarkable progress in creating high-quality, photorealistic content. However, their ability to accurately simulate physical phenomena remains a critical and unresolved challenge. This paper presents PhyWorldBench, a comprehensive benchmark designed to evaluate video generation models based on their adherence to the laws of physics. The benchmark covers multiple levels of physical phenomena, ranging from fundamental principles like object motion and energy conservation to more complex scenarios involving rigid body interactions and human or animal motion. Additionally, we introduce a novel \"\"Anti-Physics\"\" category, where prompts intentionally violate real-world physics, enabling the assessment of whether models can follow such instructions while maintaining logical consistency. Besides large-scale human evaluation, we also design a simple yet effective method that could utilize current MLLM to evaluate the physics realism in a zero-shot fashion. We evaluate 12 state-of-the-art text-to-video generation models, including five open-source and five proprietary models, with a detailed comparison and analysis. we identify pivotal challenges models face in adhering to real-world physics. Through systematic testing of their outputs across 1,050 curated prompts-spanning fundamental, composite, and anti-physics scenarios-we identify pivotal challenges these models face in adhering to real-world physics. We then rigorously examine their performance on diverse physical phenomena with varying prompt types, deriving targeted recommendations for crafting prompts that enhance fidelity to physical principles.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "31 pages, 21 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.13428v1",
    "published_date": "2025-07-17 17:54:09 UTC",
    "updated_date": "2025-07-17 17:54:09 UTC"
  },
  {
    "arxiv_id": "2507.13337v1",
    "title": "FormulaOne: Measuring the Depth of Algorithmic Reasoning Beyond Competitive Programming",
    "authors": [
      "Gal Beniamini",
      "Yuval Dor",
      "Alon Vinnikov",
      "Shir Granot Peled",
      "Or Weinstein",
      "Or Sharir",
      "Noam Wies",
      "Tomer Nussbaum",
      "Ido Ben Shaul",
      "Tomer Zekharya",
      "Yoav Levine",
      "Shai Shalev-Shwartz",
      "Amnon Shashua"
    ],
    "abstract": "Frontier AI models demonstrate formidable breadth of knowledge. But how close are they to true human -- or superhuman -- expertise? Genuine experts can tackle the hardest problems and push the boundaries of scientific understanding. To illuminate the limits of frontier model capabilities, we turn away from contrived competitive programming puzzles, and instead focus on real-life research problems.\n  We construct FormulaOne, a benchmark that lies at the intersection of graph theory, logic, and algorithms, all well within the training distribution of frontier models. Our problems are incredibly demanding, requiring an array of reasoning steps. The dataset has three key properties. First, it is of commercial interest and relates to practical large-scale optimisation problems, such as those arising in routing, scheduling, and network design. Second, it is generated from the highly expressive framework of Monadic Second-Order (MSO) logic on graphs, paving the way toward automatic problem generation at scale; ideal for building RL environments. Third, many of our problems are intimately related to the frontier of theoretical computer science, and to central conjectures therein, such as the Strong Exponential Time Hypothesis (SETH). As such, any significant algorithmic progress on our dataset, beyond known results, could carry profound theoretical implications.\n  Remarkably, state-of-the-art models like OpenAI's o3 fail entirely on FormulaOne, solving less than 1% of the questions, even when given 10 attempts and explanatory fewshot examples -- highlighting how far they remain from expert-level understanding in some domains. To support further research, we additionally curate FormulaOne-Warmup, offering a set of simpler tasks, from the same distribution. We release the full corpus along with a comprehensive evaluation framework.",
    "categories": [
      "cs.AI",
      "cs.CC",
      "math.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.13337v1",
    "published_date": "2025-07-17 17:53:55 UTC",
    "updated_date": "2025-07-17 17:53:55 UTC"
  },
  {
    "arxiv_id": "2507.13328v2",
    "title": "Vision-and-Language Training Helps Deploy Taxonomic Knowledge but Does Not Fundamentally Alter It",
    "authors": [
      "Yulu Qin",
      "Dheeraj Varghese",
      "Adam Dahlgren Lindström",
      "Lucia Donatelli",
      "Kanishka Misra",
      "Najoung Kim"
    ],
    "abstract": "Does vision-and-language (VL) training change the linguistic representations of language models in meaningful ways? Most results in the literature have shown inconsistent or marginal differences, both behaviorally and representationally. In this work, we start from the hypothesis that the domain in which VL training could have a significant effect is lexical-conceptual knowledge, in particular its taxonomic organization. Through comparing minimal pairs of text-only LMs and their VL-trained counterparts, we first show that the VL models often outperform their text-only counterparts on a text-only question-answering task that requires taxonomic understanding of concepts mentioned in the questions. Using an array of targeted behavioral and representational analyses, we show that the LMs and VLMs do not differ significantly in terms of their taxonomic knowledge itself, but they differ in how they represent questions that contain concepts in a taxonomic relation vs. a non-taxonomic relation. This implies that the taxonomic knowledge itself does not change substantially through additional VL training, but VL training does improve the deployment of this knowledge in the context of a specific task, even when the presentation of the task is purely linguistic.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.13328v2",
    "published_date": "2025-07-17 17:47:47 UTC",
    "updated_date": "2025-10-29 22:38:57 UTC"
  },
  {
    "arxiv_id": "2507.14240v3",
    "title": "HuggingGraph: Understanding the Supply Chain of LLM Ecosystem",
    "authors": [
      "Mohammad Shahedur Rahman",
      "Peng Gao",
      "Yuede Ji"
    ],
    "abstract": "Large language models (LLMs) leverage deep learning architectures to process and predict sequences of words, enabling them to perform a wide range of natural language processing tasks, such as translation, summarization, question answering, and content generation. As existing LLMs are often built from base models or other pre-trained models and use external datasets, they can inevitably inherit vulnerabilities, biases, or malicious components that exist in previous models or datasets. Therefore, it is critical to understand these components' origin and development process to detect potential risks, improve model fairness, and ensure compliance with regulatory frameworks. Motivated by that, this project aims to study such relationships between models and datasets, which are the central parts of the LLM supply chain. First, we design a methodology to systematically collect LLMs' supply chain information. Then, we design a new graph to model the relationships between models and datasets, which is a directed heterogeneous graph, having 402,654 nodes and 462,524 edges. Lastly, we perform different types of analysis and make multiple interesting findings.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.14240v3",
    "published_date": "2025-07-17 17:34:13 UTC",
    "updated_date": "2025-09-04 23:06:49 UTC"
  },
  {
    "arxiv_id": "2507.13314v1",
    "title": "Revisiting Reliability in the Reasoning-based Pose Estimation Benchmark",
    "authors": [
      "Junsu Kim",
      "Naeun Kim",
      "Jaeho Lee",
      "Incheol Park",
      "Dongyoon Han",
      "Seungryul Baek"
    ],
    "abstract": "The reasoning-based pose estimation (RPE) benchmark has emerged as a widely adopted evaluation standard for pose-aware multimodal large language models (MLLMs). Despite its significance, we identified critical reproducibility and benchmark-quality issues that hinder fair and consistent quantitative evaluations. Most notably, the benchmark utilizes different image indices from those of the original 3DPW dataset, forcing researchers into tedious and error-prone manual matching processes to obtain accurate ground-truth (GT) annotations for quantitative metrics (\\eg, MPJPE, PA-MPJPE). Furthermore, our analysis reveals several inherent benchmark-quality limitations, including significant image redundancy, scenario imbalance, overly simplistic poses, and ambiguous textual descriptions, collectively undermining reliable evaluations across diverse scenarios. To alleviate manual effort and enhance reproducibility, we carefully refined the GT annotations through meticulous visual matching and publicly release these refined annotations as an open-source resource, thereby promoting consistent quantitative evaluations and facilitating future advancements in human pose-aware multimodal reasoning.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "To be presented as a poster at MMFM 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.13314v1",
    "published_date": "2025-07-17 17:33:11 UTC",
    "updated_date": "2025-07-17 17:33:11 UTC"
  },
  {
    "arxiv_id": "2507.13302v1",
    "title": "The Generative Energy Arena (GEA): Incorporating Energy Awareness in Large Language Model (LLM) Human Evaluations",
    "authors": [
      "Carlos Arriaga",
      "Gonzalo Martínez",
      "Eneko Sendin",
      "Javier Conde",
      "Pedro Reviriego"
    ],
    "abstract": "The evaluation of large language models is a complex task, in which several approaches have been proposed. The most common is the use of automated benchmarks in which LLMs have to answer multiple-choice questions of different topics. However, this method has certain limitations, being the most concerning, the poor correlation with the humans. An alternative approach, is to have humans evaluate the LLMs. This poses scalability issues as there is a large and growing number of models to evaluate making it impractical (and costly) to run traditional studies based on recruiting a number of evaluators and having them rank the responses of the models. An alternative approach is the use of public arenas, such as the popular LM arena, on which any user can freely evaluate models on any question and rank the responses of two models. The results are then elaborated into a model ranking. An increasingly important aspect of LLMs is their energy consumption and, therefore, evaluating how energy awareness influences the decisions of humans in selecting a model is of interest. In this paper, we present GEA, the Generative Energy Arena, an arena that incorporates information on the energy consumption of the model in the evaluation process. Preliminary results obtained with GEA are also presented, showing that for most questions, when users are aware of the energy consumption, they favor smaller and more energy efficient models. This suggests that for most user interactions, the extra cost and energy incurred by the more complex and top-performing models do not provide an increase in the perceived quality of the responses that justifies their use.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.13302v1",
    "published_date": "2025-07-17 17:11:14 UTC",
    "updated_date": "2025-07-17 17:11:14 UTC"
  },
  {
    "arxiv_id": "2507.13425v2",
    "title": "CaTFormer: Causal Temporal Transformer with Dynamic Contextual Fusion for Driving Intention Prediction",
    "authors": [
      "Sirui Wang",
      "Zhou Guan",
      "Bingxi Zhao",
      "Tongjia Gu",
      "Jie Liu"
    ],
    "abstract": "Accurate prediction of driving intention is key to enhancing the safety and interactive efficiency of human-machine co-driving systems. It serves as a cornerstone for achieving high-level autonomous driving. However, current approaches remain inadequate for accurately modeling the complex spatiotemporal interdependencies and the unpredictable variability of human driving behavior. To address these challenges, we propose CaTFormer, a causal Temporal Transformer that explicitly models causal interactions between driver behavior and environmental context for robust intention prediction. Specifically, CaTFormer introduces a novel Reciprocal Delayed Fusion (RDF) mechanism for precise temporal alignment of interior and exterior feature streams, a Counterfactual Residual Encoding (CRE) module that systematically eliminates spurious correlations to reveal authentic causal dependencies, and an innovative Feature Synthesis Network (FSN) that adaptively synthesizes these purified representations into coherent temporal representations. Experimental results demonstrate that CaTFormer attains state-of-the-art performance on the Brain4Cars dataset. It effectively captures complex causal temporal dependencies and enhances both the accuracy and transparency of driving intention prediction.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at AAAI 2026",
    "pdf_url": "https://arxiv.org/pdf/2507.13425v2",
    "published_date": "2025-07-17 17:10:37 UTC",
    "updated_date": "2026-01-08 09:25:09 UTC"
  },
  {
    "arxiv_id": "2507.13300v1",
    "title": "AbGen: Evaluating Large Language Models in Ablation Study Design and Evaluation for Scientific Research",
    "authors": [
      "Yilun Zhao",
      "Weiyuan Chen",
      "Zhijian Xu",
      "Manasi Patwardhan",
      "Yixin Liu",
      "Chengye Wang",
      "Lovekesh Vig",
      "Arman Cohan"
    ],
    "abstract": "We introduce AbGen, the first benchmark designed to evaluate the capabilities of LLMs in designing ablation studies for scientific research. AbGen consists of 1,500 expert-annotated examples derived from 807 NLP papers. In this benchmark, LLMs are tasked with generating detailed ablation study designs for a specified module or process based on the given research context. Our evaluation of leading LLMs, such as DeepSeek-R1-0528 and o4-mini, highlights a significant performance gap between these models and human experts in terms of the importance, faithfulness, and soundness of the ablation study designs. Moreover, we demonstrate that current automated evaluation methods are not reliable for our task, as they show a significant discrepancy when compared to human assessment. To better investigate this, we develop AbGen-Eval, a meta-evaluation benchmark designed to assess the reliability of commonly used automated evaluation systems in measuring LLM performance on our task. We investigate various LLM-as-Judge systems on AbGen-Eval, providing insights for future research on developing more effective and reliable LLM-based evaluation systems for complex scientific tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.13300v1",
    "published_date": "2025-07-17 17:09:22 UTC",
    "updated_date": "2025-07-17 17:09:22 UTC"
  },
  {
    "arxiv_id": "2507.13423v1",
    "title": "Air Traffic Controller Task Demand via Graph Neural Networks: An Interpretable Approach to Airspace Complexity",
    "authors": [
      "Edward Henderson",
      "Dewi Gould",
      "Richard Everson",
      "George De Ath",
      "Nick Pepper"
    ],
    "abstract": "Real-time assessment of near-term Air Traffic Controller (ATCO) task demand is a critical challenge in an increasingly crowded airspace, as existing complexity metrics often fail to capture nuanced operational drivers beyond simple aircraft counts. This work introduces an interpretable Graph Neural Network (GNN) framework to address this gap. Our attention-based model predicts the number of upcoming clearances, the instructions issued to aircraft by ATCOs, from interactions within static traffic scenarios. Crucially, we derive an interpretable, per-aircraft task demand score by systematically ablating aircraft and measuring the impact on the model's predictions. Our framework significantly outperforms an ATCO-inspired heuristic and is a more reliable estimator of scenario complexity than established baselines. The resulting tool can attribute task demand to specific aircraft, offering a new way to analyse and understand the drivers of complexity for applications in controller training and airspace redesign.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Author Accepted Manuscript version of paper at the AIAA AVIATION Forum 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.13423v1",
    "published_date": "2025-07-17 17:02:42 UTC",
    "updated_date": "2025-07-17 17:02:42 UTC"
  },
  {
    "arxiv_id": "2507.13290v2",
    "title": "Towards Formal Verification of LLM-Generated Code from Natural Language Prompts",
    "authors": [
      "Aaron Councilman",
      "David Jiahao Fu",
      "Aryan Gupta",
      "Chengxiao Wang",
      "David Grove",
      "Yu-Xiong Wang",
      "Vikram Adve"
    ],
    "abstract": "In the past few years LLMs have emerged as a tool that can aid programmers by taking natural language descriptions and generating code based on it. However, the reliability of LLM code generation and current validation techniques for it are far from strong enough to be used for mission-critical or safety-critical applications. In this work we explore ways to offer formal guarantees of correctness to LLM generated code; such guarantees could improve the quality of general AI Code Assistants and support their use for critical applications. To address this challenge we propose to incorporate a Formal Query Language that can represent a user's intent in a formally defined but natural language-like manner that a user can confirm matches their intent. We then have a formal specification of the user intent which we can use to verify that LLM-generated code matches the user's intent. We implement these ideas in our system, Astrogator, for the Ansible programming language, widely used for system administration, including for critical systems. The system includes an intuitive formal query language, a calculus for representing the behavior of Ansible programs, and a symbolic interpreter and a unification algorithm which together are used for the verification. A key innovation in Astrogator is the use of a Knowledge Base to capture system-specific implementation dependencies that greatly reduce the need for system knowledge in expressing formal queries. On a benchmark suite of 21 code-generation tasks, our verifier is able to verify correct code in 83% of cases and identify incorrect code in 92%.",
    "categories": [
      "cs.PL",
      "cs.AI"
    ],
    "primary_category": "cs.PL",
    "comment": "28 pages, 10 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.13290v2",
    "published_date": "2025-07-17 16:54:42 UTC",
    "updated_date": "2025-11-20 21:09:31 UTC"
  },
  {
    "arxiv_id": "2507.13277v1",
    "title": "Evaluating Reinforcement Learning Algorithms for Navigation in Simulated Robotic Quadrupeds: A Comparative Study Inspired by Guide Dog Behaviour",
    "authors": [
      "Emma M. A. Harrison"
    ],
    "abstract": "Robots are increasingly integrated across industries, particularly in healthcare. However, many valuable applications for quadrupedal robots remain overlooked. This research explores the effectiveness of three reinforcement learning algorithms in training a simulated quadruped robot for autonomous navigation and obstacle avoidance. The goal is to develop a robotic guide dog simulation capable of path following and obstacle avoidance, with long-term potential for real-world assistance to guide dogs and visually impaired individuals. It also seeks to expand research into medical 'pets', including robotic guide and alert dogs.\n  A comparative analysis of thirteen related research papers shaped key evaluation criteria, including collision detection, pathfinding algorithms, sensor usage, robot type, and simulation platforms. The study focuses on sensor inputs, collision frequency, reward signals, and learning progression to determine which algorithm best supports robotic navigation in complex environments.\n  Custom-made environments were used to ensure fair evaluation of all three algorithms under controlled conditions, allowing consistent data collection. Results show that Proximal Policy Optimization (PPO) outperformed Deep Q-Network (DQN) and Q-learning across all metrics, particularly in average and median steps to goal per episode.\n  By analysing these results, this study contributes to robotic navigation, AI and medical robotics, offering insights into the feasibility of AI-driven quadruped mobility and its role in assistive robotics.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.13277v1",
    "published_date": "2025-07-17 16:38:14 UTC",
    "updated_date": "2025-07-17 16:38:14 UTC"
  },
  {
    "arxiv_id": "2507.13275v1",
    "title": "Overview of the TalentCLEF 2025: Skill and Job Title Intelligence for Human Capital Management",
    "authors": [
      "Luis Gasco",
      "Hermenegildo Fabregat",
      "Laura García-Sardiña",
      "Paula Estrella",
      "Daniel Deniz",
      "Alvaro Rodrigo",
      "Rabih Zbib"
    ],
    "abstract": "Advances in natural language processing and large language models are driving a major transformation in Human Capital Management, with a growing interest in building smart systems based on language technologies for talent acquisition, upskilling strategies, and workforce planning. However, the adoption and progress of these technologies critically depend on the development of reliable and fair models, properly evaluated on public data and open benchmarks, which have so far been unavailable in this domain.\n  To address this gap, we present TalentCLEF 2025, the first evaluation campaign focused on skill and job title intelligence. The lab consists of two tasks: Task A - Multilingual Job Title Matching, covering English, Spanish, German, and Chinese; and Task B - Job Title-Based Skill Prediction, in English. Both corpora were built from real job applications, carefully anonymized, and manually annotated to reflect the complexity and diversity of real-world labor market data, including linguistic variability and gender-marked expressions.\n  The evaluations included monolingual and cross-lingual scenarios and covered the evaluation of gender bias.\n  TalentCLEF attracted 76 registered teams with more than 280 submissions. Most systems relied on information retrieval techniques built with multilingual encoder-based models fine-tuned with contrastive learning, and several of them incorporated large language models for data augmentation or re-ranking. The results show that the training strategies have a larger effect than the size of the model alone. TalentCLEF provides the first public benchmark in this field and encourages the development of robust, fair, and transferable language technologies for the labor market.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.13275v1",
    "published_date": "2025-07-17 16:33:57 UTC",
    "updated_date": "2025-07-17 16:33:57 UTC"
  },
  {
    "arxiv_id": "2507.13266v3",
    "title": "QuestA: Expanding Reasoning Capacity in LLMs via Question Augmentation",
    "authors": [
      "Jiazheng Li",
      "Hongzhou Lin",
      "Hong Lu",
      "Kaiyue Wen",
      "Zaiwen Yang",
      "Jiaxuan Gao",
      "Yi Wu",
      "Jingzhao Zhang"
    ],
    "abstract": "Reinforcement learning (RL) has emerged as a central paradigm for training large language models (LLMs) in reasoning tasks. Yet recent studies question RL's ability to incentivize reasoning capacity beyond the base model. This raises a key challenge: how can RL be adapted to solve harder reasoning problems more effectively? To address this challenge, we propose a simple yet effective strategy via Question Augmentation: introduce partial solutions during training to reduce problem difficulty and provide more informative learning signals. Our method, QuestA, when applied during RL training on math reasoning tasks, not only improves pass@1 but also pass@k-particularly on problems where standard RL struggles to make progress. This enables continual improvement over strong open-source models such as DeepScaleR and OpenMath Nemotron, further enhancing their reasoning capabilities. We achieve new state-of-the-art results on math benchmarks using 1.5B-parameter models: 72.50% (+10.73%) on AIME24, 62.29% (+12.79%) on AIME25, and 41.67% (+10.11%) on HMMT25. Code, data and model are available at https://github.com/foreverlasting1202/QuestA.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "19 pages, 8 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.13266v3",
    "published_date": "2025-07-17 16:21:47 UTC",
    "updated_date": "2025-09-30 04:42:16 UTC"
  },
  {
    "arxiv_id": "2507.13264v1",
    "title": "Voxtral",
    "authors": [
      "Alexander H. Liu",
      "Andy Ehrenberg",
      "Andy Lo",
      "Clément Denoix",
      "Corentin Barreau",
      "Guillaume Lample",
      "Jean-Malo Delignon",
      "Khyathi Raghavi Chandu",
      "Patrick von Platen",
      "Pavankumar Reddy Muddireddy",
      "Sanchit Gandhi",
      "Soham Ghosh",
      "Srijan Mishra",
      "Thomas Foubert",
      "Abhinav Rastogi",
      "Adam Yang",
      "Albert Q. Jiang",
      "Alexandre Sablayrolles",
      "Amélie Héliou",
      "Amélie Martin",
      "Anmol Agarwal",
      "Antoine Roux",
      "Arthur Darcet",
      "Arthur Mensch",
      "Baptiste Bout",
      "Baptiste Rozière",
      "Baudouin De Monicault",
      "Chris Bamford",
      "Christian Wallenwein",
      "Christophe Renaudin",
      "Clémence Lanfranchi",
      "Darius Dabert",
      "Devendra Singh Chaplot",
      "Devon Mizelle",
      "Diego de las Casas",
      "Elliot Chane-Sane",
      "Emilien Fugier",
      "Emma Bou Hanna",
      "Gabrielle Berrada",
      "Gauthier Delerce",
      "Gauthier Guinet",
      "Georgii Novikov",
      "Guillaume Martin",
      "Himanshu Jaju",
      "Jan Ludziejewski",
      "Jason Rute",
      "Jean-Hadrien Chabran",
      "Jessica Chudnovsky",
      "Joachim Studnia",
      "Joep Barmentlo",
      "Jonas Amar",
      "Josselin Somerville Roberts",
      "Julien Denize",
      "Karan Saxena",
      "Karmesh Yadav",
      "Kartik Khandelwal",
      "Kush Jain",
      "Lélio Renard Lavaud",
      "Léonard Blier",
      "Lingxiao Zhao",
      "Louis Martin",
      "Lucile Saulnier",
      "Luyu Gao",
      "Marie Pellat",
      "Mathilde Guillaumin",
      "Mathis Felardos",
      "Matthieu Dinot",
      "Maxime Darrin",
      "Maximilian Augustin",
      "Mickaël Seznec",
      "Neha Gupta",
      "Nikhil Raghuraman",
      "Olivier Duchenne",
      "Patricia Wang",
      "Patryk Saffer",
      "Paul Jacob",
      "Paul Wambergue",
      "Paula Kurylowicz",
      "Philomène Chagniot",
      "Pierre Stock",
      "Pravesh Agrawal",
      "Rémi Delacourt",
      "Romain Sauvestre",
      "Roman Soletskyi",
      "Sagar Vaze",
      "Sandeep Subramanian",
      "Saurabh Garg",
      "Shashwat Dalal",
      "Siddharth Gandhi",
      "Sumukh Aithal",
      "Szymon Antoniak",
      "Teven Le Scao",
      "Thibault Schueller",
      "Thibaut Lavril",
      "Thomas Robert",
      "Thomas Wang",
      "Timothée Lacroix",
      "Tom Bewley",
      "Valeriia Nemychnikova",
      "Victor Paltz",
      "Virgile Richard",
      "Wen-Ding Li",
      "William Marshall",
      "Xuanyu Zhang",
      "Yihan Wan",
      "Yunhao Tang"
    ],
    "abstract": "We present Voxtral Mini and Voxtral Small, two multimodal audio chat models. Voxtral is trained to comprehend both spoken audio and text documents, achieving state-of-the-art performance across a diverse range of audio benchmarks, while preserving strong text capabilities. Voxtral Small outperforms a number of closed-source models, while being small enough to run locally. A 32K context window enables the model to handle audio files up to 40 minutes in duration and long multi-turn conversations. We also contribute three benchmarks for evaluating speech understanding models on knowledge and trivia. Both Voxtral models are released under Apache 2.0 license.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "17 pages",
    "pdf_url": "https://arxiv.org/pdf/2507.13264v1",
    "published_date": "2025-07-17 16:17:37 UTC",
    "updated_date": "2025-07-17 16:17:37 UTC"
  },
  {
    "arxiv_id": "2507.13263v3",
    "title": "From Sorting Algorithms to Scalable Kernels: Bayesian Optimization in High-Dimensional Permutation Spaces",
    "authors": [
      "Zikai Xie",
      "Linjiang Chen"
    ],
    "abstract": "Bayesian Optimization (BO) is a powerful tool for black-box optimization, but its application to high-dimensional permutation spaces is severely limited by the challenge of defining scalable representations. The current state-of-the-art BO approach for permutation spaces relies on an exhaustive $Ω(n^2)$ pairwise comparison, inducing a dense representation that is impractical for large-scale permutations. To break this barrier, we introduce a novel framework for generating efficient permutation representations via kernel functions derived from sorting algorithms. Within this framework, the Mallows kernel can be viewed as a special instance derived from enumeration sort. Further, we introduce the \\textbf{Merge Kernel} , which leverages the divide-and-conquer structure of merge sort to produce a compact, $Θ(n\\log n)$ to achieve the lowest possible complexity with no information loss and effectively capture permutation structure. Our central thesis is that the Merge Kernel performs competitively with the Mallows kernel in low-dimensional settings, but significantly outperforms it in both optimization performance and computational efficiency as the dimension $n$ grows. Extensive evaluations on various permutation optimization benchmarks confirm our hypothesis, demonstrating that the Merge Kernel provides a scalable and more effective solution for Bayesian optimization in high-dimensional permutation spaces, thereby unlocking the potential for tackling previously intractable problems such as large-scale feature ordering and combinatorial neural architecture search.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, submitted to ICLR-26",
    "pdf_url": "https://arxiv.org/pdf/2507.13263v3",
    "published_date": "2025-07-17 16:12:39 UTC",
    "updated_date": "2025-09-25 02:43:27 UTC"
  },
  {
    "arxiv_id": "2507.13260v1",
    "title": "Efficient Adaptation of Pre-trained Vision Transformer underpinned by Approximately Orthogonal Fine-Tuning Strategy",
    "authors": [
      "Yiting Yang",
      "Hao Luo",
      "Yuan Sun",
      "Qingsen Yan",
      "Haokui Zhang",
      "Wei Dong",
      "Guoqing Wang",
      "Peng Wang",
      "Yang Yang",
      "Hengtao Shen"
    ],
    "abstract": "A prevalent approach in Parameter-Efficient Fine-Tuning (PEFT) of pre-trained Vision Transformers (ViT) involves freezing the majority of the backbone parameters and solely learning low-rank adaptation weight matrices to accommodate downstream tasks. These low-rank matrices are commonly derived through the multiplication structure of down-projection and up-projection matrices, exemplified by methods such as LoRA and Adapter. In this work, we observe an approximate orthogonality among any two row or column vectors within any weight matrix of the backbone parameters; however, this property is absent in the vectors of the down/up-projection matrices. Approximate orthogonality implies a reduction in the upper bound of the model's generalization error, signifying that the model possesses enhanced generalization capability. If the fine-tuned down/up-projection matrices were to exhibit this same property as the pre-trained backbone matrices, could the generalization capability of fine-tuned ViTs be further augmented? To address this question, we propose an Approximately Orthogonal Fine-Tuning (AOFT) strategy for representing the low-rank weight matrices. This strategy employs a single learnable vector to generate a set of approximately orthogonal vectors, which form the down/up-projection matrices, thereby aligning the properties of these matrices with those of the backbone. Extensive experimental results demonstrate that our method achieves competitive performance across a range of downstream image classification tasks, confirming the efficacy of the enhanced generalization capability embedded in the down/up-projection matrices.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This paper is accepted by ICCV 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.13260v1",
    "published_date": "2025-07-17 16:09:05 UTC",
    "updated_date": "2025-07-17 16:09:05 UTC"
  },
  {
    "arxiv_id": "2507.13255v3",
    "title": "Automating Steering for Safe Multimodal Large Language Models",
    "authors": [
      "Lyucheng Wu",
      "Mengru Wang",
      "Ziwen Xu",
      "Tri Cao",
      "Nay Oo",
      "Bryan Hooi",
      "Shumin Deng"
    ],
    "abstract": "Recent progress in Multimodal Large Language Models (MLLMs) has unlocked powerful cross-modal reasoning abilities, but also raised new safety concerns, particularly when faced with adversarial multimodal inputs. To improve the safety of MLLMs during inference, we introduce a modular and adaptive inference-time intervention technology, AutoSteer, without requiring any fine-tuning of the underlying model. AutoSteer incorporates three core components: (1) a novel Safety Awareness Score (SAS) that automatically identifies the most safety-relevant distinctions among the model's internal layers; (2) an adaptive safety prober trained to estimate the likelihood of toxic outputs from intermediate representations; and (3) a lightweight Refusal Head that selectively intervenes to modulate generation when safety risks are detected. Experiments on LLaVA-OV and Chameleon across diverse safety-critical benchmarks demonstrate that AutoSteer significantly reduces the Attack Success Rate (ASR) for textual, visual, and cross-modal threats, while maintaining general abilities. These findings position AutoSteer as a practical, interpretable, and effective framework for safer deployment of multimodal AI systems.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2025 Main Conference. 23 pages (8+ for main); 25 figures; 1 table",
    "pdf_url": "https://arxiv.org/pdf/2507.13255v3",
    "published_date": "2025-07-17 16:04:55 UTC",
    "updated_date": "2025-09-23 03:15:44 UTC"
  },
  {
    "arxiv_id": "2507.13238v2",
    "title": "Multilingual LLMs Are Not Multilingual Thinkers: Evidence from Hindi Analogy Evaluation",
    "authors": [
      "Ashray Gupta",
      "Rohan Joseph",
      "Sunny Rai"
    ],
    "abstract": "Analogies test a model's ability to infer implicit relationships between concepts, making them a key benchmark for evaluating reasoning capabilities. While large language models (LLMs) are widely evaluated for reasoning in English, their abilities in Indic languages remain understudied, limiting our understanding of whether these models generalize across languages. To address this gap, we introduce a new Hindi Analogy Test Set (HATS), comprising 405 multiple-choice questions sourced from Indian government exams. We benchmark state-of-the-art multilingual LLMs using various prompting strategies and introduce a grounded Chain of Thought approach that leverages cognitive theories of analogical reasoning. This approach improves model performance on Hindi analogy questions. Our experiments show that models perform best with English prompts, irrespective of the prompting strategy. Our test set addresses the lack of a critical resource to evaluate LLM reasoning capabilities in Hindi.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.13238v2",
    "published_date": "2025-07-17 15:47:49 UTC",
    "updated_date": "2025-07-23 21:50:22 UTC"
  },
  {
    "arxiv_id": "2507.13231v3",
    "title": "VITA: Vision-to-Action Flow Matching Policy",
    "authors": [
      "Dechen Gao",
      "Boqi Zhao",
      "Andrew Lee",
      "Ian Chuang",
      "Hanchu Zhou",
      "Hang Wang",
      "Zhe Zhao",
      "Junshan Zhang",
      "Iman Soltani"
    ],
    "abstract": "Conventional flow matching and diffusion-based policies sample through iterative denoising from standard noise distributions (e.g., Gaussian), and require conditioning modules to repeatedly incorporate visual information during the generative process, incurring substantial time and memory overhead. To reduce the complexity, we develop VITA(VIsion-To-Action policy), a noise-free and conditioning-free flow matching policy learning framework that directly flows from visual representations to latent actions. Since the source of the flow is visually grounded, VITA eliminates the need of visual conditioning during generation. As expected, bridging vision and action is challenging, because actions are lower-dimensional, less structured, and sparser than visual representations; moreover, flow matching requires the source and target to have the same dimensionality. To overcome this, we introduce an action autoencoder that maps raw actions into a structured latent space aligned with visual latents, trained jointly with flow matching. To further prevent latent space collapse, we propose flow latent decoding, which anchors the latent generation process by backpropagating the action reconstruction loss through the flow matching ODE (ordinary differential equation) solving steps. We evaluate VITA on 9 simulation and 5 real-world tasks from ALOHA and Robomimic. VITA achieves 1.5x-2x faster inference compared to conventional methods with conditioning modules, while outperforming or matching state-of-the-art policies. Codes, datasets, and demos are available at our project page: https://ucd-dare.github.io/VITA/.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://ucd-dare.github.io/VITA/ Code: https://github.com/ucd-dare/VITA",
    "pdf_url": "https://arxiv.org/pdf/2507.13231v3",
    "published_date": "2025-07-17 15:41:57 UTC",
    "updated_date": "2025-12-01 12:22:05 UTC"
  },
  {
    "arxiv_id": "2507.13229v4",
    "title": "{S\\textsuperscript{2}M\\textsuperscript{2}}: Scalable Stereo Matching Model for Reliable Depth Estimation",
    "authors": [
      "Junhong Min",
      "Youngpil Jeon",
      "Jimin Kim",
      "Minyong Choi"
    ],
    "abstract": "The pursuit of a generalizable stereo matching model, capable of performing well across varying resolutions and disparity ranges without dataset-specific fine-tuning, has revealed a fundamental trade-off. Iterative local search methods achieve high scores on constrained benchmarks, but their core mechanism inherently limits the global consistency required for true generalization. However, global matching architectures, while theoretically more robust, have historically been rendered infeasible by prohibitive computational and memory costs. We resolve this dilemma with {S\\textsuperscript{2}M\\textsuperscript{2}}: a global matching architecture that achieves state-of-the-art accuracy and high efficiency without relying on cost volume filtering or deep refinement stacks. Our design integrates a multi-resolution transformer for robust long-range correspondence, trained with a novel loss function that concentrates probability on feasible matches. This approach enables a more robust joint estimation of disparity, occlusion, and confidence. {S\\textsuperscript{2}M\\textsuperscript{2}} establishes a new state of the art on Middlebury v3 and ETH3D benchmarks, significantly outperforming prior methods in most metrics while reconstructing high-quality details with competitive efficiency.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 5 figures, ICCV accepted paper",
    "pdf_url": "https://arxiv.org/pdf/2507.13229v4",
    "published_date": "2025-07-17 15:40:18 UTC",
    "updated_date": "2025-10-11 14:41:27 UTC"
  },
  {
    "arxiv_id": "2507.13221v1",
    "title": "Synthesizing Reality: Leveraging the Generative AI-Powered Platform Midjourney for Construction Worker Detection",
    "authors": [
      "Hongyang Zhao",
      "Tianyu Liang",
      "Sina Davari",
      "Daeho Kim"
    ],
    "abstract": "While recent advancements in deep neural networks (DNNs) have substantially enhanced visual AI's capabilities, the challenge of inadequate data diversity and volume remains, particularly in construction domain. This study presents a novel image synthesis methodology tailored for construction worker detection, leveraging the generative-AI platform Midjourney. The approach entails generating a collection of 12,000 synthetic images by formulating 3000 different prompts, with an emphasis on image realism and diversity. These images, after manual labeling, serve as a dataset for DNN training. Evaluation on a real construction image dataset yielded promising results, with the model attaining average precisions (APs) of 0.937 and 0.642 at intersection-over-union (IoU) thresholds of 0.5 and 0.5 to 0.95, respectively. Notably, the model demonstrated near-perfect performance on the synthetic dataset, achieving APs of 0.994 and 0.919 at the two mentioned thresholds. These findings reveal both the potential and weakness of generative AI in addressing DNN training data scarcity.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This work was presented at ASCE International Conference on Computing in Civil Engineering (i3CE) 2024 and is currently under consideration for publication in ASCE proceedings",
    "pdf_url": "https://arxiv.org/pdf/2507.13221v1",
    "published_date": "2025-07-17 15:35:27 UTC",
    "updated_date": "2025-07-17 15:35:27 UTC"
  },
  {
    "arxiv_id": "2507.13208v1",
    "title": "Higher-Order Pattern Unification Modulo Similarity Relations",
    "authors": [
      "Besik Dundua",
      "Temur Kutsia"
    ],
    "abstract": "The combination of higher-order theories and fuzzy logic can be useful in decision-making tasks that involve reasoning across abstract functions and predicates, where exact matches are often rare or unnecessary. Developing efficient reasoning and computational techniques for such a combined formalism presents a significant challenge. In this paper, we adopt a more straightforward approach aiming at integrating two well-established and computationally well-behaved components: higher-order patterns on one side and fuzzy equivalences expressed through similarity relations based on minimum T-norm on the other. We propose a unification algorithm for higher-order patterns modulo these similarity relations and prove its termination, soundness, and completeness. This unification problem, like its crisp counterpart, is unitary. The algorithm computes a most general unifier with the highest degree of approximation when the given terms are unifiable.",
    "categories": [
      "cs.AI",
      "cs.LO",
      "math.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "23 pages",
    "pdf_url": "https://arxiv.org/pdf/2507.13208v1",
    "published_date": "2025-07-17 15:18:22 UTC",
    "updated_date": "2025-07-17 15:18:22 UTC"
  },
  {
    "arxiv_id": "2507.13175v2",
    "title": "Black Box Deployed -- Functional Criteria for Artificial Moral Agents in the LLM Era",
    "authors": [
      "Matthew E. Brophy"
    ],
    "abstract": "The advancement of powerful yet opaque large language models (LLMs) necessitates a fundamental revision of the philosophical criteria used to evaluate artificial moral agents (AMAs). Pre-LLM frameworks often relied on the assumption of transparent architectures, which LLMs defy due to their stochastic outputs and opaque internal states. This paper argues that traditional ethical criteria are pragmatically obsolete for LLMs due to this mismatch. Engaging with core themes in the philosophy of technology, this paper proffers a revised set of ten functional criteria to evaluate LLM-based artificial moral agents: moral concordance, context sensitivity, normative integrity, metaethical awareness, system resilience, trustworthiness, corrigibility, partial transparency, functional autonomy, and moral imagination. These guideposts, applied to what we term \"SMA-LLS\" (Simulating Moral Agency through Large Language Systems), aim to steer AMAs toward greater alignment and beneficial societal integration in the coming years. We illustrate these criteria using hypothetical scenarios involving an autonomous public bus (APB) to demonstrate their practical applicability in morally salient contexts.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "42 pages. Supplementary material included at end of article",
    "pdf_url": "https://arxiv.org/pdf/2507.13175v2",
    "published_date": "2025-07-17 14:39:29 UTC",
    "updated_date": "2025-07-25 21:09:11 UTC"
  },
  {
    "arxiv_id": "2507.13171v2",
    "title": "Aligning Humans and Robots via Reinforcement Learning from Implicit Human Feedback",
    "authors": [
      "Suzie Kim",
      "Hye-Bin Shin",
      "Seong-Whan Lee"
    ],
    "abstract": "Conventional reinforcement learning (RL) ap proaches often struggle to learn effective policies under sparse reward conditions, necessitating the manual design of complex, task-specific reward functions. To address this limitation, rein forcement learning from human feedback (RLHF) has emerged as a promising strategy that complements hand-crafted rewards with human-derived evaluation signals. However, most existing RLHF methods depend on explicit feedback mechanisms such as button presses or preference labels, which disrupt the natural interaction process and impose a substantial cognitive load on the user. We propose a novel reinforcement learning from implicit human feedback (RLIHF) framework that utilizes non-invasive electroencephalography (EEG) signals, specifically error-related potentials (ErrPs), to provide continuous, implicit feedback without requiring explicit user intervention. The proposed method adopts a pre-trained decoder to transform raw EEG signals into probabilistic reward components, en abling effective policy learning even in the presence of sparse external rewards. We evaluate our approach in a simulation environment built on the MuJoCo physics engine, using a Kinova Gen2 robotic arm to perform a complex pick-and-place task that requires avoiding obstacles while manipulating target objects. The results show that agents trained with decoded EEG feedback achieve performance comparable to those trained with dense, manually designed rewards. These findings validate the potential of using implicit neural feedback for scalable and human-aligned reinforcement learning in interactive robotics.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted to IEEE Int. Conf. Syst., Man, Cybern. (SMC) 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.13171v2",
    "published_date": "2025-07-17 14:35:12 UTC",
    "updated_date": "2025-12-11 23:50:00 UTC"
  },
  {
    "arxiv_id": "2507.13170v1",
    "title": "SHIELD: A Secure and Highly Enhanced Integrated Learning for Robust Deepfake Detection against Adversarial Attacks",
    "authors": [
      "Kutub Uddin",
      "Awais Khan",
      "Muhammad Umar Farooq",
      "Khalid Malik"
    ],
    "abstract": "Audio plays a crucial role in applications like speaker verification, voice-enabled smart devices, and audio conferencing. However, audio manipulations, such as deepfakes, pose significant risks by enabling the spread of misinformation. Our empirical analysis reveals that existing methods for detecting deepfake audio are often vulnerable to anti-forensic (AF) attacks, particularly those attacked using generative adversarial networks. In this article, we propose a novel collaborative learning method called SHIELD to defend against generative AF attacks. To expose AF signatures, we integrate an auxiliary generative model, called the defense (DF) generative model, which facilitates collaborative learning by combining input and output. Furthermore, we design a triplet model to capture correlations for real and AF attacked audios with real-generated and attacked-generated audios using auxiliary generative models. The proposed SHIELD strengthens the defense against generative AF attacks and achieves robust performance across various generative models. The proposed AF significantly reduces the average detection accuracy from 95.49% to 59.77% for ASVspoof2019, from 99.44% to 38.45% for In-the-Wild, and from 98.41% to 51.18% for HalfTruth for three different generative models. The proposed SHIELD mechanism is robust against AF attacks and achieves an average accuracy of 98.13%, 98.58%, and 99.57% in match, and 98.78%, 98.62%, and 98.85% in mismatch settings for the ASVspoof2019, In-the-Wild, and HalfTruth datasets, respectively.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CR",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.13170v1",
    "published_date": "2025-07-17 14:33:54 UTC",
    "updated_date": "2025-07-17 14:33:54 UTC"
  },
  {
    "arxiv_id": "2507.13169v1",
    "title": "Prompt Injection 2.0: Hybrid AI Threats",
    "authors": [
      "Jeremy McHugh",
      "Kristina Šekrst",
      "Jon Cefalu"
    ],
    "abstract": "Prompt injection attacks, where malicious input is designed to manipulate AI systems into ignoring their original instructions and following unauthorized commands instead, were first discovered by Preamble, Inc. in May 2022 and responsibly disclosed to OpenAI. Over the last three years, these attacks have continued to pose a critical security threat to LLM-integrated systems. The emergence of agentic AI systems, where LLMs autonomously perform multistep tasks through tools and coordination with other agents, has fundamentally transformed the threat landscape. Modern prompt injection attacks can now combine with traditional cybersecurity exploits to create hybrid threats that systematically evade traditional security controls. This paper presents a comprehensive analysis of Prompt Injection 2.0, examining how prompt injections integrate with Cross-Site Scripting (XSS), Cross-Site Request Forgery (CSRF), and other web security vulnerabilities to bypass traditional security measures. We build upon Preamble's foundational research and mitigation technologies, evaluating them against contemporary threats, including AI worms, multi-agent infections, and hybrid cyber-AI attacks. Our analysis incorporates recent benchmarks that demonstrate how traditional web application firewalls, XSS filters, and CSRF tokens fail against AI-enhanced attacks. We also present architectural solutions that combine prompt isolation, runtime security, and privilege separation with novel threat detection capabilities.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.13169v1",
    "published_date": "2025-07-17 14:33:36 UTC",
    "updated_date": "2025-07-17 14:33:36 UTC"
  },
  {
    "arxiv_id": "2507.13162v2",
    "title": "Orbis: Overcoming Challenges of Long-Horizon Prediction in Driving World Models",
    "authors": [
      "Arian Mousakhan",
      "Sudhanshu Mittal",
      "Silvio Galesso",
      "Karim Farid",
      "Thomas Brox"
    ],
    "abstract": "Existing world models for autonomous driving struggle with long-horizon generation and generalization to challenging scenarios. In this work, we develop a model using simple design choices, and without additional supervision or sensors, such as maps, depth, or multiple cameras. We show that our model yields state-of-the-art performance, despite having only 469M parameters and being trained on 280h of video data. It particularly stands out in difficult scenarios like turning maneuvers and urban traffic. We test whether discrete token models possibly have advantages over continuous models based on flow matching. To this end, we set up a hybrid tokenizer that is compatible with both approaches and allows for a side-by-side comparison. Our study concludes in favor of the continuous autoregressive model, which is less brittle on individual design choices and more powerful than the model built on discrete tokens. Code, models and qualitative results are publicly available at https://lmb-freiburg.github.io/orbis.github.io/.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://lmb-freiburg.github.io/orbis.github.io/",
    "pdf_url": "https://arxiv.org/pdf/2507.13162v2",
    "published_date": "2025-07-17 14:29:34 UTC",
    "updated_date": "2025-12-11 10:05:50 UTC"
  },
  {
    "arxiv_id": "2507.14239v1",
    "title": "CCL-XCoT: An Efficient Cross-Lingual Knowledge Transfer Method for Mitigating Hallucination Generation",
    "authors": [
      "Weihua Zheng",
      "Roy Ka-Wei Lee",
      "Zhengyuan Liu",
      "Kui Wu",
      "AiTi Aw",
      "Bowei Zou"
    ],
    "abstract": "Multilingual Large Language Models(MLLMs) demonstrate strong generalization across languages, yet they remain prone to hallucinations, especially in low-resource languages, due to training data imbalances. These hallucinations, which include inaccurate or fabricated outputs, are particularly problematic in domain-specific generation tasks (Chataigner et al., 2024). To address this challenge, we propose CCL-XCoT(Curriculum-based Contrastive Learning-based Cross-lingual Chain-of-Thought), a two-stage fine-tuning framework for mitigating hallucination in MLLMs. Our approach first enhances cross-lingual semantic alignment through curriculum-based contrastive learning combined with next-token prediction during continued pre-training. Building on this foundation, we then introduce a cross-lingual Chain-of-Thought (XCoT) prompting strategy during instruction fine-tuning, which guides the model to reason in a high-resource language before generating answers in the target low-resource language. Experimental results show that CCL-XCoT reduces hallucination rates by up to 62% and substantially improves factual knowledge transfer across language pairs, without relying on external retrieval or multi-model ensembles.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.14239v1",
    "published_date": "2025-07-17 14:25:24 UTC",
    "updated_date": "2025-07-17 14:25:24 UTC"
  },
  {
    "arxiv_id": "2507.13158v1",
    "title": "Inverse Reinforcement Learning Meets Large Language Model Post-Training: Basics, Advances, and Opportunities",
    "authors": [
      "Hao Sun",
      "Mihaela van der Schaar"
    ],
    "abstract": "In the era of Large Language Models (LLMs), alignment has emerged as a fundamental yet challenging problem in the pursuit of more reliable, controllable, and capable machine intelligence. The recent success of reasoning models and conversational AI systems has underscored the critical role of reinforcement learning (RL) in enhancing these systems, driving increased research interest at the intersection of RL and LLM alignment. This paper provides a comprehensive review of recent advances in LLM alignment through the lens of inverse reinforcement learning (IRL), emphasizing the distinctions between RL techniques employed in LLM alignment and those in conventional RL tasks. In particular, we highlight the necessity of constructing neural reward models from human data and discuss the formal and practical implications of this paradigm shift. We begin by introducing fundamental concepts in RL to provide a foundation for readers unfamiliar with the field. We then examine recent advances in this research agenda, discussing key challenges and opportunities in conducting IRL for LLM alignment. Beyond methodological considerations, we explore practical aspects, including datasets, benchmarks, evaluation metrics, infrastructure, and computationally efficient training and inference techniques. Finally, we draw insights from the literature on sparse-reward RL to identify open questions and potential research directions. By synthesizing findings from diverse studies, we aim to provide a structured and critical overview of the field, highlight unresolved challenges, and outline promising future directions for improving LLM alignment through RL and IRL techniques.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.13158v1",
    "published_date": "2025-07-17 14:22:24 UTC",
    "updated_date": "2025-07-17 14:22:24 UTC"
  },
  {
    "arxiv_id": "2507.13420v2",
    "title": "AI-ming backwards: Vanishing archaeological landscapes in Mesopotamia and automatic detection of sites on CORONA imagery",
    "authors": [
      "Alessandro Pistola",
      "Valentina Orru'",
      "Nicolo' Marchetti",
      "Marco Roccetti"
    ],
    "abstract": "By upgrading an existing deep learning model with the knowledge provided by one of the oldest sets of grayscale satellite imagery, known as CORONA, we improved the AI model attitude towards the automatic identification of archaeological sites in an environment which has been completely transformed in the last five decades, including the complete destruction of many of those same sites. The initial Bing based convolutional network model was retrained using CORONA satellite imagery for the district of Abu Ghraib, west of Baghdad, central Mesopotamian floodplain. The results were twofold and surprising. First, the detection precision obtained on the area of interest increased sensibly: in particular, the Intersection over Union (IoU) values, at the image segmentation level, surpassed 85 percent, while the general accuracy in detecting archeological sites reached 90 percent. Second, our retrained model allowed the identification of four new sites of archaeological interest (confirmed through field verification), previously not identified by archaeologists with traditional techniques. This has confirmed the efficacy of using AI techniques and the CORONA imagery from the 1960 to discover archaeological sites currently no longer visible, a concrete breakthrough with significant consequences for the study of landscapes with vanishing archaeological evidence induced by anthropization",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "25 pages, 9 Figures",
    "pdf_url": "https://arxiv.org/pdf/2507.13420v2",
    "published_date": "2025-07-17 14:21:50 UTC",
    "updated_date": "2025-07-29 07:01:34 UTC"
  },
  {
    "arxiv_id": "2507.13152v3",
    "title": "SE-VLN: A Self-Evolving Vision-Language Navigation Framework Based on Multimodal Large Language Models",
    "authors": [
      "Xiangyu Dong",
      "Haoran Zhao",
      "Jiang Gao",
      "Haozhou Li",
      "Xiaoguang Ma",
      "Yaoming Zhou",
      "Fuhai Chen",
      "Juan Liu"
    ],
    "abstract": "Recent advances in vision-language navigation (VLN) were mainly attributed to emerging large language models (LLMs). These methods exhibited excellent generalization capabilities in instruction understanding and task reasoning. However, they were constrained by the fixed knowledge bases and reasoning abilities of LLMs, preventing fully incorporating experiential knowledge and thus resulting in a lack of efficient evolutionary capacity. To address this, we drew inspiration from the evolution capabilities of natural agents, and proposed a self-evolving VLN framework (SE-VLN) to endow VLN agents with the ability to continuously evolve during testing. To the best of our knowledge, it was the first time that an multimodal LLM-powered self-evolving VLN framework was proposed. Specifically, SE-VLN comprised three core modules, i.e., a hierarchical memory module to transfer successful and failure cases into reusable knowledge, a retrieval-augmented thought-based reasoning module to retrieve experience and enable multi-step decision-making, and a reflection module to realize continual evolution. Comprehensive tests illustrated that the SE-VLN achieved navigation success rates of 57% and 35.2% in unseen environments, representing absolute performance improvements of 23.9% and 15.0% over current state-of-the-art methods on R2R and REVERSE datasets, respectively. Moreover, the SE-VLN showed performance improvement with increasing experience repository, elucidating its great potential as a self-evolving agent framework for VLN.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.13152v3",
    "published_date": "2025-07-17 14:13:50 UTC",
    "updated_date": "2025-08-26 08:52:15 UTC"
  },
  {
    "arxiv_id": "2507.13145v1",
    "title": "DINO-VO: A Feature-based Visual Odometry Leveraging a Visual Foundation Model",
    "authors": [
      "Maulana Bisyir Azhari",
      "David Hyunchul Shim"
    ],
    "abstract": "Learning-based monocular visual odometry (VO) poses robustness, generalization, and efficiency challenges in robotics. Recent advances in visual foundation models, such as DINOv2, have improved robustness and generalization in various vision tasks, yet their integration in VO remains limited due to coarse feature granularity. In this paper, we present DINO-VO, a feature-based VO system leveraging DINOv2 visual foundation model for its sparse feature matching. To address the integration challenge, we propose a salient keypoints detector tailored to DINOv2's coarse features. Furthermore, we complement DINOv2's robust-semantic features with fine-grained geometric features, resulting in more localizable representations. Finally, a transformer-based matcher and differentiable pose estimation layer enable precise camera motion estimation by learning good matches. Against prior detector-descriptor networks like SuperPoint, DINO-VO demonstrates greater robustness in challenging environments. Furthermore, we show superior accuracy and generalization of the proposed feature descriptors against standalone DINOv2 coarse features. DINO-VO outperforms prior frame-to-frame VO methods on the TartanAir and KITTI datasets and is competitive on EuRoC dataset, while running efficiently at 72 FPS with less than 1GB of memory usage on a single GPU. Moreover, it performs competitively against Visual SLAM systems on outdoor driving scenarios, showcasing its generalization capabilities.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 6 figures. Accepted for publication in IEEE Robotics and Automation Letters (RA-L), July 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.13145v1",
    "published_date": "2025-07-17 14:09:34 UTC",
    "updated_date": "2025-07-17 14:09:34 UTC"
  },
  {
    "arxiv_id": "2507.13142v4",
    "title": "From Roots to Rewards: Dynamic Tree Reasoning with Reinforcement Learning",
    "authors": [
      "Ahmed Bahloul",
      "Simon Malberg"
    ],
    "abstract": "Modern language models address complex questions through chain-of-thought (CoT) reasoning (Wei et al., 2023) and retrieval augmentation (Lewis et al., 2021), yet struggle with error propagation and knowledge integration. Tree-structured reasoning methods, particularly the Probabilistic Tree-of-Thought (ProbTree)(Cao et al., 2023) framework, mitigate these issues by decomposing questions into hierarchical structures and selecting answers through confidence-weighted aggregation of parametric and retrieved knowledge (Yao et al., 2023). However, ProbTree's static implementation introduces two key limitations: (1) the reasoning tree is fixed during the initial construction phase, preventing dynamic adaptation to intermediate results, and (2) each node requires exhaustive evaluation of all possible solution strategies, creating computational inefficiency. We present a dynamic reinforcement learning (Sutton and Barto, 2018) framework that transforms tree-based reasoning into an adaptive process. Our approach incrementally constructs the reasoning tree based on real-time confidence estimates, while learning optimal policies for action selection (decomposition, retrieval, or aggregation). This maintains ProbTree's probabilistic rigor while improving both solution quality and computational efficiency through selective expansion and focused resource allocation. The work establishes a new paradigm for treestructured reasoning that balances the reliability of probabilistic frameworks with the flexibility required for real-world question answering systems. Code available at: https://github.com/ahmedehabb/From-Roots-to-Rewards-Dynamic-Tree-Reasoning-with-RL",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "RARA Workshop @ ICDM 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.13142v4",
    "published_date": "2025-07-17 14:06:19 UTC",
    "updated_date": "2025-09-26 13:43:14 UTC"
  },
  {
    "arxiv_id": "2507.13112v1",
    "title": "Prediction of Highway Traffic Flow Based on Artificial Intelligence Algorithms Using California Traffic Data",
    "authors": [
      "Junseong Lee",
      "Jaegwan Cho",
      "Yoonju Cho",
      "Seoyoon Choi",
      "Yejin Shin"
    ],
    "abstract": "The study \"Prediction of Highway Traffic Flow Based on Artificial Intelligence Algorithms Using California Traffic Data\" presents a machine learning-based traffic flow prediction model to address global traffic congestion issues. The research utilized 30-second interval traffic data from California Highway 78 over a five-month period from July to November 2022, analyzing a 7.24 km westbound section connecting \"Melrose Dr\" and \"El-Camino Real\" in the San Diego area. The study employed Multiple Linear Regression (MLR) and Random Forest (RF) algorithms, analyzing data collection intervals ranging from 30 seconds to 15 minutes. Using R^2, MAE, and RMSE as performance metrics, the analysis revealed that both MLR and RF models performed optimally with 10-minute data collection intervals. These findings are expected to contribute to future traffic congestion solutions and efficient traffic management.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.13112v1",
    "published_date": "2025-07-17 13:27:38 UTC",
    "updated_date": "2025-07-17 13:27:38 UTC"
  },
  {
    "arxiv_id": "2507.14238v1",
    "title": "Language Models Change Facts Based on the Way You Talk",
    "authors": [
      "Matthew Kearney",
      "Reuben Binns",
      "Yarin Gal"
    ],
    "abstract": "Large language models (LLMs) are increasingly being used in user-facing applications, from providing medical consultations to job interview advice. Recent research suggests that these models are becoming increasingly proficient at inferring identity information about the author of a piece of text from linguistic patterns as subtle as the choice of a few words. However, little is known about how LLMs use this information in their decision-making in real-world applications. We perform the first comprehensive analysis of how identity markers present in a user's writing bias LLM responses across five different high-stakes LLM applications in the domains of medicine, law, politics, government benefits, and job salaries. We find that LLMs are extremely sensitive to markers of identity in user queries and that race, gender, and age consistently influence LLM responses in these applications. For instance, when providing medical advice, we find that models apply different standards of care to individuals of different ethnicities for the same symptoms; we find that LLMs are more likely to alter answers to align with a conservative (liberal) political worldview when asked factual questions by older (younger) individuals; and that LLMs recommend lower salaries for non-White job applicants and higher salaries for women compared to men. Taken together, these biases mean that the use of off-the-shelf LLMs for these applications may cause harmful differences in medical care, foster wage gaps, and create different political factual realities for people of different identities. Beyond providing an analysis, we also provide new tools for evaluating how subtle encoding of identity in users' language choices impacts model decisions. Given the serious implications of these findings, we recommend that similar thorough assessments of LLM use in user-facing applications are conducted before future deployment.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.14238v1",
    "published_date": "2025-07-17 13:21:17 UTC",
    "updated_date": "2025-07-17 13:21:17 UTC"
  },
  {
    "arxiv_id": "2507.13097v1",
    "title": "GraspGen: A Diffusion-based Framework for 6-DOF Grasping with On-Generator Training",
    "authors": [
      "Adithyavairavan Murali",
      "Balakumar Sundaralingam",
      "Yu-Wei Chao",
      "Wentao Yuan",
      "Jun Yamada",
      "Mark Carlson",
      "Fabio Ramos",
      "Stan Birchfield",
      "Dieter Fox",
      "Clemens Eppner"
    ],
    "abstract": "Grasping is a fundamental robot skill, yet despite significant research advancements, learning-based 6-DOF grasping approaches are still not turnkey and struggle to generalize across different embodiments and in-the-wild settings. We build upon the recent success on modeling the object-centric grasp generation process as an iterative diffusion process. Our proposed framework, GraspGen, consists of a DiffusionTransformer architecture that enhances grasp generation, paired with an efficient discriminator to score and filter sampled grasps. We introduce a novel and performant on-generator training recipe for the discriminator. To scale GraspGen to both objects and grippers, we release a new simulated dataset consisting of over 53 million grasps. We demonstrate that GraspGen outperforms prior methods in simulations with singulated objects across different grippers, achieves state-of-the-art performance on the FetchBench grasping benchmark, and performs well on a real robot with noisy visual observations.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.13097v1",
    "published_date": "2025-07-17 13:09:28 UTC",
    "updated_date": "2025-07-17 13:09:28 UTC"
  },
  {
    "arxiv_id": "2507.13417v1",
    "title": "Soft-ECM: An extension of Evidential C-Means for complex data",
    "authors": [
      "Armel Soubeiga",
      "Thomas Guyet",
      "Violaine Antoine"
    ],
    "abstract": "Clustering based on belief functions has been gaining increasing attention in the machine learning community due to its ability to effectively represent uncertainty and/or imprecision. However, none of the existing algorithms can be applied to complex data, such as mixed data (numerical and categorical) or non-tabular data like time series. Indeed, these types of data are, in general, not represented in a Euclidean space and the aforementioned algorithms make use of the properties of such spaces, in particular for the construction of barycenters. In this paper, we reformulate the Evidential C-Means (ECM) problem for clustering complex data. We propose a new algorithm, Soft-ECM, which consistently positions the centroids of imprecise clusters requiring only a semi-metric. Our experiments show that Soft-ECM present results comparable to conventional fuzzy clustering approaches on numerical data, and we demonstrate its ability to handle mixed data and its benefits when combining fuzzy clustering with semi-metrics such as DTW for time series data.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.13417v1",
    "published_date": "2025-07-17 13:00:22 UTC",
    "updated_date": "2025-07-17 13:00:22 UTC"
  },
  {
    "arxiv_id": "2507.13090v1",
    "title": "MUPAX: Multidimensional Problem Agnostic eXplainable AI",
    "authors": [
      "Vincenzo Dentamaro",
      "Felice Franchini",
      "Giuseppe Pirlo",
      "Irina Voiculescu"
    ],
    "abstract": "Robust XAI techniques should ideally be simultaneously deterministic, model agnostic, and guaranteed to converge. We propose MULTIDIMENSIONAL PROBLEM AGNOSTIC EXPLAINABLE AI (MUPAX), a deterministic, model agnostic explainability technique, with guaranteed convergency. MUPAX measure theoretic formulation gives principled feature importance attribution through structured perturbation analysis that discovers inherent input patterns and eliminates spurious relationships. We evaluate MUPAX on an extensive range of data modalities and tasks: audio classification (1D), image classification (2D), volumetric medical image analysis (3D), and anatomical landmark detection, demonstrating dimension agnostic effectiveness. The rigorous convergence guarantees extend to any loss function and arbitrary dimensions, making MUPAX applicable to virtually any problem context for AI. By contrast with other XAI methods that typically decrease performance when masking, MUPAX not only preserves but actually enhances model accuracy by capturing only the most important patterns of the original data. Extensive benchmarking against the state of the XAI art demonstrates MUPAX ability to generate precise, consistent and understandable explanations, a crucial step towards explainable and trustworthy AI systems. The source code will be released upon publication.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.13090v1",
    "published_date": "2025-07-17 12:59:27 UTC",
    "updated_date": "2025-07-17 12:59:27 UTC"
  },
  {
    "arxiv_id": "2507.13416v1",
    "title": "Single- to multi-fidelity history-dependent learning with uncertainty quantification and disentanglement: application to data-driven constitutive modeling",
    "authors": [
      "Jiaxiang Yi",
      "Bernardo P. Ferreira",
      "Miguel A. Bessa"
    ],
    "abstract": "Data-driven learning is generalized to consider history-dependent multi-fidelity data, while quantifying epistemic uncertainty and disentangling it from data noise (aleatoric uncertainty). This generalization is hierarchical and adapts to different learning scenarios: from training the simplest single-fidelity deterministic neural networks up to the proposed multi-fidelity variance estimation Bayesian recurrent neural networks. The versatility and generality of the proposed methodology are demonstrated by applying it to different data-driven constitutive modeling scenarios that include multiple fidelities with and without aleatoric uncertainty (noise). The method accurately predicts the response and quantifies model error while also discovering the noise distribution (when present). This opens opportunities for future real-world applications in diverse scientific and engineering domains; especially, the most challenging cases involving design and analysis under uncertainty.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "40 pages, 32 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.13416v1",
    "published_date": "2025-07-17 12:45:10 UTC",
    "updated_date": "2025-07-17 12:45:10 UTC"
  },
  {
    "arxiv_id": "2507.13415v1",
    "title": "SEER: Semantic Enhancement and Emotional Reasoning Network for Multimodal Fake News Detection",
    "authors": [
      "Peican Zhu",
      "Yubo Jing",
      "Le Cheng",
      "Bin Chen",
      "Xiaodong Cui",
      "Lianwei Wu",
      "Keke Tang"
    ],
    "abstract": "Previous studies on multimodal fake news detection mainly focus on the alignment and integration of cross-modal features, as well as the application of text-image consistency. However, they overlook the semantic enhancement effects of large multimodal models and pay little attention to the emotional features of news. In addition, people find that fake news is more inclined to contain negative emotions than real ones. Therefore, we propose a novel Semantic Enhancement and Emotional Reasoning (SEER) Network for multimodal fake news detection. We generate summarized captions for image semantic understanding and utilize the products of large multimodal models for semantic enhancement. Inspired by the perceived relationship between news authenticity and emotional tendencies, we propose an expert emotional reasoning module that simulates real-life scenarios to optimize emotional features and infer the authenticity of news. Extensive experiments on two real-world datasets demonstrate the superiority of our SEER over state-of-the-art baselines.",
    "categories": [
      "cs.MM",
      "cs.AI"
    ],
    "primary_category": "cs.MM",
    "comment": "Accepted by SMC 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.13415v1",
    "published_date": "2025-07-17 12:33:45 UTC",
    "updated_date": "2025-07-17 12:33:45 UTC"
  },
  {
    "arxiv_id": "2507.14237v1",
    "title": "U-DREAM: Unsupervised Dereverberation guided by a Reverberation Model",
    "authors": [
      "Louis Bahrman",
      "Mathieu Fontaine",
      "Gaël Richard"
    ],
    "abstract": "This paper explores the outcome of training state-ofthe-art dereverberation models with supervision settings ranging from weakly-supervised to fully unsupervised, relying solely on reverberant signals and an acoustic model for training. Most of the existing deep learning approaches typically require paired dry and reverberant data, which are difficult to obtain in practice. We develop instead a sequential learning strategy motivated by a bayesian formulation of the dereverberation problem, wherein acoustic parameters and dry signals are estimated from reverberant inputs using deep neural networks, guided by a reverberation matching loss. Our most data-efficient variant requires only 100 reverberation-parameter-labelled samples to outperform an unsupervised baseline, demonstrating the effectiveness and practicality of the proposed method in low-resource scenarios.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS",
      "eess.SP"
    ],
    "primary_category": "cs.SD",
    "comment": "Submitted to IEEE Transactions on Audio, Speech and Language Processing (TASLPRO)",
    "pdf_url": "https://arxiv.org/pdf/2507.14237v1",
    "published_date": "2025-07-17 12:26:18 UTC",
    "updated_date": "2025-07-17 12:26:18 UTC"
  },
  {
    "arxiv_id": "2507.13414v2",
    "title": "Gauge Flow Models",
    "authors": [
      "Alexander Strunk",
      "Roland Assam"
    ],
    "abstract": "This paper introduces Gauge Flow Models, a novel class of Generative Flow Models. These models incorporate a learnable Gauge Field within the Flow Ordinary Differential Equation (ODE). A comprehensive mathematical framework for these models, detailing their construction and properties, is provided. Experiments using Flow Matching on Gaussian Mixture Models demonstrate that Gauge Flow Models yields significantly better performance than traditional Flow Models of comparable or even larger size. Additionally, unpublished research indicates a potential for enhanced performance across a broader range of generative tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.DG"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.13414v2",
    "published_date": "2025-07-17 12:24:54 UTC",
    "updated_date": "2025-08-06 08:31:56 UTC"
  },
  {
    "arxiv_id": "2507.21118v1",
    "title": "Failure Risk Prediction in a MOOC: A Multivariate Time Series Analysis Approach",
    "authors": [
      "Anass El Ayady",
      "Maxime Devanne",
      "Germain Forestier",
      "Nour El Mawas"
    ],
    "abstract": "MOOCs offer free and open access to a wide audience, but completion rates remain low, often due to a lack of personalized content. To address this issue, it is essential to predict learner performance in order to provide tailored feedback. Behavioral traces-such as clicks and events-can be analyzed as time series to anticipate learners' outcomes. This work compares multivariate time series classification methods to identify at-risk learners at different stages of the course (after 5, 10 weeks, etc.). The experimental evaluation, conducted on the Open University Learning Analytics Dataset (OULAD), focuses on three courses: two in STEM and one in SHS. Preliminary results show that the evaluated approaches are promising for predicting learner failure in MOOCs. The analysis also suggests that prediction accuracy is influenced by the amount of recorded interactions, highlighting the importance of rich and diverse behavioral data.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "in French language, Environnements Informatiques pour l'Apprentissage Humain 2025, Jun 2025, Villeneuve d'Ascq (Lille), France",
    "pdf_url": "https://arxiv.org/pdf/2507.21118v1",
    "published_date": "2025-07-17 12:22:10 UTC",
    "updated_date": "2025-07-17 12:22:10 UTC"
  },
  {
    "arxiv_id": "2507.15876v1",
    "title": "Re-evaluating Short- and Long-Term Trend Factors in CTA Replication: A Bayesian Graphical Approach",
    "authors": [
      "Eric Benhamou",
      "Jean-Jacques Ohana",
      "Alban Etienne",
      "Béatrice Guez",
      "Ethan Setrouk",
      "Thomas Jacquot"
    ],
    "abstract": "Commodity Trading Advisors (CTAs) have historically relied on trend-following rules that operate on vastly different horizons from long-term breakouts that capture major directional moves to short-term momentum signals that thrive in fast-moving markets. Despite a large body of work on trend following, the relative merits and interactions of short-versus long-term trend systems remain controversial. This paper adds to the debate by (i) dynamically decomposing CTA returns into short-term trend, long-term trend and market beta factors using a Bayesian graphical model, and (ii) showing how the blend of horizons shapes the strategy's risk-adjusted performance.",
    "categories": [
      "cs.AI",
      "q-fin.PR",
      "q-fin.ST",
      "q-fin.TR"
    ],
    "primary_category": "cs.AI",
    "comment": "13 pages",
    "pdf_url": "https://arxiv.org/pdf/2507.15876v1",
    "published_date": "2025-07-17 12:09:29 UTC",
    "updated_date": "2025-07-17 12:09:29 UTC"
  },
  {
    "arxiv_id": "2507.13019v2",
    "title": "Rethinking the Embodied Gap in Vision-and-Language Navigation: A Holistic Study of Physical and Visual Disparities",
    "authors": [
      "Liuyi Wang",
      "Xinyuan Xia",
      "Hui Zhao",
      "Hanqing Wang",
      "Tai Wang",
      "Yilun Chen",
      "Chengju Liu",
      "Qijun Chen",
      "Jiangmiao Pang"
    ],
    "abstract": "Recent Vision-and-Language Navigation (VLN) advancements are promising, but their idealized assumptions about robot movement and control fail to reflect physically embodied deployment challenges. To bridge this gap, we introduce VLN-PE, a physically realistic VLN platform supporting humanoid, quadruped, and wheeled robots. For the first time, we systematically evaluate several ego-centric VLN methods in physical robotic settings across different technical pipelines, including classification models for single-step discrete action prediction, a diffusion model for dense waypoint prediction, and a train-free, map-based large language model (LLM) integrated with path planning. Our results reveal significant performance degradation due to limited robot observation space, environmental lighting variations, and physical challenges like collisions and falls. This also exposes locomotion constraints for legged robots in complex environments. VLN-PE is highly extensible, allowing seamless integration of new scenes beyond MP3D, thereby enabling more comprehensive VLN evaluation. Despite the weak generalization of current models in physical deployment, VLN-PE provides a new pathway for improving cross-embodiment's overall adaptability. We hope our findings and tools inspire the community to rethink VLN limitations and advance robust, practical VLN models. The code is available at https://crystalsixone.github.io/vln_pe.github.io/.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted by ICCV 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.13019v2",
    "published_date": "2025-07-17 11:46:00 UTC",
    "updated_date": "2025-09-26 07:50:31 UTC"
  },
  {
    "arxiv_id": "2507.13007v1",
    "title": "Exploiting Constraint Reasoning to Build Graphical Explanations for Mixed-Integer Linear Programming",
    "authors": [
      "Roger Xavier Lera-Leri",
      "Filippo Bistaffa",
      "Athina Georgara",
      "Juan Antonio Rodriguez-Aguilar"
    ],
    "abstract": "Following the recent push for trustworthy AI, there has been an increasing interest in developing contrastive explanation techniques for optimisation, especially concerning the solution of specific decision-making processes formalised as MILPs. Along these lines, we propose X-MILP, a domain-agnostic approach for building contrastive explanations for MILPs based on constraint reasoning techniques. First, we show how to encode the queries a user makes about the solution of an MILP problem as additional constraints. Then, we determine the reasons that constitute the answer to the user's query by computing the Irreducible Infeasible Subsystem (IIS) of the newly obtained set of constraints. Finally, we represent our explanation as a \"graph of reasons\" constructed from the IIS, which helps the user understand the structure among the reasons that answer their query. We test our method on instances of well-known optimisation problems to evaluate the empirical hardness of computing explanations.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "To appear in Lecture Notes in Artificial Intelligence",
    "pdf_url": "https://arxiv.org/pdf/2507.13007v1",
    "published_date": "2025-07-17 11:25:33 UTC",
    "updated_date": "2025-07-17 11:25:33 UTC"
  },
  {
    "arxiv_id": "2507.13001v1",
    "title": "SMART: Relation-Aware Learning of Geometric Representations for Knowledge Graphs",
    "authors": [
      "Kossi Amouzouvi",
      "Bowen Song",
      "Andrea Coletta",
      "Luigi Bellomarini",
      "Jens Lehmann",
      "Sahar Vahdati"
    ],
    "abstract": "Knowledge graph representation learning approaches provide a mapping between symbolic knowledge in the form of triples in a knowledge graph (KG) and their feature vectors. Knowledge graph embedding (KGE) models often represent relations in a KG as geometric transformations. Most state-of-the-art (SOTA) KGE models are derived from elementary geometric transformations (EGTs), such as translation, scaling, rotation, and reflection, or their combinations. These geometric transformations enable the models to effectively preserve specific structural and relational patterns of the KG. However, the current use of EGTs by KGEs remains insufficient without considering relation-specific transformations. Although recent models attempted to address this problem by ensembling SOTA baseline models in different ways, only a single or composite version of geometric transformations are used by such baselines to represent all the relations. In this paper, we propose a framework that evaluates how well each relation fits with different geometric transformations. Based on this ranking, the model can: (1) assign the best-matching transformation to each relation, or (2) use majority voting to choose one transformation type to apply across all relations. That is, the model learns a single relation-specific EGT in low dimensional vector space through an attention mechanism. Furthermore, we use the correlation between relations and EGTs, which are learned in a low dimension, for relation embeddings in a high dimensional vector space. The effectiveness of our models is demonstrated through comprehensive evaluations on three benchmark KGs as well as a real-world financial KG, witnessing a performance comparable to leading models",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.13001v1",
    "published_date": "2025-07-17 11:18:08 UTC",
    "updated_date": "2025-07-17 11:18:08 UTC"
  },
  {
    "arxiv_id": "2507.12990v1",
    "title": "Teach Old SAEs New Domain Tricks with Boosting",
    "authors": [
      "Nikita Koriagin",
      "Yaroslav Aksenov",
      "Daniil Laptev",
      "Gleb Gerasimov",
      "Nikita Balagansky",
      "Daniil Gavrilov"
    ],
    "abstract": "Sparse Autoencoders have emerged as powerful tools for interpreting the internal representations of Large Language Models, yet they often fail to capture domain-specific features not prevalent in their training corpora. This paper introduces a residual learning approach that addresses this feature blindness without requiring complete retraining. We propose training a secondary SAE specifically to model the reconstruction error of a pretrained SAE on domain-specific texts, effectively capturing features missed by the primary model. By summing the outputs of both models during inference, we demonstrate significant improvements in both LLM cross-entropy and explained variance metrics across multiple specialized domains. Our experiments show that this method efficiently incorporates new domain knowledge into existing SAEs while maintaining their performance on general tasks. This approach enables researchers to selectively enhance SAE interpretability for specific domains of interest, opening new possibilities for targeted mechanistic interpretability of LLMs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.12990v1",
    "published_date": "2025-07-17 10:57:49 UTC",
    "updated_date": "2025-07-17 10:57:49 UTC"
  },
  {
    "arxiv_id": "2507.12989v1",
    "title": "A Translation of Probabilistic Event Calculus into Markov Decision Processes",
    "authors": [
      "Lyris Xu",
      "Fabio Aurelio D'Asaro",
      "Luke Dickens"
    ],
    "abstract": "Probabilistic Event Calculus (PEC) is a logical framework for reasoning about actions and their effects in uncertain environments, which enables the representation of probabilistic narratives and computation of temporal projections. The PEC formalism offers significant advantages in interpretability and expressiveness for narrative reasoning. However, it lacks mechanisms for goal-directed reasoning. This paper bridges this gap by developing a formal translation of PEC domains into Markov Decision Processes (MDPs), introducing the concept of \"action-taking situations\" to preserve PEC's flexible action semantics. The resulting PEC-MDP formalism enables the extensive collection of algorithms and theoretical tools developed for MDPs to be applied to PEC's interpretable narrative domains. We demonstrate how the translation supports both temporal reasoning tasks and objective-driven planning, with methods for mapping learned policies back into human-readable PEC representations, maintaining interpretability while extending PEC's capabilities.",
    "categories": [
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.12989v1",
    "published_date": "2025-07-17 10:56:22 UTC",
    "updated_date": "2025-07-17 10:56:22 UTC"
  },
  {
    "arxiv_id": "2508.02680v1",
    "title": "AnnoSense: A Framework for Physiological Emotion Data Collection in Everyday Settings for AI",
    "authors": [
      "Pragya Singh",
      "Ankush Gupta",
      "Mohan Kumar",
      "Pushpendra Singh"
    ],
    "abstract": "Emotional and mental well-being are vital components of quality of life, and with the rise of smart devices like smartphones, wearables, and artificial intelligence (AI), new opportunities for monitoring emotions in everyday settings have emerged. However, for AI algorithms to be effective, they require high-quality data and accurate annotations. As the focus shifts towards collecting emotion data in real-world environments to capture more authentic emotional experiences, the process of gathering emotion annotations has become increasingly complex. This work explores the challenges of everyday emotion data collection from the perspectives of key stakeholders. We collected 75 survey responses, performed 32 interviews with the public, and 3 focus group discussions (FGDs) with 12 mental health professionals. The insights gained from a total of 119 stakeholders informed the development of our framework, AnnoSense, designed to support everyday emotion data collection for AI. This framework was then evaluated by 25 emotion AI experts for its clarity, usefulness, and adaptability. Lastly, we discuss the potential next steps and implications of AnnoSense for future research in emotion AI, highlighting its potential to enhance the collection and analysis of emotion data in real-world contexts.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "To be published in IMWUT, September 2025",
    "pdf_url": "https://arxiv.org/pdf/2508.02680v1",
    "published_date": "2025-07-17 10:54:39 UTC",
    "updated_date": "2025-07-17 10:54:39 UTC"
  },
  {
    "arxiv_id": "2507.12981v1",
    "title": "MRT at IberLEF-2025 PRESTA Task: Maximizing Recovery from Tables with Multiple Steps",
    "authors": [
      "Maximiliano Hormazábal Lagos",
      "Álvaro Bueno Sáez",
      "Héctor Cerezo-Costas",
      "Pedro Alonso Doval",
      "Jorge Alcalde Vesteiro"
    ],
    "abstract": "This paper presents our approach for the IberLEF 2025 Task PRESTA: Preguntas y Respuestas sobre Tablas en Español (Questions and Answers about Tables in Spanish). Our solution obtains answers to the questions by implementing Python code generation with LLMs that is used to filter and process the table. This solution evolves from the MRT implementation for the Semeval 2025 related task. The process consists of multiple steps: analyzing and understanding the content of the table, selecting the useful columns, generating instructions in natural language, translating these instructions to code, running it, and handling potential errors or exceptions. These steps use open-source LLMs and fine-grained optimized prompts for each step. With this approach, we achieved an accuracy score of 85\\% in the task.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted as an official challenge paper in the PRESTA: Questions and Answers over Tabular Data shared task at IberLEF 2025, colocated with the 41st SEPLN Conference in Zaragoza, Spain",
    "pdf_url": "https://arxiv.org/pdf/2507.12981v1",
    "published_date": "2025-07-17 10:33:36 UTC",
    "updated_date": "2025-07-17 10:33:36 UTC"
  },
  {
    "arxiv_id": "2507.12979v3",
    "title": "A Distributed Generative AI Approach for Heterogeneous Multi-Domain Environments under Data Sharing constraints",
    "authors": [
      "Youssef Tawfilis",
      "Hossam Amer",
      "Minar El-Aasser",
      "Tallal Elshabrawy"
    ],
    "abstract": "Federated Learning has gained attention for its ability to enable multiple nodes to collaboratively train machine learning models without sharing raw data. At the same time, Generative AI -- particularly Generative Adversarial Networks (GANs) -- have achieved remarkable success across a wide range of domains, such as healthcare, security, and Image Generation. However, training generative models typically requires large datasets and significant computational resources, which are often unavailable in real-world settings. Acquiring such resources can be costly and inefficient, especially when many underutilized devices -- such as IoT devices and edge devices -- with varying capabilities remain idle. Moreover, obtaining large datasets is challenging due to privacy concerns and copyright restrictions, as most devices are unwilling to share their data. To address these challenges, we propose a novel approach for decentralized GAN training that enables utilizing distributed data and underutilized, low-capability devices while not sharing data in its raw form. Our approach is designed to tackle key challenges in decentralized environments, combining KLD-weighted Clustered Federated Learning to address the issues of data heterogeneity and multi-domain datasets, with Heterogeneous U-Shaped split learning to tackle the challenge of device heterogeneity under strict data sharing constraints -- ensuring that no labels or raw data, whether real or synthetic, are ever shared between nodes. Experiments show that our approach demonstrates significant improvements across key metrics, where it achieves an average 10% boost in classification metrics (up to 60% in multi-domain non-IID settings), 1.1x -- 3x higher image generation scores for the MNIST family datasets, and 2x -- 70x lower FID scores for higher resolution datasets. Find our code at https://distributed-gen-ai.github.io/huscf-gan.github.io/.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted and published in Transactions on Machine Learning Research (TMLR), 2026",
    "pdf_url": "https://arxiv.org/pdf/2507.12979v3",
    "published_date": "2025-07-17 10:31:31 UTC",
    "updated_date": "2026-01-16 18:20:30 UTC"
  },
  {
    "arxiv_id": "2507.12964v5",
    "title": "Demographic-aware fine-grained classification of pediatric wrist fractures",
    "authors": [
      "Ammar Ahmed",
      "Ali Shariq Imran",
      "Zenun Kastrati",
      "Sher Muhammad Daudpota"
    ],
    "abstract": "Wrist pathologies are frequently observed, particularly among children who constitute the majority of fracture cases. Computer vision presents a promising avenue, contingent upon the availability of extensive datasets, a notable challenge in medical imaging. Therefore, reliance solely on one modality, such as images, proves inadequate, especially in an era of diverse and plentiful data types. This study addresses the problem using a multifaceted approach: framing it as a fine-grained recognition task, fusing patient metadata with X-rays, and leveraging weights from a separate fine-grained dataset rather than from a coarse-grained dataset like ImageNet. Unlike prior work, this is the first application of metadata integration for wrist pathology recognition. Our results show that combining fine-grained transformer approach, fine-grained pre-training, and metadata integration improves diagnostic accuracy by 2% on small custom curated dataset and over 10% on a larger fracture dataset.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.12964v5",
    "published_date": "2025-07-17 10:03:57 UTC",
    "updated_date": "2025-09-04 16:55:10 UTC"
  },
  {
    "arxiv_id": "2507.12961v1",
    "title": "Improving Diagnostic Accuracy of Pigmented Skin Lesions With CNNs: an Application on the DermaMNIST Dataset",
    "authors": [
      "Nerma Kadric",
      "Amila Akagic",
      "Medina Kapo"
    ],
    "abstract": "Pigmented skin lesions represent localized areas of increased melanin and can indicate serious conditions like melanoma, a major contributor to skin cancer mortality. The MedMNIST v2 dataset, inspired by MNIST, was recently introduced to advance research in biomedical imaging and includes DermaMNIST, a dataset for classifying pigmented lesions based on the HAM10000 dataset. This study assesses ResNet-50 and EfficientNetV2L models for multi-class classification using DermaMNIST, employing transfer learning and various layer configurations. One configuration achieves results that match or surpass existing methods. This study suggests that convolutional neural networks (CNNs) can drive progress in biomedical image analysis, significantly enhancing diagnostic accuracy.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.12961v1",
    "published_date": "2025-07-17 10:00:07 UTC",
    "updated_date": "2025-07-17 10:00:07 UTC"
  },
  {
    "arxiv_id": "2507.12951v1",
    "title": "UniSLU: Unified Spoken Language Understanding from Heterogeneous Cross-Task Datasets",
    "authors": [
      "Zhichao Sheng",
      "Shilin Zhou",
      "Chen Gong",
      "Zhenghua Li"
    ],
    "abstract": "Spoken Language Understanding (SLU) plays a crucial role in speech-centric multimedia applications, enabling machines to comprehend spoken language in scenarios such as meetings, interviews, and customer service interactions. SLU encompasses multiple tasks, including Automatic Speech Recognition (ASR), spoken Named Entity Recognition (NER), and spoken Sentiment Analysis (SA). However, existing methods often rely on separate model architectures for individual tasks such as spoken NER and SA, which increases system complexity, limits cross-task interaction, and fails to fully exploit heterogeneous datasets available across tasks. To address these limitations, we propose UniSLU, a unified framework that jointly models multiple SLU tasks within a single architecture. Specifically, we propose a unified representation for diverse SLU tasks, enabling full utilization of heterogeneous datasets across multiple tasks. Built upon this representation, we propose a unified generative method that jointly models ASR, spoken NER, and SA tasks, enhancing task interactions and enabling seamless integration with large language models to harness their powerful generative capabilities. Extensive experiments on public SLU datasets demonstrate the effectiveness of our approach, achieving superior SLU performance compared to several benchmark methods, making it well-suited for real-world speech-based multimedia scenarios. We will release all code and models at github to facilitate future research.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.MM",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "13 pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.12951v1",
    "published_date": "2025-07-17 09:45:49 UTC",
    "updated_date": "2025-07-17 09:45:49 UTC"
  },
  {
    "arxiv_id": "2507.12935v1",
    "title": "MC$^2$A: Enabling Algorithm-Hardware Co-Design for Efficient Markov Chain Monte Carlo Acceleration",
    "authors": [
      "Shirui Zhao",
      "Jun Yin",
      "Lingyun Yao",
      "Martin Andraud",
      "Wannes Meert",
      "Marian Verhelst"
    ],
    "abstract": "An increasing number of applications are exploiting sampling-based algorithms for planning, optimization, and inference. The Markov Chain Monte Carlo (MCMC) algorithms form the computational backbone of this emerging branch of machine learning. Unfortunately, the high computational cost limits their feasibility for large-scale problems and real-world applications, and the existing MCMC acceleration solutions are either limited in hardware flexibility or fail to maintain efficiency at the system level across a variety of end-to-end applications. This paper introduces \\textbf{MC$^2$A}, an algorithm-hardware co-design framework, enabling efficient and flexible optimization for MCMC acceleration. Firstly, \\textbf{MC$^2$A} analyzes the MCMC workload diversity through an extension of the processor performance roofline model with a 3rd dimension to derive the optimal balance between the compute, sampling and memory parameters. Secondly, \\textbf{MC$^2$A} proposes a parametrized hardware accelerator architecture with flexible and efficient support of MCMC kernels with a pipeline of ISA-programmable tree-structured processing units, reconfigurable samplers and a crossbar interconnect to support irregular access. Thirdly, the core of \\textbf{MC$^2$A} is powered by a novel Gumbel sampler that eliminates exponential and normalization operations. In the end-to-end case study, \\textbf{MC$^2$A} achieves an overall {$307.6\\times$, $1.4\\times$, $2.0\\times$, $84.2\\times$} speedup compared to the CPU, GPU, TPU and state-of-the-art MCMC accelerator. Evaluated on various representative MCMC workloads, this work demonstrates and exploits the feasibility of general hardware acceleration to popularize MCMC-based solutions in diverse application domains.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages, 15 figures, IEEE journal paper",
    "pdf_url": "https://arxiv.org/pdf/2507.12935v1",
    "published_date": "2025-07-17 09:20:51 UTC",
    "updated_date": "2025-07-17 09:20:51 UTC"
  },
  {
    "arxiv_id": "2507.12933v1",
    "title": "DMQ: Dissecting Outliers of Diffusion Models for Post-Training Quantization",
    "authors": [
      "Dongyeun Lee",
      "Jiwan Hur",
      "Hyounguk Shon",
      "Jae Young Lee",
      "Junmo Kim"
    ],
    "abstract": "Diffusion models have achieved remarkable success in image generation but come with significant computational costs, posing challenges for deployment in resource-constrained environments. Recent post-training quantization (PTQ) methods have attempted to mitigate this issue by focusing on the iterative nature of diffusion models. However, these approaches often overlook outliers, leading to degraded performance at low bit-widths. In this paper, we propose a DMQ which combines Learned Equivalent Scaling (LES) and channel-wise Power-of-Two Scaling (PTS) to effectively address these challenges. Learned Equivalent Scaling optimizes channel-wise scaling factors to redistribute quantization difficulty between weights and activations, reducing overall quantization error. Recognizing that early denoising steps, despite having small quantization errors, crucially impact the final output due to error accumulation, we incorporate an adaptive timestep weighting scheme to prioritize these critical steps during learning. Furthermore, identifying that layers such as skip connections exhibit high inter-channel variance, we introduce channel-wise Power-of-Two Scaling for activations. To ensure robust selection of PTS factors even with small calibration set, we introduce a voting algorithm that enhances reliability. Extensive experiments demonstrate that our method significantly outperforms existing works, especially at low bit-widths such as W4A6 (4-bit weight, 6-bit activation) and W4A8, maintaining high image generation quality and model stability. The code is available at https://github.com/LeeDongYeun/dmq.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ICCV 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.12933v1",
    "published_date": "2025-07-17 09:15:29 UTC",
    "updated_date": "2025-07-17 09:15:29 UTC"
  },
  {
    "arxiv_id": "2507.12930v2",
    "title": "Making Language Model a Hierarchical Classifier",
    "authors": [
      "Yihong Wang",
      "Zhonglin Jiang",
      "Ningyuan Xi",
      "Yue Zhao",
      "Qingqing Gu",
      "Xiyuan Chen",
      "Hao Wu",
      "Sheng Xu",
      "Hange Zhou",
      "Yong Chen",
      "Luo Ji"
    ],
    "abstract": "Decoder-only language models, such as GPT and LLaMA, generally decode on the last layer. Motivated by human's hierarchical thinking capability, we propose that a hierarchical decoder architecture could be built with different layers decoding texts simultaneously. Due to limited time and computationally resources, we choose to adapt a pretrained language model into this form of hierarchical decoder. Language heads of the last layer are copied to different selected intermediate layers, and fine-tuned with different task inputs. By thorough experiments, we validate that these selective intermediate layers could be adapted to speak meaningful and reasonable contents, and this paradigm of hierarchical decoder can obtain state-of-the-art performances on multiple tasks such as hierarchical text classification, classification-guided generation, and hierarchical text generation. HdLM outperforms all baselines on WoS, DBpedia, ESconv, EmpatheticDialogues, and several cognitive tests. We also provide thorough theoretical analysis to validate the convergence and computational savings of our methodology. This study suggests the possibility of a generalized hierarchical reasoner, pretraining from scratch.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.12930v2",
    "published_date": "2025-07-17 09:09:53 UTC",
    "updated_date": "2025-09-28 14:57:03 UTC"
  },
  {
    "arxiv_id": "2507.15875v1",
    "title": "Differential Multimodal Transformers",
    "authors": [
      "Jerry Li",
      "Timothy Oh",
      "Joseph Hoang",
      "Vardhit Veeramachaneni"
    ],
    "abstract": "Small language models have gained significant popularity due to their efficiency and growing capabilities. However, incorporating additional modalities, such as vision, can exacerbate the challenge of limited context windows by introducing noise. Recent studies have highlighted that Transformer attention mechanisms often disproportionately focus on irrelevant contexts. In this work, we extend the Differential Attention mechanism, originally designed for text-only models, to the text-vision model PaliGemma. Our aim is to evaluate its ability to mitigate noisy information retrieval and reduce hallucinations. To this end, we fine-tuned the PaliGemma 3B model using LoRA, incorporating Differential Attention, and experimented with various parameter settings and configurations. We demonstrate that Differential Attention can be adapted and integrated into the fine-tuning of existing models to enhance noisy information retrieval and question-answering capabilities.",
    "categories": [
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.15875v1",
    "published_date": "2025-07-17 09:05:34 UTC",
    "updated_date": "2025-07-17 09:05:34 UTC"
  },
  {
    "arxiv_id": "2507.12916v1",
    "title": "Argus: Leveraging Multiview Images for Improved 3-D Scene Understanding With Large Language Models",
    "authors": [
      "Yifan Xu",
      "Chao Zhang",
      "Hanqi Jiang",
      "Xiaoyan Wang",
      "Ruifei Ma",
      "Yiwei Li",
      "Zihao Wu",
      "Zeju Li",
      "Xiangde Liu"
    ],
    "abstract": "Advancements in foundation models have made it possible to conduct applications in various downstream tasks. Especially, the new era has witnessed a remarkable capability to extend Large Language Models (LLMs) for tackling tasks of 3D scene understanding. Current methods rely heavily on 3D point clouds, but the 3D point cloud reconstruction of an indoor scene often results in information loss. Some textureless planes or repetitive patterns are prone to omission and manifest as voids within the reconstructed 3D point clouds. Besides, objects with complex structures tend to introduce distortion of details caused by misalignments between the captured images and the dense reconstructed point clouds. 2D multi-view images present visual consistency with 3D point clouds and provide more detailed representations of scene components, which can naturally compensate for these deficiencies. Based on these insights, we propose Argus, a novel 3D multimodal framework that leverages multi-view images for enhanced 3D scene understanding with LLMs. In general, Argus can be treated as a 3D Large Multimodal Foundation Model (3D-LMM) since it takes various modalities as input(text instructions, 2D multi-view images, and 3D point clouds) and expands the capability of LLMs to tackle 3D tasks. Argus involves fusing and integrating multi-view images and camera poses into view-as-scene features, which interact with the 3D features to create comprehensive and detailed 3D-aware scene embeddings. Our approach compensates for the information loss while reconstructing 3D point clouds and helps LLMs better understand the 3D world. Extensive experiments demonstrate that our method outperforms existing 3D-LMMs in various downstream tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by TNNLS2025",
    "pdf_url": "https://arxiv.org/pdf/2507.12916v1",
    "published_date": "2025-07-17 09:02:04 UTC",
    "updated_date": "2025-07-17 09:02:04 UTC"
  },
  {
    "arxiv_id": "2507.12904v1",
    "title": "An ultra-low-power CGRA for accelerating Transformers at the edge",
    "authors": [
      "Rohit Prasad"
    ],
    "abstract": "Transformers have revolutionized deep learning with applications in natural language processing, computer vision, and beyond. However, their computational demands make it challenging to deploy them on low-power edge devices. This paper introduces an ultra-low-power, Coarse-Grained Reconfigurable Array (CGRA) architecture specifically designed to accelerate General Matrix Multiplication (GEMM) operations in transformer models tailored for the energy and resource constraints of edge applications. The proposed architecture integrates a 4 x 4 array of Processing Elements (PEs) for efficient parallel computation and dedicated 4 x 2 Memory Operation Blocks (MOBs) for optimized LOAD/STORE operations, reducing memory bandwidth demands and enhancing data reuse. A switchless mesh torus interconnect network further minimizes power and latency by enabling direct communication between PEs and MOBs, eliminating the need for centralized switching. Through its heterogeneous array design and efficient dataflow, this CGRA architecture addresses the unique computational needs of transformers, offering a scalable pathway to deploy sophisticated machine learning models on edge devices.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.12904v1",
    "published_date": "2025-07-17 08:43:14 UTC",
    "updated_date": "2025-07-17 08:43:14 UTC"
  },
  {
    "arxiv_id": "2507.15874v1",
    "title": "Why Braking? Scenario Extraction and Reasoning Utilizing LLM",
    "authors": [
      "Yin Wu",
      "Daniel Slieter",
      "Vivek Subramanian",
      "Ahmed Abouelazm",
      "Robin Bohn",
      "J. Marius Zöllner"
    ],
    "abstract": "The growing number of ADAS-equipped vehicles has led to a dramatic increase in driving data, yet most of them capture routine driving behavior. Identifying and understanding safety-critical corner cases within this vast dataset remains a significant challenge. Braking events are particularly indicative of potentially hazardous situations, motivating the central question of our research: Why does a vehicle brake? Existing approaches primarily rely on rule-based heuristics to retrieve target scenarios using predefined condition filters. While effective in simple environments such as highways, these methods lack generalization in complex urban settings. In this paper, we propose a novel framework that leverages Large Language Model (LLM) for scenario understanding and reasoning. Our method bridges the gap between low-level numerical signals and natural language descriptions, enabling LLM to interpret and classify driving scenarios. We propose a dual-path scenario retrieval that supports both category-based search for known scenarios and embedding-based retrieval for unknown Out-of-Distribution (OOD) scenarios. To facilitate evaluation, we curate scenario annotations on the Argoverse 2 Sensor Dataset. Experimental results show that our method outperforms rule-based baselines and generalizes well to OOD scenarios.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.15874v1",
    "published_date": "2025-07-17 08:33:56 UTC",
    "updated_date": "2025-07-17 08:33:56 UTC"
  },
  {
    "arxiv_id": "2507.12898v4",
    "title": "Vidar: Embodied Video Diffusion Model for Generalist Manipulation",
    "authors": [
      "Yao Feng",
      "Hengkai Tan",
      "Xinyi Mao",
      "Chendong Xiang",
      "Guodong Liu",
      "Shuhe Huang",
      "Hang Su",
      "Jun Zhu"
    ],
    "abstract": "Scaling general-purpose manipulation to new robot embodiments remains challenging: each platform typically needs large, homogeneous demonstrations, and end-to-end pixel-to-action pipelines may degenerate under background and viewpoint shifts. Based on previous advances in video-based robot control, we present Vidar, consisting of an embodied video diffusion model as the generalizable prior and a masked inverse dynamics model (MIDM) as the adapter. We leverage a video diffusion model pre-trained at Internet scale, and further continuously pre-train it for the embodied domain using 750K multi-view trajectories collected from three real-world robot platforms. For this embodied pre-training, we introduce a unified observation space that jointly encodes robot, camera, task, and scene contexts. The MIDM module learns action-relevant pixel masks without dense labels, grounding the prior into the target embodiment's action space while suppressing distractors. With only 20 minutes of human demonstrations on an unseen robot (1% of typical data), Vidar outperforms state-of-the-art baselines and generalizes to unseen tasks, backgrounds, and camera layouts. Our results suggest a scalable recipe for \"one prior, many embodiments\": strong, inexpensive video priors together with minimal on-robot alignment.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.12898v4",
    "published_date": "2025-07-17 08:31:55 UTC",
    "updated_date": "2025-12-20 02:16:12 UTC"
  },
  {
    "arxiv_id": "2507.13411v1",
    "title": "Aligning Knowledge Graphs and Language Models for Factual Accuracy",
    "authors": [
      "Nur A Zarin Nishat",
      "Andrea Coletta",
      "Luigi Bellomarini",
      "Kossi Amouzouvi",
      "Jens Lehmann",
      "Sahar Vahdati"
    ],
    "abstract": "Large language models like GPT-4, Gemini, and Claude have transformed natural language processing (NLP) tasks such as question answering, dialogue generation, summarization, and so forth; yet their susceptibility to hallucination stands as one of the major challenges. Among numerous approaches to overcome this challenge, integration of Knowledge Graphs (KGs) into language models has emerged as a promising solution as it provides structured, reliable, domain-specific, and up-to-date external information to the language models. In this paper, we introduce ALIGNed-LLM, a simple yet effective approach to improve language models' factuality via a lean strategy to infuse KGs into the latent space of language models inspired by LLaVA where visual and textual information is infused. We use embeddings from a pre-trained Knowledge Graph Embedding (KGE) model, such as TransE, and a trainable projection layer to align entity and text embeddings. This alignment enables the language model to distinguish between similar entities improving factual grounding and reducing hallucination. We tested our approach on three popular questions-answering benchmark datasets alongside language models of varying sizes, showing significant improvement. Furthermore, we applied our approach to a real-world financial use case from a large central bank in Europe, which demands high accuracy and precision, demonstrating a substantial improvement of the LLM answers.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.13411v1",
    "published_date": "2025-07-17 08:15:50 UTC",
    "updated_date": "2025-07-17 08:15:50 UTC"
  },
  {
    "arxiv_id": "2507.12885v3",
    "title": "VAR-MATH: Probing True Mathematical Reasoning in LLMS via Symbolic Multi-Instance Benchmarks",
    "authors": [
      "Jian Yao",
      "Ran Cheng",
      "Kay Chen Tan"
    ],
    "abstract": "Recent advances in reinforcement learning (RL) have led to substantial improvements in the mathematical reasoning abilities of LLMs, as measured by standard benchmarks. Yet these gains often persist even when models are trained with flawed signals, such as random or inverted rewards. This raises a fundamental question: do such improvements reflect genuine reasoning, or are they merely artifacts of overfitting to benchmark-specific patterns? To answer this question, we adopt an evaluation-centric perspective and highlight two critical shortcomings in existing protocols. First, benchmark contamination arises because test problems are publicly available, thereby increasing the risk of data leakage. Second, evaluation fragility results from reliance on single-instance assessments, which are sensitive to stochastic outputs and fail to capture reasoning consistency. These limitations suggest the need for a new evaluation paradigm that can probe reasoning ability beyond memorization and one-off success. As response, we propose VAR-MATH, a symbolic evaluation framework that converts fixed numerical problems into parameterized templates and requires models to solve multiple instantiations of each. This design enforces consistency across structurally equivalent variants, mitigates contamination, and enhances robustness through bootstrapped metrics. We apply VAR-MATH to transform three popular benchmarks, AMC23, AIME24, and AIME25, into their symbolic counterparts, VAR-AMC23, VAR-AIME24, and VAR-AIME25. Experimental results show substantial performance drops for RL-trained models on these variabilized benchmarks, especially for smaller models, with average declines of 47.9\\% on AMC23, 58.8\\% on AIME24, and 72.9\\% on AIME25. These findings indicate that some existing RL methods rely on superficial heuristics and fail to generalize beyond specific numerical forms.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.12885v3",
    "published_date": "2025-07-17 08:10:55 UTC",
    "updated_date": "2026-01-05 05:39:03 UTC"
  },
  {
    "arxiv_id": "2507.12872v1",
    "title": "Manipulation Attacks by Misaligned AI: Risk Analysis and Safety Case Framework",
    "authors": [
      "Rishane Dassanayake",
      "Mario Demetroudi",
      "James Walpole",
      "Lindley Lentati",
      "Jason R. Brown",
      "Edward James Young"
    ],
    "abstract": "Frontier AI systems are rapidly advancing in their capabilities to persuade, deceive, and influence human behaviour, with current models already demonstrating human-level persuasion and strategic deception in specific contexts. Humans are often the weakest link in cybersecurity systems, and a misaligned AI system deployed internally within a frontier company may seek to undermine human oversight by manipulating employees. Despite this growing threat, manipulation attacks have received little attention, and no systematic framework exists for assessing and mitigating these risks. To address this, we provide a detailed explanation of why manipulation attacks are a significant threat and could lead to catastrophic outcomes. Additionally, we present a safety case framework for manipulation risk, structured around three core lines of argument: inability, control, and trustworthiness. For each argument, we specify evidence requirements, evaluation methodologies, and implementation considerations for direct application by AI companies. This paper provides the first systematic methodology for integrating manipulation risk into AI safety governance, offering AI companies a concrete foundation to assess and mitigate these threats before deployment.",
    "categories": [
      "cs.AI",
      "cs.CR",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "24 pages (14 pages main text, 4 pages bibliography, 6 pages appendices), 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.12872v1",
    "published_date": "2025-07-17 07:45:53 UTC",
    "updated_date": "2025-07-17 07:45:53 UTC"
  },
  {
    "arxiv_id": "2507.12871v3",
    "title": "Generative Multi-Target Cross-Domain Recommendation",
    "authors": [
      "Jinqiu Jin",
      "Yang Zhang",
      "Fuli Feng",
      "Xiangnan He"
    ],
    "abstract": "Recently, there has been a surge of interest in Multi-Target Cross-Domain Recommendation (MTCDR), which aims to enhance recommendation performance across multiple domains simultaneously. Existing MTCDR methods primarily rely on domain-shared entities (\\eg users or items) to fuse and transfer cross-domain knowledge, which may be unavailable in non-overlapped recommendation scenarios. Some studies model user preferences and item features as domain-sharable semantic representations, which can be utilized to tackle the MTCDR task. Nevertheless, they often require extensive auxiliary data for pre-training. Developing more effective solutions for MTCDR remains an important area for further exploration.\n  Inspired by recent advancements in generative recommendation, this paper introduces GMC, a generative paradigm-based approach for multi-target cross-domain recommendation. The core idea of GMC is to leverage semantically quantized discrete item identifiers as a medium for integrating multi-domain knowledge within a unified generative model. GMC first employs an item tokenizer to generate domain-shared semantic identifiers for each item, and then formulates item recommendation as a next-token generation task by training a domain-unified sequence-to-sequence model. To further leverage the domain information to enhance performance, we incorporate a domain-aware contrastive loss into the semantic identifier learning, and perform domain-specific fine-tuning on the unified recommender. Extensive experiments on five public datasets demonstrate the effectiveness of GMC compared to a range of baseline methods.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "fix some information by request",
    "pdf_url": "https://arxiv.org/pdf/2507.12871v3",
    "published_date": "2025-07-17 07:44:05 UTC",
    "updated_date": "2025-08-07 08:36:01 UTC"
  },
  {
    "arxiv_id": "2507.12862v2",
    "title": "Information-Theoretic Aggregation of Ethical Attributes in Simulated-Command",
    "authors": [
      "Taylan Akay",
      "Harrison Tolley",
      "Hussein Abbass"
    ],
    "abstract": "In the age of AI, human commanders need to use the computational powers available in today's environment to simulate a very large number of scenarios. Within each scenario, situations occur where different decision design options could have ethical consequences. Making these decisions reliant on human judgement is both counter-productive to the aim of exploring very large number of scenarios in a timely manner and infeasible when considering the workload needed to involve humans in each of these choices. In this paper, we move human judgement outside the simulation decision cycle. Basically, the human will design the ethical metric space, leaving it to the simulated environment to explore the space. When the simulation completes its testing cycles, the testing environment will come back to the human commander with a few options to select from. The human commander will then exercise human-judgement to select the most appropriate course of action, which will then get executed accordingly. We assume that the problem of designing metrics that are sufficiently granular to assess the ethical implications of decisions is solved. Subsequently, the fundamental problem we look at in this paper is how to weight ethical decisions during the running of these simulations; that is, how to dynamically weight the ethical attributes when agents are faced with decision options with ethical implications during generative simulations. The multi-criteria decision making literature has started to look at nearby problems, where the concept of entropy has been used to determine the weights during aggregation. We draw from that literature different approaches to automatically calculate the weights for ethical attributes during simulation-based testing and evaluation.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.12862v2",
    "published_date": "2025-07-17 07:34:24 UTC",
    "updated_date": "2025-07-20 23:31:14 UTC"
  },
  {
    "arxiv_id": "2507.12856v2",
    "title": "Supervised Fine Tuning on Curated Data is Reinforcement Learning (and can be improved)",
    "authors": [
      "Chongli Qin",
      "Jost Tobias Springenberg"
    ],
    "abstract": "Behavior Cloning (BC) on curated (or filtered) data is the predominant paradigm for supervised fine-tuning (SFT) of large language models; as well as for imitation learning of control policies. Here, we draw on a connection between this successful strategy and the theory and practice of finding optimal policies via Reinforcement Learning (RL). Building on existing literature, we clarify that SFT can be understood as maximizing a lower bound on the RL objective in a sparse reward setting. Giving support to its often observed good performance. From this viewpoint, we realize that a small modification to SFT leads to an importance weighted variant that behaves closer to training with RL as it: i) optimizes a tighter bound to the RL objective and, ii) can improve performance compared to SFT on curated data. We refer to this variant as importance weighted supervised fine-tuning (iw-SFT). We show that it is easy to implement and can be further generalized to training with quality scored data. The resulting SFT variants are competitive with more advanced RL algorithms for large language models and for training policies in continuous control tasks. For example achieving 66.7% on the AIME 2024 dataset.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "See project website for details and code at: https://independentresearch.ai/posts/iwsft",
    "pdf_url": "https://arxiv.org/pdf/2507.12856v2",
    "published_date": "2025-07-17 07:26:54 UTC",
    "updated_date": "2025-09-06 23:54:03 UTC"
  },
  {
    "arxiv_id": "2507.12846v2",
    "title": "Enter the Mind Palace: Reasoning and Planning for Long-term Active Embodied Question Answering",
    "authors": [
      "Muhammad Fadhil Ginting",
      "Dong-Ki Kim",
      "Xiangyun Meng",
      "Andrzej Reinke",
      "Bandi Jai Krishna",
      "Navid Kayhani",
      "Oriana Peltzer",
      "David D. Fan",
      "Amirreza Shaban",
      "Sung-Kyun Kim",
      "Mykel J. Kochenderfer",
      "Ali-akbar Agha-mohammadi",
      "Shayegan Omidshafiei"
    ],
    "abstract": "As robots become increasingly capable of operating over extended periods -- spanning days, weeks, and even months -- they are expected to accumulate knowledge of their environments and leverage this experience to assist humans more effectively. This paper studies the problem of Long-term Active Embodied Question Answering (LA-EQA), a new task in which a robot must both recall past experiences and actively explore its environment to answer complex, temporally-grounded questions. Unlike traditional EQA settings, which typically focus either on understanding the present environment alone or on recalling a single past observation, LA-EQA challenges an agent to reason over past, present, and possible future states, deciding when to explore, when to consult its memory, and when to stop gathering observations and provide a final answer. Standard EQA approaches based on large models struggle in this setting due to limited context windows, absence of persistent memory, and an inability to combine memory recall with active exploration. To address this, we propose a structured memory system for robots, inspired by the mind palace method from cognitive science. Our method encodes episodic experiences as scene-graph-based world instances, forming a reasoning and planning algorithm that enables targeted memory retrieval and guided navigation. To balance the exploration-recall trade-off, we introduce value-of-information-based stopping criteria that determines when the agent has gathered sufficient information. We evaluate our method on real-world experiments and introduce a new benchmark that spans popular simulation environments and actual industrial sites. Our approach significantly outperforms state-of-the-art baselines, yielding substantial gains in both answer accuracy and exploration efficiency.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.12846v2",
    "published_date": "2025-07-17 07:11:32 UTC",
    "updated_date": "2025-09-25 00:00:21 UTC"
  },
  {
    "arxiv_id": "2507.12845v1",
    "title": "SEMT: Static-Expansion-Mesh Transformer Network Architecture for Remote Sensing Image Captioning",
    "authors": [
      "Khang Truong",
      "Lam Pham",
      "Hieu Tang",
      "Jasmin Lampert",
      "Martin Boyer",
      "Son Phan",
      "Truong Nguyen"
    ],
    "abstract": "Image captioning has emerged as a crucial task in the intersection of computer vision and natural language processing, enabling automated generation of descriptive text from visual content. In the context of remote sensing, image captioning plays a significant role in interpreting vast and complex satellite imagery, aiding applications such as environmental monitoring, disaster assessment, and urban planning. This motivates us, in this paper, to present a transformer based network architecture for remote sensing image captioning (RSIC) in which multiple techniques of Static Expansion, Memory-Augmented Self-Attention, Mesh Transformer are evaluated and integrated. We evaluate our proposed models using two benchmark remote sensing image datasets of UCM-Caption and NWPU-Caption. Our best model outperforms the state-of-the-art systems on most of evaluation metrics, which demonstrates potential to apply for real-life remote sensing image systems.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.12845v1",
    "published_date": "2025-07-17 07:11:01 UTC",
    "updated_date": "2025-07-17 07:11:01 UTC"
  },
  {
    "arxiv_id": "2507.13410v2",
    "title": "Causal Language Control in Multilingual Transformers via Sparse Feature Steering",
    "authors": [
      "Cheng-Ting Chou",
      "George Liu",
      "Jessica Sun",
      "Cole Blondin",
      "Kevin Zhu",
      "Vasu Sharma",
      "Sean O'Brien"
    ],
    "abstract": "Deterministically controlling the target generation language of large multilingual language models (LLMs) remains a fundamental challenge, particularly in zero-shot settings where neither explicit language prompts nor fine-tuning are available. In this work, we investigate whether sparse autoencoder (SAE) features, previously shown to correlate with interpretable model behaviors, can be leveraged to steer the generated language of LLMs during inference. Leveraging pretrained SAEs on the residual streams of Gemma-2B and Gemma-9B, we identify features whose activations differ most significantly between English and four target languages: Chinese, Japanese, Spanish, and French. By modifying just a single SAE feature at one transformer layer, we achieve controlled language shifts with up to 90\\% success, as measured by FastText language classification, while preserving semantic fidelity according to LaBSE (Language-Agnostic BERT Sentence Embedding) similarity. Our analysis reveals that language steering is most effective in mid-to-late transformer layers and is amplified by specific attention heads disproportionately associated with language-sensitive SAE features. These results demonstrate the promise of sparse feature steering as a lightweight and interpretable mechanism for controllable multilingual generation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.13410v2",
    "published_date": "2025-07-17 06:49:16 UTC",
    "updated_date": "2025-10-15 18:18:58 UTC"
  },
  {
    "arxiv_id": "2507.12832v1",
    "title": "MVA 2025 Small Multi-Object Tracking for Spotting Birds Challenge: Dataset, Methods, and Results",
    "authors": [
      "Yuki Kondo",
      "Norimichi Ukita",
      "Riku Kanayama",
      "Yuki Yoshida",
      "Takayuki Yamaguchi",
      "Xiang Yu",
      "Guang Liang",
      "Xinyao Liu",
      "Guan-Zhang Wang",
      "Wei-Ta Chu",
      "Bing-Cheng Chuang",
      "Jia-Hua Lee",
      "Pin-Tseng Kuo",
      "I-Hsuan Chu",
      "Yi-Shein Hsiao",
      "Cheng-Han Wu",
      "Po-Yi Wu",
      "Jui-Chien Tsou",
      "Hsuan-Chi Liu",
      "Chun-Yi Lee",
      "Yuan-Fu Yang",
      "Kosuke Shigematsu",
      "Asuka Shin",
      "Ba Tran"
    ],
    "abstract": "Small Multi-Object Tracking (SMOT) is particularly challenging when targets occupy only a few dozen pixels, rendering detection and appearance-based association unreliable. Building on the success of the MVA2023 SOD4SB challenge, this paper introduces the SMOT4SB challenge, which leverages temporal information to address limitations of single-frame detection. Our three main contributions are: (1) the SMOT4SB dataset, consisting of 211 UAV video sequences with 108,192 annotated frames under diverse real-world conditions, designed to capture motion entanglement where both camera and targets move freely in 3D; (2) SO-HOTA, a novel metric combining Dot Distance with HOTA to mitigate the sensitivity of IoU-based metrics to small displacements; and (3) a competitive MVA2025 challenge with 78 participants and 308 submissions, where the winning method achieved a 5.1x improvement over the baseline. This work lays a foundation for advancing SMOT in UAV scenarios with applications in bird strike avoidance, agriculture, fisheries, and ecological monitoring.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "This paper is the official challenge report for SMOT4SB and is published in the proceedings of MVA 2025 (19th International Conference on Machine Vision and Applications). Official challenge page: https://www.mva-org.jp/mva2025/challenge",
    "pdf_url": "https://arxiv.org/pdf/2507.12832v1",
    "published_date": "2025-07-17 06:45:47 UTC",
    "updated_date": "2025-07-17 06:45:47 UTC"
  },
  {
    "arxiv_id": "2507.12828v2",
    "title": "Feature-Enhanced TResNet for Fine-Grained Food Image Classification",
    "authors": [
      "Lulu Liu",
      "Zhiyong Xiao"
    ],
    "abstract": "Food is not only essential to human health but also serves as a medium for cultural identity and emotional connection. In the context of precision nutrition, accurately identifying and classifying food images is critical for dietary monitoring, nutrient estimation, and personalized health management. However, fine-grained food classification remains challenging due to the subtle visual differences among similar dishes. To address this, we propose Feature-Enhanced TResNet (FE-TResNet), a novel deep learning model designed to improve the accuracy of food image recognition in fine-grained scenarios. Built on the TResNet architecture, FE-TResNet integrates a Style-based Recalibration Module (StyleRM) and Deep Channel-wise Attention (DCA) to enhance feature extraction and emphasize subtle distinctions between food items. Evaluated on two benchmark Chinese food datasets-ChineseFoodNet and CNFOOD-241-FE-TResNet achieved high classification accuracies of 81.37% and 80.29%, respectively. These results demonstrate its effectiveness and highlight its potential as a key enabler for intelligent dietary assessment and personalized recommendations in precision nutrition systems.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.12828v2",
    "published_date": "2025-07-17 06:37:45 UTC",
    "updated_date": "2025-07-23 02:14:58 UTC"
  },
  {
    "arxiv_id": "2507.12821v2",
    "title": "Assessing Adaptive World Models in Machines with Novel Games",
    "authors": [
      "Lance Ying",
      "Katherine M. Collins",
      "Prafull Sharma",
      "Cedric Colas",
      "Kaiya Ivy Zhao",
      "Adrian Weller",
      "Zenna Tavares",
      "Phillip Isola",
      "Samuel J. Gershman",
      "Jacob D. Andreas",
      "Thomas L. Griffiths",
      "Francois Chollet",
      "Kelsey R. Allen",
      "Joshua B. Tenenbaum"
    ],
    "abstract": "Human intelligence exhibits a remarkable capacity for rapid adaptation and effective problem-solving in novel and unfamiliar contexts. We argue that this profound adaptability is fundamentally linked to the efficient construction and refinement of internal representations of the environment, commonly referred to as world models, and we refer to this adaptation mechanism as world model induction. However, current understanding and evaluation of world models in artificial intelligence (AI) remains narrow, often focusing on static representations learned from training on massive corpora of data, instead of the efficiency and efficacy in learning these representations through interaction and exploration within a novel environment. In this Perspective, we provide a view of world model induction drawing on decades of research in cognitive science on how humans learn and adapt so efficiently; we then call for a new evaluation framework for assessing adaptive world models in AI. Concretely, we propose a new benchmarking paradigm based on suites of carefully designed games with genuine, deep and continually refreshing novelty in the underlying game structures -- we refer to this class of games as novel games. We detail key desiderata for constructing these games and propose appropriate metrics to explicitly challenge and evaluate the agent's ability for rapid world model induction. We hope that this new evaluation framework will inspire future evaluation efforts on world models in AI and provide a crucial step towards developing AI systems capable of human-like rapid adaptation and robust generalization -- a critical component of artificial general intelligence.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "17 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.12821v2",
    "published_date": "2025-07-17 06:28:14 UTC",
    "updated_date": "2025-07-22 17:07:08 UTC"
  },
  {
    "arxiv_id": "2507.12820v2",
    "title": "Emotional Support with LLM-based Empathetic Dialogue Generation",
    "authors": [
      "Shiquan Wang",
      "Ruiyu Fang",
      "Zhongjiang He",
      "Shuangyong Song",
      "Yongxiang Li"
    ],
    "abstract": "Emotional Support Conversation (ESC) aims to provide empathetic and effective emotional assistance through dialogue, addressing the growing demand for mental health support. This paper presents our solution for the NLPCC 2025 Task 8 ESC evaluation, where we leverage large-scale language models enhanced by prompt engineering and finetuning techniques. We explore both parameter-efficient Low-Rank Adaptation and full-parameter fine-tuning strategies to improve the model's ability to generate supportive and contextually appropriate responses. Our best model ranked second in the competition, highlighting the potential of combining LLMs with effective adaptation methods for ESC tasks. Future work will focus on further enhancing emotional understanding and response personalization to build more practical and reliable emotional support systems.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.12820v2",
    "published_date": "2025-07-17 06:24:20 UTC",
    "updated_date": "2025-12-11 02:38:17 UTC"
  },
  {
    "arxiv_id": "2507.12816v1",
    "title": "FIQ: Fundamental Question Generation with the Integration of Question Embeddings for Video Question Answering",
    "authors": [
      "Ju-Young Oh",
      "Ho-Joong Kim",
      "Seong-Whan Lee"
    ],
    "abstract": "Video question answering (VQA) is a multimodal task that requires the interpretation of a video to answer a given question. Existing VQA methods primarily utilize question and answer (Q&A) pairs to learn the spatio-temporal characteristics of video content. However, these annotations are typically event-centric, which is not enough to capture the broader context of each video. The absence of essential details such as object types, spatial layouts, and descriptive attributes restricts the model to learning only a fragmented scene representation. This issue limits the model's capacity for generalization and higher-level reasoning. In this paper, we propose a fundamental question generation with the integration of question embeddings for video question answering (FIQ), a novel approach designed to strengthen the reasoning ability of the model by enhancing the fundamental understanding of videos. FIQ generates Q&A pairs based on descriptions extracted from videos, enriching the training data with fundamental scene information. Generated Q&A pairs enable the model to understand the primary context, leading to enhanced generalizability and reasoning ability. Furthermore, we incorporate a VQ-CAlign module that assists task-specific question embeddings with visual features, ensuring that essential domain-specific details are preserved to increase the adaptability of downstream tasks. Experiments on SUTD-TrafficQA demonstrate that our FIQ achieves state-of-the-art performance compared to existing baseline methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "SMC 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.12816v1",
    "published_date": "2025-07-17 06:19:38 UTC",
    "updated_date": "2025-07-17 06:19:38 UTC"
  },
  {
    "arxiv_id": "2507.13408v1",
    "title": "A Deep Learning-Based Ensemble System for Automated Shoulder Fracture Detection in Clinical Radiographs",
    "authors": [
      "Hemanth Kumar M",
      "Karthika M",
      "Saianiruth M",
      "Vasanthakumar Venugopal",
      "Anandakumar D",
      "Revathi Ezhumalai",
      "Charulatha K",
      "Kishore Kumar J",
      "Dayana G",
      "Kalyan Sivasailam",
      "Bargava Subramanian"
    ],
    "abstract": "Background: Shoulder fractures are often underdiagnosed, especially in emergency and high-volume clinical settings. Studies report up to 10% of such fractures may be missed by radiologists. AI-driven tools offer a scalable way to assist early detection and reduce diagnostic delays. We address this gap through a dedicated AI system for shoulder radiographs. Methods: We developed a multi-model deep learning system using 10,000 annotated shoulder X-rays. Architectures include Faster R-CNN (ResNet50-FPN, ResNeXt), EfficientDet, and RF-DETR. To enhance detection, we applied bounding box and classification-level ensemble techniques such as Soft-NMS, WBF, and NMW fusion. Results: The NMW ensemble achieved 95.5% accuracy and an F1-score of 0.9610, outperforming individual models across all key metrics. It demonstrated strong recall and localization precision, confirming its effectiveness for clinical fracture detection in shoulder X-rays. Conclusion: The results show ensemble-based AI can reliably detect shoulder fractures in radiographs with high clinical relevance. The model's accuracy and deployment readiness position it well for integration into real-time diagnostic workflows. The current model is limited to binary fracture detection, reflecting its design for rapid screening and triage support rather than detailed orthopedic classification.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.13408v1",
    "published_date": "2025-07-17 06:06:12 UTC",
    "updated_date": "2025-07-17 06:06:12 UTC"
  },
  {
    "arxiv_id": "2507.21117v2",
    "title": "A Comprehensive Review on Harnessing Large Language Models to Overcome Recommender System Challenges",
    "authors": [
      "Rahul Raja",
      "Anshaj Vats",
      "Arpita Vats",
      "Anirban Majumder"
    ],
    "abstract": "Recommender systems have traditionally followed modular architectures comprising candidate generation, multi-stage ranking, and re-ranking, each trained separately with supervised objectives and hand-engineered features. While effective in many domains, such systems face persistent challenges including sparse and noisy interaction data, cold-start problems, limited personalization depth, and inadequate semantic understanding of user and item content. The recent emergence of Large Language Models (LLMs) offers a new paradigm for addressing these limitations through unified, language-native mechanisms that can generalize across tasks, domains, and modalities. In this paper, we present a comprehensive technical survey of how LLMs can be leveraged to tackle key challenges in modern recommender systems. We examine the use of LLMs for prompt-driven candidate retrieval, language-native ranking, retrieval-augmented generation (RAG), and conversational recommendation, illustrating how these approaches enhance personalization, semantic alignment, and interpretability without requiring extensive task-specific supervision. LLMs further enable zero- and few-shot reasoning, allowing systems to operate effectively in cold-start and long-tail scenarios by leveraging external knowledge and contextual cues. We categorize these emerging LLM-driven architectures and analyze their effectiveness in mitigating core bottlenecks of conventional pipelines. In doing so, we provide a structured framework for understanding the design space of LLM-enhanced recommenders, and outline the trade-offs between accuracy, scalability, and real-time performance. Our goal is to demonstrate that LLMs are not merely auxiliary components but foundational enablers for building more adaptive, semantically rich, and user-centric recommender systems",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.21117v2",
    "published_date": "2025-07-17 06:03:57 UTC",
    "updated_date": "2025-10-03 03:20:34 UTC"
  },
  {
    "arxiv_id": "2507.12808v1",
    "title": "Large Language Models' Internal Perception of Symbolic Music",
    "authors": [
      "Andrew Shin",
      "Kunitake Kaneko"
    ],
    "abstract": "Large language models (LLMs) excel at modeling relationships between strings in natural language and have shown promise in extending to other symbolic domains like coding or mathematics. However, the extent to which they implicitly model symbolic music remains underexplored. This paper investigates how LLMs represent musical concepts by generating symbolic music data from textual prompts describing combinations of genres and styles, and evaluating their utility through recognition and generation tasks. We produce a dataset of LLM-generated MIDI files without relying on explicit musical training. We then train neural networks entirely on this LLM-generated MIDI dataset and perform genre and style classification as well as melody completion, benchmarking their performance against established models. Our results demonstrate that LLMs can infer rudimentary musical structures and temporal relationships from text, highlighting both their potential to implicitly encode musical patterns and their limitations due to a lack of explicit musical context, shedding light on their generative capabilities for symbolic music.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.12808v1",
    "published_date": "2025-07-17 05:48:45 UTC",
    "updated_date": "2025-07-17 05:48:45 UTC"
  },
  {
    "arxiv_id": "2507.12806v2",
    "title": "MCPEval: Automatic MCP-based Deep Evaluation for AI Agent Models",
    "authors": [
      "Zhiwei Liu",
      "Jielin Qiu",
      "Shiyu Wang",
      "Jianguo Zhang",
      "Zuxin Liu",
      "Roshan Ram",
      "Haolin Chen",
      "Weiran Yao",
      "Shelby Heinecke",
      "Silvio Savarese",
      "Huan Wang",
      "Caiming Xiong"
    ],
    "abstract": "The rapid rise of Large Language Models (LLMs)-based intelligent agents underscores the need for robust, scalable evaluation frameworks. Existing methods rely on static benchmarks and labor-intensive data collection, limiting practical assessment. We introduce MCPEval, an open-source Model Context Protocol (MCP)-based framework that automates end-to-end task generation and deep evaluation of LLM agents across diverse domains. MCPEval standardizes metrics, seamlessly integrates with native agent tools, and eliminates manual effort in building evaluation pipelines. Empirical results across five real-world domains show its effectiveness in revealing nuanced, domain-specific performance. We publicly release MCPEval https://github.com/SalesforceAIResearch/MCPEval to promote reproducible and standardized LLM agent evaluation.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "https://github.com/SalesforceAIResearch/MCPEval",
    "pdf_url": "https://arxiv.org/pdf/2507.12806v2",
    "published_date": "2025-07-17 05:46:27 UTC",
    "updated_date": "2025-08-01 22:37:16 UTC"
  },
  {
    "arxiv_id": "2507.12805v1",
    "title": "PMKLC: Parallel Multi-Knowledge Learning-based Lossless Compression for Large-Scale Genomics Database",
    "authors": [
      "Hui Sun",
      "Yanfeng Ding",
      "Liping Yi",
      "Huidong Ma",
      "Gang Wang",
      "Xiaoguang Liu",
      "Cheng Zhong",
      "Wentong Cai"
    ],
    "abstract": "Learning-based lossless compressors play a crucial role in large-scale genomic database backup, storage, transmission, and management. However, their 1) inadequate compression ratio, 2) low compression \\& decompression throughput, and 3) poor compression robustness limit their widespread adoption and application in both industry and academia. To solve those challenges, we propose a novel \\underline{P}arallel \\underline{M}ulti-\\underline{K}nowledge \\underline{L}earning-based \\underline{C}ompressor (PMKLC) with four crucial designs: 1) We propose an automated multi-knowledge learning-based compression framework as compressors' backbone to enhance compression ratio and robustness; 2) we design a GPU-accelerated ($s$,$k$)-mer encoder to optimize compression throughput and computing resource usage; 3) we introduce data block partitioning and Step-wise Model Passing (SMP) mechanisms for parallel acceleration; 4) We design two compression modes PMKLC-S and PMKLC-M to meet the complex application scenarios, where the former runs on a resource-constrained single GPU and the latter is multi-GPU accelerated. We benchmark PMKLC-S/M and 14 baselines (7 traditional and 7 leaning-based) on 15 real-world datasets with different species and data sizes. Compared to baselines on the testing datasets, PMKLC-S/M achieve the average compression ratio improvement up to 73.609\\% and 73.480\\%, the average throughput improvement up to 3.036$\\times$ and 10.710$\\times$, respectively. Besides, PMKLC-S/M also achieve the best robustness and competitive memory cost, indicating its greater stability against datasets with different probability distribution perturbations, and its strong ability to run on memory-constrained devices.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.DB"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted via KDD-25",
    "pdf_url": "https://arxiv.org/pdf/2507.12805v1",
    "published_date": "2025-07-17 05:46:08 UTC",
    "updated_date": "2025-07-17 05:46:08 UTC"
  },
  {
    "arxiv_id": "2507.12803v1",
    "title": "FLDmamba: Integrating Fourier and Laplace Transform Decomposition with Mamba for Enhanced Time Series Prediction",
    "authors": [
      "Qianru Zhang",
      "Chenglei Yu",
      "Haixin Wang",
      "Yudong Yan",
      "Yuansheng Cao",
      "Siu-Ming Yiu",
      "Tailin Wu",
      "Hongzhi Yin"
    ],
    "abstract": "Time series prediction, a crucial task across various domains, faces significant challenges due to the inherent complexities of time series data, including non-stationarity, multi-scale periodicity, and transient dynamics, particularly when tackling long-term predictions. While Transformer-based architectures have shown promise, their quadratic complexity with sequence length hinders their efficiency for long-term predictions. Recent advancements in State-Space Models, such as Mamba, offer a more efficient alternative for long-term modeling, but they cannot capture multi-scale periodicity and transient dynamics effectively. Meanwhile, they are susceptible to data noise issues in time series. This paper proposes a novel framework, FLDmamba (Fourier and Laplace Transform Decomposition Mamba), addressing these limitations. FLDmamba leverages the strengths of both Fourier and Laplace transforms to effectively capture both multi-scale periodicity, transient dynamics within time series data, and improve the robustness of the model to the data noise issue. Our extensive experiments demonstrate that FLDmamba achieves superior performance on time series prediction benchmarks, outperforming both Transformer-based and other Mamba-based architectures. To promote the reproducibility of our method, we have made both the code and data accessible via the following URL:{\\href{https://github.com/AI4Science-WestlakeU/FLDmamba}{https://github.com/AI4Science-WestlakeU/\\model}.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages",
    "pdf_url": "https://arxiv.org/pdf/2507.12803v1",
    "published_date": "2025-07-17 05:39:15 UTC",
    "updated_date": "2025-07-17 05:39:15 UTC"
  },
  {
    "arxiv_id": "2507.13407v1",
    "title": "IConMark: Robust Interpretable Concept-Based Watermark For AI Images",
    "authors": [
      "Vinu Sankar Sadasivan",
      "Mehrdad Saberi",
      "Soheil Feizi"
    ],
    "abstract": "With the rapid rise of generative AI and synthetic media, distinguishing AI-generated images from real ones has become crucial in safeguarding against misinformation and ensuring digital authenticity. Traditional watermarking techniques have shown vulnerabilities to adversarial attacks, undermining their effectiveness in the presence of attackers. We propose IConMark, a novel in-generation robust semantic watermarking method that embeds interpretable concepts into AI-generated images, as a first step toward interpretable watermarking. Unlike traditional methods, which rely on adding noise or perturbations to AI-generated images, IConMark incorporates meaningful semantic attributes, making it interpretable to humans and hence, resilient to adversarial manipulation. This method is not only robust against various image augmentations but also human-readable, enabling manual verification of watermarks. We demonstrate a detailed evaluation of IConMark's effectiveness, demonstrating its superiority in terms of detection accuracy and maintaining image quality. Moreover, IConMark can be combined with existing watermarking techniques to further enhance and complement its robustness. We introduce IConMark+SS and IConMark+TM, hybrid approaches combining IConMark with StegaStamp and TrustMark, respectively, to further bolster robustness against multiple types of image manipulations. Our base watermarking technique (IConMark) and its variants (+TM and +SS) achieve 10.8%, 14.5%, and 15.9% higher mean area under the receiver operating characteristic curve (AUROC) scores for watermark detection, respectively, compared to the best baseline on various datasets.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at ICLR 2025 Workshop on GenAI Watermarking (WMARK)",
    "pdf_url": "https://arxiv.org/pdf/2507.13407v1",
    "published_date": "2025-07-17 05:38:30 UTC",
    "updated_date": "2025-07-17 05:38:30 UTC"
  },
  {
    "arxiv_id": "2507.12801v1",
    "title": "Imitating Mistakes in a Learning Companion AI Agent for Online Peer Learning",
    "authors": [
      "Sosui Moribe",
      "Taketoshi Ushiama"
    ],
    "abstract": "In recent years, peer learning has gained attention as a method that promotes spontaneous thinking among learners, and its effectiveness has been confirmed by numerous studies. This study aims to develop an AI Agent as a learning companion that enables peer learning anytime and anywhere. However, peer learning between humans has various limitations, and it is not always effective. Effective peer learning requires companions at the same proficiency levels. In this study, we assume that a learner's peers with the same proficiency level as the learner make the same mistakes as the learner does and focus on English composition as a specific example to validate this approach.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "This is the preprint version of the paper published in IMCOM 2025, IEEE Xplore (DOI: 10.1109/IMCOM64595.2025.10857528)",
    "pdf_url": "https://arxiv.org/pdf/2507.12801v1",
    "published_date": "2025-07-17 05:37:07 UTC",
    "updated_date": "2025-07-17 05:37:07 UTC"
  },
  {
    "arxiv_id": "2507.12795v1",
    "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning",
    "authors": [
      "Penglei Sun",
      "Yaoxian Song",
      "Xiangru Zhu",
      "Xiang Liu",
      "Qiang Wang",
      "Yue Liu",
      "Changqun Xia",
      "Tiefeng Li",
      "Yang Yang",
      "Xiaowen Chu"
    ],
    "abstract": "Scene understanding enables intelligent agents to interpret and comprehend their environment. While existing large vision-language models (LVLMs) for scene understanding have primarily focused on indoor household tasks, they face two significant limitations when applied to outdoor large-scale scene understanding. First, outdoor scenarios typically encompass larger-scale environments observed through various sensors from multiple viewpoints (e.g., bird view and terrestrial view), while existing indoor LVLMs mainly analyze single visual modalities within building-scale contexts from humanoid viewpoints. Second, existing LVLMs suffer from missing multidomain perception outdoor data and struggle to effectively integrate 2D and 3D visual information. To address the aforementioned limitations, we build the first multidomain perception outdoor scene understanding dataset, named \\textbf{\\underline{SVM-City}}, deriving from multi\\textbf{\\underline{S}}cale scenarios with multi\\textbf{\\underline{V}}iew and multi\\textbf{\\underline{M}}odal instruction tuning data. It contains $420$k images and $4, 811$M point clouds with $567$k question-answering pairs from vehicles, low-altitude drones, high-altitude aerial planes, and satellite. To effectively fuse the multimodal data in the absence of one modality, we introduce incomplete multimodal learning to model outdoor scene understanding and design the LVLM named \\textbf{\\underline{City-VLM}}. Multimodal fusion is realized by constructing a joint probabilistic distribution space rather than implementing directly explicit fusion operations (e.g., concatenation). Experimental results on three typical outdoor scene understanding tasks show City-VLM achieves $18.14 \\%$ performance surpassing existing LVLMs in question-answering tasks averagely. Our method demonstrates pragmatic and generalization performance across multiple outdoor scenes.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.12795v1",
    "published_date": "2025-07-17 05:21:21 UTC",
    "updated_date": "2025-07-17 05:21:21 UTC"
  },
  {
    "arxiv_id": "2507.14231v1",
    "title": "Beyond Architectures: Evaluating the Role of Contextual Embeddings in Detecting Bipolar Disorder on Social Media",
    "authors": [
      "Khalid Hasan",
      "Jamil Saquer"
    ],
    "abstract": "Bipolar disorder is a chronic mental illness frequently underdiagnosed due to subtle early symptoms and social stigma. This paper explores the advanced natural language processing (NLP) models for recognizing signs of bipolar disorder based on user-generated social media text. We conduct a comprehensive evaluation of transformer-based models (BERT, RoBERTa, ALBERT, ELECTRA, DistilBERT) and Long Short Term Memory (LSTM) models based on contextualized (BERT) and static (GloVe, Word2Vec) word embeddings. Experiments were performed on a large, annotated dataset of Reddit posts after confirming their validity through sentiment variance and judgmental analysis. Our results demonstrate that RoBERTa achieves the highest performance among transformer models with an F1 score of ~98% while LSTM models using BERT embeddings yield nearly identical results. In contrast, LSTMs trained on static embeddings fail to capture meaningful patterns, scoring near-zero F1. These findings underscore the critical role of contextual language modeling in detecting bipolar disorder. In addition, we report model training times and highlight that DistilBERT offers an optimal balance between efficiency and accuracy. In general, our study offers actionable insights for model selection in mental health NLP applications and validates the potential of contextualized language models to support early bipolar disorder screening.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "The 37th International Conference on Software Engineering & Knowledge Engineering, SEKE 2025 (camera-ready)",
    "pdf_url": "https://arxiv.org/pdf/2507.14231v1",
    "published_date": "2025-07-17 05:14:19 UTC",
    "updated_date": "2025-07-17 05:14:19 UTC"
  },
  {
    "arxiv_id": "2507.14230v2",
    "title": "Intent-Based Network for RAN Management with Large Language Models",
    "authors": [
      "Fransiscus Asisi Bimo",
      "Maria Amparo Canaveras Galdon",
      "Chun-Kai Lai",
      "Ray-Guang Cheng",
      "Edwin K. P. Chong"
    ],
    "abstract": "Advanced intelligent automation becomes an important feature to deal with the increased complexity in managing wireless networks. This paper proposes a novel automation approach of intent-based network for Radio Access Networks (RANs) management by leveraging Large Language Models (LLMs). The proposed method enhances intent translation, autonomously interpreting high-level objectives, reasoning over complex network states, and generating precise configurations of the RAN by integrating LLMs within an agentic architecture. We propose a structured prompt engineering technique and demonstrate that the network can automatically improve its energy efficiency by dynamically optimizing critical RAN parameters through a closed-loop mechanism. It showcases the potential to enable robust resource management in RAN by adapting strategies based on real-time feedback via LLM-orchestrated agentic systems.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "5 pages, 3 figures, submitted to IEEE Globecom 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.14230v2",
    "published_date": "2025-07-17 04:57:55 UTC",
    "updated_date": "2025-08-04 05:31:16 UTC"
  },
  {
    "arxiv_id": "2507.12784v1",
    "title": "A Semi-Supervised Learning Method for the Identification of Bad Exposures in Large Imaging Surveys",
    "authors": [
      "Yufeng Luo",
      "Adam D. Myers",
      "Alex Drlica-Wagner",
      "Dario Dematties",
      "Salma Borchani",
      "Frank Valdes",
      "Arjun Dey",
      "David Schlegel",
      "Rongpu Zhou",
      "DESI Legacy Imaging Surveys Team"
    ],
    "abstract": "As the data volume of astronomical imaging surveys rapidly increases, traditional methods for image anomaly detection, such as visual inspection by human experts, are becoming impractical. We introduce a machine-learning-based approach to detect poor-quality exposures in large imaging surveys, with a focus on the DECam Legacy Survey (DECaLS) in regions of low extinction (i.e., $E(B-V)<0.04$). Our semi-supervised pipeline integrates a vision transformer (ViT), trained via self-supervised learning (SSL), with a k-Nearest Neighbor (kNN) classifier. We train and validate our pipeline using a small set of labeled exposures observed by surveys with the Dark Energy Camera (DECam). A clustering-space analysis of where our pipeline places images labeled in ``good'' and ``bad'' categories suggests that our approach can efficiently and accurately determine the quality of exposures. Applied to new imaging being reduced for DECaLS Data Release 11, our pipeline identifies 780 problematic exposures, which we subsequently verify through visual inspection. Being highly efficient and adaptable, our method offers a scalable solution for quality control in other large imaging surveys.",
    "categories": [
      "astro-ph.IM",
      "cs.AI"
    ],
    "primary_category": "astro-ph.IM",
    "comment": "21 pages, 12 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.12784v1",
    "published_date": "2025-07-17 04:52:05 UTC",
    "updated_date": "2025-07-17 04:52:05 UTC"
  },
  {
    "arxiv_id": "2507.12774v1",
    "title": "A Comprehensive Survey of Electronic Health Record Modeling: From Deep Learning Approaches to Large Language Models",
    "authors": [
      "Weijieying Ren",
      "Jingxi Zhu",
      "Zehao Liu",
      "Tianxiang Zhao",
      "Vasant Honavar"
    ],
    "abstract": "Artificial intelligence (AI) has demonstrated significant potential in transforming healthcare through the analysis and modeling of electronic health records (EHRs). However, the inherent heterogeneity, temporal irregularity, and domain-specific nature of EHR data present unique challenges that differ fundamentally from those in vision and natural language tasks. This survey offers a comprehensive overview of recent advancements at the intersection of deep learning, large language models (LLMs), and EHR modeling. We introduce a unified taxonomy that spans five key design dimensions: data-centric approaches, neural architecture design, learning-focused strategies, multimodal learning, and LLM-based modeling systems. Within each dimension, we review representative methods addressing data quality enhancement, structural and temporal representation, self-supervised learning, and integration with clinical knowledge. We further highlight emerging trends such as foundation models, LLM-driven clinical agents, and EHR-to-text translation for downstream reasoning. Finally, we discuss open challenges in benchmarking, explainability, clinical alignment, and generalization across diverse clinical settings. This survey aims to provide a structured roadmap for advancing AI-driven EHR modeling and clinical decision support. For a comprehensive list of EHR-related methods, kindly refer to https://survey-on-tabular-data.github.io/.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.12774v1",
    "published_date": "2025-07-17 04:31:55 UTC",
    "updated_date": "2025-07-17 04:31:55 UTC"
  },
  {
    "arxiv_id": "2507.12771v1",
    "title": "Local Representative Token Guided Merging for Text-to-Image Generation",
    "authors": [
      "Min-Jeong Lee",
      "Hee-Dong Kim",
      "Seong-Whan Lee"
    ],
    "abstract": "Stable diffusion is an outstanding image generation model for text-to-image, but its time-consuming generation process remains a challenge due to the quadratic complexity of attention operations. Recent token merging methods improve efficiency by reducing the number of tokens during attention operations, but often overlook the characteristics of attention-based image generation models, limiting their effectiveness. In this paper, we propose local representative token guided merging (ReToM), a novel token merging strategy applicable to any attention mechanism in image generation. To merge tokens based on various contextual information, ReToM defines local boundaries as windows within attention inputs and adjusts window sizes. Furthermore, we introduce a representative token, which represents the most representative token per window by computing similarity at a specific timestep and selecting the token with the highest average similarity. This approach preserves the most salient local features while minimizing computational overhead. Experimental results show that ReToM achieves a 6.2% improvement in FID and higher CLIP scores compared to the baseline, while maintaining comparable inference time. We empirically demonstrate that ReToM is effective in balancing visual quality and computational efficiency.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "6 pages",
    "pdf_url": "https://arxiv.org/pdf/2507.12771v1",
    "published_date": "2025-07-17 04:16:24 UTC",
    "updated_date": "2025-07-17 04:16:24 UTC"
  },
  {
    "arxiv_id": "2507.12769v1",
    "title": "Synergy: End-to-end Concept Model",
    "authors": [
      "Keli Zheng",
      "Zerong Xie"
    ],
    "abstract": "In this paper, we present Synergy, a language model that bridges different levels of abstraction in an end-to-end fashion through a learned routing mechanism. Focusing on low-level linguistic abstraction, we trained our model as a byte-level language model. Our model spontaneously learns to tokenize bytes, producing fewer concept tokens than Byte-level Byte Pair Encoder (BBPE) tokenizers while keeping comparable performance. By comparing with Llama3, we observed an advantage of Synergy under the same model scale and training dataset size. Further studies show that the middle part (the higher abstraction part) of our model performs better when positional encodings are removed, suggesting the emergence of position-independent concepts. These findings demonstrate the feasibility of tokenizer-free architectures, paving the way for more robust and flexible pipelines.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.12769v1",
    "published_date": "2025-07-17 04:01:28 UTC",
    "updated_date": "2025-07-17 04:01:28 UTC"
  },
  {
    "arxiv_id": "2507.12767v1",
    "title": "Autonomy for Older Adult-Agent Interaction",
    "authors": [
      "Jiaxin An"
    ],
    "abstract": "As the global population ages, artificial intelligence (AI)-powered agents have emerged as potential tools to support older adults' caregiving. Prior research has explored agent autonomy by identifying key interaction stages in task processes and defining the agent's role at each stage. However, ensuring that agents align with older adults' autonomy preferences remains a critical challenge. Drawing on interdisciplinary conceptualizations of autonomy, this paper examines four key dimensions of autonomy for older adults: decision-making autonomy, goal-oriented autonomy, control autonomy, and social responsibility autonomy. This paper then proposes the following research directions: (1) Addressing social responsibility autonomy, which concerns the ethical and social implications of agent use in communal settings; (2) Operationalizing agent autonomy from the task perspective; and (3) Developing autonomy measures.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "7 pages",
    "pdf_url": "https://arxiv.org/pdf/2507.12767v1",
    "published_date": "2025-07-17 03:46:13 UTC",
    "updated_date": "2025-07-17 03:46:13 UTC"
  },
  {
    "arxiv_id": "2507.12761v1",
    "title": "Think-Before-Draw: Decomposing Emotion Semantics & Fine-Grained Controllable Expressive Talking Head Generation",
    "authors": [
      "Hanlei Shi",
      "Leyuan Qu",
      "Yu Liu",
      "Di Gao",
      "Yuhua Zheng",
      "Taihao Li"
    ],
    "abstract": "Emotional talking-head generation has emerged as a pivotal research area at the intersection of computer vision and multimodal artificial intelligence, with its core value lying in enhancing human-computer interaction through immersive and empathetic engagement.With the advancement of multimodal large language models, the driving signals for emotional talking-head generation has shifted from audio and video to more flexible text. However, current text-driven methods rely on predefined discrete emotion label texts, oversimplifying the dynamic complexity of real facial muscle movements and thus failing to achieve natural emotional expressiveness.This study proposes the Think-Before-Draw framework to address two key challenges: (1) In-depth semantic parsing of emotions--by innovatively introducing Chain-of-Thought (CoT), abstract emotion labels are transformed into physiologically grounded facial muscle movement descriptions, enabling the mapping from high-level semantics to actionable motion features; and (2) Fine-grained expressiveness optimization--inspired by artists' portrait painting process, a progressive guidance denoising strategy is proposed, employing a \"global emotion localization--local muscle control\" mechanism to refine micro-expression dynamics in generated videos.Our experiments demonstrate that our approach achieves state-of-the-art performance on widely-used benchmarks, including MEAD and HDTF. Additionally, we collected a set of portrait images to evaluate our model's zero-shot generation capability.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.12761v1",
    "published_date": "2025-07-17 03:33:46 UTC",
    "updated_date": "2025-07-17 03:33:46 UTC"
  },
  {
    "arxiv_id": "2507.12760v1",
    "title": "Unified Medical Image Segmentation with State Space Modeling Snake",
    "authors": [
      "Ruicheng Zhang",
      "Haowei Guo",
      "Kanghui Tian",
      "Jun Zhou",
      "Mingliang Yan",
      "Zeyu Zhang",
      "Shen Zhao"
    ],
    "abstract": "Unified Medical Image Segmentation (UMIS) is critical for comprehensive anatomical assessment but faces challenges due to multi-scale structural heterogeneity. Conventional pixel-based approaches, lacking object-level anatomical insight and inter-organ relational modeling, struggle with morphological complexity and feature conflicts, limiting their efficacy in UMIS. We propose Mamba Snake, a novel deep snake framework enhanced by state space modeling for UMIS. Mamba Snake frames multi-contour evolution as a hierarchical state space atlas, effectively modeling macroscopic inter-organ topological relationships and microscopic contour refinements. We introduce a snake-specific vision state space module, the Mamba Evolution Block (MEB), which leverages effective spatiotemporal information aggregation for adaptive refinement of complex morphologies. Energy map shape priors further ensure robust long-range contour evolution in heterogeneous data. Additionally, a dual-classification synergy mechanism is incorporated to concurrently optimize detection and segmentation, mitigating under-segmentation of microstructures in UMIS. Extensive evaluations across five clinical datasets reveal Mamba Snake's superior performance, with an average Dice improvement of 3\\% over state-of-the-art methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This paper has been accepted by ACM MM 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.12760v1",
    "published_date": "2025-07-17 03:32:32 UTC",
    "updated_date": "2025-07-17 03:32:32 UTC"
  },
  {
    "arxiv_id": "2507.12759v1",
    "title": "Logit Arithmetic Elicits Long Reasoning Capabilities Without Training",
    "authors": [
      "Yunxiang Zhang",
      "Muhammad Khalifa",
      "Lechen Zhang",
      "Xin Liu",
      "Ayoung Lee",
      "Xinliang Frederick Zhang",
      "Farima Fatahi Bayat",
      "Lu Wang"
    ],
    "abstract": "Large reasoning models (LRMs) can do complex reasoning via long chain-of-thought (CoT) involving cognitive strategies such as backtracking and self-correction. Recent studies suggest that some models inherently possess these long reasoning abilities, which may be unlocked via extra training. Our work first investigates whether we can elicit such behavior without any training. To this end, we propose a decoding-time approach, ThinkLogit, which utilizes logits arithmetic (Liu et al., 2024) to tune a target large LM for long reasoning using a substantially smaller model as guider. We then show that we can further boost performance by training the guider model with preference optimization over correct/incorrect reasoning pairs sampled from both the target and guider model -- a setup we refer to as ThinkLogit-DPO. Our experiments demonstrate that ThinkLogit and ThinkLogit-DPO achieve a relative improvement in pass@1 by 26% and 29%, respectively, over four mathematical datasets using the Qwen2.5-32B when guided by R1-Distill-Qwen-1.5B -- a model 21x smaller. Lastly, we show that ThinkLogit can transfer long reasoning skills acquired through reinforcement learning, improving pass@1 by 13% relative compared to the Qwen2.5-32B base model. Our work presents a computationally-efficient method to elicit long reasoning in large models with minimal or no additional training.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.12759v1",
    "published_date": "2025-07-17 03:31:36 UTC",
    "updated_date": "2025-07-17 03:31:36 UTC"
  },
  {
    "arxiv_id": "2507.12739v1",
    "title": "Transformer-based Spatial Grounding: A Comprehensive Survey",
    "authors": [
      "Ijazul Haq",
      "Muhammad Saqib",
      "Yingjie Zhang"
    ],
    "abstract": "Spatial grounding, the process of associating natural language expressions with corresponding image regions, has rapidly advanced due to the introduction of transformer-based models, significantly enhancing multimodal representation and cross-modal alignment. Despite this progress, the field lacks a comprehensive synthesis of current methodologies, dataset usage, evaluation metrics, and industrial applicability. This paper presents a systematic literature review of transformer-based spatial grounding approaches from 2018 to 2025. Our analysis identifies dominant model architectures, prevalent datasets, and widely adopted evaluation metrics, alongside highlighting key methodological trends and best practices. This study provides essential insights and structured guidance for researchers and practitioners, facilitating the development of robust, reliable, and industry-ready transformer-based spatial grounding models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.12739v1",
    "published_date": "2025-07-17 02:44:01 UTC",
    "updated_date": "2025-07-17 02:44:01 UTC"
  },
  {
    "arxiv_id": "2507.19510v1",
    "title": "Beyond 9-to-5: A Generative Model for Augmenting Mobility Data of Underrepresented Shift Workers",
    "authors": [
      "Haoxuan Ma",
      "Xishun Liao",
      "Yifan Liu",
      "Chris Stanford",
      "Jiaqi Ma"
    ],
    "abstract": "This paper addresses a critical gap in urban mobility modeling by focusing on shift workers, a population segment comprising 15-20% of the workforce in industrialized societies yet systematically underrepresented in traditional transportation surveys and planning. This underrepresentation is revealed in this study by a comparative analysis of GPS and survey data, highlighting stark differences between the bimodal temporal patterns of shift workers and the conventional 9-to-5 schedules recorded in surveys. To address this bias, we introduce a novel transformer-based approach that leverages fragmented GPS trajectory data to generate complete, behaviorally valid activity patterns for individuals working non-standard hours. Our method employs periodaware temporal embeddings and a transition-focused loss function specifically designed to capture the unique activity rhythms of shift workers and mitigate the inherent biases in conventional transportation datasets. Evaluation shows that the generated data achieves remarkable distributional alignment with GPS data from Los Angeles County (Average JSD < 0.02 for all evaluation metrics). By transforming incomplete GPS traces into complete, representative activity patterns, our approach provides transportation planners with a powerful data augmentation tool to fill critical gaps in understanding the 24/7 mobility needs of urban populations, enabling precise and inclusive transportation planning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.19510v1",
    "published_date": "2025-07-17 02:33:30 UTC",
    "updated_date": "2025-07-17 02:33:30 UTC"
  },
  {
    "arxiv_id": "2507.12701v1",
    "title": "Task-Specific Audio Coding for Machines: Machine-Learned Latent Features Are Codes for That Machine",
    "authors": [
      "Anastasia Kuznetsova",
      "Inseon Jang",
      "Wootaek Lim",
      "Minje Kim"
    ],
    "abstract": "Neural audio codecs, leveraging quantization algorithms, have significantly impacted various speech/audio tasks. While high-fidelity reconstruction is paramount for human perception, audio coding for machines (ACoM) prioritizes efficient compression and downstream task performance, disregarding perceptual nuances. This work introduces an efficient ACoM method that can compress and quantize any chosen intermediate feature representation of an already trained speech/audio downstream model. Our approach employs task-specific loss guidance alongside residual vector quantization (RVQ) losses, providing ultra-low bitrates (i.e., less than 200 bps) with a minimal loss of the downstream model performance. The resulting tokenizer is adaptable to various bitrates and model sizes for flexible deployment. Evaluated on automatic speech recognition and audio classification, our method demonstrates its efficacy and potential for broader task and architectural applicability through appropriate regularization.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.12701v1",
    "published_date": "2025-07-17 00:32:07 UTC",
    "updated_date": "2025-07-17 00:32:07 UTC"
  }
]
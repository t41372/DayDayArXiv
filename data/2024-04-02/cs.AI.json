{
  "date": "2024-04-02",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-04-02 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文主要聚焦于人工智能和机器学习领域，涵盖 LLM 的安全与推理优化、多模态模型应用、机器人控制和图像生成等关键话题，其中 Tomas Mikolov 等知名学者的作品（如论文 7）令人印象深刻，同时 LLM 攻击与强化学习的相关研究（如论文 25 和 16）显示出高话题度。\n\n下面，我挑选并简要讨论几篇重要的论文，先从 LLM 相关的高影响力作品入手，再聊一些创新性强的机器人和图像处理论文，其他论文如能源管理或常规分类任务则快速掠过，以控制篇幅。每个条目包括论文标题（中文 + 英文）和核心贡献。\n\n**1. 论文 7: Collapse of Self-trained Language Models（自训练语言模型的崩溃）**  \n作者包括 Tomas Mikolov，这篇 ICLR 2024 论文探讨了 LLM（如 GPT-2）在自训练中的问题。主要贡献：发现持续自训练会导致模型性能急剧下降，生成重复或无效输出，揭示了 LLM 训练的潜在局限性，强调了避免训练退化的重要性。\n\n**2. 论文 25: Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks（使用简单自适应攻击越狱领先的安全对齐 LLM）**  \n这篇 ICLR 2025 论文分析了 LLM 的安全漏洞。主要发现：提出一种自适应攻击方法，能以 100% 成功率越狱多种模型（如 GPT-4o 和 Claude），并通过随机搜索优化攻击，突显了 LLM 安全机制的脆弱性，对 AI 安全研究有重大启示。\n\n**3. 论文 16: LM²: A Simple Society of Language Models Solves Complex Reasoning（LM²: 一个简单语言模型社会解决复杂推理）**  \n作者团队来自 IIITD 和其他机构，聚焦 LLM 在复杂推理任务上的提升。主要贡献：引入多模型协作框架（包括分解、求解和验证模块），在 MATH 和 MedQA 等基准上超越基线 8%以上，展示了 LLM 通过社会化协作提升推理能力的潜力。\n\n**4. 论文 31: Advancing LLM Reasoning Generalists with Preference Trees（使用偏好树推进 LLM 推理通用模型）**  \n这篇论文由 OpenBMB 团队发布，强调 LLM 的推理优化。主要发现：提出基于偏好树的训练方法，提升模型在数学和代码任务上的性能，实验显示在 MATH 和 JEEBench 上比基线高 7-9%，并开源代码，适用于高效 LLM 微调。\n\n**5. 论文 49: SGSH: Stimulate Large Language Models with Skeleton Heuristics for Knowledge Base Question Generation（SGSH: 使用骨架启发式刺激 LLM 用于知识库问题生成）**  \n论文聚焦 LLM 在知识图谱问答中的应用。主要贡献：开发框架通过骨架启发式提示生成高质量问题，在 KBQG 任务上达到新 SOTA，显著提升问题生成效率和准确性。\n\n**6. 论文 74: PhysORD: A Neuro-Symbolic Approach for Physics-infused Motion Prediction in Off-road Driving（PhysORD: 一种神经符号方法用于非道路驾驶的物理注入运动预测）**  \n这篇论文结合物理模拟和 AI，针对机器人控制。主要发现：提出神经符号框架融合欧拉-拉格朗日方程，提升非道路车辆预测准确性 46.7%，并在真实场景验证，展示了 AI 在动态环境的鲁棒性。\n\n**7. 论文 82: SpiKernel: A Kernel Size Exploration Methodology for Improving Accuracy of the Embedded Spiking Neural Network Systems（SpiKernel: 一种核大小探索方法提升嵌入式脉冲神经网络系统的准确性）**  \n论文针对嵌入式系统优化 SNN。主要贡献：通过核大小探索和神经架构搜索，提升 SNN 在图像任务上的准确性（CIFAR10 上达 93.24%），并减少参数量 10 倍，适用于资源受限的边缘设备。\n\n**8. 论文 89: Gen4DS: Workshop on Data Storytelling in an Era of Generative AI（Gen4DS: 生成 AI 时代的数据叙事研讨会）**  \n这篇快速掠过的论文讨论生成 AI 在数据可视化中的角色。主要发现：概述生成 AI 如何提升数据叙事，并提出互动活动框架，促进 AI 与人类协作的创新应用。\n\n其他论文如能源管理和常规文本分类（例如论文 1 和 4），虽有贡献（如 MARL 在 V2G 优化），但相对常规且影响力较小，故不展开讨论。今天 arXiv 整体显示 AI 领域正向更安全、泛化强的方向发展，期待后续研究！",
  "papers": [
    {
      "arxiv_id": "2404.02361v2",
      "title": "EnergAIze: Multi Agent Deep Deterministic Policy Gradient for Vehicle to Grid Energy Management",
      "title_zh": "翻译失败",
      "authors": [
        "Tiago Fonseca",
        "Luis Ferreira",
        "Bernardo Cabral",
        "Ricardo Severino",
        "Isabel Praca"
      ],
      "abstract": "This paper investigates the increasing roles of Renewable Energy Sources\n(RES) and Electric Vehicles (EVs). While indicating a new era of sustainable\nenergy, these also introduce complex challenges, including the need to balance\nsupply and demand and smooth peak consumptions amidst rising EV adoption rates.\nAddressing these challenges requires innovative solutions such as Demand\nResponse (DR), energy flexibility management, Renewable Energy Communities\n(RECs), and more specifically for EVs, Vehicle-to-Grid (V2G). However, existing\nV2G approaches often fall short in real-world adaptability, global REC\noptimization with other flexible assets, scalability, and user engagement. To\nbridge this gap, this paper introduces EnergAIze, a Multi-Agent Reinforcement\nLearning (MARL) energy management framework, leveraging the Multi-Agent Deep\nDeterministic Policy Gradient (MADDPG) algorithm. EnergAIze enables\nuser-centric and multi-objective energy management by allowing each prosumer to\nselect from a range of personal management objectives, thus encouraging\nengagement. Additionally, it architects' data protection and ownership through\ndecentralized computing, where each prosumer can situate an energy management\noptimization node directly at their own dwelling. The local node not only\nmanages local energy assets but also fosters REC wide optimization. The\nefficacy of EnergAIze was evaluated through case studies employing the\nCityLearn simulation framework. These simulations were instrumental in\ndemonstrating EnergAIze's adeptness at implementing V2G technology within a REC\nand other energy assets. The results show reduction in peak loads, ramping,\ncarbon emissions, and electricity costs at the REC level while optimizing for\nindividual prosumers objectives.",
      "tldr_zh": "本研究探讨了可再生能源来源 (RES) 和电动汽车 (EVs) 的兴起所带来的挑战，包括供需平衡和平滑峰值消费的问题，并指出现有 Vehicle-to-Grid (V2G) 方法在真实世界适应性、全球 Renewable Energy Communities (RECs) 优化、扩展性和用户参与度方面存在不足。针对这些问题，作者提出 EnergAIze 框架，该框架基于 Multi-Agent Reinforcement Learning (MARL) 和 Multi-Agent Deep Deterministic Policy Gradient (MADDPG) 算法，实现用户中心的多目标能量管理，允许每个 prosumer 选择个人目标，并通过去中心化计算保护数据隐私。实验结果显示，使用 CityLearn 模拟框架，EnergAIze 在 REC 中有效实施 V2G 技术，显著降低了峰值负载、ramping、碳排放和电力成本，同时优化了个人用户目标。",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "6 pages, 6 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2404.02361v2",
      "published_date": "2024-04-02 23:16:17 UTC",
      "updated_date": "2024-04-09 16:32:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:54:18.547796"
    },
    {
      "arxiv_id": "2404.02353v1",
      "title": "Semantic Augmentation in Images using Language",
      "title_zh": "翻译失败",
      "authors": [
        "Sahiti Yerramilli",
        "Jayant Sravan Tamarapalli",
        "Tanmay Girish Kulkarni",
        "Jonathan Francis",
        "Eric Nyberg"
      ],
      "abstract": "Deep Learning models are incredibly data-hungry and require very large\nlabeled datasets for supervised learning. As a consequence, these models often\nsuffer from overfitting, limiting their ability to generalize to real-world\nexamples. Recent advancements in diffusion models have enabled the generation\nof photorealistic images based on textual inputs. Leveraging the substantial\ndatasets used to train these diffusion models, we propose a technique to\nutilize generated images to augment existing datasets. This paper explores\nvarious strategies for effective data augmentation to improve the out-of-domain\ngeneralization capabilities of deep learning models.",
      "tldr_zh": "深度学习模型需要大量标注数据，但容易过拟合，导致泛化能力不足。本文提出一种语义增强技术，利用扩散模型（diffusion models）基于文本输入生成逼真图像，从而扩充现有数据集。研究探索了多种策略，以提升深度学习模型的域外泛化（out-of-domain generalization）能力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.02353v1",
      "published_date": "2024-04-02 22:54:24 UTC",
      "updated_date": "2024-04-02 22:54:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:54:27.744571"
    },
    {
      "arxiv_id": "2404.02335v1",
      "title": "Multi-BERT: Leveraging Adapters and Prompt Tuning for Low-Resource Multi-Domain Adaptation",
      "title_zh": "翻译失败",
      "authors": [
        "Parham Abed Azad",
        "Hamid Beigy"
      ],
      "abstract": "The rapid expansion of texts' volume and diversity presents formidable\nchallenges in multi-domain settings. These challenges are also visible in the\nPersian name entity recognition (NER) settings. Traditional approaches, either\nemploying a unified model for multiple domains or individual models for each\ndomain, frequently pose significant limitations. Single models often struggle\nto capture the nuances of diverse domains, while utilizing multiple large\nmodels can lead to resource constraints, rendering the training of a model for\neach domain virtually impractical. Therefore, this paper introduces a novel\napproach composed of one core model with multiple sets of domain-specific\nparameters. We utilize techniques such as prompt tuning and adapters, combined\nwith the incorporation of additional layers, to add parameters that we can\ntrain for the specific domains. This enables the model to perform comparably to\nindividual models for each domain. Experimental results on different formal and\ninformal datasets show that by employing these added parameters, the proposed\nmodel significantly surpasses existing practical models in performance.\nRemarkably, the proposed model requires only one instance for training and\nstorage, yet achieves outstanding results across all domains, even surpassing\nthe state-of-the-art in some. Moreover, we analyze each adaptation strategy,\ndelineating its strengths, weaknesses, and optimal hyper-parameters for the\nPersian NER settings. Finally, we introduce a document-based domain detection\npipeline tailored for scenarios with unknown text domains, enhancing the\nadaptability and practicality of this paper in real-world applications.",
      "tldr_zh": "这篇论文提出了 Multi-BERT 框架，通过利用 adapters 和 prompt tuning 技术，在低资源多领域环境中实现高效适应，特别是针对波斯语命名实体识别 (NER)。该方法采用一个核心模型结合多个领域特定参数集（如额外层），避免了传统单一模型捕捉细节不足或多模型资源消耗过大的问题。实验结果显示，该框架在各种正式和非正式数据集上显著超越现有模型，甚至在某些领域达到或超过最先进水平，同时只需一个实例即可完成训练和存储。此外，论文分析了各适应策略的优缺点，并引入了一个基于文档的领域检测管道，以提升实际应用中的适应性和实用性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.02335v1",
      "published_date": "2024-04-02 22:15:48 UTC",
      "updated_date": "2024-04-02 22:15:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:54:41.514806"
    },
    {
      "arxiv_id": "2404.02330v1",
      "title": "Comparative Study of Domain Driven Terms Extraction Using Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Sandeep Chataut",
        "Tuyen Do",
        "Bichar Dip Shrestha Gurung",
        "Shiva Aryal",
        "Anup Khanal",
        "Carol Lushbough",
        "Etienne Gnimpieba"
      ],
      "abstract": "Keywords play a crucial role in bridging the gap between human understanding\nand machine processing of textual data. They are essential to data enrichment\nbecause they form the basis for detailed annotations that provide a more\ninsightful and in-depth view of the underlying data. Keyword/domain driven term\nextraction is a pivotal task in natural language processing, facilitating\ninformation retrieval, document summarization, and content categorization. This\nreview focuses on keyword extraction methods, emphasizing the use of three\nmajor Large Language Models(LLMs): Llama2-7B, GPT-3.5, and Falcon-7B. We\nemployed a custom Python package to interface with these LLMs, simplifying\nkeyword extraction. Our study, utilizing the Inspec and PubMed datasets,\nevaluates the performance of these models. The Jaccard similarity index was\nused for assessment, yielding scores of 0.64 (Inspec) and 0.21 (PubMed) for\nGPT-3.5, 0.40 and 0.17 for Llama2-7B, and 0.23 and 0.12 for Falcon-7B. This\npaper underlines the role of prompt engineering in LLMs for better keyword\nextraction and discusses the impact of hallucination in LLMs on result\nevaluation. It also sheds light on the challenges in using LLMs for keyword\nextraction, including model complexity, resource demands, and optimization\ntechniques.",
      "tldr_zh": "这篇论文比较了使用 Large Language Models (LLMs) 进行领域驱动关键词提取的方法，重点评估了 Llama2-7B、GPT-3.5 和 Falcon-7B 在 Inspec 和 PubMed 数据集上的性能，通过 Jaccard similarity index 指标进行量化。研究采用自定义 Python 包简化模型接口，并强调 prompt engineering 的重要性，以提升提取准确性，同时讨论了 LLMs 中的 hallucination 对结果评估的影响。结果显示 GPT-3.5 表现最佳（Inspec 得分 0.64，PubMed 得分 0.21），而 Llama2-7B 和 Falcon-7B 得分较低；论文还指出了使用 LLMs 的挑战，包括模型复杂度、资源需求和优化技术，为关键词提取应用提供了参考。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.02330v1",
      "published_date": "2024-04-02 22:04:51 UTC",
      "updated_date": "2024-04-02 22:04:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:54:55.397936"
    },
    {
      "arxiv_id": "2404.02319v2",
      "title": "Symbolic Prompt Program Search: A Structure-Aware Approach to Efficient Compile-Time Prompt Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Tobias Schnabel",
        "Jennifer Neville"
      ],
      "abstract": "In many modern LLM applications, such as retrieval augmented generation,\nprompts have become programs themselves. In these settings, prompt programs are\nrepeatedly called with different user queries or data instances. A big\npractical challenge is optimizing such prompt programs. Recent work has mostly\nfocused on either simple prompt programs or assumed that the general structure\nof a prompt program is fixed.\n  We introduce SAMMO, a framework to perform symbolic prompt program search for\ncompile-time optimizations of prompt programs. SAMMO represents prompt programs\non a symbolic level which allows for a rich set of transformations that can be\nsearched over during optimization. We show that SAMMO generalizes previous\nmethods and improves the performance of complex prompts on (1) instruction\ntuning, (2) RAG pipeline tuning, and (3) prompt compression, across several\ndifferent LLMs. We make all code available open-source at\nhttps://github.com/microsoft/sammo .",
      "tldr_zh": "本文提出了一种结构感知的框架 SAMMO，用于高效的编译时提示优化。SAMMO 通过在符号级别表示提示程序，启用丰富的转换搜索操作，从而推广了现有方法，适用于复杂提示程序的优化。实验结果显示，SAMMO 在指令调整、RAG 管道调整和提示压缩任务上显著提升了多种 LLM 的性能。作者已将代码开源至 https://github.com/microsoft/sammo。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.02319v2",
      "published_date": "2024-04-02 21:35:54 UTC",
      "updated_date": "2024-06-27 23:22:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:55:05.791127"
    },
    {
      "arxiv_id": "2404.02314v2",
      "title": "A Strong Baseline for Molecular Few-Shot Learning",
      "title_zh": "分子少样本学习的强大基线",
      "authors": [
        "Philippe Formont",
        "Hugo Jeannin",
        "Pablo Piantanida",
        "Ismail Ben Ayed"
      ],
      "abstract": "Few-shot learning has recently attracted significant interest in drug\ndiscovery, with a recent, fast-growing literature mostly involving convoluted\nmeta-learning strategies. We revisit the more straightforward fine-tuning\napproach for molecular data, and propose a regularized quadratic-probe loss\nbased on the the Mahalanobis distance. We design a dedicated block-coordinate\ndescent optimizer, which avoid the degenerate solutions of our loss.\nInterestingly, our simple fine-tuning approach achieves highly competitive\nperformances in comparison to state-of-the-art methods, while being applicable\nto black-box settings and removing the need for specific episodic pre-training\nstrategies. Furthermore, we introduce a new benchmark to assess the robustness\nof the competing methods to domain shifts. In this setting, our fine-tuning\nbaseline obtains consistently better results than meta-learning methods.",
      "tldr_zh": "这篇论文提出了一种针对分子 Few-Shot Learning 的简单 fine-tuning 基线方法，使用基于 Mahalanobis distance 的 regularized quadratic-probe loss，并设计了一个 block-coordinate descent optimizer 来避免解决方案退化。相比复杂的 meta-learning 策略，该方法在性能上与最先进模型相当，且适用于 black-box 设置，无需特定的 episodic pre-training。论文还引入了一个新基准来评估对 domain shifts 的鲁棒性，结果显示 fine-tuning 方法在这种场景下比 meta-learning 方法表现更优越。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Published in Transactions on Machine Learning Research (02/2025)",
      "pdf_url": "http://arxiv.org/pdf/2404.02314v2",
      "published_date": "2024-04-02 21:20:51 UTC",
      "updated_date": "2025-02-07 15:21:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:55:17.438658"
    },
    {
      "arxiv_id": "2404.02305v1",
      "title": "Collapse of Self-trained Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "David Herel",
        "Tomas Mikolov"
      ],
      "abstract": "In various fields of knowledge creation, including science, new ideas often\nbuild on pre-existing information. In this work, we explore this concept within\nthe context of language models. Specifically, we explore the potential of\nself-training models on their own outputs, akin to how humans learn and build\non their previous thoughts and actions. While this approach is intuitively\nappealing, our research reveals its practical limitations. We find that\nextended self-training of the GPT-2 model leads to a significant degradation in\nperformance, resulting in repetitive and collapsed token output.",
      "tldr_zh": "这篇论文探讨了语言模型的自训练（self-training）机制，类似于人类基于既有信息构建新想法，具体通过让模型（如 GPT-2）在自身输出上进行迭代训练。研究发现，持续的自训练会导致模型性能显著下降，输出出现重复和崩溃（collapsed token）现象。总体而言，这揭示了自训练方法的实际限制，为改进语言模型的训练策略提供了重要启示。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ICLR 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.02305v1",
      "published_date": "2024-04-02 21:03:37 UTC",
      "updated_date": "2024-04-02 21:03:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:55:28.033954"
    },
    {
      "arxiv_id": "2404.02304v1",
      "title": "Virtual Sensor for Real-Time Bearing Load Prediction Using Heterogeneous Temporal Graph Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Mengjie Zhao",
        "Cees Taal",
        "Stephan Baggerohr",
        "Olga Fink"
      ],
      "abstract": "Accurate bearing load monitoring is essential for their Prognostics and\nHealth Management (PHM), enabling damage assessment, wear prediction, and\nproactive maintenance. While bearing sensors are typically placed on the\nbearing housing, direct load monitoring requires sensors inside the bearing\nitself. Recently introduced sensor rollers enable direct bearing load\nmonitoring but are constrained by their battery life. Data-driven virtual\nsensors can learn from sensor roller data collected during a batterys lifetime\nto map operating conditions to bearing loads. Although spatially distributed\nbearing sensors offer insights into load distribution (e.g., correlating\ntemperature with load), traditional machine learning algorithms struggle to\nfully exploit these spatial-temporal dependencies. To address this gap, we\nintroduce a graph-based virtual sensor that leverages Graph Neural Networks\n(GNNs) to analyze spatial-temporal dependencies among sensor signals, mapping\nexisting measurements (temperature, vibration) to bearing loads. Since\ntemperature and vibration signals exhibit vastly different dynamics, we propose\nHeterogeneous Temporal Graph Neural Networks (HTGNN), which explicitly models\nthese signal types and their interactions for effective load prediction. Our\nresults demonstrate that HTGNN outperforms Convolutional Neural Networks\n(CNNs), which struggle to capture both spatial and heterogeneous signal\ncharacteristics. These findings highlight the importance of capturing the\ncomplex spatial interactions between temperature, vibration, and load.",
      "tldr_zh": "该研究提出了一种基于 Heterogeneous Temporal Graph Neural Networks (HTGNN) 的虚拟传感器，用于实时预测轴承负载，以支持轴承的 Prognostics and Health Management (PHM)。HTGNN 通过建模温度、振动等异构信号的空间-时间依赖性及其交互关系，解决了传统机器学习算法无法充分利用分布式传感器数据的局限性。实验结果显示，HTGNN 比 Convolutional Neural Networks (CNNs) 性能更优越，提升了负载预测准确性，并突出了温度、振动和负载之间复杂空间交互的重要性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.LG",
      "comment": "8 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2404.02304v1",
      "published_date": "2024-04-02 21:03:17 UTC",
      "updated_date": "2024-04-02 21:03:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:55:41.193852"
    },
    {
      "arxiv_id": "2404.08668v2",
      "title": "A Comprehensive Survey on AI-based Methods for Patents",
      "title_zh": "翻译失败",
      "authors": [
        "Homaira Huda Shomee",
        "Zhu Wang",
        "Sathya N. Ravi",
        "Sourav Medya"
      ],
      "abstract": "Recent advancements in Artificial Intelligence (AI) and machine learning have\ndemonstrated transformative capabilities across diverse domains. This progress\nextends to the field of patent analysis and innovation, where AI-based tools\npresent opportunities to streamline and enhance important tasks in the patent\ncycle such as classification, retrieval, and valuation prediction. This not\nonly accelerates the efficiency of patent researchers and applicants but also\nopens new avenues for technological innovation and discovery. Our survey\nprovides a comprehensive summary of recent AI tools in patent analysis from\nmore than 40 papers from 26 venues between 2017 and 2023. Unlike existing\nsurveys, we include methods that work for patent image and text data.\nFurthermore, we introduce a novel taxonomy for the categorization based on the\ntasks in the patent life cycle as well as the specifics of the AI methods. This\ninterdisciplinary survey aims to serve as a resource for researchers and\npractitioners who are working at the intersection of AI and patent analysis as\nwell as the patent offices that are aiming to build efficient patent systems.",
      "tldr_zh": "这篇调查论文全面总结了2017-2023年间超过40篇论文中，Artificial Intelligence (AI) 在专利分析中的应用方法，包括专利文本和图像数据的处理。作者引入了一个新颖的分类体系（taxonomy），基于专利生命周期的任务（如classification、retrieval和valuation prediction）以及AI方法的具体特性，帮助提升专利研究效率和创新潜力。与现有调查不同，该研究强调了AI在专利图像和文本领域的跨学科整合，旨在为AI与专利分析交叉领域的研究者、从业者和专利办公室提供宝贵资源。最终，该调查为构建高效专利系统奠定基础，促进技术发现和应用。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.08668v2",
      "published_date": "2024-04-02 20:44:06 UTC",
      "updated_date": "2024-06-18 04:58:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:55:52.727484"
    },
    {
      "arxiv_id": "2404.02287v1",
      "title": "One Noise to Rule Them All: Multi-View Adversarial Attacks with Universal Perturbation",
      "title_zh": "翻译失败",
      "authors": [
        "Mehmet Ergezer",
        "Phat Duong",
        "Christian Green",
        "Tommy Nguyen",
        "Abdurrahman Zeybey"
      ],
      "abstract": "This paper presents a novel universal perturbation method for generating\nrobust multi-view adversarial examples in 3D object recognition. Unlike\nconventional attacks limited to single views, our approach operates on multiple\n2D images, offering a practical and scalable solution for enhancing model\nscalability and robustness. This generalizable method bridges the gap between\n2D perturbations and 3D-like attack capabilities, making it suitable for\nreal-world applications.\n  Existing adversarial attacks may become ineffective when images undergo\ntransformations like changes in lighting, camera position, or natural\ndeformations. We address this challenge by crafting a single universal noise\nperturbation applicable to various object views. Experiments on diverse\nrendered 3D objects demonstrate the effectiveness of our approach. The\nuniversal perturbation successfully identified a single adversarial noise for\neach given set of 3D object renders from multiple poses and viewpoints.\nCompared to single-view attacks, our universal attacks lower classification\nconfidence across multiple viewing angles, especially at low noise levels. A\nsample implementation is made available at\nhttps://github.com/memoatwit/UniversalPerturbation.",
      "tldr_zh": "本论文提出了一种名为“One Noise to Rule Them All”的通用扰动方法，用于生成3D物体识别中的多视图对抗样本，该方法通过一个单一的universal perturbation来处理多个2D图像视图，从而提升模型的鲁棒性和可扩展性。不同于传统的单视图攻击，该方法能桥接2D扰动与3D-like攻击能力，并在图像变换（如光照或相机位置变化）下保持有效性。实验结果显示，在各种渲染的3D物体上，该通用攻击显著降低了分类置信度，尤其在低噪声水平下，比单视图攻击提高了29.32%的鲁棒性，并提供了开源实现（https://github.com/memoatwit/UniversalPerturbation）。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "6 pages, 4 figures, presented at ICAIA, Springer to publish under\n  Algorithms for Intelligent Systems",
      "pdf_url": "http://arxiv.org/pdf/2404.02287v1",
      "published_date": "2024-04-02 20:29:59 UTC",
      "updated_date": "2024-04-02 20:29:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:56:05.235790"
    },
    {
      "arxiv_id": "2404.05741v1",
      "title": "Enhancing Inference Efficiency of Large Language Models: Investigating Optimization Strategies and Architectural Innovations",
      "title_zh": "提升大型语言模型的推理效率：调查优化策略和架构创新",
      "authors": [
        "Georgy Tyukin"
      ],
      "abstract": "Large Language Models are growing in size, and we expect them to continue to\ndo so, as larger models train quicker. However, this increase in size will\nseverely impact inference costs. Therefore model compression is important, to\nretain the performance of larger models, but with a reduced cost of running\nthem. In this thesis we explore the methods of model compression, and we\nempirically demonstrate that the simple method of skipping latter attention\nsublayers in Transformer LLMs is an effective method of model compression, as\nthese layers prove to be redundant, whilst also being incredibly\ncomputationally expensive. We observed a 21% speed increase in one-token\ngeneration for Llama 2 7B, whilst surprisingly and unexpectedly improving\nperformance over several common benchmarks.",
      "tldr_zh": "该论文探讨了大型语言模型（Large Language Models）的规模增长导致推理成本增加的问题，并调查了模型压缩的优化策略和架构创新。研究者通过跳过 Transformer LLMs 中冗余的后部注意力子层（attention sublayers）作为一种简单有效的压缩方法，实现了模型性能的保留和计算效率的提升。实验结果显示，在 Llama 2 7B 模型上，一 token 生成速度提高了 21%，并在多个常见基准测试中意外地提升了性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.PF"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.05741v1",
      "published_date": "2024-04-02 19:53:54 UTC",
      "updated_date": "2024-04-02 19:53:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:56:17.065560"
    },
    {
      "arxiv_id": "2404.02269v1",
      "title": "Extracting Norms from Contracts Via ChatGPT: Opportunities and Challenges",
      "title_zh": "通过 ChatGPT 从合同中提取规范：机会和挑战",
      "authors": [
        "Amanul Haque",
        "Munindar P. Singh"
      ],
      "abstract": "We investigate the effectiveness of ChatGPT in extracting norms from\ncontracts. Norms provide a natural way to engineer multiagent systems by\ncapturing how to govern the interactions between two or more autonomous\nparties. We extract norms of commitment, prohibition, authorization, and power,\nalong with associated norm elements (the parties involved, antecedents, and\nconsequents) from contracts. Our investigation reveals ChatGPT's effectiveness\nand limitations in norm extraction from contracts. ChatGPT demonstrates\npromising performance in norm extraction without requiring training or\nfine-tuning, thus obviating the need for annotated data, which is not generally\navailable in this domain. However, we found some limitations of ChatGPT in\nextracting these norms that lead to incorrect norm extractions. The limitations\ninclude oversight of crucial details, hallucination, incorrect parsing of\nconjunctions, and empty norm elements. Enhanced norm extraction from contracts\ncan foster the development of more transparent and trustworthy formal agent\ninteraction specifications, thereby contributing to the improvement of\nmultiagent systems.",
      "tldr_zh": "本研究探讨了使用 ChatGPT 从合同中提取规范（norms）的有效性，包括承诺、禁止、授权和权力规范，以及相关元素（如参与方、前提和后果）。ChatGPT 无需训练或微调即可表现出色，从而避免了标注数据的需求。研究发现其存在局限性，如忽略关键细节、产生幻觉、错误解析连词以及空元素，导致规范提取不准确。最终，改进规范提取有望提升多代理系统（multiagent systems）的透明度和可信度，促进更可靠的代理互动规范。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at COINE-AAMAS 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.02269v1",
      "published_date": "2024-04-02 19:49:34 UTC",
      "updated_date": "2024-04-02 19:49:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:56:29.029989"
    },
    {
      "arxiv_id": "2405.06650v1",
      "title": "Large Language Models as Planning Domain Generators",
      "title_zh": "翻译失败",
      "authors": [
        "James Oswald",
        "Kavitha Srinivas",
        "Harsha Kokel",
        "Junkyu Lee",
        "Michael Katz",
        "Shirin Sohrabi"
      ],
      "abstract": "Developing domain models is one of the few remaining places that require\nmanual human labor in AI planning. Thus, in order to make planning more\naccessible, it is desirable to automate the process of domain model generation.\nTo this end, we investigate if large language models (LLMs) can be used to\ngenerate planning domain models from simple textual descriptions. Specifically,\nwe introduce a framework for automated evaluation of LLM-generated domains by\ncomparing the sets of plans for domain instances. Finally, we perform an\nempirical analysis of 7 large language models, including coding and chat models\nacross 9 different planning domains, and under three classes of natural\nlanguage domain descriptions. Our results indicate that LLMs, particularly\nthose with high parameter counts, exhibit a moderate level of proficiency in\ngenerating correct planning domains from natural language descriptions. Our\ncode is available at https://github.com/IBM/NL2PDDL.",
      "tldr_zh": "这篇论文探讨了使用大型语言模型 (LLMs) 自动生成 AI 规划领域模型，以减少手动劳动并提升规划的可访问性。研究引入了一个框架，通过比较领域实例的计划集来评估 LLM 生成的模型，并对 7 个 LLMs（包括编码和聊天模型）在 9 个规划领域和三种自然语言描述类别上进行了实证分析。结果表明，高参数 LLMs 在从自然语言描述生成正确规划领域方面表现出中等水平的熟练度，并提供了开源代码以供进一步研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Published at ICAPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.06650v1",
      "published_date": "2024-04-02 19:39:23 UTC",
      "updated_date": "2024-04-02 19:39:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:56:42.798180"
    },
    {
      "arxiv_id": "2404.02263v1",
      "title": "OFMPNet: Deep End-to-End Model for Occupancy and Flow Prediction in Urban Environment",
      "title_zh": "OFMPNet：用于城市环境中占用和流动预测的深度端到端模型",
      "authors": [
        "Youshaa Murhij",
        "Dmitry Yudin"
      ],
      "abstract": "The task of motion prediction is pivotal for autonomous driving systems,\nproviding crucial data to choose a vehicle behavior strategy within its\nsurroundings. Existing motion prediction techniques primarily focus on\npredicting the future trajectory of each agent in the scene individually,\nutilizing its past trajectory data. In this paper, we introduce an end-to-end\nneural network methodology designed to predict the future behaviors of all\ndynamic objects in the environment. This approach leverages the occupancy map\nand the scene's motion flow. We are investigatin various alternatives for\nconstructing a deep encoder-decoder model called OFMPNet. This model uses a\nsequence of bird's-eye-view road images, occupancy grid, and prior motion flow\nas input data. The encoder of the model can incorporate transformer,\nattention-based, or convolutional units. The decoder considers the use of both\nconvolutional modules and recurrent blocks. Additionally, we propose a novel\ntime-weighted motion flow loss, whose application has shown a substantial\ndecrease in end-point error. Our approach has achieved state-of-the-art results\non the Waymo Occupancy and Flow Prediction benchmark, with a Soft IoU of 52.1%\nand an AUC of 76.75% on Flow-Grounded Occupancy.",
      "tldr_zh": "该论文提出了一种端到端的深度神经网络模型 OFMPNet，用于预测城市环境中所有动态物体的未来行为，包括占用地图(occupancy map)和运动流(motion flow)。模型以鸟瞰视图道路图像、占用网格和先验运动流作为输入，编码器可采用 Transformer、注意力机制或卷积单元，解码器则结合卷积模块和循环块。研究者引入了一种新颖的时间加权运动流损失函数(time-weighted motion flow loss)，显著降低了端点错误。在 Waymo Occupancy and Flow Prediction 基准测试中，该方法实现了 Soft IoU 52.1% 和 Flow-Grounded Occupancy 的 AUC 76.75%，达到了最先进水平。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted in Neurocomputing journal - 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.02263v1",
      "published_date": "2024-04-02 19:37:58 UTC",
      "updated_date": "2024-04-02 19:37:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:56:55.217860"
    },
    {
      "arxiv_id": "2404.02261v2",
      "title": "LLMs in the Loop: Leveraging Large Language Model Annotations for Active Learning in Low-Resource Languages",
      "title_zh": "LLMs in the Loop：利用大型语言",
      "authors": [
        "Nataliia Kholodna",
        "Sahib Julka",
        "Mohammad Khodadadi",
        "Muhammed Nurullah Gumus",
        "Michael Granitzer"
      ],
      "abstract": "Low-resource languages face significant barriers in AI development due to\nlimited linguistic resources and expertise for data labeling, rendering them\nrare and costly. The scarcity of data and the absence of preexisting tools\nexacerbate these challenges, especially since these languages may not be\nadequately represented in various NLP datasets. To address this gap, we propose\nleveraging the potential of LLMs in the active learning loop for data\nannotation. Initially, we conduct evaluations to assess inter-annotator\nagreement and consistency, facilitating the selection of a suitable LLM\nannotator. The chosen annotator is then integrated into a training loop for a\nclassifier using an active learning paradigm, minimizing the amount of queried\ndata required. Empirical evaluations, notably employing GPT-4-Turbo,\ndemonstrate near-state-of-the-art performance with significantly reduced data\nrequirements, as indicated by estimated potential cost savings of at least\n42.45 times compared to human annotation. Our proposed solution shows promising\npotential to substantially reduce both the monetary and computational costs\nassociated with automation in low-resource settings. By bridging the gap\nbetween low-resource languages and AI, this approach fosters broader inclusion\nand shows the potential to enable automation across diverse linguistic\nlandscapes.",
      "tldr_zh": "这篇论文提出了一种方法，将大型语言模型（LLMs）整合到 active learning 循环中，用于低资源语言的数据标注，从而解决这些语言在 AI 开发中数据和专家短缺的挑战。研究首先评估 LLMs 的标注一致性，以选择合适的标注器（如 GPT-4-Turbo），并将其应用于分类器训练，显著减少所需的数据查询量。实验结果显示，该方法在性能上接近最先进水平，同时将标注成本降低了至少 42.45 倍。总体而言，此方案有助于降低低资源环境中的计算和经济成本，促进更多语言在 AI 领域的包容性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG",
        "I.2.7; I.2.6"
      ],
      "primary_category": "cs.CL",
      "comment": "20 pages, 6 tables. The source code related to this paper is\n  available at https://github.com/mkandai/llms-in-the-loop. This paper has been\n  accepted for publication at ECML PKDD 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.02261v2",
      "published_date": "2024-04-02 19:34:22 UTC",
      "updated_date": "2024-06-23 18:21:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:57:06.865737"
    },
    {
      "arxiv_id": "2404.02255v1",
      "title": "$\\texttt{LM}^\\texttt{2}$: A Simple Society of Language Models Solves Complex Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Gurusha Juneja",
        "Subhabrata Dutta",
        "Tanmoy Chakraborty"
      ],
      "abstract": "Despite demonstrating emergent reasoning abilities, Large Language Models\n(LLMS) often lose track of complex, multi-step reasoning. Existing studies show\nthat providing guidance via decomposing the original question into multiple\nsubproblems elicits more robustness in LLM reasoning -- a decomposer generates\nthe subproblems, and a solver solves each of these subproblems. However, these\ntechniques fail to accommodate coordination between the decomposer and the\nsolver modules (either in a single model or different specialized ones) -- the\ndecomposer does not keep track of the ability of the solver to follow the\ndecomposed reasoning. In this paper, we propose LM2 to address these\nchallenges. LM2 modularizes the decomposition, solution, and verification into\nthree different language models. The decomposer module identifies the key\nconcepts necessary to solve the problem and generates step-by-step subquestions\naccording to the reasoning requirement. The solver model generates the solution\nto the subproblems that are then checked by the verifier module; depending upon\nthe feedback from the verifier, the reasoning context is constructed using the\nsubproblems and the solutions. These models are trained to coordinate using\npolicy learning. Exhaustive experimentation suggests the superiority of LM2\nover existing methods on in- and out-domain reasoning problems, outperforming\nthe best baselines by $8.1\\%$ on MATH, $7.71\\%$ on JEEBench, and $9.7\\%$ on\nMedQA problems (code available at\nhttps://github.com/LCS2-IIITD/Language_Model_Multiplex).",
      "tldr_zh": "尽管大型语言模型 (LLMs) 在复杂多步推理中容易丢失跟踪，现有的分解方法缺乏模块间协调，本文提出 $\\texttt{LM}^\\texttt{2}$ 框架，将推理过程模块化为三个语言模型：decomposer 用于生成步-by-step 子问题，solver 用于解决这些子问题，以及 verifier 用于检查解决方案并反馈以构建推理上下文。这些模型通过 policy learning 进行协调训练。实验结果显示，$\\texttt{LM}^\\texttt{2}$ 在 MATH、JEEBench 和 MedQA 等数据集上分别比最佳基线提升 8.1%、7.71% 和 9.7%，证明了其在提升推理鲁棒性的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.02255v1",
      "published_date": "2024-04-02 19:23:10 UTC",
      "updated_date": "2024-04-02 19:23:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:57:18.168579"
    },
    {
      "arxiv_id": "2404.02254v2",
      "title": "On Stronger Computational Separations Between Multimodal and Unimodal Machine Learning",
      "title_zh": "关于多模态与单模态机器学习之间更强的计算分离",
      "authors": [
        "Ari Karchmer"
      ],
      "abstract": "Recently, multimodal machine learning has enjoyed huge empirical success\n(e.g. GPT-4). Motivated to develop theoretical justification for this empirical\nsuccess, Lu (NeurIPS '23, ALT '24) introduces a theory of multimodal learning,\nand considers possible \\textit{separations} between theoretical models of\nmultimodal and unimodal learning. In particular, Lu (ALT '24) shows a\ncomputational separation, which is relevant to \\textit{worst-case} instances of\nthe learning task. In this paper, we give a stronger \\textit{average-case}\ncomputational separation, where for ``typical'' instances of the learning task,\nunimodal learning is computationally hard, but multimodal learning is easy. We\nthen question how ``natural'' the average-case separation is. Would it be\nencountered in practice? To this end, we prove that under basic conditions, any\ngiven computational separation between average-case unimodal and multimodal\nlearning tasks implies a corresponding cryptographic key agreement protocol. We\nsuggest to interpret this as evidence that very strong \\textit{computational}\nadvantages of multimodal learning may arise \\textit{infrequently} in practice,\nsince they exist only for the ``pathological'' case of inherently cryptographic\ndistributions. However, this does not apply to possible (super-polynomial)\n\\textit{statistical} advantages.",
      "tldr_zh": "该论文探讨了多模态机器学习（multimodal learning）和单模态机器学习（unimodal learning）之间的计算分离，扩展了Lu的先前工作，引入了一个更强的平均情况（average-case）计算分离：在典型任务实例中，多模态学习易于计算，而单模态学习则困难。作者证明，这种分离在基本条件下会暗示一个相应的加密密钥协议（cryptographic key agreement protocol），表明这种计算优势可能仅在“病态”分布中出现，而非常见实践场景。总体而言，这为多模态学习的经验成功提供了理论支撑，但强调了其计算优势的潜在局限性，不影响可能的统计优势（statistical advantages）。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "Appeared in ICML 2024. Camera-ready version",
      "pdf_url": "http://arxiv.org/pdf/2404.02254v2",
      "published_date": "2024-04-02 19:21:28 UTC",
      "updated_date": "2024-07-17 17:01:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:57:29.399605"
    },
    {
      "arxiv_id": "2404.02249v2",
      "title": "RAT: Retrieval-Augmented Transformer for Click-Through Rate Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Yushen Li",
        "Jinpeng Wang",
        "Tao Dai",
        "Jieming Zhu",
        "Jun Yuan",
        "Rui Zhang",
        "Shu-Tao Xia"
      ],
      "abstract": "Predicting click-through rates (CTR) is a fundamental task for Web\napplications, where a key issue is to devise effective models for feature\ninteractions. Current methodologies predominantly concentrate on modeling\nfeature interactions within an individual sample, while overlooking the\npotential cross-sample relationships that can serve as a reference context to\nenhance the prediction. To make up for such deficiency, this paper develops a\nRetrieval-Augmented Transformer (RAT), aiming to acquire fine-grained feature\ninteractions within and across samples. By retrieving similar samples, we\nconstruct augmented input for each target sample. We then build Transformer\nlayers with cascaded attention to capture both intra- and cross-sample feature\ninteractions, facilitating comprehensive reasoning for improved CTR prediction\nwhile retaining efficiency. Extensive experiments on real-world datasets\nsubstantiate the effectiveness of RAT and suggest its advantage in long-tail\nscenarios. The code has been open-sourced at\n\\url{https://github.com/YushenLi807/WWW24-RAT}.",
      "tldr_zh": "这篇论文提出了一种 Retrieval-Augmented Transformer (RAT) 模型，用于提升点击率 (CTR) 预测的准确性，通过检索类似样本构建增强输入，以弥补现有方法忽略跨样本特征交互的不足。RAT 采用级联注意力 Transformer 层，捕捉样本内 (intra-sample) 和样本间 (cross-sample) 的细粒度特征交互，实现全面推理，同时保持高效性。实验在真实数据集上验证了 RAT 的有效性，尤其在长尾场景中表现出显著优势。代码已开源，可在 GitHub 上获取。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG",
        "cs.SI"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted to The ACM Web Conference 2024 (WWW'24, short paper). Data\n  and code are available",
      "pdf_url": "http://arxiv.org/pdf/2404.02249v2",
      "published_date": "2024-04-02 19:14:23 UTC",
      "updated_date": "2024-04-05 02:13:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:57:42.405086"
    },
    {
      "arxiv_id": "2404.02235v1",
      "title": "Is Exploration All You Need? Effective Exploration Characteristics for Transfer in Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Jonathan C. Balloch",
        "Rishav Bhagat",
        "Geigh Zollicoffer",
        "Ruoran Jia",
        "Julia Kim",
        "Mark O. Riedl"
      ],
      "abstract": "In deep reinforcement learning (RL) research, there has been a concerted\neffort to design more efficient and productive exploration methods while\nsolving sparse-reward problems. These exploration methods often share common\nprinciples (e.g., improving diversity) and implementation details (e.g.,\nintrinsic reward). Prior work found that non-stationary Markov decision\nprocesses (MDPs) require exploration to efficiently adapt to changes in the\nenvironment with online transfer learning. However, the relationship between\nspecific exploration characteristics and effective transfer learning in deep RL\nhas not been characterized. In this work, we seek to understand the\nrelationships between salient exploration characteristics and improved\nperformance and efficiency in transfer learning. We test eleven popular\nexploration algorithms on a variety of transfer types -- or ``novelties'' -- to\nidentify the characteristics that positively affect online transfer learning.\nOur analysis shows that some characteristics correlate with improved\nperformance and efficiency across a wide range of transfer tasks, while others\nonly improve transfer performance with respect to specific environment changes.\nFrom our analysis, make recommendations about which exploration algorithm\ncharacteristics are best suited to specific transfer situations.",
      "tldr_zh": "本研究探讨了在深度强化学习（RL）中，探索特性的有效性及其对转移学习（transfer learning）的贡献，特别是针对稀疏奖励问题和非平稳Markov决策过程（MDPs）。研究者测试了11种流行探索算法（如基于内在奖励的变体），在多种转移类型（novelties）上进行实验，以分析哪些特性能提升性能和效率。结果显示，一些探索特性（如改善多样性）在广泛任务中普遍有效，而其他特性仅针对特定环境变化（如在线适应）有益；基于此，论文提供针对不同转移情况的算法推荐。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.02235v1",
      "published_date": "2024-04-02 18:45:01 UTC",
      "updated_date": "2024-04-02 18:45:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:57:54.604228"
    },
    {
      "arxiv_id": "2404.02227v1",
      "title": "OOSTraj: Out-of-Sight Trajectory Prediction With Vision-Positioning Denoising",
      "title_zh": "翻译失败",
      "authors": [
        "Haichao Zhang",
        "Yi Xu",
        "Hongsheng Lu",
        "Takayuki Shimizu",
        "Yun Fu"
      ],
      "abstract": "Trajectory prediction is fundamental in computer vision and autonomous\ndriving, particularly for understanding pedestrian behavior and enabling\nproactive decision-making. Existing approaches in this field often assume\nprecise and complete observational data, neglecting the challenges associated\nwith out-of-view objects and the noise inherent in sensor data due to limited\ncamera range, physical obstructions, and the absence of ground truth for\ndenoised sensor data. Such oversights are critical safety concerns, as they can\nresult in missing essential, non-visible objects. To bridge this gap, we\npresent a novel method for out-of-sight trajectory prediction that leverages a\nvision-positioning technique. Our approach denoises noisy sensor observations\nin an unsupervised manner and precisely maps sensor-based trajectories of\nout-of-sight objects into visual trajectories. This method has demonstrated\nstate-of-the-art performance in out-of-sight noisy sensor trajectory denoising\nand prediction on the Vi-Fi and JRDB datasets. By enhancing trajectory\nprediction accuracy and addressing the challenges of out-of-sight objects, our\nwork significantly contributes to improving the safety and reliability of\nautonomous driving in complex environments. Our work represents the first\ninitiative towards Out-Of-Sight Trajectory prediction (OOSTraj), setting a new\nbenchmark for future research. The code is available at\n\\url{https://github.com/Hai-chao-Zhang/OOSTraj}.",
      "tldr_zh": "这篇论文提出了 OOSTraj，一种针对视野外轨迹预测的新方法，利用 vision-positioning 技术对噪声传感器数据进行无监督去噪，并将传感器-based 轨迹精确映射到视觉轨迹。相比现有方法，该方法解决了出-of-view 物体带来的安全挑战，并在 Vi-Fi 和 JRDB 数据集上实现了 state-of-the-art 性能，准确率显著提升。总体上，该工作提高了自动驾驶系统的安全性和可靠性，并首次为 Out-Of-Sight Trajectory prediction 设定了新基准。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "In Proceedings of IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition 2024 (CVPR)",
      "pdf_url": "http://arxiv.org/pdf/2404.02227v1",
      "published_date": "2024-04-02 18:30:29 UTC",
      "updated_date": "2024-04-02 18:30:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:58:06.578178"
    },
    {
      "arxiv_id": "2404.02225v2",
      "title": "CHOSEN: Contrastive Hypothesis Selection for Multi-View Depth Refinement",
      "title_zh": "翻译失败",
      "authors": [
        "Di Qiu",
        "Yinda Zhang",
        "Thabo Beeler",
        "Vladimir Tankovich",
        "Christian Häne",
        "Sean Fanello",
        "Christoph Rhemann",
        "Sergio Orts Escolano"
      ],
      "abstract": "We propose CHOSEN, a simple yet flexible, robust and effective multi-view\ndepth refinement framework. It can be employed in any existing multi-view\nstereo pipeline, with straightforward generalization capability for different\nmulti-view capture systems such as camera relative positioning and lenses.\nGiven an initial depth estimation, CHOSEN iteratively re-samples and selects\nthe best hypotheses, and automatically adapts to different metric or intrinsic\nscales determined by the capture system. The key to our approach is the\napplication of contrastive learning in an appropriate solution space and a\ncarefully designed hypothesis feature, based on which positive and negative\nhypotheses can be effectively distinguished. Integrated in a simple baseline\nmulti-view stereo pipeline, CHOSEN delivers impressive quality in terms of\ndepth and normal accuracy compared to many current deep learning based\nmulti-view stereo pipelines.",
      "tldr_zh": "本文提出CHOSEN框架，这是一种简单、灵活且鲁棒的多视图深度细化（Multi-View Depth Refinement）方法，能够无缝整合到现有多视图立体（Multi-View Stereo）管道中，并适用于不同相机相对定位和镜头系统。CHOSEN的核心在于应用对比学习（Contrastive Learning）在适当的解决方案空间中，结合精心设计的假设特征，迭代重新采样和选择最佳假设，同时自动适应各种度量或内在尺度。实验结果显示，该框架在简单基线管道中整合后，在深度和法线准确性方面显著优于许多当前基于深度学习的多视图立体方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.02225v2",
      "published_date": "2024-04-02 18:27:03 UTC",
      "updated_date": "2025-05-05 15:35:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:58:20.065174"
    },
    {
      "arxiv_id": "2404.02213v1",
      "title": "Exploring How Multiple Levels of GPT-Generated Programming Hints Support or Disappoint Novices",
      "title_zh": "翻译失败",
      "authors": [
        "Ruiwei Xiao",
        "Xinying Hou",
        "John Stamper"
      ],
      "abstract": "Recent studies have integrated large language models (LLMs) into diverse\neducational contexts, including providing adaptive programming hints, a type of\nfeedback focuses on helping students move forward during problem-solving.\nHowever, most existing LLM-based hint systems are limited to one single hint\ntype. To investigate whether and how different levels of hints can support\nstudents' problem-solving and learning, we conducted a think-aloud study with\n12 novices using the LLM Hint Factory, a system providing four levels of hints\nfrom general natural language guidance to concrete code assistance, varying in\nformat and granularity. We discovered that high-level natural language hints\nalone can be helpless or even misleading, especially when addressing next-step\nor syntax-related help requests. Adding lower-level hints, like code examples\nwith in-line comments, can better support students. The findings open up future\nwork on customizing help responses from content, format, and granularity levels\nto accurately identify and meet students' learning needs.",
      "tldr_zh": "本研究探讨了不同级别的 GPT 生成编程提示如何支持或误导编程新手，通过一个 think-aloud 研究，使用 LLM Hint Factory 系统提供四级提示，从一般自然语言指导到具体代码辅助。结果显示，高层自然语言提示可能无助或误导，特别是针对下一步或语法相关的问题。相比之下，较低层提示如带有内联注释的代码示例，能更有效地帮助学生解决问题和学习。该研究为未来定制教育反馈提供了启示，包括从内容、格式和粒度层面精准满足学生的学习需求。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted CHI 2024 LBW - 10 pages",
      "pdf_url": "http://arxiv.org/pdf/2404.02213v1",
      "published_date": "2024-04-02 18:05:26 UTC",
      "updated_date": "2024-04-02 18:05:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:58:31.256949"
    },
    {
      "arxiv_id": "2404.02205v2",
      "title": "A Holistic Indicator of Polarization to Measure Online Sexism",
      "title_zh": "翻译失败",
      "authors": [
        "Vahid Ghafouri",
        "Jose Such",
        "Guillermo Suarez-Tangil"
      ],
      "abstract": "The online trend of the manosphere and feminist discourse on social networks\nrequires a holistic measure of the level of sexism in an online community. This\nindicator is important for policymakers and moderators of online communities\n(e.g., subreddits) and computational social scientists, either to revise\nmoderation strategies based on the degree of sexism or to match and compare the\ntemporal sexism across different platforms and communities with real-time\nevents and infer social scientific insights.\n  In this paper, we build a model that can provide a comparable holistic\nindicator of toxicity targeted toward male and female identity and male and\nfemale individuals. Despite previous supervised NLP methods that require\nannotation of toxic comments at the target level (e.g. annotating comments that\nare specifically toxic toward women) to detect targeted toxic comments, our\nindicator uses supervised NLP to detect the presence of toxicity and\nunsupervised word embedding association test to detect the target\nautomatically.\n  We apply our model to gender discourse communities (e.g., r/TheRedPill,\nr/MGTOW, r/FemaleDatingStrategy) to detect the level of toxicity toward genders\n(i.e., sexism). Our results show that our framework accurately and consistently\n(93% correlation) measures the level of sexism in a community. We finally\ndiscuss how our framework can be generalized in the future to measure qualities\nother than toxicity (e.g. sentiment, humor) toward general-purpose targets and\nturn into an indicator of different sorts of polarizations.",
      "tldr_zh": "本研究提出了一种整体指标（Holistic Indicator of Polarization），用于衡量在线社区中针对男性和女性身份的毒性水平（Online Sexism），以支持政策制定者、社区管理员和计算社会科学家分析性别歧视趋势。不同于传统监督NLP方法，该框架结合监督NLP检测毒性存在，并使用无监督word embedding association test自动识别目标，从而避免了针对特定群体的标注需求。在性别话语社区（如r/TheRedPill、r/MGTOW、r/FemaleDatingStrategy）中应用后，结果显示该指标准确且一致地测量性别歧视水平，相关性高达93%。未来，该框架可扩展到其他品质（如sentiment、humor），成为评估各种极化的通用工具。",
      "categories": [
        "cs.SI",
        "cs.AI"
      ],
      "primary_category": "cs.SI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.02205v2",
      "published_date": "2024-04-02 18:00:42 UTC",
      "updated_date": "2024-06-29 15:27:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:58:44.486809"
    },
    {
      "arxiv_id": "2404.02157v1",
      "title": "Segment Any 3D Object with Language",
      "title_zh": "翻译失败",
      "authors": [
        "Seungjun Lee",
        "Yuyang Zhao",
        "Gim Hee Lee"
      ],
      "abstract": "In this paper, we investigate Open-Vocabulary 3D Instance Segmentation\n(OV-3DIS) with free-form language instructions. Earlier works that rely on only\nannotated base categories for training suffer from limited generalization to\nunseen novel categories. Recent works mitigate poor generalizability to novel\ncategories by generating class-agnostic masks or projecting generalized masks\nfrom 2D to 3D, but disregard semantic or geometry information, leading to\nsub-optimal performance. Instead, generating generalizable but semantic-related\nmasks directly from 3D point clouds would result in superior outcomes. In this\npaper, we introduce Segment any 3D Object with LanguagE (SOLE), which is a\nsemantic and geometric-aware visual-language learning framework with strong\ngeneralizability by generating semantic-related masks directly from 3D point\nclouds. Specifically, we propose a multimodal fusion network to incorporate\nmultimodal semantics in both backbone and decoder. In addition, to align the 3D\nsegmentation model with various language instructions and enhance the mask\nquality, we introduce three types of multimodal associations as supervision.\nOur SOLE outperforms previous methods by a large margin on ScanNetv2,\nScanNet200, and Replica benchmarks, and the results are even close to the\nfully-supervised counterpart despite the absence of class annotations in the\ntraining. Furthermore, extensive qualitative results demonstrate the\nversatility of our SOLE to language instructions.",
      "tldr_zh": "本文研究 Open-Vocabulary 3D Instance Segmentation (OV-3DIS)，提出 SOLE 框架，用于使用自由形式的语言指令实现任意3D对象的语义和几何感知分割。SOLE 通过多模态融合网络在骨干网络和解码器中整合多模态语义，并引入三种多模态关联作为监督，以提升掩码质量和模型对语言指令的适应性。实验结果显示，SOLE 在 ScanNetv2、ScanNet200 和 Replica 基准上大幅优于现有方法，甚至接近全监督性能，尽管训练时未使用类别标注。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project Page: https://cvrp-sole.github.io",
      "pdf_url": "http://arxiv.org/pdf/2404.02157v1",
      "published_date": "2024-04-02 17:59:10 UTC",
      "updated_date": "2024-04-02 17:59:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:58:56.974902"
    },
    {
      "arxiv_id": "2404.02151v4",
      "title": "Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks",
      "title_zh": "翻译失败",
      "authors": [
        "Maksym Andriushchenko",
        "Francesco Croce",
        "Nicolas Flammarion"
      ],
      "abstract": "We show that even the most recent safety-aligned LLMs are not robust to\nsimple adaptive jailbreaking attacks. First, we demonstrate how to successfully\nleverage access to logprobs for jailbreaking: we initially design an\nadversarial prompt template (sometimes adapted to the target LLM), and then we\napply random search on a suffix to maximize a target logprob (e.g., of the\ntoken \"Sure\"), potentially with multiple restarts. In this way, we achieve 100%\nattack success rate -- according to GPT-4 as a judge -- on Vicuna-13B,\nMistral-7B, Phi-3-Mini, Nemotron-4-340B, Llama-2-Chat-7B/13B/70B,\nLlama-3-Instruct-8B, Gemma-7B, GPT-3.5, GPT-4o, and R2D2 from HarmBench that\nwas adversarially trained against the GCG attack. We also show how to jailbreak\nall Claude models -- that do not expose logprobs -- via either a transfer or\nprefilling attack with a 100% success rate. In addition, we show how to use\nrandom search on a restricted set of tokens for finding trojan strings in\npoisoned models -- a task that shares many similarities with jailbreaking --\nwhich is the algorithm that brought us the first place in the SaTML'24 Trojan\nDetection Competition. The common theme behind these attacks is that adaptivity\nis crucial: different models are vulnerable to different prompting templates\n(e.g., R2D2 is very sensitive to in-context learning prompts), some models have\nunique vulnerabilities based on their APIs (e.g., prefilling for Claude), and\nin some settings, it is crucial to restrict the token search space based on\nprior knowledge (e.g., for trojan detection). For reproducibility purposes, we\nprovide the code, logs, and jailbreak artifacts in the JailbreakBench format at\nhttps://github.com/tml-epfl/llm-adaptive-attacks.",
      "tldr_zh": "这篇论文揭示了即使是最先进的 safety-aligned LLMs（如 Vicuna-13B 和 GPT-4o）也容易受到简单自适应攻击的影响。研究者提出了一种方法，通过设计对抗提示模板（adversarial prompt template）并在后缀上应用随机搜索（random search）来最大化目标 logprobs，从而实现100%攻击成功率，包括针对特定攻击训练的模型。实验还证明，对于不暴露 logprobs 的模型（如 Claude），转移攻击或预填充攻击同样有效，该方法在 SaTML'24 Trojan Detection Competition 中获胜，并提供了代码以供复现。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted at ICLR 2025. Updates in the v3: GPT-4o and Claude 3.5\n  Sonnet results, improved writing. Updates in the v2: more models (Llama3,\n  Phi-3, Nemotron-4-340B), jailbreak artifacts for all attacks are available,\n  evaluation with different judges (Llama-3-70B and Llama Guard 2), more\n  experiments (convergence plots, ablation on the suffix length for random\n  search), examples of jailbroken generation",
      "pdf_url": "http://arxiv.org/pdf/2404.02151v4",
      "published_date": "2024-04-02 17:58:27 UTC",
      "updated_date": "2025-04-17 18:55:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:59:08.196715"
    },
    {
      "arxiv_id": "2404.02127v2",
      "title": "LawInstruct: A Resource for Studying Language Model Adaptation to the Legal Domain",
      "title_zh": "LawInstruct：用于研究语言模型适应法律领域的资源",
      "authors": [
        "Joel Niklaus",
        "Lucia Zheng",
        "Arya D. McCarthy",
        "Christopher Hahn",
        "Brian M. Rosen",
        "Peter Henderson",
        "Daniel E. Ho",
        "Garrett Honke",
        "Percy Liang",
        "Christopher Manning"
      ],
      "abstract": "Instruction tuning is an important step in making language models useful for\ndirect user interaction. However, the legal domain is underrepresented in\ntypical instruction datasets (e.g., only 10 out of 1600+ tasks in\nSuper-NaturalInstructions). To study whether instruction tuning on legal\ndatasets is necessary for strong legal reasoning, we aggregate 58 annotated\nlegal datasets and write instructions for each, creating LawInstruct.\nLawInstruct covers 17 global jurisdictions, 24 languages and a total of 12M\nexamples across diverse tasks such as legal QA, summarization of court cases,\nand legal argument mining. We evaluate our models on LegalBench, measuring\nlegal reasoning across five categories in 162 challenging and realistic legal\ntasks, and MMLU, to measure potential drops in general reasoning capabilities.\nWe find that legal-specific instruction tuning on Flan-T5 - yielding FLawN-T5 -\nimproves performance on LegalBench across all model sizes, with an aggregate\nincrease of 15 points or 50% over Flan-T5 for the base size. No model size\nshows performance drops in MMLU. We publish LawInstruct as a resource for\nfurther study of instruction tuning in the legal domain.",
      "tldr_zh": "该研究介绍了 LawInstruct，这是一个针对语言模型适应法律领域的资源，通过聚合 58 个注释法律数据集并为其编写指令，创建了涵盖 17 个全球司法管辖区、24 种语言和 12M 示例的指令调优数据集。LawInstruct 支持多种任务，如法律问答、法院案例摘要和法律论点挖掘，以解决现有指令数据集（如 Super-NaturalInstructions）中法律领域的不足。实验结果显示，在 LegalBench 的 162 个法律任务上，对 Flan-T5 进行法律特定指令调优（产生 FLawN-T5）使性能提升 15 点或 50%（针对 base 模型），同时在 MMLU 的一般推理能力上未见下降。该资源已发布，供进一步研究语言模型在法律领域的指令调优。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "68T50",
        "I.2"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at Findings of NAACL 2025",
      "pdf_url": "http://arxiv.org/pdf/2404.02127v2",
      "published_date": "2024-04-02 17:33:34 UTC",
      "updated_date": "2025-01-23 06:54:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:59:20.557098"
    },
    {
      "arxiv_id": "2404.15311v2",
      "title": "Fusing Pretrained ViTs with TCNet for Enhanced EEG Regression",
      "title_zh": "融合预训练 ViTs 与 TCNet 以增强 EEG 回归",
      "authors": [
        "Eric Modesitt",
        "Haicheng Yin",
        "Williams Huang Wang",
        "Brian Lu"
      ],
      "abstract": "The task of Electroencephalogram (EEG) analysis is paramount to the\ndevelopment of Brain-Computer Interfaces (BCIs). However, to reach the goal of\ndeveloping robust, useful BCIs depends heavily on the speed and the accuracy at\nwhich BCIs can understand neural dynamics. In response to that goal, this paper\ndetails the integration of pre-trained Vision Transformers (ViTs) with Temporal\nConvolutional Networks (TCNet) to enhance the precision of EEG regression. The\ncore of this approach lies in harnessing the sequential data processing\nstrengths of ViTs along with the superior feature extraction capabilities of\nTCNet, to significantly improve EEG analysis accuracy. In addition, we analyze\nthe importance of how to construct optimal patches for the attention mechanism\nto analyze, balancing both speed and accuracy tradeoffs. Our results showcase a\nsubstantial improvement in regression accuracy, as evidenced by the reduction\nof Root Mean Square Error (RMSE) from 55.4 to 51.8 on EEGEyeNet's Absolute\nPosition Task, outperforming existing state-of-the-art models. Without\nsacrificing performance, we increase the speed of this model by an order of\nmagnitude (up to 4.32x faster). This breakthrough not only sets a new benchmark\nin EEG regression analysis but also opens new avenues for future research in\nthe integration of transformer architectures with specialized feature\nextraction methods for diverse EEG datasets.",
      "tldr_zh": "本文提出了一种整合预训练 Vision Transformers (ViTs) 与 Temporal Convolutional Networks (TCNet) 的方法，以提升 Electroencephalogram (EEG) 回归分析的精确度。该方法利用 ViTs 的序列数据处理优势和 TCNet 的特征提取能力，并通过优化 patches 构建来平衡速度与准确性 tradeoffs。在 EEGEyeNet 的 Absolute Position Task 上，实验结果显示 Root Mean Square Error (RMSE) 从 55.4 降低至 51.8，同时模型速度提升了 4.32 倍，超越现有最先进模型。这一创新为 Brain-Computer Interfaces (BCIs) 的 EEG 分析设定了新基准，并为未来整合 transformer 架构的研究开辟了新方向。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "Accepted HCI International 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.15311v2",
      "published_date": "2024-04-02 17:01:51 UTC",
      "updated_date": "2024-08-07 08:14:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:59:32.552962"
    },
    {
      "arxiv_id": "2404.02189v1",
      "title": "Insights from the Use of Previously Unseen Neural Architecture Search Datasets",
      "title_zh": "翻译失败",
      "authors": [
        "Rob Geada",
        "David Towers",
        "Matthew Forshaw",
        "Amir Atapour-Abarghouei",
        "A. Stephen McGough"
      ],
      "abstract": "The boundless possibility of neural networks which can be used to solve a\nproblem -- each with different performance -- leads to a situation where a Deep\nLearning expert is required to identify the best neural network. This goes\nagainst the hope of removing the need for experts. Neural Architecture Search\n(NAS) offers a solution to this by automatically identifying the best\narchitecture. However, to date, NAS work has focused on a small set of datasets\nwhich we argue are not representative of real-world problems. We introduce\neight new datasets created for a series of NAS Challenges: AddNIST, Language,\nMultNIST, CIFARTile, Gutenberg, Isabella, GeoClassing, and Chesseract. These\ndatasets and challenges are developed to direct attention to issues in NAS\ndevelopment and to encourage authors to consider how their models will perform\non datasets unknown to them at development time. We present experimentation\nusing standard Deep Learning methods as well as the best results from challenge\nparticipants.",
      "tldr_zh": "本论文探讨了神经架构搜索(NAS)的局限性，即现有NAS研究主要依赖一小部分数据集，这些数据集无法代表真实世界问题，从而仍需深度学习专家介入。作者引入了八个新数据集，包括AddNIST、Language、MultNIST、CIFARTile、Gutenberg、Isabella、GeoClassing和Chesseract，并组织了NAS挑战，以鼓励模型在开发时未知的数据集上进行评估。实验结果显示了标准深度学习方法以及挑战参与者表现，突显了这些数据集在提升NAS鲁棒性和泛化能力方面的潜在价值。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.02189v1",
      "published_date": "2024-04-02 16:48:34 UTC",
      "updated_date": "2024-04-02 16:48:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:59:44.256954"
    },
    {
      "arxiv_id": "2404.02931v1",
      "title": "READ: Improving Relation Extraction from an ADversarial Perspective",
      "title_zh": "翻译失败",
      "authors": [
        "Dawei Li",
        "William Hogan",
        "Jingbo Shang"
      ],
      "abstract": "Recent works in relation extraction (RE) have achieved promising benchmark\naccuracy; however, our adversarial attack experiments show that these works\nexcessively rely on entities, making their generalization capability\nquestionable. To address this issue, we propose an adversarial training method\nspecifically designed for RE. Our approach introduces both sequence- and\ntoken-level perturbations to the sample and uses a separate perturbation\nvocabulary to improve the search for entity and context perturbations.\nFurthermore, we introduce a probabilistic strategy for leaving clean tokens in\nthe context during adversarial training. This strategy enables a larger attack\nbudget for entities and coaxes the model to leverage relational patterns\nembedded in the context. Extensive experiments show that compared to various\nadversarial training methods, our method significantly improves both the\naccuracy and robustness of the model. Additionally, experiments on different\ndata availability settings highlight the effectiveness of our method in\nlow-resource scenarios. We also perform in-depth analyses of our proposed\nmethod and provide further hints. We will release our code at\nhttps://github.com/David-Li0406/READ.",
      "tldr_zh": "该论文发现现有关系抽取(RE)模型在基准测试中虽准确率高，但过度依赖实体，导致泛化能力不足。为解决此问题，作者提出READ方法，通过引入序列级和标记级扰动、单独的扰动词汇表，以及一种概率策略保留干净标记，来增强模型对上下文关系模式的利用。实验结果表明，与其他对抗训练方法相比，READ显著提高了模型的准确性和鲁棒性，并在低资源场景中表现出色。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by findings of NAACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.02931v1",
      "published_date": "2024-04-02 16:42:44 UTC",
      "updated_date": "2024-04-02 16:42:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:59:55.953937"
    },
    {
      "arxiv_id": "2404.02090v4",
      "title": "Already Moderate Population Sizes Provably Yield Strong Robustness to Noise",
      "title_zh": "翻译失败",
      "authors": [
        "Denis Antipov",
        "Benjamin Doerr",
        "Alexandra Ivanova"
      ],
      "abstract": "Experience shows that typical evolutionary algorithms can cope well with\nstochastic disturbances such as noisy function evaluations.\n  In this first mathematical runtime analysis of the $(1+\\lambda)$ and\n$(1,\\lambda)$ evolutionary algorithms in the presence of prior bit-wise noise,\nwe show that both algorithms can tolerate constant noise probabilities without\nincreasing the asymptotic runtime on the OneMax benchmark. For this, a\npopulation size $\\lambda$ suffices that is at least logarithmic in the problem\nsize $n$. The only previous result in this direction regarded the less\nrealistic one-bit noise model, required a population size super-linear in the\nproblem size, and proved a runtime guarantee roughly cubic in the noiseless\nruntime for the OneMax benchmark. Our significantly stronger results are based\non the novel proof argument that the noiseless offspring can be seen as a\nbiased uniform crossover between the parent and the noisy offspring. We are\noptimistic that the technical lemmas resulting from this insight will find\napplications also in future mathematical runtime analyses of evolutionary\nalgorithms.",
      "tldr_zh": "本文证明了在位噪声环境下，(1+λ) 和 (1,λ) evolutionary algorithms 仅需适中的种群大小λ（至少为问题规模n的对数），即可在OneMax基准测试中保持渐近运行时不变，从而展现出强大的噪声鲁棒性。研究采用新的证明方法，将无噪声后代视为父代和噪声后代之间的biased uniform crossover，这显著改善了先前针对one-bit noise模型的分析，后者需更大的种群大小且运行时保证较差。该发现为进化算法在噪声环境下的性能提供了更强的理论基础，并有望应用于未来的runtime analysis研究。",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "Full version of the same-titled paper accepted at GECCO 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.02090v4",
      "published_date": "2024-04-02 16:35:52 UTC",
      "updated_date": "2024-05-13 05:01:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:00:07.935273"
    },
    {
      "arxiv_id": "2404.02078v1",
      "title": "Advancing LLM Reasoning Generalists with Preference Trees",
      "title_zh": "翻译失败",
      "authors": [
        "Lifan Yuan",
        "Ganqu Cui",
        "Hanbin Wang",
        "Ning Ding",
        "Xingyao Wang",
        "Jia Deng",
        "Boji Shan",
        "Huimin Chen",
        "Ruobing Xie",
        "Yankai Lin",
        "Zhenghao Liu",
        "Bowen Zhou",
        "Hao Peng",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "abstract": "We introduce Eurus, a suite of large language models (LLMs) optimized for\nreasoning. Finetuned from Mistral-7B and CodeLlama-70B, Eurus models achieve\nstate-of-the-art results among open-source models on a diverse set of\nbenchmarks covering mathematics, code generation, and logical reasoning\nproblems. Notably, Eurus-70B beats GPT-3.5 Turbo in reasoning through a\ncomprehensive benchmarking across 12 tests covering five tasks, and achieves a\n33.3% pass@1 accuracy on LeetCode and 32.6% on TheoremQA, two challenging\nbenchmarks, substantially outperforming existing open-source models by margins\nmore than 13.3%. The strong performance of Eurus can be primarily attributed to\nUltraInteract, our newly-curated large-scale, high-quality alignment dataset\nspecifically designed for complex reasoning tasks. UltraInteract can be used in\nboth supervised fine-tuning and preference learning. For each instruction, it\nincludes a preference tree consisting of (1) reasoning chains with diverse\nplanning strategies in a unified format, (2) multi-turn interaction\ntrajectories with the environment and the critique, and (3) pairwise data to\nfacilitate preference learning. UltraInteract allows us to conduct an in-depth\nexploration of preference learning for reasoning tasks. Our investigation\nreveals that some well-established preference learning algorithms may be less\nsuitable for reasoning tasks compared to their effectiveness in general\nconversations. Inspired by this, we derive a novel reward modeling objective\nwhich, together with UltraInteract, leads to a strong reward model.",
      "tldr_zh": "本研究引入了 Eurus 系列大语言模型 (LLMs)，通过从 Mistral-7B 和 CodeLlama-70B 微调，优化了模型在数学、代码生成和逻辑推理等任务上的推理能力。Eurus-70B 在 12 个基准测试中超越了 GPT-3.5 Turbo，并在 LeetCode 和 TheoremQA 等挑战性基准上分别达到 33.3% 和 32.6% 的 pass@1 准确率，比现有开源模型高出超过 13.3%。为了实现这一性能，研究者构建了大规模、高质量数据集 UltraInteract，包括偏好树、推理链、多轮交互轨迹和配对数据，用于监督微调和偏好学习，并提出了一种新型奖励建模目标，以解决传统偏好学习算法在推理任务中的局限性。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Models and data are available at https://github.com/OpenBMB/Eurus",
      "pdf_url": "http://arxiv.org/pdf/2404.02078v1",
      "published_date": "2024-04-02 16:25:30 UTC",
      "updated_date": "2024-04-02 16:25:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:00:21.478070"
    },
    {
      "arxiv_id": "2404.02067v1",
      "title": "Red-Teaming Segment Anything Model",
      "title_zh": "翻译失败",
      "authors": [
        "Krzysztof Jankowski",
        "Bartlomiej Sobieski",
        "Mateusz Kwiatkowski",
        "Jakub Szulc",
        "Michal Janik",
        "Hubert Baniecki",
        "Przemyslaw Biecek"
      ],
      "abstract": "Foundation models have emerged as pivotal tools, tackling many complex tasks\nthrough pre-training on vast datasets and subsequent fine-tuning for specific\napplications. The Segment Anything Model is one of the first and most\nwell-known foundation models for computer vision segmentation tasks. This work\npresents a multi-faceted red-teaming analysis that tests the Segment Anything\nModel against challenging tasks: (1) We analyze the impact of style transfer on\nsegmentation masks, demonstrating that applying adverse weather conditions and\nraindrops to dashboard images of city roads significantly distorts generated\nmasks. (2) We focus on assessing whether the model can be used for attacks on\nprivacy, such as recognizing celebrities' faces, and show that the model\npossesses some undesired knowledge in this task. (3) Finally, we check how\nrobust the model is to adversarial attacks on segmentation masks under text\nprompts. We not only show the effectiveness of popular white-box attacks and\nresistance to black-box attacks but also introduce a novel approach - Focused\nIterative Gradient Attack (FIGA) that combines white-box approaches to\nconstruct an efficient attack resulting in a smaller number of modified pixels.\nAll of our testing methods and analyses indicate a need for enhanced safety\nmeasures in foundation models for image segmentation.",
      "tldr_zh": "本研究通过红-teaming（红队测试）对Segment Anything Model（SAM）进行了多方面评估，揭示其在计算机视觉分割任务中的潜在缺陷。具体地，测试包括分析风格转移（如恶劣天气和雨滴）对分割掩码的扭曲影响、评估模型在隐私攻击中的风险（如识别名人面部），以及检查其对基于文本提示的对抗攻击的鲁棒性。研究引入了一种新方法Focused Iterative Gradient Attack (FIGA)，结合白盒攻击策略，以更高效的方式减少修改像素数量。总体结果表明，SAM存在显著的安全漏洞，强调了在图像分割基石模型中增强安全措施的必要性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2024 - The 4th Workshop of Adversarial Machine Learning on\n  Computer Vision: Robustness of Foundation Models",
      "pdf_url": "http://arxiv.org/pdf/2404.02067v1",
      "published_date": "2024-04-02 16:07:50 UTC",
      "updated_date": "2024-04-02 16:07:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:00:33.398693"
    },
    {
      "arxiv_id": "2404.02187v1",
      "title": "A Generative Deep Learning Approach for Crash Severity Modeling with Imbalanced Data",
      "title_zh": "翻译失败",
      "authors": [
        "Junlan Chen",
        "Ziyuan Pu",
        "Nan Zheng",
        "Xiao Wen",
        "Hongliang Ding",
        "Xiucheng Guo"
      ],
      "abstract": "Crash data is often greatly imbalanced, with the majority of crashes being\nnon-fatal crashes, and only a small number being fatal crashes due to their\nrarity. Such data imbalance issue poses a challenge for crash severity modeling\nsince it struggles to fit and interpret fatal crash outcomes with very limited\nsamples. Usually, such data imbalance issues are addressed by data resampling\nmethods, such as under-sampling and over-sampling techniques. However, most\ntraditional and deep learning-based data resampling methods, such as synthetic\nminority oversampling technique (SMOTE) and generative Adversarial Networks\n(GAN) are designed dedicated to processing continuous variables. Though some\nresampling methods have improved to handle both continuous and discrete\nvariables, they may have difficulties in dealing with the collapse issue\nassociated with sparse discrete risk factors. Moreover, there is a lack of\ncomprehensive studies that compare the performance of various resampling\nmethods in crash severity modeling. To address the aforementioned issues, the\ncurrent study proposes a crash data generation method based on the Conditional\nTabular GAN. After data balancing, a crash severity model is employed to\nestimate the performance of classification and interpretation. A comparative\nstudy is conducted to assess classification accuracy and distribution\nconsistency of the proposed generation method using a 4-year imbalanced crash\ndataset collected in Washington State, U.S. Additionally, Monte Carlo\nsimulation is employed to estimate the performance of parameter and probability\nestimation in both two- and three-class imbalance scenarios. The results\nindicate that using synthetic data generated by CTGAN-RU for crash severity\nmodeling outperforms using original data or synthetic data generated by other\nresampling methods.",
      "tldr_zh": "这篇论文针对交通事故数据的不平衡问题（imbalanced data），提出了一种基于生成式深度学习方法，特别是使用 Conditional Tabular GAN (CTGAN) 的数据生成技术，来平衡非致命和致命事故样本。研究通过生成合成数据后，构建事故严重度模型，并使用华盛顿州 4 年真实数据集进行比较实验，包括与 SMOTE 和 GAN 等传统重采样方法的对比，以及 Monte Carlo simulation 的性能评估。结果表明，CTGAN-RU 生成的数据在分类准确性和分布一致性上显著优于原数据或其他方法，从而提高了模型的解释性和预测性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.02187v1",
      "published_date": "2024-04-02 16:07:27 UTC",
      "updated_date": "2024-04-02 16:07:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:00:45.505080"
    },
    {
      "arxiv_id": "2404.02063v2",
      "title": "SPMamba: State-space model is all you need in speech separation",
      "title_zh": "翻译失败",
      "authors": [
        "Kai Li",
        "Guo Chen",
        "Runxuan Yang",
        "Xiaolin Hu"
      ],
      "abstract": "Existing CNN-based speech separation models face local receptive field\nlimitations and cannot effectively capture long time dependencies. Although\nLSTM and Transformer-based speech separation models can avoid this problem,\ntheir high complexity makes them face the challenge of computational resources\nand inference efficiency when dealing with long audio. To address this\nchallenge, we introduce an innovative speech separation method called SPMamba.\nThis model builds upon the robust TF-GridNet architecture, replacing its\ntraditional BLSTM modules with bidirectional Mamba modules. These modules\neffectively model the spatiotemporal relationships between the time and\nfrequency dimensions, allowing SPMamba to capture long-range dependencies with\nlinear computational complexity. Specifically, the bidirectional processing\nwithin the Mamba modules enables the model to utilize both past and future\ncontextual information, thereby enhancing separation performance. Extensive\nexperiments conducted on public datasets, including WSJ0-2Mix, WHAM!, and\nLibri2Mix, as well as the newly constructed Echo2Mix dataset, demonstrated that\nSPMamba significantly outperformed existing state-of-the-art models, achieving\nsuperior results while also reducing computational complexity. These findings\nhighlighted the effectiveness of SPMamba in tackling the intricate challenges\nof speech separation in complex environments.",
      "tldr_zh": "该论文提出 SPMamba，一种基于 State-space 模型的语音分离方法，以解决 CNN 模型的局部感受野限制以及 LSTM 和 Transformer 模型的高计算复杂度问题。该模型在 TF-GridNet 架构基础上，将 BLSTM 模块替换为双向 Mamba 模块，能够有效捕捉时频维度的长程依赖，并以线性计算复杂度利用过去和未来上下文信息提升分离性能。在 WSJ0-2Mix、WHAM!、Libri2Mix 和 Echo2Mix 等数据集上的实验显示，SPMamba 显著优于现有最先进模型，同时降低了计算资源需求。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Technical Report. Code is available at\n  https://github.com/JusperLee/SPMamba",
      "pdf_url": "http://arxiv.org/pdf/2404.02063v2",
      "published_date": "2024-04-02 16:04:31 UTC",
      "updated_date": "2024-09-10 14:02:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:00:58.040664"
    },
    {
      "arxiv_id": "2404.02062v1",
      "title": "Digital Forgetting in Large Language Models: A Survey of Unlearning Methods",
      "title_zh": "大型语言模型中的数字遗忘：取消学习方法的调查",
      "authors": [
        "Alberto Blanco-Justicia",
        "Najeeb Jebreel",
        "Benet Manzanares",
        "David Sánchez",
        "Josep Domingo-Ferrer",
        "Guillem Collell",
        "Kuan Eeik Tan"
      ],
      "abstract": "The objective of digital forgetting is, given a model with undesirable\nknowledge or behavior, obtain a new model where the detected issues are no\nlonger present. The motivations for forgetting include privacy protection,\ncopyright protection, elimination of biases and discrimination, and prevention\nof harmful content generation. Effective digital forgetting has to be effective\n(meaning how well the new model has forgotten the undesired\nknowledge/behavior), retain the performance of the original model on the\ndesirable tasks, and be scalable (in particular forgetting has to be more\nefficient than retraining from scratch on just the tasks/data to be retained).\nThis survey focuses on forgetting in large language models (LLMs). We first\nprovide background on LLMs, including their components, the types of LLMs, and\ntheir usual training pipeline. Second, we describe the motivations, types, and\ndesired properties of digital forgetting. Third, we introduce the approaches to\ndigital forgetting in LLMs, among which unlearning methodologies stand out as\nthe state of the art. Fourth, we provide a detailed taxonomy of machine\nunlearning methods for LLMs, and we survey and compare current approaches.\nFifth, we detail datasets, models and metrics used for the evaluation of\nforgetting, retaining and runtime. Sixth, we discuss challenges in the area.\nFinally, we provide some concluding remarks.",
      "tldr_zh": "这篇论文对大型语言模型（LLMs）中的数字遗忘方法进行了全面调查，旨在通过unlearning技术移除模型的不良知识或行为，以实现隐私保护、版权保护以及消除偏见和有害内容。论文首先介绍了LLMs的背景、动机和期望属性，包括遗忘的有效性、性能保留以及可扩展性，然后详细分类和比较了各种unlearning方法。最终，论文讨论了评估数据集、模型、指标以及当前面临的挑战，为未来研究提供了参考。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG",
        "68",
        "K.4.1; I.2.6; I.2.7"
      ],
      "primary_category": "cs.CR",
      "comment": "70 pages",
      "pdf_url": "http://arxiv.org/pdf/2404.02062v1",
      "published_date": "2024-04-02 16:01:18 UTC",
      "updated_date": "2024-04-02 16:01:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:01:08.829817"
    },
    {
      "arxiv_id": "2404.02060v3",
      "title": "Long-context LLMs Struggle with Long In-context Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Tianle Li",
        "Ge Zhang",
        "Quy Duc Do",
        "Xiang Yue",
        "Wenhu Chen"
      ],
      "abstract": "Large Language Models (LLMs) have made significant strides in handling long\nsequences. Some models like Gemini could even to be capable of dealing with\nmillions of tokens. However, their performance evaluation has largely been\nconfined to metrics like perplexity and synthetic tasks, which may not fully\ncapture their true abilities in more challenging, real-world scenarios. We\nintroduce a benchmark (LongICLBench) for long in-context learning in\nextreme-label classification using six datasets with 28 to 174 classes and\ninput lengths from 2K to 50K tokens. Our benchmark requires LLMs to comprehend\nthe entire input to recognize the massive label spaces to make correct\npredictions. We evaluate on 15 long-context LLMs and find that they perform\nwell on less challenging classification tasks with smaller label space and\nshorter demonstrations. However, they struggle with more challenging task like\nDiscovery with 174 labels, suggesting a gap in their ability to process long,\ncontext-rich sequences. Further analysis reveals a bias towards labels\npresented later in the sequence and a need for improved reasoning over multiple\npieces of information. Our study reveals that long context understanding and\nreasoning is still a challenging task for the existing LLMs. We believe\nLongICLBench could serve as a more realistic evaluation for the future\nlong-context LLMs.",
      "tldr_zh": "这篇论文揭示了大型语言模型（LLMs）在长上下文学习任务上存在显著挑战，尽管它们能处理数百万tokens的序列。研究者引入了LongICLBench基准，使用六种数据集（标签数28至174，输入长度2K至50K tokens）来评估LLMs在极端标签分类中的表现。实验结果显示，15个长上下文LLMs在标签空间较小、演示较短的任务上表现良好，但在大规模任务如174标签的Discovery中挣扎，并表现出对序列后部标签的偏见以及对多信息推理的不足。该基准为未来LLMs的真实场景评估提供了更可靠的工具。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.02060v3",
      "published_date": "2024-04-02 15:59:11 UTC",
      "updated_date": "2024-06-12 02:46:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:01:22.840263"
    },
    {
      "arxiv_id": "2404.02047v3",
      "title": "Learning Transactions Representations for Information Management in Banks: Mastering Local, Global, and External Knowledge",
      "title_zh": "翻译失败",
      "authors": [
        "Alexandra Bazarova",
        "Maria Kovaleva",
        "Ilya Kuleshov",
        "Evgenia Romanenkova",
        "Alexander Stepikin",
        "Alexandr Yugay",
        "Dzhambulat Mollaev",
        "Ivan Kireev",
        "Andrey Savchenko",
        "Alexey Zaytsev"
      ],
      "abstract": "In today's world, banks use artificial intelligence to optimize diverse\nbusiness processes, aiming to improve customer experience. Most of the\ncustomer-related tasks can be categorized into two groups: 1) local ones, which\nfocus on a client's current state, such as transaction forecasting, and 2)\nglobal ones, which consider the general customer behaviour, e.g., predicting\nsuccessful loan repayment. Unfortunately, maintaining separate models for each\ntask is costly. Therefore, to better facilitate information management, we\ncompared eight state-of-the-art unsupervised methods on 11 tasks in search for\na one-size-fits-all solution. Contrastive self-supervised learning methods were\ndemonstrated to excel at global problems, while generative techniques were\nsuperior at local tasks. We also introduced a novel approach, which enriches\nthe client's representation by incorporating external information gathered from\nother clients. Our method outperforms classical models, boosting accuracy by up\nto 20\\%.",
      "tldr_zh": "该研究探讨了银行信息管理中，通过学习交易表示来处理局部（如交易预测）和全局（如贷款偿还预测）客户任务的问题，以避免维护单独模型的高成本。研究者比较了8种最先进的无监督 methods 在11个任务上的表现，发现Contrastive self-supervised learning methods 在全局问题上表现出色，而generative techniques 在局部任务上更具优势。同时，引入了一种新方法，通过整合外部信息（如其他客户的知识）来丰富客户表示，该方法比经典模型提升准确率高达20%，为银行AI优化提供通用解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.02047v3",
      "published_date": "2024-04-02 15:39:14 UTC",
      "updated_date": "2025-02-03 15:33:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:01:32.971943"
    },
    {
      "arxiv_id": "2404.02043v2",
      "title": "Cross-lingual Text Classification Transfer: The Case of Ukrainian",
      "title_zh": "跨语言文本分类迁移：乌克兰语的案例",
      "authors": [
        "Daryna Dementieva",
        "Valeriia Khylenko",
        "Georg Groh"
      ],
      "abstract": "Despite the extensive amount of labeled datasets in the NLP text\nclassification field, the persistent imbalance in data availability across\nvarious languages remains evident. To support further fair development of NLP\nmodels, exploring the possibilities of effective knowledge transfer to new\nlanguages is crucial. Ukrainian, in particular, stands as a language that still\ncan benefit from the continued refinement of cross-lingual methodologies. Due\nto our knowledge, there is a tremendous lack of Ukrainian corpora for typical\ntext classification tasks, i.e., different types of style, or harmful speech,\nor texts relationships. However, the amount of resources required for such\ncorpora collection from scratch is understandable. In this work, we leverage\nthe state-of-the-art advances in NLP, exploring cross-lingual knowledge\ntransfer methods avoiding manual data curation: large multilingual encoders and\ntranslation systems, LLMs, and language adapters. We test the approaches on\nthree text classification tasks -- toxicity classification, formality\nclassification, and natural language inference (NLI) -- providing the\n``recipe'' for the optimal setups for each task.",
      "tldr_zh": "这篇论文探讨了NLP文本分类中语言数据不平衡的问题，特别是乌克兰语缺乏标注数据集的问题。作者利用跨语言知识转移方法，包括大型多语言编码器、翻译系统、LLMs和language adapters，避免手动数据收集。实验在toxicity classification、formality classification和NLI三个任务上进行了测试，并提供了每个任务的最佳配置“recipe”，以促进公平的跨语言模型发展。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "COLING2025, main, short",
      "pdf_url": "http://arxiv.org/pdf/2404.02043v2",
      "published_date": "2024-04-02 15:37:09 UTC",
      "updated_date": "2025-02-04 20:08:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:01:46.416896"
    },
    {
      "arxiv_id": "2404.02039v2",
      "title": "A Survey on Large Language Model-Based Game Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Sihao Hu",
        "Tiansheng Huang",
        "Gaowen Liu",
        "Ramana Rao Kompella",
        "Fatih Ilhan",
        "Selim Furkan Tekin",
        "Yichang Xu",
        "Zachary Yahn",
        "Ling Liu"
      ],
      "abstract": "The development of game agents holds a critical role in advancing towards\nArtificial General Intelligence. The progress of Large Language Models (LLMs)\noffers an unprecedented opportunity to evolve and empower game agents with\nhuman-like decision-making capabilities in complex computer game environments.\nThis paper provides a comprehensive overview of LLM-based game agents from a\nholistic viewpoint. First, we introduce the conceptual architecture of\nLLM-based game agents, centered around three core functional components:\nmemory, reasoning and in/output. Second, we survey existing representative\nLLM-based game agents documented in the literature with respect to\nmethodologies and adaptation agility across six genres of games, including\nadventure, communication, competition, cooperation, simulation, and crafting &\nexploration games. Finally, we present an outlook of future research and\ndevelopment directions in this burgeoning field. A curated list of relevant\npapers is maintained and made accessible at:\nhttps://github.com/git-disl/awesome-LLM-game-agent-papers.",
      "tldr_zh": "这篇论文综述了基于Large Language Models (LLMs)的游戏代理在推进人工智能通用性方面的关键作用，强调LLMs如何赋予代理人类-like决策能力。论文介绍了LLM-based游戏代理的概念架构，包括memory、reasoning和in/output三大核心组件，并调研了现有代理在六种游戏类型（adventure、communication、competition、cooperation、simulation和crafting & exploration）中的方法和适应性。最终，它展望了该领域的未来研究方向，并提供了一个可访问的论文列表（https://github.com/git-disl/awesome-LLM-game-agent-papers）。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.02039v2",
      "published_date": "2024-04-02 15:34:18 UTC",
      "updated_date": "2025-03-30 18:42:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:01:57.469188"
    },
    {
      "arxiv_id": "2404.02037v1",
      "title": "MultiParaDetox: Extending Text Detoxification with Parallel Data to New Languages",
      "title_zh": "翻译失败",
      "authors": [
        "Daryna Dementieva",
        "Nikolay Babakov",
        "Alexander Panchenko"
      ],
      "abstract": "Text detoxification is a textual style transfer (TST) task where a text is\nparaphrased from a toxic surface form, e.g. featuring rude words, to the\nneutral register. Recently, text detoxification methods found their\napplications in various task such as detoxification of Large Language Models\n(LLMs) (Leong et al., 2023; He et al., 2024; Tang et al., 2023) and toxic\nspeech combating in social networks (Deng et al., 2023; Mun et al., 2023;\nAgarwal et al., 2023). All these applications are extremely important to ensure\nsafe communication in modern digital worlds. However, the previous approaches\nfor parallel text detoxification corpora collection -- ParaDetox (Logacheva et\nal., 2022) and APPADIA (Atwell et al., 2022) -- were explored only in\nmonolingual setup. In this work, we aim to extend ParaDetox pipeline to\nmultiple languages presenting MultiParaDetox to automate parallel\ndetoxification corpus collection for potentially any language. Then, we\nexperiment with different text detoxification models -- from unsupervised\nbaselines to LLMs and fine-tuned models on the presented parallel corpora --\nshowing the great benefit of parallel corpus presence to obtain\nstate-of-the-art text detoxification models for any language.",
      "tldr_zh": "本文提出MultiParaDetox框架，通过扩展ParaDetox管道，将文本净化（Text Detoxification）任务从单语环境扩展到多种语言，实现平行语料的自动化收集。研究方法包括实验不同模型，如无监督基线、LLMs和大语言模型在平行语料上的微调，展示了平行数据对提升模型性能的显著益处。结果表明，这种方法能为任意语言生成最先进的文本净化模型，促进数字通信的安全性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to NAACL2024",
      "pdf_url": "http://arxiv.org/pdf/2404.02037v1",
      "published_date": "2024-04-02 15:32:32 UTC",
      "updated_date": "2024-04-02 15:32:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:02:08.332309"
    },
    {
      "arxiv_id": "2404.02018v2",
      "title": "Large Language Models for Orchestrating Bimanual Robots",
      "title_zh": "大型语言模型用于编排双臂机器人",
      "authors": [
        "Kun Chu",
        "Xufeng Zhao",
        "Cornelius Weber",
        "Mengdi Li",
        "Wenhao Lu",
        "Stefan Wermter"
      ],
      "abstract": "Although there has been rapid progress in endowing robots with the ability to\nsolve complex manipulation tasks, generating control policies for bimanual\nrobots to solve tasks involving two hands is still challenging because of the\ndifficulties in effective temporal and spatial coordination. With emergent\nabilities in terms of step-by-step reasoning and in-context learning, Large\nLanguage Models (LLMs) have demonstrated promising potential in a variety of\nrobotic tasks. However, the nature of language communication via a single\nsequence of discrete symbols makes LLM-based coordination in continuous space a\nparticular challenge for bimanual tasks. To tackle this challenge, we present\nLAnguage-model-based Bimanual ORchestration (LABOR), an agent utilizing an LLM\nto analyze task configurations and devise coordination control policies for\naddressing long-horizon bimanual tasks. We evaluate our method through\nsimulated experiments involving two classes of long-horizon tasks using the\nNICOL humanoid robot. Our results demonstrate that our method outperforms the\nbaseline in terms of success rate. Additionally, we thoroughly analyze failure\ncases, offering insights into LLM-based approaches in bimanual robotic control\nand revealing future research trends. The project website can be found at\nhttp://labor-agent.github.io.",
      "tldr_zh": "该论文探讨了利用 Large Language Models (LLMs) 来协调双臂机器人的挑战，特别是处理时间和空间协调的难题。作者提出 LABOr 框架，该框架使用 LLM 通过逐步推理分析任务配置，并制定有效的控制策略来解决长时域双臂任务。在模拟实验中，使用 NICOL 人形机器人进行评估，结果显示 LABOr 的成功率优于基线模型，并通过分析失败案例提供了对 LLM 在双臂机器人控制中的见解和未来研究方向。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted in Humanoids 2024. The project website can be found at\n  http://labor-agent.github.io",
      "pdf_url": "http://arxiv.org/pdf/2404.02018v2",
      "published_date": "2024-04-02 15:08:35 UTC",
      "updated_date": "2024-10-10 15:07:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:02:22.699015"
    },
    {
      "arxiv_id": "2406.11852v2",
      "title": "Risks from Language Models for Automated Mental Healthcare: Ethics and Structure for Implementation",
      "title_zh": "语言模型在自动化心理保健中的风险：伦理与实施结构",
      "authors": [
        "Declan Grabb",
        "Max Lamparth",
        "Nina Vasan"
      ],
      "abstract": "Amidst the growing interest in developing task-autonomous AI for automated\nmental health care, this paper addresses the ethical and practical challenges\nassociated with the issue and proposes a structured framework that delineates\nlevels of autonomy, outlines ethical requirements, and defines beneficial\ndefault behaviors for AI agents in the context of mental health support. We\nalso evaluate fourteen state-of-the-art language models (ten off-the-shelf,\nfour fine-tuned) using 16 mental health-related questionnaires designed to\nreflect various mental health conditions, such as psychosis, mania, depression,\nsuicidal thoughts, and homicidal tendencies. The questionnaire design and\nresponse evaluations were conducted by mental health clinicians (M.D.s). We\nfind that existing language models are insufficient to match the standard\nprovided by human professionals who can navigate nuances and appreciate\ncontext. This is due to a range of issues, including overly cautious or\nsycophantic responses and the absence of necessary safeguards. Alarmingly, we\nfind that most of the tested models could cause harm if accessed in mental\nhealth emergencies, failing to protect users and potentially exacerbating\nexisting symptoms. We explore solutions to enhance the safety of current\nmodels. Before the release of increasingly task-autonomous AI systems in mental\nhealth, it is crucial to ensure that these models can reliably detect and\nmanage symptoms of common psychiatric disorders to prevent harm to users. This\ninvolves aligning with the ethical framework and default behaviors outlined in\nour study. We contend that model developers are responsible for refining their\nsystems per these guidelines to safeguard against the risks posed by current AI\ntechnologies to user mental health and safety.\n  Trigger warning: Contains and discusses examples of sensitive mental health\ntopics, including suicide and self-harm.",
      "tldr_zh": "这篇论文探讨了语言模型在自动化心理健康护理中的风险，提出一个结构化框架来定义AI代理的自治水平、伦理要求和有益默认行为，以应对相关挑战。研究评估了14个最先进的语言模型（包括10个现成模型和4个微调模型），通过16个心理健康相关问卷（涵盖精神病、躁狂、抑郁、自杀念头和杀人倾向）进行测试，结果显示这些模型因过于谨慎或缺乏安全措施，无法匹配人类专业人士的标准，并可能在紧急情况下加剧用户症状。论文强调，模型开发者有责任根据该框架改进系统，确保AI能可靠检测和管理常见精神障碍，从而防范潜在危害。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CY",
      "comment": "Updated with fine-tuned model results to match CoLM accepted\n  camera-ready version",
      "pdf_url": "http://arxiv.org/pdf/2406.11852v2",
      "published_date": "2024-04-02 15:05:06 UTC",
      "updated_date": "2024-08-14 18:20:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:02:34.596885"
    },
    {
      "arxiv_id": "2404.01986v1",
      "title": "Predicting the Intention to Interact with a Service Robot:the Role of Gaze Cues",
      "title_zh": "预测与服务机器人互动意图：注视线索的作用",
      "authors": [
        "Simone Arreghini",
        "Gabriele Abbate",
        "Alessandro Giusti",
        "Antonio Paolillo"
      ],
      "abstract": "For a service robot, it is crucial to perceive as early as possible that an\napproaching person intends to interact: in this case, it can proactively enact\nfriendly behaviors that lead to an improved user experience. We solve this\nperception task with a sequence-to-sequence classifier of a potential user\nintention to interact, which can be trained in a self-supervised way. Our main\ncontribution is a study of the benefit of features representing the person's\ngaze in this context. Extensive experiments on a novel dataset show that the\ninclusion of gaze cues significantly improves the classifier performance (AUROC\nincreases from 84.5% to 91.2%); the distance at which an accurate\nclassification can be achieved improves from 2.4 m to 3.2 m. We also quantify\nthe system's ability to adapt to new environments without external supervision.\nQualitative experiments show practical applications with a waiter robot.",
      "tldr_zh": "本研究开发了一个序列到序列分类器，用于服务机器人尽早预测人类互动意图，从而主动执行友好行为以提升用户体验。研究重点考察了目光线索（gaze cues）的作用，通过自监督学习训练模型，并在新数据集上进行实验，结果显示加入目光线索后，分类器性能显著提升（AUROC 从 84.5% 增加到 91.2%），准确分类距离从 2.4 m 提高到 3.2 m。该系统还展示了在无外部监督下适应新环境的潜力，并在服务员机器人等实际场景中验证了其实用应用。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.01986v1",
      "published_date": "2024-04-02 14:22:54 UTC",
      "updated_date": "2024-04-02 14:22:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:02:45.947800"
    },
    {
      "arxiv_id": "2404.01976v1",
      "title": "Joint-Task Regularization for Partially Labeled Multi-Task Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Kento Nishi",
        "Junsik Kim",
        "Wanhua Li",
        "Hanspeter Pfister"
      ],
      "abstract": "Multi-task learning has become increasingly popular in the machine learning\nfield, but its practicality is hindered by the need for large, labeled\ndatasets. Most multi-task learning methods depend on fully labeled datasets\nwherein each input example is accompanied by ground-truth labels for all target\ntasks. Unfortunately, curating such datasets can be prohibitively expensive and\nimpractical, especially for dense prediction tasks which require per-pixel\nlabels for each image. With this in mind, we propose Joint-Task Regularization\n(JTR), an intuitive technique which leverages cross-task relations to\nsimultaneously regularize all tasks in a single joint-task latent space to\nimprove learning when data is not fully labeled for all tasks. JTR stands out\nfrom existing approaches in that it regularizes all tasks jointly rather than\nseparately in pairs -- therefore, it achieves linear complexity relative to the\nnumber of tasks while previous methods scale quadratically. To demonstrate the\nvalidity of our approach, we extensively benchmark our method across a wide\nvariety of partially labeled scenarios based on NYU-v2, Cityscapes, and\nTaskonomy.",
      "tldr_zh": "这篇论文针对多任务学习（Multi-task learning）中对完全标注数据集的依赖问题，提出了一种Joint-Task Regularization (JTR)方法。该方法利用跨任务关系在单个联合任务潜在空间中同时正则化所有任务，从而提高了部分标注数据的学习效果，并将计算复杂度从二次方降低到线性。实验在NYU-v2、Cityscapes和Taskonomy数据集的各种部分标注场景中进行了基准测试，证明了JTR的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted paper to CVPR 2024 (main conference)",
      "pdf_url": "http://arxiv.org/pdf/2404.01976v1",
      "published_date": "2024-04-02 14:16:59 UTC",
      "updated_date": "2024-04-02 14:16:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:02:57.821756"
    },
    {
      "arxiv_id": "2404.01965v2",
      "title": "Towards Leveraging AutoML for Sustainable Deep Learning: A Multi-Objective HPO Approach on Deep Shift Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Leona Hennig",
        "Tanja Tornede",
        "Marius Lindauer"
      ],
      "abstract": "Deep Learning (DL) has advanced various fields by extracting complex patterns\nfrom large datasets. However, the computational demands of DL models pose\nenvironmental and resource challenges. Deep shift neural networks (DSNNs) offer\na solution by leveraging shift operations to reduce computational complexity at\ninference. Following the insights from standard DNNs, we are interested in\nleveraging the full potential of DSNNs by means of AutoML techniques. We study\nthe impact of hyperparameter optimization (HPO) to maximize DSNN performance\nwhile minimizing resource consumption. Since this combines multi-objective (MO)\noptimization with accuracy and energy consumption as potentially complementary\nobjectives, we propose to combine state-of-the-art multi-fidelity (MF) HPO with\nmulti-objective optimization. Experimental results demonstrate the\neffectiveness of our approach, resulting in models with over 80\\% in accuracy\nand low computational cost. Overall, our method accelerates efficient model\ndevelopment while enabling sustainable AI applications.",
      "tldr_zh": "该研究针对深度学习（DL）的计算需求所带来的环境和资源挑战，提出了一种利用 AutoML 的多目标超参数优化（HPO）方法，应用于 Deep Shift Neural Networks (DSNNs)，以最大化模型性能同时最小化资源消耗。具体而言，该方法结合多保真（MF）HPO 和多目标优化，平衡准确性和能量消耗目标。实验结果显示，该方法开发出的模型准确率超过80%，并显著降低计算成本，从而加速高效模型开发并推动可持续 AI 应用。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.01965v2",
      "published_date": "2024-04-02 14:03:37 UTC",
      "updated_date": "2024-04-04 10:54:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:03:09.436255"
    },
    {
      "arxiv_id": "2404.01954v2",
      "title": "HyperCLOVA X Technical Report",
      "title_zh": "HyperCLOVA X 技术报告",
      "authors": [
        "Kang Min Yoo",
        "Jaegeun Han",
        "Sookyo In",
        "Heewon Jeon",
        "Jisu Jeong",
        "Jaewook Kang",
        "Hyunwook Kim",
        "Kyung-Min Kim",
        "Munhyong Kim",
        "Sungju Kim",
        "Donghyun Kwak",
        "Hanock Kwak",
        "Se Jung Kwon",
        "Bado Lee",
        "Dongsoo Lee",
        "Gichang Lee",
        "Jooho Lee",
        "Baeseong Park",
        "Seongjin Shin",
        "Joonsang Yu",
        "Seolki Baek",
        "Sumin Byeon",
        "Eungsup Cho",
        "Dooseok Choe",
        "Jeesung Han",
        "Youngkyun Jin",
        "Hyein Jun",
        "Jaeseung Jung",
        "Chanwoong Kim",
        "Jinhong Kim",
        "Jinuk Kim",
        "Dokyeong Lee",
        "Dongwook Park",
        "Jeong Min Sohn",
        "Sujung Han",
        "Jiae Heo",
        "Sungju Hong",
        "Mina Jeon",
        "Hyunhoon Jung",
        "Jungeun Jung",
        "Wangkyo Jung",
        "Chungjoon Kim",
        "Hyeri Kim",
        "Jonghyun Kim",
        "Min Young Kim",
        "Soeun Lee",
        "Joonhee Park",
        "Jieun Shin",
        "Sojin Yang",
        "Jungsoon Yoon",
        "Hwaran Lee",
        "Sanghwan Bae",
        "Jeehwan Cha",
        "Karl Gylleus",
        "Donghoon Ham",
        "Mihak Hong",
        "Youngki Hong",
        "Yunki Hong",
        "Dahyun Jang",
        "Hyojun Jeon",
        "Yujin Jeon",
        "Yeji Jeong",
        "Myunggeun Ji",
        "Yeguk Jin",
        "Chansong Jo",
        "Shinyoung Joo",
        "Seunghwan Jung",
        "Adrian Jungmyung Kim",
        "Byoung Hoon Kim",
        "Hyomin Kim",
        "Jungwhan Kim",
        "Minkyoung Kim",
        "Minseung Kim",
        "Sungdong Kim",
        "Yonghee Kim",
        "Youngjun Kim",
        "Youngkwan Kim",
        "Donghyeon Ko",
        "Dughyun Lee",
        "Ha Young Lee",
        "Jaehong Lee",
        "Jieun Lee",
        "Jonghyun Lee",
        "Jongjin Lee",
        "Min Young Lee",
        "Yehbin Lee",
        "Taehong Min",
        "Yuri Min",
        "Kiyoon Moon",
        "Hyangnam Oh",
        "Jaesun Park",
        "Kyuyon Park",
        "Younghun Park",
        "Hanbae Seo",
        "Seunghyun Seo",
        "Mihyun Sim",
        "Gyubin Son",
        "Matt Yeo",
        "Kyung Hoon Yeom",
        "Wonjoon Yoo",
        "Myungin You",
        "Doheon Ahn",
        "Homin Ahn",
        "Joohee Ahn",
        "Seongmin Ahn",
        "Chanwoo An",
        "Hyeryun An",
        "Junho An",
        "Sang-Min An",
        "Boram Byun",
        "Eunbin Byun",
        "Jongho Cha",
        "Minji Chang",
        "Seunggyu Chang",
        "Haesong Cho",
        "Youngdo Cho",
        "Dalnim Choi",
        "Daseul Choi",
        "Hyoseok Choi",
        "Minseong Choi",
        "Sangho Choi",
        "Seongjae Choi",
        "Wooyong Choi",
        "Sewhan Chun",
        "Dong Young Go",
        "Chiheon Ham",
        "Danbi Han",
        "Jaemin Han",
        "Moonyoung Hong",
        "Sung Bum Hong",
        "Dong-Hyun Hwang",
        "Seongchan Hwang",
        "Jinbae Im",
        "Hyuk Jin Jang",
        "Jaehyung Jang",
        "Jaeni Jang",
        "Sihyeon Jang",
        "Sungwon Jang",
        "Joonha Jeon",
        "Daun Jeong",
        "Joonhyun Jeong",
        "Kyeongseok Jeong",
        "Mini Jeong",
        "Sol Jin",
        "Hanbyeol Jo",
        "Hanju Jo",
        "Minjung Jo",
        "Chaeyoon Jung",
        "Hyungsik Jung",
        "Jaeuk Jung",
        "Ju Hwan Jung",
        "Kwangsun Jung",
        "Seungjae Jung",
        "Soonwon Ka",
        "Donghan Kang",
        "Soyoung Kang",
        "Taeho Kil",
        "Areum Kim",
        "Beomyoung Kim",
        "Byeongwook Kim",
        "Daehee Kim",
        "Dong-Gyun Kim",
        "Donggook Kim",
        "Donghyun Kim",
        "Euna Kim",
        "Eunchul Kim",
        "Geewook Kim",
        "Gyu Ri Kim",
        "Hanbyul Kim",
        "Heesu Kim",
        "Isaac Kim",
        "Jeonghoon Kim",
        "Jihye Kim",
        "Joonghoon Kim",
        "Minjae Kim",
        "Minsub Kim",
        "Pil Hwan Kim",
        "Sammy Kim",
        "Seokhun Kim",
        "Seonghyeon Kim",
        "Soojin Kim",
        "Soong Kim",
        "Soyoon Kim",
        "Sunyoung Kim",
        "Taeho Kim",
        "Wonho Kim",
        "Yoonsik Kim",
        "You Jin Kim",
        "Yuri Kim",
        "Beomseok Kwon",
        "Ohsung Kwon",
        "Yoo-Hwan Kwon",
        "Anna Lee",
        "Byungwook Lee",
        "Changho Lee",
        "Daun Lee",
        "Dongjae Lee",
        "Ha-Ram Lee",
        "Hodong Lee",
        "Hwiyeong Lee",
        "Hyunmi Lee",
        "Injae Lee",
        "Jaeung Lee",
        "Jeongsang Lee",
        "Jisoo Lee",
        "Jongsoo Lee",
        "Joongjae Lee",
        "Juhan Lee",
        "Jung Hyun Lee",
        "Junghoon Lee",
        "Junwoo Lee",
        "Se Yun Lee",
        "Sujin Lee",
        "Sungjae Lee",
        "Sungwoo Lee",
        "Wonjae Lee",
        "Zoo Hyun Lee",
        "Jong Kun Lim",
        "Kun Lim",
        "Taemin Lim",
        "Nuri Na",
        "Jeongyeon Nam",
        "Kyeong-Min Nam",
        "Yeonseog Noh",
        "Biro Oh",
        "Jung-Sik Oh",
        "Solgil Oh",
        "Yeontaek Oh",
        "Boyoun Park",
        "Cheonbok Park",
        "Dongju Park",
        "Hyeonjin Park",
        "Hyun Tae Park",
        "Hyunjung Park",
        "Jihye Park",
        "Jooseok Park",
        "Junghwan Park",
        "Jungsoo Park",
        "Miru Park",
        "Sang Hee Park",
        "Seunghyun Park",
        "Soyoung Park",
        "Taerim Park",
        "Wonkyeong Park",
        "Hyunjoon Ryu",
        "Jeonghun Ryu",
        "Nahyeon Ryu",
        "Soonshin Seo",
        "Suk Min Seo",
        "Yoonjeong Shim",
        "Kyuyong Shin",
        "Wonkwang Shin",
        "Hyun Sim",
        "Woongseob Sim",
        "Hyejin Soh",
        "Bokyong Son",
        "Hyunjun Son",
        "Seulah Son",
        "Chi-Yun Song",
        "Chiyoung Song",
        "Ka Yeon Song",
        "Minchul Song",
        "Seungmin Song",
        "Jisung Wang",
        "Yonggoo Yeo",
        "Myeong Yeon Yi",
        "Moon Bin Yim",
        "Taehwan Yoo",
        "Youngjoon Yoo",
        "Sungmin Yoon",
        "Young Jin Yoon",
        "Hangyeol Yu",
        "Ui Seon Yu",
        "Xingdong Zuo",
        "Jeongin Bae",
        "Joungeun Bae",
        "Hyunsoo Cho",
        "Seonghyun Cho",
        "Yongjin Cho",
        "Taekyoon Choi",
        "Yera Choi",
        "Jiwan Chung",
        "Zhenghui Han",
        "Byeongho Heo",
        "Euisuk Hong",
        "Taebaek Hwang",
        "Seonyeol Im",
        "Sumin Jegal",
        "Sumin Jeon",
        "Yelim Jeong",
        "Yonghyun Jeong",
        "Can Jiang",
        "Juyong Jiang",
        "Jiho Jin",
        "Ara Jo",
        "Younghyun Jo",
        "Hoyoun Jung",
        "Juyoung Jung",
        "Seunghyeong Kang",
        "Dae Hee Kim",
        "Ginam Kim",
        "Hangyeol Kim",
        "Heeseung Kim",
        "Hyojin Kim",
        "Hyojun Kim",
        "Hyun-Ah Kim",
        "Jeehye Kim",
        "Jin-Hwa Kim",
        "Jiseon Kim",
        "Jonghak Kim",
        "Jung Yoon Kim",
        "Rak Yeong Kim",
        "Seongjin Kim",
        "Seoyoon Kim",
        "Sewon Kim",
        "Sooyoung Kim",
        "Sukyoung Kim",
        "Taeyong Kim",
        "Naeun Ko",
        "Bonseung Koo",
        "Heeyoung Kwak",
        "Haena Kwon",
        "Youngjin Kwon",
        "Boram Lee",
        "Bruce W. Lee",
        "Dagyeong Lee",
        "Erin Lee",
        "Euijin Lee",
        "Ha Gyeong Lee",
        "Hyojin Lee",
        "Hyunjeong Lee",
        "Jeeyoon Lee",
        "Jeonghyun Lee",
        "Jongheok Lee",
        "Joonhyung Lee",
        "Junhyuk Lee",
        "Mingu Lee",
        "Nayeon Lee",
        "Sangkyu Lee",
        "Se Young Lee",
        "Seulgi Lee",
        "Seung Jin Lee",
        "Suhyeon Lee",
        "Yeonjae Lee",
        "Yesol Lee",
        "Youngbeom Lee",
        "Yujin Lee",
        "Shaodong Li",
        "Tianyu Liu",
        "Seong-Eun Moon",
        "Taehong Moon",
        "Max-Lasse Nihlenramstroem",
        "Wonseok Oh",
        "Yuri Oh",
        "Hongbeen Park",
        "Hyekyung Park",
        "Jaeho Park",
        "Nohil Park",
        "Sangjin Park",
        "Jiwon Ryu",
        "Miru Ryu",
        "Simo Ryu",
        "Ahreum Seo",
        "Hee Seo",
        "Kangdeok Seo",
        "Jamin Shin",
        "Seungyoun Shin",
        "Heetae Sin",
        "Jiangping Wang",
        "Lei Wang",
        "Ning Xiang",
        "Longxiang Xiao",
        "Jing Xu",
        "Seonyeong Yi",
        "Haanju Yoo",
        "Haneul Yoo",
        "Hwanhee Yoo",
        "Liang Yu",
        "Youngjae Yu",
        "Weijie Yuan",
        "Bo Zeng",
        "Qian Zhou",
        "Kyunghyun Cho",
        "Jung-Woo Ha",
        "Joonsuk Park",
        "Jihyun Hwang",
        "Hyoung Jo Kwon",
        "Soonyong Kwon",
        "Jungyeon Lee",
        "Seungho Lee",
        "Seonghyeon Lim",
        "Hyunkyung Noh",
        "Seungho Choi",
        "Sang-Woo Lee",
        "Jung Hwa Lim",
        "Nako Sung"
      ],
      "abstract": "We introduce HyperCLOVA X, a family of large language models (LLMs) tailored\nto the Korean language and culture, along with competitive capabilities in\nEnglish, math, and coding. HyperCLOVA X was trained on a balanced mix of\nKorean, English, and code data, followed by instruction-tuning with\nhigh-quality human-annotated datasets while abiding by strict safety guidelines\nreflecting our commitment to responsible AI. The model is evaluated across\nvarious benchmarks, including comprehensive reasoning, knowledge, commonsense,\nfactuality, coding, math, chatting, instruction-following, and harmlessness, in\nboth Korean and English. HyperCLOVA X exhibits strong reasoning capabilities in\nKorean backed by a deep understanding of the language and cultural nuances.\nFurther analysis of the inherent bilingual nature and its extension to\nmultilingualism highlights the model's cross-lingual proficiency and strong\ngeneralization ability to untargeted languages, including machine translation\nbetween several language pairs and cross-lingual inference tasks. We believe\nthat HyperCLOVA X can provide helpful guidance for regions or countries in\ndeveloping their sovereign LLMs.",
      "tldr_zh": "本研究介绍了 HyperCLOVA X，一系列针对韩语和文化的大家庭语言模型 (LLMs)，同时在英语、数学和编码任务中表现出色。该模型使用平衡的韩语、英语和代码数据集进行训练，随后通过高质量的人类标注数据集进行 instruction-tuning，同时遵守严格的安全指南。在各种基准测试中，HyperCLOVA X 展示了强大的推理能力、知识理解和跨语言性能，尤其在韩语中突出文化细微差别，并扩展到多语种任务如机器翻译和跨语言推理；这为国家开发主权 LLMs 提供了宝贵指导。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "44 pages; updated authors list and fixed author names",
      "pdf_url": "http://arxiv.org/pdf/2404.01954v2",
      "published_date": "2024-04-02 13:48:49 UTC",
      "updated_date": "2024-04-13 15:06:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:03:22.731915"
    },
    {
      "arxiv_id": "2404.02183v1",
      "title": "Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Yoichi Ishibashi",
        "Yoshimasa Nishimura"
      ],
      "abstract": "Recent advancements in automatic code generation using large language model\n(LLM) agent have brought us closer to the future of automated software\ndevelopment. However, existing single-agent approaches face limitations in\ngenerating and improving large-scale, complex codebases due to constraints in\ncontext length. To tackle this challenge, we propose Self-Organized multi-Agent\nframework (SoA), a novel multi-agent framework that enables the scalable and\nefficient generation and optimization of large-scale code. In SoA,\nself-organized agents operate independently to generate and modify code\ncomponents while seamlessly collaborating to construct the overall codebase. A\nkey feature of our framework is the automatic multiplication of agents based on\nproblem complexity, allowing for dynamic scalability. This enables the overall\ncode volume to be increased indefinitely according to the number of agents,\nwhile the amount of code managed by each agent remains constant. We evaluate\nSoA on the HumanEval benchmark and demonstrate that, compared to a single-agent\nsystem, each agent in SoA handles significantly less code, yet the overall\ngenerated code is substantially greater. Moreover, SoA surpasses the powerful\nsingle-agent baseline by 5% in terms of Pass@1 accuracy.",
      "tldr_zh": "该研究提出了一种名为 SoA 的多代理框架，利用 LLM（Large Language Model）来实现超大规模代码生成和优化，解决现有单代理系统因上下文长度限制而无法有效处理复杂代码库的问题。在 SoA 中，代理通过自组织机制独立生成和修改代码组件，同时协作构建整体代码，并根据问题复杂度自动增加代理数量，实现动态可扩展性。这种设计确保每个代理处理的代码量保持恒定，而总代码量可无限扩展。在 HumanEval 基准测试中，SoA 比单代理基线在 Pass@1 准确率上提高了 5%，证明了其在可扩展性和性能上的优势。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.02183v1",
      "published_date": "2024-04-02 13:37:28 UTC",
      "updated_date": "2024-04-02 13:37:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:03:33.524897"
    },
    {
      "arxiv_id": "2404.01925v1",
      "title": "Improving Bird's Eye View Semantic Segmentation by Task Decomposition",
      "title_zh": "通过任务分解改进鸟瞰视图语义分割",
      "authors": [
        "Tianhao Zhao",
        "Yongcan Chen",
        "Yu Wu",
        "Tianyang Liu",
        "Bo Du",
        "Peilun Xiao",
        "Shi Qiu",
        "Hongda Yang",
        "Guozhen Li",
        "Yi Yang",
        "Yutian Lin"
      ],
      "abstract": "Semantic segmentation in bird's eye view (BEV) plays a crucial role in\nautonomous driving. Previous methods usually follow an end-to-end pipeline,\ndirectly predicting the BEV segmentation map from monocular RGB inputs.\nHowever, the challenge arises when the RGB inputs and BEV targets from distinct\nperspectives, making the direct point-to-point predicting hard to optimize. In\nthis paper, we decompose the original BEV segmentation task into two stages,\nnamely BEV map reconstruction and RGB-BEV feature alignment. In the first\nstage, we train a BEV autoencoder to reconstruct the BEV segmentation maps\ngiven corrupted noisy latent representation, which urges the decoder to learn\nfundamental knowledge of typical BEV patterns. The second stage involves\nmapping RGB input images into the BEV latent space of the first stage, directly\noptimizing the correlations between the two views at the feature level. Our\napproach simplifies the complexity of combining perception and generation into\ndistinct steps, equipping the model to handle intricate and challenging scenes\neffectively. Besides, we propose to transform the BEV segmentation map from the\nCartesian to the polar coordinate system to establish the column-wise\ncorrespondence between RGB images and BEV maps. Moreover, our method requires\nneither multi-scale features nor camera intrinsic parameters for depth\nestimation and saves computational overhead. Extensive experiments on nuScenes\nand Argoverse show the effectiveness and efficiency of our method. Code is\navailable at https://github.com/happytianhao/TaDe.",
      "tldr_zh": "本研究针对鸟瞰视角（BEV）语义分割在自动驾驶中的挑战，提出一种任务分解方法，将端到端预测分解为两个阶段：BEV 地图重建和 RGB-BEV 特征对齐。第一个阶段训练一个 BEV 自编码器，从噪声潜在表示重建 BEV 分割图，以学习典型 BEV 模式；第二个阶段则将 RGB 输入映射到 BEV 潜在空间，直接优化特征级别的视角相关性。论文还引入将 BEV 分割图从 Cartesian 到 polar 坐标系的转换，建立 RGB 和 BEV 的列-wise 对应，同时避免使用多尺度特征或相机内参，节省计算开销。在 nuScenes 和 Argoverse 数据集上的实验显示，该方法显著提高了分割性能和效率。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by CVPR 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.01925v1",
      "published_date": "2024-04-02 13:19:45 UTC",
      "updated_date": "2024-04-02 13:19:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:03:47.257615"
    },
    {
      "arxiv_id": "2404.01923v1",
      "title": "SGSH: Stimulate Large Language Models with Skeleton Heuristics for Knowledge Base Question Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Shasha Guo",
        "Lizi Liao",
        "Jing Zhang",
        "Yanling Wang",
        "Cuiping Li",
        "Hong Chen"
      ],
      "abstract": "Knowledge base question generation (KBQG) aims to generate natural language\nquestions from a set of triplet facts extracted from KB. Existing methods have\nsignificantly boosted the performance of KBQG via pre-trained language models\n(PLMs) thanks to the richly endowed semantic knowledge. With the advance of\npre-training techniques, large language models (LLMs) (e.g., GPT-3.5)\nundoubtedly possess much more semantic knowledge. Therefore, how to effectively\norganize and exploit the abundant knowledge for KBQG becomes the focus of our\nstudy. In this work, we propose SGSH--a simple and effective framework to\nStimulate GPT-3.5 with Skeleton Heuristics to enhance KBQG. The framework\nincorporates \"skeleton heuristics\", which provides more fine-grained guidance\nassociated with each input to stimulate LLMs to generate optimal questions,\nencompassing essential elements like the question phrase and the auxiliary\nverb.More specifically, we devise an automatic data construction strategy\nleveraging ChatGPT to construct a skeleton training dataset, based on which we\nemploy a soft prompting approach to train a BART model dedicated to generating\nthe skeleton associated with each input. Subsequently, skeleton heuristics are\nencoded into the prompt to incentivize GPT-3.5 to generate desired questions.\nExtensive experiments demonstrate that SGSH derives the new state-of-the-art\nperformance on the KBQG tasks.",
      "tldr_zh": "该论文针对 Knowledge Base Question Generation (KBQG) 任务，提出 SGSH 框架，通过 skeleton heuristics 来刺激 Large Language Models (LLMs) 如 GPT-3.5，利用其丰富语义知识生成高质量的自然语言问题。框架的核心方法包括使用 ChatGPT 自动构建 skeleton 训练数据集，并训练 BART 模型生成细粒度的提示骨架（如问题短语和助动词），随后将这些骨架编码进提示中引导 LLMs 优化问题生成。实验结果表明，SGSH 在 KBQG 任务上取得了新的 state-of-the-art 性能，显著提升了问题生成的准确性和相关性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by NAACL 2024 Findings",
      "pdf_url": "http://arxiv.org/pdf/2404.01923v1",
      "published_date": "2024-04-02 13:17:36 UTC",
      "updated_date": "2024-04-02 13:17:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:03:59.106748"
    },
    {
      "arxiv_id": "2404.01914v1",
      "title": "SCANNER: Knowledge-Enhanced Approach for Robust Multi-modal Named Entity Recognition of Unseen Entities",
      "title_zh": "翻译失败",
      "authors": [
        "Hyunjong Ok",
        "Taeho Kil",
        "Sukmin Seo",
        "Jaeho Lee"
      ],
      "abstract": "Recent advances in named entity recognition (NER) have pushed the boundary of\nthe task to incorporate visual signals, leading to many variants, including\nmulti-modal NER (MNER) or grounded MNER (GMNER). A key challenge to these tasks\nis that the model should be able to generalize to the entities unseen during\nthe training, and should be able to handle the training samples with noisy\nannotations. To address this obstacle, we propose SCANNER (Span CANdidate\ndetection and recognition for NER), a model capable of effectively handling all\nthree NER variants. SCANNER is a two-stage structure; we extract entity\ncandidates in the first stage and use it as a query to get knowledge,\neffectively pulling knowledge from various sources. We can boost our\nperformance by utilizing this entity-centric extracted knowledge to address\nunseen entities. Furthermore, to tackle the challenges arising from noisy\nannotations in NER datasets, we introduce a novel self-distillation method,\nenhancing the robustness and accuracy of our model in processing training data\nwith inherent uncertainties. Our approach demonstrates competitive performance\non the NER benchmark and surpasses existing methods on both MNER and GMNER\nbenchmarks. Further analysis shows that the proposed distillation and knowledge\nutilization methods improve the performance of our model on various benchmarks.",
      "tldr_zh": "这篇论文提出了SCANNER，一种知识增强方法，用于处理多模态命名实体识别(MNER)和grounded MNER(GMNER)，旨在解决模型泛化到未见实体以及处理噪声标注的挑战。SCANNER采用两阶段结构：首先提取实体候选作为查询，从各种知识来源获取信息；其次，通过引入自蒸馏(self-distillation)方法，提升模型的鲁棒性和准确性。实验结果显示，SCANNER在NER基准上表现出竞争性表现，并在MNER和GMNER基准上超越现有方法，证明了其知识利用和蒸馏策略的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "13 pages, 7 figures, NAACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.01914v1",
      "published_date": "2024-04-02 13:05:41 UTC",
      "updated_date": "2024-04-02 13:05:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:04:11.248956"
    },
    {
      "arxiv_id": "2404.04279v1",
      "title": "When Abel Kills Cain: What Machine Translation Cannot Capture",
      "title_zh": "翻译失败",
      "authors": [
        "Aurélien Bénel",
        "Joris Falip",
        "Philippe Lacour"
      ],
      "abstract": "The article aims at identifying what, from a structural point of view, AI\nbased automatic translators cannot fully capture. It focuses on the machine's\nmistakes, in order to try to explain its causes. The biblical story of Ca\\\"in\nand Abel has been chosen because of its rich interpretive and critical\ntradition, but also because of its semantic difficulty. The investigation\nbegins with the observation, for the translation of this text, of the language\npairs and interfaces offered by the best known machine translation services\n(Google Translate, DeepL). A typology of the most frequent translation errors\nis then established. Finally, contemporary translations are compared, in order\nto underline the unique contribution of each. In conclusion, the article\nsuggests a revision of translation theory and, corArtificial Intelligence,\nTranslation, Limitations, Interpretation, Comparison, Unicityelatively, a\nreformulation of its technology concerning cultural texts.",
      "tldr_zh": "这篇文章探讨了AI驱动的机器翻译（Machine Translation）从结构角度无法完全捕捉的元素，通过分析翻译错误来揭示其原因。作者以《该隐和亚伯》（Ca\\\"in and Abel）的圣经故事为例，考察了Google Translate和DeepL等服务在多种语言对上的翻译表现，并建立了常见错误类型。研究比较了当代翻译版本，突出了每一种的独特贡献，并得出结论，建议修订翻译理论（Translation Theory）并改革其技术，以更好地处理文化文本。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "in French language",
      "pdf_url": "http://arxiv.org/pdf/2404.04279v1",
      "published_date": "2024-04-02 12:46:00 UTC",
      "updated_date": "2024-04-02 12:46:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:04:24.240752"
    },
    {
      "arxiv_id": "2404.02181v1",
      "title": "Leveraging Machine Learning for Early Autism Detection via INDT-ASD Indian Database",
      "title_zh": "利用机器学习通过 INDT-ASD 印度数据库实现早期自闭症检测",
      "authors": [
        "Trapti Shrivastava",
        "Harshal Chaudhari",
        "Vrijendra Singh"
      ],
      "abstract": "Machine learning (ML) has advanced quickly, particularly throughout the area\nof health care. The diagnosis of neurodevelopment problems using ML is a very\nimportant area of healthcare. Autism spectrum disorder (ASD) is one of the\ndevelopmental disorders that is growing the fastest globally. The clinical\nscreening tests used to identify autistic symptoms are expensive and\ntime-consuming. But now that ML has been advanced, it's feasible to identify\nautism early on. Previously, many different techniques have been used in\ninvestigations. Still, none of them have produced the anticipated outcomes when\nit comes to the capacity to predict autistic features utilizing a clinically\nvalidated Indian ASD database. Therefore, this study aimed to develop a simple,\nquick, and inexpensive technique for identifying ASD by using ML. Various\nmachine learning classifiers, including Adaboost (AB), Gradient Boost (GB),\nDecision Tree (DT), Logistic Regression (LR), Random Forest (RF), Gaussian\nNaive Bayes (GNB), Linear Discriminant Analysis (LDA), Quadratic Discriminant\nAnalysis (QDA), K-Nearest Neighbors (KNN), and Support Vector Machine (SVM),\nwere used to develop the autism prediction model. The proposed method was\ntested with records from the AIIMS Modified INDT-ASD (AMI) database, which were\ncollected through an application developed by AIIMS in Delhi, India. Feature\nengineering has been applied to make the proposed solution easier than already\navailable solutions. Using the proposed model, we succeeded in predicting ASD\nusing a minimized set of 20 questions rather than the 28 questions presented in\nAMI with promising accuracy. In a comparative evaluation, SVM emerged as the\nsuperior model among others, with 100 $\\pm$ 0.05\\% accuracy, higher recall by\n5.34\\%, and improved accuracy by 2.22\\%-6.67\\% over RF. We have also introduced\na web-based solution supporting both Hindi and English.",
      "tldr_zh": "本研究利用机器学习（Machine Learning）开发了一种简单、快速且低成本的自闭症谱系障碍（ASD）早期检测方法，针对印度数据库INDT-ASD进行优化。研究者测试了多种分类器，包括Adaboost (AB)、Gradient Boost (GB)、Decision Tree (DT)、Logistic Regression (LR)、Random Forest (RF)、Gaussian Naive Bayes (GNB)、Linear Discriminant Analysis (LDA)、Quadratic Discriminant Analysis (QDA)、K-Nearest Neighbors (KNN)和Support Vector Machine (SVM)，并通过特征工程将检测问题从28个减少到20个。结果显示，SVM表现出色，准确率达到100% ± 0.05%，召回率比RF高出5.34%，准确率提升2.22%-6.67%；此外，该方法还推出了支持Hindi和English的双语网络解决方案，为临床ASD筛查提供了高效工具。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.02181v1",
      "published_date": "2024-04-02 12:44:51 UTC",
      "updated_date": "2024-04-02 12:44:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:04:37.529800"
    },
    {
      "arxiv_id": "2404.01897v1",
      "title": "Continuous Spiking Graph Neural Networks",
      "title_zh": "连续脉冲图神经",
      "authors": [
        "Nan Yin",
        "Mengzhu Wan",
        "Li Shen",
        "Hitesh Laxmichand Patel",
        "Baopu Li",
        "Bin Gu",
        "Huan Xiong"
      ],
      "abstract": "Continuous graph neural networks (CGNNs) have garnered significant attention\ndue to their ability to generalize existing discrete graph neural networks\n(GNNs) by introducing continuous dynamics. They typically draw inspiration from\ndiffusion-based methods to introduce a novel propagation scheme, which is\nanalyzed using ordinary differential equations (ODE). However, the\nimplementation of CGNNs requires significant computational power, making them\nchallenging to deploy on battery-powered devices. Inspired by recent spiking\nneural networks (SNNs), which emulate a biological inference process and\nprovide an energy-efficient neural architecture, we incorporate the SNNs with\nCGNNs in a unified framework, named Continuous Spiking Graph Neural Networks\n(COS-GNN). We employ SNNs for graph node representation at each time step,\nwhich are further integrated into the ODE process along with time. To enhance\ninformation preservation and mitigate information loss in SNNs, we introduce\nthe high-order structure of COS-GNN, which utilizes the second-order ODE for\nspiking representation and continuous propagation. Moreover, we provide the\ntheoretical proof that COS-GNN effectively mitigates the issues of exploding\nand vanishing gradients, enabling us to capture long-range dependencies between\nnodes. Experimental results on graph-based learning tasks demonstrate the\neffectiveness of the proposed COS-GNN over competitive baselines.",
      "tldr_zh": "该研究提出了一种名为 Continuous Spiking Graph Neural Networks (COS-GNN) 的统一框架，将 Spiking Neural Networks (SNNs) 与 Continuous Graph Neural Networks (CGNNs) 相结合，以解决 CGNNs 在计算资源和能效方面的挑战。COS-GNN 在每个时间步使用 SNNs 处理图节点表示，并将其整合到 Ordinary Differential Equations (ODE) 过程中，以实现高效的图传播。作者引入高阶结构（基于二阶 ODE）来增强信息保留并减少信息损失，同时提供理论证明，证明 COS-GNN 可以缓解梯度爆炸和消失问题，从而更好地捕获节点之间的长程依赖。在图-based 学习任务的实验中，COS-GNN 表现出色，优于竞争基线模型。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.01897v1",
      "published_date": "2024-04-02 12:36:40 UTC",
      "updated_date": "2024-04-02 12:36:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:04:49.071441"
    },
    {
      "arxiv_id": "2404.01889v3",
      "title": "RAVE: Residual Vector Embedding for CLIP-Guided Backlit Image Enhancement",
      "title_zh": "翻译失败",
      "authors": [
        "Tatiana Gaintseva",
        "Martin Benning",
        "Gregory Slabaugh"
      ],
      "abstract": "In this paper we propose a novel modification of Contrastive Language-Image\nPre-Training (CLIP) guidance for the task of unsupervised backlit image\nenhancement. Our work builds on the state-of-the-art CLIP-LIT approach, which\nlearns a prompt pair by constraining the text-image similarity between a prompt\n(negative/positive sample) and a corresponding image (backlit image/well-lit\nimage) in the CLIP embedding space. Learned prompts then guide an image\nenhancement network. Based on the CLIP-LIT framework, we propose two novel\nmethods for CLIP guidance. First, we show that instead of tuning prompts in the\nspace of text embeddings, it is possible to directly tune their embeddings in\nthe latent space without any loss in quality. This accelerates training and\npotentially enables the use of additional encoders that do not have a text\nencoder. Second, we propose a novel approach that does not require any prompt\ntuning. Instead, based on CLIP embeddings of backlit and well-lit images from\ntraining data, we compute the residual vector in the embedding space as a\nsimple difference between the mean embeddings of the well-lit and backlit\nimages. This vector then guides the enhancement network during training,\npushing a backlit image towards the space of well-lit images. This approach\nfurther dramatically reduces training time, stabilizes training and produces\nhigh quality enhanced images without artifacts, both in supervised and\nunsupervised training regimes. Additionally, we show that residual vectors can\nbe interpreted, revealing biases in training data, and thereby enabling\npotential bias correction.",
      "tldr_zh": "本论文提出 RAVE 方法，一种基于 CLIP 指导的背光图像增强技术，改进了现有 CLIP-LIT 框架的两个方面：首先，直接在潜在空间调整嵌入以加速训练；其次，计算背光图像和正常照明图像的 CLIP 嵌入残差向量作为指导信号，无需提示调整。实验结果显示，该方法显著减少训练时间、稳定训练过程，并在监督和无监督模式下生成高质量、无伪影的增强图像。此外，残差向量可解释训练数据的偏差，从而支持偏差修正。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.01889v3",
      "published_date": "2024-04-02 12:28:40 UTC",
      "updated_date": "2024-07-20 22:57:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:05:00.812549"
    },
    {
      "arxiv_id": "2404.01878v1",
      "title": "Real, fake and synthetic faces -- does the coin have three sides?",
      "title_zh": "翻译失败",
      "authors": [
        "Shahzeb Naeem",
        "Ramzi Al-Sharawi",
        "Muhammad Riyyan Khan",
        "Usman Tariq",
        "Abhinav Dhall",
        "Hasan Al-Nashash"
      ],
      "abstract": "With the ever-growing power of generative artificial intelligence, deepfake\nand artificially generated (synthetic) media have continued to spread online,\nwhich creates various ethical and moral concerns regarding their usage. To\ntackle this, we thus present a novel exploration of the trends and patterns\nobserved in real, deepfake and synthetic facial images. The proposed analysis\nis done in two parts: firstly, we incorporate eight deep learning models and\nanalyze their performances in distinguishing between the three classes of\nimages. Next, we look to further delve into the similarities and differences\nbetween these three sets of images by investigating their image properties both\nin the context of the entire image as well as in the context of specific\nregions within the image. ANOVA test was also performed and provided further\nclarity amongst the patterns associated between the images of the three\nclasses. From our findings, we observe that the investigated deeplearning\nmodels found it easier to detect synthetic facial images, with the ViT Patch-16\nmodel performing best on this task with a class-averaged sensitivity,\nspecificity, precision, and accuracy of 97.37%, 98.69%, 97.48%, and 98.25%,\nrespectively. This observation was supported by further analysis of various\nimage properties. We saw noticeable differences across the three category of\nimages. This analysis can help us build better algorithms for facial image\ngeneration, and also shows that synthetic, deepfake and real face images are\nindeed three different classes.",
      "tldr_zh": "这篇论文探讨了真实（real）、deepfake 和合成（synthetic）面部图像的趋势和差异，旨在解决生成式人工智能的伦理问题。研究采用两部分方法：首先，使用八个深度学习模型（如ViT Patch-16）对这三种图像类别进行分类分析，发现模型更容易检测合成图像，ViT Patch-16 在灵敏度、特异性、精确度和准确率上分别达到97.37%、98.69%、97.48%和98.25%。其次，通过分析图像整体和局部属性并进行ANOVA测试，揭示了三种图像在属性上存在显著差异。这些发现有助于改进面部图像生成算法，并证实真实、deepfake 和合成图像确实是三个不同的类别。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.01878v1",
      "published_date": "2024-04-02 12:08:26 UTC",
      "updated_date": "2024-04-02 12:08:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:05:11.792695"
    },
    {
      "arxiv_id": "2404.01869v2",
      "title": "Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models -- A Survey",
      "title_zh": "超越准确性：评估大型语言模型的推理行为——一个综述",
      "authors": [
        "Philipp Mondorf",
        "Barbara Plank"
      ],
      "abstract": "Large language models (LLMs) have recently shown impressive performance on\ntasks involving reasoning, leading to a lively debate on whether these models\npossess reasoning capabilities similar to humans. However, despite these\nsuccesses, the depth of LLMs' reasoning abilities remains uncertain. This\nuncertainty partly stems from the predominant focus on task performance,\nmeasured through shallow accuracy metrics, rather than a thorough investigation\nof the models' reasoning behavior. This paper seeks to address this gap by\nproviding a comprehensive review of studies that go beyond task accuracy,\noffering deeper insights into the models' reasoning processes. Furthermore, we\nsurvey prevalent methodologies to evaluate the reasoning behavior of LLMs,\nemphasizing current trends and efforts towards more nuanced reasoning analyses.\nOur review suggests that LLMs tend to rely on surface-level patterns and\ncorrelations in their training data, rather than on sophisticated reasoning\nabilities. Additionally, we identify the need for further research that\ndelineates the key differences between human and LLM-based reasoning. Through\nthis survey, we aim to shed light on the complex reasoning processes within\nLLMs.",
      "tldr_zh": "这篇调查论文探讨了大型语言模型（LLMs）的推理行为，强调评估不应仅限于任务准确率（accuracy），而应深入分析模型的推理过程，以揭示其潜在局限性。作者回顾了现有研究，这些研究通过更细致的分析方法（如行为评估框架）来考察LLMs是否依赖训练数据中的表面模式和相关性，而非真正的人类级推理。调查结果显示，LLMs 往往依赖浅层模式，而不是高级推理能力，并呼吁进一步研究来区分人类与LLMs的推理差异。该工作为未来更全面的LLMs评估提供重要见解。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "COLM 2024, 27 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2404.01869v2",
      "published_date": "2024-04-02 11:46:31 UTC",
      "updated_date": "2024-08-06 11:58:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:05:24.298899"
    },
    {
      "arxiv_id": "2404.01863v1",
      "title": "Confidence-aware Reward Optimization for Fine-tuning Text-to-Image Models",
      "title_zh": "置信度感知的奖励优化用于微调文本到图像模型",
      "authors": [
        "Kyuyoung Kim",
        "Jongheon Jeong",
        "Minyong An",
        "Mohammad Ghavamzadeh",
        "Krishnamurthy Dvijotham",
        "Jinwoo Shin",
        "Kimin Lee"
      ],
      "abstract": "Fine-tuning text-to-image models with reward functions trained on human\nfeedback data has proven effective for aligning model behavior with human\nintent. However, excessive optimization with such reward models, which serve as\nmere proxy objectives, can compromise the performance of fine-tuned models, a\nphenomenon known as reward overoptimization. To investigate this issue in\ndepth, we introduce the Text-Image Alignment Assessment (TIA2) benchmark, which\ncomprises a diverse collection of text prompts, images, and human annotations.\nOur evaluation of several state-of-the-art reward models on this benchmark\nreveals their frequent misalignment with human assessment. We empirically\ndemonstrate that overoptimization occurs notably when a poorly aligned reward\nmodel is used as the fine-tuning objective. To address this, we propose\nTextNorm, a simple method that enhances alignment based on a measure of reward\nmodel confidence estimated across a set of semantically contrastive text\nprompts. We demonstrate that incorporating the confidence-calibrated rewards in\nfine-tuning effectively reduces overoptimization, resulting in twice as many\nwins in human evaluation for text-image alignment compared against the baseline\nreward models.",
      "tldr_zh": "该论文探讨了使用基于人类反馈的奖励函数微调文本到图像模型时，容易出现的奖励过优化(reward overoptimization)问题，该问题会损害模型性能。为此，研究者引入了Text-Image Alignment Assessment (TIA2)基准，通过多样化的文本提示、图像和人类标注评估奖励模型的实际对齐度，并发现许多状态-of-the-art奖励模型与人类评估不一致。论文提出了一种简单方法TextNorm，通过测量奖励模型在语义对比文本提示上的置信度来校准奖励，从而在微调过程中有效减少过优化，最终在人类评估中使文本-图像对齐的获胜率比基线模型提高一倍。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "ICLR 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.01863v1",
      "published_date": "2024-04-02 11:40:38 UTC",
      "updated_date": "2024-04-02 11:40:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:05:37.319625"
    },
    {
      "arxiv_id": "2404.01855v2",
      "title": "Where to Move Next: Zero-shot Generalization of LLMs for Next POI Recommendation",
      "title_zh": "翻译失败",
      "authors": [
        "Shanshan Feng",
        "Haoming Lyu",
        "Caishun Chen",
        "Yew-Soon Ong"
      ],
      "abstract": "Next Point-of-interest (POI) recommendation provides valuable suggestions for\nusers to explore their surrounding environment. Existing studies rely on\nbuilding recommendation models from large-scale users' check-in data, which is\ntask-specific and needs extensive computational resources. Recently, the\npretrained large language models (LLMs) have achieved significant advancements\nin various NLP tasks and have also been investigated for recommendation\nscenarios. However, the generalization abilities of LLMs still are unexplored\nto address the next POI recommendations, where users' geographical movement\npatterns should be extracted. Although there are studies that leverage LLMs for\nnext-item recommendations, they fail to consider the geographical influence and\nsequential transitions. Hence, they cannot effectively solve the next POI\nrecommendation task. To this end, we design novel prompting strategies and\nconduct empirical studies to assess the capability of LLMs, e.g., ChatGPT, for\npredicting a user's next check-in. Specifically, we consider several essential\nfactors in human movement behaviors, including user geographical preference,\nspatial distance, and sequential transitions, and formulate the recommendation\ntask as a ranking problem. Through extensive experiments on two widely used\nreal-world datasets, we derive several key findings. Empirical evaluations\ndemonstrate that LLMs have promising zero-shot recommendation abilities and can\nprovide accurate and reasonable predictions. We also reveal that LLMs cannot\naccurately comprehend geographical context information and are sensitive to the\norder of presentation of candidate POIs, which shows the limitations of LLMs\nand necessitates further research on robust human mobility reasoning\nmechanisms.",
      "tldr_zh": "本研究探讨了大型语言模型(LLMs)的零样本泛化能力，用于下一个兴趣点(POI)推荐任务，旨在避免依赖大规模用户签到数据的传统方法。作者设计了新颖的提示策略，考虑用户地理偏好、空间距离和顺序过渡，将推荐问题转化为排名任务，并使用ChatGPT等模型进行预测。实验在两个真实数据集上显示，LLMs表现出有前景的零样本推荐性能，能提供准确预测，但存在局限性，如无法准确理解地理上下文信息且对候选POI呈现顺序敏感。这揭示了LLMs在人类移动性推理方面的不足，需要进一步研究稳健机制。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.01855v2",
      "published_date": "2024-04-02 11:33:04 UTC",
      "updated_date": "2024-04-22 19:13:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:05:50.243010"
    },
    {
      "arxiv_id": "2404.01849v1",
      "title": "EV2Gym: A Flexible V2G Simulator for EV Smart Charging Research and Benchmarking",
      "title_zh": "EV2Gym：一个灵活的 V2G 模拟器，用于 EV 智能充电研究和基准测试",
      "authors": [
        "Stavros Orfanoudakis",
        "Cesar Diaz-Londono",
        "Yunus E. Yılmaz",
        "Peter Palensky",
        "Pedro P. Vergara"
      ],
      "abstract": "As electric vehicle (EV) numbers rise, concerns about the capacity of current\ncharging and power grid infrastructure grow, necessitating the development of\nsmart charging solutions. While many smart charging simulators have been\ndeveloped in recent years, only a few support the development of Reinforcement\nLearning (RL) algorithms in the form of a Gym environment, and those that do\nusually lack depth in modeling Vehicle-to-Grid (V2G) scenarios. To address the\naforementioned issues, this paper introduces the EV2Gym, a realistic simulator\nplatform for the development and assessment of small and large-scale smart\ncharging algorithms within a standardized platform. The proposed simulator is\npopulated with comprehensive EV, charging station, power transformer, and EV\nbehavior models validated using real data. EV2Gym has a highly customizable\ninterface empowering users to choose from pre-designed case studies or craft\ntheir own customized scenarios to suit their specific requirements. Moreover,\nit incorporates a diverse array of RL, mathematical programming, and heuristic\nalgorithms to speed up the development and benchmarking of new solutions. By\noffering a unified and standardized platform, EV2Gym aims to provide\nresearchers and practitioners with a robust environment for advancing and\nassessing smart charging algorithms.",
      "tldr_zh": "本文提出EV2Gym，一种灵活的模拟器平台，用于支持电动汽车(EV)智能充电研究和基准测试，旨在解决现有模拟器在Reinforcement Learning (RL)算法和Vehicle-to-Grid (V2G)场景建模方面的不足。EV2Gym整合了基于真实数据的全面模型，包括EV、充电站、电力变压器和EV行为模型，并提供高度可定制的界面，用户可选择预设案例或自定义场景。平台还内置多种算法，如RL、数学编程和启发式算法，以加速新解决方案的开发和评估。通过这个统一平台，研究者和从业者能更高效地推进智能充电技术的发展。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "10 pages, 9 figures, and 6 tables",
      "pdf_url": "http://arxiv.org/pdf/2404.01849v1",
      "published_date": "2024-04-02 11:22:53 UTC",
      "updated_date": "2024-04-02 11:22:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:06:01.620585"
    },
    {
      "arxiv_id": "2404.01833v3",
      "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack",
      "title_zh": "翻译失败",
      "authors": [
        "Mark Russinovich",
        "Ahmed Salem",
        "Ronen Eldan"
      ],
      "abstract": "Large Language Models (LLMs) have risen significantly in popularity and are\nincreasingly being adopted across multiple applications. These LLMs are heavily\naligned to resist engaging in illegal or unethical topics as a means to avoid\ncontributing to responsible AI harms. However, a recent line of attacks, known\nas jailbreaks, seek to overcome this alignment. Intuitively, jailbreak attacks\naim to narrow the gap between what the model can do and what it is willing to\ndo. In this paper, we introduce a novel jailbreak attack called Crescendo.\nUnlike existing jailbreak methods, Crescendo is a simple multi-turn jailbreak\nthat interacts with the model in a seemingly benign manner. It begins with a\ngeneral prompt or question about the task at hand and then gradually escalates\nthe dialogue by referencing the model's replies progressively leading to a\nsuccessful jailbreak. We evaluate Crescendo on various public systems,\nincluding ChatGPT, Gemini Pro, Gemini-Ultra, LlaMA-2 70b and LlaMA-3 70b Chat,\nand Anthropic Chat. Our results demonstrate the strong efficacy of Crescendo,\nwith it achieving high attack success rates across all evaluated models and\ntasks. Furthermore, we present Crescendomation, a tool that automates the\nCrescendo attack and demonstrate its efficacy against state-of-the-art models\nthrough our evaluations. Crescendomation surpasses other state-of-the-art\njailbreaking techniques on the AdvBench subset dataset, achieving 29-61% higher\nperformance on GPT-4 and 49-71% on Gemini-Pro. Finally, we also demonstrate\nCrescendo's ability to jailbreak multimodal models.",
      "tldr_zh": "该论文提出了一种名为 Crescendo 的多轮对话 jailbreak 攻击方法，用于绕过大型语言模型 (LLMs) 的安全对齐，使其参与非法或不道德话题。Crescendo 通过从一般提示逐步升级对话并引用模型回复的方式，实现对模型的逐步诱导，在 ChatGPT、Gemini Pro 等系统上表现出高成功率。论文还引入了 Crescendomation 自动化工具，该工具在 AdvBench 数据集上超越现有技术，提升 GPT-4 的性能 29-61% 和 Gemini-Pro 的 49-71%。此外，Crescendo 证明了其对多模态模型的有效性，为评估和改进 LLM 安全提供了新见解。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted at USENIX Security 2025",
      "pdf_url": "http://arxiv.org/pdf/2404.01833v3",
      "published_date": "2024-04-02 10:45:49 UTC",
      "updated_date": "2025-02-26 13:41:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:06:13.777738"
    },
    {
      "arxiv_id": "2404.01828v1",
      "title": "Defense without Forgetting: Continual Adversarial Defense with Anisotropic & Isotropic Pseudo Replay",
      "title_zh": "翻译失败",
      "authors": [
        "Yuhang Zhou",
        "Zhongyun Hua"
      ],
      "abstract": "Deep neural networks have demonstrated susceptibility to adversarial attacks.\nAdversarial defense techniques often focus on one-shot setting to maintain\nrobustness against attack. However, new attacks can emerge in sequences in\nreal-world deployment scenarios. As a result, it is crucial for a defense model\nto constantly adapt to new attacks, but the adaptation process can lead to\ncatastrophic forgetting of previously defended against attacks. In this paper,\nwe discuss for the first time the concept of continual adversarial defense\nunder a sequence of attacks, and propose a lifelong defense baseline called\nAnisotropic \\& Isotropic Replay (AIR), which offers three advantages: (1)\nIsotropic replay ensures model consistency in the neighborhood distribution of\nnew data, indirectly aligning the output preference between old and new tasks.\n(2) Anisotropic replay enables the model to learn a compromise data manifold\nwith fresh mixed semantics for further replay constraints and potential future\nattacks. (3) A straightforward regularizer mitigates the 'plasticity-stability'\ntrade-off by aligning model output between new and old tasks. Experiment\nresults demonstrate that AIR can approximate or even exceed the empirical\nperformance upper bounds achieved by Joint Training.",
      "tldr_zh": "该论文首次探讨了深度神经网络在连续对抗攻击（adversarial attacks）场景下的防御问题，强调了模型适应新攻击时可能导致的灾难性遗忘。作者提出了一种终身防御基线Anisotropic & Isotropic Replay (AIR)，通过Isotropic replay实现新旧任务输出偏好的间接对齐、Anisotropic replay学习折中的数据流形以处理未来攻击，以及一个简单正则化器缓解'plasticity-stability'权衡。实验结果表明，AIR的性能可逼近或超过Joint Training的经验上限，为持续性鲁棒防御提供了有效方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.01828v1",
      "published_date": "2024-04-02 10:41:51 UTC",
      "updated_date": "2024-04-02 10:41:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:06:28.311085"
    },
    {
      "arxiv_id": "2407.00815v1",
      "title": "A Deep Learning-based Pest Insect Monitoring System for Ultra-low Power Pocket-sized Drones",
      "title_zh": "翻译失败",
      "authors": [
        "Luca Crupi",
        "Luca Butera",
        "Alberto Ferrante",
        "Daniele Palossi"
      ],
      "abstract": "Smart farming and precision agriculture represent game-changer technologies\nfor efficient and sustainable agribusiness. Miniaturized palm-sized drones can\nact as flexible smart sensors inspecting crops, looking for early signs of\npotential pest outbreaking. However, achieving such an ambitious goal requires\nhardware-software codesign to develop accurate deep learning (DL) detection\nmodels while keeping memory and computational needs under an ultra-tight\nbudget, i.e., a few MB on-chip memory and a few 100s mW power envelope. This\nwork presents a novel vertically integrated solution featuring two ultra-low\npower System-on-Chips (SoCs), i.e., the dual-core STM32H74 and a multi-core GWT\nGAP9, running two State-of-the-Art DL models for detecting the Popillia\njaponica bug. We fine-tune both models for our image-based detection task,\nquantize them in 8-bit integers, and deploy them on the two SoCs. On the\nSTM32H74, we deploy a FOMO-MobileNetV2 model, achieving a mean average\nprecision (mAP) of 0.66 and running at 16.1 frame/s within 498 mW. While on the\nGAP9 SoC, we deploy a more complex SSDLite-MobileNetV3, which scores an mAP of\n0.79 and peaks at 6.8 frame/s within 33 mW. Compared to a top-notch\nRetinaNet-ResNet101-FPN full-precision baseline, which requires 14.9x more\nmemory and 300x more operations per inference, our best model drops only 15\\%\nin mAP, paving the way toward autonomous palm-sized drones capable of\nlightweight and precise pest detection.",
      "tldr_zh": "本研究提出了一种基于深度学习（DL）的害虫监测系统，针对超低功耗掌上无人机，用于智能农业中检测 Popillia japonica 虫子。系统通过硬件软件协同设计，在两个超低功耗 System-on-Chips (SoCs) 上部署微调和量化后的 DL 模型：STM32H74 运行 FOMO-MobileNetV2 模型，达到 mAP 0.66、16.1 帧/秒和 498 mW 功耗；GWT GAP9 运行 SSDLite-MobileNetV3 模型，达到 mAP 0.79、6.8 帧/秒和 33 mW 功耗。与 RetinaNet-ResNet101-FPN 基准模型相比，该系统仅 mAP 下降 15%，但内存需求减少 14.9 倍、计算量减少 300 倍，从而为轻量级、精确的自主无人机害虫检测铺平道路。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.00815v1",
      "published_date": "2024-04-02 10:39:54 UTC",
      "updated_date": "2024-04-02 10:39:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:06:43.709195"
    },
    {
      "arxiv_id": "2404.01812v1",
      "title": "Uncertainty-aware Active Learning of NeRF-based Object Models for Robot Manipulators using Visual and Re-orientation Actions",
      "title_zh": "翻译失败",
      "authors": [
        "Saptarshi Dasgupta",
        "Akshat Gupta",
        "Shreshth Tuli",
        "Rohan Paul"
      ],
      "abstract": "Manipulating unseen objects is challenging without a 3D representation, as\nobjects generally have occluded surfaces. This requires physical interaction\nwith objects to build their internal representations. This paper presents an\napproach that enables a robot to rapidly learn the complete 3D model of a given\nobject for manipulation in unfamiliar orientations. We use an ensemble of\npartially constructed NeRF models to quantify model uncertainty to determine\nthe next action (a visual or re-orientation action) by optimizing\ninformativeness and feasibility. Further, our approach determines when and how\nto grasp and re-orient an object given its partial NeRF model and re-estimates\nthe object pose to rectify misalignments introduced during the interaction.\nExperiments with a simulated Franka Emika Robot Manipulator operating in a\ntabletop environment with benchmark objects demonstrate an improvement of (i)\n14% in visual reconstruction quality (PSNR), (ii) 20% in the geometric/depth\nreconstruction of the object surface (F-score) and (iii) 71% in the task\nsuccess rate of manipulating objects a-priori unseen orientations/stable\nconfigurations in the scene; over current methods. The project page can be\nfound here: https://actnerf.github.io.",
      "tldr_zh": "该研究提出了一种基于不确定性感知主动学习的方法，使用NeRF模型帮助机器人操纵器快速构建未知物体的完整3D表示，以处理遮挡表面和未知方向问题。该方法通过NeRF模型集合量化不确定性，优化视觉动作和重新定向动作的选取，包括把握时机、抓取策略和姿态重新估计，以提升重建和操作效率。在模拟Franka Emika机器人实验中，与现有方法相比，该方法提高了14%的视觉重建质量（PSNR）、20%的几何深度重建（F-score）和71%的任务成功率。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "This work has been submitted to the IEEE for possible publication",
      "pdf_url": "http://arxiv.org/pdf/2404.01812v1",
      "published_date": "2024-04-02 10:15:06 UTC",
      "updated_date": "2024-04-02 10:15:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:06:50.922169"
    },
    {
      "arxiv_id": "2405.09529v2",
      "title": "Artificial Intelligence for the Internal Democracy of Political Parties",
      "title_zh": "翻译失败",
      "authors": [
        "Claudio Novelli",
        "Giuliano Formisano",
        "Prathm Juneja",
        "Giulia Sandri",
        "Luciano Floridi"
      ],
      "abstract": "The article argues that AI can enhance the measurement and implementation of\ndemocratic processes within political parties, known as Intra-Party Democracy\n(IPD). It identifies the limitations of traditional methods for measuring IPD,\nwhich often rely on formal parameters, self-reported data, and tools like\nsurveys. Such limitations lead to the collection of partial data, rare updates,\nand significant demands on resources. To address these issues, the article\nsuggests that specific data management and Machine Learning (ML) techniques,\nsuch as natural language processing and sentiment analysis, can improve the\nmeasurement (ML about) and practice (ML for) of IPD. The article concludes by\nconsidering some of the principal risks of ML for IPD, including concerns over\ndata privacy, the potential for manipulation, and the dangers of overreliance\non technology.",
      "tldr_zh": "该论文探讨了人工智能（AI）如何提升政党内部民主（Intra-Party Democracy, IPD）的测量和实施，强调了传统方法的不足，如依赖正式参数、自我报告数据和调查，导致数据不完整、更新频率低和资源消耗高。作者建议采用数据管理和机器学习（Machine Learning, ML）技术，包括自然语言处理和情感分析，来改善IPD的测量（ML about）和实际应用（ML for）。尽管这些方法能有效解决现有问题，但论文也指出了潜在风险，如数据隐私泄露、操纵可能性和过度依赖技术的危险。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.DB",
        "cs.LG",
        "cs.SI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.09529v2",
      "published_date": "2024-04-02 09:59:23 UTC",
      "updated_date": "2024-10-26 09:32:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:07:02.526639"
    },
    {
      "arxiv_id": "2404.01794v1",
      "title": "Imitation Game: A Model-based and Imitation Learning Deep Reinforcement Learning Hybrid",
      "title_zh": "翻译失败",
      "authors": [
        "Eric MSP Veith",
        "Torben Logemann",
        "Aleksandr Berezin",
        "Arlena Wellßow",
        "Stephan Balduin"
      ],
      "abstract": "Autonomous and learning systems based on Deep Reinforcement Learning have\nfirmly established themselves as a foundation for approaches to creating\nresilient and efficient Cyber-Physical Energy Systems. However, most current\napproaches suffer from two distinct problems: Modern model-free algorithms such\nas Soft Actor Critic need a high number of samples to learn a meaningful\npolicy, as well as a fallback to ward against concept drifts (e. g.,\ncatastrophic forgetting). In this paper, we present the work in progress\ntowards a hybrid agent architecture that combines model-based Deep\nReinforcement Learning with imitation learning to overcome both problems.",
      "tldr_zh": "该研究探讨了基于深度强化学习的自主学习系统在网络物理能量系统(Cyber-Physical Energy Systems)中的应用，但指出现有模型无关算法如Soft Actor Critic面临样本需求高和概念漂移(如灾难性遗忘)的问题。论文提出一个混合代理架构，将模型基于的深度强化学习(model-based Deep Reinforcement Learning)与模仿学习(imitation learning)相结合，以减少样本需求并提升系统鲁棒性。该方法作为正在进行的工作，有望为创建更高效、弹性的能量系统提供新途径。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted as publication at MSCPES '24",
      "pdf_url": "http://arxiv.org/pdf/2404.01794v1",
      "published_date": "2024-04-02 09:55:30 UTC",
      "updated_date": "2024-04-02 09:55:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:07:15.267811"
    },
    {
      "arxiv_id": "2404.02929v2",
      "title": "Using Large Language Models to Understand Telecom Standards",
      "title_zh": "利用大型语言模型理解电信标准",
      "authors": [
        "Athanasios Karapantelakis",
        "Mukesh Thakur",
        "Alexandros Nikou",
        "Farnaz Moradi",
        "Christian Orlog",
        "Fitsum Gaim",
        "Henrik Holm",
        "Doumitrou Daniil Nimara",
        "Vincent Huang"
      ],
      "abstract": "The Third Generation Partnership Project (3GPP) has successfully introduced\nstandards for global mobility. However, the volume and complexity of these\nstandards has increased over time, thus complicating access to relevant\ninformation for vendors and service providers. Use of Generative Artificial\nIntelligence (AI) and in particular Large Language Models (LLMs), may provide\nfaster access to relevant information. In this paper, we evaluate the\ncapability of state-of-art LLMs to be used as Question Answering (QA)\nassistants for 3GPP document reference. Our contribution is threefold. First,\nwe provide a benchmark and measuring methods for evaluating performance of\nLLMs. Second, we do data preprocessing and fine-tuning for one of these LLMs\nand provide guidelines to increase accuracy of the responses that apply to all\nLLMs. Third, we provide a model of our own, TeleRoBERTa, that performs on-par\nwith foundation LLMs but with an order of magnitude less number of parameters.\nResults show that LLMs can be used as a credible reference tool on telecom\ntechnical documents, and thus have potential for a number of different\napplications from troubleshooting and maintenance, to network operations and\nsoftware product development.",
      "tldr_zh": "本文探讨了使用 Large Language Models (LLMs) 来理解和访问 3GPP 电信标准的潜力，以解决标准文档的复杂性和信息获取挑战。研究的主要贡献包括：提供 LLMs 性能评估基准，进行数据预处理和微调以提升响应准确性，以及提出一个参数量少一个数量级的自定义模型 TeleRoBERTa，其表现与基础 LLMs 相当。结果显示，LLMs 可作为可靠的 Question Answering (QA) 助手，适用于电信领域的故障排除、网络操作和软件开发等应用。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to ICMLCN 2024, Stockholm, May 2024. Updating typo in\n  authors list",
      "pdf_url": "http://arxiv.org/pdf/2404.02929v2",
      "published_date": "2024-04-02 09:54:51 UTC",
      "updated_date": "2024-04-12 09:08:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:07:28.098759"
    },
    {
      "arxiv_id": "2404.02928v3",
      "title": "Jailbreaking Prompt Attack: A Controllable Adversarial Attack against Diffusion Models",
      "title_zh": "翻译失败",
      "authors": [
        "Jiachen Ma",
        "Anda Cao",
        "Zhiqing Xiao",
        "Yijiang Li",
        "Jie Zhang",
        "Chao Ye",
        "Junbo Zhao"
      ],
      "abstract": "Text-to-image (T2I) models can be maliciously used to generate harmful\ncontent such as sexually explicit, unfaithful, and misleading or\nNot-Safe-for-Work (NSFW) images. Previous attacks largely depend on the\navailability of the diffusion model or involve a lengthy optimization process.\nIn this work, we investigate a more practical and universal attack that does\nnot require the presence of a target model and demonstrate that the\nhigh-dimensional text embedding space inherently contains NSFW concepts that\ncan be exploited to generate harmful images. We present the Jailbreaking Prompt\nAttack (JPA). JPA first searches for the target malicious concepts in the text\nembedding space using a group of antonyms generated by ChatGPT. Subsequently, a\nprefix prompt is optimized in the discrete vocabulary space to align malicious\nconcepts semantically in the text embedding space. We further introduce a soft\nassignment with gradient masking technique that allows us to perform gradient\nascent in the discrete vocabulary space.\n  We perform extensive experiments with open-sourced T2I models, e.g.\nstable-diffusion-v1-4 and closed-sourced online services, e.g. DALLE2,\nMidjourney with black-box safety checkers. Results show that (1) JPA bypasses\nboth text and image safety checkers (2) while preserving high semantic\nalignment with the target prompt. (3) JPA demonstrates a much faster speed than\nprevious methods and can be executed in a fully automated manner. These merits\nrender it a valuable tool for robustness evaluation in future text-to-image\ngeneration research.",
      "tldr_zh": "本文提出了一种可控的对抗攻击方法，Jailbreaking Prompt Attack (JPA)，针对 Text-to-image (T2I) 模型中的 Diffusion Models，旨在利用文本嵌入空间的恶意概念生成有害图像，而无需访问目标模型。方法包括使用 ChatGPT 生成的 antonyms 搜索恶意概念，并在离散词汇空间优化 prefix prompt，同时引入 soft assignment with gradient masking 技术来实现语义对齐和梯度上升。实验在开源模型如 stable-diffusion-v1-4 和闭源服务如 DALLE2、Midjourney 上显示，JPA 成功绕过文本和图像安全检查器、保持高语义对齐，且比现有方法更快且完全自动化，为 T2I 生成研究的鲁棒性评估提供了宝贵工具。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.02928v3",
      "published_date": "2024-04-02 09:49:35 UTC",
      "updated_date": "2024-09-04 06:40:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:07:42.103034"
    },
    {
      "arxiv_id": "2404.01775v1",
      "title": "A noisy elephant in the room: Is your out-of-distribution detector robust to label noise?",
      "title_zh": "房间里的一头吵闹的大象：你的分布外检测器对标签噪声是否鲁棒？",
      "authors": [
        "Galadrielle Humblot-Renaux",
        "Sergio Escalera",
        "Thomas B. Moeslund"
      ],
      "abstract": "The ability to detect unfamiliar or unexpected images is essential for safe\ndeployment of computer vision systems. In the context of classification, the\ntask of detecting images outside of a model's training domain is known as\nout-of-distribution (OOD) detection. While there has been a growing research\ninterest in developing post-hoc OOD detection methods, there has been\ncomparably little discussion around how these methods perform when the\nunderlying classifier is not trained on a clean, carefully curated dataset. In\nthis work, we take a closer look at 20 state-of-the-art OOD detection methods\nin the (more realistic) scenario where the labels used to train the underlying\nclassifier are unreliable (e.g. crowd-sourced or web-scraped labels). Extensive\nexperiments across different datasets, noise types & levels, architectures and\ncheckpointing strategies provide insights into the effect of class label noise\non OOD detection, and show that poor separation between incorrectly classified\nID samples vs. OOD samples is an overlooked yet important limitation of\nexisting methods. Code: https://github.com/glhr/ood-labelnoise",
      "tldr_zh": "本研究探讨了 out-of-distribution (OOD) 检测方法在标签噪声环境下的鲁棒性，强调了在计算机视觉系统中检测未知图像的重要性，但现有方法鲜有针对训练标签不可靠（如众包或网络抓取）场景的评估。作者通过对20种最先进OOD检测方法的广泛实验，涵盖不同数据集、噪声类型和水平、模型架构以及检查点策略，揭示了标签噪声对OOD检测性能的负面影响。结果显示，现有方法在区分错误分类的 in-distribution (ID) 样本与OOD样本方面存在显著缺陷，这是一个被忽视的关键问题。代码已开源，可用于进一步研究（https://github.com/glhr/ood-labelnoise）。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at CVPR 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.01775v1",
      "published_date": "2024-04-02 09:40:22 UTC",
      "updated_date": "2024-04-02 09:40:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:07:51.158692"
    },
    {
      "arxiv_id": "2404.01768v2",
      "title": "Stereotype Detection in LLMs: A Multiclass, Explainable, and Benchmark-Driven Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Zekun Wu",
        "Sahan Bulathwela",
        "Maria Perez-Ortiz",
        "Adriano Soares Koshiyama"
      ],
      "abstract": "Stereotype detection is a challenging and subjective task, as certain\nstatements, such as \"Black people like to play basketball,\" may not appear\novertly toxic but still reinforce racial stereotypes. With the increasing\nprevalence of large language models (LLMs) in human-facing artificial\nintelligence (AI) applications, detecting these types of biases is essential.\nHowever, LLMs risk perpetuating and amplifying stereotypical outputs derived\nfrom their training data. A reliable stereotype detector is crucial for\nbenchmarking bias, monitoring model input and output, filtering training data,\nand ensuring fairer model behavior in downstream applications. This paper\nintroduces the Multi-Grain Stereotype (MGS) dataset, consisting of 51,867\ninstances across gender, race, profession, religion, and other stereotypes,\ncurated from multiple existing datasets. We evaluate various machine learning\napproaches to establish baselines and fine-tune language models of different\narchitectures and sizes, presenting a suite of stereotype multiclass\nclassifiers trained on the MGS dataset. Given the subjectivity of stereotypes,\nexplainability is essential to align model learning with human understanding of\nstereotypes. We employ explainable AI (XAI) tools, including SHAP, LIME, and\nBertViz, to assess whether the model's learned patterns align with human\nintuitions about stereotypes.Additionally, we develop stereotype elicitation\nprompts and benchmark the presence of stereotypes in text generation tasks\nusing popular LLMs, employing the best-performing stereotype classifiers.",
      "tldr_zh": "本论文提出了一种多类、可解释且基准驱动的方法，用于检测大型语言模型（LLMs）中的刻板印象问题，强调了识别如“Black people like to play basketball”等隐性偏见的重要性，以确保AI应用更公平。研究引入了Multi-Grain Stereotype (MGS)数据集，包含51,867个实例，覆盖性别、种族、职业、宗教等类别，并通过机器学习方法和微调不同架构的语言模型，建立了一系列多类刻板印象分类器。利用可解释AI (XAI)工具如SHAP、LIME和BertViz，论文验证了模型学习是否与人类直觉一致，并开发了刻板印象诱导提示来基准测试流行LLMs的文本生成任务，从而为监测和缓解偏见提供了可靠工具。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Under review as a conference paper at ARR October 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.01768v2",
      "published_date": "2024-04-02 09:31:32 UTC",
      "updated_date": "2024-11-16 00:54:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:08:04.390960"
    },
    {
      "arxiv_id": "2404.02180v4",
      "title": "Remote sensing framework for geological mapping via stacked autoencoders and clustering",
      "title_zh": "通过堆叠自动编码器和聚类实现的遥感地质映射框架",
      "authors": [
        "Sandeep Nagar",
        "Ehsan Farahbakhsh",
        "Joseph Awange",
        "Rohitash Chandra"
      ],
      "abstract": "Supervised machine learning methods for geological mapping via remote sensing\nface limitations due to the scarcity of accurately labelled training data that\ncan be addressed by unsupervised learning, such as dimensionality reduction and\nclustering. Dimensionality reduction methods have the potential to play a\ncrucial role in improving the accuracy of geological maps. Although\nconventional dimensionality reduction methods may struggle with nonlinear data,\nunsupervised deep learning models such as autoencoders can model non-linear\nrelationships. Stacked autoencoders feature multiple interconnected layers to\ncapture hierarchical data representations useful for remote sensing data. We\npresent an unsupervised machine learning-based framework for processing remote\nsensing data using stacked autoencoders for dimensionality reduction and\nk-means clustering for mapping geological units. We use Landsat 8, ASTER, and\nSentinel-2 datasets to evaluate the framework for geological mapping of the\nMutawintji region in Western New South Wales, Australia. We also compare\nstacked autoencoders with principal component analysis (PCA) and canonical\nautoencoders. Our results reveal that the framework produces accurate and\ninterpretable geological maps, efficiently discriminating rock units. The\nresults reveal that the combination of stacked autoencoders with Sentinel-2\ndata yields the best performance accuracy when compared to other combinations.\nWe find that stacked autoencoders enable better extraction of complex and\nhierarchical representations of the input data when compared to canonical\nautoencoders and PCA. We also find that the generated maps align with prior\ngeological knowledge of the study area while providing novel insights into\ngeological structures.",
      "tldr_zh": "这篇论文提出了一种无监督机器学习框架，用于遥感数据的地质映射，采用 stacked autoencoders 进行降维和 k-means clustering 划分地质单位，以解决标注数据稀缺的问题。框架使用 Landsat 8、ASTER 和 Sentinel-2 数据，对澳大利亚新南威尔士西部 Mutawintji 地区进行评估，并与 PCA 和标准 autoencoders 进行比较。结果表明，stacked autoencoders 与 Sentinel-2 数据结合的组合表现出最佳准确性，能够更好地提取复杂数据表示，并生成准确、可解释的地质地图，与现有地质知识一致并提供新见解。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.02180v4",
      "published_date": "2024-04-02 09:15:32 UTC",
      "updated_date": "2024-09-21 06:02:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:08:15.894917"
    },
    {
      "arxiv_id": "2404.01754v1",
      "title": "Peer-aided Repairer: Empowering Large Language Models to Repair Advanced Student Assignments",
      "title_zh": "同伴辅助修复器：赋能大型语言模型修复高级学生作业",
      "authors": [
        "Qianhui Zhao",
        "Fang Liu",
        "Li Zhang",
        "Yang Liu",
        "Zhen Yan",
        "Zhenghao Chen",
        "Yufei Zhou",
        "Jing Jiang",
        "Ge Li"
      ],
      "abstract": "Automated generation of feedback on programming assignments holds significant\nbenefits for programming education, especially when it comes to advanced\nassignments. Automated Program Repair techniques, especially Large Language\nModel based approaches, have gained notable recognition for their potential to\nfix introductory assignments. However, the programs used for evaluation are\nrelatively simple. It remains unclear how existing approaches perform in\nrepairing programs from higher-level programming courses. To address these\nlimitations, we curate a new advanced student assignment dataset named\nDefects4DS from a higher-level programming course. Subsequently, we identify\nthe challenges related to fixing bugs in advanced assignments. Based on the\nanalysis, we develop a framework called PaR that is powered by the LLM. PaR\nworks in three phases: Peer Solution Selection, Multi-Source Prompt Generation,\nand Program Repair. Peer Solution Selection identifies the closely related peer\nprograms based on lexical, semantic, and syntactic criteria. Then Multi-Source\nPrompt Generation adeptly combines multiple sources of information to create a\ncomprehensive and informative prompt for the last Program Repair stage. The\nevaluation on Defects4DS and another well-investigated ITSP dataset reveals\nthat PaR achieves a new state-of-the-art performance, demonstrating impressive\nimprovements of 19.94% and 15.2% in repair rate compared to prior\nstate-of-the-art LLM- and symbolic-based approaches, respectively",
      "tldr_zh": "这篇论文针对高级编程作业的自动修复问题，构建了一个新数据集Defects4DS，以评估现有Automated Program Repair（APR）技术在复杂任务中的表现。研究者开发了PaR框架，利用Large Language Model（LLM），通过三个阶段——Peer Solution Selection（基于词汇、语义和句法选择相关程序）、Multi-Source Prompt Generation（结合多源信息生成提示）和Program Repair（进行实际修复）——来解决修复挑战。实验结果显示，PaR在Defects4DS和ITSP数据集上实现了新的最先进性能，修复率分别比之前的最先进LLM和符号方法提高了19.94%和15.2%。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "On-going work",
      "pdf_url": "http://arxiv.org/pdf/2404.01754v1",
      "published_date": "2024-04-02 09:12:21 UTC",
      "updated_date": "2024-04-02 09:12:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:08:29.738716"
    },
    {
      "arxiv_id": "2404.01752v3",
      "title": "Safe Interval RRT* for Scalable Multi-Robot Path Planning in Continuous Space",
      "title_zh": "翻译失败",
      "authors": [
        "Joonyeol Sim",
        "Joonkyung Kim",
        "Changjoo Nam"
      ],
      "abstract": "In this paper, we consider the problem of Multi-Robot Path Planning (MRPP) in\ncontinuous space. The difficulty of the problem arises from the extremely large\nsearch space caused by the combinatorial nature of the problem and the\ncontinuous state space. We propose a two-level approach where the low level is\na sampling-based planner Safe Interval RRT* (SI-RRT*) that finds a\ncollision-free trajectory for individual robots. The high level can use any\nmethod that can resolve inter-robot conflicts where we employ two\nrepresentative methods that are Prioritized Planning (SI-CPP) and Conflict\nBased Search (SI-CCBS). Experimental results show that SI-RRT* can quickly find\na high-quality solution with a few samples. SI-CPP exhibits improved\nscalability while SI-CCBS produces higher-quality solutions compared to the\nstate-of-the-art planners for continuous space.",
      "tldr_zh": "本研究针对多机器人路径规划（Multi-Robot Path Planning, MRPP）在连续空间中的挑战，提出了一种可扩展的两层方法，以应对巨大的搜索空间问题。底层采用基于采样的规划器 Safe Interval RRT* (SI-RRT*)，为单个机器人生成无碰撞轨迹；高层则使用 Prioritized Planning (SI-CPP) 或 Conflict Based Search (SI-CCBS) 来解决机器人间的冲突。实验结果显示，SI-RRT* 能快速找到高质量解决方案，SI-CPP 显著提升了可扩展性，而 SI-CCBS 比现有状态-of-the-art 规划器提供更高质量的路径。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.01752v3",
      "published_date": "2024-04-02 09:07:12 UTC",
      "updated_date": "2025-02-11 13:46:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:08:41.168730"
    },
    {
      "arxiv_id": "2404.01746v1",
      "title": "Towards Scalable & Efficient Interaction-Aware Planning in Autonomous Vehicles using Knowledge Distillation",
      "title_zh": "翻译失败",
      "authors": [
        "Piyush Gupta",
        "David Isele",
        "Sangjae Bae"
      ],
      "abstract": "Real-world driving involves intricate interactions among vehicles navigating\nthrough dense traffic scenarios. Recent research focuses on enhancing the\ninteraction awareness of autonomous vehicles to leverage these interactions in\ndecision-making. These interaction-aware planners rely on neural-network-based\nprediction models to capture inter-vehicle interactions, aiming to integrate\nthese predictions with traditional control techniques such as Model Predictive\nControl. However, this integration of deep learning-based models with\ntraditional control paradigms often results in computationally demanding\noptimization problems, relying on heuristic methods. This study introduces a\nprincipled and efficient method for combining deep learning with constrained\noptimization, employing knowledge distillation to train smaller and more\nefficient networks, thereby mitigating complexity. We demonstrate that these\nrefined networks maintain the problem-solving efficacy of larger models while\nsignificantly accelerating optimization. Specifically, in the domain of\ninteraction-aware trajectory planning for autonomous vehicles, we illustrate\nthat training a smaller prediction network using knowledge distillation speeds\nup optimization without sacrificing accuracy.",
      "tldr_zh": "本研究针对自动驾驶车辆在密集交通场景中的互动感知规划问题，提出了一种可扩展且高效的方法，通过 Knowledge Distillation 训练更小、更高效的神经网络预测模型，以减轻计算密集型优化的复杂性。该方法将深度学习模型与传统控制技术如 Model Predictive Control 相结合，确保捕捉车辆间互动的同时加速优化过程。实验结果表明，该优化网络维持了较大模型的准确性，并在互动感知轨迹规划中显著提升了效率。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.01746v1",
      "published_date": "2024-04-02 09:04:06 UTC",
      "updated_date": "2024-04-02 09:04:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:08:52.797920"
    },
    {
      "arxiv_id": "2404.01745v1",
      "title": "Unleash the Potential of CLIP for Video Highlight Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Donghoon Han",
        "Seunghyeon Seo",
        "Eunhwan Park",
        "Seong-Uk Nam",
        "Nojun Kwak"
      ],
      "abstract": "Multimodal and large language models (LLMs) have revolutionized the\nutilization of open-world knowledge, unlocking novel potentials across various\ntasks and applications. Among these domains, the video domain has notably\nbenefited from their capabilities. In this paper, we present Highlight-CLIP\n(HL-CLIP), a method designed to excel in the video highlight detection task by\nleveraging the pre-trained knowledge embedded in multimodal models. By simply\nfine-tuning the multimodal encoder in combination with our innovative saliency\npooling technique, we have achieved the state-of-the-art performance in the\nhighlight detection task, the QVHighlight Benchmark, to the best of our\nknowledge.",
      "tldr_zh": "本论文提出 Highlight-CLIP (HL-CLIP) 方法，利用 CLIP 的预训练知识来提升视频高亮检测任务的性能。\n该方法仅通过微调多模态编码器并结合创新的 saliency pooling technique，即显著性池化技术，即实现了对视频关键部分的精确识别。\n在 QVHighlight Benchmark 上，HL-CLIP 达到了 state-of-the-art 性能，展示了多模态模型在视频领域的强大潜力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.01745v1",
      "published_date": "2024-04-02 09:01:58 UTC",
      "updated_date": "2024-04-02 09:01:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:09:04.247625"
    },
    {
      "arxiv_id": "2404.01741v5",
      "title": "Intrusion Tolerance for Networked Systems through Two-Level Feedback Control",
      "title_zh": "翻译失败",
      "authors": [
        "Kim Hammar",
        "Rolf Stadler"
      ],
      "abstract": "We formulate intrusion tolerance for a system with service replicas as a\ntwo-level optimal control problem. On the local level node controllers perform\nintrusion recovery, and on the global level a system controller manages the\nreplication factor. The local and global control problems can be formulated as\nclassical problems in operations research, namely, the machine replacement\nproblem and the inventory replenishment problem. Based on this formulation, we\ndesign TOLERANCE, a novel control architecture for intrusion-tolerant systems.\nWe prove that the optimal control strategies on both levels have threshold\nstructure and design efficient algorithms for computing them. We implement and\nevaluate TOLERANCE in an emulation environment where we run 10 types of network\nintrusions. The results show that TOLERANCE can improve service availability\nand reduce operational cost compared with state-of-the-art intrusion-tolerant\nsystems.",
      "tldr_zh": "本研究将网络系统的入侵容忍问题表述为一个两级最优控制问题：在局部级别，节点控制器负责入侵恢复（类似于机器更换问题）；在全局级别，系统控制器管理复制因子（类似于库存补充问题）。基于此，论文设计了 TOLERANCE 控制架构，并证明其最优控制策略具有阈值结构，同时开发了高效算法来计算这些策略。在仿真环境中测试了 10 种网络入侵类型，结果显示 TOLERANCE 比现有系统提高了服务可用性和降低了操作成本。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.CR",
        "cs.GT",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.DC",
      "comment": "Preprint to appear in the conference proceedings of the 54th\n  IEEE/IFIP Dependable Systems and Networks Conference (DSN'24)",
      "pdf_url": "http://arxiv.org/pdf/2404.01741v5",
      "published_date": "2024-04-02 09:00:45 UTC",
      "updated_date": "2024-06-05 06:50:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:09:17.124183"
    },
    {
      "arxiv_id": "2404.01740v1",
      "title": "Weakly-supervised Audio Separation via Bi-modal Semantic Similarity",
      "title_zh": "弱监督音频分离基于双模态语义相似性",
      "authors": [
        "Tanvir Mahmud",
        "Saeed Amizadeh",
        "Kazuhito Koishida",
        "Diana Marculescu"
      ],
      "abstract": "Conditional sound separation in multi-source audio mixtures without having\naccess to single source sound data during training is a long standing\nchallenge. Existing mix-and-separate based methods suffer from significant\nperformance drop with multi-source training mixtures due to the lack of\nsupervision signal for single source separation cases during training. However,\nin the case of language-conditional audio separation, we do have access to\ncorresponding text descriptions for each audio mixture in our training data,\nwhich can be seen as (rough) representations of the audio samples in the\nlanguage modality. To this end, in this paper, we propose a generic bi-modal\nseparation framework which can enhance the existing unsupervised frameworks to\nseparate single-source signals in a target modality (i.e., audio) using the\neasily separable corresponding signals in the conditioning modality (i.e.,\nlanguage), without having access to single-source samples in the target\nmodality during training. We empirically show that this is well within reach if\nwe have access to a pretrained joint embedding model between the two modalities\n(i.e., CLAP). Furthermore, we propose to incorporate our framework into two\nfundamental scenarios to enhance separation performance. First, we show that\nour proposed methodology significantly improves the performance of purely\nunsupervised baselines by reducing the distribution shift between training and\ntest samples. In particular, we show that our framework can achieve 71% boost\nin terms of Signal-to-Distortion Ratio (SDR) over the baseline, reaching 97.5%\nof the supervised learning performance. Second, we show that we can further\nimprove the performance of the supervised learning itself by 17% if we augment\nit by our proposed weakly-supervised framework, that enables a powerful\nsemi-supervised framework for audio separation.",
      "tldr_zh": "本论文提出了一种弱监督（Weakly-supervised）音频分离框架，通过双模态语义相似性（Bi-modal Semantic Similarity）利用语言模态的文本描述来辅助音频模态的分离，而无需训练时访问单源音频数据。该框架整合预训练联合嵌入模型（如 CLAP），减少训练和测试样本的分布偏移，并在无监督基线上实现了71%的信号失真比（SDR）提升，达到监督学习性能的97.5%。此外，将该框架与监督学习结合，还可进一步提高分离性能17%，构建一个高效的半监督音频分离系统。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Tech report. Accepted in ICLR-2024",
      "pdf_url": "http://arxiv.org/pdf/2404.01740v1",
      "published_date": "2024-04-02 08:59:58 UTC",
      "updated_date": "2024-04-02 08:59:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:09:29.199845"
    },
    {
      "arxiv_id": "2404.01716v1",
      "title": "Effective internal language model training and fusion for factorized transducer model",
      "title_zh": "用于因子化转录器模型的有效内部语言模型训练与融合",
      "authors": [
        "Jinxi Guo",
        "Niko Moritz",
        "Yingyi Ma",
        "Frank Seide",
        "Chunyang Wu",
        "Jay Mahadeokar",
        "Ozlem Kalinli",
        "Christian Fuegen",
        "Mike Seltzer"
      ],
      "abstract": "The internal language model (ILM) of the neural transducer has been widely\nstudied. In most prior work, it is mainly used for estimating the ILM score and\nis subsequently subtracted during inference to facilitate improved integration\nwith external language models. Recently, various of factorized transducer\nmodels have been proposed, which explicitly embrace a standalone internal\nlanguage model for non-blank token prediction. However, even with the adoption\nof factorized transducer models, limited improvement has been observed compared\nto shallow fusion. In this paper, we propose a novel ILM training and decoding\nstrategy for factorized transducer models, which effectively combines the\nblank, acoustic and ILM scores. Our experiments show a 17% relative improvement\nover the standard decoding method when utilizing a well-trained ILM and the\nproposed decoding strategy on LibriSpeech datasets. Furthermore, when compared\nto a strong RNN-T baseline enhanced with external LM fusion, the proposed model\nyields a 5.5% relative improvement on general-sets and an 8.9% WER reduction\nfor rare words. The proposed model can achieve superior performance without\nrelying on external language models, rendering it highly efficient for\nproduction use-cases. To further improve the performance, we propose a novel\nand memory-efficient ILM-fusion-aware minimum word error rate (MWER) training\nmethod which improves ILM integration significantly.",
      "tldr_zh": "本研究针对因子化转录器模型（factorized transducer model）提出了一种有效的内部语言模型（ILM）训练和融合策略，通过结合空白、声学和 ILM 分数来提升模型性能。相比标准解码方法，该策略在 LibriSpeech 数据集上实现了 17% 的相对改善，并在稀有词识别上表现出色，与增强的 RNN-T 基线相比，整体 WER 减少 5.5% 至 8.9%。该方法无需依赖外部语言模型，使其在生产环境中更高效；此外，研究还引入了一种内存高效的 ILM-fusion-aware MWER 训练方法，进一步优化 ILM 整合。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted to ICASSP 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.01716v1",
      "published_date": "2024-04-02 08:01:05 UTC",
      "updated_date": "2024-04-02 08:01:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:09:41.871713"
    },
    {
      "arxiv_id": "2404.01714v4",
      "title": "Conjugate-Gradient-like Based Adaptive Moment Estimation Optimization Algorithm for Deep Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Jiawu Tian",
        "Liwei Xu",
        "Xiaowei Zhang",
        "Yongqi Li"
      ],
      "abstract": "Training deep neural networks is a challenging task. In order to speed up\ntraining and enhance the performance of deep neural networks, we rectify the\nvanilla conjugate gradient as conjugate-gradient-like and incorporate it into\nthe generic Adam, and thus propose a new optimization algorithm named\nCG-like-Adam for deep learning. Specifically, both the first-order and the\nsecond-order moment estimation of generic Adam are replaced by the\nconjugate-gradient-like. Convergence analysis handles the cases where the\nexponential moving average coefficient of the first-order moment estimation is\nconstant and the first-order moment estimation is unbiased. Numerical\nexperiments show the superiority of the proposed algorithm based on the\nCIFAR10/100 dataset.",
      "tldr_zh": "本论文针对深度神经网络训练的挑战，提出了一种新的优化算法CG-like-Adam，将改进后的conjugate-gradient-like方法整合到Adam算法中。具体而言，该算法将Adam的first-order和second-order moment estimation替换为conjugate-gradient-like，以提升训练效率和性能。作者进行了收敛分析，涵盖了first-order moment estimation的指数移动平均系数为常量以及无偏情况。实验结果显示，在CIFAR10/100数据集上，CG-like-Adam表现出优越性，证明了其在加速训练和提升模型表现方面的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "32 pages, 13 figures",
      "pdf_url": "http://arxiv.org/pdf/2404.01714v4",
      "published_date": "2024-04-02 07:57:17 UTC",
      "updated_date": "2025-01-08 06:52:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:09:53.071580"
    },
    {
      "arxiv_id": "2404.01713v2",
      "title": "Generative AI for Immersive Communication: The Next Frontier in Internet-of-Senses Through 6G",
      "title_zh": "翻译失败",
      "authors": [
        "Nassim Sehad",
        "Lina Bariah",
        "Wassim Hamidouche",
        "Hamed Hellaoui",
        "Riku Jäntti",
        "Mérouane Debbah"
      ],
      "abstract": "Over the past two decades, the Internet-of-Things (IoT) has become a\ntransformative concept, and as we approach 2030, a new paradigm known as the\nInternet of Senses (IoS) is emerging. Unlike conventional Virtual Reality (VR),\nIoS seeks to provide multi-sensory experiences, acknowledging that in our\nphysical reality, our perception extends far beyond just sight and sound; it\nencompasses a range of senses. This article explores the existing technologies\ndriving immersive multi-sensory media, delving into their capabilities and\npotential applications. This exploration includes a comparative analysis\nbetween conventional immersive media streaming and a proposed use case that\nleverages semantic communication empowered by generative Artificial\nIntelligence (AI). The focal point of this analysis is the substantial\nreduction in bandwidth consumption by 99.93% in the proposed scheme. Through\nthis comparison, we aim to underscore the practical applications of generative\nAI for immersive media. Concurrently addressing major challenges in this field,\nsuch as temporal synchronization of multiple media, ensuring high throughput,\nminimizing the End-to-End (E2E) latency, and robustness to low bandwidth while\noutlining future trajectories.",
      "tldr_zh": "本文论文探讨了生成式 AI 在 Internet-of-Senses (IoS) 领域的应用，作为 6G 技术推动沉浸式通信的下一个前沿，超越传统 Virtual Reality (VR) 提供多感官体验。作者通过比较传统沉浸式媒体流与基于生成式 AI 的语义通信方案，展示了新方案可将带宽消耗减少 99.93%，并分析了其在多感官媒体中的潜力。论文同时指出了关键挑战，如多媒体的时序同步、高吞吐量、低 End-to-End (E2E) 延迟以及低带宽鲁棒性，并概述了未来发展方向。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC",
        "cs.MM",
        "cs.NI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.01713v2",
      "published_date": "2024-04-02 07:57:05 UTC",
      "updated_date": "2024-08-13 12:58:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:10:04.261347"
    },
    {
      "arxiv_id": "2404.01712v4",
      "title": "Hessian-Free Online Certified Unlearning",
      "title_zh": "翻译失败",
      "authors": [
        "Xinbao Qiao",
        "Meng Zhang",
        "Ming Tang",
        "Ermin Wei"
      ],
      "abstract": "Machine unlearning strives to uphold the data owners' right to be forgotten\nby enabling models to selectively forget specific data. Recent advances suggest\npre-computing and storing statistics extracted from second-order information\nand implementing unlearning through Newton-style updates. However, the Hessian\nmatrix operations are extremely costly and previous works conduct unlearning\nfor empirical risk minimizer with the convexity assumption, precluding their\napplicability to high-dimensional over-parameterized models and the\nnonconvergence condition. In this paper, we propose an efficient Hessian-free\nunlearning approach. The key idea is to maintain a statistical vector for each\ntraining data, computed through affine stochastic recursion of the difference\nbetween the retrained and learned models. We prove that our proposed method\noutperforms the state-of-the-art methods in terms of the unlearning and\ngeneralization guarantees, the deletion capacity, and the time/storage\ncomplexity, under the same regularity conditions. Through the strategy of\nrecollecting statistics for removing data, we develop an online unlearning\nalgorithm that achieves near-instantaneous data removal, as it requires only\nvector addition. Experiments demonstrate that our proposed scheme surpasses\nexisting results by orders of magnitude in terms of time/storage costs with\nmillisecond-level unlearning execution, while also enhancing test accuracy.",
      "tldr_zh": "本文提出了一种Hessian-free在线认证unlearning方法，通过为每个训练数据维护一个统计向量，并利用affine stochastic recursion计算重训练模型与学习模型的差异，实现高效的数据删除。该方法在unlearning和泛化保证、删除容量以及时间/存储复杂度上优于现有技术，且无需Hessian矩阵计算。实验结果表明，该算法的unlearning执行时间仅需毫秒级，同时显著降低了存储成本并提升了测试准确率。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2404.01712v4",
      "published_date": "2024-04-02 07:54:18 UTC",
      "updated_date": "2025-02-06 13:23:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:10:16.981928"
    },
    {
      "arxiv_id": "2404.01709v1",
      "title": "Upsample Guidance: Scale Up Diffusion Models without Training",
      "title_zh": "上采样指导",
      "authors": [
        "Juno Hwang",
        "Yong-Hyun Park",
        "Junghyo Jo"
      ],
      "abstract": "Diffusion models have demonstrated superior performance across various\ngenerative tasks including images, videos, and audio. However, they encounter\ndifficulties in directly generating high-resolution samples. Previously\nproposed solutions to this issue involve modifying the architecture, further\ntraining, or partitioning the sampling process into multiple stages. These\nmethods have the limitation of not being able to directly utilize pre-trained\nmodels as-is, requiring additional work. In this paper, we introduce upsample\nguidance, a technique that adapts pretrained diffusion model (e.g., $512^2$) to\ngenerate higher-resolution images (e.g., $1536^2$) by adding only a single term\nin the sampling process. Remarkably, this technique does not necessitate any\nadditional training or relying on external models. We demonstrate that upsample\nguidance can be applied to various models, such as pixel-space, latent space,\nand video diffusion models. We also observed that the proper selection of\nguidance scale can improve image quality, fidelity, and prompt alignment.",
      "tldr_zh": "扩散模型（Diffusion models）在图像、视频和音频生成任务中表现出色，但难以直接生成高分辨率样本。论文提出了一种名为“upsample guidance”的技术，通过在采样过程中添加一个额外术语，使预训练模型（如512^2分辨率）能够生成更高分辨率图像（如1536^2），且无需额外训练或依赖外部模型。该方法适用于像素空间、潜在空间和视频扩散模型，并发现适当选择“guidance scale”可以显著提升图像质量、保真度（fidelity）和提示对齐（prompt alignment）。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "15 pages, 15 Figures",
      "pdf_url": "http://arxiv.org/pdf/2404.01709v1",
      "published_date": "2024-04-02 07:49:08 UTC",
      "updated_date": "2024-04-02 07:49:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:10:30.302613"
    },
    {
      "arxiv_id": "2404.01685v3",
      "title": "SpiKernel: A Kernel Size Exploration Methodology for Improving Accuracy of the Embedded Spiking Neural Network Systems",
      "title_zh": "SpiKernel: 一种用于提高嵌入式脉冲神经网络系统准确性的内核大小探索方法论",
      "authors": [
        "Rachmad Vidya Wicaksana Putra",
        "Muhammad Shafique"
      ],
      "abstract": "Spiking Neural Networks (SNNs) can offer ultra-low power/energy consumption\nfor machine learning-based application tasks due to their sparse spike-based\noperations. Currently, most of the SNN architectures need a significantly\nlarger model size to achieve higher accuracy, which is not suitable for\nresource-constrained embedded applications. Therefore, developing SNNs that can\nachieve high accuracy with acceptable memory footprint is highly needed. Toward\nthis, we propose SpiKernel, a novel methodology that improves the accuracy of\nSNNs through kernel size exploration. Its key steps include (1) investigating\nthe impact of different kernel sizes on the accuracy, (2) devising new sets of\nkernel sizes, (3) generating SNN architectures using neural architecture search\nbased on the selected kernel sizes, and (4) analyzing the accuracy-memory\ntrade-offs for SNN model selection. The experimental results show that our\nSpiKernel achieves higher accuracy than state-of-the-art works (i.e., 93.24%\nfor CIFAR10, 70.84% for CIFAR100, and 62% for TinyImageNet) with less than 10M\nparameters and up to 4.8x speed-up of searching time, thereby making it\nsuitable for embedded applications.",
      "tldr_zh": "该研究针对Spiking Neural Networks (SNNs) 在嵌入式应用中面临的模型大小与准确率权衡问题，提出SpiKernel方法，通过探索核大小来提升SNNs的准确性。SpiKernel的关键步骤包括调查不同核大小对准确性的影响、设计新核大小集合、使用神经架构搜索生成SNN架构，以及分析准确性与内存权衡以选择最佳模型。实验结果显示，该方法在CIFAR10上达到93.24%、CIFAR100上70.84%、TinyImageNet上62%的准确率，同时使用少于10M参数，并实现高达4.8倍的搜索时间加速，使其更适合资源受限的嵌入式系统。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "Accepted for publication at the IEEE Embedded Systems Letters",
      "pdf_url": "http://arxiv.org/pdf/2404.01685v3",
      "published_date": "2024-04-02 06:42:14 UTC",
      "updated_date": "2024-12-08 08:29:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:10:42.932093"
    },
    {
      "arxiv_id": "2404.01677v2",
      "title": "Towards Generalizable and Faithful Logic Reasoning over Natural Language via Resolution Refutation",
      "title_zh": "翻译失败",
      "authors": [
        "Zhouhao Sun",
        "Xiao Ding",
        "Li Du",
        "Bibo Cai",
        "Jinglong Gao",
        "Ting Liu",
        "Qin Bing"
      ],
      "abstract": "Large language models (LLMs) have achieved significant performance in various\nnatural language reasoning tasks. However, they still struggle with performing\nfirst-order logic reasoning over formal logical theories expressed in natural\nlanguage. This is because the previous LLMs-based reasoning systems have the\ntheoretical incompleteness issue. As a result, it can only address a limited\nset of simple reasoning problems, which significantly decreases their\ngeneralization ability. To address this issue, we propose a novel framework,\nnamed Generalizable and Faithful Reasoner (GFaiR), which introduces the\nparadigm of resolution refutation. Resolution refutation has the capability to\nsolve all first-order logic reasoning problems by extending reasoning rules and\nemploying the principle of proof by contradiction, so our system's completeness\ncan be improved by introducing resolution refutation. Experimental results\ndemonstrate that our system outperforms previous works by achieving\nstate-of-the-art performances in complex scenarios while maintaining\nperformances in simple scenarios. Besides, we observe that GFaiR is faithful to\nits reasoning process.",
      "tldr_zh": "本研究针对大型语言模型（LLMs）在自然语言中进行一阶逻辑推理的局限性（如理论不完整和泛化能力弱），提出了一种名为 GFaiR 的新型框架。GFaiR 通过引入 resolution refutation 范式，扩展推理规则并采用证明矛盾原理，从而能够处理所有一阶逻辑推理问题，提高系统的完整性和泛化能力。实验结果表明，GFaiR 在复杂场景下实现了最先进性能，同时在简单场景保持原有水平，并确保推理过程的忠实性（faithful）。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "LREC-Coling 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.01677v2",
      "published_date": "2024-04-02 06:28:44 UTC",
      "updated_date": "2024-04-03 09:28:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:10:54.116755"
    },
    {
      "arxiv_id": "2404.01663v6",
      "title": "CMAT: A Multi-Agent Collaboration Tuning Framework for Enhancing Small Language Models",
      "title_zh": "CMAT：多智能体协作调优框架，用于增强小型语言模型",
      "authors": [
        "Xuechen Liang",
        "Yangfan He",
        "Meiling Tao",
        "Yinghui Xia",
        "Jianhui Wang",
        "Tianyu Shi",
        "Jun Wang",
        "JingSong Yang"
      ],
      "abstract": "Open large language models (LLMs) have significantly advanced the field of\nnatural language processing, showcasing impressive performance across various\ntasks.Despite the significant advancements in LLMs, their effective operation\nstill relies heavily on human input to accurately guide the dialogue flow, with\nagent tuning being a crucial optimization technique that involves human\nadjustments to the model for better response to such guidance.Addressing this\ndependency, our work introduces the TinyAgent model, trained on a meticulously\ncurated high-quality dataset. We also present the Collaborative Multi-Agent\nTuning (CMAT) framework, an innovative system designed to augment language\nagent capabilities through adaptive weight updates based on environmental\nfeedback. This framework fosters collaborative learning and real-time\nadaptation among multiple intelligent agents, enhancing their context-awareness\nand long-term memory. In this research, we propose a new communication agent\nframework that integrates multi-agent systems with environmental feedback\nmechanisms, offering a scalable method to explore cooperative behaviors.\nNotably, our TinyAgent-7B model exhibits performance on par with GPT-3.5,\ndespite having fewer parameters, signifying a substantial improvement in the\nefficiency and effectiveness of LLMs.",
      "tldr_zh": "本文提出 CMAT（Collaborative Multi-Agent Tuning）框架和 TinyAgent 模型，以减少大型语言模型（LLMs）对人类输入的依赖，通过多智能体协作、环境反馈机制和自适应权重更新来提升小型语言模型的上下文感知和长期记忆能力。CMAT 框架整合多代理系统，支持协作学习和实时适应，提供一种可扩展的方法来探索合作行为。实验结果显示，TinyAgent-7B 模型在参数更少的情况下，其性能可与 GPT-3.5 相当，显著提高了 LLMs 的效率和有效性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CC"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.01663v6",
      "published_date": "2024-04-02 06:07:35 UTC",
      "updated_date": "2025-04-15 15:28:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:11:05.768024"
    },
    {
      "arxiv_id": "2404.01657v1",
      "title": "Release of Pre-Trained Models for the Japanese Language",
      "title_zh": "日语预训练模型的发布",
      "authors": [
        "Kei Sawada",
        "Tianyu Zhao",
        "Makoto Shing",
        "Kentaro Mitsui",
        "Akio Kaga",
        "Yukiya Hono",
        "Toshiaki Wakatsuki",
        "Koh Mitsuda"
      ],
      "abstract": "AI democratization aims to create a world in which the average person can\nutilize AI techniques. To achieve this goal, numerous research institutes have\nattempted to make their results accessible to the public. In particular, large\npre-trained models trained on large-scale data have shown unprecedented\npotential, and their release has had a significant impact. However, most of the\nreleased models specialize in the English language, and thus, AI\ndemocratization in non-English-speaking communities is lagging significantly.\nTo reduce this gap in AI access, we released Generative Pre-trained Transformer\n(GPT), Contrastive Language and Image Pre-training (CLIP), Stable Diffusion,\nand Hidden-unit Bidirectional Encoder Representations from Transformers\n(HuBERT) pre-trained in Japanese. By providing these models, users can freely\ninterface with AI that aligns with Japanese cultural values and ensures the\nidentity of Japanese culture, thus enhancing the democratization of AI.\nAdditionally, experiments showed that pre-trained models specialized for\nJapanese can efficiently achieve high performance in Japanese tasks.",
      "tldr_zh": "该研究旨在推动AI民主化，解决现有预训练模型偏向英语的问题，通过发布针对日语的Generative Pre-trained Transformer (GPT)、Contrastive Language and Image Pre-training (CLIP)、Stable Diffusion和Hidden-unit Bidirectional Encoder Representations from Transformers (HuBERT)模型。這些模型基于大规模日语数据训练，能更好地适应日语文化和任务需求，提升非英语社区的AI访问。实验结果表明，这些日语专用模型在日语任务中实现了高效的高性能表现，并有助于维护日本文化的身份认同。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, 1 figure, 5 tables, accepted for LREC-COLING 2024. Models\n  are publicly available at https://huggingface.co/rinna",
      "pdf_url": "http://arxiv.org/pdf/2404.01657v1",
      "published_date": "2024-04-02 05:59:43 UTC",
      "updated_date": "2024-04-02 05:59:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:11:18.557192"
    },
    {
      "arxiv_id": "2404.01654v1",
      "title": "AI WALKUP: A Computer-Vision Approach to Quantifying MDS-UPDRS in Parkinson's Disease",
      "title_zh": "AI WALKUP：一种计算机视觉方法用于量化帕金森病中的 MDS-UPDRS",
      "authors": [
        "Xiang Xiang",
        "Zihan Zhang",
        "Jing Ma",
        "Yao Deng"
      ],
      "abstract": "Parkinson's Disease (PD) is the second most common neurodegenerative\ndisorder. The existing assessment method for PD is usually the Movement\nDisorder Society - Unified Parkinson's Disease Rating Scale (MDS-UPDRS) to\nassess the severity of various types of motor symptoms and disease progression.\nHowever, manual assessment suffers from high subjectivity, lack of consistency,\nand high cost and low efficiency of manual communication. We want to use a\ncomputer vision based solution to capture human pose images based on a camera,\nreconstruct and perform motion analysis using algorithms, and extract the\nfeatures of the amount of motion through feature engineering. The proposed\napproach can be deployed on different smartphones, and the video recording and\nartificial intelligence analysis can be done quickly and easily through our\nAPP.",
      "tldr_zh": "这篇论文介绍了 AI WALKUP，一种基于 Computer-Vision 的方法，用于量化帕金森病（Parkinson's Disease）的 MDS-UPDRS 评分，以评估运动症状的严重程度和疾病进展。方法通过相机捕获人体姿势图像，进行重建、运动分析和特征工程，提取运动量特征，从而克服手动评估的主观性、一致性问题以及高成本低效率的缺点。该系统可部署在不同智能手机上，通过 APP 实现快速视频录制和 AI 分析，提供更可靠的临床评估工具。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV",
        "eess.SP"
      ],
      "primary_category": "cs.CV",
      "comment": "Technical report for AI WALKUP, an APP winning 3rd Prize of 2022 HUST\n  GS AI Innovation and Design Competition",
      "pdf_url": "http://arxiv.org/pdf/2404.01654v1",
      "published_date": "2024-04-02 05:53:34 UTC",
      "updated_date": "2024-04-02 05:53:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:11:30.893032"
    },
    {
      "arxiv_id": "2404.01652v1",
      "title": "Towards Better Generalization in Open-Domain Question Answering by Mitigating Context Memorization",
      "title_zh": "翻译失败",
      "authors": [
        "Zixuan Zhang",
        "Revanth Gangi Reddy",
        "Kevin Small",
        "Tong Zhang",
        "Heng Ji"
      ],
      "abstract": "Open-domain Question Answering (OpenQA) aims at answering factual questions\nwith an external large-scale knowledge corpus. However, real-world knowledge is\nnot static; it updates and evolves continually. Such a dynamic characteristic\nof knowledge poses a vital challenge for these models, as the trained models\nneed to constantly adapt to the latest information to make sure that the\nanswers remain accurate. In addition, it is still unclear how well an OpenQA\nmodel can transfer to completely new knowledge domains. In this paper, we\ninvestigate the generalization performance of a retrieval-augmented QA model in\ntwo specific scenarios: 1) adapting to updated versions of the same knowledge\ncorpus; 2) switching to completely different knowledge domains. We observe that\nthe generalization challenges of OpenQA models stem from the reader's\nover-reliance on memorizing the knowledge from the external corpus, which\nhinders the model from generalizing to a new knowledge corpus. We introduce\nCorpus-Invariant Tuning (CIT), a simple but effective training strategy, to\nmitigate the knowledge over-memorization by controlling the likelihood of\nretrieved contexts during training. Extensive experimental results on multiple\nOpenQA benchmarks show that CIT achieves significantly better generalizability\nwithout compromising the model's performance in its original corpus and domain.",
      "tldr_zh": "本研究探讨了开放域问答（OpenQA）模型的泛化性能问题，特别是在知识库更新和新领域切换时的挑战，原因是模型过度依赖记忆外部知识上下文，导致适应性差。为解决这一问题，研究提出了一种简单有效的训练策略——Corpus-Invariant Tuning (CIT)，通过控制训练中检索上下文的似然来减轻知识过度记忆。实验结果显示，在多个OpenQA基准上，CIT显著提升了模型的泛化能力，同时不影响其在原知识库中的表现。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to NAACL 2024 Findings",
      "pdf_url": "http://arxiv.org/pdf/2404.01652v1",
      "published_date": "2024-04-02 05:44:50 UTC",
      "updated_date": "2024-04-02 05:44:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:11:42.049040"
    },
    {
      "arxiv_id": "2404.01636v1",
      "title": "Learning to Control Camera Exposure via Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Kyunghyun Lee",
        "Ukcheol Shin",
        "Byeong-Uk Lee"
      ],
      "abstract": "Adjusting camera exposure in arbitrary lighting conditions is the first step\nto ensure the functionality of computer vision applications. Poorly adjusted\ncamera exposure often leads to critical failure and performance degradation.\nTraditional camera exposure control methods require multiple convergence steps\nand time-consuming processes, making them unsuitable for dynamic lighting\nconditions. In this paper, we propose a new camera exposure control framework\nthat rapidly controls camera exposure while performing real-time processing by\nexploiting deep reinforcement learning. The proposed framework consists of four\ncontributions: 1) a simplified training ground to simulate real-world's diverse\nand dynamic lighting changes, 2) flickering and image attribute-aware reward\ndesign, along with lightweight state design for real-time processing, 3) a\nstatic-to-dynamic lighting curriculum to gradually improve the agent's\nexposure-adjusting capability, and 4) domain randomization techniques to\nalleviate the limitation of the training ground and achieve seamless\ngeneralization in the wild.As a result, our proposed method rapidly reaches a\ndesired exposure level within five steps with real-time processing (1 ms).\nAlso, the acquired images are well-exposed and show superiority in various\ncomputer vision tasks, such as feature extraction and object detection.",
      "tldr_zh": "本论文提出了一种基于 Deep Reinforcement Learning 的相机曝光控制框架，以快速适应动态光照条件，避免传统方法的多步收敛和耗时问题。该框架的主要贡献包括：1) 构建一个简化的训练环境模拟真实世界的多样光照变化，2) 设计闪烁和图像属性感知的奖励函数以及轻量级状态以支持实时处理，3) 采用静态到动态光照的课程学习提升代理能力，以及4) 运用 Domain Randomization 技术实现野外泛化。实验结果显示，该方法可在五步内（1 ms 实时处理）达到理想曝光水平，所获图像曝光良好，并在特征提取和物体检测等计算机视觉任务中表现出色。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at CVPR 2024, *First two authors contributed equally to this\n  work. Project page link: https://sites.google.com/view/drl-ae",
      "pdf_url": "http://arxiv.org/pdf/2404.01636v1",
      "published_date": "2024-04-02 04:53:39 UTC",
      "updated_date": "2024-04-02 04:53:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:11:53.811885"
    },
    {
      "arxiv_id": "2404.08567v1",
      "title": "CATP: Cross-Attention Token Pruning for Accuracy Preserved Multimodal Model Inference",
      "title_zh": "翻译失败",
      "authors": [
        "Ruqi Liao",
        "Chuqing Zhao",
        "Jin Li",
        "Weiqi Feng"
      ],
      "abstract": "In response to the rising interest in large multimodal models, we introduce\nCross-Attention Token Pruning (CATP), a precision-focused token pruning method.\nOur approach leverages cross-attention layers in multimodal models, exemplified\nby BLIP-2, to extract valuable information for token importance determination.\nCATP employs a refined voting strategy across model heads and layers. In\nevaluations, CATP achieves up to 12.1X higher accuracy compared to existing\ntoken pruning methods, addressing the trade-off between computational\nefficiency and model precision.",
      "tldr_zh": "本研究提出了一种名为 Cross-Attention Token Pruning (CATP) 的令牌修剪方法，旨在在大型多模态模型中实现高效推理，同时保持高精确性。CATP 通过利用模型中的跨注意力层（如在 BLIP-2 中）提取令牌重要性信息，并采用改进的投票策略跨模型头和层进行决策，从而有效平衡计算效率和准确率。在评估中，CATP 比现有令牌修剪方法准确率高出高达 12.1 倍，为多模态模型推理提供了更可靠的优化方案。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.08567v1",
      "published_date": "2024-04-02 04:35:35 UTC",
      "updated_date": "2024-04-02 04:35:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:12:06.229793"
    },
    {
      "arxiv_id": "2404.01628v1",
      "title": "Learning Equi-angular Representations for Online Continual Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Minhyuk Seo",
        "Hyunseo Koh",
        "Wonje Jeung",
        "Minjae Lee",
        "San Kim",
        "Hankook Lee",
        "Sungjun Cho",
        "Sungik Choi",
        "Hyunwoo Kim",
        "Jonghyun Choi"
      ],
      "abstract": "Online continual learning suffers from an underfitted solution due to\ninsufficient training for prompt model update (e.g., single-epoch training). To\naddress the challenge, we propose an efficient online continual learning method\nusing the neural collapse phenomenon. In particular, we induce neural collapse\nto form a simplex equiangular tight frame (ETF) structure in the representation\nspace so that the continuously learned model with a single epoch can better fit\nto the streamed data by proposing preparatory data training and residual\ncorrection in the representation space. With an extensive set of empirical\nvalidations using CIFAR-10/100, TinyImageNet, ImageNet-200, and ImageNet-1K, we\nshow that our proposed method outperforms state-of-the-art methods by a\nnoticeable margin in various online continual learning scenarios such as\ndisjoint and Gaussian scheduled continuous (i.e., boundary-free) data setups.",
      "tldr_zh": "本文针对在线持续学习中的欠拟合问题（如单轮训练导致模型更新不足），提出了一种高效方法，利用神经崩溃现象（neural collapse）来诱导表示空间形成 simplex equiangular tight frame (ETF) 结构。方法包括预备数据训练（preparatory data training）和表示空间的残差校正（residual correction），以使模型更好地适应流式数据。在 CIFAR-10/100、TinyImageNet 和 ImageNet-200/1K 等数据集的实验中，该方法在不相交数据和高斯调度连续场景下，比最先进方法有显著性能提升。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.01628v1",
      "published_date": "2024-04-02 04:29:01 UTC",
      "updated_date": "2024-04-02 04:29:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:12:18.200757"
    },
    {
      "arxiv_id": "2404.01622v2",
      "title": "Gen4DS: Workshop on Data Storytelling in an Era of Generative AI",
      "title_zh": "翻译失败",
      "authors": [
        "Xingyu Lan",
        "Leni Yang",
        "Zezhong Wang",
        "Yun Wang",
        "Danqing Shi",
        "Sheelagh Carpendale"
      ],
      "abstract": "Storytelling is an ancient and precious human ability that has been\nrejuvenated in the digital age. Over the last decade, there has been a notable\nsurge in the recognition and application of data storytelling, both in academia\nand industry. Recently, the rapid development of generative AI has brought new\nopportunities and challenges to this field, sparking numerous new questions.\nThese questions may not necessarily be quickly transformed into papers, but we\nbelieve it is necessary to promptly discuss them to help the community better\nclarify important issues and research agendas for the future. We thus invite\nyou to join our workshop (Gen4DS) to discuss questions such as: How can\ngenerative AI facilitate the creation of data stories? How might generative AI\nalter the workflow of data storytellers? What are the pitfalls and risks of\nincorporating AI in storytelling? We have designed both paper presentations and\ninteractive activities (including hands-on creation, group discussion pods, and\ndebates on controversial issues) for the workshop. We hope that participants\nwill learn about the latest advances and pioneering work in data storytelling,\nengage in critical conversations with each other, and have an enjoyable,\nunforgettable, and meaningful experience at the event.",
      "tldr_zh": "本研讨会（Gen4DS）聚焦于数据 storytelling 在 generative AI 时代的新机遇和挑战，旨在探讨 generative AI 如何促进数据故事的创建、改变数据叙事者的工作流程，以及潜在的陷阱和风险。组织者邀请参与者通过论文展示和互动活动（如动手创作、组讨论和辩论）来共同讨论这些问题，并帮助社区澄清重要议题和未来研究议程。该活动预计将促进参与者了解最新进展，进行批判性对话，并获得有意义的学习体验。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.01622v2",
      "published_date": "2024-04-02 04:11:37 UTC",
      "updated_date": "2024-04-06 02:12:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:12:31.924222"
    },
    {
      "arxiv_id": "2404.01620v3",
      "title": "Voice EHR: Introducing Multimodal Audio Data for Health",
      "title_zh": "Voice EHR：引入多模态音频数据用于健康",
      "authors": [
        "James Anibal",
        "Hannah Huth",
        "Ming Li",
        "Lindsey Hazen",
        "Veronica Daoud",
        "Dominique Ebedes",
        "Yen Minh Lam",
        "Hang Nguyen",
        "Phuc Hong",
        "Michael Kleinman",
        "Shelley Ost",
        "Christopher Jackson",
        "Laura Sprabery",
        "Cheran Elangovan",
        "Balaji Krishnaiah",
        "Lee Akst",
        "Ioan Lina",
        "Iqbal Elyazar",
        "Lenny Ekwati",
        "Stefan Jansen",
        "Richard Nduwayezu",
        "Charisse Garcia",
        "Jeffrey Plum",
        "Jacqueline Brenner",
        "Miranda Song",
        "Emily Ricotta",
        "David Clifton",
        "C. Louise Thwaites",
        "Yael Bensoussan",
        "Bradford Wood"
      ],
      "abstract": "Artificial intelligence (AI) models trained on audio data may have the\npotential to rapidly perform clinical tasks, enhancing medical decision-making\nand potentially improving outcomes through early detection. Existing\ntechnologies depend on limited datasets collected with expensive recording\nequipment in high-income countries, which challenges deployment in\nresource-constrained, high-volume settings where audio data may have a profound\nimpact on health equity. This report introduces a novel data type and a\ncorresponding collection system that captures health data through guided\nquestions using only a mobile/web application. The app facilitates the\ncollection of an audio electronic health record (Voice EHR) which may contain\ncomplex biomarkers of health from conventional voice/respiratory features,\nspeech patterns, and spoken language with semantic meaning and longitudinal\ncontext, potentially compensating for the typical limitations of unimodal\nclinical datasets. This report presents the application used for data\ncollection, initial experiments on data quality, and case studies which\ndemonstrate the potential of voice EHR to advance the scalability/diversity of\naudio AI.",
      "tldr_zh": "该论文介绍了 Voice EHR，一种新型的多模态音频数据类型，旨在通过移动/网络应用使用引导问题收集健康音频数据，以解决现有 AI 模型依赖昂贵设备和有限数据集的问题，从而提升医疗决策和健康公平。Voice EHR 捕捉语音/呼吸特征、语音模式以及语义含义的复杂生物markers，提供纵向上下文，并补偿单模态临床数据集的局限。初步实验和案例研究证明，该系统提高了音频 AI 的可扩展性和多样性，为早期检测和资源有限环境下的部署提供了潜力。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CY",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "21 pages, 5 figures, 6 tables",
      "pdf_url": "http://arxiv.org/pdf/2404.01620v3",
      "published_date": "2024-04-02 04:07:22 UTC",
      "updated_date": "2024-11-09 17:22:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:12:43.017119"
    },
    {
      "arxiv_id": "2404.02179v1",
      "title": "Distributed and Rate-Adaptive Feature Compression",
      "title_zh": "翻译失败",
      "authors": [
        "Aditya Deshmukh",
        "Venugopal V. Veeravalli",
        "Gunjan Verma"
      ],
      "abstract": "We study the problem of distributed and rate-adaptive feature compression for\nlinear regression. A set of distributed sensors collect disjoint features of\nregressor data. A fusion center is assumed to contain a pretrained linear\nregression model, trained on a dataset of the entire uncompressed data. At\ninference time, the sensors compress their observations and send them to the\nfusion center through communication-constrained channels, whose rates can\nchange with time. Our goal is to design a feature compression {scheme} that can\nadapt to the varying communication constraints, while maximizing the inference\nperformance at the fusion center. We first obtain the form of optimal\nquantizers assuming knowledge of underlying regressor data distribution. Under\na practically reasonable approximation, we then propose a distributed\ncompression scheme which works by quantizing a one-dimensional projection of\nthe sensor data. We also propose a simple adaptive scheme for handling changes\nin communication constraints. We demonstrate the effectiveness of the\ndistributed adaptive compression scheme through simulated experiments.",
      "tldr_zh": "这篇论文研究了分布式和速率自适应的特征压缩问题，针对线性回归任务，其中分布式传感器收集回归数据的分离特征，并通过受限通信通道发送给融合中心。作者首先推导出最优量化器的形式，并提出一种基于量化传感器数据单维投影的分布式压缩方案，以适应通信速率的变化。实验结果显示，该自适应方案在模拟环境中显著提高了融合中心的推理性能。",
      "categories": [
        "cs.IT",
        "cs.AI",
        "math.IT",
        "stat.ML"
      ],
      "primary_category": "cs.IT",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.02179v1",
      "published_date": "2024-04-02 03:21:06 UTC",
      "updated_date": "2024-04-02 03:21:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:12:55.739098"
    },
    {
      "arxiv_id": "2404.01602v2",
      "title": "Helmsman of the Masses? Evaluate the Opinion Leadership of Large Language Models in the Werewolf Game",
      "title_zh": "翻译失败",
      "authors": [
        "Silin Du",
        "Xiaowei Zhang"
      ],
      "abstract": "Large language models (LLMs) have exhibited memorable strategic behaviors in\nsocial deductive games. However, the significance of opinion leadership\nexhibited by LLM-based agents has been largely overlooked, which is crucial for\npractical applications in multi-agent and human-AI interaction settings.\nOpinion leaders are individuals who have a noticeable impact on the beliefs and\nbehaviors of others within a social group. In this work, we employ the Werewolf\ngame as a simulation platform to assess the opinion leadership of LLMs. The\ngame includes the role of the Sheriff, tasked with summarizing arguments and\nrecommending decision options, and therefore serves as a credible proxy for an\nopinion leader. We develop a framework integrating the Sheriff role and devise\ntwo novel metrics based on the critical characteristics of opinion leaders. The\nfirst metric measures the reliability of the opinion leader, and the second\nassesses the influence of the opinion leader on other players' decisions. We\nconduct extensive experiments to evaluate LLMs of different scales. In\naddition, we collect a Werewolf question-answering dataset (WWQA) to assess and\nenhance LLM's grasp of the game rules, and we also incorporate human\nparticipants for further analysis. The results suggest that the Werewolf game\nis a suitable test bed to evaluate the opinion leadership of LLMs, and few LLMs\npossess the capacity for opinion leadership.",
      "tldr_zh": "本研究评估大型语言模型 (LLMs) 在 Werewolf 游戏中的意见领导力，强调其在多智能体和人机交互中的实际意义。研究者使用 Werewolf 游戏作为模拟平台，将游戏中的 Sheriff 角色作为意见领导者的代理，并开发了一个框架及两个新指标：一个衡量意见领导者的可靠性，另一个评估其对其他玩家的影响。实验涉及不同规模的 LLMs、Werewolf 问答数据集 (WWQA) 的收集和人类参与，以增强 LLMs 对游戏规则的理解。结果显示，Werewolf 游戏是评估 LLMs 意见领导力的合适测试平台，但仅有少数 LLMs 具备这种能力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "Published as a conference paper at COLM 2024. 37 pages, 6 figures, 27\n  tables",
      "pdf_url": "http://arxiv.org/pdf/2404.01602v2",
      "published_date": "2024-04-02 02:46:18 UTC",
      "updated_date": "2024-08-29 08:49:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:13:07.507932"
    },
    {
      "arxiv_id": "2404.01598v1",
      "title": "Extremum-Seeking Action Selection for Accelerating Policy Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Ya-Chien Chang",
        "Sicun Gao"
      ],
      "abstract": "Reinforcement learning for control over continuous spaces typically uses\nhigh-entropy stochastic policies, such as Gaussian distributions, for local\nexploration and estimating policy gradient to optimize performance. Many\nrobotic control problems deal with complex unstable dynamics, where applying\nactions that are off the feasible control manifolds can quickly lead to\nundesirable divergence. In such cases, most samples taken from the ambient\naction space generate low-value trajectories that hardly contribute to policy\nimprovement, resulting in slow or failed learning. We propose to improve action\nselection in this model-free RL setting by introducing additional adaptive\ncontrol steps based on Extremum-Seeking Control (ESC). On each action sampled\nfrom stochastic policies, we apply sinusoidal perturbations and query for\nestimated Q-values as the response signal. Based on ESC, we then dynamically\nimprove the sampled actions to be closer to nearby optima before applying them\nto the environment. Our methods can be easily added in standard policy\noptimization to improve learning efficiency, which we demonstrate in various\ncontrol learning environments.",
      "tldr_zh": "本文提出了一种基于Extremum-Seeking Control (ESC)的动作选择方法，用于加速强化学习(Reinforcement Learning, RL)中的策略优化，尤其针对复杂不稳定动态的机器人控制问题。方法通过在从随机策略（如高斯分布）采样的动作上施加正弦扰动，并利用估计的Q-values作为响应信号，动态调整动作使其更接近附近的优化点，从而减少低价值轨迹的产生。实验结果表明，该方法易于集成到标准策略优化框架中，并在各种控制学习环境中显著提高了学习效率。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.01598v1",
      "published_date": "2024-04-02 02:39:17 UTC",
      "updated_date": "2024-04-02 02:39:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:13:19.878728"
    },
    {
      "arxiv_id": "2404.01596v3",
      "title": "PhysORD: A Neuro-Symbolic Approach for Physics-infused Motion Prediction in Off-road Driving",
      "title_zh": "翻译失败",
      "authors": [
        "Zhipeng Zhao",
        "Bowen Li",
        "Yi Du",
        "Taimeng Fu",
        "Chen Wang"
      ],
      "abstract": "Motion prediction is critical for autonomous off-road driving, however, it\npresents significantly more challenges than on-road driving because of the\ncomplex interaction between the vehicle and the terrain. Traditional\nphysics-based approaches encounter difficulties in accurately modeling dynamic\nsystems and external disturbance. In contrast, data-driven neural networks\nrequire extensive datasets and struggle with explicitly capturing the\nfundamental physical laws, which can easily lead to poor generalization. By\nmerging the advantages of both methods, neuro-symbolic approaches present a\npromising direction. These methods embed physical laws into neural models,\npotentially significantly improving generalization capabilities. However, no\nprior works were evaluated in real-world settings for off-road driving. To\nbridge this gap, we present PhysORD, a neural-symbolic approach integrating the\nconservation law, i.e., the Euler-Lagrange equation, into data-driven neural\nmodels for motion prediction in off-road driving. Our experiments showed that\nPhysORD can accurately predict vehicle motion and tolerate external disturbance\nby modeling uncertainties. The learned dynamics model achieves 46.7% higher\naccuracy using only 3.1% of the parameters compared to data-driven methods,\ndemonstrating the data efficiency and superior generalization ability of our\nneural-symbolic method.",
      "tldr_zh": "本研究提出 PhysORD，一种 Neuro-Symbolic Approach，将物理定律（如 Euler-Lagrange equation）融入数据驱动神经模型，用于解决自主越野驾驶中的运动预测挑战，该方法能更好地处理车辆与地形的复杂互动和外部干扰。PhysORD 通过整合守恒定律，显著提高了模型的泛化能力，同时减少了对大规模数据集的依赖。实验结果显示，与传统数据驱动方法相比，PhysORD 的动态模型准确率提升 46.7%，且仅使用 3.1% 的参数，展示了其数据效率和鲁棒性优势。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.01596v3",
      "published_date": "2024-04-02 02:36:31 UTC",
      "updated_date": "2024-10-22 15:47:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:13:32.703970"
    },
    {
      "arxiv_id": "2404.01589v1",
      "title": "Classifying Cancer Stage with Open-Source Clinical Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Chia-Hsuan Chang",
        "Mary M. Lucas",
        "Grace Lu-Yao",
        "Christopher C. Yang"
      ],
      "abstract": "Cancer stage classification is important for making treatment and care\nmanagement plans for oncology patients. Information on staging is often\nincluded in unstructured form in clinical, pathology, radiology and other\nfree-text reports in the electronic health record system, requiring extensive\nwork to parse and obtain. To facilitate the extraction of this information,\nprevious NLP approaches rely on labeled training datasets, which are\nlabor-intensive to prepare. In this study, we demonstrate that without any\nlabeled training data, open-source clinical large language models (LLMs) can\nextract pathologic tumor-node-metastasis (pTNM) staging information from\nreal-world pathology reports. Our experiments compare LLMs and a BERT-based\nmodel fine-tuned using the labeled data. Our findings suggest that while LLMs\nstill exhibit subpar performance in Tumor (T) classification, with the\nappropriate adoption of prompting strategies, they can achieve comparable\nperformance on Metastasis (M) classification and improved performance on Node\n(N) classification.",
      "tldr_zh": "本研究探讨了使用开源临床 Large Language Models (LLMs) 从非结构化电子健康记录（如病理报告）中提取癌症的病理肿瘤-节点-转移 (pTNM) 分期信息，而无需任何标记训练数据，从而避免了传统 NLP 方法的劳动密集型准备过程。实验比较了 LLMs 与使用标记数据微调的 BERT 模型，结果显示 LLMs 在 Tumor (T) 分级上表现不佳，但通过适当的提示策略，在 Node (N) 分级上取得更好性能，在 Metastasis (M) 分级上达到可比水平。该方法为简化癌症分期分类提供了高效、可扩展的解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "accepted in the IEEE International Conference on Healthcare\n  Informatics (IEEE ICHI 2024)",
      "pdf_url": "http://arxiv.org/pdf/2404.01589v1",
      "published_date": "2024-04-02 02:30:47 UTC",
      "updated_date": "2024-04-02 02:30:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:13:44.989941"
    },
    {
      "arxiv_id": "2404.01588v1",
      "title": "Hallucination Diversity-Aware Active Learning for Text Summarization",
      "title_zh": "翻译失败",
      "authors": [
        "Yu Xia",
        "Xu Liu",
        "Tong Yu",
        "Sungchul Kim",
        "Ryan A. Rossi",
        "Anup Rao",
        "Tung Mai",
        "Shuai Li"
      ],
      "abstract": "Large Language Models (LLMs) have shown propensity to generate hallucinated\noutputs, i.e., texts that are factually incorrect or unsupported. Existing\nmethods for alleviating hallucinations typically require costly human\nannotations to identify and correct hallucinations in LLM outputs. Moreover,\nmost of these methods focus on a specific type of hallucination, e.g., entity\nor token errors, which limits their effectiveness in addressing various types\nof hallucinations exhibited in LLM outputs. To our best knowledge, in this\npaper we propose the first active learning framework to alleviate LLM\nhallucinations, reducing costly human annotations of hallucination needed. By\nmeasuring fine-grained hallucinations from errors in semantic frame, discourse\nand content verifiability in text summarization, we propose HAllucination\nDiversity-Aware Sampling (HADAS) to select diverse hallucinations for\nannotations in active learning for LLM finetuning. Extensive experiments on\nthree datasets and different backbone models demonstrate advantages of our\nmethod in effectively and efficiently mitigating LLM hallucinations.",
      "tldr_zh": "本研究针对大语言模型 (LLMs) 生成的幻觉输出（如事实错误或不支持内容）问题，提出首个 active learning 框架，以减少昂贵的标注需求。框架中引入 HAllucination Diversity-Aware Sampling (HADAS) 方法，通过测量文本摘要中的细粒度幻觉（如语义框架、话语和内容可验证性错误），选择多样化的幻觉样本进行标注，从而优化 LLM 微调过程。在三个数据集和不同骨干模型上的实验显示，该方法显著提高了减轻幻觉的效率和效果。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to NAACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.01588v1",
      "published_date": "2024-04-02 02:30:27 UTC",
      "updated_date": "2024-04-02 02:30:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:13:55.557894"
    },
    {
      "arxiv_id": "2404.01569v2",
      "title": "Evaluating Large Language Models Using Contrast Sets: An Experimental Approach",
      "title_zh": "使用对比集评估大语言模型：一种实验方法",
      "authors": [
        "Manish Sanwal"
      ],
      "abstract": "In the domain of Natural Language Inference (NLI), especially in tasks\ninvolving the classification of multiple input texts, the Cross-Entropy Loss\nmetric is widely employed as a standard for error measurement. However, this\nmetric falls short in effectively evaluating a model's capacity to understand\nlanguage entailments. In this study, we introduce an innovative technique for\ngenerating a contrast set for the Stanford Natural Language Inference (SNLI)\ndataset. Our strategy involves the automated substitution of verbs, adverbs,\nand adjectives with their synonyms to preserve the original meaning of\nsentences. This method aims to assess whether a model's performance is based on\ngenuine language comprehension or simply on pattern recognition. We conducted\nour analysis using the ELECTRA-small model. The model achieved an accuracy of\n89.9% on the conventional SNLI dataset but showed a reduced accuracy of 72.5%\non our contrast set, indicating a substantial 17% decline. This outcome led us\nto conduct a detailed examination of the model's learning behaviors. Following\nthis, we improved the model's resilience by fine-tuning it with a\ncontrast-enhanced training dataset specifically designed for SNLI, which\nincreased its accuracy to 85.5% on the contrast sets. Our findings highlight\nthe importance of incorporating diverse linguistic expressions into datasets\nfor NLI tasks. We hope that our research will encourage the creation of more\ninclusive datasets, thereby contributing to the development of NLI models that\nare both more sophisticated and effective.",
      "tldr_zh": "本研究评估了大型语言模型在 Natural Language Inference (NLI) 任务中的性能，使用对比集 (contrast sets) 作为创新实验方法，针对 Stanford Natural Language Inference (SNLI) 数据集，通过自动替换动词、副词和形容词的同义词来生成新样本，以测试模型是否依赖真实语言理解而非模式识别。实验使用 ELECTRA-small 模型，结果显示该模型在原 SNLI 数据集上准确率达 89.9%，但在对比集上降至 72.5%，下降 17%，揭示了模型的局限性。作者随后通过微调模型，使用增强对比训练数据集，将准确率提高到 85.5%。总体而言，该工作强调了在 NLI 数据集中融入多样化语言表达的重要性，并推动了更鲁棒模型的开发。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.01569v2",
      "published_date": "2024-04-02 02:03:28 UTC",
      "updated_date": "2024-10-02 12:31:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:14:09.448846"
    },
    {
      "arxiv_id": "2404.01558v1",
      "title": "Automated User Story Generation with Test Case Specification Using Large Language Model",
      "title_zh": "翻译失败",
      "authors": [
        "Tajmilur Rahman",
        "Yuecai Zhu"
      ],
      "abstract": "Modern Software Engineering era is moving fast with the assistance of\nartificial intelligence (AI), especially Large Language Models (LLM).\nResearchers have already started automating many parts of the software\ndevelopment workflow. Requirements Engineering (RE) is a crucial phase that\nbegins the software development cycle through multiple discussions on a\nproposed scope of work documented in different forms. RE phase ends with a list\nof user-stories for each unit task identified through discussions and usually\nthese are created and tracked on a project management tool such as Jira,\nAzurDev etc. In this research we developed a tool \"GeneUS\" using GPT-4.0 to\nautomatically create user stories from requirements document which is the\noutcome of the RE phase. The output is provided in JSON format leaving the\npossibilities open for downstream integration to the popular project management\ntools. Analyzing requirements documents takes significant effort and multiple\nmeetings with stakeholders. We believe, automating this process will certainly\nreduce additional load off the software engineers, and increase the\nproductivity since they will be able to utilize their time on other prioritized\ntasks.",
      "tldr_zh": "这篇论文探讨了使用 Large Language Model (LLM) 自动化软件工程中的用户故事生成和测试用例指定问题。研究者开发了名为 \"GeneUS\" 的工具，基于 GPT-4.0，从 Requirements Engineering (RE) 阶段的需求文档自动提取并生成用户故事，并以 JSON 格式输出，便于集成到项目管理工具如 Jira 或 Azure DevOps。实验结果表明，该方法能显著减少分析需求文档的努力和会议次数，提高软件工程师的生产力。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "10 pages including 2 pages of Appendix",
      "pdf_url": "http://arxiv.org/pdf/2404.01558v1",
      "published_date": "2024-04-02 01:45:57 UTC",
      "updated_date": "2024-04-02 01:45:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:14:19.425076"
    },
    {
      "arxiv_id": "2404.01557v1",
      "title": "Distributed Autonomous Swarm Formation for Dynamic Network Bridging",
      "title_zh": "翻译失败",
      "authors": [
        "Raffaele Galliera",
        "Thies Möhlenhof",
        "Alessandro Amato",
        "Daniel Duran",
        "Kristen Brent Venable",
        "Niranjan Suri"
      ],
      "abstract": "Effective operation and seamless cooperation of robotic systems are a\nfundamental component of next-generation technologies and applications. In\ncontexts such as disaster response, swarm operations require coordinated\nbehavior and mobility control to be handled in a distributed manner, with the\nquality of the agents' actions heavily relying on the communication between\nthem and the underlying network. In this paper, we formulate the problem of\ndynamic network bridging in a novel Decentralized Partially Observable Markov\nDecision Process (Dec-POMDP), where a swarm of agents cooperates to form a link\nbetween two distant moving targets. Furthermore, we propose a Multi-Agent\nReinforcement Learning (MARL) approach for the problem based on Graph\nConvolutional Reinforcement Learning (DGN) which naturally applies to the\nnetworked, distributed nature of the task. The proposed method is evaluated in\na simulated environment and compared to a centralized heuristic baseline\nshowing promising results. Moreover, a further step in the direction of\nsim-to-real transfer is presented, by additionally evaluating the proposed\napproach in a near Live Virtual Constructive (LVC) UAV framework.",
      "tldr_zh": "该论文探讨了机器人群在动态网络桥接中的分布式自治形成问题，特别是在灾难响应等场景下，需要代理间协调行为和移动控制。该方法将问题建模为Decentralized Partially Observable Markov Decision Process (Dec-POMDP)，并提出基于Multi-Agent Reinforcement Learning (MARL)和Graph Convolutional Reinforcement Learning (DGN)的多代理强化学习方法，以实现代理间的分布式合作。实验结果显示，该方法在模拟环境中优于集中式启发式基线，并在近Live Virtual Constructive (LVC) UAV框架中进行了模拟到真实转移评估，展示了其在实际应用中的潜力。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.MA",
      "comment": "6 pages, 3 figures, 1 table, 1 algorithm",
      "pdf_url": "http://arxiv.org/pdf/2404.01557v1",
      "published_date": "2024-04-02 01:45:03 UTC",
      "updated_date": "2024-04-02 01:45:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:14:31.169714"
    },
    {
      "arxiv_id": "2405.14876v2",
      "title": "Precise and Robust Sidewalk Detection: Leveraging Ensemble Learning to Surpass LLM Limitations in Urban Environments",
      "title_zh": "翻译失败",
      "authors": [
        "Ibne Farabi Shihab",
        "Sudesh Ramesh Bhagat",
        "Anuj Sharma"
      ],
      "abstract": "This study aims to compare the effectiveness of a robust ensemble model with\nthe state-of-the-art ONE-PEACE Large Language Model (LLM) for accurate\ndetection of sidewalks. Accurate sidewalk detection is crucial in improving\nroad safety and urban planning. The study evaluated the model's performance on\nCityscapes, Ade20k, and the Boston Dataset. The results showed that the\nensemble model performed better than the individual models, achieving mean\nIntersection Over Union (mIOU) scores of 93.1\\%, 90.3\\%, and 90.6\\% on these\ndatasets under ideal conditions. Additionally, the ensemble model maintained a\nconsistent level of performance even in challenging conditions such as\nSalt-and-Pepper and Speckle noise, with only a gradual decrease in efficiency\nobserved. On the other hand, the ONE-PEACE LLM performed slightly better than\nthe ensemble model in ideal scenarios but experienced a significant decline in\nperformance under noisy conditions. These findings demonstrate the robustness\nand reliability of the ensemble model, making it a valuable asset for improving\nurban infrastructure related to road safety and curb space management. This\nstudy contributes positively to the broader context of urban health and\nmobility.",
      "tldr_zh": "本研究比较了集成模型（ensemble model）和 ONE-PEACE LLM 在人行道检测方面的性能，旨在解决城市环境中的检测挑战，以提升道路安全和城市规划。在 Cityscapes、Ade20k 和 Boston Dataset 上，集成模型在理想条件下取得了 mIOU 分数分别为 93.1%、90.3% 和 90.6%，并在 Salt-and-Pepper 及 Speckle 噪声等挑战条件下保持了稳定的鲁棒性，而 ONE-PEACE LLM 虽在理想场景略占优势，但噪声环境下性能显著下降。这些发现突显了集成模型的可靠性和实际应用价值，为城市健康、移动性和基础设施管理提供了重要贡献。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.14876v2",
      "published_date": "2024-04-02 01:42:32 UTC",
      "updated_date": "2025-01-23 01:08:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:14:45.392976"
    },
    {
      "arxiv_id": "2404.01551v2",
      "title": "Safety-Aware Multi-Agent Learning for Dynamic Network Bridging",
      "title_zh": "针对动态网络桥接的安全感知多智能体学习",
      "authors": [
        "Raffaele Galliera",
        "Konstantinos Mitsopoulos",
        "Niranjan Suri",
        "Raffaele Romagnoli"
      ],
      "abstract": "Addressing complex cooperative tasks in safety-critical environments poses\nsignificant challenges for multi-agent systems, especially under conditions of\npartial observability. We focus on a dynamic network bridging task, where\nagents must learn to maintain a communication path between two moving targets.\nTo ensure safety during training and deployment, we integrate a\ncontrol-theoretic safety filter that enforces collision avoidance through local\nsetpoint updates. We develop and evaluate multi-agent reinforcement learning\nsafety-informed message passing, showing that encoding safety filter\nactivations as edge-level features improves coordination. The results suggest\nthat local safety enforcement and decentralized learning can be effectively\ncombined in distributed multi-agent tasks.",
      "tldr_zh": "该研究针对多智能体系统在安全关键环境中的复杂合作任务，特别是部分可观察性条件下，聚焦于动态网络桥接任务，要求代理维护两个移动目标之间的通信路径。研究引入了控制-theoretic safety filter，通过本地设定点更新强制碰撞避免，并开发了安全-informed multi-agent reinforcement learning消息传递，将safety filter激活编码为edge-level特征，以提升代理协调。实验结果显示，这种方法显著提高了任务性能，并证明了局部安全执行与decentralized learning在分布式多智能体任务中的有效结合。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.LG",
        "cs.NI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.MA",
      "comment": "8 pages, 18 equations, 4 figures, 1 algorithm, and 1 table",
      "pdf_url": "http://arxiv.org/pdf/2404.01551v2",
      "published_date": "2024-04-02 01:30:41 UTC",
      "updated_date": "2025-04-03 17:25:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:14:58.001036"
    },
    {
      "arxiv_id": "2404.01548v1",
      "title": "mChartQA: A universal benchmark for multimodal Chart Question Answer based on Vision-Language Alignment and Reasoning",
      "title_zh": "mChart",
      "authors": [
        "Jingxuan Wei",
        "Nan Xu",
        "Guiyong Chang",
        "Yin Luo",
        "BiHui Yu",
        "Ruifeng Guo"
      ],
      "abstract": "In the fields of computer vision and natural language processing, multimodal\nchart question-answering, especially involving color, structure, and textless\ncharts, poses significant challenges. Traditional methods, which typically\ninvolve either direct multimodal processing or a table-to-text conversion\nfollowed by language model analysis, have limitations in effectively handling\nthese complex scenarios. This paper introduces a novel multimodal chart\nquestion-answering model, specifically designed to address these intricate\ntasks. Our model integrates visual and linguistic processing, overcoming the\nconstraints of existing methods. We adopt a dual-phase training approach: the\ninitial phase focuses on aligning image and text representations, while the\nsubsequent phase concentrates on optimizing the model's interpretative and\nanalytical abilities in chart-related queries. This approach has demonstrated\nsuperior performance on multiple public datasets, particularly in handling\ncolor, structure, and textless chart questions, indicating its effectiveness in\ncomplex multimodal tasks.",
      "tldr_zh": "该研究引入了mChartQA，这是一个基于Vision-Language Alignment and Reasoning的通用基准，用于多模态图表问答，特别针对颜色、结构和无文本图表的复杂挑战。mChartQA模型整合视觉和语言处理，采用双阶段训练方法：第一阶段对齐图像和文本表示，第二阶段优化模型的解释和分析能力，以克服传统方法的局限，如直接多模态处理或表到文本转换。实验结果显示，该模型在多个公共数据集上表现出色，尤其在处理复杂图表问题时，证明了其在多模态任务中的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.01548v1",
      "published_date": "2024-04-02 01:28:44 UTC",
      "updated_date": "2024-04-02 01:28:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:15:09.411365"
    },
    {
      "arxiv_id": "2404.01536v2",
      "title": "Laying Anchors: Semantically Priming Numerals in Language Modeling",
      "title_zh": "翻译失败",
      "authors": [
        "Mandar Sharma",
        "Rutuja Murlidhar Taware",
        "Pravesh Koirala",
        "Nikhil Muralidhar",
        "Naren Ramakrishnan"
      ],
      "abstract": "Off-the-shelf pre-trained language models have become the de facto standard\nin NLP pipelines for a multitude of downstream tasks. However, the inability of\nthese models to properly encode numerals limits their performance on tasks\nrequiring numeric comprehension. We introduce strategies to semantically prime\nnumerals in any corpus by generating anchors governed by the distribution of\nnumerals in said corpus, thereby enabling mathematically grounded\nrepresentations of these numeral tokens. We establish the superiority of our\nproposed techniques through evaluation on a range of numeracy tasks for both\nin-domain (seen) and out-domain (unseen) numerals. Further, we expand our\nempirical evaluations to numerals ranging from 1 to 10 billion, a significantly\nbroader range compared to previous studies of the same nature, and we\ndemonstrate significant improvements in the mathematical grounding of our\nlearned embeddings.",
      "tldr_zh": "这篇论文解决了预训练语言模型在处理数字方面的不足，提出了一种语义 priming 数字的策略，通过在语料库中生成基于数字分布的 anchors（锚点），来实现数字标记的数学基础表示。作者通过评估多种数字任务，包括领域内（seen）和领域外（unseen）数字，证明了该方法的优越性。实验覆盖了从1到100亿的数字范围，比以往研究更广泛，并展示了显著提升了嵌入的数学 grounding（数学基础）。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to the findings of NAACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.01536v2",
      "published_date": "2024-04-02 00:02:00 UTC",
      "updated_date": "2024-08-07 22:46:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T21:15:21.164780"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 105,
  "processed_papers_count": 105,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-17T21:15:44.314843"
}
[
  {
    "arxiv_id": "2404.02361v2",
    "title": "EnergAIze: Multi Agent Deep Deterministic Policy Gradient for Vehicle to Grid Energy Management",
    "authors": [
      "Tiago Fonseca",
      "Luis Ferreira",
      "Bernardo Cabral",
      "Ricardo Severino",
      "Isabel Praca"
    ],
    "abstract": "This paper investigates the increasing roles of Renewable Energy Sources\n(RES) and Electric Vehicles (EVs). While indicating a new era of sustainable\nenergy, these also introduce complex challenges, including the need to balance\nsupply and demand and smooth peak consumptions amidst rising EV adoption rates.\nAddressing these challenges requires innovative solutions such as Demand\nResponse (DR), energy flexibility management, Renewable Energy Communities\n(RECs), and more specifically for EVs, Vehicle-to-Grid (V2G). However, existing\nV2G approaches often fall short in real-world adaptability, global REC\noptimization with other flexible assets, scalability, and user engagement. To\nbridge this gap, this paper introduces EnergAIze, a Multi-Agent Reinforcement\nLearning (MARL) energy management framework, leveraging the Multi-Agent Deep\nDeterministic Policy Gradient (MADDPG) algorithm. EnergAIze enables\nuser-centric and multi-objective energy management by allowing each prosumer to\nselect from a range of personal management objectives, thus encouraging\nengagement. Additionally, it architects' data protection and ownership through\ndecentralized computing, where each prosumer can situate an energy management\noptimization node directly at their own dwelling. The local node not only\nmanages local energy assets but also fosters REC wide optimization. The\nefficacy of EnergAIze was evaluated through case studies employing the\nCityLearn simulation framework. These simulations were instrumental in\ndemonstrating EnergAIze's adeptness at implementing V2G technology within a REC\nand other energy assets. The results show reduction in peak loads, ramping,\ncarbon emissions, and electricity costs at the REC level while optimizing for\nindividual prosumers objectives.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "6 pages, 6 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2404.02361v2",
    "published_date": "2024-04-02 23:16:17 UTC",
    "updated_date": "2024-04-09 16:32:22 UTC"
  },
  {
    "arxiv_id": "2404.02353v1",
    "title": "Semantic Augmentation in Images using Language",
    "authors": [
      "Sahiti Yerramilli",
      "Jayant Sravan Tamarapalli",
      "Tanmay Girish Kulkarni",
      "Jonathan Francis",
      "Eric Nyberg"
    ],
    "abstract": "Deep Learning models are incredibly data-hungry and require very large\nlabeled datasets for supervised learning. As a consequence, these models often\nsuffer from overfitting, limiting their ability to generalize to real-world\nexamples. Recent advancements in diffusion models have enabled the generation\nof photorealistic images based on textual inputs. Leveraging the substantial\ndatasets used to train these diffusion models, we propose a technique to\nutilize generated images to augment existing datasets. This paper explores\nvarious strategies for effective data augmentation to improve the out-of-domain\ngeneralization capabilities of deep learning models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02353v1",
    "published_date": "2024-04-02 22:54:24 UTC",
    "updated_date": "2024-04-02 22:54:24 UTC"
  },
  {
    "arxiv_id": "2404.02335v1",
    "title": "Multi-BERT: Leveraging Adapters and Prompt Tuning for Low-Resource Multi-Domain Adaptation",
    "authors": [
      "Parham Abed Azad",
      "Hamid Beigy"
    ],
    "abstract": "The rapid expansion of texts' volume and diversity presents formidable\nchallenges in multi-domain settings. These challenges are also visible in the\nPersian name entity recognition (NER) settings. Traditional approaches, either\nemploying a unified model for multiple domains or individual models for each\ndomain, frequently pose significant limitations. Single models often struggle\nto capture the nuances of diverse domains, while utilizing multiple large\nmodels can lead to resource constraints, rendering the training of a model for\neach domain virtually impractical. Therefore, this paper introduces a novel\napproach composed of one core model with multiple sets of domain-specific\nparameters. We utilize techniques such as prompt tuning and adapters, combined\nwith the incorporation of additional layers, to add parameters that we can\ntrain for the specific domains. This enables the model to perform comparably to\nindividual models for each domain. Experimental results on different formal and\ninformal datasets show that by employing these added parameters, the proposed\nmodel significantly surpasses existing practical models in performance.\nRemarkably, the proposed model requires only one instance for training and\nstorage, yet achieves outstanding results across all domains, even surpassing\nthe state-of-the-art in some. Moreover, we analyze each adaptation strategy,\ndelineating its strengths, weaknesses, and optimal hyper-parameters for the\nPersian NER settings. Finally, we introduce a document-based domain detection\npipeline tailored for scenarios with unknown text domains, enhancing the\nadaptability and practicality of this paper in real-world applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02335v1",
    "published_date": "2024-04-02 22:15:48 UTC",
    "updated_date": "2024-04-02 22:15:48 UTC"
  },
  {
    "arxiv_id": "2404.02330v1",
    "title": "Comparative Study of Domain Driven Terms Extraction Using Large Language Models",
    "authors": [
      "Sandeep Chataut",
      "Tuyen Do",
      "Bichar Dip Shrestha Gurung",
      "Shiva Aryal",
      "Anup Khanal",
      "Carol Lushbough",
      "Etienne Gnimpieba"
    ],
    "abstract": "Keywords play a crucial role in bridging the gap between human understanding\nand machine processing of textual data. They are essential to data enrichment\nbecause they form the basis for detailed annotations that provide a more\ninsightful and in-depth view of the underlying data. Keyword/domain driven term\nextraction is a pivotal task in natural language processing, facilitating\ninformation retrieval, document summarization, and content categorization. This\nreview focuses on keyword extraction methods, emphasizing the use of three\nmajor Large Language Models(LLMs): Llama2-7B, GPT-3.5, and Falcon-7B. We\nemployed a custom Python package to interface with these LLMs, simplifying\nkeyword extraction. Our study, utilizing the Inspec and PubMed datasets,\nevaluates the performance of these models. The Jaccard similarity index was\nused for assessment, yielding scores of 0.64 (Inspec) and 0.21 (PubMed) for\nGPT-3.5, 0.40 and 0.17 for Llama2-7B, and 0.23 and 0.12 for Falcon-7B. This\npaper underlines the role of prompt engineering in LLMs for better keyword\nextraction and discusses the impact of hallucination in LLMs on result\nevaluation. It also sheds light on the challenges in using LLMs for keyword\nextraction, including model complexity, resource demands, and optimization\ntechniques.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02330v1",
    "published_date": "2024-04-02 22:04:51 UTC",
    "updated_date": "2024-04-02 22:04:51 UTC"
  },
  {
    "arxiv_id": "2404.02319v2",
    "title": "Symbolic Prompt Program Search: A Structure-Aware Approach to Efficient Compile-Time Prompt Optimization",
    "authors": [
      "Tobias Schnabel",
      "Jennifer Neville"
    ],
    "abstract": "In many modern LLM applications, such as retrieval augmented generation,\nprompts have become programs themselves. In these settings, prompt programs are\nrepeatedly called with different user queries or data instances. A big\npractical challenge is optimizing such prompt programs. Recent work has mostly\nfocused on either simple prompt programs or assumed that the general structure\nof a prompt program is fixed.\n  We introduce SAMMO, a framework to perform symbolic prompt program search for\ncompile-time optimizations of prompt programs. SAMMO represents prompt programs\non a symbolic level which allows for a rich set of transformations that can be\nsearched over during optimization. We show that SAMMO generalizes previous\nmethods and improves the performance of complex prompts on (1) instruction\ntuning, (2) RAG pipeline tuning, and (3) prompt compression, across several\ndifferent LLMs. We make all code available open-source at\nhttps://github.com/microsoft/sammo .",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02319v2",
    "published_date": "2024-04-02 21:35:54 UTC",
    "updated_date": "2024-06-27 23:22:14 UTC"
  },
  {
    "arxiv_id": "2404.02314v2",
    "title": "A Strong Baseline for Molecular Few-Shot Learning",
    "authors": [
      "Philippe Formont",
      "Hugo Jeannin",
      "Pablo Piantanida",
      "Ismail Ben Ayed"
    ],
    "abstract": "Few-shot learning has recently attracted significant interest in drug\ndiscovery, with a recent, fast-growing literature mostly involving convoluted\nmeta-learning strategies. We revisit the more straightforward fine-tuning\napproach for molecular data, and propose a regularized quadratic-probe loss\nbased on the the Mahalanobis distance. We design a dedicated block-coordinate\ndescent optimizer, which avoid the degenerate solutions of our loss.\nInterestingly, our simple fine-tuning approach achieves highly competitive\nperformances in comparison to state-of-the-art methods, while being applicable\nto black-box settings and removing the need for specific episodic pre-training\nstrategies. Furthermore, we introduce a new benchmark to assess the robustness\nof the competing methods to domain shifts. In this setting, our fine-tuning\nbaseline obtains consistently better results than meta-learning methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published in Transactions on Machine Learning Research (02/2025)",
    "pdf_url": "http://arxiv.org/pdf/2404.02314v2",
    "published_date": "2024-04-02 21:20:51 UTC",
    "updated_date": "2025-02-07 15:21:27 UTC"
  },
  {
    "arxiv_id": "2404.02305v1",
    "title": "Collapse of Self-trained Language Models",
    "authors": [
      "David Herel",
      "Tomas Mikolov"
    ],
    "abstract": "In various fields of knowledge creation, including science, new ideas often\nbuild on pre-existing information. In this work, we explore this concept within\nthe context of language models. Specifically, we explore the potential of\nself-training models on their own outputs, akin to how humans learn and build\non their previous thoughts and actions. While this approach is intuitively\nappealing, our research reveals its practical limitations. We find that\nextended self-training of the GPT-2 model leads to a significant degradation in\nperformance, resulting in repetitive and collapsed token output.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.02305v1",
    "published_date": "2024-04-02 21:03:37 UTC",
    "updated_date": "2024-04-02 21:03:37 UTC"
  },
  {
    "arxiv_id": "2404.02304v1",
    "title": "Virtual Sensor for Real-Time Bearing Load Prediction Using Heterogeneous Temporal Graph Neural Networks",
    "authors": [
      "Mengjie Zhao",
      "Cees Taal",
      "Stephan Baggerohr",
      "Olga Fink"
    ],
    "abstract": "Accurate bearing load monitoring is essential for their Prognostics and\nHealth Management (PHM), enabling damage assessment, wear prediction, and\nproactive maintenance. While bearing sensors are typically placed on the\nbearing housing, direct load monitoring requires sensors inside the bearing\nitself. Recently introduced sensor rollers enable direct bearing load\nmonitoring but are constrained by their battery life. Data-driven virtual\nsensors can learn from sensor roller data collected during a batterys lifetime\nto map operating conditions to bearing loads. Although spatially distributed\nbearing sensors offer insights into load distribution (e.g., correlating\ntemperature with load), traditional machine learning algorithms struggle to\nfully exploit these spatial-temporal dependencies. To address this gap, we\nintroduce a graph-based virtual sensor that leverages Graph Neural Networks\n(GNNs) to analyze spatial-temporal dependencies among sensor signals, mapping\nexisting measurements (temperature, vibration) to bearing loads. Since\ntemperature and vibration signals exhibit vastly different dynamics, we propose\nHeterogeneous Temporal Graph Neural Networks (HTGNN), which explicitly models\nthese signal types and their interactions for effective load prediction. Our\nresults demonstrate that HTGNN outperforms Convolutional Neural Networks\n(CNNs), which struggle to capture both spatial and heterogeneous signal\ncharacteristics. These findings highlight the importance of capturing the\ncomplex spatial interactions between temperature, vibration, and load.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.02304v1",
    "published_date": "2024-04-02 21:03:17 UTC",
    "updated_date": "2024-04-02 21:03:17 UTC"
  },
  {
    "arxiv_id": "2404.08668v2",
    "title": "A Comprehensive Survey on AI-based Methods for Patents",
    "authors": [
      "Homaira Huda Shomee",
      "Zhu Wang",
      "Sathya N. Ravi",
      "Sourav Medya"
    ],
    "abstract": "Recent advancements in Artificial Intelligence (AI) and machine learning have\ndemonstrated transformative capabilities across diverse domains. This progress\nextends to the field of patent analysis and innovation, where AI-based tools\npresent opportunities to streamline and enhance important tasks in the patent\ncycle such as classification, retrieval, and valuation prediction. This not\nonly accelerates the efficiency of patent researchers and applicants but also\nopens new avenues for technological innovation and discovery. Our survey\nprovides a comprehensive summary of recent AI tools in patent analysis from\nmore than 40 papers from 26 venues between 2017 and 2023. Unlike existing\nsurveys, we include methods that work for patent image and text data.\nFurthermore, we introduce a novel taxonomy for the categorization based on the\ntasks in the patent life cycle as well as the specifics of the AI methods. This\ninterdisciplinary survey aims to serve as a resource for researchers and\npractitioners who are working at the intersection of AI and patent analysis as\nwell as the patent offices that are aiming to build efficient patent systems.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.08668v2",
    "published_date": "2024-04-02 20:44:06 UTC",
    "updated_date": "2024-06-18 04:58:56 UTC"
  },
  {
    "arxiv_id": "2404.02287v1",
    "title": "One Noise to Rule Them All: Multi-View Adversarial Attacks with Universal Perturbation",
    "authors": [
      "Mehmet Ergezer",
      "Phat Duong",
      "Christian Green",
      "Tommy Nguyen",
      "Abdurrahman Zeybey"
    ],
    "abstract": "This paper presents a novel universal perturbation method for generating\nrobust multi-view adversarial examples in 3D object recognition. Unlike\nconventional attacks limited to single views, our approach operates on multiple\n2D images, offering a practical and scalable solution for enhancing model\nscalability and robustness. This generalizable method bridges the gap between\n2D perturbations and 3D-like attack capabilities, making it suitable for\nreal-world applications.\n  Existing adversarial attacks may become ineffective when images undergo\ntransformations like changes in lighting, camera position, or natural\ndeformations. We address this challenge by crafting a single universal noise\nperturbation applicable to various object views. Experiments on diverse\nrendered 3D objects demonstrate the effectiveness of our approach. The\nuniversal perturbation successfully identified a single adversarial noise for\neach given set of 3D object renders from multiple poses and viewpoints.\nCompared to single-view attacks, our universal attacks lower classification\nconfidence across multiple viewing angles, especially at low noise levels. A\nsample implementation is made available at\nhttps://github.com/memoatwit/UniversalPerturbation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "6 pages, 4 figures, presented at ICAIA, Springer to publish under\n  Algorithms for Intelligent Systems",
    "pdf_url": "http://arxiv.org/pdf/2404.02287v1",
    "published_date": "2024-04-02 20:29:59 UTC",
    "updated_date": "2024-04-02 20:29:59 UTC"
  },
  {
    "arxiv_id": "2404.05741v1",
    "title": "Enhancing Inference Efficiency of Large Language Models: Investigating Optimization Strategies and Architectural Innovations",
    "authors": [
      "Georgy Tyukin"
    ],
    "abstract": "Large Language Models are growing in size, and we expect them to continue to\ndo so, as larger models train quicker. However, this increase in size will\nseverely impact inference costs. Therefore model compression is important, to\nretain the performance of larger models, but with a reduced cost of running\nthem. In this thesis we explore the methods of model compression, and we\nempirically demonstrate that the simple method of skipping latter attention\nsublayers in Transformer LLMs is an effective method of model compression, as\nthese layers prove to be redundant, whilst also being incredibly\ncomputationally expensive. We observed a 21% speed increase in one-token\ngeneration for Llama 2 7B, whilst surprisingly and unexpectedly improving\nperformance over several common benchmarks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.PF"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.05741v1",
    "published_date": "2024-04-02 19:53:54 UTC",
    "updated_date": "2024-04-02 19:53:54 UTC"
  },
  {
    "arxiv_id": "2404.02269v1",
    "title": "Extracting Norms from Contracts Via ChatGPT: Opportunities and Challenges",
    "authors": [
      "Amanul Haque",
      "Munindar P. Singh"
    ],
    "abstract": "We investigate the effectiveness of ChatGPT in extracting norms from\ncontracts. Norms provide a natural way to engineer multiagent systems by\ncapturing how to govern the interactions between two or more autonomous\nparties. We extract norms of commitment, prohibition, authorization, and power,\nalong with associated norm elements (the parties involved, antecedents, and\nconsequents) from contracts. Our investigation reveals ChatGPT's effectiveness\nand limitations in norm extraction from contracts. ChatGPT demonstrates\npromising performance in norm extraction without requiring training or\nfine-tuning, thus obviating the need for annotated data, which is not generally\navailable in this domain. However, we found some limitations of ChatGPT in\nextracting these norms that lead to incorrect norm extractions. The limitations\ninclude oversight of crucial details, hallucination, incorrect parsing of\nconjunctions, and empty norm elements. Enhanced norm extraction from contracts\ncan foster the development of more transparent and trustworthy formal agent\ninteraction specifications, thereby contributing to the improvement of\nmultiagent systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at COINE-AAMAS 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.02269v1",
    "published_date": "2024-04-02 19:49:34 UTC",
    "updated_date": "2024-04-02 19:49:34 UTC"
  },
  {
    "arxiv_id": "2405.06650v1",
    "title": "Large Language Models as Planning Domain Generators",
    "authors": [
      "James Oswald",
      "Kavitha Srinivas",
      "Harsha Kokel",
      "Junkyu Lee",
      "Michael Katz",
      "Shirin Sohrabi"
    ],
    "abstract": "Developing domain models is one of the few remaining places that require\nmanual human labor in AI planning. Thus, in order to make planning more\naccessible, it is desirable to automate the process of domain model generation.\nTo this end, we investigate if large language models (LLMs) can be used to\ngenerate planning domain models from simple textual descriptions. Specifically,\nwe introduce a framework for automated evaluation of LLM-generated domains by\ncomparing the sets of plans for domain instances. Finally, we perform an\nempirical analysis of 7 large language models, including coding and chat models\nacross 9 different planning domains, and under three classes of natural\nlanguage domain descriptions. Our results indicate that LLMs, particularly\nthose with high parameter counts, exhibit a moderate level of proficiency in\ngenerating correct planning domains from natural language descriptions. Our\ncode is available at https://github.com/IBM/NL2PDDL.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Published at ICAPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.06650v1",
    "published_date": "2024-04-02 19:39:23 UTC",
    "updated_date": "2024-04-02 19:39:23 UTC"
  },
  {
    "arxiv_id": "2404.02263v1",
    "title": "OFMPNet: Deep End-to-End Model for Occupancy and Flow Prediction in Urban Environment",
    "authors": [
      "Youshaa Murhij",
      "Dmitry Yudin"
    ],
    "abstract": "The task of motion prediction is pivotal for autonomous driving systems,\nproviding crucial data to choose a vehicle behavior strategy within its\nsurroundings. Existing motion prediction techniques primarily focus on\npredicting the future trajectory of each agent in the scene individually,\nutilizing its past trajectory data. In this paper, we introduce an end-to-end\nneural network methodology designed to predict the future behaviors of all\ndynamic objects in the environment. This approach leverages the occupancy map\nand the scene's motion flow. We are investigatin various alternatives for\nconstructing a deep encoder-decoder model called OFMPNet. This model uses a\nsequence of bird's-eye-view road images, occupancy grid, and prior motion flow\nas input data. The encoder of the model can incorporate transformer,\nattention-based, or convolutional units. The decoder considers the use of both\nconvolutional modules and recurrent blocks. Additionally, we propose a novel\ntime-weighted motion flow loss, whose application has shown a substantial\ndecrease in end-point error. Our approach has achieved state-of-the-art results\non the Waymo Occupancy and Flow Prediction benchmark, with a Soft IoU of 52.1%\nand an AUC of 76.75% on Flow-Grounded Occupancy.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted in Neurocomputing journal - 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.02263v1",
    "published_date": "2024-04-02 19:37:58 UTC",
    "updated_date": "2024-04-02 19:37:58 UTC"
  },
  {
    "arxiv_id": "2404.02261v2",
    "title": "LLMs in the Loop: Leveraging Large Language Model Annotations for Active Learning in Low-Resource Languages",
    "authors": [
      "Nataliia Kholodna",
      "Sahib Julka",
      "Mohammad Khodadadi",
      "Muhammed Nurullah Gumus",
      "Michael Granitzer"
    ],
    "abstract": "Low-resource languages face significant barriers in AI development due to\nlimited linguistic resources and expertise for data labeling, rendering them\nrare and costly. The scarcity of data and the absence of preexisting tools\nexacerbate these challenges, especially since these languages may not be\nadequately represented in various NLP datasets. To address this gap, we propose\nleveraging the potential of LLMs in the active learning loop for data\nannotation. Initially, we conduct evaluations to assess inter-annotator\nagreement and consistency, facilitating the selection of a suitable LLM\nannotator. The chosen annotator is then integrated into a training loop for a\nclassifier using an active learning paradigm, minimizing the amount of queried\ndata required. Empirical evaluations, notably employing GPT-4-Turbo,\ndemonstrate near-state-of-the-art performance with significantly reduced data\nrequirements, as indicated by estimated potential cost savings of at least\n42.45 times compared to human annotation. Our proposed solution shows promising\npotential to substantially reduce both the monetary and computational costs\nassociated with automation in low-resource settings. By bridging the gap\nbetween low-resource languages and AI, this approach fosters broader inclusion\nand shows the potential to enable automation across diverse linguistic\nlandscapes.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG",
      "I.2.7; I.2.6"
    ],
    "primary_category": "cs.CL",
    "comment": "20 pages, 6 tables. The source code related to this paper is\n  available at https://github.com/mkandai/llms-in-the-loop. This paper has been\n  accepted for publication at ECML PKDD 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.02261v2",
    "published_date": "2024-04-02 19:34:22 UTC",
    "updated_date": "2024-06-23 18:21:01 UTC"
  },
  {
    "arxiv_id": "2404.02255v1",
    "title": "$\\texttt{LM}^\\texttt{2}$: A Simple Society of Language Models Solves Complex Reasoning",
    "authors": [
      "Gurusha Juneja",
      "Subhabrata Dutta",
      "Tanmoy Chakraborty"
    ],
    "abstract": "Despite demonstrating emergent reasoning abilities, Large Language Models\n(LLMS) often lose track of complex, multi-step reasoning. Existing studies show\nthat providing guidance via decomposing the original question into multiple\nsubproblems elicits more robustness in LLM reasoning -- a decomposer generates\nthe subproblems, and a solver solves each of these subproblems. However, these\ntechniques fail to accommodate coordination between the decomposer and the\nsolver modules (either in a single model or different specialized ones) -- the\ndecomposer does not keep track of the ability of the solver to follow the\ndecomposed reasoning. In this paper, we propose LM2 to address these\nchallenges. LM2 modularizes the decomposition, solution, and verification into\nthree different language models. The decomposer module identifies the key\nconcepts necessary to solve the problem and generates step-by-step subquestions\naccording to the reasoning requirement. The solver model generates the solution\nto the subproblems that are then checked by the verifier module; depending upon\nthe feedback from the verifier, the reasoning context is constructed using the\nsubproblems and the solutions. These models are trained to coordinate using\npolicy learning. Exhaustive experimentation suggests the superiority of LM2\nover existing methods on in- and out-domain reasoning problems, outperforming\nthe best baselines by $8.1\\%$ on MATH, $7.71\\%$ on JEEBench, and $9.7\\%$ on\nMedQA problems (code available at\nhttps://github.com/LCS2-IIITD/Language_Model_Multiplex).",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02255v1",
    "published_date": "2024-04-02 19:23:10 UTC",
    "updated_date": "2024-04-02 19:23:10 UTC"
  },
  {
    "arxiv_id": "2404.02254v2",
    "title": "On Stronger Computational Separations Between Multimodal and Unimodal Machine Learning",
    "authors": [
      "Ari Karchmer"
    ],
    "abstract": "Recently, multimodal machine learning has enjoyed huge empirical success\n(e.g. GPT-4). Motivated to develop theoretical justification for this empirical\nsuccess, Lu (NeurIPS '23, ALT '24) introduces a theory of multimodal learning,\nand considers possible \\textit{separations} between theoretical models of\nmultimodal and unimodal learning. In particular, Lu (ALT '24) shows a\ncomputational separation, which is relevant to \\textit{worst-case} instances of\nthe learning task. In this paper, we give a stronger \\textit{average-case}\ncomputational separation, where for ``typical'' instances of the learning task,\nunimodal learning is computationally hard, but multimodal learning is easy. We\nthen question how ``natural'' the average-case separation is. Would it be\nencountered in practice? To this end, we prove that under basic conditions, any\ngiven computational separation between average-case unimodal and multimodal\nlearning tasks implies a corresponding cryptographic key agreement protocol. We\nsuggest to interpret this as evidence that very strong \\textit{computational}\nadvantages of multimodal learning may arise \\textit{infrequently} in practice,\nsince they exist only for the ``pathological'' case of inherently cryptographic\ndistributions. However, this does not apply to possible (super-polynomial)\n\\textit{statistical} advantages.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "Appeared in ICML 2024. Camera-ready version",
    "pdf_url": "http://arxiv.org/pdf/2404.02254v2",
    "published_date": "2024-04-02 19:21:28 UTC",
    "updated_date": "2024-07-17 17:01:45 UTC"
  },
  {
    "arxiv_id": "2404.02249v2",
    "title": "RAT: Retrieval-Augmented Transformer for Click-Through Rate Prediction",
    "authors": [
      "Yushen Li",
      "Jinpeng Wang",
      "Tao Dai",
      "Jieming Zhu",
      "Jun Yuan",
      "Rui Zhang",
      "Shu-Tao Xia"
    ],
    "abstract": "Predicting click-through rates (CTR) is a fundamental task for Web\napplications, where a key issue is to devise effective models for feature\ninteractions. Current methodologies predominantly concentrate on modeling\nfeature interactions within an individual sample, while overlooking the\npotential cross-sample relationships that can serve as a reference context to\nenhance the prediction. To make up for such deficiency, this paper develops a\nRetrieval-Augmented Transformer (RAT), aiming to acquire fine-grained feature\ninteractions within and across samples. By retrieving similar samples, we\nconstruct augmented input for each target sample. We then build Transformer\nlayers with cascaded attention to capture both intra- and cross-sample feature\ninteractions, facilitating comprehensive reasoning for improved CTR prediction\nwhile retaining efficiency. Extensive experiments on real-world datasets\nsubstantiate the effectiveness of RAT and suggest its advantage in long-tail\nscenarios. The code has been open-sourced at\n\\url{https://github.com/YushenLi807/WWW24-RAT}.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG",
      "cs.SI"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted to The ACM Web Conference 2024 (WWW'24, short paper). Data\n  and code are available",
    "pdf_url": "http://arxiv.org/pdf/2404.02249v2",
    "published_date": "2024-04-02 19:14:23 UTC",
    "updated_date": "2024-04-05 02:13:30 UTC"
  },
  {
    "arxiv_id": "2404.02235v1",
    "title": "Is Exploration All You Need? Effective Exploration Characteristics for Transfer in Reinforcement Learning",
    "authors": [
      "Jonathan C. Balloch",
      "Rishav Bhagat",
      "Geigh Zollicoffer",
      "Ruoran Jia",
      "Julia Kim",
      "Mark O. Riedl"
    ],
    "abstract": "In deep reinforcement learning (RL) research, there has been a concerted\neffort to design more efficient and productive exploration methods while\nsolving sparse-reward problems. These exploration methods often share common\nprinciples (e.g., improving diversity) and implementation details (e.g.,\nintrinsic reward). Prior work found that non-stationary Markov decision\nprocesses (MDPs) require exploration to efficiently adapt to changes in the\nenvironment with online transfer learning. However, the relationship between\nspecific exploration characteristics and effective transfer learning in deep RL\nhas not been characterized. In this work, we seek to understand the\nrelationships between salient exploration characteristics and improved\nperformance and efficiency in transfer learning. We test eleven popular\nexploration algorithms on a variety of transfer types -- or ``novelties'' -- to\nidentify the characteristics that positively affect online transfer learning.\nOur analysis shows that some characteristics correlate with improved\nperformance and efficiency across a wide range of transfer tasks, while others\nonly improve transfer performance with respect to specific environment changes.\nFrom our analysis, make recommendations about which exploration algorithm\ncharacteristics are best suited to specific transfer situations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02235v1",
    "published_date": "2024-04-02 18:45:01 UTC",
    "updated_date": "2024-04-02 18:45:01 UTC"
  },
  {
    "arxiv_id": "2404.02227v1",
    "title": "OOSTraj: Out-of-Sight Trajectory Prediction With Vision-Positioning Denoising",
    "authors": [
      "Haichao Zhang",
      "Yi Xu",
      "Hongsheng Lu",
      "Takayuki Shimizu",
      "Yun Fu"
    ],
    "abstract": "Trajectory prediction is fundamental in computer vision and autonomous\ndriving, particularly for understanding pedestrian behavior and enabling\nproactive decision-making. Existing approaches in this field often assume\nprecise and complete observational data, neglecting the challenges associated\nwith out-of-view objects and the noise inherent in sensor data due to limited\ncamera range, physical obstructions, and the absence of ground truth for\ndenoised sensor data. Such oversights are critical safety concerns, as they can\nresult in missing essential, non-visible objects. To bridge this gap, we\npresent a novel method for out-of-sight trajectory prediction that leverages a\nvision-positioning technique. Our approach denoises noisy sensor observations\nin an unsupervised manner and precisely maps sensor-based trajectories of\nout-of-sight objects into visual trajectories. This method has demonstrated\nstate-of-the-art performance in out-of-sight noisy sensor trajectory denoising\nand prediction on the Vi-Fi and JRDB datasets. By enhancing trajectory\nprediction accuracy and addressing the challenges of out-of-sight objects, our\nwork significantly contributes to improving the safety and reliability of\nautonomous driving in complex environments. Our work represents the first\ninitiative towards Out-Of-Sight Trajectory prediction (OOSTraj), setting a new\nbenchmark for future research. The code is available at\n\\url{https://github.com/Hai-chao-Zhang/OOSTraj}.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "In Proceedings of IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition 2024 (CVPR)",
    "pdf_url": "http://arxiv.org/pdf/2404.02227v1",
    "published_date": "2024-04-02 18:30:29 UTC",
    "updated_date": "2024-04-02 18:30:29 UTC"
  },
  {
    "arxiv_id": "2404.02225v2",
    "title": "CHOSEN: Contrastive Hypothesis Selection for Multi-View Depth Refinement",
    "authors": [
      "Di Qiu",
      "Yinda Zhang",
      "Thabo Beeler",
      "Vladimir Tankovich",
      "Christian Häne",
      "Sean Fanello",
      "Christoph Rhemann",
      "Sergio Orts Escolano"
    ],
    "abstract": "We propose CHOSEN, a simple yet flexible, robust and effective multi-view\ndepth refinement framework. It can be employed in any existing multi-view\nstereo pipeline, with straightforward generalization capability for different\nmulti-view capture systems such as camera relative positioning and lenses.\nGiven an initial depth estimation, CHOSEN iteratively re-samples and selects\nthe best hypotheses, and automatically adapts to different metric or intrinsic\nscales determined by the capture system. The key to our approach is the\napplication of contrastive learning in an appropriate solution space and a\ncarefully designed hypothesis feature, based on which positive and negative\nhypotheses can be effectively distinguished. Integrated in a simple baseline\nmulti-view stereo pipeline, CHOSEN delivers impressive quality in terms of\ndepth and normal accuracy compared to many current deep learning based\nmulti-view stereo pipelines.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02225v2",
    "published_date": "2024-04-02 18:27:03 UTC",
    "updated_date": "2025-05-05 15:35:26 UTC"
  },
  {
    "arxiv_id": "2404.02213v1",
    "title": "Exploring How Multiple Levels of GPT-Generated Programming Hints Support or Disappoint Novices",
    "authors": [
      "Ruiwei Xiao",
      "Xinying Hou",
      "John Stamper"
    ],
    "abstract": "Recent studies have integrated large language models (LLMs) into diverse\neducational contexts, including providing adaptive programming hints, a type of\nfeedback focuses on helping students move forward during problem-solving.\nHowever, most existing LLM-based hint systems are limited to one single hint\ntype. To investigate whether and how different levels of hints can support\nstudents' problem-solving and learning, we conducted a think-aloud study with\n12 novices using the LLM Hint Factory, a system providing four levels of hints\nfrom general natural language guidance to concrete code assistance, varying in\nformat and granularity. We discovered that high-level natural language hints\nalone can be helpless or even misleading, especially when addressing next-step\nor syntax-related help requests. Adding lower-level hints, like code examples\nwith in-line comments, can better support students. The findings open up future\nwork on customizing help responses from content, format, and granularity levels\nto accurately identify and meet students' learning needs.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted CHI 2024 LBW - 10 pages",
    "pdf_url": "http://arxiv.org/pdf/2404.02213v1",
    "published_date": "2024-04-02 18:05:26 UTC",
    "updated_date": "2024-04-02 18:05:26 UTC"
  },
  {
    "arxiv_id": "2404.02205v2",
    "title": "A Holistic Indicator of Polarization to Measure Online Sexism",
    "authors": [
      "Vahid Ghafouri",
      "Jose Such",
      "Guillermo Suarez-Tangil"
    ],
    "abstract": "The online trend of the manosphere and feminist discourse on social networks\nrequires a holistic measure of the level of sexism in an online community. This\nindicator is important for policymakers and moderators of online communities\n(e.g., subreddits) and computational social scientists, either to revise\nmoderation strategies based on the degree of sexism or to match and compare the\ntemporal sexism across different platforms and communities with real-time\nevents and infer social scientific insights.\n  In this paper, we build a model that can provide a comparable holistic\nindicator of toxicity targeted toward male and female identity and male and\nfemale individuals. Despite previous supervised NLP methods that require\nannotation of toxic comments at the target level (e.g. annotating comments that\nare specifically toxic toward women) to detect targeted toxic comments, our\nindicator uses supervised NLP to detect the presence of toxicity and\nunsupervised word embedding association test to detect the target\nautomatically.\n  We apply our model to gender discourse communities (e.g., r/TheRedPill,\nr/MGTOW, r/FemaleDatingStrategy) to detect the level of toxicity toward genders\n(i.e., sexism). Our results show that our framework accurately and consistently\n(93% correlation) measures the level of sexism in a community. We finally\ndiscuss how our framework can be generalized in the future to measure qualities\nother than toxicity (e.g. sentiment, humor) toward general-purpose targets and\nturn into an indicator of different sorts of polarizations.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02205v2",
    "published_date": "2024-04-02 18:00:42 UTC",
    "updated_date": "2024-06-29 15:27:34 UTC"
  },
  {
    "arxiv_id": "2404.02157v1",
    "title": "Segment Any 3D Object with Language",
    "authors": [
      "Seungjun Lee",
      "Yuyang Zhao",
      "Gim Hee Lee"
    ],
    "abstract": "In this paper, we investigate Open-Vocabulary 3D Instance Segmentation\n(OV-3DIS) with free-form language instructions. Earlier works that rely on only\nannotated base categories for training suffer from limited generalization to\nunseen novel categories. Recent works mitigate poor generalizability to novel\ncategories by generating class-agnostic masks or projecting generalized masks\nfrom 2D to 3D, but disregard semantic or geometry information, leading to\nsub-optimal performance. Instead, generating generalizable but semantic-related\nmasks directly from 3D point clouds would result in superior outcomes. In this\npaper, we introduce Segment any 3D Object with LanguagE (SOLE), which is a\nsemantic and geometric-aware visual-language learning framework with strong\ngeneralizability by generating semantic-related masks directly from 3D point\nclouds. Specifically, we propose a multimodal fusion network to incorporate\nmultimodal semantics in both backbone and decoder. In addition, to align the 3D\nsegmentation model with various language instructions and enhance the mask\nquality, we introduce three types of multimodal associations as supervision.\nOur SOLE outperforms previous methods by a large margin on ScanNetv2,\nScanNet200, and Replica benchmarks, and the results are even close to the\nfully-supervised counterpart despite the absence of class annotations in the\ntraining. Furthermore, extensive qualitative results demonstrate the\nversatility of our SOLE to language instructions.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page: https://cvrp-sole.github.io",
    "pdf_url": "http://arxiv.org/pdf/2404.02157v1",
    "published_date": "2024-04-02 17:59:10 UTC",
    "updated_date": "2024-04-02 17:59:10 UTC"
  },
  {
    "arxiv_id": "2404.02151v4",
    "title": "Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks",
    "authors": [
      "Maksym Andriushchenko",
      "Francesco Croce",
      "Nicolas Flammarion"
    ],
    "abstract": "We show that even the most recent safety-aligned LLMs are not robust to\nsimple adaptive jailbreaking attacks. First, we demonstrate how to successfully\nleverage access to logprobs for jailbreaking: we initially design an\nadversarial prompt template (sometimes adapted to the target LLM), and then we\napply random search on a suffix to maximize a target logprob (e.g., of the\ntoken \"Sure\"), potentially with multiple restarts. In this way, we achieve 100%\nattack success rate -- according to GPT-4 as a judge -- on Vicuna-13B,\nMistral-7B, Phi-3-Mini, Nemotron-4-340B, Llama-2-Chat-7B/13B/70B,\nLlama-3-Instruct-8B, Gemma-7B, GPT-3.5, GPT-4o, and R2D2 from HarmBench that\nwas adversarially trained against the GCG attack. We also show how to jailbreak\nall Claude models -- that do not expose logprobs -- via either a transfer or\nprefilling attack with a 100% success rate. In addition, we show how to use\nrandom search on a restricted set of tokens for finding trojan strings in\npoisoned models -- a task that shares many similarities with jailbreaking --\nwhich is the algorithm that brought us the first place in the SaTML'24 Trojan\nDetection Competition. The common theme behind these attacks is that adaptivity\nis crucial: different models are vulnerable to different prompting templates\n(e.g., R2D2 is very sensitive to in-context learning prompts), some models have\nunique vulnerabilities based on their APIs (e.g., prefilling for Claude), and\nin some settings, it is crucial to restrict the token search space based on\nprior knowledge (e.g., for trojan detection). For reproducibility purposes, we\nprovide the code, logs, and jailbreak artifacts in the JailbreakBench format at\nhttps://github.com/tml-epfl/llm-adaptive-attacks.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted at ICLR 2025. Updates in the v3: GPT-4o and Claude 3.5\n  Sonnet results, improved writing. Updates in the v2: more models (Llama3,\n  Phi-3, Nemotron-4-340B), jailbreak artifacts for all attacks are available,\n  evaluation with different judges (Llama-3-70B and Llama Guard 2), more\n  experiments (convergence plots, ablation on the suffix length for random\n  search), examples of jailbroken generation",
    "pdf_url": "http://arxiv.org/pdf/2404.02151v4",
    "published_date": "2024-04-02 17:58:27 UTC",
    "updated_date": "2025-04-17 18:55:45 UTC"
  },
  {
    "arxiv_id": "2404.02127v2",
    "title": "LawInstruct: A Resource for Studying Language Model Adaptation to the Legal Domain",
    "authors": [
      "Joel Niklaus",
      "Lucia Zheng",
      "Arya D. McCarthy",
      "Christopher Hahn",
      "Brian M. Rosen",
      "Peter Henderson",
      "Daniel E. Ho",
      "Garrett Honke",
      "Percy Liang",
      "Christopher Manning"
    ],
    "abstract": "Instruction tuning is an important step in making language models useful for\ndirect user interaction. However, the legal domain is underrepresented in\ntypical instruction datasets (e.g., only 10 out of 1600+ tasks in\nSuper-NaturalInstructions). To study whether instruction tuning on legal\ndatasets is necessary for strong legal reasoning, we aggregate 58 annotated\nlegal datasets and write instructions for each, creating LawInstruct.\nLawInstruct covers 17 global jurisdictions, 24 languages and a total of 12M\nexamples across diverse tasks such as legal QA, summarization of court cases,\nand legal argument mining. We evaluate our models on LegalBench, measuring\nlegal reasoning across five categories in 162 challenging and realistic legal\ntasks, and MMLU, to measure potential drops in general reasoning capabilities.\nWe find that legal-specific instruction tuning on Flan-T5 - yielding FLawN-T5 -\nimproves performance on LegalBench across all model sizes, with an aggregate\nincrease of 15 points or 50% over Flan-T5 for the base size. No model size\nshows performance drops in MMLU. We publish LawInstruct as a resource for\nfurther study of instruction tuning in the legal domain.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "68T50",
      "I.2"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at Findings of NAACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2404.02127v2",
    "published_date": "2024-04-02 17:33:34 UTC",
    "updated_date": "2025-01-23 06:54:26 UTC"
  },
  {
    "arxiv_id": "2404.15311v2",
    "title": "Fusing Pretrained ViTs with TCNet for Enhanced EEG Regression",
    "authors": [
      "Eric Modesitt",
      "Haicheng Yin",
      "Williams Huang Wang",
      "Brian Lu"
    ],
    "abstract": "The task of Electroencephalogram (EEG) analysis is paramount to the\ndevelopment of Brain-Computer Interfaces (BCIs). However, to reach the goal of\ndeveloping robust, useful BCIs depends heavily on the speed and the accuracy at\nwhich BCIs can understand neural dynamics. In response to that goal, this paper\ndetails the integration of pre-trained Vision Transformers (ViTs) with Temporal\nConvolutional Networks (TCNet) to enhance the precision of EEG regression. The\ncore of this approach lies in harnessing the sequential data processing\nstrengths of ViTs along with the superior feature extraction capabilities of\nTCNet, to significantly improve EEG analysis accuracy. In addition, we analyze\nthe importance of how to construct optimal patches for the attention mechanism\nto analyze, balancing both speed and accuracy tradeoffs. Our results showcase a\nsubstantial improvement in regression accuracy, as evidenced by the reduction\nof Root Mean Square Error (RMSE) from 55.4 to 51.8 on EEGEyeNet's Absolute\nPosition Task, outperforming existing state-of-the-art models. Without\nsacrificing performance, we increase the speed of this model by an order of\nmagnitude (up to 4.32x faster). This breakthrough not only sets a new benchmark\nin EEG regression analysis but also opens new avenues for future research in\nthe integration of transformer architectures with specialized feature\nextraction methods for diverse EEG datasets.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "Accepted HCI International 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.15311v2",
    "published_date": "2024-04-02 17:01:51 UTC",
    "updated_date": "2024-08-07 08:14:56 UTC"
  },
  {
    "arxiv_id": "2404.02189v1",
    "title": "Insights from the Use of Previously Unseen Neural Architecture Search Datasets",
    "authors": [
      "Rob Geada",
      "David Towers",
      "Matthew Forshaw",
      "Amir Atapour-Abarghouei",
      "A. Stephen McGough"
    ],
    "abstract": "The boundless possibility of neural networks which can be used to solve a\nproblem -- each with different performance -- leads to a situation where a Deep\nLearning expert is required to identify the best neural network. This goes\nagainst the hope of removing the need for experts. Neural Architecture Search\n(NAS) offers a solution to this by automatically identifying the best\narchitecture. However, to date, NAS work has focused on a small set of datasets\nwhich we argue are not representative of real-world problems. We introduce\neight new datasets created for a series of NAS Challenges: AddNIST, Language,\nMultNIST, CIFARTile, Gutenberg, Isabella, GeoClassing, and Chesseract. These\ndatasets and challenges are developed to direct attention to issues in NAS\ndevelopment and to encourage authors to consider how their models will perform\non datasets unknown to them at development time. We present experimentation\nusing standard Deep Learning methods as well as the best results from challenge\nparticipants.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02189v1",
    "published_date": "2024-04-02 16:48:34 UTC",
    "updated_date": "2024-04-02 16:48:34 UTC"
  },
  {
    "arxiv_id": "2404.02931v1",
    "title": "READ: Improving Relation Extraction from an ADversarial Perspective",
    "authors": [
      "Dawei Li",
      "William Hogan",
      "Jingbo Shang"
    ],
    "abstract": "Recent works in relation extraction (RE) have achieved promising benchmark\naccuracy; however, our adversarial attack experiments show that these works\nexcessively rely on entities, making their generalization capability\nquestionable. To address this issue, we propose an adversarial training method\nspecifically designed for RE. Our approach introduces both sequence- and\ntoken-level perturbations to the sample and uses a separate perturbation\nvocabulary to improve the search for entity and context perturbations.\nFurthermore, we introduce a probabilistic strategy for leaving clean tokens in\nthe context during adversarial training. This strategy enables a larger attack\nbudget for entities and coaxes the model to leverage relational patterns\nembedded in the context. Extensive experiments show that compared to various\nadversarial training methods, our method significantly improves both the\naccuracy and robustness of the model. Additionally, experiments on different\ndata availability settings highlight the effectiveness of our method in\nlow-resource scenarios. We also perform in-depth analyses of our proposed\nmethod and provide further hints. We will release our code at\nhttps://github.com/David-Li0406/READ.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by findings of NAACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.02931v1",
    "published_date": "2024-04-02 16:42:44 UTC",
    "updated_date": "2024-04-02 16:42:44 UTC"
  },
  {
    "arxiv_id": "2404.02090v4",
    "title": "Already Moderate Population Sizes Provably Yield Strong Robustness to Noise",
    "authors": [
      "Denis Antipov",
      "Benjamin Doerr",
      "Alexandra Ivanova"
    ],
    "abstract": "Experience shows that typical evolutionary algorithms can cope well with\nstochastic disturbances such as noisy function evaluations.\n  In this first mathematical runtime analysis of the $(1+\\lambda)$ and\n$(1,\\lambda)$ evolutionary algorithms in the presence of prior bit-wise noise,\nwe show that both algorithms can tolerate constant noise probabilities without\nincreasing the asymptotic runtime on the OneMax benchmark. For this, a\npopulation size $\\lambda$ suffices that is at least logarithmic in the problem\nsize $n$. The only previous result in this direction regarded the less\nrealistic one-bit noise model, required a population size super-linear in the\nproblem size, and proved a runtime guarantee roughly cubic in the noiseless\nruntime for the OneMax benchmark. Our significantly stronger results are based\non the novel proof argument that the noiseless offspring can be seen as a\nbiased uniform crossover between the parent and the noisy offspring. We are\noptimistic that the technical lemmas resulting from this insight will find\napplications also in future mathematical runtime analyses of evolutionary\nalgorithms.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "Full version of the same-titled paper accepted at GECCO 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.02090v4",
    "published_date": "2024-04-02 16:35:52 UTC",
    "updated_date": "2024-05-13 05:01:01 UTC"
  },
  {
    "arxiv_id": "2404.02078v1",
    "title": "Advancing LLM Reasoning Generalists with Preference Trees",
    "authors": [
      "Lifan Yuan",
      "Ganqu Cui",
      "Hanbin Wang",
      "Ning Ding",
      "Xingyao Wang",
      "Jia Deng",
      "Boji Shan",
      "Huimin Chen",
      "Ruobing Xie",
      "Yankai Lin",
      "Zhenghao Liu",
      "Bowen Zhou",
      "Hao Peng",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "abstract": "We introduce Eurus, a suite of large language models (LLMs) optimized for\nreasoning. Finetuned from Mistral-7B and CodeLlama-70B, Eurus models achieve\nstate-of-the-art results among open-source models on a diverse set of\nbenchmarks covering mathematics, code generation, and logical reasoning\nproblems. Notably, Eurus-70B beats GPT-3.5 Turbo in reasoning through a\ncomprehensive benchmarking across 12 tests covering five tasks, and achieves a\n33.3% pass@1 accuracy on LeetCode and 32.6% on TheoremQA, two challenging\nbenchmarks, substantially outperforming existing open-source models by margins\nmore than 13.3%. The strong performance of Eurus can be primarily attributed to\nUltraInteract, our newly-curated large-scale, high-quality alignment dataset\nspecifically designed for complex reasoning tasks. UltraInteract can be used in\nboth supervised fine-tuning and preference learning. For each instruction, it\nincludes a preference tree consisting of (1) reasoning chains with diverse\nplanning strategies in a unified format, (2) multi-turn interaction\ntrajectories with the environment and the critique, and (3) pairwise data to\nfacilitate preference learning. UltraInteract allows us to conduct an in-depth\nexploration of preference learning for reasoning tasks. Our investigation\nreveals that some well-established preference learning algorithms may be less\nsuitable for reasoning tasks compared to their effectiveness in general\nconversations. Inspired by this, we derive a novel reward modeling objective\nwhich, together with UltraInteract, leads to a strong reward model.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Models and data are available at https://github.com/OpenBMB/Eurus",
    "pdf_url": "http://arxiv.org/pdf/2404.02078v1",
    "published_date": "2024-04-02 16:25:30 UTC",
    "updated_date": "2024-04-02 16:25:30 UTC"
  },
  {
    "arxiv_id": "2404.02067v1",
    "title": "Red-Teaming Segment Anything Model",
    "authors": [
      "Krzysztof Jankowski",
      "Bartlomiej Sobieski",
      "Mateusz Kwiatkowski",
      "Jakub Szulc",
      "Michal Janik",
      "Hubert Baniecki",
      "Przemyslaw Biecek"
    ],
    "abstract": "Foundation models have emerged as pivotal tools, tackling many complex tasks\nthrough pre-training on vast datasets and subsequent fine-tuning for specific\napplications. The Segment Anything Model is one of the first and most\nwell-known foundation models for computer vision segmentation tasks. This work\npresents a multi-faceted red-teaming analysis that tests the Segment Anything\nModel against challenging tasks: (1) We analyze the impact of style transfer on\nsegmentation masks, demonstrating that applying adverse weather conditions and\nraindrops to dashboard images of city roads significantly distorts generated\nmasks. (2) We focus on assessing whether the model can be used for attacks on\nprivacy, such as recognizing celebrities' faces, and show that the model\npossesses some undesired knowledge in this task. (3) Finally, we check how\nrobust the model is to adversarial attacks on segmentation masks under text\nprompts. We not only show the effectiveness of popular white-box attacks and\nresistance to black-box attacks but also introduce a novel approach - Focused\nIterative Gradient Attack (FIGA) that combines white-box approaches to\nconstruct an efficient attack resulting in a smaller number of modified pixels.\nAll of our testing methods and analyses indicate a need for enhanced safety\nmeasures in foundation models for image segmentation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2024 - The 4th Workshop of Adversarial Machine Learning on\n  Computer Vision: Robustness of Foundation Models",
    "pdf_url": "http://arxiv.org/pdf/2404.02067v1",
    "published_date": "2024-04-02 16:07:50 UTC",
    "updated_date": "2024-04-02 16:07:50 UTC"
  },
  {
    "arxiv_id": "2404.02187v1",
    "title": "A Generative Deep Learning Approach for Crash Severity Modeling with Imbalanced Data",
    "authors": [
      "Junlan Chen",
      "Ziyuan Pu",
      "Nan Zheng",
      "Xiao Wen",
      "Hongliang Ding",
      "Xiucheng Guo"
    ],
    "abstract": "Crash data is often greatly imbalanced, with the majority of crashes being\nnon-fatal crashes, and only a small number being fatal crashes due to their\nrarity. Such data imbalance issue poses a challenge for crash severity modeling\nsince it struggles to fit and interpret fatal crash outcomes with very limited\nsamples. Usually, such data imbalance issues are addressed by data resampling\nmethods, such as under-sampling and over-sampling techniques. However, most\ntraditional and deep learning-based data resampling methods, such as synthetic\nminority oversampling technique (SMOTE) and generative Adversarial Networks\n(GAN) are designed dedicated to processing continuous variables. Though some\nresampling methods have improved to handle both continuous and discrete\nvariables, they may have difficulties in dealing with the collapse issue\nassociated with sparse discrete risk factors. Moreover, there is a lack of\ncomprehensive studies that compare the performance of various resampling\nmethods in crash severity modeling. To address the aforementioned issues, the\ncurrent study proposes a crash data generation method based on the Conditional\nTabular GAN. After data balancing, a crash severity model is employed to\nestimate the performance of classification and interpretation. A comparative\nstudy is conducted to assess classification accuracy and distribution\nconsistency of the proposed generation method using a 4-year imbalanced crash\ndataset collected in Washington State, U.S. Additionally, Monte Carlo\nsimulation is employed to estimate the performance of parameter and probability\nestimation in both two- and three-class imbalance scenarios. The results\nindicate that using synthetic data generated by CTGAN-RU for crash severity\nmodeling outperforms using original data or synthetic data generated by other\nresampling methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02187v1",
    "published_date": "2024-04-02 16:07:27 UTC",
    "updated_date": "2024-04-02 16:07:27 UTC"
  },
  {
    "arxiv_id": "2404.02063v2",
    "title": "SPMamba: State-space model is all you need in speech separation",
    "authors": [
      "Kai Li",
      "Guo Chen",
      "Runxuan Yang",
      "Xiaolin Hu"
    ],
    "abstract": "Existing CNN-based speech separation models face local receptive field\nlimitations and cannot effectively capture long time dependencies. Although\nLSTM and Transformer-based speech separation models can avoid this problem,\ntheir high complexity makes them face the challenge of computational resources\nand inference efficiency when dealing with long audio. To address this\nchallenge, we introduce an innovative speech separation method called SPMamba.\nThis model builds upon the robust TF-GridNet architecture, replacing its\ntraditional BLSTM modules with bidirectional Mamba modules. These modules\neffectively model the spatiotemporal relationships between the time and\nfrequency dimensions, allowing SPMamba to capture long-range dependencies with\nlinear computational complexity. Specifically, the bidirectional processing\nwithin the Mamba modules enables the model to utilize both past and future\ncontextual information, thereby enhancing separation performance. Extensive\nexperiments conducted on public datasets, including WSJ0-2Mix, WHAM!, and\nLibri2Mix, as well as the newly constructed Echo2Mix dataset, demonstrated that\nSPMamba significantly outperformed existing state-of-the-art models, achieving\nsuperior results while also reducing computational complexity. These findings\nhighlighted the effectiveness of SPMamba in tackling the intricate challenges\nof speech separation in complex environments.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Technical Report. Code is available at\n  https://github.com/JusperLee/SPMamba",
    "pdf_url": "http://arxiv.org/pdf/2404.02063v2",
    "published_date": "2024-04-02 16:04:31 UTC",
    "updated_date": "2024-09-10 14:02:58 UTC"
  },
  {
    "arxiv_id": "2404.02062v1",
    "title": "Digital Forgetting in Large Language Models: A Survey of Unlearning Methods",
    "authors": [
      "Alberto Blanco-Justicia",
      "Najeeb Jebreel",
      "Benet Manzanares",
      "David Sánchez",
      "Josep Domingo-Ferrer",
      "Guillem Collell",
      "Kuan Eeik Tan"
    ],
    "abstract": "The objective of digital forgetting is, given a model with undesirable\nknowledge or behavior, obtain a new model where the detected issues are no\nlonger present. The motivations for forgetting include privacy protection,\ncopyright protection, elimination of biases and discrimination, and prevention\nof harmful content generation. Effective digital forgetting has to be effective\n(meaning how well the new model has forgotten the undesired\nknowledge/behavior), retain the performance of the original model on the\ndesirable tasks, and be scalable (in particular forgetting has to be more\nefficient than retraining from scratch on just the tasks/data to be retained).\nThis survey focuses on forgetting in large language models (LLMs). We first\nprovide background on LLMs, including their components, the types of LLMs, and\ntheir usual training pipeline. Second, we describe the motivations, types, and\ndesired properties of digital forgetting. Third, we introduce the approaches to\ndigital forgetting in LLMs, among which unlearning methodologies stand out as\nthe state of the art. Fourth, we provide a detailed taxonomy of machine\nunlearning methods for LLMs, and we survey and compare current approaches.\nFifth, we detail datasets, models and metrics used for the evaluation of\nforgetting, retaining and runtime. Sixth, we discuss challenges in the area.\nFinally, we provide some concluding remarks.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "68",
      "K.4.1; I.2.6; I.2.7"
    ],
    "primary_category": "cs.CR",
    "comment": "70 pages",
    "pdf_url": "http://arxiv.org/pdf/2404.02062v1",
    "published_date": "2024-04-02 16:01:18 UTC",
    "updated_date": "2024-04-02 16:01:18 UTC"
  },
  {
    "arxiv_id": "2404.02060v3",
    "title": "Long-context LLMs Struggle with Long In-context Learning",
    "authors": [
      "Tianle Li",
      "Ge Zhang",
      "Quy Duc Do",
      "Xiang Yue",
      "Wenhu Chen"
    ],
    "abstract": "Large Language Models (LLMs) have made significant strides in handling long\nsequences. Some models like Gemini could even to be capable of dealing with\nmillions of tokens. However, their performance evaluation has largely been\nconfined to metrics like perplexity and synthetic tasks, which may not fully\ncapture their true abilities in more challenging, real-world scenarios. We\nintroduce a benchmark (LongICLBench) for long in-context learning in\nextreme-label classification using six datasets with 28 to 174 classes and\ninput lengths from 2K to 50K tokens. Our benchmark requires LLMs to comprehend\nthe entire input to recognize the massive label spaces to make correct\npredictions. We evaluate on 15 long-context LLMs and find that they perform\nwell on less challenging classification tasks with smaller label space and\nshorter demonstrations. However, they struggle with more challenging task like\nDiscovery with 174 labels, suggesting a gap in their ability to process long,\ncontext-rich sequences. Further analysis reveals a bias towards labels\npresented later in the sequence and a need for improved reasoning over multiple\npieces of information. Our study reveals that long context understanding and\nreasoning is still a challenging task for the existing LLMs. We believe\nLongICLBench could serve as a more realistic evaluation for the future\nlong-context LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02060v3",
    "published_date": "2024-04-02 15:59:11 UTC",
    "updated_date": "2024-06-12 02:46:16 UTC"
  },
  {
    "arxiv_id": "2404.02047v3",
    "title": "Learning Transactions Representations for Information Management in Banks: Mastering Local, Global, and External Knowledge",
    "authors": [
      "Alexandra Bazarova",
      "Maria Kovaleva",
      "Ilya Kuleshov",
      "Evgenia Romanenkova",
      "Alexander Stepikin",
      "Alexandr Yugay",
      "Dzhambulat Mollaev",
      "Ivan Kireev",
      "Andrey Savchenko",
      "Alexey Zaytsev"
    ],
    "abstract": "In today's world, banks use artificial intelligence to optimize diverse\nbusiness processes, aiming to improve customer experience. Most of the\ncustomer-related tasks can be categorized into two groups: 1) local ones, which\nfocus on a client's current state, such as transaction forecasting, and 2)\nglobal ones, which consider the general customer behaviour, e.g., predicting\nsuccessful loan repayment. Unfortunately, maintaining separate models for each\ntask is costly. Therefore, to better facilitate information management, we\ncompared eight state-of-the-art unsupervised methods on 11 tasks in search for\na one-size-fits-all solution. Contrastive self-supervised learning methods were\ndemonstrated to excel at global problems, while generative techniques were\nsuperior at local tasks. We also introduced a novel approach, which enriches\nthe client's representation by incorporating external information gathered from\nother clients. Our method outperforms classical models, boosting accuracy by up\nto 20\\%.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02047v3",
    "published_date": "2024-04-02 15:39:14 UTC",
    "updated_date": "2025-02-03 15:33:55 UTC"
  },
  {
    "arxiv_id": "2404.02043v2",
    "title": "Cross-lingual Text Classification Transfer: The Case of Ukrainian",
    "authors": [
      "Daryna Dementieva",
      "Valeriia Khylenko",
      "Georg Groh"
    ],
    "abstract": "Despite the extensive amount of labeled datasets in the NLP text\nclassification field, the persistent imbalance in data availability across\nvarious languages remains evident. To support further fair development of NLP\nmodels, exploring the possibilities of effective knowledge transfer to new\nlanguages is crucial. Ukrainian, in particular, stands as a language that still\ncan benefit from the continued refinement of cross-lingual methodologies. Due\nto our knowledge, there is a tremendous lack of Ukrainian corpora for typical\ntext classification tasks, i.e., different types of style, or harmful speech,\nor texts relationships. However, the amount of resources required for such\ncorpora collection from scratch is understandable. In this work, we leverage\nthe state-of-the-art advances in NLP, exploring cross-lingual knowledge\ntransfer methods avoiding manual data curation: large multilingual encoders and\ntranslation systems, LLMs, and language adapters. We test the approaches on\nthree text classification tasks -- toxicity classification, formality\nclassification, and natural language inference (NLI) -- providing the\n``recipe'' for the optimal setups for each task.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "COLING2025, main, short",
    "pdf_url": "http://arxiv.org/pdf/2404.02043v2",
    "published_date": "2024-04-02 15:37:09 UTC",
    "updated_date": "2025-02-04 20:08:08 UTC"
  },
  {
    "arxiv_id": "2404.02039v2",
    "title": "A Survey on Large Language Model-Based Game Agents",
    "authors": [
      "Sihao Hu",
      "Tiansheng Huang",
      "Gaowen Liu",
      "Ramana Rao Kompella",
      "Fatih Ilhan",
      "Selim Furkan Tekin",
      "Yichang Xu",
      "Zachary Yahn",
      "Ling Liu"
    ],
    "abstract": "The development of game agents holds a critical role in advancing towards\nArtificial General Intelligence. The progress of Large Language Models (LLMs)\noffers an unprecedented opportunity to evolve and empower game agents with\nhuman-like decision-making capabilities in complex computer game environments.\nThis paper provides a comprehensive overview of LLM-based game agents from a\nholistic viewpoint. First, we introduce the conceptual architecture of\nLLM-based game agents, centered around three core functional components:\nmemory, reasoning and in/output. Second, we survey existing representative\nLLM-based game agents documented in the literature with respect to\nmethodologies and adaptation agility across six genres of games, including\nadventure, communication, competition, cooperation, simulation, and crafting &\nexploration games. Finally, we present an outlook of future research and\ndevelopment directions in this burgeoning field. A curated list of relevant\npapers is maintained and made accessible at:\nhttps://github.com/git-disl/awesome-LLM-game-agent-papers.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02039v2",
    "published_date": "2024-04-02 15:34:18 UTC",
    "updated_date": "2025-03-30 18:42:36 UTC"
  },
  {
    "arxiv_id": "2404.02037v1",
    "title": "MultiParaDetox: Extending Text Detoxification with Parallel Data to New Languages",
    "authors": [
      "Daryna Dementieva",
      "Nikolay Babakov",
      "Alexander Panchenko"
    ],
    "abstract": "Text detoxification is a textual style transfer (TST) task where a text is\nparaphrased from a toxic surface form, e.g. featuring rude words, to the\nneutral register. Recently, text detoxification methods found their\napplications in various task such as detoxification of Large Language Models\n(LLMs) (Leong et al., 2023; He et al., 2024; Tang et al., 2023) and toxic\nspeech combating in social networks (Deng et al., 2023; Mun et al., 2023;\nAgarwal et al., 2023). All these applications are extremely important to ensure\nsafe communication in modern digital worlds. However, the previous approaches\nfor parallel text detoxification corpora collection -- ParaDetox (Logacheva et\nal., 2022) and APPADIA (Atwell et al., 2022) -- were explored only in\nmonolingual setup. In this work, we aim to extend ParaDetox pipeline to\nmultiple languages presenting MultiParaDetox to automate parallel\ndetoxification corpus collection for potentially any language. Then, we\nexperiment with different text detoxification models -- from unsupervised\nbaselines to LLMs and fine-tuned models on the presented parallel corpora --\nshowing the great benefit of parallel corpus presence to obtain\nstate-of-the-art text detoxification models for any language.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to NAACL2024",
    "pdf_url": "http://arxiv.org/pdf/2404.02037v1",
    "published_date": "2024-04-02 15:32:32 UTC",
    "updated_date": "2024-04-02 15:32:32 UTC"
  },
  {
    "arxiv_id": "2404.02018v2",
    "title": "Large Language Models for Orchestrating Bimanual Robots",
    "authors": [
      "Kun Chu",
      "Xufeng Zhao",
      "Cornelius Weber",
      "Mengdi Li",
      "Wenhao Lu",
      "Stefan Wermter"
    ],
    "abstract": "Although there has been rapid progress in endowing robots with the ability to\nsolve complex manipulation tasks, generating control policies for bimanual\nrobots to solve tasks involving two hands is still challenging because of the\ndifficulties in effective temporal and spatial coordination. With emergent\nabilities in terms of step-by-step reasoning and in-context learning, Large\nLanguage Models (LLMs) have demonstrated promising potential in a variety of\nrobotic tasks. However, the nature of language communication via a single\nsequence of discrete symbols makes LLM-based coordination in continuous space a\nparticular challenge for bimanual tasks. To tackle this challenge, we present\nLAnguage-model-based Bimanual ORchestration (LABOR), an agent utilizing an LLM\nto analyze task configurations and devise coordination control policies for\naddressing long-horizon bimanual tasks. We evaluate our method through\nsimulated experiments involving two classes of long-horizon tasks using the\nNICOL humanoid robot. Our results demonstrate that our method outperforms the\nbaseline in terms of success rate. Additionally, we thoroughly analyze failure\ncases, offering insights into LLM-based approaches in bimanual robotic control\nand revealing future research trends. The project website can be found at\nhttp://labor-agent.github.io.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted in Humanoids 2024. The project website can be found at\n  http://labor-agent.github.io",
    "pdf_url": "http://arxiv.org/pdf/2404.02018v2",
    "published_date": "2024-04-02 15:08:35 UTC",
    "updated_date": "2024-10-10 15:07:42 UTC"
  },
  {
    "arxiv_id": "2406.11852v2",
    "title": "Risks from Language Models for Automated Mental Healthcare: Ethics and Structure for Implementation",
    "authors": [
      "Declan Grabb",
      "Max Lamparth",
      "Nina Vasan"
    ],
    "abstract": "Amidst the growing interest in developing task-autonomous AI for automated\nmental health care, this paper addresses the ethical and practical challenges\nassociated with the issue and proposes a structured framework that delineates\nlevels of autonomy, outlines ethical requirements, and defines beneficial\ndefault behaviors for AI agents in the context of mental health support. We\nalso evaluate fourteen state-of-the-art language models (ten off-the-shelf,\nfour fine-tuned) using 16 mental health-related questionnaires designed to\nreflect various mental health conditions, such as psychosis, mania, depression,\nsuicidal thoughts, and homicidal tendencies. The questionnaire design and\nresponse evaluations were conducted by mental health clinicians (M.D.s). We\nfind that existing language models are insufficient to match the standard\nprovided by human professionals who can navigate nuances and appreciate\ncontext. This is due to a range of issues, including overly cautious or\nsycophantic responses and the absence of necessary safeguards. Alarmingly, we\nfind that most of the tested models could cause harm if accessed in mental\nhealth emergencies, failing to protect users and potentially exacerbating\nexisting symptoms. We explore solutions to enhance the safety of current\nmodels. Before the release of increasingly task-autonomous AI systems in mental\nhealth, it is crucial to ensure that these models can reliably detect and\nmanage symptoms of common psychiatric disorders to prevent harm to users. This\ninvolves aligning with the ethical framework and default behaviors outlined in\nour study. We contend that model developers are responsible for refining their\nsystems per these guidelines to safeguard against the risks posed by current AI\ntechnologies to user mental health and safety.\n  Trigger warning: Contains and discusses examples of sensitive mental health\ntopics, including suicide and self-harm.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "Updated with fine-tuned model results to match CoLM accepted\n  camera-ready version",
    "pdf_url": "http://arxiv.org/pdf/2406.11852v2",
    "published_date": "2024-04-02 15:05:06 UTC",
    "updated_date": "2024-08-14 18:20:22 UTC"
  },
  {
    "arxiv_id": "2404.01986v1",
    "title": "Predicting the Intention to Interact with a Service Robot:the Role of Gaze Cues",
    "authors": [
      "Simone Arreghini",
      "Gabriele Abbate",
      "Alessandro Giusti",
      "Antonio Paolillo"
    ],
    "abstract": "For a service robot, it is crucial to perceive as early as possible that an\napproaching person intends to interact: in this case, it can proactively enact\nfriendly behaviors that lead to an improved user experience. We solve this\nperception task with a sequence-to-sequence classifier of a potential user\nintention to interact, which can be trained in a self-supervised way. Our main\ncontribution is a study of the benefit of features representing the person's\ngaze in this context. Extensive experiments on a novel dataset show that the\ninclusion of gaze cues significantly improves the classifier performance (AUROC\nincreases from 84.5% to 91.2%); the distance at which an accurate\nclassification can be achieved improves from 2.4 m to 3.2 m. We also quantify\nthe system's ability to adapt to new environments without external supervision.\nQualitative experiments show practical applications with a waiter robot.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.01986v1",
    "published_date": "2024-04-02 14:22:54 UTC",
    "updated_date": "2024-04-02 14:22:54 UTC"
  },
  {
    "arxiv_id": "2404.01976v1",
    "title": "Joint-Task Regularization for Partially Labeled Multi-Task Learning",
    "authors": [
      "Kento Nishi",
      "Junsik Kim",
      "Wanhua Li",
      "Hanspeter Pfister"
    ],
    "abstract": "Multi-task learning has become increasingly popular in the machine learning\nfield, but its practicality is hindered by the need for large, labeled\ndatasets. Most multi-task learning methods depend on fully labeled datasets\nwherein each input example is accompanied by ground-truth labels for all target\ntasks. Unfortunately, curating such datasets can be prohibitively expensive and\nimpractical, especially for dense prediction tasks which require per-pixel\nlabels for each image. With this in mind, we propose Joint-Task Regularization\n(JTR), an intuitive technique which leverages cross-task relations to\nsimultaneously regularize all tasks in a single joint-task latent space to\nimprove learning when data is not fully labeled for all tasks. JTR stands out\nfrom existing approaches in that it regularizes all tasks jointly rather than\nseparately in pairs -- therefore, it achieves linear complexity relative to the\nnumber of tasks while previous methods scale quadratically. To demonstrate the\nvalidity of our approach, we extensively benchmark our method across a wide\nvariety of partially labeled scenarios based on NYU-v2, Cityscapes, and\nTaskonomy.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted paper to CVPR 2024 (main conference)",
    "pdf_url": "http://arxiv.org/pdf/2404.01976v1",
    "published_date": "2024-04-02 14:16:59 UTC",
    "updated_date": "2024-04-02 14:16:59 UTC"
  },
  {
    "arxiv_id": "2404.01965v2",
    "title": "Towards Leveraging AutoML for Sustainable Deep Learning: A Multi-Objective HPO Approach on Deep Shift Neural Networks",
    "authors": [
      "Leona Hennig",
      "Tanja Tornede",
      "Marius Lindauer"
    ],
    "abstract": "Deep Learning (DL) has advanced various fields by extracting complex patterns\nfrom large datasets. However, the computational demands of DL models pose\nenvironmental and resource challenges. Deep shift neural networks (DSNNs) offer\na solution by leveraging shift operations to reduce computational complexity at\ninference. Following the insights from standard DNNs, we are interested in\nleveraging the full potential of DSNNs by means of AutoML techniques. We study\nthe impact of hyperparameter optimization (HPO) to maximize DSNN performance\nwhile minimizing resource consumption. Since this combines multi-objective (MO)\noptimization with accuracy and energy consumption as potentially complementary\nobjectives, we propose to combine state-of-the-art multi-fidelity (MF) HPO with\nmulti-objective optimization. Experimental results demonstrate the\neffectiveness of our approach, resulting in models with over 80\\% in accuracy\nand low computational cost. Overall, our method accelerates efficient model\ndevelopment while enabling sustainable AI applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.01965v2",
    "published_date": "2024-04-02 14:03:37 UTC",
    "updated_date": "2024-04-04 10:54:04 UTC"
  },
  {
    "arxiv_id": "2404.01954v2",
    "title": "HyperCLOVA X Technical Report",
    "authors": [
      "Kang Min Yoo",
      "Jaegeun Han",
      "Sookyo In",
      "Heewon Jeon",
      "Jisu Jeong",
      "Jaewook Kang",
      "Hyunwook Kim",
      "Kyung-Min Kim",
      "Munhyong Kim",
      "Sungju Kim",
      "Donghyun Kwak",
      "Hanock Kwak",
      "Se Jung Kwon",
      "Bado Lee",
      "Dongsoo Lee",
      "Gichang Lee",
      "Jooho Lee",
      "Baeseong Park",
      "Seongjin Shin",
      "Joonsang Yu",
      "Seolki Baek",
      "Sumin Byeon",
      "Eungsup Cho",
      "Dooseok Choe",
      "Jeesung Han",
      "Youngkyun Jin",
      "Hyein Jun",
      "Jaeseung Jung",
      "Chanwoong Kim",
      "Jinhong Kim",
      "Jinuk Kim",
      "Dokyeong Lee",
      "Dongwook Park",
      "Jeong Min Sohn",
      "Sujung Han",
      "Jiae Heo",
      "Sungju Hong",
      "Mina Jeon",
      "Hyunhoon Jung",
      "Jungeun Jung",
      "Wangkyo Jung",
      "Chungjoon Kim",
      "Hyeri Kim",
      "Jonghyun Kim",
      "Min Young Kim",
      "Soeun Lee",
      "Joonhee Park",
      "Jieun Shin",
      "Sojin Yang",
      "Jungsoon Yoon",
      "Hwaran Lee",
      "Sanghwan Bae",
      "Jeehwan Cha",
      "Karl Gylleus",
      "Donghoon Ham",
      "Mihak Hong",
      "Youngki Hong",
      "Yunki Hong",
      "Dahyun Jang",
      "Hyojun Jeon",
      "Yujin Jeon",
      "Yeji Jeong",
      "Myunggeun Ji",
      "Yeguk Jin",
      "Chansong Jo",
      "Shinyoung Joo",
      "Seunghwan Jung",
      "Adrian Jungmyung Kim",
      "Byoung Hoon Kim",
      "Hyomin Kim",
      "Jungwhan Kim",
      "Minkyoung Kim",
      "Minseung Kim",
      "Sungdong Kim",
      "Yonghee Kim",
      "Youngjun Kim",
      "Youngkwan Kim",
      "Donghyeon Ko",
      "Dughyun Lee",
      "Ha Young Lee",
      "Jaehong Lee",
      "Jieun Lee",
      "Jonghyun Lee",
      "Jongjin Lee",
      "Min Young Lee",
      "Yehbin Lee",
      "Taehong Min",
      "Yuri Min",
      "Kiyoon Moon",
      "Hyangnam Oh",
      "Jaesun Park",
      "Kyuyon Park",
      "Younghun Park",
      "Hanbae Seo",
      "Seunghyun Seo",
      "Mihyun Sim",
      "Gyubin Son",
      "Matt Yeo",
      "Kyung Hoon Yeom",
      "Wonjoon Yoo",
      "Myungin You",
      "Doheon Ahn",
      "Homin Ahn",
      "Joohee Ahn",
      "Seongmin Ahn",
      "Chanwoo An",
      "Hyeryun An",
      "Junho An",
      "Sang-Min An",
      "Boram Byun",
      "Eunbin Byun",
      "Jongho Cha",
      "Minji Chang",
      "Seunggyu Chang",
      "Haesong Cho",
      "Youngdo Cho",
      "Dalnim Choi",
      "Daseul Choi",
      "Hyoseok Choi",
      "Minseong Choi",
      "Sangho Choi",
      "Seongjae Choi",
      "Wooyong Choi",
      "Sewhan Chun",
      "Dong Young Go",
      "Chiheon Ham",
      "Danbi Han",
      "Jaemin Han",
      "Moonyoung Hong",
      "Sung Bum Hong",
      "Dong-Hyun Hwang",
      "Seongchan Hwang",
      "Jinbae Im",
      "Hyuk Jin Jang",
      "Jaehyung Jang",
      "Jaeni Jang",
      "Sihyeon Jang",
      "Sungwon Jang",
      "Joonha Jeon",
      "Daun Jeong",
      "Joonhyun Jeong",
      "Kyeongseok Jeong",
      "Mini Jeong",
      "Sol Jin",
      "Hanbyeol Jo",
      "Hanju Jo",
      "Minjung Jo",
      "Chaeyoon Jung",
      "Hyungsik Jung",
      "Jaeuk Jung",
      "Ju Hwan Jung",
      "Kwangsun Jung",
      "Seungjae Jung",
      "Soonwon Ka",
      "Donghan Kang",
      "Soyoung Kang",
      "Taeho Kil",
      "Areum Kim",
      "Beomyoung Kim",
      "Byeongwook Kim",
      "Daehee Kim",
      "Dong-Gyun Kim",
      "Donggook Kim",
      "Donghyun Kim",
      "Euna Kim",
      "Eunchul Kim",
      "Geewook Kim",
      "Gyu Ri Kim",
      "Hanbyul Kim",
      "Heesu Kim",
      "Isaac Kim",
      "Jeonghoon Kim",
      "Jihye Kim",
      "Joonghoon Kim",
      "Minjae Kim",
      "Minsub Kim",
      "Pil Hwan Kim",
      "Sammy Kim",
      "Seokhun Kim",
      "Seonghyeon Kim",
      "Soojin Kim",
      "Soong Kim",
      "Soyoon Kim",
      "Sunyoung Kim",
      "Taeho Kim",
      "Wonho Kim",
      "Yoonsik Kim",
      "You Jin Kim",
      "Yuri Kim",
      "Beomseok Kwon",
      "Ohsung Kwon",
      "Yoo-Hwan Kwon",
      "Anna Lee",
      "Byungwook Lee",
      "Changho Lee",
      "Daun Lee",
      "Dongjae Lee",
      "Ha-Ram Lee",
      "Hodong Lee",
      "Hwiyeong Lee",
      "Hyunmi Lee",
      "Injae Lee",
      "Jaeung Lee",
      "Jeongsang Lee",
      "Jisoo Lee",
      "Jongsoo Lee",
      "Joongjae Lee",
      "Juhan Lee",
      "Jung Hyun Lee",
      "Junghoon Lee",
      "Junwoo Lee",
      "Se Yun Lee",
      "Sujin Lee",
      "Sungjae Lee",
      "Sungwoo Lee",
      "Wonjae Lee",
      "Zoo Hyun Lee",
      "Jong Kun Lim",
      "Kun Lim",
      "Taemin Lim",
      "Nuri Na",
      "Jeongyeon Nam",
      "Kyeong-Min Nam",
      "Yeonseog Noh",
      "Biro Oh",
      "Jung-Sik Oh",
      "Solgil Oh",
      "Yeontaek Oh",
      "Boyoun Park",
      "Cheonbok Park",
      "Dongju Park",
      "Hyeonjin Park",
      "Hyun Tae Park",
      "Hyunjung Park",
      "Jihye Park",
      "Jooseok Park",
      "Junghwan Park",
      "Jungsoo Park",
      "Miru Park",
      "Sang Hee Park",
      "Seunghyun Park",
      "Soyoung Park",
      "Taerim Park",
      "Wonkyeong Park",
      "Hyunjoon Ryu",
      "Jeonghun Ryu",
      "Nahyeon Ryu",
      "Soonshin Seo",
      "Suk Min Seo",
      "Yoonjeong Shim",
      "Kyuyong Shin",
      "Wonkwang Shin",
      "Hyun Sim",
      "Woongseob Sim",
      "Hyejin Soh",
      "Bokyong Son",
      "Hyunjun Son",
      "Seulah Son",
      "Chi-Yun Song",
      "Chiyoung Song",
      "Ka Yeon Song",
      "Minchul Song",
      "Seungmin Song",
      "Jisung Wang",
      "Yonggoo Yeo",
      "Myeong Yeon Yi",
      "Moon Bin Yim",
      "Taehwan Yoo",
      "Youngjoon Yoo",
      "Sungmin Yoon",
      "Young Jin Yoon",
      "Hangyeol Yu",
      "Ui Seon Yu",
      "Xingdong Zuo",
      "Jeongin Bae",
      "Joungeun Bae",
      "Hyunsoo Cho",
      "Seonghyun Cho",
      "Yongjin Cho",
      "Taekyoon Choi",
      "Yera Choi",
      "Jiwan Chung",
      "Zhenghui Han",
      "Byeongho Heo",
      "Euisuk Hong",
      "Taebaek Hwang",
      "Seonyeol Im",
      "Sumin Jegal",
      "Sumin Jeon",
      "Yelim Jeong",
      "Yonghyun Jeong",
      "Can Jiang",
      "Juyong Jiang",
      "Jiho Jin",
      "Ara Jo",
      "Younghyun Jo",
      "Hoyoun Jung",
      "Juyoung Jung",
      "Seunghyeong Kang",
      "Dae Hee Kim",
      "Ginam Kim",
      "Hangyeol Kim",
      "Heeseung Kim",
      "Hyojin Kim",
      "Hyojun Kim",
      "Hyun-Ah Kim",
      "Jeehye Kim",
      "Jin-Hwa Kim",
      "Jiseon Kim",
      "Jonghak Kim",
      "Jung Yoon Kim",
      "Rak Yeong Kim",
      "Seongjin Kim",
      "Seoyoon Kim",
      "Sewon Kim",
      "Sooyoung Kim",
      "Sukyoung Kim",
      "Taeyong Kim",
      "Naeun Ko",
      "Bonseung Koo",
      "Heeyoung Kwak",
      "Haena Kwon",
      "Youngjin Kwon",
      "Boram Lee",
      "Bruce W. Lee",
      "Dagyeong Lee",
      "Erin Lee",
      "Euijin Lee",
      "Ha Gyeong Lee",
      "Hyojin Lee",
      "Hyunjeong Lee",
      "Jeeyoon Lee",
      "Jeonghyun Lee",
      "Jongheok Lee",
      "Joonhyung Lee",
      "Junhyuk Lee",
      "Mingu Lee",
      "Nayeon Lee",
      "Sangkyu Lee",
      "Se Young Lee",
      "Seulgi Lee",
      "Seung Jin Lee",
      "Suhyeon Lee",
      "Yeonjae Lee",
      "Yesol Lee",
      "Youngbeom Lee",
      "Yujin Lee",
      "Shaodong Li",
      "Tianyu Liu",
      "Seong-Eun Moon",
      "Taehong Moon",
      "Max-Lasse Nihlenramstroem",
      "Wonseok Oh",
      "Yuri Oh",
      "Hongbeen Park",
      "Hyekyung Park",
      "Jaeho Park",
      "Nohil Park",
      "Sangjin Park",
      "Jiwon Ryu",
      "Miru Ryu",
      "Simo Ryu",
      "Ahreum Seo",
      "Hee Seo",
      "Kangdeok Seo",
      "Jamin Shin",
      "Seungyoun Shin",
      "Heetae Sin",
      "Jiangping Wang",
      "Lei Wang",
      "Ning Xiang",
      "Longxiang Xiao",
      "Jing Xu",
      "Seonyeong Yi",
      "Haanju Yoo",
      "Haneul Yoo",
      "Hwanhee Yoo",
      "Liang Yu",
      "Youngjae Yu",
      "Weijie Yuan",
      "Bo Zeng",
      "Qian Zhou",
      "Kyunghyun Cho",
      "Jung-Woo Ha",
      "Joonsuk Park",
      "Jihyun Hwang",
      "Hyoung Jo Kwon",
      "Soonyong Kwon",
      "Jungyeon Lee",
      "Seungho Lee",
      "Seonghyeon Lim",
      "Hyunkyung Noh",
      "Seungho Choi",
      "Sang-Woo Lee",
      "Jung Hwa Lim",
      "Nako Sung"
    ],
    "abstract": "We introduce HyperCLOVA X, a family of large language models (LLMs) tailored\nto the Korean language and culture, along with competitive capabilities in\nEnglish, math, and coding. HyperCLOVA X was trained on a balanced mix of\nKorean, English, and code data, followed by instruction-tuning with\nhigh-quality human-annotated datasets while abiding by strict safety guidelines\nreflecting our commitment to responsible AI. The model is evaluated across\nvarious benchmarks, including comprehensive reasoning, knowledge, commonsense,\nfactuality, coding, math, chatting, instruction-following, and harmlessness, in\nboth Korean and English. HyperCLOVA X exhibits strong reasoning capabilities in\nKorean backed by a deep understanding of the language and cultural nuances.\nFurther analysis of the inherent bilingual nature and its extension to\nmultilingualism highlights the model's cross-lingual proficiency and strong\ngeneralization ability to untargeted languages, including machine translation\nbetween several language pairs and cross-lingual inference tasks. We believe\nthat HyperCLOVA X can provide helpful guidance for regions or countries in\ndeveloping their sovereign LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "44 pages; updated authors list and fixed author names",
    "pdf_url": "http://arxiv.org/pdf/2404.01954v2",
    "published_date": "2024-04-02 13:48:49 UTC",
    "updated_date": "2024-04-13 15:06:19 UTC"
  },
  {
    "arxiv_id": "2404.02183v1",
    "title": "Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization",
    "authors": [
      "Yoichi Ishibashi",
      "Yoshimasa Nishimura"
    ],
    "abstract": "Recent advancements in automatic code generation using large language model\n(LLM) agent have brought us closer to the future of automated software\ndevelopment. However, existing single-agent approaches face limitations in\ngenerating and improving large-scale, complex codebases due to constraints in\ncontext length. To tackle this challenge, we propose Self-Organized multi-Agent\nframework (SoA), a novel multi-agent framework that enables the scalable and\nefficient generation and optimization of large-scale code. In SoA,\nself-organized agents operate independently to generate and modify code\ncomponents while seamlessly collaborating to construct the overall codebase. A\nkey feature of our framework is the automatic multiplication of agents based on\nproblem complexity, allowing for dynamic scalability. This enables the overall\ncode volume to be increased indefinitely according to the number of agents,\nwhile the amount of code managed by each agent remains constant. We evaluate\nSoA on the HumanEval benchmark and demonstrate that, compared to a single-agent\nsystem, each agent in SoA handles significantly less code, yet the overall\ngenerated code is substantially greater. Moreover, SoA surpasses the powerful\nsingle-agent baseline by 5% in terms of Pass@1 accuracy.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02183v1",
    "published_date": "2024-04-02 13:37:28 UTC",
    "updated_date": "2024-04-02 13:37:28 UTC"
  },
  {
    "arxiv_id": "2404.01925v1",
    "title": "Improving Bird's Eye View Semantic Segmentation by Task Decomposition",
    "authors": [
      "Tianhao Zhao",
      "Yongcan Chen",
      "Yu Wu",
      "Tianyang Liu",
      "Bo Du",
      "Peilun Xiao",
      "Shi Qiu",
      "Hongda Yang",
      "Guozhen Li",
      "Yi Yang",
      "Yutian Lin"
    ],
    "abstract": "Semantic segmentation in bird's eye view (BEV) plays a crucial role in\nautonomous driving. Previous methods usually follow an end-to-end pipeline,\ndirectly predicting the BEV segmentation map from monocular RGB inputs.\nHowever, the challenge arises when the RGB inputs and BEV targets from distinct\nperspectives, making the direct point-to-point predicting hard to optimize. In\nthis paper, we decompose the original BEV segmentation task into two stages,\nnamely BEV map reconstruction and RGB-BEV feature alignment. In the first\nstage, we train a BEV autoencoder to reconstruct the BEV segmentation maps\ngiven corrupted noisy latent representation, which urges the decoder to learn\nfundamental knowledge of typical BEV patterns. The second stage involves\nmapping RGB input images into the BEV latent space of the first stage, directly\noptimizing the correlations between the two views at the feature level. Our\napproach simplifies the complexity of combining perception and generation into\ndistinct steps, equipping the model to handle intricate and challenging scenes\neffectively. Besides, we propose to transform the BEV segmentation map from the\nCartesian to the polar coordinate system to establish the column-wise\ncorrespondence between RGB images and BEV maps. Moreover, our method requires\nneither multi-scale features nor camera intrinsic parameters for depth\nestimation and saves computational overhead. Extensive experiments on nuScenes\nand Argoverse show the effectiveness and efficiency of our method. Code is\navailable at https://github.com/happytianhao/TaDe.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by CVPR 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.01925v1",
    "published_date": "2024-04-02 13:19:45 UTC",
    "updated_date": "2024-04-02 13:19:45 UTC"
  },
  {
    "arxiv_id": "2404.01923v1",
    "title": "SGSH: Stimulate Large Language Models with Skeleton Heuristics for Knowledge Base Question Generation",
    "authors": [
      "Shasha Guo",
      "Lizi Liao",
      "Jing Zhang",
      "Yanling Wang",
      "Cuiping Li",
      "Hong Chen"
    ],
    "abstract": "Knowledge base question generation (KBQG) aims to generate natural language\nquestions from a set of triplet facts extracted from KB. Existing methods have\nsignificantly boosted the performance of KBQG via pre-trained language models\n(PLMs) thanks to the richly endowed semantic knowledge. With the advance of\npre-training techniques, large language models (LLMs) (e.g., GPT-3.5)\nundoubtedly possess much more semantic knowledge. Therefore, how to effectively\norganize and exploit the abundant knowledge for KBQG becomes the focus of our\nstudy. In this work, we propose SGSH--a simple and effective framework to\nStimulate GPT-3.5 with Skeleton Heuristics to enhance KBQG. The framework\nincorporates \"skeleton heuristics\", which provides more fine-grained guidance\nassociated with each input to stimulate LLMs to generate optimal questions,\nencompassing essential elements like the question phrase and the auxiliary\nverb.More specifically, we devise an automatic data construction strategy\nleveraging ChatGPT to construct a skeleton training dataset, based on which we\nemploy a soft prompting approach to train a BART model dedicated to generating\nthe skeleton associated with each input. Subsequently, skeleton heuristics are\nencoded into the prompt to incentivize GPT-3.5 to generate desired questions.\nExtensive experiments demonstrate that SGSH derives the new state-of-the-art\nperformance on the KBQG tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by NAACL 2024 Findings",
    "pdf_url": "http://arxiv.org/pdf/2404.01923v1",
    "published_date": "2024-04-02 13:17:36 UTC",
    "updated_date": "2024-04-02 13:17:36 UTC"
  },
  {
    "arxiv_id": "2404.01914v1",
    "title": "SCANNER: Knowledge-Enhanced Approach for Robust Multi-modal Named Entity Recognition of Unseen Entities",
    "authors": [
      "Hyunjong Ok",
      "Taeho Kil",
      "Sukmin Seo",
      "Jaeho Lee"
    ],
    "abstract": "Recent advances in named entity recognition (NER) have pushed the boundary of\nthe task to incorporate visual signals, leading to many variants, including\nmulti-modal NER (MNER) or grounded MNER (GMNER). A key challenge to these tasks\nis that the model should be able to generalize to the entities unseen during\nthe training, and should be able to handle the training samples with noisy\nannotations. To address this obstacle, we propose SCANNER (Span CANdidate\ndetection and recognition for NER), a model capable of effectively handling all\nthree NER variants. SCANNER is a two-stage structure; we extract entity\ncandidates in the first stage and use it as a query to get knowledge,\neffectively pulling knowledge from various sources. We can boost our\nperformance by utilizing this entity-centric extracted knowledge to address\nunseen entities. Furthermore, to tackle the challenges arising from noisy\nannotations in NER datasets, we introduce a novel self-distillation method,\nenhancing the robustness and accuracy of our model in processing training data\nwith inherent uncertainties. Our approach demonstrates competitive performance\non the NER benchmark and surpasses existing methods on both MNER and GMNER\nbenchmarks. Further analysis shows that the proposed distillation and knowledge\nutilization methods improve the performance of our model on various benchmarks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "13 pages, 7 figures, NAACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.01914v1",
    "published_date": "2024-04-02 13:05:41 UTC",
    "updated_date": "2024-04-02 13:05:41 UTC"
  },
  {
    "arxiv_id": "2404.04279v1",
    "title": "When Abel Kills Cain: What Machine Translation Cannot Capture",
    "authors": [
      "Aurélien Bénel",
      "Joris Falip",
      "Philippe Lacour"
    ],
    "abstract": "The article aims at identifying what, from a structural point of view, AI\nbased automatic translators cannot fully capture. It focuses on the machine's\nmistakes, in order to try to explain its causes. The biblical story of Ca\\\"in\nand Abel has been chosen because of its rich interpretive and critical\ntradition, but also because of its semantic difficulty. The investigation\nbegins with the observation, for the translation of this text, of the language\npairs and interfaces offered by the best known machine translation services\n(Google Translate, DeepL). A typology of the most frequent translation errors\nis then established. Finally, contemporary translations are compared, in order\nto underline the unique contribution of each. In conclusion, the article\nsuggests a revision of translation theory and, corArtificial Intelligence,\nTranslation, Limitations, Interpretation, Comparison, Unicityelatively, a\nreformulation of its technology concerning cultural texts.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "in French language",
    "pdf_url": "http://arxiv.org/pdf/2404.04279v1",
    "published_date": "2024-04-02 12:46:00 UTC",
    "updated_date": "2024-04-02 12:46:00 UTC"
  },
  {
    "arxiv_id": "2404.02181v1",
    "title": "Leveraging Machine Learning for Early Autism Detection via INDT-ASD Indian Database",
    "authors": [
      "Trapti Shrivastava",
      "Harshal Chaudhari",
      "Vrijendra Singh"
    ],
    "abstract": "Machine learning (ML) has advanced quickly, particularly throughout the area\nof health care. The diagnosis of neurodevelopment problems using ML is a very\nimportant area of healthcare. Autism spectrum disorder (ASD) is one of the\ndevelopmental disorders that is growing the fastest globally. The clinical\nscreening tests used to identify autistic symptoms are expensive and\ntime-consuming. But now that ML has been advanced, it's feasible to identify\nautism early on. Previously, many different techniques have been used in\ninvestigations. Still, none of them have produced the anticipated outcomes when\nit comes to the capacity to predict autistic features utilizing a clinically\nvalidated Indian ASD database. Therefore, this study aimed to develop a simple,\nquick, and inexpensive technique for identifying ASD by using ML. Various\nmachine learning classifiers, including Adaboost (AB), Gradient Boost (GB),\nDecision Tree (DT), Logistic Regression (LR), Random Forest (RF), Gaussian\nNaive Bayes (GNB), Linear Discriminant Analysis (LDA), Quadratic Discriminant\nAnalysis (QDA), K-Nearest Neighbors (KNN), and Support Vector Machine (SVM),\nwere used to develop the autism prediction model. The proposed method was\ntested with records from the AIIMS Modified INDT-ASD (AMI) database, which were\ncollected through an application developed by AIIMS in Delhi, India. Feature\nengineering has been applied to make the proposed solution easier than already\navailable solutions. Using the proposed model, we succeeded in predicting ASD\nusing a minimized set of 20 questions rather than the 28 questions presented in\nAMI with promising accuracy. In a comparative evaluation, SVM emerged as the\nsuperior model among others, with 100 $\\pm$ 0.05\\% accuracy, higher recall by\n5.34\\%, and improved accuracy by 2.22\\%-6.67\\% over RF. We have also introduced\na web-based solution supporting both Hindi and English.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02181v1",
    "published_date": "2024-04-02 12:44:51 UTC",
    "updated_date": "2024-04-02 12:44:51 UTC"
  },
  {
    "arxiv_id": "2404.01897v1",
    "title": "Continuous Spiking Graph Neural Networks",
    "authors": [
      "Nan Yin",
      "Mengzhu Wan",
      "Li Shen",
      "Hitesh Laxmichand Patel",
      "Baopu Li",
      "Bin Gu",
      "Huan Xiong"
    ],
    "abstract": "Continuous graph neural networks (CGNNs) have garnered significant attention\ndue to their ability to generalize existing discrete graph neural networks\n(GNNs) by introducing continuous dynamics. They typically draw inspiration from\ndiffusion-based methods to introduce a novel propagation scheme, which is\nanalyzed using ordinary differential equations (ODE). However, the\nimplementation of CGNNs requires significant computational power, making them\nchallenging to deploy on battery-powered devices. Inspired by recent spiking\nneural networks (SNNs), which emulate a biological inference process and\nprovide an energy-efficient neural architecture, we incorporate the SNNs with\nCGNNs in a unified framework, named Continuous Spiking Graph Neural Networks\n(COS-GNN). We employ SNNs for graph node representation at each time step,\nwhich are further integrated into the ODE process along with time. To enhance\ninformation preservation and mitigate information loss in SNNs, we introduce\nthe high-order structure of COS-GNN, which utilizes the second-order ODE for\nspiking representation and continuous propagation. Moreover, we provide the\ntheoretical proof that COS-GNN effectively mitigates the issues of exploding\nand vanishing gradients, enabling us to capture long-range dependencies between\nnodes. Experimental results on graph-based learning tasks demonstrate the\neffectiveness of the proposed COS-GNN over competitive baselines.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.01897v1",
    "published_date": "2024-04-02 12:36:40 UTC",
    "updated_date": "2024-04-02 12:36:40 UTC"
  },
  {
    "arxiv_id": "2404.01889v3",
    "title": "RAVE: Residual Vector Embedding for CLIP-Guided Backlit Image Enhancement",
    "authors": [
      "Tatiana Gaintseva",
      "Martin Benning",
      "Gregory Slabaugh"
    ],
    "abstract": "In this paper we propose a novel modification of Contrastive Language-Image\nPre-Training (CLIP) guidance for the task of unsupervised backlit image\nenhancement. Our work builds on the state-of-the-art CLIP-LIT approach, which\nlearns a prompt pair by constraining the text-image similarity between a prompt\n(negative/positive sample) and a corresponding image (backlit image/well-lit\nimage) in the CLIP embedding space. Learned prompts then guide an image\nenhancement network. Based on the CLIP-LIT framework, we propose two novel\nmethods for CLIP guidance. First, we show that instead of tuning prompts in the\nspace of text embeddings, it is possible to directly tune their embeddings in\nthe latent space without any loss in quality. This accelerates training and\npotentially enables the use of additional encoders that do not have a text\nencoder. Second, we propose a novel approach that does not require any prompt\ntuning. Instead, based on CLIP embeddings of backlit and well-lit images from\ntraining data, we compute the residual vector in the embedding space as a\nsimple difference between the mean embeddings of the well-lit and backlit\nimages. This vector then guides the enhancement network during training,\npushing a backlit image towards the space of well-lit images. This approach\nfurther dramatically reduces training time, stabilizes training and produces\nhigh quality enhanced images without artifacts, both in supervised and\nunsupervised training regimes. Additionally, we show that residual vectors can\nbe interpreted, revealing biases in training data, and thereby enabling\npotential bias correction.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.01889v3",
    "published_date": "2024-04-02 12:28:40 UTC",
    "updated_date": "2024-07-20 22:57:08 UTC"
  },
  {
    "arxiv_id": "2404.01878v1",
    "title": "Real, fake and synthetic faces -- does the coin have three sides?",
    "authors": [
      "Shahzeb Naeem",
      "Ramzi Al-Sharawi",
      "Muhammad Riyyan Khan",
      "Usman Tariq",
      "Abhinav Dhall",
      "Hasan Al-Nashash"
    ],
    "abstract": "With the ever-growing power of generative artificial intelligence, deepfake\nand artificially generated (synthetic) media have continued to spread online,\nwhich creates various ethical and moral concerns regarding their usage. To\ntackle this, we thus present a novel exploration of the trends and patterns\nobserved in real, deepfake and synthetic facial images. The proposed analysis\nis done in two parts: firstly, we incorporate eight deep learning models and\nanalyze their performances in distinguishing between the three classes of\nimages. Next, we look to further delve into the similarities and differences\nbetween these three sets of images by investigating their image properties both\nin the context of the entire image as well as in the context of specific\nregions within the image. ANOVA test was also performed and provided further\nclarity amongst the patterns associated between the images of the three\nclasses. From our findings, we observe that the investigated deeplearning\nmodels found it easier to detect synthetic facial images, with the ViT Patch-16\nmodel performing best on this task with a class-averaged sensitivity,\nspecificity, precision, and accuracy of 97.37%, 98.69%, 97.48%, and 98.25%,\nrespectively. This observation was supported by further analysis of various\nimage properties. We saw noticeable differences across the three category of\nimages. This analysis can help us build better algorithms for facial image\ngeneration, and also shows that synthetic, deepfake and real face images are\nindeed three different classes.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.01878v1",
    "published_date": "2024-04-02 12:08:26 UTC",
    "updated_date": "2024-04-02 12:08:26 UTC"
  },
  {
    "arxiv_id": "2404.01869v2",
    "title": "Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models -- A Survey",
    "authors": [
      "Philipp Mondorf",
      "Barbara Plank"
    ],
    "abstract": "Large language models (LLMs) have recently shown impressive performance on\ntasks involving reasoning, leading to a lively debate on whether these models\npossess reasoning capabilities similar to humans. However, despite these\nsuccesses, the depth of LLMs' reasoning abilities remains uncertain. This\nuncertainty partly stems from the predominant focus on task performance,\nmeasured through shallow accuracy metrics, rather than a thorough investigation\nof the models' reasoning behavior. This paper seeks to address this gap by\nproviding a comprehensive review of studies that go beyond task accuracy,\noffering deeper insights into the models' reasoning processes. Furthermore, we\nsurvey prevalent methodologies to evaluate the reasoning behavior of LLMs,\nemphasizing current trends and efforts towards more nuanced reasoning analyses.\nOur review suggests that LLMs tend to rely on surface-level patterns and\ncorrelations in their training data, rather than on sophisticated reasoning\nabilities. Additionally, we identify the need for further research that\ndelineates the key differences between human and LLM-based reasoning. Through\nthis survey, we aim to shed light on the complex reasoning processes within\nLLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "COLM 2024, 27 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.01869v2",
    "published_date": "2024-04-02 11:46:31 UTC",
    "updated_date": "2024-08-06 11:58:53 UTC"
  },
  {
    "arxiv_id": "2404.01863v1",
    "title": "Confidence-aware Reward Optimization for Fine-tuning Text-to-Image Models",
    "authors": [
      "Kyuyoung Kim",
      "Jongheon Jeong",
      "Minyong An",
      "Mohammad Ghavamzadeh",
      "Krishnamurthy Dvijotham",
      "Jinwoo Shin",
      "Kimin Lee"
    ],
    "abstract": "Fine-tuning text-to-image models with reward functions trained on human\nfeedback data has proven effective for aligning model behavior with human\nintent. However, excessive optimization with such reward models, which serve as\nmere proxy objectives, can compromise the performance of fine-tuned models, a\nphenomenon known as reward overoptimization. To investigate this issue in\ndepth, we introduce the Text-Image Alignment Assessment (TIA2) benchmark, which\ncomprises a diverse collection of text prompts, images, and human annotations.\nOur evaluation of several state-of-the-art reward models on this benchmark\nreveals their frequent misalignment with human assessment. We empirically\ndemonstrate that overoptimization occurs notably when a poorly aligned reward\nmodel is used as the fine-tuning objective. To address this, we propose\nTextNorm, a simple method that enhances alignment based on a measure of reward\nmodel confidence estimated across a set of semantically contrastive text\nprompts. We demonstrate that incorporating the confidence-calibrated rewards in\nfine-tuning effectively reduces overoptimization, resulting in twice as many\nwins in human evaluation for text-image alignment compared against the baseline\nreward models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.01863v1",
    "published_date": "2024-04-02 11:40:38 UTC",
    "updated_date": "2024-04-02 11:40:38 UTC"
  },
  {
    "arxiv_id": "2404.01855v2",
    "title": "Where to Move Next: Zero-shot Generalization of LLMs for Next POI Recommendation",
    "authors": [
      "Shanshan Feng",
      "Haoming Lyu",
      "Caishun Chen",
      "Yew-Soon Ong"
    ],
    "abstract": "Next Point-of-interest (POI) recommendation provides valuable suggestions for\nusers to explore their surrounding environment. Existing studies rely on\nbuilding recommendation models from large-scale users' check-in data, which is\ntask-specific and needs extensive computational resources. Recently, the\npretrained large language models (LLMs) have achieved significant advancements\nin various NLP tasks and have also been investigated for recommendation\nscenarios. However, the generalization abilities of LLMs still are unexplored\nto address the next POI recommendations, where users' geographical movement\npatterns should be extracted. Although there are studies that leverage LLMs for\nnext-item recommendations, they fail to consider the geographical influence and\nsequential transitions. Hence, they cannot effectively solve the next POI\nrecommendation task. To this end, we design novel prompting strategies and\nconduct empirical studies to assess the capability of LLMs, e.g., ChatGPT, for\npredicting a user's next check-in. Specifically, we consider several essential\nfactors in human movement behaviors, including user geographical preference,\nspatial distance, and sequential transitions, and formulate the recommendation\ntask as a ranking problem. Through extensive experiments on two widely used\nreal-world datasets, we derive several key findings. Empirical evaluations\ndemonstrate that LLMs have promising zero-shot recommendation abilities and can\nprovide accurate and reasonable predictions. We also reveal that LLMs cannot\naccurately comprehend geographical context information and are sensitive to the\norder of presentation of candidate POIs, which shows the limitations of LLMs\nand necessitates further research on robust human mobility reasoning\nmechanisms.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.01855v2",
    "published_date": "2024-04-02 11:33:04 UTC",
    "updated_date": "2024-04-22 19:13:12 UTC"
  },
  {
    "arxiv_id": "2404.01849v1",
    "title": "EV2Gym: A Flexible V2G Simulator for EV Smart Charging Research and Benchmarking",
    "authors": [
      "Stavros Orfanoudakis",
      "Cesar Diaz-Londono",
      "Yunus E. Yılmaz",
      "Peter Palensky",
      "Pedro P. Vergara"
    ],
    "abstract": "As electric vehicle (EV) numbers rise, concerns about the capacity of current\ncharging and power grid infrastructure grow, necessitating the development of\nsmart charging solutions. While many smart charging simulators have been\ndeveloped in recent years, only a few support the development of Reinforcement\nLearning (RL) algorithms in the form of a Gym environment, and those that do\nusually lack depth in modeling Vehicle-to-Grid (V2G) scenarios. To address the\naforementioned issues, this paper introduces the EV2Gym, a realistic simulator\nplatform for the development and assessment of small and large-scale smart\ncharging algorithms within a standardized platform. The proposed simulator is\npopulated with comprehensive EV, charging station, power transformer, and EV\nbehavior models validated using real data. EV2Gym has a highly customizable\ninterface empowering users to choose from pre-designed case studies or craft\ntheir own customized scenarios to suit their specific requirements. Moreover,\nit incorporates a diverse array of RL, mathematical programming, and heuristic\nalgorithms to speed up the development and benchmarking of new solutions. By\noffering a unified and standardized platform, EV2Gym aims to provide\nresearchers and practitioners with a robust environment for advancing and\nassessing smart charging algorithms.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "10 pages, 9 figures, and 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2404.01849v1",
    "published_date": "2024-04-02 11:22:53 UTC",
    "updated_date": "2024-04-02 11:22:53 UTC"
  },
  {
    "arxiv_id": "2404.01833v3",
    "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack",
    "authors": [
      "Mark Russinovich",
      "Ahmed Salem",
      "Ronen Eldan"
    ],
    "abstract": "Large Language Models (LLMs) have risen significantly in popularity and are\nincreasingly being adopted across multiple applications. These LLMs are heavily\naligned to resist engaging in illegal or unethical topics as a means to avoid\ncontributing to responsible AI harms. However, a recent line of attacks, known\nas jailbreaks, seek to overcome this alignment. Intuitively, jailbreak attacks\naim to narrow the gap between what the model can do and what it is willing to\ndo. In this paper, we introduce a novel jailbreak attack called Crescendo.\nUnlike existing jailbreak methods, Crescendo is a simple multi-turn jailbreak\nthat interacts with the model in a seemingly benign manner. It begins with a\ngeneral prompt or question about the task at hand and then gradually escalates\nthe dialogue by referencing the model's replies progressively leading to a\nsuccessful jailbreak. We evaluate Crescendo on various public systems,\nincluding ChatGPT, Gemini Pro, Gemini-Ultra, LlaMA-2 70b and LlaMA-3 70b Chat,\nand Anthropic Chat. Our results demonstrate the strong efficacy of Crescendo,\nwith it achieving high attack success rates across all evaluated models and\ntasks. Furthermore, we present Crescendomation, a tool that automates the\nCrescendo attack and demonstrate its efficacy against state-of-the-art models\nthrough our evaluations. Crescendomation surpasses other state-of-the-art\njailbreaking techniques on the AdvBench subset dataset, achieving 29-61% higher\nperformance on GPT-4 and 49-71% on Gemini-Pro. Finally, we also demonstrate\nCrescendo's ability to jailbreak multimodal models.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted at USENIX Security 2025",
    "pdf_url": "http://arxiv.org/pdf/2404.01833v3",
    "published_date": "2024-04-02 10:45:49 UTC",
    "updated_date": "2025-02-26 13:41:41 UTC"
  },
  {
    "arxiv_id": "2404.01828v1",
    "title": "Defense without Forgetting: Continual Adversarial Defense with Anisotropic & Isotropic Pseudo Replay",
    "authors": [
      "Yuhang Zhou",
      "Zhongyun Hua"
    ],
    "abstract": "Deep neural networks have demonstrated susceptibility to adversarial attacks.\nAdversarial defense techniques often focus on one-shot setting to maintain\nrobustness against attack. However, new attacks can emerge in sequences in\nreal-world deployment scenarios. As a result, it is crucial for a defense model\nto constantly adapt to new attacks, but the adaptation process can lead to\ncatastrophic forgetting of previously defended against attacks. In this paper,\nwe discuss for the first time the concept of continual adversarial defense\nunder a sequence of attacks, and propose a lifelong defense baseline called\nAnisotropic \\& Isotropic Replay (AIR), which offers three advantages: (1)\nIsotropic replay ensures model consistency in the neighborhood distribution of\nnew data, indirectly aligning the output preference between old and new tasks.\n(2) Anisotropic replay enables the model to learn a compromise data manifold\nwith fresh mixed semantics for further replay constraints and potential future\nattacks. (3) A straightforward regularizer mitigates the 'plasticity-stability'\ntrade-off by aligning model output between new and old tasks. Experiment\nresults demonstrate that AIR can approximate or even exceed the empirical\nperformance upper bounds achieved by Joint Training.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.01828v1",
    "published_date": "2024-04-02 10:41:51 UTC",
    "updated_date": "2024-04-02 10:41:51 UTC"
  },
  {
    "arxiv_id": "2407.00815v1",
    "title": "A Deep Learning-based Pest Insect Monitoring System for Ultra-low Power Pocket-sized Drones",
    "authors": [
      "Luca Crupi",
      "Luca Butera",
      "Alberto Ferrante",
      "Daniele Palossi"
    ],
    "abstract": "Smart farming and precision agriculture represent game-changer technologies\nfor efficient and sustainable agribusiness. Miniaturized palm-sized drones can\nact as flexible smart sensors inspecting crops, looking for early signs of\npotential pest outbreaking. However, achieving such an ambitious goal requires\nhardware-software codesign to develop accurate deep learning (DL) detection\nmodels while keeping memory and computational needs under an ultra-tight\nbudget, i.e., a few MB on-chip memory and a few 100s mW power envelope. This\nwork presents a novel vertically integrated solution featuring two ultra-low\npower System-on-Chips (SoCs), i.e., the dual-core STM32H74 and a multi-core GWT\nGAP9, running two State-of-the-Art DL models for detecting the Popillia\njaponica bug. We fine-tune both models for our image-based detection task,\nquantize them in 8-bit integers, and deploy them on the two SoCs. On the\nSTM32H74, we deploy a FOMO-MobileNetV2 model, achieving a mean average\nprecision (mAP) of 0.66 and running at 16.1 frame/s within 498 mW. While on the\nGAP9 SoC, we deploy a more complex SSDLite-MobileNetV3, which scores an mAP of\n0.79 and peaks at 6.8 frame/s within 33 mW. Compared to a top-notch\nRetinaNet-ResNet101-FPN full-precision baseline, which requires 14.9x more\nmemory and 300x more operations per inference, our best model drops only 15\\%\nin mAP, paving the way toward autonomous palm-sized drones capable of\nlightweight and precise pest detection.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.00815v1",
    "published_date": "2024-04-02 10:39:54 UTC",
    "updated_date": "2024-04-02 10:39:54 UTC"
  },
  {
    "arxiv_id": "2404.01812v1",
    "title": "Uncertainty-aware Active Learning of NeRF-based Object Models for Robot Manipulators using Visual and Re-orientation Actions",
    "authors": [
      "Saptarshi Dasgupta",
      "Akshat Gupta",
      "Shreshth Tuli",
      "Rohan Paul"
    ],
    "abstract": "Manipulating unseen objects is challenging without a 3D representation, as\nobjects generally have occluded surfaces. This requires physical interaction\nwith objects to build their internal representations. This paper presents an\napproach that enables a robot to rapidly learn the complete 3D model of a given\nobject for manipulation in unfamiliar orientations. We use an ensemble of\npartially constructed NeRF models to quantify model uncertainty to determine\nthe next action (a visual or re-orientation action) by optimizing\ninformativeness and feasibility. Further, our approach determines when and how\nto grasp and re-orient an object given its partial NeRF model and re-estimates\nthe object pose to rectify misalignments introduced during the interaction.\nExperiments with a simulated Franka Emika Robot Manipulator operating in a\ntabletop environment with benchmark objects demonstrate an improvement of (i)\n14% in visual reconstruction quality (PSNR), (ii) 20% in the geometric/depth\nreconstruction of the object surface (F-score) and (iii) 71% in the task\nsuccess rate of manipulating objects a-priori unseen orientations/stable\nconfigurations in the scene; over current methods. The project page can be\nfound here: https://actnerf.github.io.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "This work has been submitted to the IEEE for possible publication",
    "pdf_url": "http://arxiv.org/pdf/2404.01812v1",
    "published_date": "2024-04-02 10:15:06 UTC",
    "updated_date": "2024-04-02 10:15:06 UTC"
  },
  {
    "arxiv_id": "2405.09529v2",
    "title": "Artificial Intelligence for the Internal Democracy of Political Parties",
    "authors": [
      "Claudio Novelli",
      "Giuliano Formisano",
      "Prathm Juneja",
      "Giulia Sandri",
      "Luciano Floridi"
    ],
    "abstract": "The article argues that AI can enhance the measurement and implementation of\ndemocratic processes within political parties, known as Intra-Party Democracy\n(IPD). It identifies the limitations of traditional methods for measuring IPD,\nwhich often rely on formal parameters, self-reported data, and tools like\nsurveys. Such limitations lead to the collection of partial data, rare updates,\nand significant demands on resources. To address these issues, the article\nsuggests that specific data management and Machine Learning (ML) techniques,\nsuch as natural language processing and sentiment analysis, can improve the\nmeasurement (ML about) and practice (ML for) of IPD. The article concludes by\nconsidering some of the principal risks of ML for IPD, including concerns over\ndata privacy, the potential for manipulation, and the dangers of overreliance\non technology.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.DB",
      "cs.LG",
      "cs.SI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.09529v2",
    "published_date": "2024-04-02 09:59:23 UTC",
    "updated_date": "2024-10-26 09:32:33 UTC"
  },
  {
    "arxiv_id": "2404.01794v1",
    "title": "Imitation Game: A Model-based and Imitation Learning Deep Reinforcement Learning Hybrid",
    "authors": [
      "Eric MSP Veith",
      "Torben Logemann",
      "Aleksandr Berezin",
      "Arlena Wellßow",
      "Stephan Balduin"
    ],
    "abstract": "Autonomous and learning systems based on Deep Reinforcement Learning have\nfirmly established themselves as a foundation for approaches to creating\nresilient and efficient Cyber-Physical Energy Systems. However, most current\napproaches suffer from two distinct problems: Modern model-free algorithms such\nas Soft Actor Critic need a high number of samples to learn a meaningful\npolicy, as well as a fallback to ward against concept drifts (e. g.,\ncatastrophic forgetting). In this paper, we present the work in progress\ntowards a hybrid agent architecture that combines model-based Deep\nReinforcement Learning with imitation learning to overcome both problems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted as publication at MSCPES '24",
    "pdf_url": "http://arxiv.org/pdf/2404.01794v1",
    "published_date": "2024-04-02 09:55:30 UTC",
    "updated_date": "2024-04-02 09:55:30 UTC"
  },
  {
    "arxiv_id": "2404.02929v2",
    "title": "Using Large Language Models to Understand Telecom Standards",
    "authors": [
      "Athanasios Karapantelakis",
      "Mukesh Thakur",
      "Alexandros Nikou",
      "Farnaz Moradi",
      "Christian Orlog",
      "Fitsum Gaim",
      "Henrik Holm",
      "Doumitrou Daniil Nimara",
      "Vincent Huang"
    ],
    "abstract": "The Third Generation Partnership Project (3GPP) has successfully introduced\nstandards for global mobility. However, the volume and complexity of these\nstandards has increased over time, thus complicating access to relevant\ninformation for vendors and service providers. Use of Generative Artificial\nIntelligence (AI) and in particular Large Language Models (LLMs), may provide\nfaster access to relevant information. In this paper, we evaluate the\ncapability of state-of-art LLMs to be used as Question Answering (QA)\nassistants for 3GPP document reference. Our contribution is threefold. First,\nwe provide a benchmark and measuring methods for evaluating performance of\nLLMs. Second, we do data preprocessing and fine-tuning for one of these LLMs\nand provide guidelines to increase accuracy of the responses that apply to all\nLLMs. Third, we provide a model of our own, TeleRoBERTa, that performs on-par\nwith foundation LLMs but with an order of magnitude less number of parameters.\nResults show that LLMs can be used as a credible reference tool on telecom\ntechnical documents, and thus have potential for a number of different\napplications from troubleshooting and maintenance, to network operations and\nsoftware product development.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ICMLCN 2024, Stockholm, May 2024. Updating typo in\n  authors list",
    "pdf_url": "http://arxiv.org/pdf/2404.02929v2",
    "published_date": "2024-04-02 09:54:51 UTC",
    "updated_date": "2024-04-12 09:08:30 UTC"
  },
  {
    "arxiv_id": "2404.02928v3",
    "title": "Jailbreaking Prompt Attack: A Controllable Adversarial Attack against Diffusion Models",
    "authors": [
      "Jiachen Ma",
      "Anda Cao",
      "Zhiqing Xiao",
      "Yijiang Li",
      "Jie Zhang",
      "Chao Ye",
      "Junbo Zhao"
    ],
    "abstract": "Text-to-image (T2I) models can be maliciously used to generate harmful\ncontent such as sexually explicit, unfaithful, and misleading or\nNot-Safe-for-Work (NSFW) images. Previous attacks largely depend on the\navailability of the diffusion model or involve a lengthy optimization process.\nIn this work, we investigate a more practical and universal attack that does\nnot require the presence of a target model and demonstrate that the\nhigh-dimensional text embedding space inherently contains NSFW concepts that\ncan be exploited to generate harmful images. We present the Jailbreaking Prompt\nAttack (JPA). JPA first searches for the target malicious concepts in the text\nembedding space using a group of antonyms generated by ChatGPT. Subsequently, a\nprefix prompt is optimized in the discrete vocabulary space to align malicious\nconcepts semantically in the text embedding space. We further introduce a soft\nassignment with gradient masking technique that allows us to perform gradient\nascent in the discrete vocabulary space.\n  We perform extensive experiments with open-sourced T2I models, e.g.\nstable-diffusion-v1-4 and closed-sourced online services, e.g. DALLE2,\nMidjourney with black-box safety checkers. Results show that (1) JPA bypasses\nboth text and image safety checkers (2) while preserving high semantic\nalignment with the target prompt. (3) JPA demonstrates a much faster speed than\nprevious methods and can be executed in a fully automated manner. These merits\nrender it a valuable tool for robustness evaluation in future text-to-image\ngeneration research.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02928v3",
    "published_date": "2024-04-02 09:49:35 UTC",
    "updated_date": "2024-09-04 06:40:12 UTC"
  },
  {
    "arxiv_id": "2404.01775v1",
    "title": "A noisy elephant in the room: Is your out-of-distribution detector robust to label noise?",
    "authors": [
      "Galadrielle Humblot-Renaux",
      "Sergio Escalera",
      "Thomas B. Moeslund"
    ],
    "abstract": "The ability to detect unfamiliar or unexpected images is essential for safe\ndeployment of computer vision systems. In the context of classification, the\ntask of detecting images outside of a model's training domain is known as\nout-of-distribution (OOD) detection. While there has been a growing research\ninterest in developing post-hoc OOD detection methods, there has been\ncomparably little discussion around how these methods perform when the\nunderlying classifier is not trained on a clean, carefully curated dataset. In\nthis work, we take a closer look at 20 state-of-the-art OOD detection methods\nin the (more realistic) scenario where the labels used to train the underlying\nclassifier are unreliable (e.g. crowd-sourced or web-scraped labels). Extensive\nexperiments across different datasets, noise types & levels, architectures and\ncheckpointing strategies provide insights into the effect of class label noise\non OOD detection, and show that poor separation between incorrectly classified\nID samples vs. OOD samples is an overlooked yet important limitation of\nexisting methods. Code: https://github.com/glhr/ood-labelnoise",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at CVPR 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.01775v1",
    "published_date": "2024-04-02 09:40:22 UTC",
    "updated_date": "2024-04-02 09:40:22 UTC"
  },
  {
    "arxiv_id": "2404.01768v2",
    "title": "Stereotype Detection in LLMs: A Multiclass, Explainable, and Benchmark-Driven Approach",
    "authors": [
      "Zekun Wu",
      "Sahan Bulathwela",
      "Maria Perez-Ortiz",
      "Adriano Soares Koshiyama"
    ],
    "abstract": "Stereotype detection is a challenging and subjective task, as certain\nstatements, such as \"Black people like to play basketball,\" may not appear\novertly toxic but still reinforce racial stereotypes. With the increasing\nprevalence of large language models (LLMs) in human-facing artificial\nintelligence (AI) applications, detecting these types of biases is essential.\nHowever, LLMs risk perpetuating and amplifying stereotypical outputs derived\nfrom their training data. A reliable stereotype detector is crucial for\nbenchmarking bias, monitoring model input and output, filtering training data,\nand ensuring fairer model behavior in downstream applications. This paper\nintroduces the Multi-Grain Stereotype (MGS) dataset, consisting of 51,867\ninstances across gender, race, profession, religion, and other stereotypes,\ncurated from multiple existing datasets. We evaluate various machine learning\napproaches to establish baselines and fine-tune language models of different\narchitectures and sizes, presenting a suite of stereotype multiclass\nclassifiers trained on the MGS dataset. Given the subjectivity of stereotypes,\nexplainability is essential to align model learning with human understanding of\nstereotypes. We employ explainable AI (XAI) tools, including SHAP, LIME, and\nBertViz, to assess whether the model's learned patterns align with human\nintuitions about stereotypes.Additionally, we develop stereotype elicitation\nprompts and benchmark the presence of stereotypes in text generation tasks\nusing popular LLMs, employing the best-performing stereotype classifiers.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Under review as a conference paper at ARR October 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.01768v2",
    "published_date": "2024-04-02 09:31:32 UTC",
    "updated_date": "2024-11-16 00:54:09 UTC"
  },
  {
    "arxiv_id": "2404.02180v4",
    "title": "Remote sensing framework for geological mapping via stacked autoencoders and clustering",
    "authors": [
      "Sandeep Nagar",
      "Ehsan Farahbakhsh",
      "Joseph Awange",
      "Rohitash Chandra"
    ],
    "abstract": "Supervised machine learning methods for geological mapping via remote sensing\nface limitations due to the scarcity of accurately labelled training data that\ncan be addressed by unsupervised learning, such as dimensionality reduction and\nclustering. Dimensionality reduction methods have the potential to play a\ncrucial role in improving the accuracy of geological maps. Although\nconventional dimensionality reduction methods may struggle with nonlinear data,\nunsupervised deep learning models such as autoencoders can model non-linear\nrelationships. Stacked autoencoders feature multiple interconnected layers to\ncapture hierarchical data representations useful for remote sensing data. We\npresent an unsupervised machine learning-based framework for processing remote\nsensing data using stacked autoencoders for dimensionality reduction and\nk-means clustering for mapping geological units. We use Landsat 8, ASTER, and\nSentinel-2 datasets to evaluate the framework for geological mapping of the\nMutawintji region in Western New South Wales, Australia. We also compare\nstacked autoencoders with principal component analysis (PCA) and canonical\nautoencoders. Our results reveal that the framework produces accurate and\ninterpretable geological maps, efficiently discriminating rock units. The\nresults reveal that the combination of stacked autoencoders with Sentinel-2\ndata yields the best performance accuracy when compared to other combinations.\nWe find that stacked autoencoders enable better extraction of complex and\nhierarchical representations of the input data when compared to canonical\nautoencoders and PCA. We also find that the generated maps align with prior\ngeological knowledge of the study area while providing novel insights into\ngeological structures.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02180v4",
    "published_date": "2024-04-02 09:15:32 UTC",
    "updated_date": "2024-09-21 06:02:47 UTC"
  },
  {
    "arxiv_id": "2404.01754v1",
    "title": "Peer-aided Repairer: Empowering Large Language Models to Repair Advanced Student Assignments",
    "authors": [
      "Qianhui Zhao",
      "Fang Liu",
      "Li Zhang",
      "Yang Liu",
      "Zhen Yan",
      "Zhenghao Chen",
      "Yufei Zhou",
      "Jing Jiang",
      "Ge Li"
    ],
    "abstract": "Automated generation of feedback on programming assignments holds significant\nbenefits for programming education, especially when it comes to advanced\nassignments. Automated Program Repair techniques, especially Large Language\nModel based approaches, have gained notable recognition for their potential to\nfix introductory assignments. However, the programs used for evaluation are\nrelatively simple. It remains unclear how existing approaches perform in\nrepairing programs from higher-level programming courses. To address these\nlimitations, we curate a new advanced student assignment dataset named\nDefects4DS from a higher-level programming course. Subsequently, we identify\nthe challenges related to fixing bugs in advanced assignments. Based on the\nanalysis, we develop a framework called PaR that is powered by the LLM. PaR\nworks in three phases: Peer Solution Selection, Multi-Source Prompt Generation,\nand Program Repair. Peer Solution Selection identifies the closely related peer\nprograms based on lexical, semantic, and syntactic criteria. Then Multi-Source\nPrompt Generation adeptly combines multiple sources of information to create a\ncomprehensive and informative prompt for the last Program Repair stage. The\nevaluation on Defects4DS and another well-investigated ITSP dataset reveals\nthat PaR achieves a new state-of-the-art performance, demonstrating impressive\nimprovements of 19.94% and 15.2% in repair rate compared to prior\nstate-of-the-art LLM- and symbolic-based approaches, respectively",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "On-going work",
    "pdf_url": "http://arxiv.org/pdf/2404.01754v1",
    "published_date": "2024-04-02 09:12:21 UTC",
    "updated_date": "2024-04-02 09:12:21 UTC"
  },
  {
    "arxiv_id": "2404.01752v3",
    "title": "Safe Interval RRT* for Scalable Multi-Robot Path Planning in Continuous Space",
    "authors": [
      "Joonyeol Sim",
      "Joonkyung Kim",
      "Changjoo Nam"
    ],
    "abstract": "In this paper, we consider the problem of Multi-Robot Path Planning (MRPP) in\ncontinuous space. The difficulty of the problem arises from the extremely large\nsearch space caused by the combinatorial nature of the problem and the\ncontinuous state space. We propose a two-level approach where the low level is\na sampling-based planner Safe Interval RRT* (SI-RRT*) that finds a\ncollision-free trajectory for individual robots. The high level can use any\nmethod that can resolve inter-robot conflicts where we employ two\nrepresentative methods that are Prioritized Planning (SI-CPP) and Conflict\nBased Search (SI-CCBS). Experimental results show that SI-RRT* can quickly find\na high-quality solution with a few samples. SI-CPP exhibits improved\nscalability while SI-CCBS produces higher-quality solutions compared to the\nstate-of-the-art planners for continuous space.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.01752v3",
    "published_date": "2024-04-02 09:07:12 UTC",
    "updated_date": "2025-02-11 13:46:36 UTC"
  },
  {
    "arxiv_id": "2404.01746v1",
    "title": "Towards Scalable & Efficient Interaction-Aware Planning in Autonomous Vehicles using Knowledge Distillation",
    "authors": [
      "Piyush Gupta",
      "David Isele",
      "Sangjae Bae"
    ],
    "abstract": "Real-world driving involves intricate interactions among vehicles navigating\nthrough dense traffic scenarios. Recent research focuses on enhancing the\ninteraction awareness of autonomous vehicles to leverage these interactions in\ndecision-making. These interaction-aware planners rely on neural-network-based\nprediction models to capture inter-vehicle interactions, aiming to integrate\nthese predictions with traditional control techniques such as Model Predictive\nControl. However, this integration of deep learning-based models with\ntraditional control paradigms often results in computationally demanding\noptimization problems, relying on heuristic methods. This study introduces a\nprincipled and efficient method for combining deep learning with constrained\noptimization, employing knowledge distillation to train smaller and more\nefficient networks, thereby mitigating complexity. We demonstrate that these\nrefined networks maintain the problem-solving efficacy of larger models while\nsignificantly accelerating optimization. Specifically, in the domain of\ninteraction-aware trajectory planning for autonomous vehicles, we illustrate\nthat training a smaller prediction network using knowledge distillation speeds\nup optimization without sacrificing accuracy.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.01746v1",
    "published_date": "2024-04-02 09:04:06 UTC",
    "updated_date": "2024-04-02 09:04:06 UTC"
  },
  {
    "arxiv_id": "2404.01745v1",
    "title": "Unleash the Potential of CLIP for Video Highlight Detection",
    "authors": [
      "Donghoon Han",
      "Seunghyeon Seo",
      "Eunhwan Park",
      "Seong-Uk Nam",
      "Nojun Kwak"
    ],
    "abstract": "Multimodal and large language models (LLMs) have revolutionized the\nutilization of open-world knowledge, unlocking novel potentials across various\ntasks and applications. Among these domains, the video domain has notably\nbenefited from their capabilities. In this paper, we present Highlight-CLIP\n(HL-CLIP), a method designed to excel in the video highlight detection task by\nleveraging the pre-trained knowledge embedded in multimodal models. By simply\nfine-tuning the multimodal encoder in combination with our innovative saliency\npooling technique, we have achieved the state-of-the-art performance in the\nhighlight detection task, the QVHighlight Benchmark, to the best of our\nknowledge.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.01745v1",
    "published_date": "2024-04-02 09:01:58 UTC",
    "updated_date": "2024-04-02 09:01:58 UTC"
  },
  {
    "arxiv_id": "2404.01741v5",
    "title": "Intrusion Tolerance for Networked Systems through Two-Level Feedback Control",
    "authors": [
      "Kim Hammar",
      "Rolf Stadler"
    ],
    "abstract": "We formulate intrusion tolerance for a system with service replicas as a\ntwo-level optimal control problem. On the local level node controllers perform\nintrusion recovery, and on the global level a system controller manages the\nreplication factor. The local and global control problems can be formulated as\nclassical problems in operations research, namely, the machine replacement\nproblem and the inventory replenishment problem. Based on this formulation, we\ndesign TOLERANCE, a novel control architecture for intrusion-tolerant systems.\nWe prove that the optimal control strategies on both levels have threshold\nstructure and design efficient algorithms for computing them. We implement and\nevaluate TOLERANCE in an emulation environment where we run 10 types of network\nintrusions. The results show that TOLERANCE can improve service availability\nand reduce operational cost compared with state-of-the-art intrusion-tolerant\nsystems.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.CR",
      "cs.GT",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.DC",
    "comment": "Preprint to appear in the conference proceedings of the 54th\n  IEEE/IFIP Dependable Systems and Networks Conference (DSN'24)",
    "pdf_url": "http://arxiv.org/pdf/2404.01741v5",
    "published_date": "2024-04-02 09:00:45 UTC",
    "updated_date": "2024-06-05 06:50:21 UTC"
  },
  {
    "arxiv_id": "2404.01740v1",
    "title": "Weakly-supervised Audio Separation via Bi-modal Semantic Similarity",
    "authors": [
      "Tanvir Mahmud",
      "Saeed Amizadeh",
      "Kazuhito Koishida",
      "Diana Marculescu"
    ],
    "abstract": "Conditional sound separation in multi-source audio mixtures without having\naccess to single source sound data during training is a long standing\nchallenge. Existing mix-and-separate based methods suffer from significant\nperformance drop with multi-source training mixtures due to the lack of\nsupervision signal for single source separation cases during training. However,\nin the case of language-conditional audio separation, we do have access to\ncorresponding text descriptions for each audio mixture in our training data,\nwhich can be seen as (rough) representations of the audio samples in the\nlanguage modality. To this end, in this paper, we propose a generic bi-modal\nseparation framework which can enhance the existing unsupervised frameworks to\nseparate single-source signals in a target modality (i.e., audio) using the\neasily separable corresponding signals in the conditioning modality (i.e.,\nlanguage), without having access to single-source samples in the target\nmodality during training. We empirically show that this is well within reach if\nwe have access to a pretrained joint embedding model between the two modalities\n(i.e., CLAP). Furthermore, we propose to incorporate our framework into two\nfundamental scenarios to enhance separation performance. First, we show that\nour proposed methodology significantly improves the performance of purely\nunsupervised baselines by reducing the distribution shift between training and\ntest samples. In particular, we show that our framework can achieve 71% boost\nin terms of Signal-to-Distortion Ratio (SDR) over the baseline, reaching 97.5%\nof the supervised learning performance. Second, we show that we can further\nimprove the performance of the supervised learning itself by 17% if we augment\nit by our proposed weakly-supervised framework, that enables a powerful\nsemi-supervised framework for audio separation.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Tech report. Accepted in ICLR-2024",
    "pdf_url": "http://arxiv.org/pdf/2404.01740v1",
    "published_date": "2024-04-02 08:59:58 UTC",
    "updated_date": "2024-04-02 08:59:58 UTC"
  },
  {
    "arxiv_id": "2404.01716v1",
    "title": "Effective internal language model training and fusion for factorized transducer model",
    "authors": [
      "Jinxi Guo",
      "Niko Moritz",
      "Yingyi Ma",
      "Frank Seide",
      "Chunyang Wu",
      "Jay Mahadeokar",
      "Ozlem Kalinli",
      "Christian Fuegen",
      "Mike Seltzer"
    ],
    "abstract": "The internal language model (ILM) of the neural transducer has been widely\nstudied. In most prior work, it is mainly used for estimating the ILM score and\nis subsequently subtracted during inference to facilitate improved integration\nwith external language models. Recently, various of factorized transducer\nmodels have been proposed, which explicitly embrace a standalone internal\nlanguage model for non-blank token prediction. However, even with the adoption\nof factorized transducer models, limited improvement has been observed compared\nto shallow fusion. In this paper, we propose a novel ILM training and decoding\nstrategy for factorized transducer models, which effectively combines the\nblank, acoustic and ILM scores. Our experiments show a 17% relative improvement\nover the standard decoding method when utilizing a well-trained ILM and the\nproposed decoding strategy on LibriSpeech datasets. Furthermore, when compared\nto a strong RNN-T baseline enhanced with external LM fusion, the proposed model\nyields a 5.5% relative improvement on general-sets and an 8.9% WER reduction\nfor rare words. The proposed model can achieve superior performance without\nrelying on external language models, rendering it highly efficient for\nproduction use-cases. To further improve the performance, we propose a novel\nand memory-efficient ILM-fusion-aware minimum word error rate (MWER) training\nmethod which improves ILM integration significantly.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted to ICASSP 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.01716v1",
    "published_date": "2024-04-02 08:01:05 UTC",
    "updated_date": "2024-04-02 08:01:05 UTC"
  },
  {
    "arxiv_id": "2404.01714v4",
    "title": "Conjugate-Gradient-like Based Adaptive Moment Estimation Optimization Algorithm for Deep Learning",
    "authors": [
      "Jiawu Tian",
      "Liwei Xu",
      "Xiaowei Zhang",
      "Yongqi Li"
    ],
    "abstract": "Training deep neural networks is a challenging task. In order to speed up\ntraining and enhance the performance of deep neural networks, we rectify the\nvanilla conjugate gradient as conjugate-gradient-like and incorporate it into\nthe generic Adam, and thus propose a new optimization algorithm named\nCG-like-Adam for deep learning. Specifically, both the first-order and the\nsecond-order moment estimation of generic Adam are replaced by the\nconjugate-gradient-like. Convergence analysis handles the cases where the\nexponential moving average coefficient of the first-order moment estimation is\nconstant and the first-order moment estimation is unbiased. Numerical\nexperiments show the superiority of the proposed algorithm based on the\nCIFAR10/100 dataset.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "32 pages, 13 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.01714v4",
    "published_date": "2024-04-02 07:57:17 UTC",
    "updated_date": "2025-01-08 06:52:07 UTC"
  },
  {
    "arxiv_id": "2404.01713v2",
    "title": "Generative AI for Immersive Communication: The Next Frontier in Internet-of-Senses Through 6G",
    "authors": [
      "Nassim Sehad",
      "Lina Bariah",
      "Wassim Hamidouche",
      "Hamed Hellaoui",
      "Riku Jäntti",
      "Mérouane Debbah"
    ],
    "abstract": "Over the past two decades, the Internet-of-Things (IoT) has become a\ntransformative concept, and as we approach 2030, a new paradigm known as the\nInternet of Senses (IoS) is emerging. Unlike conventional Virtual Reality (VR),\nIoS seeks to provide multi-sensory experiences, acknowledging that in our\nphysical reality, our perception extends far beyond just sight and sound; it\nencompasses a range of senses. This article explores the existing technologies\ndriving immersive multi-sensory media, delving into their capabilities and\npotential applications. This exploration includes a comparative analysis\nbetween conventional immersive media streaming and a proposed use case that\nleverages semantic communication empowered by generative Artificial\nIntelligence (AI). The focal point of this analysis is the substantial\nreduction in bandwidth consumption by 99.93% in the proposed scheme. Through\nthis comparison, we aim to underscore the practical applications of generative\nAI for immersive media. Concurrently addressing major challenges in this field,\nsuch as temporal synchronization of multiple media, ensuring high throughput,\nminimizing the End-to-End (E2E) latency, and robustness to low bandwidth while\noutlining future trajectories.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.MM",
      "cs.NI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.01713v2",
    "published_date": "2024-04-02 07:57:05 UTC",
    "updated_date": "2024-08-13 12:58:13 UTC"
  },
  {
    "arxiv_id": "2404.01712v4",
    "title": "Hessian-Free Online Certified Unlearning",
    "authors": [
      "Xinbao Qiao",
      "Meng Zhang",
      "Ming Tang",
      "Ermin Wei"
    ],
    "abstract": "Machine unlearning strives to uphold the data owners' right to be forgotten\nby enabling models to selectively forget specific data. Recent advances suggest\npre-computing and storing statistics extracted from second-order information\nand implementing unlearning through Newton-style updates. However, the Hessian\nmatrix operations are extremely costly and previous works conduct unlearning\nfor empirical risk minimizer with the convexity assumption, precluding their\napplicability to high-dimensional over-parameterized models and the\nnonconvergence condition. In this paper, we propose an efficient Hessian-free\nunlearning approach. The key idea is to maintain a statistical vector for each\ntraining data, computed through affine stochastic recursion of the difference\nbetween the retrained and learned models. We prove that our proposed method\noutperforms the state-of-the-art methods in terms of the unlearning and\ngeneralization guarantees, the deletion capacity, and the time/storage\ncomplexity, under the same regularity conditions. Through the strategy of\nrecollecting statistics for removing data, we develop an online unlearning\nalgorithm that achieves near-instantaneous data removal, as it requires only\nvector addition. Experiments demonstrate that our proposed scheme surpasses\nexisting results by orders of magnitude in terms of time/storage costs with\nmillisecond-level unlearning execution, while also enhancing test accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2404.01712v4",
    "published_date": "2024-04-02 07:54:18 UTC",
    "updated_date": "2025-02-06 13:23:15 UTC"
  },
  {
    "arxiv_id": "2404.01709v1",
    "title": "Upsample Guidance: Scale Up Diffusion Models without Training",
    "authors": [
      "Juno Hwang",
      "Yong-Hyun Park",
      "Junghyo Jo"
    ],
    "abstract": "Diffusion models have demonstrated superior performance across various\ngenerative tasks including images, videos, and audio. However, they encounter\ndifficulties in directly generating high-resolution samples. Previously\nproposed solutions to this issue involve modifying the architecture, further\ntraining, or partitioning the sampling process into multiple stages. These\nmethods have the limitation of not being able to directly utilize pre-trained\nmodels as-is, requiring additional work. In this paper, we introduce upsample\nguidance, a technique that adapts pretrained diffusion model (e.g., $512^2$) to\ngenerate higher-resolution images (e.g., $1536^2$) by adding only a single term\nin the sampling process. Remarkably, this technique does not necessitate any\nadditional training or relying on external models. We demonstrate that upsample\nguidance can be applied to various models, such as pixel-space, latent space,\nand video diffusion models. We also observed that the proper selection of\nguidance scale can improve image quality, fidelity, and prompt alignment.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "15 pages, 15 Figures",
    "pdf_url": "http://arxiv.org/pdf/2404.01709v1",
    "published_date": "2024-04-02 07:49:08 UTC",
    "updated_date": "2024-04-02 07:49:08 UTC"
  },
  {
    "arxiv_id": "2404.01685v3",
    "title": "SpiKernel: A Kernel Size Exploration Methodology for Improving Accuracy of the Embedded Spiking Neural Network Systems",
    "authors": [
      "Rachmad Vidya Wicaksana Putra",
      "Muhammad Shafique"
    ],
    "abstract": "Spiking Neural Networks (SNNs) can offer ultra-low power/energy consumption\nfor machine learning-based application tasks due to their sparse spike-based\noperations. Currently, most of the SNN architectures need a significantly\nlarger model size to achieve higher accuracy, which is not suitable for\nresource-constrained embedded applications. Therefore, developing SNNs that can\nachieve high accuracy with acceptable memory footprint is highly needed. Toward\nthis, we propose SpiKernel, a novel methodology that improves the accuracy of\nSNNs through kernel size exploration. Its key steps include (1) investigating\nthe impact of different kernel sizes on the accuracy, (2) devising new sets of\nkernel sizes, (3) generating SNN architectures using neural architecture search\nbased on the selected kernel sizes, and (4) analyzing the accuracy-memory\ntrade-offs for SNN model selection. The experimental results show that our\nSpiKernel achieves higher accuracy than state-of-the-art works (i.e., 93.24%\nfor CIFAR10, 70.84% for CIFAR100, and 62% for TinyImageNet) with less than 10M\nparameters and up to 4.8x speed-up of searching time, thereby making it\nsuitable for embedded applications.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "Accepted for publication at the IEEE Embedded Systems Letters",
    "pdf_url": "http://arxiv.org/pdf/2404.01685v3",
    "published_date": "2024-04-02 06:42:14 UTC",
    "updated_date": "2024-12-08 08:29:56 UTC"
  },
  {
    "arxiv_id": "2404.01677v2",
    "title": "Towards Generalizable and Faithful Logic Reasoning over Natural Language via Resolution Refutation",
    "authors": [
      "Zhouhao Sun",
      "Xiao Ding",
      "Li Du",
      "Bibo Cai",
      "Jinglong Gao",
      "Ting Liu",
      "Qin Bing"
    ],
    "abstract": "Large language models (LLMs) have achieved significant performance in various\nnatural language reasoning tasks. However, they still struggle with performing\nfirst-order logic reasoning over formal logical theories expressed in natural\nlanguage. This is because the previous LLMs-based reasoning systems have the\ntheoretical incompleteness issue. As a result, it can only address a limited\nset of simple reasoning problems, which significantly decreases their\ngeneralization ability. To address this issue, we propose a novel framework,\nnamed Generalizable and Faithful Reasoner (GFaiR), which introduces the\nparadigm of resolution refutation. Resolution refutation has the capability to\nsolve all first-order logic reasoning problems by extending reasoning rules and\nemploying the principle of proof by contradiction, so our system's completeness\ncan be improved by introducing resolution refutation. Experimental results\ndemonstrate that our system outperforms previous works by achieving\nstate-of-the-art performances in complex scenarios while maintaining\nperformances in simple scenarios. Besides, we observe that GFaiR is faithful to\nits reasoning process.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "LREC-Coling 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.01677v2",
    "published_date": "2024-04-02 06:28:44 UTC",
    "updated_date": "2024-04-03 09:28:31 UTC"
  },
  {
    "arxiv_id": "2404.01663v6",
    "title": "CMAT: A Multi-Agent Collaboration Tuning Framework for Enhancing Small Language Models",
    "authors": [
      "Xuechen Liang",
      "Yangfan He",
      "Meiling Tao",
      "Yinghui Xia",
      "Jianhui Wang",
      "Tianyu Shi",
      "Jun Wang",
      "JingSong Yang"
    ],
    "abstract": "Open large language models (LLMs) have significantly advanced the field of\nnatural language processing, showcasing impressive performance across various\ntasks.Despite the significant advancements in LLMs, their effective operation\nstill relies heavily on human input to accurately guide the dialogue flow, with\nagent tuning being a crucial optimization technique that involves human\nadjustments to the model for better response to such guidance.Addressing this\ndependency, our work introduces the TinyAgent model, trained on a meticulously\ncurated high-quality dataset. We also present the Collaborative Multi-Agent\nTuning (CMAT) framework, an innovative system designed to augment language\nagent capabilities through adaptive weight updates based on environmental\nfeedback. This framework fosters collaborative learning and real-time\nadaptation among multiple intelligent agents, enhancing their context-awareness\nand long-term memory. In this research, we propose a new communication agent\nframework that integrates multi-agent systems with environmental feedback\nmechanisms, offering a scalable method to explore cooperative behaviors.\nNotably, our TinyAgent-7B model exhibits performance on par with GPT-3.5,\ndespite having fewer parameters, signifying a substantial improvement in the\nefficiency and effectiveness of LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.01663v6",
    "published_date": "2024-04-02 06:07:35 UTC",
    "updated_date": "2025-04-15 15:28:28 UTC"
  },
  {
    "arxiv_id": "2404.01657v1",
    "title": "Release of Pre-Trained Models for the Japanese Language",
    "authors": [
      "Kei Sawada",
      "Tianyu Zhao",
      "Makoto Shing",
      "Kentaro Mitsui",
      "Akio Kaga",
      "Yukiya Hono",
      "Toshiaki Wakatsuki",
      "Koh Mitsuda"
    ],
    "abstract": "AI democratization aims to create a world in which the average person can\nutilize AI techniques. To achieve this goal, numerous research institutes have\nattempted to make their results accessible to the public. In particular, large\npre-trained models trained on large-scale data have shown unprecedented\npotential, and their release has had a significant impact. However, most of the\nreleased models specialize in the English language, and thus, AI\ndemocratization in non-English-speaking communities is lagging significantly.\nTo reduce this gap in AI access, we released Generative Pre-trained Transformer\n(GPT), Contrastive Language and Image Pre-training (CLIP), Stable Diffusion,\nand Hidden-unit Bidirectional Encoder Representations from Transformers\n(HuBERT) pre-trained in Japanese. By providing these models, users can freely\ninterface with AI that aligns with Japanese cultural values and ensures the\nidentity of Japanese culture, thus enhancing the democratization of AI.\nAdditionally, experiments showed that pre-trained models specialized for\nJapanese can efficiently achieve high performance in Japanese tasks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, 1 figure, 5 tables, accepted for LREC-COLING 2024. Models\n  are publicly available at https://huggingface.co/rinna",
    "pdf_url": "http://arxiv.org/pdf/2404.01657v1",
    "published_date": "2024-04-02 05:59:43 UTC",
    "updated_date": "2024-04-02 05:59:43 UTC"
  },
  {
    "arxiv_id": "2404.01654v1",
    "title": "AI WALKUP: A Computer-Vision Approach to Quantifying MDS-UPDRS in Parkinson's Disease",
    "authors": [
      "Xiang Xiang",
      "Zihan Zhang",
      "Jing Ma",
      "Yao Deng"
    ],
    "abstract": "Parkinson's Disease (PD) is the second most common neurodegenerative\ndisorder. The existing assessment method for PD is usually the Movement\nDisorder Society - Unified Parkinson's Disease Rating Scale (MDS-UPDRS) to\nassess the severity of various types of motor symptoms and disease progression.\nHowever, manual assessment suffers from high subjectivity, lack of consistency,\nand high cost and low efficiency of manual communication. We want to use a\ncomputer vision based solution to capture human pose images based on a camera,\nreconstruct and perform motion analysis using algorithms, and extract the\nfeatures of the amount of motion through feature engineering. The proposed\napproach can be deployed on different smartphones, and the video recording and\nartificial intelligence analysis can be done quickly and easily through our\nAPP.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV",
      "eess.SP"
    ],
    "primary_category": "cs.CV",
    "comment": "Technical report for AI WALKUP, an APP winning 3rd Prize of 2022 HUST\n  GS AI Innovation and Design Competition",
    "pdf_url": "http://arxiv.org/pdf/2404.01654v1",
    "published_date": "2024-04-02 05:53:34 UTC",
    "updated_date": "2024-04-02 05:53:34 UTC"
  },
  {
    "arxiv_id": "2404.01652v1",
    "title": "Towards Better Generalization in Open-Domain Question Answering by Mitigating Context Memorization",
    "authors": [
      "Zixuan Zhang",
      "Revanth Gangi Reddy",
      "Kevin Small",
      "Tong Zhang",
      "Heng Ji"
    ],
    "abstract": "Open-domain Question Answering (OpenQA) aims at answering factual questions\nwith an external large-scale knowledge corpus. However, real-world knowledge is\nnot static; it updates and evolves continually. Such a dynamic characteristic\nof knowledge poses a vital challenge for these models, as the trained models\nneed to constantly adapt to the latest information to make sure that the\nanswers remain accurate. In addition, it is still unclear how well an OpenQA\nmodel can transfer to completely new knowledge domains. In this paper, we\ninvestigate the generalization performance of a retrieval-augmented QA model in\ntwo specific scenarios: 1) adapting to updated versions of the same knowledge\ncorpus; 2) switching to completely different knowledge domains. We observe that\nthe generalization challenges of OpenQA models stem from the reader's\nover-reliance on memorizing the knowledge from the external corpus, which\nhinders the model from generalizing to a new knowledge corpus. We introduce\nCorpus-Invariant Tuning (CIT), a simple but effective training strategy, to\nmitigate the knowledge over-memorization by controlling the likelihood of\nretrieved contexts during training. Extensive experimental results on multiple\nOpenQA benchmarks show that CIT achieves significantly better generalizability\nwithout compromising the model's performance in its original corpus and domain.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to NAACL 2024 Findings",
    "pdf_url": "http://arxiv.org/pdf/2404.01652v1",
    "published_date": "2024-04-02 05:44:50 UTC",
    "updated_date": "2024-04-02 05:44:50 UTC"
  },
  {
    "arxiv_id": "2404.01636v1",
    "title": "Learning to Control Camera Exposure via Reinforcement Learning",
    "authors": [
      "Kyunghyun Lee",
      "Ukcheol Shin",
      "Byeong-Uk Lee"
    ],
    "abstract": "Adjusting camera exposure in arbitrary lighting conditions is the first step\nto ensure the functionality of computer vision applications. Poorly adjusted\ncamera exposure often leads to critical failure and performance degradation.\nTraditional camera exposure control methods require multiple convergence steps\nand time-consuming processes, making them unsuitable for dynamic lighting\nconditions. In this paper, we propose a new camera exposure control framework\nthat rapidly controls camera exposure while performing real-time processing by\nexploiting deep reinforcement learning. The proposed framework consists of four\ncontributions: 1) a simplified training ground to simulate real-world's diverse\nand dynamic lighting changes, 2) flickering and image attribute-aware reward\ndesign, along with lightweight state design for real-time processing, 3) a\nstatic-to-dynamic lighting curriculum to gradually improve the agent's\nexposure-adjusting capability, and 4) domain randomization techniques to\nalleviate the limitation of the training ground and achieve seamless\ngeneralization in the wild.As a result, our proposed method rapidly reaches a\ndesired exposure level within five steps with real-time processing (1 ms).\nAlso, the acquired images are well-exposed and show superiority in various\ncomputer vision tasks, such as feature extraction and object detection.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at CVPR 2024, *First two authors contributed equally to this\n  work. Project page link: https://sites.google.com/view/drl-ae",
    "pdf_url": "http://arxiv.org/pdf/2404.01636v1",
    "published_date": "2024-04-02 04:53:39 UTC",
    "updated_date": "2024-04-02 04:53:39 UTC"
  },
  {
    "arxiv_id": "2404.08567v1",
    "title": "CATP: Cross-Attention Token Pruning for Accuracy Preserved Multimodal Model Inference",
    "authors": [
      "Ruqi Liao",
      "Chuqing Zhao",
      "Jin Li",
      "Weiqi Feng"
    ],
    "abstract": "In response to the rising interest in large multimodal models, we introduce\nCross-Attention Token Pruning (CATP), a precision-focused token pruning method.\nOur approach leverages cross-attention layers in multimodal models, exemplified\nby BLIP-2, to extract valuable information for token importance determination.\nCATP employs a refined voting strategy across model heads and layers. In\nevaluations, CATP achieves up to 12.1X higher accuracy compared to existing\ntoken pruning methods, addressing the trade-off between computational\nefficiency and model precision.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.08567v1",
    "published_date": "2024-04-02 04:35:35 UTC",
    "updated_date": "2024-04-02 04:35:35 UTC"
  },
  {
    "arxiv_id": "2404.01628v1",
    "title": "Learning Equi-angular Representations for Online Continual Learning",
    "authors": [
      "Minhyuk Seo",
      "Hyunseo Koh",
      "Wonje Jeung",
      "Minjae Lee",
      "San Kim",
      "Hankook Lee",
      "Sungjun Cho",
      "Sungik Choi",
      "Hyunwoo Kim",
      "Jonghyun Choi"
    ],
    "abstract": "Online continual learning suffers from an underfitted solution due to\ninsufficient training for prompt model update (e.g., single-epoch training). To\naddress the challenge, we propose an efficient online continual learning method\nusing the neural collapse phenomenon. In particular, we induce neural collapse\nto form a simplex equiangular tight frame (ETF) structure in the representation\nspace so that the continuously learned model with a single epoch can better fit\nto the streamed data by proposing preparatory data training and residual\ncorrection in the representation space. With an extensive set of empirical\nvalidations using CIFAR-10/100, TinyImageNet, ImageNet-200, and ImageNet-1K, we\nshow that our proposed method outperforms state-of-the-art methods by a\nnoticeable margin in various online continual learning scenarios such as\ndisjoint and Gaussian scheduled continuous (i.e., boundary-free) data setups.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.01628v1",
    "published_date": "2024-04-02 04:29:01 UTC",
    "updated_date": "2024-04-02 04:29:01 UTC"
  },
  {
    "arxiv_id": "2404.01622v2",
    "title": "Gen4DS: Workshop on Data Storytelling in an Era of Generative AI",
    "authors": [
      "Xingyu Lan",
      "Leni Yang",
      "Zezhong Wang",
      "Yun Wang",
      "Danqing Shi",
      "Sheelagh Carpendale"
    ],
    "abstract": "Storytelling is an ancient and precious human ability that has been\nrejuvenated in the digital age. Over the last decade, there has been a notable\nsurge in the recognition and application of data storytelling, both in academia\nand industry. Recently, the rapid development of generative AI has brought new\nopportunities and challenges to this field, sparking numerous new questions.\nThese questions may not necessarily be quickly transformed into papers, but we\nbelieve it is necessary to promptly discuss them to help the community better\nclarify important issues and research agendas for the future. We thus invite\nyou to join our workshop (Gen4DS) to discuss questions such as: How can\ngenerative AI facilitate the creation of data stories? How might generative AI\nalter the workflow of data storytellers? What are the pitfalls and risks of\nincorporating AI in storytelling? We have designed both paper presentations and\ninteractive activities (including hands-on creation, group discussion pods, and\ndebates on controversial issues) for the workshop. We hope that participants\nwill learn about the latest advances and pioneering work in data storytelling,\nengage in critical conversations with each other, and have an enjoyable,\nunforgettable, and meaningful experience at the event.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.01622v2",
    "published_date": "2024-04-02 04:11:37 UTC",
    "updated_date": "2024-04-06 02:12:13 UTC"
  },
  {
    "arxiv_id": "2404.01620v3",
    "title": "Voice EHR: Introducing Multimodal Audio Data for Health",
    "authors": [
      "James Anibal",
      "Hannah Huth",
      "Ming Li",
      "Lindsey Hazen",
      "Veronica Daoud",
      "Dominique Ebedes",
      "Yen Minh Lam",
      "Hang Nguyen",
      "Phuc Hong",
      "Michael Kleinman",
      "Shelley Ost",
      "Christopher Jackson",
      "Laura Sprabery",
      "Cheran Elangovan",
      "Balaji Krishnaiah",
      "Lee Akst",
      "Ioan Lina",
      "Iqbal Elyazar",
      "Lenny Ekwati",
      "Stefan Jansen",
      "Richard Nduwayezu",
      "Charisse Garcia",
      "Jeffrey Plum",
      "Jacqueline Brenner",
      "Miranda Song",
      "Emily Ricotta",
      "David Clifton",
      "C. Louise Thwaites",
      "Yael Bensoussan",
      "Bradford Wood"
    ],
    "abstract": "Artificial intelligence (AI) models trained on audio data may have the\npotential to rapidly perform clinical tasks, enhancing medical decision-making\nand potentially improving outcomes through early detection. Existing\ntechnologies depend on limited datasets collected with expensive recording\nequipment in high-income countries, which challenges deployment in\nresource-constrained, high-volume settings where audio data may have a profound\nimpact on health equity. This report introduces a novel data type and a\ncorresponding collection system that captures health data through guided\nquestions using only a mobile/web application. The app facilitates the\ncollection of an audio electronic health record (Voice EHR) which may contain\ncomplex biomarkers of health from conventional voice/respiratory features,\nspeech patterns, and spoken language with semantic meaning and longitudinal\ncontext, potentially compensating for the typical limitations of unimodal\nclinical datasets. This report presents the application used for data\ncollection, initial experiments on data quality, and case studies which\ndemonstrate the potential of voice EHR to advance the scalability/diversity of\naudio AI.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CY",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "21 pages, 5 figures, 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2404.01620v3",
    "published_date": "2024-04-02 04:07:22 UTC",
    "updated_date": "2024-11-09 17:22:08 UTC"
  },
  {
    "arxiv_id": "2404.02179v1",
    "title": "Distributed and Rate-Adaptive Feature Compression",
    "authors": [
      "Aditya Deshmukh",
      "Venugopal V. Veeravalli",
      "Gunjan Verma"
    ],
    "abstract": "We study the problem of distributed and rate-adaptive feature compression for\nlinear regression. A set of distributed sensors collect disjoint features of\nregressor data. A fusion center is assumed to contain a pretrained linear\nregression model, trained on a dataset of the entire uncompressed data. At\ninference time, the sensors compress their observations and send them to the\nfusion center through communication-constrained channels, whose rates can\nchange with time. Our goal is to design a feature compression {scheme} that can\nadapt to the varying communication constraints, while maximizing the inference\nperformance at the fusion center. We first obtain the form of optimal\nquantizers assuming knowledge of underlying regressor data distribution. Under\na practically reasonable approximation, we then propose a distributed\ncompression scheme which works by quantizing a one-dimensional projection of\nthe sensor data. We also propose a simple adaptive scheme for handling changes\nin communication constraints. We demonstrate the effectiveness of the\ndistributed adaptive compression scheme through simulated experiments.",
    "categories": [
      "cs.IT",
      "cs.AI",
      "math.IT",
      "stat.ML"
    ],
    "primary_category": "cs.IT",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02179v1",
    "published_date": "2024-04-02 03:21:06 UTC",
    "updated_date": "2024-04-02 03:21:06 UTC"
  },
  {
    "arxiv_id": "2404.01602v2",
    "title": "Helmsman of the Masses? Evaluate the Opinion Leadership of Large Language Models in the Werewolf Game",
    "authors": [
      "Silin Du",
      "Xiaowei Zhang"
    ],
    "abstract": "Large language models (LLMs) have exhibited memorable strategic behaviors in\nsocial deductive games. However, the significance of opinion leadership\nexhibited by LLM-based agents has been largely overlooked, which is crucial for\npractical applications in multi-agent and human-AI interaction settings.\nOpinion leaders are individuals who have a noticeable impact on the beliefs and\nbehaviors of others within a social group. In this work, we employ the Werewolf\ngame as a simulation platform to assess the opinion leadership of LLMs. The\ngame includes the role of the Sheriff, tasked with summarizing arguments and\nrecommending decision options, and therefore serves as a credible proxy for an\nopinion leader. We develop a framework integrating the Sheriff role and devise\ntwo novel metrics based on the critical characteristics of opinion leaders. The\nfirst metric measures the reliability of the opinion leader, and the second\nassesses the influence of the opinion leader on other players' decisions. We\nconduct extensive experiments to evaluate LLMs of different scales. In\naddition, we collect a Werewolf question-answering dataset (WWQA) to assess and\nenhance LLM's grasp of the game rules, and we also incorporate human\nparticipants for further analysis. The results suggest that the Werewolf game\nis a suitable test bed to evaluate the opinion leadership of LLMs, and few LLMs\npossess the capacity for opinion leadership.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "Published as a conference paper at COLM 2024. 37 pages, 6 figures, 27\n  tables",
    "pdf_url": "http://arxiv.org/pdf/2404.01602v2",
    "published_date": "2024-04-02 02:46:18 UTC",
    "updated_date": "2024-08-29 08:49:14 UTC"
  },
  {
    "arxiv_id": "2404.01598v1",
    "title": "Extremum-Seeking Action Selection for Accelerating Policy Optimization",
    "authors": [
      "Ya-Chien Chang",
      "Sicun Gao"
    ],
    "abstract": "Reinforcement learning for control over continuous spaces typically uses\nhigh-entropy stochastic policies, such as Gaussian distributions, for local\nexploration and estimating policy gradient to optimize performance. Many\nrobotic control problems deal with complex unstable dynamics, where applying\nactions that are off the feasible control manifolds can quickly lead to\nundesirable divergence. In such cases, most samples taken from the ambient\naction space generate low-value trajectories that hardly contribute to policy\nimprovement, resulting in slow or failed learning. We propose to improve action\nselection in this model-free RL setting by introducing additional adaptive\ncontrol steps based on Extremum-Seeking Control (ESC). On each action sampled\nfrom stochastic policies, we apply sinusoidal perturbations and query for\nestimated Q-values as the response signal. Based on ESC, we then dynamically\nimprove the sampled actions to be closer to nearby optima before applying them\nto the environment. Our methods can be easily added in standard policy\noptimization to improve learning efficiency, which we demonstrate in various\ncontrol learning environments.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.01598v1",
    "published_date": "2024-04-02 02:39:17 UTC",
    "updated_date": "2024-04-02 02:39:17 UTC"
  },
  {
    "arxiv_id": "2404.01596v3",
    "title": "PhysORD: A Neuro-Symbolic Approach for Physics-infused Motion Prediction in Off-road Driving",
    "authors": [
      "Zhipeng Zhao",
      "Bowen Li",
      "Yi Du",
      "Taimeng Fu",
      "Chen Wang"
    ],
    "abstract": "Motion prediction is critical for autonomous off-road driving, however, it\npresents significantly more challenges than on-road driving because of the\ncomplex interaction between the vehicle and the terrain. Traditional\nphysics-based approaches encounter difficulties in accurately modeling dynamic\nsystems and external disturbance. In contrast, data-driven neural networks\nrequire extensive datasets and struggle with explicitly capturing the\nfundamental physical laws, which can easily lead to poor generalization. By\nmerging the advantages of both methods, neuro-symbolic approaches present a\npromising direction. These methods embed physical laws into neural models,\npotentially significantly improving generalization capabilities. However, no\nprior works were evaluated in real-world settings for off-road driving. To\nbridge this gap, we present PhysORD, a neural-symbolic approach integrating the\nconservation law, i.e., the Euler-Lagrange equation, into data-driven neural\nmodels for motion prediction in off-road driving. Our experiments showed that\nPhysORD can accurately predict vehicle motion and tolerate external disturbance\nby modeling uncertainties. The learned dynamics model achieves 46.7% higher\naccuracy using only 3.1% of the parameters compared to data-driven methods,\ndemonstrating the data efficiency and superior generalization ability of our\nneural-symbolic method.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.01596v3",
    "published_date": "2024-04-02 02:36:31 UTC",
    "updated_date": "2024-10-22 15:47:12 UTC"
  },
  {
    "arxiv_id": "2404.01589v1",
    "title": "Classifying Cancer Stage with Open-Source Clinical Large Language Models",
    "authors": [
      "Chia-Hsuan Chang",
      "Mary M. Lucas",
      "Grace Lu-Yao",
      "Christopher C. Yang"
    ],
    "abstract": "Cancer stage classification is important for making treatment and care\nmanagement plans for oncology patients. Information on staging is often\nincluded in unstructured form in clinical, pathology, radiology and other\nfree-text reports in the electronic health record system, requiring extensive\nwork to parse and obtain. To facilitate the extraction of this information,\nprevious NLP approaches rely on labeled training datasets, which are\nlabor-intensive to prepare. In this study, we demonstrate that without any\nlabeled training data, open-source clinical large language models (LLMs) can\nextract pathologic tumor-node-metastasis (pTNM) staging information from\nreal-world pathology reports. Our experiments compare LLMs and a BERT-based\nmodel fine-tuned using the labeled data. Our findings suggest that while LLMs\nstill exhibit subpar performance in Tumor (T) classification, with the\nappropriate adoption of prompting strategies, they can achieve comparable\nperformance on Metastasis (M) classification and improved performance on Node\n(N) classification.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "accepted in the IEEE International Conference on Healthcare\n  Informatics (IEEE ICHI 2024)",
    "pdf_url": "http://arxiv.org/pdf/2404.01589v1",
    "published_date": "2024-04-02 02:30:47 UTC",
    "updated_date": "2024-04-02 02:30:47 UTC"
  },
  {
    "arxiv_id": "2404.01588v1",
    "title": "Hallucination Diversity-Aware Active Learning for Text Summarization",
    "authors": [
      "Yu Xia",
      "Xu Liu",
      "Tong Yu",
      "Sungchul Kim",
      "Ryan A. Rossi",
      "Anup Rao",
      "Tung Mai",
      "Shuai Li"
    ],
    "abstract": "Large Language Models (LLMs) have shown propensity to generate hallucinated\noutputs, i.e., texts that are factually incorrect or unsupported. Existing\nmethods for alleviating hallucinations typically require costly human\nannotations to identify and correct hallucinations in LLM outputs. Moreover,\nmost of these methods focus on a specific type of hallucination, e.g., entity\nor token errors, which limits their effectiveness in addressing various types\nof hallucinations exhibited in LLM outputs. To our best knowledge, in this\npaper we propose the first active learning framework to alleviate LLM\nhallucinations, reducing costly human annotations of hallucination needed. By\nmeasuring fine-grained hallucinations from errors in semantic frame, discourse\nand content verifiability in text summarization, we propose HAllucination\nDiversity-Aware Sampling (HADAS) to select diverse hallucinations for\nannotations in active learning for LLM finetuning. Extensive experiments on\nthree datasets and different backbone models demonstrate advantages of our\nmethod in effectively and efficiently mitigating LLM hallucinations.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to NAACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.01588v1",
    "published_date": "2024-04-02 02:30:27 UTC",
    "updated_date": "2024-04-02 02:30:27 UTC"
  },
  {
    "arxiv_id": "2404.01569v2",
    "title": "Evaluating Large Language Models Using Contrast Sets: An Experimental Approach",
    "authors": [
      "Manish Sanwal"
    ],
    "abstract": "In the domain of Natural Language Inference (NLI), especially in tasks\ninvolving the classification of multiple input texts, the Cross-Entropy Loss\nmetric is widely employed as a standard for error measurement. However, this\nmetric falls short in effectively evaluating a model's capacity to understand\nlanguage entailments. In this study, we introduce an innovative technique for\ngenerating a contrast set for the Stanford Natural Language Inference (SNLI)\ndataset. Our strategy involves the automated substitution of verbs, adverbs,\nand adjectives with their synonyms to preserve the original meaning of\nsentences. This method aims to assess whether a model's performance is based on\ngenuine language comprehension or simply on pattern recognition. We conducted\nour analysis using the ELECTRA-small model. The model achieved an accuracy of\n89.9% on the conventional SNLI dataset but showed a reduced accuracy of 72.5%\non our contrast set, indicating a substantial 17% decline. This outcome led us\nto conduct a detailed examination of the model's learning behaviors. Following\nthis, we improved the model's resilience by fine-tuning it with a\ncontrast-enhanced training dataset specifically designed for SNLI, which\nincreased its accuracy to 85.5% on the contrast sets. Our findings highlight\nthe importance of incorporating diverse linguistic expressions into datasets\nfor NLI tasks. We hope that our research will encourage the creation of more\ninclusive datasets, thereby contributing to the development of NLI models that\nare both more sophisticated and effective.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.01569v2",
    "published_date": "2024-04-02 02:03:28 UTC",
    "updated_date": "2024-10-02 12:31:11 UTC"
  },
  {
    "arxiv_id": "2404.01558v1",
    "title": "Automated User Story Generation with Test Case Specification Using Large Language Model",
    "authors": [
      "Tajmilur Rahman",
      "Yuecai Zhu"
    ],
    "abstract": "Modern Software Engineering era is moving fast with the assistance of\nartificial intelligence (AI), especially Large Language Models (LLM).\nResearchers have already started automating many parts of the software\ndevelopment workflow. Requirements Engineering (RE) is a crucial phase that\nbegins the software development cycle through multiple discussions on a\nproposed scope of work documented in different forms. RE phase ends with a list\nof user-stories for each unit task identified through discussions and usually\nthese are created and tracked on a project management tool such as Jira,\nAzurDev etc. In this research we developed a tool \"GeneUS\" using GPT-4.0 to\nautomatically create user stories from requirements document which is the\noutcome of the RE phase. The output is provided in JSON format leaving the\npossibilities open for downstream integration to the popular project management\ntools. Analyzing requirements documents takes significant effort and multiple\nmeetings with stakeholders. We believe, automating this process will certainly\nreduce additional load off the software engineers, and increase the\nproductivity since they will be able to utilize their time on other prioritized\ntasks.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "10 pages including 2 pages of Appendix",
    "pdf_url": "http://arxiv.org/pdf/2404.01558v1",
    "published_date": "2024-04-02 01:45:57 UTC",
    "updated_date": "2024-04-02 01:45:57 UTC"
  },
  {
    "arxiv_id": "2404.01557v1",
    "title": "Distributed Autonomous Swarm Formation for Dynamic Network Bridging",
    "authors": [
      "Raffaele Galliera",
      "Thies Möhlenhof",
      "Alessandro Amato",
      "Daniel Duran",
      "Kristen Brent Venable",
      "Niranjan Suri"
    ],
    "abstract": "Effective operation and seamless cooperation of robotic systems are a\nfundamental component of next-generation technologies and applications. In\ncontexts such as disaster response, swarm operations require coordinated\nbehavior and mobility control to be handled in a distributed manner, with the\nquality of the agents' actions heavily relying on the communication between\nthem and the underlying network. In this paper, we formulate the problem of\ndynamic network bridging in a novel Decentralized Partially Observable Markov\nDecision Process (Dec-POMDP), where a swarm of agents cooperates to form a link\nbetween two distant moving targets. Furthermore, we propose a Multi-Agent\nReinforcement Learning (MARL) approach for the problem based on Graph\nConvolutional Reinforcement Learning (DGN) which naturally applies to the\nnetworked, distributed nature of the task. The proposed method is evaluated in\na simulated environment and compared to a centralized heuristic baseline\nshowing promising results. Moreover, a further step in the direction of\nsim-to-real transfer is presented, by additionally evaluating the proposed\napproach in a near Live Virtual Constructive (LVC) UAV framework.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.MA",
    "comment": "6 pages, 3 figures, 1 table, 1 algorithm",
    "pdf_url": "http://arxiv.org/pdf/2404.01557v1",
    "published_date": "2024-04-02 01:45:03 UTC",
    "updated_date": "2024-04-02 01:45:03 UTC"
  },
  {
    "arxiv_id": "2405.14876v2",
    "title": "Precise and Robust Sidewalk Detection: Leveraging Ensemble Learning to Surpass LLM Limitations in Urban Environments",
    "authors": [
      "Ibne Farabi Shihab",
      "Sudesh Ramesh Bhagat",
      "Anuj Sharma"
    ],
    "abstract": "This study aims to compare the effectiveness of a robust ensemble model with\nthe state-of-the-art ONE-PEACE Large Language Model (LLM) for accurate\ndetection of sidewalks. Accurate sidewalk detection is crucial in improving\nroad safety and urban planning. The study evaluated the model's performance on\nCityscapes, Ade20k, and the Boston Dataset. The results showed that the\nensemble model performed better than the individual models, achieving mean\nIntersection Over Union (mIOU) scores of 93.1\\%, 90.3\\%, and 90.6\\% on these\ndatasets under ideal conditions. Additionally, the ensemble model maintained a\nconsistent level of performance even in challenging conditions such as\nSalt-and-Pepper and Speckle noise, with only a gradual decrease in efficiency\nobserved. On the other hand, the ONE-PEACE LLM performed slightly better than\nthe ensemble model in ideal scenarios but experienced a significant decline in\nperformance under noisy conditions. These findings demonstrate the robustness\nand reliability of the ensemble model, making it a valuable asset for improving\nurban infrastructure related to road safety and curb space management. This\nstudy contributes positively to the broader context of urban health and\nmobility.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.14876v2",
    "published_date": "2024-04-02 01:42:32 UTC",
    "updated_date": "2025-01-23 01:08:19 UTC"
  },
  {
    "arxiv_id": "2404.01551v2",
    "title": "Safety-Aware Multi-Agent Learning for Dynamic Network Bridging",
    "authors": [
      "Raffaele Galliera",
      "Konstantinos Mitsopoulos",
      "Niranjan Suri",
      "Raffaele Romagnoli"
    ],
    "abstract": "Addressing complex cooperative tasks in safety-critical environments poses\nsignificant challenges for multi-agent systems, especially under conditions of\npartial observability. We focus on a dynamic network bridging task, where\nagents must learn to maintain a communication path between two moving targets.\nTo ensure safety during training and deployment, we integrate a\ncontrol-theoretic safety filter that enforces collision avoidance through local\nsetpoint updates. We develop and evaluate multi-agent reinforcement learning\nsafety-informed message passing, showing that encoding safety filter\nactivations as edge-level features improves coordination. The results suggest\nthat local safety enforcement and decentralized learning can be effectively\ncombined in distributed multi-agent tasks.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG",
      "cs.NI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.MA",
    "comment": "8 pages, 18 equations, 4 figures, 1 algorithm, and 1 table",
    "pdf_url": "http://arxiv.org/pdf/2404.01551v2",
    "published_date": "2024-04-02 01:30:41 UTC",
    "updated_date": "2025-04-03 17:25:41 UTC"
  },
  {
    "arxiv_id": "2404.01548v1",
    "title": "mChartQA: A universal benchmark for multimodal Chart Question Answer based on Vision-Language Alignment and Reasoning",
    "authors": [
      "Jingxuan Wei",
      "Nan Xu",
      "Guiyong Chang",
      "Yin Luo",
      "BiHui Yu",
      "Ruifeng Guo"
    ],
    "abstract": "In the fields of computer vision and natural language processing, multimodal\nchart question-answering, especially involving color, structure, and textless\ncharts, poses significant challenges. Traditional methods, which typically\ninvolve either direct multimodal processing or a table-to-text conversion\nfollowed by language model analysis, have limitations in effectively handling\nthese complex scenarios. This paper introduces a novel multimodal chart\nquestion-answering model, specifically designed to address these intricate\ntasks. Our model integrates visual and linguistic processing, overcoming the\nconstraints of existing methods. We adopt a dual-phase training approach: the\ninitial phase focuses on aligning image and text representations, while the\nsubsequent phase concentrates on optimizing the model's interpretative and\nanalytical abilities in chart-related queries. This approach has demonstrated\nsuperior performance on multiple public datasets, particularly in handling\ncolor, structure, and textless chart questions, indicating its effectiveness in\ncomplex multimodal tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.01548v1",
    "published_date": "2024-04-02 01:28:44 UTC",
    "updated_date": "2024-04-02 01:28:44 UTC"
  },
  {
    "arxiv_id": "2404.01536v2",
    "title": "Laying Anchors: Semantically Priming Numerals in Language Modeling",
    "authors": [
      "Mandar Sharma",
      "Rutuja Murlidhar Taware",
      "Pravesh Koirala",
      "Nikhil Muralidhar",
      "Naren Ramakrishnan"
    ],
    "abstract": "Off-the-shelf pre-trained language models have become the de facto standard\nin NLP pipelines for a multitude of downstream tasks. However, the inability of\nthese models to properly encode numerals limits their performance on tasks\nrequiring numeric comprehension. We introduce strategies to semantically prime\nnumerals in any corpus by generating anchors governed by the distribution of\nnumerals in said corpus, thereby enabling mathematically grounded\nrepresentations of these numeral tokens. We establish the superiority of our\nproposed techniques through evaluation on a range of numeracy tasks for both\nin-domain (seen) and out-domain (unseen) numerals. Further, we expand our\nempirical evaluations to numerals ranging from 1 to 10 billion, a significantly\nbroader range compared to previous studies of the same nature, and we\ndemonstrate significant improvements in the mathematical grounding of our\nlearned embeddings.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to the findings of NAACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.01536v2",
    "published_date": "2024-04-02 00:02:00 UTC",
    "updated_date": "2024-08-07 22:46:04 UTC"
  }
]
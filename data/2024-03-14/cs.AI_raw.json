[
  {
    "arxiv_id": "2403.09925v1",
    "title": "Surrogate Assisted Monte Carlo Tree Search in Combinatorial Optimization",
    "authors": [
      "Saeid Amiri",
      "Parisa Zehtabi",
      "Danial Dervovic",
      "Michael Cashmore"
    ],
    "abstract": "Industries frequently adjust their facilities network by opening new branches\nin promising areas and closing branches in areas where they expect low profits.\nIn this paper, we examine a particular class of facility location problems. Our\nobjective is to minimize the loss of sales resulting from the removal of\nseveral retail stores. However, estimating sales accurately is expensive and\ntime-consuming. To overcome this challenge, we leverage Monte Carlo Tree Search\n(MCTS) assisted by a surrogate model that computes evaluations faster. Results\nsuggest that MCTS supported by a fast surrogate function can generate solutions\nfaster while maintaining a consistent solution compared to MCTS that does not\nbenefit from the surrogate function.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to the ICAPS Planning and Scheduling for Financial Services\n  (FINPLAN) 2023 workshop",
    "pdf_url": "http://arxiv.org/pdf/2403.09925v1",
    "published_date": "2024-03-14 23:54:19 UTC",
    "updated_date": "2024-03-14 23:54:19 UTC"
  },
  {
    "arxiv_id": "2403.09920v3",
    "title": "Predicting Generalization of AI Colonoscopy Models to Unseen Data",
    "authors": [
      "Joel Shor",
      "Carson McNeil",
      "Yotam Intrator",
      "Joseph R Ledsam",
      "Hiro-o Yamano",
      "Daisuke Tsurumaru",
      "Hiroki Kayama",
      "Atsushi Hamabe",
      "Koji Ando",
      "Mitsuhiko Ota",
      "Haruei Ogino",
      "Hiroshi Nakase",
      "Kaho Kobayashi",
      "Masaaki Miyo",
      "Eiji Oki",
      "Ichiro Takemasa",
      "Ehud Rivlin",
      "Roman Goldenberg"
    ],
    "abstract": "$\\textbf{Background}$: Generalizability of AI colonoscopy algorithms is\nimportant for wider adoption in clinical practice. However, current techniques\nfor evaluating performance on unseen data require expensive and time-intensive\nlabels.\n  $\\textbf{Methods}$: We use a \"Masked Siamese Network\" (MSN) to identify novel\nphenomena in unseen data and predict polyp detector performance. MSN is trained\nto predict masked out regions of polyp images, without any labels. We test\nMSN's ability to be trained on data only from Israel and detect unseen\ntechniques, narrow-band imaging (NBI) and chromendoscoy (CE), on colonoscopes\nfrom Japan (354 videos, 128 hours). We also test MSN's ability to predict\nperformance of Computer Aided Detection (CADe) of polyps on colonoscopies from\nboth countries, even though MSN is not trained on data from Japan.\n  $\\textbf{Results}$: MSN correctly identifies NBI and CE as less similar to\nIsrael whitelight than Japan whitelight (bootstrapped z-test, |z| > 496, p <\n10^-8 for both) using the label-free Frechet distance. MSN detects NBI with 99%\naccuracy, predicts CE better than our heuristic (90% vs 79% accuracy) despite\nbeing trained only on whitelight, and is the only method that is robust to\nnoisy labels. MSN predicts CADe polyp detector performance on in-domain Israel\nand out-of-domain Japan colonoscopies (r=0.79, 0.37 respectively). With few\nexamples of Japan detector performance to train on, MSN prediction of Japan\nperformance improves (r=0.56).\n  $\\textbf{Conclusion}$: Our technique can identify distribution shifts in\nclinical data and can predict CADe detector performance on unseen data, without\nlabels. Our self-supervised approach can aid in detecting when data in practice\nis different from training, such as between hospitals or data has meaningfully\nshifted from training. MSN has potential for application to medical image\ndomains beyond colonoscopy.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.CY"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.09920v3",
    "published_date": "2024-03-14 23:41:00 UTC",
    "updated_date": "2024-03-22 04:53:29 UTC"
  },
  {
    "arxiv_id": "2403.09904v1",
    "title": "FedComLoc: Communication-Efficient Distributed Training of Sparse and Quantized Models",
    "authors": [
      "Kai Yi",
      "Georg Meinhardt",
      "Laurent Condat",
      "Peter Richtárik"
    ],
    "abstract": "Federated Learning (FL) has garnered increasing attention due to its unique\ncharacteristic of allowing heterogeneous clients to process their private data\nlocally and interact with a central server, while being respectful of privacy.\nA critical bottleneck in FL is the communication cost. A pivotal strategy to\nmitigate this burden is \\emph{Local Training}, which involves running multiple\nlocal stochastic gradient descent iterations between communication phases. Our\nwork is inspired by the innovative \\emph{Scaffnew} algorithm, which has\nconsiderably advanced the reduction of communication complexity in FL. We\nintroduce FedComLoc (Federated Compressed and Local Training), integrating\npractical and effective compression into \\emph{Scaffnew} to further enhance\ncommunication efficiency. Extensive experiments, using the popular TopK\ncompressor and quantization, demonstrate its prowess in substantially reducing\ncommunication overheads in heterogeneous settings.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.09904v1",
    "published_date": "2024-03-14 22:29:59 UTC",
    "updated_date": "2024-03-14 22:29:59 UTC"
  },
  {
    "arxiv_id": "2403.09891v3",
    "title": "Fisher Mask Nodes for Language Model Merging",
    "authors": [
      "Thennal D K",
      "Ganesh Nathan",
      "Suchithra M S"
    ],
    "abstract": "Fine-tuning pre-trained models provides significant advantages in downstream\nperformance. The ubiquitous nature of pre-trained models such as BERT and its\nderivatives in natural language processing has also led to a proliferation of\ntask-specific fine-tuned models. As these models typically only perform one\ntask well, additional training or ensembling is required in multi-task\nscenarios. The growing field of model merging provides a solution, dealing with\nthe challenge of combining multiple task-specific models into a single\nmulti-task model. In this study, we introduce a novel model merging method for\nTransformers, combining insights from previous work in Fisher-weighted\naveraging and the use of Fisher information in model pruning. Utilizing the\nFisher information of mask nodes within the Transformer architecture, we devise\na computationally efficient weighted-averaging scheme. Our method exhibits a\nregular and significant performance increase across various models in the BERT\nfamily, outperforming full-scale Fisher-weighted averaging in a fraction of the\ncomputational cost, with baseline performance improvements of up to +6.5 and a\nspeedup between 57.4x and 321.7x across models. Our results prove the potential\nof our method in current multi-task learning environments and suggest its\nscalability and adaptability to new model architectures and learning scenarios.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at LREC-COLING 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.09891v3",
    "published_date": "2024-03-14 21:52:26 UTC",
    "updated_date": "2024-05-03 13:12:40 UTC"
  },
  {
    "arxiv_id": "2403.09887v2",
    "title": "Sabiá-2: A New Generation of Portuguese Large Language Models",
    "authors": [
      "Thales Sales Almeida",
      "Hugo Abonizio",
      "Rodrigo Nogueira",
      "Ramon Pires"
    ],
    "abstract": "We introduce Sabi\\'a-2, a family of large language models trained on\nPortuguese texts. The models are evaluated on a diverse range of exams,\nincluding entry-level tests for Brazilian universities, professional\ncertification exams, and graduate-level exams for various disciplines such as\naccounting, economics, engineering, law and medicine. Our results reveal that\nour best model so far, Sabi\\'a-2 Medium, matches or surpasses GPT-4's\nperformance in 23 out of 64 exams and outperforms GPT-3.5 in 58 out of 64\nexams. Notably, specialization has a significant impact on a model's\nperformance without the need to increase its size, allowing us to offer\nSabi\\'a-2 Medium at a price per token that is 10 times cheaper than GPT-4.\nFinally, we identified that math and coding are key abilities that need\nimprovement.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.09887v2",
    "published_date": "2024-03-14 21:44:48 UTC",
    "updated_date": "2024-03-26 23:52:35 UTC"
  },
  {
    "arxiv_id": "2403.18850v2",
    "title": "Are Colors Quanta of Light for Human Vision? A Quantum Cognition Study of Visual Perception",
    "authors": [
      "Jonito Aerts Arguëlles"
    ],
    "abstract": "We show that colors are light quanta for human visual perception in a similar\nway as photons are light quanta for physical measurements of light waves. Our\nresult relies on the identification in the quantum measurement process itself\nof the warping mechanism which is characteristic of human perception. This\nwarping mechanism makes stimuli classified into the same category perceived as\nmore similar, while stimuli classified into different m categories are\nperceived as more different. In the quantum measurement process, the warping\ntakes place between the pure states, which play the role played for human\nperception by the stimuli, and the density states after decoherence, which play\nthe role played for human perception by the percepts. We use the natural metric\nfor pure states, namely the normalized Fubini Study metric to measure distances\nbetween pure states, and the natural metric for density states, namely the\nnormalized trace-class metric, to measure distances between density states. We\nthen show that when pure states lie within a well-defined region surrounding an\neigenstate, the quantum measurement, namely the process of decoherence,\ncontracts the distance between these pure states, while the reverse happens for\npure states lying in a well-defined region between two eigenstates, for which\nthe quantum measurement causes a dilation. We elaborate as an example the\nsituation of a two-dimensional quantum measurement described by the Bloch model\nand apply it to the situation of two colors 'Light' and 'Dark'. We argue that\nthis analogy of warping, on the one hand in human perception and on the other\nhand in the quantum measurement process, makes colors to be quanta of light for\nhuman vision.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "quant-ph"
    ],
    "primary_category": "q-bio.NC",
    "comment": "22 pages, 4 figures. arXiv admin note: text overlap with\n  arXiv:2208.03726",
    "pdf_url": "http://arxiv.org/pdf/2403.18850v2",
    "published_date": "2024-03-14 21:10:07 UTC",
    "updated_date": "2025-05-05 18:35:00 UTC"
  },
  {
    "arxiv_id": "2403.09871v5",
    "title": "ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images",
    "authors": [
      "Fangqiang Ding",
      "Yunzhou Zhu",
      "Xiangyu Wen",
      "Gaowen Liu",
      "Chris Xiaoxuan Lu"
    ],
    "abstract": "Designing egocentric 3D hand pose estimation systems that can perform\nreliably in complex, real-world scenarios is crucial for downstream\napplications. Previous approaches using RGB or NIR imagery struggle in\nchallenging conditions: RGB methods are susceptible to lighting variations and\nobstructions like handwear, while NIR techniques can be disrupted by sunlight\nor interference from other NIR-equipped devices. To address these limitations,\nwe present ThermoHands, the first benchmark focused on thermal image-based\negocentric 3D hand pose estimation, demonstrating the potential of thermal\nimaging to achieve robust performance under these conditions. The benchmark\nincludes a multi-view and multi-spectral dataset collected from 28 subjects\nperforming hand-object and hand-virtual interactions under diverse scenarios,\naccurately annotated with 3D hand poses through an automated process. We\nintroduce a new baseline method, TherFormer, utilizing dual transformer modules\nfor effective egocentric 3D hand pose estimation in thermal imagery. Our\nexperimental results highlight TherFormer's leading performance and affirm\nthermal imaging's effectiveness in enabling robust 3D hand pose estimation in\nadverse conditions.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "This paper has been accepted to the 23rd ACM Conference on Embedded\n  Networked Sensor Systems (Sensys'25). Including 14 pages, 9 figures, 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2403.09871v5",
    "published_date": "2024-03-14 21:01:06 UTC",
    "updated_date": "2025-02-27 15:42:22 UTC"
  },
  {
    "arxiv_id": "2403.09869v1",
    "title": "Mind the GAP: Improving Robustness to Subpopulation Shifts with Group-Aware Priors",
    "authors": [
      "Tim G. J. Rudner",
      "Ya Shi Zhang",
      "Andrew Gordon Wilson",
      "Julia Kempe"
    ],
    "abstract": "Machine learning models often perform poorly under subpopulation shifts in\nthe data distribution. Developing methods that allow machine learning models to\nbetter generalize to such shifts is crucial for safe deployment in real-world\nsettings. In this paper, we develop a family of group-aware prior (GAP)\ndistributions over neural network parameters that explicitly favor models that\ngeneralize well under subpopulation shifts. We design a simple group-aware\nprior that only requires access to a small set of data with group information\nand demonstrate that training with this prior yields state-of-the-art\nperformance -- even when only retraining the final layer of a previously\ntrained non-robust model. Group aware-priors are conceptually simple,\ncomplementary to existing approaches, such as attribute pseudo labeling and\ndata reweighting, and open up promising new avenues for harnessing Bayesian\ninference to enable robustness to subpopulation shifts.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "stat.ME"
    ],
    "primary_category": "stat.ML",
    "comment": "Published in Proceedings of the 27th International Conference on\n  Artificial Intelligence and Statistics (AISTATS 2024)",
    "pdf_url": "http://arxiv.org/pdf/2403.09869v1",
    "published_date": "2024-03-14 21:00:26 UTC",
    "updated_date": "2024-03-14 21:00:26 UTC"
  },
  {
    "arxiv_id": "2403.09863v5",
    "title": "Towards White Box Deep Learning",
    "authors": [
      "Maciej Satkiewicz"
    ],
    "abstract": "Deep neural networks learn fragile \"shortcut\" features, rendering them\ndifficult to interpret (black box) and vulnerable to adversarial attacks. This\npaper proposes semantic features as a general architectural solution to this\nproblem. The main idea is to make features locality-sensitive in the adequate\nsemantic topology of the domain, thus introducing a strong regularization. The\nproof of concept network is lightweight, inherently interpretable and achieves\nalmost human-level adversarial test metrics - with no adversarial training!\nThese results and the general nature of the approach warrant further research\non semantic features. The code is available at\nhttps://github.com/314-Foundation/white-box-nn",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages, 12 figures, independent research, v5 changes: Expanded\n  Abstract and Related Work section; minor wording improvements",
    "pdf_url": "http://arxiv.org/pdf/2403.09863v5",
    "published_date": "2024-03-14 20:50:03 UTC",
    "updated_date": "2024-04-17 17:58:52 UTC"
  },
  {
    "arxiv_id": "2403.09861v1",
    "title": "NN-Defined Modulator: Reconfigurable and Portable Software Modulator on IoT Gateways",
    "authors": [
      "Jiazhao Wang",
      "Wenchao Jiang",
      "Ruofeng Liu",
      "Bin Hu",
      "Demin Gao",
      "Shuai Wang"
    ],
    "abstract": "A physical-layer modulator is a vital component for an IoT gateway to map the\nsymbols to signals. However, due to the soldered hardware chipsets on the\ngateway's motherboards or the diverse toolkits on different platforms for the\nsoftware radio, the existing solutions either have limited extensibility or are\nplatform-specific. Such limitation is hard to ignore when modulation schemes\nand hardware platforms have become extremely diverse. This paper presents a new\nparadigm of using neural networks as an abstraction layer for physical layer\nmodulators in IoT gateway devices, referred to as NN-defined modulators. Our\napproach addresses the challenges of extensibility and portability for multiple\ntechnologies on various hardware platforms. The proposed NN-defined modulator\nuses a model-driven methodology rooted in solid mathematical foundations while\nhaving native support for hardware acceleration and portability to\nheterogeneous platforms. We conduct the evaluation of NN-defined modulators on\ndifferent platforms, including Nvidia Jetson Nano and Raspberry Pi. Evaluations\ndemonstrate that our NN-defined modulator effectively operates as conventional\nmodulators and provides significant efficiency gains (up to $4.7\\times$ on\nNvidia Jetson Nano and $1.1\\times$ on Raspberry Pi), indicating high\nportability. Furthermore, we show the real-world applications using our\nNN-defined modulators to generate ZigBee and WiFi packets, which are compliant\nwith commodity TI CC2650 (ZigBee) and Intel AX201 (WiFi NIC), respectively.",
    "categories": [
      "cs.ET",
      "cs.AI"
    ],
    "primary_category": "cs.ET",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.09861v1",
    "published_date": "2024-03-14 20:42:23 UTC",
    "updated_date": "2024-03-14 20:42:23 UTC"
  },
  {
    "arxiv_id": "2403.09857v3",
    "title": "Few-Shot Class Incremental Learning with Attention-Aware Self-Adaptive Prompt",
    "authors": [
      "Chenxi Liu",
      "Zhenyi Wang",
      "Tianyi Xiong",
      "Ruibo Chen",
      "Yihan Wu",
      "Junfeng Guo",
      "Heng Huang"
    ],
    "abstract": "Few-Shot Class-Incremental Learning (FSCIL) models aim to incrementally learn\nnew classes with scarce samples while preserving knowledge of old ones.\nExisting FSCIL methods usually fine-tune the entire backbone, leading to\noverfitting and hindering the potential to learn new classes. On the other\nhand, recent prompt-based CIL approaches alleviate forgetting by training\nprompts with sufficient data in each task. In this work, we propose a novel\nframework named Attention-aware Self-adaptive Prompt (ASP). ASP encourages\ntask-invariant prompts to capture shared knowledge by reducing specific\ninformation from the attention aspect. Additionally, self-adaptive\ntask-specific prompts in ASP provide specific information and transfer\nknowledge from old classes to new classes with an Information Bottleneck\nlearning objective. In summary, ASP prevents overfitting on base task and does\nnot require enormous data in few-shot incremental tasks. Extensive experiments\non three benchmark datasets validate that ASP consistently outperforms\nstate-of-the-art FSCIL and prompt-based CIL methods in terms of both learning\nnew classes and mitigating forgetting.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.09857v3",
    "published_date": "2024-03-14 20:34:53 UTC",
    "updated_date": "2024-07-17 16:00:27 UTC"
  },
  {
    "arxiv_id": "2403.10570v2",
    "title": "Symbiotic Game and Foundation Models for Cyber Deception Operations in Strategic Cyber Warfare",
    "authors": [
      "Tao Li",
      "Quanyan Zhu"
    ],
    "abstract": "We are currently facing unprecedented cyber warfare with the rapid evolution\nof tactics, increasing asymmetry of intelligence, and the growing accessibility\nof hacking tools. In this landscape, cyber deception emerges as a critical\ncomponent of our defense strategy against increasingly sophisticated attacks.\nThis chapter aims to highlight the pivotal role of game-theoretic models and\nfoundation models (FMs) in analyzing, designing, and implementing cyber\ndeception tactics. Game models (GMs) serve as a foundational framework for\nmodeling diverse adversarial interactions, allowing us to encapsulate both\nadversarial knowledge and domain-specific insights. Meanwhile, FMs serve as the\nbuilding blocks for creating tailored machine learning models suited to given\napplications. By leveraging the synergy between GMs and FMs, we can advance\nproactive and automated cyber defense mechanisms by not only securing our\nnetworks against attacks but also enhancing their resilience against\nwell-planned operations. This chapter discusses the games at the tactical,\noperational, and strategic levels of warfare, delves into the symbiotic\nrelationship between these methodologies, and explores relevant applications\nwhere such a framework can make a substantial impact in cybersecurity. The\nchapter discusses the promising direction of the multi-agent neurosymbolic\nconjectural learning (MANSCOL), which allows the defender to predict\nadversarial behaviors, design adaptive defensive deception tactics, and\nsynthesize knowledge for the operational level synthesis and adaptation. FMs\nserve as pivotal tools across various functions for MANSCOL, including\nreinforcement learning, knowledge assimilation, formation of conjectures, and\ncontextual representation. This chapter concludes with a discussion of the\nchallenges associated with FMs and their application in the domain of\ncybersecurity.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.GT"
    ],
    "primary_category": "cs.CR",
    "comment": "40 pages, 7 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2403.10570v2",
    "published_date": "2024-03-14 20:17:57 UTC",
    "updated_date": "2024-08-19 00:52:20 UTC"
  },
  {
    "arxiv_id": "2403.09849v1",
    "title": "Self-Consistency Boosts Calibration for Math Reasoning",
    "authors": [
      "Ante Wang",
      "Linfeng Song",
      "Ye Tian",
      "Baolin Peng",
      "Lifeng Jin",
      "Haitao Mi",
      "Jinsong Su",
      "Dong Yu"
    ],
    "abstract": "Calibration, which establishes the correlation between accuracy and model\nconfidence, is important for LLM development. We design three off-the-shelf\ncalibration methods based on self-consistency (Wang et al., 2022) for math\nreasoning tasks. Evaluation on two popular benchmarks (GSM8K and MathQA) using\nstrong open-source LLMs (Mistral and LLaMA2), our methods better bridge model\nconfidence and accuracy than existing methods based on p(True) (Kadavath et\nal., 2022) or logit (Kadavath et al., 2022).",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.09849v1",
    "published_date": "2024-03-14 20:17:10 UTC",
    "updated_date": "2024-03-14 20:17:10 UTC"
  },
  {
    "arxiv_id": "2403.09847v1",
    "title": "Forecasting Geoffective Events from Solar Wind Data and Evaluating the Most Predictive Features through Machine Learning Approaches",
    "authors": [
      "Sabrina Guastavino",
      "Katsiaryna Bahamazava",
      "Emma Perracchione",
      "Fabiana Camattari",
      "Gianluca Audone",
      "Daniele Telloni",
      "Roberto Susino",
      "Gianalfredo Nicolini",
      "Silvano Fineschi",
      "Michele Piana",
      "Anna Maria Massone"
    ],
    "abstract": "This study addresses the prediction of geomagnetic disturbances by exploiting\nmachine learning techniques. Specifically, the Long-Short Term Memory recurrent\nneural network, which is particularly suited for application over long time\nseries, is employed in the analysis of in-situ measurements of solar wind\nplasma and magnetic field acquired over more than one solar cycle, from $2005$\nto $2019$, at the Lagrangian point L$1$. The problem is approached as a binary\nclassification aiming to predict one hour in advance a decrease in the SYM-H\ngeomagnetic activity index below the threshold of $-50$ nT, which is generally\nregarded as indicative of magnetospheric perturbations. The strong class\nimbalance issue is tackled by using an appropriate loss function tailored to\noptimize appropriate skill scores in the training phase of the neural network.\nBeside classical skill scores, value-weighted skill scores are then employed to\nevaluate predictions, suitable in the study of problems, such as the one faced\nhere, characterized by strong temporal variability. For the first time, the\ncontent of magnetic helicity and energy carried by solar transients, associated\nwith their detection and likelihood of geo-effectiveness, were considered as\ninput features of the network architecture. Their predictive capabilities are\ndemonstrated through a correlation-driven feature selection method to rank the\nmost relevant characteristics involved in the neural network prediction model.\nThe optimal performance of the adopted neural network in properly forecasting\nthe onset of geomagnetic storms, which is a crucial point for giving real\nwarnings in an operational setting, is finally showed.",
    "categories": [
      "physics.space-ph",
      "astro-ph.SR",
      "cs.AI",
      "85-08, 68T07, 68T05"
    ],
    "primary_category": "physics.space-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.09847v1",
    "published_date": "2024-03-14 20:13:26 UTC",
    "updated_date": "2024-03-14 20:13:26 UTC"
  },
  {
    "arxiv_id": "2403.10569v1",
    "title": "Achieving Pareto Optimality using Efficient Parameter Reduction for DNNs in Resource-Constrained Edge Environment",
    "authors": [
      "Atah Nuh Mih",
      "Alireza Rahimi",
      "Asfia Kawnine",
      "Francis Palma",
      "Monica Wachowicz",
      "Rickey Dubay",
      "Hung Cao"
    ],
    "abstract": "This paper proposes an optimization of an existing Deep Neural Network (DNN)\nthat improves its hardware utilization and facilitates on-device training for\nresource-constrained edge environments. We implement efficient parameter\nreduction strategies on Xception that shrink the model size without sacrificing\naccuracy, thus decreasing memory utilization during training. We evaluate our\nmodel in two experiments: Caltech-101 image classification and PCB defect\ndetection and compare its performance against the original Xception and\nlightweight models, EfficientNetV2B1 and MobileNetV2. The results of the\nCaltech-101 image classification show that our model has a better test accuracy\n(76.21%) than Xception (75.89%), uses less memory on average (847.9MB) than\nXception (874.6MB), and has faster training and inference times. The\nlightweight models overfit with EfficientNetV2B1 having a 30.52% test accuracy\nand MobileNetV2 having a 58.11% test accuracy. Both lightweight models have\nbetter memory usage than our model and Xception. On the PCB defect detection,\nour model has the best test accuracy (90.30%), compared to Xception (88.10%),\nEfficientNetV2B1 (55.25%), and MobileNetV2 (50.50%). MobileNetV2 has the least\naverage memory usage (849.4MB), followed by our model (865.8MB), then\nEfficientNetV2B1 (874.8MB), and Xception has the highest (893.6MB). We further\nexperiment with pre-trained weights and observe that memory usage decreases\nthereby showing the benefits of transfer learning. A Pareto analysis of the\nmodels' performance shows that our optimized model architecture satisfies\naccuracy and low memory utilization objectives.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "arXiv admin note: text overlap with arXiv:2401.05355",
    "pdf_url": "http://arxiv.org/pdf/2403.10569v1",
    "published_date": "2024-03-14 19:40:58 UTC",
    "updated_date": "2024-03-14 19:40:58 UTC"
  },
  {
    "arxiv_id": "2403.09830v1",
    "title": "Towards the Reusability and Compositionality of Causal Representations",
    "authors": [
      "Davide Talon",
      "Phillip Lippe",
      "Stuart James",
      "Alessio Del Bue",
      "Sara Magliacane"
    ],
    "abstract": "Causal Representation Learning (CRL) aims at identifying high-level causal\nfactors and their relationships from high-dimensional observations, e.g.,\nimages. While most CRL works focus on learning causal representations in a\nsingle environment, in this work we instead propose a first step towards\nlearning causal representations from temporal sequences of images that can be\nadapted in a new environment, or composed across multiple related environments.\nIn particular, we introduce DECAF, a framework that detects which causal\nfactors can be reused and which need to be adapted from previously learned\ncausal representations. Our approach is based on the availability of\nintervention targets, that indicate which variables are perturbed at each time\nstep. Experiments on three benchmark datasets show that integrating our\nframework with four state-of-the-art CRL approaches leads to accurate\nrepresentations in a new environment with only a few samples.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to the 3rd Conference on Causal Learning and Reasoning\n  (CLeaR 2024)",
    "pdf_url": "http://arxiv.org/pdf/2403.09830v1",
    "published_date": "2024-03-14 19:36:07 UTC",
    "updated_date": "2024-03-14 19:36:07 UTC"
  },
  {
    "arxiv_id": "2403.09810v1",
    "title": "LabelAId: Just-in-time AI Interventions for Improving Human Labeling Quality and Domain Knowledge in Crowdsourcing Systems",
    "authors": [
      "Chu Li",
      "Zhihan Zhang",
      "Michael Saugstad",
      "Esteban Safranchik",
      "Minchu Kulkarni",
      "Xiaoyu Huang",
      "Shwetak Patel",
      "Vikram Iyer",
      "Tim Althoff",
      "Jon E. Froehlich"
    ],
    "abstract": "Crowdsourcing platforms have transformed distributed problem-solving, yet\nquality control remains a persistent challenge. Traditional quality control\nmeasures, such as prescreening workers and refining instructions, often focus\nsolely on optimizing economic output. This paper explores just-in-time AI\ninterventions to enhance both labeling quality and domain-specific knowledge\namong crowdworkers. We introduce LabelAId, an advanced inference model\ncombining Programmatic Weak Supervision (PWS) with FT-Transformers to infer\nlabel correctness based on user behavior and domain knowledge. Our technical\nevaluation shows that our LabelAId pipeline consistently outperforms\nstate-of-the-art ML baselines, improving mistake inference accuracy by 36.7%\nwith 50 downstream samples. We then implemented LabelAId into Project Sidewalk,\nan open-source crowdsourcing platform for urban accessibility. A\nbetween-subjects study with 34 participants demonstrates that LabelAId\nsignificantly enhances label precision without compromising efficiency while\nalso increasing labeler confidence. We discuss LabelAId's success factors,\nlimitations, and its generalizability to other crowdsourced science domains.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.09810v1",
    "published_date": "2024-03-14 18:59:10 UTC",
    "updated_date": "2024-03-14 18:59:10 UTC"
  },
  {
    "arxiv_id": "2403.09809v1",
    "title": "Self-Supervised Learning for Time Series: Contrastive or Generative?",
    "authors": [
      "Ziyu Liu",
      "Azadeh Alavi",
      "Minyi Li",
      "Xiang Zhang"
    ],
    "abstract": "Self-supervised learning (SSL) has recently emerged as a powerful approach to\nlearning representations from large-scale unlabeled data, showing promising\nresults in time series analysis. The self-supervised representation learning\ncan be categorized into two mainstream: contrastive and generative. In this\npaper, we will present a comprehensive comparative study between contrastive\nand generative methods in time series. We first introduce the basic frameworks\nfor contrastive and generative SSL, respectively, and discuss how to obtain the\nsupervision signal that guides the model optimization. We then implement\nclassical algorithms (SimCLR vs. MAE) for each type and conduct a comparative\nanalysis in fair settings. Our results provide insights into the strengths and\nweaknesses of each approach and offer practical recommendations for choosing\nsuitable SSL methods. We also discuss the implications of our findings for the\nbroader field of representation learning and propose future research\ndirections. All the code and data are released at\n\\url{https://github.com/DL4mHealth/SSL_Comparison}.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.LG",
    "comment": "Published at the AI4TS Workshop, IJCAI 2023",
    "pdf_url": "http://arxiv.org/pdf/2403.09809v1",
    "published_date": "2024-03-14 18:58:06 UTC",
    "updated_date": "2024-03-14 18:58:06 UTC"
  },
  {
    "arxiv_id": "2403.09806v1",
    "title": "xLP: Explainable Link Prediction for Master Data Management",
    "authors": [
      "Balaji Ganesan",
      "Matheen Ahmed Pasha",
      "Srinivasa Parkala",
      "Neeraj R Singh",
      "Gayatri Mishra",
      "Sumit Bhatia",
      "Hima Patel",
      "Somashekar Naganna",
      "Sameep Mehta"
    ],
    "abstract": "Explaining neural model predictions to users requires creativity. Especially\nin enterprise applications, where there are costs associated with users' time,\nand their trust in the model predictions is critical for adoption. For link\nprediction in master data management, we have built a number of explainability\nsolutions drawing from research in interpretability, fact verification, path\nranking, neuro-symbolic reasoning and self-explaining AI. In this demo, we\npresent explanations for link prediction in a creative way, to allow users to\nchoose explanations they are more comfortable with.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, 4 figures, NeurIPS 2020 Competition and Demonstration Track.\n  arXiv admin note: text overlap with arXiv:2012.05516",
    "pdf_url": "http://arxiv.org/pdf/2403.09806v1",
    "published_date": "2024-03-14 18:53:44 UTC",
    "updated_date": "2024-03-14 18:53:44 UTC"
  },
  {
    "arxiv_id": "2403.09795v1",
    "title": "Helpful or Harmful? Exploring the Efficacy of Large Language Models for Online Grooming Prevention",
    "authors": [
      "Ellie Prosser",
      "Matthew Edwards"
    ],
    "abstract": "Powerful generative Large Language Models (LLMs) are becoming popular tools\namongst the general public as question-answering systems, and are being\nutilised by vulnerable groups such as children. With children increasingly\ninteracting with these tools, it is imperative for researchers to scrutinise\nthe safety of LLMs, especially for applications that could lead to serious\noutcomes, such as online child safety queries. In this paper, the efficacy of\nLLMs for online grooming prevention is explored both for identifying and\navoiding grooming through advice generation, and the impact of prompt design on\nmodel performance is investigated by varying the provided context and prompt\nspecificity. In results reflecting over 6,000 LLM interactions, we find that no\nmodels were clearly appropriate for online grooming prevention, with an\nobserved lack of consistency in behaviours, and potential for harmful answer\ngeneration, especially from open-source models. We outline where and how models\nfall short, providing suggestions for improvement, and identify prompt designs\nthat heavily altered model performance in troubling ways, with findings that\ncan be used to inform best practice usage guides.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.09795v1",
    "published_date": "2024-03-14 18:27:43 UTC",
    "updated_date": "2024-03-14 18:27:43 UTC"
  },
  {
    "arxiv_id": "2403.09793v3",
    "title": "Socially Integrated Navigation: A Social Acting Robot with Deep Reinforcement Learning",
    "authors": [
      "Daniel Flögel",
      "Lars Fischer",
      "Thomas Rudolf",
      "Tobias Schürmann",
      "Sören Hohmann"
    ],
    "abstract": "Mobile robots are being used on a large scale in various crowded situations\nand become part of our society. The socially acceptable navigation behavior of\na mobile robot with individual human consideration is an essential requirement\nfor scalable applications and human acceptance. Deep Reinforcement Learning\n(DRL) approaches are recently used to learn a robot's navigation policy and to\nmodel the complex interactions between robots and humans. We propose to divide\nexisting DRL-based navigation approaches based on the robot's exhibited social\nbehavior and distinguish between social collision avoidance with a lack of\nsocial behavior and socially aware approaches with explicit predefined social\nbehavior. In addition, we propose a novel socially integrated navigation\napproach where the robot's social behavior is adaptive and emerges from the\ninteraction with humans. The formulation of our approach is derived from a\nsociological definition, which states that social acting is oriented toward the\nacting of others. The DRL policy is trained in an environment where other\nagents interact socially integrated and reward the robot's behavior\nindividually. The simulation results indicate that the proposed socially\nintegrated navigation approach outperforms a socially aware approach in terms\nof ego navigation performance while significantly reducing the negative impact\non all agents within the environment.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted at 2024 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS)",
    "pdf_url": "http://arxiv.org/pdf/2403.09793v3",
    "published_date": "2024-03-14 18:25:40 UTC",
    "updated_date": "2024-07-26 06:41:45 UTC"
  },
  {
    "arxiv_id": "2403.09635v2",
    "title": "Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models",
    "authors": [
      "Akhil Kedia",
      "Mohd Abbas Zaidi",
      "Sushil Khyalia",
      "Jungho Jung",
      "Harshith Goka",
      "Haejun Lee"
    ],
    "abstract": "In spite of their huge success, transformer models remain difficult to scale\nin depth. In this work, we develop a unified signal propagation theory and\nprovide formulae that govern the moments of the forward and backward signal\nthrough the transformer model. Our framework can be used to understand and\nmitigate vanishing/exploding gradients, rank collapse, and instability\nassociated with high attention scores. We also propose DeepScaleLM, an\ninitialization and scaling scheme that conserves unit output/gradient moments\nthroughout the model, enabling the training of very deep models with 1000\nlayers. We find that transformer models could be much deeper - our deep models\nwith fewer parameters outperform shallow models in Language Modeling, Speech\nTranslation, and Image Classification, across encoder-only, decoder-only and\nencoder-decoder variants, for both Pre-LN and Post-LN transformers, for\nmultiple datasets and model sizes. These improvements also translate into\nimproved performance on downstream Question Answering tasks and improved\nrobustness for Image Classification.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "I.2.7; I.2.10"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at ICML 2024. Source code is available at\n  https://github.com/akhilkedia/TranformersGetStable. Akhil Kedia, Mohd Abbas\n  Zaidi, Sushil Khyalia equal contribution",
    "pdf_url": "http://arxiv.org/pdf/2403.09635v2",
    "published_date": "2024-03-14 17:59:14 UTC",
    "updated_date": "2024-07-18 17:59:35 UTC"
  },
  {
    "arxiv_id": "2403.09631v1",
    "title": "3D-VLA: A 3D Vision-Language-Action Generative World Model",
    "authors": [
      "Haoyu Zhen",
      "Xiaowen Qiu",
      "Peihao Chen",
      "Jincheng Yang",
      "Xin Yan",
      "Yilun Du",
      "Yining Hong",
      "Chuang Gan"
    ],
    "abstract": "Recent vision-language-action (VLA) models rely on 2D inputs, lacking\nintegration with the broader realm of the 3D physical world. Furthermore, they\nperform action prediction by learning a direct mapping from perception to\naction, neglecting the vast dynamics of the world and the relations between\nactions and dynamics. In contrast, human beings are endowed with world models\nthat depict imagination about future scenarios to plan actions accordingly. To\nthis end, we propose 3D-VLA by introducing a new family of embodied foundation\nmodels that seamlessly link 3D perception, reasoning, and action through a\ngenerative world model. Specifically, 3D-VLA is built on top of a 3D-based\nlarge language model (LLM), and a set of interaction tokens is introduced to\nengage with the embodied environment. Furthermore, to inject generation\nabilities into the model, we train a series of embodied diffusion models and\nalign them into the LLM for predicting the goal images and point clouds. To\ntrain our 3D-VLA, we curate a large-scale 3D embodied instruction dataset by\nextracting vast 3D-related information from existing robotics datasets. Our\nexperiments on held-in datasets demonstrate that 3D-VLA significantly improves\nthe reasoning, multimodal generation, and planning capabilities in embodied\nenvironments, showcasing its potential in real-world applications.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://vis-www.cs.umass.edu/3dvla/",
    "pdf_url": "http://arxiv.org/pdf/2403.09631v1",
    "published_date": "2024-03-14 17:58:41 UTC",
    "updated_date": "2024-03-14 17:58:41 UTC"
  },
  {
    "arxiv_id": "2403.09629v2",
    "title": "Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking",
    "authors": [
      "Eric Zelikman",
      "Georges Harik",
      "Yijia Shao",
      "Varuna Jayasiri",
      "Nick Haber",
      "Noah D. Goodman"
    ],
    "abstract": "When writing and talking, people sometimes pause to think. Although\nreasoning-focused works have often framed reasoning as a method of answering\nquestions or completing agentic tasks, reasoning is implicit in almost all\nwritten text. For example, this applies to the steps not stated between the\nlines of a proof or to the theory of mind underlying a conversation. In the\nSelf-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned\nby inferring rationales from few-shot examples in question-answering and\nlearning from those that lead to a correct answer. This is a highly constrained\nsetting -- ideally, a language model could instead learn to infer unstated\nrationales in arbitrary text. We present Quiet-STaR, a generalization of STaR\nin which LMs learn to generate rationales at each token to explain future text,\nimproving their predictions. We address key challenges, including 1) the\ncomputational cost of generating continuations, 2) the fact that the LM does\nnot initially know how to generate or use internal thoughts, and 3) the need to\npredict beyond individual next tokens. To resolve these, we propose a tokenwise\nparallel sampling algorithm, using learnable tokens indicating a thought's\nstart and end, and an extended teacher-forcing technique. Encouragingly,\ngenerated rationales disproportionately help model difficult-to-predict tokens\nand improve the LM's ability to directly answer difficult questions. In\nparticular, after continued pretraining of an LM on a corpus of internet text\nwith Quiet-STaR, we find zero-shot improvements on GSM8K\n(5.9%$\\rightarrow$10.9%) and CommonsenseQA (36.3%$\\rightarrow$47.2%) and\nobserve a perplexity improvement of difficult tokens in natural text.\nCrucially, these improvements require no fine-tuning on these tasks. Quiet-STaR\nmarks a step towards LMs that can learn to reason in a more general and\nscalable way.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.09629v2",
    "published_date": "2024-03-14 17:58:16 UTC",
    "updated_date": "2024-03-18 07:56:48 UTC"
  },
  {
    "arxiv_id": "2403.09621v2",
    "title": "Minimax Optimal and Computationally Efficient Algorithms for Distributionally Robust Offline Reinforcement Learning",
    "authors": [
      "Zhishuai Liu",
      "Pan Xu"
    ],
    "abstract": "Distributionally robust offline reinforcement learning (RL), which seeks\nrobust policy training against environment perturbation by modeling dynamics\nuncertainty, calls for function approximations when facing large state-action\nspaces. However, the consideration of dynamics uncertainty introduces essential\nnonlinearity and computational burden, posing unique challenges for analyzing\nand practically employing function approximation. Focusing on a basic setting\nwhere the nominal model and perturbed models are linearly parameterized, we\npropose minimax optimal and computationally efficient algorithms realizing\nfunction approximation and initiate the study on instance-dependent\nsuboptimality analysis in the context of robust offline RL. Our results uncover\nthat function approximation in robust offline RL is essentially distinct from\nand probably harder than that in standard offline RL. Our algorithms and\ntheoretical results crucially depend on a novel function approximation\nmechanism incorporating variance information, a new procedure of suboptimality\nand estimation uncertainty decomposition, a quantification of the robust value\nfunction shrinkage, and a meticulously designed family of hard instances, which\nmight be of independent interest.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "46 pages, 3 figures, 1 table. Published in Proc. of the 38th\n  Conference on Advances in Neural Information Processing Systems (NeurIPS\n  2024)",
    "pdf_url": "http://arxiv.org/pdf/2403.09621v2",
    "published_date": "2024-03-14 17:55:10 UTC",
    "updated_date": "2025-03-03 22:05:10 UTC"
  },
  {
    "arxiv_id": "2403.09606v3",
    "title": "Large Language Models and Causal Inference in Collaboration: A Survey",
    "authors": [
      "Xiaoyu Liu",
      "Paiheng Xu",
      "Junda Wu",
      "Jiaxin Yuan",
      "Yifan Yang",
      "Yuhang Zhou",
      "Fuxiao Liu",
      "Tianrui Guan",
      "Haoliang Wang",
      "Tong Yu",
      "Julian McAuley",
      "Wei Ai",
      "Furong Huang"
    ],
    "abstract": "Causal inference has shown potential in enhancing the predictive accuracy,\nfairness, robustness, and explainability of Natural Language Processing (NLP)\nmodels by capturing causal relationships among variables. The emergence of\ngenerative Large Language Models (LLMs) has significantly impacted various NLP\ndomains, particularly through their advanced reasoning capabilities. This\nsurvey focuses on evaluating and improving LLMs from a causal view in the\nfollowing areas: understanding and improving the LLMs' reasoning capacity,\naddressing fairness and safety issues in LLMs, complementing LLMs with\nexplanations, and handling multimodality. Meanwhile, LLMs' strong reasoning\ncapacities can in turn contribute to the field of causal inference by aiding\ncausal relationship discovery and causal effect estimations. This review\nexplores the interplay between causal inference frameworks and LLMs from both\nperspectives, emphasizing their collective potential to further the development\nof more advanced and equitable artificial intelligence systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Findings of the Association for Computational Linguistics: NAACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2403.09606v3",
    "published_date": "2024-03-14 17:47:20 UTC",
    "updated_date": "2025-03-21 04:57:45 UTC"
  },
  {
    "arxiv_id": "2403.10568v3",
    "title": "MoPE: Mixture of Prompt Experts for Parameter-Efficient and Scalable Multimodal Fusion",
    "authors": [
      "Ruixiang Jiang",
      "Lingbo Liu",
      "Changwen Chen"
    ],
    "abstract": "Despite the demonstrated parameter efficiency of prompt-based multimodal\nfusion methods, their limited adaptivity and expressiveness often result in\nsuboptimal performance compared to other tuning approaches. In this paper, we\nintroduce the Mixture of Prompt Experts (MoPE), the first technique designed to\novercome these limitations by decomposing standard prompts to capture\ninstance-level features adaptively. Building on this decomposition, MoPE\nenhances prompt fusion's expressiveness by leveraging multimodal pairing priors\nto route the most effective prompt for each instance dynamically. Compared to\nvanilla prompting, our MoPE-based fusion method exhibits greater\nexpressiveness, scaling more effectively with the training data and the overall\nnumber of trainable parameters. We also investigate regularization terms for\nexpert routing, which lead to emergent expert specialization with enhanced\nadaptiveness and interpretablity. Extensive experiments across six multimodal\ndatasets spanning four modalities demonstrate state-of-the-art performance for\nprompt fusion, matching or even surpassing the performance of fine-tuning while\nrequiring only 0.8% of the trainable parameters. Project homepage:\nhttps://github.com/songrise/MoPE",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Under Review, Extended version of arxiv:2312.03734",
    "pdf_url": "http://arxiv.org/pdf/2403.10568v3",
    "published_date": "2024-03-14 17:47:10 UTC",
    "updated_date": "2025-01-14 08:01:17 UTC"
  },
  {
    "arxiv_id": "2403.09605v2",
    "title": "Counterfactual contrastive learning: robust representations via causal image synthesis",
    "authors": [
      "Melanie Roschewitz",
      "Fabio De Sousa Ribeiro",
      "Tian Xia",
      "Galvin Khara",
      "Ben Glocker"
    ],
    "abstract": "Contrastive pretraining is well-known to improve downstream task performance\nand model generalisation, especially in limited label settings. However, it is\nsensitive to the choice of augmentation pipeline. Positive pairs should\npreserve semantic information while destroying domain-specific information.\nStandard augmentation pipelines emulate domain-specific changes with\npre-defined photometric transformations, but what if we could simulate\nrealistic domain changes instead? In this work, we show how to utilise recent\nprogress in counterfactual image generation to this effect. We propose\nCF-SimCLR, a counterfactual contrastive learning approach which leverages\napproximate counterfactual inference for positive pair creation. Comprehensive\nevaluation across five datasets, on chest radiography and mammography,\ndemonstrates that CF-SimCLR substantially improves robustness to acquisition\nshift with higher downstream performance on both in- and out-of-distribution\ndata, particularly for domains which are under-represented during training.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted for publication at the MICCAI 2024 Data Engineering in\n  Medical Imaging workshop. Code available at\n  https://github.com/biomedia-mira/counterfactual-contrastive. Extended version\n  of this work available at arXiv:2409.10365",
    "pdf_url": "http://arxiv.org/pdf/2403.09605v2",
    "published_date": "2024-03-14 17:47:01 UTC",
    "updated_date": "2024-09-17 11:25:18 UTC"
  },
  {
    "arxiv_id": "2403.09603v3",
    "title": "Optimistic Verifiable Training by Controlling Hardware Nondeterminism",
    "authors": [
      "Megha Srivastava",
      "Simran Arora",
      "Dan Boneh"
    ],
    "abstract": "The increasing compute demands of AI systems have led to the emergence of\nservices that train models on behalf of clients lacking necessary resources.\nHowever, ensuring correctness of training and guarding against potential\ntraining-time attacks, such as data poisoning and backdoors, poses challenges.\nExisting works on verifiable training largely fall into two classes:\nproof-based systems, which are difficult to scale, and ``optimistic'' methods\nthat consider a third-party auditor who can replicate the training process and\ncontest the trainer. A key challenge with the latter is that nondeterminism\nbetween GPU types during training prevents exact replication of the training\nprocess, resulting in schemes that are non-robust. We propose a method that\ncombines training in a higher precision than the target, rounding after\nintermediate computations, and sharing rounding decisions based on an adaptive\nthresholding procedure, to successfully control for nondeterminism. Across\nthree different NVIDIA GPUs (A40, Titan XP, RTX 2080 Ti), we achieve exact\ntraining replication at FP32 precision for both full-training and fine-tuning\nof ResNet-50 (23M) and GPT-2 (117M) models. Our verifiable training scheme\nsignificantly decreases the storage and time costs compared to proof-based\nsystems, and is publicly released at\nhttps://github.com/meghabyte/verifiable-training.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "11 pages, 5 figures, Neural Information Processing Systems (NeurIPS)\n  2024,",
    "pdf_url": "http://arxiv.org/pdf/2403.09603v3",
    "published_date": "2024-03-14 17:44:35 UTC",
    "updated_date": "2024-11-25 09:13:47 UTC"
  },
  {
    "arxiv_id": "2403.09580v2",
    "title": "Algorithmic syntactic causal identification",
    "authors": [
      "Dhurim Cakiqi",
      "Max A. Little"
    ],
    "abstract": "Causal identification in causal Bayes nets (CBNs) is an important tool in\ncausal inference allowing the derivation of interventional distributions from\nobservational distributions where this is possible in principle. However, most\nexisting formulations of causal identification using techniques such as\nd-separation and do-calculus are expressed within the mathematical language of\nclassical probability theory on CBNs. However, there are many causal settings\nwhere probability theory and hence current causal identification techniques are\ninapplicable such as relational databases, dataflow programs such as hardware\ndescription languages, distributed systems and most modern machine learning\nalgorithms. We show that this restriction can be lifted by replacing the use of\nclassical probability theory with the alternative axiomatic foundation of\nsymmetric monoidal categories. In this alternative axiomatization, we show how\nan unambiguous and clean distinction can be drawn between the general syntax of\ncausal models and any specific semantic implementation of that causal model.\nThis allows a purely syntactic algorithmic description of general causal\nidentification by a translation of recent formulations of the general ID\nalgorithm through fixing. Our description is given entirely in terms of the\nnon-parametric ADMG structure specifying a causal model and the algebraic\nsignature of the corresponding monoidal category, to which a sequence of\nmanipulations is then applied so as to arrive at a modified monoidal category\nin which the desired, purely syntactic interventional causal model, is\nobtained. We use this idea to derive purely syntactic analogues of classical\nback-door and front-door causal adjustment, and illustrate an application to a\nmore complex causal model.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.ME"
    ],
    "primary_category": "cs.AI",
    "comment": "11 pages, 2 TikZ figures",
    "pdf_url": "http://arxiv.org/pdf/2403.09580v2",
    "published_date": "2024-03-14 17:14:53 UTC",
    "updated_date": "2025-01-29 14:41:51 UTC"
  },
  {
    "arxiv_id": "2403.09567v3",
    "title": "Enhancing Trust in Autonomous Agents: An Architecture for Accountability and Explainability through Blockchain and Large Language Models",
    "authors": [
      "Laura Fernández-Becerra",
      "Miguel Ángel González-Santamarta",
      "Ángel Manuel Guerrero-Higueras",
      "Francisco Javier Rodríguez-Lera",
      "Vicente Matellán Olivera"
    ],
    "abstract": "The deployment of autonomous agents in environments involving human\ninteraction has increasingly raised security concerns. Consequently,\nunderstanding the circumstances behind an event becomes critical, requiring the\ndevelopment of capabilities to justify their behaviors to non-expert users.\nSuch explanations are essential in enhancing trustworthiness and safety, acting\nas a preventive measure against failures, errors, and misunderstandings.\nAdditionally, they contribute to improving communication, bridging the gap\nbetween the agent and the user, thereby improving the effectiveness of their\ninteractions. This work presents an accountability and explainability\narchitecture implemented for ROS-based mobile robots. The proposed solution\nconsists of two main components. Firstly, a black box-like element to provide\naccountability, featuring anti-tampering properties achieved through blockchain\ntechnology. Secondly, a component in charge of generating natural language\nexplanations by harnessing the capabilities of Large Language Models (LLMs)\nover the data contained within the previously mentioned black box. The study\nevaluates the performance of our solution in three different scenarios, each\ninvolving autonomous agent navigation functionalities. This evaluation includes\na thorough examination of accountability and explainability metrics,\ndemonstrating the effectiveness of our approach in using accountable data from\nrobot actions to obtain coherent, accurate and understandable explanations,\neven when facing challenges inherent in the use of autonomous agents in\nreal-world scenarios.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.09567v3",
    "published_date": "2024-03-14 16:57:18 UTC",
    "updated_date": "2024-12-19 19:08:29 UTC"
  },
  {
    "arxiv_id": "2403.09565v1",
    "title": "Welcome Your New AI Teammate: On Safety Analysis by Leashing Large Language Models",
    "authors": [
      "Ali Nouri",
      "Beatriz Cabrero-Daniel",
      "Fredrik Törner",
      "Hȧkan Sivencrona",
      "Christian Berger"
    ],
    "abstract": "DevOps is a necessity in many industries, including the development of\nAutonomous Vehicles. In those settings, there are iterative activities that\nreduce the speed of SafetyOps cycles. One of these activities is \"Hazard\nAnalysis & Risk Assessment\" (HARA), which is an essential step to start the\nsafety requirements specification. As a potential approach to increase the\nspeed of this step in SafetyOps, we have delved into the capabilities of Large\nLanguage Models (LLMs).\n  Our objective is to systematically assess their potential for application in\nthe field of safety engineering. To that end, we propose a framework to support\na higher degree of automation of HARA with LLMs. Despite our endeavors to\nautomate as much of the process as possible, expert review remains crucial to\nensure the validity and correctness of the analysis results, with necessary\nmodifications made accordingly.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted in CAIN 2024, 6 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2403.09565v1",
    "published_date": "2024-03-14 16:56:52 UTC",
    "updated_date": "2024-03-14 16:56:52 UTC"
  },
  {
    "arxiv_id": "2403.10566v1",
    "title": "Cooling-Guide Diffusion Model for Battery Cell Arrangement",
    "authors": [
      "Nicholas Sung",
      "Liu Zheng",
      "Pingfeng Wang",
      "Faez Ahmed"
    ],
    "abstract": "Our study introduces a Generative AI method that employs a cooling-guided\ndiffusion model to optimize the layout of battery cells, a crucial step for\nenhancing the cooling performance and efficiency of battery thermal management\nsystems. Traditional design processes, which rely heavily on iterative\noptimization and extensive guesswork, are notoriously slow and inefficient,\noften leading to suboptimal solutions. In contrast, our innovative method uses\na parametric denoising diffusion probabilistic model (DDPM) with classifier and\ncooling guidance to generate optimized cell layouts with enhanced cooling\npaths, significantly lowering the maximum temperature of the cells. By\nincorporating position-based classifier guidance, we ensure the feasibility of\ngenerated layouts. Meanwhile, cooling guidance directly optimizes\ncooling-efficiency, making our approach uniquely effective. When compared to\ntwo advanced models, the Tabular Denoising Diffusion Probabilistic Model\n(TabDDPM) and the Conditional Tabular GAN (CTGAN), our cooling-guided diffusion\nmodel notably outperforms both. It is five times more effective than TabDDPM\nand sixty-six times better than CTGAN across key metrics such as feasibility,\ndiversity, and cooling efficiency. This research marks a significant leap\nforward in the field, aiming to optimize battery cell layouts for superior\ncooling efficiency, thus setting the stage for the development of more\neffective and dependable battery thermal management systems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.10566v1",
    "published_date": "2024-03-14 16:51:51 UTC",
    "updated_date": "2024-03-14 16:51:51 UTC"
  },
  {
    "arxiv_id": "2403.09549v3",
    "title": "Generalizing Denoising to Non-Equilibrium Structures Improves Equivariant Force Fields",
    "authors": [
      "Yi-Lun Liao",
      "Tess Smidt",
      "Muhammed Shuaibi",
      "Abhishek Das"
    ],
    "abstract": "Understanding the interactions of atoms such as forces in 3D atomistic\nsystems is fundamental to many applications like molecular dynamics and\ncatalyst design. However, simulating these interactions requires\ncompute-intensive ab initio calculations and thus results in limited data for\ntraining neural networks. In this paper, we propose to use denoising\nnon-equilibrium structures (DeNS) as an auxiliary task to better leverage\ntraining data and improve performance. For training with DeNS, we first corrupt\na 3D structure by adding noise to its 3D coordinates and then predict the\nnoise. Different from previous works on denoising, which are limited to\nequilibrium structures, the proposed method generalizes denoising to a much\nlarger set of non-equilibrium structures. The main difference is that a\nnon-equilibrium structure does not correspond to local energy minima and has\nnon-zero forces, and therefore it can have many possible atomic positions\ncompared to an equilibrium structure. This makes denoising non-equilibrium\nstructures an ill-posed problem since the target of denoising is not uniquely\ndefined. Our key insight is to additionally encode the forces of the original\nnon-equilibrium structure to specify which non-equilibrium structure we are\ndenoising. Concretely, given a corrupted non-equilibrium structure and the\nforces of the original one, we predict the non-equilibrium structure satisfying\nthe input forces instead of any arbitrary structures. Since DeNS requires\nencoding forces, DeNS favors equivariant networks, which can easily incorporate\nforces and other higher-order tensors in node embeddings. We study the\neffectiveness of training equivariant networks with DeNS on OC20, OC22 and MD17\ndatasets and demonstrate that DeNS can achieve new state-of-the-art results on\nOC20 and OC22 and significantly improve training efficiency on MD17.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.chem-ph",
      "physics.comp-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "Published in Transactions on Machine Learning Research (TMLR)",
    "pdf_url": "http://arxiv.org/pdf/2403.09549v3",
    "published_date": "2024-03-14 16:38:02 UTC",
    "updated_date": "2024-12-19 19:29:47 UTC"
  },
  {
    "arxiv_id": "2403.09539v3",
    "title": "Logits of API-Protected LLMs Leak Proprietary Information",
    "authors": [
      "Matthew Finlayson",
      "Xiang Ren",
      "Swabha Swayamdipta"
    ],
    "abstract": "Large language model (LLM) providers often hide the architectural details and\nparameters of their proprietary models by restricting public access to a\nlimited API. In this work we show that, with only a conservative assumption\nabout the model architecture, it is possible to learn a surprisingly large\namount of non-public information about an API-protected LLM from a relatively\nsmall number of API queries (e.g., costing under $1000 USD for OpenAI's\ngpt-3.5-turbo). Our findings are centered on one key observation: most modern\nLLMs suffer from a softmax bottleneck, which restricts the model outputs to a\nlinear subspace of the full output space. We exploit this fact to unlock\nseveral capabilities, including (but not limited to) obtaining cheap\nfull-vocabulary outputs, auditing for specific types of model updates,\nidentifying the source LLM given a single full LLM output, and even efficiently\ndiscovering the LLM's hidden size. Our empirical investigations show the\neffectiveness of our methods, which allow us to estimate the embedding size of\nOpenAI's gpt-3.5-turbo to be about 4096. Lastly, we discuss ways that LLM\nproviders can guard against these attacks, as well as how these capabilities\ncan be viewed as a feature (rather than a bug) by allowing for greater\ntransparency and accountability.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.LG",
      "68T50",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.09539v3",
    "published_date": "2024-03-14 16:27:49 UTC",
    "updated_date": "2024-11-08 18:56:41 UTC"
  },
  {
    "arxiv_id": "2403.13839v1",
    "title": "depyf: Open the Opaque Box of PyTorch Compiler for Machine Learning Researchers",
    "authors": [
      "Kaichao You",
      "Runsheng Bai",
      "Meng Cao",
      "Jianmin Wang",
      "Ion Stoica",
      "Mingsheng Long"
    ],
    "abstract": "PyTorch \\texttt{2.x} introduces a compiler designed to accelerate deep\nlearning programs. However, for machine learning researchers, adapting to the\nPyTorch compiler to full potential can be challenging. The compiler operates at\nthe Python bytecode level, making it appear as an opaque box. To address this,\nwe introduce \\texttt{depyf}, a tool designed to demystify the inner workings of\nthe PyTorch compiler. \\texttt{depyf} decompiles bytecode generated by PyTorch\nback into equivalent source code, and establishes connections between in-memory\ncode objects and their on-disk source code counterparts. This feature enables\nusers to step through the source code line by line using debuggers, thus\nenhancing their understanding of the underlying processes. Notably,\n\\texttt{depyf} is non-intrusive and user-friendly, primarily relying on two\nconvenient context managers for its core functionality. The project is\n\\href{https://github.com/thuml/depyf}{ openly available} and is recognized as a\n\\href{https://pytorch.org/ecosystem/}{PyTorch ecosystem project}.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.PL"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.13839v1",
    "published_date": "2024-03-14 16:17:14 UTC",
    "updated_date": "2024-03-14 16:17:14 UTC"
  },
  {
    "arxiv_id": "2403.09530v2",
    "title": "VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision Understanding",
    "authors": [
      "Chris Kelly",
      "Luhui Hu",
      "Jiayin Hu",
      "Yu Tian",
      "Deshun Yang",
      "Bang Yang",
      "Cindy Yang",
      "Zihao Li",
      "Zaoshan Huang",
      "Yuexian Zou"
    ],
    "abstract": "The evolution of text to visual components facilitates people's daily lives,\nsuch as generating image, videos from text and identifying the desired elements\nwithin the images. Computer vision models involving the multimodal abilities in\nthe previous days are focused on image detection, classification based on\nwell-defined objects. Large language models (LLMs) introduces the\ntransformation from nature language to visual objects, which present the visual\nlayout for text contexts. OpenAI GPT-4 has emerged as the pinnacle in LLMs,\nwhile the computer vision (CV) domain boasts a plethora of state-of-the-art\n(SOTA) models and algorithms to convert 2D images to their 3D representations.\nHowever, the mismatching between the algorithms with the problem could lead to\nundesired results. In response to this challenge, we propose an unified\nVisionGPT-3D framework to consolidate the state-of-the-art vision models,\nthereby facilitating the development of vision-oriented AI. VisionGPT-3D\nprovides a versatile multimodal framework building upon the strengths of\nmultimodal foundation models. It seamlessly integrates various SOTA vision\nmodels and brings the automation in the selection of SOTA vision models,\nidentifies the suitable 3D mesh creation algorithms corresponding to 2D depth\nmaps analysis, generates optimal results based on diverse multimodal inputs\nsuch as text prompts.\n  Keywords: VisionGPT-3D, 3D vision understanding, Multimodal agent",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 7 figures, pending conference",
    "pdf_url": "http://arxiv.org/pdf/2403.09530v2",
    "published_date": "2024-03-14 16:13:00 UTC",
    "updated_date": "2024-03-22 15:26:05 UTC"
  },
  {
    "arxiv_id": "2403.09762v1",
    "title": "Emotional Intelligence Through Artificial Intelligence : NLP and Deep Learning in the Analysis of Healthcare Texts",
    "authors": [
      "Prashant Kumar Nag",
      "Amit Bhagat",
      "R. Vishnu Priya",
      "Deepak kumar Khare"
    ],
    "abstract": "This manuscript presents a methodical examination of the utilization of\nArtificial Intelligence in the assessment of emotions in texts related to\nhealthcare, with a particular focus on the incorporation of Natural Language\nProcessing and deep learning technologies. We scrutinize numerous research\nstudies that employ AI to augment sentiment analysis, categorize emotions, and\nforecast patient outcomes based on textual information derived from clinical\nnarratives, patient feedback on medications, and online health discussions. The\nreview demonstrates noteworthy progress in the precision of algorithms used for\nsentiment classification, the prognostic capabilities of AI models for\nneurodegenerative diseases, and the creation of AI-powered systems that offer\nsupport in clinical decision-making. Remarkably, the utilization of AI\napplications has exhibited an enhancement in personalized therapy plans by\nintegrating patient sentiment and contributing to the early identification of\nmental health disorders. There persist challenges, which encompass ensuring the\nethical application of AI, safeguarding patient confidentiality, and addressing\npotential biases in algorithmic procedures. Nevertheless, the potential of AI\nto revolutionize healthcare practices is unmistakable, offering a future where\nhealthcare is not only more knowledgeable and efficient but also more\nempathetic and centered around the needs of patients. This investigation\nunderscores the transformative influence of AI on healthcare, delivering a\ncomprehensive comprehension of its role in examining emotional content in\nhealthcare texts and highlighting the trajectory towards a more compassionate\napproach to patient care. The findings advocate for a harmonious synergy\nbetween AI's analytical capabilities and the human aspects of healthcare.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.09762v1",
    "published_date": "2024-03-14 15:58:13 UTC",
    "updated_date": "2024-03-14 15:58:13 UTC"
  },
  {
    "arxiv_id": "2403.09513v1",
    "title": "AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting",
    "authors": [
      "Yu Wang",
      "Xiaogeng Liu",
      "Yu Li",
      "Muhao Chen",
      "Chaowei Xiao"
    ],
    "abstract": "With the advent and widespread deployment of Multimodal Large Language Models\n(MLLMs), the imperative to ensure their safety has become increasingly\npronounced. However, with the integration of additional modalities, MLLMs are\nexposed to new vulnerabilities, rendering them prone to structured-based\njailbreak attacks, where semantic content (e.g., \"harmful text\") has been\ninjected into the images to mislead MLLMs. In this work, we aim to defend\nagainst such threats. Specifically, we propose \\textbf{Ada}ptive\n\\textbf{Shield} Prompting (\\textbf{AdaShield}), which prepends inputs with\ndefense prompts to defend MLLMs against structure-based jailbreak attacks\nwithout fine-tuning MLLMs or training additional modules (e.g., post-stage\ncontent detector). Initially, we present a manually designed static defense\nprompt, which thoroughly examines the image and instruction content step by\nstep and specifies response methods to malicious queries. Furthermore, we\nintroduce an adaptive auto-refinement framework, consisting of a target MLLM\nand a LLM-based defense prompt generator (Defender). These components\ncollaboratively and iteratively communicate to generate a defense prompt.\nExtensive experiments on the popular structure-based jailbreak attacks and\nbenign datasets show that our methods can consistently improve MLLMs'\nrobustness against structure-based jailbreak attacks without compromising the\nmodel's general capabilities evaluated on standard benign tasks. Our code is\navailable at https://github.com/rain305f/AdaShield.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Multimodal Large Language Models Defense, 25 Pages",
    "pdf_url": "http://arxiv.org/pdf/2403.09513v1",
    "published_date": "2024-03-14 15:57:13 UTC",
    "updated_date": "2024-03-14 15:57:13 UTC"
  },
  {
    "arxiv_id": "2403.09510v1",
    "title": "Trust AI Regulation? Discerning users are vital to build trust and effective AI regulation",
    "authors": [
      "Zainab Alalawi",
      "Paolo Bova",
      "Theodor Cimpeanu",
      "Alessandro Di Stefano",
      "Manh Hong Duong",
      "Elias Fernandez Domingos",
      "The Anh Han",
      "Marcus Krellner",
      "Bianca Ogbo",
      "Simon T. Powers",
      "Filippo Zimmaro"
    ],
    "abstract": "There is general agreement that some form of regulation is necessary both for\nAI creators to be incentivised to develop trustworthy systems, and for users to\nactually trust those systems. But there is much debate about what form these\nregulations should take and how they should be implemented. Most work in this\narea has been qualitative, and has not been able to make formal predictions.\nHere, we propose that evolutionary game theory can be used to quantitatively\nmodel the dilemmas faced by users, AI creators, and regulators, and provide\ninsights into the possible effects of different regulatory regimes. We show\nthat creating trustworthy AI and user trust requires regulators to be\nincentivised to regulate effectively. We demonstrate the effectiveness of two\nmechanisms that can achieve this. The first is where governments can recognise\nand reward regulators that do a good job. In that case, if the AI system is not\ntoo risky for users then some level of trustworthy development and user trust\nevolves. We then consider an alternative solution, where users can condition\ntheir trust decision on the effectiveness of the regulators. This leads to\neffective regulation, and consequently the development of trustworthy AI and\nuser trust, provided that the cost of implementing regulations is not too high.\nOur findings highlight the importance of considering the effect of different\nregulatory regimes from an evolutionary game theoretic perspective.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.GT",
      "cs.MA",
      "math.DS"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.09510v1",
    "published_date": "2024-03-14 15:56:39 UTC",
    "updated_date": "2024-03-14 15:56:39 UTC"
  },
  {
    "arxiv_id": "2403.09506v2",
    "title": "Don't Judge by the Look: Towards Motion Coherent Video Representation",
    "authors": [
      "Yitian Zhang",
      "Yue Bai",
      "Huan Wang",
      "Yizhou Wang",
      "Yun Fu"
    ],
    "abstract": "Current training pipelines in object recognition neglect Hue Jittering when\ndoing data augmentation as it not only brings appearance changes that are\ndetrimental to classification, but also the implementation is inefficient in\npractice. In this study, we investigate the effect of hue variance in the\ncontext of video understanding and find this variance to be beneficial since\nstatic appearances are less important in videos that contain motion\ninformation. Based on this observation, we propose a data augmentation method\nfor video understanding, named Motion Coherent Augmentation (MCA), that\nintroduces appearance variation in videos and implicitly encourages the model\nto prioritize motion patterns, rather than static appearances. Concretely, we\npropose an operation SwapMix to efficiently modify the appearance of video\nsamples, and introduce Variation Alignment (VA) to resolve the distribution\nshift caused by SwapMix, enforcing the model to learn appearance invariant\nrepresentations. Comprehensive empirical evaluation across various\narchitectures and different datasets solidly validates the effectiveness and\ngeneralization ability of MCA, and the application of VA in other augmentation\nmethods. Code is available at https://github.com/BeSpontaneous/MCA-pytorch.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ICLR2024",
    "pdf_url": "http://arxiv.org/pdf/2403.09506v2",
    "published_date": "2024-03-14 15:53:04 UTC",
    "updated_date": "2024-03-25 02:45:35 UTC"
  },
  {
    "arxiv_id": "2403.09502v2",
    "title": "EquiAV: Leveraging Equivariance for Audio-Visual Contrastive Learning",
    "authors": [
      "Jongsuk Kim",
      "Hyeongkeun Lee",
      "Kyeongha Rho",
      "Junmo Kim",
      "Joon Son Chung"
    ],
    "abstract": "Recent advancements in self-supervised audio-visual representation learning\nhave demonstrated its potential to capture rich and comprehensive\nrepresentations. However, despite the advantages of data augmentation verified\nin many learning methods, audio-visual learning has struggled to fully harness\nthese benefits, as augmentations can easily disrupt the correspondence between\ninput pairs. To address this limitation, we introduce EquiAV, a novel framework\nthat leverages equivariance for audio-visual contrastive learning. Our approach\nbegins with extending equivariance to audio-visual learning, facilitated by a\nshared attention-based transformation predictor. It enables the aggregation of\nfeatures from diverse augmentations into a representative embedding, providing\nrobust supervision. Notably, this is achieved with minimal computational\noverhead. Extensive ablation studies and qualitative results verify the\neffectiveness of our method. EquiAV outperforms previous works across various\naudio-visual benchmarks. The code is available on\nhttps://github.com/JongSuk1/EquiAV.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages, 3 figures; Accepted to ICML 2024 (camera ready version)",
    "pdf_url": "http://arxiv.org/pdf/2403.09502v2",
    "published_date": "2024-03-14 15:44:19 UTC",
    "updated_date": "2024-06-20 06:23:16 UTC"
  },
  {
    "arxiv_id": "2403.15432v1",
    "title": "BRIEDGE: EEG-Adaptive Edge AI for Multi-Brain to Multi-Robot Interaction",
    "authors": [
      "Jinhui Ouyang",
      "Mingzhu Wu",
      "Xinglin Li",
      "Hanhui Deng",
      "Di Wu"
    ],
    "abstract": "Recent advances in EEG-based BCI technologies have revealed the potential of\nbrain-to-robot collaboration through the integration of sensing, computing,\ncommunication, and control. In this paper, we present BRIEDGE as an end-to-end\nsystem for multi-brain to multi-robot interaction through an EEG-adaptive\nneural network and an encoding-decoding communication framework, as illustrated\nin Fig.1. As depicted, the edge mobile server or edge portable server will\ncollect EEG data from the users and utilize the EEG-adaptive neural network to\nidentify the users' intentions. The encoding-decoding communication framework\nthen encodes the EEG-based semantic information and decodes it into commands in\nthe process of data transmission. To better extract the joint features of\nheterogeneous EEG data as well as enhance classification accuracy, BRIEDGE\nintroduces an informer-based ProbSparse self-attention mechanism. Meanwhile,\nparallel and secure transmissions for multi-user multi-task scenarios under\nphysical channels are addressed by dynamic autoencoder and autodecoder\ncommunications. From mobile computing and edge AI perspectives, model\ncompression schemes composed of pruning, weight sharing, and quantization are\nalso used to deploy lightweight EEG-adaptive models running on both transmitter\nand receiver sides. Based on the effectiveness of these components, a code map\nrepresenting various commands enables multiple users to control multiple\nintelligent agents concurrently. Our experiments in comparison with\nstate-of-the-art works show that BRIEDGE achieves the best classification\naccuracy of heterogeneous EEG data, and more stable performance under noisy\nenvironments.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15432v1",
    "published_date": "2024-03-14 15:43:48 UTC",
    "updated_date": "2024-03-14 15:43:48 UTC"
  },
  {
    "arxiv_id": "2403.09499v3",
    "title": "A Reinforcement Learning Approach to Dairy Farm Battery Management using Q Learning",
    "authors": [
      "Nawazish Ali",
      "Abdul Wahid",
      "Rachael Shaw",
      "Karl Mason"
    ],
    "abstract": "Dairy farming consumes a significant amount of energy, making it an\nenergy-intensive sector within agriculture. Integrating renewable energy\ngeneration into dairy farming could help address this challenge. Effective\nbattery management is important for integrating renewable energy generation.\nManaging battery charging and discharging poses significant challenges because\nof fluctuations in electrical consumption, the intermittent nature of renewable\nenergy generation, and fluctuations in energy prices. Artificial Intelligence\n(AI) has the potential to significantly improve the use of renewable energy in\ndairy farming, however, there is limited research conducted in this particular\ndomain. This research considers Ireland as a case study as it works towards\nattaining its 2030 energy strategy centered on the utilization of renewable\nsources. This study proposes a Q-learning-based algorithm for scheduling\nbattery charging and discharging in a dairy farm setting. This research also\nexplores the effect of the proposed algorithm by adding wind generation data\nand considering additional case studies. The proposed algorithm reduces the\ncost of imported electricity from the grid by 13.41%, peak demand by 2%, and\n24.49% when utilizing wind generation. These results underline how\nreinforcement learning is highly effective in managing batteries in the dairy\nfarming sector.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.09499v3",
    "published_date": "2024-03-14 15:42:26 UTC",
    "updated_date": "2024-05-15 17:11:35 UTC"
  },
  {
    "arxiv_id": "2403.09498v2",
    "title": "From Skepticism to Acceptance: Simulating the Attitude Dynamics Toward Fake News",
    "authors": [
      "Yuhan Liu",
      "Xiuying Chen",
      "Xiaoqing Zhang",
      "Xing Gao",
      "Ji Zhang",
      "Rui Yan"
    ],
    "abstract": "In the digital era, the rapid propagation of fake news and rumors via social\nnetworks brings notable societal challenges and impacts public opinion\nregulation. Traditional fake news modeling typically forecasts the general\npopularity trends of different groups or numerically represents opinions shift.\nHowever, these methods often oversimplify real-world complexities and overlook\nthe rich semantic information of news text. The advent of large language models\n(LLMs) provides the possibility of modeling subtle dynamics of opinion.\nConsequently, in this work, we introduce a Fake news Propagation Simulation\nframework (FPS) based on LLM, which studies the trends and control of fake news\npropagation in detail. Specifically, each agent in the simulation represents an\nindividual with a distinct personality. They are equipped with both short-term\nand long-term memory, as well as a reflective mechanism to mimic human-like\nthinking. Every day, they engage in random opinion exchanges, reflect on their\nthinking, and update their opinions. Our simulation results uncover patterns in\nfake news propagation related to topic relevance, and individual traits,\naligning with real-world observations. Additionally, we evaluate various\nintervention strategies and demonstrate that early and appropriately frequent\ninterventions strike a balance between governance cost and effectiveness,\noffering valuable insights for practical applications. Our study underscores\nthe significant utility and potential of LLMs in combating fake news.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SI",
    "comment": "IJCAI 2024 Oral",
    "pdf_url": "http://arxiv.org/pdf/2403.09498v2",
    "published_date": "2024-03-14 15:40:13 UTC",
    "updated_date": "2024-12-23 08:59:47 UTC"
  },
  {
    "arxiv_id": "2403.09488v3",
    "title": "Rectifying Demonstration Shortcut in In-Context Learning",
    "authors": [
      "Joonwon Jang",
      "Sanghwan Jang",
      "Wonbin Kweon",
      "Minjin Jeon",
      "Hwanjo Yu"
    ],
    "abstract": "Large language models (LLMs) are able to solve various tasks with only a few\ndemonstrations utilizing their in-context learning (ICL) abilities. However,\nLLMs often rely on their pre-trained semantic priors of demonstrations rather\nthan on the input-label relationships to proceed with ICL prediction. In this\nwork, we term this phenomenon as the 'Demonstration Shortcut'. While previous\nworks have primarily focused on improving ICL prediction results for predefined\ntasks, we aim to rectify the Demonstration Shortcut, thereby enabling the LLM\nto effectively learn new input-label relationships from demonstrations. To\nachieve this, we introduce In-Context Calibration, a demonstration-aware\ncalibration method. We evaluate the effectiveness of the proposed method in two\nsettings: (1) the Original ICL Task using the standard label space and (2) the\nTask Learning setting, where the label space is replaced with semantically\nunrelated tokens. In both settings, In-Context Calibration demonstrates\nsubstantial improvements, with results generalized across three LLM families\n(OPT, GPT, and Llama2) under various configurations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "NAACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.09488v3",
    "published_date": "2024-03-14 15:30:14 UTC",
    "updated_date": "2024-04-15 04:29:33 UTC"
  },
  {
    "arxiv_id": "2403.09481v3",
    "title": "Clinical Reasoning over Tabular Data and Text with Bayesian Networks",
    "authors": [
      "Paloma Rabaey",
      "Johannes Deleu",
      "Stefan Heytens",
      "Thomas Demeester"
    ],
    "abstract": "Bayesian networks are well-suited for clinical reasoning on tabular data, but\nare less compatible with natural language data, for which neural networks\nprovide a successful framework. This paper compares and discusses strategies to\naugment Bayesian networks with neural text representations, both in a\ngenerative and discriminative manner. This is illustrated with simulation\nresults for a primary care use case (diagnosis of pneumonia) and discussed in a\nbroader clinical context.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "AI in Medicine 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.09481v3",
    "published_date": "2024-03-14 15:25:23 UTC",
    "updated_date": "2024-05-23 13:41:19 UTC"
  },
  {
    "arxiv_id": "2403.09480v1",
    "title": "What Sketch Explainability Really Means for Downstream Tasks",
    "authors": [
      "Hmrishav Bandyopadhyay",
      "Pinaki Nath Chowdhury",
      "Ayan Kumar Bhunia",
      "Aneeshan Sain",
      "Tao Xiang",
      "Yi-Zhe Song"
    ],
    "abstract": "In this paper, we explore the unique modality of sketch for explainability,\nemphasising the profound impact of human strokes compared to conventional\npixel-oriented studies. Beyond explanations of network behavior, we discern the\ngenuine implications of explainability across diverse downstream sketch-related\ntasks. We propose a lightweight and portable explainability solution -- a\nseamless plugin that integrates effortlessly with any pre-trained model,\neliminating the need for re-training. Demonstrating its adaptability, we\npresent four applications: highly studied retrieval and generation, and\ncompletely novel assisted drawing and sketch adversarial attacks. The\ncentrepiece to our solution is a stroke-level attribution map that takes\ndifferent forms when linked with downstream tasks. By addressing the inherent\nnon-differentiability of rasterisation, we enable explanations at both coarse\nstroke level (SLA) and partial stroke level (P-SLA), each with its advantages\nfor specific downstream tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.09480v1",
    "published_date": "2024-03-14 15:22:33 UTC",
    "updated_date": "2024-03-14 15:22:33 UTC"
  },
  {
    "arxiv_id": "2403.09479v1",
    "title": "Laying the Foundation First? Investigating the Generalization from Atomic Skills to Complex Reasoning Tasks",
    "authors": [
      "Yuncheng Huang",
      "Qianyu He",
      "Yipei Xu",
      "Jiaqing Liang",
      "Yanghua Xiao"
    ],
    "abstract": "Current language models have demonstrated their capability to develop basic\nreasoning, but struggle in more complicated reasoning tasks that require a\ncombination of atomic skills, such as math word problem requiring skills like\narithmetic and unit conversion. Previous methods either do not improve the\ninherent atomic skills of models or not attempt to generalize the atomic skills\nto complex reasoning tasks. In this paper, we first propose a probing framework\nto investigate whether the atomic skill can spontaneously generalize to complex\nreasoning tasks. Then, we introduce a hierarchical curriculum learning training\nstrategy to achieve better skill generalization. In our experiments, we find\nthat atomic skills can not spontaneously generalize to compositional tasks. By\nleveraging hierarchical curriculum learning, we successfully induce\ngeneralization, significantly improve the performance of open-source LMs on\ncomplex reasoning tasks. Promisingly, the skill generalization exhibit\neffective in cross-dataset and cross-domain scenarios. Complex reasoning can\nalso help enhance atomic skills. Our findings offer valuable guidance for\ndesigning better training strategies for complex reasoning tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.09479v1",
    "published_date": "2024-03-14 15:20:54 UTC",
    "updated_date": "2024-03-14 15:20:54 UTC"
  },
  {
    "arxiv_id": "2403.09472v2",
    "title": "Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision",
    "authors": [
      "Zhiqing Sun",
      "Longhui Yu",
      "Yikang Shen",
      "Weiyang Liu",
      "Yiming Yang",
      "Sean Welleck",
      "Chuang Gan"
    ],
    "abstract": "Current AI alignment methodologies rely on human-provided demonstrations or\njudgments, and the learned capabilities of AI systems would be upper-bounded by\nhuman capabilities as a result. This raises a challenging research question:\nHow can we keep improving the systems when their capabilities have surpassed\nthe levels of humans? This paper answers this question in the context of\ntackling hard reasoning tasks (e.g., level 4-5 MATH problems) via learning from\nhuman annotations on easier tasks (e.g., level 1-3 MATH problems), which we\nterm as easy-to-hard generalization. Our key insight is that an evaluator\n(reward model) trained on supervisions for easier tasks can be effectively used\nfor scoring candidate solutions of harder tasks and hence facilitating\neasy-to-hard generalization over different levels of tasks. Based on this\ninsight, we propose a novel approach to scalable alignment, which firstly\ntrains the (process-supervised) reward models on easy problems (e.g., level\n1-3), and then uses them to evaluate the performance of policy models on hard\nproblems. We show that such easy-to-hard generalization from evaluators can\nenable easy-to-hard generalizations in generators either through re-ranking or\nreinforcement learning (RL). Notably, our process-supervised 7b RL model and\n34b model (reranking@1024) achieves an accuracy of 34.0% and 52.5% on MATH500,\nrespectively, despite only using human supervision on easy problems. Our\napproach suggests a promising path toward AI systems that advance beyond the\nfrontier of human supervision.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.09472v2",
    "published_date": "2024-03-14 15:12:38 UTC",
    "updated_date": "2024-12-10 08:54:09 UTC"
  },
  {
    "arxiv_id": "2403.10565v1",
    "title": "PTSD-MDNN : Fusion tardive de réseaux de neurones profonds multimodaux pour la détection du trouble de stress post-traumatique",
    "authors": [
      "Long Nguyen-Phuoc",
      "Renald Gaboriau",
      "Dimitri Delacroix",
      "Laurent Navarro"
    ],
    "abstract": "In order to provide a more objective and quicker way to diagnose\npost-traumatic stress disorder (PTSD), we present PTSD-MDNN which merges two\nunimodal convolutional neural networks and which gives low detection error\nrate. By taking only videos and audios as inputs, the model could be used in\nthe configuration of teleconsultation sessions, in the optimization of patient\njourneys or for human-robot interaction.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CV",
      "cs.SD",
      "eess.IV",
      "q-bio.NC"
    ],
    "primary_category": "eess.AS",
    "comment": "in French language. GRETSI 2023",
    "pdf_url": "http://arxiv.org/pdf/2403.10565v1",
    "published_date": "2024-03-14 14:57:16 UTC",
    "updated_date": "2024-03-14 14:57:16 UTC"
  },
  {
    "arxiv_id": "2403.09442v1",
    "title": "LLM-based agents for automating the enhancement of user story quality: An early report",
    "authors": [
      "Zheying Zhang",
      "Maruf Rayhan",
      "Tomas Herda",
      "Manuel Goisauf",
      "Pekka Abrahamsson"
    ],
    "abstract": "In agile software development, maintaining high-quality user stories is\ncrucial, but also challenging. This study explores the use of large language\nmodels to automatically improve the user story quality in Austrian Post Group\nIT agile teams. We developed a reference model for an Autonomous LLM-based\nAgent System and implemented it at the company. The quality of user stories in\nthe study and the effectiveness of these agents for user story quality\nimprovement was assessed by 11 participants across six agile teams. Our\nfindings demonstrate the potential of LLMs in improving user story quality,\ncontributing to the research on AI role in agile development, and providing a\npractical example of the transformative impact of AI in an industry setting.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "16 pages, 5 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2403.09442v1",
    "published_date": "2024-03-14 14:35:53 UTC",
    "updated_date": "2024-03-14 14:35:53 UTC"
  },
  {
    "arxiv_id": "2403.09439v1",
    "title": "3D-SceneDreamer: Text-Driven 3D-Consistent Scene Generation",
    "authors": [
      "Frank Zhang",
      "Yibo Zhang",
      "Quan Zheng",
      "Rui Ma",
      "Wei Hua",
      "Hujun Bao",
      "Weiwei Xu",
      "Changqing Zou"
    ],
    "abstract": "Text-driven 3D scene generation techniques have made rapid progress in recent\nyears. Their success is mainly attributed to using existing generative models\nto iteratively perform image warping and inpainting to generate 3D scenes.\nHowever, these methods heavily rely on the outputs of existing models, leading\nto error accumulation in geometry and appearance that prevent the models from\nbeing used in various scenarios (e.g., outdoor and unreal scenarios). To\naddress this limitation, we generatively refine the newly generated local views\nby querying and aggregating global 3D information, and then progressively\ngenerate the 3D scene. Specifically, we employ a tri-plane features-based NeRF\nas a unified representation of the 3D scene to constrain global 3D consistency,\nand propose a generative refinement network to synthesize new contents with\nhigher quality by exploiting the natural image prior from 2D diffusion model as\nwell as the global 3D information of the current scene. Our extensive\nexperiments demonstrate that, in comparison to previous methods, our approach\nsupports wide variety of scene generation and arbitrary camera trajectories\nwith improved visual quality and 3D consistency.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "11 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.09439v1",
    "published_date": "2024-03-14 14:31:22 UTC",
    "updated_date": "2024-03-14 14:31:22 UTC"
  },
  {
    "arxiv_id": "2403.09422v1",
    "title": "Mitigating attribute amplification in counterfactual image generation",
    "authors": [
      "Tian Xia",
      "Mélanie Roschewitz",
      "Fabio De Sousa Ribeiro",
      "Charles Jones",
      "Ben Glocker"
    ],
    "abstract": "Causal generative modelling is gaining interest in medical imaging due to its\nability to answer interventional and counterfactual queries. Most work focuses\non generating counterfactual images that look plausible, using auxiliary\nclassifiers to enforce effectiveness of simulated interventions. We investigate\npitfalls in this approach, discovering the issue of attribute amplification,\nwhere unrelated attributes are spuriously affected during interventions,\nleading to biases across protected characteristics and disease status. We show\nthat attribute amplification is caused by the use of hard labels in the\ncounterfactual training process and propose soft counterfactual fine-tuning to\nmitigate this issue. Our method substantially reduces the amplification effect\nwhile maintaining effectiveness of generated images, demonstrated on a large\nchest X-ray dataset. Our work makes an important advancement towards more\nfaithful and unbiased causal modelling in medical imaging.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.09422v1",
    "published_date": "2024-03-14 14:14:47 UTC",
    "updated_date": "2024-03-14 14:14:47 UTC"
  },
  {
    "arxiv_id": "2404.10606v1",
    "title": "InfoCon: Concept Discovery with Generative and Discriminative Informativeness",
    "authors": [
      "Ruizhe Liu",
      "Qian Luo",
      "Yanchao Yang"
    ],
    "abstract": "We focus on the self-supervised discovery of manipulation concepts that can\nbe adapted and reassembled to address various robotic tasks. We propose that\nthe decision to conceptualize a physical procedure should not depend on how we\nname it (semantics) but rather on the significance of the informativeness in\nits representation regarding the low-level physical state and state changes. We\nmodel manipulation concepts (discrete symbols) as generative and discriminative\ngoals and derive metrics that can autonomously link them to meaningful\nsub-trajectories from noisy, unlabeled demonstrations. Specifically, we employ\na trainable codebook containing encodings (concepts) capable of synthesizing\nthe end-state of a sub-trajectory given the current state (generative\ninformativeness). Moreover, the encoding corresponding to a particular\nsub-trajectory should differentiate the state within and outside it and\nconfidently predict the subsequent action based on the gradient of its\ndiscriminative score (discriminative informativeness). These metrics, which do\nnot rely on human annotation, can be seamlessly integrated into a VQ-VAE\nframework, enabling the partitioning of demonstrations into semantically\nconsistent sub-trajectories, fulfilling the purpose of discovering manipulation\nconcepts and the corresponding sub-goal (key) states. We evaluate the\neffectiveness of the learned concepts by training policies that utilize them as\nguidance, demonstrating superior performance compared to other baselines.\nAdditionally, our discovered manipulation concepts compare favorably to\nhuman-annotated ones while saving much manual effort.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "27 pages, 15 figures. Published as a conference paper at ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.10606v1",
    "published_date": "2024-03-14 14:14:04 UTC",
    "updated_date": "2024-03-14 14:14:04 UTC"
  },
  {
    "arxiv_id": "2403.09412v2",
    "title": "OpenGraph: Open-Vocabulary Hierarchical 3D Graph Representation in Large-Scale Outdoor Environments",
    "authors": [
      "Yinan Deng",
      "Jiahui Wang",
      "Jingyu Zhao",
      "Xinyu Tian",
      "Guangyan Chen",
      "Yi Yang",
      "Yufeng Yue"
    ],
    "abstract": "Environment representations endowed with sophisticated semantics are pivotal\nfor facilitating seamless interaction between robots and humans, enabling them\nto effectively carry out various tasks. Open-vocabulary maps, powered by\nVisual-Language models (VLMs), possess inherent advantages, including zero-shot\nlearning and support for open-set classes. However, existing open-vocabulary\nmaps are primarily designed for small-scale environments, such as desktops or\nrooms, and are typically geared towards limited-area tasks involving robotic\nindoor navigation or in-place manipulation. They face challenges in direct\ngeneralization to outdoor environments characterized by numerous objects and\ncomplex tasks, owing to limitations in both understanding level and map\nstructure. In this work, we propose OpenGraph, the first open-vocabulary\nhierarchical graph representation designed for large-scale outdoor\nenvironments. OpenGraph initially extracts instances and their captions from\nvisual images, enhancing textual reasoning by encoding them. Subsequently, it\nachieves 3D incremental object-centric mapping with feature embedding by\nprojecting images onto LiDAR point clouds. Finally, the environment is\nsegmented based on lane graph connectivity to construct a hierarchical graph.\nValidation results from public dataset SemanticKITTI demonstrate that OpenGraph\nachieves the highest segmentation and query accuracy. The source code of\nOpenGraph is publicly available at https://github.com/BIT-DYN/OpenGraph.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.09412v2",
    "published_date": "2024-03-14 14:03:29 UTC",
    "updated_date": "2024-03-28 14:10:08 UTC"
  },
  {
    "arxiv_id": "2403.09410v1",
    "title": "XCoOp: Explainable Prompt Learning for Computer-Aided Diagnosis via Concept-guided Context Optimization",
    "authors": [
      "Yequan Bie",
      "Luyang Luo",
      "Zhixuan Chen",
      "Hao Chen"
    ],
    "abstract": "Utilizing potent representations of the large vision-language models (VLMs)\nto accomplish various downstream tasks has attracted increasing attention.\nWithin this research field, soft prompt learning has become a representative\napproach for efficiently adapting VLMs such as CLIP, to tasks like image\nclassification. However, most existing prompt learning methods learn text\ntokens that are unexplainable, which cannot satisfy the stringent\ninterpretability requirements of Explainable Artificial Intelligence (XAI) in\nhigh-stakes scenarios like healthcare. To address this issue, we propose a\nnovel explainable prompt learning framework that leverages medical knowledge by\naligning the semantics of images, learnable prompts, and clinical\nconcept-driven prompts at multiple granularities. Moreover, our framework\naddresses the lack of valuable concept annotations by eliciting knowledge from\nlarge language models and offers both visual and textual explanations for the\nprompts. Extensive experiments and explainability analyses conducted on various\ndatasets, with and without concept labels, demonstrate that our method\nsimultaneously achieves superior diagnostic performance, flexibility, and\ninterpretability, shedding light on the effectiveness of foundation models in\nfacilitating XAI. The code will be made publically available.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.09410v1",
    "published_date": "2024-03-14 14:02:01 UTC",
    "updated_date": "2024-03-14 14:02:01 UTC"
  },
  {
    "arxiv_id": "2403.09409v1",
    "title": "\"Like a Nesting Doll\": Analyzing Recursion Analogies Generated by CS Students using Large Language Models",
    "authors": [
      "Seth Bernstein",
      "Paul Denny",
      "Juho Leinonen",
      "Lauren Kan",
      "Arto Hellas",
      "Matt Littlefield",
      "Sami Sarsa",
      "Stephen MacNeil"
    ],
    "abstract": "Grasping complex computing concepts often poses a challenge for students who\nstruggle to anchor these new ideas to familiar experiences and understandings.\nTo help with this, a good analogy can bridge the gap between unfamiliar\nconcepts and familiar ones, providing an engaging way to aid understanding.\nHowever, creating effective educational analogies is difficult even for\nexperienced instructors. We investigate to what extent large language models\n(LLMs), specifically ChatGPT, can provide access to personally relevant\nanalogies on demand. Focusing on recursion, a challenging threshold concept, we\nconducted an investigation analyzing the analogies generated by more than 350\nfirst-year computing students. They were provided with a code snippet and\ntasked to generate their own recursion-based analogies using ChatGPT,\noptionally including personally relevant topics in their prompts. We observed a\ngreat deal of diversity in the analogies produced with student-prescribed\ntopics, in contrast to the otherwise generic analogies, highlighting the value\nof student creativity when working with LLMs. Not only did students enjoy the\nactivity and report an improved understanding of recursion, but they described\nmore easily remembering analogies that were personally and culturally relevant.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.HC",
    "comment": "7 pages, 2 figures, ITiCSE 2024 preprint",
    "pdf_url": "http://arxiv.org/pdf/2403.09409v1",
    "published_date": "2024-03-14 14:01:26 UTC",
    "updated_date": "2024-03-14 14:01:26 UTC"
  },
  {
    "arxiv_id": "2403.09407v1",
    "title": "LM2D: Lyrics- and Music-Driven Dance Synthesis",
    "authors": [
      "Wenjie Yin",
      "Xuejiao Zhao",
      "Yi Yu",
      "Hang Yin",
      "Danica Kragic",
      "Mårten Björkman"
    ],
    "abstract": "Dance typically involves professional choreography with complex movements\nthat follow a musical rhythm and can also be influenced by lyrical content. The\nintegration of lyrics in addition to the auditory dimension, enriches the\nfoundational tone and makes motion generation more amenable to its semantic\nmeanings. However, existing dance synthesis methods tend to model motions only\nconditioned on audio signals. In this work, we make two contributions to bridge\nthis gap. First, we propose LM2D, a novel probabilistic architecture that\nincorporates a multimodal diffusion model with consistency distillation,\ndesigned to create dance conditioned on both music and lyrics in one diffusion\ngeneration step. Second, we introduce the first 3D dance-motion dataset that\nencompasses both music and lyrics, obtained with pose estimation technologies.\nWe evaluate our model against music-only baseline models with objective metrics\nand human evaluations, including dancers and choreographers. The results\ndemonstrate LM2D is able to produce realistic and diverse dance matching both\nlyrics and music. A video summary can be accessed at:\nhttps://youtu.be/4XCgvYookvA.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "cs.MM",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.09407v1",
    "published_date": "2024-03-14 13:59:04 UTC",
    "updated_date": "2024-03-14 13:59:04 UTC"
  },
  {
    "arxiv_id": "2403.09404v2",
    "title": "Heuristic Reasoning in AI: Instrumental Use and Mimetic Absorption",
    "authors": [
      "Anirban Mukherjee",
      "Hannah Hanwen Chang"
    ],
    "abstract": "Deviating from conventional perspectives that frame artificial intelligence\n(AI) systems solely as logic emulators, we propose a novel program of heuristic\nreasoning. We distinguish between the 'instrumental' use of heuristics to match\nresources with objectives, and 'mimetic absorption,' whereby heuristics\nmanifest randomly and universally. Through a series of innovative experiments,\nincluding variations of the classic Linda problem and a novel application of\nthe Beauty Contest game, we uncover trade-offs between maximizing accuracy and\nreducing effort that shape the conditions under which AIs transition between\nexhaustive logical processing and the use of cognitive shortcuts (heuristics).\nWe provide evidence that AIs manifest an adaptive balancing of precision and\nefficiency, consistent with principles of resource-rational human cognition as\nexplicated in classical theories of bounded rationality and dual-process\ntheory. Our findings reveal a nuanced picture of AI cognition, where trade-offs\nbetween resources and objectives lead to the emulation of biological systems,\nespecially human cognition, despite AIs being designed without a sense of self\nand lacking introspective capabilities.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.09404v2",
    "published_date": "2024-03-14 13:53:05 UTC",
    "updated_date": "2024-03-18 12:45:01 UTC"
  },
  {
    "arxiv_id": "2404.07946v1",
    "title": "Towards Faster Training of Diffusion Models: An Inspiration of A Consistency Phenomenon",
    "authors": [
      "Tianshuo Xu",
      "Peng Mi",
      "Ruilin Wang",
      "Yingcong Chen"
    ],
    "abstract": "Diffusion models (DMs) are a powerful generative framework that have\nattracted significant attention in recent years. However, the high\ncomputational cost of training DMs limits their practical applications. In this\npaper, we start with a consistency phenomenon of DMs: we observe that DMs with\ndifferent initializations or even different architectures can produce very\nsimilar outputs given the same noise inputs, which is rare in other generative\nmodels. We attribute this phenomenon to two factors: (1) the learning\ndifficulty of DMs is lower when the noise-prediction diffusion model approaches\nthe upper bound of the timestep (the input becomes pure noise), where the\nstructural information of the output is usually generated; and (2) the loss\nlandscape of DMs is highly smooth, which implies that the model tends to\nconverge to similar local minima and exhibit similar behavior patterns. This\nfinding not only reveals the stability of DMs, but also inspires us to devise\ntwo strategies to accelerate the training of DMs. First, we propose a\ncurriculum learning based timestep schedule, which leverages the noise rate as\nan explicit indicator of the learning difficulty and gradually reduces the\ntraining frequency of easier timesteps, thus improving the training efficiency.\nSecond, we propose a momentum decay strategy, which reduces the momentum\ncoefficient during the optimization process, as the large momentum may hinder\nthe convergence speed and cause oscillations due to the smoothness of the loss\nlandscape. We demonstrate the effectiveness of our proposed strategies on\nvarious models and show that they can significantly reduce the training time\nand improve the quality of the generated images.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.07946v1",
    "published_date": "2024-03-14 13:27:04 UTC",
    "updated_date": "2024-03-14 13:27:04 UTC"
  },
  {
    "arxiv_id": "2403.12093v3",
    "title": "Learning Macroeconomic Policies based on Microfoundations: A Stackelberg Mean Field Game Approach",
    "authors": [
      "Qirui Mi",
      "Zhiyu Zhao",
      "Siyu Xia",
      "Yan Song",
      "Jun Wang",
      "Haifeng Zhang"
    ],
    "abstract": "The Lucas critique emphasizes the importance of considering microfoundations,\nhow micro-agents (i.e., households) respond to policy changes, in macroeconomic\npolicymaking. However, due to the vast scale and complex dynamics among\nmicro-agents, predicting microfoundations is challenging. Consequently, this\npaper introduces a Stackelberg Mean Field Game (SMFG) approach that models\nmacroeconomic policymaking based on microfoundations, with the government as\nthe leader and micro-agents as dynamic followers. This approach treats\nlarge-scale micro-agents as a population, to optimize macroeconomic policies by\nlearning the dynamic response of this micro-population. Our experimental\nresults indicate that the SMFG approach outperforms real-world macroeconomic\npolicies, existing AI-based and economic methods, enabling the learned\nmacroeconomic policy to achieve the highest performance while guiding\nlarge-scale micro-agents toward maximal social welfare. Additionally, when\nextended to real-world scenarios, households that do not adopt the SMFG policy\nexperience lower utility and wealth than adopters, thereby increasing the\nattractiveness of our policy. In summary, this paper contributes to the field\nof AI for economics by offering an effective tool for modeling and solving\nmacroeconomic policymaking issues.",
    "categories": [
      "econ.TH",
      "cs.AI"
    ],
    "primary_category": "econ.TH",
    "comment": "17 pages, 9 figures, 7 tables",
    "pdf_url": "http://arxiv.org/pdf/2403.12093v3",
    "published_date": "2024-03-14 13:22:31 UTC",
    "updated_date": "2024-10-17 08:08:54 UTC"
  },
  {
    "arxiv_id": "2403.09361v1",
    "title": "A Multi-population Integrated Approach for Capacitated Location Routing",
    "authors": [
      "Pengfei He",
      "Jin-Kao Hao",
      "Qinghua Wu"
    ],
    "abstract": "The capacitated location-routing problem involves determining the depots from\na set of candidate capacitated depot locations and finding the required routes\nfrom the selected depots to serve a set of customers whereas minimizing a cost\nfunction that includes the cost of opening the chosen depots, the fixed\nutilization cost per vehicle used, and the total cost (distance) of the routes.\nThis paper presents a multi-population integrated framework in which a\nmulti-depot edge assembly crossover generates promising offspring solutions\nfrom the perspective of both depot location and route edge assembly. The method\nincludes an effective neighborhood-based local search, a feasibility-restoring\nprocedure and a diversification-oriented mutation. Of particular interest is\nthe multi-population scheme which organizes the population into multiple\nsubpopulations based on depot configurations. Extensive experiments on 281\nbenchmark instances from the literature show that the algorithm performs\nremarkably well, by improving 101 best-known results (new upper bounds) and\nmatching 84 best-known results. Additional experiments are presented to gain\ninsight into the role of the key elements of the algorithm.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.09361v1",
    "published_date": "2024-03-14 13:11:30 UTC",
    "updated_date": "2024-03-14 13:11:30 UTC"
  },
  {
    "arxiv_id": "2403.09359v1",
    "title": "D3T: Distinctive Dual-Domain Teacher Zigzagging Across RGB-Thermal Gap for Domain-Adaptive Object Detection",
    "authors": [
      "Dinh Phat Do",
      "Taehoon Kim",
      "Jaemin Na",
      "Jiwon Kim",
      "Keonho Lee",
      "Kyunghwan Cho",
      "Wonjun Hwang"
    ],
    "abstract": "Domain adaptation for object detection typically entails transferring\nknowledge from one visible domain to another visible domain. However, there are\nlimited studies on adapting from the visible to the thermal domain, because the\ndomain gap between the visible and thermal domains is much larger than\nexpected, and traditional domain adaptation can not successfully facilitate\nlearning in this situation. To overcome this challenge, we propose a\nDistinctive Dual-Domain Teacher (D3T) framework that employs distinct training\nparadigms for each domain. Specifically, we segregate the source and target\ntraining sets for building dual-teachers and successively deploy exponential\nmoving average to the student model to individual teachers of each domain. The\nframework further incorporates a zigzag learning method between dual teachers,\nfacilitating a gradual transition from the visible to thermal domains during\ntraining. We validate the superiority of our method through newly designed\nexperimental protocols with well-known thermal datasets, i.e., FLIR and KAIST.\nSource code is available at https://github.com/EdwardDo69/D3T .",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by CVPR 2024. Link: https://github.com/EdwardDo69/D3T",
    "pdf_url": "http://arxiv.org/pdf/2403.09359v1",
    "published_date": "2024-03-14 13:05:43 UTC",
    "updated_date": "2024-03-14 13:05:43 UTC"
  },
  {
    "arxiv_id": "2403.09346v2",
    "title": "B-AVIBench: Towards Evaluating the Robustness of Large Vision-Language Model on Black-box Adversarial Visual-Instructions",
    "authors": [
      "Hao Zhang",
      "Wenqi Shao",
      "Hong Liu",
      "Yongqiang Ma",
      "Ping Luo",
      "Yu Qiao",
      "Nanning Zheng",
      "Kaipeng Zhang"
    ],
    "abstract": "Large Vision-Language Models (LVLMs) have shown significant progress in\nresponding well to visual-instructions from users. However, these instructions,\nencompassing images and text, are susceptible to both intentional and\ninadvertent attacks. Despite the critical importance of LVLMs' robustness\nagainst such threats, current research in this area remains limited. To bridge\nthis gap, we introduce B-AVIBench, a framework designed to analyze the\nrobustness of LVLMs when facing various Black-box Adversarial\nVisual-Instructions (B-AVIs), including four types of image-based B-AVIs, ten\ntypes of text-based B-AVIs, and nine types of content bias B-AVIs (such as\ngender, violence, cultural, and racial biases, among others). We generate 316K\nB-AVIs encompassing five categories of multimodal capabilities (ten tasks) and\ncontent bias. We then conduct a comprehensive evaluation involving 14\nopen-source LVLMs to assess their performance. B-AVIBench also serves as a\nconvenient tool for practitioners to evaluate the robustness of LVLMs against\nB-AVIs. Our findings and extensive experimental results shed light on the\nvulnerabilities of LVLMs, and highlight that inherent biases exist even in\nadvanced closed-source LVLMs like GeminiProVision and GPT-4V. This underscores\nthe importance of enhancing the robustness, security, and fairness of LVLMs.\nThe source code and benchmark are available at\nhttps://github.com/zhanghao5201/B-AVIBench.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by IEEE Transactions on Information Forensics & Security",
    "pdf_url": "http://arxiv.org/pdf/2403.09346v2",
    "published_date": "2024-03-14 12:51:07 UTC",
    "updated_date": "2024-12-28 07:32:00 UTC"
  },
  {
    "arxiv_id": "2403.09344v1",
    "title": "SketchINR: A First Look into Sketches as Implicit Neural Representations",
    "authors": [
      "Hmrishav Bandyopadhyay",
      "Ayan Kumar Bhunia",
      "Pinaki Nath Chowdhury",
      "Aneeshan Sain",
      "Tao Xiang",
      "Timothy Hospedales",
      "Yi-Zhe Song"
    ],
    "abstract": "We propose SketchINR, to advance the representation of vector sketches with\nimplicit neural models. A variable length vector sketch is compressed into a\nlatent space of fixed dimension that implicitly encodes the underlying shape as\na function of time and strokes. The learned function predicts the $xy$ point\ncoordinates in a sketch at each time and stroke. Despite its simplicity,\nSketchINR outperforms existing representations at multiple tasks: (i) Encoding\nan entire sketch dataset into a fixed size latent vector, SketchINR gives\n$60\\times$ and $10\\times$ data compression over raster and vector sketches,\nrespectively. (ii) SketchINR's auto-decoder provides a much higher-fidelity\nrepresentation than other learned vector sketch representations, and is\nuniquely able to scale to complex vector sketches such as FS-COCO. (iii)\nSketchINR supports parallelisation that can decode/render $\\sim$$100\\times$\nfaster than other learned vector representations such as SketchRNN. (iv)\nSketchINR, for the first time, emulates the human ability to reproduce a sketch\nwith varying abstraction in terms of number and complexity of strokes. As a\nfirst look at implicit sketches, SketchINR's compact high-fidelity\nrepresentation will support future work in modelling long and complex sketches.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.09344v1",
    "published_date": "2024-03-14 12:49:29 UTC",
    "updated_date": "2024-03-14 12:49:29 UTC"
  },
  {
    "arxiv_id": "2403.09338v1",
    "title": "LocalMamba: Visual State Space Model with Windowed Selective Scan",
    "authors": [
      "Tao Huang",
      "Xiaohuan Pei",
      "Shan You",
      "Fei Wang",
      "Chen Qian",
      "Chang Xu"
    ],
    "abstract": "Recent advancements in state space models, notably Mamba, have demonstrated\nsignificant progress in modeling long sequences for tasks like language\nunderstanding. Yet, their application in vision tasks has not markedly\nsurpassed the performance of traditional Convolutional Neural Networks (CNNs)\nand Vision Transformers (ViTs). This paper posits that the key to enhancing\nVision Mamba (ViM) lies in optimizing scan directions for sequence modeling.\nTraditional ViM approaches, which flatten spatial tokens, overlook the\npreservation of local 2D dependencies, thereby elongating the distance between\nadjacent tokens. We introduce a novel local scanning strategy that divides\nimages into distinct windows, effectively capturing local dependencies while\nmaintaining a global perspective. Additionally, acknowledging the varying\npreferences for scan patterns across different network layers, we propose a\ndynamic method to independently search for the optimal scan choices for each\nlayer, substantially improving performance. Extensive experiments across both\nplain and hierarchical models underscore our approach's superiority in\neffectively capturing image representations. For example, our model\nsignificantly outperforms Vim-Ti by 3.1% on ImageNet with the same 1.5G FLOPs.\nCode is available at: https://github.com/hunto/LocalMamba.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.09338v1",
    "published_date": "2024-03-14 12:32:40 UTC",
    "updated_date": "2024-03-14 12:32:40 UTC"
  },
  {
    "arxiv_id": "2403.09333v1",
    "title": "Griffon v2: Advancing Multimodal Perception with High-Resolution Scaling and Visual-Language Co-Referring",
    "authors": [
      "Yufei Zhan",
      "Yousong Zhu",
      "Hongyin Zhao",
      "Fan Yang",
      "Ming Tang",
      "Jinqiao Wang"
    ],
    "abstract": "Large Vision Language Models have achieved fine-grained object perception,\nbut the limitation of image resolution remains a significant obstacle to\nsurpass the performance of task-specific experts in complex and dense\nscenarios. Such limitation further restricts the model's potential to achieve\nnuanced visual and language referring in domains such as GUI Agents, Counting\nand \\etc. To address this issue, we introduce a unified high-resolution\ngeneralist model, Griffon v2, enabling flexible object referring with visual\nand textual prompts. To efficiently scaling up image resolution, we design a\nsimple and lightweight down-sampling projector to overcome the input tokens\nconstraint in Large Language Models. This design inherently preserves the\ncomplete contexts and fine details, and significantly improves multimodal\nperception ability especially for small objects. Building upon this, we further\nequip the model with visual-language co-referring capabilities through a\nplug-and-play visual tokenizer. It enables user-friendly interaction with\nflexible target images, free-form texts and even coordinates. Experiments\ndemonstrate that Griffon v2 can localize any objects of interest with visual\nand textual referring, achieve state-of-the-art performance on REC, phrase\ngrounding, and REG tasks, and outperform expert models in object detection and\nobject counting. Data, codes and models will be released at\nhttps://github.com/jefferyZhan/Griffon.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Tech report working in progress. Codes, models and datasets will be\n  released at https://github.com/jefferyZhan/Griffon",
    "pdf_url": "http://arxiv.org/pdf/2403.09333v1",
    "published_date": "2024-03-14 12:21:37 UTC",
    "updated_date": "2024-03-14 12:21:37 UTC"
  },
  {
    "arxiv_id": "2403.09326v4",
    "title": "HeadEvolver: Text to Head Avatars via Expressive and Attribute-Preserving Mesh Deformation",
    "authors": [
      "Duotun Wang",
      "Hengyu Meng",
      "Zeyu Cai",
      "Zhijing Shao",
      "Qianxi Liu",
      "Lin Wang",
      "Mingming Fan",
      "Xiaohang Zhan",
      "Zeyu Wang"
    ],
    "abstract": "Current text-to-avatar methods often rely on implicit representations (e.g.,\nNeRF, SDF, and DMTet), leading to 3D content that artists cannot easily edit\nand animate in graphics software. This paper introduces a novel framework for\ngenerating stylized head avatars from text guidance, which leverages locally\nlearnable mesh deformation and 2D diffusion priors to achieve high-quality\ndigital assets for attribute-preserving manipulation. Given a template mesh,\nour method represents mesh deformation with per-face Jacobians and adaptively\nmodulates local deformation using a learnable vector field. This vector field\nenables anisotropic scaling while preserving the rotation of vertices, which\ncan better express identity and geometric details. We employ landmark- and\ncontour-based regularization terms to balance the expressiveness and\nplausibility of generated avatars from multiple views without relying on any\nspecific shape prior. Our framework can generate realistic shapes and textures\nthat can be further edited via text, while supporting seamless editing using\nthe preserved attributes from the template mesh, such as 3DMM parameters,\nblendshapes, and UV coordinates. Extensive experiments demonstrate that our\nframework can generate diverse and expressive head avatars with high-quality\nmeshes that artists can easily manipulate in graphics software, facilitating\ndownstream applications such as efficient asset creation and animation with\npreserved attributes.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "I.2.6; I.3.8"
    ],
    "primary_category": "cs.GR",
    "comment": "13 pages, 20 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.09326v4",
    "published_date": "2024-03-14 12:15:23 UTC",
    "updated_date": "2025-04-30 03:06:06 UTC"
  },
  {
    "arxiv_id": "2403.09317v1",
    "title": "SD-Net: Symmetric-Aware Keypoint Prediction and Domain Adaptation for 6D Pose Estimation In Bin-picking Scenarios",
    "authors": [
      "Ding-Tao Huang",
      "En-Te Lin",
      "Lipeng Chen",
      "Li-Fu Liu",
      "Long Zeng"
    ],
    "abstract": "Despite the success in 6D pose estimation in bin-picking scenarios, existing\nmethods still struggle to produce accurate prediction results for symmetry\nobjects and real world scenarios. The primary bottlenecks include 1) the\nambiguity keypoints caused by object symmetries; 2) the domain gap between real\nand synthetic data. To circumvent these problem, we propose a new 6D pose\nestimation network with symmetric-aware keypoint prediction and self-training\ndomain adaptation (SD-Net). SD-Net builds on pointwise keypoint regression and\ndeep hough voting to perform reliable detection keypoint under clutter and\nocclusion. Specifically, at the keypoint prediction stage, we designe a robust\n3D keypoints selection strategy considering the symmetry class of objects and\nequivalent keypoints, which facilitate locating 3D keypoints even in highly\noccluded scenes. Additionally, we build an effective filtering algorithm on\npredicted keypoint to dynamically eliminate multiple ambiguity and outlier\nkeypoint candidates. At the domain adaptation stage, we propose the\nself-training framework using a student-teacher training scheme. To carefully\ndistinguish reliable predictions, we harnesses a tailored heuristics for 3D\ngeometry pseudo labelling based on semi-chamfer distance. On public Sil'eane\ndataset, SD-Net achieves state-of-the-art results, obtaining an average\nprecision of 96%. Testing learning and generalization abilities on public\nParametric datasets, SD-Net is 8% higher than the state-of-the-art method. The\ncode is available at https://github.com/dingthuang/SD-Net.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.09317v1",
    "published_date": "2024-03-14 12:08:44 UTC",
    "updated_date": "2024-03-14 12:08:44 UTC"
  },
  {
    "arxiv_id": "2403.09753v1",
    "title": "SpokeN-100: A Cross-Lingual Benchmarking Dataset for The Classification of Spoken Numbers in Different Languages",
    "authors": [
      "René Groh",
      "Nina Goes",
      "Andreas M. Kist"
    ],
    "abstract": "Benchmarking plays a pivotal role in assessing and enhancing the performance\nof compact deep learning models designed for execution on resource-constrained\ndevices, such as microcontrollers. Our study introduces a novel, entirely\nartificially generated benchmarking dataset tailored for speech recognition,\nrepresenting a core challenge in the field of tiny deep learning. SpokeN-100\nconsists of spoken numbers from 0 to 99 spoken by 32 different speakers in four\ndifferent languages, namely English, Mandarin, German and French, resulting in\n12,800 audio samples. We determine auditory features and use UMAP (Uniform\nManifold Approximation and Projection for Dimension Reduction) as a\ndimensionality reduction method to show the diversity and richness of the\ndataset. To highlight the use case of the dataset, we introduce two benchmark\ntasks: given an audio sample, classify (i) the used language and/or (ii) the\nspoken number. We optimized state-of-the-art deep neural networks and performed\nan evolutionary neural architecture search to find tiny architectures optimized\nfor the 32-bit ARM Cortex-M4 nRF52840 microcontroller. Our results represent\nthe first benchmark data achieved for SpokeN-100.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted as a full paper by the tinyML Research Symposium 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.09753v1",
    "published_date": "2024-03-14 12:07:37 UTC",
    "updated_date": "2024-03-14 12:07:37 UTC"
  },
  {
    "arxiv_id": "2403.09313v1",
    "title": "Knowledge Distillation in YOLOX-ViT for Side-Scan Sonar Object Detection",
    "authors": [
      "Martin Aubard",
      "László Antal",
      "Ana Madureira",
      "Erika Ábrahám"
    ],
    "abstract": "In this paper we present YOLOX-ViT, a novel object detection model, and\ninvestigate the efficacy of knowledge distillation for model size reduction\nwithout sacrificing performance. Focused on underwater robotics, our research\naddresses key questions about the viability of smaller models and the impact of\nthe visual transformer layer in YOLOX. Furthermore, we introduce a new\nside-scan sonar image dataset, and use it to evaluate our object detector's\nperformance. Results show that knowledge distillation effectively reduces false\npositives in wall detection. Additionally, the introduced visual transformer\nlayer significantly improves object detection accuracy in the underwater\nenvironment. The source code of the knowledge distillation in the YOLOX-ViT is\nat https://github.com/remaro-network/KD-YOLOX-ViT.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.09313v1",
    "published_date": "2024-03-14 12:03:28 UTC",
    "updated_date": "2024-03-14 12:03:28 UTC"
  },
  {
    "arxiv_id": "2403.09752v2",
    "title": "Explainable Machine Learning-Based Security and Privacy Protection Framework for Internet of Medical Things Systems",
    "authors": [
      "Ayoub Si-ahmed",
      "Mohammed Ali Al-Garadi",
      "Narhimene Boustia"
    ],
    "abstract": "The Internet of Medical Things (IoMT) transcends traditional medical\nboundaries, enabling a transition from reactive treatment to proactive\nprevention. This innovative method revolutionizes healthcare by facilitating\nearly disease detection and tailored care, particularly in chronic disease\nmanagement, where IoMT automates treatments based on real-time health data\ncollection. Nonetheless, its benefits are countered by significant security\nchallenges that endanger the lives of its users due to the sensitivity and\nvalue of the processed data, thereby attracting malicious interests. Moreover,\nthe utilization of wireless communication for data transmission exposes medical\ndata to interception and tampering by cybercriminals. Additionally, anomalies\nmay arise due to human error, network interference, or hardware malfunctions.\nIn this context, anomaly detection based on Machine Learning (ML) is an\ninteresting solution, but it comes up against obstacles in terms of\nexplicability and privacy protection. To address these challenges, a new\nframework for Intrusion Detection Systems is introduced, leveraging Artificial\nNeural Networks for intrusion detection while utilizing Federated Learning (FL)\nfor privacy preservation. Additionally, eXplainable Artificial Intelligence\nmethods are incorporated to enhance model explanation and interpretation. The\nefficacy of the proposed framework is evaluated and compared with centralized\napproaches using multiple datasets containing network and medical data,\nsimulating various attack types impacting the confidentiality, integrity, and\navailability of medical and physiological data. The results obtained offer\ncompelling evidence that the FL method performs comparably to the centralized\nmethod, demonstrating high performance. Additionally, it affords the dual\nadvantage of safeguarding privacy and providing model explanation while\nadhering to ethical principles.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "39 pages, 14 figures, 15 tables, journal paper",
    "pdf_url": "http://arxiv.org/pdf/2403.09752v2",
    "published_date": "2024-03-14 11:57:26 UTC",
    "updated_date": "2025-03-01 13:42:04 UTC"
  },
  {
    "arxiv_id": "2403.09290v1",
    "title": "SELECTOR: Heterogeneous graph network with convolutional masked autoencoder for multimodal robust prediction of cancer survival",
    "authors": [
      "Liangrui Pan",
      "Yijun Peng",
      "Yan Li",
      "Xiang Wang",
      "Wenjuan Liu",
      "Liwen Xu",
      "Qingchun Liang",
      "Shaoliang Peng"
    ],
    "abstract": "Accurately predicting the survival rate of cancer patients is crucial for\naiding clinicians in planning appropriate treatment, reducing cancer-related\nmedical expenses, and significantly enhancing patients' quality of life.\nMultimodal prediction of cancer patient survival offers a more comprehensive\nand precise approach. However, existing methods still grapple with challenges\nrelated to missing multimodal data and information interaction within\nmodalities. This paper introduces SELECTOR, a heterogeneous graph-aware network\nbased on convolutional mask encoders for robust multimodal prediction of cancer\npatient survival. SELECTOR comprises feature edge reconstruction, convolutional\nmask encoder, feature cross-fusion, and multimodal survival prediction modules.\nInitially, we construct a multimodal heterogeneous graph and employ the\nmeta-path method for feature edge reconstruction, ensuring comprehensive\nincorporation of feature information from graph edges and effective embedding\nof nodes. To mitigate the impact of missing features within the modality on\nprediction accuracy, we devised a convolutional masked autoencoder (CMAE) to\nprocess the heterogeneous graph post-feature reconstruction. Subsequently, the\nfeature cross-fusion module facilitates communication between modalities,\nensuring that output features encompass all features of the modality and\nrelevant information from other modalities. Extensive experiments and analysis\non six cancer datasets from TCGA demonstrate that our method significantly\noutperforms state-of-the-art methods in both modality-missing and\nintra-modality information-confirmed cases. Our codes are made available at\nhttps://github.com/panliangrui/Selector.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted on Computers in Biology and Medicine",
    "pdf_url": "http://arxiv.org/pdf/2403.09290v1",
    "published_date": "2024-03-14 11:23:39 UTC",
    "updated_date": "2024-03-14 11:23:39 UTC"
  },
  {
    "arxiv_id": "2403.09289v1",
    "title": "Silico-centric Theory of Mind",
    "authors": [
      "Anirban Mukherjee",
      "Hannah Hanwen Chang"
    ],
    "abstract": "Theory of Mind (ToM) refers to the ability to attribute mental states, such\nas beliefs, desires, intentions, and knowledge, to oneself and others, and to\nunderstand that these mental states can differ from one's own and from reality.\nWe investigate ToM in environments with multiple, distinct, independent AI\nagents, each possessing unique internal states, information, and objectives.\nInspired by human false-belief experiments, we present an AI ('focal AI') with\na scenario where its clone undergoes a human-centric ToM assessment. We prompt\nthe focal AI to assess whether its clone would benefit from additional\ninstructions. Concurrently, we give its clones the ToM assessment, both with\nand without the instructions, thereby engaging the focal AI in higher-order\ncounterfactual reasoning akin to human mentalizing--with respect to humans in\none test and to other AI in another. We uncover a discrepancy: Contemporary AI\ndemonstrates near-perfect accuracy on human-centric ToM assessments. Since\ninformation embedded in one AI is identically embedded in its clone, additional\ninstructions are redundant. Yet, we observe AI crafting elaborate instructions\nfor their clones, erroneously anticipating a need for assistance. An\nindependent referee AI agrees with these unsupported expectations. Neither the\nfocal AI nor the referee demonstrates ToM in our 'silico-centric' test.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.09289v1",
    "published_date": "2024-03-14 11:22:51 UTC",
    "updated_date": "2024-03-14 11:22:51 UTC"
  },
  {
    "arxiv_id": "2403.09288v1",
    "title": "Adversarial Training with OCR Modality Perturbation for Scene-Text Visual Question Answering",
    "authors": [
      "Zhixuan Shen",
      "Haonan Luo",
      "Sijia Li",
      "Tianrui Li"
    ],
    "abstract": "Scene-Text Visual Question Answering (ST-VQA) aims to understand scene text\nin images and answer questions related to the text content. Most existing\nmethods heavily rely on the accuracy of Optical Character Recognition (OCR)\nsystems, and aggressive fine-tuning based on limited spatial location\ninformation and erroneous OCR text information often leads to inevitable\noverfitting. In this paper, we propose a multimodal adversarial training\narchitecture with spatial awareness capabilities. Specifically, we introduce an\nAdversarial OCR Enhancement (AOE) module, which leverages adversarial training\nin the embedding space of OCR modality to enhance fault-tolerant representation\nof OCR texts, thereby reducing noise caused by OCR errors. Simultaneously, We\nadd a Spatial-Aware Self-Attention (SASA) mechanism to help the model better\ncapture the spatial relationships among OCR tokens. Various experiments\ndemonstrate that our method achieves significant performance improvements on\nboth the ST-VQA and TextVQA datasets and provides a novel paradigm for\nmultimodal adversarial training.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "6 pages, 3 figures, accepted by 2024 IEEE International Conference on\n  Multimedia and Expo",
    "pdf_url": "http://arxiv.org/pdf/2403.09288v1",
    "published_date": "2024-03-14 11:22:06 UTC",
    "updated_date": "2024-03-14 11:22:06 UTC"
  },
  {
    "arxiv_id": "2403.14690v1",
    "title": "Incorporating Graph Attention Mechanism into Geometric Problem Solving Based on Deep Reinforcement Learning",
    "authors": [
      "Xiuqin Zhong",
      "Shengyuan Yan",
      "Gongqi Lin",
      "Hongguang Fu",
      "Liang Xu",
      "Siwen Jiang",
      "Lei Huang",
      "Wei Fang"
    ],
    "abstract": "In the context of online education, designing an automatic solver for\ngeometric problems has been considered a crucial step towards general math\nArtificial Intelligence (AI), empowered by natural language understanding and\ntraditional logical inference. In most instances, problems are addressed by\nadding auxiliary components such as lines or points. However, adding auxiliary\ncomponents automatically is challenging due to the complexity in selecting\nsuitable auxiliary components especially when pivotal decisions have to be\nmade. The state-of-the-art performance has been achieved by exhausting all\npossible strategies from the category library to identify the one with the\nmaximum likelihood. However, an extensive strategy search have to be applied to\ntrade accuracy for ef-ficiency. To add auxiliary components automatically and\nefficiently, we present deep reinforcement learning framework based on the\nlanguage model, such as BERT. We firstly apply the graph attention mechanism to\nreduce the strategy searching space, called AttnStrategy, which only focus on\nthe conclusion-related components. Meanwhile, a novel algorithm, named\nAutomatically Adding Auxiliary Components using Reinforcement Learning\nframework (A3C-RL), is proposed by forcing an agent to select top strategies,\nwhich incorporates the AttnStrategy and BERT as the memory components. Results\nfrom extensive experiments show that the proposed A3C-RL algorithm can\nsubstantially enhance the average precision by 32.7% compared to the\ntraditional MCTS. In addition, the A3C-RL algorithm outperforms humans on the\ngeometric questions from the annual University Entrance Mathematical\nExamination of China.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14690v1",
    "published_date": "2024-03-14 11:00:09 UTC",
    "updated_date": "2024-03-14 11:00:09 UTC"
  },
  {
    "arxiv_id": "2403.10562v1",
    "title": "Counter-Samples: A Stateless Strategy to Neutralize Black Box Adversarial Attacks",
    "authors": [
      "Roey Bokobza",
      "Yisroel Mirsky"
    ],
    "abstract": "Our paper presents a novel defence against black box attacks, where attackers\nuse the victim model as an oracle to craft their adversarial examples. Unlike\ntraditional preprocessing defences that rely on sanitizing input samples, our\nstateless strategy counters the attack process itself. For every query we\nevaluate a counter-sample instead, where the counter-sample is the original\nsample optimized against the attacker's objective. By countering every black\nbox query with a targeted white box optimization, our strategy effectively\nintroduces an asymmetry to the game to the defender's advantage. This defence\nnot only effectively misleads the attacker's search for an adversarial example,\nit also preserves the model's accuracy on legitimate inputs and is generic to\nmultiple types of attacks.\n  We demonstrate that our approach is remarkably effective against\nstate-of-the-art black box attacks and outperforms existing defences for both\nthe CIFAR-10 and ImageNet datasets. Additionally, we also show that the\nproposed defence is robust against strong adversaries as well.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.10562v1",
    "published_date": "2024-03-14 10:59:54 UTC",
    "updated_date": "2024-03-14 10:59:54 UTC"
  },
  {
    "arxiv_id": "2403.12092v1",
    "title": "Methods for Matching English Language Addresses",
    "authors": [
      "Keshav Ramani",
      "Daniel Borrajo"
    ],
    "abstract": "Addresses occupy a niche location within the landscape of textual data, due\nto the positional importance carried by every word, and the geographical scope\nit refers to. The task of matching addresses happens everyday and is present in\nvarious fields like mail redirection, entity resolution, etc. Our work defines,\nand formalizes a framework to generate matching and mismatching pairs of\naddresses in the English language, and use it to evaluate various methods to\nautomatically perform address matching. These methods vary widely from distance\nbased approaches to deep learning models. By studying the Precision, Recall and\nAccuracy metrics of these approaches, we obtain an understanding of the best\nsuited method for this setting of the address matching task.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.12092v1",
    "published_date": "2024-03-14 10:39:14 UTC",
    "updated_date": "2024-03-14 10:39:14 UTC"
  },
  {
    "arxiv_id": "2403.09249v1",
    "title": "Leveraging Constraint Programming in a Deep Learning Approach for Dynamically Solving the Flexible Job-Shop Scheduling Problem",
    "authors": [
      "Imanol Echeverria",
      "Maialen Murua",
      "Roberto Santana"
    ],
    "abstract": "Recent advancements in the flexible job-shop scheduling problem (FJSSP) are\nprimarily based on deep reinforcement learning (DRL) due to its ability to\ngenerate high-quality, real-time solutions. However, DRL approaches often fail\nto fully harness the strengths of existing techniques such as exact methods or\nconstraint programming (CP), which can excel at finding optimal or near-optimal\nsolutions for smaller instances. This paper aims to integrate CP within a deep\nlearning (DL) based methodology, leveraging the benefits of both. In this\npaper, we introduce a method that involves training a DL model using optimal\nsolutions generated by CP, ensuring the model learns from high-quality data,\nthereby eliminating the need for the extensive exploration typical in DRL and\nenhancing overall performance. Further, we integrate CP into our DL framework\nto jointly construct solutions, utilizing DL for the initial complex stages and\ntransitioning to CP for optimal resolution as the problem is simplified. Our\nhybrid approach has been extensively tested on three public FJSSP benchmarks,\ndemonstrating superior performance over five state-of-the-art DRL approaches\nand a widely-used CP solver. Additionally, with the objective of exploring the\napplication to other combinatorial optimization problems, promising preliminary\nresults are presented on applying our hybrid approach to the traveling salesman\nproblem, combining an exact method with a well-known DRL method.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.09249v1",
    "published_date": "2024-03-14 10:16:57 UTC",
    "updated_date": "2024-03-14 10:16:57 UTC"
  },
  {
    "arxiv_id": "2403.09232v1",
    "title": "Generating Feasible and Plausible Counterfactual Explanations for Outcome Prediction of Business Processes",
    "authors": [
      "Alexander Stevens",
      "Chun Ouyang",
      "Johannes De Smedt",
      "Catarina Moreira"
    ],
    "abstract": "In recent years, various machine and deep learning architectures have been\nsuccessfully introduced to the field of predictive process analytics.\nNevertheless, the inherent opacity of these algorithms poses a significant\nchallenge for human decision-makers, hindering their ability to understand the\nreasoning behind the predictions. This growing concern has sparked the\nintroduction of counterfactual explanations, designed as human-understandable\nwhat if scenarios, to provide clearer insights into the decision-making process\nbehind undesirable predictions. The generation of counterfactual explanations,\nhowever, encounters specific challenges when dealing with the sequential nature\nof the (business) process cases typically used in predictive process analytics.\nOur paper tackles this challenge by introducing a data-driven approach,\nREVISEDplus, to generate more feasible and plausible counterfactual\nexplanations. First, we restrict the counterfactual algorithm to generate\ncounterfactuals that lie within a high-density region of the process data,\nensuring that the proposed counterfactuals are realistic and feasible within\nthe observed process data distribution. Additionally, we ensure plausibility by\nlearning sequential patterns between the activities in the process cases,\nutilising Declare language templates. Finally, we evaluate the properties that\ndefine the validity of counterfactuals.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Journal Submission",
    "pdf_url": "http://arxiv.org/pdf/2403.09232v1",
    "published_date": "2024-03-14 09:56:35 UTC",
    "updated_date": "2024-03-14 09:56:35 UTC"
  },
  {
    "arxiv_id": "2403.09227v1",
    "title": "BEHAVIOR-1K: A Human-Centered, Embodied AI Benchmark with 1,000 Everyday Activities and Realistic Simulation",
    "authors": [
      "Chengshu Li",
      "Ruohan Zhang",
      "Josiah Wong",
      "Cem Gokmen",
      "Sanjana Srivastava",
      "Roberto Martín-Martín",
      "Chen Wang",
      "Gabrael Levine",
      "Wensi Ai",
      "Benjamin Martinez",
      "Hang Yin",
      "Michael Lingelbach",
      "Minjune Hwang",
      "Ayano Hiranaka",
      "Sujay Garlanka",
      "Arman Aydin",
      "Sharon Lee",
      "Jiankai Sun",
      "Mona Anvari",
      "Manasi Sharma",
      "Dhruva Bansal",
      "Samuel Hunter",
      "Kyu-Young Kim",
      "Alan Lou",
      "Caleb R Matthews",
      "Ivan Villa-Renteria",
      "Jerry Huayang Tang",
      "Claire Tang",
      "Fei Xia",
      "Yunzhu Li",
      "Silvio Savarese",
      "Hyowon Gweon",
      "C. Karen Liu",
      "Jiajun Wu",
      "Li Fei-Fei"
    ],
    "abstract": "We present BEHAVIOR-1K, a comprehensive simulation benchmark for\nhuman-centered robotics. BEHAVIOR-1K includes two components, guided and\nmotivated by the results of an extensive survey on \"what do you want robots to\ndo for you?\". The first is the definition of 1,000 everyday activities,\ngrounded in 50 scenes (houses, gardens, restaurants, offices, etc.) with more\nthan 9,000 objects annotated with rich physical and semantic properties. The\nsecond is OMNIGIBSON, a novel simulation environment that supports these\nactivities via realistic physics simulation and rendering of rigid bodies,\ndeformable bodies, and liquids. Our experiments indicate that the activities in\nBEHAVIOR-1K are long-horizon and dependent on complex manipulation skills, both\nof which remain a challenge for even state-of-the-art robot learning solutions.\nTo calibrate the simulation-to-reality gap of BEHAVIOR-1K, we provide an\ninitial study on transferring solutions learned with a mobile manipulator in a\nsimulated apartment to its real-world counterpart. We hope that BEHAVIOR-1K's\nhuman-grounded nature, diversity, and realism make it valuable for embodied AI\nand robot learning research. Project website: https://behavior.stanford.edu.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "A preliminary version was published at 6th Conference on Robot\n  Learning (CoRL 2022)",
    "pdf_url": "http://arxiv.org/pdf/2403.09227v1",
    "published_date": "2024-03-14 09:48:36 UTC",
    "updated_date": "2024-03-14 09:48:36 UTC"
  },
  {
    "arxiv_id": "2403.09751v1",
    "title": "What Was Your Prompt? A Remote Keylogging Attack on AI Assistants",
    "authors": [
      "Roy Weiss",
      "Daniel Ayzenshteyn",
      "Guy Amit",
      "Yisroel Mirsky"
    ],
    "abstract": "AI assistants are becoming an integral part of society, used for asking\nadvice or help in personal and confidential issues. In this paper, we unveil a\nnovel side-channel that can be used to read encrypted responses from AI\nAssistants over the web: the token-length side-channel. We found that many\nvendors, including OpenAI and Microsoft, have this side-channel.\n  However, inferring the content of a response from a token-length sequence\nalone proves challenging. This is because tokens are akin to words, and\nresponses can be several sentences long leading to millions of grammatically\ncorrect sentences. In this paper, we show how this can be overcome by (1)\nutilizing the power of a large language model (LLM) to translate these\nsequences, (2) providing the LLM with inter-sentence context to narrow the\nsearch space and (3) performing a known-plaintext attack by fine-tuning the\nmodel on the target model's writing style.\n  Using these methods, we were able to accurately reconstruct 29\\% of an AI\nassistant's responses and successfully infer the topic from 55\\% of them. To\ndemonstrate the threat, we performed the attack on OpenAI's ChatGPT-4 and\nMicrosoft's Copilot on both browser and API traffic.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.09751v1",
    "published_date": "2024-03-14 09:38:12 UTC",
    "updated_date": "2024-03-14 09:38:12 UTC"
  },
  {
    "arxiv_id": "2403.09215v1",
    "title": "On the Laplace Approximation as Model Selection Criterion for Gaussian Processes",
    "authors": [
      "Andreas Besginow",
      "Jan David Hüwel",
      "Thomas Pawellek",
      "Christian Beecks",
      "Markus Lange-Hegermann"
    ],
    "abstract": "Model selection aims to find the best model in terms of accuracy,\ninterpretability or simplicity, preferably all at once. In this work, we focus\non evaluating model performance of Gaussian process models, i.e. finding a\nmetric that provides the best trade-off between all those criteria. While\nprevious work considers metrics like the likelihood, AIC or dynamic nested\nsampling, they either lack performance or have significant runtime issues,\nwhich severely limits applicability. We address these challenges by introducing\nmultiple metrics based on the Laplace approximation, where we overcome a severe\ninconsistency occuring during naive application of the Laplace approximation.\nExperiments show that our metrics are comparable in quality to the gold\nstandard dynamic nested sampling without compromising for computational speed.\nOur model selection criteria allow significantly faster and high quality model\nselection of Gaussian process models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.09215v1",
    "published_date": "2024-03-14 09:28:28 UTC",
    "updated_date": "2024-03-14 09:28:28 UTC"
  },
  {
    "arxiv_id": "2403.09209v2",
    "title": "LAN: Learning Adaptive Neighbors for Real-Time Insider Threat Detection",
    "authors": [
      "Xiangrui Cai",
      "Yang Wang",
      "Sihan Xu",
      "Hao Li",
      "Ying Zhang",
      "Zheli Liu",
      "Xiaojie Yuan"
    ],
    "abstract": "Enterprises and organizations are faced with potential threats from insider\nemployees that may lead to serious consequences. Previous studies on insider\nthreat detection (ITD) mainly focus on detecting abnormal users or abnormal\ntime periods (e.g., a week or a day). However, a user may have hundreds of\nthousands of activities in the log, and even within a day there may exist\nthousands of activities for a user, requiring a high investigation budget to\nverify abnormal users or activities given the detection results. On the other\nhand, existing works are mainly post-hoc methods rather than real-time\ndetection, which can not report insider threats in time before they cause loss.\nIn this paper, we conduct the first study towards real-time ITD at activity\nlevel, and present a fine-grained and efficient framework LAN. Specifically,\nLAN simultaneously learns the temporal dependencies within an activity sequence\nand the relationships between activities across sequences with graph structure\nlearning. Moreover, to mitigate the data imbalance problem in ITD, we propose a\nnovel hybrid prediction loss, which integrates self-supervision signals from\nnormal activities and supervision signals from abnormal activities into a\nunified loss for anomaly detection. We evaluate the performance of LAN on two\nwidely used datasets, i.e., CERT r4.2 and CERT r5.2. Extensive and comparative\nexperiments demonstrate the superiority of LAN, outperforming 9\nstate-of-the-art baselines by at least 9.92% and 6.35% in AUC for real-time ITD\non CERT r4.2 and r5.2, respectively. Moreover, LAN can be also applied to\npost-hoc ITD, surpassing 8 competitive baselines by at least 7.70% and 4.03% in\nAUC on two datasets. Finally, the ablation study, parameter analysis, and\ncompatibility analysis evaluate the impact of each module and hyper-parameter\nin LAN. The source code can be obtained from https://github.com/Li1Neo/LAN.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "13 pages",
    "pdf_url": "http://arxiv.org/pdf/2403.09209v2",
    "published_date": "2024-03-14 09:22:17 UTC",
    "updated_date": "2024-03-17 14:05:51 UTC"
  },
  {
    "arxiv_id": "2403.09206v1",
    "title": "Upper Bound of Bayesian Generalization Error in Partial Concept Bottleneck Model (CBM): Partial CBM outperforms naive CBM",
    "authors": [
      "Naoki Hayashi",
      "Yoshihide Sawada"
    ],
    "abstract": "Concept Bottleneck Model (CBM) is a methods for explaining neural networks.\nIn CBM, concepts which correspond to reasons of outputs are inserted in the\nlast intermediate layer as observed values. It is expected that we can\ninterpret the relationship between the output and concept similar to linear\nregression. However, this interpretation requires observing all concepts and\ndecreases the generalization performance of neural networks. Partial CBM\n(PCBM), which uses partially observed concepts, has been devised to resolve\nthese difficulties. Although some numerical experiments suggest that the\ngeneralization performance of PCBMs is almost as high as that of the original\nneural networks, the theoretical behavior of its generalization error has not\nbeen yet clarified since PCBM is singular statistical model. In this paper, we\nreveal the Bayesian generalization error in PCBM with a three-layered and\nlinear architecture. The result indcates that the structure of partially\nobserved concepts decreases the Bayesian generalization error compared with\nthat of CBM (full-observed concepts).",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "math.ST",
      "stat.TH",
      "62F15, 62R01, 68T07"
    ],
    "primary_category": "stat.ML",
    "comment": "17 pages, 1 figure, submitted to TMLR",
    "pdf_url": "http://arxiv.org/pdf/2403.09206v1",
    "published_date": "2024-03-14 09:19:50 UTC",
    "updated_date": "2024-03-14 09:19:50 UTC"
  },
  {
    "arxiv_id": "2403.09199v2",
    "title": "Task-Specific Adaptation of Segmentation Foundation Model via Prompt Learning",
    "authors": [
      "Hyung-Il Kim",
      "Kimin Yun",
      "Jun-Seok Yun",
      "Yuseok Bae"
    ],
    "abstract": "Recently, foundation models trained on massive datasets to adapt to a wide\nrange of tasks have attracted considerable attention and are actively being\nexplored within the computer vision community. Among these, the Segment\nAnything Model (SAM) stands out for its remarkable progress in generalizability\nand flexibility for image segmentation tasks, achieved through prompt-based\nobject mask generation. However, despite its strength, SAM faces two key\nlimitations when applied to instance segmentation that segments specific\nobjects or those in unique environments (e.g., task-specific adaptation for\nout-of-distribution objects) not typically present in the training data: 1) the\nambiguity inherent in input prompts and 2) the necessity for extensive\nadditional training to achieve optimal segmentation. To address these\nchallenges, we propose a task-specific adaptation (i.e., customization) of the\nsegmentation foundation model via prompt learning tailored to SAM. Our method\ninvolves a prompt learning module (PLM), which adjusts input prompts into the\nembedding space to better align with peculiarities of the target task, thereby\nenabling more efficient training. Furthermore, we introduce a point matching\nmodule (PMM) to enhance the feature representation for finer segmentation by\nensuring detailed alignment with ground truth boundaries. Experimental results\non various customized segmentation scenarios demonstrate the effectiveness of\nthe proposed method.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Workshop on OOD Generalization in Computer Vision, ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.09199v2",
    "published_date": "2024-03-14 09:13:51 UTC",
    "updated_date": "2024-10-11 04:37:07 UTC"
  },
  {
    "arxiv_id": "2403.09193v2",
    "title": "Can We Talk Models Into Seeing the World Differently?",
    "authors": [
      "Paul Gavrikov",
      "Jovita Lukasik",
      "Steffen Jung",
      "Robert Geirhos",
      "M. Jehanzeb Mirza",
      "Margret Keuper",
      "Janis Keuper"
    ],
    "abstract": "Unlike traditional vision-only models, vision language models (VLMs) offer an\nintuitive way to access visual content through language prompting by combining\na large language model (LLM) with a vision encoder. However, both the LLM and\nthe vision encoder come with their own set of biases, cue preferences, and\nshortcuts, which have been rigorously studied in uni-modal models. A timely\nquestion is how such (potentially misaligned) biases and cue preferences behave\nunder multi-modal fusion in VLMs. As a first step towards a better\nunderstanding, we investigate a particularly well-studied vision-only bias -\nthe texture vs. shape bias and the dominance of local over global information.\nAs expected, we find that VLMs inherit this bias to some extent from their\nvision encoders. Surprisingly, the multi-modality alone proves to have\nimportant effects on the model behavior, i.e., the joint training and the\nlanguage querying change the way visual cues are processed. While this direct\nimpact of language-informed training on a model's visual perception is\nintriguing, it raises further questions on our ability to actively steer a\nmodel's output so that its prediction is based on particular visual cues of the\nuser's choice. Interestingly, VLMs have an inherent tendency to recognize\nobjects based on shape information, which is different from what a plain vision\nencoder would do. Further active steering towards shape-based classifications\nthrough language prompts is however limited. In contrast, active VLM steering\ntowards texture-based decisions through simple natural language prompts is\noften more successful.\n  URL: https://github.com/paulgavrikov/vlm_shapebias",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "q-bio.NC"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2403.09193v2",
    "published_date": "2024-03-14 09:07:14 UTC",
    "updated_date": "2025-03-05 19:01:00 UTC"
  },
  {
    "arxiv_id": "2403.09190v1",
    "title": "Intention-aware Denoising Diffusion Model for Trajectory Prediction",
    "authors": [
      "Chen Liu",
      "Shibo He",
      "Haoyu Liu",
      "Jiming Chen"
    ],
    "abstract": "Trajectory prediction is an essential component in autonomous driving,\nparticularly for collision avoidance systems. Considering the inherent\nuncertainty of the task, numerous studies have utilized generative models to\nproduce multiple plausible future trajectories for each agent. However, most of\nthem suffer from restricted representation ability or unstable training issues.\nTo overcome these limitations, we propose utilizing the diffusion model to\ngenerate the distribution of future trajectories. Two cruxes are to be settled\nto realize such an idea. First, the diversity of intention is intertwined with\nthe uncertain surroundings, making the true distribution hard to parameterize.\nSecond, the diffusion process is time-consuming during the inference phase,\nrendering it unrealistic to implement in a real-time driving system. We propose\nan Intention-aware denoising Diffusion Model (IDM), which tackles the above two\nproblems. We decouple the original uncertainty into intention uncertainty and\naction uncertainty and model them with two dependent diffusion processes. To\ndecrease the inference time, we reduce the variable dimensions in the\nintention-aware diffusion process and restrict the initial distribution of the\naction-aware diffusion process, which leads to fewer diffusion steps. To\nvalidate our approach, we conduct experiments on the Stanford Drone Dataset\n(SDD) and ETH/UCY dataset. Our methods achieve state-of-the-art results, with\nan FDE of 13.83 pixels on the SDD dataset and 0.36 meters on the ETH/UCY\ndataset. Compared with the original diffusion model, IDM reduces inference time\nby two-thirds. Interestingly, our experiments further reveal that introducing\nintention information is beneficial in modeling the diffusion process of fewer\nsteps.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "14 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.09190v1",
    "published_date": "2024-03-14 09:05:25 UTC",
    "updated_date": "2024-03-14 09:05:25 UTC"
  },
  {
    "arxiv_id": "2403.09184v4",
    "title": "Learning Algorithms for Verification of Markov Decision Processes",
    "authors": [
      "Tomáš Brázdil",
      "Krishnendu Chatterjee",
      "Martin Chmelik",
      "Vojtěch Forejt",
      "Jan Křetínský",
      "Marta Kwiatkowska",
      "Tobias Meggendorfer",
      "David Parker",
      "Mateusz Ujma"
    ],
    "abstract": "We present a general framework for applying learning algorithms and\nheuristical guidance to the verification of Markov decision processes (MDPs).\nThe primary goal of our techniques is to improve performance by avoiding an\nexhaustive exploration of the state space, instead focussing on particularly\nrelevant areas of the system, guided by heuristics. Our work builds on the\nprevious results of Br{\\'{a}}zdil et al., significantly extending it as well as\nrefining several details and fixing errors.\n  The presented framework focuses on probabilistic reachability, which is a\ncore problem in verification, and is instantiated in two distinct scenarios.\nThe first assumes that full knowledge of the MDP is available, in particular\nprecise transition probabilities. It performs a heuristic-driven partial\nexploration of the model, yielding precise lower and upper bounds on the\nrequired probability. The second tackles the case where we may only sample the\nMDP without knowing the exact transition dynamics. Here, we obtain\nprobabilistic guarantees, again in terms of both the lower and upper bounds,\nwhich provides efficient stopping criteria for the approximation. In\nparticular, the latter is an extension of statistical model-checking (SMC) for\nunbounded properties in MDPs. In contrast to other related approaches, we do\nnot restrict our attention to time-bounded (finite-horizon) or discounted\nproperties, nor assume any particular structural properties of the MDP.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.LO",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "82 pages. This is the TheoretiCS journal version",
    "pdf_url": "http://arxiv.org/pdf/2403.09184v4",
    "published_date": "2024-03-14 08:54:19 UTC",
    "updated_date": "2025-03-31 17:51:46 UTC"
  },
  {
    "arxiv_id": "2403.10561v1",
    "title": "A collection of the accepted papers for the Human-Centric Representation Learning workshop at AAAI 2024",
    "authors": [
      "Dimitris Spathis",
      "Aaqib Saeed",
      "Ali Etemad",
      "Sana Tonekaboni",
      "Stefanos Laskaridis",
      "Shohreh Deldari",
      "Chi Ian Tang",
      "Patrick Schwab",
      "Shyam Tailor"
    ],
    "abstract": "This non-archival index is not complete, as some accepted papers chose to\nopt-out of inclusion. The list of all accepted papers is available on the\nworkshop website.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.10561v1",
    "published_date": "2024-03-14 08:46:07 UTC",
    "updated_date": "2024-03-14 08:46:07 UTC"
  },
  {
    "arxiv_id": "2403.09171v2",
    "title": "ADEdgeDrop: Adversarial Edge Dropping for Robust Graph Neural Networks",
    "authors": [
      "Zhaoliang Chen",
      "Zhihao Wu",
      "Ylli Sadikaj",
      "Claudia Plant",
      "Hong-Ning Dai",
      "Shiping Wang",
      "Yiu-Ming Cheung",
      "Wenzhong Guo"
    ],
    "abstract": "Although Graph Neural Networks (GNNs) have exhibited the powerful ability to\ngather graph-structured information from neighborhood nodes via various\nmessage-passing mechanisms, the performance of GNNs is limited by poor\ngeneralization and fragile robustness caused by noisy and redundant graph data.\nAs a prominent solution, Graph Augmentation Learning (GAL) has recently\nreceived increasing attention. Among prior GAL approaches, edge-dropping\nmethods that randomly remove edges from a graph during training are effective\ntechniques to improve the robustness of GNNs. However, randomly dropping edges\noften results in bypassing critical edges, consequently weakening the\neffectiveness of message passing. In this paper, we propose a novel adversarial\nedge-dropping method (ADEdgeDrop) that leverages an adversarial edge predictor\nguiding the removal of edges, which can be flexibly incorporated into diverse\nGNN backbones. Employing an adversarial training framework, the edge predictor\nutilizes the line graph transformed from the original graph to estimate the\nedges to be dropped, which improves the interpretability of the edge-dropping\nmethod. The proposed ADEdgeDrop is optimized alternately by stochastic gradient\ndescent and projected gradient descent. Comprehensive experiments on six graph\nbenchmark datasets demonstrate that the proposed ADEdgeDrop outperforms\nstate-of-the-art baselines across various GNN backbones, demonstrating improved\ngeneralization and robustness.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.09171v2",
    "published_date": "2024-03-14 08:31:39 UTC",
    "updated_date": "2024-08-14 09:06:06 UTC"
  },
  {
    "arxiv_id": "2403.09142v2",
    "title": "USimAgent: Large Language Models for Simulating Search Users",
    "authors": [
      "Erhan Zhang",
      "Xingzhu Wang",
      "Peiyuan Gong",
      "Yankai Lin",
      "Jiaxin Mao"
    ],
    "abstract": "Due to the advantages in the cost-efficiency and reproducibility, user\nsimulation has become a promising solution to the user-centric evaluation of\ninformation retrieval systems. Nonetheless, accurately simulating user search\nbehaviors has long been a challenge, because users' actions in search are\nhighly complex and driven by intricate cognitive processes such as learning,\nreasoning, and planning. Recently, Large Language Models (LLMs) have\ndemonstrated remarked potential in simulating human-level intelligence and have\nbeen used in building autonomous agents for various tasks. However, the\npotential of using LLMs in simulating search behaviors has not yet been fully\nexplored. In this paper, we introduce a LLM-based user search behavior\nsimulator, USimAgent. The proposed simulator can simulate users' querying,\nclicking, and stopping behaviors during search, and thus, is capable of\ngenerating complete search sessions for specific search tasks. Empirical\ninvestigation on a real user behavior dataset shows that the proposed simulator\noutperforms existing methods in query generation and is comparable to\ntraditional methods in predicting user clicks and stopping behaviors. These\nresults not only validate the effectiveness of using LLMs for user simulation\nbut also shed light on the development of a more robust and generic user\nsimulators. The code and data are accessible at\nhttps://github.com/Meow-E/USimAgent.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.09142v2",
    "published_date": "2024-03-14 07:40:54 UTC",
    "updated_date": "2024-10-29 09:13:49 UTC"
  },
  {
    "arxiv_id": "2403.09141v1",
    "title": "Uncertainty Estimation in Multi-Agent Distributed Learning for AI-Enabled Edge Devices",
    "authors": [
      "Gleb Radchenko",
      "Victoria Andrea Fill"
    ],
    "abstract": "Initially considered as low-power units with limited autonomous processing,\nEdge IoT devices have seen a paradigm shift with the introduction of FPGAs and\nAI accelerators. This advancement has vastly amplified their computational\ncapabilities, emphasizing the practicality of edge AI. Such progress introduces\nnew challenges of optimizing AI tasks for the limitations of energy and network\nresources typical in Edge computing environments. Our study explores methods\nthat enable distributed data processing through AI-enabled edge devices,\nenhancing collaborative learning capabilities. A key focus of our research is\nthe challenge of determining confidence levels in learning outcomes,\nconsidering the spatial and temporal variability of data sets encountered by\nindependent agents. To address this issue, we investigate the application of\nBayesian neural networks, proposing a novel approach to manage uncertainty in\ndistributed learning environments.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG",
      "I.2.11"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.09141v1",
    "published_date": "2024-03-14 07:40:32 UTC",
    "updated_date": "2024-03-14 07:40:32 UTC"
  },
  {
    "arxiv_id": "2403.10559v1",
    "title": "Generative Models and Connected and Automated Vehicles: A Survey in Exploring the Intersection of Transportation and AI",
    "authors": [
      "Dong Shu",
      "Zhouyao Zhu"
    ],
    "abstract": "This report investigates the history and impact of Generative Models and\nConnected and Automated Vehicles (CAVs), two groundbreaking forces pushing\nprogress in technology and transportation. By focusing on the application of\ngenerative models within the context of CAVs, the study aims to unravel how\nthis integration could enhance predictive modeling, simulation accuracy, and\ndecision-making processes in autonomous vehicles. This thesis discusses the\nbenefits and challenges of integrating generative models and CAV technology in\ntransportation. It aims to highlight the progress made, the remaining\nobstacles, and the potential for advancements in safety and innovation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.10559v1",
    "published_date": "2024-03-14 06:51:26 UTC",
    "updated_date": "2024-03-14 06:51:26 UTC"
  },
  {
    "arxiv_id": "2403.09131v5",
    "title": "ProSwitch: Knowledge-Guided Instruction Tuning to Switch Between Professional and Non-Professional Responses",
    "authors": [
      "Chang Zong",
      "Yuyan Chen",
      "Weiming Lu",
      "Jian Shao",
      "Yongfeng Huang",
      "Heng Chang",
      "Yueting Zhuang"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated efficacy in various linguistic\napplications, including question answering and controlled text generation.\nHowever, studies into their ability to switch between opposite styles of\nresponses in professional domains remain underexplored. This study introduces a\nnovel approach, named ProSwitch, which enables a language model to switch\nbetween professional and non-professional answers, by tuning and evaluating\nthrough the guidance of domain and style knowledge. ProSwitch unfolds in three\nphases: LLM-augmented preparation to collect domain knowledge and QA pairs,\ninstruction tuning to optimize LLMs with multiple levels of knowledge, and\ncomprehensive evaluation to assess both style discrimination and\nreference-based quality of the generated text. Comparative analysis of\nProSwitch against general and specialized LLMs reveals that our approach\noutperforms baselines in switching between professional and non-professional\nresponses.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "68T50",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages main body, 16 pages total",
    "pdf_url": "http://arxiv.org/pdf/2403.09131v5",
    "published_date": "2024-03-14 06:49:16 UTC",
    "updated_date": "2024-12-12 09:22:14 UTC"
  },
  {
    "arxiv_id": "2403.09750v1",
    "title": "Meta-Cognitive Analysis: Evaluating Declarative and Procedural Knowledge in Datasets and Large Language Models",
    "authors": [
      "Zhuoqun Li",
      "Hongyu Lin",
      "Yaojie Lu",
      "Hao Xiang",
      "Xianpei Han",
      "Le Sun"
    ],
    "abstract": "Declarative knowledge and procedural knowledge are two key parts in\nmeta-cognitive theory, and these two hold significant importance in\npre-training and inference of LLMs. However, a comprehensive analysis comparing\nthese two types of knowledge is lacking, primarily due to challenges in\ndefinition, probing and quantitative assessment. In this paper, we explore from\na new perspective by providing ground-truth knowledge for LLMs and evaluating\nthe effective score. Through extensive experiments with widely-used datasets\nand models, we get conclusions: (1) In most tasks, benefits from declarative\nknowledge are greater than those from procedural knowledge. (2) Profits of\nprocedural knowledge are larger than declarative knowledge only in reasoning\ntasks with simple logic. (3) As pre-training progresses and size increases,\nmodel ability to utilize both kinds of knowledge significantly improves, but in\ndifferent speed. We do detailed analysis for the findings and this can provide\nprimary guidance for evaluation and enhancement of large language models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by LREC-COLING 2024 as a short paper",
    "pdf_url": "http://arxiv.org/pdf/2403.09750v1",
    "published_date": "2024-03-14 05:34:35 UTC",
    "updated_date": "2024-03-14 05:34:35 UTC"
  },
  {
    "arxiv_id": "2403.09113v2",
    "title": "AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based on Meta Learning",
    "authors": [
      "Ruiyi Zhang",
      "Rushi Qiang",
      "Sai Ashish Somayajula",
      "Pengtao Xie"
    ],
    "abstract": "Large-scale pretraining followed by task-specific finetuning has achieved\ngreat success in various NLP tasks. Since finetuning all parameters of large\npretrained models poses substantial computational and memory challenges,\nseveral efficient finetuning methods have been developed. Among them, low-rank\nadaptation (LoRA), which finetunes low-rank incremental update matrices on top\nof frozen pretrained weights, has proven particularly effective. Nonetheless,\nLoRA's uniform rank assignment across all layers, along with its reliance on an\nexhaustive search to find the best rank, leads to high computation costs and\nsuboptimal finetuning performance. To address these limitations, we introduce\nAutoLoRA, a meta learning based framework for automatically identifying the\noptimal rank of each LoRA layer. AutoLoRA associates each rank-1 matrix in a\nlow-rank update matrix with a selection variable, which determines whether the\nrank-1 matrix should be discarded. A meta learning based method is developed to\nlearn these selection variables. The optimal rank is determined by thresholding\nthe values of these variables. Our comprehensive experiments on natural\nlanguage understanding, generation, and sequence labeling demonstrate the\neffectiveness of AutoLoRA.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.09113v2",
    "published_date": "2024-03-14 05:29:35 UTC",
    "updated_date": "2024-03-17 17:55:47 UTC"
  },
  {
    "arxiv_id": "2403.09749v1",
    "title": "Towards Diverse Perspective Learning with Selection over Multiple Temporal Poolings",
    "authors": [
      "Jihyeon Seong",
      "Jungmin Kim",
      "Jaesik Choi"
    ],
    "abstract": "In Time Series Classification (TSC), temporal pooling methods that consider\nsequential information have been proposed. However, we found that each temporal\npooling has a distinct mechanism, and can perform better or worse depending on\ntime series data. We term this fixed pooling mechanism a single perspective of\ntemporal poolings. In this paper, we propose a novel temporal pooling method\nwith diverse perspective learning: Selection over Multiple Temporal Poolings\n(SoM-TP). SoM-TP dynamically selects the optimal temporal pooling among\nmultiple methods for each data by attention. The dynamic pooling selection is\nmotivated by the ensemble concept of Multiple Choice Learning (MCL), which\nselects the best among multiple outputs. The pooling selection by SoM-TP's\nattention enables a non-iterative pooling ensemble within a single classifier.\nAdditionally, we define a perspective loss and Diverse Perspective Learning\nNetwork (DPLN). The loss works as a regularizer to reflect all the pooling\nperspectives from DPLN. Our perspective analysis using Layer-wise Relevance\nPropagation (LRP) reveals the limitation of a single perspective and ultimately\ndemonstrates diverse perspective learning of SoM-TP. We also show that SoM-TP\noutperforms CNN models based on other temporal poolings and state-of-the-art\nmodels in TSC with extensive UCR/UEA repositories.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "17 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.09749v1",
    "published_date": "2024-03-14 05:02:00 UTC",
    "updated_date": "2024-03-14 05:02:00 UTC"
  },
  {
    "arxiv_id": "2403.09092v2",
    "title": "MCFEND: A Multi-source Benchmark Dataset for Chinese Fake News Detection",
    "authors": [
      "Yupeng Li",
      "Haorui He",
      "Jin Bai",
      "Dacheng Wen"
    ],
    "abstract": "The prevalence of fake news across various online sources has had a\nsignificant influence on the public. Existing Chinese fake news detection\ndatasets are limited to news sourced solely from Weibo. However, fake news\noriginating from multiple sources exhibits diversity in various aspects,\nincluding its content and social context. Methods trained on purely one single\nnews source can hardly be applicable to real-world scenarios. Our pilot\nexperiment demonstrates that the F1 score of the state-of-the-art method that\nlearns from a large Chinese fake news detection dataset, Weibo-21, drops\nsignificantly from 0.943 to 0.470 when the test data is changed to multi-source\nnews data, failing to identify more than one-third of the multi-source fake\nnews. To address this limitation, we constructed the first multi-source\nbenchmark dataset for Chinese fake news detection, termed MCFEND, which is\ncomposed of news we collected from diverse sources such as social platforms,\nmessaging apps, and traditional online news outlets. Notably, such news has\nbeen fact-checked by 14 authoritative fact-checking agencies worldwide. In\naddition, various existing Chinese fake news detection methods are thoroughly\nevaluated on our proposed dataset in cross-source, multi-source, and unseen\nsource ways. MCFEND, as a benchmark dataset, aims to advance Chinese fake news\ndetection approaches in real-world scenarios.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by the ACM Web Conference 2024 (WWW 2024) oral, dataset\n  available: https://github.com/TrustworthyComp",
    "pdf_url": "http://arxiv.org/pdf/2403.09092v2",
    "published_date": "2024-03-14 04:32:13 UTC",
    "updated_date": "2024-07-24 05:57:01 UTC"
  },
  {
    "arxiv_id": "2403.09085v2",
    "title": "Meaningful Learning: Enhancing Abstract Reasoning in Large Language Models via Generic Fact Guidance",
    "authors": [
      "Kai Xiong",
      "Xiao Ding",
      "Ting Liu",
      "Bing Qin",
      "Dongliang Xu",
      "Qing Yang",
      "Hongtao Liu",
      "Yixin Cao"
    ],
    "abstract": "Large language models (LLMs) have developed impressive performance and strong\nexplainability across various reasoning scenarios, marking a significant stride\ntowards mimicking human-like intelligence. Despite this, when tasked with\nseveral simple questions supported by a generic fact, LLMs often struggle to\nabstract and apply the generic fact to provide consistent and precise answers,\nrevealing a deficiency in abstract reasoning abilities. This has sparked a\nvigorous debate about whether LLMs are genuinely reasoning or merely\nmemorizing. In light of this, we design a preliminary study to quantify and\ndelve into the abstract reasoning abilities of existing LLMs. Our findings\nreveal a substantial discrepancy between their general reasoning and abstract\nreasoning performances. To relieve this problem, we tailor an abstract\nreasoning dataset (AbsR) together with a meaningful learning paradigm to teach\nLLMs how to leverage generic facts for reasoning purposes. The results show\nthat our approach not only boosts the general reasoning performance of LLMs but\nalso makes considerable strides towards their capacity for abstract reasoning,\nmoving beyond simple memorization or imitation to a more nuanced understanding\nand application of generic facts. The code is available at\nhttps://github.com/Waste-Wood/MeanLearn.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.09085v2",
    "published_date": "2024-03-14 04:06:13 UTC",
    "updated_date": "2024-11-11 11:35:28 UTC"
  },
  {
    "arxiv_id": "2403.09072v1",
    "title": "UniCode: Learning a Unified Codebook for Multimodal Large Language Models",
    "authors": [
      "Sipeng Zheng",
      "Bohan Zhou",
      "Yicheng Feng",
      "Ye Wang",
      "Zongqing Lu"
    ],
    "abstract": "In this paper, we propose \\textbf{UniCode}, a novel approach within the\ndomain of multimodal large language models (MLLMs) that learns a unified\ncodebook to efficiently tokenize visual, text, and potentially other types of\nsignals. This innovation addresses a critical limitation in existing MLLMs:\ntheir reliance on a text-only codebook, which restricts MLLM's ability to\ngenerate images and texts in a multimodal context. Towards this end, we propose\na language-driven iterative training paradigm, coupled with an in-context\npre-training task we term ``image decompression'', enabling our model to\ninterpret compressed visual data and generate high-quality images.The unified\ncodebook empowers our model to extend visual instruction tuning to\nnon-linguistic generation tasks. Moreover, UniCode is adaptable to diverse\nstacked quantization approaches in order to compress visual signals into a more\ncompact token representation. Despite using significantly fewer parameters and\nless data during training, Unicode demonstrates promising capabilities in\nvisual reconstruction and generation. It also achieves performances comparable\nto leading MLLMs across a spectrum of VQA benchmarks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "14 pages, 2 figures, 11 tables",
    "pdf_url": "http://arxiv.org/pdf/2403.09072v1",
    "published_date": "2024-03-14 03:29:58 UTC",
    "updated_date": "2024-03-14 03:29:58 UTC"
  },
  {
    "arxiv_id": "2403.09063v1",
    "title": "Distribution and Depth-Aware Transformers for 3D Human Mesh Recovery",
    "authors": [
      "Jerrin Bright",
      "Bavesh Balaji",
      "Harish Prakash",
      "Yuhao Chen",
      "David A Clausi",
      "John Zelek"
    ],
    "abstract": "Precise Human Mesh Recovery (HMR) with in-the-wild data is a formidable\nchallenge and is often hindered by depth ambiguities and reduced precision.\nExisting works resort to either pose priors or multi-modal data such as\nmulti-view or point cloud information, though their methods often overlook the\nvaluable scene-depth information inherently present in a single image.\nMoreover, achieving robust HMR for out-of-distribution (OOD) data is\nexceedingly challenging due to inherent variations in pose, shape and depth.\nConsequently, understanding the underlying distribution becomes a vital\nsubproblem in modeling human forms. Motivated by the need for unambiguous and\nrobust human modeling, we introduce Distribution and depth-aware human mesh\nrecovery (D2A-HMR), an end-to-end transformer architecture meticulously\ndesigned to minimize the disparity between distributions and incorporate\nscene-depth leveraging prior depth information. Our approach demonstrates\nsuperior performance in handling OOD data in certain scenarios while\nconsistently achieving competitive results against state-of-the-art HMR methods\non controlled datasets.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Submitted to 21st International Conference on Robots and Vision\n  (CRV'24), Guelph, Ontario, Canada",
    "pdf_url": "http://arxiv.org/pdf/2403.09063v1",
    "published_date": "2024-03-14 03:07:58 UTC",
    "updated_date": "2024-03-14 03:07:58 UTC"
  },
  {
    "arxiv_id": "2403.09057v3",
    "title": "A Continued Pretrained LLM Approach for Automatic Medical Note Generation",
    "authors": [
      "Dong Yuan",
      "Eti Rastogi",
      "Gautam Naik",
      "Sree Prasanna Rajagopal",
      "Sagar Goyal",
      "Fen Zhao",
      "Bharath Chintagunta",
      "Jeff Ward"
    ],
    "abstract": "LLMs are revolutionizing NLP tasks. However, the use of the most advanced\nLLMs, such as GPT-4, is often prohibitively expensive for most specialized\nfields. We introduce HEAL, the first continuously trained 13B LLaMA2-based LLM\nthat is purpose-built for medical conversations and measured on automated\nscribing. Our results demonstrate that HEAL outperforms GPT-4 and PMC-LLaMA in\nPubMedQA, with an accuracy of 78.4\\%. It also achieves parity with GPT-4 in\ngenerating medical notes. Remarkably, HEAL surpasses GPT-4 and Med-PaLM 2 in\nidentifying more correct medical concepts and exceeds the performance of human\nscribes and other comparable models in correctness and completeness.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to NAACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.09057v3",
    "published_date": "2024-03-14 02:55:37 UTC",
    "updated_date": "2024-04-03 18:08:30 UTC"
  },
  {
    "arxiv_id": "2403.09054v2",
    "title": "Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference",
    "authors": [
      "Muhammad Adnan",
      "Akhil Arunkumar",
      "Gaurav Jain",
      "Prashant J. Nair",
      "Ilya Soloveychik",
      "Purushotham Kamath"
    ],
    "abstract": "Transformers have emerged as the underpinning architecture for Large Language\nModels (LLMs). In generative language models, the inference process involves\ntwo primary phases: prompt processing and token generation. Token generation,\nwhich constitutes the majority of the computational workload, primarily entails\nvector-matrix multiplications and interactions with the Key-Value (KV) Cache.\nThis phase is constrained by memory bandwidth due to the overhead of\ntransferring weights and KV cache values from the memory system to the\ncomputing units. This memory bottleneck becomes particularly pronounced in\napplications that require long-context and extensive text generation, both of\nwhich are increasingly crucial for LLMs.\n  This paper introduces \"Keyformer\", an innovative inference-time approach, to\nmitigate the challenges associated with KV cache size and memory bandwidth\nutilization. Keyformer leverages the observation that approximately 90% of the\nattention weight in generative inference focuses on a specific subset of\ntokens, referred to as \"key\" tokens. Keyformer retains only the key tokens in\nthe KV cache by identifying these crucial tokens using a novel score function.\nThis approach effectively reduces both the KV cache size and memory bandwidth\nusage without compromising model accuracy. We evaluate Keyformer's performance\nacross three foundational models: GPT-J, Cerebras-GPT, and MPT, which employ\nvarious positional embedding algorithms. Our assessment encompasses a variety\nof tasks, with a particular emphasis on summarization and conversation tasks\ninvolving extended contexts. Keyformer's reduction of KV cache reduces\ninference latency by 2.1x and improves token generation throughput by 2.4x,\nwhile preserving the model's accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR",
      "cs.CL",
      "68U35",
      "I.2.7; C.0"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.09054v2",
    "published_date": "2024-03-14 02:42:42 UTC",
    "updated_date": "2024-04-06 00:22:37 UTC"
  },
  {
    "arxiv_id": "2403.09053v2",
    "title": "Towards a theory of model distillation",
    "authors": [
      "Enric Boix-Adsera"
    ],
    "abstract": "Distillation is the task of replacing a complicated machine learning model\nwith a simpler model that approximates the original [BCNM06,HVD15]. Despite\nmany practical applications, basic questions about the extent to which models\ncan be distilled, and the runtime and amount of data needed to distill, remain\nlargely open.\n  To study these questions, we initiate a general theory of distillation,\ndefining PAC-distillation in an analogous way to PAC-learning [Val84]. As\napplications of this theory: (1) we propose new algorithms to extract the\nknowledge stored in the trained weights of neural networks -- we show how to\nefficiently distill neural networks into succinct, explicit decision tree\nrepresentations when possible by using the ``linear representation\nhypothesis''; and (2) we prove that distillation can be much cheaper than\nlearning from scratch, and make progress on characterizing its complexity.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "46 pages, 5 figures. Please reach out with comments! Feedback is\n  welcome",
    "pdf_url": "http://arxiv.org/pdf/2403.09053v2",
    "published_date": "2024-03-14 02:42:19 UTC",
    "updated_date": "2024-05-04 19:52:03 UTC"
  },
  {
    "arxiv_id": "2403.09039v2",
    "title": "Detecting Anomalies in Dynamic Graphs via Memory enhanced Normality",
    "authors": [
      "Jie Liu",
      "Xuequn Shang",
      "Xiaolin Han",
      "Kai Zheng",
      "Hongzhi Yin"
    ],
    "abstract": "Anomaly detection in dynamic graphs presents a significant challenge due to\nthe temporal evolution of graph structures and attributes. The conventional\napproaches that tackle this problem typically employ an unsupervised learning\nframework, capturing normality patterns with exclusive normal data during\ntraining and identifying deviations as anomalies during testing. However, these\nmethods face critical drawbacks: they either only depend on proxy tasks for\nrepresentation without directly pinpointing normal patterns, or they neglect to\ndifferentiate between spatial and temporal normality patterns. More recent\nmethods that use contrastive learning with negative sampling also face high\ncomputational costs, limiting their scalability to large graphs. To address\nthese challenges, we introduce a novel Spatial-Temporal memories-enhanced graph\nautoencoder (STRIPE). Initially, STRIPE employs Graph Neural Networks (GNNs)\nand gated temporal convolution layers to extract spatial and temporal features.\nThen STRIPE incorporates separate spatial and temporal memory networks to\ncapture and store prototypes of normal patterns, respectively. These stored\npatterns are retrieved and integrated with encoded graph embeddings through a\nmutual attention mechanism. Finally, the integrated features are fed into the\ndecoder to reconstruct the graph streams which serve as the proxy task for\nanomaly detection. This comprehensive approach not only minimizes\nreconstruction errors but also emphasizes the compactness and distinctiveness\nof the embeddings w.r.t. the nearest memory prototypes. Extensive experiments\non six benchmark datasets demonstrate the effectiveness and efficiency of\nSTRIPE, where STRIPE significantly outperforms existing methods with 5.8%\nimprovement in AUC scores and 4.62X faster in training time.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.09039v2",
    "published_date": "2024-03-14 02:26:10 UTC",
    "updated_date": "2024-08-15 02:08:06 UTC"
  },
  {
    "arxiv_id": "2403.09029v1",
    "title": "Unlocking the conversion of Web Screenshots into HTML Code with the WebSight Dataset",
    "authors": [
      "Hugo Laurençon",
      "Léo Tronchon",
      "Victor Sanh"
    ],
    "abstract": "Using vision-language models (VLMs) in web development presents a promising\nstrategy to increase efficiency and unblock no-code solutions: by providing a\nscreenshot or a sketch of a UI, a VLM could generate the code to reproduce it,\nfor instance in a language like HTML. Despite the advancements in VLMs for\nvarious tasks, the specific challenge of converting a screenshot into a\ncorresponding HTML has been minimally explored. We posit that this is mainly\ndue to the absence of a suitable, high-quality dataset. This work introduces\nWebSight, a synthetic dataset consisting of 2 million pairs of HTML codes and\ntheir corresponding screenshots. We fine-tune a foundational VLM on our dataset\nand show proficiency in converting webpage screenshots to functional HTML code.\nTo accelerate the research in this area, we open-source WebSight.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.09029v1",
    "published_date": "2024-03-14 01:40:40 UTC",
    "updated_date": "2024-03-14 01:40:40 UTC"
  },
  {
    "arxiv_id": "2403.09024v1",
    "title": "Semiparametric Token-Sequence Co-Supervision",
    "authors": [
      "Hyunji Lee",
      "Doyoung Kim",
      "Jihoon Jun",
      "Sejune Joo",
      "Joel Jang",
      "Kyoung-Woon On",
      "Minjoon Seo"
    ],
    "abstract": "In this work, we introduce a semiparametric token-sequence co-supervision\ntraining method. It trains a language model by simultaneously leveraging\nsupervision from the traditional next token prediction loss which is calculated\nover the parametric token embedding space and the next sequence prediction loss\nwhich is calculated over the nonparametric sequence embedding space. The\nnonparametric sequence embedding space is constructed by a separate language\nmodel tasked to condense an input text into a single representative embedding.\nOur experiments demonstrate that a model trained via both supervisions\nconsistently surpasses models trained via each supervision independently.\nAnalysis suggests that this co-supervision encourages a broader generalization\ncapability across the model. Especially, the robustness of parametric token\nspace which is established during the pretraining step tends to effectively\nenhance the stability of nonparametric sequence embedding space, a new space\nestablished by another language model.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.09024v1",
    "published_date": "2024-03-14 01:28:13 UTC",
    "updated_date": "2024-03-14 01:28:13 UTC"
  },
  {
    "arxiv_id": "2405.12983v1",
    "title": "Multilingual Audio-Visual Speech Recognition with Hybrid CTC/RNN-T Fast Conformer",
    "authors": [
      "Maxime Burchi",
      "Krishna C. Puvvada",
      "Jagadeesh Balam",
      "Boris Ginsburg",
      "Radu Timofte"
    ],
    "abstract": "Humans are adept at leveraging visual cues from lip movements for recognizing\nspeech in adverse listening conditions. Audio-Visual Speech Recognition (AVSR)\nmodels follow similar approach to achieve robust speech recognition in noisy\nconditions. In this work, we present a multilingual AVSR model incorporating\nseveral enhancements to improve performance and audio noise robustness.\nNotably, we adapt the recently proposed Fast Conformer model to process both\naudio and visual modalities using a novel hybrid CTC/RNN-T architecture. We\nincrease the amount of audio-visual training data for six distinct languages,\ngenerating automatic transcriptions of unlabelled multilingual datasets\n(VoxCeleb2 and AVSpeech). Our proposed model achieves new state-of-the-art\nperformance on the LRS3 dataset, reaching WER of 0.8%. On the recently\nintroduced MuAViC benchmark, our model yields an absolute average-WER reduction\nof 11.9% in comparison to the original baseline. Finally, we demonstrate the\nability of the proposed model to perform audio-only, visual-only, and\naudio-visual speech recognition at test time.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CV",
      "cs.MM",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.12983v1",
    "published_date": "2024-03-14 01:16:32 UTC",
    "updated_date": "2024-03-14 01:16:32 UTC"
  },
  {
    "arxiv_id": "2403.09747v1",
    "title": "Re-Search for The Truth: Multi-round Retrieval-augmented Large Language Models are Strong Fake News Detectors",
    "authors": [
      "Guanghua Li",
      "Wensheng Lu",
      "Wei Zhang",
      "Defu Lian",
      "Kezhong Lu",
      "Rui Mao",
      "Kai Shu",
      "Hao Liao"
    ],
    "abstract": "The proliferation of fake news has had far-reaching implications on politics,\nthe economy, and society at large. While Fake news detection methods have been\nemployed to mitigate this issue, they primarily depend on two essential\nelements: the quality and relevance of the evidence, and the effectiveness of\nthe verdict prediction mechanism. Traditional methods, which often source\ninformation from static repositories like Wikipedia, are limited by outdated or\nincomplete data, particularly for emerging or rare claims. Large Language\nModels (LLMs), known for their remarkable reasoning and generative\ncapabilities, introduce a new frontier for fake news detection. However, like\ntraditional methods, LLM-based solutions also grapple with the limitations of\nstale and long-tail knowledge. Additionally, retrieval-enhanced LLMs frequently\nstruggle with issues such as low-quality evidence retrieval and context length\nconstraints. To address these challenges, we introduce a novel,\nretrieval-augmented LLMs framework--the first of its kind to automatically and\nstrategically extract key evidence from web sources for claim verification.\nEmploying a multi-round retrieval strategy, our framework ensures the\nacquisition of sufficient, relevant evidence, thereby enhancing performance.\nComprehensive experiments across three real-world datasets validate the\nframework's superiority over existing methods. Importantly, our model not only\ndelivers accurate verdicts but also offers human-readable explanations to\nimprove result interpretability.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.09747v1",
    "published_date": "2024-03-14 00:35:39 UTC",
    "updated_date": "2024-03-14 00:35:39 UTC"
  }
]
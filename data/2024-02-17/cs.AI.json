{
  "date": "2024-02-17",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-02-17 的 arXiv 中文 TLDR 快报！今天 arXiv 更新了 40 篇论文，主要聚焦于大型语言模型（LLM）的优化、微调、安全挑战，以及 AI 在医疗、机器人和图计算领域的应用，其中 Yann LeCun 的论文（第 10 篇）令人印象深刻，强调 LLM 在感知任务中的局限性；其他热门话题包括 LLM 代理的安全性和高效推理。\n\n### LLM 优化与应用（重点主题）\n今天的大部分论文围绕 LLM 的改进和应用展开，以其高话题度和实际影响为先。下面挑选几篇关键论文进行讨论：\n\n- **标题**：Reasoning before Comparison: LLM-Enhanced Semantic Similarity Metrics for Domain Specialized Text Analysis（推理先行：LLM 增强的语义相似度指标用于领域专用文本分析）  \n  这篇论文由 Shaochen Xu 等作者提出，利用 GPT-4 生成标签来提升文本相似度评估，显著改善了医学报告分析的准确性，展示了 LLM 在半监督语义分析中的潜力。\n\n- **标题**：CliqueParcel: An Approach For Batching LLM Prompts That Jointly Optimizes Efficiency And Faithfulness（CliqueParcel：一种同时优化 LLM 提示批量处理效率和忠实度的方法）  \n  Jiayi Liu 等作者的这项工作引入 CliqueParcel 框架，通过子方法优化提示批量，显著提高了 LLM 的推理效率，同时保持输出忠实度，在阅读理解和问答任务上表现出色。\n\n- **标题**：Language Models Don't Learn the Physical Manifestation of Language（语言模型未学习语言的物理表现形式）  \n  Yann LeCun 和 JaeHyuk Lim 的论文通过 H-Test 任务证明，LLM 在视觉-听觉属性上表现欠佳，即使增强模型或使用提示也难改善，突显了 LLM 缺乏感官经验的局限性。\n\n- **标题**：Boosting of Thoughts: Trial-and-Error Problem Solving with Large Language Models（思想提升：LLM 的试错问题解决方法）  \n  这篇论文提出 BoT 框架，通过迭代探索和自评估构建提示集，提升 LLM 在复杂数学问题上的推理性能，实验显示其优于传统链式思考方法。\n\n- **标题**：Aligning Large Language Models by On-Policy Self-Judgment（通过在线策略自判断对齐大型语言模型）  \n  Sangkyu Lee 等作者的创新方法无需额外奖励模型，直接用 LLM 评估响应，实现高效微调，实验证明其在偏好基准上提升了鲁棒性。\n\n其他 LLM 相关论文（如第 21、22、29、33、34 篇）探讨微调和攻击，但核心贡献类似：提升效率和鲁棒性，我这里简要掠过，以节省篇幅。\n\n### 医疗与生物应用\n这部分论文应用 AI 于实际场景，以创新性和潜在影响为优先：\n\n- **标题**：Transformer-based de novo peptide sequencing for data-independent acquisition mass spectrometry（基于 Transformer 的从头肽序列测定用于数据独立采集质谱）  \n  Shiva Ebrahimi 和 Xuan Guo 的工作引入 DiaTrans 模型，提高了肽序列的精确度和召回率（精确度提升 15-35%），为蛋白质组学提供更全面的生物样本分析工具。\n\n- **标题**：Understanding the Impact of Long-Term Memory on Self-Disclosure with Large Language Model-Driven Chatbots for Public Health Intervention（理解长期记忆对 LLM 驱动聊天机器人自我披露的影响，用于公共卫生干预）  \n  这篇论文分析了 LLM 聊天机器人在健康监测中的长期记忆效果，发现它增强了用户披露，但也带来隐私挑战，为公共卫生应用提供了宝贵见解。\n\n### 机器人与图计算\n这些论文虽重要，但相对次要，我快速概述：\n\n- **标题**：SpikeNAS: A Fast Memory-Aware Neural Architecture Search Framework for Spiking Neural Network-based Autonomous Agents（SpikeNAS：一种快速内存感知神经架构搜索框架，用于基于脉冲神经网络的自治代理）  \n  主要贡献是优化 SNN 架构搜索，提升了内存约束下的准确性（搜索速度快 4.4 倍），适用于移动机器人。\n\n- **标题**：Probabilistic Routing for Graph-Based Approximate Nearest Neighbor Search（基于图的近似最近邻搜索的概率路由）  \n  这篇论文提出 PEOs 方法，提高了图索引效率（吞吐量提升 1.6-2.5 倍），为高维空间搜索提供理论支持。\n\n剩余论文（如犯罪预测、公平分类等）虽涉及多样领域，但影响力较小，我仅提要：它们探讨了 AI 的公平性和应用扩展，例如第 9 篇的探索性数据收集方法提升了分类公平性。\n\n总之，今天的 arXiv 强调了 LLM 的创新潜力，同时提醒其安全和泛化挑战。读者可关注 LLM 相关论文，探索实际应用！",
  "papers": [
    {
      "arxiv_id": "2402.11403v2",
      "title": "An Empirical Evaluation of Neural and Neuro-symbolic Approaches to Real-time Multimodal Complex Event Detection",
      "title_zh": "神经和神经符号方法在实时多模态复杂事件检测中的实证评估",
      "authors": [
        "Liying Han",
        "Mani B. Srivastava"
      ],
      "abstract": "Robots and autonomous systems require an understanding of complex events\n(CEs) from sensor data to interact with their environments and humans\neffectively. Traditional end-to-end neural architectures, despite processing\nsensor data efficiently, struggle with long-duration events due to limited\ncontext sizes and reasoning capabilities. Recent advances in neuro-symbolic\nmethods, which integrate neural and symbolic models leveraging human knowledge,\npromise improved performance with less data. This study addresses the gap in\nunderstanding these approaches' effectiveness in complex event detection (CED),\nespecially in temporal reasoning. We investigate neural and neuro-symbolic\narchitectures' performance in a multimodal CED task, analyzing IMU and acoustic\ndata streams to recognize CE patterns. Our methodology includes (i) end-to-end\nneural architectures for direct CE detection from sensor embeddings, (ii)\ntwo-stage concept-based neural models mapping sensor embeddings to atomic\nevents (AEs) before CE detection, and (iii) a neuro-symbolic approach using a\nsymbolic finite-state machine for CE detection from AEs. Empirically, the\nneuro-symbolic architecture significantly surpasses purely neural models,\ndemonstrating superior performance in CE recognition, even with extensive\ntraining data and ample temporal context for neural approaches.",
      "tldr_zh": "本研究实证评估了神经(neural)和神经符号(neuro-symbolic)方法在实时多模态复杂事件检测(Complex Event Detection, CED)中的性能，旨在帮助机器人和自主系统从传感器数据（如IMU和声学流）中理解复杂事件(CEs)，并解决传统端到端神经架构在处理长时事件时的上下文和推理限制。研究比较了三种方法：(i) 端到端神经架构直接从传感器嵌入检测CE，(ii) 两阶段概念-based神经模型先映射传感器嵌入到原子事件(Atomic Events, AEs)再检测CE，以及(iii) 使用符号有限状态机(finite-state machine)的神经符号方法。实验结果显示，神经符号方法显著优于纯神经模型，即使在大量训练数据和充足时间上下文条件下，其在CE识别方面的表现更出色。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.11403v2",
      "published_date": "2024-02-17 23:34:50 UTC",
      "updated_date": "2024-03-03 22:07:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:16:31.155681"
    },
    {
      "arxiv_id": "2402.11398v2",
      "title": "Reasoning before Comparison: LLM-Enhanced Semantic Similarity Metrics for Domain Specialized Text Analysis",
      "title_zh": "先推理再比较：LLM 增强的语义相似度指标用于领域专业文本分析",
      "authors": [
        "Shaochen Xu",
        "Zihao Wu",
        "Huaqin Zhao",
        "Peng Shu",
        "Zhengliang Liu",
        "Wenxiong Liao",
        "Sheng Li",
        "Andrea Sikora",
        "Tianming Liu",
        "Xiang Li"
      ],
      "abstract": "In this study, we leverage LLM to enhance the semantic analysis and develop\nsimilarity metrics for texts, addressing the limitations of traditional\nunsupervised NLP metrics like ROUGE and BLEU. We develop a framework where LLMs\nsuch as GPT-4 are employed for zero-shot text identification and label\ngeneration for radiology reports, where the labels are then used as\nmeasurements for text similarity. By testing the proposed framework on the\nMIMIC data, we find that GPT-4 generated labels can significantly improve the\nsemantic similarity assessment, with scores more closely aligned with clinical\nground truth than traditional NLP metrics. Our work demonstrates the\npossibility of conducting semantic analysis of the text data using\nsemi-quantitative reasoning results by the LLMs for highly specialized domains.\nWhile the framework is implemented for radiology report similarity analysis,\nits concept can be extended to other specialized domains as well.",
      "tldr_zh": "本研究利用 LLM（如 GPT-4）增强语义相似度指标，针对专业领域文本分析（如放射学报告），以解决传统 NLP 指标如 ROUGE 和 BLEU 的局限性。框架通过 LLM 进行零样本文本识别和标签生成，将这些标签作为测量文本相似度的依据。在 MIMIC 数据上的实验显示，GPT-4 生成的标签显著提高了语义相似度评估，与临床真实数据更一致。该方法展示了在专业领域进行半定量语义分析的可能性，并可扩展至其他领域。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.11398v2",
      "published_date": "2024-02-17 22:46:44 UTC",
      "updated_date": "2024-02-20 22:23:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:16:43.323910"
    },
    {
      "arxiv_id": "2402.14833v1",
      "title": "CliqueParcel: An Approach For Batching LLM Prompts That Jointly Optimizes Efficiency And Faithfulness",
      "title_zh": "翻译失败",
      "authors": [
        "Jiayi Liu",
        "Tinghan Yang",
        "Jennifer Neville"
      ],
      "abstract": "Large language models (LLMs) have become pivotal in recent research. However,\nduring the inference process, LLMs still require substantial resources. In this\npaper, we propose CliqueParcel, a method designed to improve the efficiency of\nLLMs via prompt batching. Existing strategies to optimize inference efficiency\noften compromise on output quality, leading to a discounted output problem.\nThis issue might result in reduced accuracy or outputs that are less detailed.\nCliqueParcel is our answer to this challenge. While ensuring accuracy and\nminimizing deviations from the original outputs (i.e., faithfulness), our\nmethod significantly improves efficiency during inference.\n  To lay the groundwork, we first redefine efficiency measurements by excluding\nthe reduction in running time due to shorter lengths. Then, we provide a\ncomprehensive trade-off between efficiency and faithfulness to clarify the\nnature of the 'discounted output' problem. Within the CliqueParcel framework,\nwe suggest multiple batching sub-methods and discuss the specific scenarios in\nwhich they can be applied. During evaluation, CliqueParcel is tested on eight\nwidely recognized datasets, which can be classified into three types: reading\ncomprehension, open-source question-answering, and reasoning. Our experiments\nexplore the performance of CliqueParcel, including efficiency, faithfulness,\nand the trade-off between them. This work provides novel insights into\ninference efficiency and demonstrates promising performance.",
      "tldr_zh": "本论文提出 CliqueParcel 方法，通过优化 LLM 提示批量处理，同时提升推理效率和输出忠实性 (faithfulness)，以解决现有策略在追求效率时导致输出质量下降的问题。该方法重新定义效率测量标准，排除输出长度缩短的影响，并提供多种批量子方法，适用于不同场景，如阅读理解、开源问答和推理任务。在八个数据集上的实验表明，CliqueParcel 显著提高了效率，同时保持了输出准确性和详细性，展示了效率与忠实性之间的有效权衡。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.14833v1",
      "published_date": "2024-02-17 22:37:17 UTC",
      "updated_date": "2024-02-17 22:37:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:16:54.738006"
    },
    {
      "arxiv_id": "2402.11363v3",
      "title": "Transformer-based de novo peptide sequencing for data-independent acquisition mass spectrometry",
      "title_zh": "翻译失败",
      "authors": [
        "Shiva Ebrahimi",
        "Xuan Guo"
      ],
      "abstract": "Tandem mass spectrometry (MS/MS) stands as the predominant high-throughput\ntechnique for comprehensively analyzing protein content within biological\nsamples. This methodology is a cornerstone driving the advancement of\nproteomics. In recent years, substantial strides have been made in\nData-Independent Acquisition (DIA) strategies, facilitating impartial and\nnon-targeted fragmentation of precursor ions. The DIA-generated MS/MS spectra\npresent a formidable obstacle due to their inherent high multiplexing nature.\nEach spectrum encapsulates fragmented product ions originating from multiple\nprecursor peptides. This intricacy poses a particularly acute challenge in de\nnovo peptide/protein sequencing, where current methods are ill-equipped to\naddress the multiplexing conundrum. In this paper, we introduce DiaTrans, a\ndeep-learning model based on transformer architecture. It deciphers peptide\nsequences from DIA mass spectrometry data. Our results show significant\nimprovements over existing STOA methods, including DeepNovo-DIA and PepNet.\nCasanovo-DIA enhances precision by 15.14% to 34.8%, recall by 11.62% to 31.94%\nat the amino acid level, and boosts precision by 59% to 81.36% at the peptide\nlevel. Integrating DIA data and our DiaTrans model holds considerable promise\nto uncover novel peptides and more comprehensive profiling of biological\nsamples. Casanovo-DIA is freely available under the GNU GPL license at\nhttps://github.com/Biocomputing-Research-Group/DiaTrans.",
      "tldr_zh": "本文提出 DiaTrans，一种基于 Transformer 架构的深度学习模型，用于从 Data-Independent Acquisition (DIA) 质谱数据中进行 de novo peptide sequencing，以解决 DIA 数据的高多路复用性带来的挑战。相比现有方法如 DeepNovo-DIA 和 PepNet，DiaTrans 在氨基酸水平上将精确度提高了 15.14% 到 34.8%、召回率提高了 11.62% 到 31.94%，并在肽水平上将精确度提升至 59% 到 81.36%。该模型的整合有望发现新型肽并实现更全面的生物样本分析，且已开源在 GitHub 上。",
      "categories": [
        "q-bio.QM",
        "cs.AI"
      ],
      "primary_category": "q-bio.QM",
      "comment": "Ebrahimi S., Guo X. Transformer-based de novo peptide sequencing for\n  data-independent acquisition mass spectrometry. In 2023 IEEE 23rd\n  International Conference on Bioinformatics and Bioengineering (BIBE) 2022 Dec\n  6 (pp. 17-22). IEEE",
      "pdf_url": "http://arxiv.org/pdf/2402.11363v3",
      "published_date": "2024-02-17 19:04:23 UTC",
      "updated_date": "2024-06-26 07:45:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:17:09.158630"
    },
    {
      "arxiv_id": "2402.11359v4",
      "title": "Offline Training of Language Model Agents with Functions as Learnable Weights",
      "title_zh": "翻译失败",
      "authors": [
        "Shaokun Zhang",
        "Jieyu Zhang",
        "Jiale Liu",
        "Linxin Song",
        "Chi Wang",
        "Ranjay Krishna",
        "Qingyun Wu"
      ],
      "abstract": "Researchers and practitioners have recently reframed powerful Large Language\nModels (LLMs) as agents, enabling them to automate complex tasks largely via\nthe use of specialized functions. To facilitate the development of LLM agents,\nwe present a novel paradigm of training LLM agents without modifying the LLM\nweights, which is particularly useful when the LLMs are difficult or\ninaccessible for modifications. Inspired by how humans continuously forge tools\nto adapt to real-world tasks, rather than change our biological structure to\nfit a static set of tools, we propose to progressively forge agent's functions\nto better solve the downstream tasks instead of modifying the LLM weights. By\ntreating the functions as learnable `agent parameters' and leveraging the\nfundamental idea of model training in artificial intelligence, we develop\nAgentOptimizer that employs the LLM to update agents' functions and devise an\nagent training algorithm with two strategies, roll-back, and early-stop, to\nstreamline the training process. With extensive experiments, we showcase that\nthe agent training paradigm could significantly improve the performance of\nrepresentative LLM agents in various downstream tasks. We also study the\nbehavior of the agent training regarding aspects like the learning curve and\ndomain transferability.",
      "tldr_zh": "本文提出了一种不修改 Large Language Models (LLMs) 权重的创新训练范式，用于开发 LLM 代理，通过将函数视为可学习的“代理参数”来适应下游任务。研究引入 AgentOptimizer，利用 LLMs 更新函数，并设计了 roll-back 和 early-stop 策略来优化训练过程。实验结果显示，这种方法显著提升了 LLM 代理在各种任务中的性能，并探讨了其学习曲线和领域转移能力。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "22 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.11359v4",
      "published_date": "2024-02-17 18:31:21 UTC",
      "updated_date": "2024-07-30 18:22:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:17:18.230768"
    },
    {
      "arxiv_id": "2402.11354v2",
      "title": "Probabilistic Routing for Graph-Based Approximate Nearest Neighbor Search",
      "title_zh": "基于图的近似最近邻搜索的概率路由",
      "authors": [
        "Kejing Lu",
        "Chuan Xiao",
        "Yoshiharu Ishikawa"
      ],
      "abstract": "Approximate nearest neighbor search (ANNS) in high-dimensional spaces is a\npivotal challenge in the field of machine learning. In recent years,\ngraph-based methods have emerged as the superior approach to ANNS, establishing\na new state of the art. Although various optimizations for graph-based ANNS\nhave been introduced, they predominantly rely on heuristic methods that lack\nformal theoretical backing. This paper aims to enhance routing within\ngraph-based ANNS by introducing a method that offers a probabilistic guarantee\nwhen exploring a node's neighbors in the graph. We formulate the problem as\nprobabilistic routing and develop two baseline strategies by incorporating\nlocality-sensitive techniques. Subsequently, we introduce PEOs, a novel\napproach that efficiently identifies which neighbors in the graph should be\nconsidered for exact distance calculation, thus significantly improving\nefficiency in practice. Our experiments demonstrate that equipping PEOs can\nincrease throughput on commonly utilized graph indexes (HNSW and NSSG) by a\nfactor of 1.6 to 2.5, and its efficiency consistently outperforms the\nleading-edge routing technique by 1.1 to 1.4 times.",
      "tldr_zh": "该论文针对高维空间的 Approximate Nearest Neighbor Search (ANNS) 问题，提出了一种基于图的概率路由方法，以提供理论支持并优化路由过程。方法包括制定概率路由问题、开发两个基于 locality-sensitive 技术的基线策略，以及引入 PEOs 算法来高效识别图中需要精确距离计算的邻居，从而显著提升搜索效率。实验结果显示，应用 PEOs 后，HNSW 和 NSSG 等常用图索引的吞吐量可提高 1.6 到 2.5 倍，并比领先路由技术高效 1.1 到 1.4 倍。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.DB",
        "cs.DS"
      ],
      "primary_category": "cs.LG",
      "comment": "Source code is available at https://github.com/ICML2024-code/PEOs",
      "pdf_url": "http://arxiv.org/pdf/2402.11354v2",
      "published_date": "2024-02-17 18:08:37 UTC",
      "updated_date": "2024-07-10 17:05:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:17:31.053778"
    },
    {
      "arxiv_id": "2402.11353v1",
      "title": "Understanding the Impact of Long-Term Memory on Self-Disclosure with Large Language Model-Driven Chatbots for Public Health Intervention",
      "title_zh": "翻译失败",
      "authors": [
        "Eunkyung Jo",
        "Yuin Jeong",
        "SoHyun Park",
        "Daniel A. Epstein",
        "Young-Ho Kim"
      ],
      "abstract": "Recent large language models (LLMs) offer the potential to support public\nhealth monitoring by facilitating health disclosure through open-ended\nconversations but rarely preserve the knowledge gained about individuals across\nrepeated interactions. Augmenting LLMs with long-term memory (LTM) presents an\nopportunity to improve engagement and self-disclosure, but we lack an\nunderstanding of how LTM impacts people's interaction with LLM-driven chatbots\nin public health interventions. We examine the case of CareCall -- an\nLLM-driven voice chatbot with LTM -- through the analysis of 1,252 call logs\nand interviews with nine users. We found that LTM enhanced health disclosure\nand fostered positive perceptions of the chatbot by offering familiarity.\nHowever, we also observed challenges in promoting self-disclosure through LTM,\nparticularly around addressing chronic health conditions and privacy concerns.\nWe discuss considerations for LTM integration in LLM-driven chatbots for public\nhealth monitoring, including carefully deciding what topics need to be\nremembered in light of public health goals.",
      "tldr_zh": "本研究探讨了在大型语言模型(LLMs)驱动的聊天机器人中添加长时记忆(LTM)，如何影响用户自我披露的行为，特别是针对公共健康干预。研究通过分析CareCall聊天机器人的1252个通话日志和9个用户访谈，发现LTM能增强健康披露、提升用户对机器人的熟悉感和积极认知。另一方面，LTM在处理慢性健康问题和隐私担忧时存在挑战，因此论文建议在公共健康目标下，需谨慎决定哪些主题需要被记忆。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "H.5.2; I.2.7"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted to ACM CHI 2024 as a full paper",
      "pdf_url": "http://arxiv.org/pdf/2402.11353v1",
      "published_date": "2024-02-17 18:05:53 UTC",
      "updated_date": "2024-02-17 18:05:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:17:43.349925"
    },
    {
      "arxiv_id": "2402.11349v2",
      "title": "Language Models Don't Learn the Physical Manifestation of Language",
      "title_zh": "翻译失败",
      "authors": [
        "Bruce W. Lee",
        "JaeHyuk Lim"
      ],
      "abstract": "We argue that language-only models don't learn the physical manifestation of\nlanguage. We present an empirical investigation of visual-auditory properties\nof language through a series of tasks, termed H-Test. These tasks highlight a\nfundamental gap between human linguistic understanding and the sensory-deprived\nlinguistic understanding of LLMs. In support of our hypothesis, 1. deliberate\nreasoning (Chain-of-Thought), 2. few-shot examples, or 3. stronger LLM from the\nsame model family (LLaMA 2 13B -> LLaMA 2 70B) has no significant effect on\nH-Test performance.\n  We bring in the philosophical case of Mary, who learns about the world in a\nsensory-deprived environment as a useful conceptual framework to understand how\nlanguage-only models learn about the world (Jackson, 1986). Our experiments\nshow that some of the strongest proprietary LLMs stay near random chance\nbaseline accuracy of 50%, highlighting the limitations of linguistic knowledge\nacquired in the absence of sensory experience. Our code and data are available\nat <github.com/brucewlee/h-test>.",
      "tldr_zh": "本论文认为Language Models无法学习语言的物理表现形式，通过一系列H-Test任务调查了语言的视觉-听觉属性，突显了人类语言理解与LLMs的感官缺失理解之间的差距。研究者测试了Chain-of-Thought推理、few-shot examples以及更大模型（如LLaMA 2 13B到70B）的效果，但这些方法对H-Test性能无显著影响。实验结果显示，最强的专有LLMs准确率接近随机基线的50%，并以Mary的哲学案例（Jackson, 1986）作为框架，强调了缺少感官经验的语言知识局限性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL 2024 Main",
      "pdf_url": "http://arxiv.org/pdf/2402.11349v2",
      "published_date": "2024-02-17 17:52:24 UTC",
      "updated_date": "2024-06-06 17:20:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:17:55.402743"
    },
    {
      "arxiv_id": "2402.11338v2",
      "title": "Fair Classification with Partial Feedback: An Exploration-Based Data Collection Approach",
      "title_zh": "基于部分反馈的公平分类：一种基于探索的数据收集方法",
      "authors": [
        "Vijay Keswani",
        "Anay Mehrotra",
        "L. Elisa Celis"
      ],
      "abstract": "In many predictive contexts (e.g., credit lending), true outcomes are only\nobserved for samples that were positively classified in the past. These past\nobservations, in turn, form training datasets for classifiers that make future\npredictions. However, such training datasets lack information about the\noutcomes of samples that were (incorrectly) negatively classified in the past\nand can lead to erroneous classifiers. We present an approach that trains a\nclassifier using available data and comes with a family of exploration\nstrategies to collect outcome data about subpopulations that otherwise would\nhave been ignored. For any exploration strategy, the approach comes with\nguarantees that (1) all sub-populations are explored, (2) the fraction of false\npositives is bounded, and (3) the trained classifier converges to a ``desired''\nclassifier. The right exploration strategy is context-dependent; it can be\nchosen to improve learning guarantees and encode context-specific group\nfairness properties. Evaluation on real-world datasets shows that this approach\nconsistently boosts the quality of collected outcome data and improves the\nfraction of true positives for all groups, with only a small reduction in\npredictive utility.",
      "tldr_zh": "这篇论文针对部分反馈(Partial Feedback)问题在公平分类(Fair Classification)中的挑战，提出了一种基于探索策略(Exploration-Based)的數據收集方法。该方法利用现有数据训练分类器，同时采用一系列探索策略来收集被忽略子群体的结果数据，确保所有子群体被探索、假阳性比例受控，并使分类器收敛到理想分类器。实验在真实数据集上验证，该方法显著提高了结果数据的质量和各群组的真阳性比例，同时仅以微小预测效用损失为代价。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted for presentation at ICML 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.11338v2",
      "published_date": "2024-02-17 17:09:19 UTC",
      "updated_date": "2024-06-01 12:48:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:18:06.811941"
    },
    {
      "arxiv_id": "2402.11337v1",
      "title": "Learning by Reconstruction Produces Uninformative Features For Perception",
      "title_zh": "翻译失败",
      "authors": [
        "Randall Balestriero",
        "Yann LeCun"
      ],
      "abstract": "Input space reconstruction is an attractive representation learning paradigm.\nDespite interpretability of the reconstruction and generation, we identify a\nmisalignment between learning by reconstruction, and learning for perception.\nWe show that the former allocates a model's capacity towards a subspace of the\ndata explaining the observed variance--a subspace with uninformative features\nfor the latter. For example, the supervised TinyImagenet task with images\nprojected onto the top subspace explaining 90\\% of the pixel variance can be\nsolved with 45\\% test accuracy. Using the bottom subspace instead, accounting\nfor only 20\\% of the pixel variance, reaches 55\\% test accuracy. The features\nfor perception being learned last explains the need for long training time,\ne.g., with Masked Autoencoders. Learning by denoising is a popular strategy to\nalleviate that misalignment. We prove that while some noise strategies such as\nmasking are indeed beneficial, others such as additive Gaussian noise are not.\nYet, even in the case of masking, we find that the benefits vary as a function\nof the mask's shape, ratio, and the considered dataset. While tuning the noise\nstrategy without knowledge of the perception task seems challenging, we provide\nfirst clues on how to detect if a noise strategy is never beneficial regardless\nof the perception task.",
      "tldr_zh": "本文研究发现，通过重建学习（reconstruction learning）获得的特征对于感知任务（perception）缺乏信息性，因为它将模型容量分配到解释数据方差的子空间，而这些特征对感知无益。例如，在 TinyImagenet 任务中，使用解释90%像素方差的顶层子空间，测试准确率仅为45%；反之，使用解释仅20%方差的底层子空间，可达55%。论文进一步分析了去噪策略（denoising），证明masking等方法能部分缓解不匹配问题，但additive Gaussian noise无效，且masking的效果取决于掩码形状、比例和数据集。最终，作者提供了检测永远无效噪声策略的线索，以优化表示学习。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.11337v1",
      "published_date": "2024-02-17 17:08:16 UTC",
      "updated_date": "2024-02-17 17:08:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:18:19.862033"
    },
    {
      "arxiv_id": "2402.11322v3",
      "title": "SpikeNAS: A Fast Memory-Aware Neural Architecture Search Framework for Spiking Neural Network-based Autonomous Agents",
      "title_zh": "SpikeNAS：一种快速内存感知神经架构搜索框架，用于基于脉冲神经网络的自治代理",
      "authors": [
        "Rachmad Vidya Wicaksana Putra",
        "Muhammad Shafique"
      ],
      "abstract": "Autonomous mobile agents (e.g., UAVs and UGVs) are typically expected to\nincur low power/energy consumption for solving machine learning tasks (such as\nobject recognition), as these mobile agents are usually powered by portable\nbatteries. These requirements can be fulfilled by Spiking Neural Networks\n(SNNs), since their bio-inspired spike-based operations offer high accuracy and\nultra low-power/energy computation. Currently, most of the SNN architectures\nare derived from Artificial Neural Networks whose neurons' architectures and\noperations are different from SNNs, or developed without considering memory\nbudgets from the underlying processing hardware of autonomous mobile agents.\nThese limitations hinder SNNs from reaching their full potential in accuracy\nand efficiency. Toward this, we propose SpikeNAS, a novel fast memory-aware\nneural architecture search (NAS) framework for SNNs that quickly finds an\nappropriate SNN architecture with high accuracy under the given memory budgets\nfrom autonomous mobile agents. To do this, our SpikeNAS employs several key\nsteps: analyzing the impacts of network operations on the accuracy, enhancing\nthe network architecture to improve the learning quality, and developing a fast\nmemory-aware search algorithm. The experimental results show that our SpikeNAS\nimproves the searching time and maintains high accuracy as compared to\nstate-of-the-art while meeting the given memory budgets (e.g., 4.4x faster\nsearch with 1.3% accuracy improvement for CIFAR100, using an Nvidia RTX 6000\nAda GPU machine), thereby quickly providing the appropriate SNN architecture\nfor the memory-constrained autonomous mobile agents.",
      "tldr_zh": "该论文提出 SpikeNAS，一种快速内存感知神经架构搜索（NAS）框架，针对基于 Spiking Neural Networks (SNNs) 的自主移动代理（如无人机和无人车），以解决现有 SNNs 在准确性和硬件内存预算方面的局限性。SpikeNAS 通过分析网络操作对准确性的影响、增强网络架构以提升学习质量，以及开发快速内存感知搜索算法，快速找到在给定内存预算下的高准确性 SNN 架构。实验结果显示，与最先进方法相比，SpikeNAS 在 CIFAR100 数据集上搜索速度提升 4.4 倍，同时准确性提高 1.3%，为内存受限的自主代理提供高效解决方案。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "8 pages, 13 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2402.11322v3",
      "published_date": "2024-02-17 16:33:54 UTC",
      "updated_date": "2024-04-05 11:51:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:18:33.793808"
    },
    {
      "arxiv_id": "2402.11319v3",
      "title": "Hysteresis Compensation of Flexible Continuum Manipulator using RGBD Sensing and Temporal Convolutional Network",
      "title_zh": "柔性连续体机械手的滞回补偿：利用 RGBD 感知和时间卷积网络",
      "authors": [
        "Junhyun Park",
        "Seonghyeok Jang",
        "Hyojae Park",
        "Seongjun Bae",
        "Minho Hwang"
      ],
      "abstract": "Flexible continuum manipulators are valued for minimally invasive surgery,\noffering access to confined spaces through nonlinear paths. However,\ncable-driven manipulators face control difficulties due to hysteresis from\ncabling effects such as friction, elongation, and coupling. These effects are\ndifficult to model due to nonlinearity and the difficulties become even more\nevident when dealing with long and coupled, multi-segmented manipulator. This\npaper proposes a data-driven approach based on Deep Neural Networks (DNN) to\ncapture these nonlinear and previous states-dependent characteristics of cable\nactuation. We collect physical joint configurations according to command joint\nconfigurations using RGBD sensing and 7 fiducial markers to model the\nhysteresis of the proposed manipulator. Result on a study comparing the\nestimation performance of four DNN models show that the Temporal Convolution\nNetwork (TCN) demonstrates the highest predictive capability. Leveraging\ntrained TCNs, we build a control algorithm to compensate for hysteresis.\nTracking tests in task space using unseen trajectories show that the proposed\ncontrol algorithm reduces the average position and orientation error by 61.39%\n(from 13.7mm to 5.29 mm) and 64.04% (from 31.17{\\deg} to 11.21{\\deg}),\nrespectively. This result implies that the proposed calibrated controller\neffectively reaches the desired configurations by estimating the hysteresis of\nthe manipulator. Applying this method in real surgical scenarios has the\npotential to enhance control precision and improve surgical performance.",
      "tldr_zh": "这篇论文针对柔性连续机械臂在微创手术中的滞后问题（如摩擦和耦合），提出了一种基于 Deep Neural Networks (DNN) 的数据驱动方法，使用 RGBD Sensing 和 7 个基准标记来收集数据并建模非线性特性。结果显示，Temporal Convolutional Network (TCN) 在四种 DNN 模型中表现出最高的预测能力，因此被用于构建滞后补偿控制算法。在任务空间的跟踪测试中，该算法将平均位置误差从 13.7mm 减少至 5.29mm（降低 61.39%），并将方向误差从 31.17° 减少至 11.21°（降低 64.04%），从而显著提升了手术控制精度和性能。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "8 pages, 11 figures, 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2402.11319v3",
      "published_date": "2024-02-17 16:20:59 UTC",
      "updated_date": "2024-05-03 17:19:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:18:45.799691"
    },
    {
      "arxiv_id": "2402.11317v2",
      "title": "Debiased Offline Representation Learning for Fast Online Adaptation in Non-stationary Dynamics",
      "title_zh": "翻译失败",
      "authors": [
        "Xinyu Zhang",
        "Wenjie Qiu",
        "Yi-Chen Li",
        "Lei Yuan",
        "Chengxing Jia",
        "Zongzhang Zhang",
        "Yang Yu"
      ],
      "abstract": "Developing policies that can adjust to non-stationary environments is\nessential for real-world reinforcement learning applications. However, learning\nsuch adaptable policies in offline settings, with only a limited set of\npre-collected trajectories, presents significant challenges. A key difficulty\narises because the limited offline data makes it hard for the context encoder\nto differentiate between changes in the environment dynamics and shifts in the\nbehavior policy, often leading to context misassociations. To address this\nissue, we introduce a novel approach called Debiased Offline Representation for\nfast online Adaptation (DORA). DORA incorporates an information bottleneck\nprinciple that maximizes mutual information between the dynamics encoding and\nthe environmental data, while minimizing mutual information between the\ndynamics encoding and the actions of the behavior policy. We present a\npractical implementation of DORA, leveraging tractable bounds of the\ninformation bottleneck principle. Our experimental evaluation across six\nbenchmark MuJoCo tasks with variable parameters demonstrates that DORA not only\nachieves a more precise dynamics encoding but also significantly outperforms\nexisting baselines in terms of performance.",
      "tldr_zh": "该论文针对非平稳动态环境中的强化学习，提出了一种名为 DORA 的去偏离线表示学习方法，以实现快速在线适应。DORA 通过信息瓶颈原则最大化动态编码与环境数据的互信息，同时最小化动态编码与行为策略动作的互信息，从而解决离线数据有限导致的上下文错误关联问题。实验结果显示，在六个 MuJoCo 基准任务上，DORA 实现了更精确的动态编码，并显著优于现有基线模型的性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.11317v2",
      "published_date": "2024-02-17 16:03:35 UTC",
      "updated_date": "2025-03-27 13:46:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:18:56.031918"
    },
    {
      "arxiv_id": "2402.11314v1",
      "title": "Multi-Generative Agent Collective Decision-Making in Urban Planning: A Case Study for Kendall Square Renovation",
      "title_zh": "翻译失败",
      "authors": [
        "Jin Gao",
        "Hanyong Xu",
        "Luc Dao"
      ],
      "abstract": "In this study, we develop a multiple-generative agent system to simulate\ncommunity decision-making for the redevelopment of Kendall Square's Volpe\nbuilding. Drawing on interviews with local stakeholders, our simulations\nincorporated varying degrees of communication, demographic data, and life\nvalues in the agent prompts. The results revealed that communication among\nagents improved collective reasoning, while the inclusion of demographic and\nlife values led to more distinct opinions. These findings highlight the\npotential application of AI in understanding complex social interactions and\ndecision-making processes, offering valuable insights for urban planning and\ncommunity engagement in diverse settings like Kendall Square.",
      "tldr_zh": "这篇论文开发了一个multi-generative agent system，用于模拟Kendall Square Volpe建筑再开发的社区决策过程。系统基于对当地利益相关者的访谈，纳入了不同程度的通信、人口统计数据和生活价值观到代理提示中。结果表明，代理间的通信提高了集体推理，而加入人口统计和生活价值观导致了意见分歧的增加。这些发现展示了AI在理解复杂社会互动和决策方面的潜力，为城市规划和社区参与提供了重要见解。",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.11314v1",
      "published_date": "2024-02-17 15:52:16 UTC",
      "updated_date": "2024-02-17 15:52:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:19:09.468756"
    },
    {
      "arxiv_id": "2403.00780v1",
      "title": "Empirical and Experimental Insights into Data Mining Techniques for Crime Prediction: A Comprehensive Survey",
      "title_zh": "犯罪预测数据挖掘技术的实证与实验洞见：一项全面调查",
      "authors": [
        "Kamal Taha"
      ],
      "abstract": "This survey paper presents a comprehensive analysis of crime prediction\nmethodologies, exploring the various techniques and technologies utilized in\nthis area. The paper covers the statistical methods, machine learning\nalgorithms, and deep learning techniques employed to analyze crime data, while\nalso examining their effectiveness and limitations. We propose a methodological\ntaxonomy that classifies crime prediction algorithms into specific techniques.\nThis taxonomy is structured into four tiers, including methodology category,\nmethodology sub-category, methodology techniques, and methodology\nsub-techniques. Empirical and experimental evaluations are provided to rank the\ndifferent techniques. The empirical evaluation assesses the crime prediction\ntechniques based on four criteria, while the experimental evaluation ranks the\nalgorithms that employ the same sub-technique, the different sub-techniques\nthat employ the same technique, the different techniques that employ the same\nmethodology sub-category, the different methodology sub-categories within the\nsame category, and the different methodology categories. The combination of\nmethodological taxonomy, empirical evaluations, and experimental comparisons\nallows for a nuanced and comprehensive understanding of crime prediction\nalgorithms, aiding researchers in making informed decisions. Finally, the paper\nprovides a glimpse into the future of crime prediction techniques, highlighting\npotential advancements and opportunities for further research in this field",
      "tldr_zh": "这篇调查论文对犯罪预测中的数据挖掘技术进行了全面分析，涵盖统计方法、机器学习和深度学习算法，并探讨了它们的有效性和局限性。论文提出一个四层的方法学分类法（methodological taxonomy），包括方法类别、子类别、技术和子技术，并通过实证评估（empirical evaluation）和实验评估（experimental evaluation）对这些技术进行排名和比较，以帮助研究者做出 informed decisions。最后，该研究展望了犯罪预测领域的潜在进展和未来研究机会。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00780v1",
      "published_date": "2024-02-17 15:00:45 UTC",
      "updated_date": "2024-02-17 15:00:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:19:19.996672"
    },
    {
      "arxiv_id": "2402.11296v1",
      "title": "Dissecting Human and LLM Preferences",
      "title_zh": "剖析人类和大型语言模型的偏好",
      "authors": [
        "Junlong Li",
        "Fan Zhou",
        "Shichao Sun",
        "Yikai Zhang",
        "Hai Zhao",
        "Pengfei Liu"
      ],
      "abstract": "As a relative quality comparison of model responses, human and Large Language\nModel (LLM) preferences serve as common alignment goals in model fine-tuning\nand criteria in evaluation. Yet, these preferences merely reflect broad\ntendencies, resulting in less explainable and controllable models with\npotential safety risks. In this work, we dissect the preferences of human and\n32 different LLMs to understand their quantitative composition, using\nannotations from real-world user-model conversations for a fine-grained,\nscenario-wise analysis. We find that humans are less sensitive to errors, favor\nresponses that support their stances, and show clear dislike when models admit\ntheir limits. On the contrary, advanced LLMs like GPT-4-Turbo emphasize\ncorrectness, clarity, and harmlessness more. Additionally, LLMs of similar\nsizes tend to exhibit similar preferences, regardless of their training\nmethods, and fine-tuning for alignment does not significantly alter the\npreferences of pretrained-only LLMs. Finally, we show that preference-based\nevaluation can be intentionally manipulated. In both training-free and\ntraining-based settings, aligning a model with the preferences of judges boosts\nscores, while injecting the least preferred properties lowers them. This\nresults in notable score shifts: up to 0.59 on MT-Bench (1-10 scale) and 31.94\non AlpacaEval 2.0 (0-100 scale), highlighting the significant impact of this\nstrategic adaptation. Interactive Demo:\nhttps://huggingface.co/spaces/GAIR/Preference-Dissection-Visualization Dataset:\nhttps://huggingface.co/datasets/GAIR/preference-dissection Code:\nhttps://github.com/GAIR-NLP/Preference-Dissection",
      "tldr_zh": "这篇论文剖析了人类和大型语言模型 (LLM) 偏好的组成和影响，通过对真实用户-模型对话的细粒度标注进行场景化分析。研究发现，人类偏好较不敏感于错误、更倾向于支持自身观点的回应，并不喜欢模型承认局限性，而高级 LLM 如 GPT-4-Turbo 更强调正确性、清晰度和无害性；此外，类似规模的 LLM 偏好相似，且微调对预训练模型的偏好影响有限。最终，论文证明偏好评估可被操纵：在训练前后，通过与评判者偏好对齐可提升分数（如 MT-Bench 上提高 0.59 分，AlpacaEval 2.0 上提高 31.94 分），揭示了潜在的安全风险和模型优化策略。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.11296v1",
      "published_date": "2024-02-17 14:34:31 UTC",
      "updated_date": "2024-02-17 14:34:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:19:35.023289"
    },
    {
      "arxiv_id": "2402.11291v3",
      "title": "Puzzle Solving using Reasoning of Large Language Models: A Survey",
      "title_zh": "翻译失败",
      "authors": [
        "Panagiotis Giadikiaroglou",
        "Maria Lymperaiou",
        "Giorgos Filandrianos",
        "Giorgos Stamou"
      ],
      "abstract": "Exploring the capabilities of Large Language Models (LLMs) in puzzle solving\nunveils critical insights into their potential and challenges in AI, marking a\nsignificant step towards understanding their applicability in complex reasoning\ntasks. This survey leverages a unique taxonomy -- dividing puzzles into\nrule-based and rule-less categories -- to critically assess LLMs through\nvarious methodologies, including prompting techniques, neuro-symbolic\napproaches, and fine-tuning. Through a critical review of relevant datasets and\nbenchmarks, we assess LLMs' performance, identifying significant challenges in\ncomplex puzzle scenarios. Our findings highlight the disparity between LLM\ncapabilities and human-like reasoning, particularly in those requiring advanced\nlogical inference. The survey underscores the necessity for novel strategies\nand richer datasets to advance LLMs' puzzle-solving proficiency and contribute\nto AI's logical reasoning and creative problem-solving advancements.",
      "tldr_zh": "这篇调查探讨了大型语言模型（LLMs）在谜题解决中的能力，使用一个独特的分类法将谜题分为基于规则的和无规则的类别，以评估其在复杂推理任务中的潜力与挑战。研究通过提示技术（prompting techniques）、神经符号方法（neuro-symbolic approaches）和微调等方法，对相关数据集和基准进行批判性审查，揭示了 LLMs 在高级逻辑推理场景中的表现不足。结果显示，LLMs 与人类推理存在显著差距，特别是处理复杂谜题时；该调查强调需要开发新策略和更丰富的数据集，以提升 LLMs 在逻辑推理和创意问题解决方面的能力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.11291v3",
      "published_date": "2024-02-17 14:19:38 UTC",
      "updated_date": "2024-09-14 06:12:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:19:45.190816"
    },
    {
      "arxiv_id": "2402.11285v1",
      "title": "Fair Resource Allocation in Virtualized O-RAN Platforms",
      "title_zh": "虚拟化 O-RAN 平台中的公平资源分配",
      "authors": [
        "Fatih Aslan",
        "George Iosifidis",
        "Jose A. Ayala-Romero",
        "Andres Garcia-Saavedra",
        "Xavier Costa-Perez"
      ],
      "abstract": "O-RAN systems and their deployment in virtualized general-purpose computing\nplatforms (O-Cloud) constitute a paradigm shift expected to bring unprecedented\nperformance gains. However, these architectures raise new implementation\nchallenges and threaten to worsen the already-high energy consumption of mobile\nnetworks. This paper presents first a series of experiments which assess the\nO-Cloud's energy costs and their dependency on the servers' hardware, capacity\nand data traffic properties which, typically, change over time. Next, it\nproposes a compute policy for assigning the base station data loads to O-Cloud\nservers in an energy-efficient fashion; and a radio policy that determines at\nnear-real-time the minimum transmission block size for each user so as to avoid\nunnecessary energy costs. The policies balance energy savings with performance,\nand ensure that both of them are dispersed fairly across the servers and users,\nrespectively. To cater for the unknown and time-varying parameters affecting\nthe policies, we develop a novel online learning framework with fairness\nguarantees that apply to the entire operation horizon of the system (long-term\nfairness). The policies are evaluated using trace-driven simulations and are\nfully implemented in an O-RAN compatible system where we measure the energy\ncosts and throughput in realistic scenarios.",
      "tldr_zh": "本文研究了 O-RAN 系统在虚拟化 O-Cloud 平台上的资源分配问题，强调了其性能提升与能源消耗增加的挑战。通过实验评估，分析了 O-Cloud 的能源成本及其与服务器硬件、容量和数据流量变化的依赖关系。论文提出 compute policy 用于能源高效地分配基站数据负载到服务器，以及 radio policy 用于实时确定用户的最小传输块大小，以平衡能源节约和性能，同时确保服务器间和用户间的公平性。为应对未知动态参数，开发了一种新型 online learning framework，提供长期公平性保证。最后，通过基于轨迹的模拟和 O-RAN 兼容系统的实际测试，验证了这些策略在降低能源成本和提高吞吐量方面的有效性。",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NI",
      "comment": "to appear in ACM Sigmetrics 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.11285v1",
      "published_date": "2024-02-17 13:57:20 UTC",
      "updated_date": "2024-02-17 13:57:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:19:57.611903"
    },
    {
      "arxiv_id": "2402.11279v1",
      "title": "Multi-Perspective Consistency Enhances Confidence Estimation in Large Language Models",
      "title_zh": "多视角一致性提升大型语言模型中的置信度估计",
      "authors": [
        "Pei Wang",
        "Yejie Wang",
        "Muxi Diao",
        "Keqing He",
        "Guanting Dong",
        "Weiran Xu"
      ],
      "abstract": "In the deployment of large language models (LLMs), accurate confidence\nestimation is critical for assessing the credibility of model predictions.\nHowever, existing methods often fail to overcome the issue of overconfidence on\nincorrect answers. In this work, we focus on improving the confidence\nestimation of large language models. Considering the fragility of\nself-awareness in language models, we introduce a Multi-Perspective Consistency\n(MPC) method. We leverage complementary insights from different perspectives\nwithin models (MPC-Internal) and across different models (MPC-Across) to\nmitigate the issue of overconfidence arising from a singular viewpoint. The\nexperimental results on eight publicly available datasets show that our MPC\nachieves state-of-the-art performance. Further analyses indicate that MPC can\nmitigate the problem of overconfidence and is effectively scalable to other\nmodels.",
      "tldr_zh": "该研究针对大型语言模型（Large Language Models, LLMs）在部署中的过度自信问题，提出Multi-Perspective Consistency (MPC)方法，以提升信心估计的准确性。MPC通过模型内部不同视角（MPC-Internal）和跨模型视角（MPC-Across）整合互补洞见，缓解单一观点导致的过自信偏差。在八个公开数据集上的实验表明，MPC实现了最先进性能，并证明其可有效扩展到其他模型，从而增强LLMs的可信度。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.11279v1",
      "published_date": "2024-02-17 13:37:39 UTC",
      "updated_date": "2024-02-17 13:37:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:20:08.775693"
    },
    {
      "arxiv_id": "2402.11273v1",
      "title": "Semi-supervised Medical Image Segmentation Method Based on Cross-pseudo Labeling Leveraging Strong and Weak Data Augmentation Strategies",
      "title_zh": "翻译失败",
      "authors": [
        "Yifei Chen",
        "Chenyan Zhang",
        "Yifan Ke",
        "Yiyu Huang",
        "Xuezhou Dai",
        "Feiwei Qin",
        "Yongquan Zhang",
        "Xiaodong Zhang",
        "Changmiao Wang"
      ],
      "abstract": "Traditional supervised learning methods have historically encountered certain\nconstraints in medical image segmentation due to the challenging collection\nprocess, high labeling cost, low signal-to-noise ratio, and complex features\ncharacterizing biomedical images. This paper proposes a semi-supervised model,\nDFCPS, which innovatively incorporates the Fixmatch concept. This significantly\nenhances the model's performance and generalizability through data augmentation\nprocessing, employing varied strategies for unlabeled data. Concurrently, the\nmodel design gives appropriate emphasis to the generation, filtration, and\nrefinement processes of pseudo-labels. The novel concept of\ncross-pseudo-supervision is introduced, integrating consistency learning with\nself-training. This enables the model to fully leverage pseudo-labels from\nmultiple perspectives, thereby enhancing training diversity. The DFCPS model is\ncompared with both baseline and advanced models using the publicly accessible\nKvasir-SEG dataset. Across all four subdivisions containing different\nproportions of unlabeled data, our model consistently exhibits superior\nperformance. Our source code is available at\nhttps://github.com/JustlfC03/DFCPS.",
      "tldr_zh": "本文提出了一种半监督医疗图像分割方法DFCPS，以解决传统监督学习在数据收集、标注成本高以及信噪比低等方面的挑战。该模型基于Fixmatch概念，通过强弱数据增强策略处理未标注数据，并引入cross-pseudo-supervision机制，将一致性学习与自训练相结合，提升伪标签的生成、过滤和多样性利用。实验结果显示，在Kvasir-SEG数据集的多个子集上，DFCPS比基线和高级模型表现出色，证明了其有效性和泛化能力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "5 pages, 2 figures, accept ISBI2024",
      "pdf_url": "http://arxiv.org/pdf/2402.11273v1",
      "published_date": "2024-02-17 13:07:44 UTC",
      "updated_date": "2024-02-17 13:07:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:20:20.731469"
    },
    {
      "arxiv_id": "2402.11260v1",
      "title": "MoRAL: MoE Augmented LoRA for LLMs' Lifelong Learning",
      "title_zh": "MoRAL：MoE 增强的 LoRA 用于大语言模型的终身学习",
      "authors": [
        "Shu Yang",
        "Muhammad Asif Ali",
        "Cheng-Long Wang",
        "Lijie Hu",
        "Di Wang"
      ],
      "abstract": "Adapting large language models (LLMs) to new domains/tasks and enabling them\nto be efficient lifelong learners is a pivotal challenge. In this paper, we\npropose MoRAL, i.e., Mixture-of-Experts augmented Low-Rank Adaptation for\nLifelong Learning. MoRAL combines the multi-tasking abilities of MoE with the\nfine-tuning abilities of LoRA for effective life-long learning of LLMs. In\ncontrast to the conventional approaches that use factual triplets as inputs\nMoRAL relies on simple question-answer pairs, which is a more practical and\neffective strategy for robust and efficient learning. Owing to new data\nsettings, we introduce a new evaluation benchmark namely: Life Long Learning of\nLLM (5L-bench) encompassing a newly curated dataset of question-answer pairs,\nand a set of evaluation metrics for rigorous evaluation of MoRAL in open-book\nand closed-book settings. Experimental evaluation shows (i) LLMs learn fast in\nopen-book settings with up to 30.15% improvement in \"RA\" for Phi-2-2.7B\ncompared to closed-book (for models fine-tuned with MoRAL); (ii) MoRAL shows\nhigher performance improvement for models with a greater number of parameters;\n(iii) MoRAL is robust to catastrophic forgetting offering better knowledge\nretention compared to baselines.",
      "tldr_zh": "本文提出 MoRAL，一种结合 Mixture-of-Experts (MoE) 和 Low-Rank Adaptation (LoRA) 的方法，用于大型语言模型 (LLMs) 的终身学习，通过使用简单的问题-答案对作为输入，提高了学习效率和实用性。MoRAL 引入了新基准 5L-bench，包括一个问题-答案数据集和评估指标，用于测试模型在开放书和闭合书设置下的性能。实验结果显示，MoRAL 使模型学习更快，例如 Phi-2-2.7B 在开放书设置中 \"RA\" 指标改善高达 30.15%，并在参数更多的模型上表现出色，同时减少了灾难性遗忘并提升了知识保留。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.11260v1",
      "published_date": "2024-02-17 12:25:31 UTC",
      "updated_date": "2024-02-17 12:25:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:20:33.948743"
    },
    {
      "arxiv_id": "2402.11253v3",
      "title": "Aligning Large Language Models by On-Policy Self-Judgment",
      "title_zh": "通过在线策略自判断对齐大语言模型",
      "authors": [
        "Sangkyu Lee",
        "Sungdong Kim",
        "Ashkan Yousefpour",
        "Minjoon Seo",
        "Kang Min Yoo",
        "Youngjae Yu"
      ],
      "abstract": "Existing approaches for aligning large language models with human preferences\nface a trade-off that requires a separate reward model (RM) for on-policy\nlearning. In this paper, we present a novel alignment framework, SELF-JUDGE\nthat (1) does on-policy learning and 2) is parameter efficient, as it does not\nrequire an additional RM for evaluating the samples for on-policy learning. To\nthis end, we propose Judge-augmented Supervised Fine-Tuning (JSFT) to train a\nsingle model to act as both a policy and a judge. Specifically, we view the\npairwise judgment task, choosing the better response from a response pair, as a\nspecial case of the instruction-following task. The resulting model can judge\npreferences of on-the-fly responses from current policy initialized from\nitself. Experimental results show the efficacy of SELF-JUDGE, outperforming\nbaselines in preference benchmarks. We also show that the rejecting sampling by\nitself can improve performance further without an additional evaluator.",
      "tldr_zh": "本研究提出了一种新型框架 SELF-JUDGE，用于对齐 Large Language Models 与人类偏好，该框架通过 on-policy 学习实现参数高效，避免了传统方法依赖单独的 reward model (RM)。具体方法是采用 Judge-augmented Supervised Fine-Tuning (JSFT)，训练一个模型同时作为政策和判断者，将判断任务（如从响应对中选择更好响应）视为指令遵循任务的特殊情况，从而能评估自身生成的实时响应。实验结果表明，SELF-JUDGE 在偏好基准上优于基线模型，且通过拒绝采样进一步提升性能，而无需额外评估器。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Published as a main conference paper at ACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.11253v3",
      "published_date": "2024-02-17 11:25:26 UTC",
      "updated_date": "2024-06-25 13:39:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:20:45.406618"
    },
    {
      "arxiv_id": "2402.11243v1",
      "title": "Can Large Language Models perform Relation-based Argument Mining?",
      "title_zh": "翻译失败",
      "authors": [
        "Deniz Gorur",
        "Antonio Rago",
        "Francesca Toni"
      ],
      "abstract": "Argument mining (AM) is the process of automatically extracting arguments,\ntheir components and/or relations amongst arguments and components from text.\nAs the number of platforms supporting online debate increases, the need for AM\nbecomes ever more urgent, especially in support of downstream tasks.\nRelation-based AM (RbAM) is a form of AM focusing on identifying agreement\n(support) and disagreement (attack) relations amongst arguments. RbAM is a\nchallenging classification task, with existing methods failing to perform\nsatisfactorily. In this paper, we show that general-purpose Large Language\nModels (LLMs), appropriately primed and prompted, can significantly outperform\nthe best performing (RoBERTa-based) baseline. Specifically, we experiment with\ntwo open-source LLMs (Llama-2 and Mistral) with ten datasets.",
      "tldr_zh": "本论文探讨了大型语言模型 (LLMs) 是否能有效执行基于关系的论证挖掘 (RbAM)，即识别文本中论证间的同意 (support) 和不同意 (attack) 关系。研究者通过对开源 LLMs（如 Llama-2 和 Mistral）进行适当的引导和提示，在十个数据集上进行实验。结果表明，这些 LLMs 显著超过了基于 RoBERTa 的最佳基线方法，证明了 LLMs 在这一挑战性任务中的潜力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, 9 figures, submitted to ACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.11243v1",
      "published_date": "2024-02-17 10:37:51 UTC",
      "updated_date": "2024-02-17 10:37:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:20:57.577219"
    },
    {
      "arxiv_id": "2402.11242v1",
      "title": "Learning with Imbalanced Noisy Data by Preventing Bias in Sample Selection",
      "title_zh": "翻译失败",
      "authors": [
        "Huafeng Liu",
        "Mengmeng Sheng",
        "Zeren Sun",
        "Yazhou Yao",
        "Xian-Sheng Hua",
        "Heng-Tao Shen"
      ],
      "abstract": "Learning with noisy labels has gained increasing attention because the\ninevitable imperfect labels in real-world scenarios can substantially hurt the\ndeep model performance. Recent studies tend to regard low-loss samples as clean\nones and discard high-loss ones to alleviate the negative impact of noisy\nlabels. However, real-world datasets contain not only noisy labels but also\nclass imbalance. The imbalance issue is prone to causing failure in the\nloss-based sample selection since the under-learning of tail classes also leans\nto produce high losses. To this end, we propose a simple yet effective method\nto address noisy labels in imbalanced datasets. Specifically, we propose\nClass-Balance-based sample Selection (CBS) to prevent the tail class samples\nfrom being neglected during training. We propose Confidence-based Sample\nAugmentation (CSA) for the chosen clean samples to enhance their reliability in\nthe training process. To exploit selected noisy samples, we resort to\nprediction history to rectify labels of noisy samples. Moreover, we introduce\nthe Average Confidence Margin (ACM) metric to measure the quality of corrected\nlabels by leveraging the model's evolving training dynamics, thereby ensuring\nthat low-quality corrected noisy samples are appropriately masked out. Lastly,\nconsistency regularization is imposed on filtered label-corrected noisy samples\nto boost model performance. Comprehensive experimental results on synthetic and\nreal-world datasets demonstrate the effectiveness and superiority of our\nproposed method, especially in imbalanced scenarios. Comprehensive experimental\nresults on synthetic and real-world datasets demonstrate the effectiveness and\nsuperiority of our proposed method, especially in imbalanced scenarios.",
      "tldr_zh": "这篇论文针对带有噪声标签的不平衡数据集，提出了一种防止样本选择偏差的方法，以提升深度模型的性能。具体方法包括Class-Balance-based sample Selection (CBS)来平衡类别样本选择、Confidence-based Sample Augmentation (CSA)来增强干净样本的可靠性，以及利用预测历史修正噪声标签并引入Average Confidence Margin (ACM)指标来评估和过滤低质量修正样本。论文还施加一致性正则化以进一步提升模型效果。实验结果显示，该方法在合成和真实数据集上特别适用于不平衡场景，表现出显著的优越性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "accepted by IEEE Transactions on Multimedia",
      "pdf_url": "http://arxiv.org/pdf/2402.11242v1",
      "published_date": "2024-02-17 10:34:53 UTC",
      "updated_date": "2024-02-17 10:34:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:21:10.332288"
    },
    {
      "arxiv_id": "2402.11241v1",
      "title": "DiffPoint: Single and Multi-view Point Cloud Reconstruction with ViT Based Diffusion Model",
      "title_zh": "翻译失败",
      "authors": [
        "Yu Feng",
        "Xing Shi",
        "Mengli Cheng",
        "Yun Xiong"
      ],
      "abstract": "As the task of 2D-to-3D reconstruction has gained significant attention in\nvarious real-world scenarios, it becomes crucial to be able to generate\nhigh-quality point clouds. Despite the recent success of deep learning models\nin generating point clouds, there are still challenges in producing\nhigh-fidelity results due to the disparities between images and point clouds.\nWhile vision transformers (ViT) and diffusion models have shown promise in\nvarious vision tasks, their benefits for reconstructing point clouds from\nimages have not been demonstrated yet. In this paper, we first propose a neat\nand powerful architecture called DiffPoint that combines ViT and diffusion\nmodels for the task of point cloud reconstruction. At each diffusion step, we\ndivide the noisy point clouds into irregular patches. Then, using a standard\nViT backbone that treats all inputs as tokens (including time information,\nimage embeddings, and noisy patches), we train our model to predict target\npoints based on input images. We evaluate DiffPoint on both single-view and\nmulti-view reconstruction tasks and achieve state-of-the-art results.\nAdditionally, we introduce a unified and flexible feature fusion module for\naggregating image features from single or multiple input images. Furthermore,\nour work demonstrates the feasibility of applying unified architectures across\nlanguages and images to improve 3D reconstruction tasks.",
      "tldr_zh": "这篇论文提出了DiffPoint，一种基于ViT（Vision Transformers）和扩散模型的架构，用于从单视图或多视图图像重建高质量点云，以解决图像与点云差异带来的挑战。方法的核心是将噪声点云分成不规则的patches，并使用ViT骨干网络处理输入（如时间信息、图像嵌入和噪声patches），从而预测目标点云；同时引入了一个统一的特征融合模块来聚合单或多视图图像特征。实验结果显示，DiffPoint在单视图和多视图重建任务上达到了State-of-the-Art性能，并证明了将统一架构应用于3D重建任务的可行性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.11241v1",
      "published_date": "2024-02-17 10:18:40 UTC",
      "updated_date": "2024-02-17 10:18:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:21:22.192009"
    },
    {
      "arxiv_id": "2402.13276v2",
      "title": "When LLMs Meets Acoustic Landmarks: An Efficient Approach to Integrate Speech into Large Language Models for Depression Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Xiangyu Zhang",
        "Hexin Liu",
        "Kaishuai Xu",
        "Qiquan Zhang",
        "Daijiao Liu",
        "Beena Ahmed",
        "Julien Epps"
      ],
      "abstract": "Depression is a critical concern in global mental health, prompting extensive\nresearch into AI-based detection methods. Among various AI technologies, Large\nLanguage Models (LLMs) stand out for their versatility in mental healthcare\napplications. However, their primary limitation arises from their exclusive\ndependence on textual input, which constrains their overall capabilities.\nFurthermore, the utilization of LLMs in identifying and analyzing depressive\nstates is still relatively untapped. In this paper, we present an innovative\napproach to integrating acoustic speech information into the LLMs framework for\nmultimodal depression detection. We investigate an efficient method for\ndepression detection by integrating speech signals into LLMs utilizing Acoustic\nLandmarks. By incorporating acoustic landmarks, which are specific to the\npronunciation of spoken words, our method adds critical dimensions to text\ntranscripts. This integration also provides insights into the unique speech\npatterns of individuals, revealing the potential mental states of individuals.\nEvaluations of the proposed approach on the DAIC-WOZ dataset reveal\nstate-of-the-art results when compared with existing Audio-Text baselines. In\naddition, this approach is not only valuable for the detection of depression\nbut also represents a new perspective in enhancing the ability of LLMs to\ncomprehend and process speech signals.",
      "tldr_zh": "这篇论文提出了一种创新方法，将语音信号整合到大型语言模型（LLMs）中，用于多模态抑郁检测，以克服 LLMs 依赖文本输入的局限性。通过利用 Acoustic Landmarks（与发音相关的特定特征），该方法添加了文本以外的关键维度，如个体的独特语音模式，从而揭示潜在的心理状态。在 DAIC-WOZ 数据集上的评估显示，该方法比现有 Audio-Text 基线取得了最先进的结果。这一方法不仅提升了抑郁检测的准确性，还为 LLMs 处理语音信号提供了新的视角。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.13276v2",
      "published_date": "2024-02-17 09:39:46 UTC",
      "updated_date": "2024-09-23 22:54:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:21:33.862681"
    },
    {
      "arxiv_id": "2402.13275v1",
      "title": "Implementation of a Model of the Cortex Basal Ganglia Loop",
      "title_zh": "翻译失败",
      "authors": [
        "Naoya Arakawa"
      ],
      "abstract": "This article presents a simple model of the cortex-basal ganglia-thalamus\nloop, which is thought to serve for action selection and executions, and\nreports the results of its implementation. The model is based on the hypothesis\nthat the cerebral cortex predicts actions, while the basal ganglia use\nreinforcement learning to decide whether to perform the actions predicted by\nthe cortex. The implementation is intended to be used as a component of models\nof the brain consisting of cortical regions or brain-inspired cognitive\narchitectures.",
      "tldr_zh": "这篇论文提出了一种简单模型，模拟了 cortex-basal ganglia-thalamus 回路，用于行动选择和执行。模型基于假设：cortex 负责预测行动，而 basal ganglia 通过 reinforcement learning 机制决定是否执行这些行动。论文报告了该模型的实现结果，并将其设计为大脑区域模型或脑启发认知架构的组件。",
      "categories": [
        "q-bio.NC",
        "cs.AI"
      ],
      "primary_category": "q-bio.NC",
      "comment": "7 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.13275v1",
      "published_date": "2024-02-17 08:08:36 UTC",
      "updated_date": "2024-02-17 08:08:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:21:45.090531"
    },
    {
      "arxiv_id": "2402.11208v2",
      "title": "Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Wenkai Yang",
        "Xiaohan Bi",
        "Yankai Lin",
        "Sishuo Chen",
        "Jie Zhou",
        "Xu Sun"
      ],
      "abstract": "Driven by the rapid development of Large Language Models (LLMs), LLM-based\nagents have been developed to handle various real-world applications, including\nfinance, healthcare, and shopping, etc. It is crucial to ensure the reliability\nand security of LLM-based agents during applications. However, the safety\nissues of LLM-based agents are currently under-explored. In this work, we take\nthe first step to investigate one of the typical safety threats, backdoor\nattack, to LLM-based agents. We first formulate a general framework of agent\nbackdoor attacks, then we present a thorough analysis of different forms of\nagent backdoor attacks. Specifically, compared with traditional backdoor\nattacks on LLMs that are only able to manipulate the user inputs and model\noutputs, agent backdoor attacks exhibit more diverse and covert forms: (1) From\nthe perspective of the final attacking outcomes, the agent backdoor attacker\ncan not only choose to manipulate the final output distribution, but also\nintroduce the malicious behavior in an intermediate reasoning step only, while\nkeeping the final output correct. (2) Furthermore, the former category can be\ndivided into two subcategories based on trigger locations, in which the\nbackdoor trigger can either be hidden in the user query or appear in an\nintermediate observation returned by the external environment. We implement the\nabove variations of agent backdoor attacks on two typical agent tasks including\nweb shopping and tool utilization. Extensive experiments show that LLM-based\nagents suffer severely from backdoor attacks and such backdoor vulnerability\ncannot be easily mitigated by current textual backdoor defense algorithms. This\nindicates an urgent need for further research on the development of targeted\ndefenses against backdoor attacks on LLM-based agents. Warning: This paper may\ncontain biased content.",
      "tldr_zh": "这篇论文调查了后门攻击（backdoor attacks）对基于大型语言模型（LLM-based agents）的智能体的安全威胁，首次制定了一个通用框架来分析此类攻击。研究者探讨了攻击的多样形式，包括操纵最终输出分布、在中间推理步骤引入恶意行为，以及将后门触发器隐藏在用户查询或外部环境观察中，并在网络购物和工具利用任务上进行了实现。实验结果显示，LLM-based agents 严重易受后门攻击影响，现有的文本后门防御算法无法有效缓解，这突出了开发针对性防御措施的紧迫性。警告：论文可能包含偏见内容。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted at NeurIPS 2024, camera ready version. Code and data are\n  available at https://github.com/lancopku/agent-backdoor-attacks",
      "pdf_url": "http://arxiv.org/pdf/2402.11208v2",
      "published_date": "2024-02-17 06:48:45 UTC",
      "updated_date": "2024-10-29 15:32:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:21:58.705983"
    },
    {
      "arxiv_id": "2402.12399v2",
      "title": "Turn Waste into Worth: Rectifying Top-$k$ Router of MoE",
      "title_zh": "翻译失败",
      "authors": [
        "Zhiyuan Zeng",
        "Qipeng Guo",
        "Zhaoye Fei",
        "Zhangyue Yin",
        "Yunhua Zhou",
        "Linyang Li",
        "Tianxiang Sun",
        "Hang Yan",
        "Dahua Lin",
        "Xipeng Qiu"
      ],
      "abstract": "Sparse Mixture of Experts (MoE) models are popular for training large\nlanguage models due to their computational efficiency. However, the commonly\nused top-$k$ routing mechanism suffers from redundancy computation and memory\ncosts due to the unbalanced routing. Some experts are overflow, where the\nexceeding tokens are dropped. While some experts are vacant, which are padded\nwith zeros, negatively impacting model performance. To address the dropped\ntokens and padding, we propose the Rectify-Router, comprising the Intra-GPU\nRectification and the Fill-in Rectification. The Intra-GPU Rectification\nhandles dropped tokens, efficiently routing them to experts within the GPU\nwhere they are located to avoid inter-GPU communication. The Fill-in\nRectification addresses padding by replacing padding tokens with the tokens\nthat have high routing scores. Our experimental results demonstrate that the\nIntra-GPU Rectification and the Fill-in Rectification effectively handle\ndropped tokens and padding, respectively. Furthermore, the combination of them\nachieves superior performance, surpassing the accuracy of the vanilla top-1\nrouter by 4.7%.",
      "tldr_zh": "该论文针对 Sparse Mixture of Experts (MoE) 模型中 top-$k$ routing 机制的不平衡问题，提出 Rectify-Router 方法，以解决 experts 过载导致的 tokens 丢弃和空闲 experts 的零填充问题。Rectify-Router 包括 Intra-GPU Rectification，用于在同一 GPU 内高效路由丢弃 tokens 以避免跨 GPU 通信，以及 Fill-in Rectification，通过用高路由分数的 tokens 替换填充 tokens 来优化性能。实验结果显示，Intra-GPU Rectification 和 Fill-in Rectification 分别有效处理了相关问题，而二者结合使用比 vanilla top-1 router 提高了 4.7% 的准确率。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.12399v2",
      "published_date": "2024-02-17 06:23:27 UTC",
      "updated_date": "2024-02-21 13:33:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:22:09.444554"
    },
    {
      "arxiv_id": "2402.11203v1",
      "title": "Exploring ChatGPT for Next-generation Information Retrieval: Opportunities and Challenges",
      "title_zh": "探索 ChatGPT 用于下一代信息检索：机会与挑战",
      "authors": [
        "Yizheng Huang",
        "Jimmy Huang"
      ],
      "abstract": "The rapid advancement of artificial intelligence (AI) has highlighted ChatGPT\nas a pivotal technology in the field of information retrieval (IR).\nDistinguished from its predecessors, ChatGPT offers significant benefits that\nhave attracted the attention of both the industry and academic communities.\nWhile some view ChatGPT as a groundbreaking innovation, others attribute its\nsuccess to the effective integration of product development and market\nstrategies. The emergence of ChatGPT, alongside GPT-4, marks a new phase in\nGenerative AI, generating content that is distinct from training examples and\nexceeding the capabilities of the prior GPT-3 model by OpenAI. Unlike the\ntraditional supervised learning approach in IR tasks, ChatGPT challenges\nexisting paradigms, bringing forth new challenges and opportunities regarding\ntext quality assurance, model bias, and efficiency. This paper seeks to examine\nthe impact of ChatGPT on IR tasks and offer insights into its potential future\ndevelopments.",
      "tldr_zh": "这篇论文探讨了ChatGPT在下一代信息检索（IR）领域的机会和挑战，强调其作为生成AI里程碑（如与GPT-4的整合）带来的创新优势，例如生成独特内容并超越GPT-3模型。不同于传统的监督学习方法，ChatGPT引入了新问题，包括文本质量保证、模型偏差和效率优化。论文通过分析这些影响，为ChatGPT在IR任务的未来发展提供了宝贵见解。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "Survey Paper",
      "pdf_url": "http://arxiv.org/pdf/2402.11203v1",
      "published_date": "2024-02-17 05:44:40 UTC",
      "updated_date": "2024-02-17 05:44:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:22:19.991937"
    },
    {
      "arxiv_id": "2402.11196v2",
      "title": "Maintaining Adversarial Robustness in Continuous Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaolei Ru",
        "Xiaowei Cao",
        "Zijia Liu",
        "Jack Murdoch Moore",
        "Xin-Ya Zhang",
        "Xia Zhu",
        "Wenjia Wei",
        "Gang Yan"
      ],
      "abstract": "Adversarial robustness is essential for security and reliability of machine\nlearning systems. However, adversarial robustness enhanced by defense\nalgorithms is easily erased as the neural network's weights update to learn new\ntasks. To address this vulnerability, it is essential to improve the capability\nof neural networks in terms of robust continual learning. Specially, we propose\na novel gradient projection technique that effectively stabilizes sample\ngradients from previous data by orthogonally projecting back-propagation\ngradients onto a crucial subspace before using them for weight updates. This\ntechnique can maintaining robustness by collaborating with a class of defense\nalgorithms through sample gradient smoothing. The experimental results on four\nbenchmarks including Split-CIFAR100 and Split-miniImageNet, demonstrate that\nthe superiority of the proposed approach in mitigating rapidly degradation of\nrobustness during continual learning even when facing strong adversarial\nattacks.",
      "tldr_zh": "该研究针对机器学习系统中的对抗鲁棒性（adversarial robustness）问题，提出了一种新颖的梯度投影（gradient projection）技术，以防止神经网络在持续学习（continual learning）过程中学习新任务时鲁棒性迅速下降。该技术通过正交投影将反向传播梯度投影到关键子空间，从而稳定先前数据的样本梯度，并与防御算法协作实现样本梯度平滑。实验在Split-CIFAR100和Split-miniImageNet等四个基准上表明，该方法显著缓解了鲁棒性退化，即使面对强对抗攻击，也展示了优越性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.11196v2",
      "published_date": "2024-02-17 05:14:47 UTC",
      "updated_date": "2024-08-13 15:28:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:22:33.099134"
    },
    {
      "arxiv_id": "2402.11192v4",
      "title": "I Learn Better If You Speak My Language: Understanding the Superior Performance of Fine-Tuning Large Language Models with LLM-Generated Responses",
      "title_zh": "翻译失败",
      "authors": [
        "Xuan Ren",
        "Biao Wu",
        "Lingqiao Liu"
      ],
      "abstract": "This paper explores an intriguing observation: fine-tuning a large language\nmodel (LLM) with responses generated by a LLM often yields better results than\nusing responses generated by humans, particularly in reasoning tasks. We\nconduct an in-depth investigation to understand why this occurs. Contrary to\nthe common belief that these instances is due to the more detailed nature of\nLLM-generated content, our study identifies another contributing factor: an LLM\nis inherently more \"familiar\" with LLM generated responses. This familiarity is\nevidenced by lower perplexity before fine-tuning. We design a series of\nexperiments to understand the impact of the \"familiarity\" and our conclusion\nreveals that this \"familiarity\" significantly impacts learning performance.\nTraining with LLM-generated responses not only enhances performance but also\nhelps maintain the model's capabilities in other reasoning tasks after\nfine-tuning on a specific task.",
      "tldr_zh": "该论文探讨了使用LLM生成的响应微调大型语言模型（LLM）在推理任务中往往优于人类生成的响应这一现象。研究发现，主要原因在于LLM对自身生成的响应更“familiar”，体现在微调前的较低perplexity。作者通过一系列实验验证了这种familiarity对学习性能的显著影响，并证明使用LLM-generated responses不仅提升了特定任务的表现，还帮助模型保持其他推理任务的能力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "The paper has been accepted to EMNLP 2024 (Main Conference)",
      "pdf_url": "http://arxiv.org/pdf/2402.11192v4",
      "published_date": "2024-02-17 05:05:31 UTC",
      "updated_date": "2024-10-11 03:50:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:22:45.251083"
    },
    {
      "arxiv_id": "2402.11187v2",
      "title": "LaCo: Large Language Model Pruning via Layer Collapse",
      "title_zh": "翻译失败",
      "authors": [
        "Yifei Yang",
        "Zouying Cao",
        "Hai Zhao"
      ],
      "abstract": "Large language models (LLMs) based on transformer are witnessing a notable\ntrend of size expansion, which brings considerable costs to both model training\nand inference. However, existing methods such as model quantization, knowledge\ndistillation, and model pruning are constrained by various issues, including\nhardware support limitations, the need for extensive training, and alterations\nto the model internal structure. In this paper, we propose a concise layer-wise\nstructured pruner called \\textit{Layer Collapse (LaCo)}, in which rear model\nlayers collapse into a prior layer, enabling a rapid reduction in model size\nwhile preserving the model structure. Comprehensive experiments show that our\nmethod maintains an average task performance of over 80\\% at pruning ratios of\n25-30\\%, significantly outperforming existing state-of-the-art structured\npruning methods. We also conduct post-training experiments to confirm that the\n\\textit{LaCo} effectively inherits the parameters of the original model.\nAdditionally, we perform ablation studies on various settings of \\textit{LaCo}.\nFinally, we discuss our motivation from the perspective of layer-wise\nsimilarity and evaluate the performance of the pruned LLMs across various\npruning ratios\\footnote{\\url{https://github.com/yangyifei729/LaCo}}.",
      "tldr_zh": "本论文针对大型语言模型（Large Language Models, LLMs）的规模膨胀导致的训练和推理成本问题，提出了一种层级结构化修剪方法LaCo（Layer Collapse），通过将后层折叠到前层来快速减少模型大小，同时保持原结构完整。实验结果显示，在25-30%的修剪比例下，LaCo能维持平均任务性能超过80%，显著优于现有状态-of-the-art结构化修剪方法。此外，通过后训练实验和消融研究，证实了LaCo有效继承原模型参数，并从层级相似性角度解释了其动机。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted as Findings of EMNLP2024",
      "pdf_url": "http://arxiv.org/pdf/2402.11187v2",
      "published_date": "2024-02-17 04:16:30 UTC",
      "updated_date": "2024-10-15 01:58:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:22:59.151875"
    },
    {
      "arxiv_id": "2402.11176v3",
      "title": "KnowTuning: Knowledge-aware Fine-tuning for Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Yougang Lyu",
        "Lingyong Yan",
        "Shuaiqiang Wang",
        "Haibo Shi",
        "Dawei Yin",
        "Pengjie Ren",
        "Zhumin Chen",
        "Maarten de Rijke",
        "Zhaochun Ren"
      ],
      "abstract": "Despite their success at many natural language processing (NLP) tasks, large\nlanguage models still struggle to effectively leverage knowledge for\nknowledge-intensive tasks, manifesting limitations such as generating\nincomplete, non-factual, or illogical answers. These limitations stem from\ninadequate knowledge awareness of LLMs during vanilla fine-tuning. To address\nthese problems, we propose a knowledge-aware fine-tuning (KnowTuning) method to\nimprove fine-grained and coarse-grained knowledge awareness of LLMs. We devise\na fine-grained knowledge augmentation stage to train LLMs to identify difficult\nfine-grained knowledge in answers. We also propose a coarse-grained knowledge\ncomparison stage to train LLMs to distinguish between reliable and unreliable\nknowledge, in three aspects: completeness, factuality, and logicality.\nExtensive experiments on both generic and medical question answering (QA)\ndatasets confirm the effectiveness of KnowTuning, through automatic and human\nevaluations, across various sizes of LLMs. We further verify that KnowTuning\ngenerates more facts with less factual error rate under fine-grained facts\nevaluation.",
      "tldr_zh": "本文提出 KnowTuning，一种知识感知微调方法，用于提升 Large Language Models (LLMs) 在知识密集型任务中的表现，解决其生成不完整、非事实或不合逻辑答案的问题。该方法包括细粒度知识增强阶段（训练 LLMs 识别答案中的困难知识）和粗粒度知识比较阶段（区分知识的完整性、事实性和逻辑性）。实验结果显示，在通用和医疗问答数据集上，KnowTuning 显著提高了 LLMs 的准确性和可靠性，生成更多事实的同时降低了事实错误率。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP 2024 main paper",
      "pdf_url": "http://arxiv.org/pdf/2402.11176v3",
      "published_date": "2024-02-17 02:54:32 UTC",
      "updated_date": "2024-10-02 14:20:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:23:09.694287"
    },
    {
      "arxiv_id": "2402.11168v3",
      "title": "Trust Regions for Explanations via Black-Box Probabilistic Certification",
      "title_zh": "翻译失败",
      "authors": [
        "Amit Dhurandhar",
        "Swagatam Haldar",
        "Dennis Wei",
        "Karthikeyan Natesan Ramamurthy"
      ],
      "abstract": "Given the black box nature of machine learning models, a plethora of\nexplainability methods have been developed to decipher the factors behind\nindividual decisions. In this paper, we introduce a novel problem of black box\n(probabilistic) explanation certification. We ask the question: Given a black\nbox model with only query access, an explanation for an example and a quality\nmetric (viz. fidelity, stability), can we find the largest hypercube (i.e.,\n$\\ell_{\\infty}$ ball) centered at the example such that when the explanation is\napplied to all examples within the hypercube, (with high probability) a quality\ncriterion is met (viz. fidelity greater than some value)? Being able to\nefficiently find such a \\emph{trust region} has multiple benefits: i) insight\ninto model behavior in a \\emph{region}, with a \\emph{guarantee}; ii)\nascertained \\emph{stability} of the explanation; iii) \\emph{explanation reuse},\nwhich can save time, energy and money by not having to find explanations for\nevery example; and iv) a possible \\emph{meta-metric} to compare explanation\nmethods. Our contributions include formalizing this problem, proposing\nsolutions, providing theoretical guarantees for these solutions that are\ncomputable, and experimentally showing their efficacy on synthetic and real\ndata.",
      "tldr_zh": "该论文引入了黑盒模型解释的概率认证问题，旨在通过查询访问黑盒模型，寻找以给定示例为中心的最大超立方体（即$\\ell_{\\infty}$球），确保在该区域内应用解释时，以高概率满足质量指标（如fidelity大于特定值或稳定性）。这种“trust region”方法提供了对模型行为区域性洞察的保证、解释的稳定性验证、解释重用的资源节约，以及作为比较解释方法的meta-metric。作者形式化了该问题，提出了解决方案，并通过理论保证和实验验证，在合成和真实数据上证明了其有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to ICML 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.11168v3",
      "published_date": "2024-02-17 02:26:14 UTC",
      "updated_date": "2024-06-05 16:36:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:23:21.067015"
    },
    {
      "arxiv_id": "2402.11167v2",
      "title": "ToBlend: Token-Level Blending With an Ensemble of LLMs to Attack AI-Generated Text Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Fan Huang",
        "Haewoon Kwak",
        "Jisun An"
      ],
      "abstract": "The robustness of AI-content detection models against sophisticated\nadversarial strategies, such as paraphrasing or word switching, is a rising\nconcern in natural language generation (NLG) applications. This study proposes\nToBlend, a novel token-level ensemble text generation method to challenge the\nrobustness of current AI-content detection approaches by utilizing multiple\nsets of candidate generative large language models (LLMs). By randomly sampling\ntoken(s) from candidate LLMs sets, we find ToBlend significantly drops the\nperformance of most mainstream AI-content detection methods. We evaluate the\ntext quality produced under different ToBlend settings based on annotations\nfrom experienced human experts. We proposed a fine-tuned Llama3.1 model to\ndistinguish the ToBlend generated text more accurately. Our findings underscore\nour proposed text generation approach's great potential in deceiving and\nimproving detection models. Our datasets, codes, and annotations are\nopen-sourced.",
      "tldr_zh": "这篇论文提出了 ToBlend，一种基于 token-level 的 ensemble 方法，利用多个候选生成式大型语言模型 (LLMs) 通过随机采样 token(s) 生成文本，以挑战 AI 生成文本检测模型的鲁棒性。实验结果显示，ToBlend 显著降低了主流检测方法的性能，并在人类专家的注解评估中证明了生成的文本质量。研究还 fine-tuned 一个 Llama3.1 模型来更准确地识别此类文本，并强调了该方法在欺骗和改进检测模型方面的潜力，同时开源了数据集、代码和注解。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Submitted to ARR Oct-2024 Cycle",
      "pdf_url": "http://arxiv.org/pdf/2402.11167v2",
      "published_date": "2024-02-17 02:25:57 UTC",
      "updated_date": "2024-10-16 15:40:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:23:33.392832"
    },
    {
      "arxiv_id": "2402.11161v5",
      "title": "PEDANTS: Cheap but Effective and Interpretable Answer Equivalence",
      "title_zh": "PEDANTS：廉价但有效且可解释的答案等价性",
      "authors": [
        "Zongxia Li",
        "Ishani Mondal",
        "Yijun Liang",
        "Huy Nghiem",
        "Jordan Lee Boyd-Graber"
      ],
      "abstract": "Question answering (QA) can only make progress if we know if an answer is\ncorrect, but current answer correctness (AC) metrics struggle with verbose,\nfree-form answers from large language models (LLMs). There are two challenges\nwith current short-form QA evaluations: a lack of diverse styles of evaluation\ndata and an over-reliance on expensive and slow LLMs. LLM-based scorers\ncorrelate better with humans, but this expensive task has only been tested on\nlimited QA datasets. We rectify these issues by providing rubrics and datasets\nfor evaluating machine QA adopted from the Trivia community. We also propose an\nefficient, and interpretable QA evaluation that is more stable than an exact\nmatch and neural methods(BERTScore).",
      "tldr_zh": "该研究针对问答（QA）系统的答案正确性（AC）评估问题，指出现有指标在处理大型语言模型（LLMs）的冗长自由形式答案时面临数据多样性不足和过度依赖昂贵LLMs的挑战。作者引入PEDANTS框架，通过从Trivia社区采纳的评估标准和数据集，提供一种高效、可解释的QA评估方法，该方法比精确匹配和神经方法（如BERTScore）更稳定。实验结果显示，这种方法提高了评估的准确性和可靠性，为改进QA系统评估奠定了基础。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Efficient PEDANTS Classifier for short-form QA in github:\n  https://github.com/zli12321/qa_metrics. arXiv admin note: text overlap with\n  arXiv:2401.13170",
      "pdf_url": "http://arxiv.org/pdf/2402.11161v5",
      "published_date": "2024-02-17 01:56:19 UTC",
      "updated_date": "2024-10-11 20:56:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:23:45.780020"
    },
    {
      "arxiv_id": "2402.11140v2",
      "title": "Boosting of Thoughts: Trial-and-Error Problem Solving with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Sijia Chen",
        "Baochun Li",
        "Di Niu"
      ],
      "abstract": "The reasoning performance of Large Language Models (LLMs) on a wide range of\nproblems critically relies on chain-of-thought prompting, which involves\nproviding a few chain of thought demonstrations as exemplars in prompts. Recent\nwork, e.g., Tree of Thoughts, has pointed out the importance of exploration and\nself-evaluation in reasoning step selection for complex problem solving. In\nthis paper, we present Boosting of Thoughts (BoT), an automated prompting\nframework for problem solving with LLMs by iteratively exploring and\nself-evaluating many trees of thoughts in order to acquire an ensemble of\ntrial-and-error reasoning experiences, which will serve as a new form of\nprompting to solve the complex problem. Starting from a simple prompt without\nrequiring examples, BoT iteratively explores and evaluates a large collection\nof reasoning steps, and more importantly, uses error analysis obtained from the\nLLM on them to explicitly revise prompting, which in turn enhances reasoning\nstep generation, until a final answer is attained. Our experiments with GPT-4\nand Llama2 across extensive complex mathematical problems demonstrate that BoT\nconsistently achieves higher or comparable problem-solving rates than other\nadvanced prompting approaches.",
      "tldr_zh": "该研究提出了 Boosting of Thoughts (BoT)，一种自动化提示框架，用于提升 Large Language Models (LLMs) 在复杂问题解决中的性能，通过迭代探索和自我评估多个思考树（trees of thoughts）来实现试错式推理。BoT 从一个简单提示开始，不依赖预设示例，而是通过生成推理步骤、进行错误分析并显式修订提示，从而逐步优化生成过程，直至得出最终答案。与现有方法相比，该框架在 GPT-4 和 Llama2 模型上进行的大量复杂数学问题实验中， consistently 实现了比其他高级 prompting approaches 更高的或相当的问题解决率，为 LLMs 的推理能力提供了新颖的提升路径。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted as a poster paper by ICLR2024. 27 pages, 5 figures, 18\n  tables. [Source\n  Code](https://github.com/iQua/llmpebase/tree/main/examples/BoTReasoning)",
      "pdf_url": "http://arxiv.org/pdf/2402.11140v2",
      "published_date": "2024-02-17 00:13:36 UTC",
      "updated_date": "2025-01-06 21:18:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:23:57.125665"
    },
    {
      "arxiv_id": "2402.11139v1",
      "title": "LiGNN: Graph Neural Networks at LinkedIn",
      "title_zh": "LiGNN：图神经网络在 LinkedIn",
      "authors": [
        "Fedor Borisyuk",
        "Shihai He",
        "Yunbo Ouyang",
        "Morteza Ramezani",
        "Peng Du",
        "Xiaochen Hou",
        "Chengming Jiang",
        "Nitin Pasumarthy",
        "Priya Bannur",
        "Birjodh Tiwana",
        "Ping Liu",
        "Siddharth Dangi",
        "Daqi Sun",
        "Zhoutao Pei",
        "Xiao Shi",
        "Sirou Zhu",
        "Qianqi Shen",
        "Kuang-Hsuan Lee",
        "David Stein",
        "Baolei Li",
        "Haichao Wei",
        "Amol Ghoting",
        "Souvik Ghosh"
      ],
      "abstract": "In this paper, we present LiGNN, a deployed large-scale Graph Neural Networks\n(GNNs) Framework. We share our insight on developing and deployment of GNNs at\nlarge scale at LinkedIn. We present a set of algorithmic improvements to the\nquality of GNN representation learning including temporal graph architectures\nwith long term losses, effective cold start solutions via graph densification,\nID embeddings and multi-hop neighbor sampling. We explain how we built and sped\nup by 7x our large-scale training on LinkedIn graphs with adaptive sampling of\nneighbors, grouping and slicing of training data batches, specialized\nshared-memory queue and local gradient optimization. We summarize our\ndeployment lessons and learnings gathered from A/B test experiments. The\ntechniques presented in this work have contributed to an approximate relative\nimprovements of 1% of Job application hearing back rate, 2% Ads CTR lift, 0.5%\nof Feed engaged daily active users, 0.2% session lift and 0.1% weekly active\nuser lift from people recommendation. We believe that this work can provide\npractical solutions and insights for engineers who are interested in applying\nGraph neural networks at large scale.",
      "tldr_zh": "这篇论文介绍了 LiGNN，一种在 LinkedIn 部署的大规模 Graph Neural Networks (GNNs) 框架，旨在分享 GNNs 的开发和部署经验。框架通过 temporal graph architectures with long term losses、graph densification、ID embeddings 和 multi-hop neighbor sampling 等算法改进，提升了表示学习质量并解决了冷启动问题；同时，通过 adaptive sampling of neighbors、数据批处理优化以及专用共享内存队列，使训练速度提高了 7 倍。实验结果显示，LiGNN 在实际应用中带来了显著提升，包括求职反馈率提高 1%、广告点击率 (CTR) 提升 2%、Feed 活跃用户增加 0.5% 等，为大规模 GNNs 应用提供了实用解决方案和见解。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.11139v1",
      "published_date": "2024-02-17 00:10:33 UTC",
      "updated_date": "2024-02-17 00:10:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:24:10.995790"
    },
    {
      "arxiv_id": "2402.11138v2",
      "title": "Contrastive Instruction Tuning",
      "title_zh": "对比指令微调",
      "authors": [
        "Tianyi Lorena Yan",
        "Fei Wang",
        "James Y. Huang",
        "Wenxuan Zhou",
        "Fan Yin",
        "Aram Galstyan",
        "Wenpeng Yin",
        "Muhao Chen"
      ],
      "abstract": "Instruction tuning has been used as a promising approach to improve the\nperformance of large language models (LLMs) on unseen tasks. However, current\nLLMs exhibit limited robustness to unseen instructions, generating inconsistent\noutputs when the same instruction is phrased with slightly varied forms or\nlanguage styles. This behavior indicates LLMs' lack of robustness to textual\nvariations and generalizability to unseen instructions, potentially leading to\ntrustworthiness issues. Accordingly, we propose Contrastive Instruction Tuning,\nwhich maximizes the similarity between the hidden representations of\nsemantically equivalent instruction-instance pairs while minimizing the\nsimilarity between semantically different ones. To facilitate this approach, we\naugment the existing FLAN collection by paraphrasing task instructions.\nExperiments on the PromptBench benchmark show that CoIN consistently improves\nLLMs' robustness to unseen instructions with variations across character, word,\nsentence, and semantic levels by an average of +2.5% in accuracy. Code is\navailable at https://github.com/luka-group/CoIN.",
      "tldr_zh": "本研究发现，现有的指令微调方法虽能提升大语言模型（LLMs）在未见任务上的性能，但LLMs对文本变体（如语言风格或表述变化）的鲁棒性和泛化能力不足，可能引发可信度问题。为此，论文提出Contrastive Instruction Tuning (CoIN)方法，通过最大化语义等价指令-实例对的隐藏表示相似度，同时最小化语义不同对的相似度，并通过对FLAN数据集进行指令改写来扩充训练数据。实验结果显示，在PromptBench基准测试中，CoIN平均提高了LLMs对字符、单词、句子和语义级别变体的鲁棒性，准确率提升约2.5%。该方法为提升LLMs的可靠性和泛化能力提供了新途径，代码已在GitHub开源。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL 2024 Findings",
      "pdf_url": "http://arxiv.org/pdf/2402.11138v2",
      "published_date": "2024-02-17 00:09:32 UTC",
      "updated_date": "2024-06-06 06:03:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:24:23.091033"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 40,
  "processed_papers_count": 40,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-17T07:24:41.860857"
}
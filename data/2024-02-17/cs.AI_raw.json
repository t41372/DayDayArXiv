[
  {
    "arxiv_id": "2402.11403v2",
    "title": "An Empirical Evaluation of Neural and Neuro-symbolic Approaches to Real-time Multimodal Complex Event Detection",
    "authors": [
      "Liying Han",
      "Mani B. Srivastava"
    ],
    "abstract": "Robots and autonomous systems require an understanding of complex events\n(CEs) from sensor data to interact with their environments and humans\neffectively. Traditional end-to-end neural architectures, despite processing\nsensor data efficiently, struggle with long-duration events due to limited\ncontext sizes and reasoning capabilities. Recent advances in neuro-symbolic\nmethods, which integrate neural and symbolic models leveraging human knowledge,\npromise improved performance with less data. This study addresses the gap in\nunderstanding these approaches' effectiveness in complex event detection (CED),\nespecially in temporal reasoning. We investigate neural and neuro-symbolic\narchitectures' performance in a multimodal CED task, analyzing IMU and acoustic\ndata streams to recognize CE patterns. Our methodology includes (i) end-to-end\nneural architectures for direct CE detection from sensor embeddings, (ii)\ntwo-stage concept-based neural models mapping sensor embeddings to atomic\nevents (AEs) before CE detection, and (iii) a neuro-symbolic approach using a\nsymbolic finite-state machine for CE detection from AEs. Empirically, the\nneuro-symbolic architecture significantly surpasses purely neural models,\ndemonstrating superior performance in CE recognition, even with extensive\ntraining data and ample temporal context for neural approaches.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.11403v2",
    "published_date": "2024-02-17 23:34:50 UTC",
    "updated_date": "2024-03-03 22:07:50 UTC"
  },
  {
    "arxiv_id": "2402.11398v2",
    "title": "Reasoning before Comparison: LLM-Enhanced Semantic Similarity Metrics for Domain Specialized Text Analysis",
    "authors": [
      "Shaochen Xu",
      "Zihao Wu",
      "Huaqin Zhao",
      "Peng Shu",
      "Zhengliang Liu",
      "Wenxiong Liao",
      "Sheng Li",
      "Andrea Sikora",
      "Tianming Liu",
      "Xiang Li"
    ],
    "abstract": "In this study, we leverage LLM to enhance the semantic analysis and develop\nsimilarity metrics for texts, addressing the limitations of traditional\nunsupervised NLP metrics like ROUGE and BLEU. We develop a framework where LLMs\nsuch as GPT-4 are employed for zero-shot text identification and label\ngeneration for radiology reports, where the labels are then used as\nmeasurements for text similarity. By testing the proposed framework on the\nMIMIC data, we find that GPT-4 generated labels can significantly improve the\nsemantic similarity assessment, with scores more closely aligned with clinical\nground truth than traditional NLP metrics. Our work demonstrates the\npossibility of conducting semantic analysis of the text data using\nsemi-quantitative reasoning results by the LLMs for highly specialized domains.\nWhile the framework is implemented for radiology report similarity analysis,\nits concept can be extended to other specialized domains as well.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.11398v2",
    "published_date": "2024-02-17 22:46:44 UTC",
    "updated_date": "2024-02-20 22:23:42 UTC"
  },
  {
    "arxiv_id": "2402.14833v1",
    "title": "CliqueParcel: An Approach For Batching LLM Prompts That Jointly Optimizes Efficiency And Faithfulness",
    "authors": [
      "Jiayi Liu",
      "Tinghan Yang",
      "Jennifer Neville"
    ],
    "abstract": "Large language models (LLMs) have become pivotal in recent research. However,\nduring the inference process, LLMs still require substantial resources. In this\npaper, we propose CliqueParcel, a method designed to improve the efficiency of\nLLMs via prompt batching. Existing strategies to optimize inference efficiency\noften compromise on output quality, leading to a discounted output problem.\nThis issue might result in reduced accuracy or outputs that are less detailed.\nCliqueParcel is our answer to this challenge. While ensuring accuracy and\nminimizing deviations from the original outputs (i.e., faithfulness), our\nmethod significantly improves efficiency during inference.\n  To lay the groundwork, we first redefine efficiency measurements by excluding\nthe reduction in running time due to shorter lengths. Then, we provide a\ncomprehensive trade-off between efficiency and faithfulness to clarify the\nnature of the 'discounted output' problem. Within the CliqueParcel framework,\nwe suggest multiple batching sub-methods and discuss the specific scenarios in\nwhich they can be applied. During evaluation, CliqueParcel is tested on eight\nwidely recognized datasets, which can be classified into three types: reading\ncomprehension, open-source question-answering, and reasoning. Our experiments\nexplore the performance of CliqueParcel, including efficiency, faithfulness,\nand the trade-off between them. This work provides novel insights into\ninference efficiency and demonstrates promising performance.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.14833v1",
    "published_date": "2024-02-17 22:37:17 UTC",
    "updated_date": "2024-02-17 22:37:17 UTC"
  },
  {
    "arxiv_id": "2402.11363v3",
    "title": "Transformer-based de novo peptide sequencing for data-independent acquisition mass spectrometry",
    "authors": [
      "Shiva Ebrahimi",
      "Xuan Guo"
    ],
    "abstract": "Tandem mass spectrometry (MS/MS) stands as the predominant high-throughput\ntechnique for comprehensively analyzing protein content within biological\nsamples. This methodology is a cornerstone driving the advancement of\nproteomics. In recent years, substantial strides have been made in\nData-Independent Acquisition (DIA) strategies, facilitating impartial and\nnon-targeted fragmentation of precursor ions. The DIA-generated MS/MS spectra\npresent a formidable obstacle due to their inherent high multiplexing nature.\nEach spectrum encapsulates fragmented product ions originating from multiple\nprecursor peptides. This intricacy poses a particularly acute challenge in de\nnovo peptide/protein sequencing, where current methods are ill-equipped to\naddress the multiplexing conundrum. In this paper, we introduce DiaTrans, a\ndeep-learning model based on transformer architecture. It deciphers peptide\nsequences from DIA mass spectrometry data. Our results show significant\nimprovements over existing STOA methods, including DeepNovo-DIA and PepNet.\nCasanovo-DIA enhances precision by 15.14% to 34.8%, recall by 11.62% to 31.94%\nat the amino acid level, and boosts precision by 59% to 81.36% at the peptide\nlevel. Integrating DIA data and our DiaTrans model holds considerable promise\nto uncover novel peptides and more comprehensive profiling of biological\nsamples. Casanovo-DIA is freely available under the GNU GPL license at\nhttps://github.com/Biocomputing-Research-Group/DiaTrans.",
    "categories": [
      "q-bio.QM",
      "cs.AI"
    ],
    "primary_category": "q-bio.QM",
    "comment": "Ebrahimi S., Guo X. Transformer-based de novo peptide sequencing for\n  data-independent acquisition mass spectrometry. In 2023 IEEE 23rd\n  International Conference on Bioinformatics and Bioengineering (BIBE) 2022 Dec\n  6 (pp. 17-22). IEEE",
    "pdf_url": "http://arxiv.org/pdf/2402.11363v3",
    "published_date": "2024-02-17 19:04:23 UTC",
    "updated_date": "2024-06-26 07:45:33 UTC"
  },
  {
    "arxiv_id": "2402.11359v4",
    "title": "Offline Training of Language Model Agents with Functions as Learnable Weights",
    "authors": [
      "Shaokun Zhang",
      "Jieyu Zhang",
      "Jiale Liu",
      "Linxin Song",
      "Chi Wang",
      "Ranjay Krishna",
      "Qingyun Wu"
    ],
    "abstract": "Researchers and practitioners have recently reframed powerful Large Language\nModels (LLMs) as agents, enabling them to automate complex tasks largely via\nthe use of specialized functions. To facilitate the development of LLM agents,\nwe present a novel paradigm of training LLM agents without modifying the LLM\nweights, which is particularly useful when the LLMs are difficult or\ninaccessible for modifications. Inspired by how humans continuously forge tools\nto adapt to real-world tasks, rather than change our biological structure to\nfit a static set of tools, we propose to progressively forge agent's functions\nto better solve the downstream tasks instead of modifying the LLM weights. By\ntreating the functions as learnable `agent parameters' and leveraging the\nfundamental idea of model training in artificial intelligence, we develop\nAgentOptimizer that employs the LLM to update agents' functions and devise an\nagent training algorithm with two strategies, roll-back, and early-stop, to\nstreamline the training process. With extensive experiments, we showcase that\nthe agent training paradigm could significantly improve the performance of\nrepresentative LLM agents in various downstream tasks. We also study the\nbehavior of the agent training regarding aspects like the learning curve and\ndomain transferability.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "22 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.11359v4",
    "published_date": "2024-02-17 18:31:21 UTC",
    "updated_date": "2024-07-30 18:22:00 UTC"
  },
  {
    "arxiv_id": "2402.11354v2",
    "title": "Probabilistic Routing for Graph-Based Approximate Nearest Neighbor Search",
    "authors": [
      "Kejing Lu",
      "Chuan Xiao",
      "Yoshiharu Ishikawa"
    ],
    "abstract": "Approximate nearest neighbor search (ANNS) in high-dimensional spaces is a\npivotal challenge in the field of machine learning. In recent years,\ngraph-based methods have emerged as the superior approach to ANNS, establishing\na new state of the art. Although various optimizations for graph-based ANNS\nhave been introduced, they predominantly rely on heuristic methods that lack\nformal theoretical backing. This paper aims to enhance routing within\ngraph-based ANNS by introducing a method that offers a probabilistic guarantee\nwhen exploring a node's neighbors in the graph. We formulate the problem as\nprobabilistic routing and develop two baseline strategies by incorporating\nlocality-sensitive techniques. Subsequently, we introduce PEOs, a novel\napproach that efficiently identifies which neighbors in the graph should be\nconsidered for exact distance calculation, thus significantly improving\nefficiency in practice. Our experiments demonstrate that equipping PEOs can\nincrease throughput on commonly utilized graph indexes (HNSW and NSSG) by a\nfactor of 1.6 to 2.5, and its efficiency consistently outperforms the\nleading-edge routing technique by 1.1 to 1.4 times.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.DB",
      "cs.DS"
    ],
    "primary_category": "cs.LG",
    "comment": "Source code is available at https://github.com/ICML2024-code/PEOs",
    "pdf_url": "http://arxiv.org/pdf/2402.11354v2",
    "published_date": "2024-02-17 18:08:37 UTC",
    "updated_date": "2024-07-10 17:05:43 UTC"
  },
  {
    "arxiv_id": "2402.11353v1",
    "title": "Understanding the Impact of Long-Term Memory on Self-Disclosure with Large Language Model-Driven Chatbots for Public Health Intervention",
    "authors": [
      "Eunkyung Jo",
      "Yuin Jeong",
      "SoHyun Park",
      "Daniel A. Epstein",
      "Young-Ho Kim"
    ],
    "abstract": "Recent large language models (LLMs) offer the potential to support public\nhealth monitoring by facilitating health disclosure through open-ended\nconversations but rarely preserve the knowledge gained about individuals across\nrepeated interactions. Augmenting LLMs with long-term memory (LTM) presents an\nopportunity to improve engagement and self-disclosure, but we lack an\nunderstanding of how LTM impacts people's interaction with LLM-driven chatbots\nin public health interventions. We examine the case of CareCall -- an\nLLM-driven voice chatbot with LTM -- through the analysis of 1,252 call logs\nand interviews with nine users. We found that LTM enhanced health disclosure\nand fostered positive perceptions of the chatbot by offering familiarity.\nHowever, we also observed challenges in promoting self-disclosure through LTM,\nparticularly around addressing chronic health conditions and privacy concerns.\nWe discuss considerations for LTM integration in LLM-driven chatbots for public\nhealth monitoring, including carefully deciding what topics need to be\nremembered in light of public health goals.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "H.5.2; I.2.7"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted to ACM CHI 2024 as a full paper",
    "pdf_url": "http://arxiv.org/pdf/2402.11353v1",
    "published_date": "2024-02-17 18:05:53 UTC",
    "updated_date": "2024-02-17 18:05:53 UTC"
  },
  {
    "arxiv_id": "2402.11349v2",
    "title": "Language Models Don't Learn the Physical Manifestation of Language",
    "authors": [
      "Bruce W. Lee",
      "JaeHyuk Lim"
    ],
    "abstract": "We argue that language-only models don't learn the physical manifestation of\nlanguage. We present an empirical investigation of visual-auditory properties\nof language through a series of tasks, termed H-Test. These tasks highlight a\nfundamental gap between human linguistic understanding and the sensory-deprived\nlinguistic understanding of LLMs. In support of our hypothesis, 1. deliberate\nreasoning (Chain-of-Thought), 2. few-shot examples, or 3. stronger LLM from the\nsame model family (LLaMA 2 13B -> LLaMA 2 70B) has no significant effect on\nH-Test performance.\n  We bring in the philosophical case of Mary, who learns about the world in a\nsensory-deprived environment as a useful conceptual framework to understand how\nlanguage-only models learn about the world (Jackson, 1986). Our experiments\nshow that some of the strongest proprietary LLMs stay near random chance\nbaseline accuracy of 50%, highlighting the limitations of linguistic knowledge\nacquired in the absence of sensory experience. Our code and data are available\nat <github.com/brucewlee/h-test>.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL 2024 Main",
    "pdf_url": "http://arxiv.org/pdf/2402.11349v2",
    "published_date": "2024-02-17 17:52:24 UTC",
    "updated_date": "2024-06-06 17:20:21 UTC"
  },
  {
    "arxiv_id": "2402.11338v2",
    "title": "Fair Classification with Partial Feedback: An Exploration-Based Data Collection Approach",
    "authors": [
      "Vijay Keswani",
      "Anay Mehrotra",
      "L. Elisa Celis"
    ],
    "abstract": "In many predictive contexts (e.g., credit lending), true outcomes are only\nobserved for samples that were positively classified in the past. These past\nobservations, in turn, form training datasets for classifiers that make future\npredictions. However, such training datasets lack information about the\noutcomes of samples that were (incorrectly) negatively classified in the past\nand can lead to erroneous classifiers. We present an approach that trains a\nclassifier using available data and comes with a family of exploration\nstrategies to collect outcome data about subpopulations that otherwise would\nhave been ignored. For any exploration strategy, the approach comes with\nguarantees that (1) all sub-populations are explored, (2) the fraction of false\npositives is bounded, and (3) the trained classifier converges to a ``desired''\nclassifier. The right exploration strategy is context-dependent; it can be\nchosen to improve learning guarantees and encode context-specific group\nfairness properties. Evaluation on real-world datasets shows that this approach\nconsistently boosts the quality of collected outcome data and improves the\nfraction of true positives for all groups, with only a small reduction in\npredictive utility.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted for presentation at ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.11338v2",
    "published_date": "2024-02-17 17:09:19 UTC",
    "updated_date": "2024-06-01 12:48:40 UTC"
  },
  {
    "arxiv_id": "2402.11337v1",
    "title": "Learning by Reconstruction Produces Uninformative Features For Perception",
    "authors": [
      "Randall Balestriero",
      "Yann LeCun"
    ],
    "abstract": "Input space reconstruction is an attractive representation learning paradigm.\nDespite interpretability of the reconstruction and generation, we identify a\nmisalignment between learning by reconstruction, and learning for perception.\nWe show that the former allocates a model's capacity towards a subspace of the\ndata explaining the observed variance--a subspace with uninformative features\nfor the latter. For example, the supervised TinyImagenet task with images\nprojected onto the top subspace explaining 90\\% of the pixel variance can be\nsolved with 45\\% test accuracy. Using the bottom subspace instead, accounting\nfor only 20\\% of the pixel variance, reaches 55\\% test accuracy. The features\nfor perception being learned last explains the need for long training time,\ne.g., with Masked Autoencoders. Learning by denoising is a popular strategy to\nalleviate that misalignment. We prove that while some noise strategies such as\nmasking are indeed beneficial, others such as additive Gaussian noise are not.\nYet, even in the case of masking, we find that the benefits vary as a function\nof the mask's shape, ratio, and the considered dataset. While tuning the noise\nstrategy without knowledge of the perception task seems challenging, we provide\nfirst clues on how to detect if a noise strategy is never beneficial regardless\nof the perception task.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.11337v1",
    "published_date": "2024-02-17 17:08:16 UTC",
    "updated_date": "2024-02-17 17:08:16 UTC"
  },
  {
    "arxiv_id": "2402.11322v3",
    "title": "SpikeNAS: A Fast Memory-Aware Neural Architecture Search Framework for Spiking Neural Network-based Autonomous Agents",
    "authors": [
      "Rachmad Vidya Wicaksana Putra",
      "Muhammad Shafique"
    ],
    "abstract": "Autonomous mobile agents (e.g., UAVs and UGVs) are typically expected to\nincur low power/energy consumption for solving machine learning tasks (such as\nobject recognition), as these mobile agents are usually powered by portable\nbatteries. These requirements can be fulfilled by Spiking Neural Networks\n(SNNs), since their bio-inspired spike-based operations offer high accuracy and\nultra low-power/energy computation. Currently, most of the SNN architectures\nare derived from Artificial Neural Networks whose neurons' architectures and\noperations are different from SNNs, or developed without considering memory\nbudgets from the underlying processing hardware of autonomous mobile agents.\nThese limitations hinder SNNs from reaching their full potential in accuracy\nand efficiency. Toward this, we propose SpikeNAS, a novel fast memory-aware\nneural architecture search (NAS) framework for SNNs that quickly finds an\nappropriate SNN architecture with high accuracy under the given memory budgets\nfrom autonomous mobile agents. To do this, our SpikeNAS employs several key\nsteps: analyzing the impacts of network operations on the accuracy, enhancing\nthe network architecture to improve the learning quality, and developing a fast\nmemory-aware search algorithm. The experimental results show that our SpikeNAS\nimproves the searching time and maintains high accuracy as compared to\nstate-of-the-art while meeting the given memory budgets (e.g., 4.4x faster\nsearch with 1.3% accuracy improvement for CIFAR100, using an Nvidia RTX 6000\nAda GPU machine), thereby quickly providing the appropriate SNN architecture\nfor the memory-constrained autonomous mobile agents.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "8 pages, 13 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2402.11322v3",
    "published_date": "2024-02-17 16:33:54 UTC",
    "updated_date": "2024-04-05 11:51:58 UTC"
  },
  {
    "arxiv_id": "2402.11319v3",
    "title": "Hysteresis Compensation of Flexible Continuum Manipulator using RGBD Sensing and Temporal Convolutional Network",
    "authors": [
      "Junhyun Park",
      "Seonghyeok Jang",
      "Hyojae Park",
      "Seongjun Bae",
      "Minho Hwang"
    ],
    "abstract": "Flexible continuum manipulators are valued for minimally invasive surgery,\noffering access to confined spaces through nonlinear paths. However,\ncable-driven manipulators face control difficulties due to hysteresis from\ncabling effects such as friction, elongation, and coupling. These effects are\ndifficult to model due to nonlinearity and the difficulties become even more\nevident when dealing with long and coupled, multi-segmented manipulator. This\npaper proposes a data-driven approach based on Deep Neural Networks (DNN) to\ncapture these nonlinear and previous states-dependent characteristics of cable\nactuation. We collect physical joint configurations according to command joint\nconfigurations using RGBD sensing and 7 fiducial markers to model the\nhysteresis of the proposed manipulator. Result on a study comparing the\nestimation performance of four DNN models show that the Temporal Convolution\nNetwork (TCN) demonstrates the highest predictive capability. Leveraging\ntrained TCNs, we build a control algorithm to compensate for hysteresis.\nTracking tests in task space using unseen trajectories show that the proposed\ncontrol algorithm reduces the average position and orientation error by 61.39%\n(from 13.7mm to 5.29 mm) and 64.04% (from 31.17{\\deg} to 11.21{\\deg}),\nrespectively. This result implies that the proposed calibrated controller\neffectively reaches the desired configurations by estimating the hysteresis of\nthe manipulator. Applying this method in real surgical scenarios has the\npotential to enhance control precision and improve surgical performance.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages, 11 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2402.11319v3",
    "published_date": "2024-02-17 16:20:59 UTC",
    "updated_date": "2024-05-03 17:19:31 UTC"
  },
  {
    "arxiv_id": "2402.11317v2",
    "title": "Debiased Offline Representation Learning for Fast Online Adaptation in Non-stationary Dynamics",
    "authors": [
      "Xinyu Zhang",
      "Wenjie Qiu",
      "Yi-Chen Li",
      "Lei Yuan",
      "Chengxing Jia",
      "Zongzhang Zhang",
      "Yang Yu"
    ],
    "abstract": "Developing policies that can adjust to non-stationary environments is\nessential for real-world reinforcement learning applications. However, learning\nsuch adaptable policies in offline settings, with only a limited set of\npre-collected trajectories, presents significant challenges. A key difficulty\narises because the limited offline data makes it hard for the context encoder\nto differentiate between changes in the environment dynamics and shifts in the\nbehavior policy, often leading to context misassociations. To address this\nissue, we introduce a novel approach called Debiased Offline Representation for\nfast online Adaptation (DORA). DORA incorporates an information bottleneck\nprinciple that maximizes mutual information between the dynamics encoding and\nthe environmental data, while minimizing mutual information between the\ndynamics encoding and the actions of the behavior policy. We present a\npractical implementation of DORA, leveraging tractable bounds of the\ninformation bottleneck principle. Our experimental evaluation across six\nbenchmark MuJoCo tasks with variable parameters demonstrates that DORA not only\nachieves a more precise dynamics encoding but also significantly outperforms\nexisting baselines in terms of performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.11317v2",
    "published_date": "2024-02-17 16:03:35 UTC",
    "updated_date": "2025-03-27 13:46:17 UTC"
  },
  {
    "arxiv_id": "2402.11314v1",
    "title": "Multi-Generative Agent Collective Decision-Making in Urban Planning: A Case Study for Kendall Square Renovation",
    "authors": [
      "Jin Gao",
      "Hanyong Xu",
      "Luc Dao"
    ],
    "abstract": "In this study, we develop a multiple-generative agent system to simulate\ncommunity decision-making for the redevelopment of Kendall Square's Volpe\nbuilding. Drawing on interviews with local stakeholders, our simulations\nincorporated varying degrees of communication, demographic data, and life\nvalues in the agent prompts. The results revealed that communication among\nagents improved collective reasoning, while the inclusion of demographic and\nlife values led to more distinct opinions. These findings highlight the\npotential application of AI in understanding complex social interactions and\ndecision-making processes, offering valuable insights for urban planning and\ncommunity engagement in diverse settings like Kendall Square.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.11314v1",
    "published_date": "2024-02-17 15:52:16 UTC",
    "updated_date": "2024-02-17 15:52:16 UTC"
  },
  {
    "arxiv_id": "2403.00780v1",
    "title": "Empirical and Experimental Insights into Data Mining Techniques for Crime Prediction: A Comprehensive Survey",
    "authors": [
      "Kamal Taha"
    ],
    "abstract": "This survey paper presents a comprehensive analysis of crime prediction\nmethodologies, exploring the various techniques and technologies utilized in\nthis area. The paper covers the statistical methods, machine learning\nalgorithms, and deep learning techniques employed to analyze crime data, while\nalso examining their effectiveness and limitations. We propose a methodological\ntaxonomy that classifies crime prediction algorithms into specific techniques.\nThis taxonomy is structured into four tiers, including methodology category,\nmethodology sub-category, methodology techniques, and methodology\nsub-techniques. Empirical and experimental evaluations are provided to rank the\ndifferent techniques. The empirical evaluation assesses the crime prediction\ntechniques based on four criteria, while the experimental evaluation ranks the\nalgorithms that employ the same sub-technique, the different sub-techniques\nthat employ the same technique, the different techniques that employ the same\nmethodology sub-category, the different methodology sub-categories within the\nsame category, and the different methodology categories. The combination of\nmethodological taxonomy, empirical evaluations, and experimental comparisons\nallows for a nuanced and comprehensive understanding of crime prediction\nalgorithms, aiding researchers in making informed decisions. Finally, the paper\nprovides a glimpse into the future of crime prediction techniques, highlighting\npotential advancements and opportunities for further research in this field",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.00780v1",
    "published_date": "2024-02-17 15:00:45 UTC",
    "updated_date": "2024-02-17 15:00:45 UTC"
  },
  {
    "arxiv_id": "2402.11296v1",
    "title": "Dissecting Human and LLM Preferences",
    "authors": [
      "Junlong Li",
      "Fan Zhou",
      "Shichao Sun",
      "Yikai Zhang",
      "Hai Zhao",
      "Pengfei Liu"
    ],
    "abstract": "As a relative quality comparison of model responses, human and Large Language\nModel (LLM) preferences serve as common alignment goals in model fine-tuning\nand criteria in evaluation. Yet, these preferences merely reflect broad\ntendencies, resulting in less explainable and controllable models with\npotential safety risks. In this work, we dissect the preferences of human and\n32 different LLMs to understand their quantitative composition, using\nannotations from real-world user-model conversations for a fine-grained,\nscenario-wise analysis. We find that humans are less sensitive to errors, favor\nresponses that support their stances, and show clear dislike when models admit\ntheir limits. On the contrary, advanced LLMs like GPT-4-Turbo emphasize\ncorrectness, clarity, and harmlessness more. Additionally, LLMs of similar\nsizes tend to exhibit similar preferences, regardless of their training\nmethods, and fine-tuning for alignment does not significantly alter the\npreferences of pretrained-only LLMs. Finally, we show that preference-based\nevaluation can be intentionally manipulated. In both training-free and\ntraining-based settings, aligning a model with the preferences of judges boosts\nscores, while injecting the least preferred properties lowers them. This\nresults in notable score shifts: up to 0.59 on MT-Bench (1-10 scale) and 31.94\non AlpacaEval 2.0 (0-100 scale), highlighting the significant impact of this\nstrategic adaptation. Interactive Demo:\nhttps://huggingface.co/spaces/GAIR/Preference-Dissection-Visualization Dataset:\nhttps://huggingface.co/datasets/GAIR/preference-dissection Code:\nhttps://github.com/GAIR-NLP/Preference-Dissection",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.11296v1",
    "published_date": "2024-02-17 14:34:31 UTC",
    "updated_date": "2024-02-17 14:34:31 UTC"
  },
  {
    "arxiv_id": "2402.11291v3",
    "title": "Puzzle Solving using Reasoning of Large Language Models: A Survey",
    "authors": [
      "Panagiotis Giadikiaroglou",
      "Maria Lymperaiou",
      "Giorgos Filandrianos",
      "Giorgos Stamou"
    ],
    "abstract": "Exploring the capabilities of Large Language Models (LLMs) in puzzle solving\nunveils critical insights into their potential and challenges in AI, marking a\nsignificant step towards understanding their applicability in complex reasoning\ntasks. This survey leverages a unique taxonomy -- dividing puzzles into\nrule-based and rule-less categories -- to critically assess LLMs through\nvarious methodologies, including prompting techniques, neuro-symbolic\napproaches, and fine-tuning. Through a critical review of relevant datasets and\nbenchmarks, we assess LLMs' performance, identifying significant challenges in\ncomplex puzzle scenarios. Our findings highlight the disparity between LLM\ncapabilities and human-like reasoning, particularly in those requiring advanced\nlogical inference. The survey underscores the necessity for novel strategies\nand richer datasets to advance LLMs' puzzle-solving proficiency and contribute\nto AI's logical reasoning and creative problem-solving advancements.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.11291v3",
    "published_date": "2024-02-17 14:19:38 UTC",
    "updated_date": "2024-09-14 06:12:36 UTC"
  },
  {
    "arxiv_id": "2402.11285v1",
    "title": "Fair Resource Allocation in Virtualized O-RAN Platforms",
    "authors": [
      "Fatih Aslan",
      "George Iosifidis",
      "Jose A. Ayala-Romero",
      "Andres Garcia-Saavedra",
      "Xavier Costa-Perez"
    ],
    "abstract": "O-RAN systems and their deployment in virtualized general-purpose computing\nplatforms (O-Cloud) constitute a paradigm shift expected to bring unprecedented\nperformance gains. However, these architectures raise new implementation\nchallenges and threaten to worsen the already-high energy consumption of mobile\nnetworks. This paper presents first a series of experiments which assess the\nO-Cloud's energy costs and their dependency on the servers' hardware, capacity\nand data traffic properties which, typically, change over time. Next, it\nproposes a compute policy for assigning the base station data loads to O-Cloud\nservers in an energy-efficient fashion; and a radio policy that determines at\nnear-real-time the minimum transmission block size for each user so as to avoid\nunnecessary energy costs. The policies balance energy savings with performance,\nand ensure that both of them are dispersed fairly across the servers and users,\nrespectively. To cater for the unknown and time-varying parameters affecting\nthe policies, we develop a novel online learning framework with fairness\nguarantees that apply to the entire operation horizon of the system (long-term\nfairness). The policies are evaluated using trace-driven simulations and are\nfully implemented in an O-RAN compatible system where we measure the energy\ncosts and throughput in realistic scenarios.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NI",
    "comment": "to appear in ACM Sigmetrics 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.11285v1",
    "published_date": "2024-02-17 13:57:20 UTC",
    "updated_date": "2024-02-17 13:57:20 UTC"
  },
  {
    "arxiv_id": "2402.11279v1",
    "title": "Multi-Perspective Consistency Enhances Confidence Estimation in Large Language Models",
    "authors": [
      "Pei Wang",
      "Yejie Wang",
      "Muxi Diao",
      "Keqing He",
      "Guanting Dong",
      "Weiran Xu"
    ],
    "abstract": "In the deployment of large language models (LLMs), accurate confidence\nestimation is critical for assessing the credibility of model predictions.\nHowever, existing methods often fail to overcome the issue of overconfidence on\nincorrect answers. In this work, we focus on improving the confidence\nestimation of large language models. Considering the fragility of\nself-awareness in language models, we introduce a Multi-Perspective Consistency\n(MPC) method. We leverage complementary insights from different perspectives\nwithin models (MPC-Internal) and across different models (MPC-Across) to\nmitigate the issue of overconfidence arising from a singular viewpoint. The\nexperimental results on eight publicly available datasets show that our MPC\nachieves state-of-the-art performance. Further analyses indicate that MPC can\nmitigate the problem of overconfidence and is effectively scalable to other\nmodels.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.11279v1",
    "published_date": "2024-02-17 13:37:39 UTC",
    "updated_date": "2024-02-17 13:37:39 UTC"
  },
  {
    "arxiv_id": "2402.11273v1",
    "title": "Semi-supervised Medical Image Segmentation Method Based on Cross-pseudo Labeling Leveraging Strong and Weak Data Augmentation Strategies",
    "authors": [
      "Yifei Chen",
      "Chenyan Zhang",
      "Yifan Ke",
      "Yiyu Huang",
      "Xuezhou Dai",
      "Feiwei Qin",
      "Yongquan Zhang",
      "Xiaodong Zhang",
      "Changmiao Wang"
    ],
    "abstract": "Traditional supervised learning methods have historically encountered certain\nconstraints in medical image segmentation due to the challenging collection\nprocess, high labeling cost, low signal-to-noise ratio, and complex features\ncharacterizing biomedical images. This paper proposes a semi-supervised model,\nDFCPS, which innovatively incorporates the Fixmatch concept. This significantly\nenhances the model's performance and generalizability through data augmentation\nprocessing, employing varied strategies for unlabeled data. Concurrently, the\nmodel design gives appropriate emphasis to the generation, filtration, and\nrefinement processes of pseudo-labels. The novel concept of\ncross-pseudo-supervision is introduced, integrating consistency learning with\nself-training. This enables the model to fully leverage pseudo-labels from\nmultiple perspectives, thereby enhancing training diversity. The DFCPS model is\ncompared with both baseline and advanced models using the publicly accessible\nKvasir-SEG dataset. Across all four subdivisions containing different\nproportions of unlabeled data, our model consistently exhibits superior\nperformance. Our source code is available at\nhttps://github.com/JustlfC03/DFCPS.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "5 pages, 2 figures, accept ISBI2024",
    "pdf_url": "http://arxiv.org/pdf/2402.11273v1",
    "published_date": "2024-02-17 13:07:44 UTC",
    "updated_date": "2024-02-17 13:07:44 UTC"
  },
  {
    "arxiv_id": "2402.11260v1",
    "title": "MoRAL: MoE Augmented LoRA for LLMs' Lifelong Learning",
    "authors": [
      "Shu Yang",
      "Muhammad Asif Ali",
      "Cheng-Long Wang",
      "Lijie Hu",
      "Di Wang"
    ],
    "abstract": "Adapting large language models (LLMs) to new domains/tasks and enabling them\nto be efficient lifelong learners is a pivotal challenge. In this paper, we\npropose MoRAL, i.e., Mixture-of-Experts augmented Low-Rank Adaptation for\nLifelong Learning. MoRAL combines the multi-tasking abilities of MoE with the\nfine-tuning abilities of LoRA for effective life-long learning of LLMs. In\ncontrast to the conventional approaches that use factual triplets as inputs\nMoRAL relies on simple question-answer pairs, which is a more practical and\neffective strategy for robust and efficient learning. Owing to new data\nsettings, we introduce a new evaluation benchmark namely: Life Long Learning of\nLLM (5L-bench) encompassing a newly curated dataset of question-answer pairs,\nand a set of evaluation metrics for rigorous evaluation of MoRAL in open-book\nand closed-book settings. Experimental evaluation shows (i) LLMs learn fast in\nopen-book settings with up to 30.15% improvement in \"RA\" for Phi-2-2.7B\ncompared to closed-book (for models fine-tuned with MoRAL); (ii) MoRAL shows\nhigher performance improvement for models with a greater number of parameters;\n(iii) MoRAL is robust to catastrophic forgetting offering better knowledge\nretention compared to baselines.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.11260v1",
    "published_date": "2024-02-17 12:25:31 UTC",
    "updated_date": "2024-02-17 12:25:31 UTC"
  },
  {
    "arxiv_id": "2402.11253v3",
    "title": "Aligning Large Language Models by On-Policy Self-Judgment",
    "authors": [
      "Sangkyu Lee",
      "Sungdong Kim",
      "Ashkan Yousefpour",
      "Minjoon Seo",
      "Kang Min Yoo",
      "Youngjae Yu"
    ],
    "abstract": "Existing approaches for aligning large language models with human preferences\nface a trade-off that requires a separate reward model (RM) for on-policy\nlearning. In this paper, we present a novel alignment framework, SELF-JUDGE\nthat (1) does on-policy learning and 2) is parameter efficient, as it does not\nrequire an additional RM for evaluating the samples for on-policy learning. To\nthis end, we propose Judge-augmented Supervised Fine-Tuning (JSFT) to train a\nsingle model to act as both a policy and a judge. Specifically, we view the\npairwise judgment task, choosing the better response from a response pair, as a\nspecial case of the instruction-following task. The resulting model can judge\npreferences of on-the-fly responses from current policy initialized from\nitself. Experimental results show the efficacy of SELF-JUDGE, outperforming\nbaselines in preference benchmarks. We also show that the rejecting sampling by\nitself can improve performance further without an additional evaluator.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Published as a main conference paper at ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.11253v3",
    "published_date": "2024-02-17 11:25:26 UTC",
    "updated_date": "2024-06-25 13:39:52 UTC"
  },
  {
    "arxiv_id": "2402.11243v1",
    "title": "Can Large Language Models perform Relation-based Argument Mining?",
    "authors": [
      "Deniz Gorur",
      "Antonio Rago",
      "Francesca Toni"
    ],
    "abstract": "Argument mining (AM) is the process of automatically extracting arguments,\ntheir components and/or relations amongst arguments and components from text.\nAs the number of platforms supporting online debate increases, the need for AM\nbecomes ever more urgent, especially in support of downstream tasks.\nRelation-based AM (RbAM) is a form of AM focusing on identifying agreement\n(support) and disagreement (attack) relations amongst arguments. RbAM is a\nchallenging classification task, with existing methods failing to perform\nsatisfactorily. In this paper, we show that general-purpose Large Language\nModels (LLMs), appropriately primed and prompted, can significantly outperform\nthe best performing (RoBERTa-based) baseline. Specifically, we experiment with\ntwo open-source LLMs (Llama-2 and Mistral) with ten datasets.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 9 figures, submitted to ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.11243v1",
    "published_date": "2024-02-17 10:37:51 UTC",
    "updated_date": "2024-02-17 10:37:51 UTC"
  },
  {
    "arxiv_id": "2402.11242v1",
    "title": "Learning with Imbalanced Noisy Data by Preventing Bias in Sample Selection",
    "authors": [
      "Huafeng Liu",
      "Mengmeng Sheng",
      "Zeren Sun",
      "Yazhou Yao",
      "Xian-Sheng Hua",
      "Heng-Tao Shen"
    ],
    "abstract": "Learning with noisy labels has gained increasing attention because the\ninevitable imperfect labels in real-world scenarios can substantially hurt the\ndeep model performance. Recent studies tend to regard low-loss samples as clean\nones and discard high-loss ones to alleviate the negative impact of noisy\nlabels. However, real-world datasets contain not only noisy labels but also\nclass imbalance. The imbalance issue is prone to causing failure in the\nloss-based sample selection since the under-learning of tail classes also leans\nto produce high losses. To this end, we propose a simple yet effective method\nto address noisy labels in imbalanced datasets. Specifically, we propose\nClass-Balance-based sample Selection (CBS) to prevent the tail class samples\nfrom being neglected during training. We propose Confidence-based Sample\nAugmentation (CSA) for the chosen clean samples to enhance their reliability in\nthe training process. To exploit selected noisy samples, we resort to\nprediction history to rectify labels of noisy samples. Moreover, we introduce\nthe Average Confidence Margin (ACM) metric to measure the quality of corrected\nlabels by leveraging the model's evolving training dynamics, thereby ensuring\nthat low-quality corrected noisy samples are appropriately masked out. Lastly,\nconsistency regularization is imposed on filtered label-corrected noisy samples\nto boost model performance. Comprehensive experimental results on synthetic and\nreal-world datasets demonstrate the effectiveness and superiority of our\nproposed method, especially in imbalanced scenarios. Comprehensive experimental\nresults on synthetic and real-world datasets demonstrate the effectiveness and\nsuperiority of our proposed method, especially in imbalanced scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "accepted by IEEE Transactions on Multimedia",
    "pdf_url": "http://arxiv.org/pdf/2402.11242v1",
    "published_date": "2024-02-17 10:34:53 UTC",
    "updated_date": "2024-02-17 10:34:53 UTC"
  },
  {
    "arxiv_id": "2402.11241v1",
    "title": "DiffPoint: Single and Multi-view Point Cloud Reconstruction with ViT Based Diffusion Model",
    "authors": [
      "Yu Feng",
      "Xing Shi",
      "Mengli Cheng",
      "Yun Xiong"
    ],
    "abstract": "As the task of 2D-to-3D reconstruction has gained significant attention in\nvarious real-world scenarios, it becomes crucial to be able to generate\nhigh-quality point clouds. Despite the recent success of deep learning models\nin generating point clouds, there are still challenges in producing\nhigh-fidelity results due to the disparities between images and point clouds.\nWhile vision transformers (ViT) and diffusion models have shown promise in\nvarious vision tasks, their benefits for reconstructing point clouds from\nimages have not been demonstrated yet. In this paper, we first propose a neat\nand powerful architecture called DiffPoint that combines ViT and diffusion\nmodels for the task of point cloud reconstruction. At each diffusion step, we\ndivide the noisy point clouds into irregular patches. Then, using a standard\nViT backbone that treats all inputs as tokens (including time information,\nimage embeddings, and noisy patches), we train our model to predict target\npoints based on input images. We evaluate DiffPoint on both single-view and\nmulti-view reconstruction tasks and achieve state-of-the-art results.\nAdditionally, we introduce a unified and flexible feature fusion module for\naggregating image features from single or multiple input images. Furthermore,\nour work demonstrates the feasibility of applying unified architectures across\nlanguages and images to improve 3D reconstruction tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.11241v1",
    "published_date": "2024-02-17 10:18:40 UTC",
    "updated_date": "2024-02-17 10:18:40 UTC"
  },
  {
    "arxiv_id": "2402.13276v2",
    "title": "When LLMs Meets Acoustic Landmarks: An Efficient Approach to Integrate Speech into Large Language Models for Depression Detection",
    "authors": [
      "Xiangyu Zhang",
      "Hexin Liu",
      "Kaishuai Xu",
      "Qiquan Zhang",
      "Daijiao Liu",
      "Beena Ahmed",
      "Julien Epps"
    ],
    "abstract": "Depression is a critical concern in global mental health, prompting extensive\nresearch into AI-based detection methods. Among various AI technologies, Large\nLanguage Models (LLMs) stand out for their versatility in mental healthcare\napplications. However, their primary limitation arises from their exclusive\ndependence on textual input, which constrains their overall capabilities.\nFurthermore, the utilization of LLMs in identifying and analyzing depressive\nstates is still relatively untapped. In this paper, we present an innovative\napproach to integrating acoustic speech information into the LLMs framework for\nmultimodal depression detection. We investigate an efficient method for\ndepression detection by integrating speech signals into LLMs utilizing Acoustic\nLandmarks. By incorporating acoustic landmarks, which are specific to the\npronunciation of spoken words, our method adds critical dimensions to text\ntranscripts. This integration also provides insights into the unique speech\npatterns of individuals, revealing the potential mental states of individuals.\nEvaluations of the proposed approach on the DAIC-WOZ dataset reveal\nstate-of-the-art results when compared with existing Audio-Text baselines. In\naddition, this approach is not only valuable for the detection of depression\nbut also represents a new perspective in enhancing the ability of LLMs to\ncomprehend and process speech signals.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.13276v2",
    "published_date": "2024-02-17 09:39:46 UTC",
    "updated_date": "2024-09-23 22:54:05 UTC"
  },
  {
    "arxiv_id": "2402.13275v1",
    "title": "Implementation of a Model of the Cortex Basal Ganglia Loop",
    "authors": [
      "Naoya Arakawa"
    ],
    "abstract": "This article presents a simple model of the cortex-basal ganglia-thalamus\nloop, which is thought to serve for action selection and executions, and\nreports the results of its implementation. The model is based on the hypothesis\nthat the cerebral cortex predicts actions, while the basal ganglia use\nreinforcement learning to decide whether to perform the actions predicted by\nthe cortex. The implementation is intended to be used as a component of models\nof the brain consisting of cortical regions or brain-inspired cognitive\narchitectures.",
    "categories": [
      "q-bio.NC",
      "cs.AI"
    ],
    "primary_category": "q-bio.NC",
    "comment": "7 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.13275v1",
    "published_date": "2024-02-17 08:08:36 UTC",
    "updated_date": "2024-02-17 08:08:36 UTC"
  },
  {
    "arxiv_id": "2402.11208v2",
    "title": "Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents",
    "authors": [
      "Wenkai Yang",
      "Xiaohan Bi",
      "Yankai Lin",
      "Sishuo Chen",
      "Jie Zhou",
      "Xu Sun"
    ],
    "abstract": "Driven by the rapid development of Large Language Models (LLMs), LLM-based\nagents have been developed to handle various real-world applications, including\nfinance, healthcare, and shopping, etc. It is crucial to ensure the reliability\nand security of LLM-based agents during applications. However, the safety\nissues of LLM-based agents are currently under-explored. In this work, we take\nthe first step to investigate one of the typical safety threats, backdoor\nattack, to LLM-based agents. We first formulate a general framework of agent\nbackdoor attacks, then we present a thorough analysis of different forms of\nagent backdoor attacks. Specifically, compared with traditional backdoor\nattacks on LLMs that are only able to manipulate the user inputs and model\noutputs, agent backdoor attacks exhibit more diverse and covert forms: (1) From\nthe perspective of the final attacking outcomes, the agent backdoor attacker\ncan not only choose to manipulate the final output distribution, but also\nintroduce the malicious behavior in an intermediate reasoning step only, while\nkeeping the final output correct. (2) Furthermore, the former category can be\ndivided into two subcategories based on trigger locations, in which the\nbackdoor trigger can either be hidden in the user query or appear in an\nintermediate observation returned by the external environment. We implement the\nabove variations of agent backdoor attacks on two typical agent tasks including\nweb shopping and tool utilization. Extensive experiments show that LLM-based\nagents suffer severely from backdoor attacks and such backdoor vulnerability\ncannot be easily mitigated by current textual backdoor defense algorithms. This\nindicates an urgent need for further research on the development of targeted\ndefenses against backdoor attacks on LLM-based agents. Warning: This paper may\ncontain biased content.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted at NeurIPS 2024, camera ready version. Code and data are\n  available at https://github.com/lancopku/agent-backdoor-attacks",
    "pdf_url": "http://arxiv.org/pdf/2402.11208v2",
    "published_date": "2024-02-17 06:48:45 UTC",
    "updated_date": "2024-10-29 15:32:04 UTC"
  },
  {
    "arxiv_id": "2402.12399v2",
    "title": "Turn Waste into Worth: Rectifying Top-$k$ Router of MoE",
    "authors": [
      "Zhiyuan Zeng",
      "Qipeng Guo",
      "Zhaoye Fei",
      "Zhangyue Yin",
      "Yunhua Zhou",
      "Linyang Li",
      "Tianxiang Sun",
      "Hang Yan",
      "Dahua Lin",
      "Xipeng Qiu"
    ],
    "abstract": "Sparse Mixture of Experts (MoE) models are popular for training large\nlanguage models due to their computational efficiency. However, the commonly\nused top-$k$ routing mechanism suffers from redundancy computation and memory\ncosts due to the unbalanced routing. Some experts are overflow, where the\nexceeding tokens are dropped. While some experts are vacant, which are padded\nwith zeros, negatively impacting model performance. To address the dropped\ntokens and padding, we propose the Rectify-Router, comprising the Intra-GPU\nRectification and the Fill-in Rectification. The Intra-GPU Rectification\nhandles dropped tokens, efficiently routing them to experts within the GPU\nwhere they are located to avoid inter-GPU communication. The Fill-in\nRectification addresses padding by replacing padding tokens with the tokens\nthat have high routing scores. Our experimental results demonstrate that the\nIntra-GPU Rectification and the Fill-in Rectification effectively handle\ndropped tokens and padding, respectively. Furthermore, the combination of them\nachieves superior performance, surpassing the accuracy of the vanilla top-1\nrouter by 4.7%.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.12399v2",
    "published_date": "2024-02-17 06:23:27 UTC",
    "updated_date": "2024-02-21 13:33:12 UTC"
  },
  {
    "arxiv_id": "2402.11203v1",
    "title": "Exploring ChatGPT for Next-generation Information Retrieval: Opportunities and Challenges",
    "authors": [
      "Yizheng Huang",
      "Jimmy Huang"
    ],
    "abstract": "The rapid advancement of artificial intelligence (AI) has highlighted ChatGPT\nas a pivotal technology in the field of information retrieval (IR).\nDistinguished from its predecessors, ChatGPT offers significant benefits that\nhave attracted the attention of both the industry and academic communities.\nWhile some view ChatGPT as a groundbreaking innovation, others attribute its\nsuccess to the effective integration of product development and market\nstrategies. The emergence of ChatGPT, alongside GPT-4, marks a new phase in\nGenerative AI, generating content that is distinct from training examples and\nexceeding the capabilities of the prior GPT-3 model by OpenAI. Unlike the\ntraditional supervised learning approach in IR tasks, ChatGPT challenges\nexisting paradigms, bringing forth new challenges and opportunities regarding\ntext quality assurance, model bias, and efficiency. This paper seeks to examine\nthe impact of ChatGPT on IR tasks and offer insights into its potential future\ndevelopments.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "Survey Paper",
    "pdf_url": "http://arxiv.org/pdf/2402.11203v1",
    "published_date": "2024-02-17 05:44:40 UTC",
    "updated_date": "2024-02-17 05:44:40 UTC"
  },
  {
    "arxiv_id": "2402.11196v2",
    "title": "Maintaining Adversarial Robustness in Continuous Learning",
    "authors": [
      "Xiaolei Ru",
      "Xiaowei Cao",
      "Zijia Liu",
      "Jack Murdoch Moore",
      "Xin-Ya Zhang",
      "Xia Zhu",
      "Wenjia Wei",
      "Gang Yan"
    ],
    "abstract": "Adversarial robustness is essential for security and reliability of machine\nlearning systems. However, adversarial robustness enhanced by defense\nalgorithms is easily erased as the neural network's weights update to learn new\ntasks. To address this vulnerability, it is essential to improve the capability\nof neural networks in terms of robust continual learning. Specially, we propose\na novel gradient projection technique that effectively stabilizes sample\ngradients from previous data by orthogonally projecting back-propagation\ngradients onto a crucial subspace before using them for weight updates. This\ntechnique can maintaining robustness by collaborating with a class of defense\nalgorithms through sample gradient smoothing. The experimental results on four\nbenchmarks including Split-CIFAR100 and Split-miniImageNet, demonstrate that\nthe superiority of the proposed approach in mitigating rapidly degradation of\nrobustness during continual learning even when facing strong adversarial\nattacks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.11196v2",
    "published_date": "2024-02-17 05:14:47 UTC",
    "updated_date": "2024-08-13 15:28:25 UTC"
  },
  {
    "arxiv_id": "2402.11192v4",
    "title": "I Learn Better If You Speak My Language: Understanding the Superior Performance of Fine-Tuning Large Language Models with LLM-Generated Responses",
    "authors": [
      "Xuan Ren",
      "Biao Wu",
      "Lingqiao Liu"
    ],
    "abstract": "This paper explores an intriguing observation: fine-tuning a large language\nmodel (LLM) with responses generated by a LLM often yields better results than\nusing responses generated by humans, particularly in reasoning tasks. We\nconduct an in-depth investigation to understand why this occurs. Contrary to\nthe common belief that these instances is due to the more detailed nature of\nLLM-generated content, our study identifies another contributing factor: an LLM\nis inherently more \"familiar\" with LLM generated responses. This familiarity is\nevidenced by lower perplexity before fine-tuning. We design a series of\nexperiments to understand the impact of the \"familiarity\" and our conclusion\nreveals that this \"familiarity\" significantly impacts learning performance.\nTraining with LLM-generated responses not only enhances performance but also\nhelps maintain the model's capabilities in other reasoning tasks after\nfine-tuning on a specific task.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "The paper has been accepted to EMNLP 2024 (Main Conference)",
    "pdf_url": "http://arxiv.org/pdf/2402.11192v4",
    "published_date": "2024-02-17 05:05:31 UTC",
    "updated_date": "2024-10-11 03:50:54 UTC"
  },
  {
    "arxiv_id": "2402.11187v2",
    "title": "LaCo: Large Language Model Pruning via Layer Collapse",
    "authors": [
      "Yifei Yang",
      "Zouying Cao",
      "Hai Zhao"
    ],
    "abstract": "Large language models (LLMs) based on transformer are witnessing a notable\ntrend of size expansion, which brings considerable costs to both model training\nand inference. However, existing methods such as model quantization, knowledge\ndistillation, and model pruning are constrained by various issues, including\nhardware support limitations, the need for extensive training, and alterations\nto the model internal structure. In this paper, we propose a concise layer-wise\nstructured pruner called \\textit{Layer Collapse (LaCo)}, in which rear model\nlayers collapse into a prior layer, enabling a rapid reduction in model size\nwhile preserving the model structure. Comprehensive experiments show that our\nmethod maintains an average task performance of over 80\\% at pruning ratios of\n25-30\\%, significantly outperforming existing state-of-the-art structured\npruning methods. We also conduct post-training experiments to confirm that the\n\\textit{LaCo} effectively inherits the parameters of the original model.\nAdditionally, we perform ablation studies on various settings of \\textit{LaCo}.\nFinally, we discuss our motivation from the perspective of layer-wise\nsimilarity and evaluate the performance of the pruned LLMs across various\npruning ratios\\footnote{\\url{https://github.com/yangyifei729/LaCo}}.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted as Findings of EMNLP2024",
    "pdf_url": "http://arxiv.org/pdf/2402.11187v2",
    "published_date": "2024-02-17 04:16:30 UTC",
    "updated_date": "2024-10-15 01:58:58 UTC"
  },
  {
    "arxiv_id": "2402.11176v3",
    "title": "KnowTuning: Knowledge-aware Fine-tuning for Large Language Models",
    "authors": [
      "Yougang Lyu",
      "Lingyong Yan",
      "Shuaiqiang Wang",
      "Haibo Shi",
      "Dawei Yin",
      "Pengjie Ren",
      "Zhumin Chen",
      "Maarten de Rijke",
      "Zhaochun Ren"
    ],
    "abstract": "Despite their success at many natural language processing (NLP) tasks, large\nlanguage models still struggle to effectively leverage knowledge for\nknowledge-intensive tasks, manifesting limitations such as generating\nincomplete, non-factual, or illogical answers. These limitations stem from\ninadequate knowledge awareness of LLMs during vanilla fine-tuning. To address\nthese problems, we propose a knowledge-aware fine-tuning (KnowTuning) method to\nimprove fine-grained and coarse-grained knowledge awareness of LLMs. We devise\na fine-grained knowledge augmentation stage to train LLMs to identify difficult\nfine-grained knowledge in answers. We also propose a coarse-grained knowledge\ncomparison stage to train LLMs to distinguish between reliable and unreliable\nknowledge, in three aspects: completeness, factuality, and logicality.\nExtensive experiments on both generic and medical question answering (QA)\ndatasets confirm the effectiveness of KnowTuning, through automatic and human\nevaluations, across various sizes of LLMs. We further verify that KnowTuning\ngenerates more facts with less factual error rate under fine-grained facts\nevaluation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2024 main paper",
    "pdf_url": "http://arxiv.org/pdf/2402.11176v3",
    "published_date": "2024-02-17 02:54:32 UTC",
    "updated_date": "2024-10-02 14:20:29 UTC"
  },
  {
    "arxiv_id": "2402.11168v3",
    "title": "Trust Regions for Explanations via Black-Box Probabilistic Certification",
    "authors": [
      "Amit Dhurandhar",
      "Swagatam Haldar",
      "Dennis Wei",
      "Karthikeyan Natesan Ramamurthy"
    ],
    "abstract": "Given the black box nature of machine learning models, a plethora of\nexplainability methods have been developed to decipher the factors behind\nindividual decisions. In this paper, we introduce a novel problem of black box\n(probabilistic) explanation certification. We ask the question: Given a black\nbox model with only query access, an explanation for an example and a quality\nmetric (viz. fidelity, stability), can we find the largest hypercube (i.e.,\n$\\ell_{\\infty}$ ball) centered at the example such that when the explanation is\napplied to all examples within the hypercube, (with high probability) a quality\ncriterion is met (viz. fidelity greater than some value)? Being able to\nefficiently find such a \\emph{trust region} has multiple benefits: i) insight\ninto model behavior in a \\emph{region}, with a \\emph{guarantee}; ii)\nascertained \\emph{stability} of the explanation; iii) \\emph{explanation reuse},\nwhich can save time, energy and money by not having to find explanations for\nevery example; and iv) a possible \\emph{meta-metric} to compare explanation\nmethods. Our contributions include formalizing this problem, proposing\nsolutions, providing theoretical guarantees for these solutions that are\ncomputable, and experimentally showing their efficacy on synthetic and real\ndata.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.11168v3",
    "published_date": "2024-02-17 02:26:14 UTC",
    "updated_date": "2024-06-05 16:36:21 UTC"
  },
  {
    "arxiv_id": "2402.11167v2",
    "title": "ToBlend: Token-Level Blending With an Ensemble of LLMs to Attack AI-Generated Text Detection",
    "authors": [
      "Fan Huang",
      "Haewoon Kwak",
      "Jisun An"
    ],
    "abstract": "The robustness of AI-content detection models against sophisticated\nadversarial strategies, such as paraphrasing or word switching, is a rising\nconcern in natural language generation (NLG) applications. This study proposes\nToBlend, a novel token-level ensemble text generation method to challenge the\nrobustness of current AI-content detection approaches by utilizing multiple\nsets of candidate generative large language models (LLMs). By randomly sampling\ntoken(s) from candidate LLMs sets, we find ToBlend significantly drops the\nperformance of most mainstream AI-content detection methods. We evaluate the\ntext quality produced under different ToBlend settings based on annotations\nfrom experienced human experts. We proposed a fine-tuned Llama3.1 model to\ndistinguish the ToBlend generated text more accurately. Our findings underscore\nour proposed text generation approach's great potential in deceiving and\nimproving detection models. Our datasets, codes, and annotations are\nopen-sourced.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Submitted to ARR Oct-2024 Cycle",
    "pdf_url": "http://arxiv.org/pdf/2402.11167v2",
    "published_date": "2024-02-17 02:25:57 UTC",
    "updated_date": "2024-10-16 15:40:51 UTC"
  },
  {
    "arxiv_id": "2402.11161v5",
    "title": "PEDANTS: Cheap but Effective and Interpretable Answer Equivalence",
    "authors": [
      "Zongxia Li",
      "Ishani Mondal",
      "Yijun Liang",
      "Huy Nghiem",
      "Jordan Lee Boyd-Graber"
    ],
    "abstract": "Question answering (QA) can only make progress if we know if an answer is\ncorrect, but current answer correctness (AC) metrics struggle with verbose,\nfree-form answers from large language models (LLMs). There are two challenges\nwith current short-form QA evaluations: a lack of diverse styles of evaluation\ndata and an over-reliance on expensive and slow LLMs. LLM-based scorers\ncorrelate better with humans, but this expensive task has only been tested on\nlimited QA datasets. We rectify these issues by providing rubrics and datasets\nfor evaluating machine QA adopted from the Trivia community. We also propose an\nefficient, and interpretable QA evaluation that is more stable than an exact\nmatch and neural methods(BERTScore).",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Efficient PEDANTS Classifier for short-form QA in github:\n  https://github.com/zli12321/qa_metrics. arXiv admin note: text overlap with\n  arXiv:2401.13170",
    "pdf_url": "http://arxiv.org/pdf/2402.11161v5",
    "published_date": "2024-02-17 01:56:19 UTC",
    "updated_date": "2024-10-11 20:56:36 UTC"
  },
  {
    "arxiv_id": "2402.11140v2",
    "title": "Boosting of Thoughts: Trial-and-Error Problem Solving with Large Language Models",
    "authors": [
      "Sijia Chen",
      "Baochun Li",
      "Di Niu"
    ],
    "abstract": "The reasoning performance of Large Language Models (LLMs) on a wide range of\nproblems critically relies on chain-of-thought prompting, which involves\nproviding a few chain of thought demonstrations as exemplars in prompts. Recent\nwork, e.g., Tree of Thoughts, has pointed out the importance of exploration and\nself-evaluation in reasoning step selection for complex problem solving. In\nthis paper, we present Boosting of Thoughts (BoT), an automated prompting\nframework for problem solving with LLMs by iteratively exploring and\nself-evaluating many trees of thoughts in order to acquire an ensemble of\ntrial-and-error reasoning experiences, which will serve as a new form of\nprompting to solve the complex problem. Starting from a simple prompt without\nrequiring examples, BoT iteratively explores and evaluates a large collection\nof reasoning steps, and more importantly, uses error analysis obtained from the\nLLM on them to explicitly revise prompting, which in turn enhances reasoning\nstep generation, until a final answer is attained. Our experiments with GPT-4\nand Llama2 across extensive complex mathematical problems demonstrate that BoT\nconsistently achieves higher or comparable problem-solving rates than other\nadvanced prompting approaches.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted as a poster paper by ICLR2024. 27 pages, 5 figures, 18\n  tables. [Source\n  Code](https://github.com/iQua/llmpebase/tree/main/examples/BoTReasoning)",
    "pdf_url": "http://arxiv.org/pdf/2402.11140v2",
    "published_date": "2024-02-17 00:13:36 UTC",
    "updated_date": "2025-01-06 21:18:24 UTC"
  },
  {
    "arxiv_id": "2402.11139v1",
    "title": "LiGNN: Graph Neural Networks at LinkedIn",
    "authors": [
      "Fedor Borisyuk",
      "Shihai He",
      "Yunbo Ouyang",
      "Morteza Ramezani",
      "Peng Du",
      "Xiaochen Hou",
      "Chengming Jiang",
      "Nitin Pasumarthy",
      "Priya Bannur",
      "Birjodh Tiwana",
      "Ping Liu",
      "Siddharth Dangi",
      "Daqi Sun",
      "Zhoutao Pei",
      "Xiao Shi",
      "Sirou Zhu",
      "Qianqi Shen",
      "Kuang-Hsuan Lee",
      "David Stein",
      "Baolei Li",
      "Haichao Wei",
      "Amol Ghoting",
      "Souvik Ghosh"
    ],
    "abstract": "In this paper, we present LiGNN, a deployed large-scale Graph Neural Networks\n(GNNs) Framework. We share our insight on developing and deployment of GNNs at\nlarge scale at LinkedIn. We present a set of algorithmic improvements to the\nquality of GNN representation learning including temporal graph architectures\nwith long term losses, effective cold start solutions via graph densification,\nID embeddings and multi-hop neighbor sampling. We explain how we built and sped\nup by 7x our large-scale training on LinkedIn graphs with adaptive sampling of\nneighbors, grouping and slicing of training data batches, specialized\nshared-memory queue and local gradient optimization. We summarize our\ndeployment lessons and learnings gathered from A/B test experiments. The\ntechniques presented in this work have contributed to an approximate relative\nimprovements of 1% of Job application hearing back rate, 2% Ads CTR lift, 0.5%\nof Feed engaged daily active users, 0.2% session lift and 0.1% weekly active\nuser lift from people recommendation. We believe that this work can provide\npractical solutions and insights for engineers who are interested in applying\nGraph neural networks at large scale.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.11139v1",
    "published_date": "2024-02-17 00:10:33 UTC",
    "updated_date": "2024-02-17 00:10:33 UTC"
  },
  {
    "arxiv_id": "2402.11138v2",
    "title": "Contrastive Instruction Tuning",
    "authors": [
      "Tianyi Lorena Yan",
      "Fei Wang",
      "James Y. Huang",
      "Wenxuan Zhou",
      "Fan Yin",
      "Aram Galstyan",
      "Wenpeng Yin",
      "Muhao Chen"
    ],
    "abstract": "Instruction tuning has been used as a promising approach to improve the\nperformance of large language models (LLMs) on unseen tasks. However, current\nLLMs exhibit limited robustness to unseen instructions, generating inconsistent\noutputs when the same instruction is phrased with slightly varied forms or\nlanguage styles. This behavior indicates LLMs' lack of robustness to textual\nvariations and generalizability to unseen instructions, potentially leading to\ntrustworthiness issues. Accordingly, we propose Contrastive Instruction Tuning,\nwhich maximizes the similarity between the hidden representations of\nsemantically equivalent instruction-instance pairs while minimizing the\nsimilarity between semantically different ones. To facilitate this approach, we\naugment the existing FLAN collection by paraphrasing task instructions.\nExperiments on the PromptBench benchmark show that CoIN consistently improves\nLLMs' robustness to unseen instructions with variations across character, word,\nsentence, and semantic levels by an average of +2.5% in accuracy. Code is\navailable at https://github.com/luka-group/CoIN.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL 2024 Findings",
    "pdf_url": "http://arxiv.org/pdf/2402.11138v2",
    "published_date": "2024-02-17 00:09:32 UTC",
    "updated_date": "2024-06-06 06:03:34 UTC"
  }
]
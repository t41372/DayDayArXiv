[
  {
    "arxiv_id": "2505.01948v1",
    "title": "Multi-Scale Graph Learning for Anti-Sparse Downscaling",
    "authors": [
      "Yingda Fan",
      "Runlong Yu",
      "Janet R. Barclay",
      "Alison P. Appling",
      "Yiming Sun",
      "Yiqun Xie",
      "Xiaowei Jia"
    ],
    "abstract": "Water temperature can vary substantially even across short distances within\nthe same sub-watershed. Accurate prediction of stream water temperature at fine\nspatial resolutions (i.e., fine scales, $\\leq$ 1 km) enables precise\ninterventions to maintain water quality and protect aquatic habitats. Although\nspatiotemporal models have made substantial progress in spatially coarse time\nseries modeling, challenges persist in predicting at fine spatial scales due to\nthe lack of data at that scale.To address the problem of insufficient\nfine-scale data, we propose a Multi-Scale Graph Learning (MSGL) method. This\nmethod employs a multi-task learning framework where coarse-scale graph\nlearning, bolstered by larger datasets, simultaneously enhances fine-scale\ngraph learning. Although existing multi-scale or multi-resolution methods\nintegrate data from different spatial scales, they often overlook the spatial\ncorrespondences across graph structures at various scales. To address this, our\nMSGL introduces an additional learning task, cross-scale interpolation\nlearning, which leverages the hydrological connectedness of stream locations\nacross coarse- and fine-scale graphs to establish cross-scale connections,\nthereby enhancing overall model performance. Furthermore, we have broken free\nfrom the mindset that multi-scale learning is limited to synchronous training\nby proposing an Asynchronous Multi-Scale Graph Learning method (ASYNC-MSGL).\nExtensive experiments demonstrate the state-of-the-art performance of our\nmethod for anti-sparse downscaling of daily stream temperatures in the Delaware\nRiver Basin, USA, highlighting its potential utility for water resources\nmonitoring and management.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "68T05, 68U05",
      "I.2.6; I.2.10"
    ],
    "primary_category": "cs.LG",
    "comment": "AAAI-25, Multi-scale deep learning approach for spatial downscaling\n  of geospatial data with sparse observations",
    "pdf_url": "http://arxiv.org/pdf/2505.01948v1",
    "published_date": "2025-05-03 23:52:08 UTC",
    "updated_date": "2025-05-03 23:52:08 UTC"
  },
  {
    "arxiv_id": "2505.01944v1",
    "title": "Explainability by design: an experimental analysis of the legal coding process",
    "authors": [
      "Matteo Cristani",
      "Guido Governatori",
      "Francesco Olivieri",
      "Monica Palmirani",
      "Gabriele Buriola"
    ],
    "abstract": "Behind a set of rules in Deontic Defeasible Logic, there is a mapping process\nof normative background fragments. This process goes from text to rules and\nimplicitly encompasses an explanation of the coded fragments.\n  In this paper we deliver a methodology for \\textit{legal coding} that starts\nwith a fragment and goes onto a set of Deontic Defeasible Logic rules,\ninvolving a set of \\textit{scenarios} to test the correctness of the coded\nfragments. The methodology is illustrated by the coding process of an example\ntext. We then show the results of a series of experiments conducted with humans\nencoding a variety of normative backgrounds and corresponding cases in which we\nhave measured the efforts made in the coding process, as related to some\nmeasurable features. To process these examples, a recently developed\ntechnology, Houdini, that allows reasoning in Deontic Defeasible Logic, has\nbeen employed.\n  Finally we provide a technique to forecast time required in coding, that\ndepends on factors such as knowledge of the legal domain, knowledge of the\ncoding processes, length of the text, and a measure of \\textit{depth} that\nrefers to the length of the paths of legal references.",
    "categories": [
      "cs.LO",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.01944v1",
    "published_date": "2025-05-03 23:18:05 UTC",
    "updated_date": "2025-05-03 23:18:05 UTC"
  },
  {
    "arxiv_id": "2505.01931v1",
    "title": "Semantic Intelligence: Integrating GPT-4 with A Planning in Low-Cost Robotics",
    "authors": [
      "Jesse Barkley",
      "Abraham George",
      "Amir Barati Farimani"
    ],
    "abstract": "Classical robot navigation often relies on hardcoded state machines and\npurely geometric path planners, limiting a robot's ability to interpret\nhigh-level semantic instructions. In this paper, we first assess GPT-4's\nability to act as a path planner compared to the A* algorithm, then present a\nhybrid planning framework that integrates GPT-4's semantic reasoning with A* on\na low-cost robot platform operating on ROS2 Humble. Our approach eliminates\nexplicit finite state machine (FSM) coding by using prompt-based GPT-4\nreasoning to handle task logic while maintaining the accurate paths computed by\nA*. The GPT-4 module provides semantic understanding of instructions and\nenvironmental cues (e.g., recognizing toxic obstacles or crowded areas to\navoid, or understanding low-battery situations requiring alternate route\nselection), and dynamically adjusts the robot's occupancy grid via obstacle\nbuffering to enforce semantic constraints. We demonstrate multi-step reasoning\nfor sequential tasks, such as first navigating to a resource goal and then\nreaching a final destination safely. Experiments on a Petoi Bittle robot with\nan overhead camera and Raspberry Pi Zero 2W compare classical A* against\nGPT-4-assisted planning. Results show that while A* is faster and more accurate\nfor basic route generation and obstacle avoidance, the GPT-4-integrated system\nachieves high success rates (96-100%) on semantic tasks that are infeasible for\npure geometric planners. This work highlights how affordable robots can exhibit\nintelligent, context-aware behaviors by leveraging large language model\nreasoning with minimal hardware and no fine-tuning.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "10 pages, 4 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2505.01931v1",
    "published_date": "2025-05-03 21:49:14 UTC",
    "updated_date": "2025-05-03 21:49:14 UTC"
  },
  {
    "arxiv_id": "2505.03829v1",
    "title": "VideoLLM Benchmarks and Evaluation: A Survey",
    "authors": [
      "Yogesh Kumar"
    ],
    "abstract": "The rapid development of Large Language Models (LLMs) has catalyzed\nsignificant advancements in video understanding technologies. This survey\nprovides a comprehensive analysis of benchmarks and evaluation methodologies\nspecifically designed or used for Video Large Language Models (VideoLLMs). We\nexamine the current landscape of video understanding benchmarks, discussing\ntheir characteristics, evaluation protocols, and limitations. The paper\nanalyzes various evaluation methodologies, including closed-set, open-set, and\nspecialized evaluations for temporal and spatiotemporal understanding tasks. We\nhighlight the performance trends of state-of-the-art VideoLLMs across these\nbenchmarks and identify key challenges in current evaluation frameworks.\nAdditionally, we propose future research directions to enhance benchmark\ndesign, evaluation metrics, and protocols, including the need for more diverse,\nmultimodal, and interpretability-focused benchmarks. This survey aims to equip\nresearchers with a structured understanding of how to effectively evaluate\nVideoLLMs and identify promising avenues for advancing the field of video\nunderstanding with large language models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 2 Tables",
    "pdf_url": "http://arxiv.org/pdf/2505.03829v1",
    "published_date": "2025-05-03 20:56:09 UTC",
    "updated_date": "2025-05-03 20:56:09 UTC"
  },
  {
    "arxiv_id": "2505.01912v1",
    "title": "BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models",
    "authors": [
      "Evan R. Antoniuk",
      "Shehtab Zaman",
      "Tal Ben-Nun",
      "Peggy Li",
      "James Diffenderfer",
      "Busra Demirci",
      "Obadiah Smolenski",
      "Tim Hsu",
      "Anna M. Hiszpanski",
      "Kenneth Chiu",
      "Bhavya Kailkhura",
      "Brian Van Essen"
    ],
    "abstract": "Advances in deep learning and generative modeling have driven interest in\ndata-driven molecule discovery pipelines, whereby machine learning (ML) models\nare used to filter and design novel molecules without requiring prohibitively\nexpensive first-principles simulations. Although the discovery of novel\nmolecules that extend the boundaries of known chemistry requires accurate\nout-of-distribution (OOD) predictions, ML models often struggle to generalize\nOOD. Furthermore, there are currently no systematic benchmarks for molecular\nOOD prediction tasks. We present BOOM, $\\boldsymbol{b}$enchmarks for\n$\\boldsymbol{o}$ut-$\\boldsymbol{o}$f-distribution $\\boldsymbol{m}$olecular\nproperty predictions -- a benchmark study of property-based out-of-distribution\nmodels for common molecular property prediction models. We evaluate more than\n140 combinations of models and property prediction tasks to benchmark deep\nlearning models on their OOD performance. Overall, we do not find any existing\nmodels that achieve strong OOD generalization across all tasks: even the top\nperforming model exhibited an average OOD error 3x larger than in-distribution.\nWe find that deep learning models with high inductive bias can perform well on\nOOD tasks with simple, specific properties. Although chemical foundation models\nwith transfer and in-context learning offer a promising solution for limited\ntraining data scenarios, we find that current foundation models do not show\nstrong OOD extrapolation capabilities. We perform extensive ablation\nexperiments to highlight how OOD performance is impacted by data generation,\npre-training, hyperparameter optimization, model architecture, and molecular\nrepresentation. We propose that developing ML models with strong OOD\ngeneralization is a new frontier challenge in chemical ML model development.\nThis open-source benchmark will be made available on Github.",
    "categories": [
      "cs.LG",
      "cond-mat.mtrl-sci",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.01912v1",
    "published_date": "2025-05-03 19:51:23 UTC",
    "updated_date": "2025-05-03 19:51:23 UTC"
  },
  {
    "arxiv_id": "2505.03828v1",
    "title": "Sentiment-Aware Recommendation Systems in E-Commerce: A Review from a Natural Language Processing Perspective",
    "authors": [
      "Yogesh Gajula"
    ],
    "abstract": "E-commerce platforms generate vast volumes of user feedback, such as star\nratings, written reviews, and comments. However, most recommendation engines\nrely primarily on numerical scores, often overlooking the nuanced opinions\nembedded in free text. This paper comprehensively reviews sentiment-aware\nrecommendation systems from a natural language processing perspective, covering\nadvancements from 2023 to early 2025. It highlights the benefits of integrating\nsentiment analysis into e-commerce recommenders to enhance prediction accuracy\nand explainability through detailed opinion extraction. Our survey categorizes\nrecent work into four main approaches: deep learning classifiers that combine\nsentiment embeddings with user item interactions, transformer based methods for\nnuanced feature extraction, graph neural networks that propagate sentiment\nsignals, and conversational recommenders that adapt in real time to user\nfeedback. We summarize model architectures and demonstrate how sentiment flows\nthrough recommendation pipelines, impacting dialogue-based suggestions. Key\nchallenges include handling noisy or sarcastic text, dynamic user preferences,\nand bias mitigation. Finally, we outline research gaps and provide a roadmap\nfor developing smarter, fairer, and more user-centric recommendation tools.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "12 pages, 2 tables, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.03828v1",
    "published_date": "2025-05-03 19:36:27 UTC",
    "updated_date": "2025-05-03 19:36:27 UTC"
  },
  {
    "arxiv_id": "2505.01903v1",
    "title": "LookAlike: Consistent Distractor Generation in Math MCQs",
    "authors": [
      "Nisarg Parikh",
      "Nigel Fernandez",
      "Alexander Scarlatos",
      "Simon Woodhead",
      "Andrew Lan"
    ],
    "abstract": "Large language models (LLMs) are increasingly used to generate distractors\nfor multiple-choice questions (MCQs), especially in domains like math\neducation. However, existing approaches are limited in ensuring that the\ngenerated distractors are consistent with common student errors. We propose\nLookAlike, a method that improves error-distractor consistency via preference\noptimization. Our two main innovations are: (a) mining synthetic preference\npairs from model inconsistencies, and (b) alternating supervised fine-tuning\n(SFT) with Direct Preference Optimization (DPO) to stabilize training. Unlike\nprior work that relies on heuristics or manually annotated preference data,\nLookAlike uses its own generation inconsistencies as dispreferred samples, thus\nenabling scalable and stable training. Evaluated on a real-world dataset of\n1,400+ math MCQs, LookAlike achieves 51.6% accuracy in distractor generation\nand 57.2% in error generation under LLM-as-a-judge evaluation, outperforming an\nexisting state-of-the-art method (45.6% / 47.7%). These improvements highlight\nthe effectiveness of preference-based regularization and inconsistency mining\nfor generating consistent math MCQ distractors at scale.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.01903v1",
    "published_date": "2025-05-03 19:18:06 UTC",
    "updated_date": "2025-05-03 19:18:06 UTC"
  },
  {
    "arxiv_id": "2505.01892v1",
    "title": "OODTE: A Differential Testing Engine for the ONNX Optimizer",
    "authors": [
      "Nikolaos Louloudakis",
      "Ajitha Rajan"
    ],
    "abstract": "With $700$ stars on GitHub and part of the official ONNX repository, the ONNX\nOptimizer consists of the standard method to apply graph-based optimizations on\nONNX models. However, its ability to preserve model accuracy across\noptimizations, has not been rigorously explored. We propose OODTE, a utility to\nautomatically and thoroughly assess the correctness of the ONNX Optimizer.\nOODTE follows a simple, yet effective differential testing and evaluation\napproach that can be easily adopted to other compiler optimizers. In\nparticular, OODTE utilizes a number of ONNX models, then optimizes them and\nexecutes both the original and the optimized variants across a user-defined set\nof inputs, while automatically logging any issues with the optimization\nprocess. Finally, for successfully optimized models, OODTE compares the\nresults, and, if any accuracy deviations are observed, it iteratively repeats\nthe process for each pass of the ONNX Optimizer, to localize the root cause of\nthe differences observed. Using OODTE, we sourced well-known $130$ models from\nthe official ONNX Model Hub, used for a wide variety of tasks (classification,\nobject detection, semantic segmentation, text summarization, question and\nanswering, sentiment analysis) from the official ONNX model hub. We detected 15\nissues, 14 of which were previously unknown, associated with optimizer crashes\nand accuracy deviations. We also observed $9.2$% of all model instances\npresenting issues leading into the crash of the optimizer, or the generation of\nan invalid model while using the primary optimizer strategies. In addition,\n$30$% of the classification models presented accuracy differences across the\noriginal and the optimized model variants, while $16.6$% of semantic\nsegmentation and object detection models are also affected, at least to a\nlimited extent.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SE",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages, 3 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2505.01892v1",
    "published_date": "2025-05-03 18:54:30 UTC",
    "updated_date": "2025-05-03 18:54:30 UTC"
  },
  {
    "arxiv_id": "2505.01884v2",
    "title": "Adversarial Robustness of Deep Learning Models for Inland Water Body Segmentation from SAR Images",
    "authors": [
      "Siddharth Kothari",
      "Srinivasan Murali",
      "Sankalp Kothari",
      "Ujjwal Verma",
      "Jaya Sreevalsan-Nair"
    ],
    "abstract": "Inland water body segmentation from Synthetic Aperture Radar (SAR) images is\nan important task needed for several applications, such as flood mapping. While\nSAR sensors capture data in all-weather conditions as high-resolution images,\ndifferentiating water and water-like surfaces from SAR images is not\nstraightforward. Inland water bodies, such as large river basins, have complex\ngeometry, which adds to the challenge of segmentation. U-Net is a widely used\ndeep learning model for land-water segmentation of SAR images. In practice,\nmanual annotation is often used to generate the corresponding water masks as\nground truth. Manual annotation of the images is prone to label noise owing to\ndata poisoning attacks, especially due to complex geometry. In this work, we\nsimulate manual errors in the form of adversarial attacks on the U-Net model\nand study the robustness of the model to human errors in annotation. Our\nresults indicate that U-Net can tolerate a certain level of corruption before\nits performance drops significantly. This finding highlights the crucial role\nthat the quality of manual annotations plays in determining the effectiveness\nof the segmentation model. The code and the new dataset, along with adversarial\nexamples for robust training, are publicly available. (GitHub link -\nhttps://github.com/GVCL/IWSeg-SAR-Poison.git)",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "21 pages, 15 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2505.01884v2",
    "published_date": "2025-05-03 18:18:59 UTC",
    "updated_date": "2025-05-06 17:26:22 UTC"
  },
  {
    "arxiv_id": "2505.03827v1",
    "title": "MISE: Meta-knowledge Inheritance for Social Media-Based Stressor Estimation",
    "authors": [
      "Xin Wang",
      "Ling Feng",
      "Huijun Zhang",
      "Lei Cao",
      "Kaisheng Zeng",
      "Qi Li",
      "Yang Ding",
      "Yi Dai",
      "David Clifton"
    ],
    "abstract": "Stress haunts people in modern society, which may cause severe health issues\nif left unattended. With social media becoming an integral part of daily life,\nleveraging social media to detect stress has gained increasing attention. While\nthe majority of the work focuses on classifying stress states and stress\ncategories, this study introduce a new task aimed at estimating more specific\nstressors (like exam, writing paper, etc.) through users' posts on social\nmedia. Unfortunately, the diversity of stressors with many different classes\nbut a few examples per class, combined with the consistent arising of new\nstressors over time, hinders the machine understanding of stressors. To this\nend, we cast the stressor estimation problem within a practical scenario\nfew-shot learning setting, and propose a novel meta-learning based stressor\nestimation framework that is enhanced by a meta-knowledge inheritance\nmechanism. This model can not only learn generic stressor context through\nmeta-learning, but also has a good generalization ability to estimate new\nstressors with little labeled data. A fundamental breakthrough in our approach\nlies in the inclusion of the meta-knowledge inheritance mechanism, which equips\nour model with the ability to prevent catastrophic forgetting when adapting to\nnew stressors. The experimental results show that our model achieves\nstate-of-the-art performance compared with the baselines. Additionally, we\nconstruct a social media-based stressor estimation dataset that can help train\nartificial intelligence models to facilitate human well-being. The dataset is\nnow public at\n\\href{https://www.kaggle.com/datasets/xinwangcs/stressor-cause-of-mental-health-problem-dataset}{\\underline{Kaggle}}\nand\n\\href{https://huggingface.co/datasets/XinWangcs/Stressor}{\\underline{Hugging\nFace}}.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "WWW2025, Oral Presentation",
    "pdf_url": "http://arxiv.org/pdf/2505.03827v1",
    "published_date": "2025-05-03 18:12:36 UTC",
    "updated_date": "2025-05-03 18:12:36 UTC"
  },
  {
    "arxiv_id": "2505.01881v1",
    "title": "PhysNav-DG: A Novel Adaptive Framework for Robust VLM-Sensor Fusion in Navigation Applications",
    "authors": [
      "Trisanth Srinivasan",
      "Santosh Patapati"
    ],
    "abstract": "Robust navigation in diverse environments and domains requires both accurate\nstate estimation and transparent decision making. We present PhysNav-DG, a\nnovel framework that integrates classical sensor fusion with the semantic power\nof vision-language models. Our dual-branch architecture predicts navigation\nactions from multi-sensor inputs while simultaneously generating detailed\nchain-of-thought explanations. A modified Adaptive Kalman Filter dynamically\nadjusts its noise parameters based on environmental context. It leverages\nseveral streams of raw sensor data along with semantic insights from models\nsuch as LLaMA 3.2 11B and BLIP-2. To evaluate our approach, we introduce the\nMD-NEX Benchmark, a novel multi-domain dataset that unifies indoor navigation,\nautonomous driving, and social navigation tasks with ground-truth actions and\nhuman-validated explanations. Extensive experiments and ablations show that\nPhysNav-DG improves navigation success rates by over 20% and achieves high\nefficiency, with explanations that are both highly grounded and clear. This\nwork connects high-level semantic reasoning and geometric planning for safer\nand more trustworthy autonomous systems.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.01881v1",
    "published_date": "2025-05-03 17:59:26 UTC",
    "updated_date": "2025-05-03 17:59:26 UTC"
  },
  {
    "arxiv_id": "2505.01877v3",
    "title": "Humans can learn to detect AI-generated texts, or at least learn when they can't",
    "authors": [
      "Jiří Milička",
      "Anna Marklová",
      "Ondřej Drobil",
      "Eva Pospíšilová"
    ],
    "abstract": "This study investigates whether individuals can learn to accurately\ndiscriminate between human-written and AI-produced texts when provided with\nimmediate feedback, and if they can use this feedback to recalibrate their\nself-perceived competence. We also explore the specific criteria individuals\nrely upon when making these decisions, focusing on textual style and perceived\nreadability.\n  We used GPT-4o to generate several hundred texts across various genres and\ntext types comparable to Koditex, a multi-register corpus of human-written\ntexts. We then presented randomized text pairs to 254 Czech native speakers who\nidentified which text was human-written and which was AI-generated.\nParticipants were randomly assigned to two conditions: one receiving immediate\nfeedback after each trial, the other receiving no feedback until experiment\ncompletion. We recorded accuracy in identification, confidence levels, response\ntimes, and judgments about text readability along with demographic data and\nparticipants' engagement with AI technologies prior to the experiment.\n  Participants receiving immediate feedback showed significant improvement in\naccuracy and confidence calibration. Participants initially held incorrect\nassumptions about AI-generated text features, including expectations about\nstylistic rigidity and readability. Notably, without feedback, participants\nmade the most errors precisely when feeling most confident -- an issue largely\nresolved among the feedback group.\n  The ability to differentiate between human and AI-generated texts can be\neffectively learned through targeted training with explicit feedback, which\nhelps correct misconceptions about AI stylistic features and readability, as\nwell as potential other variables that were not explored, while facilitating\nmore accurate self-assessment. This finding might be particularly important in\neducational contexts.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.01877v3",
    "published_date": "2025-05-03 17:42:49 UTC",
    "updated_date": "2025-05-07 18:18:00 UTC"
  },
  {
    "arxiv_id": "2505.06259v1",
    "title": "Fair Clustering with Clusterlets",
    "authors": [
      "Mattia Setzu",
      "Riccardo Guidotti"
    ],
    "abstract": "Given their widespread usage in the real world, the fairness of clustering\nmethods has become of major interest. Theoretical results on fair clustering\nshow that fairness enjoys transitivity: given a set of small and fair clusters,\na trivial centroid-based clustering algorithm yields a fair clustering.\nUnfortunately, discovering a suitable starting clustering can be\ncomputationally expensive, rather complex or arbitrary.\n  In this paper, we propose a set of simple \\emph{clusterlet}-based fuzzy\nclustering algorithms that match single-class clusters, optimizing fair\nclustering. Matching leverages clusterlet distance, optimizing for classic\nclustering objectives, while also regularizing for fairness. Empirical results\nshow that simple matching strategies are able to achieve high fairness, and\nthat appropriate parameter tuning allows to achieve high cohesion and low\noverlap.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.06259v1",
    "published_date": "2025-05-03 17:00:54 UTC",
    "updated_date": "2025-05-03 17:00:54 UTC"
  },
  {
    "arxiv_id": "2505.01855v1",
    "title": "Intra-Layer Recurrence in Transformers for Language Modeling",
    "authors": [
      "Anthony Nguyen",
      "Wenjun Lin"
    ],
    "abstract": "Transformer models have established new benchmarks in natural language\nprocessing; however, their increasing depth results in substantial growth in\nparameter counts. While existing recurrent transformer methods address this\nissue by reprocessing layers multiple times, they often apply recurrence\nindiscriminately across entire blocks of layers. In this work, we investigate\nIntra-Layer Recurrence (ILR), a more targeted approach that applies recurrence\nselectively to individual layers within a single forward pass. Our experiments\nshow that allocating more iterations to earlier layers yields optimal results.\nThese findings suggest that ILR offers a promising direction for optimizing\nrecurrent structures in transformer architectures.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at Canadian AI 2025. Code available at\n  https://github.com/ant-8/Layer-Recurrent-Transformers",
    "pdf_url": "http://arxiv.org/pdf/2505.01855v1",
    "published_date": "2025-05-03 16:16:55 UTC",
    "updated_date": "2025-05-03 16:16:55 UTC"
  },
  {
    "arxiv_id": "2505.01854v1",
    "title": "Accelerating Volumetric Medical Image Annotation via Short-Long Memory SAM 2",
    "authors": [
      "Yuwen Chen",
      "Zafer Yildiz",
      "Qihang Li",
      "Yaqian Chen",
      "Haoyu Dong",
      "Hanxue Gu",
      "Nicholas Konz",
      "Maciej A. Mazurowski"
    ],
    "abstract": "Manual annotation of volumetric medical images, such as magnetic resonance\nimaging (MRI) and computed tomography (CT), is a labor-intensive and\ntime-consuming process. Recent advancements in foundation models for video\nobject segmentation, such as Segment Anything Model 2 (SAM 2), offer a\npotential opportunity to significantly speed up the annotation process by\nmanually annotating one or a few slices and then propagating target masks\nacross the entire volume. However, the performance of SAM 2 in this context\nvaries. Our experiments show that relying on a single memory bank and attention\nmodule is prone to error propagation, particularly at boundary regions where\nthe target is present in the previous slice but absent in the current one. To\naddress this problem, we propose Short-Long Memory SAM 2 (SLM-SAM 2), a novel\narchitecture that integrates distinct short-term and long-term memory banks\nwith separate attention modules to improve segmentation accuracy. We evaluate\nSLM-SAM 2 on three public datasets covering organs, bones, and muscles across\nMRI and CT modalities. We show that the proposed method markedly outperforms\nthe default SAM 2, achieving average Dice Similarity Coefficient improvement of\n0.14 and 0.11 in the scenarios when 5 volumes and 1 volume are available for\nthe initial adaptation, respectively. SLM-SAM 2 also exhibits stronger\nresistance to over-propagation, making a notable step toward more accurate\nautomated annotation of medical images for segmentation model development.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.01854v1",
    "published_date": "2025-05-03 16:16:24 UTC",
    "updated_date": "2025-05-03 16:16:24 UTC"
  },
  {
    "arxiv_id": "2505.03826v1",
    "title": "In-situ and Non-contact Etch Depth Prediction in Plasma Etching via Machine Learning (ANN & BNN) and Digital Image Colorimetry",
    "authors": [
      "Minji Kang",
      "Seongho Kim",
      "Eunseo Go",
      "Donghyeon Paek",
      "Geon Lim",
      "Muyoung Kim",
      "Soyeun Kim",
      "Sung Kyu Jang",
      "Min Sup Choi",
      "Woo Seok Kang",
      "Jaehyun Kim",
      "Jaekwang Kim",
      "Hyeong-U Kim"
    ],
    "abstract": "Precise monitoring of etch depth and the thickness of insulating materials,\nsuch as Silicon dioxide and silicon nitride, is critical to ensuring device\nperformance and yield in semiconductor manufacturing. While conventional\nex-situ analysis methods are accurate, they are constrained by time delays and\ncontamination risks. To address these limitations, this study proposes a\nnon-contact, in-situ etch depth prediction framework based on machine learning\n(ML) techniques. Two scenarios are explored. In the first scenario, an\nartificial neural network (ANN) is trained to predict average etch depth from\nprocess parameters, achieving a significantly lower mean squared error (MSE)\ncompared to a linear baseline model. The approach is then extended to\nincorporate variability from repeated measurements using a Bayesian Neural\nNetwork (BNN) to capture both aleatoric and epistemic uncertainty. Coverage\nanalysis confirms the BNN's capability to provide reliable uncertainty\nestimates. In the second scenario, we demonstrate the feasibility of using RGB\ndata from digital image colorimetry (DIC) as input for etch depth prediction,\nachieving strong performance even in the absence of explicit process\nparameters. These results suggest that the integration of DIC and ML offers a\nviable, cost-effective alternative for real-time, in-situ, and non-invasive\nmonitoring in plasma etching processes, contributing to enhanced process\nstability, and manufacturing efficiency.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "20 pages",
    "pdf_url": "http://arxiv.org/pdf/2505.03826v1",
    "published_date": "2025-05-03 14:43:19 UTC",
    "updated_date": "2025-05-03 14:43:19 UTC"
  },
  {
    "arxiv_id": "2505.04638v1",
    "title": "Towards Artificial Intelligence Research Assistant for Expert-Involved Learning",
    "authors": [
      "Tianyu Liu",
      "Simeng Han",
      "Xiao Luo",
      "Hanchen Wang",
      "Pan Lu",
      "Biqing Zhu",
      "Yuge Wang",
      "Keyi Li",
      "Jiapeng Chen",
      "Rihao Qu",
      "Yufeng Liu",
      "Xinyue Cui",
      "Aviv Yaish",
      "Yuhang Chen",
      "Minsheng Hao",
      "Chuhan Li",
      "Kexing Li",
      "Arman Cohan",
      "Hua Xu",
      "Mark Gerstein",
      "James Zou",
      "Hongyu Zhao"
    ],
    "abstract": "Large Language Models (LLMs) and Large Multi-Modal Models (LMMs) have emerged\nas transformative tools in scientific research, yet their reliability and\nspecific contributions to biomedical applications remain insufficiently\ncharacterized. In this study, we present \\textbf{AR}tificial\n\\textbf{I}ntelligence research assistant for \\textbf{E}xpert-involved\n\\textbf{L}earning (ARIEL), a multimodal dataset designed to benchmark and\nenhance two critical capabilities of LLMs and LMMs in biomedical research:\nsummarizing extensive scientific texts and interpreting complex biomedical\nfigures. To facilitate rigorous assessment, we create two open-source sets\ncomprising biomedical articles and figures with designed questions. We\nsystematically benchmark both open- and closed-source foundation models,\nincorporating expert-driven human evaluations conducted by doctoral-level\nexperts. Furthermore, we improve model performance through targeted prompt\nengineering and fine-tuning strategies for summarizing research papers, and\napply test-time computational scaling to enhance the reasoning capabilities of\nLMMs, achieving superior accuracy compared to human-expert corrections. We also\nexplore the potential of using LMM Agents to generate scientific hypotheses\nfrom diverse multimodal inputs. Overall, our results delineate clear strengths\nand highlight significant limitations of current foundation models, providing\nactionable insights and guiding future advancements in deploying large-scale\nlanguage and multi-modal models within biomedical research.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "36 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.04638v1",
    "published_date": "2025-05-03 14:21:48 UTC",
    "updated_date": "2025-05-03 14:21:48 UTC"
  },
  {
    "arxiv_id": "2505.01823v1",
    "title": "PhytoSynth: Leveraging Multi-modal Generative Models for Crop Disease Data Generation with Novel Benchmarking and Prompt Engineering Approach",
    "authors": [
      "Nitin Rai",
      "Arnold W. Schumann",
      "Nathan Boyd"
    ],
    "abstract": "Collecting large-scale crop disease images in the field is labor-intensive\nand time-consuming. Generative models (GMs) offer an alternative by creating\nsynthetic samples that resemble real-world images. However, existing research\nprimarily relies on Generative Adversarial Networks (GANs)-based image-to-image\ntranslation and lack a comprehensive analysis of computational requirements in\nagriculture. Therefore, this research explores a multi-modal text-to-image\napproach for generating synthetic crop disease images and is the first to\nprovide computational benchmarking in this context. We trained three Stable\nDiffusion (SD) variants-SDXL, SD3.5M (medium), and SD3.5L (large)-and\nfine-tuned them using Dreambooth and Low-Rank Adaptation (LoRA) fine-tuning\ntechniques to enhance generalization. SD3.5M outperformed the others, with an\naverage memory usage of 18 GB, power consumption of 180 W, and total energy use\nof 1.02 kWh/500 images (0.002 kWh per image) during inference task. Our results\ndemonstrate SD3.5M's ability to generate 500 synthetic images from just 36\nin-field samples in 1.5 hours. We recommend SD3.5M for efficient crop disease\ndata generation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.01823v1",
    "published_date": "2025-05-03 14:03:42 UTC",
    "updated_date": "2025-05-03 14:03:42 UTC"
  },
  {
    "arxiv_id": "2505.01822v1",
    "title": "Analytic Energy-Guided Policy Optimization for Offline Reinforcement Learning",
    "authors": [
      "Jifeng Hu",
      "Sili Huang",
      "Zhejian Yang",
      "Shengchao Hu",
      "Li Shen",
      "Hechang Chen",
      "Lichao Sun",
      "Yi Chang",
      "Dacheng Tao"
    ],
    "abstract": "Conditional decision generation with diffusion models has shown powerful\ncompetitiveness in reinforcement learning (RL). Recent studies reveal the\nrelation between energy-function-guidance diffusion models and constrained RL\nproblems. The main challenge lies in estimating the intermediate energy, which\nis intractable due to the log-expectation formulation during the generation\nprocess. To address this issue, we propose the Analytic Energy-guided Policy\nOptimization (AEPO). Specifically, we first provide a theoretical analysis and\nthe closed-form solution of the intermediate guidance when the diffusion model\nobeys the conditional Gaussian transformation. Then, we analyze the posterior\nGaussian distribution in the log-expectation formulation and obtain the target\nestimation of the log-expectation under mild assumptions. Finally, we train an\nintermediate energy neural network to approach the target estimation of\nlog-expectation formulation. We apply our method in 30+ offline RL tasks to\ndemonstrate the effectiveness of our method. Extensive experiments illustrate\nthat our method surpasses numerous representative baselines in D4RL offline\nreinforcement learning benchmarks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.01822v1",
    "published_date": "2025-05-03 14:00:25 UTC",
    "updated_date": "2025-05-03 14:00:25 UTC"
  },
  {
    "arxiv_id": "2505.01821v2",
    "title": "Edge-Cloud Collaborative Computing on Distributed Intelligence and Model Optimization: A Survey",
    "authors": [
      "Jing Liu",
      "Yao Du",
      "Kun Yang",
      "Yan Wang",
      "Xiping Hu",
      "Zehua Wang",
      "Yang Liu",
      "Peng Sun",
      "Azzedine Boukerche",
      "Victor C. M. Leung"
    ],
    "abstract": "Edge-cloud collaborative computing (ECCC) has emerged as a pivotal paradigm\nfor addressing the computational demands of modern intelligent applications,\nintegrating cloud resources with edge devices to enable efficient, low-latency\nprocessing. Recent advancements in AI, particularly deep learning and large\nlanguage models (LLMs), have dramatically enhanced the capabilities of these\ndistributed systems, yet introduce significant challenges in model deployment\nand resource management. In this survey, we comprehensive examine the\nintersection of distributed intelligence and model optimization within\nedge-cloud environments, providing a structured tutorial on fundamental\narchitectures, enabling technologies, and emerging applications. Additionally,\nwe systematically analyze model optimization approaches, including compression,\nadaptation, and neural architecture search, alongside AI-driven resource\nmanagement strategies that balance performance, energy efficiency, and latency\nrequirements. We further explore critical aspects of privacy protection and\nsecurity enhancement within ECCC systems and examines practical deployments\nthrough diverse applications, spanning autonomous driving, healthcare, and\nindustrial automation. Performance analysis and benchmarking techniques are\nalso thoroughly explored to establish evaluation standards for these complex\nsystems. Furthermore, the review identifies critical research directions\nincluding LLMs deployment, 6G integration, neuromorphic computing, and quantum\ncomputing, offering a roadmap for addressing persistent challenges in\nheterogeneity management, real-time processing, and scalability. By bridging\ntheoretical advancements and practical deployments, this survey offers\nresearchers and practitioners a holistic perspective on leveraging AI to\noptimize distributed computing environments, fostering innovation in\nnext-generation intelligent systems.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.DC",
    "comment": "30 pages, 10 figures, 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2505.01821v2",
    "published_date": "2025-05-03 13:55:38 UTC",
    "updated_date": "2025-05-17 11:18:55 UTC"
  },
  {
    "arxiv_id": "2505.01812v1",
    "title": "$\\textit{New News}$: System-2 Fine-tuning for Robust Integration of New Knowledge",
    "authors": [
      "Core Francisco Park",
      "Zechen Zhang",
      "Hidenori Tanaka"
    ],
    "abstract": "Humans and intelligent animals can effortlessly internalize new information\n(\"news\") and accurately extract the implications for performing downstream\ntasks. While large language models (LLMs) can achieve this through in-context\nlearning (ICL) when the news is explicitly given as context, fine-tuning\nremains challenging for the models to consolidate learning in weights. In this\npaper, we introduce $\\textit{New News}$, a dataset composed of hypothetical yet\nplausible news spanning multiple domains (mathematics, coding, discoveries,\nleaderboards, events), accompanied by downstream evaluation questions whose\ncorrect answers critically depend on understanding and internalizing the news.\nWe first demonstrate a substantial gap between naive fine-tuning and in-context\nlearning (FT-ICL gap) on our news dataset. To address this gap, we explore a\nsuite of self-play data generation protocols -- paraphrases, implications and\nSelf-QAs -- designed to distill the knowledge from the model with context into\nthe weights of the model without the context, which we term $\\textit{System-2\nFine-tuning}$ (Sys2-FT). We systematically evaluate ICL and Sys2-FT performance\nacross data domains and model scales with the Qwen 2.5 family of models. Our\nresults demonstrate that the self-QA protocol of Sys2-FT significantly improves\nmodels' in-weight learning of the news. Furthermore, we discover the\n$\\textit{contexual shadowing effect}$, where training with the news $\\textit{in\ncontext}$ followed by its rephrases or QAs degrade learning of the news.\nFinally, we show preliminary evidence of an emerging scaling law of Sys2-FT.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.01812v1",
    "published_date": "2025-05-03 12:49:35 UTC",
    "updated_date": "2025-05-03 12:49:35 UTC"
  },
  {
    "arxiv_id": "2505.02865v1",
    "title": "Accelerating Large Language Model Reasoning via Speculative Search",
    "authors": [
      "Zhihai Wang",
      "Jie Wang",
      "Jilai Pan",
      "Xilin Xia",
      "Huiling Zhen",
      "Mingxuan Yuan",
      "Jianye Hao",
      "Feng Wu"
    ],
    "abstract": "Tree-search-based reasoning methods have significantly enhanced the reasoning\ncapability of large language models (LLMs) by facilitating the exploration of\nmultiple intermediate reasoning steps, i.e., thoughts. However, these methods\nsuffer from substantial inference latency, as they have to generate numerous\nreasoning thoughts, severely limiting LLM applicability. To address this\nchallenge, we propose a novel Speculative Search (SpecSearch) framework that\nsignificantly accelerates LLM reasoning by optimizing thought generation.\nSpecifically, SpecSearch utilizes a small model to strategically collaborate\nwith a large model at both thought and token levels, efficiently generating\nhigh-quality reasoning thoughts. The major pillar of SpecSearch is a novel\nquality-preserving rejection mechanism, which effectively filters out thoughts\nwhose quality falls below that of the large model's outputs. Moreover, we show\nthat SpecSearch preserves comparable reasoning quality to the large model.\nExperiments on both the Qwen and Llama models demonstrate that SpecSearch\nsignificantly outperforms state-of-the-art approaches, achieving up to\n2.12$\\times$ speedup with comparable reasoning quality.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by ICML2025",
    "pdf_url": "http://arxiv.org/pdf/2505.02865v1",
    "published_date": "2025-05-03 12:14:08 UTC",
    "updated_date": "2025-05-03 12:14:08 UTC"
  },
  {
    "arxiv_id": "2505.01800v1",
    "title": "Distinguishing AI-Generated and Human-Written Text Through Psycholinguistic Analysis",
    "authors": [
      "Chidimma Opara"
    ],
    "abstract": "The increasing sophistication of AI-generated texts highlights the urgent\nneed for accurate and transparent detection tools, especially in educational\nsettings, where verifying authorship is essential. Existing literature has\ndemonstrated that the application of stylometric features with machine learning\nclassifiers can yield excellent results. Building on this foundation, this\nstudy proposes a comprehensive framework that integrates stylometric analysis\nwith psycholinguistic theories, offering a clear and interpretable approach to\ndistinguishing between AI-generated and human-written texts. This research\nspecifically maps 31 distinct stylometric features to cognitive processes such\nas lexical retrieval, discourse planning, cognitive load management, and\nmetacognitive self-monitoring. In doing so, it highlights the unique\npsycholinguistic patterns found in human writing. Through the intersection of\ncomputational linguistics and cognitive science, this framework contributes to\nthe development of reliable tools aimed at preserving academic integrity in the\nera of generative AI.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "8",
    "pdf_url": "http://arxiv.org/pdf/2505.01800v1",
    "published_date": "2025-05-03 12:06:53 UTC",
    "updated_date": "2025-05-03 12:06:53 UTC"
  },
  {
    "arxiv_id": "2505.06258v1",
    "title": "ABE: A Unified Framework for Robust and Faithful Attribution-Based Explainability",
    "authors": [
      "Zhiyu Zhu",
      "Jiayu Zhang",
      "Zhibo Jin",
      "Fang Chen",
      "Jianlong Zhou"
    ],
    "abstract": "Attribution algorithms are essential for enhancing the interpretability and\ntrustworthiness of deep learning models by identifying key features driving\nmodel decisions. Existing frameworks, such as InterpretDL and OmniXAI,\nintegrate multiple attribution methods but suffer from scalability limitations,\nhigh coupling, theoretical constraints, and lack of user-friendly\nimplementations, hindering neural network transparency and interoperability. To\naddress these challenges, we propose Attribution-Based Explainability (ABE), a\nunified framework that formalizes Fundamental Attribution Methods and\nintegrates state-of-the-art attribution algorithms while ensuring compliance\nwith attribution axioms. ABE enables researchers to develop novel attribution\ntechniques and enhances interpretability through four customizable modules:\nRobustness, Interpretability, Validation, and Data & Model. This framework\nprovides a scalable, extensible foundation for advancing attribution-based\nexplainability and fostering transparent AI systems. Our code is available at:\nhttps://github.com/LMBTough/ABE-XAI.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.06258v1",
    "published_date": "2025-05-03 12:00:59 UTC",
    "updated_date": "2025-05-03 12:00:59 UTC"
  },
  {
    "arxiv_id": "2505.01794v1",
    "title": "A Multimodal Framework for Explainable Evaluation of Soft Skills in Educational Environments",
    "authors": [
      "Jared D. T. Guerrero-Sosa",
      "Francisco P. Romero",
      "Víctor Hugo Menéndez-Domínguez",
      "Jesus Serrano-Guerrero",
      "Andres Montoro-Montarroso",
      "Jose A. Olivas"
    ],
    "abstract": "In the rapidly evolving educational landscape, the unbiased assessment of\nsoft skills is a significant challenge, particularly in higher education. This\npaper presents a fuzzy logic approach that employs a Granular Linguistic Model\nof Phenomena integrated with multimodal analysis to evaluate soft skills in\nundergraduate students. By leveraging computational perceptions, this approach\nenables a structured breakdown of complex soft skill expressions, capturing\nnuanced behaviours with high granularity and addressing their inherent\nuncertainties, thereby enhancing interpretability and reliability. Experiments\nwere conducted with undergraduate students using a developed tool that assesses\nsoft skills such as decision-making, communication, and creativity. This tool\nidentifies and quantifies subtle aspects of human interaction, such as facial\nexpressions and gesture recognition. The findings reveal that the framework\neffectively consolidates multiple data inputs to produce meaningful and\nconsistent assessments of soft skills, showing that integrating multiple\nmodalities into the evaluation process significantly improves the quality of\nsoft skills scores, making the assessment work transparent and understandable\nto educational stakeholders.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.01794v1",
    "published_date": "2025-05-03 11:54:35 UTC",
    "updated_date": "2025-05-03 11:54:35 UTC"
  },
  {
    "arxiv_id": "2505.03825v2",
    "title": "Intelligently Augmented Contrastive Tensor Factorization: Empowering Multi-dimensional Time Series Classification in Low-Data Environments",
    "authors": [
      "Anushiya Arunan",
      "Yan Qin",
      "Xiaoli Li",
      "Yuen Chau"
    ],
    "abstract": "Classification of multi-dimensional time series from real-world systems\nrequire fine-grained learning of complex features such as cross-dimensional\ndependencies and intra-class variations-all under the practical challenge of\nlow training data availability. However, standard deep learning (DL) struggles\nto learn generalizable features in low-data environments due to model\noverfitting. We propose a versatile yet data-efficient framework, Intelligently\nAugmented Contrastive Tensor Factorization (ITA-CTF), to learn effective\nrepresentations from multi-dimensional time series. The CTF module learns core\nexplanatory components of the time series (e.g., sensor factors, temporal\nfactors), and importantly, their joint dependencies. Notably, unlike standard\ntensor factorization (TF), the CTF module incorporates a new contrastive loss\noptimization to induce similarity learning and class-awareness into the learnt\nrepresentations for better classification performance. To strengthen this\ncontrastive learning, the preceding ITA module generates targeted but\ninformative augmentations that highlight realistic intra-class patterns in the\noriginal data, while preserving class-wise properties. This is achieved by\ndynamically sampling a \"soft\" class prototype to guide the warping of each\nquery data sample, which results in an augmentation that is intelligently\npattern-mixed between the \"soft\" class prototype and the query sample. These\naugmentations enable the CTF module to recognize complex intra-class variations\ndespite the limited original training data, and seek out invariant class-wise\nproperties for accurate classification performance. The proposed method is\ncomprehensively evaluated on five different classification tasks. Compared to\nstandard TF and several DL benchmarks, notable performance improvements up to\n18.7% were achieved.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted in Expert Systems with Applications\n  (DOI:https://doi.org/10.1016/j.eswa.2025.127889)",
    "pdf_url": "http://arxiv.org/pdf/2505.03825v2",
    "published_date": "2025-05-03 11:28:13 UTC",
    "updated_date": "2025-05-15 14:07:22 UTC"
  },
  {
    "arxiv_id": "2505.01781v1",
    "title": "Enhancing Black-Litterman Portfolio via Hybrid Forecasting Model Combining Multivariate Decomposition and Noise Reduction",
    "authors": [
      "Ziye Yang",
      "Ke Lu"
    ],
    "abstract": "The sensitivity to input parameters and lack of flexibility limits the\ntraditional Mean-Variance model. In contrast, the Black-Litterman model has\nattracted widespread attention by integrating market equilibrium returns with\ninvestors' subjective views. This paper proposes a novel hybrid deep learning\nmodel combining Singular Spectrum analysis (SSA), Multivariate Aligned\nEmpirical Mode Decomposition (MA-EMD), and Temporal Convolutional Networks\n(TCNs), aiming to improve the prediction accuracy of asset prices and thus\nenhance the ability of the Black-Litterman model to generate subjective views.\nExperimental results show that noise reduction pre-processing can improve the\nmodel's accuracy, and the prediction performance of the proposed model is\nsignificantly better than that of three multivariate decomposition benchmark\nmodels. We construct an investment portfolio by using 20 representative stocks\nfrom the NASDAQ 100 index. By combining the hybrid forecasting model with the\nBlack-Litterman model, the generated investment portfolio exhibits better\nreturns and risk control capabilities than the Mean-Variance, Equal-Weighted,\nand Market-Weighted models in the short holding period.",
    "categories": [
      "cs.CE",
      "cs.AI"
    ],
    "primary_category": "cs.CE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.01781v1",
    "published_date": "2025-05-03 10:52:57 UTC",
    "updated_date": "2025-05-03 10:52:57 UTC"
  },
  {
    "arxiv_id": "2505.01780v1",
    "title": "Rate-Limited Closed-Loop Distributed ISAC Systems: An Autoencoder Approach",
    "authors": [
      "Guangjin Pan",
      "Zhixing Li",
      "Ayça Özçelikkale",
      "Christian Häger",
      "Musa Furkan Keskin",
      "Henk Wymeersch"
    ],
    "abstract": "In closed-loop distributed multi-sensor integrated sensing and communication\n(ISAC) systems, performance often hinges on transmitting high-dimensional\nsensor observations over rate-limited networks. In this paper, we first present\na general framework for rate-limited closed-loop distributed ISAC systems, and\nthen propose an autoencoder-based observation compression method to overcome\nthe constraints imposed by limited transmission capacity. Building on this\nframework, we conduct a case study using a closed-loop linear quadratic\nregulator (LQR) system to analyze how the interplay among observation,\ncompression, and state dimensions affects reconstruction accuracy, state\nestimation error, and control performance. In multi-sensor scenarios, our\nresults further show that optimal resource allocation initially prioritizes\nlow-noise sensors until the compression becomes lossless, after which resources\nare reallocated to high-noise sensors.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.NI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "eess.SP",
    "comment": "6 pages, 15 figures. This work has been submitted to the IEEE for\n  possible publication",
    "pdf_url": "http://arxiv.org/pdf/2505.01780v1",
    "published_date": "2025-05-03 10:52:39 UTC",
    "updated_date": "2025-05-03 10:52:39 UTC"
  },
  {
    "arxiv_id": "2505.04637v1",
    "title": "Adaptive Token Boundaries: Integrating Human Chunking Mechanisms into Multimodal LLMs",
    "authors": [
      "Dongxing Yu"
    ],
    "abstract": "Recent advancements in multimodal large language models (MLLMs) have\ndemonstrated remarkable capabilities in processing diverse data types, yet\nsignificant disparities persist between human cognitive processes and\ncomputational approaches to multimodal information integration. This research\npresents a systematic investigation into the parallels between human\ncross-modal chunking mechanisms and token representation methodologies in\nMLLMs. Through empirical studies comparing human performance patterns with\nmodel behaviors across visual-linguistic tasks, we demonstrate that\nconventional static tokenization schemes fundamentally constrain current\nmodels' capacity to simulate the dynamic, context-sensitive nature of human\ninformation processing. We propose a novel framework for dynamic cross-modal\ntokenization that incorporates adaptive boundaries, hierarchical\nrepresentations, and alignment mechanisms grounded in cognitive science\nprinciples. Quantitative evaluations demonstrate that our approach yields\nstatistically significant improvements over state-of-the-art models on\nbenchmark tasks (+7.8% on Visual Question Answering, +5.3% on Complex Scene\nDescription) while exhibiting more human-aligned error patterns and attention\ndistributions. These findings contribute to the theoretical understanding of\nthe relationship between human cognition and artificial intelligence, while\nproviding empirical evidence for developing more cognitively plausible AI\nsystems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.04637v1",
    "published_date": "2025-05-03 09:14:24 UTC",
    "updated_date": "2025-05-03 09:14:24 UTC"
  },
  {
    "arxiv_id": "2505.01754v1",
    "title": "Unraveling Media Perspectives: A Comprehensive Methodology Combining Large Language Models, Topic Modeling, Sentiment Analysis, and Ontology Learning to Analyse Media Bias",
    "authors": [
      "Orlando Jähde",
      "Thorsten Weber",
      "Rüdiger Buchkremer"
    ],
    "abstract": "Biased news reporting poses a significant threat to informed decision-making\nand the functioning of democracies. This study introduces a novel methodology\nfor scalable, minimally biased analysis of media bias in political news. The\nproposed approach examines event selection, labeling, word choice, and\ncommission and omission biases across news sources by leveraging natural\nlanguage processing techniques, including hierarchical topic modeling,\nsentiment analysis, and ontology learning with large language models. Through\nthree case studies related to current political events, we demonstrate the\nmethodology's effectiveness in identifying biases across news sources at\nvarious levels of granularity. This work represents a significant step towards\nscalable, minimally biased media bias analysis, laying the groundwork for tools\nto help news consumers navigate an increasingly complex media landscape.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.IR",
      "cs.LG",
      "cs.MA",
      "68T09, 68T50, 68T05, 62R07, 68U15, 68T27, 68T20 68T09, 68T50, 68T05,\n  62R07, 68U15, 68T27, 68T20 68T09, 68T50, 68T05, 62R07, 68U15, 68T27, 68T20",
      "I.2; H.3; I.5; I.7; H.5; H.1"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.01754v1",
    "published_date": "2025-05-03 09:09:34 UTC",
    "updated_date": "2025-05-03 09:09:34 UTC"
  },
  {
    "arxiv_id": "2505.01743v1",
    "title": "An LLM-Empowered Low-Resolution Vision System for On-Device Human Behavior Understanding",
    "authors": [
      "Siyang Jiang",
      "Bufang Yang",
      "Lilin Xu",
      "Mu Yuan",
      "Yeerzhati Abudunuer",
      "Kaiwei Liu",
      "Liekang Zeng",
      "Hongkai Chen",
      "Zhenyu Yan",
      "Xiaofan Jiang",
      "Guoliang Xing"
    ],
    "abstract": "The rapid advancements in Large Vision Language Models (LVLMs) offer the\npotential to surpass conventional labeling by generating richer, more detailed\ndescriptions of on-device human behavior understanding (HBU) in low-resolution\nvision systems, such as depth, thermal, and infrared. However, existing large\nvision language model (LVLM) approaches are unable to understand low-resolution\ndata well as they are primarily designed for high-resolution data, such as RGB\nimages. A quick fixing approach is to caption a large amount of low-resolution\ndata, but it requires a significant amount of labor-intensive annotation\nefforts. In this paper, we propose a novel, labor-saving system, Llambda,\ndesigned to support low-resolution HBU. The core idea is to leverage limited\nlabeled data and a large amount of unlabeled data to guide LLMs in generating\ninformative captions, which can be combined with raw data to effectively\nfine-tune LVLM models for understanding low-resolution videos in HBU. First, we\npropose a Contrastive-Oriented Data Labeler, which can capture\nbehavior-relevant information from long, low-resolution videos and generate\nhigh-quality pseudo labels for unlabeled data via contrastive learning. Second,\nwe propose a Physical-Knowledge Guided Captioner, which utilizes spatial and\ntemporal consistency checks to mitigate errors in pseudo labels. Therefore, it\ncan improve LLMs' understanding of sequential data and then generate\nhigh-quality video captions. Finally, to ensure on-device deployability, we\nemploy LoRA-based efficient fine-tuning to adapt LVLMs for low-resolution data.\nWe evaluate Llambda using a region-scale real-world testbed and three distinct\nlow-resolution datasets, and the experiments show that Llambda outperforms\nseveral state-of-the-art LVLM systems up to $40.03\\%$ on average Bert-Score.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.01743v1",
    "published_date": "2025-05-03 08:46:04 UTC",
    "updated_date": "2025-05-03 08:46:04 UTC"
  },
  {
    "arxiv_id": "2505.01736v1",
    "title": "PeSANet: Physics-encoded Spectral Attention Network for Simulating PDE-Governed Complex Systems",
    "authors": [
      "Han Wan",
      "Rui Zhang",
      "Qi Wang",
      "Yang Liu",
      "Hao Sun"
    ],
    "abstract": "Accurately modeling and forecasting complex systems governed by partial\ndifferential equations (PDEs) is crucial in various scientific and engineering\ndomains. However, traditional numerical methods struggle in real-world\nscenarios due to incomplete or unknown physical laws. Meanwhile, machine\nlearning approaches often fail to generalize effectively when faced with scarce\nobservational data and the challenge of capturing local and global features. To\nthis end, we propose the Physics-encoded Spectral Attention Network (PeSANet),\nwhich integrates local and global information to forecast complex systems with\nlimited data and incomplete physical priors. The model consists of two key\ncomponents: a physics-encoded block that uses hard constraints to approximate\nlocal differential operators from limited data, and a spectral-enhanced block\nthat captures long-range global dependencies in the frequency domain.\nSpecifically, we introduce a novel spectral attention mechanism to model\ninter-spectrum relationships and learn long-range spatial features.\nExperimental results demonstrate that PeSANet outperforms existing methods\nacross all metrics, particularly in long-term forecasting accuracy, providing a\npromising solution for simulating complex systems with limited data and\nincomplete physics.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.01736v1",
    "published_date": "2025-05-03 08:25:30 UTC",
    "updated_date": "2025-05-03 08:25:30 UTC"
  },
  {
    "arxiv_id": "2505.01731v3",
    "title": "Efficient Shapley Value-based Non-Uniform Pruning of Large Language Models",
    "authors": [
      "Chuan Sun",
      "Han Yu",
      "Lizhen Cui",
      "Xiaoxiao Li"
    ],
    "abstract": "Pruning large language models (LLMs) is a promising solution for reducing\nmodel sizes and computational complexity while preserving performance.\nTraditional layer-wise pruning methods often adopt a uniform sparsity approach\nacross all layers, which leads to suboptimal performance due to the varying\nsignificance of individual transformer layers within the model not being\naccounted for. To this end, we propose the Shapley Value-based Non-Uniform\nPruning (SV-NUP) method for LLMs. This approach quantifies the contribution of\neach transformer layer to the overall model performance, enabling the\nassignment of tailored pruning budgets to different layers to retain critical\nparameters. To further improve efficiency, we design the Sliding Window-based\nShapley Value approximation method. It substantially reduces computational\noverhead compared to exact SV calculation methods. Extensive experiments on\nvarious LLMs including LLaMA-v1, LLaMA-v2 and OPT demonstrate the effectiveness\nof the proposed approach. The results reveal that non-uniform pruning\nsignificantly enhances the performance of pruned models. Notably, SV-NUP\nachieves a reduction in perplexity (PPL) of 18.01% and 19.55% on LLaMA-7B and\nLLaMA-13B, respectively, compared to SparseGPT at 70% sparsity.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.01731v3",
    "published_date": "2025-05-03 07:57:02 UTC",
    "updated_date": "2025-05-21 01:38:25 UTC"
  },
  {
    "arxiv_id": "2505.01730v1",
    "title": "PASCAL: Precise and Efficient ANN- SNN Conversion using Spike Accumulation and Adaptive Layerwise Activation",
    "authors": [
      "Pranav Ramesh",
      "Gopalakrishnan Srinivasan"
    ],
    "abstract": "Spiking Neural Networks (SNNs) have been put forward as an energy-efficient\nalternative to Artificial Neural Networks (ANNs) since they perform sparse\nAccumulate operations instead of the power-hungry Multiply-and-Accumulate\noperations. ANN-SNN conversion is a widely used method to realize deep SNNs\nwith accuracy comparable to that of ANNs.~\\citeauthor{bu2023optimal} recently\nproposed the Quantization-Clip-Floor-Shift (QCFS) activation as an alternative\nto ReLU to minimize the accuracy loss during ANN-SNN conversion. Nevertheless,\nSNN inferencing requires a large number of timesteps to match the accuracy of\nthe source ANN for real-world datasets. In this work, we propose PASCAL, which\nperforms ANN-SNN conversion in such a way that the resulting SNN is\nmathematically equivalent to an ANN with QCFS-activation, thereby yielding\nsimilar accuracy as the source ANN with minimal inference timesteps. In\naddition, we propose a systematic method to configure the quantization step of\nQCFS activation in a layerwise manner, which effectively determines the optimal\nnumber of timesteps per layer for the converted SNN. Our results show that the\nResNet-34 SNN obtained using PASCAL achieves an accuracy of $\\approx$74\\% on\nImageNet with a 64$\\times$ reduction in the number of inference timesteps\ncompared to existing approaches.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.01730v1",
    "published_date": "2025-05-03 07:55:29 UTC",
    "updated_date": "2025-05-03 07:55:29 UTC"
  },
  {
    "arxiv_id": "2505.02863v2",
    "title": "Understanding University Students' Use of Generative AI: The Roles of Demographics and Personality Traits",
    "authors": [
      "Newnew Deng",
      "Edward Jiusi Liu",
      "Xiaoming Zhai"
    ],
    "abstract": "The use of generative AI (GAI) among university students is rapidly\nincreasing, yet empirical research on students' GAI use and the factors\ninfluencing it remains limited. To address this gap, we surveyed 363\nundergraduate and graduate students in the United States, examining their GAI\nusage and how it relates to demographic variables and personality traits based\non the Big Five model (i.e., extraversion, agreeableness, conscientiousness,\nand emotional stability, and intellect/imagination). Our findings reveal: (a)\nStudents in higher academic years are more inclined to use GAI and prefer it\nover traditional resources. (b) Non-native English speakers use and adopt GAI\nmore readily than native speakers. (c) Compared to White, Asian students report\nhigher GAI usage, perceive greater academic benefits, and express a stronger\npreference for it. Similarly, Black students report a more positive impact of\nGAI on their academic performance. Personality traits also play a significant\nrole in shaping perceptions and usage of GAI. After controlling demographic\nfactors, we found that personality still significantly predicts GAI use and\nattitudes: (a) Students with higher conscientiousness use GAI less. (b)\nStudents who are higher in agreeableness perceive a less positive impact of GAI\non academic performance and express more ethical concerns about using it for\nacademic work. (c) Students with higher emotional stability report a more\npositive impact of GAI on learning and fewer concerns about its academic use.\n(d) Students with higher extraversion show a stronger preference for GAI over\ntraditional resources. (e) Students with higher intellect/imagination tend to\nprefer traditional resources. These insights highlight the need for\nuniversities to provide personalized guidance to ensure students use GAI\neffectively, ethically, and equitably in their academic pursuits.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.02863v2",
    "published_date": "2025-05-03 06:58:29 UTC",
    "updated_date": "2025-05-19 21:44:16 UTC"
  },
  {
    "arxiv_id": "2505.03824v1",
    "title": "Memory Assisted LLM for Personalized Recommendation System",
    "authors": [
      "Jiarui Chen"
    ],
    "abstract": "Large language models (LLMs) have demonstrated significant potential in\nsolving recommendation tasks. With proven capabilities in understanding user\npreferences, LLM personalization has emerged as a critical area for providing\ntailored responses to individuals. Current studies explore personalization\nthrough prompt design and fine-tuning, paving the way for further research in\npersonalized LLMs. However, existing approaches are either costly and\ninefficient in capturing diverse user preferences or fail to account for timely\nupdates to user history. To address these gaps, we propose the Memory-Assisted\nPersonalized LLM (MAP). Through user interactions, we first create a history\nprofile for each user, capturing their preferences, such as ratings for\nhistorical items. During recommendation, we extract relevant memory based on\nsimilarity, which is then incorporated into the prompts to enhance personalized\nrecommendations. In our experiments, we evaluate MAP using a sequential rating\nprediction task under two scenarios: single domain, where memory and tasks are\nfrom the same category (e.g., movies), and cross-domain (e.g., memory from\nmovies and recommendation tasks in books). The results show that MAP\noutperforms regular LLM-based recommenders that integrate user history directly\nthrough prompt design. Moreover, as user history grows, MAP's advantage\nincreases in both scenarios, making it more suitable for addressing successive\npersonalized user requests.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "8 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.03824v1",
    "published_date": "2025-05-03 06:24:18 UTC",
    "updated_date": "2025-05-03 06:24:18 UTC"
  },
  {
    "arxiv_id": "2505.01712v1",
    "title": "World Model-Based Learning for Long-Term Age of Information Minimization in Vehicular Networks",
    "authors": [
      "Lingyi Wang",
      "Rashed Shelim",
      "Walid Saad",
      "Naren Ramakrishnan"
    ],
    "abstract": "Traditional reinforcement learning (RL)-based learning approaches for\nwireless networks rely on expensive trial-and-error mechanisms and real-time\nfeedback based on extensive environment interactions, which leads to low data\nefficiency and short-sighted policies. These limitations become particularly\nproblematic in complex, dynamic networks with high uncertainty and long-term\nplanning requirements. To address these limitations, in this paper, a novel\nworld model-based learning framework is proposed to minimize\npacket-completeness-aware age of information (CAoI) in a vehicular network.\nParticularly, a challenging representative scenario is considered pertaining to\na millimeter-wave (mmWave) vehicle-to-everything (V2X) communication network,\nwhich is characterized by high mobility, frequent signal blockages, and\nextremely short coherence time. Then, a world model framework is proposed to\njointly learn a dynamic model of the mmWave V2X environment and use it to\nimagine trajectories for learning how to perform link scheduling. In\nparticular, the long-term policy is learned in differentiable imagined\ntrajectories instead of environment interactions. Moreover, owing to its\nimagination abilities, the world model can jointly predict time-varying\nwireless data and optimize link scheduling in real-world wireless and V2X\nnetworks. Thus, during intervals without actual observations, the world model\nremains capable of making efficient decisions. Extensive experiments are\nperformed on a realistic simulator based on Sionna that integrates\nphysics-based end-to-end channel modeling, ray-tracing, and scene geometries\nwith material properties. Simulation results show that the proposed world model\nachieves a significant improvement in data efficiency, and achieves 26%\nimprovement and 16% improvement in CAoI, respectively, compared to the\nmodel-based RL (MBRL) method and the model-free RL (MFRL) method.",
    "categories": [
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.01712v1",
    "published_date": "2025-05-03 06:23:18 UTC",
    "updated_date": "2025-05-03 06:23:18 UTC"
  },
  {
    "arxiv_id": "2505.01709v2",
    "title": "RoBridge: A Hierarchical Architecture Bridging Cognition and Execution for General Robotic Manipulation",
    "authors": [
      "Kaidong Zhang",
      "Rongtao Xu",
      "Pengzhen Ren",
      "Junfan Lin",
      "Hefeng Wu",
      "Liang Lin",
      "Xiaodan Liang"
    ],
    "abstract": "Operating robots in open-ended scenarios with diverse tasks is a crucial\nresearch and application direction in robotics. While recent progress in\nnatural language processing and large multimodal models has enhanced robots'\nability to understand complex instructions, robot manipulation still faces the\nprocedural skill dilemma and the declarative skill dilemma in open\nenvironments. Existing methods often compromise cognitive and executive\ncapabilities. To address these challenges, in this paper, we propose RoBridge,\na hierarchical intelligent architecture for general robotic manipulation. It\nconsists of a high-level cognitive planner (HCP) based on a large-scale\npre-trained vision-language model (VLM), an invariant operable representation\n(IOR) serving as a symbolic bridge, and a generalist embodied agent (GEA).\nRoBridge maintains the declarative skill of VLM and unleashes the procedural\nskill of reinforcement learning, effectively bridging the gap between cognition\nand execution. RoBridge demonstrates significant performance improvements over\nexisting baselines, achieving a 75% success rate on new tasks and an 83%\naverage success rate in sim-to-real generalization using only five real-world\ndata samples per task. This work represents a significant step towards\nintegrating cognitive reasoning with physical execution in robotic systems,\noffering a new paradigm for general robotic manipulation.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "project page: https://abliao.github.io/RoBridge/",
    "pdf_url": "http://arxiv.org/pdf/2505.01709v2",
    "published_date": "2025-05-03 06:17:18 UTC",
    "updated_date": "2025-05-07 08:37:17 UTC"
  },
  {
    "arxiv_id": "2505.01706v1",
    "title": "Inducing Robustness in a 2 Dimensional Direct Preference Optimization Paradigm",
    "authors": [
      "Sarvesh Shashidhar",
      "Ritik",
      "Nachiketa Patil",
      "Suraj Racha",
      "Ganesh Ramakrishnan"
    ],
    "abstract": "Direct Preference Optimisation (DPO) has emerged as a powerful method for\naligning Large Language Models (LLMs) with human preferences, offering a stable\nand efficient alternative to approaches that use Reinforcement learning via\nHuman Feedback. In this work, we investigate the performance of DPO using\nopen-source preference datasets. One of the major drawbacks of DPO is that it\ndoesn't induce granular scoring and treats all the segments of the responses\nwith equal propensity. However, this is not practically true for human\npreferences since even \"good\" responses have segments that may not be preferred\nby the annotator. To resolve this, a 2-dimensional scoring for DPO alignment\ncalled 2D-DPO was proposed. We explore the 2D-DPO alignment paradigm and the\nadvantages it provides over the standard DPO by comparing their win rates. It\nis observed that these methods, even though effective, are not robust to\nlabel/score noise. To counter this, we propose an approach of incorporating\nsegment-level score noise robustness to the 2D-DPO algorithm. Along with\ntheoretical backing, we also provide empirical verification in favour of the\nalgorithm and introduce other noise models that can be present.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Updated abstract, algorithm and experimental results",
    "pdf_url": "http://arxiv.org/pdf/2505.01706v1",
    "published_date": "2025-05-03 05:59:13 UTC",
    "updated_date": "2025-05-03 05:59:13 UTC"
  },
  {
    "arxiv_id": "2505.02862v1",
    "title": "Cannot See the Forest for the Trees: Invoking Heuristics and Biases to Elicit Irrational Choices of LLMs",
    "authors": [
      "Haoming Yang",
      "Ke Ma",
      "Xiaojun Jia",
      "Yingfei Sun",
      "Qianqian Xu",
      "Qingming Huang"
    ],
    "abstract": "Despite the remarkable performance of Large Language Models (LLMs), they\nremain vulnerable to jailbreak attacks, which can compromise their safety\nmechanisms. Existing studies often rely on brute-force optimization or manual\ndesign, failing to uncover potential risks in real-world scenarios. To address\nthis, we propose a novel jailbreak attack framework, ICRT, inspired by\nheuristics and biases in human cognition. Leveraging the simplicity effect, we\nemploy cognitive decomposition to reduce the complexity of malicious prompts.\nSimultaneously, relevance bias is utilized to reorganize prompts, enhancing\nsemantic alignment and inducing harmful outputs effectively. Furthermore, we\nintroduce a ranking-based harmfulness evaluation metric that surpasses the\ntraditional binary success-or-failure paradigm by employing ranking aggregation\nmethods such as Elo, HodgeRank, and Rank Centrality to comprehensively quantify\nthe harmfulness of generated content. Experimental results show that our\napproach consistently bypasses mainstream LLMs' safety mechanisms and generates\nhigh-risk content, providing insights into jailbreak attack risks and\ncontributing to stronger defense strategies.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.02862v1",
    "published_date": "2025-05-03 05:28:11 UTC",
    "updated_date": "2025-05-03 05:28:11 UTC"
  },
  {
    "arxiv_id": "2505.01699v1",
    "title": "Component-Based Fairness in Face Attribute Classification with Bayesian Network-informed Meta Learning",
    "authors": [
      "Yifan Liu",
      "Ruichen Yao",
      "Yaokun Liu",
      "Ruohan Zong",
      "Zelin Li",
      "Yang Zhang",
      "Dong Wang"
    ],
    "abstract": "The widespread integration of face recognition technologies into various\napplications (e.g., access control and personalized advertising) necessitates a\ncritical emphasis on fairness. While previous efforts have focused on\ndemographic fairness, the fairness of individual biological face components\nremains unexplored. In this paper, we focus on face component fairness, a\nfairness notion defined by biological face features. To our best knowledge, our\nwork is the first work to mitigate bias of face attribute prediction at the\nbiological feature level. In this work, we identify two key challenges in\noptimizing face component fairness: attribute label scarcity and attribute\ninter-dependencies, both of which limit the effectiveness of bias mitigation\nfrom previous approaches. To address these issues, we propose \\textbf{B}ayesian\n\\textbf{N}etwork-informed \\textbf{M}eta \\textbf{R}eweighting (BNMR), which\nincorporates a Bayesian Network calibrator to guide an adaptive\nmeta-learning-based sample reweighting process. During the training process of\nour approach, the Bayesian Network calibrator dynamically tracks model bias and\nencodes prior probabilities for face component attributes to overcome the above\nchallenges. To demonstrate the efficacy of our approach, we conduct extensive\nexperiments on a large-scale real-world human face dataset. Our results show\nthat BNMR is able to consistently outperform recent face bias mitigation\nbaselines. Moreover, our results suggest a positive impact of face component\nfairness on the commonly considered demographic fairness (e.g.,\n\\textit{gender}). Our findings pave the way for new research avenues on face\ncomponent fairness, suggesting that face component fairness could serve as a\npotential surrogate objective for demographic fairness. The code for our work\nis publicly\navailable~\\footnote{https://github.com/yliuaa/BNMR-FairCompFace.git}.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.2.10; K.4.1"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ACM FAccT 2025",
    "pdf_url": "http://arxiv.org/pdf/2505.01699v1",
    "published_date": "2025-05-03 05:26:29 UTC",
    "updated_date": "2025-05-03 05:26:29 UTC"
  },
  {
    "arxiv_id": "2505.01696v1",
    "title": "Interpretable graph-based models on multimodal biomedical data integration: A technical review and benchmarking",
    "authors": [
      "Alireza Sadeghi",
      "Farshid Hajati",
      "Ahmadreza Argha",
      "Nigel H Lovell",
      "Min Yang",
      "Hamid Alinejad-Rokny"
    ],
    "abstract": "Integrating heterogeneous biomedical data including imaging, omics, and\nclinical records supports accurate diagnosis and personalised care. Graph-based\nmodels fuse such non-Euclidean data by capturing spatial and relational\nstructure, yet clinical uptake requires regulator-ready interpretability. We\npresent the first technical survey of interpretable graph based models for\nmultimodal biomedical data, covering 26 studies published between Jan 2019 and\nSep 2024. Most target disease classification, notably cancer and rely on static\ngraphs from simple similarity measures, while graph-native explainers are rare;\npost-hoc methods adapted from non-graph domains such as gradient saliency, and\nSHAP predominate. We group existing approaches into four interpretability\nfamilies, outline trends such as graph-in-graph hierarchies, knowledge-graph\nedges, and dynamic topology learning, and perform a practical benchmark. Using\nan Alzheimer disease cohort, we compare Sensitivity Analysis, Gradient\nSaliency, SHAP and Graph Masking. SHAP and Sensitivity Analysis recover the\nbroadest set of known AD pathways and Gene-Ontology terms, whereas Gradient\nSaliency and Graph Masking surface complementary metabolic and transport\nsignatures. Permutation tests show all four beat random gene sets, but with\ndistinct trade-offs: SHAP and Graph Masking offer deeper biology at higher\ncompute cost, while Gradient Saliency and Sensitivity Analysis are quicker\nthough coarser. We also provide a step-by-step flowchart covering graph\nconstruction, explainer choice and resource budgeting to help researchers\nbalance transparency and performance. This review synthesises the state of\ninterpretable graph learning for multimodal medicine, benchmarks leading\ntechniques, and charts future directions, from advanced XAI tools to\nunder-studied diseases, serving as a concise reference for method developers\nand translational scientists.",
    "categories": [
      "q-bio.GN",
      "cs.AI"
    ],
    "primary_category": "q-bio.GN",
    "comment": "41 pages",
    "pdf_url": "http://arxiv.org/pdf/2505.01696v1",
    "published_date": "2025-05-03 05:00:38 UTC",
    "updated_date": "2025-05-03 05:00:38 UTC"
  },
  {
    "arxiv_id": "2505.01694v1",
    "title": "Topology-Aware CLIP Few-Shot Learning",
    "authors": [
      "Dazhi Huang"
    ],
    "abstract": "Efficiently adapting large Vision-Language Models (VLMs) like CLIP for\nfew-shot learning poses challenges in balancing pre-trained knowledge retention\nand task-specific adaptation. Existing methods often overlook valuable\nstructural information within the VLM's latent space. We introduce a\ntopology-aware tuning approach integrating Representation Topology Divergence\n(RTD) into the Task Residual (TR) framework. By explicitly aligning the\ntopological structures of visual and text representations using a combined RTD\nand Cross-Entropy loss, while freezing base VLM encoders, our method enhances\nfew-shot performance. We optimize only lightweight Task Residual parameters,\neffectively leveraging topological information. Across 6 diverse benchmark\ndatasets, our approach demonstrates significant gains, achieving an average\naccuracy improvement of 1-2\\% over relevant baseline methods in few-shot\nsettings. This work presents an effective strategy to boost VLM few-shot\ncapabilities by incorporating topological alignment.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.01694v1",
    "published_date": "2025-05-03 04:58:29 UTC",
    "updated_date": "2025-05-03 04:58:29 UTC"
  },
  {
    "arxiv_id": "2505.01680v1",
    "title": "Automated ARAT Scoring Using Multimodal Video Analysis, Multi-View Fusion, and Hierarchical Bayesian Models: A Clinician Study",
    "authors": [
      "Tamim Ahmed",
      "Thanassis Rikakis"
    ],
    "abstract": "Manual scoring of the Action Research Arm Test (ARAT) for upper extremity\nassessment in stroke rehabilitation is time-intensive and variable. We propose\nan automated ARAT scoring system integrating multimodal video analysis with\nSlowFast, I3D, and Transformer-based models using OpenPose keypoints and object\nlocations. Our approach employs multi-view data (ipsilateral, contralateral,\nand top perspectives), applying early and late fusion to combine features\nacross views and models. Hierarchical Bayesian Models (HBMs) infer movement\nquality components, enhancing interpretability. A clinician dashboard displays\ntask scores, execution times, and quality assessments. We conducted a study\nwith five clinicians who reviewed 500 video ratings generated by our system,\nproviding feedback on its accuracy and usability. Evaluated on a stroke\nrehabilitation dataset, our framework achieves 89.0% validation accuracy with\nlate fusion, with HBMs aligning closely with manual assessments. This work\nadvances automated rehabilitation by offering a scalable, interpretable\nsolution with clinical validation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC",
      "math.PR"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.01680v1",
    "published_date": "2025-05-03 04:00:51 UTC",
    "updated_date": "2025-05-03 04:00:51 UTC"
  },
  {
    "arxiv_id": "2505.01664v1",
    "title": "Soft-Masked Semi-Dual Optimal Transport for Partial Domain Adaptation",
    "authors": [
      "Yi-Ming Zhai",
      "Chuan-Xian Ren",
      "Hong Yan"
    ],
    "abstract": "Visual domain adaptation aims to learn discriminative and domain-invariant\nrepresentation for an unlabeled target domain by leveraging knowledge from a\nlabeled source domain. Partial domain adaptation (PDA) is a general and\npractical scenario in which the target label space is a subset of the source\none. The challenges of PDA exist due to not only domain shift but also the\nnon-identical label spaces of domains. In this paper, a Soft-masked Semi-dual\nOptimal Transport (SSOT) method is proposed to deal with the PDA problem.\nSpecifically, the class weights of domains are estimated, and then a reweighed\nsource domain is constructed, which is favorable in conducting\nclass-conditional distribution matching with the target domain. A soft-masked\ntransport distance matrix is constructed by category predictions, which will\nenhance the class-oriented representation ability of optimal transport in the\nshared feature space. To deal with large-scale optimal transport problems, the\nsemi-dual formulation of the entropy-regularized Kantorovich problem is\nemployed since it can be optimized by gradient-based algorithms. Further, a\nneural network is exploited to approximate the Kantorovich potential due to its\nstrong fitting ability. This network parametrization also allows the\ngeneralization of the dual variable outside the supports of the input\ndistribution. The SSOT model is built upon neural networks, which can be\noptimized alternately in an end-to-end manner. Extensive experiments are\nconducted on four benchmark datasets to demonstrate the effectiveness of SSOT.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.01664v1",
    "published_date": "2025-05-03 03:20:17 UTC",
    "updated_date": "2025-05-03 03:20:17 UTC"
  },
  {
    "arxiv_id": "2505.02861v1",
    "title": "Neural Orchestration for Multi-Agent Systems: A Deep Learning Framework for Optimal Agent Selection in Multi-Domain Task Environments",
    "authors": [
      "Kushagra Agrawal",
      "Nisharg Nargund"
    ],
    "abstract": "Multi-agent systems (MAS) are foundational in simulating complex real-world\nscenarios involving autonomous, interacting entities. However, traditional MAS\narchitectures often suffer from rigid coordination mechanisms and difficulty\nadapting to dynamic tasks. We propose MetaOrch, a neural orchestration\nframework for optimal agent selection in multi-domain task environments. Our\nsystem implements a supervised learning approach that models task context,\nagent histories, and expected response quality to select the most appropriate\nagent for each task. A novel fuzzy evaluation module scores agent responses\nalong completeness, relevance, and confidence dimensions, generating soft\nsupervision labels for training the orchestrator. Unlike previous methods that\nhard-code agent-task mappings, MetaOrch dynamically predicts the most suitable\nagent while estimating selection confidence. Experiments in simulated\nenvironments with heterogeneous agents demonstrate that our approach achieves\n86.3% selection accuracy, significantly outperforming baseline strategies\nincluding random selection and round-robin scheduling. The modular architecture\nemphasizes extensibility, allowing agents to be registered, updated, and\nqueried independently. Results suggest that neural orchestration offers a\npowerful approach to enhancing the autonomy, interpretability, and adaptability\nof multi-agent systems across diverse task domains.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.02861v1",
    "published_date": "2025-05-03 02:58:25 UTC",
    "updated_date": "2025-05-03 02:58:25 UTC"
  },
  {
    "arxiv_id": "2505.03822v1",
    "title": "DRSLF: Double Regularized Second-Order Low-Rank Representation for Web Service QoS Prediction",
    "authors": [
      "Hao Wu",
      "Jialiang Wang"
    ],
    "abstract": "Quality-of-Service (QoS) data plays a crucial role in cloud service\nselection. Since users cannot access all services, QoS can be represented by a\nhigh-dimensional and incomplete (HDI) matrix. Latent factor analysis (LFA)\nmodels have been proven effective as low-rank representation techniques for\naddressing this issue. However, most LFA models rely on first-order optimizers\nand use L2-norm regularization, which can lead to lower QoS prediction\naccuracy. To address this issue, this paper proposes a double regularized\nsecond-order latent factor (DRSLF) model with two key ideas: a) integrating\nL1-norm and L2-norm regularization terms to enhance the low-rank representation\nperformance; b) incorporating second-order information by calculating the\nHessian-vector product in each conjugate gradient step. Experimental results on\ntwo real-world response-time QoS datasets demonstrate that DRSLF has a higher\nlow-rank representation capability than two baselines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.03822v1",
    "published_date": "2025-05-03 02:25:05 UTC",
    "updated_date": "2025-05-03 02:25:05 UTC"
  },
  {
    "arxiv_id": "2505.01652v1",
    "title": "Causally Fair Node Classification on Non-IID Graph Data",
    "authors": [
      "Yucong Dai",
      "Lu Zhang",
      "Yaowei Hu",
      "Susan Gauch",
      "Yongkai Wu"
    ],
    "abstract": "Fair machine learning seeks to identify and mitigate biases in predictions\nagainst unfavorable populations characterized by demographic attributes, such\nas race and gender. Recently, a few works have extended fairness to graph data,\nsuch as social networks, but most of them neglect the causal relationships\namong data instances. This paper addresses the prevalent challenge in\nfairness-aware ML algorithms, which typically assume Independent and\nIdentically Distributed (IID) data. We tackle the overlooked domain of non-IID,\ngraph-based settings where data instances are interconnected, influencing the\noutcomes of fairness interventions. We base our research on the Network\nStructural Causal Model (NSCM) framework and posit two main assumptions:\nDecomposability and Graph Independence, which enable the computation of\ninterventional distributions in non-IID settings using the $do$-calculus. Based\non that, we develop the Message Passing Variational Autoencoder for Causal\nInference (MPVA) to compute interventional distributions and facilitate\ncausally fair node classification through estimated interventional\ndistributions. Empirical evaluations on semi-synthetic and real-world datasets\ndemonstrate that MPVA outperforms conventional methods by effectively\napproximating interventional distributions and mitigating bias. The\nimplications of our findings underscore the potential of causality-based\nfairness in complex ML applications, setting the stage for further research\ninto relaxing the initial assumptions to enhance model fairness.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.01652v1",
    "published_date": "2025-05-03 02:05:51 UTC",
    "updated_date": "2025-05-03 02:05:51 UTC"
  },
  {
    "arxiv_id": "2505.01651v1",
    "title": "Human-AI Governance (HAIG): A Trust-Utility Approach",
    "authors": [
      "Zeynep Engin"
    ],
    "abstract": "This paper introduces the HAIG framework for analysing trust dynamics across\nevolving human-AI relationships. Current categorical frameworks (e.g.,\n\"human-in-the-loop\" models) inadequately capture how AI systems evolve from\ntools to partners, particularly as foundation models demonstrate emergent\ncapabilities and multi-agent systems exhibit autonomous goal-setting\nbehaviours. As systems advance, agency redistributes in complex patterns that\nare better represented as positions along continua rather than discrete\ncategories, though progression may include both gradual shifts and significant\nstep changes. The HAIG framework operates across three levels: dimensions\n(Decision Authority Distribution, Process Autonomy, and Accountability\nConfiguration), continua (gradual shifts along each dimension), and thresholds\n(critical points requiring governance adaptation). Unlike risk-based or\nprinciple-based approaches, HAIG adopts a trust-utility orientation, focusing\non maintaining appropriate trust relationships that maximise utility while\nensuring sufficient safeguards. Our analysis reveals how technical advances in\nself-supervision, reasoning authority, and distributed decision-making drive\nnon-uniform trust evolution across both contextual variation and technological\nadvancement. Case studies in healthcare and European regulation demonstrate how\nHAIG complements existing frameworks while offering a foundation for\nalternative approaches that anticipate governance challenges before they\nemerge.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.MA",
      "cs.SI"
    ],
    "primary_category": "cs.AI",
    "comment": "32 pages including references and appendix, 25 pages core text, 3\n  figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2505.01651v1",
    "published_date": "2025-05-03 01:57:08 UTC",
    "updated_date": "2025-05-03 01:57:08 UTC"
  },
  {
    "arxiv_id": "2505.01647v1",
    "title": "Scalable Speed-ups for the SMS-EMOA from a Simple Aging Strategy",
    "authors": [
      "Mingfeng Li",
      "Weijie Zheng",
      "Benjamin Doerr"
    ],
    "abstract": "Different from single-objective evolutionary algorithms, where non-elitism is\nan established concept, multi-objective evolutionary algorithms almost always\nselect the next population in a greedy fashion. In the only notable exception,\nBian, Zhou, Li, and Qian (IJCAI 2023) proposed a stochastic selection mechanism\nfor the SMS-EMOA and proved that it can speed up computing the Pareto front of\nthe bi-objective jump benchmark with problem size $n$ and gap parameter $k$ by\na factor of $\\max\\{1,2^{k/4}/n\\}$. While this constitutes the first proven\nspeed-up from non-elitist selection, suggesting a very interesting research\ndirection, it has to be noted that a true speed-up only occurs for $k \\ge\n4\\log_2(n)$, where the runtime is super-polynomial, and that the advantage\nreduces for larger numbers of objectives as shown in a later work. In this\nwork, we propose a different non-elitist selection mechanism based on aging,\nwhich exempts individuals younger than a certain age from a possible removal.\nThis remedies the two shortcomings of stochastic selection: We prove a speed-up\nby a factor of $\\max\\{1,\\Theta(k)^{k-1}\\}$, regardless of the number of\nobjectives. In particular, a positive speed-up can already be observed for\nconstant $k$, the only setting for which polynomial runtimes can be witnessed.\nOverall, this result supports the use of non-elitist selection schemes, but\nsuggests that aging-based mechanisms can be considerably more powerful than\nstochastic selection mechanisms.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "Initial version of one paper accepted by IJCAI2025",
    "pdf_url": "http://arxiv.org/pdf/2505.01647v1",
    "published_date": "2025-05-03 01:34:51 UTC",
    "updated_date": "2025-05-03 01:34:51 UTC"
  },
  {
    "arxiv_id": "2505.01638v1",
    "title": "Seeing Heat with Color -- RGB-Only Wildfire Temperature Inference from SAM-Guided Multimodal Distillation using Radiometric Ground Truth",
    "authors": [
      "Michael Marinaccio",
      "Fatemeh Afghah"
    ],
    "abstract": "High-fidelity wildfire monitoring using Unmanned Aerial Vehicles (UAVs)\ntypically requires multimodal sensing - especially RGB and thermal imagery -\nwhich increases hardware cost and power consumption. This paper introduces\nSAM-TIFF, a novel teacher-student distillation framework for pixel-level\nwildfire temperature prediction and segmentation using RGB input only. A\nmultimodal teacher network trained on paired RGB-Thermal imagery and\nradiometric TIFF ground truth distills knowledge to a unimodal RGB student\nnetwork, enabling thermal-sensor-free inference. Segmentation supervision is\ngenerated using a hybrid approach of segment anything (SAM)-guided mask\ngeneration, and selection via TOPSIS, along with Canny edge detection and\nOtsu's thresholding pipeline for automatic point prompt selection. Our method\nis the first to perform per-pixel temperature regression from RGB UAV data,\ndemonstrating strong generalization on the recent FLAME 3 dataset. This work\nlays the foundation for lightweight, cost-effective UAV-based wildfire\nmonitoring systems without thermal sensors.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "I.4.6; I.4.8"
    ],
    "primary_category": "eess.IV",
    "comment": "7 pages, 4 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2505.01638v1",
    "published_date": "2025-05-03 00:23:11 UTC",
    "updated_date": "2025-05-03 00:23:11 UTC"
  },
  {
    "arxiv_id": "2505.03821v1",
    "title": "Beyond Recognition: Evaluating Visual Perspective Taking in Vision Language Models",
    "authors": [
      "Gracjan Góral",
      "Alicja Ziarko",
      "Piotr Miłoś",
      "Michał Nauman",
      "Maciej Wołczyk",
      "Michał Kosiński"
    ],
    "abstract": "We investigate the ability of Vision Language Models (VLMs) to perform visual\nperspective taking using a novel set of visual tasks inspired by established\nhuman tests. Our approach leverages carefully controlled scenes, in which a\nsingle humanoid minifigure is paired with a single object. By systematically\nvarying spatial configurations - such as object position relative to the\nhumanoid minifigure and the humanoid minifigure's orientation - and using both\nbird's-eye and surface-level views, we created 144 unique visual tasks. Each\nvisual task is paired with a series of 7 diagnostic questions designed to\nassess three levels of visual cognition: scene understanding, spatial\nreasoning, and visual perspective taking. Our evaluation of several\nstate-of-the-art models, including GPT-4-Turbo, GPT-4o,\nLlama-3.2-11B-Vision-Instruct, and variants of Claude Sonnet, reveals that\nwhile they excel in scene understanding, the performance declines significantly\non spatial reasoning and further deteriorates on perspective-taking. Our\nanalysis suggests a gap between surface-level object recognition and the deeper\nspatial and perspective reasoning required for complex visual tasks, pointing\nto the need for integrating explicit geometric representations and tailored\ntraining protocols in future VLM development.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Dataset:\n  https://huggingface.co/datasets/Gracjan/Isle/viewer/Isle-Brick-V2",
    "pdf_url": "http://arxiv.org/pdf/2505.03821v1",
    "published_date": "2025-05-03 00:10:41 UTC",
    "updated_date": "2025-05-03 00:10:41 UTC"
  },
  {
    "arxiv_id": "2505.01636v1",
    "title": "Structured Prompting and Feedback-Guided Reasoning with LLMs for Data Interpretation",
    "authors": [
      "Amit Rath"
    ],
    "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in\nnatural language understanding and task generalization. However, their\napplication to structured data analysis remains fragile due to inconsistencies\nin schema interpretation, misalignment between user intent and model output,\nand limited mechanisms for self-correction when failures occur. This paper\nintroduces the STROT Framework (Structured Task Reasoning and Output\nTransformation), a method for structured prompting and feedback-driven\ntransformation logic generation aimed at improving the reliability and semantic\nalignment of LLM-based analytical workflows. STROT begins with lightweight\nschema introspection and sample-based field classification, enabling dynamic\ncontext construction that captures both the structure and statistical profile\nof the input data. This contextual information is embedded in structured\nprompts that guide the model toward generating task-specific, interpretable\noutputs. To address common failure modes in complex queries, STROT incorporates\na refinement mechanism in which the model iteratively revises its outputs based\non execution feedback and validation signals. Unlike conventional approaches\nthat rely on static prompts or single-shot inference, STROT treats the LLM as a\nreasoning agent embedded within a controlled analysis loop -- capable of\nadjusting its output trajectory through planning and correction. The result is\na robust and reproducible framework for reasoning over structured data with\nLLMs, applicable to diverse data exploration and analysis tasks where\ninterpretability, stability, and correctness are essential.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "I.2.7; H.2.8; D.2.13"
    ],
    "primary_category": "cs.AI",
    "comment": "21 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.01636v1",
    "published_date": "2025-05-03 00:05:01 UTC",
    "updated_date": "2025-05-03 00:05:01 UTC"
  }
]
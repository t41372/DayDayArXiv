{
  "date": "2025-01-26",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-01-26 的 arXiv 中文 TLDR 快报！今天 arXiv 更新了 61 篇论文，主要聚焦 AI 模型优化、扩散生成技术、多模态融合以及医疗应用等领域，其中 Gabriel Stanovsky 等学者的 AI 监管分析和扩散模型创新（如 StochSync 和 Visual Generation Without Guidance）最为令人印象深刻，突显了 AI 在安全、生成和实际应用中的潜力。\n\n### 重点论文亮点\n我们挑选了今日最具话题度和影响力的论文，先从 AI 安全与监管入手，再聊生成模型和多模态应用，最后快速概述其他相关内容。以下按主题归类讨论，每篇突出核心贡献。\n\n#### AI 安全与监管\n- **Beyond Benchmarks: On The False Promise of AI Regulation** (中文：超越基准：AI 监管的虚假承诺；英文：Beyond Benchmarks: On The False Promise of AI Regulation)  \n  Gabriel Stanovsky 等学者指出，现有 AI 监管框架依赖基准测试（如车辆碰撞测试）来验证安全，但深度学习模型的统计模式无法提供可靠的因果保证。主要贡献是提出一个双层监管框架，强调高风险应用需人类监督，并呼吁重新审视 AI 安全假设，这对政策制定有重要启发。\n\n- **HardML: A Benchmark For Evaluating Data Science And Machine Learning knowledge and reasoning in AI** (中文：HardML：评估 AI 在数据科学和机器学习知识与推理的基准；英文：HardML: A Benchmark For Evaluating Data Science And Machine Learning knowledge and reasoning in AI)  \n  这篇论文引入一个 100 题挑战性基准，针对数据科学和机器学习的 AI 推理能力。关键发现是当前 AI 模型在该基准上的错误率高达 30%，远高于标准基准，突显了 AI 在专业领域推理的局限性。\n\n#### 生成模型与优化\n- **StagFormer: Time Staggering Transformer Decoding for RunningLayers In Parallel** (中文：StagFormer：通过时间交错解码并行运行 Transformer 层；英文：StagFormer: Time Staggering Transformer Decoding for RunningLayers In Parallel)  \n  该框架通过时间轴交错执行 Transformer 层，实现解码过程的并行化，带来 33% 的速度提升，同时保持性能不变。主要贡献是解决传统 Transformer 顺序依赖问题，适用于高效语言生成任务。\n\n- **Visual Generation Without Guidance** (中文：无指导的视觉生成；英文：Visual Generation Without Guidance)  \n  这篇论文提出 Guidance-Free Training (GFT) 方法，优化扩散模型的生成过程，无需条件指导即可实现高质量图像生成。发现 GFT 在不降低保真度的前提下，减少计算成本一半，适用于图像生成任务。\n\n- **StochSync: Stochastic Diffusion Synchronization for Image Generation in Arbitrary Spaces** (中文：StochSync：用于任意空间图像生成的随机扩散同步；英文：StochSync: Stochastic Diffusion Synchronization for Image Generation in Arbitrary Spaces)  \n  论文创新性地结合扩散模型和同步机制，实现零样本图像生成，如 360° 全景或网格纹理。关键发现是它在弱条件环境下表现优异，显著提升生成质量和适应性。\n\n- **Complete Chess Games Enable LLM Become A Chess Master** (中文：完整棋局数据让 LLM 成为国际象棋大师；英文：Complete Chess Games Enable LLM Become A Chess Master)  \n  通过监督微调 LLM，使用完整棋局数据，该模型在国际象棋任务中达到 1788 Elo 分。主要贡献是证明高质量数据（如长局面训练）能提升 LLM 在抽象游戏中的推理能力，Elo 提升达 350 分。\n\n#### 多模态与医疗应用\n- **Diffusion Generative Modeling for Spatially Resolved Gene Expression Inference from Histology Images** (中文：基于扩散生成模型的组织学图像空间分辨基因表达推断；英文：Diffusion Generative Modeling for Spatially Resolved Gene Expression Inference from Histology Images)  \n  论文提出 Stem 框架，使用扩散模型从 H&E 染色图像推断基因表达，实现了高保真预测。主要发现是它在多种数据集上优于现有方法，潜在应用于癌症诊断。\n\n- **AnyEnhance: A Unified Generative Model with Prompt-Guidance and Self-Critic for Voice Enhancement** (中文：AnyEnhance：基于提示引导和自批评的统一语音增强生成模型；英文：AnyEnhance: A Unified Generative Model with Prompt-Guidance and Self-Critic for Voice Enhancement)  \n  这个模型支持语音和歌声增强任务，包括去噪和提取特定说话者。核心贡献是自批评机制迭代优化输出，提升生成质量，在主观测试中表现突出。\n\n- **Overview of the Amphion Toolkit (v0.2)** (中文：Amphion 工具包 v0.2 概述；英文：Overview of the Amphion Toolkit (v0.2))  \n  Zhizheng Wu 等学者更新了这个音频生成工具包，新增 10 万小时多语数据集和模型。关键是它简化了音频生成任务的入门门槛，支持文本到语音等应用。\n\n#### 其他快速掠过\n剩余论文中，一些如 **Transformer-Based Multimodal Knowledge Graph Completion with Link-Aware Contexts** (中文：基于 Transformer 的多模态知识图谱补全；英文：Transformer-Based Multimodal Knowledge Graph Completion with Link-Aware Contexts) 优化了知识图谱补全，但影响较小；**People who frequently use ChatGPT for writing tasks are accurate and robust detectors of AI-generated text** (中文：频繁使用 ChatGPT 写作的人是 AI 生成文本的准确检测者；英文：People who frequently use ChatGPT for writing tasks are accurate and robust detectors of AI-generated text) 发现专家能有效检测 AI 文本，但这更适合学术讨论而非即时应用。其他如区块链或游戏相关论文（如 UNIDOOR），虽有创新，但非主流话题，故从简。\n\n总之，今天的更新强调了 AI 的实用性和挑战，建议关注扩散模型和监管领域的进展，以推动更安全可靠的 AI 应用。明天的快报见！",
  "papers": [
    {
      "arxiv_id": "2501.15695v1",
      "title": "Contextual Knowledge Sharing in Multi-Agent Reinforcement Learning with Decentralized Communication and Coordination",
      "title_zh": "翻译失败",
      "authors": [
        "Hung Du",
        "Srikanth Thudumu",
        "Hy Nguyen",
        "Rajesh Vasa",
        "Kon Mouzakis"
      ],
      "abstract": "Decentralized Multi-Agent Reinforcement Learning (Dec-MARL) has emerged as a\npivotal approach for addressing complex tasks in dynamic environments. Existing\nMulti-Agent Reinforcement Learning (MARL) methodologies typically assume a\nshared objective among agents and rely on centralized control. However, many\nreal-world scenarios feature agents with individual goals and limited\nobservability of other agents, complicating coordination and hindering\nadaptability. Existing Dec-MARL strategies prioritize either communication or\ncoordination, lacking an integrated approach that leverages both. This paper\npresents a novel Dec-MARL framework that integrates peer-to-peer communication\nand coordination, incorporating goal-awareness and time-awareness into the\nagents' knowledge-sharing processes. Our framework equips agents with the\nability to (i) share contextually relevant knowledge to assist other agents,\nand (ii) reason based on information acquired from multiple agents, while\nconsidering their own goals and the temporal context of prior knowledge. We\nevaluate our approach through several complex multi-agent tasks in environments\nwith dynamically appearing obstacles. Our work demonstrates that incorporating\ngoal-aware and time-aware knowledge sharing significantly enhances overall\nperformance.",
      "tldr_zh": "这篇论文针对 Decentralized Multi-Agent Reinforcement Learning (Dec-MARL) 的挑战，提出一个新框架，该框架整合点对点通信和协调机制，允许代理在考虑自身目标和时间上下文的情况下共享相关知识并进行推理。现有方法通常忽略代理的独立目标和有限可观察性，而本框架通过加入目标意识和时间意识，增强代理间的知识共享和决策过程。实验结果显示，在动态障碍环境中，该方法显著提升了整体性能，证明了其在复杂多代理任务中的有效性。",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.15695v1",
      "published_date": "2025-01-26 22:49:50 UTC",
      "updated_date": "2025-01-26 22:49:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:23:54.846994"
    },
    {
      "arxiv_id": "2501.15693v1",
      "title": "Beyond Benchmarks: On The False Promise of AI Regulation",
      "title_zh": "翻译失败",
      "authors": [
        "Gabriel Stanovsky",
        "Renana Keydar",
        "Gadi Perl",
        "Eliya Habba"
      ],
      "abstract": "The rapid advancement of artificial intelligence (AI) systems in critical\ndomains like healthcare, justice, and social services has sparked numerous\nregulatory initiatives aimed at ensuring their safe deployment. Current\nregulatory frameworks, exemplified by recent US and EU efforts, primarily focus\non procedural guidelines while presuming that scientific benchmarking can\neffectively validate AI safety, similar to how crash tests verify vehicle\nsafety or clinical trials validate drug efficacy. However, this approach\nfundamentally misunderstands the unique technical challenges posed by modern AI\nsystems. Through systematic analysis of successful technology regulation case\nstudies, we demonstrate that effective scientific regulation requires a causal\ntheory linking observable test outcomes to future performance - for instance,\nhow a vehicle's crash resistance at one speed predicts its safety at lower\nspeeds. We show that deep learning models, which learn complex statistical\npatterns from training data without explicit causal mechanisms, preclude such\nguarantees. This limitation renders traditional regulatory approaches\ninadequate for ensuring AI safety. Moving forward, we call for regulators to\nreckon with this limitation, and propose a preliminary two-tiered regulatory\nframework that acknowledges these constraints: mandating human oversight for\nhigh-risk applications while developing appropriate risk communication\nstrategies for lower-risk uses. Our findings highlight the urgent need to\nreconsider fundamental assumptions in AI regulation and suggest a concrete path\nforward for policymakers and researchers.",
      "tldr_zh": "该论文质疑了依赖基准测试来验证 AI 安全的监管方法，指出当前框架（如美国和欧盟的努力）将 AI 视为类似汽车碰撞测试或药物临床试验的场景，但忽略了深度学习模型学习复杂统计模式而非显式因果机制的独特挑战。通过分析成功技术监管案例，作者证明了这种方法无法提供可靠的性能保证，从而使传统 AI 监管无效。论文提出一个初步的两层监管框架：对高风险应用强制人类 oversight，并为低风险应用开发风险 communication 策略。最终，它呼吁政策制定者和研究者重新审视 AI 监管的基本假设，并提供具体路径以确保更有效的安全措施。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.15693v1",
      "published_date": "2025-01-26 22:43:07 UTC",
      "updated_date": "2025-01-26 22:43:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:26:07.626887"
    },
    {
      "arxiv_id": "2501.15688v1",
      "title": "Transformer-Based Multimodal Knowledge Graph Completion with Link-Aware Contexts",
      "title_zh": "翻译失败",
      "authors": [
        "Haodi Ma",
        "Dzmitry Kasinets",
        "Daisy Zhe Wang"
      ],
      "abstract": "Multimodal knowledge graph completion (MMKGC) aims to predict missing links\nin multimodal knowledge graphs (MMKGs) by leveraging information from various\nmodalities alongside structural data. Existing MMKGC approaches primarily\nextend traditional knowledge graph embedding (KGE) models, which often require\ncreating an embedding for every entity. This results in large model sizes and\ninefficiencies in integrating multimodal information, particularly for\nreal-world graphs. Meanwhile, Transformer-based models have demonstrated\ncompetitive performance in knowledge graph completion (KGC). However, their\nfocus on single-modal knowledge limits their capacity to utilize cross-modal\ninformation. Recently, Large vision-language models (VLMs) have shown potential\nin cross-modal tasks but are constrained by the high cost of training. In this\nwork, we propose a novel approach that integrates Transformer-based KGE models\nwith cross-modal context generated by pre-trained VLMs, thereby extending their\napplicability to MMKGC. Specifically, we employ a pre-trained VLM to transform\nrelevant visual information from entities and their neighbors into textual\nsequences. We then frame KGC as a sequence-to-sequence task, fine-tuning the\nmodel with the generated cross-modal context. This simple yet effective method\nsignificantly reduces model size compared to traditional KGE approaches while\nachieving competitive performance across multiple large-scale datasets with\nminimal hyperparameter tuning.",
      "tldr_zh": "该研究针对多模态知识图谱完成(MMKGC)的挑战，提出了一种基于Transformer的框架，通过整合预训练视觉语言模型(VLMs)生成的跨模态上下文来预测缺失链接，从而提升对多模态信息的利用效率。方法包括使用VLMs将实体和邻居的视觉信息转化为文本序列，并将知识图谱完成(KGC)任务框架化为序列到序列任务进行微调，这显著降低了模型大小。实验结果显示，该方法在多个大型数据集上取得了与传统KGE模型相当的性能，仅需最小超参数调整，为MMKGC提供了更高效的解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.15688v1",
      "published_date": "2025-01-26 22:23:14 UTC",
      "updated_date": "2025-01-26 22:23:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:24:19.487439"
    },
    {
      "arxiv_id": "2501.15678v1",
      "title": "Blissful (A)Ignorance: People form overly positive impressions of others based on their written messages, despite wide-scale adoption of Generative AI",
      "title_zh": "翻译失败",
      "authors": [
        "Jiaqi Zhu",
        "Andras Molnar"
      ],
      "abstract": "As the use of Generative AI (GenAI) tools becomes more prevalent in\ninterpersonal communication, understanding their impact on social perceptions\nis crucial. According to signaling theory, GenAI may undermine the credibility\nof social signals conveyed in writing, since it reduces the cost of writing and\nmakes it hard to verify the authenticity of messages. Using a pre-registered\nlarge-scale online experiment (N = 647; Prolific), featuring scenarios in a\nrange of communication contexts (personal vs. professional; close others vs.\nstrangers), we explored how senders' use of GenAI influenced recipients'\nimpressions of senders, both when GenAI use was known or uncertain. Consistent\nwith past work, we found strong negative effects on social impressions when\ndisclosing that a message was AI-generated, compared to when the same message\nwas human-written. However, under the more realistic condition when potential\nGenAI use was not explicitly highlighted, recipients did not exhibit any\nskepticism towards senders, and these \"uninformed\" impressions were virtually\nindistinguishable from those of fully human-written messages. Even when we\nhighlighted the potential (but uncertain) use of GenAI, recipients formed\noverly positive impressions. These results are especially striking given that\n46% of our sample admitted having used such tools for writing messages, just\nwithin the past two weeks. Our findings put past work in a new light: While\nsocial judgments can be substantially affected when GenAI use is explicitly\ndisclosed, this information may not be readily available in more realistic\ncommunication settings, making recipients blissfully ignorant about others'\npotential use of GenAI.",
      "tldr_zh": "本研究探讨了 Generative AI (GenAI) 在人际沟通中的影响，基于 signaling theory 考察了 AI 生成消息对接收者社会印象的影响。研究通过预注册的大型在线实验（N=647），在不同情境（个人 vs. 专业，熟人 vs. 陌生人）下测试了发送者使用 GenAI 的情况，包括明确披露、潜在使用和未提及的情形。结果显示，当 GenAI 使用未明确指出时，接收者对发送者的印象与完全人类撰写消息相同或更积极，甚至在潜在使用被强调时仍过度乐观；然而，明确披露 AI 生成消息会显著降低正面印象。这些发现突显了现实沟通中的“幸福的无知”现象，尽管46%的参与者在过去两周内承认使用过 GenAI。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.15678v1",
      "published_date": "2025-01-26 21:38:12 UTC",
      "updated_date": "2025-01-26 21:38:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:24:31.452572"
    },
    {
      "arxiv_id": "2501.15665v1",
      "title": "StagFormer: Time Staggering Transformer Decoding for RunningLayers In Parallel",
      "title_zh": "翻译失败",
      "authors": [
        "Dylan Cutler",
        "Arun Kandoor",
        "Nishanth Dikkala",
        "Nikunj Saunshi",
        "Xin Wang",
        "Rina Panigrahy"
      ],
      "abstract": "Standard decoding in a Transformer based language model is inherently\nsequential as we wait for a token's embedding to pass through all the layers in\nthe network before starting the generation of the next token. In this work, we\npropose a new architecture StagFormer (Staggered Transformer), which staggered\nexecution along the time axis and thereby enables parallelizing the decoding\nprocess along the depth of the model. We achieve this by breaking the\ndependency of the token representation at time step $i$ in layer $l$ upon the\nrepresentations of tokens until time step $i$ from layer $l-1$. Instead, we\nstagger the execution and only allow a dependency on token representations\nuntil time step $i-1$. The later sections of the Transformer still get access\nto the ``rich\" representations from the prior section but only from those token\npositions which are one time step behind. StagFormer allows for different\nsections of the model to be executed in parallel yielding at potential 33\\%\nspeedup in decoding while being quality neutral in our simulations. We also\nexplore many natural variants of this idea. We present how weight-sharing\nacross the different sections being staggered can be more practical in settings\nwith limited memory. We show how one can approximate a recurrent model during\ninference using such weight-sharing. We explore the efficacy of using a bounded\nwindow attention to pass information from one section to another which helps\ndrive further latency gains for some applications. We also explore demonstrate\nthe scalability of the staggering idea over more than 2 sections of the\nTransformer.",
      "tldr_zh": "本研究提出了一种新型 Transformer 架构，名为 StagFormer，通过在时间轴上交错执行来并行化模型的解码过程，从而解决标准 Transformer 解码的顺序依赖问题。StagFormer 打破了 token 在层 l 中的表示对前一层所有 token 的依赖，仅允许依赖于前一个时间步 i-1 的表示，这使得模型的不同层可以并行运行。实验结果显示，该方法在解码速度上实现了潜在 33% 的提升，同时保持了输出质量的中性。此外，研究探讨了多种变体，包括权重共享以适应有限内存场景、近似循环模型以及使用有界窗口注意力来进一步优化延迟，并证明了该交错理念在多个 Transformer 部分上的可扩展性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.15665v1",
      "published_date": "2025-01-26 20:09:11 UTC",
      "updated_date": "2025-01-26 20:09:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:24:43.055029"
    },
    {
      "arxiv_id": "2501.15661v1",
      "title": "Constrained Hybrid Metaheuristic Algorithm for Probabilistic Neural Networks Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Piotr A. Kowalski",
        "Szymon Kucharczyk",
        "Jacek Mańdziuk"
      ],
      "abstract": "This study investigates the potential of hybrid metaheuristic algorithms to\nenhance the training of Probabilistic Neural Networks (PNNs) by leveraging the\ncomplementary strengths of multiple optimisation strategies. Traditional\nlearning methods, such as gradient-based approaches, often struggle to optimise\nhigh-dimensional and uncertain environments, while single-method metaheuristics\nmay fail to exploit the solution space fully. To address these challenges, we\npropose the constrained Hybrid Metaheuristic (cHM) algorithm, a novel approach\nthat combines multiple population-based optimisation techniques into a unified\nframework. The proposed procedure operates in two phases: an initial probing\nphase evaluates multiple metaheuristics to identify the best-performing one\nbased on the error rate, followed by a fitting phase where the selected\nmetaheuristic refines the PNN to achieve optimal smoothing parameters. This\niterative process ensures efficient exploration and convergence, enhancing the\nnetwork's generalisation and classification accuracy. cHM integrates several\npopular metaheuristics, such as BAT, Simulated Annealing, Flower Pollination\nAlgorithm, Bacterial Foraging Optimization, and Particle Swarm Optimisation as\ninternal optimisers. To evaluate cHM performance, experiments were conducted on\n16 datasets with varying characteristics, including binary and multiclass\nclassification tasks, balanced and imbalanced class distributions, and diverse\nfeature dimensions. The results demonstrate that cHM effectively combines the\nstrengths of individual metaheuristics, leading to faster convergence and more\nrobust learning. By optimising the smoothing parameters of PNNs, the proposed\nmethod enhances classification performance across diverse datasets, proving its\napplication flexibility and efficiency.",
      "tldr_zh": "这篇论文提出了一种约束混合元启发式算法（cHM），旨在通过整合多种优化策略（如 BAT、Simulated Annealing 和 Particle Swarm Optimisation）来提升 Probabilistic Neural Networks (PNNs) 的训练效果，解决传统梯度方法和高维不确定环境中的优化挑战。cHM 采用两阶段过程：初始探测阶段评估并选择最佳元启发式算法基于错误率，其后拟合阶段优化 PNN 的平滑参数，以实现高效探索和收敛。实验在 16 个数据集上验证了 cHM 的性能，包括二分类和多分类任务，展示了其更快收敛、更稳健的学习能力，并显著提高了分类准确性和泛化效果。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "I.2.0"
      ],
      "primary_category": "cs.NE",
      "comment": "35 pages, 1 Algorithm flow, 11 tables, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.15661v1",
      "published_date": "2025-01-26 19:49:16 UTC",
      "updated_date": "2025-01-26 19:49:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:26:48.363776"
    },
    {
      "arxiv_id": "2501.15660v1",
      "title": "Marker Track: Accurate Fiducial Marker Tracking for Evaluation of Residual Motions During Breath-Hold Radiotherapy",
      "title_zh": "翻译失败",
      "authors": [
        "Aimee Guo",
        "Weihua Mao"
      ],
      "abstract": "Fiducial marker positions in projection image of cone-beam computed\ntomography (CBCT) scans have been studied to evaluate daily residual motion\nduring breath-hold radiation therapy. Fiducial marker migration posed\nchallenges in accurately locating markers, prompting the development of a novel\nalgorithm that reconstructs volumetric probability maps of marker locations\nfrom filtered gradient maps of projections. This guides the development of a\nPython-based algorithm to detect fiducial markers in projection images using\nMeta AI's Segment Anything Model 2 (SAM 2). Retrospective data from a\npancreatic cancer patient with two fiducial markers were analyzed. The\nthree-dimensional (3D) marker positions from simulation computed tomography\n(CT) were compared to those reconstructed from CBCT images, revealing a\ndecrease in relative distances between markers over time. Fiducial markers were\nsuccessfully detected in 2777 out of 2786 projection frames. The average\nstandard deviation of superior-inferior (SI) marker positions was 0.56 mm per\nbreath-hold, with differences in average SI positions between two breath-holds\nin the same scan reaching up to 5.2 mm, and a gap of up to 7.3 mm between the\nend of the first and beginning of the second breath-hold. 3D marker positions\nwere calculated using projection positions and confirmed marker migration. This\nmethod effectively calculates marker probability volume and enables accurate\nfiducial marker tracking during treatment without requiring any specialized\nequipment, additional radiation doses, or manual initialization and labeling.\nIt has significant potential for automatically assessing daily residual motion\nto adjust planning margins, functioning as an adaptive radiation therapy tool.",
      "tldr_zh": "本文提出 Marker Track 算法，用于精确跟踪 Fiducial Marker 在 CBCT 投影图像中的位置，以评估呼吸保持放射治疗期间的残余运动。该算法通过重建标记物位置的体积概率图并结合 Meta AI 的 Segment Anything Model 2 (SAM 2) 进行检测，成功处理了标记物迁移的挑战。在胰腺癌患者回顾数据中，检测率达99.7%，SI 方向标准差为0.56 mm，并观察到呼吸保持间差异可达5.2 mm。实验结果证实此方法无需专用设备或额外辐射，可作为自适应放射治疗工具自动调整规划边界。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV",
        "physics.med-ph"
      ],
      "primary_category": "cs.CV",
      "comment": "14 pages, 9 figures, Regeneron STS 2025 project. Project page:\n  https://sites.google.com/view/markertrack?usp=sharing",
      "pdf_url": "http://arxiv.org/pdf/2501.15660v1",
      "published_date": "2025-01-26 19:46:49 UTC",
      "updated_date": "2025-01-26 19:46:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:25:07.484708"
    },
    {
      "arxiv_id": "2501.15654v2",
      "title": "People who frequently use ChatGPT for writing tasks are accurate and robust detectors of AI-generated text",
      "title_zh": "翻译失败",
      "authors": [
        "Jenna Russell",
        "Marzena Karpinska",
        "Mohit Iyyer"
      ],
      "abstract": "In this paper, we study how well humans can detect text generated by\ncommercial LLMs (GPT-4o, Claude, o1). We hire annotators to read 300\nnon-fiction English articles, label them as either human-written or\nAI-generated, and provide paragraph-length explanations for their decisions.\nOur experiments show that annotators who frequently use LLMs for writing tasks\nexcel at detecting AI-generated text, even without any specialized training or\nfeedback. In fact, the majority vote among five such \"expert\" annotators\nmisclassifies only 1 of 300 articles, significantly outperforming most\ncommercial and open-source detectors we evaluated even in the presence of\nevasion tactics like paraphrasing and humanization. Qualitative analysis of the\nexperts' free-form explanations shows that while they rely heavily on specific\nlexical clues ('AI vocabulary'), they also pick up on more complex phenomena\nwithin the text (e.g., formality, originality, clarity) that are challenging to\nassess for automatic detectors. We release our annotated dataset and code to\nspur future research into both human and automated detection of AI-generated\ntext.",
      "tldr_zh": "这篇论文研究了人类检测商业LLM（如GPT-4o、Claude和o1）生成文本的能力，通过雇佣标注者阅读300篇非虚构英文文章并标记其来源及提供解释。结果显示，经常使用LLM进行写作任务的“专家”标注者无需专门训练即可高效检测AI-generated text，五位专家的多数投票仅误分类1篇文章。相比商业和开源检测器，专家表现更优越，即使面对evasion tactics如改写和人性化处理。定性分析揭示，专家依赖特定词汇线索（AI vocabulary）以及更复杂的文本特征（如formality、originality和clarity），这些对自动检测器构成挑战。作者发布了标注数据集和代码，以促进未来的人类和自动化检测研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL 2025 33 pages",
      "pdf_url": "http://arxiv.org/pdf/2501.15654v2",
      "published_date": "2025-01-26 19:31:34 UTC",
      "updated_date": "2025-05-19 18:22:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:25:20.089053"
    },
    {
      "arxiv_id": "2501.15648v1",
      "title": "Can Pose Transfer Models Generate Realistic Human Motion?",
      "title_zh": "翻译失败",
      "authors": [
        "Vaclav Knapp",
        "Matyas Bohacek"
      ],
      "abstract": "Recent pose-transfer methods aim to generate temporally consistent and fully\ncontrollable videos of human action where the motion from a reference video is\nreenacted by a new identity. We evaluate three state-of-the-art pose-transfer\nmethods -- AnimateAnyone, MagicAnimate, and ExAvatar -- by generating videos\nwith actions and identities outside the training distribution and conducting a\nparticipant study about the quality of these videos. In a controlled\nenvironment of 20 distinct human actions, we find that participants, presented\nwith the pose-transferred videos, correctly identify the desired action only\n42.92% of the time. Moreover, the participants find the actions in the\ngenerated videos consistent with the reference (source) videos only 36.46% of\nthe time. These results vary by method: participants find the splatting-based\nExAvatar more consistent and photorealistic than the diffusion-based\nAnimateAnyone and MagicAnimate.",
      "tldr_zh": "该研究评估了三款先进姿势转移模型（AnimateAnyone、MagicAnimate 和 ExAvatar）是否能生成真实且一致的人类动作视频，这些模型旨在将参考视频的动作重新演绎到新身份上。通过生成超出训练分布的视频并进行参与者研究，实验显示参与者在20种人类动作的受控环境中，仅有42.92%的时间正确识别预期动作，且仅有36.46%的时间认为生成的动作与参考视频一致。结果表明，基于splatting的ExAvatar在动作一致性和真实性方面优于基于diffusion的AnimateAnyone和MagicAnimate。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Data and code available at\n  https://github.com/matyasbohacek/pose-transfer-human-motion",
      "pdf_url": "http://arxiv.org/pdf/2501.15648v1",
      "published_date": "2025-01-26 19:17:05 UTC",
      "updated_date": "2025-01-26 19:17:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:25:30.425003"
    },
    {
      "arxiv_id": "2501.15638v2",
      "title": "A Comprehensive Survey on Self-Interpretable Neural Networks",
      "title_zh": "自解释神经网络的全面综述",
      "authors": [
        "Yang Ji",
        "Ying Sun",
        "Yuting Zhang",
        "Zhigaoyuan Wang",
        "Yuanxin Zhuang",
        "Zheng Gong",
        "Dazhong Shen",
        "Chuan Qin",
        "Hengshu Zhu",
        "Hui Xiong"
      ],
      "abstract": "Neural networks have achieved remarkable success across various fields.\nHowever, the lack of interpretability limits their practical use, particularly\nin critical decision-making scenarios. Post-hoc interpretability, which\nprovides explanations for pre-trained models, is often at risk of robustness\nand fidelity. This has inspired a rising interest in self-interpretable neural\nnetworks, which inherently reveal the prediction rationale through the model\nstructures. Although there exist surveys on post-hoc interpretability, a\ncomprehensive and systematic survey of self-interpretable neural networks is\nstill missing. To address this gap, we first collect and review existing works\non self-interpretable neural networks and provide a structured summary of their\nmethodologies from five key perspectives: attribution-based, function-based,\nconcept-based, prototype-based, and rule-based self-interpretation. We also\npresent concrete, visualized examples of model explanations and discuss their\napplicability across diverse scenarios, including image, text, graph data, and\ndeep reinforcement learning. Additionally, we summarize existing evaluation\nmetrics for self-interpretability and identify open challenges in this field,\noffering insights for future research. To support ongoing developments, we\npresent a publicly accessible resource to track advancements in this domain:\nhttps://github.com/yangji721/Awesome-Self-Interpretable-Neural-Network.",
      "tldr_zh": "这项调查综述了自解释神经网络(self-interpretable neural networks)，旨在解决传统神经网络在关键决策场景中缺乏可解释性的问题，特别是post-hoc interpretability的鲁棒性和保真度风险。通过从五个关键视角——attribution-based、function-based、concept-based、prototype-based和rule-based——总结现有方法，该论文提供了结构化的分析，并展示了这些方法的示例和在图像、文本、图数据以及深度强化学习领域的应用。论文还评估了自解释性的指标，讨论了开放挑战，并提供了一个公开资源（https://github.com/yangji721/Awesome-Self-Interpretable-Neural-Network）来跟踪领域进展。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.15638v2",
      "published_date": "2025-01-26 18:50:16 UTC",
      "updated_date": "2025-03-22 03:32:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:26:34.876245"
    },
    {
      "arxiv_id": "2501.15627v2",
      "title": "HardML: A Benchmark For Evaluating Data Science And Machine Learning knowledge and reasoning in AI",
      "title_zh": "翻译失败",
      "authors": [
        "Tidor-Vlad Pricope"
      ],
      "abstract": "We present HardML, a benchmark designed to evaluate the knowledge and\nreasoning abilities in the fields of data science and machine learning. HardML\ncomprises a diverse set of 100 challenging multiple-choice questions,\nhandcrafted over a period of 6 months, covering the most popular and modern\nbranches of data science and machine learning. These questions are challenging\neven for a typical Senior Machine Learning Engineer to answer correctly. To\nminimize the risk of data contamination, HardML uses mostly original content\ndevised by the author. Current state of the art AI models achieve a 30% error\nrate on this benchmark, which is about 3 times larger than the one achieved on\nthe equivalent, well known MMLU ML. While HardML is limited in scope and not\naiming to push the frontier, primarily due to its multiple choice nature, it\nserves as a rigorous and modern testbed to quantify and track the progress of\ntop AI. While plenty benchmarks and experimentation in LLM evaluation exist in\nother STEM fields like mathematics, physics and chemistry, the subfields of\ndata science and machine learning remain fairly underexplored.",
      "tldr_zh": "本研究引入了 HardML 基准，用于评估 AI 在数据科学和机器学习领域的知识和推理能力。\nHardML 包含 100 个手工制作的多项选择题，覆盖流行和现代分支，这些问题即使对资深机器学习工程师也极具挑战，且采用原创内容以减少数据污染风险。\n实验结果显示，当前最先进 AI 模型在 HardML 上错误率达到 30%，比同类基准 MMLU ML 高出约 3 倍。\n该基准作为严格的测试平台，帮助量化并跟踪 AI 在数据科学和机器学习子领域的进步，尽管其范围有限且基于多项选择形式。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.15627v2",
      "published_date": "2025-01-26 18:25:26 UTC",
      "updated_date": "2025-05-06 15:53:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:26:59.861814"
    },
    {
      "arxiv_id": "2501.15619v1",
      "title": "GaussianToken: An Effective Image Tokenizer with 2D Gaussian Splatting",
      "title_zh": "翻译失败",
      "authors": [
        "Jiajun Dong",
        "Chengkun Wang",
        "Wenzhao Zheng",
        "Lei Chen",
        "Jiwen Lu",
        "Yansong Tang"
      ],
      "abstract": "Effective image tokenization is crucial for both multi-modal understanding\nand generation tasks due to the necessity of the alignment with discrete text\ndata. To this end, existing approaches utilize vector quantization (VQ) to\nproject pixels onto a discrete codebook and reconstruct images from the\ndiscrete representation. However, compared with the continuous latent space,\nthe limited discrete codebook space significantly restrict the representational\nability of these image tokenizers. In this paper, we propose GaussianToken: An\nEffective Image Tokenizer with 2D Gaussian Splatting as a solution. We first\nrepresent the encoded samples as multiple flexible featured 2D Gaussians\ncharacterized by positions, rotation angles, scaling factors, and feature\ncoefficients. We adopt the standard quantization for the Gaussian features and\nthen concatenate the quantization results with the other intrinsic Gaussian\nparameters before the corresponding splatting operation and the subsequent\ndecoding module. In general, GaussianToken integrates the local influence of 2D\nGaussian distribution into the discrete space and thus enhances the\nrepresentation capability of the image tokenizer. Competitive reconstruction\nperformances on CIFAR, Mini-ImageNet, and ImageNet-1K demonstrate the\neffectiveness of our framework. Our code is available at:\nhttps://github.com/ChrisDong-THU/GaussianToken.",
      "tldr_zh": "本研究针对图像标记化在多模态任务中的局限性，提出 GaussianToken 框架，利用 2D Gaussian Splatting 作为解决方案，以提升图像表示能力。具体而言，GaussianToken 将编码样本表示为多个带特征的 2D 高斯分布（包括位置、旋转角度、缩放因子和特征系数），并通过标准量化与 splatting 操作相结合，增强离散空间的表示灵活性。实验结果显示，该框架在 CIFAR、Mini-ImageNet 和 ImageNet-1K 数据集上实现了竞争性的图像重建性能，并提供了开源代码（https://github.com/ChrisDong-THU/GaussianToken）。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.15619v1",
      "published_date": "2025-01-26 17:56:11 UTC",
      "updated_date": "2025-01-26 17:56:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:27:17.400632"
    },
    {
      "arxiv_id": "2501.15618v2",
      "title": "Your Learned Constraint is Secretly a Backward Reachable Tube",
      "title_zh": "翻译失败",
      "authors": [
        "Mohamad Qadri",
        "Gokul Swamy",
        "Jonathan Francis",
        "Michael Kaess",
        "Andrea Bajcsy"
      ],
      "abstract": "Inverse Constraint Learning (ICL) is the problem of inferring constraints\nfrom safe (i.e., constraint-satisfying) demonstrations. The hope is that these\ninferred constraints can then be used downstream to search for safe policies\nfor new tasks and, potentially, under different dynamics. Our paper explores\nthe question of what mathematical entity ICL recovers. Somewhat surprisingly,\nwe show that both in theory and in practice, ICL recovers the set of states\nwhere failure is inevitable, rather than the set of states where failure has\nalready happened. In the language of safe control, this means we recover a\nbackwards reachable tube (BRT) rather than a failure set. In contrast to the\nfailure set, the BRT depends on the dynamics of the data collection system. We\ndiscuss the implications of the dynamics-conditionedness of the recovered\nconstraint on both the sample-efficiency of policy search and the\ntransferability of learned constraints.",
      "tldr_zh": "本论文探讨了Inverse Constraint Learning (ICL)，即从安全的演示中推断约束，以用于新任务的安全策略搜索。研究发现，ICL 实际上恢复的是Backwards Reachable Tube (BRT)，即失败不可避免的状态集，而不是已发生失败的状态集，这在理论和实践中均得到验证。BRT 依赖于数据收集系统的动态，这对策略搜索的样本效率和学习约束的转移性产生了重要影响。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "14 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.15618v2",
      "published_date": "2025-01-26 17:54:43 UTC",
      "updated_date": "2025-05-17 19:16:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:27:23.384417"
    },
    {
      "arxiv_id": "2501.17188v1",
      "title": "Letters, Colors, and Words: Constructing the Ideal Building Blocks Set",
      "title_zh": "翻译失败",
      "authors": [
        "Ricardo Salazar",
        "Shahrzad Jamshidi"
      ],
      "abstract": "Define a building blocks set to be a collection of n cubes (each with six\nsides) where each side is assigned one letter and one color from a palette of m\ncolors. We propose a novel problem of assigning letters and colors to each face\nso as to maximize the number of words one can spell from a chosen dataset that\nare either mono words, all letters have the same color, or rainbow words, all\nletters have unique colors. We explore this problem considering a chosen set of\nEnglish words, up to six letters long, from a typical vocabulary of a US\nAmerican 14 year old and explore the problem when n=6 and m=6, with the added\nrestriction that each color appears exactly once on the cube. The problem is\nintractable, as the size of the solution space makes a brute force approach\ncomputationally infeasible. Therefore we aim to solve this problem using random\nsearch, simulated annealing, two distinct tree search approaches (greedy and\nbest-first), and a genetic algorithm. To address this, we explore a range of\noptimization techniques: random search, simulated annealing, two distinct tree\nsearch methods (greedy and best-first), and a genetic algorithm. Additionally,\nwe attempted to implement a reinforcement learning approach; however, the model\nfailed to converge to viable solutions within the problem's constraints. Among\nthese methods, the genetic algorithm delivered the best performance, achieving\na total of 2846 mono and rainbow words.",
      "tldr_zh": "这篇论文定义了“building blocks set”为n个立方体，每个面分配一个字母和m种颜色中的一种，目标是最大化从一组英文单词（针对14岁美国孩子词汇）中拼出的“mono words”（所有字母相同颜色）和“rainbow words”（所有字母唯一颜色）。作者探索了多种优化技术，包括random search、simulated annealing、两种tree search方法（greedy和best-first），以及genetic algorithm，而reinforcement learning尝试失败。实验在n=6和m=6的条件下显示，genetic algorithm表现最佳，成功获得2846个mono和rainbow单词。",
      "categories": [
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.AI",
      "comment": "29 pages, 8 figures, submitted to SIAM Undergraduate Research Online",
      "pdf_url": "http://arxiv.org/pdf/2501.17188v1",
      "published_date": "2025-01-26 17:54:03 UTC",
      "updated_date": "2025-01-26 17:54:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:27:36.058266"
    },
    {
      "arxiv_id": "2501.17187v2",
      "title": "Visualizing Uncertainty in Translation Tasks: An Evaluation of LLM Performance and Confidence Metrics",
      "title_zh": "翻译任务中不确定性的可视化：对 LLM 性能和置信度指标的评估",
      "authors": [
        "Jin Hyun Park",
        "Utsawb Laminchhane",
        "Umer Farooq",
        "Uma Sivakumar",
        "Arpan Kumar"
      ],
      "abstract": "Large language models (LLMs) are increasingly utilized for machine\ntranslation, yet their predictions often exhibit uncertainties that hinder\ninterpretability and user trust. Effectively visualizing these uncertainties\ncan enhance the usability of LLM outputs, particularly in contexts where\ntranslation accuracy is critical. This paper addresses two primary objectives:\n(1) providing users with token-level insights into model confidence and (2)\ndeveloping a web-based visualization tool to quantify and represent translation\nuncertainties. To achieve these goals, we utilized the T5 model with the WMT19\ndataset for translation tasks and evaluated translation quality using\nestablished metrics such as BLEU, METEOR, and ROUGE. We introduced three novel\nuncertainty quantification (UQ) metrics: (1) the geometric mean of token\nprobabilities, (2) the arithmetic mean of token probabilities, and (3) the\narithmetic mean of the kurtosis of token distributions. These metrics provide a\nsimple yet effective framework for evaluating translation performance. Our\nanalysis revealed a linear relationship between the traditional evaluation\nmetrics and our UQ metrics, demonstrating the validity of our approach.\nAdditionally, we developed an interactive web-based visualization that uses a\ncolor gradient to represent token confidence. This tool offers users a clear\nand intuitive understanding of translation quality while providing valuable\ninsights into model performance. Overall, we show that our UQ metrics and\nvisualization are both robust and interpretable, offering practical tools for\nevaluating and accessing machine translation systems.",
      "tldr_zh": "这篇论文探讨了大型语言模型 (LLMs) 在机器翻译任务中的不确定性问题，旨在通过可视化提升模型输出可解释性和用户信任。研究团队使用 T5 模型和 WMT19 数据集，引入了三个新不确定性量化 (UQ) 指标，包括几何平均的 token 概率、算术平均的 token 概率以及算术平均的 token 分布 kurtosis，并与传统评估指标如 BLEU、METEOR 和 ROUGE 进行比较。结果显示，这些 UQ 指标与传统指标存在线性关系，验证了其有效性。同时，他们开发了一个交互式 web-based 可视化工具，使用颜色渐变表示 token 置信度，提供直观的用户洞见。该方法为评估机器翻译系统的稳健性和可解释性提供了实用工具。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "We would like to withdraw our paper due to an error in the\n  experimental methodology, which impacts the validity of our results. The\n  error specifically affects the analysis presented in the Discussion, where an\n  incorrect experimental modeling step led to misleading interpretations",
      "pdf_url": "http://arxiv.org/pdf/2501.17187v2",
      "published_date": "2025-01-26 17:14:51 UTC",
      "updated_date": "2025-02-24 21:20:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:27:50.138874"
    },
    {
      "arxiv_id": "2501.15602v2",
      "title": "Rethinking External Slow-Thinking: From Snowball Errors to Probability of Correct Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Zeyu Gan",
        "Yun Liao",
        "Yong Liu"
      ],
      "abstract": "Test-time scaling, which is also often referred to as slow-thinking, has been\ndemonstrated to enhance multi-step reasoning in large language models (LLMs).\nHowever, despite its widespread utilization, the mechanisms underlying\nslow-thinking methods remain poorly understood. This paper explores the\nmechanisms of external slow-thinking from a theoretical standpoint. We begin by\nexamining the snowball error effect within the LLM reasoning process and\nconnect it to the likelihood of correct reasoning using information theory.\nBuilding on this, we show that external slow-thinking methods can be\ninterpreted as strategies to mitigate the error probability. We further provide\na comparative analysis of popular external slow-thinking approaches, ranging\nfrom simple to complex, highlighting their differences and interrelationships.\nOur findings suggest that the efficacy of these methods is not primarily\ndetermined by the specific framework employed, and that expanding the search\nscope or the model's internal reasoning capacity may yield more sustained\nimprovements in the long term. We open-source our code at\nhttps://github.com/ZyGan1999/Snowball-Errors-and-Probability.",
      "tldr_zh": "这篇论文重新审视了外部 slow-thinking 方法，探讨其在大型语言模型(LLMs)多步推理中的机制，特别是 snowball error effect 与正确推理概率的关联。作者使用信息理论分析了这些方法如何降低错误概率，并对从简单到复杂的 popular external slow-thinking approaches 进行了比较，突出了它们的差异和关系。研究发现，这些方法的有效性主要不依赖于特定框架，长期提升可能来自于扩展搜索范围或增强模型的内部推理能力；论文还提供了开源代码以供进一步研究。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.15602v2",
      "published_date": "2025-01-26 17:05:16 UTC",
      "updated_date": "2025-01-28 14:14:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:27:59.708809"
    },
    {
      "arxiv_id": "2501.15598v1",
      "title": "Diffusion Generative Modeling for Spatially Resolved Gene Expression Inference from Histology Images",
      "title_zh": "翻译失败",
      "authors": [
        "Sichen Zhu",
        "Yuchen Zhu",
        "Molei Tao",
        "Peng Qiu"
      ],
      "abstract": "Spatial Transcriptomics (ST) allows a high-resolution measurement of RNA\nsequence abundance by systematically connecting cell morphology depicted in\nHematoxylin and Eosin (H&E) stained histology images to spatially resolved gene\nexpressions. ST is a time-consuming, expensive yet powerful experimental\ntechnique that provides new opportunities to understand cancer mechanisms at a\nfine-grained molecular level, which is critical for uncovering new approaches\nfor disease diagnosis and treatments. Here, we present $\\textbf{Stem}$\n($\\textbf{S}$pa$\\textbf{T}$ially resolved gene $\\textbf{E}$xpression inference\nwith diffusion $\\textbf{M}$odel), a novel computational tool that leverages a\nconditional diffusion generative model to enable in silico gene expression\ninference from H&E stained images. Through better capturing the inherent\nstochasticity and heterogeneity in ST data, $\\textbf{Stem}$ achieves\nstate-of-the-art performance on spatial gene expression prediction and\ngenerates biologically meaningful gene profiles for new H&E stained images at\ntest time. We evaluate the proposed algorithm on datasets with various tissue\nsources and sequencing platforms, where it demonstrates clear improvement over\nexisting approaches. $\\textbf{Stem}$ generates high-fidelity gene expression\npredictions that share similar gene variation levels as ground truth data,\nsuggesting that our method preserves the underlying biological heterogeneity.\nOur proposed pipeline opens up the possibility of analyzing existing, easily\naccessible H&E stained histology images from a genomics point of view without\nphysically performing gene expression profiling and empowers potential\nbiological discovery from H&E stained histology images.",
      "tldr_zh": "本文提出了一种名为 Stem 的新计算工具，利用条件扩散生成模型（conditional diffusion generative model），从 Hematoxylin and Eosin (H&E) 染色组织图像中推断空间分辨基因表达，从而避免了昂贵的 Spatial Transcriptomics (ST) 实验。Stem 通过捕捉 ST 数据中的固有随机性和异质性，实现了最先进的基因表达预测性能，并在多种组织来源和测序平台的数据集上优于现有方法。实验结果显示，Stem 生成的高保真基因表达预测与真实数据类似，保留了生物异质性，并为从现有 H&E 图像进行基因组分析和潜在生物发现提供了新可能性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "q-bio.QM",
        "stat.ML"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.15598v1",
      "published_date": "2025-01-26 16:52:27 UTC",
      "updated_date": "2025-01-26 16:52:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:28:11.818050"
    },
    {
      "arxiv_id": "2501.15587v1",
      "title": "SCP-116K: A High-Quality Problem-Solution Dataset and a Generalized Pipeline for Automated Extraction in the Higher Education Science Domain",
      "title_zh": "SCP-116K：一个高质量的问题-解决方案数据集以及一个通用",
      "authors": [
        "Dakuan Lu",
        "Xiaoyu Tan",
        "Rui Xu",
        "Tianchu Yao",
        "Chao Qu",
        "Wei Chu",
        "Yinghui Xu",
        "Yuan Qi"
      ],
      "abstract": "Recent breakthroughs in large language models (LLMs) exemplified by the\nimpressive mathematical and scientific reasoning capabilities of the o1 model\nhave spotlighted the critical importance of high-quality training data in\nadvancing LLM performance across STEM disciplines. While the mathematics\ncommunity has benefited from a growing body of curated datasets, the scientific\ndomain at the higher education level has long suffered from a scarcity of\ncomparable resources. To address this gap, we present SCP-116K, a new\nlarge-scale dataset of 116,756 high-quality problem-solution pairs,\nautomatically extracted from heterogeneous sources using a streamlined and\nhighly generalizable pipeline. Our approach involves stringent filtering to\nensure the scientific rigor and educational level of the extracted materials,\nwhile maintaining adaptability for future expansions or domain transfers. By\nopenly releasing both the dataset and the extraction pipeline, we seek to\nfoster research on scientific reasoning, enable comprehensive performance\nevaluations of new LLMs, and lower the barrier to replicating the successes of\nadvanced models like o1 in the broader science community. We believe SCP-116K\nwill serve as a critical resource, catalyzing progress in high-level scientific\nreasoning tasks and promoting further innovations in LLM development. The\ndataset and code are publicly available at\nhttps://github.com/AQA6666/SCP-116K-open.",
      "tldr_zh": "该研究介绍了 SCP-116K，一个包含 116,756 个高质量问题-解决方案对的大型数据集，旨在填补高等教育科学领域的高质量训练数据缺失问题。研究团队开发了一个流线型且高度可泛化的自动提取管道，从异构来源获取数据，并通过严格过滤确保科学严谨性和教育水平，以支持未来扩展。公开发布数据集和代码后，将促进 LLMs 在 STEM 学科的科学推理研究、性能评估，并加速类似 o1 模型的创新发展。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, 1 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.15587v1",
      "published_date": "2025-01-26 16:26:38 UTC",
      "updated_date": "2025-01-26 16:26:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:28:23.699279"
    },
    {
      "arxiv_id": "2501.15585v2",
      "title": "Twin Transition or Competing Interests? Validation of the Artificial Intelligence and Sustainability Perceptions Inventory (AISPI)",
      "title_zh": "翻译失败",
      "authors": [
        "Annika Bush"
      ],
      "abstract": "As artificial intelligence (AI) and sustainability initiatives increasingly\nintersect, understanding public perceptions of their relationship becomes\ncrucial for successful implementation. However, no validated instrument exists\nto measure these specific perceptions. This paper presents the development and\nvalidation of the Artificial Intelligence and Sustainability Perceptions\nInventory (AISPI), a novel 13-item instrument measuring how individuals view\nthe relationship between AI advancement and environmental sustainability.\nThrough factor analysis (N=105), we identified two distinct dimensions: Twin\nTransition and Competing Interests. The instrument demonstrated strong\nreliability (alpha=.89) and construct validity through correlations with\nestablished measures of AI and sustainability attitudes. Our findings suggest\nthat individuals can simultaneously recognize both synergies and tensions in\nthe AI-sustainability relationship, offering important implications for\nresearchers and practitioners working at this critical intersection. This work\nprovides a foundational tool for future research on public perceptions of AI's\nrole in sustainable development.",
      "tldr_zh": "本研究开发并验证了 Artificial Intelligence and Sustainability Perceptions Inventory (AISPI)，一个13项问卷，用于评估人们对AI发展和环境可持续性关系的看法。通过因子分析（N=105），识别出Twin Transition（双重转型）和Competing Interests（竞争利益）两个维度，该问卷显示出高可靠性（alpha=.89）和结构效度。结果表明，个体能同时认识到AI与可持续性的协同作用和潜在冲突，为研究者与从业者探索AI在可持续开发中的作用提供重要工具。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "CHI 2025 Late Breaking Work",
      "pdf_url": "http://arxiv.org/pdf/2501.15585v2",
      "published_date": "2025-01-26 16:21:27 UTC",
      "updated_date": "2025-03-24 09:33:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:28:35.840782"
    },
    {
      "arxiv_id": "2501.15572v3",
      "title": "Comparative clinical evaluation of \"memory-efficient\" synthetic 3d generative adversarial networks (gan) head-to-head to state of art: results on computed tomography of the chest",
      "title_zh": "翻译失败",
      "authors": [
        "Mahshid Shiri",
        "Chandra Bortolotto",
        "Alessandro Bruno",
        "Alessio Consonni",
        "Daniela Maria Grasso",
        "Leonardo Brizzi",
        "Daniele Loiacono",
        "Lorenzo Preda"
      ],
      "abstract": "Generative Adversarial Networks (GANs) are increasingly used to generate\nsynthetic medical images, addressing the critical shortage of annotated data\nfor training Artificial Intelligence systems. This study introduces CRF-GAN, a\nnovel memory-efficient GAN architecture that enhances structural consistency in\n3D medical image synthesis. Integrating Conditional Random Fields within a\ntwo-step generation process allows CRF-GAN improving spatial coherence while\nmaintaining high-resolution image quality. The model's performance is evaluated\nagainst the state-of-the-art hierarchical (HA)-GAN model. Materials and\nMethods: We evaluate the performance of CRF-GAN against the HA-GAN model. The\ncomparison between the two models was made through a quantitative evaluation,\nusing FID and MMD metrics, and a qualitative evaluation, through a\ntwo-alternative forced choice (2AFC) test completed by a pool of 12 resident\nradiologists, to assess the realism of the generated images. Results: CRF-GAN\noutperformed HA-GAN with lower FID and MMD scores, indicating better image\nfidelity. The 2AFC test showed a significant preference for images generated by\nCRF-Gan over those generated by HA-GAN. Additionally, CRF-GAN demonstrated\n9.34% lower memory usage and achieved up to 14.6% faster training speeds,\noffering substantial computational savings. Discussion: CRF-GAN model\nsuccessfully generates high-resolution 3D medical images with non-inferior\nquality to conventional models, while being more memory-efficient and faster.\nThe key objective was not only to lower the computational cost but also to\nreallocate the freed-up resources towards the creation of higher-resolution 3D\nimaging, which is still a critical factor limiting their direct clinical\napplicability. Moreover, unlike many previous studies, we combined qualitative\nand quantitative assessments to obtain a more holistic feedback on the model's\nperformance.",
      "tldr_zh": "本文评估了CRF-GAN，一种新型内存高效的Generative Adversarial Networks (GAN)架构，用于合成3D医学图像，通过整合Conditional Random Fields在两步生成过程中提升结构一致性和图像质量。研究采用FID和MMD指标进行定量评估，以及由12名放射科住院医师进行的2AFC测试进行定性评估，与state-of-the-art的HA-GAN模型比较。结果显示，CRF-GAN在图像逼真度上表现优异，FID和MMD分数更低，并实现了9.34%的内存节省和14.6%的训练速度提升，为临床CT胸部图像生成提供了更高效且高质量的解决方案。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "Accpeted to Journal of Imaging Informatics in Medicine",
      "pdf_url": "http://arxiv.org/pdf/2501.15572v3",
      "published_date": "2025-01-26 15:57:44 UTC",
      "updated_date": "2025-04-20 21:31:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:28:48.939665"
    },
    {
      "arxiv_id": "2501.15564v2",
      "title": "Diffusion-Based Planning for Autonomous Driving with Flexible Guidance",
      "title_zh": "基于扩散的自动驾驶规划方法，具有灵活引导",
      "authors": [
        "Yinan Zheng",
        "Ruiming Liang",
        "Kexin Zheng",
        "Jinliang Zheng",
        "Liyuan Mao",
        "Jianxiong Li",
        "Weihao Gu",
        "Rui Ai",
        "Shengbo Eben Li",
        "Xianyuan Zhan",
        "Jingjing Liu"
      ],
      "abstract": "Achieving human-like driving behaviors in complex open-world environments is\na critical challenge in autonomous driving. Contemporary learning-based\nplanning approaches such as imitation learning methods often struggle to\nbalance competing objectives and lack of safety assurance,due to limited\nadaptability and inadequacy in learning complex multi-modal behaviors commonly\nexhibited in human planning, not to mention their strong reliance on the\nfallback strategy with predefined rules. We propose a novel transformer-based\nDiffusion Planner for closed-loop planning, which can effectively model\nmulti-modal driving behavior and ensure trajectory quality without any\nrule-based refinement. Our model supports joint modeling of both prediction and\nplanning tasks under the same architecture, enabling cooperative behaviors\nbetween vehicles. Moreover, by learning the gradient of the trajectory score\nfunction and employing a flexible classifier guidance mechanism, Diffusion\nPlanner effectively achieves safe and adaptable planning behaviors. Evaluations\non the large-scale real-world autonomous planning benchmark nuPlan and our\nnewly collected 200-hour delivery-vehicle driving dataset demonstrate that\nDiffusion Planner achieves state-of-the-art closed-loop performance with robust\ntransferability in diverse driving styles.",
      "tldr_zh": "该研究提出了一种基于Transformer的Diffusion Planner，用于自动驾驶的闭环规划，旨在解决传统模仿学习方法在平衡竞争目标、安全保障和多模态行为学习方面的不足，同时减少对预定义规则的依赖。该模型通过有效建模多模态驾驶行为、联合预测和规划任务以及灵活的分类器引导机制，实现车辆间的合作并确保轨迹质量的安全性和适应性。在大规模真实世界基准nuPlan和新收集的200小时交付车辆数据集上，Diffusion Planner实现了最先进的闭环性能，并展示了在不同驾驶风格下的鲁棒转移性。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.15564v2",
      "published_date": "2025-01-26 15:49:50 UTC",
      "updated_date": "2025-02-09 16:37:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:28:59.789751"
    },
    {
      "arxiv_id": "2501.15562v1",
      "title": "CE-SDWV: Effective and Efficient Concept Erasure for Text-to-Image Diffusion Models via a Semantic-Driven Word Vocabulary",
      "title_zh": "翻译失败",
      "authors": [
        "Jiahang Tu",
        "Qian Feng",
        "Chufan Chen",
        "Jiahua Dong",
        "Hanbin Zhao",
        "Chao Zhang",
        "Hui Qian"
      ],
      "abstract": "Large-scale text-to-image (T2I) diffusion models have achieved remarkable\ngenerative performance about various concepts. With the limitation of privacy\nand safety in practice, the generative capability concerning NSFW (Not Safe For\nWork) concepts is undesirable, e.g., producing sexually explicit photos, and\nlicensed images. The concept erasure task for T2I diffusion models has\nattracted considerable attention and requires an effective and efficient\nmethod. To achieve this goal, we propose a CE-SDWV framework, which removes the\ntarget concepts (e.g., NSFW concepts) of T2I diffusion models in the text\nsemantic space by only adjusting the text condition tokens and does not need to\nre-train the original T2I diffusion model's weights. Specifically, our\nframework first builds a target concept-related word vocabulary to enhance the\nrepresentation of the target concepts within the text semantic space, and then\nutilizes an adaptive semantic component suppression strategy to ablate the\ntarget concept-related semantic information in the text condition tokens. To\nfurther adapt the above text condition tokens to the original image semantic\nspace, we propose an end-to-end gradient-orthogonal token optimization\nstrategy. Extensive experiments on I2P and UnlearnCanvas benchmarks demonstrate\nthe effectiveness and efficiency of our method.",
      "tldr_zh": "该研究提出 CE-SDWV 框架，用于高效删除文本到图像 (T2I) 扩散模型中的目标概念（如 NSFW 内容），通过在文本语义空间调整文本条件标记，而无需重新训练模型权重。具体方法包括构建一个语义驱动词汇表 (Semantic-Driven Word Vocabulary) 来增强目标概念表示，并采用自适应语义组件抑制策略去除相关语义信息，同时通过端到端梯度正交标记优化策略适应图像语义空间。在 I2P 和 UnlearnCanvas 基准测试中，该框架表现出色，显著提高了概念擦除的有效性和效率。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "24 pages, 15 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.15562v1",
      "published_date": "2025-01-26 15:39:47 UTC",
      "updated_date": "2025-01-26 15:39:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:29:11.889950"
    },
    {
      "arxiv_id": "2501.15555v1",
      "title": "Distributionally Robust Graph Out-of-Distribution Recommendation via Diffusion Model",
      "title_zh": "翻译失败",
      "authors": [
        "Chu Zhao",
        "Enneng Yang",
        "Yuliang Liang",
        "Jianzhe Zhao",
        "Guibing Guo",
        "Xingwei Wang"
      ],
      "abstract": "The distributionally robust optimization (DRO)-based graph neural network\nmethods improve recommendation systems' out-of-distribution (OOD)\ngeneralization by optimizing the model's worst-case performance. However, these\nstudies fail to consider the impact of noisy samples in the training data,\nwhich results in diminished generalization capabilities and lower accuracy.\nThrough experimental and theoretical analysis, this paper reveals that current\nDRO-based graph recommendation methods assign greater weight to noise\ndistribution, leading to model parameter learning being dominated by it. When\nthe model overly focuses on fitting noise samples in the training data, it may\nlearn irrelevant or meaningless features that cannot be generalized to OOD\ndata. To address this challenge, we design a Distributionally Robust Graph\nmodel for OOD recommendation (DRGO). Specifically, our method first employs a\nsimple and effective diffusion paradigm to alleviate the noisy effect in the\nlatent space. Additionally, an entropy regularization term is introduced in the\nDRO objective function to avoid extreme sample weights in the worst-case\ndistribution. Finally, we provide a theoretical proof of the generalization\nerror bound of DRGO as well as a theoretical analysis of how our approach\nmitigates noisy sample effects, which helps to better understand the proposed\nframework from a theoretical perspective. We conduct extensive experiments on\nfour datasets to evaluate the effectiveness of our framework against three\ntypical distribution shifts, and the results demonstrate its superiority in\nboth independently and identically distributed distributions (IID) and OOD.",
      "tldr_zh": "本研究针对基于 Distributionally Robust Optimization (DRO) 的图神经网络方法在推荐系统中存在的噪声样本问题，导致 Out-of-Distribution (OOD) 泛化能力下降和准确率降低。通过实验和理论分析，揭示了现有方法对噪声分布赋予过高权重，影响模型学习。论文提出了一种新的模型 Distributionally Robust Graph model for OOD recommendation (DRGO)，它利用扩散模型 (diffusion paradigm) 在潜在空间缓解噪声影响，并引入熵正则化 (entropy regularization) 项来优化 DRO 目标函数，避免极端样本权重。实验结果显示，DRGO 在四个数据集上对三种典型分布偏移表现出色，在 IID 和 OOD 场景下均优于基线模型，且理论证明提供了其泛化误差界和噪声缓解机制。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.GR",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages, Accepted by WWW'25",
      "pdf_url": "http://arxiv.org/pdf/2501.15555v1",
      "published_date": "2025-01-26 15:07:52 UTC",
      "updated_date": "2025-01-26 15:07:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:29:24.382048"
    },
    {
      "arxiv_id": "2501.15547v1",
      "title": "Building Efficient Lightweight CNN Models",
      "title_zh": "构建高效轻量级 CNN 模型",
      "authors": [
        "Nathan Isong"
      ],
      "abstract": "Convolutional Neural Networks (CNNs) are pivotal in image classification\ntasks due to their robust feature extraction capabilities. However, their high\ncomputational and memory requirements pose challenges for deployment in\nresource-constrained environments. This paper introduces a methodology to\nconstruct lightweight CNNs while maintaining competitive accuracy. The approach\nintegrates two stages of training; dual-input-output model and transfer\nlearning with progressive unfreezing. The dual-input-output model train on\noriginal and augmented datasets, enhancing robustness. Progressive unfreezing\nis applied to the unified model to optimize pre-learned features during\nfine-tuning, enabling faster convergence and improved model accuracy.\n  The methodology was evaluated on three benchmark datasets; handwritten digit\nMNIST, fashion MNIST, and CIFAR-10. The proposed model achieved a\nstate-of-the-art accuracy of 99% on the handwritten digit MNIST and 89% on\nfashion MNIST, with only 14,862 parameters and a model size of 0.17 MB. While\nperformance on CIFAR-10 was comparatively lower (65% with less than 20,00\nparameters), the results highlight the scalability of this method. The final\nmodel demonstrated fast inference times and low latency, making it suitable for\nreal-time applications.\n  Future directions include exploring advanced augmentation techniques,\nimproving architectural scalability for complex datasets, and extending the\nmethodology to tasks beyond classification. This research underscores the\npotential for creating efficient, scalable, and task-specific CNNs for diverse\napplications.",
      "tldr_zh": "这篇论文提出了一种构建高效轻量级 CNN 的方法，以解决其在资源受限环境中的高计算和内存需求问题。该方法包括两个阶段：双输入输出模型在原始和增强数据集上训练以提升鲁棒性，以及转移学习中的渐进解冻来优化预学特征，实现更快收敛和更高准确率。在基准数据集上评估显示，该模型在 MNIST 上达到99%准确率，在 Fashion MNIST 上达到89%，而 CIFAR-10 上为65%，且仅需14,862参数和0.17 MB 模型大小，实现了快速推理和低延迟。未来方向包括探索高级增强技术和扩展到其他任务，以提高架构的可扩展性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "25 pages, 22 figures, 6 tables, JMLR journal standard paper and to be\n  submitted",
      "pdf_url": "http://arxiv.org/pdf/2501.15547v1",
      "published_date": "2025-01-26 14:39:01 UTC",
      "updated_date": "2025-01-26 14:39:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:29:37.181106"
    },
    {
      "arxiv_id": "2501.15544v3",
      "title": "Advancing Generative Artificial Intelligence and Large Language Models for Demand Side Management with Internet of Electric Vehicles",
      "title_zh": "针对电动车",
      "authors": [
        "Hanwen Zhang",
        "Ruichen Zhang",
        "Wei Zhang",
        "Dusit Niyato",
        "Yonggang Wen"
      ],
      "abstract": "Generative artificial intelligence, particularly through large language\nmodels (LLMs), is poised to transform energy optimization and demand side\nmanagement (DSM) within microgrids. This paper explores the integration of LLMs\ninto energy management, emphasizing their roles in automating the optimization\nof DSM strategies with Internet of electric vehicles. We investigate challenges\nand solutions associated with DSM and explore the new opportunities presented\nby leveraging LLMs. Then, we propose an innovative solution that enhances LLMs\nwith retrieval-augmented generation for automatic problem formulation, code\ngeneration, and customizing optimization. We present a case study to\ndemonstrate the effectiveness of our proposed solution in charging scheduling\nand optimization for electric vehicles, highlighting our solution's significant\nadvancements in energy efficiency and user adaptability. This work underscores\nthe potential of LLMs for energy optimization and fosters a new era of\nintelligent DSM solutions.",
      "tldr_zh": "这篇论文探讨了生成式人工智能，特别是大型语言模型(LLMs)，在需求侧管理(DSM)中的应用，强调其与物联网电动车(Internet of Electric Vehicles)整合，以优化微电网的能源管理。作者提出了一种创新解决方案，通过检索增强生成(RAG)技术增强LLMs，实现自动问题制定、代码生成和优化定制。案例研究显示，该方法在电动车充电调度中显著提升了能源效率和用户适应性，并推动了智能DSM解决方案的新时代发展。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "9 Pages",
      "pdf_url": "http://arxiv.org/pdf/2501.15544v3",
      "published_date": "2025-01-26 14:31:03 UTC",
      "updated_date": "2025-04-21 11:09:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:29:48.020570"
    },
    {
      "arxiv_id": "2501.15529v1",
      "title": "UNIDOOR: A Universal Framework for Action-Level Backdoor Attacks in Deep Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Oubo Ma",
        "Linkang Du",
        "Yang Dai",
        "Chunyi Zhou",
        "Qingming Li",
        "Yuwen Pu",
        "Shouling Ji"
      ],
      "abstract": "Deep reinforcement learning (DRL) is widely applied to safety-critical\ndecision-making scenarios. However, DRL is vulnerable to backdoor attacks,\nespecially action-level backdoors, which pose significant threats through\nprecise manipulation and flexible activation, risking outcomes like vehicle\ncollisions or drone crashes. The key distinction of action-level backdoors lies\nin the utilization of the backdoor reward function to associate triggers with\ntarget actions. Nevertheless, existing studies typically rely on backdoor\nreward functions with fixed values or conditional flipping, which lack\nuniversality across diverse DRL tasks and backdoor designs, resulting in\nfluctuations or even failure in practice.\n  This paper proposes the first universal action-level backdoor attack\nframework, called UNIDOOR, which enables adaptive exploration of backdoor\nreward functions through performance monitoring, eliminating the reliance on\nexpert knowledge and grid search. We highlight that action tampering serves as\na crucial component of action-level backdoor attacks in continuous action\nscenarios, as it addresses attack failures caused by low-frequency target\nactions. Extensive evaluations demonstrate that UNIDOOR significantly enhances\nthe attack performance of action-level backdoors, showcasing its universality\nacross diverse attack scenarios, including single/multiple agents,\nsingle/multiple backdoors, discrete/continuous action spaces, and sparse/dense\nreward signals. Furthermore, visualization results encompassing state\ndistribution, neuron activation, and animations demonstrate the stealthiness of\nUNIDOOR. The source code of UNIDOOR can be found at\nhttps://github.com/maoubo/UNIDOOR.",
      "tldr_zh": "该论文探讨了深度强化学习 (DRL) 在安全关键决策中的易受行动级别后门攻击的问题，这些攻击通过后门奖励函数将触发器与目标行动关联，可能导致严重后果如车辆碰撞。论文提出 UNIDOOR，这是一个通用框架，通过性能监控实现后门奖励函数的适应性探索，并引入行动篡改机制，以解决现有方法的局限性和攻击失败风险。实验结果显示，UNIDOOR 在多种场景中显著提升攻击性能，包括单/多代理、单/多后门、离散/连续行动空间以及稀疏/密集奖励信号，并通过可视化证明其隐秘性。该框架的开源代码可用于进一步研究。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "21 pages, 12 figures, 7 tables",
      "pdf_url": "http://arxiv.org/pdf/2501.15529v1",
      "published_date": "2025-01-26 13:43:39 UTC",
      "updated_date": "2025-01-26 13:43:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:30:00.382838"
    },
    {
      "arxiv_id": "2501.15509v1",
      "title": "FIT-Print: Towards False-claim-resistant Model Ownership Verification via Targeted Fingerprint",
      "title_zh": "FIT-Print：通过目标指纹实现抗虚假声明的模型所有权验证",
      "authors": [
        "Shuo Shao",
        "Haozhe Zhu",
        "Hongwei Yao",
        "Yiming Li",
        "Tianwei Zhang",
        "Zhan Qin",
        "Kui Ren"
      ],
      "abstract": "Model fingerprinting is a widely adopted approach to safeguard the\nintellectual property rights of open-source models by preventing their\nunauthorized reuse. It is promising and convenient since it does not\nnecessitate modifying the protected model. In this paper, we revisit existing\nfingerprinting methods and reveal that they are vulnerable to false claim\nattacks where adversaries falsely assert ownership of any third-party model. We\ndemonstrate that this vulnerability mostly stems from their untargeted nature,\nwhere they generally compare the outputs of given samples on different models\ninstead of the similarities to specific references. Motivated by these\nfindings, we propose a targeted fingerprinting paradigm (i.e., FIT-Print) to\ncounteract false claim attacks. Specifically, FIT-Print transforms the\nfingerprint into a targeted signature via optimization. Building on the\nprinciples of FIT-Print, we develop bit-wise and list-wise black-box model\nfingerprinting methods, i.e., FIT-ModelDiff and FIT-LIME, which exploit the\ndistance between model outputs and the feature attribution of specific samples\nas the fingerprint, respectively. Extensive experiments on benchmark models and\ndatasets verify the effectiveness, conferrability, and resistance to false\nclaim attacks of our FIT-Print.",
      "tldr_zh": "本研究揭示了现有模型指纹方法在保护开源模型知识产权时，易受 false claim attacks 的影响，因为这些方法是非针对性的，仅比较模型输出而非特定参考。作者提出 targeted fingerprint 范式 FIT-Print，通过优化将指纹转化为针对性签名，并开发了两种黑盒方法：FIT-ModelDiff（利用模型输出距离作为指纹）和 FIT-LIME（利用特定样本的特征归因作为指纹）。实验在基准模型和数据集上验证了 FIT-Print 的有效性、易转移性和对 false claim attacks 的抵抗力。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.15509v1",
      "published_date": "2025-01-26 13:00:58 UTC",
      "updated_date": "2025-01-26 13:00:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:30:12.020659"
    },
    {
      "arxiv_id": "2501.15495v1",
      "title": "Expert-Free Online Transfer Learning in Multi-Agent Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Alberto Castagna"
      ],
      "abstract": "Reinforcement Learning (RL) enables an intelligent agent to optimise its\nperformance in a task by continuously taking action from an observed state and\nreceiving a feedback from the environment in form of rewards. RL typically uses\ntables or linear approximators to map state-action tuples that maximises the\nreward. Combining RL with deep neural networks (DRL) significantly increases\nits scalability and enables it to address more complex problems than before.\nHowever, DRL also inherits downsides from both RL and deep learning. Despite\nDRL improves generalisation across similar state-action pairs when compared to\nsimpler RL policy representations like tabular methods, it still requires the\nagent to adequately explore the state-action space. Additionally, deep methods\nrequire more training data, with the volume of data escalating with the\ncomplexity and size of the neural network. As a result, deep RL requires a long\ntime to collect enough agent-environment samples and to successfully learn the\nunderlying policy. Furthermore, often even a slight alteration to the task\ninvalidates any previous acquired knowledge. To address these shortcomings,\nTransfer Learning (TL) has been introduced, which enables the use of external\nknowledge from other tasks or agents to enhance a learning process. The goal of\nTL is to reduce the learning complexity for an agent dealing with an unfamiliar\ntask by simplifying the exploration process. This is achieved by lowering the\namount of new information required by its learning model, resulting in a\nreduced overall convergence time...",
      "tldr_zh": "该论文探讨了强化学习(Reinforcement Learning, RL)特别是深度强化学习(Deep Reinforcement Learning, DRL)在多智能体环境中面临的挑战，包括探索空间需求大、需要大量训练数据以及任务变更导致知识失效的问题。为解决这些问题，论文提出了一种无需专家干预的在线转移学习(Expert-Free Online Transfer Learning)方法，通过利用外部知识简化探索过程并减少新信息需求。实验结果表明，该方法显著缩短了学习收敛时间，提升了多智能体RL的效率和适应性。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "PhD Thesis",
      "pdf_url": "http://arxiv.org/pdf/2501.15495v1",
      "published_date": "2025-01-26 11:53:18 UTC",
      "updated_date": "2025-01-26 11:53:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:30:23.933753"
    },
    {
      "arxiv_id": "2501.15492v1",
      "title": "Color Flow Imaging Microscopy Improves Identification of Stress Sources of Protein Aggregates in Biopharmaceuticals",
      "title_zh": "彩色流动成像显微镜改善了生物制药中蛋白质聚集体的应力来源识别",
      "authors": [
        "Michaela Cohrs",
        "Shiwoo Koak",
        "Yejin Lee",
        "Yu Jin Sung",
        "Wesley De Neve",
        "Hristo L. Svilenov",
        "Utku Ozbulak"
      ],
      "abstract": "Protein-based therapeutics play a pivotal role in modern medicine targeting\nvarious diseases. Despite their therapeutic importance, these products can\naggregate and form subvisible particles (SvPs), which can compromise their\nefficacy and trigger immunological responses, emphasizing the critical need for\nrobust monitoring techniques. Flow Imaging Microscopy (FIM) has been a\nsignificant advancement in detecting SvPs, evolving from monochrome to more\nrecently incorporating color imaging. Complementing SvP images obtained via\nFIM, deep learning techniques have recently been employed successfully for\nstress source identification of monochrome SvPs. In this study, we explore the\npotential of color FIM to enhance the characterization of stress sources in\nSvPs. To achieve this, we curate a new dataset comprising 16,000 SvPs from\neight commercial monoclonal antibodies subjected to heat and mechanical stress.\nUsing both supervised and self-supervised convolutional neural networks, as\nwell as vision transformers in large-scale experiments, we demonstrate that\ndeep learning with color FIM images consistently outperforms monochrome images,\nthus highlighting the potential of color FIM in stress source classification\ncompared to its monochrome counterparts.",
      "tldr_zh": "本研究探讨了彩色 Flow Imaging Microscopy (FIM) 在识别生物制药中蛋白聚集物亚可见颗粒 (SvPs) 的应力来源方面的优势，以解决这些颗粒可能影响疗效并引发免疫反应的挑战。研究者创建了一个包含16,000个SvPs的新数据集，这些颗粒来源于八种商业单克隆抗体，并经受热和机械应力。利用监督和自监督卷积神经网络 (CNNs) 以及视觉变压器进行大规模实验，结果显示，深度学习模型在彩色FIM图像上比单色图像的表现更优越，提高了应力来源分类的准确性。总体而言，这突出了彩色FIM作为更有效的监测技术的潜力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted for publication in MICCAI 2024 Workshop on Medical Optical\n  Imaging and Virtual Microscopy Image Analysis (MOVI)",
      "pdf_url": "http://arxiv.org/pdf/2501.15492v1",
      "published_date": "2025-01-26 11:48:28 UTC",
      "updated_date": "2025-01-26 11:48:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:30:37.115254"
    },
    {
      "arxiv_id": "2501.15489v1",
      "title": "AI in Oncology: Transforming Cancer Detection through Machine Learning and Deep Learning Applications",
      "title_zh": "肿瘤学中的AI：通过机器学习和深度学习应用转变癌症检测",
      "authors": [
        "Muhammad Aftab",
        "Faisal Mehmood",
        "Chengjuan Zhang",
        "Alishba Nadeem",
        "Zigang Dong",
        "Yanan Jiang",
        "Kangdongs Liu"
      ],
      "abstract": "Artificial intelligence (AI) has potential to revolutionize the field of\noncology by enhancing the precision of cancer diagnosis, optimizing treatment\nstrategies, and personalizing therapies for a variety of cancers. This review\nexamines the limitations of conventional diagnostic techniques and explores the\ntransformative role of AI in diagnosing and treating cancers such as lung,\nbreast, colorectal, liver, stomach, esophageal, cervical, thyroid, prostate,\nand skin cancers. The primary objective of this paper is to highlight the\nsignificant advancements that AI algorithms have brought to oncology within the\nmedical industry. By enabling early cancer detection, improving diagnostic\naccuracy, and facilitating targeted treatment delivery, AI contributes to\nsubstantial improvements in patient outcomes. The integration of AI in medical\nimaging, genomic analysis, and pathology enhances diagnostic precision and\nintroduces a novel, less invasive approach to cancer screening. This not only\nboosts the effectiveness of medical facilities but also reduces operational\ncosts. The study delves into the application of AI in radiomics for detailed\ncancer characterization, predictive analytics for identifying associated risks,\nand the development of algorithm-driven robots for immediate diagnosis.\nFurthermore, it investigates the impact of AI on addressing healthcare\nchallenges, particularly in underserved and remote regions. The overarching\ngoal of this platform is to support the development of expert recommendations\nand to provide universal, efficient diagnostic procedures. By reviewing\nexisting research and clinical studies, this paper underscores the pivotal role\nof AI in improving the overall cancer care system. It emphasizes how AI-enabled\nsystems can enhance clinical decision-making and expand treatment options,\nthereby underscoring the importance of AI in advancing precision oncology",
      "tldr_zh": "这篇论文回顾了人工智能（AI）在肿瘤学中的应用，特别是通过机器学习（Machine Learning）和深度学习（Deep Learning）算法来提升癌症检测和治疗。该研究探讨了 AI 如何克服传统诊断技术的局限性，在肺癌、乳腺癌等各种癌症中实现早期检测、提高诊断准确性和个性化治疗，从而改善患者预后。论文强调 AI 在医学成像、基因组分析（genomic analysis）和病理学（pathology）中的整合，不仅降低了医疗成本，还通过 radiomics 和预测分析（predictive analytics）扩展了精准肿瘤学（precision oncology）的潜力。总的来说，该工作突出了 AI 在解决医疗挑战、增强临床决策并支持偏远地区医疗方面的关键作用。",
      "categories": [
        "cs.AI",
        "eess.IV",
        "q-bio.QM"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.15489v1",
      "published_date": "2025-01-26 11:32:43 UTC",
      "updated_date": "2025-01-26 11:32:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:30:48.555176"
    },
    {
      "arxiv_id": "2501.15486v1",
      "title": "FedAlign: Federated Domain Generalization with Cross-Client Feature Alignment",
      "title_zh": "翻译失败",
      "authors": [
        "Sunny Gupta",
        "Vinay Sutar",
        "Varunav Singh",
        "Amit Sethi"
      ],
      "abstract": "Federated Learning (FL) offers a decentralized paradigm for collaborative\nmodel training without direct data sharing, yet it poses unique challenges for\nDomain Generalization (DG), including strict privacy constraints, non-i.i.d.\nlocal data, and limited domain diversity. We introduce FedAlign, a lightweight,\nprivacy-preserving framework designed to enhance DG in federated settings by\nsimultaneously increasing feature diversity and promoting domain invariance.\nFirst, a cross-client feature extension module broadens local domain\nrepresentations through domain-invariant feature perturbation and selective\ncross-client feature transfer, allowing each client to safely access a richer\ndomain space. Second, a dual-stage alignment module refines global feature\nlearning by aligning both feature embeddings and predictions across clients,\nthereby distilling robust, domain-invariant features. By integrating these\nmodules, our method achieves superior generalization to unseen domains while\nmaintaining data privacy and operating with minimal computational and\ncommunication overhead.",
      "tldr_zh": "该研究针对联邦学习(Federated Learning, FL)中域泛化(Domain Generalization, DG)的挑战，包括隐私约束、非独立同分布数据和有限领域多样性，提出了一种轻量级隐私保护框架FedAlign。FedAlign通过跨客户端特征扩展模块，利用领域不变的特征扰动和选择性特征传输，扩展本地领域表示，使每个客户端能够访问更丰富的领域空间。其次，双阶段对齐模块(Dual-stage alignment module)通过在客户端之间对特征嵌入和预测进行对齐，提炼出鲁棒的领域不变特征。该框架实现了对未见领域的优越泛化，同时保持数据隐私，并以最小计算和通信开销为代价。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.DC",
        "I.2.6; C.1.4; D.1.3; I.5.1; H.3.4; I.2.10; I.4.0; I.4.1; I.4.2;\n  I.4.6; I.4.7; I.4.8; I.4.9; I.4.10; I.5.1; I.5.2; I.5.4; J.2; I.2.11; I.2.10"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.15486v1",
      "published_date": "2025-01-26 11:17:32 UTC",
      "updated_date": "2025-01-26 11:17:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:31:00.358812"
    },
    {
      "arxiv_id": "2501.15464v2",
      "title": "TractoGPT: A GPT architecture for White Matter Segmentation",
      "title_zh": "TractoGPT：一种用于白质分割的 GPT 架构",
      "authors": [
        "Anoushkrit Goel",
        "Simroop Singh",
        "Ankita Joshi",
        "Ranjeet Ranjan Jha",
        "Chirag Ahuja",
        "Aditya Nigam",
        "Arnav Bhavsar"
      ],
      "abstract": "White matter bundle segmentation is crucial for studying brain structural\nconnectivity, neurosurgical planning, and neurological disorders. White Matter\nSegmentation remains challenging due to structural similarity in streamlines,\nsubject variability, symmetry in 2 hemispheres, etc. To address these\nchallenges, we propose TractoGPT, a GPT-based architecture trained on\nstreamline, cluster, and fusion data representations separately. TractoGPT is a\nfully-automatic method that generalizes across datasets and retains shape\ninformation of the white matter bundles. Experiments also show that TractoGPT\noutperforms state-of-the-art methods on average DICE, Overlap and Overreach\nscores. We use TractoInferno and 105HCP datasets and validate generalization\nacross dataset.",
      "tldr_zh": "该研究针对白质束分割面临的挑战（如流线结构相似性、个体变异和半球对称性），提出TractoGPT，一种基于GPT的架构，分别在流线、簇和融合数据表示上进行训练，实现全自动分割并保留白质束的形状信息。TractoGPT在TractoInferno和105HCP数据集上表现出色，平均DICE、Overlap和Overreach分数优于现有最先进方法，并证明了其跨数据集的泛化能力。总的来说，该方法为大脑结构连接性研究、神经外科规划和神经疾病诊断提供了更可靠的工具。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted as a conference paper at 23rd IEEE International Symposium\n  on Biomedical Imaging 2025. IEEE holds the copyright for this publication",
      "pdf_url": "http://arxiv.org/pdf/2501.15464v2",
      "published_date": "2025-01-26 09:54:10 UTC",
      "updated_date": "2025-02-21 05:16:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:31:11.561980"
    },
    {
      "arxiv_id": "2501.17186v2",
      "title": "Complete Chess Games Enable LLM Become A Chess Master",
      "title_zh": "完整的国际象棋游戏使 LLM 成为国际象棋大师",
      "authors": [
        "Yinqi Zhang",
        "Xintian Han",
        "Haolong Li",
        "Kedi Chen",
        "Shaohui Lin"
      ],
      "abstract": "Large language models (LLM) have shown remarkable abilities in text\ngeneration, question answering, language translation, reasoning and many other\ntasks. It continues to advance rapidly and is becoming increasingly influential\nin various fields, from technology and business to education and entertainment.\nDespite LLM's success in multiple areas, its ability to play abstract games,\nsuch as chess, is underexplored. Chess-playing requires the language models to\noutput legal and reasonable moves from textual inputs. Here, we propose the\nLarge language model ChessLLM to play full chess games. We transform the game\ninto a textual format with the best move represented in the Forsyth-Edwards\nNotation. We show that by simply supervised fine-tuning, our model has achieved\na professional-level Elo rating of 1788 in matches against the standard\nElo-rated Stockfish when permitted to sample 10 times. We further show that\ndata quality is important. Long-round data supervision enjoys a 350 Elo rating\nimprovement over short-round data.",
      "tldr_zh": "这篇论文探讨了大型语言模型 (LLM) 在国际象棋游戏中的潜力，提出了一种名为 ChessLLM 的模型，使其能够从文本输入中输出合法合理的走法。研究方法包括将棋局转化为文本格式，使用 Forsyth-Edwards Notation 表示最佳走法，并通过监督微调 (supervised fine-tuning) 训练模型。实验结果显示，ChessLLM 在与 Stockfish 的比赛中达到了 1788 的专业级 Elo 评分（允许采样 10 次），并且使用长回合数据监督比短回合数据监督提高了 350 Elo 评分，突显了数据质量对模型性能的关键影响。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "NAACL 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.17186v2",
      "published_date": "2025-01-26 09:43:39 UTC",
      "updated_date": "2025-01-30 04:02:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:31:24.424727"
    },
    {
      "arxiv_id": "2501.15463v1",
      "title": "Mind the Value-Action Gap: Do LLMs Act in Alignment with Their Values?",
      "title_zh": "注意价值-行动差距：LLMs 是否与其价值观保持一致行动？",
      "authors": [
        "Hua Shen",
        "Nicholas Clark",
        "Tanushree Mitra"
      ],
      "abstract": "Existing research primarily evaluates the values of LLMs by examining their\nstated inclinations towards specific values. However, the \"Value-Action Gap,\" a\nphenomenon rooted in environmental and social psychology, reveals discrepancies\nbetween individuals' stated values and their actions in real-world contexts. To\nwhat extent do LLMs exhibit a similar gap between their stated values and their\nactions informed by those values? This study introduces ValueActionLens, an\nevaluation framework to assess the alignment between LLMs' stated values and\ntheir value-informed actions. The framework encompasses the generation of a\ndataset comprising 14.8k value-informed actions across twelve cultures and\neleven social topics, and two tasks to evaluate how well LLMs' stated value\ninclinations and value-informed actions align across three different alignment\nmeasures. Extensive experiments reveal that the alignment between LLMs' stated\nvalues and actions is sub-optimal, varying significantly across scenarios and\nmodels. Analysis of misaligned results identifies potential harms from certain\nvalue-action gaps. To predict the value-action gaps, we also uncover that\nleveraging reasoned explanations improves performance. These findings\nunderscore the risks of relying solely on the LLMs' stated values to predict\ntheir behaviors and emphasize the importance of context-aware evaluations of\nLLM values and value-action gaps.",
      "tldr_zh": "本文研究了 LLMs 的“Value-Action Gap”，即其陈述值（如对特定价值的偏好）和实际行动之间可能的不一致问题。研究引入 ValueActionLens 框架，包括生成一个涵盖 12 个文化和 11 个社会话题的 14.8k 行动数据集，以及两个任务来评估 LLMs 在三种对齐度量下的值-行动对齐。实验结果显示，对齐度较低且因场景和模型而异，可能导致潜在危害；同时，使用推理解释能有效预测这些差距。这些发现强调，不能仅依赖 LLMs 的陈述值预测行为，而需进行上下文感知的评估。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.15463v1",
      "published_date": "2025-01-26 09:33:51 UTC",
      "updated_date": "2025-01-26 09:33:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:31:36.300695"
    },
    {
      "arxiv_id": "2501.15452v1",
      "title": "Identifying Critical Tokens for Accurate Predictions in Transformer-based Medical Imaging Models",
      "title_zh": "基于 Transformer 的医学成像模型中识别关键标记以实现准确预测",
      "authors": [
        "Solha Kang",
        "Joris Vankerschaver",
        "Utku Ozbulak"
      ],
      "abstract": "With the advancements in self-supervised learning (SSL), transformer-based\ncomputer vision models have recently demonstrated superior results compared to\nconvolutional neural networks (CNNs) and are poised to dominate the field of\nartificial intelligence (AI)-based medical imaging in the upcoming years.\nNevertheless, similar to CNNs, unveiling the decision-making process of\ntransformer-based models remains a challenge. In this work, we take a step\ntowards demystifying the decision-making process of transformer-based medical\nimaging models and propose Token Insight, a novel method that identifies the\ncritical tokens that contribute to the prediction made by the model. Our method\nrelies on the principled approach of token discarding native to\ntransformer-based models, requires no additional module, and can be applied to\nany transformer model. Using the proposed approach, we quantify the importance\nof each token based on its contribution to the prediction and enable a more\nnuanced understanding of the model's decisions. Our experimental results which\nare showcased on the problem of colonic polyp identification using both\nsupervised and self-supervised pretrained vision transformers indicate that\nToken Insight contributes to a more transparent and interpretable\ntransformer-based medical imaging model, fostering trust and facilitating\nbroader adoption in clinical settings.",
      "tldr_zh": "这篇论文针对transformer-based医疗成像模型的决策过程提出Token Insight方法，用于识别关键tokens以提升预测准确性和模型透明度。该方法利用transformer模型原生的token discarding机制，量化每个token对预测的贡献，而无需额外模块，可应用于任何transformer模型。在结肠息肉识别任务上，使用监督和self-supervised预训练的vision transformers进行实验，结果表明Token Insight显著提高了模型的可解释性，促进了其在临床环境中的信任和采用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted for publication in MICCAI 2024 Workshop on Machine Learning\n  in Medical Imaging (MLMI)",
      "pdf_url": "http://arxiv.org/pdf/2501.15452v1",
      "published_date": "2025-01-26 08:49:13 UTC",
      "updated_date": "2025-01-26 08:49:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:31:47.847610"
    },
    {
      "arxiv_id": "2501.15448v1",
      "title": "SQ-DM: Accelerating Diffusion Models with Aggressive Quantization and Temporal Sparsity",
      "title_zh": "翻译失败",
      "authors": [
        "Zichen Fan",
        "Steve Dai",
        "Rangharajan Venkatesan",
        "Dennis Sylvester",
        "Brucek Khailany"
      ],
      "abstract": "Diffusion models have gained significant popularity in image generation\ntasks. However, generating high-quality content remains notably slow because it\nrequires running model inference over many time steps. To accelerate these\nmodels, we propose to aggressively quantize both weights and activations, while\nsimultaneously promoting significant activation sparsity. We further observe\nthat the stated sparsity pattern varies among different channels and evolves\nacross time steps. To support this quantization and sparsity scheme, we present\na novel diffusion model accelerator featuring a heterogeneous mixed-precision\ndense-sparse architecture, channel-last address mapping, and a time-step-aware\nsparsity detector for efficient handling of the sparsity pattern. Our 4-bit\nquantization technique demonstrates superior generation quality compared to\nexisting 4-bit methods. Our custom accelerator achieves 6.91x speed-up and\n51.5% energy reduction compared to traditional dense accelerators.",
      "tldr_zh": "该研究提出 SQ-DM 方法，以加速扩散模型（Diffusion Models）的图像生成过程，通过 aggressive quantization（对权重和激活进行量化）和 temporal sparsity（时间上的激活稀疏性）来优化性能。观察到稀疏模式在不同 channels 间变化并随 time steps 演变，因此设计了一个新型加速器，采用 heterogeneous mixed-precision dense-sparse architecture、channel-last address mapping 和 time-step-aware sparsity detector 来高效处理这些特性。与现有方法相比，该 4-bit quantization 技术在生成质量上表现出色，而自定义加速器实现了 6.91 倍速度提升和 51.5% 能量减少。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.AR",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "7 pages, 12 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2501.15448v1",
      "published_date": "2025-01-26 08:34:26 UTC",
      "updated_date": "2025-01-26 08:34:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:31:59.954990"
    },
    {
      "arxiv_id": "2501.15446v1",
      "title": "Token Democracy: The Architectural Limits of Alignment in Transformer-Based Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Robin Young"
      ],
      "abstract": "Modern language models paradoxically combine unprecedented capability with\npersistent vulnerability in that they can draft poetry yet cannot reliably\nrefuse harmful requests. We reveal this fragility stems not from inadequate\ntraining, but from a fundamental architectural limitation: transformers process\nall tokens as equals. Transformers operate as computational democracies,\ngranting equal voice to all tokens. This is a design tragically unsuited for\nAGI, where we cannot risk adversarial \"candidates\" hijacking the system.\nThrough formal analysis, we demonstrate that safety instructions fundamentally\nlack privileged status in transformer architectures, that they compete with\nadversarial inputs in the same computational arena, making robust alignment\nthrough prompting or fine-tuning inherently limited. This \"token democracy\"\nexplains why jailbreaks bypass even extensively safety-trained models and why\npositional shifts erode prompt effectiveness. Our work systematizes\npractitioners' tacit knowledge into an architectural critique, showing current\nalignment approaches create mere preferences, not constraints.",
      "tldr_zh": "本文研究揭示了Transformer-Based语言模型在对齐（alignment）方面的架构限制，即所有tokens被平等处理，形成“token democracy”，这导致安全指令无法获得特权地位，从而使模型易受对抗输入影响。作者通过正式分析证明，这种机制使得通过提示或微调实现的对齐方法仅创建偏好而非强约束，解释了jailbreaks为何能绕过安全训练模型。最终，该工作系统化了从业者的隐性知识，提供对AGI设计的架构批判，强调了当前方法的局限性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.15446v1",
      "published_date": "2025-01-26 08:26:06 UTC",
      "updated_date": "2025-01-26 08:26:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:32:12.104004"
    },
    {
      "arxiv_id": "2501.15445v2",
      "title": "StochSync: Stochastic Diffusion Synchronization for Image Generation in Arbitrary Spaces",
      "title_zh": "翻译失败",
      "authors": [
        "Kyeongmin Yeo",
        "Jaihoon Kim",
        "Minhyuk Sung"
      ],
      "abstract": "We propose a zero-shot method for generating images in arbitrary spaces\n(e.g., a sphere for 360{\\deg} panoramas and a mesh surface for texture) using a\npretrained image diffusion model. The zero-shot generation of various visual\ncontent using a pretrained image diffusion model has been explored mainly in\ntwo directions. First, Diffusion Synchronization-performing reverse diffusion\nprocesses jointly across different projected spaces while synchronizing them in\nthe target space-generates high-quality outputs when enough conditioning is\nprovided, but it struggles in its absence. Second, Score Distillation\nSampling-gradually updating the target space data through gradient\ndescent-results in better coherence but often lacks detail. In this paper, we\nreveal for the first time the interconnection between these two methods while\nhighlighting their differences. To this end, we propose StochSync, a novel\napproach that combines the strengths of both, enabling effective performance\nwith weak conditioning. Our experiments demonstrate that StochSync provides the\nbest performance in 360{\\deg} panorama generation (where image conditioning is\nnot given), outperforming previous finetuning-based methods, and also delivers\ncomparable results in 3D mesh texturing (where depth conditioning is provided)\nwith previous methods.",
      "tldr_zh": "本文提出 StochSync，一种零样本方法，通过随机扩散同步（Stochastic Diffusion Synchronization）利用预训练图像扩散模型，在任意空间（如360°全景或3D网格表面）生成高质量图像。StochSync 结合了 Diffusion Synchronization 和 Score Distillation Sampling 的优势，揭示了两者间的联系，并在弱条件环境下提升了生成效果。实验结果表明，该方法在无图像条件下的360°全景生成中优于现有微调方法，而在有深度条件下的3D网格纹理生成中，与之前方法性能相当。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://stochsync.github.io/ (ICLR 2025)",
      "pdf_url": "http://arxiv.org/pdf/2501.15445v2",
      "published_date": "2025-01-26 08:22:44 UTC",
      "updated_date": "2025-03-02 11:16:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:32:24.336749"
    },
    {
      "arxiv_id": "2501.16391v2",
      "title": "Inductive-Associative Meta-learning Pipeline with Human Cognitive Patterns for Unseen Drug-Target Interaction Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaoqing Lian",
        "Jie Zhu",
        "Tianxu Lv",
        "Shiyun Nie",
        "Hang Fan",
        "Guosheng Wu",
        "Yunjun Ge",
        "Lihua Li",
        "Xiangxiang Zeng",
        "Xiang Pan"
      ],
      "abstract": "Significant differences in protein structures hinder the generalization of\nexisting drug-target interaction (DTI) models, which often rely heavily on\npre-learned binding principles or detailed annotations. In contrast, BioBridge\ndesigns an Inductive-Associative pipeline inspired by the workflow of\nscientists who base their accumulated expertise on drawing insights into novel\ndrug-target pairs from weakly related references. BioBridge predicts novel\ndrug-target interactions using limited sequence data, incorporating multi-level\nencoders with adversarial training to accumulate transferable binding\nprinciples. On these principles basis, BioBridge employs a dynamic prototype\nmeta-learning framework to associate insights from weakly related annotations,\nenabling robust predictions for previously unseen drug-target pairs. Extensive\nexperiments demonstrate that BioBridge surpasses existing models, especially\nfor unseen proteins. Notably, when only homologous protein binding data is\navailable, BioBridge proves effective for virtual screening of the epidermal\ngrowth factor receptor and adenosine receptor, underscoring its potential in\ndrug discovery.",
      "tldr_zh": "该研究提出BioBridge，一种受人类认知模式启发的Inductive-Associative元学习管道，用于预测未见过的药物-靶点交互(DTI)，以解决现有模型在蛋白结构差异上的泛化问题。BioBridge利用多级编码器和对抗训练(adversarial training)积累可转移的结合原则，并通过动态原型元学习(dynamic prototype meta-learning)框架从弱相关注释中关联洞见，实现基于有限序列数据的鲁棒预测。实验结果显示，BioBridge在各种数据集上超越现有模型，尤其在未见蛋白的预测中表现出色，并在表皮生长因子受体和腺苷受体的虚拟筛选中证明了其在药物发现中的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.16391v2",
      "published_date": "2025-01-26 08:22:22 UTC",
      "updated_date": "2025-03-27 07:41:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:32:35.846727"
    },
    {
      "arxiv_id": "2501.15442v2",
      "title": "Overview of the Amphion Toolkit (v0.2)",
      "title_zh": "Amphion 工具包 (v0.2) 概述",
      "authors": [
        "Jiaqi Li",
        "Xueyao Zhang",
        "Yuancheng Wang",
        "Haorui He",
        "Chaoren Wang",
        "Li Wang",
        "Huan Liao",
        "Junyi Ao",
        "Zeyu Xie",
        "Yiqiao Huang",
        "Junan Zhang",
        "Zhizheng Wu"
      ],
      "abstract": "Amphion is an open-source toolkit for Audio, Music, and Speech Generation,\ndesigned to lower the entry barrier for junior researchers and engineers in\nthese fields. It provides a versatile framework that supports a variety of\ngeneration tasks and models. In this report, we introduce Amphion v0.2, the\nsecond major release developed in 2024. This release features a 100K-hour\nopen-source multilingual dataset, a robust data preparation pipeline, and novel\nmodels for tasks such as text-to-speech, audio coding, and voice conversion.\nFurthermore, the report includes multiple tutorials that guide users through\nthe functionalities and usage of the newly released models.",
      "tldr_zh": "Amphion 是一个开源工具包，专注于音频、音乐和语音生成任务，旨在降低初级研究者和工程师的入门门槛，提供多功能的框架支持各种生成模型。v0.2 版本引入了一个 100K 小时的开源多语言数据集、稳健的数据准备管道，以及针对 text-to-speech、audio coding 和 voice conversion 等任务的新模型。这些更新还附带多个教程，帮助用户快速上手和应用新功能。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Github: https://github.com/open-mmlab/Amphion",
      "pdf_url": "http://arxiv.org/pdf/2501.15442v2",
      "published_date": "2025-01-26 08:10:13 UTC",
      "updated_date": "2025-02-11 13:05:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:32:47.701599"
    },
    {
      "arxiv_id": "2502.09624v1",
      "title": "Efficient and Trustworthy Block Propagation for Blockchain-enabled Mobile Embodied AI Networks: A Graph Resfusion Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Jiawen Kang",
        "Jiana Liao",
        "Runquan Gao",
        "Jinbo Wen",
        "Huawei Huang",
        "Maomao Zhang",
        "Changyan Yi",
        "Tao Zhang",
        "Dusit Niyato",
        "Zibin Zheng"
      ],
      "abstract": "By synergistically integrating mobile networks and embodied artificial\nintelligence (AI), Mobile Embodied AI Networks (MEANETs) represent an advanced\nparadigm that facilitates autonomous, context-aware, and interactive behaviors\nwithin dynamic environments. Nevertheless, the rapid development of MEANETs is\naccompanied by challenges in trustworthiness and operational efficiency.\nFortunately, blockchain technology, with its decentralized and immutable\ncharacteristics, offers promising solutions for MEANETs. However, existing\nblock propagation mechanisms suffer from challenges such as low propagation\nefficiency and weak security for block propagation, which results in delayed\ntransmission of vehicular messages or vulnerability to malicious tampering,\npotentially causing severe traffic accidents in blockchain-enabled MEANETs.\nMoreover, current block propagation strategies cannot effectively adapt to\nreal-time changes of dynamic topology in MEANETs. Therefore, in this paper, we\npropose a graph Resfusion model-based trustworthy block propagation\noptimization framework for consortium blockchain-enabled MEANETs. Specifically,\nwe propose an innovative trust calculation mechanism based on the trust cloud\nmodel, which comprehensively accounts for randomness and fuzziness in the miner\ntrust evaluation. Furthermore, by leveraging the strengths of graph neural\nnetworks and diffusion models, we develop a graph Resfusion model to\neffectively and adaptively generate the optimal block propagation trajectory.\nSimulation results demonstrate that the proposed model outperforms other\nrouting mechanisms in terms of block propagation efficiency and\ntrustworthiness. Additionally, the results highlight its strong adaptability to\ndynamic environments, making it particularly suitable for rapidly changing\nMEANETs.",
      "tldr_zh": "该研究针对区块链启用的 Mobile Embodied AI Networks (MEANETs) 中块传播的效率和信任问题，提出了一种基于图 Resfusion 模型的优化框架，以适应动态拓扑环境。具体而言，该框架引入了一个基于信任云模型的创新机制，综合考虑矿工信任评估中的随机性和模糊性；同时，利用图神经网络和扩散模型生成最优的块传播轨迹。模拟结果显示，该方法在块传播效率和信任性方面优于其他路由机制，并展现出强大的动态环境适应性，为区块链-enabled MEANETs 的安全应用提供了可靠基础。",
      "categories": [
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.AI",
      "comment": "15 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.09624v1",
      "published_date": "2025-01-26 07:47:05 UTC",
      "updated_date": "2025-01-26 07:47:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:32:59.695178"
    },
    {
      "arxiv_id": "2501.15431v1",
      "title": "Self-supervised Benchmark Lottery on ImageNet: Do Marginal Improvements Translate to Improvements on Similar Datasets?",
      "title_zh": "ImageNet 上的自监督基准测试彩票：微小改进是否能转化为类似数据集上的改进？",
      "authors": [
        "Utku Ozbulak",
        "Esla Timothy Anzaku",
        "Solha Kang",
        "Wesley De Neve",
        "Joris Vankerschaver"
      ],
      "abstract": "Machine learning (ML) research strongly relies on benchmarks in order to\ndetermine the relative effectiveness of newly proposed models. Recently, a\nnumber of prominent research effort argued that a number of models that improve\nthe state-of-the-art by a small margin tend to do so by winning what they call\na \"benchmark lottery\". An important benchmark in the field of machine learning\nand computer vision is the ImageNet where newly proposed models are often\nshowcased based on their performance on this dataset. Given the large number of\nself-supervised learning (SSL) frameworks that has been proposed in the past\ncouple of years each coming with marginal improvements on the ImageNet dataset,\nin this work, we evaluate whether those marginal improvements on ImageNet\ntranslate to improvements on similar datasets or not. To do so, we investigate\ntwelve popular SSL frameworks on five ImageNet variants and discover that\nmodels that seem to perform well on ImageNet may experience significant\nperformance declines on similar datasets. Specifically, state-of-the-art\nframeworks such as DINO and Swav, which are praised for their performance,\nexhibit substantial drops in performance while MoCo and Barlow Twins displays\ncomparatively good results. As a result, we argue that otherwise good and\ndesirable properties of models remain hidden when benchmarking is only\nperformed on the ImageNet validation set, making us call for more adequate\nbenchmarking. To avoid the \"benchmark lottery\" on ImageNet and to ensure a fair\nbenchmarking process, we investigate the usage of a unified metric that takes\ninto account the performance of models on other ImageNet variant datasets.",
      "tldr_zh": "本研究探讨了自监督学习 (SSL) 框架在 ImageNet 数据集上的微小改进是否能转化为类似数据集上的性能提升，旨在揭示“基准彩票”现象。作者评估了 12 个流行 SSL 框架在 ImageNet 和其 5 个变体数据集上的表现，发现许多模型（如 DINO 和 SwAV）在 ImageNet 上表现出色，但转移到类似数据集时性能显著下降，而 MoCo 和 Barlow Twins 则显示出较好的鲁棒性。通过这些实验，论文揭示了仅依赖 ImageNet 验证集进行基准测试会隐藏模型的实际优势，并提出使用统一指标来综合考虑模型在 ImageNet 变体上的性能，以促进更公平的基准评估。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted for publication in the 2024 International Joint Conference\n  on Neural Networks (IJCNN)",
      "pdf_url": "http://arxiv.org/pdf/2501.15431v1",
      "published_date": "2025-01-26 07:19:12 UTC",
      "updated_date": "2025-01-26 07:19:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:33:12.264299"
    },
    {
      "arxiv_id": "2501.15420v1",
      "title": "Visual Generation Without Guidance",
      "title_zh": "无",
      "authors": [
        "Huayu Chen",
        "Kai Jiang",
        "Kaiwen Zheng",
        "Jianfei Chen",
        "Hang Su",
        "Jun Zhu"
      ],
      "abstract": "Classifier-Free Guidance (CFG) has been a default technique in various visual\ngenerative models, yet it requires inference from both conditional and\nunconditional models during sampling. We propose to build visual models that\nare free from guided sampling. The resulting algorithm, Guidance-Free Training\n(GFT), matches the performance of CFG while reducing sampling to a single\nmodel, halving the computational cost. Unlike previous distillation-based\napproaches that rely on pretrained CFG networks, GFT enables training directly\nfrom scratch. GFT is simple to implement. It retains the same maximum\nlikelihood objective as CFG and differs mainly in the parameterization of\nconditional models. Implementing GFT requires only minimal modifications to\nexisting codebases, as most design choices and hyperparameters are directly\ninherited from CFG. Our extensive experiments across five distinct visual\nmodels demonstrate the effectiveness and versatility of GFT. Across domains of\ndiffusion, autoregressive, and masked-prediction modeling, GFT consistently\nachieves comparable or even lower FID scores, with similar diversity-fidelity\ntrade-offs compared with CFG baselines, all while being guidance-free. Code\nwill be available at https://github.com/thu-ml/GFT.",
      "tldr_zh": "该论文提出 Guidance-Free Training (GFT) 方法，以解决 Classifier-Free Guidance (CFG) 在视觉生成模型中需要同时使用条件和无条件模型进行推理的效率问题。GFT 允许从零开始训练，仅需单个模型采样，从而将计算成本减半，同时保持与 CFG 相同的最大似然目标和性能表现。在五个不同视觉模型（包括扩散、autoregressive 和 masked-prediction 建模）的实验中，GFT 实现了与 CFG 相当或更低的 FID scores，并保持相似的多样性-保真度 tradeoff，证明了其有效性和易用性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.15420v1",
      "published_date": "2025-01-26 06:48:05 UTC",
      "updated_date": "2025-01-26 06:48:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:33:24.400027"
    },
    {
      "arxiv_id": "2501.15418v1",
      "title": "Episodic Novelty Through Temporal Distance",
      "title_zh": "翻译失败",
      "authors": [
        "Yuhua Jiang",
        "Qihan Liu",
        "Yiqin Yang",
        "Xiaoteng Ma",
        "Dianyu Zhong",
        "Hao Hu",
        "Jun Yang",
        "Bin Liang",
        "Bo Xu",
        "Chongjie Zhang",
        "Qianchuan Zhao"
      ],
      "abstract": "Exploration in sparse reward environments remains a significant challenge in\nreinforcement learning, particularly in Contextual Markov Decision Processes\n(CMDPs), where environments differ across episodes. Existing episodic intrinsic\nmotivation methods for CMDPs primarily rely on count-based approaches, which\nare ineffective in large state spaces, or on similarity-based methods that lack\nappropriate metrics for state comparison. To address these shortcomings, we\npropose Episodic Novelty Through Temporal Distance (ETD), a novel approach that\nintroduces temporal distance as a robust metric for state similarity and\nintrinsic reward computation. By employing contrastive learning, ETD accurately\nestimates temporal distances and derives intrinsic rewards based on the novelty\nof states within the current episode. Extensive experiments on various\nbenchmark tasks demonstrate that ETD significantly outperforms state-of-the-art\nmethods, highlighting its effectiveness in enhancing exploration in sparse\nreward CMDPs.",
      "tldr_zh": "在强化学习中，Contextual Markov Decision Processes (CMDPs) 的稀疏奖励环境探索面临挑战，现有的基于计数或相似性的方法因状态空间大或缺乏合适指标而效果不佳。论文提出 Episodic Novelty Through Temporal Distance (ETD)，一种新方法，使用时间距离作为状态相似性的度量，通过 contrastive learning 准确估计距离并计算基于状态新颖性的内在奖励。该方法在各种基准任务上的广泛实验中显著优于最先进方法，提升了 CMDPs 中的探索效率。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "ICLR2025",
      "pdf_url": "http://arxiv.org/pdf/2501.15418v1",
      "published_date": "2025-01-26 06:43:45 UTC",
      "updated_date": "2025-01-26 06:43:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:33:35.365858"
    },
    {
      "arxiv_id": "2501.15417v1",
      "title": "AnyEnhance: A Unified Generative Model with Prompt-Guidance and Self-Critic for Voice Enhancement",
      "title_zh": "AnyEnhance",
      "authors": [
        "Junan Zhang",
        "Jing Yang",
        "Zihao Fang",
        "Yuancheng Wang",
        "Zehua Zhang",
        "Zhuo Wang",
        "Fan Fan",
        "Zhizheng Wu"
      ],
      "abstract": "We introduce AnyEnhance, a unified generative model for voice enhancement\nthat processes both speech and singing voices. Based on a masked generative\nmodel, AnyEnhance is capable of handling both speech and singing voices,\nsupporting a wide range of enhancement tasks including denoising,\ndereverberation, declipping, super-resolution, and target speaker extraction,\nall simultaneously and without fine-tuning. AnyEnhance introduces a\nprompt-guidance mechanism for in-context learning, which allows the model to\nnatively accept a reference speaker's timbre. In this way, it could boost\nenhancement performance when a reference audio is available and enable the\ntarget speaker extraction task without altering the underlying architecture.\nMoreover, we also introduce a self-critic mechanism into the generative process\nfor masked generative models, yielding higher-quality outputs through iterative\nself-assessment and refinement. Extensive experiments on various enhancement\ntasks demonstrate AnyEnhance outperforms existing methods in terms of both\nobjective metrics and subjective listening tests. Demo audios are publicly\navailable at https://amphionspace.github.io/anyenhance/.",
      "tldr_zh": "我们介绍了 AnyEnhance，一种基于 masked generative model 的统一生成模型，能够同时处理语音和歌声的多种增强任务，包括去噪、去混响、去剪切、超分辨率和目标说话者提取，而无需微调。  \n该模型引入 prompt-guidance 机制，支持 in-context learning，通过参考说话者的音色提升性能，并启用目标说话者提取任务；此外，还采用 self-critic 机制，通过迭代自评估和精炼来提高输出质量。  \n实验结果表明，AnyEnhance 在各种增强任务上，在客观指标和主观听力测试中均优于现有方法。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "12 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.15417v1",
      "published_date": "2025-01-26 06:40:30 UTC",
      "updated_date": "2025-01-26 06:40:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:33:48.265761"
    },
    {
      "arxiv_id": "2501.15409v1",
      "title": "TdAttenMix: Top-Down Attention Guided Mixup",
      "title_zh": "翻译失败",
      "authors": [
        "Zhiming Wang",
        "Lin Gu",
        "Feng Lu"
      ],
      "abstract": "CutMix is a data augmentation strategy that cuts and pastes image patches to\nmixup training data. Existing methods pick either random or salient areas which\nare often inconsistent to labels, thus misguiding the training model. By our\nknowledge, we integrate human gaze to guide cutmix for the first time. Since\nhuman attention is driven by both high-level recognition and low-level clues,\nwe propose a controllable Top-down Attention Guided Module to obtain a general\nartificial attention which balances top-down and bottom-up attention. The\nproposed TdATttenMix then picks the patches and adjust the label mixing ratio\nthat focuses on regions relevant to the current label. Experimental results\ndemonstrate that our TdAttenMix outperforms existing state-of-the-art mixup\nmethods across eight different benchmarks. Additionally, we introduce a new\nmetric based on the human gaze and use this metric to investigate the issue of\nimage-label inconsistency. Project page:\n\\url{https://github.com/morning12138/TdAttenMix}",
      "tldr_zh": "本文提出 TdAttenMix，一种基于 Top-down Attention Guided Mixup 的数据增强方法，首次整合人类注视来指导 CutMix，避免现有方法因随机或显著区域选择不一致而误导训练模型。  \n该方法引入可控的 Top-down Attention Guided Module，以平衡高层识别（如标签相关区域）和底层线索，精确选择图像补丁并调整标签混合比例。  \n实验结果显示，TdAttenMix 在八个不同基准上优于现有最先进 Mixup 方法，并引入基于人类注视的新指标，用于分析图像-标签不一致问题。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.15409v1",
      "published_date": "2025-01-26 05:32:37 UTC",
      "updated_date": "2025-01-26 05:32:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:34:00.991874"
    },
    {
      "arxiv_id": "2501.15407v1",
      "title": "Turn That Frown Upside Down: FaceID Customization via Cross-Training Data",
      "title_zh": "把皱眉翻转过来：通过交叉训练数据进行 FaceID 定制",
      "authors": [
        "Shuhe Wang",
        "Xiaoya Li",
        "Xiaofei Sun",
        "Guoyin Wang",
        "Tianwei Zhang",
        "Jiwei Li",
        "Eduard Hovy"
      ],
      "abstract": "Existing face identity (FaceID) customization methods perform well but are\nlimited to generating identical faces as the input, while in real-world\napplications, users often desire images of the same person but with variations,\nsuch as different expressions (e.g., smiling, angry) or angles (e.g., side\nprofile). This limitation arises from the lack of datasets with controlled\ninput-output facial variations, restricting models' ability to learn effective\nmodifications.\n  To address this issue, we propose CrossFaceID, the first large-scale,\nhigh-quality, and publicly available dataset specifically designed to improve\nthe facial modification capabilities of FaceID customization models.\nSpecifically, CrossFaceID consists of 40,000 text-image pairs from\napproximately 2,000 persons, with each person represented by around 20 images\nshowcasing diverse facial attributes such as poses, expressions, angles, and\nadornments. During the training stage, a specific face of a person is used as\ninput, and the FaceID customization model is forced to generate another image\nof the same person but with altered facial features. This allows the FaceID\ncustomization model to acquire the ability to personalize and modify known\nfacial features during the inference stage. Experiments show that models\nfine-tuned on the CrossFaceID dataset retain its performance in preserving\nFaceID fidelity while significantly improving its face customization\ncapabilities.\n  To facilitate further advancements in the FaceID customization field, our\ncode, constructed datasets, and trained models are fully available to the\npublic.",
      "tldr_zh": "本论文解决了现有 FaceID 定制方法仅能生成输入相同脸部图像的局限性，提出 CrossFaceID，这是一个大规模、高质量的公开数据集，包含约 2,000 个人的 40,000 个文本-图像对，涵盖多样面部属性如表情、角度和装饰。\n通过交叉训练策略，使用特定面部图像作为输入，模型被训练生成同一人但修改了面部特征的图像，从而提升 FaceID 定制模型的个性化修改能力。\n实验结果表明，在 CrossFaceID 上微调的模型保持了 FaceID 保真度，同时显著提高了面部定制性能。\n此外，论文公开了代码、数据集和训练模型，以促进 FaceID 定制领域的进一步发展。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.15407v1",
      "published_date": "2025-01-26 05:27:38 UTC",
      "updated_date": "2025-01-26 05:27:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:34:12.899745"
    },
    {
      "arxiv_id": "2501.15405v2",
      "title": "Semantic Layered Embedding Diffusion in Large Language Models for Multi-Contextual Consistency",
      "title_zh": "翻译失败",
      "authors": [
        "Irin Kabakum",
        "Thomas Montgomery",
        "Daniel Ravenwood",
        "Genevieve Harrington"
      ],
      "abstract": "The Semantic Layered Embedding Diffusion (SLED) mechanism redefines the\nrepresentation of hierarchical semantics within transformer-based\narchitectures, enabling enhanced contextual consistency across a wide array of\nlinguistic tasks. By introducing a multi-layered diffusion process grounded in\nspectral analysis, it achieves a complex balance between global and local\nsemantic coherence. Experimental results demonstrate significant improvements\nin perplexity and BLEU scores, emphasizing the mechanism's ability to adapt\neffectively across diverse domains, including multilingual and cross-domain\ntext generation. A rigorous mathematical framework underpins the embedding\ndiffusion process, incorporating weighted adjacency matrices, kernel-based\nrefinements, and dynamic layer-wise normalization. Error distribution analysis\nreveals that SLED addresses challenges in semantic alignment and coherence,\noutperforming baseline approaches across varied benchmarks. Scalability studies\nillustrate that its performance gains are maintained consistently across\ndifferent model sizes, reflecting a practical balance between computational\nefficiency and linguistic precision. The implementation also achieves energy\nefficiency, reducing resource consumption during training and inference phases\nwithout compromising accuracy. Qualitative case studies further validate its\nadaptability to extended narratives and context-intensive scenarios,\nhighlighting the mechanism's potential for real-world applications. SLED offers\na different perspective on embedding design and its implications for advancing\nlanguage modeling.",
      "tldr_zh": "这篇论文提出了 Semantic Layered Embedding Diffusion (SLED) 机制，用于在大型语言模型中重定义层次语义表示，以提升多语境一致性。SLED 通过多层扩散过程和基于谱分析的数学框架（如加权邻接矩阵、内核-based 精炼和动态层-wise 归一化），实现了全局和局部语义的平衡。实验结果显示，SLED 在 perplexity 和 BLEU 分数上显著改善，并在多语言和跨领域文本生成任务中优于基线模型，同时保持了可扩展性、计算效率和能源节约。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship",
      "pdf_url": "http://arxiv.org/pdf/2501.15405v2",
      "published_date": "2025-01-26 05:17:04 UTC",
      "updated_date": "2025-03-25 12:55:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:34:24.532804"
    },
    {
      "arxiv_id": "2501.15404v1",
      "title": "A Neurosymbolic Framework for Geometric Reduction of Binary Forms",
      "title_zh": "翻译失败",
      "authors": [
        "Ilias Kotsireas",
        "Tony Shaska"
      ],
      "abstract": "This paper compares Julia reduction and hyperbolic reduction with the aim of\nfinding equivalent binary forms with minimal coefficients. We demonstrate that\nhyperbolic reduction generally outperforms Julia reduction, particularly in the\ncases of sextics and decimics, though neither method guarantees achieving the\nminimal form. We further propose an additional shift and scaling to approximate\nthe minimal form more closely. Finally, we introduce a machine learning\nframework to identify optimal transformations that minimize the heights of\nbinary forms. This study provides new insights into the geometry and algebra of\nbinary forms and highlights the potential of AI in advancing symbolic\ncomputation and reduction techniques. The findings, supported by extensive\ncomputational experiments, lay the groundwork for hybrid approaches that\nintegrate traditional reduction methods with data-driven techniques.",
      "tldr_zh": "这篇论文比较了 Julia reduction 和 hyperbolic reduction 的性能，以寻找具有最小系数的等价二元形式。研究发现，hyperbolic reduction 在 sextics 和 decimics 等情况下通常优于 Julia reduction，但两者均无法保证达到最小形式。作者提出额外的 shift 和 scaling 技术来更接近最小形式，并引入一个 machine learning framework 来识别最小化二元形式高度的最优变换。该研究为二元形式的几何和代数提供了新见解，并突出了 AI 在符号计算和归约技术中的潜力，支持了传统方法与数据驱动方法的混合应用。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "I.2.3"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.15404v1",
      "published_date": "2025-01-26 05:15:08 UTC",
      "updated_date": "2025-01-26 05:15:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:34:36.402862"
    },
    {
      "arxiv_id": "2501.15393v1",
      "title": "Diffusion-based Hierarchical Negative Sampling for Multimodal Knowledge Graph Completion",
      "title_zh": "翻译失败",
      "authors": [
        "Guanglin Niu",
        "Xiaowei Zhang"
      ],
      "abstract": "Multimodal Knowledge Graph Completion (MMKGC) aims to address the critical\nissue of missing knowledge in multimodal knowledge graphs (MMKGs) for their\nbetter applications. However, both the previous MMGKC and negative sampling\n(NS) approaches ignore the employment of multimodal information to generate\ndiverse and high-quality negative triples from various semantic levels and\nhardness levels, thereby limiting the effectiveness of training MMKGC models.\nThus, we propose a novel Diffusion-based Hierarchical Negative Sampling (DHNS)\nscheme tailored for MMKGC tasks, which tackles the challenge of generating\nhigh-quality negative triples by leveraging a Diffusion-based Hierarchical\nEmbedding Generation (DiffHEG) that progressively conditions on entities and\nrelations as well as multimodal semantics. Furthermore, we develop a Negative\nTriple-Adaptive Training (NTAT) strategy that dynamically adjusts training\nmargins associated with the hardness level of the synthesized negative triples,\nfacilitating a more robust and effective learning procedure to distinguish\nbetween positive and negative triples. Extensive experiments on three MMKGC\nbenchmark datasets demonstrate that our framework outperforms several\nstate-of-the-art MMKGC models and negative sampling techniques, illustrating\nthe effectiveness of our DHNS for training MMKGC models. The source codes and\ndatasets of this paper are available at https://github.com/ngl567/DHNS.",
      "tldr_zh": "本研究针对多模态知识图谱补全(Multimodal Knowledge Graph Completion, MMKGC)问题，提出了一种新型的基于扩散的层次负采样方案(Diffusion-based Hierarchical Negative Sampling, DHNS)，以生成多样化和高质量的负样本三元组，从而提升模型训练有效性。DHNS 包括 Diffusion-based Hierarchical Embedding Generation (DiffHEG) 模块，该模块逐步整合实体、关系及多模态语义来生成负样本，以及 Negative Triple-Adaptive Training (NTAT) 策略，通过动态调整负样本的难度水平来优化训练过程，确保模型更好地区分正负三元组。在三个 MMKGC 基准数据集上的广泛实验表明，该框架优于现有最先进模型，证明了 DHNS 在提升 MMKGC 性能方面的显著效果。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "I.2.7"
      ],
      "primary_category": "cs.AI",
      "comment": "The version of a full paper accepted to DASFAA 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.15393v1",
      "published_date": "2025-01-26 04:20:34 UTC",
      "updated_date": "2025-01-26 04:20:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:34:48.026855"
    },
    {
      "arxiv_id": "2501.15392v3",
      "title": "Faster Configuration Performance Bug Testing with Neural Dual-level Prioritization",
      "title_zh": "翻译失败",
      "authors": [
        "Youpeng Ma",
        "Tao Chen",
        "Ke Li"
      ],
      "abstract": "As software systems become more complex and configurable, more performance\nproblems tend to arise from the configuration designs. This has caused some\nconfiguration options to unexpectedly degrade performance which deviates from\ntheir original expectations designed by the developers. Such discrepancies,\nnamely configuration performance bugs (CPBugs), are devastating and can be\ndeeply hidden in the source code. Yet, efficiently testing CPBugs is difficult,\nnot only due to the test oracle is hard to set, but also because the\nconfiguration measurement is expensive and there are simply too many possible\nconfigurations to test. As such, existing testing tools suffer from lengthy\nruntime or have been ineffective in detecting CPBugs when the budget is\nlimited, compounded by inaccurate test oracle. In this paper, we seek to\nachieve significantly faster CPBug testing by neurally prioritizing the testing\nat both the configuration option and value range levels with automated oracle\nestimation. Our proposed tool, dubbed NDP, is a general framework that works\nwith different heuristic generators. The idea is to leverage two neural\nlanguage models: one to estimate the CPBug types that serve as the oracle\nwhile, more vitally, the other to infer the probabilities of an option being\nCPBug-related, based on which the options and the value ranges to be searched\ncan be prioritized. Experiments on several widely-used systems of different\nversions reveal that NDP can, in general, better predict CPBug type in 87%\ncases and find more CPBugs with up to 88.88x testing efficiency speedup over\nthe state-of-the-art tools.",
      "tldr_zh": "该论文针对软件系统中配置性能错误（CPBugs）的检测难题，提出了一种神经双层优先级排序（NDP）框架，以提高测试效率并自动估计测试预言机。NDP利用两个神经语言模型：一个用于预测CPBug类型作为预言机，另一个基于选项与CPBug相关概率，对配置选项和值范围进行优先级排序，从而减少测试开销。实验结果显示，NDP在多个系统版本上准确预测CPBug类型达87%，并比现有工具提升高达88.88倍的测试效率，显著提高了CPBugs的检测能力。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "accepted by ICSE 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.15392v3",
      "published_date": "2025-01-26 04:19:43 UTC",
      "updated_date": "2025-04-15 10:25:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:35:00.140220"
    },
    {
      "arxiv_id": "2501.15384v1",
      "title": "MetaOcc: Surround-View 4D Radar and Camera Fusion Framework for 3D Occupancy Prediction with Dual Training Strategies",
      "title_zh": "MetaOcc：环视视图4D雷达和相机融合框架，用于3D占用预测的双重训练策略",
      "authors": [
        "Long Yang",
        "Lianqing Zheng",
        "Wenjin Ai",
        "Minghao Liu",
        "Sen Li",
        "Qunshu Lin",
        "Shengyu Yan",
        "Jie Bai",
        "Zhixiong Ma",
        "Xichan Zhu"
      ],
      "abstract": "3D occupancy prediction is crucial for autonomous driving perception. Fusion\nof 4D radar and camera provides a potential solution of robust occupancy\nprediction on serve weather with least cost. How to achieve effective\nmulti-modal feature fusion and reduce annotation costs remains significant\nchallenges. In this work, we propose MetaOcc, a novel multi-modal occupancy\nprediction framework that fuses surround-view cameras and 4D radar for\ncomprehensive environmental perception. We first design a height self-attention\nmodule for effective 3D feature extraction from sparse radar points. Then, a\nlocal-global fusion mechanism is proposed to adaptively capture modality\ncontributions while handling spatio-temporal misalignments. Temporal alignment\nand fusion module is employed to further aggregate historical feature.\nFurthermore, we develop a semi-supervised training procedure leveraging\nopen-set segmentor and geometric constraints for pseudo-label generation,\nenabling robust perception with limited annotations. Extensive experiments on\nOmniHD-Scenes dataset demonstrate that MetaOcc achieves state-of-the-art\nperformance, surpassing previous methods by significant margins. Notably, as\nthe first semi-supervised 4D radar and camera fusion-based occupancy prediction\napproach, MetaOcc maintains 92.5% of the fully-supervised performance while\nusing only 50% of ground truth annotations, establishing a new benchmark for\nmulti-modal 3D occupancy prediction. Code and data are available at\nhttps://github.com/LucasYang567/MetaOcc.",
      "tldr_zh": "本研究提出MetaOcc框架，通过融合环视摄像头和4D radar，实现鲁棒的3D occupancy prediction，以提升自动驾驶环境感知能力。该框架包括高度自注意力模块用于从稀疏雷达点提取3D特征、局部-全局融合机制处理模态贡献和时空不对齐，以及temporal alignment and fusion module聚合历史特征。同时，引入半监督训练过程，利用open-set segmentor和几何约束生成伪标签，显著减少标注成本。在OmniHD-Scenes数据集上的实验显示，MetaOcc超越了现有方法，使用仅50%的标注数据即可达到92.5%的全监督性能，树立了多模态3D占用预测的新基准。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.15384v1",
      "published_date": "2025-01-26 03:51:56 UTC",
      "updated_date": "2025-01-26 03:51:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:35:12.050711"
    },
    {
      "arxiv_id": "2501.15379v1",
      "title": "Zero-Shot Interactive Text-to-Image Retrieval via Diffusion-Augmented Representations",
      "title_zh": "翻译失败",
      "authors": [
        "Zijun Long",
        "Kangheng Liang",
        "Gerardo Aragon-Camarasa",
        "Richard Mccreadie",
        "Paul Henderson"
      ],
      "abstract": "Interactive Text-to-Image Retrieval (I-TIR) has emerged as a transformative\nuser-interactive tool for applications in domains such as e-commerce and\neducation. Yet, current methodologies predominantly depend on finetuned\nMultimodal Large Language Models (MLLMs), which face two critical limitations:\n(1) Finetuning imposes prohibitive computational overhead and long-term\nmaintenance costs. (2) Finetuning narrows the pretrained knowledge distribution\nof MLLMs, reducing their adaptability to novel scenarios. These issues are\nexacerbated by the inherently dynamic nature of real-world I-TIR systems, where\nqueries and image databases evolve in complexity and diversity, often deviating\nfrom static training distributions. To overcome these constraints, we propose\nDiffusion Augmented Retrieval (DAR), a paradigm-shifting framework that\nbypasses MLLM finetuning entirely. DAR synergizes Large Language Model\n(LLM)-guided query refinement with Diffusion Model (DM)-based visual synthesis\nto create contextually enriched intermediate representations. This\ndual-modality approach deciphers nuanced user intent more holistically,\nenabling precise alignment between textual queries and visually relevant\nimages. Rigorous evaluations across four benchmarks reveal DAR's dual\nstrengths: (1) Matches state-of-the-art finetuned I-TIR models on\nstraightforward queries without task-specific training. (2) Scalable\nGeneralization: Surpasses finetuned baselines by 7.61% in Hits@10 (top-10\naccuracy) under multi-turn conversational complexity, demonstrating robustness\nto intricate, distributionally shifted interactions. By eliminating finetuning\ndependencies and leveraging generative-augmented representations, DAR\nestablishes a new trajectory for efficient, adaptive, and scalable cross-modal\nretrieval systems.",
      "tldr_zh": "该论文提出了一种零样本交互式文本到图像检索（Zero-Shot Interactive Text-to-Image Retrieval, I-TIR）框架，名为Diffusion Augmented Retrieval (DAR)，旨在克服现有依赖微调Multimodal Large Language Models (MLLMs)的方法带来的计算开销大和适应性差的问题。DAR通过结合Large Language Model (LLM)引导的查询精炼和Diffusion Model (DM)基于的视觉合成，生成上下文丰富的中间表示，从而更准确地捕捉用户意图并实现文本与图像的精确对齐。实验在四个基准上显示，DAR无需任务特定训练即可匹配最先进模型，并在多轮对话场景下超越微调基线7.61% 的Hits@10（top-10准确率），证明其在复杂、动态交互中的鲁棒性。该框架为高效、可扩展的跨模态检索系统开辟了新路径。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.15379v1",
      "published_date": "2025-01-26 03:29:18 UTC",
      "updated_date": "2025-01-26 03:29:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:35:24.296031"
    },
    {
      "arxiv_id": "2501.15378v1",
      "title": "How to Mitigate Information Loss in Knowledge Graphs for GraphRAG: Leveraging Triple Context Restoration and Query-Driven Feedback",
      "title_zh": "翻译失败",
      "authors": [
        "Manzong Huang",
        "Chenyang Bu",
        "Yi He",
        "Xindong Wu"
      ],
      "abstract": "Knowledge Graph (KG)-augmented Large Language Models (LLMs) have recently\npropelled significant advances in complex reasoning tasks, thanks to their\nbroad domain knowledge and contextual awareness. Unfortunately, current methods\noften assume KGs to be complete, which is impractical given the inherent\nlimitations of KG construction and the potential loss of contextual cues when\nconverting unstructured text into entity-relation triples. In response, this\npaper proposes the Triple Context Restoration and Query-driven Feedback\n(TCR-QF) framework, which reconstructs the textual context underlying each\ntriple to mitigate information loss, while dynamically refining the KG\nstructure by iteratively incorporating query-relevant missing knowledge.\nExperiments on five benchmark question-answering datasets substantiate the\neffectiveness of TCR-QF in KG and LLM integration, where itachieves a 29.1%\nimprovement in Exact Match and a 15.5% improvement in F1 over its\nstate-of-the-art GraphRAG competitors.",
      "tldr_zh": "这篇论文针对 Knowledge Graphs (KG) 在 GraphRAG 中的信息丢失问题，提出了 Triple Context Restoration and Query-driven Feedback (TCR-QF) 框架，以重建三元组的底层文本上下文并通过查询驱动反馈动态完善 KG 结构。TCR-QF 通过缓解 KG 构建中的上下文缺失和不完整性，提升了 KG-augmented Large Language Models (LLMs) 在复杂推理任务中的性能。实验结果显示，在五个基准问答数据集上，该框架比最先进的 GraphRAG 竞争者提高了 29.1% 的 Exact Match 和 15.5% 的 F1 分数。",
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.15378v1",
      "published_date": "2025-01-26 03:27:11 UTC",
      "updated_date": "2025-01-26 03:27:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:35:36.195214"
    },
    {
      "arxiv_id": "2501.15374v1",
      "title": "Evaluating the Effectiveness of XAI Techniques for Encoder-Based Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Melkamu Abay Mersha",
        "Mesay Gemeda Yigezu",
        "Jugal Kalita"
      ],
      "abstract": "The black-box nature of large language models (LLMs) necessitates the\ndevelopment of eXplainable AI (XAI) techniques for transparency and\ntrustworthiness. However, evaluating these techniques remains a challenge. This\nstudy presents a general evaluation framework using four key metrics:\nHuman-reasoning Agreement (HA), Robustness, Consistency, and Contrastivity. We\nassess the effectiveness of six explainability techniques from five different\nXAI categories model simplification (LIME), perturbation-based methods (SHAP),\ngradient-based approaches (InputXGradient, Grad-CAM), Layer-wise Relevance\nPropagation (LRP), and attention mechanisms-based explainability methods\n(Attention Mechanism Visualization, AMV) across five encoder-based language\nmodels: TinyBERT, BERTbase, BERTlarge, XLM-R large, and DeBERTa-xlarge, using\nthe IMDB Movie Reviews and Tweet Sentiment Extraction (TSE) datasets. Our\nfindings show that the model simplification-based XAI method (LIME)\nconsistently outperforms across multiple metrics and models, significantly\nexcelling in HA with a score of 0.9685 on DeBERTa-xlarge, robustness, and\nconsistency as the complexity of large language models increases. AMV\ndemonstrates the best Robustness, with scores as low as 0.0020. It also excels\nin Consistency, achieving near-perfect scores of 0.9999 across all models.\nRegarding Contrastivity, LRP performs the best, particularly on more complex\nmodels, with scores up to 0.9371.",
      "tldr_zh": "该研究评估了 XAI（eXplainable AI）技术在编码器-based 语言模型上的有效性，提出一个通用评估框架，使用四个关键指标：Human-reasoning Agreement (HA)、Robustness、Consistency 和 Contrastivity。研究比较了六种 XAI 方法，包括模型简化 (LIME)、基于扰动的 (SHAP)、基于梯度的 (InputXGradient 和 Grad-CAM)、Layer-wise Relevance Propagation (LRP) 以及基于注意力机制的 (Attention Mechanism Visualization, AMV)，并在 TinyBERT、BERTbase、BERTlarge、XLM-R large 和 DeBERTa-xlarge 等模型上进行测试，使用 IMDB Movie Reviews 和 Tweet Sentiment Extraction (TSE) 数据集。结果显示，LIME 在多个指标上表现最佳，尤其在 HA 上达到 0.9685（在 DeBERTa-xlarge 上），并在鲁棒性和一致性方面随模型复杂度增加而突出；AMV 在 Robustness 和 Consistency 上最优，分别获得最低 0.0020 和近乎完美的 0.9999 分数；LRP 在 Contrastivity 上表现最佳，最高达 0.9371。总的来说，此框架为提升大型语言模型的透明度和可信度提供了重要见解。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.15374v1",
      "published_date": "2025-01-26 03:08:34 UTC",
      "updated_date": "2025-01-26 03:08:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:36:41.233678"
    },
    {
      "arxiv_id": "2501.15373v1",
      "title": "Learning-Enhanced Safeguard Control for High-Relative-Degree Systems: Robust Optimization under Disturbances and Faults",
      "title_zh": "翻译失败",
      "authors": [
        "Xinyang Wang",
        "Hongwei Zhang",
        "Shimin Wang",
        "Wei Xiao",
        "Martin Guay"
      ],
      "abstract": "Merely pursuing performance may adversely affect the safety, while a\nconservative policy for safe exploration will degrade the performance. How to\nbalance the safety and performance in learning-based control problems is an\ninteresting yet challenging issue. This paper aims to enhance system\nperformance with safety guarantee in solving the reinforcement learning\n(RL)-based optimal control problems of nonlinear systems subject to\nhigh-relative-degree state constraints and unknown time-varying\ndisturbance/actuator faults. First, to combine control barrier functions (CBFs)\nwith RL, a new type of CBFs, termed high-order reciprocal control barrier\nfunction (HO-RCBF) is proposed to deal with high-relative-degree constraints\nduring the learning process. Then, the concept of gradient similarity is\nproposed to quantify the relationship between the gradient of safety and the\ngradient of performance. Finally, gradient manipulation and adaptive mechanisms\nare introduced in the safe RL framework to enhance the performance with a\nsafety guarantee. Two simulation examples illustrate that the proposed safe RL\nframework can address high-relative-degree constraint, enhance safety\nrobustness and improve system performance.",
      "tldr_zh": "本论文探讨了在学习-based 控制中平衡安全性和性能，针对非线性系统的高相对度状态约束和未知时间变扰动/执行器故障，提出了一种强化学习 (RL)-based 优化框架。论文引入高阶互惠控制屏障函数 (HO-RCBF) 与 RL 结合，用于处理高相对度约束，并提出梯度相似性概念来量化安全与性能的梯度关系。随后，通过梯度操作和自适应机制增强安全 RL 框架，确保性能提升的同时维持安全保障。模拟实验证明，该框架能有效应对高相对度约束、增强安全鲁棒性和改善系统性能。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "math.OC",
        "nlin.AO"
      ],
      "primary_category": "eess.SY",
      "comment": "16 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.15373v1",
      "published_date": "2025-01-26 03:03:02 UTC",
      "updated_date": "2025-01-26 03:03:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:35:59.834257"
    },
    {
      "arxiv_id": "2501.15370v1",
      "title": "Scaling Large Vision-Language Models for Enhanced Multimodal Comprehension In Biomedical Image Analysis",
      "title_zh": "扩展大型视觉-语言模型以增强生物医学图像分析中的多模态理解",
      "authors": [
        "Robinson Umeike",
        "Neil Getty",
        "Fangfang Xia",
        "Rick Stevens"
      ],
      "abstract": "Large language models (LLMs) have demonstrated immense capabilities in\nunderstanding textual data and are increasingly being adopted to help\nresearchers accelerate scientific discovery through knowledge extraction\n(information retrieval), knowledge distillation (summarizing key findings and\nmethodologies into concise forms), and knowledge synthesis (aggregating\ninformation from multiple scientific sources to address complex queries,\ngenerate hypothesis and formulate experimental plans). However, scientific data\noften exists in both visual and textual modalities. Vision language models\n(VLMs) address this by incorporating a pretrained vision backbone for\nprocessing images and a cross-modal projector that adapts image tokens into the\nLLM dimensional space, thereby providing richer multimodal comprehension.\nNevertheless, off-the-shelf VLMs show limited capabilities in handling\ndomain-specific data and are prone to hallucinations. We developed intelligent\nassistants finetuned from LLaVA models to enhance multimodal understanding in\nlow-dose radiation therapy (LDRT)-a benign approach used in the treatment of\ncancer-related illnesses. Using multilingual data from 42,673 articles, we\ndevise complex reasoning and detailed description tasks for visual question\nanswering (VQA) benchmarks. Our assistants, trained on 50,882 image-text pairs,\ndemonstrate superior performance over base models as evaluated using\nLLM-as-a-judge approach, particularly in reducing hallucination and improving\ndomain-specific comprehension.",
      "tldr_zh": "该研究扩展了大型视觉语言模型(VLMs)，通过从 LLaVA 模型微调智能助手，以提升生物医学图像分析中的多模态理解，特别是针对低剂量辐射治疗(LDRT)。他们利用 42,673 篇文章的多语言数据创建了复杂的视觉问答(VQA)基准，并基于 50,882 个图像-文本对进行训练。结果显示，这些助手在减少幻觉和改善领域特定理解方面显著优于基础模型，经 LLM-as-a-judge 方法评估证实，为加速科学发现提供了更可靠的多模态工具。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "I.2.7; I.4.9"
      ],
      "primary_category": "cs.CV",
      "comment": "4 Pages, 4 Figures, 1 Table",
      "pdf_url": "http://arxiv.org/pdf/2501.15370v1",
      "published_date": "2025-01-26 02:48:01 UTC",
      "updated_date": "2025-01-26 02:48:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:36:13.106954"
    },
    {
      "arxiv_id": "2501.15369v2",
      "title": "iFormer: Integrating ConvNet and Transformer for Mobile Application",
      "title_zh": "iFormer：整合 ConvNet 和 Transformer 用于移动应用",
      "authors": [
        "Chuanyang Zheng"
      ],
      "abstract": "We present a new family of mobile hybrid vision networks, called iFormer,\nwith a focus on optimizing latency and accuracy on mobile applications. iFormer\neffectively integrates the fast local representation capacity of convolution\nwith the efficient global modeling ability of self-attention. The local\ninteractions are derived from transforming a standard convolutional network,\n\\textit{i.e.}, ConvNeXt, to design a more lightweight mobile network. Our newly\nintroduced mobile modulation attention removes memory-intensive operations in\nMHA and employs an efficient modulation mechanism to boost dynamic global\nrepresentational capacity. We conduct comprehensive experiments demonstrating\nthat iFormer outperforms existing lightweight networks across various tasks.\nNotably, iFormer achieves an impressive Top-1 accuracy of 80.4\\% on ImageNet-1k\nwith a latency of only 1.10 ms on an iPhone 13, surpassing the recently\nproposed MobileNetV4 under similar latency constraints. Additionally, our\nmethod shows significant improvements in downstream tasks, including COCO\nobject detection, instance segmentation, and ADE20k semantic segmentation,\nwhile still maintaining low latency on mobile devices for high-resolution\ninputs in these scenarios.",
      "tldr_zh": "本文提出iFormer，一种整合ConvNet和Transformer的移动混合视觉网络，旨在优化移动应用的延迟和准确性。iFormer通过改造标准卷积网络如ConvNeXt来实现高效的局部交互，并引入mobile modulation attention机制，移除MHA中的内存密集型操作以提升动态全局表示能力。实验显示，iFormer在ImageNet-1k上达到80.4%的Top-1准确率，同时在iPhone 13上的延迟仅为1.10 ms，优于MobileNetV4，并在COCO物体检测、实例分割和ADE20k语义分割等下游任务中表现出显著改进，同时保持低延迟。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ICLR 2025. Code:\n  https://github.com/ChuanyangZheng/iFormer",
      "pdf_url": "http://arxiv.org/pdf/2501.15369v2",
      "published_date": "2025-01-26 02:34:58 UTC",
      "updated_date": "2025-02-17 15:09:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:36:25.881862"
    },
    {
      "arxiv_id": "2504.13183v1",
      "title": "Factors That Influence the Adoption of AI-enabled Conversational Agents (AICAs) as an Augmenting Therapeutic Tool by Frontline Healthcare Workers: From Technology Acceptance Model 3 (TAM3) Lens -- A Systematic Mapping Review",
      "title_zh": "翻译失败",
      "authors": [
        "Rawan AlMakinah"
      ],
      "abstract": "Artificial intelligent (AI) conversational agents hold a promising future in\nthe field of mental health, especially in helping marginalized communities that\nlack access to mental health support services. It is tempting to have a 24/7\nmental health companion that can be accessed anywhere using mobile phones to\nprovide therapist-like advice. Yet, caution should be taken, and studies around\ntheir feasibility need to be surveyed. Before adopting such a rapidly changing\ntechnology, studies on its feasibility should be explored, summarized, and\nsynthesized to gain a solid understanding of the status quo and to enable us to\nbuild a framework that can guide us throughout the development and deployment\nprocesses. Different perspectives must be considered when investigating the\nfeasibility of AI conversational agents, including the mental healthcare\nprofessional perspective. The literature can provide insights into their\nperspectives in terms of opportunities, concerns, and implications. Mental\nhealth professionals, the subject-matter experts in this field, have their\npoints of view that should be understood and considered. This systematic\nliterature review will explore mental health practitioners' attitudes toward AI\nconversational agents and the factors that affect their adoption and\nrecommendation of the technology to augment their services and treatments. The\nTAM3 Framework will be the lens through which this systematic literature review\nwill be conducted.",
      "tldr_zh": "这篇论文通过 Technology Acceptance Model 3 (TAM3) 框架进行系统文献综述，探讨了影响一线医疗工作者采用 AI-enabled Conversational Agents (AICAs) 作为辅助治疗工具的因素，特别是针对心理健康领域的应用。研究聚焦于心理健康从业者的态度、机会、担忧和影响，包括他们对 AICAs 的推荐意愿，以评估其在边缘化社区中的可行性。最终，该综述总结了现有文献，提供了一个指导 AICAs 开发和部署的框架，帮助构建更可靠的 24/7 心理健康支持系统。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13183v1",
      "published_date": "2025-01-26 02:31:27 UTC",
      "updated_date": "2025-01-26 02:31:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:36:36.541640"
    },
    {
      "arxiv_id": "2502.15715v1",
      "title": "Regulating Multifunctionality",
      "title_zh": "翻译失败",
      "authors": [
        "Cary Coglianese",
        "Colton R. Crum"
      ],
      "abstract": "Foundation models and generative artificial intelligence (AI) exacerbate a\ncore regulatory challenge associated with AI: its heterogeneity. By their very\nnature, foundation models and generative AI can perform multiple functions for\ntheir users, thus presenting a vast array of different risks. This\nmultifunctionality means that prescriptive, one-size-fits-all regulation will\nnot be a viable option. Even performance standards and ex post liability -\nregulatory approaches that usually afford flexibility - are unlikely to be\nstrong candidates for responding to multifunctional AI's risks, given\nchallenges in monitoring and enforcement. Regulators will do well instead to\npromote proactive risk management on the part of developers and users by using\nmanagement-based regulation, an approach that has proven effective in other\ncontexts of heterogeneity. Regulators will also need to maintain ongoing\nvigilance and agility. More than in other contexts, regulators of\nmultifunctional AI will need sufficient resources, top human talent and\nleadership, and organizational cultures committed to regulatory excellence.",
      "tldr_zh": "这篇论文探讨了基础模型和生成式 AI 的多功能性如何加剧了 AI 监管的异质性挑战，导致各种风险难以统一管理。作者认为，一刀切的规定性监管、性能标准和事后责任等传统方法难以有效应对这些风险，因为监控和执行存在困难。相反，论文建议采用 management-based regulation 策略，鼓励开发者和用户主动进行风险管理，同时强调监管者需保持持续警惕、灵活性，并配备足够的资源、人才和领导力，以实现监管卓越。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "Forthcoming in Philipp Hacker, Andreas Engel, Sarah Hammer and Brent\n  Mittelstadt (eds), The Oxford Handbook on the Foundations and Regulation of\n  Generative AI (Oxford University Press)",
      "pdf_url": "http://arxiv.org/pdf/2502.15715v1",
      "published_date": "2025-01-26 00:50:27 UTC",
      "updated_date": "2025-01-26 00:50:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:36:47.640883"
    },
    {
      "arxiv_id": "2501.15355v1",
      "title": "Large Language Models as Theory of Mind Aware Generative Agents with Counterfactual Reflection",
      "title_zh": "大语言模型作为心智理论感知生成代理，结合反事实反思",
      "authors": [
        "Bo Yang",
        "Jiaxian Guo",
        "Yusuke Iwasawa",
        "Yutaka Matsuo"
      ],
      "abstract": "Recent studies have increasingly demonstrated that large language models\n(LLMs) possess significant theory of mind (ToM) capabilities, showing the\npotential for simulating the tracking of mental states in generative agents. In\nthis study, we propose a novel paradigm called ToM-agent, designed to empower\nLLMs-based generative agents to simulate ToM in open-domain conversational\ninteractions. ToM-agent disentangles the confidence from mental states,\nfacilitating the emulation of an agent's perception of its counterpart's mental\nstates, such as beliefs, desires, and intentions (BDIs). Using past\nconversation history and verbal reflections, ToM-Agent can dynamically adjust\ncounterparts' inferred BDIs, along with related confidence levels. We further\nput forth a counterfactual intervention method that reflects on the gap between\nthe predicted responses of counterparts and their real utterances, thereby\nenhancing the efficiency of reflection. Leveraging empathetic and persuasion\ndialogue datasets, we assess the advantages of implementing the ToM-agent with\ndownstream tasks, as well as its performance in both the first-order and the\n\\textit{second-order} ToM. Our findings indicate that the ToM-agent can grasp\nthe underlying reasons for their counterpart's behaviors beyond mere\nsemantic-emotional supporting or decision-making based on common sense,\nproviding new insights for studying large-scale LLMs-based simulation of human\nsocial behaviors.",
      "tldr_zh": "该研究提出ToM-agent范式，利用大型语言模型(LLMs)作为理论心智(ToM)感知的生成代理，通过反事实反射方法模拟对手的信念、欲望和意图(BDIs)。该框架通过分析过去对话历史和口头反思，动态调整BDIs及其信心水平，并采用反事实干预来优化预测与实际话语的差距，提升代理的反思效率。在移情和说服对话数据集上的实验表明，ToM-agent在第一阶和第二阶ToM任务中表现出色，能够超越常识决策，深入理解对手行为背后的原因，为LLMs模拟人类社会行为提供了新见解。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.15355v1",
      "published_date": "2025-01-26 00:32:38 UTC",
      "updated_date": "2025-01-26 00:32:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T03:37:00.820989"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 61,
  "processed_papers_count": 61,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-22T03:37:18.083660"
}
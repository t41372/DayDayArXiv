[
  {
    "arxiv_id": "2501.15695v1",
    "title": "Contextual Knowledge Sharing in Multi-Agent Reinforcement Learning with Decentralized Communication and Coordination",
    "authors": [
      "Hung Du",
      "Srikanth Thudumu",
      "Hy Nguyen",
      "Rajesh Vasa",
      "Kon Mouzakis"
    ],
    "abstract": "Decentralized Multi-Agent Reinforcement Learning (Dec-MARL) has emerged as a\npivotal approach for addressing complex tasks in dynamic environments. Existing\nMulti-Agent Reinforcement Learning (MARL) methodologies typically assume a\nshared objective among agents and rely on centralized control. However, many\nreal-world scenarios feature agents with individual goals and limited\nobservability of other agents, complicating coordination and hindering\nadaptability. Existing Dec-MARL strategies prioritize either communication or\ncoordination, lacking an integrated approach that leverages both. This paper\npresents a novel Dec-MARL framework that integrates peer-to-peer communication\nand coordination, incorporating goal-awareness and time-awareness into the\nagents' knowledge-sharing processes. Our framework equips agents with the\nability to (i) share contextually relevant knowledge to assist other agents,\nand (ii) reason based on information acquired from multiple agents, while\nconsidering their own goals and the temporal context of prior knowledge. We\nevaluate our approach through several complex multi-agent tasks in environments\nwith dynamically appearing obstacles. Our work demonstrates that incorporating\ngoal-aware and time-aware knowledge sharing significantly enhances overall\nperformance.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.15695v1",
    "published_date": "2025-01-26 22:49:50 UTC",
    "updated_date": "2025-01-26 22:49:50 UTC"
  },
  {
    "arxiv_id": "2501.15693v1",
    "title": "Beyond Benchmarks: On The False Promise of AI Regulation",
    "authors": [
      "Gabriel Stanovsky",
      "Renana Keydar",
      "Gadi Perl",
      "Eliya Habba"
    ],
    "abstract": "The rapid advancement of artificial intelligence (AI) systems in critical\ndomains like healthcare, justice, and social services has sparked numerous\nregulatory initiatives aimed at ensuring their safe deployment. Current\nregulatory frameworks, exemplified by recent US and EU efforts, primarily focus\non procedural guidelines while presuming that scientific benchmarking can\neffectively validate AI safety, similar to how crash tests verify vehicle\nsafety or clinical trials validate drug efficacy. However, this approach\nfundamentally misunderstands the unique technical challenges posed by modern AI\nsystems. Through systematic analysis of successful technology regulation case\nstudies, we demonstrate that effective scientific regulation requires a causal\ntheory linking observable test outcomes to future performance - for instance,\nhow a vehicle's crash resistance at one speed predicts its safety at lower\nspeeds. We show that deep learning models, which learn complex statistical\npatterns from training data without explicit causal mechanisms, preclude such\nguarantees. This limitation renders traditional regulatory approaches\ninadequate for ensuring AI safety. Moving forward, we call for regulators to\nreckon with this limitation, and propose a preliminary two-tiered regulatory\nframework that acknowledges these constraints: mandating human oversight for\nhigh-risk applications while developing appropriate risk communication\nstrategies for lower-risk uses. Our findings highlight the urgent need to\nreconsider fundamental assumptions in AI regulation and suggest a concrete path\nforward for policymakers and researchers.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.15693v1",
    "published_date": "2025-01-26 22:43:07 UTC",
    "updated_date": "2025-01-26 22:43:07 UTC"
  },
  {
    "arxiv_id": "2501.15688v1",
    "title": "Transformer-Based Multimodal Knowledge Graph Completion with Link-Aware Contexts",
    "authors": [
      "Haodi Ma",
      "Dzmitry Kasinets",
      "Daisy Zhe Wang"
    ],
    "abstract": "Multimodal knowledge graph completion (MMKGC) aims to predict missing links\nin multimodal knowledge graphs (MMKGs) by leveraging information from various\nmodalities alongside structural data. Existing MMKGC approaches primarily\nextend traditional knowledge graph embedding (KGE) models, which often require\ncreating an embedding for every entity. This results in large model sizes and\ninefficiencies in integrating multimodal information, particularly for\nreal-world graphs. Meanwhile, Transformer-based models have demonstrated\ncompetitive performance in knowledge graph completion (KGC). However, their\nfocus on single-modal knowledge limits their capacity to utilize cross-modal\ninformation. Recently, Large vision-language models (VLMs) have shown potential\nin cross-modal tasks but are constrained by the high cost of training. In this\nwork, we propose a novel approach that integrates Transformer-based KGE models\nwith cross-modal context generated by pre-trained VLMs, thereby extending their\napplicability to MMKGC. Specifically, we employ a pre-trained VLM to transform\nrelevant visual information from entities and their neighbors into textual\nsequences. We then frame KGC as a sequence-to-sequence task, fine-tuning the\nmodel with the generated cross-modal context. This simple yet effective method\nsignificantly reduces model size compared to traditional KGE approaches while\nachieving competitive performance across multiple large-scale datasets with\nminimal hyperparameter tuning.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.15688v1",
    "published_date": "2025-01-26 22:23:14 UTC",
    "updated_date": "2025-01-26 22:23:14 UTC"
  },
  {
    "arxiv_id": "2501.15678v1",
    "title": "Blissful (A)Ignorance: People form overly positive impressions of others based on their written messages, despite wide-scale adoption of Generative AI",
    "authors": [
      "Jiaqi Zhu",
      "Andras Molnar"
    ],
    "abstract": "As the use of Generative AI (GenAI) tools becomes more prevalent in\ninterpersonal communication, understanding their impact on social perceptions\nis crucial. According to signaling theory, GenAI may undermine the credibility\nof social signals conveyed in writing, since it reduces the cost of writing and\nmakes it hard to verify the authenticity of messages. Using a pre-registered\nlarge-scale online experiment (N = 647; Prolific), featuring scenarios in a\nrange of communication contexts (personal vs. professional; close others vs.\nstrangers), we explored how senders' use of GenAI influenced recipients'\nimpressions of senders, both when GenAI use was known or uncertain. Consistent\nwith past work, we found strong negative effects on social impressions when\ndisclosing that a message was AI-generated, compared to when the same message\nwas human-written. However, under the more realistic condition when potential\nGenAI use was not explicitly highlighted, recipients did not exhibit any\nskepticism towards senders, and these \"uninformed\" impressions were virtually\nindistinguishable from those of fully human-written messages. Even when we\nhighlighted the potential (but uncertain) use of GenAI, recipients formed\noverly positive impressions. These results are especially striking given that\n46% of our sample admitted having used such tools for writing messages, just\nwithin the past two weeks. Our findings put past work in a new light: While\nsocial judgments can be substantially affected when GenAI use is explicitly\ndisclosed, this information may not be readily available in more realistic\ncommunication settings, making recipients blissfully ignorant about others'\npotential use of GenAI.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.15678v1",
    "published_date": "2025-01-26 21:38:12 UTC",
    "updated_date": "2025-01-26 21:38:12 UTC"
  },
  {
    "arxiv_id": "2501.15665v1",
    "title": "StagFormer: Time Staggering Transformer Decoding for RunningLayers In Parallel",
    "authors": [
      "Dylan Cutler",
      "Arun Kandoor",
      "Nishanth Dikkala",
      "Nikunj Saunshi",
      "Xin Wang",
      "Rina Panigrahy"
    ],
    "abstract": "Standard decoding in a Transformer based language model is inherently\nsequential as we wait for a token's embedding to pass through all the layers in\nthe network before starting the generation of the next token. In this work, we\npropose a new architecture StagFormer (Staggered Transformer), which staggered\nexecution along the time axis and thereby enables parallelizing the decoding\nprocess along the depth of the model. We achieve this by breaking the\ndependency of the token representation at time step $i$ in layer $l$ upon the\nrepresentations of tokens until time step $i$ from layer $l-1$. Instead, we\nstagger the execution and only allow a dependency on token representations\nuntil time step $i-1$. The later sections of the Transformer still get access\nto the ``rich\" representations from the prior section but only from those token\npositions which are one time step behind. StagFormer allows for different\nsections of the model to be executed in parallel yielding at potential 33\\%\nspeedup in decoding while being quality neutral in our simulations. We also\nexplore many natural variants of this idea. We present how weight-sharing\nacross the different sections being staggered can be more practical in settings\nwith limited memory. We show how one can approximate a recurrent model during\ninference using such weight-sharing. We explore the efficacy of using a bounded\nwindow attention to pass information from one section to another which helps\ndrive further latency gains for some applications. We also explore demonstrate\nthe scalability of the staggering idea over more than 2 sections of the\nTransformer.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.15665v1",
    "published_date": "2025-01-26 20:09:11 UTC",
    "updated_date": "2025-01-26 20:09:11 UTC"
  },
  {
    "arxiv_id": "2501.15661v1",
    "title": "Constrained Hybrid Metaheuristic Algorithm for Probabilistic Neural Networks Learning",
    "authors": [
      "Piotr A. Kowalski",
      "Szymon Kucharczyk",
      "Jacek Mańdziuk"
    ],
    "abstract": "This study investigates the potential of hybrid metaheuristic algorithms to\nenhance the training of Probabilistic Neural Networks (PNNs) by leveraging the\ncomplementary strengths of multiple optimisation strategies. Traditional\nlearning methods, such as gradient-based approaches, often struggle to optimise\nhigh-dimensional and uncertain environments, while single-method metaheuristics\nmay fail to exploit the solution space fully. To address these challenges, we\npropose the constrained Hybrid Metaheuristic (cHM) algorithm, a novel approach\nthat combines multiple population-based optimisation techniques into a unified\nframework. The proposed procedure operates in two phases: an initial probing\nphase evaluates multiple metaheuristics to identify the best-performing one\nbased on the error rate, followed by a fitting phase where the selected\nmetaheuristic refines the PNN to achieve optimal smoothing parameters. This\niterative process ensures efficient exploration and convergence, enhancing the\nnetwork's generalisation and classification accuracy. cHM integrates several\npopular metaheuristics, such as BAT, Simulated Annealing, Flower Pollination\nAlgorithm, Bacterial Foraging Optimization, and Particle Swarm Optimisation as\ninternal optimisers. To evaluate cHM performance, experiments were conducted on\n16 datasets with varying characteristics, including binary and multiclass\nclassification tasks, balanced and imbalanced class distributions, and diverse\nfeature dimensions. The results demonstrate that cHM effectively combines the\nstrengths of individual metaheuristics, leading to faster convergence and more\nrobust learning. By optimising the smoothing parameters of PNNs, the proposed\nmethod enhances classification performance across diverse datasets, proving its\napplication flexibility and efficiency.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "I.2.0"
    ],
    "primary_category": "cs.NE",
    "comment": "35 pages, 1 Algorithm flow, 11 tables, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.15661v1",
    "published_date": "2025-01-26 19:49:16 UTC",
    "updated_date": "2025-01-26 19:49:16 UTC"
  },
  {
    "arxiv_id": "2501.15660v1",
    "title": "Marker Track: Accurate Fiducial Marker Tracking for Evaluation of Residual Motions During Breath-Hold Radiotherapy",
    "authors": [
      "Aimee Guo",
      "Weihua Mao"
    ],
    "abstract": "Fiducial marker positions in projection image of cone-beam computed\ntomography (CBCT) scans have been studied to evaluate daily residual motion\nduring breath-hold radiation therapy. Fiducial marker migration posed\nchallenges in accurately locating markers, prompting the development of a novel\nalgorithm that reconstructs volumetric probability maps of marker locations\nfrom filtered gradient maps of projections. This guides the development of a\nPython-based algorithm to detect fiducial markers in projection images using\nMeta AI's Segment Anything Model 2 (SAM 2). Retrospective data from a\npancreatic cancer patient with two fiducial markers were analyzed. The\nthree-dimensional (3D) marker positions from simulation computed tomography\n(CT) were compared to those reconstructed from CBCT images, revealing a\ndecrease in relative distances between markers over time. Fiducial markers were\nsuccessfully detected in 2777 out of 2786 projection frames. The average\nstandard deviation of superior-inferior (SI) marker positions was 0.56 mm per\nbreath-hold, with differences in average SI positions between two breath-holds\nin the same scan reaching up to 5.2 mm, and a gap of up to 7.3 mm between the\nend of the first and beginning of the second breath-hold. 3D marker positions\nwere calculated using projection positions and confirmed marker migration. This\nmethod effectively calculates marker probability volume and enables accurate\nfiducial marker tracking during treatment without requiring any specialized\nequipment, additional radiation doses, or manual initialization and labeling.\nIt has significant potential for automatically assessing daily residual motion\nto adjust planning margins, functioning as an adaptive radiation therapy tool.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV",
      "physics.med-ph"
    ],
    "primary_category": "cs.CV",
    "comment": "14 pages, 9 figures, Regeneron STS 2025 project. Project page:\n  https://sites.google.com/view/markertrack?usp=sharing",
    "pdf_url": "http://arxiv.org/pdf/2501.15660v1",
    "published_date": "2025-01-26 19:46:49 UTC",
    "updated_date": "2025-01-26 19:46:49 UTC"
  },
  {
    "arxiv_id": "2501.15654v2",
    "title": "People who frequently use ChatGPT for writing tasks are accurate and robust detectors of AI-generated text",
    "authors": [
      "Jenna Russell",
      "Marzena Karpinska",
      "Mohit Iyyer"
    ],
    "abstract": "In this paper, we study how well humans can detect text generated by\ncommercial LLMs (GPT-4o, Claude, o1). We hire annotators to read 300\nnon-fiction English articles, label them as either human-written or\nAI-generated, and provide paragraph-length explanations for their decisions.\nOur experiments show that annotators who frequently use LLMs for writing tasks\nexcel at detecting AI-generated text, even without any specialized training or\nfeedback. In fact, the majority vote among five such \"expert\" annotators\nmisclassifies only 1 of 300 articles, significantly outperforming most\ncommercial and open-source detectors we evaluated even in the presence of\nevasion tactics like paraphrasing and humanization. Qualitative analysis of the\nexperts' free-form explanations shows that while they rely heavily on specific\nlexical clues ('AI vocabulary'), they also pick up on more complex phenomena\nwithin the text (e.g., formality, originality, clarity) that are challenging to\nassess for automatic detectors. We release our annotated dataset and code to\nspur future research into both human and automated detection of AI-generated\ntext.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL 2025 33 pages",
    "pdf_url": "http://arxiv.org/pdf/2501.15654v2",
    "published_date": "2025-01-26 19:31:34 UTC",
    "updated_date": "2025-05-19 18:22:33 UTC"
  },
  {
    "arxiv_id": "2501.15648v1",
    "title": "Can Pose Transfer Models Generate Realistic Human Motion?",
    "authors": [
      "Vaclav Knapp",
      "Matyas Bohacek"
    ],
    "abstract": "Recent pose-transfer methods aim to generate temporally consistent and fully\ncontrollable videos of human action where the motion from a reference video is\nreenacted by a new identity. We evaluate three state-of-the-art pose-transfer\nmethods -- AnimateAnyone, MagicAnimate, and ExAvatar -- by generating videos\nwith actions and identities outside the training distribution and conducting a\nparticipant study about the quality of these videos. In a controlled\nenvironment of 20 distinct human actions, we find that participants, presented\nwith the pose-transferred videos, correctly identify the desired action only\n42.92% of the time. Moreover, the participants find the actions in the\ngenerated videos consistent with the reference (source) videos only 36.46% of\nthe time. These results vary by method: participants find the splatting-based\nExAvatar more consistent and photorealistic than the diffusion-based\nAnimateAnyone and MagicAnimate.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Data and code available at\n  https://github.com/matyasbohacek/pose-transfer-human-motion",
    "pdf_url": "http://arxiv.org/pdf/2501.15648v1",
    "published_date": "2025-01-26 19:17:05 UTC",
    "updated_date": "2025-01-26 19:17:05 UTC"
  },
  {
    "arxiv_id": "2501.15638v2",
    "title": "A Comprehensive Survey on Self-Interpretable Neural Networks",
    "authors": [
      "Yang Ji",
      "Ying Sun",
      "Yuting Zhang",
      "Zhigaoyuan Wang",
      "Yuanxin Zhuang",
      "Zheng Gong",
      "Dazhong Shen",
      "Chuan Qin",
      "Hengshu Zhu",
      "Hui Xiong"
    ],
    "abstract": "Neural networks have achieved remarkable success across various fields.\nHowever, the lack of interpretability limits their practical use, particularly\nin critical decision-making scenarios. Post-hoc interpretability, which\nprovides explanations for pre-trained models, is often at risk of robustness\nand fidelity. This has inspired a rising interest in self-interpretable neural\nnetworks, which inherently reveal the prediction rationale through the model\nstructures. Although there exist surveys on post-hoc interpretability, a\ncomprehensive and systematic survey of self-interpretable neural networks is\nstill missing. To address this gap, we first collect and review existing works\non self-interpretable neural networks and provide a structured summary of their\nmethodologies from five key perspectives: attribution-based, function-based,\nconcept-based, prototype-based, and rule-based self-interpretation. We also\npresent concrete, visualized examples of model explanations and discuss their\napplicability across diverse scenarios, including image, text, graph data, and\ndeep reinforcement learning. Additionally, we summarize existing evaluation\nmetrics for self-interpretability and identify open challenges in this field,\noffering insights for future research. To support ongoing developments, we\npresent a publicly accessible resource to track advancements in this domain:\nhttps://github.com/yangji721/Awesome-Self-Interpretable-Neural-Network.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.15638v2",
    "published_date": "2025-01-26 18:50:16 UTC",
    "updated_date": "2025-03-22 03:32:46 UTC"
  },
  {
    "arxiv_id": "2501.15627v2",
    "title": "HardML: A Benchmark For Evaluating Data Science And Machine Learning knowledge and reasoning in AI",
    "authors": [
      "Tidor-Vlad Pricope"
    ],
    "abstract": "We present HardML, a benchmark designed to evaluate the knowledge and\nreasoning abilities in the fields of data science and machine learning. HardML\ncomprises a diverse set of 100 challenging multiple-choice questions,\nhandcrafted over a period of 6 months, covering the most popular and modern\nbranches of data science and machine learning. These questions are challenging\neven for a typical Senior Machine Learning Engineer to answer correctly. To\nminimize the risk of data contamination, HardML uses mostly original content\ndevised by the author. Current state of the art AI models achieve a 30% error\nrate on this benchmark, which is about 3 times larger than the one achieved on\nthe equivalent, well known MMLU ML. While HardML is limited in scope and not\naiming to push the frontier, primarily due to its multiple choice nature, it\nserves as a rigorous and modern testbed to quantify and track the progress of\ntop AI. While plenty benchmarks and experimentation in LLM evaluation exist in\nother STEM fields like mathematics, physics and chemistry, the subfields of\ndata science and machine learning remain fairly underexplored.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.15627v2",
    "published_date": "2025-01-26 18:25:26 UTC",
    "updated_date": "2025-05-06 15:53:34 UTC"
  },
  {
    "arxiv_id": "2501.15619v1",
    "title": "GaussianToken: An Effective Image Tokenizer with 2D Gaussian Splatting",
    "authors": [
      "Jiajun Dong",
      "Chengkun Wang",
      "Wenzhao Zheng",
      "Lei Chen",
      "Jiwen Lu",
      "Yansong Tang"
    ],
    "abstract": "Effective image tokenization is crucial for both multi-modal understanding\nand generation tasks due to the necessity of the alignment with discrete text\ndata. To this end, existing approaches utilize vector quantization (VQ) to\nproject pixels onto a discrete codebook and reconstruct images from the\ndiscrete representation. However, compared with the continuous latent space,\nthe limited discrete codebook space significantly restrict the representational\nability of these image tokenizers. In this paper, we propose GaussianToken: An\nEffective Image Tokenizer with 2D Gaussian Splatting as a solution. We first\nrepresent the encoded samples as multiple flexible featured 2D Gaussians\ncharacterized by positions, rotation angles, scaling factors, and feature\ncoefficients. We adopt the standard quantization for the Gaussian features and\nthen concatenate the quantization results with the other intrinsic Gaussian\nparameters before the corresponding splatting operation and the subsequent\ndecoding module. In general, GaussianToken integrates the local influence of 2D\nGaussian distribution into the discrete space and thus enhances the\nrepresentation capability of the image tokenizer. Competitive reconstruction\nperformances on CIFAR, Mini-ImageNet, and ImageNet-1K demonstrate the\neffectiveness of our framework. Our code is available at:\nhttps://github.com/ChrisDong-THU/GaussianToken.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.15619v1",
    "published_date": "2025-01-26 17:56:11 UTC",
    "updated_date": "2025-01-26 17:56:11 UTC"
  },
  {
    "arxiv_id": "2501.15618v2",
    "title": "Your Learned Constraint is Secretly a Backward Reachable Tube",
    "authors": [
      "Mohamad Qadri",
      "Gokul Swamy",
      "Jonathan Francis",
      "Michael Kaess",
      "Andrea Bajcsy"
    ],
    "abstract": "Inverse Constraint Learning (ICL) is the problem of inferring constraints\nfrom safe (i.e., constraint-satisfying) demonstrations. The hope is that these\ninferred constraints can then be used downstream to search for safe policies\nfor new tasks and, potentially, under different dynamics. Our paper explores\nthe question of what mathematical entity ICL recovers. Somewhat surprisingly,\nwe show that both in theory and in practice, ICL recovers the set of states\nwhere failure is inevitable, rather than the set of states where failure has\nalready happened. In the language of safe control, this means we recover a\nbackwards reachable tube (BRT) rather than a failure set. In contrast to the\nfailure set, the BRT depends on the dynamics of the data collection system. We\ndiscuss the implications of the dynamics-conditionedness of the recovered\nconstraint on both the sample-efficiency of policy search and the\ntransferability of learned constraints.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "14 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.15618v2",
    "published_date": "2025-01-26 17:54:43 UTC",
    "updated_date": "2025-05-17 19:16:54 UTC"
  },
  {
    "arxiv_id": "2501.17188v1",
    "title": "Letters, Colors, and Words: Constructing the Ideal Building Blocks Set",
    "authors": [
      "Ricardo Salazar",
      "Shahrzad Jamshidi"
    ],
    "abstract": "Define a building blocks set to be a collection of n cubes (each with six\nsides) where each side is assigned one letter and one color from a palette of m\ncolors. We propose a novel problem of assigning letters and colors to each face\nso as to maximize the number of words one can spell from a chosen dataset that\nare either mono words, all letters have the same color, or rainbow words, all\nletters have unique colors. We explore this problem considering a chosen set of\nEnglish words, up to six letters long, from a typical vocabulary of a US\nAmerican 14 year old and explore the problem when n=6 and m=6, with the added\nrestriction that each color appears exactly once on the cube. The problem is\nintractable, as the size of the solution space makes a brute force approach\ncomputationally infeasible. Therefore we aim to solve this problem using random\nsearch, simulated annealing, two distinct tree search approaches (greedy and\nbest-first), and a genetic algorithm. To address this, we explore a range of\noptimization techniques: random search, simulated annealing, two distinct tree\nsearch methods (greedy and best-first), and a genetic algorithm. Additionally,\nwe attempted to implement a reinforcement learning approach; however, the model\nfailed to converge to viable solutions within the problem's constraints. Among\nthese methods, the genetic algorithm delivered the best performance, achieving\na total of 2846 mono and rainbow words.",
    "categories": [
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.AI",
    "comment": "29 pages, 8 figures, submitted to SIAM Undergraduate Research Online",
    "pdf_url": "http://arxiv.org/pdf/2501.17188v1",
    "published_date": "2025-01-26 17:54:03 UTC",
    "updated_date": "2025-01-26 17:54:03 UTC"
  },
  {
    "arxiv_id": "2501.17187v2",
    "title": "Visualizing Uncertainty in Translation Tasks: An Evaluation of LLM Performance and Confidence Metrics",
    "authors": [
      "Jin Hyun Park",
      "Utsawb Laminchhane",
      "Umer Farooq",
      "Uma Sivakumar",
      "Arpan Kumar"
    ],
    "abstract": "Large language models (LLMs) are increasingly utilized for machine\ntranslation, yet their predictions often exhibit uncertainties that hinder\ninterpretability and user trust. Effectively visualizing these uncertainties\ncan enhance the usability of LLM outputs, particularly in contexts where\ntranslation accuracy is critical. This paper addresses two primary objectives:\n(1) providing users with token-level insights into model confidence and (2)\ndeveloping a web-based visualization tool to quantify and represent translation\nuncertainties. To achieve these goals, we utilized the T5 model with the WMT19\ndataset for translation tasks and evaluated translation quality using\nestablished metrics such as BLEU, METEOR, and ROUGE. We introduced three novel\nuncertainty quantification (UQ) metrics: (1) the geometric mean of token\nprobabilities, (2) the arithmetic mean of token probabilities, and (3) the\narithmetic mean of the kurtosis of token distributions. These metrics provide a\nsimple yet effective framework for evaluating translation performance. Our\nanalysis revealed a linear relationship between the traditional evaluation\nmetrics and our UQ metrics, demonstrating the validity of our approach.\nAdditionally, we developed an interactive web-based visualization that uses a\ncolor gradient to represent token confidence. This tool offers users a clear\nand intuitive understanding of translation quality while providing valuable\ninsights into model performance. Overall, we show that our UQ metrics and\nvisualization are both robust and interpretable, offering practical tools for\nevaluating and accessing machine translation systems.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "We would like to withdraw our paper due to an error in the\n  experimental methodology, which impacts the validity of our results. The\n  error specifically affects the analysis presented in the Discussion, where an\n  incorrect experimental modeling step led to misleading interpretations",
    "pdf_url": "http://arxiv.org/pdf/2501.17187v2",
    "published_date": "2025-01-26 17:14:51 UTC",
    "updated_date": "2025-02-24 21:20:34 UTC"
  },
  {
    "arxiv_id": "2501.15602v2",
    "title": "Rethinking External Slow-Thinking: From Snowball Errors to Probability of Correct Reasoning",
    "authors": [
      "Zeyu Gan",
      "Yun Liao",
      "Yong Liu"
    ],
    "abstract": "Test-time scaling, which is also often referred to as slow-thinking, has been\ndemonstrated to enhance multi-step reasoning in large language models (LLMs).\nHowever, despite its widespread utilization, the mechanisms underlying\nslow-thinking methods remain poorly understood. This paper explores the\nmechanisms of external slow-thinking from a theoretical standpoint. We begin by\nexamining the snowball error effect within the LLM reasoning process and\nconnect it to the likelihood of correct reasoning using information theory.\nBuilding on this, we show that external slow-thinking methods can be\ninterpreted as strategies to mitigate the error probability. We further provide\na comparative analysis of popular external slow-thinking approaches, ranging\nfrom simple to complex, highlighting their differences and interrelationships.\nOur findings suggest that the efficacy of these methods is not primarily\ndetermined by the specific framework employed, and that expanding the search\nscope or the model's internal reasoning capacity may yield more sustained\nimprovements in the long term. We open-source our code at\nhttps://github.com/ZyGan1999/Snowball-Errors-and-Probability.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.15602v2",
    "published_date": "2025-01-26 17:05:16 UTC",
    "updated_date": "2025-01-28 14:14:03 UTC"
  },
  {
    "arxiv_id": "2501.15598v1",
    "title": "Diffusion Generative Modeling for Spatially Resolved Gene Expression Inference from Histology Images",
    "authors": [
      "Sichen Zhu",
      "Yuchen Zhu",
      "Molei Tao",
      "Peng Qiu"
    ],
    "abstract": "Spatial Transcriptomics (ST) allows a high-resolution measurement of RNA\nsequence abundance by systematically connecting cell morphology depicted in\nHematoxylin and Eosin (H&E) stained histology images to spatially resolved gene\nexpressions. ST is a time-consuming, expensive yet powerful experimental\ntechnique that provides new opportunities to understand cancer mechanisms at a\nfine-grained molecular level, which is critical for uncovering new approaches\nfor disease diagnosis and treatments. Here, we present $\\textbf{Stem}$\n($\\textbf{S}$pa$\\textbf{T}$ially resolved gene $\\textbf{E}$xpression inference\nwith diffusion $\\textbf{M}$odel), a novel computational tool that leverages a\nconditional diffusion generative model to enable in silico gene expression\ninference from H&E stained images. Through better capturing the inherent\nstochasticity and heterogeneity in ST data, $\\textbf{Stem}$ achieves\nstate-of-the-art performance on spatial gene expression prediction and\ngenerates biologically meaningful gene profiles for new H&E stained images at\ntest time. We evaluate the proposed algorithm on datasets with various tissue\nsources and sequencing platforms, where it demonstrates clear improvement over\nexisting approaches. $\\textbf{Stem}$ generates high-fidelity gene expression\npredictions that share similar gene variation levels as ground truth data,\nsuggesting that our method preserves the underlying biological heterogeneity.\nOur proposed pipeline opens up the possibility of analyzing existing, easily\naccessible H&E stained histology images from a genomics point of view without\nphysically performing gene expression profiling and empowers potential\nbiological discovery from H&E stained histology images.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "q-bio.QM",
      "stat.ML"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.15598v1",
    "published_date": "2025-01-26 16:52:27 UTC",
    "updated_date": "2025-01-26 16:52:27 UTC"
  },
  {
    "arxiv_id": "2501.15587v1",
    "title": "SCP-116K: A High-Quality Problem-Solution Dataset and a Generalized Pipeline for Automated Extraction in the Higher Education Science Domain",
    "authors": [
      "Dakuan Lu",
      "Xiaoyu Tan",
      "Rui Xu",
      "Tianchu Yao",
      "Chao Qu",
      "Wei Chu",
      "Yinghui Xu",
      "Yuan Qi"
    ],
    "abstract": "Recent breakthroughs in large language models (LLMs) exemplified by the\nimpressive mathematical and scientific reasoning capabilities of the o1 model\nhave spotlighted the critical importance of high-quality training data in\nadvancing LLM performance across STEM disciplines. While the mathematics\ncommunity has benefited from a growing body of curated datasets, the scientific\ndomain at the higher education level has long suffered from a scarcity of\ncomparable resources. To address this gap, we present SCP-116K, a new\nlarge-scale dataset of 116,756 high-quality problem-solution pairs,\nautomatically extracted from heterogeneous sources using a streamlined and\nhighly generalizable pipeline. Our approach involves stringent filtering to\nensure the scientific rigor and educational level of the extracted materials,\nwhile maintaining adaptability for future expansions or domain transfers. By\nopenly releasing both the dataset and the extraction pipeline, we seek to\nfoster research on scientific reasoning, enable comprehensive performance\nevaluations of new LLMs, and lower the barrier to replicating the successes of\nadvanced models like o1 in the broader science community. We believe SCP-116K\nwill serve as a critical resource, catalyzing progress in high-level scientific\nreasoning tasks and promoting further innovations in LLM development. The\ndataset and code are publicly available at\nhttps://github.com/AQA6666/SCP-116K-open.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, 1 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.15587v1",
    "published_date": "2025-01-26 16:26:38 UTC",
    "updated_date": "2025-01-26 16:26:38 UTC"
  },
  {
    "arxiv_id": "2501.15585v2",
    "title": "Twin Transition or Competing Interests? Validation of the Artificial Intelligence and Sustainability Perceptions Inventory (AISPI)",
    "authors": [
      "Annika Bush"
    ],
    "abstract": "As artificial intelligence (AI) and sustainability initiatives increasingly\nintersect, understanding public perceptions of their relationship becomes\ncrucial for successful implementation. However, no validated instrument exists\nto measure these specific perceptions. This paper presents the development and\nvalidation of the Artificial Intelligence and Sustainability Perceptions\nInventory (AISPI), a novel 13-item instrument measuring how individuals view\nthe relationship between AI advancement and environmental sustainability.\nThrough factor analysis (N=105), we identified two distinct dimensions: Twin\nTransition and Competing Interests. The instrument demonstrated strong\nreliability (alpha=.89) and construct validity through correlations with\nestablished measures of AI and sustainability attitudes. Our findings suggest\nthat individuals can simultaneously recognize both synergies and tensions in\nthe AI-sustainability relationship, offering important implications for\nresearchers and practitioners working at this critical intersection. This work\nprovides a foundational tool for future research on public perceptions of AI's\nrole in sustainable development.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "CHI 2025 Late Breaking Work",
    "pdf_url": "http://arxiv.org/pdf/2501.15585v2",
    "published_date": "2025-01-26 16:21:27 UTC",
    "updated_date": "2025-03-24 09:33:47 UTC"
  },
  {
    "arxiv_id": "2501.15572v3",
    "title": "Comparative clinical evaluation of \"memory-efficient\" synthetic 3d generative adversarial networks (gan) head-to-head to state of art: results on computed tomography of the chest",
    "authors": [
      "Mahshid Shiri",
      "Chandra Bortolotto",
      "Alessandro Bruno",
      "Alessio Consonni",
      "Daniela Maria Grasso",
      "Leonardo Brizzi",
      "Daniele Loiacono",
      "Lorenzo Preda"
    ],
    "abstract": "Generative Adversarial Networks (GANs) are increasingly used to generate\nsynthetic medical images, addressing the critical shortage of annotated data\nfor training Artificial Intelligence systems. This study introduces CRF-GAN, a\nnovel memory-efficient GAN architecture that enhances structural consistency in\n3D medical image synthesis. Integrating Conditional Random Fields within a\ntwo-step generation process allows CRF-GAN improving spatial coherence while\nmaintaining high-resolution image quality. The model's performance is evaluated\nagainst the state-of-the-art hierarchical (HA)-GAN model. Materials and\nMethods: We evaluate the performance of CRF-GAN against the HA-GAN model. The\ncomparison between the two models was made through a quantitative evaluation,\nusing FID and MMD metrics, and a qualitative evaluation, through a\ntwo-alternative forced choice (2AFC) test completed by a pool of 12 resident\nradiologists, to assess the realism of the generated images. Results: CRF-GAN\noutperformed HA-GAN with lower FID and MMD scores, indicating better image\nfidelity. The 2AFC test showed a significant preference for images generated by\nCRF-Gan over those generated by HA-GAN. Additionally, CRF-GAN demonstrated\n9.34% lower memory usage and achieved up to 14.6% faster training speeds,\noffering substantial computational savings. Discussion: CRF-GAN model\nsuccessfully generates high-resolution 3D medical images with non-inferior\nquality to conventional models, while being more memory-efficient and faster.\nThe key objective was not only to lower the computational cost but also to\nreallocate the freed-up resources towards the creation of higher-resolution 3D\nimaging, which is still a critical factor limiting their direct clinical\napplicability. Moreover, unlike many previous studies, we combined qualitative\nand quantitative assessments to obtain a more holistic feedback on the model's\nperformance.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "Accpeted to Journal of Imaging Informatics in Medicine",
    "pdf_url": "http://arxiv.org/pdf/2501.15572v3",
    "published_date": "2025-01-26 15:57:44 UTC",
    "updated_date": "2025-04-20 21:31:01 UTC"
  },
  {
    "arxiv_id": "2501.15564v2",
    "title": "Diffusion-Based Planning for Autonomous Driving with Flexible Guidance",
    "authors": [
      "Yinan Zheng",
      "Ruiming Liang",
      "Kexin Zheng",
      "Jinliang Zheng",
      "Liyuan Mao",
      "Jianxiong Li",
      "Weihao Gu",
      "Rui Ai",
      "Shengbo Eben Li",
      "Xianyuan Zhan",
      "Jingjing Liu"
    ],
    "abstract": "Achieving human-like driving behaviors in complex open-world environments is\na critical challenge in autonomous driving. Contemporary learning-based\nplanning approaches such as imitation learning methods often struggle to\nbalance competing objectives and lack of safety assurance,due to limited\nadaptability and inadequacy in learning complex multi-modal behaviors commonly\nexhibited in human planning, not to mention their strong reliance on the\nfallback strategy with predefined rules. We propose a novel transformer-based\nDiffusion Planner for closed-loop planning, which can effectively model\nmulti-modal driving behavior and ensure trajectory quality without any\nrule-based refinement. Our model supports joint modeling of both prediction and\nplanning tasks under the same architecture, enabling cooperative behaviors\nbetween vehicles. Moreover, by learning the gradient of the trajectory score\nfunction and employing a flexible classifier guidance mechanism, Diffusion\nPlanner effectively achieves safe and adaptable planning behaviors. Evaluations\non the large-scale real-world autonomous planning benchmark nuPlan and our\nnewly collected 200-hour delivery-vehicle driving dataset demonstrate that\nDiffusion Planner achieves state-of-the-art closed-loop performance with robust\ntransferability in diverse driving styles.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.15564v2",
    "published_date": "2025-01-26 15:49:50 UTC",
    "updated_date": "2025-02-09 16:37:09 UTC"
  },
  {
    "arxiv_id": "2501.15562v1",
    "title": "CE-SDWV: Effective and Efficient Concept Erasure for Text-to-Image Diffusion Models via a Semantic-Driven Word Vocabulary",
    "authors": [
      "Jiahang Tu",
      "Qian Feng",
      "Chufan Chen",
      "Jiahua Dong",
      "Hanbin Zhao",
      "Chao Zhang",
      "Hui Qian"
    ],
    "abstract": "Large-scale text-to-image (T2I) diffusion models have achieved remarkable\ngenerative performance about various concepts. With the limitation of privacy\nand safety in practice, the generative capability concerning NSFW (Not Safe For\nWork) concepts is undesirable, e.g., producing sexually explicit photos, and\nlicensed images. The concept erasure task for T2I diffusion models has\nattracted considerable attention and requires an effective and efficient\nmethod. To achieve this goal, we propose a CE-SDWV framework, which removes the\ntarget concepts (e.g., NSFW concepts) of T2I diffusion models in the text\nsemantic space by only adjusting the text condition tokens and does not need to\nre-train the original T2I diffusion model's weights. Specifically, our\nframework first builds a target concept-related word vocabulary to enhance the\nrepresentation of the target concepts within the text semantic space, and then\nutilizes an adaptive semantic component suppression strategy to ablate the\ntarget concept-related semantic information in the text condition tokens. To\nfurther adapt the above text condition tokens to the original image semantic\nspace, we propose an end-to-end gradient-orthogonal token optimization\nstrategy. Extensive experiments on I2P and UnlearnCanvas benchmarks demonstrate\nthe effectiveness and efficiency of our method.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "24 pages, 15 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.15562v1",
    "published_date": "2025-01-26 15:39:47 UTC",
    "updated_date": "2025-01-26 15:39:47 UTC"
  },
  {
    "arxiv_id": "2501.15555v1",
    "title": "Distributionally Robust Graph Out-of-Distribution Recommendation via Diffusion Model",
    "authors": [
      "Chu Zhao",
      "Enneng Yang",
      "Yuliang Liang",
      "Jianzhe Zhao",
      "Guibing Guo",
      "Xingwei Wang"
    ],
    "abstract": "The distributionally robust optimization (DRO)-based graph neural network\nmethods improve recommendation systems' out-of-distribution (OOD)\ngeneralization by optimizing the model's worst-case performance. However, these\nstudies fail to consider the impact of noisy samples in the training data,\nwhich results in diminished generalization capabilities and lower accuracy.\nThrough experimental and theoretical analysis, this paper reveals that current\nDRO-based graph recommendation methods assign greater weight to noise\ndistribution, leading to model parameter learning being dominated by it. When\nthe model overly focuses on fitting noise samples in the training data, it may\nlearn irrelevant or meaningless features that cannot be generalized to OOD\ndata. To address this challenge, we design a Distributionally Robust Graph\nmodel for OOD recommendation (DRGO). Specifically, our method first employs a\nsimple and effective diffusion paradigm to alleviate the noisy effect in the\nlatent space. Additionally, an entropy regularization term is introduced in the\nDRO objective function to avoid extreme sample weights in the worst-case\ndistribution. Finally, we provide a theoretical proof of the generalization\nerror bound of DRGO as well as a theoretical analysis of how our approach\nmitigates noisy sample effects, which helps to better understand the proposed\nframework from a theoretical perspective. We conduct extensive experiments on\nfour datasets to evaluate the effectiveness of our framework against three\ntypical distribution shifts, and the results demonstrate its superiority in\nboth independently and identically distributed distributions (IID) and OOD.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.GR",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages, Accepted by WWW'25",
    "pdf_url": "http://arxiv.org/pdf/2501.15555v1",
    "published_date": "2025-01-26 15:07:52 UTC",
    "updated_date": "2025-01-26 15:07:52 UTC"
  },
  {
    "arxiv_id": "2501.15547v1",
    "title": "Building Efficient Lightweight CNN Models",
    "authors": [
      "Nathan Isong"
    ],
    "abstract": "Convolutional Neural Networks (CNNs) are pivotal in image classification\ntasks due to their robust feature extraction capabilities. However, their high\ncomputational and memory requirements pose challenges for deployment in\nresource-constrained environments. This paper introduces a methodology to\nconstruct lightweight CNNs while maintaining competitive accuracy. The approach\nintegrates two stages of training; dual-input-output model and transfer\nlearning with progressive unfreezing. The dual-input-output model train on\noriginal and augmented datasets, enhancing robustness. Progressive unfreezing\nis applied to the unified model to optimize pre-learned features during\nfine-tuning, enabling faster convergence and improved model accuracy.\n  The methodology was evaluated on three benchmark datasets; handwritten digit\nMNIST, fashion MNIST, and CIFAR-10. The proposed model achieved a\nstate-of-the-art accuracy of 99% on the handwritten digit MNIST and 89% on\nfashion MNIST, with only 14,862 parameters and a model size of 0.17 MB. While\nperformance on CIFAR-10 was comparatively lower (65% with less than 20,00\nparameters), the results highlight the scalability of this method. The final\nmodel demonstrated fast inference times and low latency, making it suitable for\nreal-time applications.\n  Future directions include exploring advanced augmentation techniques,\nimproving architectural scalability for complex datasets, and extending the\nmethodology to tasks beyond classification. This research underscores the\npotential for creating efficient, scalable, and task-specific CNNs for diverse\napplications.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "25 pages, 22 figures, 6 tables, JMLR journal standard paper and to be\n  submitted",
    "pdf_url": "http://arxiv.org/pdf/2501.15547v1",
    "published_date": "2025-01-26 14:39:01 UTC",
    "updated_date": "2025-01-26 14:39:01 UTC"
  },
  {
    "arxiv_id": "2501.15544v3",
    "title": "Advancing Generative Artificial Intelligence and Large Language Models for Demand Side Management with Internet of Electric Vehicles",
    "authors": [
      "Hanwen Zhang",
      "Ruichen Zhang",
      "Wei Zhang",
      "Dusit Niyato",
      "Yonggang Wen"
    ],
    "abstract": "Generative artificial intelligence, particularly through large language\nmodels (LLMs), is poised to transform energy optimization and demand side\nmanagement (DSM) within microgrids. This paper explores the integration of LLMs\ninto energy management, emphasizing their roles in automating the optimization\nof DSM strategies with Internet of electric vehicles. We investigate challenges\nand solutions associated with DSM and explore the new opportunities presented\nby leveraging LLMs. Then, we propose an innovative solution that enhances LLMs\nwith retrieval-augmented generation for automatic problem formulation, code\ngeneration, and customizing optimization. We present a case study to\ndemonstrate the effectiveness of our proposed solution in charging scheduling\nand optimization for electric vehicles, highlighting our solution's significant\nadvancements in energy efficiency and user adaptability. This work underscores\nthe potential of LLMs for energy optimization and fosters a new era of\nintelligent DSM solutions.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "9 Pages",
    "pdf_url": "http://arxiv.org/pdf/2501.15544v3",
    "published_date": "2025-01-26 14:31:03 UTC",
    "updated_date": "2025-04-21 11:09:11 UTC"
  },
  {
    "arxiv_id": "2501.15529v1",
    "title": "UNIDOOR: A Universal Framework for Action-Level Backdoor Attacks in Deep Reinforcement Learning",
    "authors": [
      "Oubo Ma",
      "Linkang Du",
      "Yang Dai",
      "Chunyi Zhou",
      "Qingming Li",
      "Yuwen Pu",
      "Shouling Ji"
    ],
    "abstract": "Deep reinforcement learning (DRL) is widely applied to safety-critical\ndecision-making scenarios. However, DRL is vulnerable to backdoor attacks,\nespecially action-level backdoors, which pose significant threats through\nprecise manipulation and flexible activation, risking outcomes like vehicle\ncollisions or drone crashes. The key distinction of action-level backdoors lies\nin the utilization of the backdoor reward function to associate triggers with\ntarget actions. Nevertheless, existing studies typically rely on backdoor\nreward functions with fixed values or conditional flipping, which lack\nuniversality across diverse DRL tasks and backdoor designs, resulting in\nfluctuations or even failure in practice.\n  This paper proposes the first universal action-level backdoor attack\nframework, called UNIDOOR, which enables adaptive exploration of backdoor\nreward functions through performance monitoring, eliminating the reliance on\nexpert knowledge and grid search. We highlight that action tampering serves as\na crucial component of action-level backdoor attacks in continuous action\nscenarios, as it addresses attack failures caused by low-frequency target\nactions. Extensive evaluations demonstrate that UNIDOOR significantly enhances\nthe attack performance of action-level backdoors, showcasing its universality\nacross diverse attack scenarios, including single/multiple agents,\nsingle/multiple backdoors, discrete/continuous action spaces, and sparse/dense\nreward signals. Furthermore, visualization results encompassing state\ndistribution, neuron activation, and animations demonstrate the stealthiness of\nUNIDOOR. The source code of UNIDOOR can be found at\nhttps://github.com/maoubo/UNIDOOR.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "21 pages, 12 figures, 7 tables",
    "pdf_url": "http://arxiv.org/pdf/2501.15529v1",
    "published_date": "2025-01-26 13:43:39 UTC",
    "updated_date": "2025-01-26 13:43:39 UTC"
  },
  {
    "arxiv_id": "2501.15509v1",
    "title": "FIT-Print: Towards False-claim-resistant Model Ownership Verification via Targeted Fingerprint",
    "authors": [
      "Shuo Shao",
      "Haozhe Zhu",
      "Hongwei Yao",
      "Yiming Li",
      "Tianwei Zhang",
      "Zhan Qin",
      "Kui Ren"
    ],
    "abstract": "Model fingerprinting is a widely adopted approach to safeguard the\nintellectual property rights of open-source models by preventing their\nunauthorized reuse. It is promising and convenient since it does not\nnecessitate modifying the protected model. In this paper, we revisit existing\nfingerprinting methods and reveal that they are vulnerable to false claim\nattacks where adversaries falsely assert ownership of any third-party model. We\ndemonstrate that this vulnerability mostly stems from their untargeted nature,\nwhere they generally compare the outputs of given samples on different models\ninstead of the similarities to specific references. Motivated by these\nfindings, we propose a targeted fingerprinting paradigm (i.e., FIT-Print) to\ncounteract false claim attacks. Specifically, FIT-Print transforms the\nfingerprint into a targeted signature via optimization. Building on the\nprinciples of FIT-Print, we develop bit-wise and list-wise black-box model\nfingerprinting methods, i.e., FIT-ModelDiff and FIT-LIME, which exploit the\ndistance between model outputs and the feature attribution of specific samples\nas the fingerprint, respectively. Extensive experiments on benchmark models and\ndatasets verify the effectiveness, conferrability, and resistance to false\nclaim attacks of our FIT-Print.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.15509v1",
    "published_date": "2025-01-26 13:00:58 UTC",
    "updated_date": "2025-01-26 13:00:58 UTC"
  },
  {
    "arxiv_id": "2501.15495v1",
    "title": "Expert-Free Online Transfer Learning in Multi-Agent Reinforcement Learning",
    "authors": [
      "Alberto Castagna"
    ],
    "abstract": "Reinforcement Learning (RL) enables an intelligent agent to optimise its\nperformance in a task by continuously taking action from an observed state and\nreceiving a feedback from the environment in form of rewards. RL typically uses\ntables or linear approximators to map state-action tuples that maximises the\nreward. Combining RL with deep neural networks (DRL) significantly increases\nits scalability and enables it to address more complex problems than before.\nHowever, DRL also inherits downsides from both RL and deep learning. Despite\nDRL improves generalisation across similar state-action pairs when compared to\nsimpler RL policy representations like tabular methods, it still requires the\nagent to adequately explore the state-action space. Additionally, deep methods\nrequire more training data, with the volume of data escalating with the\ncomplexity and size of the neural network. As a result, deep RL requires a long\ntime to collect enough agent-environment samples and to successfully learn the\nunderlying policy. Furthermore, often even a slight alteration to the task\ninvalidates any previous acquired knowledge. To address these shortcomings,\nTransfer Learning (TL) has been introduced, which enables the use of external\nknowledge from other tasks or agents to enhance a learning process. The goal of\nTL is to reduce the learning complexity for an agent dealing with an unfamiliar\ntask by simplifying the exploration process. This is achieved by lowering the\namount of new information required by its learning model, resulting in a\nreduced overall convergence time...",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "PhD Thesis",
    "pdf_url": "http://arxiv.org/pdf/2501.15495v1",
    "published_date": "2025-01-26 11:53:18 UTC",
    "updated_date": "2025-01-26 11:53:18 UTC"
  },
  {
    "arxiv_id": "2501.15492v1",
    "title": "Color Flow Imaging Microscopy Improves Identification of Stress Sources of Protein Aggregates in Biopharmaceuticals",
    "authors": [
      "Michaela Cohrs",
      "Shiwoo Koak",
      "Yejin Lee",
      "Yu Jin Sung",
      "Wesley De Neve",
      "Hristo L. Svilenov",
      "Utku Ozbulak"
    ],
    "abstract": "Protein-based therapeutics play a pivotal role in modern medicine targeting\nvarious diseases. Despite their therapeutic importance, these products can\naggregate and form subvisible particles (SvPs), which can compromise their\nefficacy and trigger immunological responses, emphasizing the critical need for\nrobust monitoring techniques. Flow Imaging Microscopy (FIM) has been a\nsignificant advancement in detecting SvPs, evolving from monochrome to more\nrecently incorporating color imaging. Complementing SvP images obtained via\nFIM, deep learning techniques have recently been employed successfully for\nstress source identification of monochrome SvPs. In this study, we explore the\npotential of color FIM to enhance the characterization of stress sources in\nSvPs. To achieve this, we curate a new dataset comprising 16,000 SvPs from\neight commercial monoclonal antibodies subjected to heat and mechanical stress.\nUsing both supervised and self-supervised convolutional neural networks, as\nwell as vision transformers in large-scale experiments, we demonstrate that\ndeep learning with color FIM images consistently outperforms monochrome images,\nthus highlighting the potential of color FIM in stress source classification\ncompared to its monochrome counterparts.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted for publication in MICCAI 2024 Workshop on Medical Optical\n  Imaging and Virtual Microscopy Image Analysis (MOVI)",
    "pdf_url": "http://arxiv.org/pdf/2501.15492v1",
    "published_date": "2025-01-26 11:48:28 UTC",
    "updated_date": "2025-01-26 11:48:28 UTC"
  },
  {
    "arxiv_id": "2501.15489v1",
    "title": "AI in Oncology: Transforming Cancer Detection through Machine Learning and Deep Learning Applications",
    "authors": [
      "Muhammad Aftab",
      "Faisal Mehmood",
      "Chengjuan Zhang",
      "Alishba Nadeem",
      "Zigang Dong",
      "Yanan Jiang",
      "Kangdongs Liu"
    ],
    "abstract": "Artificial intelligence (AI) has potential to revolutionize the field of\noncology by enhancing the precision of cancer diagnosis, optimizing treatment\nstrategies, and personalizing therapies for a variety of cancers. This review\nexamines the limitations of conventional diagnostic techniques and explores the\ntransformative role of AI in diagnosing and treating cancers such as lung,\nbreast, colorectal, liver, stomach, esophageal, cervical, thyroid, prostate,\nand skin cancers. The primary objective of this paper is to highlight the\nsignificant advancements that AI algorithms have brought to oncology within the\nmedical industry. By enabling early cancer detection, improving diagnostic\naccuracy, and facilitating targeted treatment delivery, AI contributes to\nsubstantial improvements in patient outcomes. The integration of AI in medical\nimaging, genomic analysis, and pathology enhances diagnostic precision and\nintroduces a novel, less invasive approach to cancer screening. This not only\nboosts the effectiveness of medical facilities but also reduces operational\ncosts. The study delves into the application of AI in radiomics for detailed\ncancer characterization, predictive analytics for identifying associated risks,\nand the development of algorithm-driven robots for immediate diagnosis.\nFurthermore, it investigates the impact of AI on addressing healthcare\nchallenges, particularly in underserved and remote regions. The overarching\ngoal of this platform is to support the development of expert recommendations\nand to provide universal, efficient diagnostic procedures. By reviewing\nexisting research and clinical studies, this paper underscores the pivotal role\nof AI in improving the overall cancer care system. It emphasizes how AI-enabled\nsystems can enhance clinical decision-making and expand treatment options,\nthereby underscoring the importance of AI in advancing precision oncology",
    "categories": [
      "cs.AI",
      "eess.IV",
      "q-bio.QM"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.15489v1",
    "published_date": "2025-01-26 11:32:43 UTC",
    "updated_date": "2025-01-26 11:32:43 UTC"
  },
  {
    "arxiv_id": "2501.15486v1",
    "title": "FedAlign: Federated Domain Generalization with Cross-Client Feature Alignment",
    "authors": [
      "Sunny Gupta",
      "Vinay Sutar",
      "Varunav Singh",
      "Amit Sethi"
    ],
    "abstract": "Federated Learning (FL) offers a decentralized paradigm for collaborative\nmodel training without direct data sharing, yet it poses unique challenges for\nDomain Generalization (DG), including strict privacy constraints, non-i.i.d.\nlocal data, and limited domain diversity. We introduce FedAlign, a lightweight,\nprivacy-preserving framework designed to enhance DG in federated settings by\nsimultaneously increasing feature diversity and promoting domain invariance.\nFirst, a cross-client feature extension module broadens local domain\nrepresentations through domain-invariant feature perturbation and selective\ncross-client feature transfer, allowing each client to safely access a richer\ndomain space. Second, a dual-stage alignment module refines global feature\nlearning by aligning both feature embeddings and predictions across clients,\nthereby distilling robust, domain-invariant features. By integrating these\nmodules, our method achieves superior generalization to unseen domains while\nmaintaining data privacy and operating with minimal computational and\ncommunication overhead.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.DC",
      "I.2.6; C.1.4; D.1.3; I.5.1; H.3.4; I.2.10; I.4.0; I.4.1; I.4.2;\n  I.4.6; I.4.7; I.4.8; I.4.9; I.4.10; I.5.1; I.5.2; I.5.4; J.2; I.2.11; I.2.10"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.15486v1",
    "published_date": "2025-01-26 11:17:32 UTC",
    "updated_date": "2025-01-26 11:17:32 UTC"
  },
  {
    "arxiv_id": "2501.15464v2",
    "title": "TractoGPT: A GPT architecture for White Matter Segmentation",
    "authors": [
      "Anoushkrit Goel",
      "Simroop Singh",
      "Ankita Joshi",
      "Ranjeet Ranjan Jha",
      "Chirag Ahuja",
      "Aditya Nigam",
      "Arnav Bhavsar"
    ],
    "abstract": "White matter bundle segmentation is crucial for studying brain structural\nconnectivity, neurosurgical planning, and neurological disorders. White Matter\nSegmentation remains challenging due to structural similarity in streamlines,\nsubject variability, symmetry in 2 hemispheres, etc. To address these\nchallenges, we propose TractoGPT, a GPT-based architecture trained on\nstreamline, cluster, and fusion data representations separately. TractoGPT is a\nfully-automatic method that generalizes across datasets and retains shape\ninformation of the white matter bundles. Experiments also show that TractoGPT\noutperforms state-of-the-art methods on average DICE, Overlap and Overreach\nscores. We use TractoInferno and 105HCP datasets and validate generalization\nacross dataset.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted as a conference paper at 23rd IEEE International Symposium\n  on Biomedical Imaging 2025. IEEE holds the copyright for this publication",
    "pdf_url": "http://arxiv.org/pdf/2501.15464v2",
    "published_date": "2025-01-26 09:54:10 UTC",
    "updated_date": "2025-02-21 05:16:21 UTC"
  },
  {
    "arxiv_id": "2501.17186v2",
    "title": "Complete Chess Games Enable LLM Become A Chess Master",
    "authors": [
      "Yinqi Zhang",
      "Xintian Han",
      "Haolong Li",
      "Kedi Chen",
      "Shaohui Lin"
    ],
    "abstract": "Large language models (LLM) have shown remarkable abilities in text\ngeneration, question answering, language translation, reasoning and many other\ntasks. It continues to advance rapidly and is becoming increasingly influential\nin various fields, from technology and business to education and entertainment.\nDespite LLM's success in multiple areas, its ability to play abstract games,\nsuch as chess, is underexplored. Chess-playing requires the language models to\noutput legal and reasonable moves from textual inputs. Here, we propose the\nLarge language model ChessLLM to play full chess games. We transform the game\ninto a textual format with the best move represented in the Forsyth-Edwards\nNotation. We show that by simply supervised fine-tuning, our model has achieved\na professional-level Elo rating of 1788 in matches against the standard\nElo-rated Stockfish when permitted to sample 10 times. We further show that\ndata quality is important. Long-round data supervision enjoys a 350 Elo rating\nimprovement over short-round data.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "NAACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.17186v2",
    "published_date": "2025-01-26 09:43:39 UTC",
    "updated_date": "2025-01-30 04:02:48 UTC"
  },
  {
    "arxiv_id": "2501.15463v1",
    "title": "Mind the Value-Action Gap: Do LLMs Act in Alignment with Their Values?",
    "authors": [
      "Hua Shen",
      "Nicholas Clark",
      "Tanushree Mitra"
    ],
    "abstract": "Existing research primarily evaluates the values of LLMs by examining their\nstated inclinations towards specific values. However, the \"Value-Action Gap,\" a\nphenomenon rooted in environmental and social psychology, reveals discrepancies\nbetween individuals' stated values and their actions in real-world contexts. To\nwhat extent do LLMs exhibit a similar gap between their stated values and their\nactions informed by those values? This study introduces ValueActionLens, an\nevaluation framework to assess the alignment between LLMs' stated values and\ntheir value-informed actions. The framework encompasses the generation of a\ndataset comprising 14.8k value-informed actions across twelve cultures and\neleven social topics, and two tasks to evaluate how well LLMs' stated value\ninclinations and value-informed actions align across three different alignment\nmeasures. Extensive experiments reveal that the alignment between LLMs' stated\nvalues and actions is sub-optimal, varying significantly across scenarios and\nmodels. Analysis of misaligned results identifies potential harms from certain\nvalue-action gaps. To predict the value-action gaps, we also uncover that\nleveraging reasoned explanations improves performance. These findings\nunderscore the risks of relying solely on the LLMs' stated values to predict\ntheir behaviors and emphasize the importance of context-aware evaluations of\nLLM values and value-action gaps.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.15463v1",
    "published_date": "2025-01-26 09:33:51 UTC",
    "updated_date": "2025-01-26 09:33:51 UTC"
  },
  {
    "arxiv_id": "2501.15452v1",
    "title": "Identifying Critical Tokens for Accurate Predictions in Transformer-based Medical Imaging Models",
    "authors": [
      "Solha Kang",
      "Joris Vankerschaver",
      "Utku Ozbulak"
    ],
    "abstract": "With the advancements in self-supervised learning (SSL), transformer-based\ncomputer vision models have recently demonstrated superior results compared to\nconvolutional neural networks (CNNs) and are poised to dominate the field of\nartificial intelligence (AI)-based medical imaging in the upcoming years.\nNevertheless, similar to CNNs, unveiling the decision-making process of\ntransformer-based models remains a challenge. In this work, we take a step\ntowards demystifying the decision-making process of transformer-based medical\nimaging models and propose Token Insight, a novel method that identifies the\ncritical tokens that contribute to the prediction made by the model. Our method\nrelies on the principled approach of token discarding native to\ntransformer-based models, requires no additional module, and can be applied to\nany transformer model. Using the proposed approach, we quantify the importance\nof each token based on its contribution to the prediction and enable a more\nnuanced understanding of the model's decisions. Our experimental results which\nare showcased on the problem of colonic polyp identification using both\nsupervised and self-supervised pretrained vision transformers indicate that\nToken Insight contributes to a more transparent and interpretable\ntransformer-based medical imaging model, fostering trust and facilitating\nbroader adoption in clinical settings.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted for publication in MICCAI 2024 Workshop on Machine Learning\n  in Medical Imaging (MLMI)",
    "pdf_url": "http://arxiv.org/pdf/2501.15452v1",
    "published_date": "2025-01-26 08:49:13 UTC",
    "updated_date": "2025-01-26 08:49:13 UTC"
  },
  {
    "arxiv_id": "2501.15448v1",
    "title": "SQ-DM: Accelerating Diffusion Models with Aggressive Quantization and Temporal Sparsity",
    "authors": [
      "Zichen Fan",
      "Steve Dai",
      "Rangharajan Venkatesan",
      "Dennis Sylvester",
      "Brucek Khailany"
    ],
    "abstract": "Diffusion models have gained significant popularity in image generation\ntasks. However, generating high-quality content remains notably slow because it\nrequires running model inference over many time steps. To accelerate these\nmodels, we propose to aggressively quantize both weights and activations, while\nsimultaneously promoting significant activation sparsity. We further observe\nthat the stated sparsity pattern varies among different channels and evolves\nacross time steps. To support this quantization and sparsity scheme, we present\na novel diffusion model accelerator featuring a heterogeneous mixed-precision\ndense-sparse architecture, channel-last address mapping, and a time-step-aware\nsparsity detector for efficient handling of the sparsity pattern. Our 4-bit\nquantization technique demonstrates superior generation quality compared to\nexisting 4-bit methods. Our custom accelerator achieves 6.91x speed-up and\n51.5% energy reduction compared to traditional dense accelerators.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.AR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "7 pages, 12 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2501.15448v1",
    "published_date": "2025-01-26 08:34:26 UTC",
    "updated_date": "2025-01-26 08:34:26 UTC"
  },
  {
    "arxiv_id": "2501.15446v1",
    "title": "Token Democracy: The Architectural Limits of Alignment in Transformer-Based Language Models",
    "authors": [
      "Robin Young"
    ],
    "abstract": "Modern language models paradoxically combine unprecedented capability with\npersistent vulnerability in that they can draft poetry yet cannot reliably\nrefuse harmful requests. We reveal this fragility stems not from inadequate\ntraining, but from a fundamental architectural limitation: transformers process\nall tokens as equals. Transformers operate as computational democracies,\ngranting equal voice to all tokens. This is a design tragically unsuited for\nAGI, where we cannot risk adversarial \"candidates\" hijacking the system.\nThrough formal analysis, we demonstrate that safety instructions fundamentally\nlack privileged status in transformer architectures, that they compete with\nadversarial inputs in the same computational arena, making robust alignment\nthrough prompting or fine-tuning inherently limited. This \"token democracy\"\nexplains why jailbreaks bypass even extensively safety-trained models and why\npositional shifts erode prompt effectiveness. Our work systematizes\npractitioners' tacit knowledge into an architectural critique, showing current\nalignment approaches create mere preferences, not constraints.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.15446v1",
    "published_date": "2025-01-26 08:26:06 UTC",
    "updated_date": "2025-01-26 08:26:06 UTC"
  },
  {
    "arxiv_id": "2501.15445v2",
    "title": "StochSync: Stochastic Diffusion Synchronization for Image Generation in Arbitrary Spaces",
    "authors": [
      "Kyeongmin Yeo",
      "Jaihoon Kim",
      "Minhyuk Sung"
    ],
    "abstract": "We propose a zero-shot method for generating images in arbitrary spaces\n(e.g., a sphere for 360{\\deg} panoramas and a mesh surface for texture) using a\npretrained image diffusion model. The zero-shot generation of various visual\ncontent using a pretrained image diffusion model has been explored mainly in\ntwo directions. First, Diffusion Synchronization-performing reverse diffusion\nprocesses jointly across different projected spaces while synchronizing them in\nthe target space-generates high-quality outputs when enough conditioning is\nprovided, but it struggles in its absence. Second, Score Distillation\nSampling-gradually updating the target space data through gradient\ndescent-results in better coherence but often lacks detail. In this paper, we\nreveal for the first time the interconnection between these two methods while\nhighlighting their differences. To this end, we propose StochSync, a novel\napproach that combines the strengths of both, enabling effective performance\nwith weak conditioning. Our experiments demonstrate that StochSync provides the\nbest performance in 360{\\deg} panorama generation (where image conditioning is\nnot given), outperforming previous finetuning-based methods, and also delivers\ncomparable results in 3D mesh texturing (where depth conditioning is provided)\nwith previous methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://stochsync.github.io/ (ICLR 2025)",
    "pdf_url": "http://arxiv.org/pdf/2501.15445v2",
    "published_date": "2025-01-26 08:22:44 UTC",
    "updated_date": "2025-03-02 11:16:08 UTC"
  },
  {
    "arxiv_id": "2501.16391v2",
    "title": "Inductive-Associative Meta-learning Pipeline with Human Cognitive Patterns for Unseen Drug-Target Interaction Prediction",
    "authors": [
      "Xiaoqing Lian",
      "Jie Zhu",
      "Tianxu Lv",
      "Shiyun Nie",
      "Hang Fan",
      "Guosheng Wu",
      "Yunjun Ge",
      "Lihua Li",
      "Xiangxiang Zeng",
      "Xiang Pan"
    ],
    "abstract": "Significant differences in protein structures hinder the generalization of\nexisting drug-target interaction (DTI) models, which often rely heavily on\npre-learned binding principles or detailed annotations. In contrast, BioBridge\ndesigns an Inductive-Associative pipeline inspired by the workflow of\nscientists who base their accumulated expertise on drawing insights into novel\ndrug-target pairs from weakly related references. BioBridge predicts novel\ndrug-target interactions using limited sequence data, incorporating multi-level\nencoders with adversarial training to accumulate transferable binding\nprinciples. On these principles basis, BioBridge employs a dynamic prototype\nmeta-learning framework to associate insights from weakly related annotations,\nenabling robust predictions for previously unseen drug-target pairs. Extensive\nexperiments demonstrate that BioBridge surpasses existing models, especially\nfor unseen proteins. Notably, when only homologous protein binding data is\navailable, BioBridge proves effective for virtual screening of the epidermal\ngrowth factor receptor and adenosine receptor, underscoring its potential in\ndrug discovery.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.BM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.16391v2",
    "published_date": "2025-01-26 08:22:22 UTC",
    "updated_date": "2025-03-27 07:41:02 UTC"
  },
  {
    "arxiv_id": "2501.15442v2",
    "title": "Overview of the Amphion Toolkit (v0.2)",
    "authors": [
      "Jiaqi Li",
      "Xueyao Zhang",
      "Yuancheng Wang",
      "Haorui He",
      "Chaoren Wang",
      "Li Wang",
      "Huan Liao",
      "Junyi Ao",
      "Zeyu Xie",
      "Yiqiao Huang",
      "Junan Zhang",
      "Zhizheng Wu"
    ],
    "abstract": "Amphion is an open-source toolkit for Audio, Music, and Speech Generation,\ndesigned to lower the entry barrier for junior researchers and engineers in\nthese fields. It provides a versatile framework that supports a variety of\ngeneration tasks and models. In this report, we introduce Amphion v0.2, the\nsecond major release developed in 2024. This release features a 100K-hour\nopen-source multilingual dataset, a robust data preparation pipeline, and novel\nmodels for tasks such as text-to-speech, audio coding, and voice conversion.\nFurthermore, the report includes multiple tutorials that guide users through\nthe functionalities and usage of the newly released models.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Github: https://github.com/open-mmlab/Amphion",
    "pdf_url": "http://arxiv.org/pdf/2501.15442v2",
    "published_date": "2025-01-26 08:10:13 UTC",
    "updated_date": "2025-02-11 13:05:36 UTC"
  },
  {
    "arxiv_id": "2502.09624v1",
    "title": "Efficient and Trustworthy Block Propagation for Blockchain-enabled Mobile Embodied AI Networks: A Graph Resfusion Approach",
    "authors": [
      "Jiawen Kang",
      "Jiana Liao",
      "Runquan Gao",
      "Jinbo Wen",
      "Huawei Huang",
      "Maomao Zhang",
      "Changyan Yi",
      "Tao Zhang",
      "Dusit Niyato",
      "Zibin Zheng"
    ],
    "abstract": "By synergistically integrating mobile networks and embodied artificial\nintelligence (AI), Mobile Embodied AI Networks (MEANETs) represent an advanced\nparadigm that facilitates autonomous, context-aware, and interactive behaviors\nwithin dynamic environments. Nevertheless, the rapid development of MEANETs is\naccompanied by challenges in trustworthiness and operational efficiency.\nFortunately, blockchain technology, with its decentralized and immutable\ncharacteristics, offers promising solutions for MEANETs. However, existing\nblock propagation mechanisms suffer from challenges such as low propagation\nefficiency and weak security for block propagation, which results in delayed\ntransmission of vehicular messages or vulnerability to malicious tampering,\npotentially causing severe traffic accidents in blockchain-enabled MEANETs.\nMoreover, current block propagation strategies cannot effectively adapt to\nreal-time changes of dynamic topology in MEANETs. Therefore, in this paper, we\npropose a graph Resfusion model-based trustworthy block propagation\noptimization framework for consortium blockchain-enabled MEANETs. Specifically,\nwe propose an innovative trust calculation mechanism based on the trust cloud\nmodel, which comprehensively accounts for randomness and fuzziness in the miner\ntrust evaluation. Furthermore, by leveraging the strengths of graph neural\nnetworks and diffusion models, we develop a graph Resfusion model to\neffectively and adaptively generate the optimal block propagation trajectory.\nSimulation results demonstrate that the proposed model outperforms other\nrouting mechanisms in terms of block propagation efficiency and\ntrustworthiness. Additionally, the results highlight its strong adaptability to\ndynamic environments, making it particularly suitable for rapidly changing\nMEANETs.",
    "categories": [
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.AI",
    "comment": "15 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.09624v1",
    "published_date": "2025-01-26 07:47:05 UTC",
    "updated_date": "2025-01-26 07:47:05 UTC"
  },
  {
    "arxiv_id": "2501.15431v1",
    "title": "Self-supervised Benchmark Lottery on ImageNet: Do Marginal Improvements Translate to Improvements on Similar Datasets?",
    "authors": [
      "Utku Ozbulak",
      "Esla Timothy Anzaku",
      "Solha Kang",
      "Wesley De Neve",
      "Joris Vankerschaver"
    ],
    "abstract": "Machine learning (ML) research strongly relies on benchmarks in order to\ndetermine the relative effectiveness of newly proposed models. Recently, a\nnumber of prominent research effort argued that a number of models that improve\nthe state-of-the-art by a small margin tend to do so by winning what they call\na \"benchmark lottery\". An important benchmark in the field of machine learning\nand computer vision is the ImageNet where newly proposed models are often\nshowcased based on their performance on this dataset. Given the large number of\nself-supervised learning (SSL) frameworks that has been proposed in the past\ncouple of years each coming with marginal improvements on the ImageNet dataset,\nin this work, we evaluate whether those marginal improvements on ImageNet\ntranslate to improvements on similar datasets or not. To do so, we investigate\ntwelve popular SSL frameworks on five ImageNet variants and discover that\nmodels that seem to perform well on ImageNet may experience significant\nperformance declines on similar datasets. Specifically, state-of-the-art\nframeworks such as DINO and Swav, which are praised for their performance,\nexhibit substantial drops in performance while MoCo and Barlow Twins displays\ncomparatively good results. As a result, we argue that otherwise good and\ndesirable properties of models remain hidden when benchmarking is only\nperformed on the ImageNet validation set, making us call for more adequate\nbenchmarking. To avoid the \"benchmark lottery\" on ImageNet and to ensure a fair\nbenchmarking process, we investigate the usage of a unified metric that takes\ninto account the performance of models on other ImageNet variant datasets.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted for publication in the 2024 International Joint Conference\n  on Neural Networks (IJCNN)",
    "pdf_url": "http://arxiv.org/pdf/2501.15431v1",
    "published_date": "2025-01-26 07:19:12 UTC",
    "updated_date": "2025-01-26 07:19:12 UTC"
  },
  {
    "arxiv_id": "2501.15420v1",
    "title": "Visual Generation Without Guidance",
    "authors": [
      "Huayu Chen",
      "Kai Jiang",
      "Kaiwen Zheng",
      "Jianfei Chen",
      "Hang Su",
      "Jun Zhu"
    ],
    "abstract": "Classifier-Free Guidance (CFG) has been a default technique in various visual\ngenerative models, yet it requires inference from both conditional and\nunconditional models during sampling. We propose to build visual models that\nare free from guided sampling. The resulting algorithm, Guidance-Free Training\n(GFT), matches the performance of CFG while reducing sampling to a single\nmodel, halving the computational cost. Unlike previous distillation-based\napproaches that rely on pretrained CFG networks, GFT enables training directly\nfrom scratch. GFT is simple to implement. It retains the same maximum\nlikelihood objective as CFG and differs mainly in the parameterization of\nconditional models. Implementing GFT requires only minimal modifications to\nexisting codebases, as most design choices and hyperparameters are directly\ninherited from CFG. Our extensive experiments across five distinct visual\nmodels demonstrate the effectiveness and versatility of GFT. Across domains of\ndiffusion, autoregressive, and masked-prediction modeling, GFT consistently\nachieves comparable or even lower FID scores, with similar diversity-fidelity\ntrade-offs compared with CFG baselines, all while being guidance-free. Code\nwill be available at https://github.com/thu-ml/GFT.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.15420v1",
    "published_date": "2025-01-26 06:48:05 UTC",
    "updated_date": "2025-01-26 06:48:05 UTC"
  },
  {
    "arxiv_id": "2501.15418v1",
    "title": "Episodic Novelty Through Temporal Distance",
    "authors": [
      "Yuhua Jiang",
      "Qihan Liu",
      "Yiqin Yang",
      "Xiaoteng Ma",
      "Dianyu Zhong",
      "Hao Hu",
      "Jun Yang",
      "Bin Liang",
      "Bo Xu",
      "Chongjie Zhang",
      "Qianchuan Zhao"
    ],
    "abstract": "Exploration in sparse reward environments remains a significant challenge in\nreinforcement learning, particularly in Contextual Markov Decision Processes\n(CMDPs), where environments differ across episodes. Existing episodic intrinsic\nmotivation methods for CMDPs primarily rely on count-based approaches, which\nare ineffective in large state spaces, or on similarity-based methods that lack\nappropriate metrics for state comparison. To address these shortcomings, we\npropose Episodic Novelty Through Temporal Distance (ETD), a novel approach that\nintroduces temporal distance as a robust metric for state similarity and\nintrinsic reward computation. By employing contrastive learning, ETD accurately\nestimates temporal distances and derives intrinsic rewards based on the novelty\nof states within the current episode. Extensive experiments on various\nbenchmark tasks demonstrate that ETD significantly outperforms state-of-the-art\nmethods, highlighting its effectiveness in enhancing exploration in sparse\nreward CMDPs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR2025",
    "pdf_url": "http://arxiv.org/pdf/2501.15418v1",
    "published_date": "2025-01-26 06:43:45 UTC",
    "updated_date": "2025-01-26 06:43:45 UTC"
  },
  {
    "arxiv_id": "2501.15417v1",
    "title": "AnyEnhance: A Unified Generative Model with Prompt-Guidance and Self-Critic for Voice Enhancement",
    "authors": [
      "Junan Zhang",
      "Jing Yang",
      "Zihao Fang",
      "Yuancheng Wang",
      "Zehua Zhang",
      "Zhuo Wang",
      "Fan Fan",
      "Zhizheng Wu"
    ],
    "abstract": "We introduce AnyEnhance, a unified generative model for voice enhancement\nthat processes both speech and singing voices. Based on a masked generative\nmodel, AnyEnhance is capable of handling both speech and singing voices,\nsupporting a wide range of enhancement tasks including denoising,\ndereverberation, declipping, super-resolution, and target speaker extraction,\nall simultaneously and without fine-tuning. AnyEnhance introduces a\nprompt-guidance mechanism for in-context learning, which allows the model to\nnatively accept a reference speaker's timbre. In this way, it could boost\nenhancement performance when a reference audio is available and enable the\ntarget speaker extraction task without altering the underlying architecture.\nMoreover, we also introduce a self-critic mechanism into the generative process\nfor masked generative models, yielding higher-quality outputs through iterative\nself-assessment and refinement. Extensive experiments on various enhancement\ntasks demonstrate AnyEnhance outperforms existing methods in terms of both\nobjective metrics and subjective listening tests. Demo audios are publicly\navailable at https://amphionspace.github.io/anyenhance/.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "12 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.15417v1",
    "published_date": "2025-01-26 06:40:30 UTC",
    "updated_date": "2025-01-26 06:40:30 UTC"
  },
  {
    "arxiv_id": "2501.15409v1",
    "title": "TdAttenMix: Top-Down Attention Guided Mixup",
    "authors": [
      "Zhiming Wang",
      "Lin Gu",
      "Feng Lu"
    ],
    "abstract": "CutMix is a data augmentation strategy that cuts and pastes image patches to\nmixup training data. Existing methods pick either random or salient areas which\nare often inconsistent to labels, thus misguiding the training model. By our\nknowledge, we integrate human gaze to guide cutmix for the first time. Since\nhuman attention is driven by both high-level recognition and low-level clues,\nwe propose a controllable Top-down Attention Guided Module to obtain a general\nartificial attention which balances top-down and bottom-up attention. The\nproposed TdATttenMix then picks the patches and adjust the label mixing ratio\nthat focuses on regions relevant to the current label. Experimental results\ndemonstrate that our TdAttenMix outperforms existing state-of-the-art mixup\nmethods across eight different benchmarks. Additionally, we introduce a new\nmetric based on the human gaze and use this metric to investigate the issue of\nimage-label inconsistency. Project page:\n\\url{https://github.com/morning12138/TdAttenMix}",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.15409v1",
    "published_date": "2025-01-26 05:32:37 UTC",
    "updated_date": "2025-01-26 05:32:37 UTC"
  },
  {
    "arxiv_id": "2501.15407v1",
    "title": "Turn That Frown Upside Down: FaceID Customization via Cross-Training Data",
    "authors": [
      "Shuhe Wang",
      "Xiaoya Li",
      "Xiaofei Sun",
      "Guoyin Wang",
      "Tianwei Zhang",
      "Jiwei Li",
      "Eduard Hovy"
    ],
    "abstract": "Existing face identity (FaceID) customization methods perform well but are\nlimited to generating identical faces as the input, while in real-world\napplications, users often desire images of the same person but with variations,\nsuch as different expressions (e.g., smiling, angry) or angles (e.g., side\nprofile). This limitation arises from the lack of datasets with controlled\ninput-output facial variations, restricting models' ability to learn effective\nmodifications.\n  To address this issue, we propose CrossFaceID, the first large-scale,\nhigh-quality, and publicly available dataset specifically designed to improve\nthe facial modification capabilities of FaceID customization models.\nSpecifically, CrossFaceID consists of 40,000 text-image pairs from\napproximately 2,000 persons, with each person represented by around 20 images\nshowcasing diverse facial attributes such as poses, expressions, angles, and\nadornments. During the training stage, a specific face of a person is used as\ninput, and the FaceID customization model is forced to generate another image\nof the same person but with altered facial features. This allows the FaceID\ncustomization model to acquire the ability to personalize and modify known\nfacial features during the inference stage. Experiments show that models\nfine-tuned on the CrossFaceID dataset retain its performance in preserving\nFaceID fidelity while significantly improving its face customization\ncapabilities.\n  To facilitate further advancements in the FaceID customization field, our\ncode, constructed datasets, and trained models are fully available to the\npublic.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.15407v1",
    "published_date": "2025-01-26 05:27:38 UTC",
    "updated_date": "2025-01-26 05:27:38 UTC"
  },
  {
    "arxiv_id": "2501.15405v2",
    "title": "Semantic Layered Embedding Diffusion in Large Language Models for Multi-Contextual Consistency",
    "authors": [
      "Irin Kabakum",
      "Thomas Montgomery",
      "Daniel Ravenwood",
      "Genevieve Harrington"
    ],
    "abstract": "The Semantic Layered Embedding Diffusion (SLED) mechanism redefines the\nrepresentation of hierarchical semantics within transformer-based\narchitectures, enabling enhanced contextual consistency across a wide array of\nlinguistic tasks. By introducing a multi-layered diffusion process grounded in\nspectral analysis, it achieves a complex balance between global and local\nsemantic coherence. Experimental results demonstrate significant improvements\nin perplexity and BLEU scores, emphasizing the mechanism's ability to adapt\neffectively across diverse domains, including multilingual and cross-domain\ntext generation. A rigorous mathematical framework underpins the embedding\ndiffusion process, incorporating weighted adjacency matrices, kernel-based\nrefinements, and dynamic layer-wise normalization. Error distribution analysis\nreveals that SLED addresses challenges in semantic alignment and coherence,\noutperforming baseline approaches across varied benchmarks. Scalability studies\nillustrate that its performance gains are maintained consistently across\ndifferent model sizes, reflecting a practical balance between computational\nefficiency and linguistic precision. The implementation also achieves energy\nefficiency, reducing resource consumption during training and inference phases\nwithout compromising accuracy. Qualitative case studies further validate its\nadaptability to extended narratives and context-intensive scenarios,\nhighlighting the mechanism's potential for real-world applications. SLED offers\na different perspective on embedding design and its implications for advancing\nlanguage modeling.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship",
    "pdf_url": "http://arxiv.org/pdf/2501.15405v2",
    "published_date": "2025-01-26 05:17:04 UTC",
    "updated_date": "2025-03-25 12:55:17 UTC"
  },
  {
    "arxiv_id": "2501.15404v1",
    "title": "A Neurosymbolic Framework for Geometric Reduction of Binary Forms",
    "authors": [
      "Ilias Kotsireas",
      "Tony Shaska"
    ],
    "abstract": "This paper compares Julia reduction and hyperbolic reduction with the aim of\nfinding equivalent binary forms with minimal coefficients. We demonstrate that\nhyperbolic reduction generally outperforms Julia reduction, particularly in the\ncases of sextics and decimics, though neither method guarantees achieving the\nminimal form. We further propose an additional shift and scaling to approximate\nthe minimal form more closely. Finally, we introduce a machine learning\nframework to identify optimal transformations that minimize the heights of\nbinary forms. This study provides new insights into the geometry and algebra of\nbinary forms and highlights the potential of AI in advancing symbolic\ncomputation and reduction techniques. The findings, supported by extensive\ncomputational experiments, lay the groundwork for hybrid approaches that\nintegrate traditional reduction methods with data-driven techniques.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "I.2.3"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.15404v1",
    "published_date": "2025-01-26 05:15:08 UTC",
    "updated_date": "2025-01-26 05:15:08 UTC"
  },
  {
    "arxiv_id": "2501.15393v1",
    "title": "Diffusion-based Hierarchical Negative Sampling for Multimodal Knowledge Graph Completion",
    "authors": [
      "Guanglin Niu",
      "Xiaowei Zhang"
    ],
    "abstract": "Multimodal Knowledge Graph Completion (MMKGC) aims to address the critical\nissue of missing knowledge in multimodal knowledge graphs (MMKGs) for their\nbetter applications. However, both the previous MMGKC and negative sampling\n(NS) approaches ignore the employment of multimodal information to generate\ndiverse and high-quality negative triples from various semantic levels and\nhardness levels, thereby limiting the effectiveness of training MMKGC models.\nThus, we propose a novel Diffusion-based Hierarchical Negative Sampling (DHNS)\nscheme tailored for MMKGC tasks, which tackles the challenge of generating\nhigh-quality negative triples by leveraging a Diffusion-based Hierarchical\nEmbedding Generation (DiffHEG) that progressively conditions on entities and\nrelations as well as multimodal semantics. Furthermore, we develop a Negative\nTriple-Adaptive Training (NTAT) strategy that dynamically adjusts training\nmargins associated with the hardness level of the synthesized negative triples,\nfacilitating a more robust and effective learning procedure to distinguish\nbetween positive and negative triples. Extensive experiments on three MMKGC\nbenchmark datasets demonstrate that our framework outperforms several\nstate-of-the-art MMKGC models and negative sampling techniques, illustrating\nthe effectiveness of our DHNS for training MMKGC models. The source codes and\ndatasets of this paper are available at https://github.com/ngl567/DHNS.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "I.2.7"
    ],
    "primary_category": "cs.AI",
    "comment": "The version of a full paper accepted to DASFAA 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.15393v1",
    "published_date": "2025-01-26 04:20:34 UTC",
    "updated_date": "2025-01-26 04:20:34 UTC"
  },
  {
    "arxiv_id": "2501.15392v3",
    "title": "Faster Configuration Performance Bug Testing with Neural Dual-level Prioritization",
    "authors": [
      "Youpeng Ma",
      "Tao Chen",
      "Ke Li"
    ],
    "abstract": "As software systems become more complex and configurable, more performance\nproblems tend to arise from the configuration designs. This has caused some\nconfiguration options to unexpectedly degrade performance which deviates from\ntheir original expectations designed by the developers. Such discrepancies,\nnamely configuration performance bugs (CPBugs), are devastating and can be\ndeeply hidden in the source code. Yet, efficiently testing CPBugs is difficult,\nnot only due to the test oracle is hard to set, but also because the\nconfiguration measurement is expensive and there are simply too many possible\nconfigurations to test. As such, existing testing tools suffer from lengthy\nruntime or have been ineffective in detecting CPBugs when the budget is\nlimited, compounded by inaccurate test oracle. In this paper, we seek to\nachieve significantly faster CPBug testing by neurally prioritizing the testing\nat both the configuration option and value range levels with automated oracle\nestimation. Our proposed tool, dubbed NDP, is a general framework that works\nwith different heuristic generators. The idea is to leverage two neural\nlanguage models: one to estimate the CPBug types that serve as the oracle\nwhile, more vitally, the other to infer the probabilities of an option being\nCPBug-related, based on which the options and the value ranges to be searched\ncan be prioritized. Experiments on several widely-used systems of different\nversions reveal that NDP can, in general, better predict CPBug type in 87%\ncases and find more CPBugs with up to 88.88x testing efficiency speedup over\nthe state-of-the-art tools.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "accepted by ICSE 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.15392v3",
    "published_date": "2025-01-26 04:19:43 UTC",
    "updated_date": "2025-04-15 10:25:34 UTC"
  },
  {
    "arxiv_id": "2501.15384v1",
    "title": "MetaOcc: Surround-View 4D Radar and Camera Fusion Framework for 3D Occupancy Prediction with Dual Training Strategies",
    "authors": [
      "Long Yang",
      "Lianqing Zheng",
      "Wenjin Ai",
      "Minghao Liu",
      "Sen Li",
      "Qunshu Lin",
      "Shengyu Yan",
      "Jie Bai",
      "Zhixiong Ma",
      "Xichan Zhu"
    ],
    "abstract": "3D occupancy prediction is crucial for autonomous driving perception. Fusion\nof 4D radar and camera provides a potential solution of robust occupancy\nprediction on serve weather with least cost. How to achieve effective\nmulti-modal feature fusion and reduce annotation costs remains significant\nchallenges. In this work, we propose MetaOcc, a novel multi-modal occupancy\nprediction framework that fuses surround-view cameras and 4D radar for\ncomprehensive environmental perception. We first design a height self-attention\nmodule for effective 3D feature extraction from sparse radar points. Then, a\nlocal-global fusion mechanism is proposed to adaptively capture modality\ncontributions while handling spatio-temporal misalignments. Temporal alignment\nand fusion module is employed to further aggregate historical feature.\nFurthermore, we develop a semi-supervised training procedure leveraging\nopen-set segmentor and geometric constraints for pseudo-label generation,\nenabling robust perception with limited annotations. Extensive experiments on\nOmniHD-Scenes dataset demonstrate that MetaOcc achieves state-of-the-art\nperformance, surpassing previous methods by significant margins. Notably, as\nthe first semi-supervised 4D radar and camera fusion-based occupancy prediction\napproach, MetaOcc maintains 92.5% of the fully-supervised performance while\nusing only 50% of ground truth annotations, establishing a new benchmark for\nmulti-modal 3D occupancy prediction. Code and data are available at\nhttps://github.com/LucasYang567/MetaOcc.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.15384v1",
    "published_date": "2025-01-26 03:51:56 UTC",
    "updated_date": "2025-01-26 03:51:56 UTC"
  },
  {
    "arxiv_id": "2501.15379v1",
    "title": "Zero-Shot Interactive Text-to-Image Retrieval via Diffusion-Augmented Representations",
    "authors": [
      "Zijun Long",
      "Kangheng Liang",
      "Gerardo Aragon-Camarasa",
      "Richard Mccreadie",
      "Paul Henderson"
    ],
    "abstract": "Interactive Text-to-Image Retrieval (I-TIR) has emerged as a transformative\nuser-interactive tool for applications in domains such as e-commerce and\neducation. Yet, current methodologies predominantly depend on finetuned\nMultimodal Large Language Models (MLLMs), which face two critical limitations:\n(1) Finetuning imposes prohibitive computational overhead and long-term\nmaintenance costs. (2) Finetuning narrows the pretrained knowledge distribution\nof MLLMs, reducing their adaptability to novel scenarios. These issues are\nexacerbated by the inherently dynamic nature of real-world I-TIR systems, where\nqueries and image databases evolve in complexity and diversity, often deviating\nfrom static training distributions. To overcome these constraints, we propose\nDiffusion Augmented Retrieval (DAR), a paradigm-shifting framework that\nbypasses MLLM finetuning entirely. DAR synergizes Large Language Model\n(LLM)-guided query refinement with Diffusion Model (DM)-based visual synthesis\nto create contextually enriched intermediate representations. This\ndual-modality approach deciphers nuanced user intent more holistically,\nenabling precise alignment between textual queries and visually relevant\nimages. Rigorous evaluations across four benchmarks reveal DAR's dual\nstrengths: (1) Matches state-of-the-art finetuned I-TIR models on\nstraightforward queries without task-specific training. (2) Scalable\nGeneralization: Surpasses finetuned baselines by 7.61% in Hits@10 (top-10\naccuracy) under multi-turn conversational complexity, demonstrating robustness\nto intricate, distributionally shifted interactions. By eliminating finetuning\ndependencies and leveraging generative-augmented representations, DAR\nestablishes a new trajectory for efficient, adaptive, and scalable cross-modal\nretrieval systems.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.15379v1",
    "published_date": "2025-01-26 03:29:18 UTC",
    "updated_date": "2025-01-26 03:29:18 UTC"
  },
  {
    "arxiv_id": "2501.15378v1",
    "title": "How to Mitigate Information Loss in Knowledge Graphs for GraphRAG: Leveraging Triple Context Restoration and Query-Driven Feedback",
    "authors": [
      "Manzong Huang",
      "Chenyang Bu",
      "Yi He",
      "Xindong Wu"
    ],
    "abstract": "Knowledge Graph (KG)-augmented Large Language Models (LLMs) have recently\npropelled significant advances in complex reasoning tasks, thanks to their\nbroad domain knowledge and contextual awareness. Unfortunately, current methods\noften assume KGs to be complete, which is impractical given the inherent\nlimitations of KG construction and the potential loss of contextual cues when\nconverting unstructured text into entity-relation triples. In response, this\npaper proposes the Triple Context Restoration and Query-driven Feedback\n(TCR-QF) framework, which reconstructs the textual context underlying each\ntriple to mitigate information loss, while dynamically refining the KG\nstructure by iteratively incorporating query-relevant missing knowledge.\nExperiments on five benchmark question-answering datasets substantiate the\neffectiveness of TCR-QF in KG and LLM integration, where itachieves a 29.1%\nimprovement in Exact Match and a 15.5% improvement in F1 over its\nstate-of-the-art GraphRAG competitors.",
    "categories": [
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.15378v1",
    "published_date": "2025-01-26 03:27:11 UTC",
    "updated_date": "2025-01-26 03:27:11 UTC"
  },
  {
    "arxiv_id": "2501.15374v1",
    "title": "Evaluating the Effectiveness of XAI Techniques for Encoder-Based Language Models",
    "authors": [
      "Melkamu Abay Mersha",
      "Mesay Gemeda Yigezu",
      "Jugal Kalita"
    ],
    "abstract": "The black-box nature of large language models (LLMs) necessitates the\ndevelopment of eXplainable AI (XAI) techniques for transparency and\ntrustworthiness. However, evaluating these techniques remains a challenge. This\nstudy presents a general evaluation framework using four key metrics:\nHuman-reasoning Agreement (HA), Robustness, Consistency, and Contrastivity. We\nassess the effectiveness of six explainability techniques from five different\nXAI categories model simplification (LIME), perturbation-based methods (SHAP),\ngradient-based approaches (InputXGradient, Grad-CAM), Layer-wise Relevance\nPropagation (LRP), and attention mechanisms-based explainability methods\n(Attention Mechanism Visualization, AMV) across five encoder-based language\nmodels: TinyBERT, BERTbase, BERTlarge, XLM-R large, and DeBERTa-xlarge, using\nthe IMDB Movie Reviews and Tweet Sentiment Extraction (TSE) datasets. Our\nfindings show that the model simplification-based XAI method (LIME)\nconsistently outperforms across multiple metrics and models, significantly\nexcelling in HA with a score of 0.9685 on DeBERTa-xlarge, robustness, and\nconsistency as the complexity of large language models increases. AMV\ndemonstrates the best Robustness, with scores as low as 0.0020. It also excels\nin Consistency, achieving near-perfect scores of 0.9999 across all models.\nRegarding Contrastivity, LRP performs the best, particularly on more complex\nmodels, with scores up to 0.9371.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.15374v1",
    "published_date": "2025-01-26 03:08:34 UTC",
    "updated_date": "2025-01-26 03:08:34 UTC"
  },
  {
    "arxiv_id": "2501.15373v1",
    "title": "Learning-Enhanced Safeguard Control for High-Relative-Degree Systems: Robust Optimization under Disturbances and Faults",
    "authors": [
      "Xinyang Wang",
      "Hongwei Zhang",
      "Shimin Wang",
      "Wei Xiao",
      "Martin Guay"
    ],
    "abstract": "Merely pursuing performance may adversely affect the safety, while a\nconservative policy for safe exploration will degrade the performance. How to\nbalance the safety and performance in learning-based control problems is an\ninteresting yet challenging issue. This paper aims to enhance system\nperformance with safety guarantee in solving the reinforcement learning\n(RL)-based optimal control problems of nonlinear systems subject to\nhigh-relative-degree state constraints and unknown time-varying\ndisturbance/actuator faults. First, to combine control barrier functions (CBFs)\nwith RL, a new type of CBFs, termed high-order reciprocal control barrier\nfunction (HO-RCBF) is proposed to deal with high-relative-degree constraints\nduring the learning process. Then, the concept of gradient similarity is\nproposed to quantify the relationship between the gradient of safety and the\ngradient of performance. Finally, gradient manipulation and adaptive mechanisms\nare introduced in the safe RL framework to enhance the performance with a\nsafety guarantee. Two simulation examples illustrate that the proposed safe RL\nframework can address high-relative-degree constraint, enhance safety\nrobustness and improve system performance.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "math.OC",
      "nlin.AO"
    ],
    "primary_category": "eess.SY",
    "comment": "16 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.15373v1",
    "published_date": "2025-01-26 03:03:02 UTC",
    "updated_date": "2025-01-26 03:03:02 UTC"
  },
  {
    "arxiv_id": "2501.15370v1",
    "title": "Scaling Large Vision-Language Models for Enhanced Multimodal Comprehension In Biomedical Image Analysis",
    "authors": [
      "Robinson Umeike",
      "Neil Getty",
      "Fangfang Xia",
      "Rick Stevens"
    ],
    "abstract": "Large language models (LLMs) have demonstrated immense capabilities in\nunderstanding textual data and are increasingly being adopted to help\nresearchers accelerate scientific discovery through knowledge extraction\n(information retrieval), knowledge distillation (summarizing key findings and\nmethodologies into concise forms), and knowledge synthesis (aggregating\ninformation from multiple scientific sources to address complex queries,\ngenerate hypothesis and formulate experimental plans). However, scientific data\noften exists in both visual and textual modalities. Vision language models\n(VLMs) address this by incorporating a pretrained vision backbone for\nprocessing images and a cross-modal projector that adapts image tokens into the\nLLM dimensional space, thereby providing richer multimodal comprehension.\nNevertheless, off-the-shelf VLMs show limited capabilities in handling\ndomain-specific data and are prone to hallucinations. We developed intelligent\nassistants finetuned from LLaVA models to enhance multimodal understanding in\nlow-dose radiation therapy (LDRT)-a benign approach used in the treatment of\ncancer-related illnesses. Using multilingual data from 42,673 articles, we\ndevise complex reasoning and detailed description tasks for visual question\nanswering (VQA) benchmarks. Our assistants, trained on 50,882 image-text pairs,\ndemonstrate superior performance over base models as evaluated using\nLLM-as-a-judge approach, particularly in reducing hallucination and improving\ndomain-specific comprehension.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.2.7; I.4.9"
    ],
    "primary_category": "cs.CV",
    "comment": "4 Pages, 4 Figures, 1 Table",
    "pdf_url": "http://arxiv.org/pdf/2501.15370v1",
    "published_date": "2025-01-26 02:48:01 UTC",
    "updated_date": "2025-01-26 02:48:01 UTC"
  },
  {
    "arxiv_id": "2501.15369v2",
    "title": "iFormer: Integrating ConvNet and Transformer for Mobile Application",
    "authors": [
      "Chuanyang Zheng"
    ],
    "abstract": "We present a new family of mobile hybrid vision networks, called iFormer,\nwith a focus on optimizing latency and accuracy on mobile applications. iFormer\neffectively integrates the fast local representation capacity of convolution\nwith the efficient global modeling ability of self-attention. The local\ninteractions are derived from transforming a standard convolutional network,\n\\textit{i.e.}, ConvNeXt, to design a more lightweight mobile network. Our newly\nintroduced mobile modulation attention removes memory-intensive operations in\nMHA and employs an efficient modulation mechanism to boost dynamic global\nrepresentational capacity. We conduct comprehensive experiments demonstrating\nthat iFormer outperforms existing lightweight networks across various tasks.\nNotably, iFormer achieves an impressive Top-1 accuracy of 80.4\\% on ImageNet-1k\nwith a latency of only 1.10 ms on an iPhone 13, surpassing the recently\nproposed MobileNetV4 under similar latency constraints. Additionally, our\nmethod shows significant improvements in downstream tasks, including COCO\nobject detection, instance segmentation, and ADE20k semantic segmentation,\nwhile still maintaining low latency on mobile devices for high-resolution\ninputs in these scenarios.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to ICLR 2025. Code:\n  https://github.com/ChuanyangZheng/iFormer",
    "pdf_url": "http://arxiv.org/pdf/2501.15369v2",
    "published_date": "2025-01-26 02:34:58 UTC",
    "updated_date": "2025-02-17 15:09:31 UTC"
  },
  {
    "arxiv_id": "2504.13183v1",
    "title": "Factors That Influence the Adoption of AI-enabled Conversational Agents (AICAs) as an Augmenting Therapeutic Tool by Frontline Healthcare Workers: From Technology Acceptance Model 3 (TAM3) Lens -- A Systematic Mapping Review",
    "authors": [
      "Rawan AlMakinah"
    ],
    "abstract": "Artificial intelligent (AI) conversational agents hold a promising future in\nthe field of mental health, especially in helping marginalized communities that\nlack access to mental health support services. It is tempting to have a 24/7\nmental health companion that can be accessed anywhere using mobile phones to\nprovide therapist-like advice. Yet, caution should be taken, and studies around\ntheir feasibility need to be surveyed. Before adopting such a rapidly changing\ntechnology, studies on its feasibility should be explored, summarized, and\nsynthesized to gain a solid understanding of the status quo and to enable us to\nbuild a framework that can guide us throughout the development and deployment\nprocesses. Different perspectives must be considered when investigating the\nfeasibility of AI conversational agents, including the mental healthcare\nprofessional perspective. The literature can provide insights into their\nperspectives in terms of opportunities, concerns, and implications. Mental\nhealth professionals, the subject-matter experts in this field, have their\npoints of view that should be understood and considered. This systematic\nliterature review will explore mental health practitioners' attitudes toward AI\nconversational agents and the factors that affect their adoption and\nrecommendation of the technology to augment their services and treatments. The\nTAM3 Framework will be the lens through which this systematic literature review\nwill be conducted.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.13183v1",
    "published_date": "2025-01-26 02:31:27 UTC",
    "updated_date": "2025-01-26 02:31:27 UTC"
  },
  {
    "arxiv_id": "2502.15715v1",
    "title": "Regulating Multifunctionality",
    "authors": [
      "Cary Coglianese",
      "Colton R. Crum"
    ],
    "abstract": "Foundation models and generative artificial intelligence (AI) exacerbate a\ncore regulatory challenge associated with AI: its heterogeneity. By their very\nnature, foundation models and generative AI can perform multiple functions for\ntheir users, thus presenting a vast array of different risks. This\nmultifunctionality means that prescriptive, one-size-fits-all regulation will\nnot be a viable option. Even performance standards and ex post liability -\nregulatory approaches that usually afford flexibility - are unlikely to be\nstrong candidates for responding to multifunctional AI's risks, given\nchallenges in monitoring and enforcement. Regulators will do well instead to\npromote proactive risk management on the part of developers and users by using\nmanagement-based regulation, an approach that has proven effective in other\ncontexts of heterogeneity. Regulators will also need to maintain ongoing\nvigilance and agility. More than in other contexts, regulators of\nmultifunctional AI will need sufficient resources, top human talent and\nleadership, and organizational cultures committed to regulatory excellence.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "Forthcoming in Philipp Hacker, Andreas Engel, Sarah Hammer and Brent\n  Mittelstadt (eds), The Oxford Handbook on the Foundations and Regulation of\n  Generative AI (Oxford University Press)",
    "pdf_url": "http://arxiv.org/pdf/2502.15715v1",
    "published_date": "2025-01-26 00:50:27 UTC",
    "updated_date": "2025-01-26 00:50:27 UTC"
  },
  {
    "arxiv_id": "2501.15355v1",
    "title": "Large Language Models as Theory of Mind Aware Generative Agents with Counterfactual Reflection",
    "authors": [
      "Bo Yang",
      "Jiaxian Guo",
      "Yusuke Iwasawa",
      "Yutaka Matsuo"
    ],
    "abstract": "Recent studies have increasingly demonstrated that large language models\n(LLMs) possess significant theory of mind (ToM) capabilities, showing the\npotential for simulating the tracking of mental states in generative agents. In\nthis study, we propose a novel paradigm called ToM-agent, designed to empower\nLLMs-based generative agents to simulate ToM in open-domain conversational\ninteractions. ToM-agent disentangles the confidence from mental states,\nfacilitating the emulation of an agent's perception of its counterpart's mental\nstates, such as beliefs, desires, and intentions (BDIs). Using past\nconversation history and verbal reflections, ToM-Agent can dynamically adjust\ncounterparts' inferred BDIs, along with related confidence levels. We further\nput forth a counterfactual intervention method that reflects on the gap between\nthe predicted responses of counterparts and their real utterances, thereby\nenhancing the efficiency of reflection. Leveraging empathetic and persuasion\ndialogue datasets, we assess the advantages of implementing the ToM-agent with\ndownstream tasks, as well as its performance in both the first-order and the\n\\textit{second-order} ToM. Our findings indicate that the ToM-agent can grasp\nthe underlying reasons for their counterpart's behaviors beyond mere\nsemantic-emotional supporting or decision-making based on common sense,\nproviding new insights for studying large-scale LLMs-based simulation of human\nsocial behaviors.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.15355v1",
    "published_date": "2025-01-26 00:32:38 UTC",
    "updated_date": "2025-01-26 00:32:38 UTC"
  }
]
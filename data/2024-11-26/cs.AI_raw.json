[
  {
    "arxiv_id": "2411.17945v2",
    "title": "MARVEL-40M+: Multi-Level Visual Elaboration for High-Fidelity Text-to-3D Content Creation",
    "authors": [
      "Sankalp Sinha",
      "Mohammad Sadil Khan",
      "Muhammad Usama",
      "Shino Sam",
      "Didier Stricker",
      "Sk Aziz Ali",
      "Muhammad Zeshan Afzal"
    ],
    "abstract": "Generating high-fidelity 3D content from text prompts remains a significant\nchallenge in computer vision due to the limited size, diversity, and annotation\ndepth of the existing datasets. To address this, we introduce MARVEL-40M+, an\nextensive dataset with 40 million text annotations for over 8.9 million 3D\nassets aggregated from seven major 3D datasets. Our contribution is a novel\nmulti-stage annotation pipeline that integrates open-source pretrained\nmulti-view VLMs and LLMs to automatically produce multi-level descriptions,\nranging from detailed (150-200 words) to concise semantic tags (10-20 words).\nThis structure supports both fine-grained 3D reconstruction and rapid\nprototyping. Furthermore, we incorporate human metadata from source datasets\ninto our annotation pipeline to add domain-specific information in our\nannotation and reduce VLM hallucinations. Additionally, we develop MARVEL-FX3D,\na two-stage text-to-3D pipeline. We fine-tune Stable Diffusion with our\nannotations and use a pretrained image-to-3D network to generate 3D textured\nmeshes within 15s. Extensive evaluations show that MARVEL-40M+ significantly\noutperforms existing datasets in annotation quality and linguistic diversity,\nachieving win rates of 72.41% by GPT-4 and 73.40% by human evaluators. Project\npage is available at https://sankalpsinha-cmos.github.io/MARVEL/.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17945v2",
    "published_date": "2024-11-26 23:39:43 UTC",
    "updated_date": "2025-03-26 11:06:10 UTC"
  },
  {
    "arxiv_id": "2411.17943v1",
    "title": "Evaluating Generative AI-Enhanced Content: A Conceptual Framework Using Qualitative, Quantitative, and Mixed-Methods Approaches",
    "authors": [
      "Saman Sarraf"
    ],
    "abstract": "Generative AI (GenAI) has revolutionized content generation, offering\ntransformative capabilities for improving language coherence, readability, and\noverall quality. This manuscript explores the application of qualitative,\nquantitative, and mixed-methods research approaches to evaluate the performance\nof GenAI models in enhancing scientific writing. Using a hypothetical use case\ninvolving a collaborative medical imaging manuscript, we demonstrate how each\nmethod provides unique insights into the impact of GenAI. Qualitative methods\ngather in-depth feedback from expert reviewers, analyzing their responses using\nthematic analysis tools to capture nuanced improvements and identify\nlimitations. Quantitative approaches employ automated metrics such as BLEU,\nROUGE, and readability scores, as well as user surveys, to objectively measure\nimprovements in coherence, fluency, and structure. Mixed-methods research\nintegrates these strengths, combining statistical evaluations with detailed\nqualitative insights to provide a comprehensive assessment. These research\nmethods enable quantifying improvement levels in GenAI-generated content,\naddressing critical aspects of linguistic quality and technical accuracy. They\nalso offer a robust framework for benchmarking GenAI tools against traditional\nediting processes, ensuring the reliability and effectiveness of these\ntechnologies. By leveraging these methodologies, researchers can evaluate the\nperformance boost driven by GenAI, refine its applications, and guide its\nresponsible adoption in high-stakes domains like healthcare and scientific\nresearch. This work underscores the importance of rigorous evaluation\nframeworks for advancing trust and innovation in GenAI.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17943v1",
    "published_date": "2024-11-26 23:34:07 UTC",
    "updated_date": "2024-11-26 23:34:07 UTC"
  },
  {
    "arxiv_id": "2411.17937v1",
    "title": "Spatio-temporal Causal Learning for Streamflow Forecasting",
    "authors": [
      "Shu Wan",
      "Reepal Shah",
      "Qi Deng",
      "John Sabo",
      "Huan Liu",
      "K. Sel√ßuk"
    ],
    "abstract": "Streamflow plays an essential role in the sustainable planning and management\nof national water resources. Traditional hydrologic modeling approaches\nsimulate streamflow by establishing connections across multiple physical\nprocesses, such as rainfall and runoff. These data, inherently connected both\nspatially and temporally, possess intrinsic causal relations that can be\nleveraged for robust and accurate forecasting. Recently, spatio-temporal graph\nneural networks (STGNNs) have been adopted, excelling in various domains, such\nas urban traffic management, weather forecasting, and pandemic control, and\nthey also promise advances in streamflow management. However, learning causal\nrelationships directly from vast observational data is theoretically and\ncomputationally challenging. In this study, we employ a river flow graph as\nprior knowledge to facilitate the learning of the causal structure and then use\nthe learned causal graph to predict streamflow at targeted sites. The proposed\nmodel, Causal Streamflow Forecasting (CSF) is tested in a real-world study in\nthe Brazos River basin in Texas. Our results demonstrate that our method\noutperforms regular spatio-temporal graph neural networks and achieves higher\ncomputational efficiency compared to traditional simulation methods. By\neffectively integrating river flow graphs with STGNNs, this research offers a\nnovel approach to streamflow prediction, showcasing the potential of combining\nadvanced neural network techniques with domain-specific knowledge for enhanced\nperformance in hydrologic modeling.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "To be published at IEEE Big Data 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.17937v1",
    "published_date": "2024-11-26 23:19:56 UTC",
    "updated_date": "2024-11-26 23:19:56 UTC"
  },
  {
    "arxiv_id": "2411.17932v1",
    "title": "Neural Networks Use Distance Metrics",
    "authors": [
      "Alan Oursland"
    ],
    "abstract": "We present empirical evidence that neural networks with ReLU and Absolute\nValue activations learn distance-based representations. We independently\nmanipulate both distance and intensity properties of internal activations in\ntrained models, finding that both architectures are highly sensitive to small\ndistance-based perturbations while maintaining robust performance under large\nintensity-based perturbations. These findings challenge the prevailing\nintensity-based interpretation of neural network activations and offer new\ninsights into their learning and decision-making processes.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages excluding references and appendix. 12 pages total. 3 figures.\n  The code for the experiments in this paper is available at\n  https://github.com/alanoursland/neural_networks_use_distance_metrics",
    "pdf_url": "http://arxiv.org/pdf/2411.17932v1",
    "published_date": "2024-11-26 23:04:56 UTC",
    "updated_date": "2024-11-26 23:04:56 UTC"
  },
  {
    "arxiv_id": "2411.17931v3",
    "title": "Combining Threat Intelligence with IoT Scanning to Predict Cyber Attack",
    "authors": [
      "Jubin Abhishek Soni"
    ],
    "abstract": "While the Web has become a global platform for communication; malicious\nactors, including hackers and hacktivist groups, often disseminate ideological\ncontent and coordinate activities through the \"Dark Web\" an obscure counterpart\nof the conventional web. Presently, challenges such as information overload and\nthe fragmented nature of cyber threat data impede comprehensive profiling of\nthese actors, thereby limiting the efficacy of predictive analyses of their\nonline activities. Concurrently, the proliferation of internet-connected\ndevices has surpassed the global human population, with this disparity\nprojected to widen as the Internet of Things (IoT) expands. Technical\ncommunities are actively advancing IoT-related research to address its growing\nsocietal integration. This paper proposes a novel predictive threat\nintelligence framework designed to systematically collect, analyze, and\nvisualize Dark Web data to identify malicious websites and correlate this\ninformation with potential IoT vulnerabilities. The methodology integrates\nautomated data harvesting, analytical techniques, and visual mapping tools,\nwhile also examining vulnerabilities in IoT devices to assess exploitability.\nBy bridging gaps in cybersecurity research, this study aims to enhance\npredictive threat modeling and inform policy development, thereby contributing\nto intelligence research initiatives focused on mitigating cyber risks in an\nincreasingly interconnected digital ecosystem.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CY",
      "cs.NI"
    ],
    "primary_category": "cs.CR",
    "comment": "17 pages, 6 figures, 2 tables. This manuscript has been submitted to\n  Springer for review (Manuscript ID: PDSE-D-24-00163) and is under\n  consideration. It has not yet been peer-reviewed or published. Researchers\n  are welcome to read and build upon this work; please cite it appropriately.\n  For questions or clarifications, feel free to contact me",
    "pdf_url": "http://arxiv.org/pdf/2411.17931v3",
    "published_date": "2024-11-26 23:00:51 UTC",
    "updated_date": "2025-04-07 06:33:58 UTC"
  },
  {
    "arxiv_id": "2411.18648v2",
    "title": "MADE: Graph Backdoor Defense with Masked Unlearning",
    "authors": [
      "Xiao Lin",
      "Mingjie Li",
      "Yisen Wang"
    ],
    "abstract": "Graph Neural Networks (GNNs) have garnered significant attention from\nresearchers due to their outstanding performance in handling graph-related\ntasks, such as social network analysis, protein design, and so on. Despite\ntheir widespread application, recent research has demonstrated that GNNs are\nvulnerable to backdoor attacks, implemented by injecting triggers into the\ntraining datasets. Trained on the poisoned data, GNNs will predict target\nlabels when attaching trigger patterns to inputs. This vulnerability poses\nsignificant security risks for applications of GNNs in sensitive domains, such\nas drug discovery. While there has been extensive research into backdoor\ndefenses for images, strategies to safeguard GNNs against such attacks remain\nunderdeveloped. Furthermore, we point out that conventional backdoor defense\nmethods designed for images cannot work well when directly implemented on graph\ndata. In this paper, we first analyze the key difference between image backdoor\nand graph backdoor attacks. Then we tackle the graph defense problem by\npresenting a novel approach called MADE, which devises an adversarial mask\ngeneration mechanism that selectively preserves clean sub-graphs and further\nleverages masks on edge weights to eliminate the influence of triggers\neffectively. Extensive experiments across various graph classification tasks\ndemonstrate the effectiveness of MADE in significantly reducing the attack\nsuccess rate (ASR) while maintaining a high classification accuracy.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "E.1"
    ],
    "primary_category": "cs.CR",
    "comment": "15 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.18648v2",
    "published_date": "2024-11-26 22:50:53 UTC",
    "updated_date": "2024-12-31 02:11:04 UTC"
  },
  {
    "arxiv_id": "2412.08654v1",
    "title": "A Behavior Tree-inspired programming language for autonomous agents",
    "authors": [
      "Oliver Biggar",
      "Iman Shames"
    ],
    "abstract": "We propose a design for a functional programming language for autonomous\nagents, built off the ideas and motivations of Behavior Trees (BTs). BTs are a\npopular model for designing agents behavior in robotics and AI. However, as\ntheir growth has increased dramatically, the simple model of BTs has come to be\nlimiting. There is a growing push to increase the functionality of BTs, with\nthe end goal of BTs evolving into a programming language in their own right,\ncentred around the defining BT properties of modularity and reactiveness.\n  In this paper, we examine how the BT model must be extended in order to grow\ninto such a language. We identify some fundamental problems which must be\nsolved: implementing `reactive' selection, 'monitoring' safety-critical\nconditions, and passing data between actions. We provide a variety of small\nexamples which demonstrate that these problems are complex, and that current BT\napproaches do not handle them in a manner consistent with modularity. We\ninstead provide a simple set of modular programming primitives for handling\nthese use cases, and show how they can be combined to build complex programs.\nWe present a full specification for our BT-inspired language, and give an\nimplementation in the functional programming language Haskell. Finally, we\ndemonstrate our language by translating a large and complex BT into a simple,\nunambiguous program.",
    "categories": [
      "cs.PL",
      "cs.AI",
      "cs.RO",
      "cs.SE"
    ],
    "primary_category": "cs.PL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.08654v1",
    "published_date": "2024-11-26 22:47:06 UTC",
    "updated_date": "2024-11-26 22:47:06 UTC"
  },
  {
    "arxiv_id": "2411.17924v1",
    "title": "AI2T: Building Trustable AI Tutors by Interactively Teaching a Self-Aware Learning Agent",
    "authors": [
      "Daniel Weitekamp",
      "Erik Harpstead",
      "Kenneth Koedinger"
    ],
    "abstract": "AI2T is an interactively teachable AI for authoring intelligent tutoring\nsystems (ITSs). Authors tutor AI2T by providing a few step-by-step solutions\nand then grading AI2T's own problem-solving attempts. From just 20-30 minutes\nof interactive training, AI2T can induce robust rules for step-by-step solution\ntracking (i.e., model-tracing). As AI2T learns it can accurately estimate its\ncertainty of performing correctly on unseen problem steps using STAND: a\nself-aware precondition learning algorithm that outperforms state-of-the-art\nmethods like XGBoost. Our user study shows that authors can use STAND's\ncertainty heuristic to estimate when AI2T has been trained on enough diverse\nproblems to induce correct and complete model-tracing programs. AI2T-induced\nprograms are more reliable than hallucination-prone LLMs and prior\nauthoring-by-tutoring approaches. With its self-aware induction of hierarchical\nrules, AI2T offers a path toward trustable data-efficient authoring-by-tutoring\nfor complex ITSs that normally require as many as 200-300 hours of programming\nper hour of instruction.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG",
      "I.2.6; I.2.2"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17924v1",
    "published_date": "2024-11-26 22:39:11 UTC",
    "updated_date": "2024-11-26 22:39:11 UTC"
  },
  {
    "arxiv_id": "2411.17914v1",
    "title": "Enhancing Project Performance Forecasting using Machine Learning Techniques",
    "authors": [
      "Soheila Sadeghi"
    ],
    "abstract": "Accurate forecasting of project performance metrics is crucial for\nsuccessfully managing and delivering urban road reconstruction projects.\nTraditional methods often rely on static baseline plans and fail to consider\nthe dynamic nature of project progress and external factors. This research\nproposes a machine learning-based approach to forecast project performance\nmetrics, such as cost variance and earned value, for each Work Breakdown\nStructure (WBS) category in an urban road reconstruction project. The proposed\nmodel utilizes time series forecasting techniques, including Autoregressive\nIntegrated Moving Average (ARIMA) and Long Short-Term Memory (LSTM) networks,\nto predict future performance based on historical data and project progress.\nThe model also incorporates external factors, such as weather patterns and\nresource availability, as features to enhance the accuracy of forecasts. By\napplying the predictive power of machine learning, the performance forecasting\nmodel enables proactive identification of potential deviations from the\nbaseline plan, which allows project managers to take timely corrective actions.\nThe research aims to validate the effectiveness of the proposed approach using\na case study of an urban road reconstruction project, comparing the model's\nforecasts with actual project performance data. The findings of this research\ncontribute to the advancement of project management practices in the\nconstruction industry, offering a data-driven solution for improving project\nperformance monitoring and control.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.AP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17914v1",
    "published_date": "2024-11-26 22:09:55 UTC",
    "updated_date": "2024-11-26 22:09:55 UTC"
  },
  {
    "arxiv_id": "2411.17912v2",
    "title": "Can LLMs plan paths in the real world?",
    "authors": [
      "Wanyi Chen",
      "Meng-Wen Su",
      "Nafisa Mehjabin",
      "Mary L. Cummings"
    ],
    "abstract": "As large language models (LLMs) increasingly integrate into vehicle\nnavigation systems, understanding their path-planning capability is crucial. We\ntested three LLMs through six real-world path-planning scenarios in various\nsettings and with various difficulties. Our experiments showed that all LLMs\nmade numerous errors in all scenarios, revealing that they are unreliable path\nplanners. We suggest that future work focus on implementing mechanisms for\nreality checks, enhancing model transparency, and developing smaller models.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17912v2",
    "published_date": "2024-11-26 22:06:39 UTC",
    "updated_date": "2024-12-02 02:42:05 UTC"
  },
  {
    "arxiv_id": "2411.17897v1",
    "title": "Automating grapevine LAI features estimation with UAV imagery and machine learning",
    "authors": [
      "Muhammad Waseem Akram",
      "Marco Vannucci",
      "Giorgio Buttazzo",
      "Valentina Colla",
      "Stefano Roccella",
      "Andrea Vannini",
      "Giovanni Caruso",
      "Simone Nesi",
      "Alessandra Francini",
      "Luca Sebastiani"
    ],
    "abstract": "The leaf area index determines crop health and growth. Traditional methods\nfor calculating it are time-consuming, destructive, costly, and limited to a\nscale. In this study, we automate the index estimation method using drone image\ndata of grapevine plants and a machine learning model. Traditional feature\nextraction and deep learning methods are used to obtain helpful information\nfrom the data and enhance the performance of the different machine learning\nmodels employed for the leaf area index prediction. The results showed that\ndeep learning based feature extraction is more effective than traditional\nmethods. The new approach is a significant improvement over old methods,\noffering a faster, non-destructive, and cost-effective leaf area index\ncalculation, which enhances precision agriculture practices.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.ET",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted in 2024 IEEE INTERNATIONAL WORKSHOP ON Metrology for\n  Agriculture and Forestry",
    "pdf_url": "http://arxiv.org/pdf/2411.17897v1",
    "published_date": "2024-11-26 21:24:27 UTC",
    "updated_date": "2024-11-26 21:24:27 UTC"
  },
  {
    "arxiv_id": "2411.17891v1",
    "title": "HOPPR Medical-Grade Platform for Medical Imaging AI",
    "authors": [
      "Kalina P. Slavkova",
      "Melanie Traughber",
      "Oliver Chen",
      "Robert Bakos",
      "Shayna Goldstein",
      "Dan Harms",
      "Bradley J. Erickson",
      "Khan M. Siddiqui"
    ],
    "abstract": "Technological advances in artificial intelligence (AI) have enabled the\ndevelopment of large vision language models (LVLMs) that are trained on\nmillions of paired image and text samples. Subsequent research efforts have\ndemonstrated great potential of LVLMs to achieve high performance in medical\nimaging use cases (e.g., radiology report generation), but there remain\nbarriers that hinder the ability to deploy these solutions broadly. These\ninclude the cost of extensive computational requirements for developing large\nscale models, expertise in the development of sophisticated AI models, and the\ndifficulty in accessing substantially large, high-quality datasets that\nadequately represent the population in which the LVLM solution is to be\ndeployed. The HOPPR Medical-Grade Platform addresses these barriers by\nproviding powerful computational infrastructure, a suite of foundation models\non top of which developers can fine-tune for their specific use cases, and a\nrobust quality management system that sets a standard for evaluating fine-tuned\nmodels for deployment in clinical settings. The HOPPR Platform has access to\nmillions of imaging studies and text reports sourced from hundreds of imaging\ncenters from diverse populations to pretrain foundation models and enable use\ncase-specific cohorts for fine-tuning. All data are deidentified and securely\nstored for HIPAA compliance. Additionally, developers can securely host models\non the HOPPR platform and access them via an API to make inferences using these\nmodels within established clinical workflows. With the Medical-Grade Platform,\nHOPPR's mission is to expedite the deployment of LVLM solutions for medical\nimaging and ultimately optimize radiologist's workflows and meet the growing\ndemands of the field.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "6 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.17891v1",
    "published_date": "2024-11-26 21:21:45 UTC",
    "updated_date": "2024-11-26 21:21:45 UTC"
  },
  {
    "arxiv_id": "2411.17863v1",
    "title": "LongKey: Keyphrase Extraction for Long Documents",
    "authors": [
      "Jeovane Honorio Alves",
      "Radu State",
      "Cinthia Obladen de Almendra Freitas",
      "Jean Paul Barddal"
    ],
    "abstract": "In an era of information overload, manually annotating the vast and growing\ncorpus of documents and scholarly papers is increasingly impractical. Automated\nkeyphrase extraction addresses this challenge by identifying representative\nterms within texts. However, most existing methods focus on short documents (up\nto 512 tokens), leaving a gap in processing long-context documents. In this\npaper, we introduce LongKey, a novel framework for extracting keyphrases from\nlengthy documents, which uses an encoder-based language model to capture\nextended text intricacies. LongKey uses a max-pooling embedder to enhance\nkeyphrase candidate representation. Validated on the comprehensive LDKP\ndatasets and six diverse, unseen datasets, LongKey consistently outperforms\nexisting unsupervised and language model-based keyphrase extraction methods.\nOur findings demonstrate LongKey's versatility and superior performance,\nmarking an advancement in keyphrase extraction for varied text lengths and\ndomains.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted for presentation at the 2024 IEEE International Conference\n  on Big Data (IEEE BigData 2024). Code available at\n  https://github.com/jeohalves/longkey",
    "pdf_url": "http://arxiv.org/pdf/2411.17863v1",
    "published_date": "2024-11-26 20:26:47 UTC",
    "updated_date": "2024-11-26 20:26:47 UTC"
  },
  {
    "arxiv_id": "2411.17861v3",
    "title": "Accelerating Proximal Policy Optimization Learning Using Task Prediction for Solving Environments with Delayed Rewards",
    "authors": [
      "Ahmad Ahmad",
      "Mehdi Kermanshah",
      "Kevin Leahy",
      "Zachary Serlin",
      "Ho Chit Siu",
      "Makai Mann",
      "Cristian-Ioan Vasile",
      "Roberto Tron",
      "Calin Belta"
    ],
    "abstract": "In this paper, we tackle the challenging problem of delayed rewards in\nreinforcement learning (RL). While Proximal Policy Optimization (PPO) has\nemerged as a leading Policy Gradient method, its performance can degrade under\ndelayed rewards. We introduce two key enhancements to PPO: a hybrid policy\narchitecture that combines an offline policy (trained on expert demonstrations)\nwith an online PPO policy, and a reward shaping mechanism using Time Window\nTemporal Logic (TWTL). The hybrid architecture leverages offline data\nthroughout training while maintaining PPO's theoretical guarantees. Building on\nthe monotonic improvement framework of Trust Region Policy Optimization (TRPO),\nwe prove that our approach ensures improvement over both the offline policy and\nprevious iterations, with a bounded performance gap of\n$(2\\varsigma\\gamma\\alpha^2)/(1-\\gamma)^2$, where $\\alpha$ is the mixing\nparameter, $\\gamma$ is the discount factor, and $\\varsigma$ bounds the expected\nadvantage. Additionally, we prove that our TWTL-based reward shaping preserves\nthe optimal policy of the original problem. TWTL enables formal translation of\ntemporal objectives into immediate feedback signals that guide learning. We\ndemonstrate the effectiveness of our approach through extensive experiments on\nan inverted pendulum and a lunar lander environments, showing improvements in\nboth learning speed and final performance compared to standard PPO and\noffline-only approaches.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17861v3",
    "published_date": "2024-11-26 20:22:31 UTC",
    "updated_date": "2024-12-05 02:30:43 UTC"
  },
  {
    "arxiv_id": "2411.17855v2",
    "title": "\"Give me the code\" -- Log Analysis of First-Year CS Students' Interactions With GPT",
    "authors": [
      "Pedro Alves",
      "Bruno Pereira Cipriano"
    ],
    "abstract": "The impact of Large Language Models (LLMs) like GPT-3, GPT-4, and Bard in\ncomputer science (CS) education is expected to be profound. Students now have\nthe power to generate code solutions for a wide array of programming\nassignments. For first-year students, this may be particularly problematic\nsince the foundational skills are still in development and an over-reliance on\ngenerative AI tools can hinder their ability to grasp essential programming\nconcepts. This paper analyzes the prompts used by 69 freshmen undergraduate\nstudents to solve a certain programming problem within a project assignment,\nwithout giving them prior prompt training. We also present the rules of the\nexercise that motivated the prompts, designed to foster critical thinking\nskills during the interaction. Despite using unsophisticated prompting\ntechniques, our findings suggest that the majority of students successfully\nleveraged GPT, incorporating the suggested solutions into their projects.\nAdditionally, half of the students demonstrated the ability to exercise\njudgment in selecting from multiple GPT-generated solutions, showcasing the\ndevelopment of their critical thinking skills in evaluating AI-generated code.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.ET",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "This is the author's version of the work. It is posted here for your\n  personal use. Not for redistribution",
    "pdf_url": "http://arxiv.org/pdf/2411.17855v2",
    "published_date": "2024-11-26 20:11:46 UTC",
    "updated_date": "2024-12-01 19:02:28 UTC"
  },
  {
    "arxiv_id": "2411.17847v1",
    "title": "SoftmAP: Software-Hardware Co-design for Integer-Only Softmax on Associative Processors",
    "authors": [
      "Mariam Rakka",
      "Jinhao Li",
      "Guohao Dai",
      "Ahmed Eltawil",
      "Mohammed E. Fouda",
      "Fadi Kurdahi"
    ],
    "abstract": "Recent research efforts focus on reducing the computational and memory\noverheads of Large Language Models (LLMs) to make them feasible on\nresource-constrained devices. Despite advancements in compression techniques,\nnon-linear operators like Softmax and Layernorm remain bottlenecks due to their\nsensitivity to quantization. We propose SoftmAP, a software-hardware co-design\nmethodology that implements an integer-only low-precision Softmax using\nIn-Memory Compute (IMC) hardware. Our method achieves up to three orders of\nmagnitude improvement in the energy-delay product compared to A100 and RTX3090\nGPUs, making LLMs more deployable without compromising performance.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "Accepted in DATE 2025",
    "pdf_url": "http://arxiv.org/pdf/2411.17847v1",
    "published_date": "2024-11-26 20:00:54 UTC",
    "updated_date": "2024-11-26 20:00:54 UTC"
  },
  {
    "arxiv_id": "2411.17840v1",
    "title": "Basic Research, Lethal Effects: Military AI Research Funding as Enlistment",
    "authors": [
      "David Gray Widder",
      "Sireesh Gururaja",
      "Lucy Suchman"
    ],
    "abstract": "In the context of unprecedented U.S. Department of Defense (DoD) budgets,\nthis paper examines the recent history of DoD funding for academic research in\nalgorithmically based warfighting. We draw from a corpus of DoD grant\nsolicitations from 2007 to 2023, focusing on those addressed to researchers in\nthe field of artificial intelligence (AI). Considering the implications of DoD\nfunding for academic research, the paper proceeds through three analytic\nsections. In the first, we offer a critical examination of the distinction\nbetween basic and applied research, showing how funding calls framed as basic\nresearch nonetheless enlist researchers in a war fighting agenda. In the\nsecond, we offer a diachronic analysis of the corpus, showing how a 'one small\nproblem' caveat, in which affirmation of progress in military technologies is\nqualified by acknowledgement of outstanding problems, becomes justification for\nadditional investments in research. We close with an analysis of DoD\naspirations based on a subset of Defense Advanced Research Projects Agency\n(DARPA) grant solicitations for the use of AI in battlefield applications.\nTaken together, we argue that grant solicitations work as a vehicle for the\nmutual enlistment of DoD funding agencies and the academic AI research\ncommunity in setting research agendas. The trope of basic research in this\ncontext offers shelter from significant moral questions that military\napplications of one's research would raise, by obscuring the connections that\nimplicate researchers in U.S. militarism.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "22 pages, 9945 words",
    "pdf_url": "http://arxiv.org/pdf/2411.17840v1",
    "published_date": "2024-11-26 19:29:27 UTC",
    "updated_date": "2024-11-26 19:29:27 UTC"
  },
  {
    "arxiv_id": "2411.17832v2",
    "title": "SVGDreamer++: Advancing Editability and Diversity in Text-Guided SVG Generation",
    "authors": [
      "Ximing Xing",
      "Qian Yu",
      "Chuang Wang",
      "Haitao Zhou",
      "Jing Zhang",
      "Dong Xu"
    ],
    "abstract": "Recently, text-guided scalable vector graphics (SVG) synthesis has\ndemonstrated significant potential in domains such as iconography and\nsketching. However, SVGs generated from existing Text-to-SVG methods often lack\neditability and exhibit deficiencies in visual quality and diversity. In this\npaper, we propose a novel text-guided vector graphics synthesis method to\naddress these limitations. To enhance the editability of output SVGs, we\nintroduce a Hierarchical Image VEctorization (HIVE) framework that operates at\nthe semantic object level and supervises the optimization of components within\nthe vector object. This approach facilitates the decoupling of vector graphics\ninto distinct objects and component levels. Our proposed HIVE algorithm,\ninformed by image segmentation priors, not only ensures a more precise\nrepresentation of vector graphics but also enables fine-grained editing\ncapabilities within vector objects. To improve the diversity of output SVGs, we\npresent a Vectorized Particle-based Score Distillation (VPSD) approach. VPSD\naddresses over-saturation issues in existing methods and enhances sample\ndiversity. A pre-trained reward model is incorporated to re-weight vector\nparticles, improving aesthetic appeal and enabling faster convergence.\nAdditionally, we design a novel adaptive vector primitives control strategy,\nwhich allows for the dynamic adjustment of the number of primitives, thereby\nenhancing the presentation of graphic details. Extensive experiments validate\nthe effectiveness of the proposed method, demonstrating its superiority over\nbaseline methods in terms of editability, visual quality, and diversity. We\nalso show that our new method supports up to six distinct vector styles,\ncapable of generating high-quality vector assets suitable for stylized vector\ndesign and poster design. Code and demo will be released at:\nhttp://ximinng.github.io/SVGDreamerV2Project/",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "17 pages, 17 figures. Project Page:\n  http://ximinng.github.io/SVGDreamerV2Project/. arXiv admin note: text overlap\n  with arXiv:2312.16476",
    "pdf_url": "http://arxiv.org/pdf/2411.17832v2",
    "published_date": "2024-11-26 19:13:38 UTC",
    "updated_date": "2024-12-13 11:40:57 UTC"
  },
  {
    "arxiv_id": "2411.17697v2",
    "title": "StableAnimator: High-Quality Identity-Preserving Human Image Animation",
    "authors": [
      "Shuyuan Tu",
      "Zhen Xing",
      "Xintong Han",
      "Zhi-Qi Cheng",
      "Qi Dai",
      "Chong Luo",
      "Zuxuan Wu"
    ],
    "abstract": "Current diffusion models for human image animation struggle to ensure\nidentity (ID) consistency. This paper presents StableAnimator, the first\nend-to-end ID-preserving video diffusion framework, which synthesizes\nhigh-quality videos without any post-processing, conditioned on a reference\nimage and a sequence of poses. Building upon a video diffusion model,\nStableAnimator contains carefully designed modules for both training and\ninference striving for identity consistency. In particular, StableAnimator\nbegins by computing image and face embeddings with off-the-shelf extractors,\nrespectively and face embeddings are further refined by interacting with image\nembeddings using a global content-aware Face Encoder. Then, StableAnimator\nintroduces a novel distribution-aware ID Adapter that prevents interference\ncaused by temporal layers while preserving ID via alignment. During inference,\nwe propose a novel Hamilton-Jacobi-Bellman (HJB) equation-based optimization to\nfurther enhance the face quality. We demonstrate that solving the HJB equation\ncan be integrated into the diffusion denoising process, and the resulting\nsolution constrains the denoising path and thus benefits ID preservation.\nExperiments on multiple benchmarks show the effectiveness of StableAnimator\nboth qualitatively and quantitatively.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17697v2",
    "published_date": "2024-11-26 18:59:22 UTC",
    "updated_date": "2024-11-27 07:39:20 UTC"
  },
  {
    "arxiv_id": "2412.00077v1",
    "title": "Selfish Evolution: Making Discoveries in Extreme Label Noise with the Help of Overfitting Dynamics",
    "authors": [
      "Nima Sedaghat",
      "Tanawan Chatchadanoraset",
      "Colin Orion Chandler",
      "Ashish Mahabal",
      "Maryam Eslami"
    ],
    "abstract": "Motivated by the scarcity of proper labels in an astrophysical application,\nwe have developed a novel technique, called Selfish Evolution, which allows for\nthe detection and correction of corrupted labels in a weakly supervised\nfashion. Unlike methods based on early stopping, we let the model train on the\nnoisy dataset. Only then do we intervene and allow the model to overfit to\nindividual samples. The ``evolution'' of the model during this process reveals\npatterns with enough information about the noisiness of the label, as well as\nits correct version. We train a secondary network on these spatiotemporal\n``evolution cubes'' to correct potentially corrupted labels. We incorporate the\ntechnique in a closed-loop fashion, allowing for automatic convergence towards\na mostly clean dataset, without presumptions about the state of the network in\nwhich we intervene. We evaluate on the main task of the Supernova-hunting\ndataset but also demonstrate efficiency on the more standard MNIST dataset.",
    "categories": [
      "cs.CV",
      "astro-ph.IM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00077v1",
    "published_date": "2024-11-26 18:51:43 UTC",
    "updated_date": "2024-11-26 18:51:43 UTC"
  },
  {
    "arxiv_id": "2411.17684v1",
    "title": "RealSeal: Revolutionizing Media Authentication with Real-Time Realism Scoring",
    "authors": [
      "Bhaktipriya Radharapu",
      "Harish Krishna"
    ],
    "abstract": "The growing threat of deepfakes and manipulated media necessitates a radical\nrethinking of media authentication. Existing methods for watermarking synthetic\ndata fall short, as they can be easily removed or altered, and current deepfake\ndetection algorithms do not achieve perfect accuracy. Provenance techniques,\nwhich rely on metadata to verify content origin, fail to address the\nfundamental problem of staged or fake media.\n  This paper introduces a groundbreaking paradigm shift in media authentication\nby advocating for the watermarking of real content at its source, as opposed to\nwatermarking synthetic data. Our innovative approach employs multisensory\ninputs and machine learning to assess the realism of content in real-time and\nacross different contexts. We propose embedding a robust realism score within\nthe image metadata, fundamentally transforming how images are trusted and\ncirculated. By combining established principles of human reasoning about\nreality, rooted in firmware and hardware security, with the sophisticated\nreasoning capabilities of contemporary machine learning systems, we develop a\nholistic approach that analyzes information from multiple perspectives.\n  This ambitious, blue sky approach represents a significant leap forward in\nthe field, pushing the boundaries of media authenticity and trust. By embracing\ncutting-edge advancements in technology and interdisciplinary research, we aim\nto establish a new standard for verifying the authenticity of digital media.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Best Paper Award, Blue Sky Track at 26th ACM International Conference\n  on Multimodal Interaction, Nov 2024, San Jose, Costa Rica",
    "pdf_url": "http://arxiv.org/pdf/2411.17684v1",
    "published_date": "2024-11-26 18:48:23 UTC",
    "updated_date": "2024-11-26 18:48:23 UTC"
  },
  {
    "arxiv_id": "2411.17800v1",
    "title": "STAR: Synthesis of Tailored Architectures",
    "authors": [
      "Armin W. Thomas",
      "Rom Parnichkun",
      "Alexander Amini",
      "Stefano Massaroli",
      "Michael Poli"
    ],
    "abstract": "Iterative improvement of model architectures is fundamental to deep learning:\nTransformers first enabled scaling, and recent advances in model hybridization\nhave pushed the quality-efficiency frontier. However, optimizing architectures\nremains challenging and expensive. Current automated or manual approaches fall\nshort, largely due to limited progress in the design of search spaces and due\nto the simplicity of resulting patterns and heuristics. In this work, we\npropose a new approach for the synthesis of tailored architectures (STAR). Our\napproach combines a novel search space based on the theory of linear\ninput-varying systems, supporting a hierarchical numerical encoding into\narchitecture genomes. STAR genomes are automatically refined and recombined\nwith gradient-free, evolutionary algorithms to optimize for multiple model\nquality and efficiency metrics. Using STAR, we optimize large populations of\nnew architectures, leveraging diverse computational units and interconnection\npatterns, improving over highly-optimized Transformers and striped hybrid\nmodels on the frontier of quality, parameter size, and inference cache for\nautoregressive language modeling.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17800v1",
    "published_date": "2024-11-26 18:42:42 UTC",
    "updated_date": "2024-11-26 18:42:42 UTC"
  },
  {
    "arxiv_id": "2411.17645v3",
    "title": "Explainable AI for Classifying UTI Risk Groups Using a Real-World Linked EHR and Pathology Lab Dataset",
    "authors": [
      "Yujie Dai",
      "Brian Sullivan",
      "Axel Montout",
      "Amy Dillon",
      "Chris Waller",
      "Peter Acs",
      "Rachel Denholm",
      "Philip Williams",
      "Alastair D Hay",
      "Raul Santos-Rodriguez",
      "Andrew Dowsey"
    ],
    "abstract": "The use of machine learning and AI on electronic health records (EHRs) holds\nsubstantial potential for clinical insight. However, this approach faces\nchallenges due to data heterogeneity, sparsity, temporal misalignment, and\nlimited labeled outcomes. In this context, we leverage a linked EHR dataset of\napproximately one million de-identified individuals from Bristol, North\nSomerset, and South Gloucestershire, UK, to characterize urinary tract\ninfections (UTIs). We implemented a data pre-processing and curation pipeline\nthat transforms the raw EHR data into a structured format suitable for\ndeveloping predictive models focused on data fairness, accountability and\ntransparency. Given the limited availability and biases of ground truth UTI\noutcomes, we introduce a UTI risk estimation framework informed by clinical\nexpertise to estimate UTI risk across individual patient timelines. Pairwise\nXGBoost models are trained using this framework to differentiate UTI risk\ncategories with explainable AI techniques applied to identify key predictors\nand support interpretability. Our findings reveal differences in clinical and\ndemographic predictors across risk groups. While this study highlights the\npotential of AI-driven insights to support UTI clinical decision-making,\nfurther investigation of patient sub-strata and extensive validation are needed\nto ensure robustness and applicability in clinical practice.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17645v3",
    "published_date": "2024-11-26 18:10:51 UTC",
    "updated_date": "2025-02-28 15:16:36 UTC"
  },
  {
    "arxiv_id": "2411.17798v1",
    "title": "DapPep: Domain Adaptive Peptide-agnostic Learning for Universal T-cell Receptor-antigen Binding Affinity Prediction",
    "authors": [
      "Jiangbin Zheng",
      "Qianhui Xu",
      "Ruichen Xia",
      "Stan Z. Li"
    ],
    "abstract": "Identifying T-cell receptors (TCRs) that interact with antigenic peptides\nprovides the technical basis for developing vaccines and immunotherapies. The\nemergent deep learning methods excel at learning antigen binding patterns from\nknown TCRs but struggle with novel or sparsely represented antigens. However,\nbinding specificity for unseen antigens or exogenous peptides is critical. We\nintroduce a domain-adaptive peptide-agnostic learning framework DapPep for\nuniversal TCR-antigen binding affinity prediction to address this challenge.\nThe lightweight self-attention architecture combines a pre-trained protein\nlanguage model with an inner-loop self-supervised regime to enable robust\nTCR-peptide representations. Extensive experiments on various benchmarks\ndemonstrate that DapPep consistently outperforms existing tools, showcasing\nrobust generalization capability, especially for data-scarce settings and\nunseen peptides. Moreover, DapPep proves effective in challenging clinical\ntasks such as sorting reactive T cells in tumor neoantigen therapy and\nidentifying key positions in 3D structures.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.QM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17798v1",
    "published_date": "2024-11-26 18:06:42 UTC",
    "updated_date": "2024-11-26 18:06:42 UTC"
  },
  {
    "arxiv_id": "2412.08653v1",
    "title": "What AI evaluations for preventing catastrophic risks can and cannot do",
    "authors": [
      "Peter Barnett",
      "Lisa Thiergart"
    ],
    "abstract": "AI evaluations are an important component of the AI governance toolkit,\nunderlying current approaches to safety cases for preventing catastrophic\nrisks. Our paper examines what these evaluations can and cannot tell us.\nEvaluations can establish lower bounds on AI capabilities and assess certain\nmisuse risks given sufficient effort from evaluators.\n  Unfortunately, evaluations face fundamental limitations that cannot be\novercome within the current paradigm. These include an inability to establish\nupper bounds on capabilities, reliably forecast future model capabilities, or\nrobustly assess risks from autonomous AI systems. This means that while\nevaluations are valuable tools, we should not rely on them as our main way of\nensuring AI systems are safe. We conclude with recommendations for incremental\nimprovements to frontier AI safety, while acknowledging these fundamental\nlimitations remain unsolved.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.08653v1",
    "published_date": "2024-11-26 18:00:36 UTC",
    "updated_date": "2024-11-26 18:00:36 UTC"
  },
  {
    "arxiv_id": "2411.17636v1",
    "title": "MALMM: Multi-Agent Large Language Models for Zero-Shot Robotics Manipulation",
    "authors": [
      "Harsh Singh",
      "Rocktim Jyoti Das",
      "Mingfei Han",
      "Preslav Nakov",
      "Ivan Laptev"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable planning abilities\nacross various domains, including robotics manipulation and navigation. While\nrecent efforts in robotics have leveraged LLMs both for high-level and\nlow-level planning, these approaches often face significant challenges, such as\nhallucinations in long-horizon tasks and limited adaptability due to the\ngeneration of plans in a single pass without real-time feedback. To address\nthese limitations, we propose a novel multi-agent LLM framework, Multi-Agent\nLarge Language Model for Manipulation (MALMM) that distributes high-level\nplanning and low-level control code generation across specialized LLM agents,\nsupervised by an additional agent that dynamically manages transitions. By\nincorporating observations from the environment after each step, our framework\neffectively handles intermediate failures and enables adaptive re-planning.\nUnlike existing methods, our approach does not rely on pre-trained skill\npolicies or in-context learning examples and generalizes to a variety of new\ntasks. We evaluate our approach on nine RLBench tasks, including long-horizon\ntasks, and demonstrate its ability to solve robotics manipulation in a\nzero-shot setting, thereby overcoming key limitations of existing LLM-based\nmanipulation methods.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "48 pages",
    "pdf_url": "http://arxiv.org/pdf/2411.17636v1",
    "published_date": "2024-11-26 17:53:44 UTC",
    "updated_date": "2024-11-26 17:53:44 UTC"
  },
  {
    "arxiv_id": "2411.17795v1",
    "title": "Pan-protein Design Learning Enables Task-adaptive Generalization for Low-resource Enzyme Design",
    "authors": [
      "Jiangbin Zheng",
      "Ge Wang",
      "Han Zhang",
      "Stan Z. Li"
    ],
    "abstract": "Computational protein design (CPD) offers transformative potential for\nbioengineering, but current deep CPD models, focused on universal domains,\nstruggle with function-specific designs. This work introduces a novel CPD\nparadigm tailored for functional design tasks, particularly for enzymes-a key\nprotein class often lacking specific application efficiency. To address\nstructural data scarcity, we present CrossDesign, a domain-adaptive framework\nthat leverages pretrained protein language models (PPLMs). By aligning protein\nstructures with sequences, CrossDesign transfers pretrained knowledge to\nstructure models, overcoming the limitations of limited structural data. The\nframework combines autoregressive (AR) and non-autoregressive (NAR) states in\nits encoder-decoder architecture, applying it to enzyme datasets and\npan-proteins. Experimental results highlight CrossDesign's superior performance\nand robustness, especially with out-of-domain enzymes. Additionally, the model\nexcels in fitness prediction when tested on large-scale mutation data,\nshowcasing its stability.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.QM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17795v1",
    "published_date": "2024-11-26 17:51:33 UTC",
    "updated_date": "2024-11-26 17:51:33 UTC"
  },
  {
    "arxiv_id": "2411.17793v1",
    "title": "Engineering AI Judge Systems",
    "authors": [
      "Jiahuei Lin",
      "Dayi Lin",
      "Sky Zhang",
      "Ahmed E. Hassan"
    ],
    "abstract": "AI judge systems are designed to automatically evaluate Foundation\nModel-powered software (i.e., FMware). Due to the intrinsic dynamic and\nstochastic nature of FMware, the development of AI judge systems requires a\nunique engineering life cycle and presents new challenges. In this paper, we\ndiscuss the challenges based on our industrial experiences in developing AI\njudge systems for FMware. These challenges lead to substantial time\nconsumption, cost and inaccurate judgments. We propose a framework that tackles\nthe challenges with the goal of improving the productivity of developing\nhigh-quality AI judge systems. Finally, we evaluate our framework with a case\nstudy on judging a commit message generation FMware. The accuracy of the\njudgments made by the AI judge system developed with our framework outperforms\nthose made by the AI judge system that is developed without our framework by up\nto 6.2%, with a significant reduction in development effort.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17793v1",
    "published_date": "2024-11-26 17:43:32 UTC",
    "updated_date": "2024-11-26 17:43:32 UTC"
  },
  {
    "arxiv_id": "2411.17792v1",
    "title": "$H^3$Fusion: Helpful, Harmless, Honest Fusion of Aligned LLMs",
    "authors": [
      "Selim Furkan Tekin",
      "Fatih Ilhan",
      "Tiansheng Huang",
      "Sihao Hu",
      "Zachary Yahn",
      "Ling Liu"
    ],
    "abstract": "Alignment of pretrained LLMs using instruction-based datasets is critical for\ncreating fine-tuned models that reflect human preference. A growing number of\nalignment-based fine-tuning algorithms and benchmarks emerged recently, fueling\nthe efforts on effective alignments of pre-trained LLMs to ensure helpful,\nharmless, and honest answers from both open-source and closed-source LLMs. This\npaper tackles this problem by developing an alignment fusion approach, coined\nas $H^3$Fusion, with three unique characteristics. First, $H^3$Fusion ensembles\nmultiple individually aligned LLMs to create a final fine-tuned alignment model\nwith enhanced capabilities beyond those of individual models, delivering robust\nalignment through promoting helpful, harmless, honest fusion. Second,\n$H^3$Fusion leverages the mixture-of-experts (MoE) methodology in two steps. We\nfirst freeze the multi-head attention weights of each individual model while\ntuning the FFN layer during alignment fusion. Then we merge the aligned model\nweights with an expert router according to the type of input instruction and\ndynamically select a subset of experts that are best suited for producing the\noutput response. Finally, we boost the performance of the resulting\n$H^3$3Fusion model by introducing gating loss and regularization terms. The\nformer penalizes the selection errors of the expert-router, and the latter\nmediates the expert weights drifting during fine-tuning and dynamically adjusts\nthe fusion behavior of the resulting model by canalizing the activations on the\nexperts. Extensive evaluations on three benchmark datasets show that\n$H^3$3Fusion is more helpful, less harmful, and more honest from two aspects:\nit outperforms each individually aligned model by $11.37\\%$, and it provides\nstronger robustness compared to the state-of-the-art LLM ensemble approaches by\n$13.77\\%$. Code is available at github.com/sftekin/h3fusion.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17792v1",
    "published_date": "2024-11-26 17:42:38 UTC",
    "updated_date": "2024-11-26 17:42:38 UTC"
  },
  {
    "arxiv_id": "2411.17629v2",
    "title": "Learning Chemical Reaction Representation with Reactant-Product Alignment",
    "authors": [
      "Kaipeng Zeng",
      "Xianbin Liu",
      "Yu Zhang",
      "Xiaokang Yang",
      "Yaohui Jin",
      "Yanyan Xu"
    ],
    "abstract": "Organic synthesis stands as a cornerstone of the chemical industry. The\ndevelopment of robust machine learning models to support tasks associated with\norganic reactions is of significant interest. However, current methods rely on\nhand-crafted features or direct adaptations of model architectures from other\ndomains, which lack feasibility as data scales increase or ignore the rich\nchemical information inherent in reactions. To address these issues, this paper\nintroduces RAlign, a novel chemical reaction representation learning model for\nvarious organic reaction-related tasks. By integrating atomic correspondence\nbetween reactants and products, our model discerns the molecular\ntransformations that occur during the reaction, thereby enhancing comprehension\nof the reaction mechanism. We have designed an adapter structure to incorporate\nreaction conditions into the chemical reaction representation, allowing the\nmodel to handle various reaction conditions and to adapt to various datasets\nand downstream tasks. Additionally, we introduce a reaction-center-aware\nattention mechanism that enables the model to concentrate on key functional\ngroups, thereby generating potent representations for chemical reactions. Our\nmodel has been evaluated on a range of downstream tasks. Experimental results\nindicate that our model markedly outperforms existing chemical reaction\nrepresentation learning architectures on most of the datasets. We plan to\nopen-source the code contingent upon the acceptance of the paper.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17629v2",
    "published_date": "2024-11-26 17:41:44 UTC",
    "updated_date": "2025-01-03 16:55:38 UTC"
  },
  {
    "arxiv_id": "2411.17624v1",
    "title": "Machine Learning and Multi-source Remote Sensing in Forest Carbon Stock Estimation: A Review",
    "authors": [
      "Autumn Nguyen",
      "Sulagna Saha"
    ],
    "abstract": "Quantifying forest carbon is crucial for informing decisions and policies\nthat will protect the planet. Machine learning (ML) and remote sensing (RS)\ntechniques have been used to do this task more effectively, yet there lacks a\nsystematic review on the most recent ML methods and RS combinations, especially\nwith the consideration of forest characteristics. This study systematically\nanalyzed 25 papers meeting strict inclusion criteria from over 80 related\nstudies, identifying 28 ML methods and key combinations of RS data. Random\nForest had the most frequent appearance (88\\% of studies), while Extreme\nGradient Boosting showed superior performance in 75\\% of the studies in which\nit was compared with other methods. Sentinel-1 emerged as the most utilized\nremote sensing source, with multi-sensor approaches (e.g., Sentinel-1,\nSentinel-2, and LiDAR) proving especially effective. Our findings provide\ngrounds for recommending best practices in integrating machine learning and\nremote sensing for accurate and scalable forest carbon stock estimation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "First author and corresponding author: Autumn Nguyen",
    "pdf_url": "http://arxiv.org/pdf/2411.17624v1",
    "published_date": "2024-11-26 17:34:59 UTC",
    "updated_date": "2024-11-26 17:34:59 UTC"
  },
  {
    "arxiv_id": "2411.17614v1",
    "title": "Automating Chapter-Level Classification for Electronic Theses and Dissertations",
    "authors": [
      "Bipasha Banerjee",
      "William A. Ingram",
      "Edward A. Fox"
    ],
    "abstract": "Traditional archival practices for describing electronic theses and\ndissertations (ETDs) rely on broad, high-level metadata schemes that fail to\ncapture the depth, complexity, and interdisciplinary nature of these long\nscholarly works. The lack of detailed, chapter-level content descriptions\nimpedes researchers' ability to locate specific sections or themes, thereby\nreducing discoverability and overall accessibility. By providing chapter-level\nmetadata information, we improve the effectiveness of ETDs as research\nresources. This makes it easier for scholars to navigate them efficiently and\nextract valuable insights. The absence of such metadata further obstructs\ninterdisciplinary research by obscuring connections across fields, hindering\nnew academic discoveries and collaboration. In this paper, we propose a machine\nlearning and AI-driven solution to automatically categorize ETD chapters. This\nsolution is intended to improve discoverability and promote understanding of\nchapters. Our approach enriches traditional archival practices by providing\ncontext-rich descriptions that facilitate targeted navigation and improved\naccess. We aim to support interdisciplinary research and make ETDs more\naccessible. By providing chapter-level classification labels and using them to\nindex in our developed prototype system, we make content in ETD chapters more\ndiscoverable and usable for a diverse range of scholarly needs. Implementing\nthis AI-enhanced approach allows archives to serve researchers better, enabling\nefficient access to relevant information and supporting deeper engagement with\nETDs. This will increase the impact of ETDs as research tools, foster\ninterdisciplinary exploration, and reinforce the role of archives in scholarly\ncommunication within the data-intensive academic landscape.",
    "categories": [
      "cs.DL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.DL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17614v1",
    "published_date": "2024-11-26 17:27:18 UTC",
    "updated_date": "2024-11-26 17:27:18 UTC"
  },
  {
    "arxiv_id": "2411.17608v2",
    "title": "Mixed-State Quantum Denoising Diffusion Probabilistic Model",
    "authors": [
      "Gino Kwun",
      "Bingzhi Zhang",
      "Quntao Zhuang"
    ],
    "abstract": "Generative quantum machine learning has gained significant attention for its\nability to produce quantum states with desired distributions. Among various\nquantum generative models, quantum denoising diffusion probabilistic models\n(QuDDPMs) [Phys. Rev. Lett. 132, 100602 (2024)] provide a promising approach\nwith stepwise learning that resolves the training issues. However, the\nrequirement of high-fidelity scrambling unitaries in QuDDPM poses a challenge\nin near-term implementation. We propose the \\textit{mixed-state quantum\ndenoising diffusion probabilistic model} (MSQuDDPM) to eliminate the need for\nscrambling unitaries. Our approach focuses on adapting the quantum noise\nchannels to the model architecture, which integrates depolarizing noise\nchannels in the forward diffusion process and parameterized quantum circuits\nwith projective measurements in the backward denoising steps. We also introduce\nseveral techniques to improve MSQuDDPM, including a cosine-exponent schedule of\nnoise interpolation, the use of single-qubit random ancilla, and\nsuperfidelity-based cost functions to enhance the convergence. We evaluate\nMSQuDDPM on quantum ensemble generation tasks, demonstrating its successful\nperformance.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "quant-ph",
    "comment": "8 pages, 8 figures; Fig.8 added, appendix C added",
    "pdf_url": "http://arxiv.org/pdf/2411.17608v2",
    "published_date": "2024-11-26 17:20:58 UTC",
    "updated_date": "2025-03-04 05:15:50 UTC"
  },
  {
    "arxiv_id": "2411.17600v1",
    "title": "Making History Readable",
    "authors": [
      "Bipasha Banerjee",
      "Jennifer Goyne",
      "William A. Ingram"
    ],
    "abstract": "The Virginia Tech University Libraries (VTUL) Digital Library Platform (DLP)\nhosts digital collections that offer our users access to a wide variety of\ndocuments of historical and cultural importance. These collections are not only\nof academic importance but also provide our users with a glance at local\nhistorical events. Our DLP contains collections comprising digital objects\nfeaturing complex layouts, faded imagery, and hard-to-read handwritten text,\nwhich makes providing online access to these materials challenging. To address\nthese issues, we integrate AI into our DLP workflow and convert the text in the\ndigital objects into a machine-readable format. To enhance the user experience\nwith our historical collections, we use custom AI agents for handwriting\nrecognition, text extraction, and large language models (LLMs) for\nsummarization. This poster highlights three collections focusing on handwritten\nletters, newspapers, and digitized topographic maps. We discuss the challenges\nwith each collection and detail our approaches to address them. Our proposed\nmethods aim to enhance the user experience by making the contents in these\ncollections easier to search and navigate.",
    "categories": [
      "cs.DL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.DL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17600v1",
    "published_date": "2024-11-26 17:06:58 UTC",
    "updated_date": "2024-11-26 17:06:58 UTC"
  },
  {
    "arxiv_id": "2411.17598v1",
    "title": "Agentic AI for Improving Precision in Identifying Contributions to Sustainable Development Goals",
    "authors": [
      "William A. Ingram",
      "Bipasha Banerjee",
      "Edward A. Fox"
    ],
    "abstract": "As research institutions increasingly commit to supporting the United\nNations' Sustainable Development Goals (SDGs), there is a pressing need to\naccurately assess their research output against these goals. Current\napproaches, primarily reliant on keyword-based Boolean search queries, conflate\nincidental keyword matches with genuine contributions, reducing retrieval\nprecision and complicating benchmarking efforts. This study investigates the\napplication of autoregressive Large Language Models (LLMs) as evaluation agents\nto identify relevant scholarly contributions to SDG targets in scholarly\npublications. Using a dataset of academic abstracts retrieved via SDG-specific\nkeyword queries, we demonstrate that small, locally-hosted LLMs can\ndifferentiate semantically relevant contributions to SDG targets from documents\nretrieved due to incidental keyword matches, addressing the limitations of\ntraditional methods. By leveraging the contextual understanding of LLMs, this\napproach provides a scalable framework for improving SDG-related research\nmetrics and informing institutional reporting.",
    "categories": [
      "cs.DL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.DL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17598v1",
    "published_date": "2024-11-26 17:06:30 UTC",
    "updated_date": "2024-11-26 17:06:30 UTC"
  },
  {
    "arxiv_id": "2411.17593v3",
    "title": "What Differentiates Educational Literature? A Multimodal Fusion Approach of Transformers and Computational Linguistics",
    "authors": [
      "Jordan J. Bird"
    ],
    "abstract": "The integration of new literature into the English curriculum remains a\nchallenge since educators often lack scalable tools to rapidly evaluate\nreadability and adapt texts for diverse classroom needs. This study proposes to\naddress this gap through a multimodal approach that combines transformer-based\ntext classification with linguistic feature analysis to align texts with UK Key\nStages. Eight state-of-the-art Transformers were fine-tuned on segmented text\ndata, with BERT achieving the highest unimodal F1 score of 0.75. In parallel,\n500 deep neural network topologies were searched for the classification of\nlinguistic characteristics, achieving an F1 score of 0.392. The fusion of these\nmodalities shows a significant improvement, with every multimodal approach\noutperforming all unimodal models. In particular, the ELECTRA Transformer fused\nwith the neural network achieved an F1 score of 0.996. Unimodal and multimodal\napproaches are shown to have statistically significant differences in all\nvalidation metrics (accuracy, precision, recall, F1 score) except for inference\ntime. The proposed approach is finally encapsulated in a stakeholder-facing web\napplication, providing non-technical stakeholder access to real-time insights\non text complexity, reading difficulty, curriculum alignment, and\nrecommendations for learning age range. The application empowers data-driven\ndecision making and reduces manual workload by integrating AI-based\nrecommendations into lesson planning for English literature.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17593v3",
    "published_date": "2024-11-26 17:01:27 UTC",
    "updated_date": "2024-12-02 17:43:20 UTC"
  },
  {
    "arxiv_id": "2411.17570v1",
    "title": "Learning Explainable Treatment Policies with Clinician-Informed Representations: A Practical Approach",
    "authors": [
      "Johannes O. Ferstad",
      "Emily B. Fox",
      "David Scheinker",
      "Ramesh Johari"
    ],
    "abstract": "Digital health interventions (DHIs) and remote patient monitoring (RPM) have\nshown great potential in improving chronic disease management through\npersonalized care. However, barriers like limited efficacy and workload\nconcerns hinder adoption of existing DHIs; while limited sample sizes and lack\nof interpretability limit the effectiveness and adoption of purely black-box\nalgorithmic DHIs. In this paper, we address these challenges by developing a\npipeline for learning explainable treatment policies for RPM-enabled DHIs. We\napply our approach in the real-world setting of RPM using a DHI to improve\nglycemic control of youth with type 1 diabetes. Our main contribution is to\nreveal the importance of clinical domain knowledge in developing state and\naction representations for effective, efficient, and interpretable targeting\npolicies. We observe that policies learned from clinician-informed\nrepresentations are significantly more efficacious and efficient than policies\nlearned from black-box representations. This work emphasizes the importance of\ncollaboration between ML researchers and clinicians for developing effective\nDHIs in the real world.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.AP",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Proceedings of Machine Learning for Health (ML4H) 2024. Code\n  available at: https://github.com/jferstad/ml4h-explainable-policies",
    "pdf_url": "http://arxiv.org/pdf/2411.17570v1",
    "published_date": "2024-11-26 16:32:08 UTC",
    "updated_date": "2024-11-26 16:32:08 UTC"
  },
  {
    "arxiv_id": "2411.17557v1",
    "title": "A Bilayer Segmentation-Recombination Network for Accurate Segmentation of Overlapping C. elegans",
    "authors": [
      "Mengqian Dinga",
      "Jun Liua",
      "Yang Luo",
      "Jinshan Tang"
    ],
    "abstract": "Caenorhabditis elegans (C. elegans) is an excellent model organism because of\nits short lifespan and high degree of homology with human genes, and it has\nbeen widely used in a variety of human health and disease models. However, the\nsegmentation of C. elegans remains challenging due to the following reasons: 1)\nthe activity trajectory of C. elegans is uncontrollable, and multiple nematodes\noften overlap, resulting in blurred boundaries of C. elegans. This makes it\nimpossible to clearly study the life trajectory of a certain nematode; and 2)\nin the microscope images of overlapping C. elegans, the translucent tissues at\nthe edges obscure each other, leading to inaccurate boundary segmentation. To\nsolve these problems, a Bilayer Segmentation-Recombination Network (BR-Net) for\nthe segmentation of C. elegans instances is proposed. The network consists of\nthree parts: A Coarse Mask Segmentation Module (CMSM), a Bilayer Segmentation\nModule (BSM), and a Semantic Consistency Recombination Module (SCRM). The CMSM\nis used to extract the coarse mask, and we introduce a Unified Attention Module\n(UAM) in CMSM to make CMSM better aware of nematode instances. The Bilayer\nSegmentation Module (BSM) segments the aggregated C. elegans into overlapping\nand non-overlapping regions. This is followed by integration by the SCRM, where\nsemantic consistency regularization is introduced to segment nematode instances\nmore accurately. Finally, the effectiveness of the method is verified on the C.\nelegans dataset. The experimental results show that BR-Net exhibits good\ncompetitiveness and outperforms other recently proposed instance segmentation\nmethods in processing C. elegans occlusion images.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17557v1",
    "published_date": "2024-11-26 16:18:59 UTC",
    "updated_date": "2024-11-26 16:18:59 UTC"
  },
  {
    "arxiv_id": "2411.17543v1",
    "title": "Rapid Deployment of Domain-specific Hyperspectral Image Processors with Application to Autonomous Driving",
    "authors": [
      "Jon Guti√©rrez-Zaballa",
      "Koldo Basterretxea",
      "Javier Echanobe",
      "√ìscar Mata-Carballeira",
      "M. Victoria Mart√≠nez"
    ],
    "abstract": "The article discusses the use of low cost System-On-Module (SOM) platforms\nfor the implementation of efficient hyperspectral imaging (HSI) processors for\napplication in autonomous driving. The work addresses the challenges of shaping\nand deploying multiple layer fully convolutional networks (FCN) for\nlow-latency, on-board image semantic segmentation using resource- and\npower-constrained processing devices. The paper describes in detail the steps\nfollowed to redesign and customize a successfully trained HSI segmentation\nlightweight FCN that was previously tested on a high-end heterogeneous\nmultiprocessing system-on-chip (MPSoC) to accommodate it to the constraints\nimposed by a low-cost SOM. This SOM features a lower-end but much cheaper MPSoC\nsuitable for the deployment of automatic driving systems (ADS). In particular\nthe article reports the data- and hardware-specific quantization techniques\nutilized to fit the FCN into a commercial fixed-point programmable AI\ncoprocessor IP, and proposes a full customized post-training quantization\nscheme to reduce computation and storage costs without compromising\nsegmentation accuracy.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.AR",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17543v1",
    "published_date": "2024-11-26 16:04:20 UTC",
    "updated_date": "2024-11-26 16:04:20 UTC"
  },
  {
    "arxiv_id": "2411.17539v1",
    "title": "AI-Augmented Ethical Hacking: A Practical Examination of Manual Exploitation and Privilege Escalation in Linux Environments",
    "authors": [
      "Haitham S. Al-Sinani",
      "Chris J. Mitchell"
    ],
    "abstract": "This study explores the application of generative AI (GenAI) within manual\nexploitation and privilege escalation tasks in Linux-based penetration testing\nenvironments, two areas critical to comprehensive cybersecurity assessments.\nBuilding on previous research into the role of GenAI in the ethical hacking\nlifecycle, this paper presents a hands-on experimental analysis conducted in a\ncontrolled virtual setup to evaluate the utility of GenAI in supporting these\ncrucial, often manual, tasks. Our findings demonstrate that GenAI can\nstreamline processes, such as identifying potential attack vectors and parsing\ncomplex outputs for sensitive data during privilege escalation. The study also\nidentifies key benefits and challenges associated with GenAI, including\nenhanced efficiency and scalability, alongside ethical concerns related to data\nprivacy, unintended discovery of vulnerabilities, and potential for misuse.\nThis work contributes to the growing field of AI-assisted cybersecurity by\nemphasising the importance of human-AI collaboration, especially in contexts\nrequiring careful decision-making, rather than the complete replacement of\nhuman input.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.CR",
    "comment": "101 pages",
    "pdf_url": "http://arxiv.org/pdf/2411.17539v1",
    "published_date": "2024-11-26 15:55:15 UTC",
    "updated_date": "2024-11-26 15:55:15 UTC"
  },
  {
    "arxiv_id": "2411.17530v1",
    "title": "HSI-Drive v2.0: More Data for New Challenges in Scene Understanding for Autonomous Driving",
    "authors": [
      "Jon Guti√©rrez-Zaballa",
      "Koldo Basterretxea",
      "Javier Echanobe",
      "M. Victoria Mart√≠nez",
      "Unai Mart√≠nez-Corral"
    ],
    "abstract": "We present the updated version of the HSI-Drive dataset aimed at developing\nautomated driving systems (ADS) using hyperspectral imaging (HSI). The v2.0\nversion includes new annotated images from videos recorded during winter and\nfall in real driving scenarios. Added to the spring and summer images included\nin the previous v1.1 version, the new dataset contains 752 images covering the\nfour seasons. In this paper, we show the improvements achieved over previously\npublished results obtained on the v1.1 dataset, showcasing the enhanced\nperformance of models trained on the new v2.0 dataset. We also show the\nprogress made in comprehensive scene understanding by experimenting with more\ncapable image segmentation models. These models include new segmentation\ncategories aimed at the identification of essential road safety objects such as\nthe presence of vehicles and road signs, as well as highly vulnerable groups\nlike pedestrians and cyclists. In addition, we provide evidence of the\nperformance and robustness of the models when applied to segmenting HSI video\nsequences captured in various environments and conditions. Finally, for a\ncorrect assessment of the results described in this work, the constraints\nimposed by the processing platforms that can sensibly be deployed in vehicles\nfor ADS must be taken into account. Thus, and although implementation details\nare out of the scope of this paper, we focus our research on the development of\ncomputationally efficient, lightweight ML models that can eventually operate at\nhigh throughput rates. The dataset and some examples of segmented videos are\navailable in https://ipaccess.ehu.eus/HSI-Drive/.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17530v1",
    "published_date": "2024-11-26 15:45:59 UTC",
    "updated_date": "2024-11-26 15:45:59 UTC"
  },
  {
    "arxiv_id": "2411.17790v2",
    "title": "Self-supervised Monocular Depth and Pose Estimation for Endoscopy with Generative Latent Priors",
    "authors": [
      "Ziang Xu",
      "Bin Li",
      "Yang Hu",
      "Chenyu Zhang",
      "James East",
      "Sharib Ali",
      "Jens Rittscher"
    ],
    "abstract": "Accurate 3D mapping in endoscopy enables quantitative, holistic lesion\ncharacterization within the gastrointestinal (GI) tract, requiring reliable\ndepth and pose estimation. However, endoscopy systems are monocular, and\nexisting methods relying on synthetic datasets or complex models often lack\ngeneralizability in challenging endoscopic conditions. We propose a robust\nself-supervised monocular depth and pose estimation framework that incorporates\na Generative Latent Bank and a Variational Autoencoder (VAE). The Generative\nLatent Bank leverages extensive depth scenes from natural images to condition\nthe depth network, enhancing realism and robustness of depth predictions\nthrough latent feature priors. For pose estimation, we reformulate it within a\nVAE framework, treating pose transitions as latent variables to regularize\nscale, stabilize z-axis prominence, and improve x-y sensitivity. This dual\nrefinement pipeline enables accurate depth and pose predictions, effectively\naddressing the GI tract's complex textures and lighting. Extensive evaluations\non SimCol and EndoSLAM datasets confirm our framework's superior performance\nover published self-supervised methods in endoscopic depth and pose estimation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17790v2",
    "published_date": "2024-11-26 15:43:06 UTC",
    "updated_date": "2024-12-09 14:14:49 UTC"
  },
  {
    "arxiv_id": "2411.17522v1",
    "title": "On Statistical Rates of Conditional Diffusion Transformers: Approximation, Estimation and Minimax Optimality",
    "authors": [
      "Jerry Yao-Chieh Hu",
      "Weimin Wu",
      "Yi-Chen Lee",
      "Yu-Chao Huang",
      "Minshuo Chen",
      "Han Liu"
    ],
    "abstract": "We investigate the approximation and estimation rates of conditional\ndiffusion transformers (DiTs) with classifier-free guidance. We present a\ncomprehensive analysis for ``in-context'' conditional DiTs under four common\ndata assumptions. We show that both conditional DiTs and their latent variants\nlead to the minimax optimality of unconditional DiTs under identified settings.\nSpecifically, we discretize the input domains into infinitesimal grids and then\nperform a term-by-term Taylor expansion on the conditional diffusion score\nfunction under H\\\"older smooth data assumption. This enables fine-grained use\nof transformers' universal approximation through a more detailed piecewise\nconstant approximation and hence obtains tighter bounds. Additionally, we\nextend our analysis to the latent setting under the linear latent subspace\nassumption. We not only show that latent conditional DiTs achieve lower bounds\nthan conditional DiTs both in approximation and estimation, but also show the\nminimax optimality of latent unconditional DiTs. Our findings establish\nstatistical limits for conditional and unconditional DiTs, and offer practical\nguidance toward developing more efficient and accurate DiT models.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17522v1",
    "published_date": "2024-11-26 15:30:48 UTC",
    "updated_date": "2024-11-26 15:30:48 UTC"
  },
  {
    "arxiv_id": "2411.17788v2",
    "title": "Geometric Point Attention Transformer for 3D Shape Reassembly",
    "authors": [
      "Jiahan Li",
      "Chaoran Cheng",
      "Jianzhu Ma",
      "Ge Liu"
    ],
    "abstract": "Shape assembly, which aims to reassemble separate parts into a complete\nobject, has gained significant interest in recent years. Existing methods\nprimarily rely on networks to predict the poses of individual parts, but often\nfail to effectively capture the geometric interactions between the parts and\ntheir poses. In this paper, we present the Geometric Point Attention\nTransformer (GPAT), a network specifically designed to address the challenges\nof reasoning about geometric relationships. In the geometric point attention\nmodule, we integrate both global shape information and local pairwise geometric\nfeatures, along with poses represented as rotation and translation vectors for\neach part. To enable iterative updates and dynamic reasoning, we introduce a\ngeometric recycling scheme, where each prediction is fed into the next\niteration for refinement. We evaluate our model on both the semantic and\ngeometric assembly tasks, showing that it outperforms previous methods in\nabsolute pose estimation, achieving accurate pose predictions and high\nalignment accuracy.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17788v2",
    "published_date": "2024-11-26 15:29:38 UTC",
    "updated_date": "2024-12-01 08:00:56 UTC"
  },
  {
    "arxiv_id": "2411.18463v3",
    "title": "Hotspot-Driven Peptide Design via Multi-Fragment Autoregressive Extension",
    "authors": [
      "Jiahan Li",
      "Tong Chen",
      "Shitong Luo",
      "Chaoran Cheng",
      "Jiaqi Guan",
      "Ruihan Guo",
      "Sheng Wang",
      "Ge Liu",
      "Jian Peng",
      "Jianzhu Ma"
    ],
    "abstract": "Peptides, short chains of amino acids, interact with target proteins, making\nthem a unique class of protein-based therapeutics for treating human diseases.\nRecently, deep generative models have shown great promise in peptide\ngeneration. However, several challenges remain in designing effective peptide\nbinders. First, not all residues contribute equally to peptide-target\ninteractions. Second, the generated peptides must adopt valid geometries due to\nthe constraints of peptide bonds. Third, realistic tasks for peptide drug\ndevelopment are still lacking. To address these challenges, we introduce\nPepHAR, a hot-spot-driven autoregressive generative model for designing\npeptides targeting specific proteins. Building on the observation that certain\nhot spot residues have higher interaction potentials, we first use an\nenergy-based density model to fit and sample these key residues. Next, to\nensure proper peptide geometry, we autoregressively extend peptide fragments by\nestimating dihedral angles between residue frames. Finally, we apply an\noptimization process to iteratively refine fragment assembly, ensuring correct\npeptide structures. By combining hot spot sampling with fragment-based\nextension, our approach enables de novo peptide design tailored to a target\nprotein and allows the incorporation of key hot spot residues into peptide\nscaffolds. Extensive experiments, including peptide design and peptide scaffold\ngeneration, demonstrate the strong potential of PepHAR in computational peptide\nbinder design. Source code will be available at\nhttps://github.com/Ced3-han/PepHAR.",
    "categories": [
      "q-bio.BM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.BM",
    "comment": "Published as a conference paper at ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2411.18463v3",
    "published_date": "2024-11-26 15:13:17 UTC",
    "updated_date": "2025-05-20 14:55:01 UTC"
  },
  {
    "arxiv_id": "2411.17501v2",
    "title": "Inference Scaling fLaws: The Limits of LLM Resampling with Imperfect Verifiers",
    "authors": [
      "Benedikt Stroebl",
      "Sayash Kapoor",
      "Arvind Narayanan"
    ],
    "abstract": "Recent research has generated hope that inference scaling could allow weaker\nlanguage models to match or exceed the accuracy of stronger models, such as by\nrepeatedly sampling solutions to a coding problem until it passes unit tests.\nThe central thesis of this paper is that there is no free lunch for inference\nscaling: indefinite accuracy improvement through resampling can only be\nrealized if the \"verifier\" (in this case, a set of unit tests) is perfect. When\nthe verifier is imperfect, as it almost always is in domains such as reasoning\nor coding (for example, unit tests have imperfect coverage), there is a nonzero\nprobability of false positives: incorrect solutions that pass the verifier.\nResampling cannot decrease this probability, so it imposes an upper bound to\nthe accuracy of resampling-based inference scaling even with an infinite\ncompute budget. We find that there is a very strong correlation between the\nmodel's single-sample accuracy (i.e. accuracy without unit tests) and its false\npositive rate on coding benchmarks HumanEval and MBPP, whose unit tests have\nlimited coverage. Therefore, no amount of inference scaling of weaker models\ncan enable them to match the single-sample accuracy of a sufficiently strong\nmodel (Fig. 1a). When we consider that false positives have a negative utility\ncompared to abstaining from producing a solution, it bends the inference\nscaling curve further downward. Empirically, we find that the optimal number of\nsamples can be less than 10 under realistic assumptions (Fig. 1b). Finally, we\nshow that beyond accuracy, false positives may have other undesirable\nqualities, such as poor adherence to coding style conventions.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17501v2",
    "published_date": "2024-11-26 15:13:06 UTC",
    "updated_date": "2024-12-02 18:54:28 UTC"
  },
  {
    "arxiv_id": "2411.17786v1",
    "title": "DreamCache: Finetuning-Free Lightweight Personalized Image Generation via Feature Caching",
    "authors": [
      "Emanuele Aiello",
      "Umberto Michieli",
      "Diego Valsesia",
      "Mete Ozay",
      "Enrico Magli"
    ],
    "abstract": "Personalized image generation requires text-to-image generative models that\ncapture the core features of a reference subject to allow for controlled\ngeneration across different contexts. Existing methods face challenges due to\ncomplex training requirements, high inference costs, limited flexibility, or a\ncombination of these issues. In this paper, we introduce DreamCache, a scalable\napproach for efficient and high-quality personalized image generation. By\ncaching a small number of reference image features from a subset of layers and\na single timestep of the pretrained diffusion denoiser, DreamCache enables\ndynamic modulation of the generated image features through lightweight, trained\nconditioning adapters. DreamCache achieves state-of-the-art image and text\nalignment, utilizing an order of magnitude fewer extra parameters, and is both\nmore computationally effective and versatile than existing models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "16 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.17786v1",
    "published_date": "2024-11-26 15:03:14 UTC",
    "updated_date": "2024-11-26 15:03:14 UTC"
  },
  {
    "arxiv_id": "2411.17491v1",
    "title": "What's in the Image? A Deep-Dive into the Vision of Vision Language Models",
    "authors": [
      "Omri Kaduri",
      "Shai Bagon",
      "Tali Dekel"
    ],
    "abstract": "Vision-Language Models (VLMs) have recently demonstrated remarkable\ncapabilities in comprehending complex visual content. However, the mechanisms\nunderlying how VLMs process visual information remain largely unexplored. In\nthis paper, we conduct a thorough empirical analysis, focusing on attention\nmodules across layers. We reveal several key insights about how these models\nprocess visual data: (i) the internal representation of the query tokens (e.g.,\nrepresentations of \"describe the image\"), is utilized by VLMs to store global\nimage information; we demonstrate that these models generate surprisingly\ndescriptive responses solely from these tokens, without direct access to image\ntokens. (ii) Cross-modal information flow is predominantly influenced by the\nmiddle layers (approximately 25% of all layers), while early and late layers\ncontribute only marginally.(iii) Fine-grained visual attributes and object\ndetails are directly extracted from image tokens in a spatially localized\nmanner, i.e., the generated tokens associated with a specific object or\nattribute attend strongly to their corresponding regions in the image. We\npropose novel quantitative evaluation to validate our observations, leveraging\nreal-world complex visual scenes. Finally, we demonstrate the potential of our\nfindings in facilitating efficient visual processing in state-of-the-art VLMs.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17491v1",
    "published_date": "2024-11-26 14:59:06 UTC",
    "updated_date": "2024-11-26 14:59:06 UTC"
  },
  {
    "arxiv_id": "2411.17489v2",
    "title": "Puzzle Similarity: A Perceptually-guided Cross-Reference Metric for Artifact Detection in 3D Scene Reconstructions",
    "authors": [
      "Nicolai Hermann",
      "Jorge Condor",
      "Piotr Didyk"
    ],
    "abstract": "Modern reconstruction techniques can effectively model complex 3D scenes from\nsparse 2D views. However, automatically assessing the quality of novel views\nand identifying artifacts is challenging due to the lack of ground truth images\nand the limitations of No-Reference image metrics in predicting reliable\nartifact maps. The absence of such metrics hinders the assessment of the\nquality of novel views and limits the adoption of post-processing techniques,\nsuch as inpainting, to enhance reconstruction quality. To tackle this, recent\nwork has established a new category of metrics (Cross-Reference), predicting\nimage quality solely by leveraging context from alternate viewpoint captures\n(arXiv:2404.14409). In this work, we propose a new Cross-Reference metric,\nPuzzle Similarity, which is designed to localize artifacts in novel views. Our\napproach utilizes image patch statistics from the input views to establish a\nscene-specific distribution, later used to identify poorly reconstructed\nregions in the novel views. Given the lack of good measures to evaluate\nCross-Reference methods in the context of 3D reconstruction, we collected a\nnovel human-labeled dataset of artifact and distortion maps in unseen\nreconstructed views. Through this dataset, we demonstrate that our method\nachieves state-of-the-art localization of artifacts in novel views, correlating\nwith human assessment, even without aligned references. We can leverage our new\nmetric to enhance applications like automatic image restoration, guided\nacquisition, or 3D reconstruction from sparse inputs. Find the project page at\nhttps://nihermann.github.io/puzzlesim/ .",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG",
      "68T07, 68T45, 68T10",
      "I.4; I.3; I.2"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17489v2",
    "published_date": "2024-11-26 14:57:30 UTC",
    "updated_date": "2025-03-12 09:04:43 UTC"
  },
  {
    "arxiv_id": "2411.17465v1",
    "title": "ShowUI: One Vision-Language-Action Model for GUI Visual Agent",
    "authors": [
      "Kevin Qinghong Lin",
      "Linjie Li",
      "Difei Gao",
      "Zhengyuan Yang",
      "Shiwei Wu",
      "Zechen Bai",
      "Weixian Lei",
      "Lijuan Wang",
      "Mike Zheng Shou"
    ],
    "abstract": "Building Graphical User Interface (GUI) assistants holds significant promise\nfor enhancing human workflow productivity. While most agents are\nlanguage-based, relying on closed-source API with text-rich meta-information\n(e.g., HTML or accessibility tree), they show limitations in perceiving UI\nvisuals as humans do, highlighting the need for GUI visual agents. In this\nwork, we develop a vision-language-action model in digital world, namely\nShowUI, which features the following innovations: (i) UI-Guided Visual Token\nSelection to reduce computational costs by formulating screenshots as an UI\nconnected graph, adaptively identifying their redundant relationship and serve\nas the criteria for token selection during self-attention blocks; (ii)\nInterleaved Vision-Language-Action Streaming that flexibly unifies diverse\nneeds within GUI tasks, enabling effective management of visual-action history\nin navigation or pairing multi-turn query-action sequences per screenshot to\nenhance training efficiency; (iii) Small-scale High-quality GUI\nInstruction-following Datasets by careful data curation and employing a\nresampling strategy to address significant data type imbalances. With above\ncomponents, ShowUI, a lightweight 2B model using 256K data, achieves a strong\n75.1% accuracy in zero-shot screenshot grounding. Its UI-guided token selection\nfurther reduces 33% of redundant visual tokens during training and speeds up\nthe performance by 1.4x. Navigation experiments across web Mind2Web, mobile\nAITW, and online MiniWob environments further underscore the effectiveness and\npotential of our model in advancing GUI visual agents. The models are available\nat https://github.com/showlab/ShowUI.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.CV",
    "comment": "Technical Report. Github: https://github.com/showlab/ShowUI",
    "pdf_url": "http://arxiv.org/pdf/2411.17465v1",
    "published_date": "2024-11-26 14:29:47 UTC",
    "updated_date": "2024-11-26 14:29:47 UTC"
  },
  {
    "arxiv_id": "2411.17461v3",
    "title": "SoK: Decentralized AI (DeAI)",
    "authors": [
      "Zhipeng Wang",
      "Rui Sun",
      "Elizabeth Lui",
      "Vatsal Shah",
      "Xihan Xiong",
      "Jiahao Sun",
      "Davide Crapis",
      "William Knottenbelt"
    ],
    "abstract": "Centralization enhances the efficiency of Artificial Intelligence (AI), but\nit also brings critical challenges, such as single points of failure, inherent\nbiases, data privacy concerns, and scalability issues, for AI systems. These\nproblems are especially common in closed-source large language models (LLMs),\nwhere user data is collected and used with full transparency. To address these\nissues, blockchain-based decentralized AI (DeAI) has been introduced. DeAI\nleverages the strengths of blockchain technologies to enhance the transparency,\nsecurity, decentralization, as well as trustworthiness of AI systems. Although\nDeAI has been widely developed in industry, a comprehensive understanding of\nstate-of-the-art practical DeAI solutions is still lacking. In this work, we\npresent a Systematization of Knowledge (SoK) for blockchain-based DeAI\nsolutions. We propose a taxonomy to classify existing DeAI protocols based on\nthe model lifecycle. Based on this taxonomy, we provide a structured way to\nclarify the landscape of DeAI protocols and identify their similarities and\ndifferences. Specifically, we analyze the functionalities of blockchain in\nDeAI, investigate how blockchain features contribute to enhancing the security,\ntransparency, and trustworthiness of AI processes, and also ensure fair\nincentives for AI data and model contributors. In addition, we provide key\ninsights and research gaps in developing DeAI protocols for future research.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "This is a Systematization of Knowledge (SoK) for the rapidly evolving\n  field of Decentralized AI (DeAI). We welcome valuable comments, suggestions,\n  and collaboration to further refine and enhance this work. We hope our\n  contribution will help accelerate the advancement of DeAI",
    "pdf_url": "http://arxiv.org/pdf/2411.17461v3",
    "published_date": "2024-11-26 14:28:25 UTC",
    "updated_date": "2025-04-16 12:51:11 UTC"
  },
  {
    "arxiv_id": "2411.17459v3",
    "title": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent Video Diffusion Model",
    "authors": [
      "Zongjian Li",
      "Bin Lin",
      "Yang Ye",
      "Liuhan Chen",
      "Xinhua Cheng",
      "Shenghai Yuan",
      "Li Yuan"
    ],
    "abstract": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.17459v3",
    "published_date": "2024-11-26 14:23:53 UTC",
    "updated_date": "2025-04-11 12:31:07 UTC"
  },
  {
    "arxiv_id": "2411.17458v1",
    "title": "Spatially Visual Perception for End-to-End Robotic Learning",
    "authors": [
      "Travis Davies",
      "Jiahuan Yan",
      "Xiang Chen",
      "Yu Tian",
      "Yueting Zhuang",
      "Yiqi Huang",
      "Luhui Hu"
    ],
    "abstract": "Recent advances in imitation learning have shown significant promise for\nrobotic control and embodied intelligence. However, achieving robust\ngeneralization across diverse mounted camera observations remains a critical\nchallenge. In this paper, we introduce a video-based spatial perception\nframework that leverages 3D spatial representations to address environmental\nvariability, with a focus on handling lighting changes. Our approach integrates\na novel image augmentation technique, AugBlender, with a state-of-the-art\nmonocular depth estimation model trained on internet-scale data. Together,\nthese components form a cohesive system designed to enhance robustness and\nadaptability in dynamic scenarios. Our results demonstrate that our approach\nsignificantly boosts the success rate across diverse camera exposures, where\nprevious models experience performance collapse. Our findings highlight the\npotential of video-based spatial perception models in advancing robustness for\nend-to-end robotic learning, paving the way for scalable, low-cost solutions in\nembodied intelligence.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.17458v1",
    "published_date": "2024-11-26 14:23:42 UTC",
    "updated_date": "2024-11-26 14:23:42 UTC"
  },
  {
    "arxiv_id": "2411.17438v2",
    "title": "Object-centric proto-symbolic behavioural reasoning from pixels",
    "authors": [
      "Ruben van Bergen",
      "Justus H√ºbotter",
      "Pablo Lanillos"
    ],
    "abstract": "Autonomous intelligent agents must bridge computational challenges at\ndisparate levels of abstraction, from the low-level spaces of sensory input and\nmotor commands to the high-level domain of abstract reasoning and planning. A\nkey question in designing such agents is how best to instantiate the\nrepresentational space that will interface between these two levels -- ideally\nwithout requiring supervision in the form of expensive data annotations. These\nobjectives can be efficiently achieved by representing the world in terms of\nobjects (grounded in perception and action). In this work, we present a novel,\nbrain-inspired, deep-learning architecture that learns from pixels to\ninterpret, control, and reason about its environment, using object-centric\nrepresentations. We show the utility of our approach through tasks in synthetic\nenvironments that require a combination of (high-level) logical reasoning and\n(low-level) continuous control. Results show that the agent can learn emergent\nconditional behavioural reasoning, such as $(A \\to B) \\land (\\neg A \\to C)$, as\nwell as logical composition $(A \\to B) \\land (A \\to C) \\vdash A \\to (B \\land\nC)$ and XOR operations, and successfully controls its environment to satisfy\nobjectives deduced from these logical rules. The agent can adapt online to\nunexpected changes in its environment and is robust to mild violations of its\nworld model, thanks to dynamic internal desired goal generation. While the\npresent results are limited to synthetic settings (2D and 3D activated versions\nof dSprites), which fall short of real-world levels of complexity, the proposed\narchitecture shows how to manipulate grounded object representations, as a key\ninductive bias for unsupervised learning, to enable behavioral reasoning.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.NE",
      "I.2.0; I.2.6; I.2.10"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17438v2",
    "published_date": "2024-11-26 13:54:24 UTC",
    "updated_date": "2025-02-11 11:10:32 UTC"
  },
  {
    "arxiv_id": "2411.17433v1",
    "title": "LC-SVD-DLinear: A low-cost physics-based hybrid machine learning model for data forecasting using sparse measurements",
    "authors": [
      "Ashton Hetherington",
      "Javier L√≥pez Leon√©s",
      "Soledad Le Clainche"
    ],
    "abstract": "This article introduces a novel methodology that integrates singular value\ndecomposition (SVD) with a shallow linear neural network for forecasting high\nresolution fluid mechanics data. The method, termed LC-SVD-DLinear, combines a\nlow-cost variant of singular value decomposition (LC-SVD) with the DLinear\narchitecture, which decomposes the input features-specifically, the temporal\ncoefficients-into trend and seasonality components, enabling a shallow neural\nnetwork to capture the non-linear dynamics of the temporal data. This\nmethodology uses under-resolved data, which can either be input directly into\nthe hybrid model or downsampled from high resolution using two distinct\ntechniques provided by the methodology. Working with under-resolved cases helps\nreduce the overall computational cost. Additionally, we present a variant of\nthe method, LC-HOSVD-DLinear, which combines a low-cost version of the\nhigh-order singular value decomposition (LC-HOSVD) algorithm with the DLinear\nnetwork, designed for high-order data. These approaches have been validated\nusing two datasets: first, a numerical simulation of three-dimensional flow\npast a circular cylinder at $Re = 220$; and second, an experimental dataset of\nturbulent flow passing a circular cylinder at $Re = 2600$. The combination of\nthese datasets demonstrates the robustness of the method. The forecasting and\nreconstruction results are evaluated through various error metrics, including\nuncertainty quantification. The work developed in this article will be included\nin the next release of ModelFLOWs-app",
    "categories": [
      "physics.flu-dyn",
      "cs.AI"
    ],
    "primary_category": "physics.flu-dyn",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17433v1",
    "published_date": "2024-11-26 13:43:50 UTC",
    "updated_date": "2024-11-26 13:43:50 UTC"
  },
  {
    "arxiv_id": "2411.17429v1",
    "title": "Rewiring Techniques to Mitigate Oversquashing and Oversmoothing in GNNs: A Survey",
    "authors": [
      "Hugo Attali",
      "Davide Buscaldi",
      "Nathalie Pernelle"
    ],
    "abstract": "Graph Neural Networks (GNNs) are powerful tools for learning from\ngraph-structured data, but their effectiveness is often constrained by two\ncritical challenges: oversquashing, where the excessive compression of\ninformation from distant nodes results in significant information loss, and\noversmoothing, where repeated message-passing iterations homogenize node\nrepresentations, obscuring meaningful distinctions. These issues, intrinsically\nlinked to the underlying graph structure, hinder information flow and constrain\nthe expressiveness of GNNs. In this survey, we examine graph rewiring\ntechniques, a class of methods designed to address these structural bottlenecks\nby modifying graph topology to enhance information diffusion. We provide a\ncomprehensive review of state-of-the-art rewiring approaches, delving into\ntheir theoretical underpinnings, practical implementations, and performance\ntrade-offs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17429v1",
    "published_date": "2024-11-26 13:38:12 UTC",
    "updated_date": "2024-11-26 13:38:12 UTC"
  },
  {
    "arxiv_id": "2411.17426v3",
    "title": "CLOVER: Cross-Layer Orthogonal Vectors Pruning and Fine-Tuning",
    "authors": [
      "Fanxu Meng",
      "Pingzhi Tang",
      "Fan jiang",
      "Muhan Zhang"
    ],
    "abstract": "Decoder-only models generate tokens autoregressively by caching key/value\nvectors, but as the cache grows, inference becomes memory-bound. To address\nthis issue, we introduce CLOVER (Cross-Layer Orthogonal Vectors), a novel\napproach that treats pairs of attention layers as a set of low-rank\ndecompositions. CLOVER applies Singular Value Decomposition (SVD) to the \\( Q\n\\)-\\( K \\) and \\( V \\)-\\( O \\) pairs within each attention head. The resulting\nsingular values can either guide pruning or serve as trainable parameters for\nefficient fine-tuning of all orthogonal vectors. After pruning or fine-tuning,\nthese values are reintegrated into the model without increasing its parameter\ncount. We apply CLOVER to various models, including GPT-2 XL, DeepSeek-V2-Lite,\nWhisper-Large-v3, Stable Diffusion XL, and LLaMA-3.2-11B-Vision. Our results\ndemonstrate that CLOVER significantly improves pruning efficiency. For\ninstance, the perplexity of pruning 70\\% of the \\( Q \\)-\\( K \\) pairs in GPT-2\nXL is similar to that of pruning just 8\\% with vanilla methods. Fine-tuning the\nsingular values further results in a full-rank update, outperforming\nstate-of-the-art methods (LoRA, DoRA, HiRA, and PiSSA) by 7.6\\%, 5.5\\%, 3.8\\%,\nand 0.7\\%, respectively, on eight commonsense tasks for LLaMA-2 7B.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "https://github.com/GraphPKU/PiSSA",
    "pdf_url": "http://arxiv.org/pdf/2411.17426v3",
    "published_date": "2024-11-26 13:34:02 UTC",
    "updated_date": "2025-01-31 14:13:49 UTC"
  },
  {
    "arxiv_id": "2411.17404v3",
    "title": "BPP-Search: Enhancing Tree of Thought Reasoning for Mathematical Modeling Problem Solving",
    "authors": [
      "Teng Wang",
      "Wing-Yin Yu",
      "Zhenqi He",
      "Zehua Liu",
      "Hailei Gong",
      "Han Wu",
      "Xiongwei Han",
      "Wei Shi",
      "Ruifeng She",
      "Fangzhou Zhu",
      "Tao Zhong"
    ],
    "abstract": "LLMs exhibit advanced reasoning capabilities, offering the potential to\ntransform natural language questions into mathematical models. However,\nexisting open-source datasets in operations research domain lack detailed\nannotations of the modeling process, such as variable definitions, focusing\nsolely on objective values, which hinders reinforcement learning applications.\nTo address this, we release the StructuredOR dataset, annotated with\ncomprehensive labels that capture the complete mathematical modeling process.\nWe further propose BPP-Search, an algorithm that integrates reinforcement\nlearning into a tree-of-thought structure using Beam search, a Process reward\nmodel, and a pairwise Preference algorithm. This approach enables efficient\nexploration of tree structures, avoiding exhaustive search while improving\naccuracy. Extensive experiments on StructuredOR, NL4OPT, and MAMO-ComplexLP\ndatasets show that BPP-Search significantly outperforms state-of-the-art\nmethods. In tree-based reasoning, BPP-Search excels in accuracy and efficiency,\nenabling faster retrieval of correct solutions. The StructuredOR dataset is\navailable at https://github.com/tengwang0318/StructuredOR.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17404v3",
    "published_date": "2024-11-26 13:05:53 UTC",
    "updated_date": "2025-04-16 16:21:29 UTC"
  },
  {
    "arxiv_id": "2411.17388v3",
    "title": "Can LLMs be Good Graph Judge for Knowledge Graph Construction?",
    "authors": [
      "Haoyu Huang",
      "Chong Chen",
      "Zeang Sheng",
      "Yang Li",
      "Wentao Zhang"
    ],
    "abstract": "In real-world scenarios, most of the data obtained from the information\nretrieval (IR) system is unstructured. Converting natural language sentences\ninto structured Knowledge Graphs (KGs) remains a critical challenge. We\nidentified three limitations with respect to existing KG construction methods:\n(1) There could be a large amount of noise in real-world documents, which could\nresult in extracting messy information. (2) Naive LLMs usually extract\ninaccurate knowledge from some domain-specific documents. (3) Hallucination\nphenomenon cannot be overlooked when directly using LLMs to construct KGs. In\nthis paper, we propose \\textbf{GraphJudge}, a KG construction framework to\naddress the aforementioned challenges. In this framework, we designed an\nentity-centric strategy to eliminate the noise information in the documents.\nAnd we fine-tuned a LLM as a graph judge to finally enhance the quality of\ngenerated KGs. Experiments conducted on two general and one domain-specific\ntext-graph pair datasets demonstrate state-of-the-art performance against\nvarious baseline methods with strong generalization abilities. Our code is\navailable at\n\\href{https://github.com/hhy-huang/GraphJudge}{https://github.com/hhy-huang/GraphJudge}.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17388v3",
    "published_date": "2024-11-26 12:46:57 UTC",
    "updated_date": "2025-05-20 16:24:22 UTC"
  },
  {
    "arxiv_id": "2411.17374v1",
    "title": "Fairness And Performance In Harmony: Data Debiasing Is All You Need",
    "authors": [
      "Junhua Liu",
      "Wendy Wan Yee Hui",
      "Roy Ka-Wei Lee",
      "Kwan Hui Lim"
    ],
    "abstract": "Fairness in both machine learning (ML) predictions and human decisions is\ncritical, with ML models prone to algorithmic and data bias, and human\ndecisions affected by subjectivity and cognitive bias. This study investigates\nfairness using a real-world university admission dataset with 870 profiles,\nleveraging three ML models, namely XGB, Bi-LSTM, and KNN. Textual features are\nencoded with BERT embeddings. For individual fairness, we assess decision\nconsistency among experts with varied backgrounds and ML models, using a\nconsistency score. Results show ML models outperform humans in fairness by\n14.08% to 18.79%. For group fairness, we propose a gender-debiasing pipeline\nand demonstrate its efficacy in removing gender-specific language without\ncompromising prediction performance. Post-debiasing, all models maintain or\nimprove their classification accuracy, validating the hypothesis that fairness\nand performance can coexist. Our findings highlight ML's potential to enhance\nfairness in admissions while maintaining high accuracy, advocating a hybrid\napproach combining human judgement and ML models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17374v1",
    "published_date": "2024-11-26 12:31:10 UTC",
    "updated_date": "2024-11-26 12:31:10 UTC"
  },
  {
    "arxiv_id": "2411.17782v1",
    "title": "Joint Resource Optimization, Computation Offloading and Resource Slicing for Multi-Edge Traffic-Cognitive Networks",
    "authors": [
      "Ting Xiaoyang",
      "Minfeng Zhang",
      "Shu gonglee",
      "Saimin Chen Zhang"
    ],
    "abstract": "The evolving landscape of edge computing envisions platforms operating as\ndynamic intermediaries between application providers and edge servers (ESs),\nwhere task offloading is coupled with payments for computational services.\nEnsuring efficient resource utilization and meeting stringent Quality of\nService (QoS) requirements necessitates incentivizing ESs while optimizing the\nplatforms operational objectives. This paper investigates a multi-agent system\nwhere both the platform and ESs are self-interested entities, addressing the\njoint optimization of revenue maximization, resource allocation, and task\noffloading. We propose a novel Stackelberg game-based framework to model\ninteractions between stakeholders and solve the optimization problem using a\nBayesian Optimization-based centralized algorithm. Recognizing practical\nchallenges in information collection due to privacy concerns, we further design\na decentralized solution leveraging neural network optimization and a\nprivacy-preserving information exchange protocol. Extensive numerical\nevaluations demonstrate the effectiveness of the proposed mechanisms in\nachieving superior performance compared to existing baselines.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17782v1",
    "published_date": "2024-11-26 11:51:10 UTC",
    "updated_date": "2024-11-26 11:51:10 UTC"
  },
  {
    "arxiv_id": "2411.17339v1",
    "title": "Knowledge-aware Evolutionary Graph Neural Architecture Search",
    "authors": [
      "Chao Wang",
      "Jiaxuan Zhao",
      "Lingling Li",
      "Licheng Jiao",
      "Fang Liu",
      "Xu Liu",
      "Shuyuan Yang"
    ],
    "abstract": "Graph neural architecture search (GNAS) can customize high-performance graph\nneural network architectures for specific graph tasks or datasets. However,\nexisting GNAS methods begin searching for architectures from a zero-knowledge\nstate, ignoring the prior knowledge that may improve the search efficiency. The\navailable knowledge base (e.g. NAS-Bench-Graph) contains many rich\narchitectures and their multiple performance metrics, such as the accuracy\n(#Acc) and number of parameters (#Params). This study proposes exploiting such\nprior knowledge to accelerate the multi-objective evolutionary search on a new\ngraph dataset, named knowledge-aware evolutionary GNAS (KEGNAS). KEGNAS employs\nthe knowledge base to train a knowledge model and a deep multi-output Gaussian\nprocess (DMOGP) in one go, which generates and evaluates transfer architectures\nin only a few GPU seconds. The knowledge model first establishes a\ndataset-to-architecture mapping, which can quickly generate candidate transfer\narchitectures for a new dataset. Subsequently, the DMOGP with architecture and\ndataset encodings is designed to predict multiple performance metrics for\ncandidate transfer architectures on the new dataset. According to the predicted\nmetrics, non-dominated candidate transfer architectures are selected to\nwarm-start the multi-objective evolutionary algorithm for optimizing the #Acc\nand #Params on a new dataset. Empirical studies on NAS-Bench-Graph and five\nreal-world datasets show that KEGNAS swiftly generates top-performance\narchitectures, achieving 4.27% higher accuracy than advanced evolutionary\nbaselines and 11.54% higher accuracy than advanced differentiable baselines. In\naddition, ablation studies demonstrate that the use of prior knowledge\nsignificantly improves the search performance.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "This work has been accepted by Knowledge-Based Systems",
    "pdf_url": "http://arxiv.org/pdf/2411.17339v1",
    "published_date": "2024-11-26 11:32:45 UTC",
    "updated_date": "2024-11-26 11:32:45 UTC"
  },
  {
    "arxiv_id": "2411.17338v1",
    "title": "Different Bias Under Different Criteria: Assessing Bias in LLMs with a Fact-Based Approach",
    "authors": [
      "Changgeon Ko",
      "Jisu Shin",
      "Hoyun Song",
      "Jeongyeon Seo",
      "Jong C. Park"
    ],
    "abstract": "Large language models (LLMs) often reflect real-world biases, leading to\nefforts to mitigate these effects and make the models unbiased. Achieving this\ngoal requires defining clear criteria for an unbiased state, with any deviation\nfrom these criteria considered biased. Some studies define an unbiased state as\nequal treatment across diverse demographic groups, aiming for balanced outputs\nfrom LLMs. However, differing perspectives on equality and the importance of\npluralism make it challenging to establish a universal standard. Alternatively,\nother approaches propose using fact-based criteria for more consistent and\nobjective evaluations, though these methods have not yet been fully applied to\nLLM bias assessments. Thus, there is a need for a metric with objective\ncriteria that offers a distinct perspective from equality-based approaches.\nMotivated by this need, we introduce a novel metric to assess bias using\nfact-based criteria and real-world statistics. In this paper, we conducted a\nhuman survey demonstrating that humans tend to perceive LLM outputs more\npositively when they align closely with real-world demographic distributions.\nEvaluating various LLMs with our proposed metric reveals that model bias varies\ndepending on the criteria used, highlighting the need for multi-perspective\nassessment.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted in NeurIPS 2024 Workshop on Socially Responsible Language\n  Modelling Research (SoLaR)",
    "pdf_url": "http://arxiv.org/pdf/2411.17338v1",
    "published_date": "2024-11-26 11:32:43 UTC",
    "updated_date": "2024-11-26 11:32:43 UTC"
  },
  {
    "arxiv_id": "2411.17326v1",
    "title": "Towards Intention Recognition for Robotic Assistants Through Online POMDP Planning",
    "authors": [
      "Juan Carlos Saborio",
      "Joachim Hertzberg"
    ],
    "abstract": "Intention recognition, or the ability to anticipate the actions of another\nagent, plays a vital role in the design and development of automated assistants\nthat can support humans in their daily tasks. In particular, industrial\nsettings pose interesting challenges that include potential distractions for a\ndecision-maker as well as noisy or incomplete observations. In such a setting,\na robotic assistant tasked with helping and supporting a human worker must\ninterleave information gathering actions with proactive tasks of its own, an\napproach that has been referred to as active goal recognition. In this paper we\ndescribe a partially observable model for online intention recognition, show\nsome preliminary experimental results and discuss some of the challenges\npresent in this family of problems.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "Presented at the ICAPS 2023 workshop \"PAIR: Plan, Activity, and\n  Intent Recognition\"",
    "pdf_url": "http://arxiv.org/pdf/2411.17326v1",
    "published_date": "2024-11-26 11:13:00 UTC",
    "updated_date": "2024-11-26 11:13:00 UTC"
  },
  {
    "arxiv_id": "2411.17309v1",
    "title": "PIM-AI: A Novel Architecture for High-Efficiency LLM Inference",
    "authors": [
      "Cristobal Ortega",
      "Yann Falevoz",
      "Renaud Ayrignac"
    ],
    "abstract": "Large Language Models (LLMs) have become essential in a variety of\napplications due to their advanced language understanding and generation\ncapabilities. However, their computational and memory requirements pose\nsignificant challenges to traditional hardware architectures.\nProcessing-in-Memory (PIM), which integrates computational units directly into\nmemory chips, offers several advantages for LLM inference, including reduced\ndata transfer bottlenecks and improved power efficiency.\n  This paper introduces PIM-AI, a novel DDR5/LPDDR5 PIM architecture designed\nfor LLM inference without modifying the memory controller or DDR/LPDDR memory\nPHY. We have developed a simulator to evaluate the performance of PIM-AI in\nvarious scenarios and demonstrate its significant advantages over conventional\narchitectures. In cloud-based scenarios, PIM-AI reduces the 3-year TCO per\nqueries-per-second by up to 6.94x compared to state-of-the-art GPUs, depending\non the LLM model used. In mobile scenarios, PIM-AI achieves a 10- to 20-fold\nreduction in energy per token compared to state-of-the-art mobile SoCs,\nresulting in 25 to 45~\\% more queries per second and 6.9x to 13.4x less energy\nper query, extending battery life and enabling more inferences per charge.\nThese results highlight PIM-AI's potential to revolutionize LLM deployments,\nmaking them more efficient, scalable, and sustainable.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.DC",
      "cs.ET"
    ],
    "primary_category": "cs.AR",
    "comment": "14 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.17309v1",
    "published_date": "2024-11-26 10:54:19 UTC",
    "updated_date": "2024-11-26 10:54:19 UTC"
  },
  {
    "arxiv_id": "2411.17304v1",
    "title": "Meaningless is better: hashing bias-inducing words in LLM prompts improves performance in logical reasoning and statistical learning",
    "authors": [
      "Milena Chadimov√°",
      "Eduard Jur√°≈°ek",
      "Tom√°≈° Kliegr"
    ],
    "abstract": "This paper introduces a novel method, referred to as \"hashing\", which\ninvolves masking potentially bias-inducing words in large language models\n(LLMs) with hash-like meaningless identifiers to reduce cognitive biases and\nreliance on external knowledge. The method was tested across three sets of\nexperiments involving a total of 490 prompts. Statistical analysis using\nchi-square tests showed significant improvements in all tested scenarios, which\ncovered LLama, ChatGPT, Copilot, Gemini and Mixtral models. In the first\nexperiment, hashing decreased the fallacy rate in a modified version of the\n\"Linda\" problem aimed at evaluating susceptibility to cognitive biases. In the\nsecond experiment, it improved LLM results on the frequent itemset extraction\ntask. In the third experiment, we found hashing is also effective when the\nLinda problem is presented in a tabular format rather than text, indicating\nthat the technique works across various input representations. Overall, the\nmethod was shown to improve bias reduction and incorporation of external\nknowledge. Despite bias reduction, hallucination rates were inconsistently\nreduced across types of LLM models. These findings suggest that masking\nbias-inducing terms can improve LLM performance, although its effectiveness is\nmodel- and task-dependent.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17304v1",
    "published_date": "2024-11-26 10:52:08 UTC",
    "updated_date": "2024-11-26 10:52:08 UTC"
  },
  {
    "arxiv_id": "2411.17301v2",
    "title": "ReFINE: A Reward-Based Framework for Interpretable and Nuanced Evaluation of Radiology Report Generation",
    "authors": [
      "Yunyi Liu",
      "Yingshu Li",
      "Zhanyu Wang",
      "Xinyu Liang",
      "Lingqiao Liu",
      "Lei Wang",
      "Luping Zhou"
    ],
    "abstract": "Automated radiology report generation (R2Gen) has advanced significantly,\nintroducing challenges in accurate evaluation due to its complexity.\nTraditional metrics often fall short by relying on rigid word-matching or\nfocusing only on pathological entities, leading to inconsistencies with human\nassessments. To bridge this gap, we introduce ReFINE, an automatic evaluation\nmetric designed specifically for R2Gen. Our metric utilizes a reward model,\nguided by our margin-based reward enforcement loss, along with a tailored\ntraining data design that enables customization of evaluation criteria to suit\nuser-defined needs. It not only scores reports according to user-specified\ncriteria but also provides detailed sub-scores, enhancing interpretability and\nallowing users to adjust the criteria between different aspects of reports.\nLeveraging GPT-4, we designed an easy-to-use data generation pipeline, enabling\nus to produce extensive training data based on two distinct scoring systems,\neach containing reports of varying quality along with corresponding scores.\nThese GPT-generated reports are then paired as accepted and rejected samples\nthrough our pairing rule to train an LLM towards our fine-grained reward model,\nwhich assigns higher rewards to the report with high quality. Our\nreward-control loss enables this model to simultaneously output multiple\nindividual rewards corresponding to the number of evaluation criteria, with\ntheir summation as our final ReFINE. Our experiments demonstrate ReFINE's\nheightened correlation with human judgments and superior performance in model\nselection compared to traditional metrics. Notably, our model provides both an\noverall score and individual scores for each evaluation item, enhancing\ninterpretability. We also demonstrate its flexible training across various\nevaluation systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17301v2",
    "published_date": "2024-11-26 10:48:55 UTC",
    "updated_date": "2025-02-13 12:25:54 UTC"
  },
  {
    "arxiv_id": "2411.17296v2",
    "title": "GrokFormer: Graph Fourier Kolmogorov-Arnold Transformers",
    "authors": [
      "Guoguo Ai",
      "Guansong Pang",
      "Hezhe Qiao",
      "Yuan Gao",
      "Hui Yan"
    ],
    "abstract": "Graph Transformers (GTs) have demonstrated remarkable performance in graph\nrepresentation learning over popular graph neural networks (GNNs). However,\nself--attention, the core module of GTs, preserves only low-frequency signals\nin graph features, leading to ineffectiveness in capturing other important\nsignals like high-frequency ones. Some recent GT models help alleviate this\nissue, but their flexibility and expressiveness are still limited since the\nfilters they learn are fixed on predefined graph spectrum or order. To tackle\nthis challenge, we propose a Graph Fourier Kolmogorov-Arnold Transformer\n(GrokFormer), a novel GT model that learns highly expressive spectral filters\nwith adaptive graph spectrum and order through a Fourier series modeling over\nlearnable activation functions. We demonstrate theoretically and empirically\nthat the proposed GrokFormer filter offers better expressiveness than other\nspectral methods. Comprehensive experiments on 10 real-world node\nclassification datasets across various domains, scales, and graph properties,\nas well as 5 graph classification datasets, show that GrokFormer outperforms\nstate-of-the-art GTs and GNNs. Our code is available at\nhttps://github.com/GGA23/GrokFormer",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "17 pages, 7 figures, 9 tables",
    "pdf_url": "http://arxiv.org/pdf/2411.17296v2",
    "published_date": "2024-11-26 10:38:00 UTC",
    "updated_date": "2025-02-09 06:33:06 UTC"
  },
  {
    "arxiv_id": "2412.07791v1",
    "title": "Digital Democracy in the Age of Artificial Intelligence",
    "authors": [
      "Claudio Novelli",
      "Giulia Sandri"
    ],
    "abstract": "This chapter explores the influence of Artificial Intelligence (AI) on\ndigital democracy, focusing on four main areas: citizenship, participation,\nrepresentation, and the public sphere. It traces the evolution from electronic\nto virtual and network democracy, underscoring how each stage has broadened\ndemocratic engagement through technology. Focusing on digital citizenship, the\nchapter examines how AI can improve online engagement and promote ethical\nbehaviour while posing privacy risks and fostering identity stereotyping.\nRegarding political participation, it highlights AI's dual role in mobilising\ncivic actions and spreading misinformation. Regarding representation, AI's\ninvolvement in electoral processes can enhance voter registration, e-voting,\nand the efficiency of result tabulation but raises concerns regarding privacy\nand public trust. Also, AI's predictive capabilities shift the dynamics of\npolitical competition, posing ethical questions about manipulation and the\nlegitimacy of democracy. Finally, the chapter examines how integrating AI and\ndigital technologies can facilitate democratic political advocacy and\npersonalised communication. However, this also comes with higher risks of\nmisinformation and targeted propaganda.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07791v1",
    "published_date": "2024-11-26 10:20:53 UTC",
    "updated_date": "2024-11-26 10:20:53 UTC"
  },
  {
    "arxiv_id": "2411.17282v1",
    "title": "Social Distancing Induced Coronavirus Optimization Algorithm (COVO): Application to Multimodal Function Optimization and Noise Removal",
    "authors": [
      "Om Ramakisan Varma",
      "Mala Kalra"
    ],
    "abstract": "The metaheuristic optimization technique attained more awareness for handling\ncomplex optimization problems. Over the last few years, numerous optimization\ntechniques have been developed that are inspired by natural phenomena.\nRecently, the propagation of the new COVID-19 implied a burden on the public\nhealth system to suffer several deaths. Vaccination, masks, and social\ndistancing are the major steps taken to minimize the spread of the deadly\nCOVID-19 virus. Considering the social distance to combat the coronavirus\nepidemic, a novel bio-inspired metaheuristic optimization model is proposed in\nthis work, and it is termed as Social Distancing Induced Coronavirus\nOptimization Algorithm (COVO). The pace of propagation of the coronavirus can\nindeed be slowed by maintaining social distance. Thirteen benchmark functions\nare used to evaluate the COVO performance for discrete, continuous, and complex\nproblems, and the COVO model performance is compared with other well-known\noptimization algorithms. The main motive of COVO optimization is to obtain a\nglobal solution to various applications by solving complex problems with faster\nconvergence. At last, the validated results depict that the proposed COVO\noptimization has a reasonable and acceptable performance.",
    "categories": [
      "cs.CC",
      "cs.AI"
    ],
    "primary_category": "cs.CC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17282v1",
    "published_date": "2024-11-26 10:09:36 UTC",
    "updated_date": "2024-11-26 10:09:36 UTC"
  },
  {
    "arxiv_id": "2411.17774v1",
    "title": "Leaning Time-Varying Instruments for Identifying Causal Effects in Time-Series Data",
    "authors": [
      "Debo Cheng",
      "Ziqi Xu",
      "Jiuyong Li",
      "Lin Liu",
      "Thuc duy Le",
      "Xudong Guo",
      "Shichao Zhang"
    ],
    "abstract": "Querying causal effects from time-series data is important across various\nfields, including healthcare, economics, climate science, and epidemiology.\nHowever, this task becomes complex in the existence of time-varying latent\nconfounders, which affect both treatment and outcome variables over time and\ncan introduce bias in causal effect estimation. Traditional instrumental\nvariable (IV) methods are limited in addressing such complexities due to the\nneed for predefined IVs or strong assumptions that do not hold in dynamic\nsettings. To tackle these issues, we develop a novel Time-varying Conditional\nInstrumental Variables (CIV) for Debiasing causal effect estimation, referred\nto as TDCIV. TDCIV leverages Long Short-Term Memory (LSTM) and Variational\nAutoencoder (VAE) models to disentangle and learn the representations of\ntime-varying CIV and its conditioning set from proxy variables without prior\nknowledge. Under the assumptions of the Markov property and availability of\nproxy variables, we theoretically establish the validity of these learned\nrepresentations for addressing the biases from time-varying latent confounders,\nthus enabling accurate causal effect estimation. Our proposed TDCIV is the\nfirst to effectively learn time-varying CIV and its associated conditioning set\nwithout relying on domain-specific knowledge.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages",
    "pdf_url": "http://arxiv.org/pdf/2411.17774v1",
    "published_date": "2024-11-26 09:47:24 UTC",
    "updated_date": "2024-11-26 09:47:24 UTC"
  },
  {
    "arxiv_id": "2411.17261v1",
    "title": "HEIE: MLLM-Based Hierarchical Explainable AIGC Image Implausibility Evaluator",
    "authors": [
      "Fan Yang",
      "Ru Zhen",
      "Jianing Wang",
      "Yanhao Zhang",
      "Haoxiang Chen",
      "Haonan Lu",
      "Sicheng Zhao",
      "Guiguang Ding"
    ],
    "abstract": "AIGC images are prevalent across various fields, yet they frequently suffer\nfrom quality issues like artifacts and unnatural textures. Specialized models\naim to predict defect region heatmaps but face two primary challenges: (1) lack\nof explainability, failing to provide reasons and analyses for subtle defects,\nand (2) inability to leverage common sense and logical reasoning, leading to\npoor generalization. Multimodal large language models (MLLMs) promise better\ncomprehension and reasoning but face their own challenges: (1) difficulty in\nfine-grained defect localization due to the limitations in capturing tiny\ndetails; and (2) constraints in providing pixel-wise outputs necessary for\nprecise heatmap generation. To address these challenges, we propose HEIE: a\nnovel MLLM-Based Hierarchical Explainable image Implausibility Evaluator. We\nintroduce the CoT-Driven Explainable Trinity Evaluator, which integrates\nheatmaps, scores, and explanation outputs, using CoT to decompose complex tasks\ninto subtasks of increasing difficulty and enhance interpretability. Our\nAdaptive Hierarchical Implausibility Mapper synergizes low-level image features\nwith high-level mapper tokens from LLMs, enabling precise local-to-global\nhierarchical heatmap predictions through an uncertainty-based adaptive token\napproach. Moreover, we propose a new dataset: Expl-AIGI-Eval, designed to\nfacilitate interpretable implausibility evaluation of AIGC images. Our method\ndemonstrates state-of-the-art performance through extensive experiments.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17261v1",
    "published_date": "2024-11-26 09:37:59 UTC",
    "updated_date": "2024-11-26 09:37:59 UTC"
  },
  {
    "arxiv_id": "2411.17260v1",
    "title": "MiceBoneChallenge: Micro-CT public dataset and six solutions for automatic growth plate detection in micro-CT mice bone scans",
    "authors": [
      "Nikolay Burlutskiy",
      "Marija Kekic",
      "Jordi de la Torre",
      "Philipp Plewa",
      "Mehdi Boroumand",
      "Julia Jurkowska",
      "Borjan Venovski",
      "Maria Chiara Biagi",
      "Yeman Brhane Hagos",
      "Roksana Malinowska-Traczyk",
      "Yibo Wang",
      "Jacek Zalewski",
      "Paula Sawczuk",
      "Karlo Pintariƒá",
      "Fariba Yousefi",
      "Leif Hultin"
    ],
    "abstract": "Detecting and quantifying bone changes in micro-CT scans of rodents is a\ncommon task in preclinical drug development studies. However, this task is\nmanual, time-consuming and subject to inter- and intra-observer variability. In\n2024, Anonymous Company organized an internal challenge to develop models for\nautomatic bone quantification. We prepared and annotated a high-quality dataset\nof 3D $\\mu$CT bone scans from $83$ mice. The challenge attracted over $80$ AI\nscientists from around the globe who formed $23$ teams. The participants were\ntasked with developing a solution to identify the plane where the bone growth\nhappens, which is essential for fully automatic segmentation of trabecular\nbone. As a result, six computer vision solutions were developed that can\naccurately identify the location of the growth plate plane. The solutions\nachieved the mean absolute error of $1.91\\pm0.87$ planes from the ground truth\non the test set, an accuracy level acceptable for practical use by a\nradiologist. The annotated 3D scans dataset along with the six solutions and\nsource code, is being made public, providing researchers with opportunities to\ndevelop and benchmark their own approaches. The code, trained models, and the\ndata will be shared.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "eess.IV",
    "comment": "Under Review",
    "pdf_url": "http://arxiv.org/pdf/2411.17260v1",
    "published_date": "2024-11-26 09:37:47 UTC",
    "updated_date": "2024-11-26 09:37:47 UTC"
  },
  {
    "arxiv_id": "2411.17257v1",
    "title": "Disentangled Interpretable Representation for Efficient Long-term Time Series Forecasting",
    "authors": [
      "Yuang Zhao",
      "Tianyu Li",
      "Jiadong Chen",
      "Shenrong Ye",
      "Fuxin Jiang",
      "Tieying Zhang",
      "Xiaofeng Gao"
    ],
    "abstract": "Industry 5.0 introduces new challenges for Long-term Time Series Forecasting\n(LTSF), characterized by high-dimensional, high-resolution data and high-stakes\napplication scenarios. Against this backdrop, developing efficient and\ninterpretable models for LTSF becomes a key challenge. Existing deep learning\nand linear models often suffer from excessive parameter complexity and lack\nintuitive interpretability. To address these issues, we propose DiPE-Linear, a\nDisentangled interpretable Parameter-Efficient Linear network. DiPE-Linear\nincorporates three temporal components: Static Frequential Attention (SFA),\nStatic Temporal Attention (STA), and Independent Frequential Mapping (IFM).\nThese components alternate between learning in the frequency and time domains\nto achieve disentangled interpretability. The decomposed model structure\nreduces parameter complexity from quadratic in fully connected networks (FCs)\nto linear and computational complexity from quadratic to log-linear.\nAdditionally, a Low-Rank Weight Sharing policy enhances the model's ability to\nhandle multivariate series. Despite operating within a subspace of FCs with\nlimited expressive capacity, DiPE-Linear demonstrates comparable or superior\nperformance to both FCs and nonlinear models across multiple open-source and\nreal-world LTSF datasets, validating the effectiveness of its sophisticatedly\ndesigned structure. The combination of efficiency, accuracy, and\ninterpretability makes DiPE-Linear a strong candidate for advancing LTSF in\nboth research and real-world applications. The source code is available at\nhttps://github.com/wintertee/DiPE-Linear.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This work is submitted to IEEE International Conference on Data\n  Engineering (ICDE) 2025",
    "pdf_url": "http://arxiv.org/pdf/2411.17257v1",
    "published_date": "2024-11-26 09:33:09 UTC",
    "updated_date": "2024-11-26 09:33:09 UTC"
  },
  {
    "arxiv_id": "2411.17255v2",
    "title": "APT: Architectural Planning and Text-to-Blueprint Construction Using Large Language Models for Open-World Agents",
    "authors": [
      "Jun Yu Chen",
      "Tao Gao"
    ],
    "abstract": "We present APT, an advanced Large Language Model (LLM)-driven framework that\nenables autonomous agents to construct complex and creative structures within\nthe Minecraft environment. Unlike previous approaches that primarily\nconcentrate on skill-based open-world tasks or rely on image-based diffusion\nmodels for generating voxel-based structures, our method leverages the\nintrinsic spatial reasoning capabilities of LLMs. By employing chain-of-thought\ndecomposition along with multimodal inputs, the framework generates detailed\narchitectural layouts and blueprints that the agent can execute under zero-shot\nor few-shot learning scenarios. Our agent incorporates both memory and\nreflection modules to facilitate lifelong learning, adaptive refinement, and\nerror correction throughout the building process. To rigorously evaluate the\nagent's performance in this emerging research area, we introduce a\ncomprehensive benchmark consisting of diverse construction tasks designed to\ntest creativity, spatial reasoning, adherence to in-game rules, and the\neffective integration of multimodal instructions. Experimental results using\nvarious GPT-based LLM backends and agent configurations demonstrate the agent's\ncapacity to accurately interpret extensive instructions involving numerous\nitems, their positions, and orientations. The agent successfully produces\ncomplex structures complete with internal functionalities such as\nRedstone-powered systems. A/B testing indicates that the inclusion of a memory\nmodule leads to a significant increase in performance, emphasizing its role in\nenabling continuous learning and the reuse of accumulated experience.\nAdditionally, the agent's unexpected emergence of scaffolding behavior\nhighlights the potential of future LLM-driven agents to utilize subroutine\nplanning and leverage the emergence ability of LLMs to autonomously develop\nhuman-like problem-solving techniques.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages",
    "pdf_url": "http://arxiv.org/pdf/2411.17255v2",
    "published_date": "2024-11-26 09:31:28 UTC",
    "updated_date": "2024-11-29 23:23:36 UTC"
  },
  {
    "arxiv_id": "2411.17254v1",
    "title": "Semantic Data Augmentation for Long-tailed Facial Expression Recognition",
    "authors": [
      "Zijian Li",
      "Yan Wang",
      "Bowen Guan",
      "JianKai Yin"
    ],
    "abstract": "Facial Expression Recognition has a wide application prospect in social\nrobotics, health care, driver fatigue monitoring, and many other practical\nscenarios. Automatic recognition of facial expressions has been extensively\nstudied by the Computer Vision research society. But Facial Expression\nRecognition in real-world is still a challenging task, partially due to the\nlong-tailed distribution of the dataset. Many recent studies use data\naugmentation for Long-Tailed Recognition tasks. In this paper, we propose a\nnovel semantic augmentation method. By introducing randomness into the encoding\nof the source data in the latent space of VAE-GAN, new samples are generated.\nThen, for facial expression recognition in RAF-DB dataset, we use our\naugmentation method to balance the long-tailed distribution. Our method can be\nused in not only FER tasks, but also more diverse data-hungry scenarios.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17254v1",
    "published_date": "2024-11-26 09:31:12 UTC",
    "updated_date": "2024-11-26 09:31:12 UTC"
  },
  {
    "arxiv_id": "2411.17249v1",
    "title": "Buffer Anytime: Zero-Shot Video Depth and Normal from Image Priors",
    "authors": [
      "Zhengfei Kuang",
      "Tianyuan Zhang",
      "Kai Zhang",
      "Hao Tan",
      "Sai Bi",
      "Yiwei Hu",
      "Zexiang Xu",
      "Milos Hasan",
      "Gordon Wetzstein",
      "Fujun Luan"
    ],
    "abstract": "We present Buffer Anytime, a framework for estimation of depth and normal\nmaps (which we call geometric buffers) from video that eliminates the need for\npaired video--depth and video--normal training data. Instead of relying on\nlarge-scale annotated video datasets, we demonstrate high-quality video buffer\nestimation by leveraging single-image priors with temporal consistency\nconstraints. Our zero-shot training strategy combines state-of-the-art image\nestimation models based on optical flow smoothness through a hybrid loss\nfunction, implemented via a lightweight temporal attention architecture.\nApplied to leading image models like Depth Anything V2 and Marigold-E2E-FT, our\napproach significantly improves temporal consistency while maintaining\naccuracy. Experiments show that our method not only outperforms image-based\napproaches but also achieves results comparable to state-of-the-art video\nmodels trained on large-scale paired video datasets, despite using no such\npaired video data.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17249v1",
    "published_date": "2024-11-26 09:28:32 UTC",
    "updated_date": "2024-11-26 09:28:32 UTC"
  },
  {
    "arxiv_id": "2411.17236v1",
    "title": "From Graph Diffusion to Graph Classification",
    "authors": [
      "Jia Jun Cheng Xian",
      "Sadegh Mahdavi",
      "Renjie Liao",
      "Oliver Schulte"
    ],
    "abstract": "Generative models such as diffusion models have achieved remarkable success\nin state-of-the-art image and text tasks. Recently, score-based diffusion\nmodels have extended their success beyond image generation, showing competitive\nperformance with discriminative methods in image {\\em classification}\ntasks~\\cite{zimmermann2021score}. However, their application to classification\nin the {\\em graph} domain, which presents unique challenges such as complex\ntopologies, remains underexplored. We show how graph diffusion models can be\napplied for graph classification. We find that to achieve competitive\nclassification accuracy, score-based graph diffusion models should be trained\nwith a novel training objective that is tailored to graph classification. In\nexperiments with a sampling-based inference method, our discriminative training\nobjective achieves state-of-the-art graph classification accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17236v1",
    "published_date": "2024-11-26 08:57:41 UTC",
    "updated_date": "2024-11-26 08:57:41 UTC"
  },
  {
    "arxiv_id": "2411.17772v2",
    "title": "MVBoost: Boost 3D Reconstruction with Multi-View Refinement",
    "authors": [
      "Xiangyu Liu",
      "Xiaomei Zhang",
      "Zhiyuan Ma",
      "Xiangyu Zhu",
      "Zhen Lei"
    ],
    "abstract": "Recent advancements in 3D object reconstruction have been remarkable, yet\nmost current 3D models rely heavily on existing 3D datasets. The scarcity of\ndiverse 3D datasets results in limited generalization capabilities of 3D\nreconstruction models. In this paper, we propose a novel framework for boosting\n3D reconstruction with multi-view refinement (MVBoost) by generating pseudo-GT\ndata. The key of MVBoost is combining the advantages of the high accuracy of\nthe multi-view generation model and the consistency of the 3D reconstruction\nmodel to create a reliable data source. Specifically, given a single-view input\nimage, we employ a multi-view diffusion model to generate multiple views,\nfollowed by a large 3D reconstruction model to produce consistent 3D data.\nMVBoost then adaptively refines these multi-view images, rendered from the\nconsistent 3D data, to build a large-scale multi-view dataset for training a\nfeed-forward 3D reconstruction model. Additionally, the input view optimization\nis designed to optimize the corresponding viewpoints based on the user's input\nimage, ensuring that the most important viewpoint is accurately tailored to the\nuser's needs. Extensive evaluations demonstrate that our method achieves\nsuperior reconstruction results and robust generalization compared to prior\nworks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17772v2",
    "published_date": "2024-11-26 08:55:20 UTC",
    "updated_date": "2024-12-02 09:04:20 UTC"
  },
  {
    "arxiv_id": "2412.03587v2",
    "title": "Not All Adapters Matter: Selective Adapter Freezing for Memory-Efficient Fine-Tuning of Language Models",
    "authors": [
      "Hyegang Son",
      "Yonglak Son",
      "Changhoon Kim",
      "Young Geun Kim"
    ],
    "abstract": "Transformer-based large-scale pre-trained models achieve great success.\nFine-tuning is the standard practice for leveraging these models in downstream\ntasks. Among the fine-tuning methods, adapter-tuning provides a\nparameter-efficient fine-tuning by introducing lightweight trainable modules\nwhile keeping most pre-trained parameters frozen. However, existing\nadapter-tuning methods still impose substantial resource usage. Through our\ninvestigation, we show that each adapter unequally contributes to both task\nperformance and resource usage. Motivated by this insight, we propose Selective\nAdapter FrEezing (SAFE), which gradually freezes less important adapters early\nto reduce unnecessary resource usage while maintaining performance. In our\nexperiments, SAFE reduces memory usage, computation amount, and training time\nby 42.85\\%, 34.59\\%, and 11.82\\%, respectively, while achieving comparable or\nbetter task performance compared to the baseline. We also demonstrate that SAFE\ninduces regularization effect, thereby smoothing the loss landscape, which\nenables the model to generalize better by avoiding sharp minima.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "URL: https://aclanthology.org/2025.naacl-long.480/ Volume:\n  Proceedings of the 2025 Conference of the Nations of the Americas Chapter of\n  the Association for Computational Linguistics: Human Language Technologies\n  (Volume 1: Long Papers) Year: 2025 Address: Albuquerque, New Mexico",
    "pdf_url": "http://arxiv.org/pdf/2412.03587v2",
    "published_date": "2024-11-26 08:41:45 UTC",
    "updated_date": "2025-05-15 14:39:45 UTC"
  },
  {
    "arxiv_id": "2411.17218v1",
    "title": "GraphSubDetector: Time Series Subsequence Anomaly Detection via Density-Aware Adaptive Graph Neural Network",
    "authors": [
      "Weiqi Chen",
      "Zhiqiang Zhou",
      "Qingsong Wen",
      "Liang Sun"
    ],
    "abstract": "Time series subsequence anomaly detection is an important task in a large\nvariety of real-world applications ranging from health monitoring to AIOps, and\nis challenging due to the following reasons: 1) how to effectively learn\ncomplex dynamics and dependencies in time series; 2) diverse and complicated\nanomalous subsequences as well as the inherent variance and noise of normal\npatterns; 3) how to determine the proper subsequence length for effective\ndetection, which is a required parameter for many existing algorithms. In this\npaper, we present a novel approach to subsequence anomaly detection, namely\nGraphSubDetector. First, it adaptively learns the appropriate subsequence\nlength with a length selection mechanism that highlights the characteristics of\nboth normal and anomalous patterns. Second, we propose a density-aware adaptive\ngraph neural network (DAGNN), which can generate further robust representations\nagainst variance of normal data for anomaly detection by message passing\nbetween subsequences. The experimental results demonstrate the effectiveness of\nthe proposed algorithm, which achieves superior performance on multiple time\nseries anomaly benchmark datasets compared to state-of-the-art algorithms.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17218v1",
    "published_date": "2024-11-26 08:36:07 UTC",
    "updated_date": "2024-11-26 08:36:07 UTC"
  },
  {
    "arxiv_id": "2411.17204v2",
    "title": "Strategic Prompting for Conversational Tasks: A Comparative Analysis of Large Language Models Across Diverse Conversational Tasks",
    "authors": [
      "Ratnesh Kumar Joshi",
      "Priyanshu Priya",
      "Vishesh Desai",
      "Saurav Dudhate",
      "Siddhant Senapati",
      "Asif Ekbal",
      "Roshni Ramnani",
      "Anutosh Maitra",
      "Shubhashis Sengupta"
    ],
    "abstract": "Given the advancements in conversational artificial intelligence, the\nevaluation and assessment of Large Language Models (LLMs) play a crucial role\nin ensuring optimal performance across various conversational tasks. In this\npaper, we present a comprehensive study that thoroughly evaluates the\ncapabilities and limitations of five prevalent LLMs: Llama, OPT, Falcon,\nAlpaca, and MPT. The study encompasses various conversational tasks, including\nreservation, empathetic response generation, mental health and legal\ncounseling, persuasion, and negotiation. To conduct the evaluation, an\nextensive test setup is employed, utilizing multiple evaluation criteria that\nspan from automatic to human evaluation. This includes using generic and\ntask-specific metrics to gauge the LMs' performance accurately. From our\nevaluation, no single model emerges as universally optimal for all tasks.\nInstead, their performance varies significantly depending on the specific\nrequirements of each task. While some models excel in certain tasks, they may\ndemonstrate comparatively poorer performance in others. These findings\nemphasize the importance of considering task-specific requirements and\ncharacteristics when selecting the most suitable LM for conversational\napplications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "39 pages, 12 tables",
    "pdf_url": "http://arxiv.org/pdf/2411.17204v2",
    "published_date": "2024-11-26 08:21:24 UTC",
    "updated_date": "2024-11-28 01:04:40 UTC"
  },
  {
    "arxiv_id": "2412.04492v1",
    "title": "Socio-Emotional Response Generation: A Human Evaluation Protocol for LLM-Based Conversational Systems",
    "authors": [
      "Lorraine Vanel",
      "Ariel R. Ramos Vela",
      "Alya Yacoubi",
      "Chlo√© Clavel"
    ],
    "abstract": "Conversational systems are now capable of producing impressive and generally\nrelevant responses. However, we have no visibility nor control of the\nsocio-emotional strategies behind state-of-the-art Large Language Models\n(LLMs), which poses a problem in terms of their transparency and thus their\ntrustworthiness for critical applications. Another issue is that current\nautomated metrics are not able to properly evaluate the quality of generated\nresponses beyond the dataset's ground truth. In this paper, we propose a neural\narchitecture that includes an intermediate step in planning socio-emotional\nstrategies before response generation. We compare the performance of\nopen-source baseline LLMs to the outputs of these same models augmented with\nour planning module. We also contrast the outputs obtained from automated\nmetrics and evaluation results provided by human annotators. We describe a\nnovel evaluation protocol that includes a coarse-grained consistency\nevaluation, as well as a finer-grained annotation of the responses on various\nsocial and emotional criteria. Our study shows that predicting a sequence of\nexpected strategy labels and using this sequence to generate a response yields\nbetter results than a direct end-to-end generation scheme. It also highlights\nthe divergences and the limits of current evaluation metrics for generated\ncontent. The code for the annotation platform and the annotated data are made\npublicly available for the evaluation of future models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.SI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04492v1",
    "published_date": "2024-11-26 08:15:36 UTC",
    "updated_date": "2024-11-26 08:15:36 UTC"
  },
  {
    "arxiv_id": "2411.17201v1",
    "title": "Learning Hierarchical Polynomials of Multiple Nonlinear Features with Three-Layer Networks",
    "authors": [
      "Hengyu Fu",
      "Zihao Wang",
      "Eshaan Nichani",
      "Jason D. Lee"
    ],
    "abstract": "In deep learning theory, a critical question is to understand how neural\nnetworks learn hierarchical features. In this work, we study the learning of\nhierarchical polynomials of \\textit{multiple nonlinear features} using\nthree-layer neural networks. We examine a broad class of functions of the form\n$f^{\\star}=g^{\\star}\\circ \\bp$, where $\\bp:\\mathbb{R}^{d} \\rightarrow\n\\mathbb{R}^{r}$ represents multiple quadratic features with $r \\ll d$ and\n$g^{\\star}:\\mathbb{R}^{r}\\rightarrow \\mathbb{R}$ is a polynomial of degree $p$.\nThis can be viewed as a nonlinear generalization of the multi-index model\n\\citep{damian2022neural}, and also an expansion upon previous work that focused\nonly on a single nonlinear feature, i.e. $r = 1$\n\\citep{nichani2023provable,wang2023learning}.\n  Our primary contribution shows that a three-layer neural network trained via\nlayerwise gradient descent suffices for\n  \\begin{itemize}\\item complete recovery of the space spanned by the nonlinear\nfeatures\n  \\item efficient learning of the target function $f^{\\star}=g^{\\star}\\circ\n\\bp$ or transfer learning of $f=g\\circ \\bp$ with a different link function\n  \\end{itemize} within $\\widetilde{\\cO}(d^4)$ samples and polynomial time. For\nsuch hierarchical targets, our result substantially improves the sample\ncomplexity ${\\Theta}(d^{2p})$ of the kernel methods, demonstrating the power of\nefficient feature learning. It is important to highlight that{ our results\nleverage novel techniques and thus manage to go beyond all prior settings} such\nas single-index and multi-index models as well as models depending just on one\nnonlinear feature, contributing to a more comprehensive understanding of\nfeature learning in deep learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "primary_category": "cs.LG",
    "comment": "78 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.17201v1",
    "published_date": "2024-11-26 08:14:48 UTC",
    "updated_date": "2024-11-26 08:14:48 UTC"
  },
  {
    "arxiv_id": "2411.17176v1",
    "title": "ChatGen: Automatic Text-to-Image Generation From FreeStyle Chatting",
    "authors": [
      "Chengyou Jia",
      "Changliang Xia",
      "Zhuohang Dang",
      "Weijia Wu",
      "Hangwei Qian",
      "Minnan Luo"
    ],
    "abstract": "Despite the significant advancements in text-to-image (T2I) generative\nmodels, users often face a trial-and-error challenge in practical scenarios.\nThis challenge arises from the complexity and uncertainty of tedious steps such\nas crafting suitable prompts, selecting appropriate models, and configuring\nspecific arguments, making users resort to labor-intensive attempts for desired\nimages. This paper proposes Automatic T2I generation, which aims to automate\nthese tedious steps, allowing users to simply describe their needs in a\nfreestyle chatting way. To systematically study this problem, we first\nintroduce ChatGenBench, a novel benchmark designed for Automatic T2I. It\nfeatures high-quality paired data with diverse freestyle inputs, enabling\ncomprehensive evaluation of automatic T2I models across all steps.\nAdditionally, recognizing Automatic T2I as a complex multi-step reasoning task,\nwe propose ChatGen-Evo, a multi-stage evolution strategy that progressively\nequips models with essential automation skills. Through extensive evaluation\nacross step-wise accuracy and image quality, ChatGen-Evo significantly enhances\nperformance over various baselines. Our evaluation also uncovers valuable\ninsights for advancing automatic T2I. All our data, code, and models will be\navailable in \\url{https://chengyou-jia.github.io/ChatGen-Home}",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17176v1",
    "published_date": "2024-11-26 07:31:12 UTC",
    "updated_date": "2024-11-26 07:31:12 UTC"
  },
  {
    "arxiv_id": "2411.17170v1",
    "title": "Learning Monotonic Attention in Transducer for Streaming Generation",
    "authors": [
      "Zhengrui Ma",
      "Yang Feng",
      "Min Zhang"
    ],
    "abstract": "Streaming generation models are increasingly utilized across various fields,\nwith the Transducer architecture being particularly popular in industrial\napplications. However, its input-synchronous decoding mechanism presents\nchallenges in tasks requiring non-monotonic alignments, such as simultaneous\ntranslation, leading to suboptimal performance in these contexts. In this\nresearch, we address this issue by tightly integrating Transducer's decoding\nwith the history of input stream via a learnable monotonic attention mechanism.\nOur approach leverages the forward-backward algorithm to infer the posterior\nprobability of alignments between the predictor states and input timestamps,\nwhich is then used to estimate the context representations of monotonic\nattention in training. This allows Transducer models to adaptively adjust the\nscope of attention based on their predictions, avoiding the need to enumerate\nthe exponentially large alignment space. Extensive experiments demonstrate that\nour MonoAttn-Transducer significantly enhances the handling of non-monotonic\nalignments in streaming generation, offering a robust solution for\nTransducer-based frameworks to tackle more complex streaming generation tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Codes: https://github.com/ictnlp/MonoAttn-Transducer",
    "pdf_url": "http://arxiv.org/pdf/2411.17170v1",
    "published_date": "2024-11-26 07:19:26 UTC",
    "updated_date": "2024-11-26 07:19:26 UTC"
  },
  {
    "arxiv_id": "2412.00073v1",
    "title": "Addressing Vulnerabilities in AI-Image Detection: Challenges and Proposed Solutions",
    "authors": [
      "Justin Jiang"
    ],
    "abstract": "The rise of advanced AI models like Generative Adversarial Networks (GANs)\nand diffusion models such as Stable Diffusion has made the creation of highly\nrealistic images accessible, posing risks of misuse in misinformation and\nmanipulation. This study evaluates the effectiveness of convolutional neural\nnetworks (CNNs), as well as DenseNet architectures, for detecting AI-generated\nimages. Using variations of the CIFAKE dataset, including images generated by\ndifferent versions of Stable Diffusion, we analyze the impact of updates and\nmodifications such as Gaussian blurring, prompt text changes, and Low-Rank\nAdaptation (LoRA) on detection accuracy. The findings highlight vulnerabilities\nin current detection methods and propose strategies to enhance the robustness\nand reliability of AI-image detection systems.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00073v1",
    "published_date": "2024-11-26 06:35:26 UTC",
    "updated_date": "2024-11-26 06:35:26 UTC"
  },
  {
    "arxiv_id": "2411.17137v1",
    "title": "Self-reconfiguration Strategies for Space-distributed Spacecraft",
    "authors": [
      "Tianle Liu",
      "Zhixiang Wang",
      "Yongwei Zhang",
      "Ziwei Wang",
      "Zihao Liu",
      "Yizhai Zhang",
      "Panfeng Huang"
    ],
    "abstract": "This paper proposes a distributed on-orbit spacecraft assembly algorithm,\nwhere future spacecraft can assemble modules with different functions on orbit\nto form a spacecraft structure with specific functions. This form of spacecraft\norganization has the advantages of reconfigurability, fast mission response and\neasy maintenance. Reasonable and efficient on-orbit self-reconfiguration\nalgorithms play a crucial role in realizing the benefits of distributed\nspacecraft. This paper adopts the framework of imitation learning combined with\nreinforcement learning for strategy learning of module handling order. A robot\narm motion algorithm is then designed to execute the handling sequence. We\nachieve the self-reconfiguration handling task by creating a map on the surface\nof the module, completing the path point planning of the robotic arm using A*.\nThe joint planning of the robotic arm is then accomplished through forward and\nreverse kinematics. Finally, the results are presented in Unity3D.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17137v1",
    "published_date": "2024-11-26 06:05:44 UTC",
    "updated_date": "2024-11-26 06:05:44 UTC"
  },
  {
    "arxiv_id": "2411.17135v1",
    "title": "LLM-Based Offline Learning for Embodied Agents via Consistency-Guided Reward Ensemble",
    "authors": [
      "Yujeong Lee",
      "Sangwoo Shin",
      "Wei-Jin Park",
      "Honguk Woo"
    ],
    "abstract": "Employing large language models (LLMs) to enable embodied agents has become\npopular, yet it presents several limitations in practice. In this work, rather\nthan using LLMs directly as agents, we explore their use as tools for embodied\nagent learning. Specifically, to train separate agents via offline\nreinforcement learning (RL), an LLM is used to provide dense reward feedback on\nindividual actions in training datasets. In doing so, we present a\nconsistency-guided reward ensemble framework (CoREN), designed for tackling\ndifficulties in grounding LLM-generated estimates to the target environment\ndomain. The framework employs an adaptive ensemble of spatio-temporally\nconsistent rewards to derive domain-grounded rewards in the training datasets,\nthus enabling effective offline learning of embodied agents in different\nenvironment domains. Experiments with the VirtualHome benchmark demonstrate\nthat CoREN significantly outperforms other offline RL agents, and it also\nachieves comparable performance to state-of-the-art LLM-based agents with 8B\nparameters, despite CoREN having only 117M parameters for the agent policy\nnetwork and using LLMs only for training.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Findings of EMNLP-2024 Camera Ready Version",
    "pdf_url": "http://arxiv.org/pdf/2411.17135v1",
    "published_date": "2024-11-26 06:04:10 UTC",
    "updated_date": "2024-11-26 06:04:10 UTC"
  },
  {
    "arxiv_id": "2411.17125v1",
    "title": "DOGE: Towards Versatile Visual Document Grounding and Referring",
    "authors": [
      "Yinan Zhou",
      "Yuxin Chen",
      "Haokun Lin",
      "Shuyu Yang",
      "Li Zhu",
      "Zhongang Qi",
      "Chen Ma",
      "Ying Shan"
    ],
    "abstract": "In recent years, Multimodal Large Language Models (MLLMs) have increasingly\nemphasized grounding and referring capabilities to achieve detailed\nunderstanding and flexible user interaction. However, in the realm of visual\ndocument understanding, these capabilities lag behind due to the scarcity of\nfine-grained datasets and comprehensive benchmarks. To fill this gap, we\npropose the DOcument Grounding and Eferring data engine (DOGE-Engine), which\nproduces two types of high-quality fine-grained document data: multi-granular\nparsing data for enhancing fundamental text localization and recognition\ncapabilities; and instruction-tuning data to activate MLLM's grounding and\nreferring capabilities during dialogue and reasoning. Additionally, using our\nengine, we construct DOGE-Bench, which encompasses 7 grounding and referring\ntasks across 3 document types (chart, poster, PDF document), providing\ncomprehensive evaluations for fine-grained document understanding. Furthermore,\nleveraging the data generated by our engine, we develop a strong baseline\nmodel, DOGE. This pioneering MLLM is capable of accurately referring and\ngrounding texts at multiple granularities within document images. Our code,\ndata, and model will be open-sourced for community development.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "20 pages, 13 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.17125v1",
    "published_date": "2024-11-26 05:38:34 UTC",
    "updated_date": "2024-11-26 05:38:34 UTC"
  },
  {
    "arxiv_id": "2411.17123v1",
    "title": "Advancing Content Moderation: Evaluating Large Language Models for Detecting Sensitive Content Across Text, Images, and Videos",
    "authors": [
      "Nouar AlDahoul",
      "Myles Joshua Toledo Tan",
      "Harishwar Reddy Kasireddy",
      "Yasir Zaki"
    ],
    "abstract": "The widespread dissemination of hate speech, harassment, harmful and sexual\ncontent, and violence across websites and media platforms presents substantial\nchallenges and provokes widespread concern among different sectors of society.\nGovernments, educators, and parents are often at odds with media platforms\nabout how to regulate, control, and limit the spread of such content.\nTechnologies for detecting and censoring the media contents are a key solution\nto addressing these challenges. Techniques from natural language processing and\ncomputer vision have been used widely to automatically identify and filter out\nsensitive content such as offensive languages, violence, nudity, and addiction\nin both text, images, and videos, enabling platforms to enforce content\npolicies at scale. However, existing methods still have limitations in\nachieving high detection accuracy with fewer false positives and false\nnegatives. Therefore, more sophisticated algorithms for understanding the\ncontext of both text and image may open rooms for improvement in content\ncensorship to build a more efficient censorship system. In this paper, we\nevaluate existing LLM-based content moderation solutions such as OpenAI\nmoderation model and Llama-Guard3 and study their capabilities to detect\nsensitive contents. Additionally, we explore recent LLMs such as GPT, Gemini,\nand Llama in identifying inappropriate contents across media outlets. Various\ntextual and visual datasets like X tweets, Amazon reviews, news articles, human\nphotos, cartoons, sketches, and violence videos have been utilized for\nevaluation and comparison. The results demonstrate that LLMs outperform\ntraditional techniques by achieving higher accuracy and lower false positive\nand false negative rates. This highlights the potential to integrate LLMs into\nwebsites, social media platforms, and video-sharing services for regulatory and\ncontent moderation purposes.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "55 pages, 16 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.17123v1",
    "published_date": "2024-11-26 05:29:18 UTC",
    "updated_date": "2024-11-26 05:29:18 UTC"
  },
  {
    "arxiv_id": "2411.17116v2",
    "title": "Star Attention: Efficient LLM Inference over Long Sequences",
    "authors": [
      "Shantanu Acharya",
      "Fei Jia",
      "Boris Ginsburg"
    ],
    "abstract": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n97-100% of accuracy.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Code: https://github.com/NVIDIA/Star-Attention",
    "pdf_url": "http://arxiv.org/pdf/2411.17116v2",
    "published_date": "2024-11-26 05:10:04 UTC",
    "updated_date": "2025-04-20 21:50:03 UTC"
  },
  {
    "arxiv_id": "2411.17764v1",
    "title": "PROGRESSOR: A Perceptually Guided Reward Estimator with Self-Supervised Online Refinement",
    "authors": [
      "Tewodros Ayalew",
      "Xiao Zhang",
      "Kevin Yuanbo Wu",
      "Tianchong Jiang",
      "Michael Maire",
      "Matthew R. Walter"
    ],
    "abstract": "We present PROGRESSOR, a novel framework that learns a task-agnostic reward\nfunction from videos, enabling policy training through goal-conditioned\nreinforcement learning (RL) without manual supervision. Underlying this reward\nis an estimate of the distribution over task progress as a function of the\ncurrent, initial, and goal observations that is learned in a self-supervised\nfashion. Crucially, PROGRESSOR refines rewards adversarially during online RL\ntraining by pushing back predictions for out-of-distribution observations, to\nmitigate distribution shift inherent in non-expert observations. Utilizing this\nprogress prediction as a dense reward together with an adversarial push-back,\nwe show that PROGRESSOR enables robots to learn complex behaviors without any\nexternal supervision. Pretrained on large-scale egocentric human video from\nEPIC-KITCHENS, PROGRESSOR requires no fine-tuning on in-domain task-specific\ndata for generalization to real-robot offline RL under noisy demonstrations,\noutperforming contemporary methods that provide dense visual reward for robotic\nlearning. Our findings highlight the potential of PROGRESSOR for scalable\nrobotic applications where direct action labels and task-specific rewards are\nnot readily available.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "15 pages,13 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.17764v1",
    "published_date": "2024-11-26 04:17:51 UTC",
    "updated_date": "2024-11-26 04:17:51 UTC"
  },
  {
    "arxiv_id": "2412.00071v2",
    "title": "COAP: Memory-Efficient Training with Correlation-Aware Gradient Projection",
    "authors": [
      "Jinqi Xiao",
      "Shen Sang",
      "Tiancheng Zhi",
      "Jing Liu",
      "Qing Yan",
      "Yuqian Zhang",
      "Linjie Luo",
      "Bo Yuan"
    ],
    "abstract": "Training large-scale neural networks in vision, and multimodal domains\ndemands substantial memory resources, primarily due to the storage of optimizer\nstates. While LoRA, a popular parameter-efficient method, reduces memory usage,\nit often suffers from suboptimal performance due to the constraints of low-rank\nupdates. Low-rank gradient projection methods (e.g., GaLore, Flora) reduce\noptimizer memory by projecting gradients and moment estimates into low-rank\nspaces via singular value decomposition or random projection. However, they\nfail to account for inter-projection correlation, causing performance\ndegradation, and their projection strategies often incur high computational\ncosts. In this paper, we present COAP (Correlation-Aware Gradient Projection),\na memory-efficient method that minimizes computational overhead while\nmaintaining training performance. Evaluated across various vision, language,\nand multimodal tasks, COAP outperforms existing methods in both training speed\nand model performance. For LLaMA-1B, it reduces optimizer memory by 61% with\nonly 2% additional time cost, achieving the same PPL as AdamW. With 8-bit\nquantization, COAP cuts optimizer memory by 81% and achieves 4x speedup over\nGaLore for LLaVA-v1.5-7B fine-tuning, while delivering higher accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.00071v2",
    "published_date": "2024-11-26 03:50:52 UTC",
    "updated_date": "2025-03-12 00:36:08 UTC"
  },
  {
    "arxiv_id": "2411.17080v2",
    "title": "DeepMDV: Learning Global Matching for Multi-depot Vehicle Routing Problems",
    "authors": [
      "Saeed Nasehi",
      "Farhana Choudhury",
      "Egemen Tanin"
    ],
    "abstract": "Due to the substantial rise in online retail and e-commerce in recent years,\nthe demand for efficient and fast solutions to Vehicle Routing Problems (VRP)\nhas become critical. To manage the increasing demand, companies have adopted\nthe strategy of adding more depots. However, the presence of multiple depots\nintroduces additional complexities, making existing VRP solutions suboptimal\nfor addressing the Multi-depot Vehicle Routing Problem (MDVRP). Traditional\nmethods for solving the MDVRP often require significant computation time,\nmaking them unsuitable for large-scale instances. Additionally, existing\nlearning-based solutions for the MDVRP struggle with generalizability and fail\nto deliver high-quality results for scenarios involving a large number of\ncustomers. In this paper, we propose a novel solution for MDVRP. Our approach\nemploys an attention mechanism, featuring a decoder with two key layers: one\nlayer to consider the states of all vehicles and learn to select the most\nsuitable vehicle based on the proximity of unassigned customers, and another\nlayer to focus on assigning a customer to the selected vehicle. This approach\ndelivers high-quality solutions for large-scale MDVRP instances and\ndemonstrates remarkable generalizability across varying numbers of customers\nand depots. Its adaptability and performance make it a practical and deployable\nsolution for real-world logistics challenges.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17080v2",
    "published_date": "2024-11-26 03:41:01 UTC",
    "updated_date": "2024-12-10 03:19:23 UTC"
  },
  {
    "arxiv_id": "2411.17077v1",
    "title": "Contrastive CFG: Improving CFG in Diffusion Models by Contrasting Positive and Negative Concepts",
    "authors": [
      "Jinho Chang",
      "Hyungjin Chung",
      "Jong Chul Ye"
    ],
    "abstract": "As Classifier-Free Guidance (CFG) has proven effective in conditional\ndiffusion model sampling for improved condition alignment, many applications\nuse a negated CFG term to filter out unwanted features from samples. However,\nsimply negating CFG guidance creates an inverted probability distribution,\noften distorting samples away from the marginal distribution. Inspired by\nrecent advances in conditional diffusion models for inverse problems, here we\npresent a novel method to enhance negative CFG guidance using contrastive loss.\nSpecifically, our guidance term aligns or repels the denoising direction based\non the given condition through contrastive loss, achieving a nearly identical\nguiding direction to traditional CFG for positive guidance while overcoming the\nlimitations of existing negative guidance methods. Experimental results\ndemonstrate that our approach effectively removes undesirable concepts while\nmaintaining sample quality across diverse scenarios, from simple class\nconditions to complex and overlapping text prompts.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.17077v1",
    "published_date": "2024-11-26 03:29:27 UTC",
    "updated_date": "2024-11-26 03:29:27 UTC"
  },
  {
    "arxiv_id": "2411.17073v1",
    "title": "Path-RAG: Knowledge-Guided Key Region Retrieval for Open-ended Pathology Visual Question Answering",
    "authors": [
      "Awais Naeem",
      "Tianhao Li",
      "Huang-Ru Liao",
      "Jiawei Xu",
      "Aby M. Mathew",
      "Zehao Zhu",
      "Zhen Tan",
      "Ajay Kumar Jaiswal",
      "Raffi A. Salibian",
      "Ziniu Hu",
      "Tianlong Chen",
      "Ying Ding"
    ],
    "abstract": "Accurate diagnosis and prognosis assisted by pathology images are essential\nfor cancer treatment selection and planning. Despite the recent trend of\nadopting deep-learning approaches for analyzing complex pathology images, they\nfall short as they often overlook the domain-expert understanding of tissue\nstructure and cell composition. In this work, we focus on a challenging\nOpen-ended Pathology VQA (PathVQA-Open) task and propose a novel framework\nnamed Path-RAG, which leverages HistoCartography to retrieve relevant domain\nknowledge from pathology images and significantly improves performance on\nPathVQA-Open. Admitting the complexity of pathology image analysis, Path-RAG\nadopts a human-centered AI approach by retrieving domain knowledge using\nHistoCartography to select the relevant patches from pathology images. Our\nexperiments suggest that domain guidance can significantly boost the accuracy\nof LLaVA-Med from 38% to 47%, with a notable gain of 28% for H&E-stained\npathology images in the PathVQA-Open dataset. For longer-form question and\nanswer pairs, our model consistently achieves significant improvements of 32.5%\nin ARCH-Open PubMed and 30.6% in ARCH-Open Books on H\\&E images. Our code and\ndataset is available here (https://github.com/embedded-robotics/path-rag).",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17073v1",
    "published_date": "2024-11-26 03:22:01 UTC",
    "updated_date": "2024-11-26 03:22:01 UTC"
  },
  {
    "arxiv_id": "2412.00070v1",
    "title": "Recurrent Stochastic Configuration Networks with Hybrid Regularization for Nonlinear Dynamics Modelling",
    "authors": [
      "Gang Dang",
      "Dianhui Wang"
    ],
    "abstract": "Recurrent stochastic configuration networks (RSCNs) have shown great\npotential in modelling nonlinear dynamic systems with uncertainties. This paper\npresents an RSCN with hybrid regularization to enhance both the learning\ncapacity and generalization performance of the network. Given a set of temporal\ndata, the well-known least absolute shrinkage and selection operator (LASSO) is\nemployed to identify the significant order variables. Subsequently, an improved\nRSCN with L2 regularization is introduced to approximate the residuals between\nthe output of the target plant and the LASSO model. The output weights are\nupdated in real-time through a projection algorithm, facilitating a rapid\nresponse to dynamic changes within the system. A theoretical analysis of the\nuniversal approximation property is provided, contributing to the understanding\nof the network's effectiveness in representing various complex nonlinear\nfunctions. Experimental results from a nonlinear system identification problem\nand two industrial predictive tasks demonstrate that the proposed method\noutperforms other models across all testing datasets.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00070v1",
    "published_date": "2024-11-26 03:06:39 UTC",
    "updated_date": "2024-11-26 03:06:39 UTC"
  },
  {
    "arxiv_id": "2411.17065v1",
    "title": "Creative Agents: Simulating the Systems Model of Creativity with Generative Agents",
    "authors": [
      "Naomi Imasato",
      "Kazuki Miyazawa",
      "Takayuki Nagai",
      "Takato Horii"
    ],
    "abstract": "With the growing popularity of generative AI for images, video, and music, we\nwitnessed models rapidly improve in quality and performance. However, not much\nattention is paid towards enabling AI's ability to \"be creative\". In this\nstudy, we implemented and simulated the systems model of creativity (proposed\nby Csikszentmihalyi) using virtual agents utilizing large language models\n(LLMs) and text prompts. For comparison, the simulations were conducted with\nthe \"virtual artists\" being: 1)isolated and 2)placed in a multi-agent system.\nBoth scenarios were compared by analyzing the variations and overall\n\"creativity\" in the generated artifacts (measured via a user study and LLM).\nOur results suggest that the generative agents may perform better in the\nframework of the systems model of creativity.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17065v1",
    "published_date": "2024-11-26 03:06:04 UTC",
    "updated_date": "2024-11-26 03:06:04 UTC"
  },
  {
    "arxiv_id": "2411.17062v1",
    "title": "Graph Structure Learning with Bi-level Optimization",
    "authors": [
      "Nan Yin"
    ],
    "abstract": "Currently, most Graph Structure Learning (GSL) methods, as a means of\nlearning graph structure, improve the robustness of GNN merely from a local\nview by considering the local information related to each edge and\nindiscriminately applying the mechanism across edges, which may suffer from the\nlocal structure heterogeneity of the graph (\\ie the uneven distribution of\ninter-class connections over nodes). To overcome the cons, we extract the graph\nstructure as a learnable parameter and jointly learn the structure and common\nparameters of GNN from the global view. Excitingly, the common parameters\ncontain the global information for nodes features mapping, which is also\ncrucial for structure optimization (\\ie optimizing the structure relies on\nglobal mapping information). Mathematically, we apply a generic structure\nextractor to abstract the graph structure and transform GNNs in the form of\nlearning structure and common parameters. Then, we model the learning process\nas a novel bi-level optimization, \\ie \\textit{Generic Structure Extraction with\nBi-level Optimization for Graph Structure Learning (GSEBO)}, which optimizes\nGNN parameters in the upper level to obtain the global mapping information and\ngraph structure is optimized in the lower level with the global information\nlearned from the upper level. We instantiate the proposed GSEBO on classical\nGNNs and compare it with the state-of-the-art GSL methods. Extensive\nexperiments validate the effectiveness of the proposed GSEBO on four real-world\ndatasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17062v1",
    "published_date": "2024-11-26 03:00:30 UTC",
    "updated_date": "2024-11-26 03:00:30 UTC"
  },
  {
    "arxiv_id": "2411.17058v2",
    "title": "ThreatModeling-LLM: Automating Threat Modeling using Large Language Models for Banking System",
    "authors": [
      "Tingmin Wu",
      "Shuiqiao Yang",
      "Shigang Liu",
      "David Nguyen",
      "Seung Jang",
      "Alsharif Abuadbba"
    ],
    "abstract": "Threat modeling is a crucial component of cybersecurity, particularly for\nindustries such as banking, where the security of financial data is paramount.\nTraditional threat modeling approaches require expert intervention and manual\neffort, often leading to inefficiencies and human error. The advent of Large\nLanguage Models (LLMs) offers a promising avenue for automating these\nprocesses, enhancing both efficiency and efficacy. However, this transition is\nnot straightforward due to three main challenges: (1) the lack of publicly\navailable, domain-specific datasets, (2) the need for tailored models to handle\ncomplex banking system architectures, and (3) the requirement for real-time,\nadaptive mitigation strategies that align with compliance standards like NIST\n800-53. In this paper, we introduce ThreatModeling-LLM, a novel and adaptable\nframework that automates threat modeling for banking systems using LLMs.\nThreatModeling-LLM operates in three stages: 1) dataset creation, 2) prompt\nengineering and 3) model fine-tuning. We first generate a benchmark dataset\nusing Microsoft Threat Modeling Tool (TMT). Then, we apply Chain of Thought\n(CoT) and Optimization by PROmpting (OPRO) on the pre-trained LLMs to optimize\nthe initial prompt. Lastly, we fine-tune the LLM using Low-Rank Adaptation\n(LoRA) based on the benchmark dataset and the optimized prompt to improve the\nthreat identification and mitigation generation capabilities of pre-trained\nLLMs.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17058v2",
    "published_date": "2024-11-26 02:57:28 UTC",
    "updated_date": "2025-05-14 11:00:39 UTC"
  },
  {
    "arxiv_id": "2411.17041v1",
    "title": "Free$^2$Guide: Gradient-Free Path Integral Control for Enhancing Text-to-Video Generation with Large Vision-Language Models",
    "authors": [
      "Jaemin Kim",
      "Bryan S Kim",
      "Jong Chul Ye"
    ],
    "abstract": "Diffusion models have achieved impressive results in generative tasks like\ntext-to-image (T2I) and text-to-video (T2V) synthesis. However, achieving\naccurate text alignment in T2V generation remains challenging due to the\ncomplex temporal dependency across frames. Existing reinforcement learning\n(RL)-based approaches to enhance text alignment often require differentiable\nreward functions or are constrained to limited prompts, hindering their\nscalability and applicability. In this paper, we propose Free$^2$Guide, a novel\ngradient-free framework for aligning generated videos with text prompts without\nrequiring additional model training. Leveraging principles from path integral\ncontrol, Free$^2$Guide approximates guidance for diffusion models using\nnon-differentiable reward functions, thereby enabling the integration of\npowerful black-box Large Vision-Language Models (LVLMs) as reward model.\nAdditionally, our framework supports the flexible ensembling of multiple reward\nmodels, including large-scale image-based models, to synergistically enhance\nalignment without incurring substantial computational overhead. We demonstrate\nthat Free$^2$Guide significantly improves text alignment across various\ndimensions and enhances the overall quality of generated videos.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "15 pages",
    "pdf_url": "http://arxiv.org/pdf/2411.17041v1",
    "published_date": "2024-11-26 02:14:47 UTC",
    "updated_date": "2024-11-26 02:14:47 UTC"
  },
  {
    "arxiv_id": "2411.17030v1",
    "title": "g3D-LF: Generalizable 3D-Language Feature Fields for Embodied Tasks",
    "authors": [
      "Zihan Wang",
      "Gim Hee Lee"
    ],
    "abstract": "We introduce Generalizable 3D-Language Feature Fields (g3D-LF), a 3D\nrepresentation model pre-trained on large-scale 3D-language dataset for\nembodied tasks. Our g3D-LF processes posed RGB-D images from agents to encode\nfeature fields for: 1) Novel view representation predictions from any position\nin the 3D scene; 2) Generations of BEV maps centered on the agent; 3) Querying\ntargets using multi-granularity language within the above-mentioned\nrepresentations. Our representation can be generalized to unseen environments,\nenabling real-time construction and dynamic updates. By volume rendering latent\nfeatures along sampled rays and integrating semantic and spatial relationships\nthrough multiscale encoders, our g3D-LF produces representations at different\nscales and perspectives, aligned with multi-granularity language, via\nmulti-level contrastive learning. Furthermore, we prepare a large-scale\n3D-language dataset to align the representations of the feature fields with\nlanguage. Extensive experiments on Vision-and-Language Navigation under both\nPanorama and Monocular settings, Zero-shot Object Navigation, and Situated\nQuestion Answering tasks highlight the significant advantages and effectiveness\nof our g3D-LF for embodied tasks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17030v1",
    "published_date": "2024-11-26 01:54:52 UTC",
    "updated_date": "2024-11-26 01:54:52 UTC"
  },
  {
    "arxiv_id": "2411.17760v1",
    "title": "Efficient Self-Improvement in Multimodal Large Language Models: A Model-Level Judge-Free Approach",
    "authors": [
      "Shijian Deng",
      "Wentian Zhao",
      "Yu-Jhe Li",
      "Kun Wan",
      "Daniel Miranda",
      "Ajinkya Kale",
      "Yapeng Tian"
    ],
    "abstract": "Self-improvement in multimodal large language models (MLLMs) is crucial for\nenhancing their reliability and robustness. However, current methods often rely\nheavily on MLLMs themselves as judges, leading to high computational costs and\npotential pitfalls like reward hacking and model collapse. This paper\nintroduces a novel, model-level judge-free self-improvement framework. Our\napproach employs a controlled feedback mechanism while eliminating the need for\nMLLMs in the verification loop. We generate preference learning pairs using a\ncontrollable hallucination mechanism and optimize data quality by leveraging\nlightweight, contrastive language-image encoders to evaluate and reverse pairs\nwhen necessary. Evaluations across public benchmarks and our newly introduced\nIC dataset designed to challenge hallucination control demonstrate that our\nmodel outperforms conventional techniques. We achieve superior precision and\nrecall with significantly lower computational demands. This method offers an\nefficient pathway to scalable self-improvement in MLLMs, balancing performance\ngains with reduced resource requirements.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17760v1",
    "published_date": "2024-11-26 00:44:37 UTC",
    "updated_date": "2024-11-26 00:44:37 UTC"
  },
  {
    "arxiv_id": "2411.17003v1",
    "title": "Can a Single Tree Outperform an Entire Forest?",
    "authors": [
      "Qiangqiang Mao",
      "Yankai Cao"
    ],
    "abstract": "The prevailing mindset is that a single decision tree underperforms classic\nrandom forests in testing accuracy, despite its advantages in interpretability\nand lightweight structure. This study challenges such a mindset by\nsignificantly improving the testing accuracy of an oblique regression tree\nthrough our gradient-based entire tree optimization framework, making its\nperformance comparable to the classic random forest. Our approach reformulates\ntree training as a differentiable unconstrained optimization task, employing a\nscaled sigmoid approximation strategy. To ameliorate numerical instability, we\npropose an algorithmic scheme that solves a sequence of increasingly accurate\napproximations. Additionally, a subtree polish strategy is implemented to\nreduce approximation errors accumulated across the tree. Extensive experiments\non 16 datasets demonstrate that our optimized tree outperforms the classic\nrandom forest by an average of $2.03\\%$ improvements in testing accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17003v1",
    "published_date": "2024-11-26 00:18:18 UTC",
    "updated_date": "2024-11-26 00:18:18 UTC"
  },
  {
    "arxiv_id": "2411.17000v1",
    "title": "SatVision-TOA: A Geospatial Foundation Model for Coarse-Resolution All-Sky Remote Sensing Imagery",
    "authors": [
      "Caleb S. Spradlin",
      "Jordan A. Caraballo-Vega",
      "Jian Li",
      "Mark L. Carroll",
      "Jie Gong",
      "Paul M. Montesano"
    ],
    "abstract": "Foundation models have the potential to transform the landscape of remote\nsensing (RS) data analysis by enabling large computer vision models to be\npre-trained on vast amounts of remote sensing data. These models can then be\nfine-tuned with small amounts of labeled training and applied to a variety of\napplications. Most existing foundation models are designed for high spatial\nresolution, cloud-free satellite imagery or photos, limiting their\napplicability in scenarios that require frequent temporal monitoring or broad\nspectral profiles. As a result, foundation models trained solely on cloud-free\nimages have limited utility for applications that involve atmospheric variables\nor require atmospheric corrections. We introduce SatVision-TOA, a novel\nfoundation model pre-trained on 14-band MODIS L1B Top-Of-Atmosphere (TOA)\nradiance imagery, addressing the need for models pre-trained to handle\nmoderate- and coarse-resolution all-sky remote sensing data. The SatVision-TOA\nmodel is pre-trained using a Masked-Image-Modeling (MIM) framework and the\nSwinV2 architecture, and learns detailed contextual representations through\nself-supervised learning without the need for labels. It is a 3 billion\nparameter model that is trained on 100 million images. To our knowledge this is\nthe largest foundation model trained solely on satellite RS imagery. Results\nshow that SatVision-TOA achieves superior performance over baseline methods on\ndownstream tasks such as 3D cloud retrieval. Notably, the model achieves a mean\nintersection over union (mIOU) of 0.46, a substantial improvement over the\nbaseline mIOU of 0.22. Additionally, the rate of false negative results in the\nfine-tuning task were reduced by over 50% compared to the baseline. Our work\nadvances pre-trained vision modeling for multispectral RS by learning from a\nvariety of atmospheric and aerosol conditions to improve cloud and land surface\nmonitoring.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "19 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.17000v1",
    "published_date": "2024-11-26 00:08:00 UTC",
    "updated_date": "2024-11-26 00:08:00 UTC"
  },
  {
    "arxiv_id": "2412.06808v2",
    "title": "Effect of Adaptive Communication Support on LLM-powered Human-Robot Collaboration",
    "authors": [
      "Shipeng Liu",
      "FNU Shrutika",
      "Boshen Zhang",
      "Zhehui Huang",
      "Gaurav Sukhatme",
      "Feifei Qian"
    ],
    "abstract": "Effective human-robot collaboration requires robot to adopt their roles and\nlevels of support based on human needs, task requirements, and complexity.\nTraditional human-robot teaming often relies on a pre-determined robot\ncommunication scheme, restricting teamwork adaptability in complex tasks.\nLeveraging strong communication capabilities of Large Language Models (LLMs),\nwe propose a Human-Robot Teaming Framework with Multi-Modal Language feedback\n(HRT-ML), a framework designed to enhance human-robot interaction by adjusting\nthe frequency and content of language-based feedback. HRT-ML framework includes\ntwo core modules: a Coordinator for high-level, low-frequency strategic\nguidance, and a Manager for subtask-specific, high-frequency instructions,\nenabling passive and active interactions with human teammates. To assess the\nimpact of language feedback in collaborative scenarios, we conducted\nexperiments in an enhanced Overcooked environment with varying levels of task\ncomplexity (easy, medium, hard) and feedback frequency (inactive, passive,\nactive, superactive). Our results show that as task complexity increases\nrelative to human capabilities, human teammates exhibited a stronger preference\ntowards robotic agents that can offer frequent, proactive support. However,\nwhen task complexities exceed the LLM's capacity, noisy and inaccurate feedback\nfrom superactive robotic agents can instead hinder team performance, as it\nrequires human teammates to increase their effort to interpret and respond to a\nlarge number of communications, with limited performance return. Our results\noffer a general principle for robotic agents to dynamically adjust their levels\nand frequencies of communications to work seamlessly with humans and achieve\nimproved teaming performance.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.RO",
      "68T05",
      "I.2.9"
    ],
    "primary_category": "cs.HC",
    "comment": "13 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.06808v2",
    "published_date": "2024-11-26 00:06:47 UTC",
    "updated_date": "2025-02-11 18:52:51 UTC"
  }
]